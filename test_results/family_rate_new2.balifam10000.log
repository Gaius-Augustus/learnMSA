Training of 5 independent models on file PF00079.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd4f7992c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd32c7980a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd508904af0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 33s - loss: 773.8801 - loglik: -7.7235e+02 - logprior: -1.5262e+00
Epoch 2/10
39/39 - 31s - loss: 647.8681 - loglik: -6.4599e+02 - logprior: -1.8794e+00
Epoch 3/10
39/39 - 31s - loss: 638.9866 - loglik: -6.3718e+02 - logprior: -1.8016e+00
Epoch 4/10
39/39 - 32s - loss: 636.0287 - loglik: -6.3426e+02 - logprior: -1.7673e+00
Epoch 5/10
39/39 - 33s - loss: 635.6878 - loglik: -6.3390e+02 - logprior: -1.7894e+00
Epoch 6/10
39/39 - 34s - loss: 634.6660 - loglik: -6.3284e+02 - logprior: -1.8291e+00
Epoch 7/10
39/39 - 35s - loss: 634.8810 - loglik: -6.3302e+02 - logprior: -1.8574e+00
Fitted a model with MAP estimate = -633.9751
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (16, 1), (52, 5), (62, 1), (63, 1), (64, 1), (66, 1), (68, 1), (69, 1), (78, 1), (79, 1), (80, 1), (84, 1), (85, 1), (86, 1), (90, 1), (91, 1), (95, 1), (98, 1), (99, 1), (100, 1), (113, 1), (118, 1), (120, 1), (122, 1), (135, 1), (140, 1), (143, 1), (144, 1), (159, 1), (160, 1), (164, 1), (165, 1), (167, 1), (168, 1), (174, 1), (185, 1), (186, 1), (189, 1), (193, 1), (195, 1), (196, 1), (197, 2), (198, 1), (199, 2), (200, 1), (214, 1), (215, 1), (217, 1), (218, 1), (220, 1), (229, 1), (235, 2), (236, 2), (238, 1), (263, 1), (264, 2), (265, 1), (266, 3), (267, 2), (268, 1), (277, 1), (279, 1), (280, 3)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 369 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 57s - loss: 622.4079 - loglik: -6.2075e+02 - logprior: -1.6542e+00
Epoch 2/2
39/39 - 57s - loss: 610.5676 - loglik: -6.0998e+02 - logprior: -5.8422e-01
Fitted a model with MAP estimate = -607.6773
expansions: []
discards: [250 296]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 64s - loss: 614.0824 - loglik: -6.1274e+02 - logprior: -1.3403e+00
Epoch 2/2
39/39 - 64s - loss: 609.8626 - loglik: -6.0962e+02 - logprior: -2.4306e-01
Fitted a model with MAP estimate = -607.5443
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 367 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 68s - loss: 613.4866 - loglik: -6.1230e+02 - logprior: -1.1870e+00
Epoch 2/10
39/39 - 67s - loss: 609.8259 - loglik: -6.0976e+02 - logprior: -6.2191e-02
Epoch 3/10
39/39 - 66s - loss: 607.3040 - loglik: -6.0732e+02 - logprior: 0.0165
Epoch 4/10
39/39 - 62s - loss: 605.9448 - loglik: -6.0600e+02 - logprior: 0.0551
Epoch 5/10
39/39 - 61s - loss: 606.1735 - loglik: -6.0632e+02 - logprior: 0.1484
Fitted a model with MAP estimate = -605.0693
Time for alignment: 1024.7130
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 771.1282 - loglik: -7.6959e+02 - logprior: -1.5339e+00
Epoch 2/10
39/39 - 41s - loss: 645.8843 - loglik: -6.4386e+02 - logprior: -2.0238e+00
Epoch 3/10
39/39 - 41s - loss: 636.0172 - loglik: -6.3404e+02 - logprior: -1.9760e+00
Epoch 4/10
39/39 - 40s - loss: 634.3146 - loglik: -6.3242e+02 - logprior: -1.8972e+00
Epoch 5/10
39/39 - 39s - loss: 633.0204 - loglik: -6.3112e+02 - logprior: -1.8958e+00
Epoch 6/10
39/39 - 39s - loss: 632.6760 - loglik: -6.3075e+02 - logprior: -1.9219e+00
Epoch 7/10
39/39 - 39s - loss: 632.8810 - loglik: -6.3096e+02 - logprior: -1.9200e+00
Fitted a model with MAP estimate = -631.9063
expansions: [(9, 1), (12, 1), (13, 2), (14, 1), (45, 1), (51, 3), (55, 1), (62, 1), (63, 2), (64, 1), (67, 1), (68, 1), (77, 1), (78, 1), (83, 1), (84, 1), (85, 1), (86, 1), (90, 1), (91, 1), (94, 1), (97, 1), (99, 1), (102, 1), (106, 1), (117, 1), (119, 1), (121, 1), (134, 1), (139, 1), (141, 1), (142, 1), (145, 1), (158, 1), (159, 1), (160, 1), (162, 1), (163, 1), (165, 1), (166, 1), (185, 2), (189, 1), (192, 1), (194, 1), (195, 1), (196, 1), (198, 1), (199, 2), (200, 1), (214, 1), (215, 1), (218, 1), (220, 1), (224, 1), (229, 1), (235, 2), (236, 2), (238, 1), (242, 1), (262, 1), (263, 1), (264, 1), (265, 3), (267, 1), (268, 1), (277, 1), (278, 2), (279, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 368 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 68s - loss: 621.6761 - loglik: -6.2008e+02 - logprior: -1.6001e+00
Epoch 2/2
39/39 - 67s - loss: 610.2183 - loglik: -6.0966e+02 - logprior: -5.5466e-01
Fitted a model with MAP estimate = -607.1927
expansions: []
discards: [250 296]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 75s - loss: 613.7134 - loglik: -6.1242e+02 - logprior: -1.2974e+00
Epoch 2/2
39/39 - 72s - loss: 609.7945 - loglik: -6.0959e+02 - logprior: -2.0649e-01
Fitted a model with MAP estimate = -607.3417
expansions: []
discards: [56]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 75s - loss: 613.1907 - loglik: -6.1204e+02 - logprior: -1.1532e+00
Epoch 2/10
39/39 - 75s - loss: 609.3376 - loglik: -6.0927e+02 - logprior: -6.6708e-02
Epoch 3/10
39/39 - 76s - loss: 607.3118 - loglik: -6.0730e+02 - logprior: -8.9816e-03
Epoch 4/10
39/39 - 74s - loss: 605.7474 - loglik: -6.0582e+02 - logprior: 0.0739
Epoch 5/10
39/39 - 77s - loss: 605.4496 - loglik: -6.0564e+02 - logprior: 0.1947
Epoch 6/10
39/39 - 74s - loss: 605.7259 - loglik: -6.0596e+02 - logprior: 0.2332
Fitted a model with MAP estimate = -604.6520
Time for alignment: 1290.4301
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 769.9271 - loglik: -7.6839e+02 - logprior: -1.5324e+00
Epoch 2/10
39/39 - 42s - loss: 648.6084 - loglik: -6.4676e+02 - logprior: -1.8532e+00
Epoch 3/10
39/39 - 38s - loss: 639.2253 - loglik: -6.3747e+02 - logprior: -1.7540e+00
Epoch 4/10
39/39 - 37s - loss: 636.1216 - loglik: -6.3440e+02 - logprior: -1.7185e+00
Epoch 5/10
39/39 - 37s - loss: 635.3132 - loglik: -6.3355e+02 - logprior: -1.7613e+00
Epoch 6/10
39/39 - 37s - loss: 635.3344 - loglik: -6.3356e+02 - logprior: -1.7790e+00
Fitted a model with MAP estimate = -634.2071
expansions: [(13, 1), (14, 2), (15, 1), (16, 1), (46, 1), (48, 1), (50, 3), (61, 1), (62, 1), (63, 1), (65, 1), (67, 1), (68, 1), (77, 1), (78, 1), (81, 1), (83, 1), (84, 1), (85, 1), (90, 1), (91, 1), (98, 1), (99, 1), (100, 1), (113, 1), (116, 2), (117, 1), (120, 1), (121, 1), (135, 1), (140, 1), (143, 1), (146, 1), (159, 1), (160, 1), (161, 1), (164, 1), (167, 1), (168, 1), (174, 1), (185, 1), (186, 1), (189, 1), (193, 1), (195, 1), (196, 1), (197, 2), (198, 1), (199, 2), (200, 1), (214, 1), (215, 1), (217, 1), (218, 1), (220, 1), (229, 1), (235, 1), (236, 2), (262, 1), (263, 1), (264, 1), (265, 1), (266, 3), (267, 3), (268, 1), (279, 2), (280, 3)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 369 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 61s - loss: 620.9478 - loglik: -6.1938e+02 - logprior: -1.5630e+00
Epoch 2/2
39/39 - 65s - loss: 609.0826 - loglik: -6.0867e+02 - logprior: -4.1320e-01
Fitted a model with MAP estimate = -606.4183
expansions: []
discards: [145 251 338 339]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 74s - loss: 614.2271 - loglik: -6.1292e+02 - logprior: -1.3119e+00
Epoch 2/2
39/39 - 73s - loss: 609.7945 - loglik: -6.0958e+02 - logprior: -2.1653e-01
Fitted a model with MAP estimate = -607.6473
expansions: []
discards: [335]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 78s - loss: 613.5074 - loglik: -6.1234e+02 - logprior: -1.1703e+00
Epoch 2/10
39/39 - 74s - loss: 609.8102 - loglik: -6.0975e+02 - logprior: -6.4129e-02
Epoch 3/10
39/39 - 74s - loss: 607.6149 - loglik: -6.0761e+02 - logprior: -3.0912e-04
Epoch 4/10
39/39 - 74s - loss: 606.1015 - loglik: -6.0620e+02 - logprior: 0.0973
Epoch 5/10
39/39 - 65s - loss: 605.9686 - loglik: -6.0615e+02 - logprior: 0.1855
Epoch 6/10
39/39 - 57s - loss: 605.1410 - loglik: -6.0539e+02 - logprior: 0.2464
Epoch 7/10
39/39 - 54s - loss: 605.9240 - loglik: -6.0621e+02 - logprior: 0.2877
Fitted a model with MAP estimate = -604.9793
Time for alignment: 1251.3117
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 40s - loss: 768.6425 - loglik: -7.6709e+02 - logprior: -1.5493e+00
Epoch 2/10
39/39 - 37s - loss: 648.4005 - loglik: -6.4651e+02 - logprior: -1.8861e+00
Epoch 3/10
39/39 - 38s - loss: 639.5179 - loglik: -6.3769e+02 - logprior: -1.8313e+00
Epoch 4/10
39/39 - 37s - loss: 636.8882 - loglik: -6.3510e+02 - logprior: -1.7848e+00
Epoch 5/10
39/39 - 38s - loss: 636.5210 - loglik: -6.3473e+02 - logprior: -1.7892e+00
Epoch 6/10
39/39 - 38s - loss: 636.0710 - loglik: -6.3427e+02 - logprior: -1.7976e+00
Epoch 7/10
39/39 - 38s - loss: 635.6646 - loglik: -6.3387e+02 - logprior: -1.7911e+00
Epoch 8/10
39/39 - 38s - loss: 635.7740 - loglik: -6.3397e+02 - logprior: -1.8053e+00
Fitted a model with MAP estimate = -635.2075
expansions: [(12, 2), (13, 1), (14, 1), (46, 1), (49, 1), (51, 3), (62, 1), (63, 1), (64, 1), (66, 1), (68, 1), (69, 1), (78, 1), (79, 1), (80, 1), (84, 1), (85, 1), (86, 1), (88, 1), (90, 1), (91, 1), (98, 1), (99, 1), (100, 1), (113, 1), (114, 1), (115, 1), (117, 1), (119, 1), (121, 1), (134, 1), (139, 1), (142, 1), (143, 1), (158, 1), (159, 1), (163, 1), (165, 2), (166, 1), (174, 1), (185, 1), (188, 1), (192, 1), (194, 1), (195, 1), (196, 1), (198, 1), (199, 2), (200, 1), (214, 1), (215, 1), (217, 1), (218, 1), (220, 1), (229, 1), (235, 2), (236, 2), (262, 1), (263, 1), (264, 1), (265, 1), (266, 3), (267, 3), (268, 1), (279, 2), (280, 3)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 368 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 58s - loss: 622.5466 - loglik: -6.2098e+02 - logprior: -1.5711e+00
Epoch 2/2
39/39 - 54s - loss: 610.6640 - loglik: -6.1018e+02 - logprior: -4.8706e-01
Fitted a model with MAP estimate = -607.7764
expansions: [(203, 1)]
discards: [249 295 337 338]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 57s - loss: 614.0134 - loglik: -6.1273e+02 - logprior: -1.2789e+00
Epoch 2/2
39/39 - 55s - loss: 610.4174 - loglik: -6.1020e+02 - logprior: -2.1474e-01
Fitted a model with MAP estimate = -607.6651
expansions: []
discards: [335]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 58s - loss: 613.6005 - loglik: -6.1234e+02 - logprior: -1.2648e+00
Epoch 2/10
39/39 - 59s - loss: 609.7203 - loglik: -6.0972e+02 - logprior: 2.1868e-04
Epoch 3/10
39/39 - 62s - loss: 608.1093 - loglik: -6.0813e+02 - logprior: 0.0192
Epoch 4/10
39/39 - 65s - loss: 605.9142 - loglik: -6.0602e+02 - logprior: 0.1104
Epoch 5/10
39/39 - 64s - loss: 606.2461 - loglik: -6.0647e+02 - logprior: 0.2258
Fitted a model with MAP estimate = -605.2583
Time for alignment: 1045.3239
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 771.4881 - loglik: -7.6996e+02 - logprior: -1.5291e+00
Epoch 2/10
39/39 - 43s - loss: 647.9135 - loglik: -6.4601e+02 - logprior: -1.9016e+00
Epoch 3/10
39/39 - 45s - loss: 638.3033 - loglik: -6.3645e+02 - logprior: -1.8492e+00
Epoch 4/10
39/39 - 45s - loss: 636.9048 - loglik: -6.3511e+02 - logprior: -1.7906e+00
Epoch 5/10
39/39 - 45s - loss: 635.5978 - loglik: -6.3381e+02 - logprior: -1.7915e+00
Epoch 6/10
39/39 - 45s - loss: 635.5787 - loglik: -6.3378e+02 - logprior: -1.8026e+00
Epoch 7/10
39/39 - 45s - loss: 635.3198 - loglik: -6.3351e+02 - logprior: -1.8133e+00
Epoch 8/10
39/39 - 45s - loss: 634.8524 - loglik: -6.3304e+02 - logprior: -1.8172e+00
Epoch 9/10
39/39 - 45s - loss: 635.1382 - loglik: -6.3329e+02 - logprior: -1.8499e+00
Fitted a model with MAP estimate = -634.3750
expansions: [(4, 1), (12, 1), (13, 1), (14, 1), (34, 1), (46, 1), (51, 3), (62, 1), (63, 1), (64, 1), (66, 1), (68, 1), (69, 1), (78, 1), (79, 1), (82, 1), (84, 1), (85, 1), (86, 1), (90, 1), (91, 1), (95, 1), (98, 1), (99, 1), (100, 1), (113, 1), (118, 1), (119, 1), (120, 1), (121, 1), (135, 1), (140, 1), (143, 1), (159, 1), (160, 1), (164, 1), (165, 1), (167, 1), (168, 1), (176, 1), (187, 1), (190, 1), (194, 1), (196, 1), (197, 1), (198, 2), (199, 1), (200, 2), (201, 1), (215, 1), (216, 1), (218, 1), (219, 1), (221, 1), (230, 1), (236, 2), (237, 2), (242, 1), (243, 1), (263, 1), (264, 1), (265, 1), (266, 3), (267, 3), (268, 1), (279, 2), (280, 3)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 368 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 62s - loss: 622.3310 - loglik: -6.2072e+02 - logprior: -1.6128e+00
Epoch 2/2
39/39 - 57s - loss: 610.5759 - loglik: -6.1004e+02 - logprior: -5.3112e-01
Fitted a model with MAP estimate = -607.7293
expansions: []
discards: [249 295 337 338]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 59s - loss: 614.4139 - loglik: -6.1310e+02 - logprior: -1.3125e+00
Epoch 2/2
39/39 - 57s - loss: 610.5822 - loglik: -6.1041e+02 - logprior: -1.7478e-01
Fitted a model with MAP estimate = -608.1200
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 364 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 60s - loss: 613.8511 - loglik: -6.1270e+02 - logprior: -1.1527e+00
Epoch 2/10
39/39 - 58s - loss: 610.2786 - loglik: -6.1025e+02 - logprior: -2.7319e-02
Epoch 3/10
39/39 - 60s - loss: 607.4771 - loglik: -6.0749e+02 - logprior: 0.0122
Epoch 4/10
39/39 - 60s - loss: 607.1581 - loglik: -6.0726e+02 - logprior: 0.0995
Epoch 5/10
39/39 - 60s - loss: 605.9169 - loglik: -6.0607e+02 - logprior: 0.1519
Epoch 6/10
39/39 - 63s - loss: 605.7272 - loglik: -6.0595e+02 - logprior: 0.2226
Epoch 7/10
39/39 - 66s - loss: 605.8972 - loglik: -6.0624e+02 - logprior: 0.3391
Fitted a model with MAP estimate = -605.0775
Time for alignment: 1295.4373
Computed alignments with likelihoods: ['-605.0693', '-604.6520', '-604.9793', '-605.2583', '-605.0775']
Best model has likelihood: -604.6520  (prior= 0.3309 )
time for generating output: 0.2919
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00079.projection.fasta
SP score = 0.920479302832244
Training of 5 independent models on file PF01381.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd384daeb80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd349c724c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd384895c70>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 150.1285 - loglik: -1.4692e+02 - logprior: -3.2060e+00
Epoch 2/10
19/19 - 1s - loss: 127.9382 - loglik: -1.2657e+02 - logprior: -1.3703e+00
Epoch 3/10
19/19 - 1s - loss: 118.8225 - loglik: -1.1723e+02 - logprior: -1.5911e+00
Epoch 4/10
19/19 - 1s - loss: 117.4990 - loglik: -1.1602e+02 - logprior: -1.4766e+00
Epoch 5/10
19/19 - 1s - loss: 117.1613 - loglik: -1.1571e+02 - logprior: -1.4529e+00
Epoch 6/10
19/19 - 1s - loss: 116.8535 - loglik: -1.1542e+02 - logprior: -1.4304e+00
Epoch 7/10
19/19 - 1s - loss: 116.8116 - loglik: -1.1539e+02 - logprior: -1.4170e+00
Epoch 8/10
19/19 - 1s - loss: 116.6826 - loglik: -1.1527e+02 - logprior: -1.4108e+00
Epoch 9/10
19/19 - 1s - loss: 116.8621 - loglik: -1.1546e+02 - logprior: -1.4068e+00
Fitted a model with MAP estimate = -116.4937
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (19, 2), (20, 1), (27, 1), (28, 2), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 121.9355 - loglik: -1.1784e+02 - logprior: -4.0995e+00
Epoch 2/2
19/19 - 1s - loss: 114.1480 - loglik: -1.1216e+02 - logprior: -1.9897e+00
Fitted a model with MAP estimate = -112.6751
expansions: [(0, 2)]
discards: [ 0 23 35 41]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.2804 - loglik: -1.1127e+02 - logprior: -3.0141e+00
Epoch 2/2
19/19 - 1s - loss: 111.2523 - loglik: -1.1004e+02 - logprior: -1.2105e+00
Fitted a model with MAP estimate = -110.6888
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.6279 - loglik: -1.1191e+02 - logprior: -3.7228e+00
Epoch 2/10
19/19 - 1s - loss: 111.5613 - loglik: -1.1024e+02 - logprior: -1.3259e+00
Epoch 3/10
19/19 - 1s - loss: 110.9614 - loglik: -1.0980e+02 - logprior: -1.1582e+00
Epoch 4/10
19/19 - 1s - loss: 110.7264 - loglik: -1.0961e+02 - logprior: -1.1200e+00
Epoch 5/10
19/19 - 1s - loss: 110.3803 - loglik: -1.0928e+02 - logprior: -1.1030e+00
Epoch 6/10
19/19 - 1s - loss: 110.3660 - loglik: -1.0927e+02 - logprior: -1.0931e+00
Epoch 7/10
19/19 - 1s - loss: 110.0830 - loglik: -1.0901e+02 - logprior: -1.0721e+00
Epoch 8/10
19/19 - 1s - loss: 110.0356 - loglik: -1.0898e+02 - logprior: -1.0559e+00
Epoch 9/10
19/19 - 1s - loss: 110.0566 - loglik: -1.0900e+02 - logprior: -1.0525e+00
Fitted a model with MAP estimate = -109.8400
Time for alignment: 41.3006
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 149.9211 - loglik: -1.4672e+02 - logprior: -3.2052e+00
Epoch 2/10
19/19 - 1s - loss: 127.5547 - loglik: -1.2622e+02 - logprior: -1.3322e+00
Epoch 3/10
19/19 - 1s - loss: 119.5757 - loglik: -1.1807e+02 - logprior: -1.5102e+00
Epoch 4/10
19/19 - 1s - loss: 118.1194 - loglik: -1.1670e+02 - logprior: -1.4220e+00
Epoch 5/10
19/19 - 1s - loss: 117.8445 - loglik: -1.1646e+02 - logprior: -1.3877e+00
Epoch 6/10
19/19 - 1s - loss: 117.6153 - loglik: -1.1624e+02 - logprior: -1.3712e+00
Epoch 7/10
19/19 - 1s - loss: 117.5715 - loglik: -1.1622e+02 - logprior: -1.3561e+00
Epoch 8/10
19/19 - 1s - loss: 117.2426 - loglik: -1.1589e+02 - logprior: -1.3501e+00
Epoch 9/10
19/19 - 1s - loss: 117.4267 - loglik: -1.1608e+02 - logprior: -1.3446e+00
Fitted a model with MAP estimate = -117.1694
expansions: [(6, 3), (15, 1), (19, 2), (20, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 121.5737 - loglik: -1.1748e+02 - logprior: -4.0900e+00
Epoch 2/2
19/19 - 1s - loss: 114.1759 - loglik: -1.1217e+02 - logprior: -2.0092e+00
Fitted a model with MAP estimate = -112.6962
expansions: [(0, 2)]
discards: [ 0 23 34 41]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.2339 - loglik: -1.1122e+02 - logprior: -3.0178e+00
Epoch 2/2
19/19 - 1s - loss: 111.2706 - loglik: -1.1006e+02 - logprior: -1.2098e+00
Fitted a model with MAP estimate = -110.7011
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.7779 - loglik: -1.1201e+02 - logprior: -3.7689e+00
Epoch 2/10
19/19 - 1s - loss: 111.5582 - loglik: -1.1022e+02 - logprior: -1.3352e+00
Epoch 3/10
19/19 - 1s - loss: 111.1099 - loglik: -1.0995e+02 - logprior: -1.1634e+00
Epoch 4/10
19/19 - 1s - loss: 110.6453 - loglik: -1.0953e+02 - logprior: -1.1190e+00
Epoch 5/10
19/19 - 1s - loss: 110.4639 - loglik: -1.0936e+02 - logprior: -1.1064e+00
Epoch 6/10
19/19 - 1s - loss: 110.5041 - loglik: -1.0941e+02 - logprior: -1.0918e+00
Fitted a model with MAP estimate = -110.2146
Time for alignment: 37.0647
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.0640 - loglik: -1.4686e+02 - logprior: -3.2027e+00
Epoch 2/10
19/19 - 1s - loss: 129.0961 - loglik: -1.2776e+02 - logprior: -1.3330e+00
Epoch 3/10
19/19 - 1s - loss: 120.0649 - loglik: -1.1856e+02 - logprior: -1.5059e+00
Epoch 4/10
19/19 - 1s - loss: 118.2286 - loglik: -1.1681e+02 - logprior: -1.4161e+00
Epoch 5/10
19/19 - 1s - loss: 117.9260 - loglik: -1.1654e+02 - logprior: -1.3873e+00
Epoch 6/10
19/19 - 1s - loss: 117.4789 - loglik: -1.1611e+02 - logprior: -1.3642e+00
Epoch 7/10
19/19 - 1s - loss: 117.6148 - loglik: -1.1626e+02 - logprior: -1.3528e+00
Fitted a model with MAP estimate = -117.2692
expansions: [(6, 3), (15, 1), (19, 2), (26, 1), (27, 2), (28, 2), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 121.8965 - loglik: -1.1780e+02 - logprior: -4.0985e+00
Epoch 2/2
19/19 - 1s - loss: 114.2482 - loglik: -1.1225e+02 - logprior: -1.9967e+00
Fitted a model with MAP estimate = -112.6151
expansions: [(0, 2)]
discards: [ 0 23 34 36 42]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.4671 - loglik: -1.1145e+02 - logprior: -3.0121e+00
Epoch 2/2
19/19 - 1s - loss: 111.2855 - loglik: -1.1007e+02 - logprior: -1.2162e+00
Fitted a model with MAP estimate = -110.7292
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.7847 - loglik: -1.1202e+02 - logprior: -3.7633e+00
Epoch 2/10
19/19 - 1s - loss: 111.6451 - loglik: -1.1031e+02 - logprior: -1.3321e+00
Epoch 3/10
19/19 - 1s - loss: 110.9493 - loglik: -1.0979e+02 - logprior: -1.1635e+00
Epoch 4/10
19/19 - 1s - loss: 110.7827 - loglik: -1.0966e+02 - logprior: -1.1223e+00
Epoch 5/10
19/19 - 1s - loss: 110.4759 - loglik: -1.0937e+02 - logprior: -1.1058e+00
Epoch 6/10
19/19 - 1s - loss: 110.3701 - loglik: -1.0928e+02 - logprior: -1.0897e+00
Epoch 7/10
19/19 - 1s - loss: 110.2257 - loglik: -1.0915e+02 - logprior: -1.0749e+00
Epoch 8/10
19/19 - 1s - loss: 109.9933 - loglik: -1.0893e+02 - logprior: -1.0607e+00
Epoch 9/10
19/19 - 1s - loss: 109.9863 - loglik: -1.0893e+02 - logprior: -1.0554e+00
Epoch 10/10
19/19 - 1s - loss: 109.9303 - loglik: -1.0889e+02 - logprior: -1.0371e+00
Fitted a model with MAP estimate = -109.8247
Time for alignment: 40.1303
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 149.9554 - loglik: -1.4675e+02 - logprior: -3.2047e+00
Epoch 2/10
19/19 - 1s - loss: 128.2162 - loglik: -1.2687e+02 - logprior: -1.3443e+00
Epoch 3/10
19/19 - 1s - loss: 120.2399 - loglik: -1.1874e+02 - logprior: -1.5006e+00
Epoch 4/10
19/19 - 1s - loss: 118.5035 - loglik: -1.1710e+02 - logprior: -1.4045e+00
Epoch 5/10
19/19 - 1s - loss: 118.1280 - loglik: -1.1675e+02 - logprior: -1.3815e+00
Epoch 6/10
19/19 - 1s - loss: 117.8322 - loglik: -1.1648e+02 - logprior: -1.3569e+00
Epoch 7/10
19/19 - 1s - loss: 117.8741 - loglik: -1.1652e+02 - logprior: -1.3495e+00
Fitted a model with MAP estimate = -117.5527
expansions: [(6, 3), (14, 1), (15, 1), (18, 2), (27, 2), (28, 2), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 121.9772 - loglik: -1.1789e+02 - logprior: -4.0879e+00
Epoch 2/2
19/19 - 1s - loss: 114.1809 - loglik: -1.1220e+02 - logprior: -1.9841e+00
Fitted a model with MAP estimate = -112.6189
expansions: [(0, 2)]
discards: [ 0 22 34 36 42]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.2326 - loglik: -1.1122e+02 - logprior: -3.0141e+00
Epoch 2/2
19/19 - 1s - loss: 111.1893 - loglik: -1.0997e+02 - logprior: -1.2165e+00
Fitted a model with MAP estimate = -110.7036
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.7292 - loglik: -1.1198e+02 - logprior: -3.7533e+00
Epoch 2/10
19/19 - 1s - loss: 111.6231 - loglik: -1.1029e+02 - logprior: -1.3369e+00
Epoch 3/10
19/19 - 1s - loss: 110.9810 - loglik: -1.0982e+02 - logprior: -1.1635e+00
Epoch 4/10
19/19 - 1s - loss: 110.8053 - loglik: -1.0968e+02 - logprior: -1.1229e+00
Epoch 5/10
19/19 - 1s - loss: 110.4881 - loglik: -1.0938e+02 - logprior: -1.1082e+00
Epoch 6/10
19/19 - 1s - loss: 110.3867 - loglik: -1.0929e+02 - logprior: -1.0945e+00
Epoch 7/10
19/19 - 1s - loss: 110.2581 - loglik: -1.0918e+02 - logprior: -1.0767e+00
Epoch 8/10
19/19 - 1s - loss: 110.0596 - loglik: -1.0900e+02 - logprior: -1.0602e+00
Epoch 9/10
19/19 - 1s - loss: 110.1306 - loglik: -1.0908e+02 - logprior: -1.0543e+00
Fitted a model with MAP estimate = -109.9261
Time for alignment: 38.9995
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.1686 - loglik: -1.4696e+02 - logprior: -3.2065e+00
Epoch 2/10
19/19 - 1s - loss: 129.9211 - loglik: -1.2857e+02 - logprior: -1.3529e+00
Epoch 3/10
19/19 - 1s - loss: 119.6587 - loglik: -1.1808e+02 - logprior: -1.5749e+00
Epoch 4/10
19/19 - 1s - loss: 117.3972 - loglik: -1.1591e+02 - logprior: -1.4870e+00
Epoch 5/10
19/19 - 1s - loss: 117.0520 - loglik: -1.1560e+02 - logprior: -1.4552e+00
Epoch 6/10
19/19 - 1s - loss: 116.8174 - loglik: -1.1538e+02 - logprior: -1.4363e+00
Epoch 7/10
19/19 - 1s - loss: 116.6989 - loglik: -1.1528e+02 - logprior: -1.4214e+00
Epoch 8/10
19/19 - 1s - loss: 116.5369 - loglik: -1.1512e+02 - logprior: -1.4134e+00
Epoch 9/10
19/19 - 1s - loss: 116.5470 - loglik: -1.1514e+02 - logprior: -1.4091e+00
Fitted a model with MAP estimate = -116.3108
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (19, 2), (23, 2), (27, 2), (28, 2), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 123.1085 - loglik: -1.1899e+02 - logprior: -4.1188e+00
Epoch 2/2
19/19 - 1s - loss: 114.3968 - loglik: -1.1235e+02 - logprior: -2.0443e+00
Fitted a model with MAP estimate = -112.8122
expansions: [(0, 2)]
discards: [ 0 22 28 35 37 43]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.3130 - loglik: -1.1129e+02 - logprior: -3.0195e+00
Epoch 2/2
19/19 - 1s - loss: 111.2508 - loglik: -1.1004e+02 - logprior: -1.2124e+00
Fitted a model with MAP estimate = -110.6947
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.7871 - loglik: -1.1204e+02 - logprior: -3.7509e+00
Epoch 2/10
19/19 - 1s - loss: 111.5441 - loglik: -1.1022e+02 - logprior: -1.3285e+00
Epoch 3/10
19/19 - 1s - loss: 110.9647 - loglik: -1.0981e+02 - logprior: -1.1555e+00
Epoch 4/10
19/19 - 1s - loss: 110.6930 - loglik: -1.0957e+02 - logprior: -1.1242e+00
Epoch 5/10
19/19 - 1s - loss: 110.3646 - loglik: -1.0926e+02 - logprior: -1.1052e+00
Epoch 6/10
19/19 - 1s - loss: 110.4109 - loglik: -1.0932e+02 - logprior: -1.0900e+00
Fitted a model with MAP estimate = -110.0840
Time for alignment: 37.6995
Computed alignments with likelihoods: ['-109.8400', '-110.2146', '-109.8247', '-109.9261', '-110.0840']
Best model has likelihood: -109.8247  (prior= -1.0754 )
time for generating output: 0.1026
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01381.projection.fasta
SP score = 0.6121201687182577
Training of 5 independent models on file PF02878.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd3848cd700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd36b644fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd36ba7d190>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 360.0008 - loglik: -3.5701e+02 - logprior: -2.9889e+00
Epoch 2/10
19/19 - 3s - loss: 304.8329 - loglik: -3.0373e+02 - logprior: -1.1059e+00
Epoch 3/10
19/19 - 3s - loss: 279.6115 - loglik: -2.7848e+02 - logprior: -1.1357e+00
Epoch 4/10
19/19 - 3s - loss: 273.6508 - loglik: -2.7250e+02 - logprior: -1.1479e+00
Epoch 5/10
19/19 - 3s - loss: 271.6081 - loglik: -2.7052e+02 - logprior: -1.0924e+00
Epoch 6/10
19/19 - 3s - loss: 270.2042 - loglik: -2.6916e+02 - logprior: -1.0463e+00
Epoch 7/10
19/19 - 3s - loss: 270.0391 - loglik: -2.6902e+02 - logprior: -1.0193e+00
Epoch 8/10
19/19 - 3s - loss: 268.9156 - loglik: -2.6792e+02 - logprior: -9.9945e-01
Epoch 9/10
19/19 - 3s - loss: 269.3522 - loglik: -2.6835e+02 - logprior: -9.9965e-01
Fitted a model with MAP estimate = -268.8906
expansions: [(12, 3), (13, 2), (14, 1), (16, 1), (26, 1), (27, 2), (28, 2), (29, 2), (43, 1), (45, 1), (48, 1), (52, 1), (58, 2), (66, 1), (70, 1), (71, 1), (97, 1), (101, 1), (103, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 270.9214 - loglik: -2.6710e+02 - logprior: -3.8189e+00
Epoch 2/2
19/19 - 5s - loss: 261.0779 - loglik: -2.5928e+02 - logprior: -1.8024e+00
Fitted a model with MAP estimate = -259.5830
expansions: [(0, 3)]
discards: [  0  12  15  38  76 139]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 262.9719 - loglik: -2.6018e+02 - logprior: -2.7887e+00
Epoch 2/2
19/19 - 4s - loss: 258.8356 - loglik: -2.5789e+02 - logprior: -9.4290e-01
Fitted a model with MAP estimate = -258.0783
expansions: []
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 264.4566 - loglik: -2.6081e+02 - logprior: -3.6467e+00
Epoch 2/10
19/19 - 4s - loss: 260.8976 - loglik: -2.5935e+02 - logprior: -1.5438e+00
Epoch 3/10
19/19 - 4s - loss: 259.5519 - loglik: -2.5880e+02 - logprior: -7.5573e-01
Epoch 4/10
19/19 - 4s - loss: 258.6376 - loglik: -2.5824e+02 - logprior: -4.0032e-01
Epoch 5/10
19/19 - 4s - loss: 257.8843 - loglik: -2.5758e+02 - logprior: -3.0181e-01
Epoch 6/10
19/19 - 4s - loss: 257.5422 - loglik: -2.5732e+02 - logprior: -2.2671e-01
Epoch 7/10
19/19 - 4s - loss: 257.2598 - loglik: -2.5709e+02 - logprior: -1.6750e-01
Epoch 8/10
19/19 - 5s - loss: 257.3434 - loglik: -2.5721e+02 - logprior: -1.3345e-01
Fitted a model with MAP estimate = -256.8706
Time for alignment: 115.3530
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 360.0214 - loglik: -3.5703e+02 - logprior: -2.9925e+00
Epoch 2/10
19/19 - 3s - loss: 303.9934 - loglik: -3.0288e+02 - logprior: -1.1156e+00
Epoch 3/10
19/19 - 3s - loss: 279.0401 - loglik: -2.7782e+02 - logprior: -1.2171e+00
Epoch 4/10
19/19 - 3s - loss: 271.2845 - loglik: -2.7006e+02 - logprior: -1.2268e+00
Epoch 5/10
19/19 - 3s - loss: 269.5226 - loglik: -2.6838e+02 - logprior: -1.1459e+00
Epoch 6/10
19/19 - 3s - loss: 268.4706 - loglik: -2.6739e+02 - logprior: -1.0801e+00
Epoch 7/10
19/19 - 3s - loss: 267.8446 - loglik: -2.6679e+02 - logprior: -1.0517e+00
Epoch 8/10
19/19 - 4s - loss: 267.5688 - loglik: -2.6653e+02 - logprior: -1.0414e+00
Epoch 9/10
19/19 - 3s - loss: 267.9007 - loglik: -2.6687e+02 - logprior: -1.0319e+00
Fitted a model with MAP estimate = -267.2101
expansions: [(12, 1), (14, 3), (16, 1), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (56, 1), (66, 1), (70, 1), (71, 1), (97, 1), (102, 1), (103, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 269.1345 - loglik: -2.6534e+02 - logprior: -3.7968e+00
Epoch 2/2
19/19 - 4s - loss: 260.0720 - loglik: -2.5833e+02 - logprior: -1.7376e+00
Fitted a model with MAP estimate = -258.5918
expansions: [(0, 3)]
discards: [  0  15  37 137]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 261.3972 - loglik: -2.5862e+02 - logprior: -2.7725e+00
Epoch 2/2
19/19 - 4s - loss: 258.2638 - loglik: -2.5732e+02 - logprior: -9.4310e-01
Fitted a model with MAP estimate = -257.4612
expansions: []
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 263.8282 - loglik: -2.6016e+02 - logprior: -3.6641e+00
Epoch 2/10
19/19 - 4s - loss: 260.2621 - loglik: -2.5870e+02 - logprior: -1.5670e+00
Epoch 3/10
19/19 - 4s - loss: 259.1166 - loglik: -2.5831e+02 - logprior: -8.0448e-01
Epoch 4/10
19/19 - 4s - loss: 257.3238 - loglik: -2.5691e+02 - logprior: -4.1146e-01
Epoch 5/10
19/19 - 4s - loss: 257.3646 - loglik: -2.5705e+02 - logprior: -3.1266e-01
Fitted a model with MAP estimate = -256.5857
Time for alignment: 105.7724
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 360.3203 - loglik: -3.5732e+02 - logprior: -2.9955e+00
Epoch 2/10
19/19 - 3s - loss: 302.0972 - loglik: -3.0097e+02 - logprior: -1.1252e+00
Epoch 3/10
19/19 - 4s - loss: 278.1183 - loglik: -2.7694e+02 - logprior: -1.1791e+00
Epoch 4/10
19/19 - 3s - loss: 270.6072 - loglik: -2.6942e+02 - logprior: -1.1899e+00
Epoch 5/10
19/19 - 3s - loss: 268.2797 - loglik: -2.6717e+02 - logprior: -1.1122e+00
Epoch 6/10
19/19 - 3s - loss: 267.7828 - loglik: -2.6673e+02 - logprior: -1.0566e+00
Epoch 7/10
19/19 - 3s - loss: 266.8482 - loglik: -2.6582e+02 - logprior: -1.0248e+00
Epoch 8/10
19/19 - 3s - loss: 266.6097 - loglik: -2.6559e+02 - logprior: -1.0177e+00
Epoch 9/10
19/19 - 3s - loss: 266.5100 - loglik: -2.6551e+02 - logprior: -1.0002e+00
Epoch 10/10
19/19 - 3s - loss: 265.9112 - loglik: -2.6491e+02 - logprior: -1.0051e+00
Fitted a model with MAP estimate = -266.1743
expansions: [(11, 1), (13, 1), (17, 1), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (66, 1), (70, 1), (71, 1), (93, 1), (101, 1), (103, 1), (107, 1), (108, 1), (109, 2), (110, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 266.5882 - loglik: -2.6368e+02 - logprior: -2.9114e+00
Epoch 2/2
19/19 - 5s - loss: 258.4330 - loglik: -2.5745e+02 - logprior: -9.8361e-01
Fitted a model with MAP estimate = -257.3150
expansions: []
discards: [ 36  74 136 137]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 260.8155 - loglik: -2.5796e+02 - logprior: -2.8541e+00
Epoch 2/2
19/19 - 4s - loss: 258.0537 - loglik: -2.5713e+02 - logprior: -9.2270e-01
Fitted a model with MAP estimate = -257.2796
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 260.1610 - loglik: -2.5734e+02 - logprior: -2.8202e+00
Epoch 2/10
19/19 - 5s - loss: 257.8737 - loglik: -2.5699e+02 - logprior: -8.8249e-01
Epoch 3/10
19/19 - 4s - loss: 257.0450 - loglik: -2.5632e+02 - logprior: -7.2835e-01
Epoch 4/10
19/19 - 4s - loss: 256.5048 - loglik: -2.5588e+02 - logprior: -6.2253e-01
Epoch 5/10
19/19 - 5s - loss: 256.1735 - loglik: -2.5563e+02 - logprior: -5.4679e-01
Epoch 6/10
19/19 - 4s - loss: 255.6656 - loglik: -2.5520e+02 - logprior: -4.7056e-01
Epoch 7/10
19/19 - 5s - loss: 255.2749 - loglik: -2.5487e+02 - logprior: -4.0901e-01
Epoch 8/10
19/19 - 5s - loss: 255.2286 - loglik: -2.5486e+02 - logprior: -3.6886e-01
Epoch 9/10
19/19 - 5s - loss: 254.9387 - loglik: -2.5460e+02 - logprior: -3.3741e-01
Epoch 10/10
19/19 - 5s - loss: 255.0928 - loglik: -2.5477e+02 - logprior: -3.1803e-01
Fitted a model with MAP estimate = -254.7554
Time for alignment: 135.6924
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 360.1413 - loglik: -3.5715e+02 - logprior: -2.9896e+00
Epoch 2/10
19/19 - 3s - loss: 304.5330 - loglik: -3.0342e+02 - logprior: -1.1158e+00
Epoch 3/10
19/19 - 3s - loss: 279.9762 - loglik: -2.7877e+02 - logprior: -1.2093e+00
Epoch 4/10
19/19 - 3s - loss: 272.3170 - loglik: -2.7115e+02 - logprior: -1.1684e+00
Epoch 5/10
19/19 - 4s - loss: 270.2167 - loglik: -2.6913e+02 - logprior: -1.0825e+00
Epoch 6/10
19/19 - 3s - loss: 269.3147 - loglik: -2.6831e+02 - logprior: -1.0058e+00
Epoch 7/10
19/19 - 4s - loss: 268.2464 - loglik: -2.6728e+02 - logprior: -9.7092e-01
Epoch 8/10
19/19 - 4s - loss: 268.0346 - loglik: -2.6708e+02 - logprior: -9.5441e-01
Epoch 9/10
19/19 - 4s - loss: 268.4717 - loglik: -2.6753e+02 - logprior: -9.3744e-01
Fitted a model with MAP estimate = -267.7593
expansions: [(12, 1), (13, 2), (14, 3), (25, 1), (26, 1), (27, 2), (28, 2), (29, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (56, 1), (60, 1), (70, 1), (71, 1), (97, 1), (99, 1), (103, 1), (107, 1), (109, 2), (110, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 266.9163 - loglik: -2.6379e+02 - logprior: -3.1237e+00
Epoch 2/2
19/19 - 5s - loss: 256.8011 - loglik: -2.5572e+02 - logprior: -1.0772e+00
Fitted a model with MAP estimate = -255.7433
expansions: []
discards: [ 16  17  18  39 140]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 260.7976 - loglik: -2.5774e+02 - logprior: -3.0608e+00
Epoch 2/2
19/19 - 4s - loss: 257.2667 - loglik: -2.5625e+02 - logprior: -1.0172e+00
Fitted a model with MAP estimate = -256.6173
expansions: [(16, 2), (137, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 259.8004 - loglik: -2.5683e+02 - logprior: -2.9739e+00
Epoch 2/10
19/19 - 5s - loss: 256.1606 - loglik: -2.5519e+02 - logprior: -9.6908e-01
Epoch 3/10
19/19 - 5s - loss: 255.5172 - loglik: -2.5476e+02 - logprior: -7.5798e-01
Epoch 4/10
19/19 - 5s - loss: 255.1282 - loglik: -2.5446e+02 - logprior: -6.6526e-01
Epoch 5/10
19/19 - 5s - loss: 253.9262 - loglik: -2.5334e+02 - logprior: -5.8366e-01
Epoch 6/10
19/19 - 5s - loss: 253.7874 - loglik: -2.5325e+02 - logprior: -5.3526e-01
Epoch 7/10
19/19 - 5s - loss: 253.8848 - loglik: -2.5339e+02 - logprior: -4.9763e-01
Fitted a model with MAP estimate = -253.4649
Time for alignment: 120.1313
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 360.0090 - loglik: -3.5701e+02 - logprior: -2.9978e+00
Epoch 2/10
19/19 - 3s - loss: 301.9244 - loglik: -3.0079e+02 - logprior: -1.1299e+00
Epoch 3/10
19/19 - 3s - loss: 278.1202 - loglik: -2.7686e+02 - logprior: -1.2569e+00
Epoch 4/10
19/19 - 4s - loss: 271.8156 - loglik: -2.7057e+02 - logprior: -1.2472e+00
Epoch 5/10
19/19 - 4s - loss: 270.2380 - loglik: -2.6908e+02 - logprior: -1.1552e+00
Epoch 6/10
19/19 - 3s - loss: 269.6839 - loglik: -2.6857e+02 - logprior: -1.1144e+00
Epoch 7/10
19/19 - 4s - loss: 269.1132 - loglik: -2.6803e+02 - logprior: -1.0809e+00
Epoch 8/10
19/19 - 4s - loss: 268.9506 - loglik: -2.6787e+02 - logprior: -1.0775e+00
Epoch 9/10
19/19 - 4s - loss: 268.1531 - loglik: -2.6708e+02 - logprior: -1.0706e+00
Epoch 10/10
19/19 - 3s - loss: 269.2783 - loglik: -2.6821e+02 - logprior: -1.0678e+00
Fitted a model with MAP estimate = -268.2057
expansions: [(12, 3), (13, 2), (14, 1), (16, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (66, 1), (70, 1), (71, 1), (97, 1), (101, 1), (103, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 270.2271 - loglik: -2.6637e+02 - logprior: -3.8529e+00
Epoch 2/2
19/19 - 4s - loss: 260.6978 - loglik: -2.5889e+02 - logprior: -1.8124e+00
Fitted a model with MAP estimate = -258.9611
expansions: [(0, 2)]
discards: [  0  12  15  38  76 139]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 262.0077 - loglik: -2.5925e+02 - logprior: -2.7579e+00
Epoch 2/2
19/19 - 5s - loss: 258.7688 - loglik: -2.5784e+02 - logprior: -9.3269e-01
Fitted a model with MAP estimate = -257.9218
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 262.7229 - loglik: -2.5902e+02 - logprior: -3.7040e+00
Epoch 2/10
19/19 - 5s - loss: 259.1608 - loglik: -2.5795e+02 - logprior: -1.2115e+00
Epoch 3/10
19/19 - 5s - loss: 257.5558 - loglik: -2.5696e+02 - logprior: -5.9427e-01
Epoch 4/10
19/19 - 5s - loss: 257.6433 - loglik: -2.5712e+02 - logprior: -5.2152e-01
Fitted a model with MAP estimate = -256.5926
Time for alignment: 110.7577
Computed alignments with likelihoods: ['-256.8706', '-256.5857', '-254.7554', '-253.4649', '-256.5926']
Best model has likelihood: -253.4649  (prior= -0.5266 )
time for generating output: 0.1696
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02878.projection.fasta
SP score = 0.7536423841059603
Training of 5 independent models on file PF00970.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd124f77610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd36b768be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd36babf9d0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 274.9243 - loglik: -2.7187e+02 - logprior: -3.0545e+00
Epoch 2/10
19/19 - 2s - loss: 236.9965 - loglik: -2.3580e+02 - logprior: -1.2013e+00
Epoch 3/10
19/19 - 2s - loss: 221.2525 - loglik: -2.1991e+02 - logprior: -1.3432e+00
Epoch 4/10
19/19 - 2s - loss: 217.3854 - loglik: -2.1605e+02 - logprior: -1.3313e+00
Epoch 5/10
19/19 - 2s - loss: 216.2628 - loglik: -2.1498e+02 - logprior: -1.2815e+00
Epoch 6/10
19/19 - 2s - loss: 216.0565 - loglik: -2.1480e+02 - logprior: -1.2531e+00
Epoch 7/10
19/19 - 2s - loss: 215.5883 - loglik: -2.1434e+02 - logprior: -1.2504e+00
Epoch 8/10
19/19 - 2s - loss: 215.4966 - loglik: -2.1427e+02 - logprior: -1.2274e+00
Epoch 9/10
19/19 - 2s - loss: 215.1200 - loglik: -2.1388e+02 - logprior: -1.2364e+00
Epoch 10/10
19/19 - 2s - loss: 215.8548 - loglik: -2.1464e+02 - logprior: -1.2102e+00
Fitted a model with MAP estimate = -214.9078
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (16, 1), (17, 1), (18, 1), (21, 1), (28, 1), (29, 1), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (67, 1), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 217.6079 - loglik: -2.1374e+02 - logprior: -3.8638e+00
Epoch 2/2
19/19 - 3s - loss: 208.0143 - loglik: -2.0678e+02 - logprior: -1.2388e+00
Fitted a model with MAP estimate = -206.4356
expansions: []
discards: [ 0 74 79 82]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 211.8404 - loglik: -2.0788e+02 - logprior: -3.9638e+00
Epoch 2/2
19/19 - 3s - loss: 207.9685 - loglik: -2.0655e+02 - logprior: -1.4151e+00
Fitted a model with MAP estimate = -206.5285
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 209.0444 - loglik: -2.0618e+02 - logprior: -2.8678e+00
Epoch 2/10
19/19 - 3s - loss: 206.6659 - loglik: -2.0558e+02 - logprior: -1.0812e+00
Epoch 3/10
19/19 - 3s - loss: 205.9673 - loglik: -2.0495e+02 - logprior: -1.0152e+00
Epoch 4/10
19/19 - 3s - loss: 205.6921 - loglik: -2.0473e+02 - logprior: -9.5990e-01
Epoch 5/10
19/19 - 3s - loss: 205.5552 - loglik: -2.0462e+02 - logprior: -9.3473e-01
Epoch 6/10
19/19 - 3s - loss: 205.3654 - loglik: -2.0445e+02 - logprior: -9.1317e-01
Epoch 7/10
19/19 - 3s - loss: 205.2632 - loglik: -2.0436e+02 - logprior: -8.9845e-01
Epoch 8/10
19/19 - 3s - loss: 205.2089 - loglik: -2.0433e+02 - logprior: -8.8080e-01
Epoch 9/10
19/19 - 3s - loss: 205.0952 - loglik: -2.0423e+02 - logprior: -8.6578e-01
Epoch 10/10
19/19 - 3s - loss: 205.0431 - loglik: -2.0419e+02 - logprior: -8.5749e-01
Fitted a model with MAP estimate = -204.9521
Time for alignment: 91.2439
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 274.8742 - loglik: -2.7182e+02 - logprior: -3.0577e+00
Epoch 2/10
19/19 - 2s - loss: 237.2502 - loglik: -2.3601e+02 - logprior: -1.2357e+00
Epoch 3/10
19/19 - 2s - loss: 220.5145 - loglik: -2.1920e+02 - logprior: -1.3096e+00
Epoch 4/10
19/19 - 2s - loss: 217.5148 - loglik: -2.1619e+02 - logprior: -1.3220e+00
Epoch 5/10
19/19 - 2s - loss: 216.4401 - loglik: -2.1517e+02 - logprior: -1.2687e+00
Epoch 6/10
19/19 - 2s - loss: 215.7103 - loglik: -2.1448e+02 - logprior: -1.2308e+00
Epoch 7/10
19/19 - 2s - loss: 215.8271 - loglik: -2.1461e+02 - logprior: -1.2202e+00
Fitted a model with MAP estimate = -215.1342
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (16, 1), (17, 1), (18, 1), (27, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 218.1509 - loglik: -2.1434e+02 - logprior: -3.8100e+00
Epoch 2/2
19/19 - 3s - loss: 207.8790 - loglik: -2.0662e+02 - logprior: -1.2609e+00
Fitted a model with MAP estimate = -206.4511
expansions: []
discards: [ 0 46 75 80 83 96]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 211.9994 - loglik: -2.0804e+02 - logprior: -3.9593e+00
Epoch 2/2
19/19 - 3s - loss: 207.9984 - loglik: -2.0658e+02 - logprior: -1.4169e+00
Fitted a model with MAP estimate = -206.5674
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 209.1110 - loglik: -2.0625e+02 - logprior: -2.8587e+00
Epoch 2/10
19/19 - 3s - loss: 206.2307 - loglik: -2.0516e+02 - logprior: -1.0710e+00
Epoch 3/10
19/19 - 3s - loss: 206.0422 - loglik: -2.0503e+02 - logprior: -1.0084e+00
Epoch 4/10
19/19 - 3s - loss: 205.6569 - loglik: -2.0469e+02 - logprior: -9.6265e-01
Epoch 5/10
19/19 - 3s - loss: 205.3946 - loglik: -2.0446e+02 - logprior: -9.3324e-01
Epoch 6/10
19/19 - 3s - loss: 205.0092 - loglik: -2.0410e+02 - logprior: -9.1129e-01
Epoch 7/10
19/19 - 3s - loss: 205.1038 - loglik: -2.0420e+02 - logprior: -9.0085e-01
Fitted a model with MAP estimate = -204.8980
Time for alignment: 74.5706
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 274.7980 - loglik: -2.7173e+02 - logprior: -3.0637e+00
Epoch 2/10
19/19 - 2s - loss: 236.5608 - loglik: -2.3536e+02 - logprior: -1.2055e+00
Epoch 3/10
19/19 - 2s - loss: 220.9883 - loglik: -2.1971e+02 - logprior: -1.2793e+00
Epoch 4/10
19/19 - 2s - loss: 217.8258 - loglik: -2.1655e+02 - logprior: -1.2800e+00
Epoch 5/10
19/19 - 2s - loss: 216.2500 - loglik: -2.1502e+02 - logprior: -1.2258e+00
Epoch 6/10
19/19 - 2s - loss: 216.0658 - loglik: -2.1488e+02 - logprior: -1.1854e+00
Epoch 7/10
19/19 - 2s - loss: 215.4952 - loglik: -2.1431e+02 - logprior: -1.1900e+00
Epoch 8/10
19/19 - 2s - loss: 215.2703 - loglik: -2.1409e+02 - logprior: -1.1814e+00
Epoch 9/10
19/19 - 2s - loss: 215.4065 - loglik: -2.1422e+02 - logprior: -1.1883e+00
Fitted a model with MAP estimate = -214.8412
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (16, 1), (18, 1), (19, 1), (30, 1), (34, 1), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (70, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 218.6456 - loglik: -2.1479e+02 - logprior: -3.8554e+00
Epoch 2/2
19/19 - 3s - loss: 208.5203 - loglik: -2.0727e+02 - logprior: -1.2520e+00
Fitted a model with MAP estimate = -206.9724
expansions: []
discards: [ 0 73 78 81 94]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 212.4284 - loglik: -2.0846e+02 - logprior: -3.9686e+00
Epoch 2/2
19/19 - 3s - loss: 208.2854 - loglik: -2.0688e+02 - logprior: -1.4049e+00
Fitted a model with MAP estimate = -207.0027
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 209.5143 - loglik: -2.0665e+02 - logprior: -2.8690e+00
Epoch 2/10
19/19 - 3s - loss: 206.9799 - loglik: -2.0589e+02 - logprior: -1.0873e+00
Epoch 3/10
19/19 - 3s - loss: 206.6240 - loglik: -2.0561e+02 - logprior: -1.0188e+00
Epoch 4/10
19/19 - 3s - loss: 206.2846 - loglik: -2.0532e+02 - logprior: -9.6943e-01
Epoch 5/10
19/19 - 3s - loss: 205.8277 - loglik: -2.0488e+02 - logprior: -9.4498e-01
Epoch 6/10
19/19 - 3s - loss: 205.7145 - loglik: -2.0478e+02 - logprior: -9.3260e-01
Epoch 7/10
19/19 - 3s - loss: 205.5052 - loglik: -2.0459e+02 - logprior: -9.1610e-01
Epoch 8/10
19/19 - 3s - loss: 205.2654 - loglik: -2.0437e+02 - logprior: -8.9482e-01
Epoch 9/10
19/19 - 3s - loss: 205.0306 - loglik: -2.0415e+02 - logprior: -8.7764e-01
Epoch 10/10
19/19 - 3s - loss: 205.4665 - loglik: -2.0460e+02 - logprior: -8.6681e-01
Fitted a model with MAP estimate = -204.8431
Time for alignment: 87.9121
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 275.1057 - loglik: -2.7204e+02 - logprior: -3.0648e+00
Epoch 2/10
19/19 - 2s - loss: 238.1918 - loglik: -2.3700e+02 - logprior: -1.1959e+00
Epoch 3/10
19/19 - 2s - loss: 221.7696 - loglik: -2.2047e+02 - logprior: -1.2952e+00
Epoch 4/10
19/19 - 2s - loss: 217.7625 - loglik: -2.1647e+02 - logprior: -1.2885e+00
Epoch 5/10
19/19 - 2s - loss: 216.7885 - loglik: -2.1555e+02 - logprior: -1.2411e+00
Epoch 6/10
19/19 - 2s - loss: 216.0702 - loglik: -2.1486e+02 - logprior: -1.2093e+00
Epoch 7/10
19/19 - 2s - loss: 215.9507 - loglik: -2.1477e+02 - logprior: -1.1770e+00
Epoch 8/10
19/19 - 2s - loss: 215.4714 - loglik: -2.1431e+02 - logprior: -1.1663e+00
Epoch 9/10
19/19 - 2s - loss: 215.2895 - loglik: -2.1414e+02 - logprior: -1.1499e+00
Epoch 10/10
19/19 - 2s - loss: 215.3039 - loglik: -2.1416e+02 - logprior: -1.1416e+00
Fitted a model with MAP estimate = -214.8851
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (16, 1), (17, 1), (18, 3), (29, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (69, 1), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 217.6359 - loglik: -2.1378e+02 - logprior: -3.8576e+00
Epoch 2/2
19/19 - 3s - loss: 207.8614 - loglik: -2.0662e+02 - logprior: -1.2372e+00
Fitted a model with MAP estimate = -206.2513
expansions: []
discards: [ 0 46 75 80 83]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 211.8426 - loglik: -2.0789e+02 - logprior: -3.9571e+00
Epoch 2/2
19/19 - 3s - loss: 207.7487 - loglik: -2.0631e+02 - logprior: -1.4395e+00
Fitted a model with MAP estimate = -206.3843
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 208.9452 - loglik: -2.0608e+02 - logprior: -2.8697e+00
Epoch 2/10
19/19 - 3s - loss: 206.5498 - loglik: -2.0548e+02 - logprior: -1.0746e+00
Epoch 3/10
19/19 - 3s - loss: 205.8596 - loglik: -2.0485e+02 - logprior: -1.0129e+00
Epoch 4/10
19/19 - 3s - loss: 205.5777 - loglik: -2.0460e+02 - logprior: -9.7282e-01
Epoch 5/10
19/19 - 3s - loss: 205.4066 - loglik: -2.0447e+02 - logprior: -9.3784e-01
Epoch 6/10
19/19 - 3s - loss: 205.1804 - loglik: -2.0426e+02 - logprior: -9.2022e-01
Epoch 7/10
19/19 - 3s - loss: 205.0902 - loglik: -2.0419e+02 - logprior: -9.0010e-01
Epoch 8/10
19/19 - 3s - loss: 205.0000 - loglik: -2.0412e+02 - logprior: -8.8099e-01
Epoch 9/10
19/19 - 3s - loss: 204.8879 - loglik: -2.0402e+02 - logprior: -8.7020e-01
Epoch 10/10
19/19 - 3s - loss: 205.1094 - loglik: -2.0426e+02 - logprior: -8.5348e-01
Fitted a model with MAP estimate = -204.8393
Time for alignment: 90.7341
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 274.4895 - loglik: -2.7143e+02 - logprior: -3.0607e+00
Epoch 2/10
19/19 - 2s - loss: 237.5441 - loglik: -2.3634e+02 - logprior: -1.2029e+00
Epoch 3/10
19/19 - 2s - loss: 221.1826 - loglik: -2.1987e+02 - logprior: -1.3113e+00
Epoch 4/10
19/19 - 2s - loss: 217.6223 - loglik: -2.1629e+02 - logprior: -1.3293e+00
Epoch 5/10
19/19 - 2s - loss: 216.5483 - loglik: -2.1529e+02 - logprior: -1.2572e+00
Epoch 6/10
19/19 - 2s - loss: 216.2170 - loglik: -2.1499e+02 - logprior: -1.2246e+00
Epoch 7/10
19/19 - 2s - loss: 215.8427 - loglik: -2.1463e+02 - logprior: -1.2175e+00
Epoch 8/10
19/19 - 2s - loss: 215.9132 - loglik: -2.1472e+02 - logprior: -1.1952e+00
Fitted a model with MAP estimate = -215.1946
expansions: [(0, 2), (4, 1), (6, 1), (8, 1), (9, 1), (10, 1), (16, 1), (17, 1), (18, 1), (21, 1), (28, 1), (29, 1), (33, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 218.1497 - loglik: -2.1432e+02 - logprior: -3.8304e+00
Epoch 2/2
19/19 - 3s - loss: 207.8774 - loglik: -2.0662e+02 - logprior: -1.2598e+00
Fitted a model with MAP estimate = -206.2377
expansions: []
discards: [ 0 74 79 82 95]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 211.8442 - loglik: -2.0787e+02 - logprior: -3.9787e+00
Epoch 2/2
19/19 - 3s - loss: 207.8645 - loglik: -2.0642e+02 - logprior: -1.4462e+00
Fitted a model with MAP estimate = -206.5153
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 209.0562 - loglik: -2.0618e+02 - logprior: -2.8744e+00
Epoch 2/10
19/19 - 3s - loss: 206.4269 - loglik: -2.0533e+02 - logprior: -1.0927e+00
Epoch 3/10
19/19 - 3s - loss: 206.0011 - loglik: -2.0497e+02 - logprior: -1.0263e+00
Epoch 4/10
19/19 - 3s - loss: 205.7727 - loglik: -2.0480e+02 - logprior: -9.6927e-01
Epoch 5/10
19/19 - 3s - loss: 205.1943 - loglik: -2.0425e+02 - logprior: -9.4593e-01
Epoch 6/10
19/19 - 3s - loss: 205.3504 - loglik: -2.0442e+02 - logprior: -9.2595e-01
Fitted a model with MAP estimate = -205.0126
Time for alignment: 74.1173
Computed alignments with likelihoods: ['-204.9521', '-204.8980', '-204.8431', '-204.8393', '-205.0126']
Best model has likelihood: -204.8393  (prior= -0.8435 )
time for generating output: 0.1928
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00970.projection.fasta
SP score = 0.6563600252776023
Training of 5 independent models on file PF00313.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd384752130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd349d25160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd384b39f70>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 176.2214 - loglik: -1.7302e+02 - logprior: -3.2062e+00
Epoch 2/10
19/19 - 1s - loss: 132.6025 - loglik: -1.3126e+02 - logprior: -1.3463e+00
Epoch 3/10
19/19 - 1s - loss: 116.3179 - loglik: -1.1495e+02 - logprior: -1.3696e+00
Epoch 4/10
19/19 - 1s - loss: 114.0909 - loglik: -1.1271e+02 - logprior: -1.3816e+00
Epoch 5/10
19/19 - 1s - loss: 113.1920 - loglik: -1.1186e+02 - logprior: -1.3321e+00
Epoch 6/10
19/19 - 1s - loss: 112.7354 - loglik: -1.1142e+02 - logprior: -1.3144e+00
Epoch 7/10
19/19 - 1s - loss: 112.2288 - loglik: -1.1092e+02 - logprior: -1.3055e+00
Epoch 8/10
19/19 - 1s - loss: 112.2089 - loglik: -1.1091e+02 - logprior: -1.2987e+00
Epoch 9/10
19/19 - 1s - loss: 111.7920 - loglik: -1.1050e+02 - logprior: -1.2925e+00
Epoch 10/10
19/19 - 1s - loss: 111.8357 - loglik: -1.1054e+02 - logprior: -1.2926e+00
Fitted a model with MAP estimate = -111.7553
expansions: [(0, 2), (8, 1), (15, 1), (20, 1), (27, 1), (28, 2), (29, 1), (30, 1), (31, 1), (32, 1), (35, 1), (36, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.3056 - loglik: -1.1012e+02 - logprior: -4.1899e+00
Epoch 2/2
19/19 - 1s - loss: 105.2909 - loglik: -1.0406e+02 - logprior: -1.2335e+00
Fitted a model with MAP estimate = -103.8169
expansions: []
discards: [ 0 35]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.4580 - loglik: -1.0538e+02 - logprior: -4.0794e+00
Epoch 2/2
19/19 - 1s - loss: 104.9925 - loglik: -1.0354e+02 - logprior: -1.4521e+00
Fitted a model with MAP estimate = -104.2404
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.4320 - loglik: -1.0432e+02 - logprior: -3.1120e+00
Epoch 2/10
19/19 - 1s - loss: 104.6502 - loglik: -1.0331e+02 - logprior: -1.3435e+00
Epoch 3/10
19/19 - 1s - loss: 103.7355 - loglik: -1.0249e+02 - logprior: -1.2490e+00
Epoch 4/10
19/19 - 1s - loss: 103.0057 - loglik: -1.0180e+02 - logprior: -1.2069e+00
Epoch 5/10
19/19 - 1s - loss: 102.5875 - loglik: -1.0141e+02 - logprior: -1.1819e+00
Epoch 6/10
19/19 - 1s - loss: 102.2086 - loglik: -1.0105e+02 - logprior: -1.1617e+00
Epoch 7/10
19/19 - 1s - loss: 101.8228 - loglik: -1.0067e+02 - logprior: -1.1486e+00
Epoch 8/10
19/19 - 1s - loss: 101.7527 - loglik: -1.0062e+02 - logprior: -1.1294e+00
Epoch 9/10
19/19 - 1s - loss: 101.8563 - loglik: -1.0073e+02 - logprior: -1.1216e+00
Fitted a model with MAP estimate = -101.6920
Time for alignment: 47.7419
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 176.3726 - loglik: -1.7317e+02 - logprior: -3.2066e+00
Epoch 2/10
19/19 - 1s - loss: 133.0312 - loglik: -1.3170e+02 - logprior: -1.3289e+00
Epoch 3/10
19/19 - 1s - loss: 116.8928 - loglik: -1.1554e+02 - logprior: -1.3499e+00
Epoch 4/10
19/19 - 1s - loss: 114.5634 - loglik: -1.1318e+02 - logprior: -1.3858e+00
Epoch 5/10
19/19 - 1s - loss: 113.3056 - loglik: -1.1197e+02 - logprior: -1.3320e+00
Epoch 6/10
19/19 - 1s - loss: 112.9333 - loglik: -1.1162e+02 - logprior: -1.3090e+00
Epoch 7/10
19/19 - 1s - loss: 112.2566 - loglik: -1.1096e+02 - logprior: -1.3013e+00
Epoch 8/10
19/19 - 1s - loss: 112.1067 - loglik: -1.1081e+02 - logprior: -1.2932e+00
Epoch 9/10
19/19 - 1s - loss: 112.0580 - loglik: -1.1076e+02 - logprior: -1.2949e+00
Epoch 10/10
19/19 - 1s - loss: 112.0700 - loglik: -1.1078e+02 - logprior: -1.2856e+00
Fitted a model with MAP estimate = -111.9041
expansions: [(0, 2), (8, 1), (15, 1), (20, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (35, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.8125 - loglik: -1.0964e+02 - logprior: -4.1744e+00
Epoch 2/2
19/19 - 1s - loss: 105.0794 - loglik: -1.0388e+02 - logprior: -1.1992e+00
Fitted a model with MAP estimate = -103.8107
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.3019 - loglik: -1.0525e+02 - logprior: -4.0509e+00
Epoch 2/2
19/19 - 1s - loss: 105.0026 - loglik: -1.0358e+02 - logprior: -1.4271e+00
Fitted a model with MAP estimate = -104.1933
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.4615 - loglik: -1.0437e+02 - logprior: -3.0961e+00
Epoch 2/10
19/19 - 1s - loss: 104.5725 - loglik: -1.0324e+02 - logprior: -1.3276e+00
Epoch 3/10
19/19 - 1s - loss: 103.6735 - loglik: -1.0244e+02 - logprior: -1.2292e+00
Epoch 4/10
19/19 - 1s - loss: 103.0570 - loglik: -1.0187e+02 - logprior: -1.1869e+00
Epoch 5/10
19/19 - 1s - loss: 102.2466 - loglik: -1.0108e+02 - logprior: -1.1648e+00
Epoch 6/10
19/19 - 1s - loss: 102.2567 - loglik: -1.0111e+02 - logprior: -1.1466e+00
Fitted a model with MAP estimate = -101.9184
Time for alignment: 43.9398
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.2016 - loglik: -1.7300e+02 - logprior: -3.2059e+00
Epoch 2/10
19/19 - 1s - loss: 132.6412 - loglik: -1.3130e+02 - logprior: -1.3367e+00
Epoch 3/10
19/19 - 1s - loss: 116.6103 - loglik: -1.1525e+02 - logprior: -1.3586e+00
Epoch 4/10
19/19 - 1s - loss: 114.4049 - loglik: -1.1302e+02 - logprior: -1.3810e+00
Epoch 5/10
19/19 - 1s - loss: 113.2757 - loglik: -1.1195e+02 - logprior: -1.3304e+00
Epoch 6/10
19/19 - 1s - loss: 112.8341 - loglik: -1.1152e+02 - logprior: -1.3104e+00
Epoch 7/10
19/19 - 1s - loss: 112.4300 - loglik: -1.1113e+02 - logprior: -1.2990e+00
Epoch 8/10
19/19 - 1s - loss: 112.1245 - loglik: -1.1083e+02 - logprior: -1.2961e+00
Epoch 9/10
19/19 - 1s - loss: 112.1753 - loglik: -1.1089e+02 - logprior: -1.2860e+00
Fitted a model with MAP estimate = -111.9765
expansions: [(0, 2), (8, 1), (15, 1), (20, 1), (27, 1), (28, 2), (29, 1), (30, 1), (31, 1), (32, 1), (35, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.1942 - loglik: -1.1003e+02 - logprior: -4.1661e+00
Epoch 2/2
19/19 - 1s - loss: 105.0829 - loglik: -1.0385e+02 - logprior: -1.2281e+00
Fitted a model with MAP estimate = -103.7993
expansions: []
discards: [ 0 35]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.3915 - loglik: -1.0535e+02 - logprior: -4.0419e+00
Epoch 2/2
19/19 - 1s - loss: 105.2457 - loglik: -1.0381e+02 - logprior: -1.4309e+00
Fitted a model with MAP estimate = -104.2363
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.5165 - loglik: -1.0442e+02 - logprior: -3.0948e+00
Epoch 2/10
19/19 - 1s - loss: 104.7279 - loglik: -1.0340e+02 - logprior: -1.3264e+00
Epoch 3/10
19/19 - 1s - loss: 103.6649 - loglik: -1.0243e+02 - logprior: -1.2347e+00
Epoch 4/10
19/19 - 1s - loss: 102.8989 - loglik: -1.0171e+02 - logprior: -1.1910e+00
Epoch 5/10
19/19 - 1s - loss: 102.4077 - loglik: -1.0124e+02 - logprior: -1.1674e+00
Epoch 6/10
19/19 - 1s - loss: 102.1250 - loglik: -1.0098e+02 - logprior: -1.1460e+00
Epoch 7/10
19/19 - 1s - loss: 101.8659 - loglik: -1.0074e+02 - logprior: -1.1295e+00
Epoch 8/10
19/19 - 1s - loss: 101.9264 - loglik: -1.0081e+02 - logprior: -1.1185e+00
Fitted a model with MAP estimate = -101.6823
Time for alignment: 44.6147
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.5584 - loglik: -1.7335e+02 - logprior: -3.2035e+00
Epoch 2/10
19/19 - 1s - loss: 133.1154 - loglik: -1.3178e+02 - logprior: -1.3326e+00
Epoch 3/10
19/19 - 1s - loss: 116.9397 - loglik: -1.1561e+02 - logprior: -1.3247e+00
Epoch 4/10
19/19 - 1s - loss: 114.6357 - loglik: -1.1329e+02 - logprior: -1.3467e+00
Epoch 5/10
19/19 - 1s - loss: 113.7011 - loglik: -1.1241e+02 - logprior: -1.2902e+00
Epoch 6/10
19/19 - 1s - loss: 112.9890 - loglik: -1.1172e+02 - logprior: -1.2717e+00
Epoch 7/10
19/19 - 1s - loss: 112.7746 - loglik: -1.1151e+02 - logprior: -1.2633e+00
Epoch 8/10
19/19 - 1s - loss: 112.7571 - loglik: -1.1150e+02 - logprior: -1.2550e+00
Epoch 9/10
19/19 - 1s - loss: 112.4856 - loglik: -1.1124e+02 - logprior: -1.2500e+00
Epoch 10/10
19/19 - 1s - loss: 112.3209 - loglik: -1.1107e+02 - logprior: -1.2487e+00
Fitted a model with MAP estimate = -112.3013
expansions: [(0, 2), (8, 1), (15, 1), (20, 1), (27, 1), (28, 3), (29, 2), (30, 1), (31, 1), (36, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.1605 - loglik: -1.0997e+02 - logprior: -4.1902e+00
Epoch 2/2
19/19 - 1s - loss: 105.0966 - loglik: -1.0387e+02 - logprior: -1.2295e+00
Fitted a model with MAP estimate = -103.8799
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.4717 - loglik: -1.0541e+02 - logprior: -4.0663e+00
Epoch 2/2
19/19 - 1s - loss: 105.0584 - loglik: -1.0362e+02 - logprior: -1.4424e+00
Fitted a model with MAP estimate = -104.2760
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.4601 - loglik: -1.0425e+02 - logprior: -3.2095e+00
Epoch 2/10
19/19 - 1s - loss: 104.6132 - loglik: -1.0326e+02 - logprior: -1.3561e+00
Epoch 3/10
19/19 - 1s - loss: 103.8621 - loglik: -1.0263e+02 - logprior: -1.2359e+00
Epoch 4/10
19/19 - 1s - loss: 102.7818 - loglik: -1.0158e+02 - logprior: -1.2014e+00
Epoch 5/10
19/19 - 1s - loss: 102.2011 - loglik: -1.0102e+02 - logprior: -1.1802e+00
Epoch 6/10
19/19 - 1s - loss: 102.6088 - loglik: -1.0145e+02 - logprior: -1.1591e+00
Fitted a model with MAP estimate = -101.9403
Time for alignment: 43.3224
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 176.1771 - loglik: -1.7297e+02 - logprior: -3.2076e+00
Epoch 2/10
19/19 - 1s - loss: 132.4096 - loglik: -1.3111e+02 - logprior: -1.3033e+00
Epoch 3/10
19/19 - 1s - loss: 117.5505 - loglik: -1.1625e+02 - logprior: -1.2979e+00
Epoch 4/10
19/19 - 1s - loss: 114.9977 - loglik: -1.1367e+02 - logprior: -1.3248e+00
Epoch 5/10
19/19 - 1s - loss: 114.1860 - loglik: -1.1291e+02 - logprior: -1.2782e+00
Epoch 6/10
19/19 - 1s - loss: 113.9641 - loglik: -1.1271e+02 - logprior: -1.2518e+00
Epoch 7/10
19/19 - 1s - loss: 113.2983 - loglik: -1.1205e+02 - logprior: -1.2503e+00
Epoch 8/10
19/19 - 1s - loss: 112.9906 - loglik: -1.1175e+02 - logprior: -1.2396e+00
Epoch 9/10
19/19 - 1s - loss: 112.9671 - loglik: -1.1173e+02 - logprior: -1.2322e+00
Epoch 10/10
19/19 - 1s - loss: 113.0644 - loglik: -1.1183e+02 - logprior: -1.2323e+00
Fitted a model with MAP estimate = -112.8242
expansions: [(0, 2), (8, 1), (15, 1), (20, 1), (27, 1), (28, 3), (29, 2), (30, 1), (31, 1), (40, 3), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.3837 - loglik: -1.1018e+02 - logprior: -4.2049e+00
Epoch 2/2
19/19 - 1s - loss: 105.2163 - loglik: -1.0395e+02 - logprior: -1.2668e+00
Fitted a model with MAP estimate = -103.9403
expansions: []
discards: [ 0 38 54]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.5243 - loglik: -1.0547e+02 - logprior: -4.0551e+00
Epoch 2/2
19/19 - 1s - loss: 105.1526 - loglik: -1.0371e+02 - logprior: -1.4415e+00
Fitted a model with MAP estimate = -104.2475
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.3945 - loglik: -1.0429e+02 - logprior: -3.1015e+00
Epoch 2/10
19/19 - 1s - loss: 104.5499 - loglik: -1.0321e+02 - logprior: -1.3366e+00
Epoch 3/10
19/19 - 1s - loss: 103.7872 - loglik: -1.0255e+02 - logprior: -1.2356e+00
Epoch 4/10
19/19 - 1s - loss: 102.9155 - loglik: -1.0172e+02 - logprior: -1.1933e+00
Epoch 5/10
19/19 - 1s - loss: 102.5438 - loglik: -1.0138e+02 - logprior: -1.1664e+00
Epoch 6/10
19/19 - 1s - loss: 102.0490 - loglik: -1.0090e+02 - logprior: -1.1508e+00
Epoch 7/10
19/19 - 1s - loss: 101.8709 - loglik: -1.0074e+02 - logprior: -1.1359e+00
Epoch 8/10
19/19 - 1s - loss: 101.8945 - loglik: -1.0077e+02 - logprior: -1.1207e+00
Fitted a model with MAP estimate = -101.6751
Time for alignment: 46.1138
Computed alignments with likelihoods: ['-101.6920', '-101.9184', '-101.6823', '-101.9403', '-101.6751']
Best model has likelihood: -101.6751  (prior= -1.1308 )
time for generating output: 0.0903
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00313.projection.fasta
SP score = 0.8018575851393189
Training of 5 independent models on file PF00009.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd36b58b250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd4f79272b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd384e15820>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 567.0085 - loglik: -5.6515e+02 - logprior: -1.8546e+00
Epoch 2/10
39/39 - 15s - loss: 487.5651 - loglik: -4.8648e+02 - logprior: -1.0811e+00
Epoch 3/10
39/39 - 16s - loss: 479.8483 - loglik: -4.7883e+02 - logprior: -1.0226e+00
Epoch 4/10
39/39 - 16s - loss: 477.6411 - loglik: -4.7663e+02 - logprior: -1.0097e+00
Epoch 5/10
39/39 - 16s - loss: 477.4185 - loglik: -4.7641e+02 - logprior: -1.0060e+00
Epoch 6/10
39/39 - 16s - loss: 476.6064 - loglik: -4.7560e+02 - logprior: -1.0044e+00
Epoch 7/10
39/39 - 16s - loss: 476.6073 - loglik: -4.7561e+02 - logprior: -9.9753e-01
Fitted a model with MAP estimate = -474.6735
expansions: [(29, 1), (30, 2), (58, 1), (59, 1), (77, 1), (81, 1), (93, 2), (102, 1), (106, 1), (137, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 479.9548 - loglik: -4.7722e+02 - logprior: -2.7317e+00
Epoch 2/2
39/39 - 18s - loss: 470.3769 - loglik: -4.6947e+02 - logprior: -9.0540e-01
Fitted a model with MAP estimate = -466.1635
expansions: [(0, 2), (100, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 470.8649 - loglik: -4.6907e+02 - logprior: -1.7955e+00
Epoch 2/2
39/39 - 17s - loss: 466.9989 - loglik: -4.6628e+02 - logprior: -7.1494e-01
Fitted a model with MAP estimate = -463.9800
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 470.2174 - loglik: -4.6794e+02 - logprior: -2.2738e+00
Epoch 2/10
39/39 - 19s - loss: 466.1594 - loglik: -4.6562e+02 - logprior: -5.3496e-01
Epoch 3/10
39/39 - 20s - loss: 463.9050 - loglik: -4.6340e+02 - logprior: -5.0126e-01
Epoch 4/10
39/39 - 19s - loss: 463.4037 - loglik: -4.6297e+02 - logprior: -4.2917e-01
Epoch 5/10
39/39 - 19s - loss: 460.7532 - loglik: -4.6041e+02 - logprior: -3.4299e-01
Epoch 6/10
39/39 - 19s - loss: 459.7870 - loglik: -4.5953e+02 - logprior: -2.5720e-01
Epoch 7/10
39/39 - 19s - loss: 459.0347 - loglik: -4.5884e+02 - logprior: -1.9116e-01
Epoch 8/10
39/39 - 19s - loss: 459.6604 - loglik: -4.5954e+02 - logprior: -1.1648e-01
Fitted a model with MAP estimate = -459.2542
Time for alignment: 419.8680
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 565.7305 - loglik: -5.6386e+02 - logprior: -1.8672e+00
Epoch 2/10
39/39 - 16s - loss: 480.3188 - loglik: -4.7918e+02 - logprior: -1.1402e+00
Epoch 3/10
39/39 - 16s - loss: 473.4597 - loglik: -4.7243e+02 - logprior: -1.0256e+00
Epoch 4/10
39/39 - 17s - loss: 472.1498 - loglik: -4.7115e+02 - logprior: -9.9540e-01
Epoch 5/10
39/39 - 17s - loss: 470.7763 - loglik: -4.6979e+02 - logprior: -9.8954e-01
Epoch 6/10
39/39 - 16s - loss: 471.0782 - loglik: -4.7009e+02 - logprior: -9.9068e-01
Fitted a model with MAP estimate = -469.0583
expansions: [(25, 1), (28, 1), (29, 1), (35, 1), (56, 1), (76, 1), (82, 1), (103, 1), (104, 1), (107, 1), (138, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 472.2216 - loglik: -4.7035e+02 - logprior: -1.8737e+00
Epoch 2/2
39/39 - 18s - loss: 466.5453 - loglik: -4.6571e+02 - logprior: -8.3431e-01
Fitted a model with MAP estimate = -463.7679
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 472.2220 - loglik: -4.6953e+02 - logprior: -2.6891e+00
Epoch 2/2
39/39 - 18s - loss: 468.1417 - loglik: -4.6704e+02 - logprior: -1.1007e+00
Fitted a model with MAP estimate = -464.6417
expansions: [(0, 2), (32, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 186 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 467.5710 - loglik: -4.6590e+02 - logprior: -1.6735e+00
Epoch 2/10
39/39 - 20s - loss: 464.4806 - loglik: -4.6387e+02 - logprior: -6.1176e-01
Epoch 3/10
39/39 - 20s - loss: 462.7401 - loglik: -4.6220e+02 - logprior: -5.4491e-01
Epoch 4/10
39/39 - 20s - loss: 463.1085 - loglik: -4.6264e+02 - logprior: -4.7313e-01
Fitted a model with MAP estimate = -461.5052
Time for alignment: 335.4382
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 567.0479 - loglik: -5.6522e+02 - logprior: -1.8275e+00
Epoch 2/10
39/39 - 17s - loss: 482.9799 - loglik: -4.8181e+02 - logprior: -1.1722e+00
Epoch 3/10
39/39 - 17s - loss: 477.5596 - loglik: -4.7652e+02 - logprior: -1.0370e+00
Epoch 4/10
39/39 - 17s - loss: 475.8256 - loglik: -4.7482e+02 - logprior: -1.0099e+00
Epoch 5/10
39/39 - 17s - loss: 474.9877 - loglik: -4.7398e+02 - logprior: -1.0112e+00
Epoch 6/10
39/39 - 16s - loss: 474.8165 - loglik: -4.7380e+02 - logprior: -1.0152e+00
Epoch 7/10
39/39 - 17s - loss: 474.6710 - loglik: -4.7366e+02 - logprior: -1.0147e+00
Epoch 8/10
39/39 - 17s - loss: 473.8227 - loglik: -4.7281e+02 - logprior: -1.0114e+00
Epoch 9/10
39/39 - 16s - loss: 474.5909 - loglik: -4.7355e+02 - logprior: -1.0411e+00
Fitted a model with MAP estimate = -471.8641
expansions: [(25, 1), (30, 1), (33, 2), (37, 1), (56, 1), (79, 1), (80, 1), (81, 1), (82, 1), (101, 1), (102, 1), (136, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 477.2497 - loglik: -4.7450e+02 - logprior: -2.7544e+00
Epoch 2/2
39/39 - 18s - loss: 469.9048 - loglik: -4.6895e+02 - logprior: -9.5618e-01
Fitted a model with MAP estimate = -466.5586
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 471.4293 - loglik: -4.6958e+02 - logprior: -1.8452e+00
Epoch 2/2
39/39 - 18s - loss: 468.3166 - loglik: -4.6756e+02 - logprior: -7.5526e-01
Fitted a model with MAP estimate = -465.6302
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 471.8465 - loglik: -4.6954e+02 - logprior: -2.3090e+00
Epoch 2/10
39/39 - 19s - loss: 467.4454 - loglik: -4.6689e+02 - logprior: -5.5846e-01
Epoch 3/10
39/39 - 19s - loss: 466.4207 - loglik: -4.6587e+02 - logprior: -5.4858e-01
Epoch 4/10
39/39 - 20s - loss: 464.4042 - loglik: -4.6393e+02 - logprior: -4.7165e-01
Epoch 5/10
39/39 - 19s - loss: 464.1701 - loglik: -4.6378e+02 - logprior: -3.9319e-01
Epoch 6/10
39/39 - 19s - loss: 464.3097 - loglik: -4.6398e+02 - logprior: -3.2650e-01
Fitted a model with MAP estimate = -464.0915
Time for alignment: 425.5928
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 567.4614 - loglik: -5.6560e+02 - logprior: -1.8571e+00
Epoch 2/10
39/39 - 16s - loss: 481.5970 - loglik: -4.8034e+02 - logprior: -1.2549e+00
Epoch 3/10
39/39 - 16s - loss: 475.2659 - loglik: -4.7412e+02 - logprior: -1.1421e+00
Epoch 4/10
39/39 - 16s - loss: 473.1603 - loglik: -4.7206e+02 - logprior: -1.0993e+00
Epoch 5/10
39/39 - 16s - loss: 472.6208 - loglik: -4.7152e+02 - logprior: -1.0985e+00
Epoch 6/10
39/39 - 16s - loss: 471.7672 - loglik: -4.7067e+02 - logprior: -1.0974e+00
Epoch 7/10
39/39 - 16s - loss: 471.9440 - loglik: -4.7084e+02 - logprior: -1.1023e+00
Fitted a model with MAP estimate = -470.0594
expansions: [(29, 1), (30, 2), (57, 1), (76, 3), (77, 1), (81, 1), (82, 1), (103, 1), (105, 1), (106, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 471.6814 - loglik: -4.6978e+02 - logprior: -1.9033e+00
Epoch 2/2
39/39 - 18s - loss: 466.5144 - loglik: -4.6567e+02 - logprior: -8.4006e-01
Fitted a model with MAP estimate = -463.6501
expansions: []
discards: [ 0 31 81]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 472.6477 - loglik: -4.6994e+02 - logprior: -2.7097e+00
Epoch 2/2
39/39 - 19s - loss: 468.5034 - loglik: -4.6735e+02 - logprior: -1.1499e+00
Fitted a model with MAP estimate = -465.2406
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 468.0634 - loglik: -4.6638e+02 - logprior: -1.6818e+00
Epoch 2/10
39/39 - 19s - loss: 464.8694 - loglik: -4.6425e+02 - logprior: -6.2426e-01
Epoch 3/10
39/39 - 19s - loss: 463.8423 - loglik: -4.6330e+02 - logprior: -5.3786e-01
Epoch 4/10
39/39 - 20s - loss: 463.3312 - loglik: -4.6284e+02 - logprior: -4.8693e-01
Epoch 5/10
39/39 - 19s - loss: 462.4041 - loglik: -4.6198e+02 - logprior: -4.2064e-01
Epoch 6/10
39/39 - 19s - loss: 461.7146 - loglik: -4.6137e+02 - logprior: -3.4742e-01
Epoch 7/10
39/39 - 19s - loss: 462.8454 - loglik: -4.6257e+02 - logprior: -2.7675e-01
Fitted a model with MAP estimate = -461.7023
Time for alignment: 404.2043
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 567.8344 - loglik: -5.6602e+02 - logprior: -1.8188e+00
Epoch 2/10
39/39 - 16s - loss: 484.4122 - loglik: -4.8330e+02 - logprior: -1.1119e+00
Epoch 3/10
39/39 - 16s - loss: 476.2834 - loglik: -4.7521e+02 - logprior: -1.0767e+00
Epoch 4/10
39/39 - 16s - loss: 474.5774 - loglik: -4.7352e+02 - logprior: -1.0538e+00
Epoch 5/10
39/39 - 16s - loss: 473.7962 - loglik: -4.7275e+02 - logprior: -1.0511e+00
Epoch 6/10
39/39 - 16s - loss: 473.4722 - loglik: -4.7242e+02 - logprior: -1.0525e+00
Epoch 7/10
39/39 - 16s - loss: 473.5024 - loglik: -4.7245e+02 - logprior: -1.0540e+00
Fitted a model with MAP estimate = -471.6067
expansions: [(29, 1), (30, 4), (53, 1), (54, 1), (56, 1), (61, 1), (62, 1), (78, 1), (79, 1), (80, 1), (91, 3), (138, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 188 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 473.2009 - loglik: -4.7131e+02 - logprior: -1.8912e+00
Epoch 2/2
39/39 - 18s - loss: 467.7111 - loglik: -4.6684e+02 - logprior: -8.7280e-01
Fitted a model with MAP estimate = -464.4889
expansions: []
discards: [  0  33  34  35 105]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 474.1500 - loglik: -4.7146e+02 - logprior: -2.6885e+00
Epoch 2/2
39/39 - 19s - loss: 469.6479 - loglik: -4.6853e+02 - logprior: -1.1155e+00
Fitted a model with MAP estimate = -465.0261
expansions: [(0, 2), (32, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 467.2205 - loglik: -4.6557e+02 - logprior: -1.6472e+00
Epoch 2/10
39/39 - 20s - loss: 464.1923 - loglik: -4.6360e+02 - logprior: -5.8760e-01
Epoch 3/10
39/39 - 19s - loss: 462.4722 - loglik: -4.6196e+02 - logprior: -5.1454e-01
Epoch 4/10
39/39 - 19s - loss: 460.9924 - loglik: -4.6054e+02 - logprior: -4.4912e-01
Epoch 5/10
39/39 - 19s - loss: 461.0489 - loglik: -4.6067e+02 - logprior: -3.7760e-01
Fitted a model with MAP estimate = -460.6409
Time for alignment: 371.6633
Computed alignments with likelihoods: ['-459.2542', '-461.5052', '-464.0915', '-461.7023', '-460.6409']
Best model has likelihood: -459.2542  (prior= -0.0726 )
time for generating output: 0.2828
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00009.projection.fasta
SP score = 0.7577348479186907
Training of 5 independent models on file PF14604.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd124e9cfd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14469a820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd5089ab400>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.8502 - loglik: -1.3963e+02 - logprior: -3.2174e+00
Epoch 2/10
19/19 - 1s - loss: 113.8861 - loglik: -1.1246e+02 - logprior: -1.4310e+00
Epoch 3/10
19/19 - 1s - loss: 103.6513 - loglik: -1.0206e+02 - logprior: -1.5962e+00
Epoch 4/10
19/19 - 1s - loss: 101.6969 - loglik: -1.0025e+02 - logprior: -1.4471e+00
Epoch 5/10
19/19 - 1s - loss: 101.1161 - loglik: -9.9683e+01 - logprior: -1.4331e+00
Epoch 6/10
19/19 - 1s - loss: 100.9694 - loglik: -9.9550e+01 - logprior: -1.4191e+00
Epoch 7/10
19/19 - 1s - loss: 100.9170 - loglik: -9.9510e+01 - logprior: -1.4070e+00
Epoch 8/10
19/19 - 1s - loss: 100.7572 - loglik: -9.9358e+01 - logprior: -1.3993e+00
Epoch 9/10
19/19 - 1s - loss: 100.7034 - loglik: -9.9310e+01 - logprior: -1.3939e+00
Epoch 10/10
19/19 - 1s - loss: 100.8119 - loglik: -9.9422e+01 - logprior: -1.3904e+00
Fitted a model with MAP estimate = -100.6124
expansions: [(6, 2), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (27, 2), (28, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 108.3685 - loglik: -1.0421e+02 - logprior: -4.1539e+00
Epoch 2/2
19/19 - 1s - loss: 100.0488 - loglik: -9.7944e+01 - logprior: -2.1044e+00
Fitted a model with MAP estimate = -98.3879
expansions: [(0, 1)]
discards: [ 0  5  8 18 27 30 40 42]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.2464 - loglik: -9.7998e+01 - logprior: -3.2489e+00
Epoch 2/2
19/19 - 1s - loss: 97.4181 - loglik: -9.6026e+01 - logprior: -1.3923e+00
Fitted a model with MAP estimate = -96.8928
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 99.6527 - loglik: -9.6399e+01 - logprior: -3.2533e+00
Epoch 2/10
19/19 - 1s - loss: 97.1668 - loglik: -9.5781e+01 - logprior: -1.3860e+00
Epoch 3/10
19/19 - 1s - loss: 96.7934 - loglik: -9.5505e+01 - logprior: -1.2887e+00
Epoch 4/10
19/19 - 1s - loss: 96.5705 - loglik: -9.5324e+01 - logprior: -1.2464e+00
Epoch 5/10
19/19 - 1s - loss: 96.1268 - loglik: -9.4918e+01 - logprior: -1.2090e+00
Epoch 6/10
19/19 - 1s - loss: 96.2775 - loglik: -9.5077e+01 - logprior: -1.2008e+00
Fitted a model with MAP estimate = -95.9810
Time for alignment: 40.1492
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.9143 - loglik: -1.3970e+02 - logprior: -3.2122e+00
Epoch 2/10
19/19 - 1s - loss: 114.3587 - loglik: -1.1293e+02 - logprior: -1.4270e+00
Epoch 3/10
19/19 - 1s - loss: 103.8749 - loglik: -1.0229e+02 - logprior: -1.5884e+00
Epoch 4/10
19/19 - 1s - loss: 101.6726 - loglik: -1.0023e+02 - logprior: -1.4425e+00
Epoch 5/10
19/19 - 1s - loss: 101.1377 - loglik: -9.9704e+01 - logprior: -1.4338e+00
Epoch 6/10
19/19 - 1s - loss: 101.0443 - loglik: -9.9623e+01 - logprior: -1.4208e+00
Epoch 7/10
19/19 - 1s - loss: 100.8439 - loglik: -9.9435e+01 - logprior: -1.4092e+00
Epoch 8/10
19/19 - 1s - loss: 100.7468 - loglik: -9.9344e+01 - logprior: -1.4032e+00
Epoch 9/10
19/19 - 1s - loss: 100.7656 - loglik: -9.9366e+01 - logprior: -1.3996e+00
Fitted a model with MAP estimate = -100.6277
expansions: [(6, 2), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (27, 2), (28, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 108.4696 - loglik: -1.0432e+02 - logprior: -4.1460e+00
Epoch 2/2
19/19 - 1s - loss: 99.9953 - loglik: -9.7897e+01 - logprior: -2.0987e+00
Fitted a model with MAP estimate = -98.3555
expansions: [(0, 1)]
discards: [ 0  6  8 18 28 30 40 42]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.5383 - loglik: -9.8291e+01 - logprior: -3.2470e+00
Epoch 2/2
19/19 - 1s - loss: 97.4469 - loglik: -9.6049e+01 - logprior: -1.3983e+00
Fitted a model with MAP estimate = -96.9250
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.6825 - loglik: -9.6426e+01 - logprior: -3.2568e+00
Epoch 2/10
19/19 - 1s - loss: 97.1949 - loglik: -9.5805e+01 - logprior: -1.3897e+00
Epoch 3/10
19/19 - 1s - loss: 96.7581 - loglik: -9.5467e+01 - logprior: -1.2916e+00
Epoch 4/10
19/19 - 1s - loss: 96.5377 - loglik: -9.5292e+01 - logprior: -1.2455e+00
Epoch 5/10
19/19 - 1s - loss: 96.2296 - loglik: -9.5014e+01 - logprior: -1.2151e+00
Epoch 6/10
19/19 - 1s - loss: 96.1139 - loglik: -9.4915e+01 - logprior: -1.1988e+00
Epoch 7/10
19/19 - 1s - loss: 95.9732 - loglik: -9.4790e+01 - logprior: -1.1836e+00
Epoch 8/10
19/19 - 1s - loss: 96.0393 - loglik: -9.4864e+01 - logprior: -1.1755e+00
Fitted a model with MAP estimate = -95.8270
Time for alignment: 40.7906
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.9407 - loglik: -1.3972e+02 - logprior: -3.2164e+00
Epoch 2/10
19/19 - 1s - loss: 114.9759 - loglik: -1.1354e+02 - logprior: -1.4367e+00
Epoch 3/10
19/19 - 1s - loss: 104.3469 - loglik: -1.0276e+02 - logprior: -1.5823e+00
Epoch 4/10
19/19 - 1s - loss: 102.0930 - loglik: -1.0065e+02 - logprior: -1.4387e+00
Epoch 5/10
19/19 - 1s - loss: 101.5609 - loglik: -1.0013e+02 - logprior: -1.4294e+00
Epoch 6/10
19/19 - 1s - loss: 101.2957 - loglik: -9.9883e+01 - logprior: -1.4124e+00
Epoch 7/10
19/19 - 1s - loss: 101.0649 - loglik: -9.9665e+01 - logprior: -1.4001e+00
Epoch 8/10
19/19 - 1s - loss: 101.1137 - loglik: -9.9722e+01 - logprior: -1.3921e+00
Fitted a model with MAP estimate = -101.0236
expansions: [(6, 2), (7, 2), (8, 1), (14, 1), (20, 2), (21, 3), (26, 1), (28, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.7800 - loglik: -1.0364e+02 - logprior: -4.1368e+00
Epoch 2/2
19/19 - 1s - loss: 99.7781 - loglik: -9.7730e+01 - logprior: -2.0486e+00
Fitted a model with MAP estimate = -98.1926
expansions: [(0, 1)]
discards: [ 0  5  8 26 29 44]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.5501 - loglik: -9.7289e+01 - logprior: -3.2607e+00
Epoch 2/2
19/19 - 1s - loss: 96.9749 - loglik: -9.5566e+01 - logprior: -1.4089e+00
Fitted a model with MAP estimate = -96.4955
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.8418 - loglik: -9.6591e+01 - logprior: -3.2507e+00
Epoch 2/10
19/19 - 1s - loss: 97.1610 - loglik: -9.5780e+01 - logprior: -1.3812e+00
Epoch 3/10
19/19 - 1s - loss: 96.8298 - loglik: -9.5545e+01 - logprior: -1.2845e+00
Epoch 4/10
19/19 - 1s - loss: 96.5208 - loglik: -9.5281e+01 - logprior: -1.2400e+00
Epoch 5/10
19/19 - 1s - loss: 96.1418 - loglik: -9.4936e+01 - logprior: -1.2056e+00
Epoch 6/10
19/19 - 1s - loss: 96.0938 - loglik: -9.4901e+01 - logprior: -1.1924e+00
Epoch 7/10
19/19 - 1s - loss: 96.0989 - loglik: -9.4918e+01 - logprior: -1.1811e+00
Fitted a model with MAP estimate = -95.8652
Time for alignment: 37.8744
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.9304 - loglik: -1.3972e+02 - logprior: -3.2149e+00
Epoch 2/10
19/19 - 1s - loss: 113.7683 - loglik: -1.1234e+02 - logprior: -1.4273e+00
Epoch 3/10
19/19 - 1s - loss: 104.4811 - loglik: -1.0290e+02 - logprior: -1.5849e+00
Epoch 4/10
19/19 - 1s - loss: 102.1557 - loglik: -1.0072e+02 - logprior: -1.4313e+00
Epoch 5/10
19/19 - 1s - loss: 101.4011 - loglik: -9.9985e+01 - logprior: -1.4163e+00
Epoch 6/10
19/19 - 1s - loss: 101.4764 - loglik: -1.0008e+02 - logprior: -1.3999e+00
Fitted a model with MAP estimate = -101.1570
expansions: [(6, 2), (7, 2), (8, 1), (13, 1), (18, 2), (19, 2), (20, 3), (27, 2), (28, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.9765 - loglik: -1.0384e+02 - logprior: -4.1408e+00
Epoch 2/2
19/19 - 1s - loss: 99.8666 - loglik: -9.7835e+01 - logprior: -2.0314e+00
Fitted a model with MAP estimate = -98.0790
expansions: [(0, 1)]
discards: [ 0  5  8 24 27 30 40 42]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.1586 - loglik: -9.7905e+01 - logprior: -3.2536e+00
Epoch 2/2
19/19 - 1s - loss: 97.3807 - loglik: -9.5989e+01 - logprior: -1.3919e+00
Fitted a model with MAP estimate = -96.8977
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.6691 - loglik: -9.6418e+01 - logprior: -3.2514e+00
Epoch 2/10
19/19 - 1s - loss: 97.2240 - loglik: -9.5838e+01 - logprior: -1.3865e+00
Epoch 3/10
19/19 - 1s - loss: 96.8015 - loglik: -9.5510e+01 - logprior: -1.2911e+00
Epoch 4/10
19/19 - 1s - loss: 96.3813 - loglik: -9.5140e+01 - logprior: -1.2412e+00
Epoch 5/10
19/19 - 1s - loss: 96.3167 - loglik: -9.5102e+01 - logprior: -1.2143e+00
Epoch 6/10
19/19 - 1s - loss: 96.3249 - loglik: -9.5129e+01 - logprior: -1.1959e+00
Fitted a model with MAP estimate = -96.0672
Time for alignment: 36.0352
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.9469 - loglik: -1.3973e+02 - logprior: -3.2147e+00
Epoch 2/10
19/19 - 1s - loss: 115.7362 - loglik: -1.1430e+02 - logprior: -1.4394e+00
Epoch 3/10
19/19 - 1s - loss: 105.7771 - loglik: -1.0420e+02 - logprior: -1.5744e+00
Epoch 4/10
19/19 - 1s - loss: 103.0619 - loglik: -1.0164e+02 - logprior: -1.4252e+00
Epoch 5/10
19/19 - 1s - loss: 102.0272 - loglik: -1.0061e+02 - logprior: -1.4170e+00
Epoch 6/10
19/19 - 1s - loss: 101.7118 - loglik: -1.0031e+02 - logprior: -1.4016e+00
Epoch 7/10
19/19 - 1s - loss: 101.5491 - loglik: -1.0016e+02 - logprior: -1.3899e+00
Epoch 8/10
19/19 - 1s - loss: 101.2494 - loglik: -9.9868e+01 - logprior: -1.3815e+00
Epoch 9/10
19/19 - 1s - loss: 101.2879 - loglik: -9.9912e+01 - logprior: -1.3760e+00
Fitted a model with MAP estimate = -101.2126
expansions: [(6, 2), (7, 2), (8, 1), (19, 2), (20, 2), (21, 1), (27, 1), (28, 3), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 108.1571 - loglik: -1.0402e+02 - logprior: -4.1380e+00
Epoch 2/2
19/19 - 1s - loss: 99.9538 - loglik: -9.7885e+01 - logprior: -2.0692e+00
Fitted a model with MAP estimate = -98.3294
expansions: [(0, 1)]
discards: [ 0  6  8 24 27 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.2666 - loglik: -9.8018e+01 - logprior: -3.2484e+00
Epoch 2/2
19/19 - 1s - loss: 97.3426 - loglik: -9.5949e+01 - logprior: -1.3936e+00
Fitted a model with MAP estimate = -96.8958
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.7125 - loglik: -9.6458e+01 - logprior: -3.2549e+00
Epoch 2/10
19/19 - 1s - loss: 97.2580 - loglik: -9.5870e+01 - logprior: -1.3879e+00
Epoch 3/10
19/19 - 1s - loss: 96.6749 - loglik: -9.5385e+01 - logprior: -1.2903e+00
Epoch 4/10
19/19 - 1s - loss: 96.7175 - loglik: -9.5474e+01 - logprior: -1.2431e+00
Fitted a model with MAP estimate = -96.3231
Time for alignment: 35.9224
Computed alignments with likelihoods: ['-95.9810', '-95.8270', '-95.8652', '-96.0672', '-96.3231']
Best model has likelihood: -95.8270  (prior= -1.1745 )
time for generating output: 0.0992
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF14604.projection.fasta
SP score = 0.7450404114621602
Training of 5 independent models on file PF00625.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd12501bbb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd508235e80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd124b163a0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 457.6466 - loglik: -4.5480e+02 - logprior: -2.8457e+00
Epoch 2/10
19/19 - 5s - loss: 382.1957 - loglik: -3.8091e+02 - logprior: -1.2864e+00
Epoch 3/10
19/19 - 5s - loss: 354.4594 - loglik: -3.5287e+02 - logprior: -1.5873e+00
Epoch 4/10
19/19 - 6s - loss: 347.7256 - loglik: -3.4603e+02 - logprior: -1.6908e+00
Epoch 5/10
19/19 - 6s - loss: 344.9841 - loglik: -3.4334e+02 - logprior: -1.6442e+00
Epoch 6/10
19/19 - 6s - loss: 341.0614 - loglik: -3.3942e+02 - logprior: -1.6448e+00
Epoch 7/10
19/19 - 6s - loss: 340.9014 - loglik: -3.3928e+02 - logprior: -1.6185e+00
Epoch 8/10
19/19 - 6s - loss: 340.0222 - loglik: -3.3841e+02 - logprior: -1.6111e+00
Epoch 9/10
19/19 - 6s - loss: 339.7315 - loglik: -3.3813e+02 - logprior: -1.6020e+00
Epoch 10/10
19/19 - 7s - loss: 338.9411 - loglik: -3.3734e+02 - logprior: -1.6018e+00
Fitted a model with MAP estimate = -336.2046
expansions: [(0, 2), (11, 1), (13, 1), (15, 1), (16, 1), (17, 1), (18, 1), (20, 1), (22, 1), (30, 1), (38, 1), (44, 1), (48, 1), (50, 1), (52, 1), (57, 1), (66, 1), (67, 1), (69, 1), (70, 1), (80, 1), (88, 1), (89, 1), (90, 1), (96, 1), (99, 1), (100, 1), (112, 1), (113, 2), (114, 2), (116, 1), (118, 1), (124, 1), (136, 1), (137, 1), (138, 1), (139, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 327.6830 - loglik: -3.2536e+02 - logprior: -2.3210e+00
Epoch 2/2
39/39 - 10s - loss: 317.6262 - loglik: -3.1673e+02 - logprior: -8.9747e-01
Fitted a model with MAP estimate = -313.0560
expansions: []
discards: [  0 143]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 321.5061 - loglik: -3.1876e+02 - logprior: -2.7447e+00
Epoch 2/2
39/39 - 9s - loss: 317.3607 - loglik: -3.1657e+02 - logprior: -7.8856e-01
Fitted a model with MAP estimate = -313.1114
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 316.5758 - loglik: -3.1489e+02 - logprior: -1.6902e+00
Epoch 2/10
39/39 - 9s - loss: 313.8212 - loglik: -3.1326e+02 - logprior: -5.6026e-01
Epoch 3/10
39/39 - 10s - loss: 312.3426 - loglik: -3.1184e+02 - logprior: -5.0508e-01
Epoch 4/10
39/39 - 10s - loss: 311.7774 - loglik: -3.1133e+02 - logprior: -4.4891e-01
Epoch 5/10
39/39 - 10s - loss: 310.4049 - loglik: -3.1000e+02 - logprior: -4.0322e-01
Epoch 6/10
39/39 - 10s - loss: 311.5156 - loglik: -3.1118e+02 - logprior: -3.3903e-01
Fitted a model with MAP estimate = -310.2631
Time for alignment: 215.1623
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 458.3268 - loglik: -4.5549e+02 - logprior: -2.8355e+00
Epoch 2/10
19/19 - 6s - loss: 380.4017 - loglik: -3.7915e+02 - logprior: -1.2494e+00
Epoch 3/10
19/19 - 6s - loss: 349.8766 - loglik: -3.4834e+02 - logprior: -1.5411e+00
Epoch 4/10
19/19 - 6s - loss: 343.9247 - loglik: -3.4227e+02 - logprior: -1.6559e+00
Epoch 5/10
19/19 - 6s - loss: 342.0217 - loglik: -3.4040e+02 - logprior: -1.6211e+00
Epoch 6/10
19/19 - 7s - loss: 340.2691 - loglik: -3.3871e+02 - logprior: -1.5627e+00
Epoch 7/10
19/19 - 7s - loss: 340.4504 - loglik: -3.3890e+02 - logprior: -1.5537e+00
Fitted a model with MAP estimate = -337.1180
expansions: [(0, 2), (14, 1), (16, 1), (17, 1), (18, 2), (19, 1), (21, 1), (22, 1), (36, 1), (43, 2), (44, 1), (48, 1), (49, 1), (50, 1), (51, 1), (66, 2), (67, 1), (69, 1), (70, 1), (80, 1), (87, 2), (88, 2), (89, 1), (101, 1), (113, 1), (114, 2), (116, 1), (118, 1), (124, 1), (136, 1), (137, 1), (138, 1), (139, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 328.5589 - loglik: -3.2618e+02 - logprior: -2.3775e+00
Epoch 2/2
39/39 - 11s - loss: 318.0061 - loglik: -3.1703e+02 - logprior: -9.8104e-01
Fitted a model with MAP estimate = -313.8852
expansions: []
discards: [  0  84 113 115]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 323.4839 - loglik: -3.2071e+02 - logprior: -2.7717e+00
Epoch 2/2
39/39 - 10s - loss: 319.3093 - loglik: -3.1847e+02 - logprior: -8.4121e-01
Fitted a model with MAP estimate = -314.8979
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 318.3757 - loglik: -3.1662e+02 - logprior: -1.7567e+00
Epoch 2/10
39/39 - 10s - loss: 316.0894 - loglik: -3.1545e+02 - logprior: -6.3666e-01
Epoch 3/10
39/39 - 11s - loss: 314.0013 - loglik: -3.1343e+02 - logprior: -5.6821e-01
Epoch 4/10
39/39 - 11s - loss: 313.7746 - loglik: -3.1325e+02 - logprior: -5.2475e-01
Epoch 5/10
39/39 - 10s - loss: 312.6212 - loglik: -3.1215e+02 - logprior: -4.7101e-01
Epoch 6/10
39/39 - 10s - loss: 312.9861 - loglik: -3.1257e+02 - logprior: -4.1637e-01
Fitted a model with MAP estimate = -312.1553
Time for alignment: 208.8736
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 458.4138 - loglik: -4.5558e+02 - logprior: -2.8329e+00
Epoch 2/10
19/19 - 6s - loss: 382.2108 - loglik: -3.8091e+02 - logprior: -1.3018e+00
Epoch 3/10
19/19 - 6s - loss: 353.3649 - loglik: -3.5178e+02 - logprior: -1.5808e+00
Epoch 4/10
19/19 - 6s - loss: 345.8662 - loglik: -3.4422e+02 - logprior: -1.6461e+00
Epoch 5/10
19/19 - 6s - loss: 342.7321 - loglik: -3.4115e+02 - logprior: -1.5869e+00
Epoch 6/10
19/19 - 6s - loss: 342.3044 - loglik: -3.4074e+02 - logprior: -1.5616e+00
Epoch 7/10
19/19 - 7s - loss: 341.3410 - loglik: -3.3980e+02 - logprior: -1.5390e+00
Epoch 8/10
19/19 - 7s - loss: 341.1220 - loglik: -3.3960e+02 - logprior: -1.5189e+00
Epoch 9/10
19/19 - 7s - loss: 340.9939 - loglik: -3.3947e+02 - logprior: -1.5269e+00
Epoch 10/10
19/19 - 7s - loss: 340.7383 - loglik: -3.3922e+02 - logprior: -1.5222e+00
Fitted a model with MAP estimate = -337.3986
expansions: [(0, 2), (14, 1), (15, 2), (16, 1), (17, 1), (18, 1), (21, 1), (22, 1), (36, 1), (43, 2), (44, 1), (48, 1), (50, 1), (52, 1), (57, 1), (66, 1), (67, 1), (69, 1), (70, 1), (80, 1), (87, 2), (88, 2), (89, 1), (91, 1), (100, 1), (112, 1), (113, 1), (114, 2), (116, 1), (118, 1), (123, 1), (136, 2), (137, 1), (138, 1), (139, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 328.2965 - loglik: -3.2593e+02 - logprior: -2.3623e+00
Epoch 2/2
39/39 - 10s - loss: 317.3348 - loglik: -3.1642e+02 - logprior: -9.1608e-01
Fitted a model with MAP estimate = -313.2318
expansions: [(93, 1)]
discards: [  0  55 112 174]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 321.5119 - loglik: -3.1878e+02 - logprior: -2.7333e+00
Epoch 2/2
39/39 - 10s - loss: 316.6339 - loglik: -3.1586e+02 - logprior: -7.7056e-01
Fitted a model with MAP estimate = -312.1292
expansions: []
discards: [112]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 316.4558 - loglik: -3.1481e+02 - logprior: -1.6438e+00
Epoch 2/10
39/39 - 9s - loss: 313.6768 - loglik: -3.1316e+02 - logprior: -5.1625e-01
Epoch 3/10
39/39 - 9s - loss: 311.8778 - loglik: -3.1144e+02 - logprior: -4.4179e-01
Epoch 4/10
39/39 - 9s - loss: 311.4609 - loglik: -3.1106e+02 - logprior: -3.9720e-01
Epoch 5/10
39/39 - 10s - loss: 310.6925 - loglik: -3.1035e+02 - logprior: -3.4650e-01
Epoch 6/10
39/39 - 10s - loss: 310.1629 - loglik: -3.0988e+02 - logprior: -2.7795e-01
Epoch 7/10
39/39 - 9s - loss: 310.2390 - loglik: -3.1003e+02 - logprior: -2.1057e-01
Fitted a model with MAP estimate = -309.8319
Time for alignment: 231.5551
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 458.2028 - loglik: -4.5535e+02 - logprior: -2.8494e+00
Epoch 2/10
19/19 - 6s - loss: 381.1844 - loglik: -3.7989e+02 - logprior: -1.2991e+00
Epoch 3/10
19/19 - 6s - loss: 350.1255 - loglik: -3.4851e+02 - logprior: -1.6197e+00
Epoch 4/10
19/19 - 6s - loss: 341.4514 - loglik: -3.3976e+02 - logprior: -1.6932e+00
Epoch 5/10
19/19 - 7s - loss: 337.4396 - loglik: -3.3580e+02 - logprior: -1.6432e+00
Epoch 6/10
19/19 - 6s - loss: 337.5170 - loglik: -3.3590e+02 - logprior: -1.6126e+00
Fitted a model with MAP estimate = -333.4933
expansions: [(0, 2), (14, 1), (16, 1), (17, 1), (18, 2), (19, 1), (21, 1), (22, 1), (36, 1), (43, 2), (44, 1), (48, 1), (50, 1), (52, 1), (57, 1), (66, 1), (67, 1), (69, 1), (70, 1), (71, 1), (85, 1), (87, 1), (88, 1), (89, 2), (101, 1), (103, 1), (113, 1), (114, 2), (116, 1), (118, 1), (123, 1), (133, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 325.1102 - loglik: -3.2284e+02 - logprior: -2.2727e+00
Epoch 2/2
39/39 - 10s - loss: 315.0821 - loglik: -3.1419e+02 - logprior: -8.9192e-01
Fitted a model with MAP estimate = -310.9416
expansions: [(141, 1), (161, 1)]
discards: [  0 115]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 318.8055 - loglik: -3.1605e+02 - logprior: -2.7552e+00
Epoch 2/2
39/39 - 10s - loss: 314.5003 - loglik: -3.1368e+02 - logprior: -8.1872e-01
Fitted a model with MAP estimate = -310.1043
expansions: []
discards: [54]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 313.6717 - loglik: -3.1197e+02 - logprior: -1.7011e+00
Epoch 2/10
39/39 - 10s - loss: 311.4453 - loglik: -3.1089e+02 - logprior: -5.5687e-01
Epoch 3/10
39/39 - 11s - loss: 309.6573 - loglik: -3.0917e+02 - logprior: -4.8906e-01
Epoch 4/10
39/39 - 10s - loss: 308.7234 - loglik: -3.0827e+02 - logprior: -4.4997e-01
Epoch 5/10
39/39 - 11s - loss: 308.3474 - loglik: -3.0795e+02 - logprior: -3.9549e-01
Epoch 6/10
39/39 - 10s - loss: 307.4955 - loglik: -3.0714e+02 - logprior: -3.5056e-01
Epoch 7/10
39/39 - 11s - loss: 308.1382 - loglik: -3.0786e+02 - logprior: -2.8233e-01
Fitted a model with MAP estimate = -307.3378
Time for alignment: 213.2656
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 458.2709 - loglik: -4.5544e+02 - logprior: -2.8352e+00
Epoch 2/10
19/19 - 7s - loss: 382.7430 - loglik: -3.8146e+02 - logprior: -1.2810e+00
Epoch 3/10
19/19 - 7s - loss: 353.5324 - loglik: -3.5201e+02 - logprior: -1.5241e+00
Epoch 4/10
19/19 - 7s - loss: 344.7833 - loglik: -3.4319e+02 - logprior: -1.5892e+00
Epoch 5/10
19/19 - 7s - loss: 341.1041 - loglik: -3.3954e+02 - logprior: -1.5592e+00
Epoch 6/10
19/19 - 7s - loss: 339.8806 - loglik: -3.3833e+02 - logprior: -1.5484e+00
Epoch 7/10
19/19 - 7s - loss: 339.4779 - loglik: -3.3796e+02 - logprior: -1.5216e+00
Epoch 8/10
19/19 - 7s - loss: 338.5397 - loglik: -3.3702e+02 - logprior: -1.5200e+00
Epoch 9/10
19/19 - 7s - loss: 338.6870 - loglik: -3.3717e+02 - logprior: -1.5194e+00
Fitted a model with MAP estimate = -335.2207
expansions: [(0, 2), (14, 1), (16, 2), (17, 1), (18, 2), (21, 1), (22, 1), (36, 1), (43, 2), (44, 1), (48, 1), (49, 1), (50, 1), (51, 1), (66, 2), (67, 1), (69, 1), (70, 1), (71, 1), (80, 1), (87, 2), (88, 2), (89, 1), (91, 1), (100, 1), (112, 1), (113, 2), (114, 2), (116, 1), (118, 1), (123, 1), (136, 1), (137, 1), (138, 1), (139, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 326.2001 - loglik: -3.2385e+02 - logprior: -2.3489e+00
Epoch 2/2
39/39 - 11s - loss: 314.9440 - loglik: -3.1400e+02 - logprior: -9.4320e-01
Fitted a model with MAP estimate = -310.4786
expansions: [(25, 1)]
discards: [  0  20  84 114 116 147]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 319.8668 - loglik: -3.1716e+02 - logprior: -2.7025e+00
Epoch 2/2
39/39 - 10s - loss: 314.9451 - loglik: -3.1421e+02 - logprior: -7.3102e-01
Fitted a model with MAP estimate = -310.7879
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 314.0577 - loglik: -3.1237e+02 - logprior: -1.6877e+00
Epoch 2/10
39/39 - 10s - loss: 311.7457 - loglik: -3.1120e+02 - logprior: -5.4710e-01
Epoch 3/10
39/39 - 10s - loss: 310.0941 - loglik: -3.0961e+02 - logprior: -4.8278e-01
Epoch 4/10
39/39 - 10s - loss: 308.9886 - loglik: -3.0856e+02 - logprior: -4.3157e-01
Epoch 5/10
39/39 - 10s - loss: 308.7753 - loglik: -3.0838e+02 - logprior: -3.9216e-01
Epoch 6/10
39/39 - 10s - loss: 307.8045 - loglik: -3.0749e+02 - logprior: -3.1281e-01
Epoch 7/10
39/39 - 10s - loss: 308.3603 - loglik: -3.0810e+02 - logprior: -2.5957e-01
Fitted a model with MAP estimate = -307.6958
Time for alignment: 234.9723
Computed alignments with likelihoods: ['-310.2631', '-312.1553', '-309.8319', '-307.3378', '-307.6958']
Best model has likelihood: -307.3378  (prior= -0.2811 )
time for generating output: 0.3861
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00625.projection.fasta
SP score = 0.35218265421166783
Training of 5 independent models on file PF00476.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd126926550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd4e5de7ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd349ca20a0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 902.9564 - loglik: -9.0157e+02 - logprior: -1.3861e+00
Epoch 2/10
39/39 - 52s - loss: 724.8077 - loglik: -7.2305e+02 - logprior: -1.7611e+00
Epoch 3/10
39/39 - 51s - loss: 708.4471 - loglik: -7.0662e+02 - logprior: -1.8263e+00
Epoch 4/10
39/39 - 56s - loss: 703.2458 - loglik: -7.0141e+02 - logprior: -1.8314e+00
Epoch 5/10
39/39 - 56s - loss: 701.2789 - loglik: -6.9947e+02 - logprior: -1.8073e+00
Epoch 6/10
39/39 - 56s - loss: 699.9627 - loglik: -6.9813e+02 - logprior: -1.8375e+00
Epoch 7/10
39/39 - 58s - loss: 699.5610 - loglik: -6.9779e+02 - logprior: -1.7743e+00
Epoch 8/10
39/39 - 54s - loss: 699.5988 - loglik: -6.9781e+02 - logprior: -1.7912e+00
Fitted a model with MAP estimate = -698.5647
expansions: [(0, 19), (25, 1), (46, 1), (55, 1), (60, 1), (62, 1), (67, 1), (73, 1), (74, 1), (80, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (94, 1), (113, 1), (114, 1), (119, 1), (123, 2), (125, 3), (127, 1), (142, 1), (144, 1), (149, 1), (151, 1), (155, 1), (158, 1), (161, 1), (162, 1), (163, 1), (165, 1), (166, 1), (180, 1), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (227, 1), (228, 1), (229, 1), (230, 3), (256, 1), (257, 3), (258, 2), (259, 2), (260, 1), (277, 1), (278, 2), (279, 1), (280, 1), (284, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 400 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 89s - loss: 684.3242 - loglik: -6.8190e+02 - logprior: -2.4264e+00
Epoch 2/2
39/39 - 85s - loss: 661.4929 - loglik: -6.6019e+02 - logprior: -1.3039e+00
Fitted a model with MAP estimate = -657.4364
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16 162
 167 246 337 363 379 380]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 87s - loss: 667.0897 - loglik: -6.6508e+02 - logprior: -2.0073e+00
Epoch 2/2
39/39 - 85s - loss: 660.5978 - loglik: -6.6044e+02 - logprior: -1.5892e-01
Fitted a model with MAP estimate = -657.7540
expansions: [(0, 19)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 395 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 87s - loss: 665.1949 - loglik: -6.6341e+02 - logprior: -1.7873e+00
Epoch 2/10
39/39 - 85s - loss: 660.0939 - loglik: -6.5964e+02 - logprior: -4.5226e-01
Epoch 3/10
39/39 - 93s - loss: 656.1161 - loglik: -6.5556e+02 - logprior: -5.5376e-01
Epoch 4/10
39/39 - 97s - loss: 652.6479 - loglik: -6.5209e+02 - logprior: -5.5548e-01
Epoch 5/10
39/39 - 78s - loss: 650.9655 - loglik: -6.5068e+02 - logprior: -2.8578e-01
Epoch 6/10
39/39 - 66s - loss: 650.0480 - loglik: -6.5013e+02 - logprior: 0.0868
Epoch 7/10
39/39 - 63s - loss: 650.1550 - loglik: -6.5051e+02 - logprior: 0.3546
Fitted a model with MAP estimate = -648.9952
Time for alignment: 1741.8107
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 42s - loss: 904.7412 - loglik: -9.0337e+02 - logprior: -1.3662e+00
Epoch 2/10
39/39 - 39s - loss: 724.0630 - loglik: -7.2243e+02 - logprior: -1.6318e+00
Epoch 3/10
39/39 - 40s - loss: 708.7451 - loglik: -7.0704e+02 - logprior: -1.7046e+00
Epoch 4/10
39/39 - 40s - loss: 704.3488 - loglik: -7.0269e+02 - logprior: -1.6601e+00
Epoch 5/10
39/39 - 40s - loss: 702.0752 - loglik: -7.0041e+02 - logprior: -1.6627e+00
Epoch 6/10
39/39 - 40s - loss: 701.1846 - loglik: -6.9956e+02 - logprior: -1.6212e+00
Epoch 7/10
39/39 - 40s - loss: 701.6495 - loglik: -6.9999e+02 - logprior: -1.6612e+00
Fitted a model with MAP estimate = -700.0205
expansions: [(0, 18), (21, 1), (46, 1), (56, 1), (60, 1), (62, 2), (66, 1), (72, 1), (73, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (94, 1), (113, 1), (114, 1), (119, 1), (123, 2), (125, 3), (127, 1), (144, 1), (146, 1), (149, 1), (151, 1), (155, 1), (158, 1), (161, 1), (162, 1), (163, 1), (165, 1), (166, 1), (182, 1), (186, 1), (188, 2), (189, 1), (194, 1), (197, 1), (206, 1), (209, 2), (210, 3), (219, 1), (229, 1), (230, 1), (231, 3), (233, 1), (256, 1), (257, 1), (258, 1), (259, 4), (260, 2), (261, 1), (279, 2), (280, 1), (281, 1), (282, 1), (284, 1), (287, 1), (289, 1), (296, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 399 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 67s - loss: 684.1489 - loglik: -6.8178e+02 - logprior: -2.3659e+00
Epoch 2/2
39/39 - 64s - loss: 662.2866 - loglik: -6.6114e+02 - logprior: -1.1483e+00
Fitted a model with MAP estimate = -657.9457
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15 161 166
 245 275 338 341 365]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 77s - loss: 667.1768 - loglik: -6.6509e+02 - logprior: -2.0827e+00
Epoch 2/2
39/39 - 85s - loss: 661.4008 - loglik: -6.6127e+02 - logprior: -1.3197e-01
Fitted a model with MAP estimate = -658.3148
expansions: [(0, 19)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 395 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 91s - loss: 665.6163 - loglik: -6.6376e+02 - logprior: -1.8547e+00
Epoch 2/10
39/39 - 82s - loss: 660.9661 - loglik: -6.6059e+02 - logprior: -3.7496e-01
Epoch 3/10
39/39 - 90s - loss: 656.6326 - loglik: -6.5614e+02 - logprior: -4.9516e-01
Epoch 4/10
39/39 - 81s - loss: 652.3534 - loglik: -6.5177e+02 - logprior: -5.8688e-01
Epoch 5/10
39/39 - 96s - loss: 652.2529 - loglik: -6.5179e+02 - logprior: -4.6470e-01
Epoch 6/10
39/39 - 100s - loss: 651.0198 - loglik: -6.5067e+02 - logprior: -3.5241e-01
Epoch 7/10
39/39 - 100s - loss: 651.0585 - loglik: -6.5090e+02 - logprior: -1.5858e-01
Fitted a model with MAP estimate = -650.1163
Time for alignment: 1537.1366
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 60s - loss: 904.9160 - loglik: -9.0354e+02 - logprior: -1.3738e+00
Epoch 2/10
39/39 - 61s - loss: 727.2629 - loglik: -7.2558e+02 - logprior: -1.6845e+00
Epoch 3/10
39/39 - 57s - loss: 710.0737 - loglik: -7.0836e+02 - logprior: -1.7111e+00
Epoch 4/10
39/39 - 54s - loss: 705.4713 - loglik: -7.0378e+02 - logprior: -1.6904e+00
Epoch 5/10
39/39 - 59s - loss: 702.6578 - loglik: -7.0093e+02 - logprior: -1.7296e+00
Epoch 6/10
39/39 - 54s - loss: 702.5285 - loglik: -7.0082e+02 - logprior: -1.7071e+00
Epoch 7/10
39/39 - 49s - loss: 702.2370 - loglik: -7.0054e+02 - logprior: -1.7015e+00
Epoch 8/10
39/39 - 55s - loss: 701.1567 - loglik: -6.9945e+02 - logprior: -1.7039e+00
Epoch 9/10
39/39 - 56s - loss: 701.6258 - loglik: -6.9988e+02 - logprior: -1.7490e+00
Fitted a model with MAP estimate = -700.6650
expansions: [(0, 18), (20, 1), (42, 1), (45, 1), (60, 1), (62, 1), (67, 1), (73, 1), (74, 1), (82, 1), (84, 3), (85, 3), (93, 1), (104, 1), (114, 1), (115, 1), (120, 1), (124, 2), (126, 3), (128, 1), (143, 1), (145, 1), (147, 1), (150, 1), (156, 1), (159, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1), (169, 1), (182, 1), (186, 1), (190, 1), (191, 1), (194, 1), (205, 1), (206, 1), (209, 2), (210, 3), (228, 1), (231, 1), (232, 3), (233, 1), (257, 1), (258, 3), (259, 2), (260, 2), (261, 3), (277, 1), (278, 2), (279, 1), (281, 1), (284, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 401 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 101s - loss: 684.3462 - loglik: -6.8192e+02 - logprior: -2.4309e+00
Epoch 2/2
39/39 - 102s - loss: 661.1085 - loglik: -6.6009e+02 - logprior: -1.0175e+00
Fitted a model with MAP estimate = -656.2711
expansions: []
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15 112 115 162
 167 275 339 340 364 380 381]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 94s - loss: 665.5441 - loglik: -6.6408e+02 - logprior: -1.4620e+00
Epoch 2/2
39/39 - 80s - loss: 660.7753 - loglik: -6.6032e+02 - logprior: -4.5340e-01
Fitted a model with MAP estimate = -657.4682
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 76s - loss: 663.9842 - loglik: -6.6267e+02 - logprior: -1.3145e+00
Epoch 2/10
39/39 - 84s - loss: 660.1803 - loglik: -6.6005e+02 - logprior: -1.3230e-01
Epoch 3/10
39/39 - 73s - loss: 656.4585 - loglik: -6.5642e+02 - logprior: -3.9908e-02
Epoch 4/10
39/39 - 88s - loss: 652.4522 - loglik: -6.5249e+02 - logprior: 0.0416
Epoch 5/10
39/39 - 92s - loss: 651.9753 - loglik: -6.5215e+02 - logprior: 0.1768
Epoch 6/10
39/39 - 88s - loss: 650.9814 - loglik: -6.5130e+02 - logprior: 0.3146
Epoch 7/10
39/39 - 91s - loss: 650.3042 - loglik: -6.5088e+02 - logprior: 0.5754
Epoch 8/10
39/39 - 85s - loss: 650.8994 - loglik: -6.5162e+02 - logprior: 0.7222
Fitted a model with MAP estimate = -649.6247
Time for alignment: 1963.5443
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 58s - loss: 904.5688 - loglik: -9.0318e+02 - logprior: -1.3855e+00
Epoch 2/10
39/39 - 58s - loss: 724.9214 - loglik: -7.2321e+02 - logprior: -1.7122e+00
Epoch 3/10
39/39 - 49s - loss: 708.7639 - loglik: -7.0698e+02 - logprior: -1.7817e+00
Epoch 4/10
39/39 - 52s - loss: 703.6243 - loglik: -7.0188e+02 - logprior: -1.7471e+00
Epoch 5/10
39/39 - 59s - loss: 701.7499 - loglik: -7.0001e+02 - logprior: -1.7429e+00
Epoch 6/10
39/39 - 52s - loss: 700.8673 - loglik: -6.9910e+02 - logprior: -1.7702e+00
Epoch 7/10
39/39 - 52s - loss: 700.9241 - loglik: -6.9915e+02 - logprior: -1.7706e+00
Fitted a model with MAP estimate = -699.3436
expansions: [(0, 18), (36, 1), (42, 1), (45, 1), (59, 1), (61, 2), (66, 1), (69, 1), (71, 1), (72, 1), (80, 1), (82, 2), (83, 1), (84, 1), (90, 1), (91, 1), (92, 1), (102, 1), (111, 1), (112, 1), (117, 1), (121, 2), (124, 2), (126, 1), (141, 1), (143, 1), (145, 1), (148, 1), (154, 1), (157, 1), (159, 1), (161, 1), (162, 1), (164, 1), (165, 1), (170, 1), (185, 1), (187, 2), (188, 1), (189, 1), (196, 1), (205, 1), (208, 2), (209, 3), (219, 1), (228, 1), (229, 1), (230, 3), (232, 1), (255, 1), (256, 1), (257, 1), (258, 2), (259, 2), (260, 1), (279, 2), (280, 1), (281, 1), (282, 1), (284, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 400 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 105s - loss: 684.7128 - loglik: -6.8236e+02 - logprior: -2.3491e+00
Epoch 2/2
39/39 - 99s - loss: 660.9979 - loglik: -6.6005e+02 - logprior: -9.4469e-01
Fitted a model with MAP estimate = -656.5077
expansions: []
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15 161 166 245
 275 338 364 379 380]
Re-initialized the encoder parameters.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 85s - loss: 665.6454 - loglik: -6.6410e+02 - logprior: -1.5429e+00
Epoch 2/2
39/39 - 79s - loss: 660.7013 - loglik: -6.6033e+02 - logprior: -3.6921e-01
Fitted a model with MAP estimate = -657.3177
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 377 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 76s - loss: 663.7526 - loglik: -6.6249e+02 - logprior: -1.2657e+00
Epoch 2/10
39/39 - 82s - loss: 659.4468 - loglik: -6.5940e+02 - logprior: -4.8641e-02
Epoch 3/10
39/39 - 92s - loss: 656.1636 - loglik: -6.5603e+02 - logprior: -1.3706e-01
Epoch 4/10
39/39 - 86s - loss: 653.3184 - loglik: -6.5350e+02 - logprior: 0.1822
Epoch 5/10
39/39 - 74s - loss: 651.2655 - loglik: -6.5157e+02 - logprior: 0.3057
Epoch 6/10
39/39 - 69s - loss: 650.5421 - loglik: -6.5100e+02 - logprior: 0.4567
Epoch 7/10
39/39 - 70s - loss: 650.8548 - loglik: -6.5143e+02 - logprior: 0.5798
Fitted a model with MAP estimate = -649.8737
Time for alignment: 1696.5889
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 902.9277 - loglik: -9.0156e+02 - logprior: -1.3692e+00
Epoch 2/10
39/39 - 47s - loss: 726.1396 - loglik: -7.2454e+02 - logprior: -1.6005e+00
Epoch 3/10
39/39 - 47s - loss: 711.6913 - loglik: -7.1014e+02 - logprior: -1.5520e+00
Epoch 4/10
39/39 - 46s - loss: 706.8469 - loglik: -7.0531e+02 - logprior: -1.5368e+00
Epoch 5/10
39/39 - 47s - loss: 704.4552 - loglik: -7.0292e+02 - logprior: -1.5349e+00
Epoch 6/10
39/39 - 47s - loss: 703.7650 - loglik: -7.0222e+02 - logprior: -1.5430e+00
Epoch 7/10
39/39 - 47s - loss: 703.6104 - loglik: -7.0203e+02 - logprior: -1.5801e+00
Epoch 8/10
39/39 - 46s - loss: 703.2344 - loglik: -7.0172e+02 - logprior: -1.5189e+00
Epoch 9/10
39/39 - 46s - loss: 702.3377 - loglik: -7.0080e+02 - logprior: -1.5421e+00
Epoch 10/10
39/39 - 47s - loss: 702.3290 - loglik: -7.0080e+02 - logprior: -1.5264e+00
Fitted a model with MAP estimate = -701.2618
expansions: [(0, 18), (21, 1), (42, 1), (45, 1), (60, 1), (62, 2), (66, 1), (72, 1), (73, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (114, 2), (119, 1), (123, 2), (125, 3), (127, 1), (142, 1), (144, 1), (146, 1), (149, 1), (155, 1), (158, 1), (160, 1), (162, 1), (163, 1), (165, 1), (166, 1), (182, 2), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (208, 2), (209, 1), (210, 1), (213, 1), (229, 1), (232, 3), (233, 1), (259, 1), (260, 2), (261, 7), (279, 2), (280, 1), (282, 1), (284, 1), (285, 1), (287, 1), (289, 1), (296, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 397 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 76s - loss: 685.7659 - loglik: -6.8341e+02 - logprior: -2.3518e+00
Epoch 2/2
39/39 - 74s - loss: 662.7814 - loglik: -6.6158e+02 - logprior: -1.1969e+00
Fitted a model with MAP estimate = -657.9704
expansions: [(150, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15 160 165
 236 245 335 362]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 70s - loss: 666.9926 - loglik: -6.6509e+02 - logprior: -1.9067e+00
Epoch 2/2
39/39 - 60s - loss: 660.8056 - loglik: -6.6072e+02 - logprior: -8.2356e-02
Fitted a model with MAP estimate = -657.3759
expansions: [(0, 6)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 382 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 63s - loss: 664.2727 - loglik: -6.6273e+02 - logprior: -1.5458e+00
Epoch 2/10
39/39 - 62s - loss: 659.5301 - loglik: -6.5940e+02 - logprior: -1.2649e-01
Epoch 3/10
39/39 - 65s - loss: 655.3597 - loglik: -6.5546e+02 - logprior: 0.0979
Epoch 4/10
39/39 - 64s - loss: 652.4614 - loglik: -6.5265e+02 - logprior: 0.1886
Epoch 5/10
39/39 - 65s - loss: 651.0219 - loglik: -6.5135e+02 - logprior: 0.3269
Epoch 6/10
39/39 - 63s - loss: 649.6979 - loglik: -6.5018e+02 - logprior: 0.4813
Epoch 7/10
39/39 - 64s - loss: 649.7217 - loglik: -6.5028e+02 - logprior: 0.5630
Fitted a model with MAP estimate = -649.1558
Time for alignment: 1490.4069
Computed alignments with likelihoods: ['-648.9952', '-650.1163', '-649.6247', '-649.8737', '-649.1558']
Best model has likelihood: -648.9952  (prior= 0.4330 )
time for generating output: 0.3651
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00476.projection.fasta
SP score = 0.9391278013325257
Training of 5 independent models on file PF02836.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd508772550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd36b632310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd36b632520>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 644.6035 - loglik: -6.4276e+02 - logprior: -1.8420e+00
Epoch 2/10
39/39 - 17s - loss: 554.0616 - loglik: -5.5258e+02 - logprior: -1.4806e+00
Epoch 3/10
39/39 - 17s - loss: 545.7698 - loglik: -5.4426e+02 - logprior: -1.5057e+00
Epoch 4/10
39/39 - 17s - loss: 544.3378 - loglik: -5.4282e+02 - logprior: -1.5149e+00
Epoch 5/10
39/39 - 17s - loss: 543.6410 - loglik: -5.4213e+02 - logprior: -1.5129e+00
Epoch 6/10
39/39 - 17s - loss: 543.4803 - loglik: -5.4196e+02 - logprior: -1.5219e+00
Epoch 7/10
39/39 - 17s - loss: 543.5777 - loglik: -5.4205e+02 - logprior: -1.5282e+00
Fitted a model with MAP estimate = -542.6333
expansions: [(4, 1), (6, 1), (31, 1), (44, 1), (84, 5), (90, 3), (92, 2), (116, 1), (118, 4), (119, 2), (120, 1), (121, 1), (128, 1), (132, 5), (137, 1), (139, 1), (142, 3), (143, 8), (147, 1), (148, 1), (155, 1), (158, 1), (161, 3), (172, 6), (174, 1), (184, 1), (212, 5)]
discards: [  0 162 163 164 165 166 167 168 169 170 176 177 178 179 180 181 182 186
 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204
 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 560.3442 - loglik: -5.5764e+02 - logprior: -2.7003e+00
Epoch 2/2
39/39 - 20s - loss: 547.8461 - loglik: -5.4674e+02 - logprior: -1.1055e+00
Fitted a model with MAP estimate = -544.4726
expansions: [(214, 4), (215, 3), (220, 1), (225, 1), (229, 2), (230, 11), (231, 7)]
discards: [  0   1 132 133 134 178 179 195 196 197 198 199 200 201 202 203 204 205
 206 207 208 209 210 222]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 551.6619 - loglik: -5.4913e+02 - logprior: -2.5340e+00
Epoch 2/2
39/39 - 21s - loss: 539.6326 - loglik: -5.3884e+02 - logprior: -7.8905e-01
Fitted a model with MAP estimate = -536.8122
expansions: [(0, 3), (130, 1), (197, 1), (201, 9), (236, 5)]
discards: [188 189 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230
 231 232 233 234 235]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 549.1094 - loglik: -5.4735e+02 - logprior: -1.7571e+00
Epoch 2/10
39/39 - 21s - loss: 542.7090 - loglik: -5.4227e+02 - logprior: -4.3609e-01
Epoch 3/10
39/39 - 21s - loss: 540.9044 - loglik: -5.4056e+02 - logprior: -3.3989e-01
Epoch 4/10
39/39 - 21s - loss: 539.0712 - loglik: -5.3878e+02 - logprior: -2.9475e-01
Epoch 5/10
39/39 - 21s - loss: 538.9689 - loglik: -5.3874e+02 - logprior: -2.3234e-01
Epoch 6/10
39/39 - 21s - loss: 538.7258 - loglik: -5.3854e+02 - logprior: -1.8816e-01
Epoch 7/10
39/39 - 22s - loss: 538.7174 - loglik: -5.3859e+02 - logprior: -1.2792e-01
Epoch 8/10
39/39 - 22s - loss: 538.2599 - loglik: -5.3818e+02 - logprior: -7.8457e-02
Epoch 9/10
39/39 - 22s - loss: 538.5723 - loglik: -5.3853e+02 - logprior: -3.9647e-02
Fitted a model with MAP estimate = -538.1230
Time for alignment: 494.7978
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 642.7144 - loglik: -6.4090e+02 - logprior: -1.8155e+00
Epoch 2/10
39/39 - 18s - loss: 554.7520 - loglik: -5.5340e+02 - logprior: -1.3475e+00
Epoch 3/10
39/39 - 18s - loss: 547.9530 - loglik: -5.4660e+02 - logprior: -1.3551e+00
Epoch 4/10
39/39 - 19s - loss: 545.6323 - loglik: -5.4425e+02 - logprior: -1.3777e+00
Epoch 5/10
39/39 - 19s - loss: 544.5865 - loglik: -5.4321e+02 - logprior: -1.3724e+00
Epoch 6/10
39/39 - 19s - loss: 543.7492 - loglik: -5.4238e+02 - logprior: -1.3675e+00
Epoch 7/10
39/39 - 19s - loss: 543.8053 - loglik: -5.4243e+02 - logprior: -1.3707e+00
Fitted a model with MAP estimate = -543.0924
expansions: [(4, 1), (6, 1), (31, 1), (34, 1), (81, 2), (82, 6), (84, 1), (85, 1), (88, 1), (89, 1), (90, 1), (114, 1), (115, 9), (116, 2), (117, 2), (126, 1), (128, 2), (129, 3), (140, 2), (141, 10), (145, 1), (147, 1), (157, 1), (158, 1), (160, 5), (172, 5), (175, 1), (187, 1), (212, 5)]
discards: [  0 150 151 152 153 154 161 162 163 164 165 176 177 178 179 180 181 182
 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205
 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 560.6564 - loglik: -5.5803e+02 - logprior: -2.6312e+00
Epoch 2/2
39/39 - 23s - loss: 545.5361 - loglik: -5.4440e+02 - logprior: -1.1348e+00
Fitted a model with MAP estimate = -542.3552
expansions: [(201, 2), (209, 6), (230, 1), (240, 17)]
discards: [  0  80  81  82  83  86 135 137 142 159 161 177 178 184 185 213 214 215
 216 217 218 219 220 223 224 234 235 236 237 238 239]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 550.5634 - loglik: -5.4808e+02 - logprior: -2.4792e+00
Epoch 2/2
39/39 - 23s - loss: 538.5745 - loglik: -5.3791e+02 - logprior: -6.6500e-01
Fitted a model with MAP estimate = -536.0171
expansions: [(0, 2), (84, 6), (166, 1), (185, 2), (188, 1), (198, 3), (235, 5)]
discards: [  0 208 209 210 219 220 221 222 223 224 225 226 227 228 229 230 231 232
 233 234]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 546.1663 - loglik: -5.4456e+02 - logprior: -1.6085e+00
Epoch 2/10
39/39 - 22s - loss: 539.6160 - loglik: -5.3923e+02 - logprior: -3.8134e-01
Epoch 3/10
39/39 - 22s - loss: 537.9896 - loglik: -5.3772e+02 - logprior: -2.7098e-01
Epoch 4/10
39/39 - 21s - loss: 537.1877 - loglik: -5.3698e+02 - logprior: -2.0576e-01
Epoch 5/10
39/39 - 20s - loss: 536.9427 - loglik: -5.3679e+02 - logprior: -1.5148e-01
Epoch 6/10
39/39 - 19s - loss: 536.6558 - loglik: -5.3657e+02 - logprior: -8.9014e-02
Epoch 7/10
39/39 - 19s - loss: 536.4548 - loglik: -5.3642e+02 - logprior: -3.5000e-02
Epoch 8/10
39/39 - 19s - loss: 536.5958 - loglik: -5.3662e+02 - logprior: 0.0216
Fitted a model with MAP estimate = -536.1744
Time for alignment: 497.5763
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 644.1268 - loglik: -6.4231e+02 - logprior: -1.8200e+00
Epoch 2/10
39/39 - 16s - loss: 555.4608 - loglik: -5.5407e+02 - logprior: -1.3929e+00
Epoch 3/10
39/39 - 16s - loss: 548.5046 - loglik: -5.4705e+02 - logprior: -1.4539e+00
Epoch 4/10
39/39 - 16s - loss: 547.4677 - loglik: -5.4603e+02 - logprior: -1.4420e+00
Epoch 5/10
39/39 - 16s - loss: 546.1084 - loglik: -5.4465e+02 - logprior: -1.4536e+00
Epoch 6/10
39/39 - 16s - loss: 546.0225 - loglik: -5.4456e+02 - logprior: -1.4632e+00
Epoch 7/10
39/39 - 16s - loss: 545.5994 - loglik: -5.4414e+02 - logprior: -1.4607e+00
Epoch 8/10
39/39 - 15s - loss: 545.7724 - loglik: -5.4433e+02 - logprior: -1.4473e+00
Fitted a model with MAP estimate = -545.0367
expansions: [(4, 1), (6, 1), (33, 1), (84, 7), (93, 3), (120, 1), (121, 7), (122, 3), (123, 1), (124, 3), (130, 1), (131, 2), (133, 2), (134, 5), (138, 1), (141, 1), (143, 3), (144, 8), (145, 1), (147, 1), (148, 2), (155, 1), (157, 3), (161, 2), (162, 2), (173, 4), (175, 1), (212, 6)]
discards: [  0  78  79  80 163 164 165 166 167 168 169 170 171 176 177 178 179 180
 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198
 199 200 201 202 203 204 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 563.3121 - loglik: -5.6069e+02 - logprior: -2.6218e+00
Epoch 2/2
39/39 - 19s - loss: 547.2314 - loglik: -5.4593e+02 - logprior: -1.2992e+00
Fitted a model with MAP estimate = -544.3154
expansions: [(4, 1), (98, 1), (237, 28)]
discards: [  0   1  82  86  87  88 133 134 135 140 146 161 163 185 186 187 202 203
 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221
 222 223 224 226 227 228 229 230 231 232 233 234 235 236]
Re-initialized the encoder parameters.
Fitting a model of length 217 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 562.9577 - loglik: -5.6054e+02 - logprior: -2.4190e+00
Epoch 2/2
39/39 - 17s - loss: 549.5980 - loglik: -5.4883e+02 - logprior: -7.6507e-01
Fitted a model with MAP estimate = -546.3356
expansions: [(3, 1), (85, 5), (202, 8), (205, 1), (217, 11)]
discards: [  0  81  82  83 188 192 206 207 209 210 213 214 215 216]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 552.0713 - loglik: -5.4982e+02 - logprior: -2.2493e+00
Epoch 2/10
39/39 - 18s - loss: 542.6361 - loglik: -5.4217e+02 - logprior: -4.6451e-01
Epoch 3/10
39/39 - 18s - loss: 540.7078 - loglik: -5.4055e+02 - logprior: -1.5506e-01
Epoch 4/10
39/39 - 18s - loss: 539.6752 - loglik: -5.3960e+02 - logprior: -7.6546e-02
Epoch 5/10
39/39 - 19s - loss: 538.7853 - loglik: -5.3875e+02 - logprior: -3.5974e-02
Epoch 6/10
39/39 - 19s - loss: 538.9889 - loglik: -5.3902e+02 - logprior: 0.0283
Fitted a model with MAP estimate = -538.4091
Time for alignment: 401.8295
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 646.4825 - loglik: -6.4466e+02 - logprior: -1.8263e+00
Epoch 2/10
39/39 - 16s - loss: 554.5098 - loglik: -5.5308e+02 - logprior: -1.4261e+00
Epoch 3/10
39/39 - 17s - loss: 547.6869 - loglik: -5.4625e+02 - logprior: -1.4367e+00
Epoch 4/10
39/39 - 17s - loss: 546.0421 - loglik: -5.4459e+02 - logprior: -1.4519e+00
Epoch 5/10
39/39 - 17s - loss: 545.6215 - loglik: -5.4417e+02 - logprior: -1.4508e+00
Epoch 6/10
39/39 - 17s - loss: 545.2894 - loglik: -5.4382e+02 - logprior: -1.4685e+00
Epoch 7/10
39/39 - 17s - loss: 545.0403 - loglik: -5.4357e+02 - logprior: -1.4711e+00
Epoch 8/10
39/39 - 17s - loss: 544.5007 - loglik: -5.4304e+02 - logprior: -1.4625e+00
Epoch 9/10
39/39 - 17s - loss: 544.5755 - loglik: -5.4310e+02 - logprior: -1.4725e+00
Fitted a model with MAP estimate = -543.9673
expansions: [(4, 1), (6, 1), (31, 1), (32, 1), (77, 1), (82, 4), (84, 3), (88, 2), (89, 3), (91, 2), (115, 1), (116, 10), (117, 1), (118, 2), (124, 2), (125, 2), (127, 7), (135, 1), (138, 3), (139, 8), (140, 1), (142, 1), (144, 1), (152, 1), (155, 1), (158, 2), (159, 2), (172, 5), (174, 1), (212, 6)]
discards: [  0  79 147 160 161 162 163 164 165 166 167 168 169 170 175 176 177 178
 179 180 181 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201
 202 203 204 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 561.1639 - loglik: -5.5858e+02 - logprior: -2.5846e+00
Epoch 2/2
39/39 - 20s - loss: 546.2934 - loglik: -5.4529e+02 - logprior: -9.9896e-01
Fitted a model with MAP estimate = -543.3167
expansions: [(87, 1), (243, 20)]
discards: [  0 136 137 138 156 158 164 165 187 188 189 204 205 206 207 222 238]
Re-initialized the encoder parameters.
Fitting a model of length 247 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 546.5678 - loglik: -5.4405e+02 - logprior: -2.5175e+00
Epoch 2/2
39/39 - 23s - loss: 538.0365 - loglik: -5.3731e+02 - logprior: -7.3049e-01
Fitted a model with MAP estimate = -535.4563
expansions: [(0, 2), (194, 4), (195, 1), (219, 9), (247, 3)]
discards: [  0 100 223 224 225 232 233 234 235 236 237 238 239 240 241 242 243 244
 245 246]
Re-initialized the encoder parameters.
Fitting a model of length 246 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 542.5475 - loglik: -5.4095e+02 - logprior: -1.5985e+00
Epoch 2/10
39/39 - 24s - loss: 533.7863 - loglik: -5.3344e+02 - logprior: -3.4871e-01
Epoch 3/10
39/39 - 24s - loss: 532.3482 - loglik: -5.3210e+02 - logprior: -2.5320e-01
Epoch 4/10
39/39 - 24s - loss: 531.3517 - loglik: -5.3117e+02 - logprior: -1.8306e-01
Epoch 5/10
39/39 - 24s - loss: 530.9098 - loglik: -5.3078e+02 - logprior: -1.2819e-01
Epoch 6/10
39/39 - 24s - loss: 530.6758 - loglik: -5.3061e+02 - logprior: -6.5449e-02
Epoch 7/10
39/39 - 24s - loss: 530.7723 - loglik: -5.3076e+02 - logprior: -1.3321e-02
Fitted a model with MAP estimate = -530.2175
Time for alignment: 513.9092
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 648.2371 - loglik: -6.4640e+02 - logprior: -1.8404e+00
Epoch 2/10
39/39 - 19s - loss: 554.6078 - loglik: -5.5310e+02 - logprior: -1.5040e+00
Epoch 3/10
39/39 - 20s - loss: 546.4735 - loglik: -5.4496e+02 - logprior: -1.5102e+00
Epoch 4/10
39/39 - 21s - loss: 545.5130 - loglik: -5.4401e+02 - logprior: -1.4986e+00
Epoch 5/10
39/39 - 21s - loss: 544.2332 - loglik: -5.4274e+02 - logprior: -1.4975e+00
Epoch 6/10
39/39 - 21s - loss: 543.6606 - loglik: -5.4215e+02 - logprior: -1.5131e+00
Epoch 7/10
39/39 - 20s - loss: 542.8550 - loglik: -5.4132e+02 - logprior: -1.5339e+00
Epoch 8/10
39/39 - 21s - loss: 542.2083 - loglik: -5.4067e+02 - logprior: -1.5395e+00
Epoch 9/10
39/39 - 21s - loss: 542.1913 - loglik: -5.4063e+02 - logprior: -1.5620e+00
Epoch 10/10
39/39 - 21s - loss: 541.8074 - loglik: -5.4026e+02 - logprior: -1.5521e+00
Fitted a model with MAP estimate = -541.3794
expansions: [(4, 1), (6, 1), (31, 1), (32, 1), (51, 1), (81, 8), (84, 1), (89, 1), (90, 1), (91, 1), (113, 1), (114, 2), (115, 10), (116, 1), (117, 3), (126, 1), (128, 2), (129, 3), (134, 1), (136, 1), (139, 3), (140, 8), (146, 1), (158, 1), (161, 7), (162, 1), (175, 1), (212, 5)]
discards: [  0  78  79 150 151 164 165 166 167 168 169 170 171 172 176 177 178 179
 180 181 182 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202
 203 204 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 561.5408 - loglik: -5.5890e+02 - logprior: -2.6425e+00
Epoch 2/2
39/39 - 24s - loss: 547.9275 - loglik: -5.4682e+02 - logprior: -1.1056e+00
Fitted a model with MAP estimate = -544.8226
expansions: [(204, 1), (223, 1), (234, 2), (235, 9), (236, 8)]
discards: [  0  82 130 133 134 135 146 162 185 186 214]
Re-initialized the encoder parameters.
Fitting a model of length 246 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 542.7875 - loglik: -5.4025e+02 - logprior: -2.5325e+00
Epoch 2/2
39/39 - 25s - loss: 534.2725 - loglik: -5.3356e+02 - logprior: -7.1476e-01
Fitted a model with MAP estimate = -531.7291
expansions: [(0, 2), (86, 3), (87, 2), (246, 4)]
discards: [  0  81  82  83  84 177 190 191 192 193 194 223 224 225 226 227 228 229
 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245]
Re-initialized the encoder parameters.
Fitting a model of length 223 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 551.4525 - loglik: -5.4990e+02 - logprior: -1.5544e+00
Epoch 2/10
39/39 - 22s - loss: 546.3336 - loglik: -5.4600e+02 - logprior: -3.3023e-01
Epoch 3/10
39/39 - 23s - loss: 544.8121 - loglik: -5.4461e+02 - logprior: -1.9889e-01
Epoch 4/10
39/39 - 23s - loss: 543.7705 - loglik: -5.4366e+02 - logprior: -1.1251e-01
Epoch 5/10
39/39 - 24s - loss: 543.4916 - loglik: -5.4344e+02 - logprior: -5.1435e-02
Epoch 6/10
39/39 - 24s - loss: 543.4594 - loglik: -5.4346e+02 - logprior: 0.0056
Epoch 7/10
39/39 - 24s - loss: 543.2900 - loglik: -5.4335e+02 - logprior: 0.0643
Epoch 8/10
39/39 - 24s - loss: 543.0536 - loglik: -5.4317e+02 - logprior: 0.1166
Epoch 9/10
39/39 - 24s - loss: 542.8978 - loglik: -5.4307e+02 - logprior: 0.1740
Epoch 10/10
39/39 - 23s - loss: 543.5959 - loglik: -5.4383e+02 - logprior: 0.2298
Fitted a model with MAP estimate = -542.5867
Time for alignment: 648.7442
Computed alignments with likelihoods: ['-536.8122', '-536.0171', '-538.4091', '-530.2175', '-531.7291']
Best model has likelihood: -530.2175  (prior= -0.0044 )
time for generating output: 0.3054
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02836.projection.fasta
SP score = 0.9125993189557321
Training of 5 independent models on file PF02777.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd126412310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1c40d5af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd519570d60>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 269.1826 - loglik: -2.6607e+02 - logprior: -3.1164e+00
Epoch 2/10
19/19 - 2s - loss: 200.5261 - loglik: -1.9919e+02 - logprior: -1.3406e+00
Epoch 3/10
19/19 - 2s - loss: 176.8057 - loglik: -1.7511e+02 - logprior: -1.6917e+00
Epoch 4/10
19/19 - 2s - loss: 173.7149 - loglik: -1.7208e+02 - logprior: -1.6304e+00
Epoch 5/10
19/19 - 2s - loss: 172.1912 - loglik: -1.7061e+02 - logprior: -1.5830e+00
Epoch 6/10
19/19 - 2s - loss: 171.7971 - loglik: -1.7022e+02 - logprior: -1.5733e+00
Epoch 7/10
19/19 - 2s - loss: 171.4143 - loglik: -1.6986e+02 - logprior: -1.5545e+00
Epoch 8/10
19/19 - 2s - loss: 171.0417 - loglik: -1.6950e+02 - logprior: -1.5435e+00
Epoch 9/10
19/19 - 2s - loss: 170.3857 - loglik: -1.6884e+02 - logprior: -1.5450e+00
Epoch 10/10
19/19 - 2s - loss: 170.9605 - loglik: -1.6942e+02 - logprior: -1.5447e+00
Fitted a model with MAP estimate = -170.3718
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 1), (47, 1), (48, 4), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 169.5325 - loglik: -1.6556e+02 - logprior: -3.9687e+00
Epoch 2/2
19/19 - 3s - loss: 157.0778 - loglik: -1.5507e+02 - logprior: -2.0050e+00
Fitted a model with MAP estimate = -155.2461
expansions: [(0, 2)]
discards: [ 0 62 63]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 156.6066 - loglik: -1.5370e+02 - logprior: -2.9082e+00
Epoch 2/2
19/19 - 3s - loss: 152.8079 - loglik: -1.5171e+02 - logprior: -1.0998e+00
Fitted a model with MAP estimate = -152.0070
expansions: []
discards: [ 0 43]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 159.1441 - loglik: -1.5532e+02 - logprior: -3.8253e+00
Epoch 2/10
19/19 - 3s - loss: 154.3210 - loglik: -1.5292e+02 - logprior: -1.3992e+00
Epoch 3/10
19/19 - 3s - loss: 152.6790 - loglik: -1.5169e+02 - logprior: -9.8568e-01
Epoch 4/10
19/19 - 3s - loss: 151.7663 - loglik: -1.5079e+02 - logprior: -9.7402e-01
Epoch 5/10
19/19 - 3s - loss: 151.1647 - loglik: -1.5023e+02 - logprior: -9.3230e-01
Epoch 6/10
19/19 - 3s - loss: 150.6804 - loglik: -1.4974e+02 - logprior: -9.3563e-01
Epoch 7/10
19/19 - 3s - loss: 150.6809 - loglik: -1.4977e+02 - logprior: -9.1063e-01
Fitted a model with MAP estimate = -150.3758
Time for alignment: 86.6402
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 269.5752 - loglik: -2.6646e+02 - logprior: -3.1192e+00
Epoch 2/10
19/19 - 2s - loss: 200.5597 - loglik: -1.9922e+02 - logprior: -1.3399e+00
Epoch 3/10
19/19 - 2s - loss: 177.3000 - loglik: -1.7561e+02 - logprior: -1.6865e+00
Epoch 4/10
19/19 - 2s - loss: 172.6728 - loglik: -1.7102e+02 - logprior: -1.6540e+00
Epoch 5/10
19/19 - 2s - loss: 171.2303 - loglik: -1.6962e+02 - logprior: -1.6055e+00
Epoch 6/10
19/19 - 2s - loss: 171.0038 - loglik: -1.6942e+02 - logprior: -1.5870e+00
Epoch 7/10
19/19 - 2s - loss: 170.8848 - loglik: -1.6932e+02 - logprior: -1.5646e+00
Epoch 8/10
19/19 - 3s - loss: 170.6027 - loglik: -1.6905e+02 - logprior: -1.5555e+00
Epoch 9/10
19/19 - 2s - loss: 170.4791 - loglik: -1.6892e+02 - logprior: -1.5599e+00
Epoch 10/10
19/19 - 2s - loss: 169.7926 - loglik: -1.6823e+02 - logprior: -1.5595e+00
Fitted a model with MAP estimate = -169.8675
expansions: [(4, 1), (6, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 1), (47, 2), (48, 3), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 165.4939 - loglik: -1.6250e+02 - logprior: -2.9951e+00
Epoch 2/2
19/19 - 3s - loss: 154.2709 - loglik: -1.5308e+02 - logprior: -1.1950e+00
Fitted a model with MAP estimate = -152.9637
expansions: []
discards: [60 64]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 156.1581 - loglik: -1.5323e+02 - logprior: -2.9250e+00
Epoch 2/2
19/19 - 3s - loss: 153.1072 - loglik: -1.5199e+02 - logprior: -1.1126e+00
Fitted a model with MAP estimate = -152.5913
expansions: [(2, 1)]
discards: [ 0 42]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 159.6474 - loglik: -1.5574e+02 - logprior: -3.9118e+00
Epoch 2/10
19/19 - 3s - loss: 154.9808 - loglik: -1.5282e+02 - logprior: -2.1596e+00
Epoch 3/10
19/19 - 3s - loss: 154.0874 - loglik: -1.5209e+02 - logprior: -2.0012e+00
Epoch 4/10
19/19 - 3s - loss: 152.9461 - loglik: -1.5138e+02 - logprior: -1.5659e+00
Epoch 5/10
19/19 - 3s - loss: 152.0577 - loglik: -1.5104e+02 - logprior: -1.0152e+00
Epoch 6/10
19/19 - 3s - loss: 151.5572 - loglik: -1.5055e+02 - logprior: -1.0114e+00
Epoch 7/10
19/19 - 3s - loss: 151.3296 - loglik: -1.5036e+02 - logprior: -9.7353e-01
Epoch 8/10
19/19 - 3s - loss: 151.2933 - loglik: -1.5032e+02 - logprior: -9.7146e-01
Epoch 9/10
19/19 - 3s - loss: 151.1660 - loglik: -1.5022e+02 - logprior: -9.4463e-01
Epoch 10/10
19/19 - 3s - loss: 150.9409 - loglik: -1.5003e+02 - logprior: -9.1123e-01
Fitted a model with MAP estimate = -150.8888
Time for alignment: 92.7823
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 269.5381 - loglik: -2.6642e+02 - logprior: -3.1180e+00
Epoch 2/10
19/19 - 2s - loss: 199.9733 - loglik: -1.9862e+02 - logprior: -1.3508e+00
Epoch 3/10
19/19 - 2s - loss: 176.3309 - loglik: -1.7462e+02 - logprior: -1.7086e+00
Epoch 4/10
19/19 - 2s - loss: 172.2187 - loglik: -1.7056e+02 - logprior: -1.6598e+00
Epoch 5/10
19/19 - 2s - loss: 171.5736 - loglik: -1.6998e+02 - logprior: -1.5935e+00
Epoch 6/10
19/19 - 2s - loss: 169.9944 - loglik: -1.6841e+02 - logprior: -1.5861e+00
Epoch 7/10
19/19 - 2s - loss: 169.9718 - loglik: -1.6841e+02 - logprior: -1.5570e+00
Epoch 8/10
19/19 - 2s - loss: 169.5799 - loglik: -1.6803e+02 - logprior: -1.5468e+00
Epoch 9/10
19/19 - 2s - loss: 169.4431 - loglik: -1.6790e+02 - logprior: -1.5420e+00
Epoch 10/10
19/19 - 2s - loss: 169.1030 - loglik: -1.6755e+02 - logprior: -1.5483e+00
Fitted a model with MAP estimate = -168.9816
expansions: [(4, 1), (6, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (44, 1), (46, 3), (47, 1), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 164.7354 - loglik: -1.6179e+02 - logprior: -2.9418e+00
Epoch 2/2
19/19 - 3s - loss: 153.7836 - loglik: -1.5262e+02 - logprior: -1.1674e+00
Fitted a model with MAP estimate = -152.7472
expansions: []
discards: [42 60]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 156.2437 - loglik: -1.5333e+02 - logprior: -2.9129e+00
Epoch 2/2
19/19 - 3s - loss: 153.3133 - loglik: -1.5220e+02 - logprior: -1.1087e+00
Fitted a model with MAP estimate = -152.6539
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 159.4191 - loglik: -1.5549e+02 - logprior: -3.9324e+00
Epoch 2/10
19/19 - 3s - loss: 154.9511 - loglik: -1.5279e+02 - logprior: -2.1641e+00
Epoch 3/10
19/19 - 3s - loss: 153.9574 - loglik: -1.5196e+02 - logprior: -1.9953e+00
Epoch 4/10
19/19 - 3s - loss: 152.8268 - loglik: -1.5137e+02 - logprior: -1.4528e+00
Epoch 5/10
19/19 - 3s - loss: 152.0699 - loglik: -1.5106e+02 - logprior: -1.0079e+00
Epoch 6/10
19/19 - 3s - loss: 151.5823 - loglik: -1.5056e+02 - logprior: -1.0207e+00
Epoch 7/10
19/19 - 3s - loss: 151.3350 - loglik: -1.5037e+02 - logprior: -9.6722e-01
Epoch 8/10
19/19 - 3s - loss: 151.3000 - loglik: -1.5033e+02 - logprior: -9.6860e-01
Epoch 9/10
19/19 - 3s - loss: 151.0502 - loglik: -1.5011e+02 - logprior: -9.4276e-01
Epoch 10/10
19/19 - 3s - loss: 151.4429 - loglik: -1.5052e+02 - logprior: -9.1887e-01
Fitted a model with MAP estimate = -150.8449
Time for alignment: 93.6399
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 269.3390 - loglik: -2.6623e+02 - logprior: -3.1136e+00
Epoch 2/10
19/19 - 2s - loss: 200.6562 - loglik: -1.9933e+02 - logprior: -1.3311e+00
Epoch 3/10
19/19 - 2s - loss: 176.8757 - loglik: -1.7519e+02 - logprior: -1.6881e+00
Epoch 4/10
19/19 - 2s - loss: 172.6312 - loglik: -1.7097e+02 - logprior: -1.6597e+00
Epoch 5/10
19/19 - 2s - loss: 170.9633 - loglik: -1.6936e+02 - logprior: -1.6006e+00
Epoch 6/10
19/19 - 2s - loss: 170.6778 - loglik: -1.6909e+02 - logprior: -1.5915e+00
Epoch 7/10
19/19 - 2s - loss: 170.0075 - loglik: -1.6844e+02 - logprior: -1.5656e+00
Epoch 8/10
19/19 - 2s - loss: 170.0795 - loglik: -1.6853e+02 - logprior: -1.5539e+00
Fitted a model with MAP estimate = -169.6408
expansions: [(4, 1), (6, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 164.8452 - loglik: -1.6189e+02 - logprior: -2.9573e+00
Epoch 2/2
19/19 - 3s - loss: 153.8110 - loglik: -1.5262e+02 - logprior: -1.1871e+00
Fitted a model with MAP estimate = -152.7003
expansions: []
discards: [42 61 62]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 156.0202 - loglik: -1.5310e+02 - logprior: -2.9214e+00
Epoch 2/2
19/19 - 3s - loss: 153.2888 - loglik: -1.5217e+02 - logprior: -1.1202e+00
Fitted a model with MAP estimate = -152.6734
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 159.5867 - loglik: -1.5558e+02 - logprior: -4.0053e+00
Epoch 2/10
19/19 - 3s - loss: 155.0048 - loglik: -1.5282e+02 - logprior: -2.1812e+00
Epoch 3/10
19/19 - 3s - loss: 154.1616 - loglik: -1.5216e+02 - logprior: -2.0016e+00
Epoch 4/10
19/19 - 3s - loss: 152.8280 - loglik: -1.5137e+02 - logprior: -1.4545e+00
Epoch 5/10
19/19 - 3s - loss: 152.1005 - loglik: -1.5109e+02 - logprior: -1.0125e+00
Epoch 6/10
19/19 - 3s - loss: 151.4054 - loglik: -1.5037e+02 - logprior: -1.0311e+00
Epoch 7/10
19/19 - 3s - loss: 151.3090 - loglik: -1.5033e+02 - logprior: -9.7829e-01
Epoch 8/10
19/19 - 3s - loss: 151.3522 - loglik: -1.5040e+02 - logprior: -9.5488e-01
Fitted a model with MAP estimate = -151.0002
Time for alignment: 83.4331
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 269.6896 - loglik: -2.6657e+02 - logprior: -3.1171e+00
Epoch 2/10
19/19 - 2s - loss: 200.9839 - loglik: -1.9965e+02 - logprior: -1.3335e+00
Epoch 3/10
19/19 - 2s - loss: 177.5803 - loglik: -1.7589e+02 - logprior: -1.6875e+00
Epoch 4/10
19/19 - 3s - loss: 172.6591 - loglik: -1.7099e+02 - logprior: -1.6670e+00
Epoch 5/10
19/19 - 2s - loss: 171.4653 - loglik: -1.6988e+02 - logprior: -1.5874e+00
Epoch 6/10
19/19 - 2s - loss: 170.9610 - loglik: -1.6938e+02 - logprior: -1.5810e+00
Epoch 7/10
19/19 - 2s - loss: 170.5707 - loglik: -1.6902e+02 - logprior: -1.5514e+00
Epoch 8/10
19/19 - 2s - loss: 170.5769 - loglik: -1.6903e+02 - logprior: -1.5419e+00
Fitted a model with MAP estimate = -170.1407
expansions: [(4, 1), (6, 1), (8, 1), (9, 1), (14, 1), (16, 1), (19, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 5), (47, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 165.7885 - loglik: -1.6284e+02 - logprior: -2.9480e+00
Epoch 2/2
19/19 - 3s - loss: 154.3753 - loglik: -1.5317e+02 - logprior: -1.2050e+00
Fitted a model with MAP estimate = -153.1766
expansions: []
discards: [42 61 62 64]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 156.5057 - loglik: -1.5358e+02 - logprior: -2.9241e+00
Epoch 2/2
19/19 - 3s - loss: 153.4890 - loglik: -1.5237e+02 - logprior: -1.1163e+00
Fitted a model with MAP estimate = -152.6784
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 159.5094 - loglik: -1.5561e+02 - logprior: -3.9017e+00
Epoch 2/10
19/19 - 3s - loss: 154.8842 - loglik: -1.5274e+02 - logprior: -2.1490e+00
Epoch 3/10
19/19 - 3s - loss: 154.1248 - loglik: -1.5216e+02 - logprior: -1.9679e+00
Epoch 4/10
19/19 - 3s - loss: 152.5592 - loglik: -1.5119e+02 - logprior: -1.3720e+00
Epoch 5/10
19/19 - 3s - loss: 152.3079 - loglik: -1.5131e+02 - logprior: -1.0025e+00
Epoch 6/10
19/19 - 3s - loss: 151.4588 - loglik: -1.5044e+02 - logprior: -1.0204e+00
Epoch 7/10
19/19 - 3s - loss: 151.2355 - loglik: -1.5026e+02 - logprior: -9.7295e-01
Epoch 8/10
19/19 - 3s - loss: 151.3336 - loglik: -1.5038e+02 - logprior: -9.5304e-01
Fitted a model with MAP estimate = -150.9608
Time for alignment: 83.8001
Computed alignments with likelihoods: ['-150.3758', '-150.8888', '-150.8449', '-151.0002', '-150.9608']
Best model has likelihood: -150.3758  (prior= -0.8769 )
time for generating output: 0.1408
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02777.projection.fasta
SP score = 0.9208510638297872
Training of 5 independent models on file PF07654.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd384e4faf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1440b5070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fcb8050bb20>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 231.5094 - loglik: -2.2839e+02 - logprior: -3.1155e+00
Epoch 2/10
19/19 - 1s - loss: 193.7702 - loglik: -1.9250e+02 - logprior: -1.2679e+00
Epoch 3/10
19/19 - 1s - loss: 183.4560 - loglik: -1.8216e+02 - logprior: -1.2973e+00
Epoch 4/10
19/19 - 1s - loss: 180.9870 - loglik: -1.7968e+02 - logprior: -1.3059e+00
Epoch 5/10
19/19 - 1s - loss: 179.9576 - loglik: -1.7868e+02 - logprior: -1.2756e+00
Epoch 6/10
19/19 - 1s - loss: 179.5181 - loglik: -1.7825e+02 - logprior: -1.2632e+00
Epoch 7/10
19/19 - 1s - loss: 178.5282 - loglik: -1.7725e+02 - logprior: -1.2734e+00
Epoch 8/10
19/19 - 1s - loss: 178.4422 - loglik: -1.7718e+02 - logprior: -1.2639e+00
Epoch 9/10
19/19 - 1s - loss: 178.1915 - loglik: -1.7693e+02 - logprior: -1.2656e+00
Epoch 10/10
19/19 - 1s - loss: 178.2717 - loglik: -1.7701e+02 - logprior: -1.2619e+00
Fitted a model with MAP estimate = -177.9181
expansions: [(7, 2), (8, 3), (9, 2), (23, 1), (32, 1), (33, 1), (34, 2), (35, 1), (38, 1), (46, 1), (48, 1), (49, 1), (52, 1), (55, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 183.6464 - loglik: -1.7974e+02 - logprior: -3.9088e+00
Epoch 2/2
19/19 - 2s - loss: 175.3778 - loglik: -1.7336e+02 - logprior: -2.0195e+00
Fitted a model with MAP estimate = -173.9638
expansions: [(0, 2)]
discards: [ 0 43 72]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 176.9048 - loglik: -1.7403e+02 - logprior: -2.8723e+00
Epoch 2/2
19/19 - 2s - loss: 173.6295 - loglik: -1.7251e+02 - logprior: -1.1231e+00
Fitted a model with MAP estimate = -172.7789
expansions: []
discards: [ 0 11]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 178.1417 - loglik: -1.7452e+02 - logprior: -3.6245e+00
Epoch 2/10
19/19 - 1s - loss: 174.3661 - loglik: -1.7321e+02 - logprior: -1.1607e+00
Epoch 3/10
19/19 - 2s - loss: 172.9202 - loglik: -1.7195e+02 - logprior: -9.7141e-01
Epoch 4/10
19/19 - 2s - loss: 172.0819 - loglik: -1.7113e+02 - logprior: -9.4886e-01
Epoch 5/10
19/19 - 2s - loss: 171.3447 - loglik: -1.7041e+02 - logprior: -9.3187e-01
Epoch 6/10
19/19 - 2s - loss: 170.5470 - loglik: -1.6963e+02 - logprior: -9.1272e-01
Epoch 7/10
19/19 - 2s - loss: 170.5569 - loglik: -1.6964e+02 - logprior: -9.1445e-01
Fitted a model with MAP estimate = -170.2377
Time for alignment: 54.7164
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 231.5318 - loglik: -2.2842e+02 - logprior: -3.1154e+00
Epoch 2/10
19/19 - 1s - loss: 195.0398 - loglik: -1.9376e+02 - logprior: -1.2780e+00
Epoch 3/10
19/19 - 1s - loss: 184.2668 - loglik: -1.8298e+02 - logprior: -1.2858e+00
Epoch 4/10
19/19 - 1s - loss: 181.4675 - loglik: -1.8017e+02 - logprior: -1.2940e+00
Epoch 5/10
19/19 - 1s - loss: 180.4023 - loglik: -1.7914e+02 - logprior: -1.2671e+00
Epoch 6/10
19/19 - 1s - loss: 179.8808 - loglik: -1.7862e+02 - logprior: -1.2613e+00
Epoch 7/10
19/19 - 1s - loss: 179.1390 - loglik: -1.7789e+02 - logprior: -1.2536e+00
Epoch 8/10
19/19 - 1s - loss: 178.9661 - loglik: -1.7771e+02 - logprior: -1.2533e+00
Epoch 9/10
19/19 - 1s - loss: 178.7243 - loglik: -1.7747e+02 - logprior: -1.2509e+00
Epoch 10/10
19/19 - 1s - loss: 178.5360 - loglik: -1.7728e+02 - logprior: -1.2534e+00
Fitted a model with MAP estimate = -178.3537
expansions: [(7, 4), (9, 2), (23, 1), (31, 1), (32, 1), (34, 1), (35, 1), (38, 1), (46, 1), (48, 1), (49, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 184.0588 - loglik: -1.8015e+02 - logprior: -3.9091e+00
Epoch 2/2
19/19 - 1s - loss: 176.2313 - loglik: -1.7422e+02 - logprior: -2.0126e+00
Fitted a model with MAP estimate = -174.5365
expansions: [(0, 2), (7, 1)]
discards: [ 0 70]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 177.2319 - loglik: -1.7435e+02 - logprior: -2.8831e+00
Epoch 2/2
19/19 - 2s - loss: 173.6132 - loglik: -1.7249e+02 - logprior: -1.1237e+00
Fitted a model with MAP estimate = -172.9571
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 177.5411 - loglik: -1.7393e+02 - logprior: -3.6125e+00
Epoch 2/10
19/19 - 2s - loss: 173.9806 - loglik: -1.7283e+02 - logprior: -1.1504e+00
Epoch 3/10
19/19 - 2s - loss: 172.5199 - loglik: -1.7155e+02 - logprior: -9.7353e-01
Epoch 4/10
19/19 - 2s - loss: 172.1348 - loglik: -1.7119e+02 - logprior: -9.4194e-01
Epoch 5/10
19/19 - 2s - loss: 170.9348 - loglik: -1.7001e+02 - logprior: -9.2512e-01
Epoch 6/10
19/19 - 1s - loss: 170.3271 - loglik: -1.6941e+02 - logprior: -9.1893e-01
Epoch 7/10
19/19 - 2s - loss: 170.2451 - loglik: -1.6934e+02 - logprior: -9.0595e-01
Epoch 8/10
19/19 - 2s - loss: 169.8941 - loglik: -1.6899e+02 - logprior: -8.9920e-01
Epoch 9/10
19/19 - 2s - loss: 170.1436 - loglik: -1.6925e+02 - logprior: -8.8931e-01
Fitted a model with MAP estimate = -169.6835
Time for alignment: 56.6030
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 231.5992 - loglik: -2.2848e+02 - logprior: -3.1204e+00
Epoch 2/10
19/19 - 1s - loss: 195.3394 - loglik: -1.9406e+02 - logprior: -1.2748e+00
Epoch 3/10
19/19 - 1s - loss: 184.1398 - loglik: -1.8285e+02 - logprior: -1.2941e+00
Epoch 4/10
19/19 - 1s - loss: 181.0694 - loglik: -1.7977e+02 - logprior: -1.3030e+00
Epoch 5/10
19/19 - 1s - loss: 180.3164 - loglik: -1.7904e+02 - logprior: -1.2806e+00
Epoch 6/10
19/19 - 1s - loss: 179.6875 - loglik: -1.7842e+02 - logprior: -1.2708e+00
Epoch 7/10
19/19 - 1s - loss: 179.3661 - loglik: -1.7809e+02 - logprior: -1.2736e+00
Epoch 8/10
19/19 - 1s - loss: 178.4730 - loglik: -1.7721e+02 - logprior: -1.2652e+00
Epoch 9/10
19/19 - 1s - loss: 178.5518 - loglik: -1.7729e+02 - logprior: -1.2648e+00
Fitted a model with MAP estimate = -178.3055
expansions: [(7, 2), (8, 3), (9, 2), (21, 1), (22, 1), (31, 2), (34, 1), (35, 1), (38, 1), (46, 1), (48, 1), (49, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 183.9757 - loglik: -1.8006e+02 - logprior: -3.9191e+00
Epoch 2/2
19/19 - 2s - loss: 175.9323 - loglik: -1.7390e+02 - logprior: -2.0309e+00
Fitted a model with MAP estimate = -174.2343
expansions: [(0, 2)]
discards: [ 0 40 72]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 177.0671 - loglik: -1.7419e+02 - logprior: -2.8804e+00
Epoch 2/2
19/19 - 1s - loss: 173.4869 - loglik: -1.7236e+02 - logprior: -1.1229e+00
Fitted a model with MAP estimate = -172.7762
expansions: []
discards: [ 0 11]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 178.1606 - loglik: -1.7454e+02 - logprior: -3.6199e+00
Epoch 2/10
19/19 - 1s - loss: 174.3002 - loglik: -1.7314e+02 - logprior: -1.1610e+00
Epoch 3/10
19/19 - 1s - loss: 172.9162 - loglik: -1.7194e+02 - logprior: -9.7343e-01
Epoch 4/10
19/19 - 2s - loss: 172.1977 - loglik: -1.7125e+02 - logprior: -9.4667e-01
Epoch 5/10
19/19 - 2s - loss: 171.0317 - loglik: -1.7010e+02 - logprior: -9.3270e-01
Epoch 6/10
19/19 - 2s - loss: 170.7976 - loglik: -1.6988e+02 - logprior: -9.1658e-01
Epoch 7/10
19/19 - 2s - loss: 170.4950 - loglik: -1.6958e+02 - logprior: -9.1277e-01
Epoch 8/10
19/19 - 2s - loss: 170.4467 - loglik: -1.6955e+02 - logprior: -9.0094e-01
Epoch 9/10
19/19 - 2s - loss: 170.2024 - loglik: -1.6931e+02 - logprior: -8.9494e-01
Epoch 10/10
19/19 - 2s - loss: 169.7722 - loglik: -1.6889e+02 - logprior: -8.8533e-01
Fitted a model with MAP estimate = -169.9260
Time for alignment: 57.0516
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 231.6738 - loglik: -2.2855e+02 - logprior: -3.1236e+00
Epoch 2/10
19/19 - 1s - loss: 195.7689 - loglik: -1.9451e+02 - logprior: -1.2630e+00
Epoch 3/10
19/19 - 1s - loss: 184.1984 - loglik: -1.8290e+02 - logprior: -1.2971e+00
Epoch 4/10
19/19 - 1s - loss: 181.3664 - loglik: -1.8008e+02 - logprior: -1.2831e+00
Epoch 5/10
19/19 - 1s - loss: 180.4874 - loglik: -1.7921e+02 - logprior: -1.2750e+00
Epoch 6/10
19/19 - 1s - loss: 179.6479 - loglik: -1.7838e+02 - logprior: -1.2651e+00
Epoch 7/10
19/19 - 1s - loss: 179.0581 - loglik: -1.7779e+02 - logprior: -1.2703e+00
Epoch 8/10
19/19 - 1s - loss: 178.8660 - loglik: -1.7761e+02 - logprior: -1.2532e+00
Epoch 9/10
19/19 - 1s - loss: 178.5791 - loglik: -1.7733e+02 - logprior: -1.2467e+00
Epoch 10/10
19/19 - 1s - loss: 178.0725 - loglik: -1.7682e+02 - logprior: -1.2496e+00
Fitted a model with MAP estimate = -178.1961
expansions: [(7, 2), (8, 3), (9, 2), (23, 1), (32, 1), (33, 1), (34, 1), (35, 1), (38, 1), (47, 1), (48, 1), (49, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 184.1245 - loglik: -1.8022e+02 - logprior: -3.9088e+00
Epoch 2/2
19/19 - 2s - loss: 175.8609 - loglik: -1.7384e+02 - logprior: -2.0180e+00
Fitted a model with MAP estimate = -174.3188
expansions: [(0, 2)]
discards: [ 0 71]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 177.1066 - loglik: -1.7423e+02 - logprior: -2.8810e+00
Epoch 2/2
19/19 - 2s - loss: 173.6007 - loglik: -1.7248e+02 - logprior: -1.1216e+00
Fitted a model with MAP estimate = -172.8050
expansions: []
discards: [ 0 11]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 178.1416 - loglik: -1.7451e+02 - logprior: -3.6289e+00
Epoch 2/10
19/19 - 1s - loss: 174.2928 - loglik: -1.7313e+02 - logprior: -1.1614e+00
Epoch 3/10
19/19 - 2s - loss: 173.0148 - loglik: -1.7205e+02 - logprior: -9.6911e-01
Epoch 4/10
19/19 - 2s - loss: 172.0418 - loglik: -1.7110e+02 - logprior: -9.4566e-01
Epoch 5/10
19/19 - 2s - loss: 171.2065 - loglik: -1.7028e+02 - logprior: -9.2276e-01
Epoch 6/10
19/19 - 2s - loss: 170.5701 - loglik: -1.6964e+02 - logprior: -9.2633e-01
Epoch 7/10
19/19 - 2s - loss: 170.6573 - loglik: -1.6974e+02 - logprior: -9.1274e-01
Fitted a model with MAP estimate = -170.2425
Time for alignment: 54.5930
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 231.9462 - loglik: -2.2883e+02 - logprior: -3.1184e+00
Epoch 2/10
19/19 - 1s - loss: 195.3638 - loglik: -1.9410e+02 - logprior: -1.2630e+00
Epoch 3/10
19/19 - 1s - loss: 184.2074 - loglik: -1.8293e+02 - logprior: -1.2725e+00
Epoch 4/10
19/19 - 1s - loss: 181.4585 - loglik: -1.8018e+02 - logprior: -1.2812e+00
Epoch 5/10
19/19 - 1s - loss: 180.3783 - loglik: -1.7911e+02 - logprior: -1.2717e+00
Epoch 6/10
19/19 - 1s - loss: 179.6047 - loglik: -1.7835e+02 - logprior: -1.2583e+00
Epoch 7/10
19/19 - 1s - loss: 178.8760 - loglik: -1.7762e+02 - logprior: -1.2528e+00
Epoch 8/10
19/19 - 1s - loss: 179.0598 - loglik: -1.7781e+02 - logprior: -1.2495e+00
Fitted a model with MAP estimate = -178.5119
expansions: [(7, 2), (8, 3), (9, 2), (23, 1), (32, 1), (33, 1), (34, 1), (35, 1), (38, 1), (47, 1), (48, 1), (49, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 183.5843 - loglik: -1.7968e+02 - logprior: -3.9078e+00
Epoch 2/2
19/19 - 2s - loss: 175.8899 - loglik: -1.7388e+02 - logprior: -2.0094e+00
Fitted a model with MAP estimate = -174.3162
expansions: [(0, 2)]
discards: [ 0 71]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 177.1530 - loglik: -1.7427e+02 - logprior: -2.8859e+00
Epoch 2/2
19/19 - 2s - loss: 173.6976 - loglik: -1.7258e+02 - logprior: -1.1224e+00
Fitted a model with MAP estimate = -172.8048
expansions: []
discards: [ 0 11]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 178.2366 - loglik: -1.7461e+02 - logprior: -3.6221e+00
Epoch 2/10
19/19 - 2s - loss: 174.2728 - loglik: -1.7312e+02 - logprior: -1.1525e+00
Epoch 3/10
19/19 - 2s - loss: 172.8956 - loglik: -1.7193e+02 - logprior: -9.6792e-01
Epoch 4/10
19/19 - 2s - loss: 172.1232 - loglik: -1.7117e+02 - logprior: -9.5146e-01
Epoch 5/10
19/19 - 2s - loss: 170.9736 - loglik: -1.7005e+02 - logprior: -9.2406e-01
Epoch 6/10
19/19 - 2s - loss: 171.2551 - loglik: -1.7033e+02 - logprior: -9.2080e-01
Fitted a model with MAP estimate = -170.5387
Time for alignment: 50.2300
Computed alignments with likelihoods: ['-170.2377', '-169.6835', '-169.9260', '-170.2425', '-170.5387']
Best model has likelihood: -169.6835  (prior= -0.8972 )
time for generating output: 0.1223
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07654.projection.fasta
SP score = 0.8346938775510204
Training of 5 independent models on file PF04082.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd3410798e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1c40bd340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd50829cdf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 645.5134 - loglik: -6.4376e+02 - logprior: -1.7503e+00
Epoch 2/10
39/39 - 19s - loss: 597.7619 - loglik: -5.9666e+02 - logprior: -1.1061e+00
Epoch 3/10
39/39 - 20s - loss: 592.7825 - loglik: -5.9163e+02 - logprior: -1.1567e+00
Epoch 4/10
39/39 - 19s - loss: 591.9648 - loglik: -5.9083e+02 - logprior: -1.1396e+00
Epoch 5/10
39/39 - 19s - loss: 591.6826 - loglik: -5.9053e+02 - logprior: -1.1484e+00
Epoch 6/10
39/39 - 19s - loss: 591.6714 - loglik: -5.9053e+02 - logprior: -1.1391e+00
Epoch 7/10
39/39 - 19s - loss: 591.5613 - loglik: -5.9043e+02 - logprior: -1.1339e+00
Epoch 8/10
39/39 - 19s - loss: 591.4880 - loglik: -5.9035e+02 - logprior: -1.1358e+00
Epoch 9/10
39/39 - 18s - loss: 591.6364 - loglik: -5.9050e+02 - logprior: -1.1371e+00
Fitted a model with MAP estimate = -590.9851
expansions: [(21, 1), (22, 1), (23, 4), (26, 1), (27, 1), (47, 4), (48, 1), (59, 1), (61, 2), (62, 2), (64, 1), (80, 1), (81, 1), (83, 1), (84, 1), (96, 1), (105, 3), (106, 2), (108, 1), (109, 1), (123, 1), (124, 1), (134, 1), (135, 1), (139, 1), (148, 1), (151, 2), (156, 1), (161, 1), (165, 1), (167, 1), (168, 1), (170, 1), (177, 1), (178, 3), (181, 1), (192, 12)]
discards: [  1 189 190 191]
Re-initialized the encoder parameters.
Fitting a model of length 250 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 590.7552 - loglik: -5.8907e+02 - logprior: -1.6818e+00
Epoch 2/2
39/39 - 26s - loss: 584.6108 - loglik: -5.8401e+02 - logprior: -6.0225e-01
Fitted a model with MAP estimate = -583.5122
expansions: [(30, 1), (60, 1)]
discards: [ 26  27  28  55  56 130 131 223 224 238 239 240 241 242 243 244 245 246
 247 248 249]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 590.3776 - loglik: -5.8881e+02 - logprior: -1.5670e+00
Epoch 2/2
39/39 - 27s - loss: 587.1758 - loglik: -5.8689e+02 - logprior: -2.8520e-01
Fitted a model with MAP estimate = -586.1426
expansions: [(26, 3), (231, 11)]
discards: [229]
Re-initialized the encoder parameters.
Fitting a model of length 244 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 588.3124 - loglik: -5.8668e+02 - logprior: -1.6281e+00
Epoch 2/10
39/39 - 23s - loss: 584.2618 - loglik: -5.8381e+02 - logprior: -4.4800e-01
Epoch 3/10
39/39 - 22s - loss: 583.5833 - loglik: -5.8318e+02 - logprior: -3.9854e-01
Epoch 4/10
39/39 - 22s - loss: 583.7104 - loglik: -5.8336e+02 - logprior: -3.5229e-01
Fitted a model with MAP estimate = -583.0049
Time for alignment: 483.9495
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 645.4482 - loglik: -6.4371e+02 - logprior: -1.7428e+00
Epoch 2/10
39/39 - 15s - loss: 596.9290 - loglik: -5.9585e+02 - logprior: -1.0744e+00
Epoch 3/10
39/39 - 15s - loss: 591.9334 - loglik: -5.9082e+02 - logprior: -1.1170e+00
Epoch 4/10
39/39 - 15s - loss: 590.9424 - loglik: -5.8985e+02 - logprior: -1.0878e+00
Epoch 5/10
39/39 - 15s - loss: 590.7775 - loglik: -5.8970e+02 - logprior: -1.0754e+00
Epoch 6/10
39/39 - 15s - loss: 590.7473 - loglik: -5.8968e+02 - logprior: -1.0707e+00
Epoch 7/10
39/39 - 15s - loss: 590.6650 - loglik: -5.8959e+02 - logprior: -1.0778e+00
Epoch 8/10
39/39 - 16s - loss: 590.5824 - loglik: -5.8952e+02 - logprior: -1.0644e+00
Epoch 9/10
39/39 - 16s - loss: 590.8640 - loglik: -5.8979e+02 - logprior: -1.0788e+00
Fitted a model with MAP estimate = -589.9829
expansions: [(21, 1), (22, 1), (23, 5), (27, 1), (46, 4), (47, 1), (48, 1), (59, 1), (61, 2), (62, 2), (64, 1), (80, 1), (81, 1), (83, 1), (84, 1), (105, 4), (106, 2), (108, 1), (109, 1), (124, 1), (127, 1), (132, 1), (135, 1), (151, 2), (156, 1), (161, 1), (163, 1), (167, 1), (168, 1), (169, 1), (170, 1), (181, 1), (192, 13)]
discards: [  1 187 189 190 191]
Re-initialized the encoder parameters.
Fitting a model of length 246 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 590.6594 - loglik: -5.8899e+02 - logprior: -1.6711e+00
Epoch 2/2
39/39 - 22s - loss: 585.0993 - loglik: -5.8450e+02 - logprior: -6.0428e-01
Fitted a model with MAP estimate = -583.7756
expansions: [(31, 3)]
discards: [ 26  27  28  53  54  55  56 131 132 234 235 236 237 238 239 240 241 242
 243 244 245]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 590.5836 - loglik: -5.8901e+02 - logprior: -1.5721e+00
Epoch 2/2
39/39 - 20s - loss: 586.8635 - loglik: -5.8654e+02 - logprior: -3.2112e-01
Fitted a model with MAP estimate = -585.5489
expansions: [(53, 3), (54, 1), (214, 2), (228, 10)]
discards: [226 227]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 588.7784 - loglik: -5.8720e+02 - logprior: -1.5738e+00
Epoch 2/10
39/39 - 22s - loss: 585.0388 - loglik: -5.8462e+02 - logprior: -4.1768e-01
Epoch 3/10
39/39 - 22s - loss: 584.3771 - loglik: -5.8403e+02 - logprior: -3.4755e-01
Epoch 4/10
39/39 - 22s - loss: 584.1633 - loglik: -5.8385e+02 - logprior: -3.1815e-01
Epoch 5/10
39/39 - 22s - loss: 583.9736 - loglik: -5.8371e+02 - logprior: -2.6840e-01
Epoch 6/10
39/39 - 22s - loss: 583.6451 - loglik: -5.8341e+02 - logprior: -2.3734e-01
Epoch 7/10
39/39 - 22s - loss: 583.9315 - loglik: -5.8374e+02 - logprior: -1.8964e-01
Fitted a model with MAP estimate = -583.1944
Time for alignment: 475.0860
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 644.8439 - loglik: -6.4310e+02 - logprior: -1.7417e+00
Epoch 2/10
39/39 - 16s - loss: 597.1469 - loglik: -5.9607e+02 - logprior: -1.0791e+00
Epoch 3/10
39/39 - 15s - loss: 592.5509 - loglik: -5.9142e+02 - logprior: -1.1346e+00
Epoch 4/10
39/39 - 15s - loss: 591.8715 - loglik: -5.9075e+02 - logprior: -1.1228e+00
Epoch 5/10
39/39 - 15s - loss: 591.6555 - loglik: -5.9054e+02 - logprior: -1.1171e+00
Epoch 6/10
39/39 - 15s - loss: 591.6807 - loglik: -5.9056e+02 - logprior: -1.1212e+00
Fitted a model with MAP estimate = -591.0392
expansions: [(23, 1), (24, 5), (26, 2), (28, 1), (47, 4), (49, 1), (50, 1), (53, 1), (59, 1), (61, 2), (62, 2), (64, 1), (80, 1), (81, 1), (82, 1), (83, 1), (96, 1), (105, 2), (106, 2), (107, 1), (109, 1), (123, 1), (124, 1), (126, 1), (135, 1), (150, 1), (151, 2), (156, 1), (161, 1), (163, 1), (165, 1), (166, 1), (167, 1), (169, 1), (192, 12)]
discards: [  0 187 188 189 190 191]
Re-initialized the encoder parameters.
Fitting a model of length 245 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 592.5292 - loglik: -5.8997e+02 - logprior: -2.5637e+00
Epoch 2/2
39/39 - 21s - loss: 585.2704 - loglik: -5.8443e+02 - logprior: -8.4408e-01
Fitted a model with MAP estimate = -583.5632
expansions: [(31, 1), (222, 2), (225, 1), (245, 3)]
discards: [ 26  27  28  29  55  56  57  58 133 236 237 238 239 240 241 242 243 244]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 588.4709 - loglik: -5.8685e+02 - logprior: -1.6221e+00
Epoch 2/2
39/39 - 21s - loss: 584.9670 - loglik: -5.8460e+02 - logprior: -3.6575e-01
Fitted a model with MAP estimate = -584.0992
expansions: [(30, 2), (52, 3), (234, 3)]
discards: [ 26  27 215 216 231 232 233]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 588.1045 - loglik: -5.8651e+02 - logprior: -1.5985e+00
Epoch 2/10
39/39 - 21s - loss: 585.1544 - loglik: -5.8481e+02 - logprior: -3.4032e-01
Epoch 3/10
39/39 - 22s - loss: 584.4448 - loglik: -5.8420e+02 - logprior: -2.4087e-01
Epoch 4/10
39/39 - 22s - loss: 583.7825 - loglik: -5.8359e+02 - logprior: -1.9051e-01
Epoch 5/10
39/39 - 23s - loss: 583.6622 - loglik: -5.8352e+02 - logprior: -1.3990e-01
Epoch 6/10
39/39 - 22s - loss: 583.4482 - loglik: -5.8336e+02 - logprior: -8.9702e-02
Epoch 7/10
39/39 - 22s - loss: 584.0947 - loglik: -5.8404e+02 - logprior: -5.5887e-02
Fitted a model with MAP estimate = -583.0689
Time for alignment: 427.9373
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 646.3422 - loglik: -6.4459e+02 - logprior: -1.7519e+00
Epoch 2/10
39/39 - 16s - loss: 598.0729 - loglik: -5.9698e+02 - logprior: -1.0942e+00
Epoch 3/10
39/39 - 17s - loss: 593.1241 - loglik: -5.9201e+02 - logprior: -1.1163e+00
Epoch 4/10
39/39 - 17s - loss: 592.8578 - loglik: -5.9176e+02 - logprior: -1.0947e+00
Epoch 5/10
39/39 - 18s - loss: 592.3000 - loglik: -5.9122e+02 - logprior: -1.0837e+00
Epoch 6/10
39/39 - 17s - loss: 592.4503 - loglik: -5.9136e+02 - logprior: -1.0869e+00
Fitted a model with MAP estimate = -591.8061
expansions: [(23, 1), (24, 5), (26, 2), (28, 1), (48, 3), (49, 2), (50, 2), (60, 1), (63, 2), (64, 1), (65, 1), (81, 1), (82, 2), (83, 2), (84, 2), (105, 4), (106, 2), (108, 1), (109, 1), (123, 1), (124, 1), (126, 1), (133, 1), (137, 1), (145, 1), (147, 1), (150, 1), (156, 1), (160, 1), (163, 1), (164, 1), (166, 1), (167, 1), (169, 1), (192, 12)]
discards: [  0 189 190 191]
Re-initialized the encoder parameters.
Fitting a model of length 251 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 592.8232 - loglik: -5.9018e+02 - logprior: -2.6392e+00
Epoch 2/2
39/39 - 25s - loss: 585.0776 - loglik: -5.8416e+02 - logprior: -9.1430e-01
Fitted a model with MAP estimate = -583.2976
expansions: [(31, 1), (226, 2), (229, 1), (251, 2)]
discards: [ 26  27  28  29  56  57  58  59 103 109 136 240 241 242 243 244 245 246
 247 248 249 250]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 588.3796 - loglik: -5.8680e+02 - logprior: -1.5757e+00
Epoch 2/2
39/39 - 23s - loss: 584.7393 - loglik: -5.8439e+02 - logprior: -3.4999e-01
Fitted a model with MAP estimate = -583.7829
expansions: [(30, 3), (52, 1), (186, 1), (215, 2), (235, 3)]
discards: [ 26  27 126 216 217 218 233 234]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 588.2365 - loglik: -5.8667e+02 - logprior: -1.5693e+00
Epoch 2/10
39/39 - 22s - loss: 585.0043 - loglik: -5.8470e+02 - logprior: -3.0801e-01
Epoch 3/10
39/39 - 22s - loss: 584.4474 - loglik: -5.8424e+02 - logprior: -2.1095e-01
Epoch 4/10
39/39 - 22s - loss: 583.7708 - loglik: -5.8362e+02 - logprior: -1.4863e-01
Epoch 5/10
39/39 - 21s - loss: 583.8509 - loglik: -5.8374e+02 - logprior: -1.0933e-01
Fitted a model with MAP estimate = -583.3085
Time for alignment: 413.7979
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 644.5787 - loglik: -6.4283e+02 - logprior: -1.7491e+00
Epoch 2/10
39/39 - 16s - loss: 597.3637 - loglik: -5.9626e+02 - logprior: -1.0987e+00
Epoch 3/10
39/39 - 16s - loss: 592.7010 - loglik: -5.9152e+02 - logprior: -1.1830e+00
Epoch 4/10
39/39 - 16s - loss: 591.9482 - loglik: -5.9080e+02 - logprior: -1.1508e+00
Epoch 5/10
39/39 - 16s - loss: 591.9843 - loglik: -5.9084e+02 - logprior: -1.1478e+00
Fitted a model with MAP estimate = -591.1894
expansions: [(21, 1), (22, 1), (23, 2), (24, 2), (28, 1), (47, 3), (49, 2), (50, 1), (53, 1), (62, 2), (63, 2), (65, 1), (81, 1), (82, 1), (84, 1), (85, 1), (106, 4), (107, 2), (111, 1), (122, 1), (124, 1), (125, 1), (127, 1), (134, 1), (139, 1), (143, 1), (148, 1), (151, 2), (156, 1), (160, 1), (163, 1), (165, 1), (166, 1), (169, 1), (181, 1), (192, 13)]
discards: [  0 187 188 189 190 191]
Re-initialized the encoder parameters.
Fitting a model of length 246 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 592.8263 - loglik: -5.9025e+02 - logprior: -2.5787e+00
Epoch 2/2
39/39 - 22s - loss: 585.3610 - loglik: -5.8454e+02 - logprior: -8.1641e-01
Fitted a model with MAP estimate = -583.5336
expansions: [(31, 1), (188, 1), (215, 1), (221, 2), (246, 2)]
discards: [ 25  26  54  55 131 132 236 237 238 239 240 241 242 243 244 245]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 588.4371 - loglik: -5.8688e+02 - logprior: -1.5546e+00
Epoch 2/2
39/39 - 21s - loss: 584.9965 - loglik: -5.8466e+02 - logprior: -3.3365e-01
Fitted a model with MAP estimate = -583.9108
expansions: [(24, 1), (28, 1), (217, 2), (237, 3)]
discards: [ 25  26  52 184 218 219 220 235 236]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 588.4500 - loglik: -5.8689e+02 - logprior: -1.5631e+00
Epoch 2/10
39/39 - 21s - loss: 585.0283 - loglik: -5.8474e+02 - logprior: -2.8669e-01
Epoch 3/10
39/39 - 21s - loss: 584.0279 - loglik: -5.8384e+02 - logprior: -1.9020e-01
Epoch 4/10
39/39 - 21s - loss: 583.9460 - loglik: -5.8383e+02 - logprior: -1.1638e-01
Epoch 5/10
39/39 - 22s - loss: 583.6807 - loglik: -5.8360e+02 - logprior: -7.9026e-02
Epoch 6/10
39/39 - 22s - loss: 583.4231 - loglik: -5.8338e+02 - logprior: -4.4725e-02
Epoch 7/10
39/39 - 22s - loss: 583.2556 - loglik: -5.8326e+02 - logprior: 6.6642e-04
Epoch 8/10
39/39 - 22s - loss: 583.3810 - loglik: -5.8342e+02 - logprior: 0.0428
Fitted a model with MAP estimate = -582.9115
Time for alignment: 440.9227
Computed alignments with likelihoods: ['-583.0049', '-583.1944', '-583.0689', '-583.2976', '-582.9115']
Best model has likelihood: -582.9115  (prior= 0.0438 )
time for generating output: 0.2408
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF04082.projection.fasta
SP score = 0.5502767527675276
Training of 5 independent models on file PF00046.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1262e9be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd32dff5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fcb805bf280>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.0545 - loglik: -1.5082e+02 - logprior: -3.2338e+00
Epoch 2/10
19/19 - 1s - loss: 123.8072 - loglik: -1.2228e+02 - logprior: -1.5239e+00
Epoch 3/10
19/19 - 1s - loss: 111.7441 - loglik: -1.1019e+02 - logprior: -1.5566e+00
Epoch 4/10
19/19 - 1s - loss: 108.7194 - loglik: -1.0718e+02 - logprior: -1.5374e+00
Epoch 5/10
19/19 - 1s - loss: 107.7660 - loglik: -1.0627e+02 - logprior: -1.4970e+00
Epoch 6/10
19/19 - 1s - loss: 107.5495 - loglik: -1.0607e+02 - logprior: -1.4834e+00
Epoch 7/10
19/19 - 1s - loss: 107.1598 - loglik: -1.0569e+02 - logprior: -1.4650e+00
Epoch 8/10
19/19 - 1s - loss: 106.9365 - loglik: -1.0548e+02 - logprior: -1.4553e+00
Epoch 9/10
19/19 - 1s - loss: 106.7354 - loglik: -1.0528e+02 - logprior: -1.4596e+00
Epoch 10/10
19/19 - 1s - loss: 106.9627 - loglik: -1.0551e+02 - logprior: -1.4535e+00
Fitted a model with MAP estimate = -106.5903
expansions: [(5, 1), (7, 2), (9, 1), (16, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 108.7309 - loglik: -1.0544e+02 - logprior: -3.2912e+00
Epoch 2/2
19/19 - 1s - loss: 100.3327 - loglik: -9.8962e+01 - logprior: -1.3710e+00
Fitted a model with MAP estimate = -99.5413
expansions: []
discards: [21 38 41]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.7709 - loglik: -9.9590e+01 - logprior: -3.1806e+00
Epoch 2/2
19/19 - 1s - loss: 99.7621 - loglik: -9.8483e+01 - logprior: -1.2786e+00
Fitted a model with MAP estimate = -99.1687
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 102.1561 - loglik: -9.9006e+01 - logprior: -3.1502e+00
Epoch 2/10
19/19 - 1s - loss: 99.6547 - loglik: -9.8385e+01 - logprior: -1.2699e+00
Epoch 3/10
19/19 - 1s - loss: 99.0927 - loglik: -9.7947e+01 - logprior: -1.1459e+00
Epoch 4/10
19/19 - 1s - loss: 98.2763 - loglik: -9.7155e+01 - logprior: -1.1218e+00
Epoch 5/10
19/19 - 1s - loss: 98.1802 - loglik: -9.7077e+01 - logprior: -1.1033e+00
Epoch 6/10
19/19 - 1s - loss: 97.8359 - loglik: -9.6734e+01 - logprior: -1.1018e+00
Epoch 7/10
19/19 - 1s - loss: 97.9383 - loglik: -9.6867e+01 - logprior: -1.0713e+00
Fitted a model with MAP estimate = -97.5595
Time for alignment: 44.3827
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.0616 - loglik: -1.5083e+02 - logprior: -3.2301e+00
Epoch 2/10
19/19 - 1s - loss: 123.4328 - loglik: -1.2191e+02 - logprior: -1.5260e+00
Epoch 3/10
19/19 - 1s - loss: 110.2339 - loglik: -1.0865e+02 - logprior: -1.5811e+00
Epoch 4/10
19/19 - 1s - loss: 107.2360 - loglik: -1.0569e+02 - logprior: -1.5455e+00
Epoch 5/10
19/19 - 1s - loss: 106.2700 - loglik: -1.0476e+02 - logprior: -1.5127e+00
Epoch 6/10
19/19 - 1s - loss: 105.7938 - loglik: -1.0429e+02 - logprior: -1.5007e+00
Epoch 7/10
19/19 - 1s - loss: 105.6276 - loglik: -1.0415e+02 - logprior: -1.4804e+00
Epoch 8/10
19/19 - 1s - loss: 105.2202 - loglik: -1.0373e+02 - logprior: -1.4918e+00
Epoch 9/10
19/19 - 1s - loss: 105.1390 - loglik: -1.0366e+02 - logprior: -1.4814e+00
Epoch 10/10
19/19 - 1s - loss: 105.2455 - loglik: -1.0376e+02 - logprior: -1.4854e+00
Fitted a model with MAP estimate = -104.9150
expansions: [(5, 1), (7, 1), (10, 1), (14, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 107.8042 - loglik: -1.0450e+02 - logprior: -3.3042e+00
Epoch 2/2
19/19 - 1s - loss: 100.3240 - loglik: -9.8940e+01 - logprior: -1.3844e+00
Fitted a model with MAP estimate = -99.4318
expansions: []
discards: [20 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.5477 - loglik: -9.9375e+01 - logprior: -3.1729e+00
Epoch 2/2
19/19 - 1s - loss: 99.7908 - loglik: -9.8517e+01 - logprior: -1.2736e+00
Fitted a model with MAP estimate = -99.1558
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 102.1461 - loglik: -9.8999e+01 - logprior: -3.1474e+00
Epoch 2/10
19/19 - 1s - loss: 99.5189 - loglik: -9.8259e+01 - logprior: -1.2602e+00
Epoch 3/10
19/19 - 1s - loss: 99.1954 - loglik: -9.8061e+01 - logprior: -1.1343e+00
Epoch 4/10
19/19 - 1s - loss: 98.4894 - loglik: -9.7378e+01 - logprior: -1.1115e+00
Epoch 5/10
19/19 - 1s - loss: 98.1281 - loglik: -9.7034e+01 - logprior: -1.0940e+00
Epoch 6/10
19/19 - 1s - loss: 97.6751 - loglik: -9.6587e+01 - logprior: -1.0881e+00
Epoch 7/10
19/19 - 1s - loss: 97.8690 - loglik: -9.6802e+01 - logprior: -1.0666e+00
Fitted a model with MAP estimate = -97.6032
Time for alignment: 42.9763
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 153.9442 - loglik: -1.5072e+02 - logprior: -3.2282e+00
Epoch 2/10
19/19 - 1s - loss: 124.2317 - loglik: -1.2271e+02 - logprior: -1.5170e+00
Epoch 3/10
19/19 - 1s - loss: 112.2729 - loglik: -1.1076e+02 - logprior: -1.5177e+00
Epoch 4/10
19/19 - 1s - loss: 109.6382 - loglik: -1.0817e+02 - logprior: -1.4696e+00
Epoch 5/10
19/19 - 1s - loss: 108.4152 - loglik: -1.0699e+02 - logprior: -1.4217e+00
Epoch 6/10
19/19 - 1s - loss: 107.9386 - loglik: -1.0653e+02 - logprior: -1.4118e+00
Epoch 7/10
19/19 - 1s - loss: 107.6470 - loglik: -1.0624e+02 - logprior: -1.4075e+00
Epoch 8/10
19/19 - 1s - loss: 107.1973 - loglik: -1.0580e+02 - logprior: -1.3935e+00
Epoch 9/10
19/19 - 1s - loss: 107.2094 - loglik: -1.0581e+02 - logprior: -1.3967e+00
Fitted a model with MAP estimate = -107.0298
expansions: [(5, 1), (7, 2), (9, 1), (16, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.8943 - loglik: -1.0462e+02 - logprior: -3.2779e+00
Epoch 2/2
19/19 - 1s - loss: 100.1606 - loglik: -9.8772e+01 - logprior: -1.3889e+00
Fitted a model with MAP estimate = -99.4505
expansions: []
discards: [21 38 42]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 102.6860 - loglik: -9.9510e+01 - logprior: -3.1759e+00
Epoch 2/2
19/19 - 1s - loss: 99.6806 - loglik: -9.8407e+01 - logprior: -1.2737e+00
Fitted a model with MAP estimate = -99.1733
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 102.1928 - loglik: -9.9056e+01 - logprior: -3.1364e+00
Epoch 2/10
19/19 - 1s - loss: 99.5958 - loglik: -9.8341e+01 - logprior: -1.2544e+00
Epoch 3/10
19/19 - 1s - loss: 98.9795 - loglik: -9.7838e+01 - logprior: -1.1416e+00
Epoch 4/10
19/19 - 1s - loss: 98.6142 - loglik: -9.7510e+01 - logprior: -1.1041e+00
Epoch 5/10
19/19 - 1s - loss: 98.0318 - loglik: -9.6942e+01 - logprior: -1.0897e+00
Epoch 6/10
19/19 - 1s - loss: 97.7188 - loglik: -9.6645e+01 - logprior: -1.0737e+00
Epoch 7/10
19/19 - 1s - loss: 97.8380 - loglik: -9.6778e+01 - logprior: -1.0602e+00
Fitted a model with MAP estimate = -97.6065
Time for alignment: 41.8096
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.0473 - loglik: -1.5082e+02 - logprior: -3.2299e+00
Epoch 2/10
19/19 - 1s - loss: 124.1615 - loglik: -1.2264e+02 - logprior: -1.5214e+00
Epoch 3/10
19/19 - 1s - loss: 111.4311 - loglik: -1.0987e+02 - logprior: -1.5652e+00
Epoch 4/10
19/19 - 1s - loss: 108.6322 - loglik: -1.0709e+02 - logprior: -1.5414e+00
Epoch 5/10
19/19 - 1s - loss: 107.9042 - loglik: -1.0641e+02 - logprior: -1.4902e+00
Epoch 6/10
19/19 - 1s - loss: 107.5329 - loglik: -1.0605e+02 - logprior: -1.4824e+00
Epoch 7/10
19/19 - 1s - loss: 107.1626 - loglik: -1.0569e+02 - logprior: -1.4696e+00
Epoch 8/10
19/19 - 1s - loss: 106.7935 - loglik: -1.0534e+02 - logprior: -1.4563e+00
Epoch 9/10
19/19 - 1s - loss: 107.0342 - loglik: -1.0558e+02 - logprior: -1.4578e+00
Fitted a model with MAP estimate = -106.6676
expansions: [(5, 1), (7, 2), (9, 1), (16, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 108.7178 - loglik: -1.0543e+02 - logprior: -3.2915e+00
Epoch 2/2
19/19 - 1s - loss: 100.4862 - loglik: -9.9103e+01 - logprior: -1.3830e+00
Fitted a model with MAP estimate = -99.5244
expansions: []
discards: [21 38 41]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.7594 - loglik: -9.9579e+01 - logprior: -3.1808e+00
Epoch 2/2
19/19 - 1s - loss: 99.6931 - loglik: -9.8416e+01 - logprior: -1.2767e+00
Fitted a model with MAP estimate = -99.1909
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 102.2349 - loglik: -9.9084e+01 - logprior: -3.1511e+00
Epoch 2/10
19/19 - 1s - loss: 99.6032 - loglik: -9.8341e+01 - logprior: -1.2625e+00
Epoch 3/10
19/19 - 1s - loss: 99.1267 - loglik: -9.7980e+01 - logprior: -1.1470e+00
Epoch 4/10
19/19 - 1s - loss: 98.4248 - loglik: -9.7304e+01 - logprior: -1.1203e+00
Epoch 5/10
19/19 - 1s - loss: 97.9472 - loglik: -9.6838e+01 - logprior: -1.1094e+00
Epoch 6/10
19/19 - 1s - loss: 97.8634 - loglik: -9.6772e+01 - logprior: -1.0918e+00
Epoch 7/10
19/19 - 1s - loss: 97.6226 - loglik: -9.6542e+01 - logprior: -1.0803e+00
Epoch 8/10
19/19 - 1s - loss: 97.6886 - loglik: -9.6619e+01 - logprior: -1.0696e+00
Fitted a model with MAP estimate = -97.5160
Time for alignment: 42.7154
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 153.9975 - loglik: -1.5076e+02 - logprior: -3.2335e+00
Epoch 2/10
19/19 - 1s - loss: 124.0401 - loglik: -1.2252e+02 - logprior: -1.5214e+00
Epoch 3/10
19/19 - 1s - loss: 110.5092 - loglik: -1.0892e+02 - logprior: -1.5937e+00
Epoch 4/10
19/19 - 1s - loss: 107.5683 - loglik: -1.0599e+02 - logprior: -1.5761e+00
Epoch 5/10
19/19 - 1s - loss: 107.2031 - loglik: -1.0569e+02 - logprior: -1.5164e+00
Epoch 6/10
19/19 - 1s - loss: 106.8144 - loglik: -1.0531e+02 - logprior: -1.5000e+00
Epoch 7/10
19/19 - 1s - loss: 106.4936 - loglik: -1.0503e+02 - logprior: -1.4638e+00
Epoch 8/10
19/19 - 1s - loss: 106.2747 - loglik: -1.0481e+02 - logprior: -1.4653e+00
Epoch 9/10
19/19 - 1s - loss: 106.2599 - loglik: -1.0480e+02 - logprior: -1.4571e+00
Epoch 10/10
19/19 - 1s - loss: 105.8830 - loglik: -1.0442e+02 - logprior: -1.4614e+00
Fitted a model with MAP estimate = -105.8623
expansions: [(5, 1), (7, 1), (10, 1), (14, 1), (16, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 2), (31, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 108.2780 - loglik: -1.0499e+02 - logprior: -3.2909e+00
Epoch 2/2
19/19 - 1s - loss: 100.3915 - loglik: -9.8996e+01 - logprior: -1.3956e+00
Fitted a model with MAP estimate = -99.5031
expansions: []
discards: [20 38 41]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.6221 - loglik: -9.9436e+01 - logprior: -3.1857e+00
Epoch 2/2
19/19 - 1s - loss: 99.7076 - loglik: -9.8435e+01 - logprior: -1.2727e+00
Fitted a model with MAP estimate = -99.1415
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 102.1779 - loglik: -9.9026e+01 - logprior: -3.1523e+00
Epoch 2/10
19/19 - 1s - loss: 99.6632 - loglik: -9.8404e+01 - logprior: -1.2591e+00
Epoch 3/10
19/19 - 1s - loss: 98.9022 - loglik: -9.7767e+01 - logprior: -1.1349e+00
Epoch 4/10
19/19 - 1s - loss: 98.5394 - loglik: -9.7426e+01 - logprior: -1.1137e+00
Epoch 5/10
19/19 - 1s - loss: 98.0373 - loglik: -9.6945e+01 - logprior: -1.0922e+00
Epoch 6/10
19/19 - 1s - loss: 97.8278 - loglik: -9.6739e+01 - logprior: -1.0884e+00
Epoch 7/10
19/19 - 1s - loss: 97.8685 - loglik: -9.6800e+01 - logprior: -1.0681e+00
Fitted a model with MAP estimate = -97.6064
Time for alignment: 44.1966
Computed alignments with likelihoods: ['-97.5595', '-97.6032', '-97.6065', '-97.5160', '-97.6064']
Best model has likelihood: -97.5160  (prior= -1.0788 )
time for generating output: 0.0997
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00046.projection.fasta
SP score = 0.9826388888888888
Training of 5 independent models on file PF01814.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd340ea83a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1ac34ceb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1251a7cd0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 366.4121 - loglik: -3.6343e+02 - logprior: -2.9807e+00
Epoch 2/10
19/19 - 3s - loss: 337.7542 - loglik: -3.3674e+02 - logprior: -1.0165e+00
Epoch 3/10
19/19 - 3s - loss: 324.5590 - loglik: -3.2332e+02 - logprior: -1.2374e+00
Epoch 4/10
19/19 - 3s - loss: 320.7352 - loglik: -3.1961e+02 - logprior: -1.1218e+00
Epoch 5/10
19/19 - 3s - loss: 319.4953 - loglik: -3.1837e+02 - logprior: -1.1239e+00
Epoch 6/10
19/19 - 3s - loss: 317.9424 - loglik: -3.1683e+02 - logprior: -1.1114e+00
Epoch 7/10
19/19 - 3s - loss: 318.2206 - loglik: -3.1712e+02 - logprior: -1.1040e+00
Fitted a model with MAP estimate = -317.7720
expansions: [(15, 1), (18, 1), (19, 2), (20, 5), (23, 2), (24, 2), (25, 1), (28, 1), (55, 3), (73, 1), (74, 1), (76, 2), (82, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 325.4633 - loglik: -3.2173e+02 - logprior: -3.7344e+00
Epoch 2/2
19/19 - 4s - loss: 317.8423 - loglik: -3.1605e+02 - logprior: -1.7959e+00
Fitted a model with MAP estimate = -316.0301
expansions: [(0, 2), (70, 1), (94, 3)]
discards: [  0  24  25  26  95 107]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 319.5989 - loglik: -3.1684e+02 - logprior: -2.7627e+00
Epoch 2/2
19/19 - 4s - loss: 315.7954 - loglik: -3.1482e+02 - logprior: -9.7547e-01
Fitted a model with MAP estimate = -314.5707
expansions: [(33, 1)]
discards: [29]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 317.9442 - loglik: -3.1520e+02 - logprior: -2.7397e+00
Epoch 2/10
19/19 - 4s - loss: 315.2834 - loglik: -3.1433e+02 - logprior: -9.5081e-01
Epoch 3/10
19/19 - 4s - loss: 313.9581 - loglik: -3.1306e+02 - logprior: -8.9333e-01
Epoch 4/10
19/19 - 4s - loss: 311.7711 - loglik: -3.1092e+02 - logprior: -8.5564e-01
Epoch 5/10
19/19 - 4s - loss: 311.2220 - loglik: -3.1038e+02 - logprior: -8.3919e-01
Epoch 6/10
19/19 - 4s - loss: 311.0935 - loglik: -3.1027e+02 - logprior: -8.2414e-01
Epoch 7/10
19/19 - 4s - loss: 310.8383 - loglik: -3.1004e+02 - logprior: -7.9874e-01
Epoch 8/10
19/19 - 4s - loss: 311.0046 - loglik: -3.1022e+02 - logprior: -7.8614e-01
Fitted a model with MAP estimate = -310.5010
Time for alignment: 100.5337
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 366.6309 - loglik: -3.6365e+02 - logprior: -2.9780e+00
Epoch 2/10
19/19 - 3s - loss: 338.5882 - loglik: -3.3757e+02 - logprior: -1.0165e+00
Epoch 3/10
19/19 - 3s - loss: 324.9475 - loglik: -3.2370e+02 - logprior: -1.2507e+00
Epoch 4/10
19/19 - 3s - loss: 321.4641 - loglik: -3.2033e+02 - logprior: -1.1357e+00
Epoch 5/10
19/19 - 3s - loss: 318.9391 - loglik: -3.1780e+02 - logprior: -1.1437e+00
Epoch 6/10
19/19 - 3s - loss: 318.7871 - loglik: -3.1764e+02 - logprior: -1.1427e+00
Epoch 7/10
19/19 - 3s - loss: 317.5654 - loglik: -3.1642e+02 - logprior: -1.1440e+00
Epoch 8/10
19/19 - 3s - loss: 317.6057 - loglik: -3.1646e+02 - logprior: -1.1433e+00
Fitted a model with MAP estimate = -317.3184
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 4), (24, 3), (26, 1), (29, 1), (32, 1), (51, 1), (53, 2), (73, 1), (74, 1), (76, 2), (82, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 324.7731 - loglik: -3.2103e+02 - logprior: -3.7398e+00
Epoch 2/2
19/19 - 4s - loss: 317.8414 - loglik: -3.1603e+02 - logprior: -1.8142e+00
Fitted a model with MAP estimate = -315.7671
expansions: [(0, 2), (31, 1), (93, 3)]
discards: [  0  94 106]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 318.5221 - loglik: -3.1573e+02 - logprior: -2.7903e+00
Epoch 2/2
19/19 - 4s - loss: 315.0419 - loglik: -3.1406e+02 - logprior: -9.8339e-01
Fitted a model with MAP estimate = -313.7556
expansions: []
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 317.6974 - loglik: -3.1499e+02 - logprior: -2.7057e+00
Epoch 2/10
19/19 - 4s - loss: 315.0937 - loglik: -3.1418e+02 - logprior: -9.1368e-01
Epoch 3/10
19/19 - 4s - loss: 313.6604 - loglik: -3.1287e+02 - logprior: -7.9487e-01
Epoch 4/10
19/19 - 4s - loss: 311.7299 - loglik: -3.1097e+02 - logprior: -7.5538e-01
Epoch 5/10
19/19 - 4s - loss: 311.2319 - loglik: -3.1037e+02 - logprior: -8.6229e-01
Epoch 6/10
19/19 - 4s - loss: 310.9625 - loglik: -3.1014e+02 - logprior: -8.1824e-01
Epoch 7/10
19/19 - 4s - loss: 310.5149 - loglik: -3.0971e+02 - logprior: -8.0452e-01
Epoch 8/10
19/19 - 4s - loss: 310.4151 - loglik: -3.0963e+02 - logprior: -7.8352e-01
Epoch 9/10
19/19 - 4s - loss: 310.3694 - loglik: -3.0960e+02 - logprior: -7.7183e-01
Epoch 10/10
19/19 - 4s - loss: 311.0216 - loglik: -3.1027e+02 - logprior: -7.5022e-01
Fitted a model with MAP estimate = -310.2625
Time for alignment: 110.2602
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 366.4520 - loglik: -3.6347e+02 - logprior: -2.9776e+00
Epoch 2/10
19/19 - 3s - loss: 339.0240 - loglik: -3.3802e+02 - logprior: -1.0085e+00
Epoch 3/10
19/19 - 3s - loss: 325.7737 - loglik: -3.2452e+02 - logprior: -1.2579e+00
Epoch 4/10
19/19 - 3s - loss: 320.3658 - loglik: -3.1920e+02 - logprior: -1.1693e+00
Epoch 5/10
19/19 - 3s - loss: 319.2333 - loglik: -3.1806e+02 - logprior: -1.1761e+00
Epoch 6/10
19/19 - 3s - loss: 318.0026 - loglik: -3.1684e+02 - logprior: -1.1634e+00
Epoch 7/10
19/19 - 3s - loss: 317.9876 - loglik: -3.1685e+02 - logprior: -1.1418e+00
Epoch 8/10
19/19 - 3s - loss: 318.0198 - loglik: -3.1688e+02 - logprior: -1.1434e+00
Fitted a model with MAP estimate = -317.5606
expansions: [(15, 1), (18, 1), (19, 2), (20, 4), (23, 2), (26, 1), (29, 1), (32, 1), (35, 2), (53, 2), (73, 1), (74, 1), (76, 2), (82, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 325.7759 - loglik: -3.2204e+02 - logprior: -3.7399e+00
Epoch 2/2
19/19 - 4s - loss: 318.0993 - loglik: -3.1629e+02 - logprior: -1.8047e+00
Fitted a model with MAP estimate = -316.2034
expansions: [(0, 2), (93, 3)]
discards: [  0  25  26  47  94 106]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 319.5740 - loglik: -3.1681e+02 - logprior: -2.7632e+00
Epoch 2/2
19/19 - 4s - loss: 315.9165 - loglik: -3.1495e+02 - logprior: -9.7092e-01
Fitted a model with MAP estimate = -314.6409
expansions: [(26, 3), (66, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 318.3115 - loglik: -3.1563e+02 - logprior: -2.6781e+00
Epoch 2/10
19/19 - 4s - loss: 315.1985 - loglik: -3.1430e+02 - logprior: -8.9423e-01
Epoch 3/10
19/19 - 4s - loss: 313.5049 - loglik: -3.1270e+02 - logprior: -8.0813e-01
Epoch 4/10
19/19 - 4s - loss: 312.0319 - loglik: -3.1114e+02 - logprior: -8.9631e-01
Epoch 5/10
19/19 - 4s - loss: 310.8665 - loglik: -3.1003e+02 - logprior: -8.3978e-01
Epoch 6/10
19/19 - 4s - loss: 310.7893 - loglik: -3.0996e+02 - logprior: -8.2591e-01
Epoch 7/10
19/19 - 4s - loss: 310.6509 - loglik: -3.0985e+02 - logprior: -8.0116e-01
Epoch 8/10
19/19 - 4s - loss: 310.3053 - loglik: -3.0952e+02 - logprior: -7.8794e-01
Epoch 9/10
19/19 - 4s - loss: 310.6752 - loglik: -3.0990e+02 - logprior: -7.7325e-01
Fitted a model with MAP estimate = -310.2474
Time for alignment: 110.5389
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 366.3639 - loglik: -3.6338e+02 - logprior: -2.9803e+00
Epoch 2/10
19/19 - 3s - loss: 338.7152 - loglik: -3.3768e+02 - logprior: -1.0326e+00
Epoch 3/10
19/19 - 3s - loss: 325.4233 - loglik: -3.2416e+02 - logprior: -1.2611e+00
Epoch 4/10
19/19 - 3s - loss: 320.6686 - loglik: -3.1949e+02 - logprior: -1.1738e+00
Epoch 5/10
19/19 - 3s - loss: 319.1189 - loglik: -3.1795e+02 - logprior: -1.1733e+00
Epoch 6/10
19/19 - 3s - loss: 318.4073 - loglik: -3.1726e+02 - logprior: -1.1483e+00
Epoch 7/10
19/19 - 3s - loss: 317.8520 - loglik: -3.1672e+02 - logprior: -1.1338e+00
Epoch 8/10
19/19 - 3s - loss: 317.6799 - loglik: -3.1655e+02 - logprior: -1.1274e+00
Epoch 9/10
19/19 - 3s - loss: 317.7337 - loglik: -3.1660e+02 - logprior: -1.1290e+00
Fitted a model with MAP estimate = -317.2954
expansions: [(15, 1), (18, 1), (19, 2), (20, 5), (23, 2), (24, 3), (29, 1), (32, 1), (38, 1), (53, 2), (73, 1), (74, 1), (76, 4), (82, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 324.9546 - loglik: -3.2124e+02 - logprior: -3.7162e+00
Epoch 2/2
19/19 - 4s - loss: 317.2068 - loglik: -3.1542e+02 - logprior: -1.7916e+00
Fitted a model with MAP estimate = -315.2151
expansions: [(0, 2)]
discards: [  0  24  25  26 110]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 318.5996 - loglik: -3.1584e+02 - logprior: -2.7619e+00
Epoch 2/2
19/19 - 4s - loss: 315.6420 - loglik: -3.1469e+02 - logprior: -9.4980e-01
Fitted a model with MAP estimate = -314.2560
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 317.6777 - loglik: -3.1495e+02 - logprior: -2.7272e+00
Epoch 2/10
19/19 - 5s - loss: 315.2475 - loglik: -3.1432e+02 - logprior: -9.2694e-01
Epoch 3/10
19/19 - 5s - loss: 313.8011 - loglik: -3.1293e+02 - logprior: -8.7301e-01
Epoch 4/10
19/19 - 5s - loss: 311.8904 - loglik: -3.1106e+02 - logprior: -8.2702e-01
Epoch 5/10
19/19 - 4s - loss: 311.3227 - loglik: -3.1052e+02 - logprior: -8.0079e-01
Epoch 6/10
19/19 - 4s - loss: 311.0399 - loglik: -3.1025e+02 - logprior: -7.9319e-01
Epoch 7/10
19/19 - 5s - loss: 310.6967 - loglik: -3.0993e+02 - logprior: -7.6454e-01
Epoch 8/10
19/19 - 5s - loss: 311.2364 - loglik: -3.1048e+02 - logprior: -7.5720e-01
Fitted a model with MAP estimate = -310.5800
Time for alignment: 119.4137
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 366.2395 - loglik: -3.6326e+02 - logprior: -2.9829e+00
Epoch 2/10
19/19 - 3s - loss: 337.3933 - loglik: -3.3637e+02 - logprior: -1.0261e+00
Epoch 3/10
19/19 - 3s - loss: 324.5716 - loglik: -3.2335e+02 - logprior: -1.2218e+00
Epoch 4/10
19/19 - 3s - loss: 321.3060 - loglik: -3.2021e+02 - logprior: -1.1009e+00
Epoch 5/10
19/19 - 3s - loss: 319.9294 - loglik: -3.1883e+02 - logprior: -1.1029e+00
Epoch 6/10
19/19 - 3s - loss: 318.7007 - loglik: -3.1761e+02 - logprior: -1.0944e+00
Epoch 7/10
19/19 - 3s - loss: 318.2140 - loglik: -3.1712e+02 - logprior: -1.0920e+00
Epoch 8/10
19/19 - 3s - loss: 317.9096 - loglik: -3.1681e+02 - logprior: -1.0972e+00
Epoch 9/10
19/19 - 3s - loss: 317.6404 - loglik: -3.1654e+02 - logprior: -1.0982e+00
Epoch 10/10
19/19 - 3s - loss: 317.4310 - loglik: -3.1633e+02 - logprior: -1.0983e+00
Fitted a model with MAP estimate = -317.3133
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 4), (23, 1), (24, 4), (25, 1), (32, 1), (73, 1), (74, 1), (76, 2), (82, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 325.4112 - loglik: -3.2168e+02 - logprior: -3.7348e+00
Epoch 2/2
19/19 - 4s - loss: 318.1705 - loglik: -3.1637e+02 - logprior: -1.7998e+00
Fitted a model with MAP estimate = -316.4403
expansions: [(0, 2), (31, 1), (65, 1)]
discards: [  0  26  92 104]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 319.7461 - loglik: -3.1697e+02 - logprior: -2.7735e+00
Epoch 2/2
19/19 - 4s - loss: 316.3137 - loglik: -3.1533e+02 - logprior: -9.8060e-01
Fitted a model with MAP estimate = -314.9779
expansions: [(28, 2), (72, 1), (93, 4)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 318.9681 - loglik: -3.1630e+02 - logprior: -2.6676e+00
Epoch 2/10
19/19 - 4s - loss: 315.1992 - loglik: -3.1429e+02 - logprior: -9.0700e-01
Epoch 3/10
19/19 - 4s - loss: 313.6837 - loglik: -3.1287e+02 - logprior: -8.1633e-01
Epoch 4/10
19/19 - 5s - loss: 312.0481 - loglik: -3.1116e+02 - logprior: -8.8773e-01
Epoch 5/10
19/19 - 4s - loss: 310.8584 - loglik: -3.1003e+02 - logprior: -8.3269e-01
Epoch 6/10
19/19 - 5s - loss: 311.1242 - loglik: -3.1032e+02 - logprior: -8.0427e-01
Fitted a model with MAP estimate = -310.4893
Time for alignment: 111.2124
Computed alignments with likelihoods: ['-310.5010', '-310.2625', '-310.2474', '-310.5800', '-310.4893']
Best model has likelihood: -310.2474  (prior= -0.7584 )
time for generating output: 0.1302
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01814.projection.fasta
SP score = 0.8540646425073457
Training of 5 independent models on file PF00155.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd36b2230d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd124941670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd4e5d47700>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 42s - loss: 921.6323 - loglik: -9.2020e+02 - logprior: -1.4334e+00
Epoch 2/10
39/39 - 38s - loss: 828.6785 - loglik: -8.2729e+02 - logprior: -1.3863e+00
Epoch 3/10
39/39 - 36s - loss: 816.5876 - loglik: -8.1517e+02 - logprior: -1.4210e+00
Epoch 4/10
39/39 - 36s - loss: 812.7272 - loglik: -8.1136e+02 - logprior: -1.3674e+00
Epoch 5/10
39/39 - 38s - loss: 811.5425 - loglik: -8.1017e+02 - logprior: -1.3684e+00
Epoch 6/10
39/39 - 40s - loss: 811.3267 - loglik: -8.0996e+02 - logprior: -1.3632e+00
Epoch 7/10
39/39 - 38s - loss: 810.8778 - loglik: -8.0950e+02 - logprior: -1.3753e+00
Epoch 8/10
39/39 - 36s - loss: 810.8372 - loglik: -8.0948e+02 - logprior: -1.3537e+00
Epoch 9/10
39/39 - 36s - loss: 810.8112 - loglik: -8.0942e+02 - logprior: -1.3904e+00
Epoch 10/10
39/39 - 39s - loss: 810.4502 - loglik: -8.0909e+02 - logprior: -1.3626e+00
Fitted a model with MAP estimate = -802.5596
expansions: [(0, 2), (16, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 2), (32, 1), (43, 2), (44, 3), (56, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 2), (86, 1), (88, 1), (95, 2), (102, 1), (103, 2), (121, 1), (124, 1), (125, 1), (131, 1), (134, 1), (145, 1), (148, 1), (150, 1), (153, 2), (154, 1), (155, 2), (182, 1), (183, 1), (184, 1), (185, 1), (187, 1), (189, 1), (201, 1), (206, 3), (207, 1), (208, 1), (209, 2), (211, 1), (223, 1), (224, 2), (226, 2), (227, 1), (237, 1), (239, 1), (240, 2), (242, 1), (245, 1), (261, 2), (262, 1), (263, 1), (270, 3), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 360 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 74s - loss: 806.3447 - loglik: -8.0437e+02 - logprior: -1.9712e+00
Epoch 2/2
39/39 - 74s - loss: 793.3511 - loglik: -7.9277e+02 - logprior: -5.7898e-01
Fitted a model with MAP estimate = -782.6241
expansions: [(54, 1), (55, 1)]
discards: [  0  30  57 104 132 193 258 259 285 289 307 333 347]
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 64s - loss: 799.3149 - loglik: -7.9702e+02 - logprior: -2.2925e+00
Epoch 2/2
39/39 - 57s - loss: 793.3138 - loglik: -7.9289e+02 - logprior: -4.2311e-01
Fitted a model with MAP estimate = -783.3304
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 350 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 56s - loss: 788.5872 - loglik: -7.8726e+02 - logprior: -1.3226e+00
Epoch 2/10
39/39 - 54s - loss: 785.3094 - loglik: -7.8506e+02 - logprior: -2.4473e-01
Epoch 3/10
39/39 - 60s - loss: 783.3212 - loglik: -7.8327e+02 - logprior: -4.8868e-02
Epoch 4/10
39/39 - 55s - loss: 779.9695 - loglik: -7.8002e+02 - logprior: 0.0532
Epoch 5/10
39/39 - 54s - loss: 781.0228 - loglik: -7.8114e+02 - logprior: 0.1166
Fitted a model with MAP estimate = -779.5905
Time for alignment: 1232.7338
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 42s - loss: 921.8434 - loglik: -9.2042e+02 - logprior: -1.4218e+00
Epoch 2/10
39/39 - 44s - loss: 829.0556 - loglik: -8.2775e+02 - logprior: -1.3033e+00
Epoch 3/10
39/39 - 46s - loss: 816.7014 - loglik: -8.1528e+02 - logprior: -1.4241e+00
Epoch 4/10
39/39 - 46s - loss: 813.7805 - loglik: -8.1242e+02 - logprior: -1.3627e+00
Epoch 5/10
39/39 - 47s - loss: 813.0508 - loglik: -8.1167e+02 - logprior: -1.3855e+00
Epoch 6/10
39/39 - 44s - loss: 812.6436 - loglik: -8.1125e+02 - logprior: -1.3927e+00
Epoch 7/10
39/39 - 43s - loss: 812.2101 - loglik: -8.1085e+02 - logprior: -1.3630e+00
Epoch 8/10
39/39 - 45s - loss: 811.9122 - loglik: -8.1054e+02 - logprior: -1.3688e+00
Epoch 9/10
39/39 - 44s - loss: 812.2781 - loglik: -8.1091e+02 - logprior: -1.3651e+00
Fitted a model with MAP estimate = -803.8987
expansions: [(0, 2), (16, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 2), (41, 1), (43, 3), (44, 2), (45, 1), (55, 1), (58, 1), (61, 1), (80, 1), (81, 1), (82, 2), (86, 1), (88, 1), (95, 2), (103, 1), (120, 1), (123, 1), (124, 1), (125, 1), (131, 1), (134, 1), (145, 1), (150, 1), (153, 2), (156, 2), (162, 1), (182, 1), (184, 2), (185, 1), (187, 2), (188, 1), (201, 1), (206, 3), (209, 1), (210, 1), (211, 1), (218, 1), (220, 2), (226, 2), (228, 1), (240, 3), (242, 1), (245, 1), (261, 2), (262, 1), (263, 1), (270, 3), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 356 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 64s - loss: 806.2346 - loglik: -8.0419e+02 - logprior: -2.0442e+00
Epoch 2/2
39/39 - 61s - loss: 793.6539 - loglik: -7.9304e+02 - logprior: -6.0974e-01
Fitted a model with MAP estimate = -783.1166
expansions: [(54, 1), (184, 1)]
discards: [  0  30 104 191 234 257 286 329]
Re-initialized the encoder parameters.
Fitting a model of length 350 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 61s - loss: 798.3319 - loglik: -7.9597e+02 - logprior: -2.3624e+00
Epoch 2/2
39/39 - 55s - loss: 792.9366 - loglik: -7.9251e+02 - logprior: -4.3010e-01
Fitted a model with MAP estimate = -782.9015
expansions: [(0, 2)]
discards: [ 0 31]
Re-initialized the encoder parameters.
Fitting a model of length 350 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 67s - loss: 788.2161 - loglik: -7.8687e+02 - logprior: -1.3510e+00
Epoch 2/10
39/39 - 69s - loss: 784.0717 - loglik: -7.8387e+02 - logprior: -2.0479e-01
Epoch 3/10
39/39 - 69s - loss: 782.8410 - loglik: -7.8276e+02 - logprior: -8.2730e-02
Epoch 4/10
39/39 - 63s - loss: 780.7365 - loglik: -7.8076e+02 - logprior: 0.0215
Epoch 5/10
39/39 - 64s - loss: 779.7928 - loglik: -7.7988e+02 - logprior: 0.0920
Epoch 6/10
39/39 - 67s - loss: 779.8911 - loglik: -7.8005e+02 - logprior: 0.1612
Fitted a model with MAP estimate = -778.7118
Time for alignment: 1313.7786
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 921.2803 - loglik: -9.1984e+02 - logprior: -1.4446e+00
Epoch 2/10
39/39 - 39s - loss: 830.0833 - loglik: -8.2869e+02 - logprior: -1.3940e+00
Epoch 3/10
39/39 - 41s - loss: 817.5729 - loglik: -8.1615e+02 - logprior: -1.4270e+00
Epoch 4/10
39/39 - 42s - loss: 813.7618 - loglik: -8.1237e+02 - logprior: -1.3954e+00
Epoch 5/10
39/39 - 38s - loss: 813.0407 - loglik: -8.1162e+02 - logprior: -1.4167e+00
Epoch 6/10
39/39 - 36s - loss: 812.3894 - loglik: -8.1100e+02 - logprior: -1.3939e+00
Epoch 7/10
39/39 - 37s - loss: 812.1204 - loglik: -8.1072e+02 - logprior: -1.3956e+00
Epoch 8/10
39/39 - 40s - loss: 811.7891 - loglik: -8.1040e+02 - logprior: -1.3882e+00
Epoch 9/10
39/39 - 41s - loss: 811.9142 - loglik: -8.1050e+02 - logprior: -1.4138e+00
Fitted a model with MAP estimate = -803.4612
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (24, 1), (25, 2), (30, 1), (33, 1), (43, 3), (44, 2), (50, 1), (56, 1), (59, 1), (65, 1), (80, 1), (81, 1), (82, 2), (86, 1), (88, 1), (95, 2), (102, 1), (103, 1), (121, 1), (124, 1), (125, 1), (131, 1), (134, 1), (145, 1), (148, 1), (154, 2), (155, 1), (156, 3), (171, 1), (182, 1), (185, 1), (186, 1), (187, 1), (188, 1), (189, 1), (198, 1), (206, 3), (209, 1), (210, 1), (212, 1), (220, 1), (221, 1), (227, 2), (228, 1), (230, 1), (241, 2), (243, 1), (246, 1), (258, 1), (261, 2), (262, 1), (263, 1), (269, 1), (270, 2), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 356 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 60s - loss: 806.4690 - loglik: -8.0452e+02 - logprior: -1.9489e+00
Epoch 2/2
39/39 - 66s - loss: 794.3811 - loglik: -7.9384e+02 - logprior: -5.4595e-01
Fitted a model with MAP estimate = -783.7281
expansions: [(55, 1), (117, 1)]
discards: [  0 103 191 195 257 286 329 343]
Re-initialized the encoder parameters.
Fitting a model of length 350 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 71s - loss: 799.0229 - loglik: -7.9666e+02 - logprior: -2.3587e+00
Epoch 2/2
39/39 - 70s - loss: 793.4955 - loglik: -7.9307e+02 - logprior: -4.2460e-01
Fitted a model with MAP estimate = -783.5629
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 351 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 65s - loss: 788.3087 - loglik: -7.8710e+02 - logprior: -1.2119e+00
Epoch 2/10
39/39 - 66s - loss: 785.2727 - loglik: -7.8507e+02 - logprior: -1.9920e-01
Epoch 3/10
39/39 - 66s - loss: 783.6496 - loglik: -7.8352e+02 - logprior: -1.2788e-01
Epoch 4/10
39/39 - 62s - loss: 781.7368 - loglik: -7.8173e+02 - logprior: -3.7015e-03
Epoch 5/10
39/39 - 56s - loss: 780.3853 - loglik: -7.8042e+02 - logprior: 0.0327
Epoch 6/10
39/39 - 61s - loss: 780.1435 - loglik: -7.8029e+02 - logprior: 0.1489
Epoch 7/10
39/39 - 56s - loss: 779.9610 - loglik: -7.8016e+02 - logprior: 0.1990
Epoch 8/10
39/39 - 53s - loss: 780.0829 - loglik: -7.8040e+02 - logprior: 0.3129
Fitted a model with MAP estimate = -779.3170
Time for alignment: 1411.5203
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 922.4429 - loglik: -9.2100e+02 - logprior: -1.4458e+00
Epoch 2/10
39/39 - 40s - loss: 828.9811 - loglik: -8.2762e+02 - logprior: -1.3600e+00
Epoch 3/10
39/39 - 37s - loss: 818.0735 - loglik: -8.1667e+02 - logprior: -1.4075e+00
Epoch 4/10
39/39 - 36s - loss: 815.2220 - loglik: -8.1383e+02 - logprior: -1.3888e+00
Epoch 5/10
39/39 - 38s - loss: 814.2018 - loglik: -8.1280e+02 - logprior: -1.4032e+00
Epoch 6/10
39/39 - 40s - loss: 814.0394 - loglik: -8.1260e+02 - logprior: -1.4415e+00
Epoch 7/10
39/39 - 43s - loss: 813.5455 - loglik: -8.1216e+02 - logprior: -1.3894e+00
Epoch 8/10
39/39 - 45s - loss: 813.5737 - loglik: -8.1220e+02 - logprior: -1.3786e+00
Fitted a model with MAP estimate = -805.1077
expansions: [(0, 2), (16, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 2), (30, 1), (40, 1), (41, 2), (42, 1), (43, 1), (44, 2), (56, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 2), (86, 1), (88, 1), (95, 2), (103, 1), (105, 2), (121, 1), (124, 2), (130, 1), (142, 1), (148, 2), (152, 1), (154, 1), (155, 3), (161, 1), (181, 1), (184, 1), (185, 1), (186, 1), (187, 1), (188, 1), (201, 1), (205, 3), (208, 1), (209, 1), (211, 1), (220, 1), (222, 1), (226, 2), (228, 1), (237, 1), (240, 2), (242, 1), (245, 1), (250, 1), (262, 1), (263, 1), (269, 1), (270, 2), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 357 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 73s - loss: 808.4023 - loglik: -8.0631e+02 - logprior: -2.0906e+00
Epoch 2/2
39/39 - 61s - loss: 794.5361 - loglik: -7.9386e+02 - logprior: -6.7330e-01
Fitted a model with MAP estimate = -783.8939
expansions: [(161, 1)]
discards: [  0  30  54 106 136 197 259 287 319 344]
Re-initialized the encoder parameters.
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 46s - loss: 799.4279 - loglik: -7.9707e+02 - logprior: -2.3556e+00
Epoch 2/2
39/39 - 43s - loss: 793.9976 - loglik: -7.9363e+02 - logprior: -3.7180e-01
Fitted a model with MAP estimate = -784.2583
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 51s - loss: 788.6344 - loglik: -7.8737e+02 - logprior: -1.2606e+00
Epoch 2/10
39/39 - 49s - loss: 785.9985 - loglik: -7.8584e+02 - logprior: -1.6325e-01
Epoch 3/10
39/39 - 50s - loss: 782.9991 - loglik: -7.8298e+02 - logprior: -2.3184e-02
Epoch 4/10
39/39 - 50s - loss: 781.8080 - loglik: -7.8186e+02 - logprior: 0.0522
Epoch 5/10
39/39 - 51s - loss: 780.5274 - loglik: -7.8067e+02 - logprior: 0.1451
Epoch 6/10
39/39 - 51s - loss: 781.4122 - loglik: -7.8157e+02 - logprior: 0.1591
Fitted a model with MAP estimate = -779.7970
Time for alignment: 1085.2344
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 35s - loss: 920.9036 - loglik: -9.1946e+02 - logprior: -1.4417e+00
Epoch 2/10
39/39 - 32s - loss: 829.5234 - loglik: -8.2821e+02 - logprior: -1.3164e+00
Epoch 3/10
39/39 - 32s - loss: 818.1334 - loglik: -8.1677e+02 - logprior: -1.3610e+00
Epoch 4/10
39/39 - 31s - loss: 815.2952 - loglik: -8.1396e+02 - logprior: -1.3366e+00
Epoch 5/10
39/39 - 31s - loss: 814.6896 - loglik: -8.1335e+02 - logprior: -1.3378e+00
Epoch 6/10
39/39 - 31s - loss: 813.5897 - loglik: -8.1230e+02 - logprior: -1.2942e+00
Epoch 7/10
39/39 - 31s - loss: 813.1718 - loglik: -8.1185e+02 - logprior: -1.3240e+00
Epoch 8/10
39/39 - 32s - loss: 813.0571 - loglik: -8.1178e+02 - logprior: -1.2759e+00
Epoch 9/10
39/39 - 31s - loss: 812.8510 - loglik: -8.1150e+02 - logprior: -1.3503e+00
Epoch 10/10
39/39 - 31s - loss: 813.0558 - loglik: -8.1176e+02 - logprior: -1.2933e+00
Fitted a model with MAP estimate = -804.5272
expansions: [(0, 2), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2), (31, 1), (41, 1), (43, 3), (56, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 2), (86, 1), (88, 1), (95, 2), (102, 1), (105, 2), (121, 1), (124, 1), (126, 1), (131, 1), (134, 1), (145, 1), (148, 1), (150, 1), (153, 2), (156, 2), (170, 1), (182, 1), (184, 2), (185, 1), (187, 1), (188, 1), (189, 1), (206, 3), (207, 1), (208, 1), (211, 1), (213, 1), (223, 1), (224, 2), (226, 2), (227, 1), (229, 1), (240, 2), (242, 1), (245, 1), (261, 2), (262, 1), (263, 1), (270, 3), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 356 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 808.4617 - loglik: -8.0647e+02 - logprior: -1.9919e+00
Epoch 2/2
39/39 - 47s - loss: 794.7897 - loglik: -7.9423e+02 - logprior: -5.5727e-01
Fitted a model with MAP estimate = -784.0739
expansions: [(58, 2), (116, 1), (254, 1)]
discards: [  0  30 102 132 191 282 285 329 343]
Re-initialized the encoder parameters.
Fitting a model of length 351 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 799.1732 - loglik: -7.9689e+02 - logprior: -2.2875e+00
Epoch 2/2
39/39 - 50s - loss: 792.9835 - loglik: -7.9260e+02 - logprior: -3.8685e-01
Fitted a model with MAP estimate = -782.9720
expansions: [(0, 2), (54, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 353 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 54s - loss: 787.6409 - loglik: -7.8634e+02 - logprior: -1.3011e+00
Epoch 2/10
39/39 - 53s - loss: 784.2345 - loglik: -7.8406e+02 - logprior: -1.7165e-01
Epoch 3/10
39/39 - 59s - loss: 781.7216 - loglik: -7.8167e+02 - logprior: -4.7866e-02
Epoch 4/10
39/39 - 57s - loss: 780.9481 - loglik: -7.8097e+02 - logprior: 0.0189
Epoch 5/10
39/39 - 57s - loss: 779.6128 - loglik: -7.7972e+02 - logprior: 0.1110
Epoch 6/10
39/39 - 57s - loss: 779.4871 - loglik: -7.7969e+02 - logprior: 0.1979
Epoch 7/10
39/39 - 57s - loss: 778.3440 - loglik: -7.7862e+02 - logprior: 0.2740
Epoch 8/10
39/39 - 59s - loss: 779.2111 - loglik: -7.7955e+02 - logprior: 0.3400
Fitted a model with MAP estimate = -778.0807
Time for alignment: 1202.7845
Computed alignments with likelihoods: ['-779.5905', '-778.7118', '-779.3170', '-779.7970', '-778.0807']
Best model has likelihood: -778.0807  (prior= 0.3697 )
time for generating output: 1.4359
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00155.projection.fasta
SP score = 0.40880866980247715
Training of 5 independent models on file PF00450.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fcb704460a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd5088464c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd34145deb0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 63s - loss: 892.1109 - loglik: -8.9062e+02 - logprior: -1.4910e+00
Epoch 2/10
39/39 - 66s - loss: 754.6166 - loglik: -7.5311e+02 - logprior: -1.5040e+00
Epoch 3/10
39/39 - 63s - loss: 744.7133 - loglik: -7.4314e+02 - logprior: -1.5761e+00
Epoch 4/10
39/39 - 61s - loss: 741.6174 - loglik: -7.4001e+02 - logprior: -1.6117e+00
Epoch 5/10
39/39 - 60s - loss: 741.0258 - loglik: -7.3940e+02 - logprior: -1.6280e+00
Epoch 6/10
39/39 - 60s - loss: 741.0175 - loglik: -7.3935e+02 - logprior: -1.6649e+00
Epoch 7/10
39/39 - 62s - loss: 739.5520 - loglik: -7.3791e+02 - logprior: -1.6430e+00
Epoch 8/10
39/39 - 63s - loss: 740.2794 - loglik: -7.3857e+02 - logprior: -1.7133e+00
Fitted a model with MAP estimate = -738.5737
expansions: [(9, 1), (19, 1), (21, 1), (34, 1), (66, 1), (77, 1), (100, 2), (103, 1), (105, 1), (111, 1), (115, 1), (120, 1), (136, 1), (138, 2), (139, 1), (140, 1), (142, 3), (143, 2), (158, 1), (160, 2), (161, 1), (162, 2), (173, 1), (175, 1), (176, 5), (179, 5), (180, 1), (182, 1), (184, 1), (186, 1), (187, 1), (188, 1), (189, 1), (190, 1), (191, 1), (203, 3), (204, 1), (210, 3), (218, 1), (219, 2), (220, 1), (225, 1), (227, 1), (232, 1), (233, 2), (235, 1), (241, 2), (245, 1), (251, 1), (253, 1), (256, 1), (277, 1), (278, 4), (282, 1), (283, 2), (289, 1), (292, 1), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1   2 192 193 194 195 196 197 198 199 206 207 208]
Re-initialized the encoder parameters.
Fitting a model of length 402 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 92s - loss: 732.9819 - loglik: -7.3138e+02 - logprior: -1.5996e+00
Epoch 2/2
39/39 - 88s - loss: 718.6056 - loglik: -7.1823e+02 - logprior: -3.8044e-01
Fitted a model with MAP estimate = -715.6826
expansions: [(0, 3), (246, 1), (247, 1)]
discards: [187 207 208 217 252 264 341 348]
Re-initialized the encoder parameters.
Fitting a model of length 399 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 93s - loss: 723.0151 - loglik: -7.2142e+02 - logprior: -1.5924e+00
Epoch 2/2
39/39 - 87s - loss: 717.8923 - loglik: -7.1768e+02 - logprior: -2.0916e-01
Fitted a model with MAP estimate = -714.4568
expansions: [(238, 1), (255, 1)]
discards: [  1   2 214 256 259]
Re-initialized the encoder parameters.
Fitting a model of length 396 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 92s - loss: 721.1918 - loglik: -7.2009e+02 - logprior: -1.1046e+00
Epoch 2/10
39/39 - 89s - loss: 717.2853 - loglik: -7.1738e+02 - logprior: 0.0939
Epoch 3/10
39/39 - 86s - loss: 714.9623 - loglik: -7.1507e+02 - logprior: 0.1114
Epoch 4/10
39/39 - 85s - loss: 713.6652 - loglik: -7.1396e+02 - logprior: 0.2965
Epoch 5/10
39/39 - 89s - loss: 713.0371 - loglik: -7.1341e+02 - logprior: 0.3772
Epoch 6/10
39/39 - 77s - loss: 713.5497 - loglik: -7.1405e+02 - logprior: 0.5007
Fitted a model with MAP estimate = -712.1773
Time for alignment: 1748.5997
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 53s - loss: 891.9565 - loglik: -8.9047e+02 - logprior: -1.4849e+00
Epoch 2/10
39/39 - 51s - loss: 753.8118 - loglik: -7.5244e+02 - logprior: -1.3749e+00
Epoch 3/10
39/39 - 50s - loss: 743.2332 - loglik: -7.4160e+02 - logprior: -1.6353e+00
Epoch 4/10
39/39 - 48s - loss: 739.6893 - loglik: -7.3810e+02 - logprior: -1.5860e+00
Epoch 5/10
39/39 - 47s - loss: 739.6906 - loglik: -7.3809e+02 - logprior: -1.5977e+00
Fitted a model with MAP estimate = -737.8814
expansions: [(9, 1), (31, 1), (67, 1), (102, 2), (104, 2), (106, 1), (109, 1), (121, 1), (122, 1), (140, 1), (142, 1), (143, 1), (144, 3), (145, 2), (161, 2), (162, 2), (163, 1), (164, 2), (167, 1), (177, 1), (178, 5), (181, 3), (182, 3), (183, 1), (185, 1), (186, 1), (188, 1), (189, 1), (190, 1), (191, 2), (203, 1), (205, 2), (212, 1), (220, 2), (221, 3), (229, 1), (234, 3), (237, 1), (240, 1), (242, 2), (243, 2), (245, 1), (246, 1), (249, 1), (252, 1), (255, 1), (265, 1), (275, 1), (277, 2), (280, 1), (281, 1), (282, 2), (287, 1), (306, 1), (313, 1), (317, 1), (319, 1), (321, 1)]
discards: [  0   1 192 193 194 195 196 197 198 199 207 208]
Re-initialized the encoder parameters.
Fitting a model of length 399 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 68s - loss: 734.4048 - loglik: -7.3192e+02 - logprior: -2.4812e+00
Epoch 2/2
39/39 - 65s - loss: 721.4214 - loglik: -7.2017e+02 - logprior: -1.2506e+00
Fitted a model with MAP estimate = -717.9168
expansions: [(4, 1), (5, 2), (18, 1), (243, 1), (244, 1)]
discards: [  0   1 186 206 207 215 216 218 247 259 295 296 347]
Re-initialized the encoder parameters.
Fitting a model of length 392 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 74s - loss: 725.7621 - loglik: -7.2361e+02 - logprior: -2.1531e+00
Epoch 2/2
39/39 - 73s - loss: 719.3093 - loglik: -7.1856e+02 - logprior: -7.5199e-01
Fitted a model with MAP estimate = -716.0220
expansions: [(4, 1), (5, 2), (213, 2), (237, 1)]
discards: [  0   1 214 230 231 232 233 234 235]
Re-initialized the encoder parameters.
Fitting a model of length 389 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 77s - loss: 722.1584 - loglik: -7.2024e+02 - logprior: -1.9194e+00
Epoch 2/10
39/39 - 75s - loss: 716.2993 - loglik: -7.1642e+02 - logprior: 0.1180
Epoch 3/10
39/39 - 67s - loss: 714.6052 - loglik: -7.1480e+02 - logprior: 0.1929
Epoch 4/10
39/39 - 63s - loss: 713.2056 - loglik: -7.1343e+02 - logprior: 0.2233
Epoch 5/10
39/39 - 62s - loss: 712.7394 - loglik: -7.1312e+02 - logprior: 0.3849
Epoch 6/10
39/39 - 61s - loss: 712.3072 - loglik: -7.1279e+02 - logprior: 0.4846
Epoch 7/10
39/39 - 61s - loss: 711.9257 - loglik: -7.1251e+02 - logprior: 0.5878
Epoch 8/10
39/39 - 61s - loss: 712.3064 - loglik: -7.1300e+02 - logprior: 0.6913
Fitted a model with MAP estimate = -711.2752
Time for alignment: 1349.7058
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 50s - loss: 893.6038 - loglik: -8.9214e+02 - logprior: -1.4672e+00
Epoch 2/10
39/39 - 47s - loss: 758.2757 - loglik: -7.5693e+02 - logprior: -1.3459e+00
Epoch 3/10
39/39 - 48s - loss: 748.0450 - loglik: -7.4657e+02 - logprior: -1.4724e+00
Epoch 4/10
39/39 - 48s - loss: 743.9213 - loglik: -7.4236e+02 - logprior: -1.5595e+00
Epoch 5/10
39/39 - 48s - loss: 743.2789 - loglik: -7.4168e+02 - logprior: -1.5987e+00
Epoch 6/10
39/39 - 48s - loss: 742.3824 - loglik: -7.4076e+02 - logprior: -1.6236e+00
Epoch 7/10
39/39 - 48s - loss: 741.5035 - loglik: -7.3992e+02 - logprior: -1.5796e+00
Epoch 8/10
39/39 - 49s - loss: 741.5689 - loglik: -7.3994e+02 - logprior: -1.6240e+00
Fitted a model with MAP estimate = -740.4850
expansions: [(9, 1), (19, 1), (34, 1), (67, 1), (69, 1), (101, 1), (104, 2), (105, 1), (108, 1), (115, 1), (120, 1), (138, 1), (139, 1), (140, 1), (141, 1), (143, 3), (144, 2), (159, 1), (161, 1), (162, 1), (163, 1), (167, 1), (176, 2), (177, 4), (182, 1), (183, 1), (188, 1), (189, 1), (190, 1), (191, 4), (204, 2), (212, 1), (213, 2), (219, 2), (220, 1), (221, 1), (226, 1), (234, 1), (235, 2), (244, 2), (245, 2), (248, 1), (254, 1), (255, 2), (258, 1), (271, 1), (280, 3), (281, 1), (282, 3), (283, 3), (289, 1), (296, 1), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1   2 208 209]
Re-initialized the encoder parameters.
Fitting a model of length 404 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 73s - loss: 730.2140 - loglik: -7.2858e+02 - logprior: -1.6385e+00
Epoch 2/2
39/39 - 70s - loss: 717.2789 - loglik: -7.1692e+02 - logprior: -3.5951e-01
Fitted a model with MAP estimate = -714.8108
expansions: [(0, 3), (109, 1), (295, 1)]
discards: [160 205 238 239 240 241 242 243 244 245 246 247 248 249 254 261 281 283
 296 297 347 348 349]
Re-initialized the encoder parameters.
Fitting a model of length 386 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 70s - loss: 726.1935 - loglik: -7.2465e+02 - logprior: -1.5475e+00
Epoch 2/2
39/39 - 67s - loss: 719.1489 - loglik: -7.1886e+02 - logprior: -2.9138e-01
Fitted a model with MAP estimate = -715.7630
expansions: [(241, 1)]
discards: [  0   1   2 283]
Re-initialized the encoder parameters.
Fitting a model of length 383 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 70s - loss: 722.3320 - loglik: -7.2103e+02 - logprior: -1.2974e+00
Epoch 2/10
39/39 - 70s - loss: 718.7662 - loglik: -7.1898e+02 - logprior: 0.2160
Epoch 3/10
39/39 - 80s - loss: 716.8593 - loglik: -7.1721e+02 - logprior: 0.3516
Epoch 4/10
39/39 - 80s - loss: 715.5613 - loglik: -7.1598e+02 - logprior: 0.4219
Epoch 5/10
39/39 - 80s - loss: 715.7955 - loglik: -7.1616e+02 - logprior: 0.3668
Fitted a model with MAP estimate = -714.6408
Time for alignment: 1342.9523
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 61s - loss: 893.9886 - loglik: -8.9250e+02 - logprior: -1.4864e+00
Epoch 2/10
39/39 - 64s - loss: 756.8366 - loglik: -7.5543e+02 - logprior: -1.4094e+00
Epoch 3/10
39/39 - 67s - loss: 744.9382 - loglik: -7.4342e+02 - logprior: -1.5196e+00
Epoch 4/10
39/39 - 68s - loss: 742.2269 - loglik: -7.4067e+02 - logprior: -1.5600e+00
Epoch 5/10
39/39 - 70s - loss: 741.3460 - loglik: -7.3977e+02 - logprior: -1.5787e+00
Epoch 6/10
39/39 - 64s - loss: 740.0519 - loglik: -7.3847e+02 - logprior: -1.5864e+00
Epoch 7/10
39/39 - 62s - loss: 740.5716 - loglik: -7.3900e+02 - logprior: -1.5715e+00
Fitted a model with MAP estimate = -739.3062
expansions: [(9, 1), (19, 1), (31, 1), (67, 1), (69, 1), (100, 2), (104, 2), (105, 1), (108, 1), (120, 1), (121, 1), (139, 1), (141, 1), (142, 1), (143, 3), (144, 3), (159, 1), (161, 2), (162, 1), (163, 2), (174, 1), (176, 1), (177, 5), (180, 1), (181, 4), (185, 1), (187, 1), (188, 1), (189, 1), (190, 1), (191, 2), (203, 4), (211, 1), (219, 2), (220, 1), (221, 1), (226, 1), (234, 3), (237, 1), (242, 2), (243, 2), (246, 1), (250, 1), (251, 1), (253, 1), (256, 1), (277, 1), (278, 2), (279, 1), (281, 3), (282, 2), (284, 1), (295, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1   2 195 196 197 198 199 200 206 207]
Re-initialized the encoder parameters.
Fitting a model of length 402 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 95s - loss: 733.6246 - loglik: -7.3202e+02 - logprior: -1.6064e+00
Epoch 2/2
39/39 - 99s - loss: 718.1689 - loglik: -7.1769e+02 - logprior: -4.7921e-01
Fitted a model with MAP estimate = -714.8765
expansions: [(0, 3), (221, 1), (340, 1)]
discards: [101 159 187 207 208 217 244 245 246 247 248 249 261 294 295 296 343 344
 345]
Re-initialized the encoder parameters.
Fitting a model of length 388 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 85s - loss: 724.7675 - loglik: -7.2320e+02 - logprior: -1.5669e+00
Epoch 2/2
39/39 - 89s - loss: 719.1832 - loglik: -7.1892e+02 - logprior: -2.6364e-01
Fitted a model with MAP estimate = -716.6861
expansions: [(243, 1), (246, 2), (285, 1)]
discards: [  0   1   2 211 212]
Re-initialized the encoder parameters.
Fitting a model of length 387 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 91s - loss: 723.1819 - loglik: -7.2186e+02 - logprior: -1.3261e+00
Epoch 2/10
39/39 - 83s - loss: 719.0302 - loglik: -7.1921e+02 - logprior: 0.1807
Epoch 3/10
39/39 - 83s - loss: 717.8152 - loglik: -7.1803e+02 - logprior: 0.2132
Epoch 4/10
39/39 - 90s - loss: 715.8975 - loglik: -7.1623e+02 - logprior: 0.3300
Epoch 5/10
39/39 - 92s - loss: 715.1636 - loglik: -7.1547e+02 - logprior: 0.3102
Epoch 6/10
39/39 - 88s - loss: 715.0651 - loglik: -7.1562e+02 - logprior: 0.5532
Epoch 7/10
39/39 - 82s - loss: 715.1874 - loglik: -7.1589e+02 - logprior: 0.7058
Fitted a model with MAP estimate = -714.5046
Time for alignment: 1815.4082
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 60s - loss: 893.7626 - loglik: -8.9228e+02 - logprior: -1.4846e+00
Epoch 2/10
39/39 - 65s - loss: 757.8383 - loglik: -7.5647e+02 - logprior: -1.3684e+00
Epoch 3/10
39/39 - 60s - loss: 746.2450 - loglik: -7.4482e+02 - logprior: -1.4243e+00
Epoch 4/10
39/39 - 48s - loss: 743.4958 - loglik: -7.4208e+02 - logprior: -1.4188e+00
Epoch 5/10
39/39 - 45s - loss: 742.5073 - loglik: -7.4107e+02 - logprior: -1.4386e+00
Epoch 6/10
39/39 - 46s - loss: 741.5831 - loglik: -7.4016e+02 - logprior: -1.4217e+00
Epoch 7/10
39/39 - 47s - loss: 742.5787 - loglik: -7.4113e+02 - logprior: -1.4478e+00
Fitted a model with MAP estimate = -740.6246
expansions: [(20, 2), (21, 1), (31, 1), (67, 1), (78, 1), (101, 1), (105, 1), (107, 1), (114, 1), (140, 1), (142, 2), (143, 1), (144, 1), (145, 3), (146, 2), (161, 2), (162, 2), (163, 2), (167, 1), (176, 3), (177, 4), (180, 1), (181, 2), (183, 2), (185, 1), (187, 1), (188, 3), (189, 1), (190, 1), (203, 2), (205, 2), (211, 3), (219, 1), (220, 2), (221, 1), (226, 1), (234, 1), (235, 2), (237, 1), (243, 2), (244, 2), (248, 1), (256, 1), (269, 1), (276, 1), (278, 2), (281, 1), (282, 2), (283, 3), (297, 1), (299, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1 193 194 195 196 197 207 208 209]
Re-initialized the encoder parameters.
Fitting a model of length 402 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 78s - loss: 731.2477 - loglik: -7.2970e+02 - logprior: -1.5497e+00
Epoch 2/2
39/39 - 78s - loss: 718.0417 - loglik: -7.1767e+02 - logprior: -3.6953e-01
Fitted a model with MAP estimate = -715.3244
expansions: [(221, 1), (230, 1), (231, 3), (247, 1), (248, 1)]
discards: [206 207 217 244 253 265 300 346 347 348]
Re-initialized the encoder parameters.
Fitting a model of length 399 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 81s - loss: 722.9396 - loglik: -7.2156e+02 - logprior: -1.3797e+00
Epoch 2/2
39/39 - 78s - loss: 717.9296 - loglik: -7.1781e+02 - logprior: -1.1508e-01
Fitted a model with MAP estimate = -715.1833
expansions: [(231, 1)]
discards: [215 216 234 235 236 237 238 239 240 249 250 251 252 253 301]
Re-initialized the encoder parameters.
Fitting a model of length 385 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 72s - loss: 726.4264 - loglik: -7.2524e+02 - logprior: -1.1840e+00
Epoch 2/10
39/39 - 64s - loss: 719.8604 - loglik: -7.1994e+02 - logprior: 0.0840
Epoch 3/10
39/39 - 62s - loss: 717.4589 - loglik: -7.1756e+02 - logprior: 0.1007
Epoch 4/10
39/39 - 63s - loss: 715.3724 - loglik: -7.1558e+02 - logprior: 0.2087
Epoch 5/10
39/39 - 66s - loss: 715.1103 - loglik: -7.1548e+02 - logprior: 0.3654
Epoch 6/10
39/39 - 69s - loss: 714.6387 - loglik: -7.1508e+02 - logprior: 0.4463
Epoch 7/10
39/39 - 71s - loss: 714.6259 - loglik: -7.1526e+02 - logprior: 0.6343
Epoch 8/10
39/39 - 71s - loss: 714.3831 - loglik: -7.1510e+02 - logprior: 0.7171
Epoch 9/10
39/39 - 69s - loss: 714.4444 - loglik: -7.1528e+02 - logprior: 0.8389
Fitted a model with MAP estimate = -713.5179
Time for alignment: 1613.0974
Computed alignments with likelihoods: ['-712.1773', '-711.2752', '-714.6408', '-714.5046', '-713.5179']
Best model has likelihood: -711.2752  (prior= 0.7433 )
time for generating output: 0.4088
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00450.projection.fasta
SP score = 0.8071329319129227
Training of 5 independent models on file PF07679.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd32ce779a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fcb702270a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fcb80067df0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 252.8208 - loglik: -2.4976e+02 - logprior: -3.0636e+00
Epoch 2/10
19/19 - 1s - loss: 220.9984 - loglik: -2.1973e+02 - logprior: -1.2706e+00
Epoch 3/10
19/19 - 1s - loss: 207.8643 - loglik: -2.0639e+02 - logprior: -1.4748e+00
Epoch 4/10
19/19 - 1s - loss: 205.6806 - loglik: -2.0429e+02 - logprior: -1.3861e+00
Epoch 5/10
19/19 - 2s - loss: 205.0437 - loglik: -2.0365e+02 - logprior: -1.3938e+00
Epoch 6/10
19/19 - 2s - loss: 204.6380 - loglik: -2.0328e+02 - logprior: -1.3557e+00
Epoch 7/10
19/19 - 2s - loss: 204.7599 - loglik: -2.0341e+02 - logprior: -1.3457e+00
Fitted a model with MAP estimate = -204.4369
expansions: [(10, 3), (11, 1), (13, 2), (14, 1), (15, 1), (24, 1), (34, 1), (39, 2), (40, 3), (42, 2), (43, 1), (48, 1), (49, 2), (62, 1), (64, 2), (65, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 208.3531 - loglik: -2.0446e+02 - logprior: -3.8958e+00
Epoch 2/2
19/19 - 2s - loss: 200.6254 - loglik: -1.9861e+02 - logprior: -2.0117e+00
Fitted a model with MAP estimate = -199.3144
expansions: [(0, 2)]
discards: [ 0 10 49 52 67 86]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 200.9373 - loglik: -1.9807e+02 - logprior: -2.8695e+00
Epoch 2/2
19/19 - 2s - loss: 197.7576 - loglik: -1.9666e+02 - logprior: -1.1006e+00
Fitted a model with MAP estimate = -197.2301
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 202.0470 - loglik: -1.9837e+02 - logprior: -3.6814e+00
Epoch 2/10
19/19 - 2s - loss: 197.8741 - loglik: -1.9671e+02 - logprior: -1.1604e+00
Epoch 3/10
19/19 - 2s - loss: 197.3177 - loglik: -1.9637e+02 - logprior: -9.4379e-01
Epoch 4/10
19/19 - 2s - loss: 197.1077 - loglik: -1.9620e+02 - logprior: -9.0820e-01
Epoch 5/10
19/19 - 2s - loss: 196.7486 - loglik: -1.9587e+02 - logprior: -8.7748e-01
Epoch 6/10
19/19 - 2s - loss: 196.8743 - loglik: -1.9601e+02 - logprior: -8.6100e-01
Fitted a model with MAP estimate = -196.4395
Time for alignment: 55.8634
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 252.4886 - loglik: -2.4943e+02 - logprior: -3.0623e+00
Epoch 2/10
19/19 - 1s - loss: 222.6145 - loglik: -2.2140e+02 - logprior: -1.2158e+00
Epoch 3/10
19/19 - 1s - loss: 208.2290 - loglik: -2.0681e+02 - logprior: -1.4145e+00
Epoch 4/10
19/19 - 1s - loss: 205.1950 - loglik: -2.0383e+02 - logprior: -1.3625e+00
Epoch 5/10
19/19 - 2s - loss: 204.5886 - loglik: -2.0318e+02 - logprior: -1.4052e+00
Epoch 6/10
19/19 - 2s - loss: 204.0910 - loglik: -2.0272e+02 - logprior: -1.3709e+00
Epoch 7/10
19/19 - 1s - loss: 203.9396 - loglik: -2.0258e+02 - logprior: -1.3580e+00
Epoch 8/10
19/19 - 1s - loss: 203.9859 - loglik: -2.0264e+02 - logprior: -1.3498e+00
Fitted a model with MAP estimate = -203.7022
expansions: [(8, 1), (9, 2), (10, 3), (12, 1), (13, 1), (24, 1), (25, 1), (39, 1), (40, 1), (43, 2), (44, 1), (48, 1), (49, 2), (60, 1), (62, 1), (64, 2), (65, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 208.0020 - loglik: -2.0413e+02 - logprior: -3.8751e+00
Epoch 2/2
19/19 - 2s - loss: 200.5011 - loglik: -1.9851e+02 - logprior: -1.9925e+00
Fitted a model with MAP estimate = -199.2799
expansions: [(0, 2)]
discards: [ 0 13 65 84]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 201.0409 - loglik: -1.9817e+02 - logprior: -2.8716e+00
Epoch 2/2
19/19 - 2s - loss: 197.7851 - loglik: -1.9669e+02 - logprior: -1.0970e+00
Fitted a model with MAP estimate = -197.2106
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 202.0990 - loglik: -1.9842e+02 - logprior: -3.6779e+00
Epoch 2/10
19/19 - 2s - loss: 197.8968 - loglik: -1.9674e+02 - logprior: -1.1589e+00
Epoch 3/10
19/19 - 2s - loss: 197.2562 - loglik: -1.9632e+02 - logprior: -9.3950e-01
Epoch 4/10
19/19 - 2s - loss: 197.1462 - loglik: -1.9624e+02 - logprior: -9.0503e-01
Epoch 5/10
19/19 - 2s - loss: 196.7557 - loglik: -1.9587e+02 - logprior: -8.8081e-01
Epoch 6/10
19/19 - 2s - loss: 196.7499 - loglik: -1.9589e+02 - logprior: -8.6168e-01
Epoch 7/10
19/19 - 2s - loss: 196.4934 - loglik: -1.9565e+02 - logprior: -8.4732e-01
Epoch 8/10
19/19 - 2s - loss: 196.5895 - loglik: -1.9575e+02 - logprior: -8.3721e-01
Fitted a model with MAP estimate = -196.3580
Time for alignment: 57.9349
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 252.7588 - loglik: -2.4969e+02 - logprior: -3.0666e+00
Epoch 2/10
19/19 - 2s - loss: 221.7177 - loglik: -2.2044e+02 - logprior: -1.2757e+00
Epoch 3/10
19/19 - 1s - loss: 208.2809 - loglik: -2.0681e+02 - logprior: -1.4758e+00
Epoch 4/10
19/19 - 2s - loss: 205.1671 - loglik: -2.0377e+02 - logprior: -1.4020e+00
Epoch 5/10
19/19 - 1s - loss: 204.6856 - loglik: -2.0329e+02 - logprior: -1.3995e+00
Epoch 6/10
19/19 - 1s - loss: 204.1144 - loglik: -2.0274e+02 - logprior: -1.3697e+00
Epoch 7/10
19/19 - 2s - loss: 204.1039 - loglik: -2.0274e+02 - logprior: -1.3614e+00
Epoch 8/10
19/19 - 1s - loss: 204.1204 - loglik: -2.0276e+02 - logprior: -1.3590e+00
Fitted a model with MAP estimate = -203.8089
expansions: [(8, 1), (9, 2), (10, 3), (12, 1), (13, 1), (22, 1), (25, 1), (39, 1), (40, 1), (43, 2), (44, 1), (48, 1), (49, 2), (50, 1), (62, 1), (64, 2), (65, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 207.9580 - loglik: -2.0408e+02 - logprior: -3.8772e+00
Epoch 2/2
19/19 - 2s - loss: 200.6990 - loglik: -1.9871e+02 - logprior: -1.9912e+00
Fitted a model with MAP estimate = -199.2737
expansions: [(0, 2)]
discards: [ 0 13 65 84]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 200.8961 - loglik: -1.9802e+02 - logprior: -2.8722e+00
Epoch 2/2
19/19 - 2s - loss: 197.7791 - loglik: -1.9668e+02 - logprior: -1.0981e+00
Fitted a model with MAP estimate = -197.2030
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 202.1075 - loglik: -1.9841e+02 - logprior: -3.6995e+00
Epoch 2/10
19/19 - 2s - loss: 198.0584 - loglik: -1.9688e+02 - logprior: -1.1761e+00
Epoch 3/10
19/19 - 2s - loss: 197.2304 - loglik: -1.9628e+02 - logprior: -9.4604e-01
Epoch 4/10
19/19 - 2s - loss: 197.0840 - loglik: -1.9618e+02 - logprior: -9.0312e-01
Epoch 5/10
19/19 - 2s - loss: 196.7329 - loglik: -1.9584e+02 - logprior: -8.8912e-01
Epoch 6/10
19/19 - 2s - loss: 196.8106 - loglik: -1.9595e+02 - logprior: -8.6445e-01
Fitted a model with MAP estimate = -196.4735
Time for alignment: 55.4631
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 252.6824 - loglik: -2.4962e+02 - logprior: -3.0663e+00
Epoch 2/10
19/19 - 1s - loss: 222.3776 - loglik: -2.2114e+02 - logprior: -1.2408e+00
Epoch 3/10
19/19 - 2s - loss: 210.0737 - loglik: -2.0868e+02 - logprior: -1.3888e+00
Epoch 4/10
19/19 - 1s - loss: 207.0194 - loglik: -2.0571e+02 - logprior: -1.3058e+00
Epoch 5/10
19/19 - 1s - loss: 206.2694 - loglik: -2.0495e+02 - logprior: -1.3188e+00
Epoch 6/10
19/19 - 1s - loss: 206.1329 - loglik: -2.0485e+02 - logprior: -1.2878e+00
Epoch 7/10
19/19 - 1s - loss: 205.9036 - loglik: -2.0462e+02 - logprior: -1.2857e+00
Epoch 8/10
19/19 - 2s - loss: 205.8051 - loglik: -2.0453e+02 - logprior: -1.2740e+00
Epoch 9/10
19/19 - 1s - loss: 206.0318 - loglik: -2.0476e+02 - logprior: -1.2756e+00
Fitted a model with MAP estimate = -205.5786
expansions: [(8, 1), (9, 2), (10, 3), (12, 1), (13, 1), (14, 1), (34, 1), (39, 1), (40, 1), (43, 2), (44, 1), (47, 1), (48, 2), (49, 2), (60, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 207.7013 - loglik: -2.0384e+02 - logprior: -3.8659e+00
Epoch 2/2
19/19 - 2s - loss: 200.5340 - loglik: -1.9854e+02 - logprior: -1.9982e+00
Fitted a model with MAP estimate = -199.3420
expansions: [(0, 2)]
discards: [ 0 13 65]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 200.8313 - loglik: -1.9795e+02 - logprior: -2.8797e+00
Epoch 2/2
19/19 - 2s - loss: 197.8386 - loglik: -1.9674e+02 - logprior: -1.0952e+00
Fitted a model with MAP estimate = -197.2019
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 202.1039 - loglik: -1.9841e+02 - logprior: -3.6924e+00
Epoch 2/10
19/19 - 2s - loss: 197.9570 - loglik: -1.9678e+02 - logprior: -1.1748e+00
Epoch 3/10
19/19 - 2s - loss: 197.2147 - loglik: -1.9628e+02 - logprior: -9.3897e-01
Epoch 4/10
19/19 - 2s - loss: 197.0941 - loglik: -1.9619e+02 - logprior: -9.0643e-01
Epoch 5/10
19/19 - 2s - loss: 196.7518 - loglik: -1.9586e+02 - logprior: -8.8852e-01
Epoch 6/10
19/19 - 2s - loss: 196.7878 - loglik: -1.9592e+02 - logprior: -8.6903e-01
Fitted a model with MAP estimate = -196.4620
Time for alignment: 55.3159
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 252.6415 - loglik: -2.4958e+02 - logprior: -3.0638e+00
Epoch 2/10
19/19 - 2s - loss: 219.3619 - loglik: -2.1808e+02 - logprior: -1.2823e+00
Epoch 3/10
19/19 - 1s - loss: 206.6081 - loglik: -2.0512e+02 - logprior: -1.4903e+00
Epoch 4/10
19/19 - 1s - loss: 203.9385 - loglik: -2.0252e+02 - logprior: -1.4146e+00
Epoch 5/10
19/19 - 1s - loss: 203.4819 - loglik: -2.0206e+02 - logprior: -1.4250e+00
Epoch 6/10
19/19 - 1s - loss: 203.2461 - loglik: -2.0186e+02 - logprior: -1.3854e+00
Epoch 7/10
19/19 - 1s - loss: 203.1260 - loglik: -2.0175e+02 - logprior: -1.3735e+00
Epoch 8/10
19/19 - 1s - loss: 203.0450 - loglik: -2.0168e+02 - logprior: -1.3677e+00
Epoch 9/10
19/19 - 1s - loss: 203.0549 - loglik: -2.0170e+02 - logprior: -1.3566e+00
Fitted a model with MAP estimate = -202.7967
expansions: [(8, 1), (9, 3), (10, 1), (13, 1), (14, 1), (15, 1), (18, 1), (34, 1), (39, 1), (40, 1), (43, 2), (44, 1), (48, 1), (49, 2), (60, 1), (62, 1), (64, 2), (65, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 207.5761 - loglik: -2.0370e+02 - logprior: -3.8731e+00
Epoch 2/2
19/19 - 2s - loss: 200.6164 - loglik: -1.9862e+02 - logprior: -1.9946e+00
Fitted a model with MAP estimate = -199.3443
expansions: [(0, 2)]
discards: [ 0 10 65 84]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 200.8676 - loglik: -1.9799e+02 - logprior: -2.8733e+00
Epoch 2/2
19/19 - 2s - loss: 197.8731 - loglik: -1.9678e+02 - logprior: -1.0973e+00
Fitted a model with MAP estimate = -197.2407
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 202.0959 - loglik: -1.9841e+02 - logprior: -3.6855e+00
Epoch 2/10
19/19 - 2s - loss: 198.0177 - loglik: -1.9685e+02 - logprior: -1.1712e+00
Epoch 3/10
19/19 - 2s - loss: 197.4300 - loglik: -1.9649e+02 - logprior: -9.3502e-01
Epoch 4/10
19/19 - 2s - loss: 196.9291 - loglik: -1.9603e+02 - logprior: -8.9884e-01
Epoch 5/10
19/19 - 2s - loss: 196.8676 - loglik: -1.9598e+02 - logprior: -8.8884e-01
Epoch 6/10
19/19 - 2s - loss: 196.5970 - loglik: -1.9573e+02 - logprior: -8.6297e-01
Epoch 7/10
19/19 - 2s - loss: 196.5817 - loglik: -1.9574e+02 - logprior: -8.4519e-01
Epoch 8/10
19/19 - 2s - loss: 196.5486 - loglik: -1.9571e+02 - logprior: -8.3965e-01
Epoch 9/10
19/19 - 2s - loss: 196.4812 - loglik: -1.9566e+02 - logprior: -8.2074e-01
Epoch 10/10
19/19 - 2s - loss: 196.4451 - loglik: -1.9564e+02 - logprior: -8.0787e-01
Fitted a model with MAP estimate = -196.2569
Time for alignment: 62.5868
Computed alignments with likelihoods: ['-196.4395', '-196.3580', '-196.4735', '-196.4620', '-196.2569']
Best model has likelihood: -196.2569  (prior= -0.8217 )
time for generating output: 0.1254
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07679.projection.fasta
SP score = 0.7879291251384275
Training of 5 independent models on file PF07686.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1264c5a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1641e03a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd32f339be0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 301.8804 - loglik: -2.9885e+02 - logprior: -3.0260e+00
Epoch 2/10
19/19 - 2s - loss: 270.6736 - loglik: -2.6947e+02 - logprior: -1.1986e+00
Epoch 3/10
19/19 - 2s - loss: 259.6659 - loglik: -2.5856e+02 - logprior: -1.1035e+00
Epoch 4/10
19/19 - 2s - loss: 257.3401 - loglik: -2.5632e+02 - logprior: -1.0167e+00
Epoch 5/10
19/19 - 2s - loss: 256.1605 - loglik: -2.5521e+02 - logprior: -9.5412e-01
Epoch 6/10
19/19 - 2s - loss: 255.7256 - loglik: -2.5478e+02 - logprior: -9.5005e-01
Epoch 7/10
19/19 - 2s - loss: 255.7490 - loglik: -2.5481e+02 - logprior: -9.4013e-01
Fitted a model with MAP estimate = -254.8431
expansions: [(0, 2), (20, 1), (21, 3), (22, 2), (23, 1), (24, 1), (28, 1), (37, 1), (41, 3), (42, 1), (51, 2), (52, 2), (53, 1), (75, 1), (76, 2), (77, 3), (78, 2), (80, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 114 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 259.0038 - loglik: -2.5496e+02 - logprior: -4.0394e+00
Epoch 2/2
19/19 - 3s - loss: 253.4916 - loglik: -2.5230e+02 - logprior: -1.1883e+00
Fitted a model with MAP estimate = -251.8174
expansions: [(54, 1)]
discards: [ 1 22 28 71 98]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 255.6230 - loglik: -2.5273e+02 - logprior: -2.8936e+00
Epoch 2/2
19/19 - 2s - loss: 252.9195 - loglik: -2.5191e+02 - logprior: -1.0054e+00
Fitted a model with MAP estimate = -251.5820
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 255.0165 - loglik: -2.5204e+02 - logprior: -2.9718e+00
Epoch 2/10
19/19 - 3s - loss: 252.2418 - loglik: -2.5126e+02 - logprior: -9.8453e-01
Epoch 3/10
19/19 - 2s - loss: 251.5464 - loglik: -2.5074e+02 - logprior: -8.0852e-01
Epoch 4/10
19/19 - 2s - loss: 250.9559 - loglik: -2.5021e+02 - logprior: -7.4226e-01
Epoch 5/10
19/19 - 2s - loss: 250.5418 - loglik: -2.4983e+02 - logprior: -7.0991e-01
Epoch 6/10
19/19 - 3s - loss: 250.2440 - loglik: -2.4955e+02 - logprior: -6.9845e-01
Epoch 7/10
19/19 - 3s - loss: 250.4868 - loglik: -2.4982e+02 - logprior: -6.6457e-01
Fitted a model with MAP estimate = -249.9658
Time for alignment: 71.1587
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 301.8332 - loglik: -2.9881e+02 - logprior: -3.0279e+00
Epoch 2/10
19/19 - 2s - loss: 271.6360 - loglik: -2.7043e+02 - logprior: -1.2101e+00
Epoch 3/10
19/19 - 2s - loss: 259.6923 - loglik: -2.5857e+02 - logprior: -1.1181e+00
Epoch 4/10
19/19 - 2s - loss: 257.5631 - loglik: -2.5656e+02 - logprior: -1.0080e+00
Epoch 5/10
19/19 - 2s - loss: 256.4335 - loglik: -2.5546e+02 - logprior: -9.7254e-01
Epoch 6/10
19/19 - 2s - loss: 256.2061 - loglik: -2.5525e+02 - logprior: -9.5491e-01
Epoch 7/10
19/19 - 2s - loss: 255.8765 - loglik: -2.5492e+02 - logprior: -9.5452e-01
Epoch 8/10
19/19 - 2s - loss: 255.6593 - loglik: -2.5470e+02 - logprior: -9.5970e-01
Epoch 9/10
19/19 - 2s - loss: 255.6629 - loglik: -2.5472e+02 - logprior: -9.4410e-01
Fitted a model with MAP estimate = -254.9144
expansions: [(0, 2), (21, 3), (22, 2), (23, 1), (24, 1), (38, 1), (39, 2), (41, 1), (42, 2), (44, 1), (45, 1), (51, 1), (52, 2), (53, 1), (73, 1), (74, 1), (75, 2), (76, 3), (77, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 114 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 259.5657 - loglik: -2.5546e+02 - logprior: -4.1034e+00
Epoch 2/2
19/19 - 3s - loss: 253.9382 - loglik: -2.5276e+02 - logprior: -1.1785e+00
Fitted a model with MAP estimate = -252.2190
expansions: [(54, 1)]
discards: [ 1 27 71]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 255.9152 - loglik: -2.5306e+02 - logprior: -2.8551e+00
Epoch 2/2
19/19 - 3s - loss: 253.1070 - loglik: -2.5209e+02 - logprior: -1.0169e+00
Fitted a model with MAP estimate = -251.9125
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 255.9664 - loglik: -2.5258e+02 - logprior: -3.3891e+00
Epoch 2/10
19/19 - 3s - loss: 252.5077 - loglik: -2.5147e+02 - logprior: -1.0373e+00
Epoch 3/10
19/19 - 3s - loss: 251.6853 - loglik: -2.5087e+02 - logprior: -8.1503e-01
Epoch 4/10
19/19 - 3s - loss: 251.1178 - loglik: -2.5035e+02 - logprior: -7.6436e-01
Epoch 5/10
19/19 - 3s - loss: 251.1574 - loglik: -2.5043e+02 - logprior: -7.2626e-01
Fitted a model with MAP estimate = -250.6095
Time for alignment: 71.7868
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 301.7981 - loglik: -2.9877e+02 - logprior: -3.0291e+00
Epoch 2/10
19/19 - 2s - loss: 274.0762 - loglik: -2.7290e+02 - logprior: -1.1782e+00
Epoch 3/10
19/19 - 2s - loss: 261.5857 - loglik: -2.6051e+02 - logprior: -1.0792e+00
Epoch 4/10
19/19 - 2s - loss: 257.3781 - loglik: -2.5640e+02 - logprior: -9.8192e-01
Epoch 5/10
19/19 - 2s - loss: 256.7219 - loglik: -2.5575e+02 - logprior: -9.7047e-01
Epoch 6/10
19/19 - 2s - loss: 256.1160 - loglik: -2.5516e+02 - logprior: -9.5484e-01
Epoch 7/10
19/19 - 2s - loss: 256.2909 - loglik: -2.5535e+02 - logprior: -9.4273e-01
Fitted a model with MAP estimate = -255.2613
expansions: [(0, 2), (17, 1), (20, 3), (21, 2), (22, 1), (39, 2), (41, 1), (42, 2), (44, 2), (45, 1), (51, 1), (52, 1), (74, 1), (75, 3), (76, 3), (77, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 259.5068 - loglik: -2.5544e+02 - logprior: -4.0619e+00
Epoch 2/2
19/19 - 3s - loss: 254.1713 - loglik: -2.5297e+02 - logprior: -1.2047e+00
Fitted a model with MAP estimate = -252.4651
expansions: [(47, 1)]
discards: [ 1 49 58 60]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 256.3546 - loglik: -2.5347e+02 - logprior: -2.8849e+00
Epoch 2/2
19/19 - 2s - loss: 253.3826 - loglik: -2.5234e+02 - logprior: -1.0406e+00
Fitted a model with MAP estimate = -252.0771
expansions: [(52, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 254.7951 - loglik: -2.5197e+02 - logprior: -2.8276e+00
Epoch 2/10
19/19 - 2s - loss: 252.5431 - loglik: -2.5152e+02 - logprior: -1.0212e+00
Epoch 3/10
19/19 - 2s - loss: 251.9229 - loglik: -2.5106e+02 - logprior: -8.6058e-01
Epoch 4/10
19/19 - 3s - loss: 251.3095 - loglik: -2.5050e+02 - logprior: -8.1387e-01
Epoch 5/10
19/19 - 3s - loss: 250.9624 - loglik: -2.5019e+02 - logprior: -7.7715e-01
Epoch 6/10
19/19 - 3s - loss: 250.7902 - loglik: -2.5006e+02 - logprior: -7.2753e-01
Epoch 7/10
19/19 - 3s - loss: 250.1600 - loglik: -2.4947e+02 - logprior: -6.9470e-01
Epoch 8/10
19/19 - 3s - loss: 250.3360 - loglik: -2.4965e+02 - logprior: -6.8727e-01
Fitted a model with MAP estimate = -250.1098
Time for alignment: 73.1346
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 301.9447 - loglik: -2.9891e+02 - logprior: -3.0337e+00
Epoch 2/10
19/19 - 2s - loss: 270.4906 - loglik: -2.6929e+02 - logprior: -1.2039e+00
Epoch 3/10
19/19 - 2s - loss: 259.7308 - loglik: -2.5860e+02 - logprior: -1.1263e+00
Epoch 4/10
19/19 - 2s - loss: 257.7020 - loglik: -2.5669e+02 - logprior: -1.0134e+00
Epoch 5/10
19/19 - 2s - loss: 256.8158 - loglik: -2.5585e+02 - logprior: -9.6540e-01
Epoch 6/10
19/19 - 2s - loss: 256.4967 - loglik: -2.5556e+02 - logprior: -9.3992e-01
Epoch 7/10
19/19 - 2s - loss: 256.3899 - loglik: -2.5545e+02 - logprior: -9.3713e-01
Epoch 8/10
19/19 - 2s - loss: 255.8438 - loglik: -2.5490e+02 - logprior: -9.4022e-01
Epoch 9/10
19/19 - 2s - loss: 255.5515 - loglik: -2.5462e+02 - logprior: -9.3429e-01
Epoch 10/10
19/19 - 2s - loss: 255.6024 - loglik: -2.5467e+02 - logprior: -9.3531e-01
Fitted a model with MAP estimate = -254.7304
expansions: [(0, 2), (21, 3), (22, 2), (23, 1), (24, 1), (39, 2), (41, 3), (42, 2), (51, 2), (52, 2), (53, 1), (73, 1), (74, 1), (75, 2), (76, 3), (77, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 114 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 259.3287 - loglik: -2.5520e+02 - logprior: -4.1311e+00
Epoch 2/2
19/19 - 3s - loss: 253.4940 - loglik: -2.5231e+02 - logprior: -1.1858e+00
Fitted a model with MAP estimate = -251.8107
expansions: []
discards: [ 1  2 27 71 98]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 255.8269 - loglik: -2.5292e+02 - logprior: -2.9026e+00
Epoch 2/2
19/19 - 3s - loss: 253.0411 - loglik: -2.5204e+02 - logprior: -1.0010e+00
Fitted a model with MAP estimate = -251.7481
expansions: [(0, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 256.3784 - loglik: -2.5189e+02 - logprior: -4.4917e+00
Epoch 2/10
19/19 - 2s - loss: 252.7449 - loglik: -2.5152e+02 - logprior: -1.2258e+00
Epoch 3/10
19/19 - 2s - loss: 251.2870 - loglik: -2.5043e+02 - logprior: -8.5732e-01
Epoch 4/10
19/19 - 3s - loss: 250.8707 - loglik: -2.5010e+02 - logprior: -7.6850e-01
Epoch 5/10
19/19 - 3s - loss: 250.6024 - loglik: -2.4987e+02 - logprior: -7.3320e-01
Epoch 6/10
19/19 - 3s - loss: 250.2720 - loglik: -2.4958e+02 - logprior: -6.9460e-01
Epoch 7/10
19/19 - 3s - loss: 250.2442 - loglik: -2.4956e+02 - logprior: -6.8430e-01
Epoch 8/10
19/19 - 3s - loss: 249.5064 - loglik: -2.4884e+02 - logprior: -6.6520e-01
Epoch 9/10
19/19 - 3s - loss: 249.9973 - loglik: -2.4933e+02 - logprior: -6.6372e-01
Fitted a model with MAP estimate = -249.6200
Time for alignment: 83.8447
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 301.7878 - loglik: -2.9876e+02 - logprior: -3.0273e+00
Epoch 2/10
19/19 - 2s - loss: 270.8396 - loglik: -2.6963e+02 - logprior: -1.2081e+00
Epoch 3/10
19/19 - 2s - loss: 259.6198 - loglik: -2.5851e+02 - logprior: -1.1070e+00
Epoch 4/10
19/19 - 2s - loss: 258.1563 - loglik: -2.5716e+02 - logprior: -9.9916e-01
Epoch 5/10
19/19 - 2s - loss: 257.4865 - loglik: -2.5654e+02 - logprior: -9.4749e-01
Epoch 6/10
19/19 - 2s - loss: 257.2167 - loglik: -2.5628e+02 - logprior: -9.3828e-01
Epoch 7/10
19/19 - 2s - loss: 256.3443 - loglik: -2.5540e+02 - logprior: -9.4223e-01
Epoch 8/10
19/19 - 2s - loss: 256.3503 - loglik: -2.5538e+02 - logprior: -9.6785e-01
Fitted a model with MAP estimate = -254.9826
expansions: [(0, 2), (21, 3), (22, 2), (23, 1), (24, 1), (32, 1), (37, 1), (39, 1), (42, 2), (44, 2), (45, 1), (52, 2), (53, 2), (54, 1), (74, 1), (75, 1), (76, 2), (77, 3), (78, 2), (80, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 116 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 259.2511 - loglik: -2.5516e+02 - logprior: -4.0895e+00
Epoch 2/2
19/19 - 3s - loss: 253.1540 - loglik: -2.5197e+02 - logprior: -1.1802e+00
Fitted a model with MAP estimate = -251.6581
expansions: []
discards: [  1  73 100]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 255.2749 - loglik: -2.5242e+02 - logprior: -2.8512e+00
Epoch 2/2
19/19 - 3s - loss: 252.8862 - loglik: -2.5186e+02 - logprior: -1.0264e+00
Fitted a model with MAP estimate = -251.6128
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 113 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 254.4897 - loglik: -2.5168e+02 - logprior: -2.8069e+00
Epoch 2/10
19/19 - 3s - loss: 252.1118 - loglik: -2.5112e+02 - logprior: -9.9641e-01
Epoch 3/10
19/19 - 3s - loss: 251.5972 - loglik: -2.5078e+02 - logprior: -8.1964e-01
Epoch 4/10
19/19 - 3s - loss: 250.8909 - loglik: -2.5011e+02 - logprior: -7.7773e-01
Epoch 5/10
19/19 - 3s - loss: 250.4102 - loglik: -2.4969e+02 - logprior: -7.2307e-01
Epoch 6/10
19/19 - 3s - loss: 250.2786 - loglik: -2.4959e+02 - logprior: -6.8632e-01
Epoch 7/10
19/19 - 3s - loss: 250.0971 - loglik: -2.4942e+02 - logprior: -6.7452e-01
Epoch 8/10
19/19 - 3s - loss: 249.9657 - loglik: -2.4930e+02 - logprior: -6.6133e-01
Epoch 9/10
19/19 - 3s - loss: 249.6040 - loglik: -2.4896e+02 - logprior: -6.4871e-01
Epoch 10/10
19/19 - 3s - loss: 250.3135 - loglik: -2.4967e+02 - logprior: -6.4024e-01
Fitted a model with MAP estimate = -249.6103
Time for alignment: 83.4886
Computed alignments with likelihoods: ['-249.9658', '-250.6095', '-250.1098', '-249.6200', '-249.6103']
Best model has likelihood: -249.6103  (prior= -0.6289 )
time for generating output: 0.1449
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07686.projection.fasta
SP score = 0.8817244752087879
Training of 5 independent models on file PF00505.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1c41d7c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd32e663430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd51924a5b0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 185.9307 - loglik: -1.8273e+02 - logprior: -3.2041e+00
Epoch 2/10
19/19 - 1s - loss: 151.3308 - loglik: -1.4972e+02 - logprior: -1.6098e+00
Epoch 3/10
19/19 - 1s - loss: 141.2019 - loglik: -1.3973e+02 - logprior: -1.4691e+00
Epoch 4/10
19/19 - 1s - loss: 138.4960 - loglik: -1.3700e+02 - logprior: -1.4965e+00
Epoch 5/10
19/19 - 1s - loss: 137.3848 - loglik: -1.3590e+02 - logprior: -1.4897e+00
Epoch 6/10
19/19 - 1s - loss: 137.1204 - loglik: -1.3564e+02 - logprior: -1.4832e+00
Epoch 7/10
19/19 - 1s - loss: 136.6958 - loglik: -1.3521e+02 - logprior: -1.4845e+00
Epoch 8/10
19/19 - 1s - loss: 136.0972 - loglik: -1.3463e+02 - logprior: -1.4714e+00
Epoch 9/10
19/19 - 1s - loss: 136.1838 - loglik: -1.3472e+02 - logprior: -1.4674e+00
Fitted a model with MAP estimate = -135.9180
expansions: [(16, 1), (17, 1), (18, 1), (22, 3), (23, 3), (33, 1), (36, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 137.0544 - loglik: -1.3373e+02 - logprior: -3.3233e+00
Epoch 2/2
19/19 - 1s - loss: 128.4697 - loglik: -1.2713e+02 - logprior: -1.3428e+00
Fitted a model with MAP estimate = -127.6284
expansions: []
discards: [27 29]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 131.3737 - loglik: -1.2819e+02 - logprior: -3.1852e+00
Epoch 2/2
19/19 - 1s - loss: 128.0984 - loglik: -1.2684e+02 - logprior: -1.2564e+00
Fitted a model with MAP estimate = -127.5104
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 130.7314 - loglik: -1.2758e+02 - logprior: -3.1553e+00
Epoch 2/10
19/19 - 1s - loss: 128.1954 - loglik: -1.2695e+02 - logprior: -1.2417e+00
Epoch 3/10
19/19 - 1s - loss: 127.2792 - loglik: -1.2618e+02 - logprior: -1.0957e+00
Epoch 4/10
19/19 - 1s - loss: 126.5612 - loglik: -1.2549e+02 - logprior: -1.0761e+00
Epoch 5/10
19/19 - 1s - loss: 125.7654 - loglik: -1.2470e+02 - logprior: -1.0652e+00
Epoch 6/10
19/19 - 1s - loss: 125.5922 - loglik: -1.2454e+02 - logprior: -1.0540e+00
Epoch 7/10
19/19 - 1s - loss: 125.4058 - loglik: -1.2436e+02 - logprior: -1.0446e+00
Epoch 8/10
19/19 - 1s - loss: 125.2145 - loglik: -1.2418e+02 - logprior: -1.0306e+00
Epoch 9/10
19/19 - 1s - loss: 125.4974 - loglik: -1.2447e+02 - logprior: -1.0267e+00
Fitted a model with MAP estimate = -125.0533
Time for alignment: 46.5988
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 186.3179 - loglik: -1.8312e+02 - logprior: -3.1984e+00
Epoch 2/10
19/19 - 1s - loss: 153.5897 - loglik: -1.5203e+02 - logprior: -1.5639e+00
Epoch 3/10
19/19 - 1s - loss: 141.5089 - loglik: -1.4001e+02 - logprior: -1.4990e+00
Epoch 4/10
19/19 - 1s - loss: 137.4642 - loglik: -1.3594e+02 - logprior: -1.5226e+00
Epoch 5/10
19/19 - 1s - loss: 135.9417 - loglik: -1.3442e+02 - logprior: -1.5257e+00
Epoch 6/10
19/19 - 1s - loss: 135.6141 - loglik: -1.3409e+02 - logprior: -1.5277e+00
Epoch 7/10
19/19 - 1s - loss: 135.0042 - loglik: -1.3349e+02 - logprior: -1.5171e+00
Epoch 8/10
19/19 - 1s - loss: 134.8544 - loglik: -1.3334e+02 - logprior: -1.5123e+00
Epoch 9/10
19/19 - 1s - loss: 134.7194 - loglik: -1.3321e+02 - logprior: -1.5083e+00
Epoch 10/10
19/19 - 1s - loss: 134.4863 - loglik: -1.3298e+02 - logprior: -1.5057e+00
Fitted a model with MAP estimate = -134.3332
expansions: [(14, 1), (16, 1), (17, 1), (22, 3), (23, 2), (30, 1), (33, 1), (35, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 136.6971 - loglik: -1.3338e+02 - logprior: -3.3184e+00
Epoch 2/2
19/19 - 1s - loss: 128.5902 - loglik: -1.2725e+02 - logprior: -1.3430e+00
Fitted a model with MAP estimate = -127.6340
expansions: []
discards: [26 29]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 131.1317 - loglik: -1.2795e+02 - logprior: -3.1863e+00
Epoch 2/2
19/19 - 1s - loss: 128.1993 - loglik: -1.2695e+02 - logprior: -1.2456e+00
Fitted a model with MAP estimate = -127.4923
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 130.8126 - loglik: -1.2765e+02 - logprior: -3.1597e+00
Epoch 2/10
19/19 - 1s - loss: 128.1589 - loglik: -1.2692e+02 - logprior: -1.2354e+00
Epoch 3/10
19/19 - 1s - loss: 127.3124 - loglik: -1.2622e+02 - logprior: -1.0959e+00
Epoch 4/10
19/19 - 1s - loss: 126.5743 - loglik: -1.2551e+02 - logprior: -1.0650e+00
Epoch 5/10
19/19 - 1s - loss: 125.8612 - loglik: -1.2480e+02 - logprior: -1.0617e+00
Epoch 6/10
19/19 - 1s - loss: 125.8391 - loglik: -1.2480e+02 - logprior: -1.0422e+00
Epoch 7/10
19/19 - 1s - loss: 125.4720 - loglik: -1.2443e+02 - logprior: -1.0439e+00
Epoch 8/10
19/19 - 1s - loss: 125.3412 - loglik: -1.2432e+02 - logprior: -1.0215e+00
Epoch 9/10
19/19 - 1s - loss: 125.1145 - loglik: -1.2410e+02 - logprior: -1.0121e+00
Epoch 10/10
19/19 - 1s - loss: 125.3583 - loglik: -1.2436e+02 - logprior: -9.9876e-01
Fitted a model with MAP estimate = -125.0216
Time for alignment: 49.6745
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 186.1030 - loglik: -1.8290e+02 - logprior: -3.2002e+00
Epoch 2/10
19/19 - 1s - loss: 154.3151 - loglik: -1.5287e+02 - logprior: -1.4411e+00
Epoch 3/10
19/19 - 1s - loss: 142.9799 - loglik: -1.4139e+02 - logprior: -1.5855e+00
Epoch 4/10
19/19 - 1s - loss: 139.2582 - loglik: -1.3778e+02 - logprior: -1.4746e+00
Epoch 5/10
19/19 - 1s - loss: 137.7321 - loglik: -1.3620e+02 - logprior: -1.5341e+00
Epoch 6/10
19/19 - 1s - loss: 137.3962 - loglik: -1.3588e+02 - logprior: -1.5165e+00
Epoch 7/10
19/19 - 1s - loss: 136.8566 - loglik: -1.3534e+02 - logprior: -1.5175e+00
Epoch 8/10
19/19 - 1s - loss: 136.5571 - loglik: -1.3505e+02 - logprior: -1.5091e+00
Epoch 9/10
19/19 - 1s - loss: 136.5954 - loglik: -1.3509e+02 - logprior: -1.5078e+00
Fitted a model with MAP estimate = -136.2819
expansions: [(12, 1), (16, 1), (17, 1), (18, 1), (22, 3), (23, 2), (29, 1), (32, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 140.6629 - loglik: -1.3659e+02 - logprior: -4.0733e+00
Epoch 2/2
19/19 - 1s - loss: 131.3053 - loglik: -1.2934e+02 - logprior: -1.9648e+00
Fitted a model with MAP estimate = -129.6087
expansions: [(0, 2)]
discards: [ 0 27]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 131.1044 - loglik: -1.2810e+02 - logprior: -3.0018e+00
Epoch 2/2
19/19 - 1s - loss: 127.9826 - loglik: -1.2680e+02 - logprior: -1.1842e+00
Fitted a model with MAP estimate = -127.2037
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.1633 - loglik: -1.2938e+02 - logprior: -3.7868e+00
Epoch 2/10
19/19 - 1s - loss: 128.2881 - loglik: -1.2699e+02 - logprior: -1.2986e+00
Epoch 3/10
19/19 - 1s - loss: 127.5549 - loglik: -1.2648e+02 - logprior: -1.0792e+00
Epoch 4/10
19/19 - 1s - loss: 126.6265 - loglik: -1.2557e+02 - logprior: -1.0570e+00
Epoch 5/10
19/19 - 1s - loss: 126.1190 - loglik: -1.2506e+02 - logprior: -1.0635e+00
Epoch 6/10
19/19 - 1s - loss: 125.5286 - loglik: -1.2447e+02 - logprior: -1.0543e+00
Epoch 7/10
19/19 - 1s - loss: 125.3306 - loglik: -1.2429e+02 - logprior: -1.0393e+00
Epoch 8/10
19/19 - 1s - loss: 125.4283 - loglik: -1.2440e+02 - logprior: -1.0241e+00
Fitted a model with MAP estimate = -125.1470
Time for alignment: 45.3100
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 186.1526 - loglik: -1.8295e+02 - logprior: -3.1996e+00
Epoch 2/10
19/19 - 1s - loss: 154.2935 - loglik: -1.5285e+02 - logprior: -1.4442e+00
Epoch 3/10
19/19 - 1s - loss: 143.1981 - loglik: -1.4159e+02 - logprior: -1.6116e+00
Epoch 4/10
19/19 - 1s - loss: 139.9697 - loglik: -1.3846e+02 - logprior: -1.5052e+00
Epoch 5/10
19/19 - 1s - loss: 138.3734 - loglik: -1.3680e+02 - logprior: -1.5702e+00
Epoch 6/10
19/19 - 1s - loss: 137.5926 - loglik: -1.3604e+02 - logprior: -1.5560e+00
Epoch 7/10
19/19 - 1s - loss: 137.2606 - loglik: -1.3571e+02 - logprior: -1.5492e+00
Epoch 8/10
19/19 - 1s - loss: 136.8598 - loglik: -1.3532e+02 - logprior: -1.5362e+00
Epoch 9/10
19/19 - 1s - loss: 136.9786 - loglik: -1.3545e+02 - logprior: -1.5325e+00
Fitted a model with MAP estimate = -136.6356
expansions: [(12, 1), (16, 1), (17, 1), (18, 1), (22, 3), (23, 2), (24, 1), (29, 1), (35, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 140.8462 - loglik: -1.3677e+02 - logprior: -4.0763e+00
Epoch 2/2
19/19 - 1s - loss: 131.1314 - loglik: -1.2914e+02 - logprior: -1.9866e+00
Fitted a model with MAP estimate = -129.5870
expansions: [(0, 2)]
discards: [ 0 26 29]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 131.0948 - loglik: -1.2810e+02 - logprior: -2.9966e+00
Epoch 2/2
19/19 - 1s - loss: 127.8791 - loglik: -1.2669e+02 - logprior: -1.1892e+00
Fitted a model with MAP estimate = -127.2193
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.1997 - loglik: -1.2941e+02 - logprior: -3.7901e+00
Epoch 2/10
19/19 - 1s - loss: 128.5273 - loglik: -1.2723e+02 - logprior: -1.2934e+00
Epoch 3/10
19/19 - 1s - loss: 127.4466 - loglik: -1.2637e+02 - logprior: -1.0750e+00
Epoch 4/10
19/19 - 1s - loss: 126.4227 - loglik: -1.2536e+02 - logprior: -1.0654e+00
Epoch 5/10
19/19 - 1s - loss: 126.0376 - loglik: -1.2498e+02 - logprior: -1.0578e+00
Epoch 6/10
19/19 - 1s - loss: 125.6264 - loglik: -1.2457e+02 - logprior: -1.0530e+00
Epoch 7/10
19/19 - 1s - loss: 125.4972 - loglik: -1.2446e+02 - logprior: -1.0396e+00
Epoch 8/10
19/19 - 1s - loss: 125.2514 - loglik: -1.2422e+02 - logprior: -1.0275e+00
Epoch 9/10
19/19 - 1s - loss: 125.1128 - loglik: -1.2409e+02 - logprior: -1.0210e+00
Epoch 10/10
19/19 - 1s - loss: 125.0919 - loglik: -1.2408e+02 - logprior: -1.0143e+00
Fitted a model with MAP estimate = -125.0327
Time for alignment: 48.1563
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 186.3523 - loglik: -1.8315e+02 - logprior: -3.2007e+00
Epoch 2/10
19/19 - 1s - loss: 154.0414 - loglik: -1.5259e+02 - logprior: -1.4483e+00
Epoch 3/10
19/19 - 1s - loss: 141.4920 - loglik: -1.3985e+02 - logprior: -1.6421e+00
Epoch 4/10
19/19 - 1s - loss: 137.7367 - loglik: -1.3618e+02 - logprior: -1.5529e+00
Epoch 5/10
19/19 - 1s - loss: 136.7502 - loglik: -1.3516e+02 - logprior: -1.5944e+00
Epoch 6/10
19/19 - 1s - loss: 136.2013 - loglik: -1.3462e+02 - logprior: -1.5789e+00
Epoch 7/10
19/19 - 1s - loss: 135.7994 - loglik: -1.3422e+02 - logprior: -1.5753e+00
Epoch 8/10
19/19 - 1s - loss: 135.5876 - loglik: -1.3402e+02 - logprior: -1.5633e+00
Epoch 9/10
19/19 - 1s - loss: 135.6559 - loglik: -1.3409e+02 - logprior: -1.5632e+00
Fitted a model with MAP estimate = -135.1938
expansions: [(12, 1), (14, 1), (16, 1), (17, 1), (18, 1), (21, 2), (23, 1), (24, 1), (33, 1), (35, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 140.2129 - loglik: -1.3614e+02 - logprior: -4.0705e+00
Epoch 2/2
19/19 - 1s - loss: 131.1439 - loglik: -1.2919e+02 - logprior: -1.9515e+00
Fitted a model with MAP estimate = -129.5927
expansions: [(0, 2)]
discards: [ 0 25]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 131.0126 - loglik: -1.2802e+02 - logprior: -2.9969e+00
Epoch 2/2
19/19 - 1s - loss: 127.8042 - loglik: -1.2662e+02 - logprior: -1.1830e+00
Fitted a model with MAP estimate = -127.2016
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 133.1024 - loglik: -1.2932e+02 - logprior: -3.7863e+00
Epoch 2/10
19/19 - 1s - loss: 128.4512 - loglik: -1.2716e+02 - logprior: -1.2877e+00
Epoch 3/10
19/19 - 1s - loss: 127.4059 - loglik: -1.2633e+02 - logprior: -1.0749e+00
Epoch 4/10
19/19 - 1s - loss: 126.6290 - loglik: -1.2556e+02 - logprior: -1.0679e+00
Epoch 5/10
19/19 - 1s - loss: 126.1440 - loglik: -1.2509e+02 - logprior: -1.0509e+00
Epoch 6/10
19/19 - 1s - loss: 125.5948 - loglik: -1.2454e+02 - logprior: -1.0518e+00
Epoch 7/10
19/19 - 1s - loss: 125.3951 - loglik: -1.2436e+02 - logprior: -1.0374e+00
Epoch 8/10
19/19 - 1s - loss: 125.1494 - loglik: -1.2412e+02 - logprior: -1.0293e+00
Epoch 9/10
19/19 - 1s - loss: 124.9040 - loglik: -1.2389e+02 - logprior: -1.0147e+00
Epoch 10/10
19/19 - 1s - loss: 125.0366 - loglik: -1.2402e+02 - logprior: -1.0151e+00
Fitted a model with MAP estimate = -124.9093
Time for alignment: 48.7613
Computed alignments with likelihoods: ['-125.0533', '-125.0216', '-125.1470', '-125.0327', '-124.9093']
Best model has likelihood: -124.9093  (prior= -0.9739 )
time for generating output: 0.1063
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00505.projection.fasta
SP score = 0.968432780391186
Training of 5 independent models on file PF13393.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd126c48910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd126c7e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fcb802d42e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 732.3878 - loglik: -7.3074e+02 - logprior: -1.6490e+00
Epoch 2/10
39/39 - 26s - loss: 620.1249 - loglik: -6.1856e+02 - logprior: -1.5608e+00
Epoch 3/10
39/39 - 27s - loss: 608.1960 - loglik: -6.0659e+02 - logprior: -1.6068e+00
Epoch 4/10
39/39 - 27s - loss: 605.0551 - loglik: -6.0345e+02 - logprior: -1.6021e+00
Epoch 5/10
39/39 - 27s - loss: 604.1542 - loglik: -6.0255e+02 - logprior: -1.6049e+00
Epoch 6/10
39/39 - 28s - loss: 603.4852 - loglik: -6.0186e+02 - logprior: -1.6226e+00
Epoch 7/10
39/39 - 28s - loss: 603.3738 - loglik: -6.0176e+02 - logprior: -1.6157e+00
Epoch 8/10
39/39 - 29s - loss: 603.4934 - loglik: -6.0187e+02 - logprior: -1.6223e+00
Fitted a model with MAP estimate = -601.9613
expansions: [(11, 1), (13, 3), (14, 2), (15, 1), (16, 1), (18, 1), (30, 1), (34, 1), (36, 1), (42, 1), (46, 1), (47, 1), (52, 1), (69, 1), (70, 2), (71, 1), (77, 1), (94, 1), (102, 2), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (131, 1), (132, 1), (135, 1), (137, 1), (140, 1), (141, 4), (153, 2), (164, 2), (173, 2), (177, 1), (180, 1), (185, 1), (186, 1), (193, 1), (194, 1), (197, 1), (211, 1), (212, 4), (213, 1), (226, 1), (227, 1), (230, 1), (231, 2)]
discards: [ 0 38 39]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 598.5645 - loglik: -5.9600e+02 - logprior: -2.5654e+00
Epoch 2/2
39/39 - 40s - loss: 581.3332 - loglik: -5.8002e+02 - logprior: -1.3140e+00
Fitted a model with MAP estimate = -575.3311
expansions: [(145, 2), (220, 1), (241, 1)]
discards: [122 216 289 290]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 46s - loss: 583.4151 - loglik: -5.8190e+02 - logprior: -1.5119e+00
Epoch 2/2
39/39 - 42s - loss: 578.0435 - loglik: -5.7774e+02 - logprior: -3.0816e-01
Fitted a model with MAP estimate = -574.3161
expansions: [(291, 1)]
discards: [145]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 44s - loss: 581.5292 - loglik: -5.8012e+02 - logprior: -1.4108e+00
Epoch 2/10
39/39 - 40s - loss: 576.5740 - loglik: -5.7645e+02 - logprior: -1.2287e-01
Epoch 3/10
39/39 - 40s - loss: 572.9921 - loglik: -5.7299e+02 - logprior: 0.0022
Epoch 4/10
39/39 - 40s - loss: 571.6994 - loglik: -5.7178e+02 - logprior: 0.0841
Epoch 5/10
39/39 - 35s - loss: 570.8617 - loglik: -5.7103e+02 - logprior: 0.1680
Epoch 6/10
39/39 - 31s - loss: 570.9485 - loglik: -5.7123e+02 - logprior: 0.2777
Fitted a model with MAP estimate = -570.0502
Time for alignment: 796.9653
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 732.3984 - loglik: -7.3075e+02 - logprior: -1.6483e+00
Epoch 2/10
39/39 - 21s - loss: 621.3052 - loglik: -6.1982e+02 - logprior: -1.4863e+00
Epoch 3/10
39/39 - 21s - loss: 606.8536 - loglik: -6.0528e+02 - logprior: -1.5693e+00
Epoch 4/10
39/39 - 21s - loss: 601.4769 - loglik: -5.9993e+02 - logprior: -1.5421e+00
Epoch 5/10
39/39 - 22s - loss: 600.3065 - loglik: -5.9876e+02 - logprior: -1.5480e+00
Epoch 6/10
39/39 - 22s - loss: 599.8701 - loglik: -5.9831e+02 - logprior: -1.5586e+00
Epoch 7/10
39/39 - 22s - loss: 599.7563 - loglik: -5.9819e+02 - logprior: -1.5614e+00
Epoch 8/10
39/39 - 22s - loss: 599.4266 - loglik: -5.9785e+02 - logprior: -1.5756e+00
Epoch 9/10
39/39 - 22s - loss: 598.9690 - loglik: -5.9739e+02 - logprior: -1.5813e+00
Epoch 10/10
39/39 - 22s - loss: 599.0626 - loglik: -5.9746e+02 - logprior: -1.6027e+00
Fitted a model with MAP estimate = -597.6803
expansions: [(11, 1), (14, 1), (15, 1), (16, 1), (17, 3), (33, 1), (34, 1), (35, 2), (42, 1), (46, 1), (47, 1), (52, 1), (53, 1), (69, 1), (70, 2), (71, 1), (77, 1), (82, 1), (102, 1), (115, 1), (116, 1), (117, 1), (121, 5), (132, 2), (134, 1), (137, 2), (146, 1), (147, 1), (154, 3), (164, 2), (173, 2), (177, 1), (180, 1), (185, 1), (186, 1), (193, 1), (194, 1), (197, 1), (211, 1), (212, 3), (214, 2), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 304 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 592.9952 - loglik: -5.9051e+02 - logprior: -2.4889e+00
Epoch 2/2
39/39 - 32s - loss: 576.8457 - loglik: -5.7542e+02 - logprior: -1.4291e+00
Fitted a model with MAP estimate = -572.3713
expansions: [(0, 3), (147, 1), (148, 1), (175, 1), (203, 1), (205, 1), (241, 1)]
discards: [  0  43 162 170 180 181 182 183 216]
Re-initialized the encoder parameters.
Fitting a model of length 304 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 580.2037 - loglik: -5.7871e+02 - logprior: -1.4975e+00
Epoch 2/2
39/39 - 37s - loss: 573.5912 - loglik: -5.7312e+02 - logprior: -4.6927e-01
Fitted a model with MAP estimate = -570.1717
expansions: [(182, 3), (194, 4)]
discards: [  0   1 177 189 197 198]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 40s - loss: 580.8547 - loglik: -5.7865e+02 - logprior: -2.2054e+00
Epoch 2/10
39/39 - 38s - loss: 572.3884 - loglik: -5.7198e+02 - logprior: -4.0767e-01
Epoch 3/10
39/39 - 37s - loss: 567.6790 - loglik: -5.6761e+02 - logprior: -6.7169e-02
Epoch 4/10
39/39 - 38s - loss: 566.6383 - loglik: -5.6665e+02 - logprior: 0.0080
Epoch 5/10
39/39 - 38s - loss: 565.6706 - loglik: -5.6577e+02 - logprior: 0.1008
Epoch 6/10
39/39 - 38s - loss: 564.9251 - loglik: -5.6512e+02 - logprior: 0.1976
Epoch 7/10
39/39 - 41s - loss: 565.2914 - loglik: -5.6560e+02 - logprior: 0.3073
Fitted a model with MAP estimate = -564.3733
Time for alignment: 791.1726
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 31s - loss: 731.1424 - loglik: -7.2949e+02 - logprior: -1.6492e+00
Epoch 2/10
39/39 - 28s - loss: 618.5487 - loglik: -6.1687e+02 - logprior: -1.6783e+00
Epoch 3/10
39/39 - 28s - loss: 607.2823 - loglik: -6.0549e+02 - logprior: -1.7875e+00
Epoch 4/10
39/39 - 29s - loss: 604.2147 - loglik: -6.0247e+02 - logprior: -1.7401e+00
Epoch 5/10
39/39 - 29s - loss: 603.1291 - loglik: -6.0139e+02 - logprior: -1.7397e+00
Epoch 6/10
39/39 - 29s - loss: 602.9222 - loglik: -6.0116e+02 - logprior: -1.7581e+00
Epoch 7/10
39/39 - 29s - loss: 602.0548 - loglik: -6.0029e+02 - logprior: -1.7605e+00
Epoch 8/10
39/39 - 28s - loss: 602.8712 - loglik: -6.0110e+02 - logprior: -1.7692e+00
Fitted a model with MAP estimate = -600.8726
expansions: [(10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (30, 1), (34, 1), (36, 1), (38, 1), (46, 1), (47, 1), (52, 1), (69, 1), (70, 2), (71, 1), (77, 1), (87, 1), (102, 2), (115, 1), (116, 1), (117, 1), (121, 5), (131, 1), (132, 1), (135, 2), (137, 1), (147, 1), (154, 7), (165, 2), (169, 1), (173, 2), (175, 1), (184, 1), (185, 1), (186, 1), (192, 2), (193, 1), (194, 1), (197, 1), (211, 1), (212, 4), (213, 3), (227, 1), (231, 2)]
discards: [ 0 43]
Re-initialized the encoder parameters.
Fitting a model of length 311 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 596.6418 - loglik: -5.9405e+02 - logprior: -2.5893e+00
Epoch 2/2
39/39 - 39s - loss: 579.3986 - loglik: -5.7807e+02 - logprior: -1.3298e+00
Fitted a model with MAP estimate = -573.5233
expansions: [(55, 1), (149, 1), (170, 3), (209, 1)]
discards: [ 52  53 123 167 182 191 194 195 196 198 200 201 202 203 245 275]
Re-initialized the encoder parameters.
Fitting a model of length 301 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 40s - loss: 587.4247 - loglik: -5.8591e+02 - logprior: -1.5131e+00
Epoch 2/2
39/39 - 40s - loss: 579.9297 - loglik: -5.7954e+02 - logprior: -3.9280e-01
Fitted a model with MAP estimate = -576.1777
expansions: [(169, 1), (190, 1), (191, 1), (192, 1), (193, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 310 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 580.7546 - loglik: -5.7936e+02 - logprior: -1.3965e+00
Epoch 2/10
39/39 - 43s - loss: 575.3408 - loglik: -5.7512e+02 - logprior: -2.1704e-01
Epoch 3/10
39/39 - 44s - loss: 572.0602 - loglik: -5.7195e+02 - logprior: -1.0547e-01
Epoch 4/10
39/39 - 42s - loss: 570.1324 - loglik: -5.7013e+02 - logprior: -4.7563e-03
Epoch 5/10
39/39 - 40s - loss: 570.2010 - loglik: -5.7031e+02 - logprior: 0.1061
Fitted a model with MAP estimate = -569.1063
Time for alignment: 790.4757
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 731.7567 - loglik: -7.3011e+02 - logprior: -1.6441e+00
Epoch 2/10
39/39 - 26s - loss: 617.6426 - loglik: -6.1601e+02 - logprior: -1.6287e+00
Epoch 3/10
39/39 - 26s - loss: 605.9349 - loglik: -6.0425e+02 - logprior: -1.6804e+00
Epoch 4/10
39/39 - 27s - loss: 602.0884 - loglik: -6.0044e+02 - logprior: -1.6532e+00
Epoch 5/10
39/39 - 26s - loss: 601.0107 - loglik: -5.9933e+02 - logprior: -1.6837e+00
Epoch 6/10
39/39 - 26s - loss: 600.6971 - loglik: -5.9900e+02 - logprior: -1.7018e+00
Epoch 7/10
39/39 - 27s - loss: 599.8082 - loglik: -5.9809e+02 - logprior: -1.7150e+00
Epoch 8/10
39/39 - 28s - loss: 599.8481 - loglik: -5.9813e+02 - logprior: -1.7218e+00
Fitted a model with MAP estimate = -598.7675
expansions: [(8, 1), (14, 1), (15, 3), (16, 3), (17, 2), (29, 1), (33, 1), (35, 1), (46, 1), (47, 1), (52, 1), (53, 1), (69, 1), (70, 2), (71, 1), (77, 1), (94, 1), (102, 2), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (131, 1), (132, 1), (135, 2), (137, 1), (140, 1), (147, 1), (148, 1), (154, 1), (165, 1), (168, 2), (169, 1), (175, 1), (180, 1), (184, 1), (186, 1), (189, 1), (193, 1), (194, 1), (197, 1), (212, 1), (213, 3), (214, 3), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [ 0 37 38]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 43s - loss: 593.0807 - loglik: -5.9053e+02 - logprior: -2.5525e+00
Epoch 2/2
39/39 - 42s - loss: 577.1476 - loglik: -5.7570e+02 - logprior: -1.4477e+00
Fitted a model with MAP estimate = -572.5113
expansions: [(0, 3), (149, 1), (241, 1)]
discards: [  0 123 167 182 195 196 197 198 199 200 201 202 203 269]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 40s - loss: 584.3995 - loglik: -5.8291e+02 - logprior: -1.4876e+00
Epoch 2/2
39/39 - 37s - loss: 576.8567 - loglik: -5.7637e+02 - logprior: -4.8524e-01
Fitted a model with MAP estimate = -573.3782
expansions: [(183, 1)]
discards: [  0   1 193 194 195]
Re-initialized the encoder parameters.
Fitting a model of length 292 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 584.8478 - loglik: -5.8272e+02 - logprior: -2.1296e+00
Epoch 2/10
39/39 - 36s - loss: 576.6102 - loglik: -5.7621e+02 - logprior: -4.0328e-01
Epoch 3/10
39/39 - 35s - loss: 573.3356 - loglik: -5.7319e+02 - logprior: -1.4221e-01
Epoch 4/10
39/39 - 36s - loss: 571.4468 - loglik: -5.7138e+02 - logprior: -6.6484e-02
Epoch 5/10
39/39 - 38s - loss: 570.8536 - loglik: -5.7089e+02 - logprior: 0.0407
Epoch 6/10
39/39 - 40s - loss: 570.7624 - loglik: -5.7089e+02 - logprior: 0.1294
Epoch 7/10
39/39 - 41s - loss: 569.7766 - loglik: -5.6998e+02 - logprior: 0.2008
Epoch 8/10
39/39 - 40s - loss: 570.8256 - loglik: -5.7117e+02 - logprior: 0.3430
Fitted a model with MAP estimate = -569.4904
Time for alignment: 864.3338
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 31s - loss: 731.3553 - loglik: -7.2971e+02 - logprior: -1.6470e+00
Epoch 2/10
39/39 - 29s - loss: 615.9866 - loglik: -6.1444e+02 - logprior: -1.5493e+00
Epoch 3/10
39/39 - 29s - loss: 604.5938 - loglik: -6.0304e+02 - logprior: -1.5573e+00
Epoch 4/10
39/39 - 28s - loss: 601.2847 - loglik: -5.9972e+02 - logprior: -1.5624e+00
Epoch 5/10
39/39 - 27s - loss: 600.0043 - loglik: -5.9841e+02 - logprior: -1.5917e+00
Epoch 6/10
39/39 - 27s - loss: 599.4476 - loglik: -5.9786e+02 - logprior: -1.5845e+00
Epoch 7/10
39/39 - 27s - loss: 598.7822 - loglik: -5.9720e+02 - logprior: -1.5842e+00
Epoch 8/10
39/39 - 26s - loss: 598.8994 - loglik: -5.9731e+02 - logprior: -1.5907e+00
Fitted a model with MAP estimate = -597.2435
expansions: [(10, 3), (11, 1), (13, 1), (14, 1), (15, 1), (16, 1), (18, 1), (31, 1), (35, 1), (41, 1), (45, 1), (46, 1), (51, 1), (64, 1), (68, 1), (69, 2), (70, 2), (76, 1), (86, 1), (101, 2), (114, 1), (115, 1), (116, 1), (119, 4), (120, 1), (131, 1), (134, 1), (137, 1), (147, 1), (151, 1), (154, 1), (156, 4), (165, 2), (166, 1), (173, 2), (175, 2), (184, 1), (185, 1), (186, 1), (192, 3), (193, 1), (196, 1), (211, 1), (212, 3), (213, 3), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 592.7845 - loglik: -5.9028e+02 - logprior: -2.5094e+00
Epoch 2/2
39/39 - 39s - loss: 575.6695 - loglik: -5.7439e+02 - logprior: -1.2800e+00
Fitted a model with MAP estimate = -569.5929
expansions: [(150, 1), (170, 3), (190, 1), (197, 1)]
discards: [ 84 124 219 274]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 577.1087 - loglik: -5.7556e+02 - logprior: -1.5533e+00
Epoch 2/2
39/39 - 43s - loss: 571.7424 - loglik: -5.7132e+02 - logprior: -4.2074e-01
Fitted a model with MAP estimate = -567.8660
expansions: [(170, 1)]
discards: [192]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 39s - loss: 574.8718 - loglik: -5.7347e+02 - logprior: -1.3985e+00
Epoch 2/10
39/39 - 35s - loss: 569.8273 - loglik: -5.6962e+02 - logprior: -2.1183e-01
Epoch 3/10
39/39 - 35s - loss: 566.8255 - loglik: -5.6669e+02 - logprior: -1.3673e-01
Epoch 4/10
39/39 - 36s - loss: 564.5471 - loglik: -5.6452e+02 - logprior: -2.6283e-02
Epoch 5/10
39/39 - 36s - loss: 564.4960 - loglik: -5.6456e+02 - logprior: 0.0620
Epoch 6/10
39/39 - 37s - loss: 563.8391 - loglik: -5.6400e+02 - logprior: 0.1635
Epoch 7/10
39/39 - 37s - loss: 563.4056 - loglik: -5.6367e+02 - logprior: 0.2681
Epoch 8/10
39/39 - 37s - loss: 563.3955 - loglik: -5.6376e+02 - logprior: 0.3622
Epoch 9/10
39/39 - 36s - loss: 563.0945 - loglik: -5.6357e+02 - logprior: 0.4718
Epoch 10/10
39/39 - 35s - loss: 562.0680 - loglik: -5.6263e+02 - logprior: 0.5601
Fitted a model with MAP estimate = -562.3377
Time for alignment: 940.4856
Computed alignments with likelihoods: ['-570.0502', '-564.3733', '-569.1063', '-569.4904', '-562.3377']
Best model has likelihood: -562.3377  (prior= 0.6491 )
time for generating output: 0.3148
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13393.projection.fasta
SP score = 0.06209215607937142
Training of 5 independent models on file PF00202.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd50896ee20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd340a845b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd384a7d5e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 972.4891 - loglik: -9.7117e+02 - logprior: -1.3216e+00
Epoch 2/10
39/39 - 48s - loss: 812.1791 - loglik: -8.1117e+02 - logprior: -1.0101e+00
Epoch 3/10
39/39 - 50s - loss: 800.1187 - loglik: -7.9909e+02 - logprior: -1.0334e+00
Epoch 4/10
39/39 - 49s - loss: 797.2626 - loglik: -7.9624e+02 - logprior: -1.0224e+00
Epoch 5/10
39/39 - 47s - loss: 796.2484 - loglik: -7.9515e+02 - logprior: -1.1014e+00
Epoch 6/10
39/39 - 46s - loss: 796.2069 - loglik: -7.9516e+02 - logprior: -1.0503e+00
Epoch 7/10
39/39 - 45s - loss: 795.6273 - loglik: -7.9454e+02 - logprior: -1.0843e+00
Epoch 8/10
39/39 - 47s - loss: 795.3292 - loglik: -7.9425e+02 - logprior: -1.0770e+00
Epoch 9/10
39/39 - 48s - loss: 795.3819 - loglik: -7.9426e+02 - logprior: -1.1174e+00
Fitted a model with MAP estimate = -786.4589
expansions: [(0, 3), (5, 2), (44, 1), (52, 1), (53, 1), (57, 1), (62, 1), (69, 1), (70, 1), (73, 2), (74, 1), (77, 1), (121, 1), (123, 1), (132, 1), (146, 3), (147, 1), (148, 1), (164, 1), (171, 1), (173, 1), (175, 1), (176, 1), (193, 2), (196, 1), (197, 3), (219, 1), (220, 1), (221, 1), (223, 2), (224, 2), (247, 1), (249, 1), (253, 1), (254, 1), (255, 1), (256, 1), (257, 1), (260, 1), (262, 3), (263, 2), (266, 2), (285, 2), (286, 2), (287, 8), (296, 1), (297, 1), (299, 1), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 397 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 68s - loss: 778.7651 - loglik: -7.7653e+02 - logprior: -2.2327e+00
Epoch 2/2
39/39 - 68s - loss: 762.2221 - loglik: -7.6164e+02 - logprior: -5.8464e-01
Fitted a model with MAP estimate = -751.9237
expansions: [(0, 2), (347, 1), (369, 2)]
discards: [  1   2   8  85 169 222 230 265 313 321 338 341 342 343 390]
Re-initialized the encoder parameters.
Fitting a model of length 387 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 69s - loss: 767.3568 - loglik: -7.6536e+02 - logprior: -1.9970e+00
Epoch 2/2
39/39 - 64s - loss: 762.1536 - loglik: -7.6207e+02 - logprior: -8.1259e-02
Fitted a model with MAP estimate = -752.8209
expansions: [(0, 2), (259, 1), (330, 1)]
discards: [1 2 5]
Re-initialized the encoder parameters.
Fitting a model of length 388 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 79s - loss: 759.2607 - loglik: -7.5710e+02 - logprior: -2.1568e+00
Epoch 2/10
39/39 - 83s - loss: 753.4307 - loglik: -7.5347e+02 - logprior: 0.0374
Epoch 3/10
39/39 - 85s - loss: 753.0388 - loglik: -7.5327e+02 - logprior: 0.2352
Epoch 4/10
39/39 - 70s - loss: 751.3776 - loglik: -7.5175e+02 - logprior: 0.3715
Epoch 5/10
39/39 - 70s - loss: 751.5760 - loglik: -7.5205e+02 - logprior: 0.4701
Fitted a model with MAP estimate = -750.2503
Time for alignment: 1436.1642
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 59s - loss: 971.8079 - loglik: -9.7048e+02 - logprior: -1.3312e+00
Epoch 2/10
39/39 - 54s - loss: 809.1959 - loglik: -8.0816e+02 - logprior: -1.0392e+00
Epoch 3/10
39/39 - 49s - loss: 796.9088 - loglik: -7.9581e+02 - logprior: -1.1001e+00
Epoch 4/10
39/39 - 47s - loss: 793.8870 - loglik: -7.9276e+02 - logprior: -1.1227e+00
Epoch 5/10
39/39 - 50s - loss: 793.5832 - loglik: -7.9242e+02 - logprior: -1.1597e+00
Epoch 6/10
39/39 - 56s - loss: 793.0731 - loglik: -7.9193e+02 - logprior: -1.1442e+00
Epoch 7/10
39/39 - 57s - loss: 792.6100 - loglik: -7.9145e+02 - logprior: -1.1606e+00
Epoch 8/10
39/39 - 50s - loss: 792.5592 - loglik: -7.9141e+02 - logprior: -1.1451e+00
Epoch 9/10
39/39 - 47s - loss: 792.1888 - loglik: -7.9097e+02 - logprior: -1.2232e+00
Epoch 10/10
39/39 - 48s - loss: 791.9934 - loglik: -7.9082e+02 - logprior: -1.1708e+00
Fitted a model with MAP estimate = -783.6198
expansions: [(0, 3), (5, 1), (34, 1), (51, 1), (53, 1), (57, 1), (63, 1), (69, 1), (70, 1), (73, 2), (99, 1), (104, 1), (119, 1), (120, 2), (122, 1), (143, 1), (145, 3), (146, 1), (166, 1), (169, 1), (170, 1), (176, 1), (188, 1), (192, 2), (195, 1), (196, 1), (199, 1), (219, 1), (220, 1), (221, 1), (223, 1), (224, 3), (247, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (258, 1), (259, 1), (261, 1), (262, 1), (263, 1), (265, 2), (284, 1), (285, 2), (286, 8), (287, 1), (288, 1), (296, 4), (309, 1), (313, 2), (314, 3)]
discards: [280]
Re-initialized the encoder parameters.
Fitting a model of length 396 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 92s - loss: 778.1729 - loglik: -7.7597e+02 - logprior: -2.2005e+00
Epoch 2/2
39/39 - 91s - loss: 762.9310 - loglik: -7.6238e+02 - logprior: -5.5073e-01
Fitted a model with MAP estimate = -752.3304
expansions: []
discards: [  1  84 136 221 263 318 335 336 337 338 339 343 389]
Re-initialized the encoder parameters.
Fitting a model of length 383 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 84s - loss: 767.6851 - loglik: -7.6630e+02 - logprior: -1.3897e+00
Epoch 2/2
39/39 - 85s - loss: 763.3032 - loglik: -7.6329e+02 - logprior: -1.2422e-02
Fitted a model with MAP estimate = -753.8827
expansions: [(0, 2), (258, 1), (332, 5)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 390 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 79s - loss: 758.9234 - loglik: -7.5695e+02 - logprior: -1.9771e+00
Epoch 2/10
39/39 - 82s - loss: 753.7211 - loglik: -7.5387e+02 - logprior: 0.1529
Epoch 3/10
39/39 - 68s - loss: 751.9659 - loglik: -7.5230e+02 - logprior: 0.3306
Epoch 4/10
39/39 - 66s - loss: 751.2609 - loglik: -7.5170e+02 - logprior: 0.4366
Epoch 5/10
39/39 - 75s - loss: 751.7776 - loglik: -7.5235e+02 - logprior: 0.5681
Fitted a model with MAP estimate = -749.8181
Time for alignment: 1646.8053
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 972.7592 - loglik: -9.7144e+02 - logprior: -1.3155e+00
Epoch 2/10
39/39 - 50s - loss: 810.0020 - loglik: -8.0897e+02 - logprior: -1.0303e+00
Epoch 3/10
39/39 - 47s - loss: 798.0612 - loglik: -7.9702e+02 - logprior: -1.0389e+00
Epoch 4/10
39/39 - 48s - loss: 794.9728 - loglik: -7.9393e+02 - logprior: -1.0461e+00
Epoch 5/10
39/39 - 52s - loss: 794.6593 - loglik: -7.9361e+02 - logprior: -1.0448e+00
Epoch 6/10
39/39 - 59s - loss: 794.3302 - loglik: -7.9331e+02 - logprior: -1.0239e+00
Epoch 7/10
39/39 - 61s - loss: 794.0969 - loglik: -7.9304e+02 - logprior: -1.0599e+00
Epoch 8/10
39/39 - 62s - loss: 794.2993 - loglik: -7.9323e+02 - logprior: -1.0682e+00
Fitted a model with MAP estimate = -785.5237
expansions: [(0, 3), (19, 1), (50, 1), (51, 1), (52, 1), (58, 1), (61, 1), (68, 1), (69, 1), (72, 2), (73, 2), (103, 1), (118, 1), (119, 2), (121, 1), (145, 3), (146, 1), (147, 1), (163, 1), (169, 1), (172, 1), (176, 1), (188, 1), (192, 2), (195, 1), (196, 1), (199, 1), (219, 1), (220, 1), (221, 1), (223, 1), (224, 3), (242, 1), (251, 1), (252, 1), (255, 1), (256, 1), (257, 1), (260, 1), (262, 3), (263, 1), (285, 1), (286, 2), (287, 9), (296, 4), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 394 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 85s - loss: 778.8409 - loglik: -7.7663e+02 - logprior: -2.2093e+00
Epoch 2/2
39/39 - 85s - loss: 762.8593 - loglik: -7.6225e+02 - logprior: -6.0639e-01
Fitted a model with MAP estimate = -752.3182
expansions: [(0, 2), (346, 2)]
discards: [  0  84  86 169 222 264 335 336 337 338 339 387]
Re-initialized the encoder parameters.
Fitting a model of length 386 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 70s - loss: 767.5062 - loglik: -7.6615e+02 - logprior: -1.3544e+00
Epoch 2/2
39/39 - 77s - loss: 763.0289 - loglik: -7.6295e+02 - logprior: -7.6558e-02
Fitted a model with MAP estimate = -753.2498
expansions: [(0, 2), (260, 1), (337, 1)]
discards: [  1   2   4   5 136 331 332 333]
Re-initialized the encoder parameters.
Fitting a model of length 382 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 66s - loss: 759.9448 - loglik: -7.5861e+02 - logprior: -1.3321e+00
Epoch 2/10
39/39 - 72s - loss: 755.4224 - loglik: -7.5564e+02 - logprior: 0.2184
Epoch 3/10
39/39 - 75s - loss: 755.0549 - loglik: -7.5539e+02 - logprior: 0.3329
Epoch 4/10
39/39 - 63s - loss: 752.3942 - loglik: -7.5287e+02 - logprior: 0.4774
Epoch 5/10
39/39 - 62s - loss: 752.9118 - loglik: -7.5349e+02 - logprior: 0.5739
Fitted a model with MAP estimate = -751.9958
Time for alignment: 1506.5416
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 55s - loss: 972.0342 - loglik: -9.7071e+02 - logprior: -1.3253e+00
Epoch 2/10
39/39 - 58s - loss: 813.0653 - loglik: -8.1200e+02 - logprior: -1.0624e+00
Epoch 3/10
39/39 - 61s - loss: 799.8035 - loglik: -7.9862e+02 - logprior: -1.1863e+00
Epoch 4/10
39/39 - 61s - loss: 796.9951 - loglik: -7.9581e+02 - logprior: -1.1848e+00
Epoch 5/10
39/39 - 62s - loss: 796.2149 - loglik: -7.9506e+02 - logprior: -1.1584e+00
Epoch 6/10
39/39 - 59s - loss: 796.0266 - loglik: -7.9487e+02 - logprior: -1.1570e+00
Epoch 7/10
39/39 - 56s - loss: 795.7195 - loglik: -7.9456e+02 - logprior: -1.1644e+00
Epoch 8/10
39/39 - 58s - loss: 795.7127 - loglik: -7.9456e+02 - logprior: -1.1569e+00
Epoch 9/10
39/39 - 60s - loss: 795.3603 - loglik: -7.9418e+02 - logprior: -1.1770e+00
Epoch 10/10
39/39 - 62s - loss: 795.2340 - loglik: -7.9399e+02 - logprior: -1.2477e+00
Fitted a model with MAP estimate = -786.8202
expansions: [(0, 3), (6, 1), (15, 1), (34, 1), (51, 1), (53, 1), (57, 1), (63, 1), (69, 1), (70, 3), (72, 2), (77, 1), (119, 1), (120, 2), (122, 1), (144, 1), (145, 1), (146, 2), (147, 1), (148, 1), (163, 1), (170, 1), (173, 1), (175, 1), (176, 1), (193, 2), (195, 1), (196, 1), (197, 1), (218, 1), (219, 1), (220, 1), (222, 1), (223, 1), (224, 2), (249, 1), (251, 1), (253, 1), (255, 1), (256, 1), (257, 1), (260, 1), (262, 3), (263, 1), (266, 2), (285, 2), (286, 2), (287, 8), (296, 2), (297, 2), (298, 2), (309, 1), (313, 2), (314, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 399 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 79s - loss: 778.7620 - loglik: -7.7703e+02 - logprior: -1.7282e+00
Epoch 2/2
39/39 - 86s - loss: 762.5007 - loglik: -7.6202e+02 - logprior: -4.8324e-01
Fitted a model with MAP estimate = -751.8007
expansions: [(0, 2), (265, 1), (348, 1), (349, 1), (365, 1)]
discards: [  1   2   3   6  81 137 167 171 224 320 337 338 339 340 344 366 370 389]
Re-initialized the encoder parameters.
Fitting a model of length 387 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 74s - loss: 767.1765 - loglik: -7.6568e+02 - logprior: -1.4930e+00
Epoch 2/2
39/39 - 78s - loss: 762.1630 - loglik: -7.6213e+02 - logprior: -3.3839e-02
Fitted a model with MAP estimate = -752.8014
expansions: [(0, 2), (5, 1), (333, 2), (334, 2)]
discards: [  1   2   3 330]
Re-initialized the encoder parameters.
Fitting a model of length 390 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 74s - loss: 757.8713 - loglik: -7.5616e+02 - logprior: -1.7095e+00
Epoch 2/10
39/39 - 81s - loss: 752.6362 - loglik: -7.5283e+02 - logprior: 0.1941
Epoch 3/10
39/39 - 90s - loss: 752.2471 - loglik: -7.5259e+02 - logprior: 0.3468
Epoch 4/10
39/39 - 90s - loss: 750.5526 - loglik: -7.5097e+02 - logprior: 0.4159
Epoch 5/10
39/39 - 90s - loss: 749.8623 - loglik: -7.5039e+02 - logprior: 0.5325
Epoch 6/10
39/39 - 84s - loss: 749.1342 - loglik: -7.4980e+02 - logprior: 0.6662
Epoch 7/10
39/39 - 83s - loss: 749.4466 - loglik: -7.5022e+02 - logprior: 0.7757
Fitted a model with MAP estimate = -748.6687
Time for alignment: 1849.0913
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 60s - loss: 972.5510 - loglik: -9.7123e+02 - logprior: -1.3233e+00
Epoch 2/10
39/39 - 61s - loss: 812.9172 - loglik: -8.1192e+02 - logprior: -1.0021e+00
Epoch 3/10
39/39 - 61s - loss: 799.3316 - loglik: -7.9822e+02 - logprior: -1.1115e+00
Epoch 4/10
39/39 - 60s - loss: 796.0087 - loglik: -7.9487e+02 - logprior: -1.1375e+00
Epoch 5/10
39/39 - 52s - loss: 795.5980 - loglik: -7.9447e+02 - logprior: -1.1288e+00
Epoch 6/10
39/39 - 49s - loss: 794.8782 - loglik: -7.9375e+02 - logprior: -1.1311e+00
Epoch 7/10
39/39 - 53s - loss: 794.6464 - loglik: -7.9352e+02 - logprior: -1.1270e+00
Epoch 8/10
39/39 - 58s - loss: 794.1807 - loglik: -7.9304e+02 - logprior: -1.1357e+00
Epoch 9/10
39/39 - 50s - loss: 793.8570 - loglik: -7.9270e+02 - logprior: -1.1525e+00
Epoch 10/10
39/39 - 46s - loss: 794.3401 - loglik: -7.9313e+02 - logprior: -1.2133e+00
Fitted a model with MAP estimate = -785.4811
expansions: [(0, 3), (5, 2), (19, 1), (50, 1), (51, 1), (52, 1), (58, 1), (69, 1), (70, 1), (73, 2), (99, 1), (120, 1), (121, 2), (123, 1), (132, 1), (143, 1), (145, 3), (146, 1), (147, 1), (166, 1), (170, 1), (175, 1), (176, 1), (193, 2), (195, 1), (196, 2), (218, 1), (219, 1), (220, 1), (223, 2), (224, 2), (247, 1), (249, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (257, 1), (262, 3), (263, 2), (264, 2), (272, 1), (284, 1), (285, 2), (286, 8), (287, 1), (288, 1), (296, 4), (309, 1), (313, 2), (314, 3)]
discards: [280]
Re-initialized the encoder parameters.
Fitting a model of length 399 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 85s - loss: 777.3879 - loglik: -7.7529e+02 - logprior: -2.0999e+00
Epoch 2/2
39/39 - 79s - loss: 761.5119 - loglik: -7.6104e+02 - logprior: -4.7193e-01
Fitted a model with MAP estimate = -751.3389
expansions: [(0, 2), (338, 1)]
discards: [  1   2   8  85 137 170 229 265 314 316 339 340 341 342 346 392]
Re-initialized the encoder parameters.
Fitting a model of length 386 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 82s - loss: 767.1988 - loglik: -7.6528e+02 - logprior: -1.9179e+00
Epoch 2/2
39/39 - 85s - loss: 762.1892 - loglik: -7.6215e+02 - logprior: -3.4684e-02
Fitted a model with MAP estimate = -752.7243
expansions: [(0, 2), (259, 1)]
discards: [  1   2   5 330 331]
Re-initialized the encoder parameters.
Fitting a model of length 384 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 87s - loss: 760.1154 - loglik: -7.5798e+02 - logprior: -2.1339e+00
Epoch 2/10
39/39 - 75s - loss: 755.4058 - loglik: -7.5554e+02 - logprior: 0.1346
Epoch 3/10
39/39 - 68s - loss: 753.9958 - loglik: -7.5437e+02 - logprior: 0.3712
Epoch 4/10
39/39 - 70s - loss: 753.0350 - loglik: -7.5352e+02 - logprior: 0.4846
Epoch 5/10
39/39 - 71s - loss: 752.5964 - loglik: -7.5318e+02 - logprior: 0.5884
Epoch 6/10
39/39 - 66s - loss: 752.2275 - loglik: -7.5295e+02 - logprior: 0.7268
Epoch 7/10
39/39 - 68s - loss: 751.8610 - loglik: -7.5264e+02 - logprior: 0.7828
Epoch 8/10
39/39 - 67s - loss: 750.8632 - loglik: -7.5175e+02 - logprior: 0.8833
Epoch 9/10
39/39 - 71s - loss: 751.9213 - loglik: -7.5294e+02 - logprior: 1.0211
Fitted a model with MAP estimate = -750.6389
Time for alignment: 1906.4861
Computed alignments with likelihoods: ['-750.2503', '-749.8181', '-751.9958', '-748.6687', '-750.6389']
Best model has likelihood: -748.6687  (prior= 0.8299 )
time for generating output: 1.6741
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00202.projection.fasta
SP score = 0.2879074547808991
Training of 5 independent models on file PF00018.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd125f8ff40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1ac738790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1262bb760>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.6373 - loglik: -1.3041e+02 - logprior: -3.2227e+00
Epoch 2/10
19/19 - 1s - loss: 108.7447 - loglik: -1.0735e+02 - logprior: -1.3943e+00
Epoch 3/10
19/19 - 1s - loss: 101.5477 - loglik: -1.0002e+02 - logprior: -1.5280e+00
Epoch 4/10
19/19 - 1s - loss: 99.0727 - loglik: -9.7650e+01 - logprior: -1.4226e+00
Epoch 5/10
19/19 - 1s - loss: 98.4205 - loglik: -9.7014e+01 - logprior: -1.4066e+00
Epoch 6/10
19/19 - 1s - loss: 98.3451 - loglik: -9.6951e+01 - logprior: -1.3936e+00
Epoch 7/10
19/19 - 1s - loss: 98.1309 - loglik: -9.6753e+01 - logprior: -1.3778e+00
Epoch 8/10
19/19 - 1s - loss: 97.9139 - loglik: -9.6545e+01 - logprior: -1.3692e+00
Epoch 9/10
19/19 - 1s - loss: 97.9053 - loglik: -9.6540e+01 - logprior: -1.3653e+00
Epoch 10/10
19/19 - 1s - loss: 97.8933 - loglik: -9.6534e+01 - logprior: -1.3596e+00
Fitted a model with MAP estimate = -97.6939
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 102.5350 - loglik: -9.8384e+01 - logprior: -4.1515e+00
Epoch 2/2
19/19 - 1s - loss: 95.4072 - loglik: -9.3363e+01 - logprior: -2.0438e+00
Fitted a model with MAP estimate = -94.0334
expansions: [(0, 2)]
discards: [ 0  9 12 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 95.8618 - loglik: -9.2785e+01 - logprior: -3.0772e+00
Epoch 2/2
19/19 - 1s - loss: 93.0118 - loglik: -9.1741e+01 - logprior: -1.2712e+00
Fitted a model with MAP estimate = -92.5970
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 96.9402 - loglik: -9.3366e+01 - logprior: -3.5741e+00
Epoch 2/10
19/19 - 1s - loss: 93.4556 - loglik: -9.2054e+01 - logprior: -1.4011e+00
Epoch 3/10
19/19 - 1s - loss: 93.1148 - loglik: -9.1806e+01 - logprior: -1.3088e+00
Epoch 4/10
19/19 - 1s - loss: 92.6295 - loglik: -9.1367e+01 - logprior: -1.2626e+00
Epoch 5/10
19/19 - 1s - loss: 92.5248 - loglik: -9.1288e+01 - logprior: -1.2367e+00
Epoch 6/10
19/19 - 1s - loss: 92.2982 - loglik: -9.1073e+01 - logprior: -1.2256e+00
Epoch 7/10
19/19 - 1s - loss: 92.4873 - loglik: -9.1278e+01 - logprior: -1.2098e+00
Fitted a model with MAP estimate = -92.1781
Time for alignment: 39.7900
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.6313 - loglik: -1.3041e+02 - logprior: -3.2251e+00
Epoch 2/10
19/19 - 1s - loss: 109.3507 - loglik: -1.0795e+02 - logprior: -1.4055e+00
Epoch 3/10
19/19 - 1s - loss: 100.8231 - loglik: -9.9465e+01 - logprior: -1.3586e+00
Epoch 4/10
19/19 - 1s - loss: 98.7572 - loglik: -9.7478e+01 - logprior: -1.2788e+00
Epoch 5/10
19/19 - 1s - loss: 98.0901 - loglik: -9.6841e+01 - logprior: -1.2491e+00
Epoch 6/10
19/19 - 1s - loss: 97.8802 - loglik: -9.6647e+01 - logprior: -1.2329e+00
Epoch 7/10
19/19 - 1s - loss: 97.9552 - loglik: -9.6738e+01 - logprior: -1.2176e+00
Fitted a model with MAP estimate = -97.6687
expansions: [(0, 2), (7, 2), (8, 1), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.7699 - loglik: -9.6518e+01 - logprior: -4.2518e+00
Epoch 2/2
19/19 - 1s - loss: 93.4995 - loglik: -9.2120e+01 - logprior: -1.3790e+00
Fitted a model with MAP estimate = -92.5815
expansions: []
discards: [ 0 11 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 98.0546 - loglik: -9.3956e+01 - logprior: -4.0986e+00
Epoch 2/2
19/19 - 1s - loss: 93.7383 - loglik: -9.2199e+01 - logprior: -1.5397e+00
Fitted a model with MAP estimate = -93.1225
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 95.7821 - loglik: -9.2501e+01 - logprior: -3.2808e+00
Epoch 2/10
19/19 - 1s - loss: 93.3847 - loglik: -9.1971e+01 - logprior: -1.4137e+00
Epoch 3/10
19/19 - 1s - loss: 92.9681 - loglik: -9.1655e+01 - logprior: -1.3128e+00
Epoch 4/10
19/19 - 1s - loss: 92.5812 - loglik: -9.1319e+01 - logprior: -1.2624e+00
Epoch 5/10
19/19 - 1s - loss: 92.5197 - loglik: -9.1286e+01 - logprior: -1.2334e+00
Epoch 6/10
19/19 - 1s - loss: 92.3279 - loglik: -9.1106e+01 - logprior: -1.2220e+00
Epoch 7/10
19/19 - 1s - loss: 92.3026 - loglik: -9.1100e+01 - logprior: -1.2022e+00
Epoch 8/10
19/19 - 1s - loss: 92.2590 - loglik: -9.1064e+01 - logprior: -1.1953e+00
Epoch 9/10
19/19 - 1s - loss: 92.2777 - loglik: -9.1098e+01 - logprior: -1.1802e+00
Fitted a model with MAP estimate = -92.1288
Time for alignment: 36.7799
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.6085 - loglik: -1.3038e+02 - logprior: -3.2238e+00
Epoch 2/10
19/19 - 1s - loss: 109.0506 - loglik: -1.0764e+02 - logprior: -1.4073e+00
Epoch 3/10
19/19 - 1s - loss: 100.5925 - loglik: -9.9260e+01 - logprior: -1.3326e+00
Epoch 4/10
19/19 - 1s - loss: 98.6676 - loglik: -9.7391e+01 - logprior: -1.2767e+00
Epoch 5/10
19/19 - 1s - loss: 98.1521 - loglik: -9.6906e+01 - logprior: -1.2465e+00
Epoch 6/10
19/19 - 1s - loss: 98.0652 - loglik: -9.6834e+01 - logprior: -1.2312e+00
Epoch 7/10
19/19 - 1s - loss: 97.8264 - loglik: -9.6612e+01 - logprior: -1.2147e+00
Epoch 8/10
19/19 - 1s - loss: 97.7464 - loglik: -9.6540e+01 - logprior: -1.2067e+00
Epoch 9/10
19/19 - 1s - loss: 97.6713 - loglik: -9.6472e+01 - logprior: -1.1990e+00
Epoch 10/10
19/19 - 1s - loss: 97.7230 - loglik: -9.6528e+01 - logprior: -1.1949e+00
Fitted a model with MAP estimate = -97.5194
expansions: [(0, 2), (7, 2), (8, 1), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 101.0159 - loglik: -9.6633e+01 - logprior: -4.3832e+00
Epoch 2/2
19/19 - 1s - loss: 93.4846 - loglik: -9.2111e+01 - logprior: -1.3732e+00
Fitted a model with MAP estimate = -92.5465
expansions: []
discards: [ 0 10 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 98.0272 - loglik: -9.3918e+01 - logprior: -4.1090e+00
Epoch 2/2
19/19 - 1s - loss: 93.7349 - loglik: -9.2191e+01 - logprior: -1.5438e+00
Fitted a model with MAP estimate = -93.1161
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 95.8320 - loglik: -9.2552e+01 - logprior: -3.2798e+00
Epoch 2/10
19/19 - 1s - loss: 93.2492 - loglik: -9.1839e+01 - logprior: -1.4101e+00
Epoch 3/10
19/19 - 1s - loss: 92.9678 - loglik: -9.1657e+01 - logprior: -1.3105e+00
Epoch 4/10
19/19 - 1s - loss: 92.6190 - loglik: -9.1356e+01 - logprior: -1.2628e+00
Epoch 5/10
19/19 - 1s - loss: 92.6379 - loglik: -9.1411e+01 - logprior: -1.2270e+00
Fitted a model with MAP estimate = -92.3552
Time for alignment: 35.9930
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 133.6198 - loglik: -1.3039e+02 - logprior: -3.2262e+00
Epoch 2/10
19/19 - 1s - loss: 108.8373 - loglik: -1.0742e+02 - logprior: -1.4162e+00
Epoch 3/10
19/19 - 1s - loss: 100.4568 - loglik: -9.9122e+01 - logprior: -1.3344e+00
Epoch 4/10
19/19 - 1s - loss: 98.6881 - loglik: -9.7411e+01 - logprior: -1.2767e+00
Epoch 5/10
19/19 - 1s - loss: 98.2021 - loglik: -9.6953e+01 - logprior: -1.2495e+00
Epoch 6/10
19/19 - 1s - loss: 97.9356 - loglik: -9.6704e+01 - logprior: -1.2312e+00
Epoch 7/10
19/19 - 1s - loss: 97.9423 - loglik: -9.6727e+01 - logprior: -1.2153e+00
Fitted a model with MAP estimate = -97.6911
expansions: [(0, 2), (7, 2), (8, 1), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.7705 - loglik: -9.6528e+01 - logprior: -4.2423e+00
Epoch 2/2
19/19 - 1s - loss: 93.4780 - loglik: -9.2095e+01 - logprior: -1.3827e+00
Fitted a model with MAP estimate = -92.5818
expansions: []
discards: [ 0 10 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 97.9740 - loglik: -9.3873e+01 - logprior: -4.1010e+00
Epoch 2/2
19/19 - 1s - loss: 93.7522 - loglik: -9.2211e+01 - logprior: -1.5413e+00
Fitted a model with MAP estimate = -93.1255
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 95.8240 - loglik: -9.2544e+01 - logprior: -3.2805e+00
Epoch 2/10
19/19 - 1s - loss: 93.3032 - loglik: -9.1889e+01 - logprior: -1.4141e+00
Epoch 3/10
19/19 - 1s - loss: 92.9485 - loglik: -9.1634e+01 - logprior: -1.3143e+00
Epoch 4/10
19/19 - 1s - loss: 92.6806 - loglik: -9.1416e+01 - logprior: -1.2648e+00
Epoch 5/10
19/19 - 1s - loss: 92.5422 - loglik: -9.1308e+01 - logprior: -1.2343e+00
Epoch 6/10
19/19 - 1s - loss: 92.3592 - loglik: -9.1135e+01 - logprior: -1.2239e+00
Epoch 7/10
19/19 - 1s - loss: 92.2930 - loglik: -9.1089e+01 - logprior: -1.2045e+00
Epoch 8/10
19/19 - 1s - loss: 92.1918 - loglik: -9.0996e+01 - logprior: -1.1961e+00
Epoch 9/10
19/19 - 1s - loss: 92.2744 - loglik: -9.1087e+01 - logprior: -1.1874e+00
Fitted a model with MAP estimate = -92.1254
Time for alignment: 38.0639
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 133.6343 - loglik: -1.3041e+02 - logprior: -3.2279e+00
Epoch 2/10
19/19 - 1s - loss: 108.3412 - loglik: -1.0692e+02 - logprior: -1.4169e+00
Epoch 3/10
19/19 - 1s - loss: 100.6836 - loglik: -9.9362e+01 - logprior: -1.3220e+00
Epoch 4/10
19/19 - 1s - loss: 98.5686 - loglik: -9.7298e+01 - logprior: -1.2710e+00
Epoch 5/10
19/19 - 1s - loss: 98.1643 - loglik: -9.6920e+01 - logprior: -1.2446e+00
Epoch 6/10
19/19 - 1s - loss: 97.9612 - loglik: -9.6730e+01 - logprior: -1.2317e+00
Epoch 7/10
19/19 - 1s - loss: 97.8192 - loglik: -9.6605e+01 - logprior: -1.2145e+00
Epoch 8/10
19/19 - 1s - loss: 97.9197 - loglik: -9.6715e+01 - logprior: -1.2046e+00
Fitted a model with MAP estimate = -97.6042
expansions: [(0, 2), (7, 2), (8, 1), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 100.8211 - loglik: -9.6520e+01 - logprior: -4.3012e+00
Epoch 2/2
19/19 - 1s - loss: 93.6199 - loglik: -9.2245e+01 - logprior: -1.3749e+00
Fitted a model with MAP estimate = -92.5807
expansions: []
discards: [ 0 11 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 98.0576 - loglik: -9.3958e+01 - logprior: -4.0996e+00
Epoch 2/2
19/19 - 1s - loss: 93.6868 - loglik: -9.2150e+01 - logprior: -1.5370e+00
Fitted a model with MAP estimate = -93.1399
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 95.8160 - loglik: -9.2533e+01 - logprior: -3.2826e+00
Epoch 2/10
19/19 - 1s - loss: 93.3356 - loglik: -9.1922e+01 - logprior: -1.4136e+00
Epoch 3/10
19/19 - 1s - loss: 92.9632 - loglik: -9.1652e+01 - logprior: -1.3116e+00
Epoch 4/10
19/19 - 1s - loss: 92.5734 - loglik: -9.1308e+01 - logprior: -1.2654e+00
Epoch 5/10
19/19 - 1s - loss: 92.5236 - loglik: -9.1289e+01 - logprior: -1.2345e+00
Epoch 6/10
19/19 - 1s - loss: 92.4961 - loglik: -9.1279e+01 - logprior: -1.2173e+00
Epoch 7/10
19/19 - 1s - loss: 92.1229 - loglik: -9.0916e+01 - logprior: -1.2071e+00
Epoch 8/10
19/19 - 1s - loss: 92.4093 - loglik: -9.1215e+01 - logprior: -1.1943e+00
Fitted a model with MAP estimate = -92.1900
Time for alignment: 36.6019
Computed alignments with likelihoods: ['-92.1781', '-92.1288', '-92.3552', '-92.1254', '-92.1900']
Best model has likelihood: -92.1254  (prior= -1.1632 )
time for generating output: 0.0915
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00018.projection.fasta
SP score = 0.8453329325116489
Training of 5 independent models on file PF00687.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd127aa0c10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd144518ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd50860ac70>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 471.3391 - loglik: -4.6953e+02 - logprior: -1.8070e+00
Epoch 2/10
39/39 - 9s - loss: 370.0946 - loglik: -3.6834e+02 - logprior: -1.7586e+00
Epoch 3/10
39/39 - 9s - loss: 361.2632 - loglik: -3.5946e+02 - logprior: -1.8044e+00
Epoch 4/10
39/39 - 9s - loss: 358.9294 - loglik: -3.5719e+02 - logprior: -1.7408e+00
Epoch 5/10
39/39 - 9s - loss: 357.3760 - loglik: -3.5564e+02 - logprior: -1.7360e+00
Epoch 6/10
39/39 - 9s - loss: 357.0381 - loglik: -3.5529e+02 - logprior: -1.7445e+00
Epoch 7/10
39/39 - 9s - loss: 355.8534 - loglik: -3.5412e+02 - logprior: -1.7359e+00
Epoch 8/10
39/39 - 9s - loss: 356.0782 - loglik: -3.5434e+02 - logprior: -1.7413e+00
Fitted a model with MAP estimate = -355.9409
expansions: [(2, 1), (3, 2), (4, 2), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 2), (41, 1), (42, 1), (43, 1), (52, 1), (58, 1), (59, 1), (61, 1), (64, 1), (73, 1), (75, 1), (98, 2), (99, 1), (100, 1), (102, 1), (103, 1), (108, 1), (113, 1), (114, 1), (117, 1), (123, 1), (127, 1), (130, 1), (131, 1), (132, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 195 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 342.1358 - loglik: -3.4037e+02 - logprior: -1.7678e+00
Epoch 2/2
39/39 - 13s - loss: 328.5910 - loglik: -3.2790e+02 - logprior: -6.8731e-01
Fitted a model with MAP estimate = -326.6856
expansions: []
discards: [  1  45 120]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 333.8076 - loglik: -3.3223e+02 - logprior: -1.5741e+00
Epoch 2/2
39/39 - 12s - loss: 329.6643 - loglik: -3.2915e+02 - logprior: -5.1468e-01
Fitted a model with MAP estimate = -327.8794
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 333.2328 - loglik: -3.3178e+02 - logprior: -1.4489e+00
Epoch 2/10
39/39 - 12s - loss: 329.2343 - loglik: -3.2885e+02 - logprior: -3.8412e-01
Epoch 3/10
39/39 - 12s - loss: 327.3565 - loglik: -3.2705e+02 - logprior: -3.0237e-01
Epoch 4/10
39/39 - 12s - loss: 325.1743 - loglik: -3.2494e+02 - logprior: -2.3385e-01
Epoch 5/10
39/39 - 12s - loss: 323.9617 - loglik: -3.2381e+02 - logprior: -1.5412e-01
Epoch 6/10
39/39 - 12s - loss: 322.9724 - loglik: -3.2291e+02 - logprior: -6.4452e-02
Epoch 7/10
39/39 - 12s - loss: 322.7202 - loglik: -3.2274e+02 - logprior: 0.0228
Epoch 8/10
39/39 - 12s - loss: 323.0764 - loglik: -3.2318e+02 - logprior: 0.1048
Fitted a model with MAP estimate = -322.2947
Time for alignment: 289.3749
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 472.0526 - loglik: -4.7024e+02 - logprior: -1.8148e+00
Epoch 2/10
39/39 - 9s - loss: 374.4614 - loglik: -3.7272e+02 - logprior: -1.7391e+00
Epoch 3/10
39/39 - 9s - loss: 366.1992 - loglik: -3.6449e+02 - logprior: -1.7081e+00
Epoch 4/10
39/39 - 9s - loss: 363.4224 - loglik: -3.6178e+02 - logprior: -1.6472e+00
Epoch 5/10
39/39 - 9s - loss: 361.8250 - loglik: -3.6018e+02 - logprior: -1.6447e+00
Epoch 6/10
39/39 - 9s - loss: 361.0976 - loglik: -3.5945e+02 - logprior: -1.6473e+00
Epoch 7/10
39/39 - 9s - loss: 360.3873 - loglik: -3.5874e+02 - logprior: -1.6448e+00
Epoch 8/10
39/39 - 9s - loss: 360.4799 - loglik: -3.5884e+02 - logprior: -1.6381e+00
Fitted a model with MAP estimate = -359.8104
expansions: [(4, 2), (5, 2), (12, 2), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (39, 1), (40, 1), (41, 1), (46, 1), (58, 1), (59, 1), (61, 1), (63, 1), (66, 1), (74, 2), (77, 1), (97, 1), (99, 3), (101, 1), (111, 1), (113, 1), (114, 1), (117, 1), (123, 1), (127, 1), (129, 2), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 196 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 343.5467 - loglik: -3.4171e+02 - logprior: -1.8380e+00
Epoch 2/2
39/39 - 13s - loss: 329.4723 - loglik: -3.2867e+02 - logprior: -8.0437e-01
Fitted a model with MAP estimate = -327.4605
expansions: []
discards: [  6  94 124 171]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 333.4138 - loglik: -3.3179e+02 - logprior: -1.6277e+00
Epoch 2/2
39/39 - 12s - loss: 329.3814 - loglik: -3.2882e+02 - logprior: -5.5848e-01
Fitted a model with MAP estimate = -327.5562
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 332.7549 - loglik: -3.3128e+02 - logprior: -1.4775e+00
Epoch 2/10
39/39 - 13s - loss: 328.9581 - loglik: -3.2855e+02 - logprior: -4.0695e-01
Epoch 3/10
39/39 - 13s - loss: 327.0058 - loglik: -3.2669e+02 - logprior: -3.1436e-01
Epoch 4/10
39/39 - 13s - loss: 324.4382 - loglik: -3.2419e+02 - logprior: -2.4821e-01
Epoch 5/10
39/39 - 13s - loss: 323.6619 - loglik: -3.2349e+02 - logprior: -1.6953e-01
Epoch 6/10
39/39 - 13s - loss: 323.1584 - loglik: -3.2307e+02 - logprior: -8.7631e-02
Epoch 7/10
39/39 - 13s - loss: 322.4037 - loglik: -3.2241e+02 - logprior: 0.0079
Epoch 8/10
39/39 - 13s - loss: 322.9585 - loglik: -3.2305e+02 - logprior: 0.0901
Fitted a model with MAP estimate = -322.1582
Time for alignment: 295.5560
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 469.8631 - loglik: -4.6805e+02 - logprior: -1.8086e+00
Epoch 2/10
39/39 - 9s - loss: 367.7123 - loglik: -3.6592e+02 - logprior: -1.7942e+00
Epoch 3/10
39/39 - 9s - loss: 359.5399 - loglik: -3.5773e+02 - logprior: -1.8062e+00
Epoch 4/10
39/39 - 9s - loss: 357.5154 - loglik: -3.5575e+02 - logprior: -1.7622e+00
Epoch 5/10
39/39 - 10s - loss: 355.8424 - loglik: -3.5409e+02 - logprior: -1.7538e+00
Epoch 6/10
39/39 - 9s - loss: 354.5869 - loglik: -3.5283e+02 - logprior: -1.7578e+00
Epoch 7/10
39/39 - 10s - loss: 354.3862 - loglik: -3.5264e+02 - logprior: -1.7508e+00
Epoch 8/10
39/39 - 10s - loss: 354.1346 - loglik: -3.5239e+02 - logprior: -1.7462e+00
Epoch 9/10
39/39 - 9s - loss: 353.7030 - loglik: -3.5197e+02 - logprior: -1.7376e+00
Epoch 10/10
39/39 - 9s - loss: 353.5429 - loglik: -3.5180e+02 - logprior: -1.7419e+00
Fitted a model with MAP estimate = -353.3578
expansions: [(2, 1), (3, 2), (4, 2), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (41, 1), (42, 1), (43, 1), (52, 1), (58, 1), (59, 1), (61, 1), (63, 1), (66, 1), (75, 1), (93, 1), (99, 1), (100, 1), (102, 1), (103, 1), (108, 1), (110, 1), (112, 1), (117, 1), (123, 1), (127, 1), (130, 1), (131, 1), (133, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 194 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 340.9465 - loglik: -3.3916e+02 - logprior: -1.7834e+00
Epoch 2/2
39/39 - 13s - loss: 327.9761 - loglik: -3.2727e+02 - logprior: -7.0273e-01
Fitted a model with MAP estimate = -325.7521
expansions: []
discards: [  2 169]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 331.9176 - loglik: -3.3030e+02 - logprior: -1.6145e+00
Epoch 2/2
39/39 - 12s - loss: 328.0323 - loglik: -3.2751e+02 - logprior: -5.2606e-01
Fitted a model with MAP estimate = -326.0815
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 331.1835 - loglik: -3.2974e+02 - logprior: -1.4411e+00
Epoch 2/10
39/39 - 13s - loss: 327.6065 - loglik: -3.2724e+02 - logprior: -3.6896e-01
Epoch 3/10
39/39 - 13s - loss: 325.4323 - loglik: -3.2516e+02 - logprior: -2.7290e-01
Epoch 4/10
39/39 - 13s - loss: 323.1118 - loglik: -3.2290e+02 - logprior: -2.1394e-01
Epoch 5/10
39/39 - 13s - loss: 321.9008 - loglik: -3.2177e+02 - logprior: -1.3301e-01
Epoch 6/10
39/39 - 13s - loss: 321.7946 - loglik: -3.2175e+02 - logprior: -4.5298e-02
Epoch 7/10
39/39 - 13s - loss: 321.0788 - loglik: -3.2112e+02 - logprior: 0.0435
Epoch 8/10
39/39 - 13s - loss: 321.2260 - loglik: -3.2136e+02 - logprior: 0.1313
Fitted a model with MAP estimate = -320.6488
Time for alignment: 318.9368
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 470.6288 - loglik: -4.6881e+02 - logprior: -1.8206e+00
Epoch 2/10
39/39 - 9s - loss: 370.2801 - loglik: -3.6852e+02 - logprior: -1.7556e+00
Epoch 3/10
39/39 - 9s - loss: 362.4408 - loglik: -3.6068e+02 - logprior: -1.7563e+00
Epoch 4/10
39/39 - 9s - loss: 359.9459 - loglik: -3.5824e+02 - logprior: -1.7109e+00
Epoch 5/10
39/39 - 9s - loss: 358.5123 - loglik: -3.5680e+02 - logprior: -1.7113e+00
Epoch 6/10
39/39 - 9s - loss: 357.3441 - loglik: -3.5562e+02 - logprior: -1.7202e+00
Epoch 7/10
39/39 - 10s - loss: 357.3301 - loglik: -3.5562e+02 - logprior: -1.7139e+00
Epoch 8/10
39/39 - 9s - loss: 356.9053 - loglik: -3.5519e+02 - logprior: -1.7114e+00
Epoch 9/10
39/39 - 9s - loss: 356.2890 - loglik: -3.5458e+02 - logprior: -1.7074e+00
Epoch 10/10
39/39 - 9s - loss: 356.6891 - loglik: -3.5499e+02 - logprior: -1.7038e+00
Fitted a model with MAP estimate = -356.0003
expansions: [(2, 1), (3, 2), (4, 2), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (40, 2), (42, 1), (52, 1), (58, 1), (59, 1), (61, 1), (63, 1), (66, 1), (75, 1), (93, 1), (97, 2), (100, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (117, 1), (120, 1), (127, 1), (130, 1), (131, 1), (132, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 195 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 342.8230 - loglik: -3.4099e+02 - logprior: -1.8295e+00
Epoch 2/2
39/39 - 13s - loss: 329.8594 - loglik: -3.2911e+02 - logprior: -7.4718e-01
Fitted a model with MAP estimate = -327.8690
expansions: []
discards: [  2 119 170]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 334.3610 - loglik: -3.3274e+02 - logprior: -1.6209e+00
Epoch 2/2
39/39 - 13s - loss: 330.6017 - loglik: -3.3007e+02 - logprior: -5.2912e-01
Fitted a model with MAP estimate = -328.8822
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 334.2466 - loglik: -3.3279e+02 - logprior: -1.4572e+00
Epoch 2/10
39/39 - 13s - loss: 330.2134 - loglik: -3.2982e+02 - logprior: -3.8914e-01
Epoch 3/10
39/39 - 13s - loss: 328.1111 - loglik: -3.2782e+02 - logprior: -2.9111e-01
Epoch 4/10
39/39 - 13s - loss: 325.8494 - loglik: -3.2562e+02 - logprior: -2.2560e-01
Epoch 5/10
39/39 - 14s - loss: 325.0210 - loglik: -3.2487e+02 - logprior: -1.5087e-01
Epoch 6/10
39/39 - 13s - loss: 323.0589 - loglik: -3.2300e+02 - logprior: -5.5418e-02
Epoch 7/10
39/39 - 13s - loss: 324.5802 - loglik: -3.2460e+02 - logprior: 0.0201
Fitted a model with MAP estimate = -323.5122
Time for alignment: 310.0888
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 471.1739 - loglik: -4.6936e+02 - logprior: -1.8152e+00
Epoch 2/10
39/39 - 9s - loss: 371.1969 - loglik: -3.6941e+02 - logprior: -1.7863e+00
Epoch 3/10
39/39 - 9s - loss: 363.9618 - loglik: -3.6220e+02 - logprior: -1.7627e+00
Epoch 4/10
39/39 - 9s - loss: 361.9774 - loglik: -3.6026e+02 - logprior: -1.7221e+00
Epoch 5/10
39/39 - 9s - loss: 360.2663 - loglik: -3.5855e+02 - logprior: -1.7155e+00
Epoch 6/10
39/39 - 9s - loss: 359.0025 - loglik: -3.5729e+02 - logprior: -1.7090e+00
Epoch 7/10
39/39 - 9s - loss: 358.9453 - loglik: -3.5724e+02 - logprior: -1.7041e+00
Epoch 8/10
39/39 - 9s - loss: 357.8015 - loglik: -3.5610e+02 - logprior: -1.6997e+00
Epoch 9/10
39/39 - 9s - loss: 358.3946 - loglik: -3.5670e+02 - logprior: -1.6964e+00
Fitted a model with MAP estimate = -357.5416
expansions: [(4, 2), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (41, 1), (42, 1), (43, 1), (52, 1), (58, 1), (59, 1), (61, 1), (63, 1), (66, 1), (74, 1), (93, 1), (97, 2), (99, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 1), (120, 1), (127, 1), (129, 2), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 195 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 343.3699 - loglik: -3.4151e+02 - logprior: -1.8645e+00
Epoch 2/2
39/39 - 13s - loss: 329.6473 - loglik: -3.2881e+02 - logprior: -8.3501e-01
Fitted a model with MAP estimate = -327.4957
expansions: []
discards: [  5 119 170]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 333.6489 - loglik: -3.3199e+02 - logprior: -1.6589e+00
Epoch 2/2
39/39 - 12s - loss: 329.7262 - loglik: -3.2914e+02 - logprior: -5.8702e-01
Fitted a model with MAP estimate = -328.1642
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 333.1679 - loglik: -3.3166e+02 - logprior: -1.5072e+00
Epoch 2/10
39/39 - 12s - loss: 329.4713 - loglik: -3.2903e+02 - logprior: -4.3755e-01
Epoch 3/10
39/39 - 12s - loss: 327.3496 - loglik: -3.2700e+02 - logprior: -3.4615e-01
Epoch 4/10
39/39 - 12s - loss: 325.0274 - loglik: -3.2477e+02 - logprior: -2.5989e-01
Epoch 5/10
39/39 - 12s - loss: 323.1001 - loglik: -3.2295e+02 - logprior: -1.5371e-01
Epoch 6/10
39/39 - 12s - loss: 322.9818 - loglik: -3.2292e+02 - logprior: -6.4808e-02
Epoch 7/10
39/39 - 12s - loss: 323.1577 - loglik: -3.2317e+02 - logprior: 0.0169
Fitted a model with MAP estimate = -322.3696
Time for alignment: 286.6209
Computed alignments with likelihoods: ['-322.2947', '-322.1582', '-320.6488', '-323.5122', '-322.3696']
Best model has likelihood: -320.6488  (prior= 0.1571 )
time for generating output: 0.1695
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00687.projection.fasta
SP score = 0.7630795406210124
Training of 5 independent models on file PF03129.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1258b1f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd124aa8a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd124aa85e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 258.2873 - loglik: -2.5519e+02 - logprior: -3.0992e+00
Epoch 2/10
19/19 - 2s - loss: 224.7604 - loglik: -2.2351e+02 - logprior: -1.2535e+00
Epoch 3/10
19/19 - 2s - loss: 212.0575 - loglik: -2.1075e+02 - logprior: -1.3079e+00
Epoch 4/10
19/19 - 2s - loss: 208.2100 - loglik: -2.0698e+02 - logprior: -1.2271e+00
Epoch 5/10
19/19 - 2s - loss: 206.4365 - loglik: -2.0527e+02 - logprior: -1.1638e+00
Epoch 6/10
19/19 - 2s - loss: 205.9253 - loglik: -2.0481e+02 - logprior: -1.1153e+00
Epoch 7/10
19/19 - 2s - loss: 205.6461 - loglik: -2.0455e+02 - logprior: -1.0921e+00
Epoch 8/10
19/19 - 2s - loss: 205.4095 - loglik: -2.0433e+02 - logprior: -1.0826e+00
Epoch 9/10
19/19 - 2s - loss: 205.0227 - loglik: -2.0394e+02 - logprior: -1.0855e+00
Epoch 10/10
19/19 - 2s - loss: 205.1948 - loglik: -2.0411e+02 - logprior: -1.0848e+00
Fitted a model with MAP estimate = -204.6644
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (25, 1), (36, 1), (37, 2), (38, 2), (50, 1), (55, 1), (64, 1), (66, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 207.2420 - loglik: -2.0331e+02 - logprior: -3.9354e+00
Epoch 2/2
19/19 - 2s - loss: 197.5727 - loglik: -1.9572e+02 - logprior: -1.8501e+00
Fitted a model with MAP estimate = -195.7966
expansions: [(0, 2)]
discards: [ 0  7 45]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 197.9485 - loglik: -1.9511e+02 - logprior: -2.8393e+00
Epoch 2/2
19/19 - 2s - loss: 194.6620 - loglik: -1.9363e+02 - logprior: -1.0286e+00
Fitted a model with MAP estimate = -193.7758
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 199.1428 - loglik: -1.9570e+02 - logprior: -3.4459e+00
Epoch 2/10
19/19 - 2s - loss: 194.8903 - loglik: -1.9377e+02 - logprior: -1.1177e+00
Epoch 3/10
19/19 - 2s - loss: 194.1105 - loglik: -1.9314e+02 - logprior: -9.7426e-01
Epoch 4/10
19/19 - 2s - loss: 193.5364 - loglik: -1.9261e+02 - logprior: -9.3128e-01
Epoch 5/10
19/19 - 2s - loss: 193.2480 - loglik: -1.9234e+02 - logprior: -9.0368e-01
Epoch 6/10
19/19 - 2s - loss: 193.0530 - loglik: -1.9217e+02 - logprior: -8.8371e-01
Epoch 7/10
19/19 - 2s - loss: 192.9895 - loglik: -1.9212e+02 - logprior: -8.6493e-01
Epoch 8/10
19/19 - 2s - loss: 192.7906 - loglik: -1.9194e+02 - logprior: -8.4952e-01
Epoch 9/10
19/19 - 2s - loss: 192.5055 - loglik: -1.9168e+02 - logprior: -8.2686e-01
Epoch 10/10
19/19 - 2s - loss: 192.5232 - loglik: -1.9171e+02 - logprior: -8.1003e-01
Fitted a model with MAP estimate = -192.4688
Time for alignment: 71.5627
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 258.1789 - loglik: -2.5508e+02 - logprior: -3.1016e+00
Epoch 2/10
19/19 - 2s - loss: 225.1519 - loglik: -2.2390e+02 - logprior: -1.2562e+00
Epoch 3/10
19/19 - 2s - loss: 212.3466 - loglik: -2.1103e+02 - logprior: -1.3170e+00
Epoch 4/10
19/19 - 2s - loss: 207.8729 - loglik: -2.0657e+02 - logprior: -1.3047e+00
Epoch 5/10
19/19 - 2s - loss: 206.4389 - loglik: -2.0502e+02 - logprior: -1.4160e+00
Epoch 6/10
19/19 - 2s - loss: 206.2852 - loglik: -2.0489e+02 - logprior: -1.3999e+00
Epoch 7/10
19/19 - 2s - loss: 205.7432 - loglik: -2.0433e+02 - logprior: -1.4123e+00
Epoch 8/10
19/19 - 2s - loss: 205.6924 - loglik: -2.0428e+02 - logprior: -1.4075e+00
Epoch 9/10
19/19 - 2s - loss: 205.3490 - loglik: -2.0394e+02 - logprior: -1.4106e+00
Epoch 10/10
19/19 - 2s - loss: 205.6168 - loglik: -2.0421e+02 - logprior: -1.4084e+00
Fitted a model with MAP estimate = -204.9701
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (23, 1), (24, 1), (36, 1), (37, 2), (38, 1), (50, 1), (55, 1), (57, 1), (64, 1), (68, 1), (69, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 207.6910 - loglik: -2.0374e+02 - logprior: -3.9463e+00
Epoch 2/2
19/19 - 2s - loss: 197.6813 - loglik: -1.9585e+02 - logprior: -1.8265e+00
Fitted a model with MAP estimate = -195.8212
expansions: [(0, 2)]
discards: [ 0  7 46]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 197.8457 - loglik: -1.9500e+02 - logprior: -2.8416e+00
Epoch 2/2
19/19 - 2s - loss: 194.6687 - loglik: -1.9364e+02 - logprior: -1.0306e+00
Fitted a model with MAP estimate = -193.8318
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 199.0694 - loglik: -1.9563e+02 - logprior: -3.4425e+00
Epoch 2/10
19/19 - 2s - loss: 194.9590 - loglik: -1.9384e+02 - logprior: -1.1205e+00
Epoch 3/10
19/19 - 2s - loss: 194.0888 - loglik: -1.9312e+02 - logprior: -9.7214e-01
Epoch 4/10
19/19 - 2s - loss: 193.6635 - loglik: -1.9272e+02 - logprior: -9.4166e-01
Epoch 5/10
19/19 - 2s - loss: 193.3804 - loglik: -1.9247e+02 - logprior: -9.0634e-01
Epoch 6/10
19/19 - 2s - loss: 193.1054 - loglik: -1.9221e+02 - logprior: -8.9193e-01
Epoch 7/10
19/19 - 2s - loss: 192.8061 - loglik: -1.9193e+02 - logprior: -8.8080e-01
Epoch 8/10
19/19 - 2s - loss: 192.6601 - loglik: -1.9181e+02 - logprior: -8.4653e-01
Epoch 9/10
19/19 - 2s - loss: 192.5343 - loglik: -1.9170e+02 - logprior: -8.3766e-01
Epoch 10/10
19/19 - 2s - loss: 193.0060 - loglik: -1.9219e+02 - logprior: -8.1530e-01
Fitted a model with MAP estimate = -192.4763
Time for alignment: 70.6147
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 258.1572 - loglik: -2.5506e+02 - logprior: -3.0959e+00
Epoch 2/10
19/19 - 2s - loss: 226.0013 - loglik: -2.2474e+02 - logprior: -1.2622e+00
Epoch 3/10
19/19 - 2s - loss: 211.1816 - loglik: -2.0985e+02 - logprior: -1.3274e+00
Epoch 4/10
19/19 - 2s - loss: 206.4227 - loglik: -2.0526e+02 - logprior: -1.1611e+00
Epoch 5/10
19/19 - 2s - loss: 205.3375 - loglik: -2.0421e+02 - logprior: -1.1311e+00
Epoch 6/10
19/19 - 2s - loss: 205.0739 - loglik: -2.0399e+02 - logprior: -1.0851e+00
Epoch 7/10
19/19 - 2s - loss: 204.3853 - loglik: -2.0331e+02 - logprior: -1.0746e+00
Epoch 8/10
19/19 - 2s - loss: 204.7540 - loglik: -2.0369e+02 - logprior: -1.0633e+00
Fitted a model with MAP estimate = -204.0177
expansions: [(7, 2), (8, 1), (9, 4), (12, 1), (14, 1), (22, 1), (24, 1), (29, 2), (37, 2), (38, 1), (50, 1), (55, 1), (59, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 207.5296 - loglik: -2.0360e+02 - logprior: -3.9326e+00
Epoch 2/2
19/19 - 2s - loss: 197.6814 - loglik: -1.9586e+02 - logprior: -1.8239e+00
Fitted a model with MAP estimate = -195.5465
expansions: [(0, 2)]
discards: [ 0  7  9 39 50]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 197.7845 - loglik: -1.9493e+02 - logprior: -2.8516e+00
Epoch 2/2
19/19 - 2s - loss: 194.4593 - loglik: -1.9341e+02 - logprior: -1.0476e+00
Fitted a model with MAP estimate = -193.5269
expansions: []
discards: [0 9]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 199.1938 - loglik: -1.9573e+02 - logprior: -3.4681e+00
Epoch 2/10
19/19 - 2s - loss: 195.0300 - loglik: -1.9391e+02 - logprior: -1.1236e+00
Epoch 3/10
19/19 - 2s - loss: 194.0415 - loglik: -1.9307e+02 - logprior: -9.7477e-01
Epoch 4/10
19/19 - 2s - loss: 193.7193 - loglik: -1.9278e+02 - logprior: -9.3873e-01
Epoch 5/10
19/19 - 2s - loss: 193.1354 - loglik: -1.9222e+02 - logprior: -9.1156e-01
Epoch 6/10
19/19 - 2s - loss: 193.1031 - loglik: -1.9221e+02 - logprior: -8.9761e-01
Epoch 7/10
19/19 - 2s - loss: 192.9282 - loglik: -1.9205e+02 - logprior: -8.7670e-01
Epoch 8/10
19/19 - 2s - loss: 192.9569 - loglik: -1.9210e+02 - logprior: -8.5912e-01
Fitted a model with MAP estimate = -192.6276
Time for alignment: 62.7613
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 257.7327 - loglik: -2.5464e+02 - logprior: -3.0963e+00
Epoch 2/10
19/19 - 2s - loss: 225.5266 - loglik: -2.2430e+02 - logprior: -1.2307e+00
Epoch 3/10
19/19 - 2s - loss: 212.9188 - loglik: -2.1166e+02 - logprior: -1.2554e+00
Epoch 4/10
19/19 - 2s - loss: 209.3519 - loglik: -2.0824e+02 - logprior: -1.1157e+00
Epoch 5/10
19/19 - 2s - loss: 208.0651 - loglik: -2.0697e+02 - logprior: -1.0912e+00
Epoch 6/10
19/19 - 2s - loss: 207.8684 - loglik: -2.0682e+02 - logprior: -1.0486e+00
Epoch 7/10
19/19 - 2s - loss: 207.4695 - loglik: -2.0643e+02 - logprior: -1.0436e+00
Epoch 8/10
19/19 - 2s - loss: 207.3487 - loglik: -2.0632e+02 - logprior: -1.0320e+00
Epoch 9/10
19/19 - 2s - loss: 206.5712 - loglik: -2.0553e+02 - logprior: -1.0425e+00
Epoch 10/10
19/19 - 2s - loss: 206.1359 - loglik: -2.0510e+02 - logprior: -1.0355e+00
Fitted a model with MAP estimate = -205.5399
expansions: [(7, 2), (8, 1), (9, 4), (12, 1), (14, 1), (24, 2), (37, 1), (38, 1), (43, 1), (50, 2), (55, 1), (63, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 208.4599 - loglik: -2.0453e+02 - logprior: -3.9291e+00
Epoch 2/2
19/19 - 2s - loss: 198.7163 - loglik: -1.9686e+02 - logprior: -1.8516e+00
Fitted a model with MAP estimate = -196.4715
expansions: [(0, 2)]
discards: [ 0  7  9 63]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 198.8113 - loglik: -1.9595e+02 - logprior: -2.8621e+00
Epoch 2/2
19/19 - 2s - loss: 195.3585 - loglik: -1.9429e+02 - logprior: -1.0649e+00
Fitted a model with MAP estimate = -194.4676
expansions: []
discards: [0 9]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 200.1943 - loglik: -1.9673e+02 - logprior: -3.4685e+00
Epoch 2/10
19/19 - 2s - loss: 195.8922 - loglik: -1.9477e+02 - logprior: -1.1260e+00
Epoch 3/10
19/19 - 2s - loss: 195.1181 - loglik: -1.9413e+02 - logprior: -9.8785e-01
Epoch 4/10
19/19 - 2s - loss: 194.5554 - loglik: -1.9361e+02 - logprior: -9.4801e-01
Epoch 5/10
19/19 - 2s - loss: 194.4467 - loglik: -1.9352e+02 - logprior: -9.2210e-01
Epoch 6/10
19/19 - 2s - loss: 194.0400 - loglik: -1.9314e+02 - logprior: -8.9882e-01
Epoch 7/10
19/19 - 2s - loss: 193.9950 - loglik: -1.9311e+02 - logprior: -8.8085e-01
Epoch 8/10
19/19 - 2s - loss: 193.8075 - loglik: -1.9294e+02 - logprior: -8.7032e-01
Epoch 9/10
19/19 - 2s - loss: 193.9268 - loglik: -1.9308e+02 - logprior: -8.4753e-01
Fitted a model with MAP estimate = -193.5323
Time for alignment: 67.9462
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 258.2133 - loglik: -2.5511e+02 - logprior: -3.1044e+00
Epoch 2/10
19/19 - 2s - loss: 225.8649 - loglik: -2.2460e+02 - logprior: -1.2687e+00
Epoch 3/10
19/19 - 2s - loss: 211.0839 - loglik: -2.0974e+02 - logprior: -1.3390e+00
Epoch 4/10
19/19 - 2s - loss: 207.2923 - loglik: -2.0603e+02 - logprior: -1.2581e+00
Epoch 5/10
19/19 - 2s - loss: 205.4977 - loglik: -2.0432e+02 - logprior: -1.1824e+00
Epoch 6/10
19/19 - 2s - loss: 204.4741 - loglik: -2.0332e+02 - logprior: -1.1546e+00
Epoch 7/10
19/19 - 2s - loss: 203.6301 - loglik: -2.0251e+02 - logprior: -1.1192e+00
Epoch 8/10
19/19 - 2s - loss: 203.5059 - loglik: -2.0239e+02 - logprior: -1.1132e+00
Epoch 9/10
19/19 - 2s - loss: 203.4965 - loglik: -2.0239e+02 - logprior: -1.1075e+00
Epoch 10/10
19/19 - 2s - loss: 203.6661 - loglik: -2.0256e+02 - logprior: -1.1021e+00
Fitted a model with MAP estimate = -202.9630
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (24, 2), (26, 1), (36, 1), (37, 2), (38, 1), (50, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 207.5644 - loglik: -2.0361e+02 - logprior: -3.9517e+00
Epoch 2/2
19/19 - 2s - loss: 197.7034 - loglik: -1.9583e+02 - logprior: -1.8753e+00
Fitted a model with MAP estimate = -195.8809
expansions: [(0, 2)]
discards: [ 0  7 31 47]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 198.1788 - loglik: -1.9533e+02 - logprior: -2.8469e+00
Epoch 2/2
19/19 - 2s - loss: 194.6145 - loglik: -1.9358e+02 - logprior: -1.0344e+00
Fitted a model with MAP estimate = -193.8698
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 199.1398 - loglik: -1.9570e+02 - logprior: -3.4351e+00
Epoch 2/10
19/19 - 2s - loss: 194.9592 - loglik: -1.9384e+02 - logprior: -1.1241e+00
Epoch 3/10
19/19 - 2s - loss: 194.1036 - loglik: -1.9313e+02 - logprior: -9.7046e-01
Epoch 4/10
19/19 - 2s - loss: 193.7870 - loglik: -1.9285e+02 - logprior: -9.3224e-01
Epoch 5/10
19/19 - 2s - loss: 192.9761 - loglik: -1.9206e+02 - logprior: -9.1273e-01
Epoch 6/10
19/19 - 2s - loss: 193.0846 - loglik: -1.9220e+02 - logprior: -8.8504e-01
Fitted a model with MAP estimate = -192.8535
Time for alignment: 62.2604
Computed alignments with likelihoods: ['-192.4688', '-192.4763', '-192.6276', '-193.5323', '-192.8535']
Best model has likelihood: -192.4688  (prior= -0.7893 )
time for generating output: 0.1296
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF03129.projection.fasta
SP score = 0.9656254555778069
Training of 5 independent models on file PF13378.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1272e27f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1c429c760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd352035cd0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 567.3805 - loglik: -5.6563e+02 - logprior: -1.7460e+00
Epoch 2/10
39/39 - 10s - loss: 491.3442 - loglik: -4.9014e+02 - logprior: -1.2083e+00
Epoch 3/10
39/39 - 10s - loss: 483.7616 - loglik: -4.8267e+02 - logprior: -1.0889e+00
Epoch 4/10
39/39 - 10s - loss: 482.2591 - loglik: -4.8120e+02 - logprior: -1.0623e+00
Epoch 5/10
39/39 - 10s - loss: 481.9401 - loglik: -4.8088e+02 - logprior: -1.0582e+00
Epoch 6/10
39/39 - 11s - loss: 481.6930 - loglik: -4.8064e+02 - logprior: -1.0504e+00
Epoch 7/10
39/39 - 10s - loss: 481.6083 - loglik: -4.8055e+02 - logprior: -1.0571e+00
Epoch 8/10
39/39 - 11s - loss: 481.4533 - loglik: -4.8040e+02 - logprior: -1.0524e+00
Epoch 9/10
39/39 - 10s - loss: 481.6532 - loglik: -4.8060e+02 - logprior: -1.0557e+00
Fitted a model with MAP estimate = -478.7136
expansions: [(0, 3), (19, 2), (20, 2), (21, 1), (34, 1), (36, 1), (45, 2), (46, 1), (51, 1), (52, 1), (65, 1), (68, 1), (72, 1), (75, 1), (85, 2), (86, 2), (87, 1), (89, 1), (90, 1), (102, 1), (103, 1), (107, 1), (108, 1), (116, 1), (125, 2), (132, 1), (142, 2), (150, 2), (151, 1), (152, 2), (153, 3), (159, 1), (161, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 219 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 474.4674 - loglik: -4.7196e+02 - logprior: -2.5109e+00
Epoch 2/2
39/39 - 16s - loss: 463.3698 - loglik: -4.6264e+02 - logprior: -7.3389e-01
Fitted a model with MAP estimate = -458.4018
expansions: []
discards: [ 23  26  56 105 107 177 194]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 465.4003 - loglik: -4.6365e+02 - logprior: -1.7469e+00
Epoch 2/2
39/39 - 14s - loss: 461.9737 - loglik: -4.6147e+02 - logprior: -5.0601e-01
Fitted a model with MAP estimate = -458.4714
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 212 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 462.1661 - loglik: -4.6053e+02 - logprior: -1.6393e+00
Epoch 2/10
39/39 - 15s - loss: 459.1217 - loglik: -4.5870e+02 - logprior: -4.2024e-01
Epoch 3/10
39/39 - 15s - loss: 457.8391 - loglik: -4.5754e+02 - logprior: -2.9995e-01
Epoch 4/10
39/39 - 15s - loss: 457.3505 - loglik: -4.5711e+02 - logprior: -2.4498e-01
Epoch 5/10
39/39 - 14s - loss: 457.0056 - loglik: -4.5683e+02 - logprior: -1.7771e-01
Epoch 6/10
39/39 - 14s - loss: 456.8236 - loglik: -4.5671e+02 - logprior: -1.1675e-01
Epoch 7/10
39/39 - 14s - loss: 456.5642 - loglik: -4.5650e+02 - logprior: -6.8770e-02
Epoch 8/10
39/39 - 14s - loss: 456.4615 - loglik: -4.5646e+02 - logprior: -5.1520e-03
Epoch 9/10
39/39 - 15s - loss: 456.4640 - loglik: -4.5651e+02 - logprior: 0.0430
Fitted a model with MAP estimate = -456.0946
Time for alignment: 368.6668
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 567.7104 - loglik: -5.6597e+02 - logprior: -1.7405e+00
Epoch 2/10
39/39 - 11s - loss: 491.4882 - loglik: -4.9026e+02 - logprior: -1.2243e+00
Epoch 3/10
39/39 - 11s - loss: 480.6347 - loglik: -4.7943e+02 - logprior: -1.2046e+00
Epoch 4/10
39/39 - 11s - loss: 478.8766 - loglik: -4.7770e+02 - logprior: -1.1737e+00
Epoch 5/10
39/39 - 11s - loss: 478.7885 - loglik: -4.7763e+02 - logprior: -1.1543e+00
Epoch 6/10
39/39 - 11s - loss: 478.4841 - loglik: -4.7733e+02 - logprior: -1.1549e+00
Epoch 7/10
39/39 - 11s - loss: 478.4934 - loglik: -4.7734e+02 - logprior: -1.1571e+00
Fitted a model with MAP estimate = -475.7156
expansions: [(0, 3), (12, 1), (20, 2), (21, 1), (26, 1), (36, 1), (45, 2), (46, 1), (51, 1), (52, 1), (69, 1), (73, 1), (76, 1), (85, 1), (86, 1), (87, 2), (88, 1), (90, 2), (93, 1), (103, 1), (108, 1), (109, 1), (113, 1), (116, 1), (125, 3), (132, 1), (141, 1), (142, 4), (149, 2), (152, 3), (153, 3), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 222 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 470.8396 - loglik: -4.6828e+02 - logprior: -2.5611e+00
Epoch 2/2
39/39 - 16s - loss: 460.1017 - loglik: -4.5928e+02 - logprior: -8.2116e-01
Fitted a model with MAP estimate = -456.1699
expansions: []
discards: [ 24  55 106 112 176 179 188 197]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 463.0669 - loglik: -4.6131e+02 - logprior: -1.7545e+00
Epoch 2/2
39/39 - 15s - loss: 459.8635 - loglik: -4.5934e+02 - logprior: -5.2483e-01
Fitted a model with MAP estimate = -456.4396
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 460.0451 - loglik: -4.5839e+02 - logprior: -1.6580e+00
Epoch 2/10
39/39 - 16s - loss: 457.5373 - loglik: -4.5711e+02 - logprior: -4.2905e-01
Epoch 3/10
39/39 - 16s - loss: 456.1976 - loglik: -4.5588e+02 - logprior: -3.1558e-01
Epoch 4/10
39/39 - 16s - loss: 455.7433 - loglik: -4.5549e+02 - logprior: -2.5573e-01
Epoch 5/10
39/39 - 15s - loss: 455.2178 - loglik: -4.5503e+02 - logprior: -1.8805e-01
Epoch 6/10
39/39 - 15s - loss: 455.2273 - loglik: -4.5510e+02 - logprior: -1.2578e-01
Fitted a model with MAP estimate = -454.6626
Time for alignment: 318.8214
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 566.7469 - loglik: -5.6500e+02 - logprior: -1.7491e+00
Epoch 2/10
39/39 - 11s - loss: 489.2510 - loglik: -4.8802e+02 - logprior: -1.2326e+00
Epoch 3/10
39/39 - 11s - loss: 479.4589 - loglik: -4.7826e+02 - logprior: -1.2023e+00
Epoch 4/10
39/39 - 11s - loss: 477.8040 - loglik: -4.7662e+02 - logprior: -1.1806e+00
Epoch 5/10
39/39 - 11s - loss: 477.6438 - loglik: -4.7649e+02 - logprior: -1.1534e+00
Epoch 6/10
39/39 - 11s - loss: 477.5434 - loglik: -4.7640e+02 - logprior: -1.1455e+00
Epoch 7/10
39/39 - 11s - loss: 477.3199 - loglik: -4.7617e+02 - logprior: -1.1489e+00
Epoch 8/10
39/39 - 11s - loss: 477.2926 - loglik: -4.7615e+02 - logprior: -1.1408e+00
Epoch 9/10
39/39 - 11s - loss: 477.3113 - loglik: -4.7617e+02 - logprior: -1.1446e+00
Fitted a model with MAP estimate = -474.5362
expansions: [(0, 3), (19, 2), (20, 2), (21, 1), (26, 1), (35, 2), (46, 1), (50, 1), (51, 1), (52, 1), (69, 1), (73, 1), (76, 1), (85, 1), (86, 1), (87, 2), (88, 1), (90, 2), (93, 1), (103, 1), (108, 1), (109, 1), (113, 1), (116, 1), (125, 2), (136, 1), (138, 1), (141, 2), (148, 1), (149, 3), (150, 2), (151, 1), (154, 1), (155, 1), (159, 1), (161, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 221 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 470.7348 - loglik: -4.6822e+02 - logprior: -2.5118e+00
Epoch 2/2
39/39 - 14s - loss: 460.8006 - loglik: -4.6002e+02 - logprior: -7.7891e-01
Fitted a model with MAP estimate = -456.9900
expansions: []
discards: [ 45 107 113 177 191]
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 463.6614 - loglik: -4.6188e+02 - logprior: -1.7839e+00
Epoch 2/2
39/39 - 13s - loss: 460.4955 - loglik: -4.5996e+02 - logprior: -5.3591e-01
Fitted a model with MAP estimate = -457.0489
expansions: []
discards: [25]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 460.8638 - loglik: -4.5921e+02 - logprior: -1.6583e+00
Epoch 2/10
39/39 - 12s - loss: 458.3786 - loglik: -4.5794e+02 - logprior: -4.3418e-01
Epoch 3/10
39/39 - 12s - loss: 456.9022 - loglik: -4.5658e+02 - logprior: -3.2390e-01
Epoch 4/10
39/39 - 13s - loss: 456.2413 - loglik: -4.5598e+02 - logprior: -2.5651e-01
Epoch 5/10
39/39 - 13s - loss: 456.1819 - loglik: -4.5599e+02 - logprior: -1.8764e-01
Epoch 6/10
39/39 - 13s - loss: 455.6331 - loglik: -4.5550e+02 - logprior: -1.3127e-01
Epoch 7/10
39/39 - 13s - loss: 455.7747 - loglik: -4.5569e+02 - logprior: -8.2286e-02
Fitted a model with MAP estimate = -455.2583
Time for alignment: 318.7661
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 568.1433 - loglik: -5.6641e+02 - logprior: -1.7319e+00
Epoch 2/10
39/39 - 10s - loss: 491.9588 - loglik: -4.9067e+02 - logprior: -1.2898e+00
Epoch 3/10
39/39 - 10s - loss: 482.4832 - loglik: -4.8126e+02 - logprior: -1.2199e+00
Epoch 4/10
39/39 - 10s - loss: 480.7996 - loglik: -4.7961e+02 - logprior: -1.1883e+00
Epoch 5/10
39/39 - 10s - loss: 479.9142 - loglik: -4.7873e+02 - logprior: -1.1820e+00
Epoch 6/10
39/39 - 10s - loss: 479.7202 - loglik: -4.7853e+02 - logprior: -1.1882e+00
Epoch 7/10
39/39 - 10s - loss: 479.5531 - loglik: -4.7837e+02 - logprior: -1.1819e+00
Epoch 8/10
39/39 - 10s - loss: 479.4453 - loglik: -4.7827e+02 - logprior: -1.1765e+00
Epoch 9/10
39/39 - 10s - loss: 479.4167 - loglik: -4.7824e+02 - logprior: -1.1815e+00
Epoch 10/10
39/39 - 10s - loss: 479.6581 - loglik: -4.7848e+02 - logprior: -1.1819e+00
Fitted a model with MAP estimate = -476.7869
expansions: [(0, 3), (12, 1), (20, 2), (21, 1), (26, 1), (35, 2), (45, 2), (46, 1), (50, 1), (51, 1), (65, 1), (68, 1), (72, 1), (75, 1), (85, 2), (86, 1), (87, 1), (90, 2), (93, 1), (102, 1), (103, 1), (107, 1), (108, 1), (116, 1), (119, 2), (125, 1), (137, 1), (138, 3), (141, 2), (149, 3), (150, 2), (151, 2), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 223 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 473.4818 - loglik: -4.7088e+02 - logprior: -2.6062e+00
Epoch 2/2
39/39 - 14s - loss: 461.3607 - loglik: -4.6054e+02 - logprior: -8.1845e-01
Fitted a model with MAP estimate = -456.7872
expansions: []
discards: [ 24  44  56 105 113 150 174 180 193]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 463.7508 - loglik: -4.6201e+02 - logprior: -1.7427e+00
Epoch 2/2
39/39 - 14s - loss: 460.2306 - loglik: -4.5973e+02 - logprior: -5.0218e-01
Fitted a model with MAP estimate = -456.7293
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 460.6151 - loglik: -4.5898e+02 - logprior: -1.6320e+00
Epoch 2/10
39/39 - 13s - loss: 457.7854 - loglik: -4.5738e+02 - logprior: -4.0511e-01
Epoch 3/10
39/39 - 13s - loss: 456.6463 - loglik: -4.5635e+02 - logprior: -2.9386e-01
Epoch 4/10
39/39 - 13s - loss: 455.9582 - loglik: -4.5574e+02 - logprior: -2.2226e-01
Epoch 5/10
39/39 - 13s - loss: 455.9037 - loglik: -4.5574e+02 - logprior: -1.6383e-01
Epoch 6/10
39/39 - 13s - loss: 455.5954 - loglik: -4.5549e+02 - logprior: -1.0803e-01
Epoch 7/10
39/39 - 13s - loss: 455.2098 - loglik: -4.5517e+02 - logprior: -4.2364e-02
Epoch 8/10
39/39 - 13s - loss: 455.3174 - loglik: -4.5532e+02 - logprior: -8.9097e-04
Fitted a model with MAP estimate = -454.8958
Time for alignment: 341.3250
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 567.1787 - loglik: -5.6542e+02 - logprior: -1.7552e+00
Epoch 2/10
39/39 - 10s - loss: 489.5480 - loglik: -4.8816e+02 - logprior: -1.3916e+00
Epoch 3/10
39/39 - 10s - loss: 481.8235 - loglik: -4.8031e+02 - logprior: -1.5093e+00
Epoch 4/10
39/39 - 10s - loss: 479.9958 - loglik: -4.7848e+02 - logprior: -1.5159e+00
Epoch 5/10
39/39 - 10s - loss: 479.6450 - loglik: -4.7813e+02 - logprior: -1.5167e+00
Epoch 6/10
39/39 - 10s - loss: 479.2945 - loglik: -4.7778e+02 - logprior: -1.5110e+00
Epoch 7/10
39/39 - 10s - loss: 479.1460 - loglik: -4.7764e+02 - logprior: -1.5042e+00
Epoch 8/10
39/39 - 10s - loss: 479.2028 - loglik: -4.7770e+02 - logprior: -1.5078e+00
Fitted a model with MAP estimate = -476.3940
expansions: [(5, 1), (6, 2), (7, 2), (8, 1), (20, 2), (21, 1), (31, 1), (35, 2), (45, 2), (46, 1), (49, 1), (51, 1), (65, 1), (68, 1), (72, 1), (75, 1), (85, 1), (86, 1), (87, 1), (88, 2), (89, 2), (92, 1), (102, 1), (107, 1), (108, 1), (116, 1), (119, 1), (125, 2), (126, 1), (135, 1), (141, 3), (145, 1), (146, 1), (148, 2), (151, 1), (154, 1), (155, 1), (159, 1), (161, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 222 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 472.5428 - loglik: -4.6987e+02 - logprior: -2.6681e+00
Epoch 2/2
39/39 - 14s - loss: 460.8365 - loglik: -4.5990e+02 - logprior: -9.3201e-01
Fitted a model with MAP estimate = -456.5425
expansions: [(0, 2)]
discards: [  0   9  25  44  57 112 114 180]
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 462.8101 - loglik: -4.6115e+02 - logprior: -1.6617e+00
Epoch 2/2
39/39 - 14s - loss: 459.6764 - loglik: -4.5905e+02 - logprior: -6.2643e-01
Fitted a model with MAP estimate = -456.1353
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 461.3574 - loglik: -4.5920e+02 - logprior: -2.1618e+00
Epoch 2/10
39/39 - 13s - loss: 457.7191 - loglik: -4.5733e+02 - logprior: -3.9051e-01
Epoch 3/10
39/39 - 14s - loss: 456.0896 - loglik: -4.5578e+02 - logprior: -3.1072e-01
Epoch 4/10
39/39 - 14s - loss: 455.4633 - loglik: -4.5524e+02 - logprior: -2.2298e-01
Epoch 5/10
39/39 - 14s - loss: 455.2192 - loglik: -4.5505e+02 - logprior: -1.7304e-01
Epoch 6/10
39/39 - 14s - loss: 455.0668 - loglik: -4.5496e+02 - logprior: -1.0208e-01
Epoch 7/10
39/39 - 14s - loss: 454.9881 - loglik: -4.5493e+02 - logprior: -5.9628e-02
Epoch 8/10
39/39 - 14s - loss: 454.7012 - loglik: -4.5469e+02 - logprior: -1.3030e-02
Epoch 9/10
39/39 - 14s - loss: 454.9594 - loglik: -4.5501e+02 - logprior: 0.0470
Fitted a model with MAP estimate = -454.3681
Time for alignment: 342.6006
Computed alignments with likelihoods: ['-456.0946', '-454.6626', '-455.2583', '-454.8958', '-454.3681']
Best model has likelihood: -454.3681  (prior= 0.0547 )
time for generating output: 0.3086
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13378.projection.fasta
SP score = 0.7006591955894669
Training of 5 independent models on file PF00084.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd126414c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1887b47f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd144561880>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 164.8717 - loglik: -1.6168e+02 - logprior: -3.1897e+00
Epoch 2/10
19/19 - 1s - loss: 135.4731 - loglik: -1.3408e+02 - logprior: -1.3968e+00
Epoch 3/10
19/19 - 1s - loss: 128.2237 - loglik: -1.2675e+02 - logprior: -1.4703e+00
Epoch 4/10
19/19 - 1s - loss: 126.9773 - loglik: -1.2566e+02 - logprior: -1.3195e+00
Epoch 5/10
19/19 - 1s - loss: 126.6082 - loglik: -1.2529e+02 - logprior: -1.3231e+00
Epoch 6/10
19/19 - 1s - loss: 126.4933 - loglik: -1.2519e+02 - logprior: -1.3057e+00
Epoch 7/10
19/19 - 1s - loss: 126.4285 - loglik: -1.2513e+02 - logprior: -1.2982e+00
Epoch 8/10
19/19 - 1s - loss: 126.4435 - loglik: -1.2515e+02 - logprior: -1.2927e+00
Fitted a model with MAP estimate = -126.2324
expansions: [(11, 1), (12, 3), (13, 3), (14, 2), (28, 2), (30, 2), (31, 1), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.7737 - loglik: -1.2971e+02 - logprior: -4.0680e+00
Epoch 2/2
19/19 - 1s - loss: 126.6668 - loglik: -1.2451e+02 - logprior: -2.1617e+00
Fitted a model with MAP estimate = -124.1429
expansions: []
discards: [12 13 16 36 41 50]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.6865 - loglik: -1.2337e+02 - logprior: -3.3185e+00
Epoch 2/2
19/19 - 1s - loss: 123.2901 - loglik: -1.2193e+02 - logprior: -1.3567e+00
Fitted a model with MAP estimate = -122.8819
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 125.7611 - loglik: -1.2256e+02 - logprior: -3.2045e+00
Epoch 2/10
19/19 - 1s - loss: 123.1227 - loglik: -1.2178e+02 - logprior: -1.3469e+00
Epoch 3/10
19/19 - 1s - loss: 122.6651 - loglik: -1.2142e+02 - logprior: -1.2467e+00
Epoch 4/10
19/19 - 1s - loss: 122.3800 - loglik: -1.2118e+02 - logprior: -1.1979e+00
Epoch 5/10
19/19 - 1s - loss: 122.1827 - loglik: -1.2102e+02 - logprior: -1.1591e+00
Epoch 6/10
19/19 - 1s - loss: 121.9254 - loglik: -1.2077e+02 - logprior: -1.1528e+00
Epoch 7/10
19/19 - 1s - loss: 121.8692 - loglik: -1.2074e+02 - logprior: -1.1327e+00
Epoch 8/10
19/19 - 1s - loss: 121.8201 - loglik: -1.2070e+02 - logprior: -1.1205e+00
Epoch 9/10
19/19 - 1s - loss: 121.8927 - loglik: -1.2078e+02 - logprior: -1.1102e+00
Fitted a model with MAP estimate = -121.6649
Time for alignment: 44.0776
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 165.0870 - loglik: -1.6190e+02 - logprior: -3.1893e+00
Epoch 2/10
19/19 - 1s - loss: 138.0497 - loglik: -1.3666e+02 - logprior: -1.3931e+00
Epoch 3/10
19/19 - 1s - loss: 128.9300 - loglik: -1.2745e+02 - logprior: -1.4836e+00
Epoch 4/10
19/19 - 1s - loss: 126.8755 - loglik: -1.2555e+02 - logprior: -1.3230e+00
Epoch 5/10
19/19 - 1s - loss: 126.6065 - loglik: -1.2529e+02 - logprior: -1.3145e+00
Epoch 6/10
19/19 - 1s - loss: 126.3081 - loglik: -1.2502e+02 - logprior: -1.2884e+00
Epoch 7/10
19/19 - 1s - loss: 126.1424 - loglik: -1.2486e+02 - logprior: -1.2817e+00
Epoch 8/10
19/19 - 1s - loss: 126.2358 - loglik: -1.2497e+02 - logprior: -1.2689e+00
Fitted a model with MAP estimate = -126.0284
expansions: [(11, 5), (12, 2), (13, 1), (26, 2), (30, 2), (34, 3), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.8784 - loglik: -1.2980e+02 - logprior: -4.0788e+00
Epoch 2/2
19/19 - 1s - loss: 126.6407 - loglik: -1.2448e+02 - logprior: -2.1631e+00
Fitted a model with MAP estimate = -124.1143
expansions: []
discards: [12 13 33 40 46 50]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.5638 - loglik: -1.2325e+02 - logprior: -3.3128e+00
Epoch 2/2
19/19 - 1s - loss: 123.3782 - loglik: -1.2202e+02 - logprior: -1.3608e+00
Fitted a model with MAP estimate = -122.8696
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.7388 - loglik: -1.2253e+02 - logprior: -3.2055e+00
Epoch 2/10
19/19 - 1s - loss: 123.1304 - loglik: -1.2178e+02 - logprior: -1.3457e+00
Epoch 3/10
19/19 - 1s - loss: 122.6977 - loglik: -1.2145e+02 - logprior: -1.2482e+00
Epoch 4/10
19/19 - 1s - loss: 122.2962 - loglik: -1.2110e+02 - logprior: -1.1916e+00
Epoch 5/10
19/19 - 1s - loss: 122.2673 - loglik: -1.2110e+02 - logprior: -1.1664e+00
Epoch 6/10
19/19 - 1s - loss: 121.8271 - loglik: -1.2068e+02 - logprior: -1.1487e+00
Epoch 7/10
19/19 - 1s - loss: 122.0340 - loglik: -1.2090e+02 - logprior: -1.1341e+00
Fitted a model with MAP estimate = -121.7731
Time for alignment: 40.4303
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 164.8903 - loglik: -1.6170e+02 - logprior: -3.1879e+00
Epoch 2/10
19/19 - 1s - loss: 135.7896 - loglik: -1.3440e+02 - logprior: -1.3926e+00
Epoch 3/10
19/19 - 1s - loss: 128.4132 - loglik: -1.2695e+02 - logprior: -1.4670e+00
Epoch 4/10
19/19 - 1s - loss: 126.9968 - loglik: -1.2568e+02 - logprior: -1.3178e+00
Epoch 5/10
19/19 - 1s - loss: 126.6826 - loglik: -1.2536e+02 - logprior: -1.3256e+00
Epoch 6/10
19/19 - 1s - loss: 126.5840 - loglik: -1.2528e+02 - logprior: -1.3029e+00
Epoch 7/10
19/19 - 1s - loss: 126.3003 - loglik: -1.2500e+02 - logprior: -1.2998e+00
Epoch 8/10
19/19 - 1s - loss: 126.3481 - loglik: -1.2506e+02 - logprior: -1.2917e+00
Fitted a model with MAP estimate = -126.2359
expansions: [(11, 1), (12, 3), (13, 3), (14, 2), (28, 2), (30, 2), (31, 1), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.7813 - loglik: -1.2971e+02 - logprior: -4.0696e+00
Epoch 2/2
19/19 - 1s - loss: 126.6679 - loglik: -1.2451e+02 - logprior: -2.1576e+00
Fitted a model with MAP estimate = -124.1606
expansions: []
discards: [12 13 16 36 41 50]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.6870 - loglik: -1.2337e+02 - logprior: -3.3215e+00
Epoch 2/2
19/19 - 1s - loss: 123.3441 - loglik: -1.2198e+02 - logprior: -1.3592e+00
Fitted a model with MAP estimate = -122.8566
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.7419 - loglik: -1.2253e+02 - logprior: -3.2078e+00
Epoch 2/10
19/19 - 1s - loss: 123.1989 - loglik: -1.2186e+02 - logprior: -1.3425e+00
Epoch 3/10
19/19 - 1s - loss: 122.7047 - loglik: -1.2146e+02 - logprior: -1.2482e+00
Epoch 4/10
19/19 - 1s - loss: 122.3176 - loglik: -1.2112e+02 - logprior: -1.1939e+00
Epoch 5/10
19/19 - 1s - loss: 122.0867 - loglik: -1.2092e+02 - logprior: -1.1645e+00
Epoch 6/10
19/19 - 1s - loss: 121.8939 - loglik: -1.2075e+02 - logprior: -1.1482e+00
Epoch 7/10
19/19 - 1s - loss: 121.8531 - loglik: -1.2072e+02 - logprior: -1.1354e+00
Epoch 8/10
19/19 - 1s - loss: 121.9496 - loglik: -1.2083e+02 - logprior: -1.1184e+00
Fitted a model with MAP estimate = -121.7125
Time for alignment: 42.1909
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 164.9856 - loglik: -1.6180e+02 - logprior: -3.1883e+00
Epoch 2/10
19/19 - 1s - loss: 137.0535 - loglik: -1.3566e+02 - logprior: -1.3900e+00
Epoch 3/10
19/19 - 1s - loss: 128.5312 - loglik: -1.2705e+02 - logprior: -1.4815e+00
Epoch 4/10
19/19 - 1s - loss: 126.8031 - loglik: -1.2548e+02 - logprior: -1.3196e+00
Epoch 5/10
19/19 - 1s - loss: 126.0988 - loglik: -1.2477e+02 - logprior: -1.3284e+00
Epoch 6/10
19/19 - 1s - loss: 125.9615 - loglik: -1.2466e+02 - logprior: -1.3044e+00
Epoch 7/10
19/19 - 1s - loss: 125.8145 - loglik: -1.2452e+02 - logprior: -1.2954e+00
Epoch 8/10
19/19 - 1s - loss: 125.8059 - loglik: -1.2452e+02 - logprior: -1.2871e+00
Epoch 9/10
19/19 - 1s - loss: 125.7306 - loglik: -1.2445e+02 - logprior: -1.2823e+00
Epoch 10/10
19/19 - 1s - loss: 125.5841 - loglik: -1.2430e+02 - logprior: -1.2793e+00
Fitted a model with MAP estimate = -125.5565
expansions: [(11, 5), (12, 2), (13, 1), (18, 1), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.6159 - loglik: -1.2957e+02 - logprior: -4.0495e+00
Epoch 2/2
19/19 - 1s - loss: 126.9200 - loglik: -1.2476e+02 - logprior: -2.1605e+00
Fitted a model with MAP estimate = -124.5098
expansions: []
discards: [12 13 39 40 49]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.7102 - loglik: -1.2336e+02 - logprior: -3.3506e+00
Epoch 2/2
19/19 - 1s - loss: 123.2708 - loglik: -1.2192e+02 - logprior: -1.3554e+00
Fitted a model with MAP estimate = -122.8807
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.7542 - loglik: -1.2255e+02 - logprior: -3.2030e+00
Epoch 2/10
19/19 - 1s - loss: 123.1546 - loglik: -1.2181e+02 - logprior: -1.3443e+00
Epoch 3/10
19/19 - 1s - loss: 122.6092 - loglik: -1.2137e+02 - logprior: -1.2413e+00
Epoch 4/10
19/19 - 1s - loss: 122.4118 - loglik: -1.2121e+02 - logprior: -1.1982e+00
Epoch 5/10
19/19 - 1s - loss: 122.2151 - loglik: -1.2105e+02 - logprior: -1.1616e+00
Epoch 6/10
19/19 - 1s - loss: 121.9146 - loglik: -1.2077e+02 - logprior: -1.1457e+00
Epoch 7/10
19/19 - 1s - loss: 121.8462 - loglik: -1.2071e+02 - logprior: -1.1353e+00
Epoch 8/10
19/19 - 1s - loss: 121.7902 - loglik: -1.2068e+02 - logprior: -1.1137e+00
Epoch 9/10
19/19 - 1s - loss: 122.0347 - loglik: -1.2092e+02 - logprior: -1.1172e+00
Fitted a model with MAP estimate = -121.6601
Time for alignment: 46.8208
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 164.9723 - loglik: -1.6178e+02 - logprior: -3.1894e+00
Epoch 2/10
19/19 - 1s - loss: 136.5656 - loglik: -1.3517e+02 - logprior: -1.3945e+00
Epoch 3/10
19/19 - 1s - loss: 128.0245 - loglik: -1.2653e+02 - logprior: -1.4920e+00
Epoch 4/10
19/19 - 1s - loss: 126.5805 - loglik: -1.2525e+02 - logprior: -1.3275e+00
Epoch 5/10
19/19 - 1s - loss: 126.0469 - loglik: -1.2472e+02 - logprior: -1.3270e+00
Epoch 6/10
19/19 - 1s - loss: 125.9644 - loglik: -1.2466e+02 - logprior: -1.3038e+00
Epoch 7/10
19/19 - 1s - loss: 125.7986 - loglik: -1.2450e+02 - logprior: -1.2938e+00
Epoch 8/10
19/19 - 1s - loss: 125.8074 - loglik: -1.2452e+02 - logprior: -1.2868e+00
Fitted a model with MAP estimate = -125.6400
expansions: [(11, 5), (12, 2), (13, 1), (18, 1), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.4748 - loglik: -1.2942e+02 - logprior: -4.0569e+00
Epoch 2/2
19/19 - 1s - loss: 126.7258 - loglik: -1.2458e+02 - logprior: -2.1475e+00
Fitted a model with MAP estimate = -124.1525
expansions: []
discards: [12 13 39 40 49]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 126.5311 - loglik: -1.2321e+02 - logprior: -3.3237e+00
Epoch 2/2
19/19 - 1s - loss: 123.2755 - loglik: -1.2191e+02 - logprior: -1.3619e+00
Fitted a model with MAP estimate = -122.8707
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.7277 - loglik: -1.2252e+02 - logprior: -3.2072e+00
Epoch 2/10
19/19 - 1s - loss: 123.1490 - loglik: -1.2180e+02 - logprior: -1.3457e+00
Epoch 3/10
19/19 - 1s - loss: 122.7639 - loglik: -1.2152e+02 - logprior: -1.2480e+00
Epoch 4/10
19/19 - 1s - loss: 122.3829 - loglik: -1.2118e+02 - logprior: -1.1980e+00
Epoch 5/10
19/19 - 1s - loss: 122.0477 - loglik: -1.2088e+02 - logprior: -1.1649e+00
Epoch 6/10
19/19 - 1s - loss: 122.1118 - loglik: -1.2096e+02 - logprior: -1.1502e+00
Fitted a model with MAP estimate = -121.8542
Time for alignment: 39.5934
Computed alignments with likelihoods: ['-121.6649', '-121.7731', '-121.7125', '-121.6601', '-121.8542']
Best model has likelihood: -121.6601  (prior= -1.1052 )
time for generating output: 0.0949
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00084.projection.fasta
SP score = 0.8644578313253012
Training of 5 independent models on file PF00232.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd124d31370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fcb68f3e940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd188620a00>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 56s - loss: 905.7786 - loglik: -9.0438e+02 - logprior: -1.3938e+00
Epoch 2/10
39/39 - 57s - loss: 723.1048 - loglik: -7.2195e+02 - logprior: -1.1519e+00
Epoch 3/10
39/39 - 59s - loss: 707.8534 - loglik: -7.0663e+02 - logprior: -1.2186e+00
Epoch 4/10
39/39 - 63s - loss: 704.5649 - loglik: -7.0339e+02 - logprior: -1.1711e+00
Epoch 5/10
39/39 - 69s - loss: 702.5762 - loglik: -7.0138e+02 - logprior: -1.1978e+00
Epoch 6/10
39/39 - 68s - loss: 702.4157 - loglik: -7.0124e+02 - logprior: -1.1749e+00
Epoch 7/10
39/39 - 66s - loss: 701.8383 - loglik: -7.0064e+02 - logprior: -1.1953e+00
Epoch 8/10
39/39 - 68s - loss: 702.2131 - loglik: -7.0102e+02 - logprior: -1.1915e+00
Fitted a model with MAP estimate = -700.7557
expansions: [(0, 4), (43, 1), (62, 1), (134, 1), (139, 1), (146, 1), (163, 1), (164, 1), (168, 1), (174, 1), (176, 9), (177, 1), (187, 1), (189, 1), (190, 1), (191, 5), (192, 2), (195, 1), (196, 3), (197, 1), (199, 1), (200, 2), (201, 2), (202, 1), (205, 1), (207, 1), (214, 1), (216, 1), (217, 1), (221, 2), (222, 5), (223, 1), (226, 1), (227, 3), (228, 1), (229, 1), (230, 2), (231, 1), (242, 1), (246, 1), (247, 2), (248, 3), (250, 2), (251, 5), (252, 1), (256, 1), (257, 1), (269, 1), (271, 1), (272, 1), (286, 1), (288, 1), (289, 2), (290, 2), (291, 1), (302, 1), (303, 2), (304, 1), (305, 1), (313, 1), (326, 1), (329, 1), (339, 1), (349, 1), (355, 6)]
discards: [  1 127]
Re-initialized the encoder parameters.
Fitting a model of length 462 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 125s - loss: 677.6298 - loglik: -6.7550e+02 - logprior: -2.1283e+00
Epoch 2/2
39/39 - 118s - loss: 659.2769 - loglik: -6.5855e+02 - logprior: -7.2711e-01
Fitted a model with MAP estimate = -656.4973
expansions: []
discards: [  2   3   4   5 188 189 190 191 192 240 273 292 318 319 320 321 460]
Re-initialized the encoder parameters.
Fitting a model of length 445 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 106s - loss: 665.4850 - loglik: -6.6410e+02 - logprior: -1.3870e+00
Epoch 2/2
39/39 - 110s - loss: 661.1981 - loglik: -6.6109e+02 - logprior: -1.0755e-01
Fitted a model with MAP estimate = -659.3728
expansions: [(0, 4), (303, 2)]
discards: [ 41 210 305]
Re-initialized the encoder parameters.
Fitting a model of length 448 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 105s - loss: 666.1216 - loglik: -6.6438e+02 - logprior: -1.7418e+00
Epoch 2/10
39/39 - 110s - loss: 661.7161 - loglik: -6.6175e+02 - logprior: 0.0378
Epoch 3/10
39/39 - 112s - loss: 659.9016 - loglik: -6.6017e+02 - logprior: 0.2643
Epoch 4/10
39/39 - 110s - loss: 659.1769 - loglik: -6.5947e+02 - logprior: 0.2937
Epoch 5/10
39/39 - 107s - loss: 658.4594 - loglik: -6.5879e+02 - logprior: 0.3275
Epoch 6/10
39/39 - 109s - loss: 657.2582 - loglik: -6.5769e+02 - logprior: 0.4338
Epoch 7/10
39/39 - 117s - loss: 656.9438 - loglik: -6.5764e+02 - logprior: 0.6937
Epoch 8/10
39/39 - 115s - loss: 657.4026 - loglik: -6.5802e+02 - logprior: 0.6181
Fitted a model with MAP estimate = -656.1519
Time for alignment: 2330.2816
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 70s - loss: 903.6646 - loglik: -9.0227e+02 - logprior: -1.3915e+00
Epoch 2/10
39/39 - 69s - loss: 725.3530 - loglik: -7.2428e+02 - logprior: -1.0689e+00
Epoch 3/10
39/39 - 69s - loss: 708.3956 - loglik: -7.0733e+02 - logprior: -1.0678e+00
Epoch 4/10
39/39 - 72s - loss: 705.0475 - loglik: -7.0409e+02 - logprior: -9.6041e-01
Epoch 5/10
39/39 - 79s - loss: 704.8683 - loglik: -7.0392e+02 - logprior: -9.5112e-01
Epoch 6/10
39/39 - 81s - loss: 703.1525 - loglik: -7.0217e+02 - logprior: -9.8484e-01
Epoch 7/10
39/39 - 64s - loss: 703.0730 - loglik: -7.0209e+02 - logprior: -9.8207e-01
Epoch 8/10
39/39 - 56s - loss: 703.2828 - loglik: -7.0229e+02 - logprior: -9.8799e-01
Fitted a model with MAP estimate = -702.0902
expansions: [(0, 3), (43, 1), (123, 1), (134, 1), (143, 1), (146, 1), (163, 1), (164, 1), (175, 1), (176, 10), (177, 1), (187, 1), (189, 1), (190, 1), (191, 5), (192, 2), (195, 1), (196, 1), (197, 1), (198, 2), (199, 1), (200, 1), (202, 1), (203, 1), (206, 1), (208, 1), (216, 1), (217, 1), (221, 2), (222, 7), (225, 1), (226, 2), (227, 3), (228, 2), (229, 1), (240, 1), (244, 1), (245, 3), (246, 2), (248, 1), (249, 5), (251, 1), (254, 1), (255, 1), (266, 1), (269, 1), (270, 1), (286, 2), (287, 4), (288, 2), (301, 1), (304, 1), (306, 1), (326, 1), (329, 1), (349, 1), (355, 6)]
discards: [  2 127]
Re-initialized the encoder parameters.
Fitting a model of length 455 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 96s - loss: 680.5084 - loglik: -6.7824e+02 - logprior: -2.2714e+00
Epoch 2/2
39/39 - 101s - loss: 663.1830 - loglik: -6.6245e+02 - logprior: -7.3734e-01
Fitted a model with MAP estimate = -659.9930
expansions: [(280, 1), (372, 1)]
discards: [  2 187 188 189 190 191 218 270 287 313 314 315 453]
Re-initialized the encoder parameters.
Fitting a model of length 444 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 116s - loss: 669.1226 - loglik: -6.6775e+02 - logprior: -1.3764e+00
Epoch 2/2
39/39 - 105s - loss: 665.3509 - loglik: -6.6494e+02 - logprior: -4.0618e-01
Fitted a model with MAP estimate = -662.9677
expansions: [(302, 2)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 445 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 120s - loss: 667.2610 - loglik: -6.6617e+02 - logprior: -1.0960e+00
Epoch 2/10
39/39 - 108s - loss: 664.1095 - loglik: -6.6419e+02 - logprior: 0.0846
Epoch 3/10
39/39 - 108s - loss: 662.9200 - loglik: -6.6318e+02 - logprior: 0.2588
Epoch 4/10
39/39 - 117s - loss: 662.0671 - loglik: -6.6237e+02 - logprior: 0.3054
Epoch 5/10
39/39 - 115s - loss: 660.5219 - loglik: -6.6090e+02 - logprior: 0.3823
Epoch 6/10
39/39 - 107s - loss: 660.4346 - loglik: -6.6103e+02 - logprior: 0.5978
Epoch 7/10
39/39 - 115s - loss: 660.3051 - loglik: -6.6106e+02 - logprior: 0.7505
Epoch 8/10
39/39 - 122s - loss: 660.1544 - loglik: -6.6080e+02 - logprior: 0.6464
Epoch 9/10
39/39 - 117s - loss: 658.9862 - loglik: -6.5993e+02 - logprior: 0.9481
Epoch 10/10
39/39 - 107s - loss: 659.4500 - loglik: -6.6054e+02 - logprior: 1.0900
Fitted a model with MAP estimate = -658.4111
Time for alignment: 2586.4860
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 75s - loss: 899.0635 - loglik: -8.9761e+02 - logprior: -1.4537e+00
Epoch 2/10
39/39 - 77s - loss: 718.4272 - loglik: -7.1702e+02 - logprior: -1.4093e+00
Epoch 3/10
39/39 - 62s - loss: 706.6835 - loglik: -7.0499e+02 - logprior: -1.6912e+00
Epoch 4/10
39/39 - 59s - loss: 704.8591 - loglik: -7.0316e+02 - logprior: -1.7035e+00
Epoch 5/10
39/39 - 56s - loss: 702.4614 - loglik: -7.0072e+02 - logprior: -1.7377e+00
Epoch 6/10
39/39 - 57s - loss: 702.6372 - loglik: -7.0084e+02 - logprior: -1.7976e+00
Fitted a model with MAP estimate = -701.2168
expansions: [(0, 3), (43, 1), (51, 1), (134, 1), (143, 1), (146, 1), (162, 2), (163, 2), (166, 1), (167, 1), (171, 9), (172, 2), (173, 1), (174, 1), (182, 1), (184, 2), (185, 2), (186, 4), (187, 1), (188, 1), (191, 1), (192, 1), (193, 1), (194, 1), (196, 1), (197, 1), (199, 1), (200, 1), (202, 1), (204, 1), (205, 1), (206, 1), (215, 1), (220, 1), (221, 6), (224, 1), (225, 3), (226, 1), (227, 1), (228, 2), (229, 1), (240, 1), (244, 1), (245, 2), (246, 2), (249, 6), (251, 1), (254, 1), (255, 1), (269, 1), (270, 1), (271, 1), (286, 1), (288, 2), (289, 2), (291, 1), (292, 1), (301, 1), (302, 2), (303, 1), (304, 1), (306, 1), (325, 1), (329, 1), (338, 1), (348, 1), (351, 1), (352, 2)]
discards: [  2 127]
Re-initialized the encoder parameters.
Fitting a model of length 458 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 95s - loss: 675.5289 - loglik: -6.7372e+02 - logprior: -1.8073e+00
Epoch 2/2
39/39 - 92s - loss: 659.0333 - loglik: -6.5846e+02 - logprior: -5.7381e-01
Fitted a model with MAP estimate = -656.0939
expansions: [(315, 1), (455, 1)]
discards: [  2   3 187 188 189 190 191 192 193 215 216 217 291 323 324 325 373]
Re-initialized the encoder parameters.
Fitting a model of length 443 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 79s - loss: 667.7692 - loglik: -6.6633e+02 - logprior: -1.4373e+00
Epoch 2/2
39/39 - 76s - loss: 663.0122 - loglik: -6.6303e+02 - logprior: 0.0183
Fitted a model with MAP estimate = -661.1945
expansions: [(184, 1), (185, 7), (307, 1), (308, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 456 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 78s - loss: 662.9937 - loglik: -6.6178e+02 - logprior: -1.2155e+00
Epoch 2/10
39/39 - 74s - loss: 658.3155 - loglik: -6.5818e+02 - logprior: -1.3639e-01
Epoch 3/10
39/39 - 73s - loss: 656.6674 - loglik: -6.5652e+02 - logprior: -1.4621e-01
Epoch 4/10
39/39 - 74s - loss: 655.9020 - loglik: -6.5598e+02 - logprior: 0.0788
Epoch 5/10
39/39 - 75s - loss: 654.6207 - loglik: -6.5479e+02 - logprior: 0.1660
Epoch 6/10
39/39 - 77s - loss: 654.1444 - loglik: -6.5440e+02 - logprior: 0.2583
Epoch 7/10
39/39 - 77s - loss: 654.0864 - loglik: -6.5451e+02 - logprior: 0.4227
Epoch 8/10
39/39 - 79s - loss: 652.8344 - loglik: -6.5345e+02 - logprior: 0.6171
Epoch 9/10
39/39 - 82s - loss: 652.7683 - loglik: -6.5348e+02 - logprior: 0.7113
Epoch 10/10
39/39 - 86s - loss: 653.2703 - loglik: -6.5393e+02 - logprior: 0.6607
Fitted a model with MAP estimate = -652.1238
Time for alignment: 1872.5776
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 58s - loss: 899.8664 - loglik: -8.9847e+02 - logprior: -1.3929e+00
Epoch 2/10
39/39 - 58s - loss: 718.4349 - loglik: -7.1730e+02 - logprior: -1.1367e+00
Epoch 3/10
39/39 - 59s - loss: 706.4473 - loglik: -7.0517e+02 - logprior: -1.2819e+00
Epoch 4/10
39/39 - 68s - loss: 703.6069 - loglik: -7.0235e+02 - logprior: -1.2589e+00
Epoch 5/10
39/39 - 75s - loss: 700.7983 - loglik: -6.9924e+02 - logprior: -1.5590e+00
Epoch 6/10
39/39 - 75s - loss: 700.4428 - loglik: -6.9898e+02 - logprior: -1.4594e+00
Epoch 7/10
39/39 - 68s - loss: 699.9716 - loglik: -6.9854e+02 - logprior: -1.4325e+00
Epoch 8/10
39/39 - 69s - loss: 700.1748 - loglik: -6.9875e+02 - logprior: -1.4221e+00
Fitted a model with MAP estimate = -698.9479
expansions: [(0, 3), (43, 1), (167, 1), (173, 1), (181, 11), (182, 1), (192, 1), (194, 2), (195, 6), (196, 1), (199, 1), (200, 3), (201, 2), (202, 1), (203, 1), (205, 1), (206, 1), (211, 1), (214, 1), (218, 1), (220, 1), (221, 1), (225, 2), (226, 5), (227, 1), (230, 1), (231, 1), (232, 1), (233, 1), (234, 1), (235, 1), (246, 1), (247, 2), (248, 1), (249, 2), (250, 4), (251, 10), (252, 2), (253, 1), (269, 1), (272, 1), (285, 1), (286, 2), (287, 4), (288, 1), (289, 1), (298, 1), (300, 1), (302, 1), (305, 1), (325, 1), (328, 1), (348, 1), (350, 2), (352, 2)]
discards: [ 1 87]
Re-initialized the encoder parameters.
Fitting a model of length 455 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 104s - loss: 674.6469 - loglik: -6.7255e+02 - logprior: -2.0998e+00
Epoch 2/2
39/39 - 125s - loss: 659.2303 - loglik: -6.5870e+02 - logprior: -5.3262e-01
Fitted a model with MAP estimate = -656.5569
expansions: [(310, 4), (320, 1), (373, 1)]
discards: [  2   3 187 188 189 190 191 192 271 311 312 313 314 315 316 317 318]
Re-initialized the encoder parameters.
Fitting a model of length 444 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 110s - loss: 665.5314 - loglik: -6.6409e+02 - logprior: -1.4449e+00
Epoch 2/2
39/39 - 98s - loss: 661.6074 - loglik: -6.6143e+02 - logprior: -1.8133e-01
Fitted a model with MAP estimate = -659.7742
expansions: [(210, 2), (300, 3), (301, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 451 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 126s - loss: 662.1438 - loglik: -6.6085e+02 - logprior: -1.2907e+00
Epoch 2/10
39/39 - 128s - loss: 658.6502 - loglik: -6.5874e+02 - logprior: 0.0944
Epoch 3/10
39/39 - 114s - loss: 657.6945 - loglik: -6.5795e+02 - logprior: 0.2586
Epoch 4/10
39/39 - 106s - loss: 656.3024 - loglik: -6.5647e+02 - logprior: 0.1710
Epoch 5/10
39/39 - 107s - loss: 655.7381 - loglik: -6.5606e+02 - logprior: 0.3265
Epoch 6/10
39/39 - 106s - loss: 654.1895 - loglik: -6.5479e+02 - logprior: 0.5998
Epoch 7/10
39/39 - 128s - loss: 655.1797 - loglik: -6.5571e+02 - logprior: 0.5352
Fitted a model with MAP estimate = -653.7021
Time for alignment: 2292.7163
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 81s - loss: 900.9865 - loglik: -8.9952e+02 - logprior: -1.4685e+00
Epoch 2/10
39/39 - 81s - loss: 725.0394 - loglik: -7.2395e+02 - logprior: -1.0870e+00
Epoch 3/10
39/39 - 74s - loss: 711.7142 - loglik: -7.1036e+02 - logprior: -1.3571e+00
Epoch 4/10
39/39 - 74s - loss: 708.2024 - loglik: -7.0685e+02 - logprior: -1.3532e+00
Epoch 5/10
39/39 - 65s - loss: 706.6235 - loglik: -7.0503e+02 - logprior: -1.5899e+00
Epoch 6/10
39/39 - 72s - loss: 706.5847 - loglik: -7.0489e+02 - logprior: -1.6958e+00
Epoch 7/10
39/39 - 65s - loss: 705.5640 - loglik: -7.0398e+02 - logprior: -1.5860e+00
Epoch 8/10
39/39 - 74s - loss: 705.2261 - loglik: -7.0370e+02 - logprior: -1.5263e+00
Epoch 9/10
39/39 - 83s - loss: 705.7797 - loglik: -7.0409e+02 - logprior: -1.6898e+00
Fitted a model with MAP estimate = -704.7180
expansions: [(0, 3), (87, 2), (135, 1), (137, 1), (163, 1), (164, 1), (165, 1), (177, 1), (178, 10), (179, 1), (192, 2), (193, 6), (194, 1), (197, 1), (198, 1), (199, 1), (200, 1), (202, 1), (203, 2), (204, 2), (205, 1), (208, 1), (210, 1), (212, 1), (216, 2), (217, 1), (218, 2), (222, 2), (223, 8), (225, 1), (226, 2), (227, 5), (228, 1), (239, 1), (241, 1), (242, 1), (243, 2), (244, 2), (247, 6), (249, 1), (253, 1), (254, 1), (266, 1), (268, 1), (269, 1), (270, 1), (282, 1), (284, 1), (286, 2), (287, 1), (289, 1), (300, 1), (302, 2), (303, 2), (305, 2), (325, 1), (328, 1), (348, 1), (351, 1), (352, 2), (355, 2)]
discards: [  1   2 128]
Re-initialized the encoder parameters.
Fitting a model of length 461 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 131s - loss: 678.3790 - loglik: -6.7614e+02 - logprior: -2.2395e+00
Epoch 2/2
39/39 - 115s - loss: 660.1523 - loglik: -6.5944e+02 - logprior: -7.1392e-01
Fitted a model with MAP estimate = -657.7315
expansions: [(284, 1), (314, 2), (322, 1), (323, 1), (456, 1)]
discards: [  2   3  89 187 188 189 190 191 192 239 262 275 316 324 325 326 372 376
 402 457 458 459 460]
Re-initialized the encoder parameters.
Fitting a model of length 444 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 115s - loss: 667.9704 - loglik: -6.6664e+02 - logprior: -1.3354e+00
Epoch 2/2
39/39 - 124s - loss: 662.3226 - loglik: -6.6227e+02 - logprior: -5.5166e-02
Fitted a model with MAP estimate = -660.9129
expansions: [(0, 3), (364, 1), (444, 3)]
discards: [307 308 309 310]
Re-initialized the encoder parameters.
Fitting a model of length 447 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 90s - loss: 665.7402 - loglik: -6.6408e+02 - logprior: -1.6602e+00
Epoch 2/10
39/39 - 87s - loss: 662.0139 - loglik: -6.6155e+02 - logprior: -4.6065e-01
Epoch 3/10
39/39 - 88s - loss: 660.8354 - loglik: -6.6063e+02 - logprior: -2.0930e-01
Epoch 4/10
39/39 - 87s - loss: 658.5798 - loglik: -6.5879e+02 - logprior: 0.2118
Epoch 5/10
39/39 - 83s - loss: 658.9202 - loglik: -6.5860e+02 - logprior: -3.2354e-01
Fitted a model with MAP estimate = -657.4458
Time for alignment: 2074.2456
Computed alignments with likelihoods: ['-656.1519', '-658.4111', '-652.1238', '-653.7021', '-657.4458']
Best model has likelihood: -652.1238  (prior= 0.9865 )
time for generating output: 0.4177
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00232.projection.fasta
SP score = 0.8311818384153127
Training of 5 independent models on file PF13522.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1c42b9340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd32d2fdf10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1264c3c10>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 408.8696 - loglik: -4.0597e+02 - logprior: -2.9023e+00
Epoch 2/10
19/19 - 4s - loss: 328.4895 - loglik: -3.2755e+02 - logprior: -9.4291e-01
Epoch 3/10
19/19 - 4s - loss: 299.2982 - loglik: -2.9823e+02 - logprior: -1.0666e+00
Epoch 4/10
19/19 - 4s - loss: 293.4113 - loglik: -2.9236e+02 - logprior: -1.0500e+00
Epoch 5/10
19/19 - 4s - loss: 291.4841 - loglik: -2.9044e+02 - logprior: -1.0484e+00
Epoch 6/10
19/19 - 4s - loss: 291.5002 - loglik: -2.9049e+02 - logprior: -1.0137e+00
Fitted a model with MAP estimate = -290.4788
expansions: [(0, 7), (7, 2), (8, 3), (9, 1), (10, 2), (34, 2), (35, 1), (37, 1), (49, 1), (52, 1), (55, 1), (58, 1), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (87, 1), (96, 1), (97, 1), (99, 1), (104, 1), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 158 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 291.5760 - loglik: -2.8779e+02 - logprior: -3.7830e+00
Epoch 2/2
19/19 - 5s - loss: 279.8234 - loglik: -2.7877e+02 - logprior: -1.0567e+00
Fitted a model with MAP estimate = -278.1584
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  21  23 101 102]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 284.8357 - loglik: -2.8098e+02 - logprior: -3.8524e+00
Epoch 2/2
19/19 - 5s - loss: 279.1942 - loglik: -2.7812e+02 - logprior: -1.0765e+00
Fitted a model with MAP estimate = -278.4095
expansions: [(0, 5)]
discards: [0 1 2 4]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 282.2368 - loglik: -2.7916e+02 - logprior: -3.0745e+00
Epoch 2/10
19/19 - 5s - loss: 278.6521 - loglik: -2.7773e+02 - logprior: -9.2642e-01
Epoch 3/10
19/19 - 5s - loss: 278.3002 - loglik: -2.7755e+02 - logprior: -7.5501e-01
Epoch 4/10
19/19 - 5s - loss: 276.6786 - loglik: -2.7601e+02 - logprior: -6.6746e-01
Epoch 5/10
19/19 - 5s - loss: 276.5292 - loglik: -2.7593e+02 - logprior: -5.9654e-01
Epoch 6/10
19/19 - 5s - loss: 275.9086 - loglik: -2.7537e+02 - logprior: -5.3741e-01
Epoch 7/10
19/19 - 5s - loss: 275.0964 - loglik: -2.7458e+02 - logprior: -5.1154e-01
Epoch 8/10
19/19 - 5s - loss: 274.5634 - loglik: -2.7411e+02 - logprior: -4.5283e-01
Epoch 9/10
19/19 - 5s - loss: 274.5428 - loglik: -2.7413e+02 - logprior: -4.1339e-01
Epoch 10/10
19/19 - 5s - loss: 274.9837 - loglik: -2.7460e+02 - logprior: -3.8827e-01
Fitted a model with MAP estimate = -274.1577
Time for alignment: 138.6239
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 408.9509 - loglik: -4.0605e+02 - logprior: -2.9042e+00
Epoch 2/10
19/19 - 4s - loss: 327.2836 - loglik: -3.2634e+02 - logprior: -9.4384e-01
Epoch 3/10
19/19 - 4s - loss: 297.8179 - loglik: -2.9676e+02 - logprior: -1.0542e+00
Epoch 4/10
19/19 - 4s - loss: 292.7808 - loglik: -2.9175e+02 - logprior: -1.0339e+00
Epoch 5/10
19/19 - 4s - loss: 291.1702 - loglik: -2.9015e+02 - logprior: -1.0198e+00
Epoch 6/10
19/19 - 4s - loss: 290.9259 - loglik: -2.8993e+02 - logprior: -9.9401e-01
Epoch 7/10
19/19 - 4s - loss: 290.3463 - loglik: -2.8939e+02 - logprior: -9.6076e-01
Epoch 8/10
19/19 - 4s - loss: 289.8850 - loglik: -2.8893e+02 - logprior: -9.5350e-01
Epoch 9/10
19/19 - 4s - loss: 290.0580 - loglik: -2.8912e+02 - logprior: -9.3859e-01
Fitted a model with MAP estimate = -289.4681
expansions: [(0, 7), (7, 2), (8, 3), (9, 1), (10, 2), (27, 1), (35, 1), (37, 1), (49, 1), (53, 1), (55, 1), (58, 1), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (97, 1), (98, 1), (99, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 158 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 291.5933 - loglik: -2.8775e+02 - logprior: -3.8453e+00
Epoch 2/2
19/19 - 5s - loss: 279.4141 - loglik: -2.7837e+02 - logprior: -1.0446e+00
Fitted a model with MAP estimate = -277.8060
expansions: [(0, 4)]
discards: [  1   2   3   4   6  21  23 100 101]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 284.7029 - loglik: -2.8087e+02 - logprior: -3.8296e+00
Epoch 2/2
19/19 - 5s - loss: 279.9497 - loglik: -2.7880e+02 - logprior: -1.1516e+00
Fitted a model with MAP estimate = -278.6892
expansions: [(0, 5)]
discards: [  0   1   2   3   4 142]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 282.4574 - loglik: -2.7938e+02 - logprior: -3.0767e+00
Epoch 2/10
19/19 - 5s - loss: 278.7656 - loglik: -2.7791e+02 - logprior: -8.5888e-01
Epoch 3/10
19/19 - 5s - loss: 277.2903 - loglik: -2.7663e+02 - logprior: -6.5918e-01
Epoch 4/10
19/19 - 5s - loss: 276.5473 - loglik: -2.7592e+02 - logprior: -6.2726e-01
Epoch 5/10
19/19 - 5s - loss: 276.2809 - loglik: -2.7570e+02 - logprior: -5.8589e-01
Epoch 6/10
19/19 - 5s - loss: 274.9064 - loglik: -2.7434e+02 - logprior: -5.6403e-01
Epoch 7/10
19/19 - 5s - loss: 274.6336 - loglik: -2.7410e+02 - logprior: -5.2919e-01
Epoch 8/10
19/19 - 5s - loss: 273.7769 - loglik: -2.7327e+02 - logprior: -5.0779e-01
Epoch 9/10
19/19 - 5s - loss: 273.9763 - loglik: -2.7350e+02 - logprior: -4.8084e-01
Fitted a model with MAP estimate = -273.8280
Time for alignment: 143.5575
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 408.8178 - loglik: -4.0591e+02 - logprior: -2.9087e+00
Epoch 2/10
19/19 - 4s - loss: 325.9794 - loglik: -3.2503e+02 - logprior: -9.5251e-01
Epoch 3/10
19/19 - 4s - loss: 296.6966 - loglik: -2.9563e+02 - logprior: -1.0633e+00
Epoch 4/10
19/19 - 4s - loss: 290.8327 - loglik: -2.8982e+02 - logprior: -1.0151e+00
Epoch 5/10
19/19 - 4s - loss: 289.6744 - loglik: -2.8868e+02 - logprior: -9.9084e-01
Epoch 6/10
19/19 - 4s - loss: 289.5633 - loglik: -2.8860e+02 - logprior: -9.6267e-01
Epoch 7/10
19/19 - 4s - loss: 288.4138 - loglik: -2.8748e+02 - logprior: -9.3236e-01
Epoch 8/10
19/19 - 4s - loss: 287.8944 - loglik: -2.8696e+02 - logprior: -9.2977e-01
Epoch 9/10
19/19 - 4s - loss: 289.1159 - loglik: -2.8820e+02 - logprior: -9.1449e-01
Fitted a model with MAP estimate = -287.9112
expansions: [(0, 7), (7, 2), (8, 3), (9, 1), (10, 2), (34, 2), (35, 1), (37, 1), (49, 1), (55, 2), (56, 2), (58, 1), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (97, 1), (98, 1), (100, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 161 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 290.8189 - loglik: -2.8697e+02 - logprior: -3.8494e+00
Epoch 2/2
19/19 - 5s - loss: 278.6933 - loglik: -2.7764e+02 - logprior: -1.0505e+00
Fitted a model with MAP estimate = -276.8338
expansions: [(0, 4)]
discards: [  1   2   3   4   6  21  23  77  78 103 104 149]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 283.9881 - loglik: -2.8018e+02 - logprior: -3.8095e+00
Epoch 2/2
19/19 - 5s - loss: 278.3135 - loglik: -2.7721e+02 - logprior: -1.1016e+00
Fitted a model with MAP estimate = -277.2406
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 281.1404 - loglik: -2.7808e+02 - logprior: -3.0648e+00
Epoch 2/10
19/19 - 5s - loss: 276.9077 - loglik: -2.7607e+02 - logprior: -8.4201e-01
Epoch 3/10
19/19 - 5s - loss: 276.3492 - loglik: -2.7574e+02 - logprior: -6.0937e-01
Epoch 4/10
19/19 - 5s - loss: 275.5252 - loglik: -2.7497e+02 - logprior: -5.5706e-01
Epoch 5/10
19/19 - 5s - loss: 274.6196 - loglik: -2.7412e+02 - logprior: -4.9691e-01
Epoch 6/10
19/19 - 5s - loss: 274.4759 - loglik: -2.7401e+02 - logprior: -4.6725e-01
Epoch 7/10
19/19 - 5s - loss: 273.6624 - loglik: -2.7324e+02 - logprior: -4.2673e-01
Epoch 8/10
19/19 - 5s - loss: 272.5865 - loglik: -2.7218e+02 - logprior: -4.0250e-01
Epoch 9/10
19/19 - 5s - loss: 273.4853 - loglik: -2.7311e+02 - logprior: -3.7182e-01
Fitted a model with MAP estimate = -272.8943
Time for alignment: 147.6542
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 409.0167 - loglik: -4.0611e+02 - logprior: -2.9031e+00
Epoch 2/10
19/19 - 4s - loss: 326.8651 - loglik: -3.2592e+02 - logprior: -9.4356e-01
Epoch 3/10
19/19 - 4s - loss: 298.9645 - loglik: -2.9791e+02 - logprior: -1.0501e+00
Epoch 4/10
19/19 - 4s - loss: 294.2775 - loglik: -2.9324e+02 - logprior: -1.0402e+00
Epoch 5/10
19/19 - 4s - loss: 291.9950 - loglik: -2.9101e+02 - logprior: -9.8923e-01
Epoch 6/10
19/19 - 4s - loss: 291.9832 - loglik: -2.9101e+02 - logprior: -9.7260e-01
Epoch 7/10
19/19 - 4s - loss: 291.9234 - loglik: -2.9098e+02 - logprior: -9.4093e-01
Epoch 8/10
19/19 - 4s - loss: 291.3875 - loglik: -2.9044e+02 - logprior: -9.4413e-01
Epoch 9/10
19/19 - 4s - loss: 291.2144 - loglik: -2.9028e+02 - logprior: -9.3522e-01
Epoch 10/10
19/19 - 4s - loss: 290.8301 - loglik: -2.8989e+02 - logprior: -9.3725e-01
Fitted a model with MAP estimate = -290.6723
expansions: [(0, 7), (7, 2), (8, 3), (9, 1), (10, 2), (34, 2), (35, 1), (37, 1), (49, 1), (53, 1), (55, 1), (58, 1), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (96, 2), (97, 1), (99, 1), (104, 1), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 158 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 291.5820 - loglik: -2.8767e+02 - logprior: -3.9162e+00
Epoch 2/2
19/19 - 5s - loss: 279.4858 - loglik: -2.7841e+02 - logprior: -1.0744e+00
Fitted a model with MAP estimate = -277.9084
expansions: [(0, 4)]
discards: [  1   2   3   4   6  21  23 101 102]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 284.7555 - loglik: -2.8090e+02 - logprior: -3.8569e+00
Epoch 2/2
19/19 - 5s - loss: 279.8845 - loglik: -2.7872e+02 - logprior: -1.1617e+00
Fitted a model with MAP estimate = -278.3560
expansions: [(0, 4)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 282.2711 - loglik: -2.7916e+02 - logprior: -3.1119e+00
Epoch 2/10
19/19 - 5s - loss: 278.5042 - loglik: -2.7766e+02 - logprior: -8.4519e-01
Epoch 3/10
19/19 - 5s - loss: 278.1448 - loglik: -2.7747e+02 - logprior: -6.7647e-01
Epoch 4/10
19/19 - 5s - loss: 277.1351 - loglik: -2.7650e+02 - logprior: -6.3671e-01
Epoch 5/10
19/19 - 5s - loss: 276.2272 - loglik: -2.7565e+02 - logprior: -5.7932e-01
Epoch 6/10
19/19 - 5s - loss: 275.6674 - loglik: -2.7512e+02 - logprior: -5.4263e-01
Epoch 7/10
19/19 - 5s - loss: 274.8373 - loglik: -2.7433e+02 - logprior: -5.0659e-01
Epoch 8/10
19/19 - 5s - loss: 275.0227 - loglik: -2.7457e+02 - logprior: -4.5241e-01
Fitted a model with MAP estimate = -274.5177
Time for alignment: 144.5913
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 409.8590 - loglik: -4.0696e+02 - logprior: -2.9033e+00
Epoch 2/10
19/19 - 4s - loss: 327.9911 - loglik: -3.2705e+02 - logprior: -9.3746e-01
Epoch 3/10
19/19 - 4s - loss: 299.9746 - loglik: -2.9889e+02 - logprior: -1.0895e+00
Epoch 4/10
19/19 - 4s - loss: 294.9151 - loglik: -2.9384e+02 - logprior: -1.0722e+00
Epoch 5/10
19/19 - 4s - loss: 293.0681 - loglik: -2.9204e+02 - logprior: -1.0298e+00
Epoch 6/10
19/19 - 4s - loss: 292.3616 - loglik: -2.9136e+02 - logprior: -1.0062e+00
Epoch 7/10
19/19 - 4s - loss: 291.3978 - loglik: -2.9041e+02 - logprior: -9.8386e-01
Epoch 8/10
19/19 - 4s - loss: 291.9971 - loglik: -2.9101e+02 - logprior: -9.8262e-01
Fitted a model with MAP estimate = -291.0406
expansions: [(0, 6), (6, 1), (8, 1), (9, 5), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (53, 1), (55, 1), (58, 1), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (96, 2), (99, 1), (104, 1), (113, 1), (115, 1)]
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 290.8658 - loglik: -2.8802e+02 - logprior: -2.8444e+00
Epoch 2/2
19/19 - 5s - loss: 281.0696 - loglik: -2.8015e+02 - logprior: -9.1687e-01
Fitted a model with MAP estimate = -279.6139
expansions: [(0, 5), (126, 1)]
discards: [ 1  2  3  4  5 20 97 98]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 285.5699 - loglik: -2.8200e+02 - logprior: -3.5730e+00
Epoch 2/2
19/19 - 5s - loss: 280.5582 - loglik: -2.7960e+02 - logprior: -9.5556e-01
Fitted a model with MAP estimate = -279.2404
expansions: [(0, 4)]
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 284.3799 - loglik: -2.8072e+02 - logprior: -3.6552e+00
Epoch 2/10
19/19 - 5s - loss: 279.9191 - loglik: -2.7880e+02 - logprior: -1.1226e+00
Epoch 3/10
19/19 - 5s - loss: 278.9721 - loglik: -2.7806e+02 - logprior: -9.1097e-01
Epoch 4/10
19/19 - 5s - loss: 278.3695 - loglik: -2.7759e+02 - logprior: -7.7591e-01
Epoch 5/10
19/19 - 5s - loss: 278.0623 - loglik: -2.7736e+02 - logprior: -6.9790e-01
Epoch 6/10
19/19 - 5s - loss: 276.3717 - loglik: -2.7572e+02 - logprior: -6.4786e-01
Epoch 7/10
19/19 - 5s - loss: 276.3033 - loglik: -2.7568e+02 - logprior: -6.2369e-01
Epoch 8/10
19/19 - 5s - loss: 275.8140 - loglik: -2.7525e+02 - logprior: -5.6771e-01
Epoch 9/10
19/19 - 5s - loss: 276.1301 - loglik: -2.7559e+02 - logprior: -5.3699e-01
Fitted a model with MAP estimate = -275.4472
Time for alignment: 133.3848
Computed alignments with likelihoods: ['-274.1577', '-273.8280', '-272.8943', '-274.5177', '-275.4472']
Best model has likelihood: -272.8943  (prior= -0.3612 )
time for generating output: 0.1599
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13522.projection.fasta
SP score = 0.6902540925453924
Training of 5 independent models on file PF00037.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fcb70f16eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1643ef220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fcb70f203d0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 68.7059 - loglik: -6.5337e+01 - logprior: -3.3690e+00
Epoch 2/10
19/19 - 0s - loss: 51.0539 - loglik: -4.9681e+01 - logprior: -1.3729e+00
Epoch 3/10
19/19 - 0s - loss: 45.3975 - loglik: -4.3933e+01 - logprior: -1.4646e+00
Epoch 4/10
19/19 - 0s - loss: 43.8879 - loglik: -4.2437e+01 - logprior: -1.4505e+00
Epoch 5/10
19/19 - 0s - loss: 43.4551 - loglik: -4.2024e+01 - logprior: -1.4309e+00
Epoch 6/10
19/19 - 0s - loss: 43.2507 - loglik: -4.1831e+01 - logprior: -1.4193e+00
Epoch 7/10
19/19 - 0s - loss: 43.0489 - loglik: -4.1642e+01 - logprior: -1.4066e+00
Epoch 8/10
19/19 - 0s - loss: 42.8949 - loglik: -4.1498e+01 - logprior: -1.3973e+00
Epoch 9/10
19/19 - 0s - loss: 42.7877 - loglik: -4.1397e+01 - logprior: -1.3907e+00
Epoch 10/10
19/19 - 0s - loss: 42.7762 - loglik: -4.1390e+01 - logprior: -1.3866e+00
Fitted a model with MAP estimate = -42.6800
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 47.3311 - loglik: -4.2543e+01 - logprior: -4.7880e+00
Epoch 2/2
19/19 - 0s - loss: 42.2877 - loglik: -4.0868e+01 - logprior: -1.4201e+00
Fitted a model with MAP estimate = -41.4589
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 44.1167 - loglik: -4.0734e+01 - logprior: -3.3827e+00
Epoch 2/2
19/19 - 0s - loss: 41.4741 - loglik: -4.0006e+01 - logprior: -1.4678e+00
Fitted a model with MAP estimate = -41.1853
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 43.7381 - loglik: -4.0436e+01 - logprior: -3.3024e+00
Epoch 2/10
19/19 - 0s - loss: 41.4414 - loglik: -3.9990e+01 - logprior: -1.4518e+00
Epoch 3/10
19/19 - 0s - loss: 40.9958 - loglik: -3.9656e+01 - logprior: -1.3402e+00
Epoch 4/10
19/19 - 0s - loss: 40.8446 - loglik: -3.9552e+01 - logprior: -1.2926e+00
Epoch 5/10
19/19 - 0s - loss: 40.6001 - loglik: -3.9327e+01 - logprior: -1.2730e+00
Epoch 6/10
19/19 - 0s - loss: 40.4119 - loglik: -3.9157e+01 - logprior: -1.2545e+00
Epoch 7/10
19/19 - 0s - loss: 40.2985 - loglik: -3.9053e+01 - logprior: -1.2450e+00
Epoch 8/10
19/19 - 0s - loss: 40.2321 - loglik: -3.8994e+01 - logprior: -1.2384e+00
Epoch 9/10
19/19 - 0s - loss: 40.0387 - loglik: -3.8808e+01 - logprior: -1.2303e+00
Epoch 10/10
19/19 - 0s - loss: 40.1290 - loglik: -3.8904e+01 - logprior: -1.2253e+00
Fitted a model with MAP estimate = -39.9949
Time for alignment: 29.0177
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 68.6526 - loglik: -6.5285e+01 - logprior: -3.3673e+00
Epoch 2/10
19/19 - 0s - loss: 50.9263 - loglik: -4.9557e+01 - logprior: -1.3693e+00
Epoch 3/10
19/19 - 0s - loss: 45.4012 - loglik: -4.3939e+01 - logprior: -1.4619e+00
Epoch 4/10
19/19 - 0s - loss: 43.8710 - loglik: -4.2419e+01 - logprior: -1.4520e+00
Epoch 5/10
19/19 - 1s - loss: 43.4935 - loglik: -4.2065e+01 - logprior: -1.4281e+00
Epoch 6/10
19/19 - 0s - loss: 43.2826 - loglik: -4.1863e+01 - logprior: -1.4196e+00
Epoch 7/10
19/19 - 0s - loss: 43.0814 - loglik: -4.1679e+01 - logprior: -1.4022e+00
Epoch 8/10
19/19 - 0s - loss: 42.8654 - loglik: -4.1469e+01 - logprior: -1.3967e+00
Epoch 9/10
19/19 - 0s - loss: 42.8666 - loglik: -4.1471e+01 - logprior: -1.3959e+00
Fitted a model with MAP estimate = -42.7385
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 47.2818 - loglik: -4.2560e+01 - logprior: -4.7217e+00
Epoch 2/2
19/19 - 0s - loss: 42.2396 - loglik: -4.0808e+01 - logprior: -1.4313e+00
Fitted a model with MAP estimate = -41.4929
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 43.5692 - loglik: -4.0420e+01 - logprior: -3.1492e+00
Epoch 2/10
19/19 - 0s - loss: 41.3987 - loglik: -4.0036e+01 - logprior: -1.3625e+00
Epoch 3/10
19/19 - 0s - loss: 41.0616 - loglik: -3.9756e+01 - logprior: -1.3054e+00
Epoch 4/10
19/19 - 0s - loss: 40.7949 - loglik: -3.9529e+01 - logprior: -1.2656e+00
Epoch 5/10
19/19 - 0s - loss: 40.6919 - loglik: -3.9452e+01 - logprior: -1.2398e+00
Epoch 6/10
19/19 - 0s - loss: 40.5043 - loglik: -3.9275e+01 - logprior: -1.2288e+00
Epoch 7/10
19/19 - 0s - loss: 40.4068 - loglik: -3.9189e+01 - logprior: -1.2180e+00
Epoch 8/10
19/19 - 0s - loss: 40.3514 - loglik: -3.9142e+01 - logprior: -1.2092e+00
Epoch 9/10
19/19 - 0s - loss: 40.2585 - loglik: -3.9055e+01 - logprior: -1.2039e+00
Epoch 10/10
19/19 - 0s - loss: 40.2227 - loglik: -3.9026e+01 - logprior: -1.1972e+00
Fitted a model with MAP estimate = -40.1465
Time for alignment: 23.0356
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 68.6539 - loglik: -6.5289e+01 - logprior: -3.3653e+00
Epoch 2/10
19/19 - 0s - loss: 50.8731 - loglik: -4.9521e+01 - logprior: -1.3524e+00
Epoch 3/10
19/19 - 0s - loss: 45.4411 - loglik: -4.3992e+01 - logprior: -1.4494e+00
Epoch 4/10
19/19 - 0s - loss: 43.9265 - loglik: -4.2480e+01 - logprior: -1.4464e+00
Epoch 5/10
19/19 - 0s - loss: 43.4678 - loglik: -4.2040e+01 - logprior: -1.4275e+00
Epoch 6/10
19/19 - 0s - loss: 43.3640 - loglik: -4.1945e+01 - logprior: -1.4186e+00
Epoch 7/10
19/19 - 0s - loss: 43.0539 - loglik: -4.1651e+01 - logprior: -1.4034e+00
Epoch 8/10
19/19 - 0s - loss: 42.9824 - loglik: -4.1582e+01 - logprior: -1.4000e+00
Epoch 9/10
19/19 - 0s - loss: 42.9689 - loglik: -4.1576e+01 - logprior: -1.3932e+00
Epoch 10/10
19/19 - 0s - loss: 42.6726 - loglik: -4.1290e+01 - logprior: -1.3831e+00
Fitted a model with MAP estimate = -42.7136
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 47.3345 - loglik: -4.2545e+01 - logprior: -4.7893e+00
Epoch 2/2
19/19 - 0s - loss: 42.2807 - loglik: -4.0853e+01 - logprior: -1.4276e+00
Fitted a model with MAP estimate = -41.4862
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 44.3089 - loglik: -4.0878e+01 - logprior: -3.4314e+00
Epoch 2/2
19/19 - 0s - loss: 41.7583 - loglik: -4.0272e+01 - logprior: -1.4865e+00
Fitted a model with MAP estimate = -41.4335
expansions: [(1, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 43.8232 - loglik: -4.0549e+01 - logprior: -3.2748e+00
Epoch 2/10
19/19 - 0s - loss: 41.4450 - loglik: -3.9997e+01 - logprior: -1.4483e+00
Epoch 3/10
19/19 - 0s - loss: 41.0791 - loglik: -3.9733e+01 - logprior: -1.3464e+00
Epoch 4/10
19/19 - 0s - loss: 40.8312 - loglik: -3.9534e+01 - logprior: -1.2970e+00
Epoch 5/10
19/19 - 0s - loss: 40.4734 - loglik: -3.9203e+01 - logprior: -1.2703e+00
Epoch 6/10
19/19 - 0s - loss: 40.4047 - loglik: -3.9151e+01 - logprior: -1.2537e+00
Epoch 7/10
19/19 - 0s - loss: 40.2634 - loglik: -3.9017e+01 - logprior: -1.2465e+00
Epoch 8/10
19/19 - 0s - loss: 40.1539 - loglik: -3.8920e+01 - logprior: -1.2343e+00
Epoch 9/10
19/19 - 1s - loss: 40.0929 - loglik: -3.8860e+01 - logprior: -1.2332e+00
Epoch 10/10
19/19 - 0s - loss: 40.0768 - loglik: -3.8855e+01 - logprior: -1.2222e+00
Fitted a model with MAP estimate = -39.9599
Time for alignment: 29.8016
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 68.7634 - loglik: -6.5389e+01 - logprior: -3.3749e+00
Epoch 2/10
19/19 - 0s - loss: 51.0435 - loglik: -4.9648e+01 - logprior: -1.3950e+00
Epoch 3/10
19/19 - 0s - loss: 45.2381 - loglik: -4.3755e+01 - logprior: -1.4830e+00
Epoch 4/10
19/19 - 0s - loss: 43.8520 - loglik: -4.2395e+01 - logprior: -1.4575e+00
Epoch 5/10
19/19 - 0s - loss: 43.4556 - loglik: -4.2026e+01 - logprior: -1.4298e+00
Epoch 6/10
19/19 - 0s - loss: 43.2393 - loglik: -4.1820e+01 - logprior: -1.4189e+00
Epoch 7/10
19/19 - 0s - loss: 43.0768 - loglik: -4.1671e+01 - logprior: -1.4062e+00
Epoch 8/10
19/19 - 0s - loss: 42.8714 - loglik: -4.1474e+01 - logprior: -1.3972e+00
Epoch 9/10
19/19 - 0s - loss: 42.8016 - loglik: -4.1411e+01 - logprior: -1.3904e+00
Epoch 10/10
19/19 - 0s - loss: 42.7495 - loglik: -4.1362e+01 - logprior: -1.3878e+00
Fitted a model with MAP estimate = -42.6646
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 47.3229 - loglik: -4.2544e+01 - logprior: -4.7788e+00
Epoch 2/2
19/19 - 1s - loss: 42.3000 - loglik: -4.0876e+01 - logprior: -1.4238e+00
Fitted a model with MAP estimate = -41.4724
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 44.0944 - loglik: -4.0711e+01 - logprior: -3.3831e+00
Epoch 2/2
19/19 - 0s - loss: 41.4863 - loglik: -4.0019e+01 - logprior: -1.4669e+00
Fitted a model with MAP estimate = -41.1758
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 43.7342 - loglik: -4.0435e+01 - logprior: -3.2988e+00
Epoch 2/10
19/19 - 0s - loss: 41.4157 - loglik: -3.9962e+01 - logprior: -1.4539e+00
Epoch 3/10
19/19 - 0s - loss: 41.0820 - loglik: -3.9737e+01 - logprior: -1.3455e+00
Epoch 4/10
19/19 - 0s - loss: 40.8730 - loglik: -3.9578e+01 - logprior: -1.2952e+00
Epoch 5/10
19/19 - 0s - loss: 40.5907 - loglik: -3.9321e+01 - logprior: -1.2696e+00
Epoch 6/10
19/19 - 0s - loss: 40.4056 - loglik: -3.9151e+01 - logprior: -1.2545e+00
Epoch 7/10
19/19 - 0s - loss: 40.3522 - loglik: -3.9105e+01 - logprior: -1.2477e+00
Epoch 8/10
19/19 - 0s - loss: 40.1768 - loglik: -3.8940e+01 - logprior: -1.2369e+00
Epoch 9/10
19/19 - 0s - loss: 40.1454 - loglik: -3.8913e+01 - logprior: -1.2321e+00
Epoch 10/10
19/19 - 0s - loss: 40.0831 - loglik: -3.8859e+01 - logprior: -1.2244e+00
Fitted a model with MAP estimate = -40.0251
Time for alignment: 28.0509
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 68.6373 - loglik: -6.5269e+01 - logprior: -3.3685e+00
Epoch 2/10
19/19 - 0s - loss: 50.9081 - loglik: -4.9521e+01 - logprior: -1.3870e+00
Epoch 3/10
19/19 - 0s - loss: 45.4915 - loglik: -4.4022e+01 - logprior: -1.4696e+00
Epoch 4/10
19/19 - 0s - loss: 43.7968 - loglik: -4.2346e+01 - logprior: -1.4505e+00
Epoch 5/10
19/19 - 0s - loss: 43.4363 - loglik: -4.2004e+01 - logprior: -1.4322e+00
Epoch 6/10
19/19 - 0s - loss: 43.2542 - loglik: -4.1836e+01 - logprior: -1.4183e+00
Epoch 7/10
19/19 - 0s - loss: 43.1285 - loglik: -4.1721e+01 - logprior: -1.4073e+00
Epoch 8/10
19/19 - 0s - loss: 42.9120 - loglik: -4.1514e+01 - logprior: -1.3983e+00
Epoch 9/10
19/19 - 0s - loss: 42.7281 - loglik: -4.1336e+01 - logprior: -1.3920e+00
Epoch 10/10
19/19 - 0s - loss: 42.7273 - loglik: -4.1344e+01 - logprior: -1.3836e+00
Fitted a model with MAP estimate = -42.6710
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 47.3259 - loglik: -4.2552e+01 - logprior: -4.7740e+00
Epoch 2/2
19/19 - 0s - loss: 42.2868 - loglik: -4.0875e+01 - logprior: -1.4120e+00
Fitted a model with MAP estimate = -41.4409
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 44.0508 - loglik: -4.0665e+01 - logprior: -3.3855e+00
Epoch 2/2
19/19 - 0s - loss: 41.5269 - loglik: -4.0060e+01 - logprior: -1.4671e+00
Fitted a model with MAP estimate = -41.1747
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 43.7441 - loglik: -4.0442e+01 - logprior: -3.3018e+00
Epoch 2/10
19/19 - 0s - loss: 41.4198 - loglik: -3.9971e+01 - logprior: -1.4484e+00
Epoch 3/10
19/19 - 0s - loss: 41.0238 - loglik: -3.9678e+01 - logprior: -1.3457e+00
Epoch 4/10
19/19 - 0s - loss: 40.8402 - loglik: -3.9546e+01 - logprior: -1.2947e+00
Epoch 5/10
19/19 - 0s - loss: 40.5457 - loglik: -3.9275e+01 - logprior: -1.2708e+00
Epoch 6/10
19/19 - 0s - loss: 40.4168 - loglik: -3.9159e+01 - logprior: -1.2576e+00
Epoch 7/10
19/19 - 0s - loss: 40.3104 - loglik: -3.9067e+01 - logprior: -1.2437e+00
Epoch 8/10
19/19 - 0s - loss: 40.2367 - loglik: -3.9001e+01 - logprior: -1.2354e+00
Epoch 9/10
19/19 - 0s - loss: 40.1212 - loglik: -3.8892e+01 - logprior: -1.2296e+00
Epoch 10/10
19/19 - 0s - loss: 40.0334 - loglik: -3.8810e+01 - logprior: -1.2234e+00
Fitted a model with MAP estimate = -39.9875
Time for alignment: 27.8735
Computed alignments with likelihoods: ['-39.9949', '-40.1465', '-39.9599', '-40.0251', '-39.9875']
Best model has likelihood: -39.9599  (prior= -1.2102 )
time for generating output: 0.0743
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00037.projection.fasta
SP score = 0.8836276083467095
Training of 5 independent models on file PF00224.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fcb70252e80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd32d542e80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd507c6fdc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 733.4379 - loglik: -7.3196e+02 - logprior: -1.4767e+00
Epoch 2/10
39/39 - 26s - loss: 528.8569 - loglik: -5.2740e+02 - logprior: -1.4548e+00
Epoch 3/10
39/39 - 26s - loss: 516.6908 - loglik: -5.1528e+02 - logprior: -1.4060e+00
Epoch 4/10
39/39 - 28s - loss: 514.4564 - loglik: -5.1313e+02 - logprior: -1.3287e+00
Epoch 5/10
39/39 - 29s - loss: 513.0947 - loglik: -5.1177e+02 - logprior: -1.3254e+00
Epoch 6/10
39/39 - 29s - loss: 512.4201 - loglik: -5.1109e+02 - logprior: -1.3311e+00
Epoch 7/10
39/39 - 29s - loss: 512.3801 - loglik: -5.1104e+02 - logprior: -1.3368e+00
Epoch 8/10
39/39 - 29s - loss: 512.0248 - loglik: -5.1067e+02 - logprior: -1.3561e+00
Epoch 9/10
39/39 - 30s - loss: 511.7448 - loglik: -5.1040e+02 - logprior: -1.3488e+00
Epoch 10/10
39/39 - 31s - loss: 511.7301 - loglik: -5.1037e+02 - logprior: -1.3636e+00
Fitted a model with MAP estimate = -510.8097
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (17, 1), (19, 1), (38, 1), (40, 1), (44, 1), (45, 1), (46, 2), (48, 1), (65, 1), (69, 1), (71, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (90, 1), (93, 1), (105, 1), (109, 1), (110, 2), (112, 2), (113, 1), (133, 1), (134, 1), (142, 1), (145, 2), (147, 1), (148, 1), (159, 1), (167, 1), (169, 1), (170, 1), (172, 2), (173, 1), (180, 1), (182, 1), (183, 1), (188, 1), (205, 1), (209, 1), (212, 1), (214, 2), (216, 1), (217, 1), (227, 1), (229, 2), (231, 1), (244, 1), (259, 1), (265, 1), (267, 1), (270, 2), (272, 3), (273, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 50s - loss: 483.9817 - loglik: -4.8198e+02 - logprior: -1.9972e+00
Epoch 2/2
39/39 - 46s - loss: 466.9286 - loglik: -4.6636e+02 - logprior: -5.7312e-01
Fitted a model with MAP estimate = -464.7136
expansions: []
discards: [  0   1  57  98 138 142 181 216]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 472.0416 - loglik: -4.7058e+02 - logprior: -1.4662e+00
Epoch 2/2
39/39 - 45s - loss: 467.7813 - loglik: -4.6791e+02 - logprior: 0.1274
Fitted a model with MAP estimate = -465.6236
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 51s - loss: 469.7726 - loglik: -4.6842e+02 - logprior: -1.3478e+00
Epoch 2/10
39/39 - 46s - loss: 465.8122 - loglik: -4.6595e+02 - logprior: 0.1404
Epoch 3/10
39/39 - 47s - loss: 464.5306 - loglik: -4.6455e+02 - logprior: 0.0184
Epoch 4/10
39/39 - 44s - loss: 462.6405 - loglik: -4.6297e+02 - logprior: 0.3327
Epoch 5/10
39/39 - 42s - loss: 461.8232 - loglik: -4.6201e+02 - logprior: 0.1833
Epoch 6/10
39/39 - 41s - loss: 461.1525 - loglik: -4.6171e+02 - logprior: 0.5573
Epoch 7/10
39/39 - 40s - loss: 461.6198 - loglik: -4.6196e+02 - logprior: 0.3366
Fitted a model with MAP estimate = -460.7096
Time for alignment: 1019.1858
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 734.3886 - loglik: -7.3293e+02 - logprior: -1.4621e+00
Epoch 2/10
39/39 - 27s - loss: 533.7670 - loglik: -5.3243e+02 - logprior: -1.3356e+00
Epoch 3/10
39/39 - 26s - loss: 521.0416 - loglik: -5.1974e+02 - logprior: -1.2973e+00
Epoch 4/10
39/39 - 26s - loss: 518.1426 - loglik: -5.1694e+02 - logprior: -1.2025e+00
Epoch 5/10
39/39 - 25s - loss: 517.1087 - loglik: -5.1590e+02 - logprior: -1.2059e+00
Epoch 6/10
39/39 - 25s - loss: 516.2624 - loglik: -5.1505e+02 - logprior: -1.2075e+00
Epoch 7/10
39/39 - 25s - loss: 515.9564 - loglik: -5.1474e+02 - logprior: -1.2197e+00
Epoch 8/10
39/39 - 25s - loss: 515.7205 - loglik: -5.1449e+02 - logprior: -1.2267e+00
Epoch 9/10
39/39 - 24s - loss: 515.7272 - loglik: -5.1449e+02 - logprior: -1.2363e+00
Fitted a model with MAP estimate = -514.7321
expansions: [(0, 2), (1, 1), (14, 1), (15, 1), (16, 1), (18, 2), (37, 1), (39, 1), (43, 1), (44, 1), (45, 2), (63, 1), (67, 2), (68, 1), (69, 1), (72, 1), (78, 3), (81, 1), (82, 1), (84, 1), (90, 1), (105, 1), (108, 1), (110, 1), (111, 1), (113, 1), (132, 2), (142, 1), (145, 2), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (181, 1), (182, 1), (183, 1), (189, 1), (206, 1), (208, 1), (214, 2), (216, 1), (217, 1), (227, 1), (230, 2), (231, 1), (236, 1), (243, 1), (265, 1), (267, 1), (270, 1), (271, 1), (272, 3), (273, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 39s - loss: 484.8730 - loglik: -4.8291e+02 - logprior: -1.9647e+00
Epoch 2/2
39/39 - 36s - loss: 467.1129 - loglik: -4.6650e+02 - logprior: -6.0835e-01
Fitted a model with MAP estimate = -464.5716
expansions: []
discards: [  0   1  57  83  99 180 215]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 40s - loss: 472.3460 - loglik: -4.7101e+02 - logprior: -1.3354e+00
Epoch 2/2
39/39 - 36s - loss: 468.1826 - loglik: -4.6828e+02 - logprior: 0.0927
Fitted a model with MAP estimate = -465.7700
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 40s - loss: 469.9822 - loglik: -4.6866e+02 - logprior: -1.3242e+00
Epoch 2/10
39/39 - 37s - loss: 466.3584 - loglik: -4.6627e+02 - logprior: -9.1545e-02
Epoch 3/10
39/39 - 39s - loss: 464.5527 - loglik: -4.6472e+02 - logprior: 0.1646
Epoch 4/10
39/39 - 41s - loss: 463.2368 - loglik: -4.6319e+02 - logprior: -4.9822e-02
Epoch 5/10
39/39 - 43s - loss: 461.5497 - loglik: -4.6194e+02 - logprior: 0.3888
Epoch 6/10
39/39 - 44s - loss: 462.3626 - loglik: -4.6253e+02 - logprior: 0.1664
Fitted a model with MAP estimate = -461.0742
Time for alignment: 805.4153
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 32s - loss: 730.5861 - loglik: -7.2911e+02 - logprior: -1.4756e+00
Epoch 2/10
39/39 - 30s - loss: 528.7510 - loglik: -5.2738e+02 - logprior: -1.3673e+00
Epoch 3/10
39/39 - 30s - loss: 517.9091 - loglik: -5.1663e+02 - logprior: -1.2785e+00
Epoch 4/10
39/39 - 31s - loss: 515.5422 - loglik: -5.1433e+02 - logprior: -1.2164e+00
Epoch 5/10
39/39 - 31s - loss: 514.1022 - loglik: -5.1288e+02 - logprior: -1.2182e+00
Epoch 6/10
39/39 - 31s - loss: 513.8030 - loglik: -5.1257e+02 - logprior: -1.2317e+00
Epoch 7/10
39/39 - 31s - loss: 513.0135 - loglik: -5.1178e+02 - logprior: -1.2367e+00
Epoch 8/10
39/39 - 31s - loss: 513.0004 - loglik: -5.1175e+02 - logprior: -1.2469e+00
Epoch 9/10
39/39 - 32s - loss: 513.1620 - loglik: -5.1191e+02 - logprior: -1.2544e+00
Fitted a model with MAP estimate = -512.0633
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (71, 1), (73, 1), (78, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (105, 1), (109, 1), (110, 1), (111, 1), (113, 1), (120, 1), (132, 1), (133, 1), (145, 2), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (179, 1), (182, 1), (183, 1), (187, 1), (204, 1), (208, 1), (214, 2), (216, 2), (227, 1), (229, 3), (230, 1), (243, 1), (265, 1), (267, 2), (269, 1), (272, 3), (273, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 347 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 51s - loss: 483.6976 - loglik: -4.8172e+02 - logprior: -1.9787e+00
Epoch 2/2
39/39 - 47s - loss: 467.1198 - loglik: -4.6665e+02 - logprior: -4.7243e-01
Fitted a model with MAP estimate = -464.6614
expansions: []
discards: [  0   1  97  98 179 214]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 472.1266 - loglik: -4.7071e+02 - logprior: -1.4168e+00
Epoch 2/2
39/39 - 47s - loss: 468.1064 - loglik: -4.6816e+02 - logprior: 0.0526
Fitted a model with MAP estimate = -466.2017
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 469.9637 - loglik: -4.6859e+02 - logprior: -1.3784e+00
Epoch 2/10
39/39 - 38s - loss: 466.3465 - loglik: -4.6637e+02 - logprior: 0.0215
Epoch 3/10
39/39 - 38s - loss: 464.2427 - loglik: -4.6445e+02 - logprior: 0.2118
Epoch 4/10
39/39 - 37s - loss: 463.2927 - loglik: -4.6332e+02 - logprior: 0.0265
Epoch 5/10
39/39 - 36s - loss: 461.6616 - loglik: -4.6206e+02 - logprior: 0.3969
Epoch 6/10
39/39 - 36s - loss: 461.7103 - loglik: -4.6200e+02 - logprior: 0.2881
Fitted a model with MAP estimate = -461.2168
Time for alignment: 931.0071
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 732.4439 - loglik: -7.3094e+02 - logprior: -1.5017e+00
Epoch 2/10
39/39 - 25s - loss: 529.7604 - loglik: -5.2828e+02 - logprior: -1.4847e+00
Epoch 3/10
39/39 - 25s - loss: 518.9525 - loglik: -5.1756e+02 - logprior: -1.3923e+00
Epoch 4/10
39/39 - 25s - loss: 515.8951 - loglik: -5.1458e+02 - logprior: -1.3165e+00
Epoch 5/10
39/39 - 25s - loss: 514.9503 - loglik: -5.1364e+02 - logprior: -1.3148e+00
Epoch 6/10
39/39 - 26s - loss: 514.1129 - loglik: -5.1279e+02 - logprior: -1.3243e+00
Epoch 7/10
39/39 - 27s - loss: 513.8177 - loglik: -5.1248e+02 - logprior: -1.3341e+00
Epoch 8/10
39/39 - 28s - loss: 513.5045 - loglik: -5.1217e+02 - logprior: -1.3386e+00
Epoch 9/10
39/39 - 28s - loss: 513.5492 - loglik: -5.1220e+02 - logprior: -1.3479e+00
Fitted a model with MAP estimate = -512.5296
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (105, 1), (108, 1), (110, 1), (111, 1), (113, 1), (120, 1), (132, 1), (133, 1), (142, 1), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (181, 1), (182, 1), (183, 1), (187, 1), (206, 1), (208, 1), (214, 2), (216, 2), (227, 1), (229, 3), (230, 1), (243, 1), (258, 1), (264, 1), (267, 1), (269, 2), (271, 1), (272, 2), (273, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 345 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 44s - loss: 483.3912 - loglik: -4.8145e+02 - logprior: -1.9437e+00
Epoch 2/2
39/39 - 44s - loss: 467.1501 - loglik: -4.6666e+02 - logprior: -4.8816e-01
Fitted a model with MAP estimate = -465.1158
expansions: [(0, 2)]
discards: [  0   1  97 212]
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 470.4916 - loglik: -4.6914e+02 - logprior: -1.3518e+00
Epoch 2/2
39/39 - 44s - loss: 466.6764 - loglik: -4.6651e+02 - logprior: -1.6349e-01
Fitted a model with MAP estimate = -464.9262
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 472.0872 - loglik: -4.7010e+02 - logprior: -1.9895e+00
Epoch 2/10
39/39 - 46s - loss: 467.3792 - loglik: -4.6735e+02 - logprior: -2.5067e-02
Epoch 3/10
39/39 - 45s - loss: 465.1573 - loglik: -4.6541e+02 - logprior: 0.2571
Epoch 4/10
39/39 - 44s - loss: 463.3581 - loglik: -4.6357e+02 - logprior: 0.2165
Epoch 5/10
39/39 - 44s - loss: 462.3329 - loglik: -4.6291e+02 - logprior: 0.5784
Epoch 6/10
39/39 - 44s - loss: 461.6949 - loglik: -4.6205e+02 - logprior: 0.3543
Epoch 7/10
39/39 - 47s - loss: 461.4300 - loglik: -4.6220e+02 - logprior: 0.7650
Epoch 8/10
39/39 - 47s - loss: 461.7294 - loglik: -4.6223e+02 - logprior: 0.5044
Fitted a model with MAP estimate = -461.3330
Time for alignment: 1001.4258
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 33s - loss: 735.7668 - loglik: -7.3431e+02 - logprior: -1.4559e+00
Epoch 2/10
39/39 - 30s - loss: 537.7334 - loglik: -5.3638e+02 - logprior: -1.3496e+00
Epoch 3/10
39/39 - 31s - loss: 526.3341 - loglik: -5.2505e+02 - logprior: -1.2886e+00
Epoch 4/10
39/39 - 30s - loss: 523.4565 - loglik: -5.2222e+02 - logprior: -1.2341e+00
Epoch 5/10
39/39 - 31s - loss: 521.4752 - loglik: -5.2023e+02 - logprior: -1.2433e+00
Epoch 6/10
39/39 - 32s - loss: 520.9704 - loglik: -5.1973e+02 - logprior: -1.2432e+00
Epoch 7/10
39/39 - 31s - loss: 520.6000 - loglik: -5.1935e+02 - logprior: -1.2529e+00
Epoch 8/10
39/39 - 32s - loss: 520.6447 - loglik: -5.1939e+02 - logprior: -1.2595e+00
Fitted a model with MAP estimate = -519.5835
expansions: [(0, 2), (1, 1), (14, 1), (15, 1), (16, 2), (18, 1), (37, 1), (39, 1), (43, 1), (44, 1), (45, 2), (47, 1), (64, 2), (68, 1), (70, 1), (72, 1), (79, 2), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (96, 1), (108, 1), (110, 1), (111, 1), (113, 1), (132, 3), (133, 1), (145, 2), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (181, 1), (182, 1), (188, 1), (189, 1), (206, 1), (212, 1), (214, 2), (216, 1), (217, 1), (227, 1), (229, 3), (230, 1), (243, 1), (258, 1), (264, 1), (267, 1), (269, 2), (271, 1), (272, 2), (273, 4)]
discards: [51]
Re-initialized the encoder parameters.
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 50s - loss: 490.1259 - loglik: -4.8811e+02 - logprior: -2.0127e+00
Epoch 2/2
39/39 - 46s - loss: 472.7695 - loglik: -4.7209e+02 - logprior: -6.7545e-01
Fitted a model with MAP estimate = -470.4295
expansions: []
discards: [  0   1  57  78  98 164 180 215]
Re-initialized the encoder parameters.
Fitting a model of length 340 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 478.0202 - loglik: -4.7649e+02 - logprior: -1.5324e+00
Epoch 2/2
39/39 - 45s - loss: 473.6673 - loglik: -4.7369e+02 - logprior: 0.0239
Fitted a model with MAP estimate = -471.3391
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 342 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 475.5521 - loglik: -4.7407e+02 - logprior: -1.4787e+00
Epoch 2/10
39/39 - 44s - loss: 471.6505 - loglik: -4.7166e+02 - logprior: 0.0113
Epoch 3/10
39/39 - 44s - loss: 470.0787 - loglik: -4.7013e+02 - logprior: 0.0558
Epoch 4/10
39/39 - 44s - loss: 468.4177 - loglik: -4.6853e+02 - logprior: 0.1091
Epoch 5/10
39/39 - 44s - loss: 467.5652 - loglik: -4.6782e+02 - logprior: 0.2563
Epoch 6/10
39/39 - 45s - loss: 466.8912 - loglik: -4.6711e+02 - logprior: 0.2143
Epoch 7/10
39/39 - 44s - loss: 466.9073 - loglik: -4.6749e+02 - logprior: 0.5875
Fitted a model with MAP estimate = -466.4989
Time for alignment: 980.1134
Computed alignments with likelihoods: ['-460.7096', '-461.0742', '-461.2168', '-461.3330', '-466.4989']
Best model has likelihood: -460.7096  (prior= 0.5692 )
time for generating output: 0.2355
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00224.projection.fasta
SP score = 0.9146049481245012
Training of 5 independent models on file PF13365.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1ac1bb9d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1ac7cc910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fcb8048a400>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 397.8327 - loglik: -3.9482e+02 - logprior: -3.0125e+00
Epoch 2/10
19/19 - 5s - loss: 328.2397 - loglik: -3.2702e+02 - logprior: -1.2243e+00
Epoch 3/10
19/19 - 5s - loss: 305.5540 - loglik: -3.0392e+02 - logprior: -1.6309e+00
Epoch 4/10
19/19 - 5s - loss: 301.7593 - loglik: -3.0024e+02 - logprior: -1.5162e+00
Epoch 5/10
19/19 - 5s - loss: 300.1663 - loglik: -2.9868e+02 - logprior: -1.4913e+00
Epoch 6/10
19/19 - 5s - loss: 299.4077 - loglik: -2.9795e+02 - logprior: -1.4598e+00
Epoch 7/10
19/19 - 5s - loss: 298.5375 - loglik: -2.9710e+02 - logprior: -1.4326e+00
Epoch 8/10
19/19 - 5s - loss: 299.0096 - loglik: -2.9760e+02 - logprior: -1.4102e+00
Fitted a model with MAP estimate = -296.8592
expansions: [(7, 2), (21, 3), (22, 1), (25, 1), (29, 1), (35, 1), (37, 1), (46, 4), (49, 1), (50, 1), (55, 1), (56, 1), (70, 1), (84, 6), (85, 3), (86, 1), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 301.8229 - loglik: -2.9874e+02 - logprior: -3.0873e+00
Epoch 2/2
39/39 - 7s - loss: 292.6910 - loglik: -2.9129e+02 - logprior: -1.4054e+00
Fitted a model with MAP estimate = -288.9839
expansions: []
discards: [ 23  56  57 105 135]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 295.1313 - loglik: -2.9280e+02 - logprior: -2.3292e+00
Epoch 2/2
39/39 - 7s - loss: 291.6643 - loglik: -2.9040e+02 - logprior: -1.2690e+00
Fitted a model with MAP estimate = -289.1479
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 293.7211 - loglik: -2.9144e+02 - logprior: -2.2852e+00
Epoch 2/10
39/39 - 7s - loss: 290.7382 - loglik: -2.8953e+02 - logprior: -1.2050e+00
Epoch 3/10
39/39 - 7s - loss: 288.4233 - loglik: -2.8731e+02 - logprior: -1.1083e+00
Epoch 4/10
39/39 - 7s - loss: 287.2236 - loglik: -2.8619e+02 - logprior: -1.0350e+00
Epoch 5/10
39/39 - 7s - loss: 287.0724 - loglik: -2.8611e+02 - logprior: -9.6390e-01
Epoch 6/10
39/39 - 7s - loss: 286.5044 - loglik: -2.8564e+02 - logprior: -8.6227e-01
Epoch 7/10
39/39 - 7s - loss: 285.6276 - loglik: -2.8486e+02 - logprior: -7.6579e-01
Epoch 8/10
39/39 - 7s - loss: 285.9910 - loglik: -2.8530e+02 - logprior: -6.8758e-01
Fitted a model with MAP estimate = -285.5230
Time for alignment: 170.1775
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 397.9301 - loglik: -3.9492e+02 - logprior: -3.0094e+00
Epoch 2/10
19/19 - 4s - loss: 328.4167 - loglik: -3.2720e+02 - logprior: -1.2149e+00
Epoch 3/10
19/19 - 4s - loss: 305.8174 - loglik: -3.0414e+02 - logprior: -1.6811e+00
Epoch 4/10
19/19 - 4s - loss: 301.3596 - loglik: -2.9981e+02 - logprior: -1.5447e+00
Epoch 5/10
19/19 - 5s - loss: 299.3973 - loglik: -2.9792e+02 - logprior: -1.4807e+00
Epoch 6/10
19/19 - 4s - loss: 299.6992 - loglik: -2.9825e+02 - logprior: -1.4445e+00
Fitted a model with MAP estimate = -297.1406
expansions: [(7, 2), (21, 1), (23, 1), (24, 1), (25, 1), (29, 1), (35, 1), (37, 1), (46, 4), (49, 1), (50, 1), (55, 1), (56, 1), (84, 2), (85, 4), (86, 6), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 301.4338 - loglik: -2.9836e+02 - logprior: -3.0757e+00
Epoch 2/2
39/39 - 7s - loss: 292.5018 - loglik: -2.9109e+02 - logprior: -1.4105e+00
Fitted a model with MAP estimate = -288.9961
expansions: []
discards: [ 55  56 104 135]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 294.8242 - loglik: -2.9247e+02 - logprior: -2.3547e+00
Epoch 2/2
39/39 - 6s - loss: 291.7858 - loglik: -2.9050e+02 - logprior: -1.2824e+00
Fitted a model with MAP estimate = -289.0424
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 9s - loss: 293.5603 - loglik: -2.9126e+02 - logprior: -2.2999e+00
Epoch 2/10
39/39 - 6s - loss: 290.5933 - loglik: -2.8938e+02 - logprior: -1.2132e+00
Epoch 3/10
39/39 - 6s - loss: 288.4409 - loglik: -2.8731e+02 - logprior: -1.1312e+00
Epoch 4/10
39/39 - 6s - loss: 286.8067 - loglik: -2.8576e+02 - logprior: -1.0468e+00
Epoch 5/10
39/39 - 6s - loss: 287.8384 - loglik: -2.8687e+02 - logprior: -9.6436e-01
Fitted a model with MAP estimate = -286.3129
Time for alignment: 128.0420
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 397.7683 - loglik: -3.9476e+02 - logprior: -3.0091e+00
Epoch 2/10
19/19 - 4s - loss: 327.6354 - loglik: -3.2642e+02 - logprior: -1.2136e+00
Epoch 3/10
19/19 - 4s - loss: 306.0768 - loglik: -3.0442e+02 - logprior: -1.6605e+00
Epoch 4/10
19/19 - 4s - loss: 301.3387 - loglik: -2.9981e+02 - logprior: -1.5325e+00
Epoch 5/10
19/19 - 4s - loss: 299.6212 - loglik: -2.9815e+02 - logprior: -1.4749e+00
Epoch 6/10
19/19 - 4s - loss: 298.1988 - loglik: -2.9676e+02 - logprior: -1.4402e+00
Epoch 7/10
19/19 - 4s - loss: 298.9456 - loglik: -2.9753e+02 - logprior: -1.4151e+00
Fitted a model with MAP estimate = -296.7316
expansions: [(7, 2), (22, 1), (23, 1), (24, 1), (25, 2), (29, 1), (35, 1), (37, 1), (46, 4), (49, 1), (50, 1), (57, 1), (71, 1), (84, 2), (85, 4), (86, 5), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 301.8381 - loglik: -2.9876e+02 - logprior: -3.0786e+00
Epoch 2/2
39/39 - 6s - loss: 292.7048 - loglik: -2.9126e+02 - logprior: -1.4421e+00
Fitted a model with MAP estimate = -288.9993
expansions: []
discards: [ 29  56  57 135]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 294.9025 - loglik: -2.9251e+02 - logprior: -2.3937e+00
Epoch 2/2
39/39 - 6s - loss: 291.5081 - loglik: -2.9020e+02 - logprior: -1.3117e+00
Fitted a model with MAP estimate = -288.9700
expansions: []
discards: [101 102 103 104]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 9s - loss: 294.0294 - loglik: -2.9172e+02 - logprior: -2.3069e+00
Epoch 2/10
39/39 - 6s - loss: 290.7316 - loglik: -2.8950e+02 - logprior: -1.2275e+00
Epoch 3/10
39/39 - 6s - loss: 289.1600 - loglik: -2.8803e+02 - logprior: -1.1322e+00
Epoch 4/10
39/39 - 6s - loss: 287.4961 - loglik: -2.8644e+02 - logprior: -1.0570e+00
Epoch 5/10
39/39 - 6s - loss: 286.5880 - loglik: -2.8559e+02 - logprior: -9.9337e-01
Epoch 6/10
39/39 - 6s - loss: 287.0983 - loglik: -2.8617e+02 - logprior: -9.2601e-01
Fitted a model with MAP estimate = -286.3603
Time for alignment: 132.0885
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 398.2924 - loglik: -3.9528e+02 - logprior: -3.0120e+00
Epoch 2/10
19/19 - 4s - loss: 329.6624 - loglik: -3.2844e+02 - logprior: -1.2260e+00
Epoch 3/10
19/19 - 4s - loss: 305.8629 - loglik: -3.0420e+02 - logprior: -1.6669e+00
Epoch 4/10
19/19 - 4s - loss: 301.3699 - loglik: -2.9981e+02 - logprior: -1.5645e+00
Epoch 5/10
19/19 - 4s - loss: 299.7612 - loglik: -2.9823e+02 - logprior: -1.5293e+00
Epoch 6/10
19/19 - 4s - loss: 298.0537 - loglik: -2.9657e+02 - logprior: -1.4811e+00
Epoch 7/10
19/19 - 4s - loss: 296.7411 - loglik: -2.9529e+02 - logprior: -1.4532e+00
Epoch 8/10
19/19 - 4s - loss: 297.2120 - loglik: -2.9579e+02 - logprior: -1.4216e+00
Fitted a model with MAP estimate = -295.9178
expansions: [(7, 2), (22, 2), (23, 1), (24, 1), (25, 2), (28, 1), (29, 1), (37, 1), (46, 4), (49, 1), (50, 1), (55, 1), (60, 1), (70, 1), (84, 5), (85, 3), (86, 1), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 301.8353 - loglik: -2.9874e+02 - logprior: -3.0903e+00
Epoch 2/2
39/39 - 6s - loss: 292.8734 - loglik: -2.9146e+02 - logprior: -1.4118e+00
Fitted a model with MAP estimate = -289.0781
expansions: [(109, 1)]
discards: [ 23  30  57  58 135]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 295.0053 - loglik: -2.9266e+02 - logprior: -2.3450e+00
Epoch 2/2
39/39 - 6s - loss: 291.7090 - loglik: -2.9045e+02 - logprior: -1.2564e+00
Fitted a model with MAP estimate = -288.9808
expansions: []
discards: [101]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 9s - loss: 293.8069 - loglik: -2.9153e+02 - logprior: -2.2787e+00
Epoch 2/10
39/39 - 6s - loss: 290.9150 - loglik: -2.8971e+02 - logprior: -1.2074e+00
Epoch 3/10
39/39 - 6s - loss: 288.5638 - loglik: -2.8747e+02 - logprior: -1.0961e+00
Epoch 4/10
39/39 - 6s - loss: 287.5100 - loglik: -2.8650e+02 - logprior: -1.0148e+00
Epoch 5/10
39/39 - 6s - loss: 286.5997 - loglik: -2.8568e+02 - logprior: -9.1632e-01
Epoch 6/10
39/39 - 6s - loss: 286.7645 - loglik: -2.8594e+02 - logprior: -8.2829e-01
Fitted a model with MAP estimate = -285.9342
Time for alignment: 135.3417
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 398.1718 - loglik: -3.9516e+02 - logprior: -3.0146e+00
Epoch 2/10
19/19 - 4s - loss: 327.9774 - loglik: -3.2675e+02 - logprior: -1.2278e+00
Epoch 3/10
19/19 - 4s - loss: 307.3076 - loglik: -3.0567e+02 - logprior: -1.6337e+00
Epoch 4/10
19/19 - 4s - loss: 302.9261 - loglik: -3.0140e+02 - logprior: -1.5217e+00
Epoch 5/10
19/19 - 4s - loss: 301.1138 - loglik: -2.9962e+02 - logprior: -1.4912e+00
Epoch 6/10
19/19 - 4s - loss: 299.3973 - loglik: -2.9793e+02 - logprior: -1.4716e+00
Epoch 7/10
19/19 - 4s - loss: 298.3833 - loglik: -2.9691e+02 - logprior: -1.4708e+00
Epoch 8/10
19/19 - 4s - loss: 298.8399 - loglik: -2.9739e+02 - logprior: -1.4460e+00
Fitted a model with MAP estimate = -297.2637
expansions: [(7, 2), (21, 3), (22, 1), (25, 1), (29, 1), (35, 1), (36, 1), (37, 1), (46, 2), (49, 1), (50, 1), (55, 1), (56, 1), (70, 1), (84, 6), (85, 3), (86, 1), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 301.8189 - loglik: -2.9874e+02 - logprior: -3.0782e+00
Epoch 2/2
39/39 - 6s - loss: 292.8908 - loglik: -2.9149e+02 - logprior: -1.4013e+00
Fitted a model with MAP estimate = -289.1752
expansions: []
discards: [ 23  56 104 134]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 294.9909 - loglik: -2.9266e+02 - logprior: -2.3356e+00
Epoch 2/2
39/39 - 6s - loss: 291.7796 - loglik: -2.9052e+02 - logprior: -1.2637e+00
Fitted a model with MAP estimate = -289.1676
expansions: [(105, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 9s - loss: 293.7506 - loglik: -2.9146e+02 - logprior: -2.2868e+00
Epoch 2/10
39/39 - 6s - loss: 290.3939 - loglik: -2.8920e+02 - logprior: -1.1961e+00
Epoch 3/10
39/39 - 6s - loss: 288.6819 - loglik: -2.8757e+02 - logprior: -1.1145e+00
Epoch 4/10
39/39 - 6s - loss: 287.4070 - loglik: -2.8639e+02 - logprior: -1.0218e+00
Epoch 5/10
39/39 - 6s - loss: 286.0477 - loglik: -2.8510e+02 - logprior: -9.4606e-01
Epoch 6/10
39/39 - 6s - loss: 286.5793 - loglik: -2.8572e+02 - logprior: -8.5666e-01
Fitted a model with MAP estimate = -285.9319
Time for alignment: 135.1639
Computed alignments with likelihoods: ['-285.5230', '-286.3129', '-286.3603', '-285.9342', '-285.9319']
Best model has likelihood: -285.5230  (prior= -0.6454 )
time for generating output: 0.2397
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13365.projection.fasta
SP score = 0.2683177632709657
Training of 5 independent models on file PF00078.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1647a9490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd518b53040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd507ba9f40>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 470.0492 - loglik: -4.6812e+02 - logprior: -1.9295e+00
Epoch 2/10
39/39 - 7s - loss: 431.0206 - loglik: -4.2985e+02 - logprior: -1.1696e+00
Epoch 3/10
39/39 - 7s - loss: 427.0154 - loglik: -4.2585e+02 - logprior: -1.1631e+00
Epoch 4/10
39/39 - 7s - loss: 426.5115 - loglik: -4.2534e+02 - logprior: -1.1694e+00
Epoch 5/10
39/39 - 7s - loss: 425.8602 - loglik: -4.2470e+02 - logprior: -1.1577e+00
Epoch 6/10
39/39 - 7s - loss: 425.5107 - loglik: -4.2434e+02 - logprior: -1.1712e+00
Epoch 7/10
39/39 - 7s - loss: 426.0461 - loglik: -4.2488e+02 - logprior: -1.1653e+00
Fitted a model with MAP estimate = -425.1715
expansions: [(19, 1), (21, 1), (28, 4), (29, 1), (32, 1), (33, 1), (47, 1), (48, 3), (49, 2), (77, 2), (78, 3), (79, 5), (93, 1), (104, 2), (105, 1), (106, 2), (107, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 159 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 422.7205 - loglik: -4.2083e+02 - logprior: -1.8947e+00
Epoch 2/2
39/39 - 8s - loss: 416.5255 - loglik: -4.1580e+02 - logprior: -7.2098e-01
Fitted a model with MAP estimate = -415.2990
expansions: []
discards: [ 29  30  58  95  96 134]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 420.0935 - loglik: -4.1828e+02 - logprior: -1.8148e+00
Epoch 2/2
39/39 - 8s - loss: 416.6943 - loglik: -4.1606e+02 - logprior: -6.3160e-01
Fitted a model with MAP estimate = -415.7510
expansions: [(37, 1)]
discards: [84 85]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 419.7735 - loglik: -4.1801e+02 - logprior: -1.7592e+00
Epoch 2/10
39/39 - 8s - loss: 417.1571 - loglik: -4.1660e+02 - logprior: -5.5644e-01
Epoch 3/10
39/39 - 8s - loss: 416.2099 - loglik: -4.1570e+02 - logprior: -5.0591e-01
Epoch 4/10
39/39 - 8s - loss: 415.7634 - loglik: -4.1530e+02 - logprior: -4.6292e-01
Epoch 5/10
39/39 - 8s - loss: 415.8954 - loglik: -4.1546e+02 - logprior: -4.3577e-01
Fitted a model with MAP estimate = -415.3225
Time for alignment: 176.3475
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 470.9255 - loglik: -4.6899e+02 - logprior: -1.9385e+00
Epoch 2/10
39/39 - 7s - loss: 428.4211 - loglik: -4.2714e+02 - logprior: -1.2853e+00
Epoch 3/10
39/39 - 7s - loss: 423.5638 - loglik: -4.2227e+02 - logprior: -1.2935e+00
Epoch 4/10
39/39 - 7s - loss: 423.2438 - loglik: -4.2199e+02 - logprior: -1.2536e+00
Epoch 5/10
39/39 - 7s - loss: 422.3228 - loglik: -4.2108e+02 - logprior: -1.2429e+00
Epoch 6/10
39/39 - 7s - loss: 422.3478 - loglik: -4.2110e+02 - logprior: -1.2441e+00
Fitted a model with MAP estimate = -422.0327
expansions: [(20, 3), (29, 1), (30, 1), (31, 1), (46, 1), (48, 1), (50, 1), (59, 2), (60, 2), (61, 1), (79, 2), (80, 6), (81, 2), (100, 4), (106, 1), (108, 1), (111, 1), (119, 9)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 420.4652 - loglik: -4.1853e+02 - logprior: -1.9401e+00
Epoch 2/2
39/39 - 9s - loss: 412.6562 - loglik: -4.1181e+02 - logprior: -8.4864e-01
Fitted a model with MAP estimate = -411.0013
expansions: [(18, 2), (19, 1)]
discards: [ 67  71  94  98 103 124 150 151 152 153 154 155]
Re-initialized the encoder parameters.
Fitting a model of length 158 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 416.3855 - loglik: -4.1461e+02 - logprior: -1.7804e+00
Epoch 2/2
39/39 - 8s - loss: 412.2928 - loglik: -4.1169e+02 - logprior: -6.0617e-01
Fitted a model with MAP estimate = -411.3866
expansions: []
discards: [19]
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 415.2390 - loglik: -4.1350e+02 - logprior: -1.7425e+00
Epoch 2/10
39/39 - 8s - loss: 412.0571 - loglik: -4.1152e+02 - logprior: -5.4155e-01
Epoch 3/10
39/39 - 8s - loss: 411.4015 - loglik: -4.1091e+02 - logprior: -4.9351e-01
Epoch 4/10
39/39 - 8s - loss: 411.1531 - loglik: -4.1071e+02 - logprior: -4.4468e-01
Epoch 5/10
39/39 - 8s - loss: 410.3292 - loglik: -4.0991e+02 - logprior: -4.1625e-01
Epoch 6/10
39/39 - 8s - loss: 410.7646 - loglik: -4.1039e+02 - logprior: -3.7895e-01
Fitted a model with MAP estimate = -410.2597
Time for alignment: 177.9132
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 468.5079 - loglik: -4.6657e+02 - logprior: -1.9357e+00
Epoch 2/10
39/39 - 7s - loss: 427.0825 - loglik: -4.2587e+02 - logprior: -1.2108e+00
Epoch 3/10
39/39 - 7s - loss: 424.2738 - loglik: -4.2308e+02 - logprior: -1.1974e+00
Epoch 4/10
39/39 - 7s - loss: 423.6969 - loglik: -4.2249e+02 - logprior: -1.2059e+00
Epoch 5/10
39/39 - 7s - loss: 422.4391 - loglik: -4.2121e+02 - logprior: -1.2295e+00
Epoch 6/10
39/39 - 7s - loss: 422.8500 - loglik: -4.2162e+02 - logprior: -1.2327e+00
Fitted a model with MAP estimate = -422.2177
expansions: [(19, 2), (20, 1), (30, 6), (31, 1), (47, 3), (49, 1), (53, 1), (77, 2), (78, 8), (79, 2), (94, 1), (98, 1), (107, 3), (109, 2)]
discards: [ 1 57 58 59]
Re-initialized the encoder parameters.
Fitting a model of length 158 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 420.9032 - loglik: -4.1900e+02 - logprior: -1.9067e+00
Epoch 2/2
39/39 - 8s - loss: 415.2102 - loglik: -4.1448e+02 - logprior: -7.2654e-01
Fitted a model with MAP estimate = -413.8900
expansions: []
discards: [ 35  57  96  97  98  99 100 101]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 419.3730 - loglik: -4.1753e+02 - logprior: -1.8418e+00
Epoch 2/2
39/39 - 8s - loss: 415.9143 - loglik: -4.1525e+02 - logprior: -6.6384e-01
Fitted a model with MAP estimate = -414.7780
expansions: [(21, 1), (94, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 417.8542 - loglik: -4.1608e+02 - logprior: -1.7731e+00
Epoch 2/10
39/39 - 8s - loss: 415.1911 - loglik: -4.1463e+02 - logprior: -5.6087e-01
Epoch 3/10
39/39 - 8s - loss: 414.3860 - loglik: -4.1387e+02 - logprior: -5.1676e-01
Epoch 4/10
39/39 - 8s - loss: 413.6670 - loglik: -4.1318e+02 - logprior: -4.8450e-01
Epoch 5/10
39/39 - 8s - loss: 413.8917 - loglik: -4.1344e+02 - logprior: -4.4927e-01
Fitted a model with MAP estimate = -413.2839
Time for alignment: 166.9640
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 470.3474 - loglik: -4.6841e+02 - logprior: -1.9364e+00
Epoch 2/10
39/39 - 7s - loss: 430.0569 - loglik: -4.2877e+02 - logprior: -1.2828e+00
Epoch 3/10
39/39 - 7s - loss: 426.4349 - loglik: -4.2512e+02 - logprior: -1.3148e+00
Epoch 4/10
39/39 - 7s - loss: 425.4328 - loglik: -4.2412e+02 - logprior: -1.3083e+00
Epoch 5/10
39/39 - 7s - loss: 425.3025 - loglik: -4.2400e+02 - logprior: -1.3020e+00
Epoch 6/10
39/39 - 7s - loss: 424.6865 - loglik: -4.2338e+02 - logprior: -1.3069e+00
Epoch 7/10
39/39 - 7s - loss: 424.5110 - loglik: -4.2320e+02 - logprior: -1.3135e+00
Epoch 8/10
39/39 - 7s - loss: 424.7427 - loglik: -4.2341e+02 - logprior: -1.3330e+00
Fitted a model with MAP estimate = -423.8623
expansions: [(19, 2), (20, 1), (31, 4), (32, 3), (33, 1), (47, 1), (49, 1), (53, 1), (55, 1), (77, 1), (78, 1), (79, 6), (80, 2), (93, 1), (94, 1), (104, 1), (105, 1), (106, 1), (107, 1), (109, 2)]
discards: [ 1 56 57 58 59]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 423.9131 - loglik: -4.2203e+02 - logprior: -1.8833e+00
Epoch 2/2
39/39 - 8s - loss: 417.6451 - loglik: -4.1695e+02 - logprior: -6.9396e-01
Fitted a model with MAP estimate = -416.5108
expansions: []
discards: [35 94 95 96 97 98]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 421.3931 - loglik: -4.1960e+02 - logprior: -1.7940e+00
Epoch 2/2
39/39 - 8s - loss: 417.9276 - loglik: -4.1730e+02 - logprior: -6.2828e-01
Fitted a model with MAP estimate = -417.0030
expansions: [(21, 1), (93, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 420.2302 - loglik: -4.1849e+02 - logprior: -1.7364e+00
Epoch 2/10
39/39 - 8s - loss: 417.1237 - loglik: -4.1658e+02 - logprior: -5.4336e-01
Epoch 3/10
39/39 - 8s - loss: 416.5174 - loglik: -4.1603e+02 - logprior: -4.8931e-01
Epoch 4/10
39/39 - 8s - loss: 416.0067 - loglik: -4.1555e+02 - logprior: -4.5863e-01
Epoch 5/10
39/39 - 8s - loss: 415.6535 - loglik: -4.1524e+02 - logprior: -4.1584e-01
Epoch 6/10
39/39 - 8s - loss: 415.5648 - loglik: -4.1516e+02 - logprior: -4.0012e-01
Epoch 7/10
39/39 - 8s - loss: 415.8503 - loglik: -4.1549e+02 - logprior: -3.5971e-01
Fitted a model with MAP estimate = -415.1393
Time for alignment: 194.0567
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 469.3735 - loglik: -4.6743e+02 - logprior: -1.9422e+00
Epoch 2/10
39/39 - 7s - loss: 429.7712 - loglik: -4.2854e+02 - logprior: -1.2289e+00
Epoch 3/10
39/39 - 7s - loss: 426.1194 - loglik: -4.2486e+02 - logprior: -1.2558e+00
Epoch 4/10
39/39 - 7s - loss: 425.2345 - loglik: -4.2398e+02 - logprior: -1.2506e+00
Epoch 5/10
39/39 - 7s - loss: 424.9626 - loglik: -4.2372e+02 - logprior: -1.2474e+00
Epoch 6/10
39/39 - 7s - loss: 424.4392 - loglik: -4.2319e+02 - logprior: -1.2479e+00
Epoch 7/10
39/39 - 7s - loss: 425.1721 - loglik: -4.2392e+02 - logprior: -1.2503e+00
Fitted a model with MAP estimate = -424.2849
expansions: [(19, 2), (20, 1), (31, 4), (33, 1), (47, 1), (48, 1), (49, 1), (50, 2), (56, 4), (77, 2), (78, 8), (79, 2), (94, 1), (97, 1), (98, 1), (106, 1), (107, 1), (109, 2)]
discards: [ 1 74]
Re-initialized the encoder parameters.
Fitting a model of length 162 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 421.4478 - loglik: -4.1959e+02 - logprior: -1.8625e+00
Epoch 2/2
39/39 - 8s - loss: 415.5733 - loglik: -4.1490e+02 - logprior: -6.7021e-01
Fitted a model with MAP estimate = -414.2435
expansions: []
discards: [ 35  57  99 100 101 102 104]
Re-initialized the encoder parameters.
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 419.0171 - loglik: -4.1724e+02 - logprior: -1.7742e+00
Epoch 2/2
39/39 - 8s - loss: 415.9006 - loglik: -4.1531e+02 - logprior: -5.9027e-01
Fitted a model with MAP estimate = -414.8138
expansions: [(21, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 156 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 418.0907 - loglik: -4.1639e+02 - logprior: -1.7056e+00
Epoch 2/10
39/39 - 8s - loss: 415.3469 - loglik: -4.1483e+02 - logprior: -5.1705e-01
Epoch 3/10
39/39 - 8s - loss: 414.4374 - loglik: -4.1397e+02 - logprior: -4.6851e-01
Epoch 4/10
39/39 - 8s - loss: 414.0019 - loglik: -4.1356e+02 - logprior: -4.3939e-01
Epoch 5/10
39/39 - 8s - loss: 413.8560 - loglik: -4.1346e+02 - logprior: -3.9970e-01
Epoch 6/10
39/39 - 8s - loss: 413.5031 - loglik: -4.1313e+02 - logprior: -3.7623e-01
Epoch 7/10
39/39 - 8s - loss: 413.6976 - loglik: -4.1336e+02 - logprior: -3.4162e-01
Fitted a model with MAP estimate = -413.2887
Time for alignment: 190.4765
Computed alignments with likelihoods: ['-415.2990', '-410.2597', '-413.2839', '-415.1393', '-413.2887']
Best model has likelihood: -410.2597  (prior= -0.3747 )
time for generating output: 0.1979
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00078.projection.fasta
SP score = 0.8126743722598645
Training of 5 independent models on file PF00150.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd32eba9430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd124bcc550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd18831f700>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 793.4055 - loglik: -7.9200e+02 - logprior: -1.4086e+00
Epoch 2/10
39/39 - 18s - loss: 725.5642 - loglik: -7.2500e+02 - logprior: -5.6638e-01
Epoch 3/10
39/39 - 18s - loss: 717.3403 - loglik: -7.1670e+02 - logprior: -6.4441e-01
Epoch 4/10
39/39 - 18s - loss: 715.4019 - loglik: -7.1477e+02 - logprior: -6.3582e-01
Epoch 5/10
39/39 - 19s - loss: 714.1511 - loglik: -7.1350e+02 - logprior: -6.5381e-01
Epoch 6/10
39/39 - 19s - loss: 714.3104 - loglik: -7.1368e+02 - logprior: -6.2816e-01
Fitted a model with MAP estimate = -712.8926
expansions: [(0, 3), (24, 1), (43, 1), (49, 1), (50, 2), (51, 2), (62, 1), (85, 3), (86, 12), (87, 3), (88, 1), (117, 3), (118, 1), (122, 1), (154, 4), (158, 3), (184, 4), (206, 5), (207, 2), (208, 1), (211, 1), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 285 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 711.6668 - loglik: -7.0960e+02 - logprior: -2.0717e+00
Epoch 2/2
39/39 - 25s - loss: 703.7995 - loglik: -7.0327e+02 - logprior: -5.3288e-01
Fitted a model with MAP estimate = -701.5512
expansions: [(0, 3), (114, 1)]
discards: [  1   2   3   4  54  59 104 105 106 107 108 192 193 194 195 197 198 256]
Re-initialized the encoder parameters.
Fitting a model of length 271 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 709.3779 - loglik: -7.0700e+02 - logprior: -2.3783e+00
Epoch 2/2
39/39 - 24s - loss: 704.3677 - loglik: -7.0402e+02 - logprior: -3.4746e-01
Fitted a model with MAP estimate = -702.4880
expansions: [(0, 3), (210, 3)]
discards: [  0   1   2 100]
Re-initialized the encoder parameters.
Fitting a model of length 273 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 706.5681 - loglik: -7.0503e+02 - logprior: -1.5373e+00
Epoch 2/10
39/39 - 24s - loss: 702.9535 - loglik: -7.0271e+02 - logprior: -2.4134e-01
Epoch 3/10
39/39 - 24s - loss: 701.7184 - loglik: -7.0157e+02 - logprior: -1.5218e-01
Epoch 4/10
39/39 - 24s - loss: 701.3768 - loglik: -7.0129e+02 - logprior: -8.3267e-02
Epoch 5/10
39/39 - 24s - loss: 700.7639 - loglik: -7.0074e+02 - logprior: -1.9645e-02
Epoch 6/10
39/39 - 24s - loss: 700.7864 - loglik: -7.0082e+02 - logprior: 0.0375
Fitted a model with MAP estimate = -700.2797
Time for alignment: 481.4732
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 795.7620 - loglik: -7.9435e+02 - logprior: -1.4103e+00
Epoch 2/10
39/39 - 18s - loss: 726.1124 - loglik: -7.2535e+02 - logprior: -7.6475e-01
Epoch 3/10
39/39 - 18s - loss: 717.0175 - loglik: -7.1605e+02 - logprior: -9.6297e-01
Epoch 4/10
39/39 - 18s - loss: 715.0615 - loglik: -7.1410e+02 - logprior: -9.5682e-01
Epoch 5/10
39/39 - 19s - loss: 714.1160 - loglik: -7.1316e+02 - logprior: -9.5307e-01
Epoch 6/10
39/39 - 18s - loss: 713.8345 - loglik: -7.1288e+02 - logprior: -9.5776e-01
Epoch 7/10
39/39 - 18s - loss: 713.7653 - loglik: -7.1278e+02 - logprior: -9.8096e-01
Epoch 8/10
39/39 - 18s - loss: 713.6385 - loglik: -7.1266e+02 - logprior: -9.7687e-01
Epoch 9/10
39/39 - 18s - loss: 713.0743 - loglik: -7.1209e+02 - logprior: -9.8625e-01
Epoch 10/10
39/39 - 18s - loss: 713.4788 - loglik: -7.1248e+02 - logprior: -9.9724e-01
Fitted a model with MAP estimate = -712.3600
expansions: [(0, 3), (9, 1), (10, 1), (23, 1), (27, 1), (50, 2), (52, 1), (62, 1), (80, 2), (85, 2), (88, 2), (89, 4), (90, 1), (96, 1), (105, 1), (118, 2), (119, 1), (122, 1), (126, 1), (128, 1), (153, 2), (154, 3), (168, 1), (176, 1), (179, 4), (183, 1), (184, 1), (207, 2), (208, 5), (211, 1), (212, 1), (213, 1), (214, 1), (216, 1)]
discards: [157]
Re-initialized the encoder parameters.
Fitting a model of length 283 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 711.1547 - loglik: -7.0899e+02 - logprior: -2.1632e+00
Epoch 2/2
39/39 - 25s - loss: 703.0222 - loglik: -7.0248e+02 - logprior: -5.3813e-01
Fitted a model with MAP estimate = -700.7235
expansions: [(0, 3), (50, 1), (96, 1)]
discards: [  1   2   3   4  57  91  92  93 105 106 186 187 191 251 252 253 254 255]
Re-initialized the encoder parameters.
Fitting a model of length 270 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 709.1327 - loglik: -7.0676e+02 - logprior: -2.3728e+00
Epoch 2/2
39/39 - 24s - loss: 703.8460 - loglik: -7.0350e+02 - logprior: -3.4157e-01
Fitted a model with MAP estimate = -701.7803
expansions: [(0, 3), (95, 2), (179, 1), (181, 4)]
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 277 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 705.5805 - loglik: -7.0404e+02 - logprior: -1.5423e+00
Epoch 2/10
39/39 - 24s - loss: 701.9219 - loglik: -7.0171e+02 - logprior: -2.1525e-01
Epoch 3/10
39/39 - 24s - loss: 700.8724 - loglik: -7.0070e+02 - logprior: -1.7234e-01
Epoch 4/10
39/39 - 24s - loss: 699.8099 - loglik: -6.9972e+02 - logprior: -9.1443e-02
Epoch 5/10
39/39 - 24s - loss: 699.5610 - loglik: -6.9953e+02 - logprior: -2.8028e-02
Epoch 6/10
39/39 - 24s - loss: 699.2753 - loglik: -6.9931e+02 - logprior: 0.0350
Epoch 7/10
39/39 - 24s - loss: 699.5573 - loglik: -6.9965e+02 - logprior: 0.0949
Fitted a model with MAP estimate = -698.7437
Time for alignment: 578.5194
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 794.1910 - loglik: -7.9278e+02 - logprior: -1.4095e+00
Epoch 2/10
39/39 - 18s - loss: 724.6185 - loglik: -7.2395e+02 - logprior: -6.6779e-01
Epoch 3/10
39/39 - 18s - loss: 716.6199 - loglik: -7.1584e+02 - logprior: -7.8241e-01
Epoch 4/10
39/39 - 18s - loss: 715.0660 - loglik: -7.1429e+02 - logprior: -7.7332e-01
Epoch 5/10
39/39 - 18s - loss: 714.6581 - loglik: -7.1388e+02 - logprior: -7.7607e-01
Epoch 6/10
39/39 - 18s - loss: 713.8173 - loglik: -7.1303e+02 - logprior: -7.8454e-01
Epoch 7/10
39/39 - 18s - loss: 713.3521 - loglik: -7.1256e+02 - logprior: -7.9181e-01
Epoch 8/10
39/39 - 18s - loss: 712.5151 - loglik: -7.1171e+02 - logprior: -8.0227e-01
Epoch 9/10
39/39 - 18s - loss: 712.4898 - loglik: -7.1169e+02 - logprior: -8.0051e-01
Epoch 10/10
39/39 - 18s - loss: 712.5903 - loglik: -7.1178e+02 - logprior: -8.1022e-01
Fitted a model with MAP estimate = -711.6031
expansions: [(0, 3), (9, 2), (24, 1), (27, 1), (43, 2), (49, 1), (50, 2), (51, 2), (73, 1), (85, 4), (87, 2), (88, 5), (89, 1), (95, 1), (97, 1), (116, 3), (117, 1), (120, 1), (153, 2), (154, 3), (184, 4), (206, 5), (207, 1), (208, 1), (211, 1), (214, 1)]
discards: [  0 157]
Re-initialized the encoder parameters.
Fitting a model of length 279 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 711.1157 - loglik: -7.0947e+02 - logprior: -1.6495e+00
Epoch 2/2
39/39 - 24s - loss: 703.8568 - loglik: -7.0333e+02 - logprior: -5.2192e-01
Fitted a model with MAP estimate = -701.7177
expansions: [(101, 2)]
discards: [  1   2  59  62 105 108 189 249 250 251 253]
Re-initialized the encoder parameters.
Fitting a model of length 270 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 707.3539 - loglik: -7.0590e+02 - logprior: -1.4556e+00
Epoch 2/2
39/39 - 24s - loss: 703.4800 - loglik: -7.0313e+02 - logprior: -3.5088e-01
Fitted a model with MAP estimate = -701.8363
expansions: [(10, 2), (183, 1), (213, 3)]
discards: [244]
Re-initialized the encoder parameters.
Fitting a model of length 275 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 705.2383 - loglik: -7.0388e+02 - logprior: -1.3576e+00
Epoch 2/10
39/39 - 24s - loss: 702.3757 - loglik: -7.0212e+02 - logprior: -2.5945e-01
Epoch 3/10
39/39 - 24s - loss: 700.8690 - loglik: -7.0071e+02 - logprior: -1.5504e-01
Epoch 4/10
39/39 - 24s - loss: 699.6391 - loglik: -6.9954e+02 - logprior: -1.0193e-01
Epoch 5/10
39/39 - 24s - loss: 700.3943 - loglik: -7.0036e+02 - logprior: -3.7275e-02
Fitted a model with MAP estimate = -699.4848
Time for alignment: 526.2023
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 793.2567 - loglik: -7.9185e+02 - logprior: -1.4072e+00
Epoch 2/10
39/39 - 19s - loss: 725.5418 - loglik: -7.2494e+02 - logprior: -6.0226e-01
Epoch 3/10
39/39 - 19s - loss: 717.4572 - loglik: -7.1674e+02 - logprior: -7.2160e-01
Epoch 4/10
39/39 - 18s - loss: 715.3973 - loglik: -7.1467e+02 - logprior: -7.2586e-01
Epoch 5/10
39/39 - 18s - loss: 714.9891 - loglik: -7.1430e+02 - logprior: -6.9148e-01
Epoch 6/10
39/39 - 19s - loss: 714.4503 - loglik: -7.1378e+02 - logprior: -6.6903e-01
Epoch 7/10
39/39 - 18s - loss: 713.9879 - loglik: -7.1333e+02 - logprior: -6.5688e-01
Epoch 8/10
39/39 - 18s - loss: 713.9317 - loglik: -7.1327e+02 - logprior: -6.6057e-01
Epoch 9/10
39/39 - 18s - loss: 713.6622 - loglik: -7.1300e+02 - logprior: -6.6036e-01
Epoch 10/10
39/39 - 18s - loss: 713.6143 - loglik: -7.1295e+02 - logprior: -6.6102e-01
Fitted a model with MAP estimate = -712.4683
expansions: [(0, 4), (40, 1), (43, 2), (49, 1), (50, 2), (51, 2), (73, 1), (85, 1), (91, 13), (118, 2), (119, 1), (121, 1), (125, 1), (152, 2), (154, 3), (155, 1), (157, 2), (181, 1), (184, 3), (206, 6), (207, 2), (209, 1), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 283 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 711.4068 - loglik: -7.0927e+02 - logprior: -2.1328e+00
Epoch 2/2
39/39 - 25s - loss: 702.5231 - loglik: -7.0203e+02 - logprior: -4.8895e-01
Fitted a model with MAP estimate = -700.1427
expansions: [(0, 4), (188, 2)]
discards: [  0   1   2   3  58  61  90  91  92  93 110 146 190 191 192 193 194 195
 196 253 254 255]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 709.6838 - loglik: -7.0817e+02 - logprior: -1.5092e+00
Epoch 2/2
39/39 - 23s - loss: 705.2034 - loglik: -7.0492e+02 - logprior: -2.8041e-01
Fitted a model with MAP estimate = -703.2088
expansions: [(0, 3), (135, 3)]
discards: [ 1  2 88]
Re-initialized the encoder parameters.
Fitting a model of length 270 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 707.2844 - loglik: -7.0547e+02 - logprior: -1.8173e+00
Epoch 2/10
39/39 - 23s - loss: 703.2620 - loglik: -7.0301e+02 - logprior: -2.4963e-01
Epoch 3/10
39/39 - 23s - loss: 702.1661 - loglik: -7.0199e+02 - logprior: -1.7411e-01
Epoch 4/10
39/39 - 23s - loss: 701.3907 - loglik: -7.0128e+02 - logprior: -1.0710e-01
Epoch 5/10
39/39 - 23s - loss: 701.1013 - loglik: -7.0106e+02 - logprior: -3.6785e-02
Epoch 6/10
39/39 - 23s - loss: 700.7423 - loglik: -7.0078e+02 - logprior: 0.0343
Epoch 7/10
39/39 - 23s - loss: 700.8386 - loglik: -7.0093e+02 - logprior: 0.0874
Fitted a model with MAP estimate = -700.3946
Time for alignment: 572.1598
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 794.3768 - loglik: -7.9296e+02 - logprior: -1.4173e+00
Epoch 2/10
39/39 - 18s - loss: 727.9527 - loglik: -7.2731e+02 - logprior: -6.3970e-01
Epoch 3/10
39/39 - 18s - loss: 717.6825 - loglik: -7.1694e+02 - logprior: -7.3922e-01
Epoch 4/10
39/39 - 18s - loss: 715.2615 - loglik: -7.1452e+02 - logprior: -7.4628e-01
Epoch 5/10
39/39 - 18s - loss: 714.2100 - loglik: -7.1348e+02 - logprior: -7.3281e-01
Epoch 6/10
39/39 - 18s - loss: 714.1670 - loglik: -7.1342e+02 - logprior: -7.4230e-01
Epoch 7/10
39/39 - 19s - loss: 713.6009 - loglik: -7.1285e+02 - logprior: -7.4736e-01
Epoch 8/10
39/39 - 18s - loss: 713.3654 - loglik: -7.1260e+02 - logprior: -7.6795e-01
Epoch 9/10
39/39 - 18s - loss: 712.9623 - loglik: -7.1219e+02 - logprior: -7.6981e-01
Epoch 10/10
39/39 - 18s - loss: 713.4664 - loglik: -7.1271e+02 - logprior: -7.5951e-01
Fitted a model with MAP estimate = -712.1556
expansions: [(0, 3), (9, 1), (10, 1), (26, 2), (27, 1), (43, 1), (51, 3), (62, 1), (86, 10), (90, 5), (91, 1), (92, 1), (96, 1), (98, 1), (104, 1), (116, 3), (119, 1), (121, 1), (153, 2), (154, 3), (158, 2), (183, 4), (205, 6), (209, 1), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 286 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 710.4645 - loglik: -7.0830e+02 - logprior: -2.1666e+00
Epoch 2/2
39/39 - 26s - loss: 702.0040 - loglik: -7.0150e+02 - logprior: -5.0139e-01
Fitted a model with MAP estimate = -699.4654
expansions: [(0, 3), (192, 1)]
discards: [  1   2   3  32  51  86  87  88  89  90  91  92  93 115 196 197 198 199
 200 201 257 258]
Re-initialized the encoder parameters.
Fitting a model of length 268 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 711.1729 - loglik: -7.0883e+02 - logprior: -2.3402e+00
Epoch 2/2
39/39 - 25s - loss: 705.2867 - loglik: -7.0492e+02 - logprior: -3.6310e-01
Fitted a model with MAP estimate = -703.2586
expansions: [(0, 3), (185, 3), (209, 1)]
discards: [ 1  2  3  4 84 85 86 87]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 710.1106 - loglik: -7.0764e+02 - logprior: -2.4689e+00
Epoch 2/10
39/39 - 24s - loss: 705.0127 - loglik: -7.0473e+02 - logprior: -2.8523e-01
Epoch 3/10
39/39 - 24s - loss: 703.8182 - loglik: -7.0376e+02 - logprior: -6.0519e-02
Epoch 4/10
39/39 - 24s - loss: 703.0884 - loglik: -7.0311e+02 - logprior: 0.0244
Epoch 5/10
39/39 - 23s - loss: 702.5446 - loglik: -7.0263e+02 - logprior: 0.0821
Epoch 6/10
39/39 - 23s - loss: 702.4424 - loglik: -7.0260e+02 - logprior: 0.1584
Epoch 7/10
39/39 - 23s - loss: 702.6879 - loglik: -7.0291e+02 - logprior: 0.2213
Fitted a model with MAP estimate = -702.1090
Time for alignment: 578.4891
Computed alignments with likelihoods: ['-700.2797', '-698.7437', '-699.4848', '-700.1427', '-699.4654']
Best model has likelihood: -698.7437  (prior= 0.1332 )
time for generating output: 0.3906
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00150.projection.fasta
SP score = 0.5810055865921788
Training of 5 independent models on file PF13561.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd32e82d5b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd125eddaf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd32e534f70>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 574.2567 - loglik: -5.7244e+02 - logprior: -1.8151e+00
Epoch 2/10
39/39 - 10s - loss: 481.7417 - loglik: -4.8012e+02 - logprior: -1.6194e+00
Epoch 3/10
39/39 - 10s - loss: 474.2470 - loglik: -4.7259e+02 - logprior: -1.6546e+00
Epoch 4/10
39/39 - 10s - loss: 472.6577 - loglik: -4.7106e+02 - logprior: -1.5959e+00
Epoch 5/10
39/39 - 10s - loss: 472.4517 - loglik: -4.7088e+02 - logprior: -1.5707e+00
Epoch 6/10
39/39 - 10s - loss: 472.1807 - loglik: -4.7062e+02 - logprior: -1.5591e+00
Epoch 7/10
39/39 - 10s - loss: 472.2066 - loglik: -4.7065e+02 - logprior: -1.5604e+00
Fitted a model with MAP estimate = -469.1656
expansions: [(8, 1), (12, 1), (18, 1), (23, 2), (24, 1), (27, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (48, 1), (50, 1), (66, 1), (67, 2), (71, 1), (77, 1), (87, 1), (88, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (129, 1), (133, 1), (150, 5), (151, 2), (153, 2), (155, 1), (156, 2), (157, 2), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 464.9595 - loglik: -4.6234e+02 - logprior: -2.6179e+00
Epoch 2/2
39/39 - 14s - loss: 455.2469 - loglik: -4.5407e+02 - logprior: -1.1783e+00
Fitted a model with MAP estimate = -450.8596
expansions: [(3, 1)]
discards: [  0  26  85 139 142 190 191 192 201]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 459.2094 - loglik: -4.5679e+02 - logprior: -2.4221e+00
Epoch 2/2
39/39 - 14s - loss: 454.8148 - loglik: -4.5411e+02 - logprior: -7.0188e-01
Fitted a model with MAP estimate = -451.0737
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 456.2901 - loglik: -4.5391e+02 - logprior: -2.3849e+00
Epoch 2/10
39/39 - 14s - loss: 452.8713 - loglik: -4.5171e+02 - logprior: -1.1594e+00
Epoch 3/10
39/39 - 14s - loss: 451.0134 - loglik: -4.5071e+02 - logprior: -3.0275e-01
Epoch 4/10
39/39 - 14s - loss: 450.8456 - loglik: -4.5060e+02 - logprior: -2.4445e-01
Epoch 5/10
39/39 - 14s - loss: 450.3033 - loglik: -4.5012e+02 - logprior: -1.8025e-01
Epoch 6/10
39/39 - 13s - loss: 450.5428 - loglik: -4.5042e+02 - logprior: -1.2167e-01
Fitted a model with MAP estimate = -449.8197
Time for alignment: 294.0836
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 576.2917 - loglik: -5.7448e+02 - logprior: -1.8133e+00
Epoch 2/10
39/39 - 10s - loss: 480.7685 - loglik: -4.7914e+02 - logprior: -1.6289e+00
Epoch 3/10
39/39 - 10s - loss: 472.0433 - loglik: -4.7036e+02 - logprior: -1.6872e+00
Epoch 4/10
39/39 - 10s - loss: 470.6519 - loglik: -4.6904e+02 - logprior: -1.6093e+00
Epoch 5/10
39/39 - 10s - loss: 470.1505 - loglik: -4.6857e+02 - logprior: -1.5836e+00
Epoch 6/10
39/39 - 10s - loss: 470.2102 - loglik: -4.6863e+02 - logprior: -1.5777e+00
Fitted a model with MAP estimate = -466.9888
expansions: [(8, 1), (12, 1), (22, 1), (23, 2), (24, 1), (28, 1), (31, 1), (32, 1), (33, 2), (35, 2), (36, 2), (37, 2), (42, 1), (45, 1), (46, 1), (47, 1), (65, 1), (66, 2), (69, 1), (72, 1), (83, 1), (88, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (128, 1), (149, 2), (150, 6), (153, 1), (154, 1), (155, 1), (156, 1), (163, 1), (168, 1), (178, 1), (183, 1), (184, 1), (188, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 245 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 465.1227 - loglik: -4.6253e+02 - logprior: -2.5975e+00
Epoch 2/2
39/39 - 14s - loss: 455.1110 - loglik: -4.5398e+02 - logprior: -1.1324e+00
Fitted a model with MAP estimate = -450.7642
expansions: [(3, 1)]
discards: [  0  26  42  45  51  87 142 145 193 194]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 459.2901 - loglik: -4.5688e+02 - logprior: -2.4137e+00
Epoch 2/2
39/39 - 13s - loss: 454.8215 - loglik: -4.5416e+02 - logprior: -6.6325e-01
Fitted a model with MAP estimate = -451.1417
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 456.3562 - loglik: -4.5398e+02 - logprior: -2.3801e+00
Epoch 2/10
39/39 - 13s - loss: 452.8209 - loglik: -4.5170e+02 - logprior: -1.1223e+00
Epoch 3/10
39/39 - 13s - loss: 451.1331 - loglik: -4.5085e+02 - logprior: -2.7920e-01
Epoch 4/10
39/39 - 13s - loss: 450.5817 - loglik: -4.5037e+02 - logprior: -2.1265e-01
Epoch 5/10
39/39 - 13s - loss: 450.4245 - loglik: -4.5026e+02 - logprior: -1.6273e-01
Epoch 6/10
39/39 - 13s - loss: 449.9882 - loglik: -4.4989e+02 - logprior: -1.0145e-01
Epoch 7/10
39/39 - 13s - loss: 450.3581 - loglik: -4.5031e+02 - logprior: -4.3586e-02
Fitted a model with MAP estimate = -449.8279
Time for alignment: 288.8482
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 574.8154 - loglik: -5.7300e+02 - logprior: -1.8121e+00
Epoch 2/10
39/39 - 10s - loss: 480.9397 - loglik: -4.7928e+02 - logprior: -1.6562e+00
Epoch 3/10
39/39 - 10s - loss: 473.0852 - loglik: -4.7139e+02 - logprior: -1.6942e+00
Epoch 4/10
39/39 - 10s - loss: 471.7004 - loglik: -4.7006e+02 - logprior: -1.6416e+00
Epoch 5/10
39/39 - 10s - loss: 471.3249 - loglik: -4.6971e+02 - logprior: -1.6114e+00
Epoch 6/10
39/39 - 10s - loss: 471.0938 - loglik: -4.6949e+02 - logprior: -1.6061e+00
Epoch 7/10
39/39 - 10s - loss: 471.0890 - loglik: -4.6948e+02 - logprior: -1.6045e+00
Epoch 8/10
39/39 - 10s - loss: 471.0492 - loglik: -4.6945e+02 - logprior: -1.6027e+00
Epoch 9/10
39/39 - 10s - loss: 471.0027 - loglik: -4.6940e+02 - logprior: -1.6025e+00
Epoch 10/10
39/39 - 10s - loss: 470.9169 - loglik: -4.6932e+02 - logprior: -1.5995e+00
Fitted a model with MAP estimate = -467.9473
expansions: [(8, 1), (11, 1), (18, 1), (21, 1), (24, 1), (28, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (46, 1), (47, 1), (49, 1), (50, 1), (66, 1), (67, 2), (71, 1), (77, 1), (87, 1), (88, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (128, 1), (149, 2), (150, 1), (151, 4), (153, 3), (154, 1), (155, 1), (156, 1), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 464.7867 - loglik: -4.6222e+02 - logprior: -2.5651e+00
Epoch 2/2
39/39 - 13s - loss: 455.3042 - loglik: -4.5412e+02 - logprior: -1.1820e+00
Fitted a model with MAP estimate = -450.9014
expansions: [(3, 1)]
discards: [  0  84 138 141 188 189 190 191]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 459.3799 - loglik: -4.5696e+02 - logprior: -2.4235e+00
Epoch 2/2
39/39 - 13s - loss: 455.0827 - loglik: -4.5434e+02 - logprior: -7.4754e-01
Fitted a model with MAP estimate = -451.2122
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 456.4342 - loglik: -4.5406e+02 - logprior: -2.3721e+00
Epoch 2/10
39/39 - 13s - loss: 453.0090 - loglik: -4.5187e+02 - logprior: -1.1375e+00
Epoch 3/10
39/39 - 13s - loss: 451.2948 - loglik: -4.5101e+02 - logprior: -2.8675e-01
Epoch 4/10
39/39 - 13s - loss: 450.8301 - loglik: -4.5062e+02 - logprior: -2.1468e-01
Epoch 5/10
39/39 - 13s - loss: 450.1451 - loglik: -4.4999e+02 - logprior: -1.6005e-01
Epoch 6/10
39/39 - 13s - loss: 450.7640 - loglik: -4.5067e+02 - logprior: -9.0961e-02
Fitted a model with MAP estimate = -449.9552
Time for alignment: 310.7748
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 575.7512 - loglik: -5.7394e+02 - logprior: -1.8082e+00
Epoch 2/10
39/39 - 10s - loss: 482.2747 - loglik: -4.8064e+02 - logprior: -1.6335e+00
Epoch 3/10
39/39 - 10s - loss: 474.4792 - loglik: -4.7282e+02 - logprior: -1.6618e+00
Epoch 4/10
39/39 - 10s - loss: 473.1306 - loglik: -4.7155e+02 - logprior: -1.5834e+00
Epoch 5/10
39/39 - 10s - loss: 472.7106 - loglik: -4.7115e+02 - logprior: -1.5634e+00
Epoch 6/10
39/39 - 10s - loss: 472.5976 - loglik: -4.7104e+02 - logprior: -1.5546e+00
Epoch 7/10
39/39 - 10s - loss: 472.4188 - loglik: -4.7086e+02 - logprior: -1.5564e+00
Epoch 8/10
39/39 - 10s - loss: 472.6336 - loglik: -4.7108e+02 - logprior: -1.5567e+00
Fitted a model with MAP estimate = -469.3231
expansions: [(8, 1), (11, 1), (18, 1), (23, 2), (24, 1), (30, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (45, 1), (46, 1), (48, 1), (49, 1), (66, 1), (67, 2), (70, 1), (71, 1), (80, 1), (87, 1), (88, 1), (89, 1), (94, 1), (97, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (133, 1), (149, 2), (150, 1), (151, 4), (153, 2), (154, 1), (155, 1), (156, 2), (157, 2), (163, 1), (169, 1), (181, 1), (183, 1), (184, 1), (188, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 245 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 465.7071 - loglik: -4.6314e+02 - logprior: -2.5622e+00
Epoch 2/2
39/39 - 13s - loss: 455.1688 - loglik: -4.5398e+02 - logprior: -1.1843e+00
Fitted a model with MAP estimate = -450.6665
expansions: [(3, 1)]
discards: [  0  26  84 139 142 189 190 191 192]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 459.3855 - loglik: -4.5695e+02 - logprior: -2.4361e+00
Epoch 2/2
39/39 - 13s - loss: 455.0492 - loglik: -4.5433e+02 - logprior: -7.2149e-01
Fitted a model with MAP estimate = -451.1671
expansions: [(3, 1)]
discards: [  0 194]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 456.4750 - loglik: -4.5409e+02 - logprior: -2.3839e+00
Epoch 2/10
39/39 - 13s - loss: 452.9388 - loglik: -4.5180e+02 - logprior: -1.1342e+00
Epoch 3/10
39/39 - 13s - loss: 451.2338 - loglik: -4.5094e+02 - logprior: -2.8975e-01
Epoch 4/10
39/39 - 13s - loss: 450.7419 - loglik: -4.5052e+02 - logprior: -2.2355e-01
Epoch 5/10
39/39 - 13s - loss: 450.2076 - loglik: -4.5003e+02 - logprior: -1.8256e-01
Epoch 6/10
39/39 - 13s - loss: 450.8039 - loglik: -4.5069e+02 - logprior: -1.1706e-01
Fitted a model with MAP estimate = -450.0180
Time for alignment: 292.7560
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 574.8531 - loglik: -5.7304e+02 - logprior: -1.8166e+00
Epoch 2/10
39/39 - 10s - loss: 482.2661 - loglik: -4.8063e+02 - logprior: -1.6349e+00
Epoch 3/10
39/39 - 10s - loss: 474.6520 - loglik: -4.7295e+02 - logprior: -1.6977e+00
Epoch 4/10
39/39 - 10s - loss: 473.0965 - loglik: -4.7147e+02 - logprior: -1.6287e+00
Epoch 5/10
39/39 - 10s - loss: 473.0055 - loglik: -4.7141e+02 - logprior: -1.6001e+00
Epoch 6/10
39/39 - 10s - loss: 472.8007 - loglik: -4.7120e+02 - logprior: -1.5983e+00
Epoch 7/10
39/39 - 10s - loss: 472.4470 - loglik: -4.7085e+02 - logprior: -1.5970e+00
Epoch 8/10
39/39 - 10s - loss: 472.6904 - loglik: -4.7109e+02 - logprior: -1.5991e+00
Fitted a model with MAP estimate = -469.4747
expansions: [(12, 1), (15, 1), (18, 1), (23, 2), (24, 1), (27, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (47, 1), (48, 1), (66, 1), (67, 2), (71, 1), (77, 1), (83, 1), (88, 1), (89, 1), (94, 1), (97, 1), (108, 1), (110, 2), (112, 2), (123, 1), (126, 1), (128, 1), (149, 2), (150, 1), (151, 4), (153, 2), (154, 1), (155, 1), (156, 2), (157, 3), (163, 1), (169, 1), (181, 1), (183, 1), (184, 1), (188, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 246 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 465.8832 - loglik: -4.6330e+02 - logprior: -2.5784e+00
Epoch 2/2
39/39 - 13s - loss: 455.2976 - loglik: -4.5411e+02 - logprior: -1.1922e+00
Fitted a model with MAP estimate = -450.7850
expansions: [(3, 1)]
discards: [  0  26  85 139 142 189 190 191 192 203 205]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 459.3889 - loglik: -4.5696e+02 - logprior: -2.4256e+00
Epoch 2/2
39/39 - 13s - loss: 455.0598 - loglik: -4.5434e+02 - logprior: -7.1949e-01
Fitted a model with MAP estimate = -451.3284
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 456.4969 - loglik: -4.5411e+02 - logprior: -2.3916e+00
Epoch 2/10
39/39 - 13s - loss: 452.9994 - loglik: -4.5184e+02 - logprior: -1.1550e+00
Epoch 3/10
39/39 - 13s - loss: 451.1617 - loglik: -4.5086e+02 - logprior: -3.0643e-01
Epoch 4/10
39/39 - 13s - loss: 450.9283 - loglik: -4.5068e+02 - logprior: -2.5211e-01
Epoch 5/10
39/39 - 13s - loss: 450.8835 - loglik: -4.5069e+02 - logprior: -1.8978e-01
Epoch 6/10
39/39 - 13s - loss: 449.7742 - loglik: -4.4964e+02 - logprior: -1.3293e-01
Epoch 7/10
39/39 - 13s - loss: 450.3640 - loglik: -4.5028e+02 - logprior: -8.1627e-02
Fitted a model with MAP estimate = -449.8956
Time for alignment: 305.0160
Computed alignments with likelihoods: ['-449.8197', '-449.8279', '-449.9552', '-450.0180', '-449.8956']
Best model has likelihood: -449.8197  (prior= -0.1091 )
time for generating output: 0.3566
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13561.projection.fasta
SP score = 0.5986089104333284
Training of 5 independent models on file PF05746.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd2d80c9d90>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd2d80c9700>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9490>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c9b20>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c96d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd2d80c90a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9a00>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c92b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c91f0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9310>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9100>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c98e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9880>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c93d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9c10>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2d80c9430> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2d80c9af0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2d80c9ac0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd2d8090040> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd125f27370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fcb70d5c2b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd12712cc10>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fd3fa684790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fd3eeffc310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2d80c91c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 321.2963 - loglik: -3.1822e+02 - logprior: -3.0729e+00
Epoch 2/10
19/19 - 2s - loss: 279.0650 - loglik: -2.7771e+02 - logprior: -1.3508e+00
Epoch 3/10
19/19 - 2s - loss: 256.3918 - loglik: -2.5477e+02 - logprior: -1.6170e+00
Epoch 4/10
19/19 - 2s - loss: 250.8055 - loglik: -2.4925e+02 - logprior: -1.5545e+00
Epoch 5/10
19/19 - 2s - loss: 247.7778 - loglik: -2.4624e+02 - logprior: -1.5411e+00
Epoch 6/10
19/19 - 2s - loss: 246.2769 - loglik: -2.4478e+02 - logprior: -1.5009e+00
Epoch 7/10
19/19 - 2s - loss: 246.3402 - loglik: -2.4485e+02 - logprior: -1.4880e+00
Fitted a model with MAP estimate = -245.9258
expansions: [(17, 1), (18, 2), (19, 3), (20, 1), (22, 1), (23, 1), (29, 1), (32, 2), (34, 3), (40, 1), (44, 1), (48, 1), (49, 1), (63, 2), (64, 2), (65, 2), (69, 2), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 251.3239 - loglik: -2.4743e+02 - logprior: -3.8897e+00
Epoch 2/2
19/19 - 3s - loss: 239.7803 - loglik: -2.3793e+02 - logprior: -1.8501e+00
Fitted a model with MAP estimate = -237.7511
expansions: [(0, 2)]
discards: [ 0 15 16 19 42 46 82 84 88 93]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 240.4807 - loglik: -2.3757e+02 - logprior: -2.9072e+00
Epoch 2/2
19/19 - 3s - loss: 236.6851 - loglik: -2.3565e+02 - logprior: -1.0310e+00
Fitted a model with MAP estimate = -235.7549
expansions: [(21, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 242.3292 - loglik: -2.3868e+02 - logprior: -3.6481e+00
Epoch 2/10
19/19 - 3s - loss: 237.7877 - loglik: -2.3677e+02 - logprior: -1.0224e+00
Epoch 3/10
19/19 - 3s - loss: 235.5347 - loglik: -2.3483e+02 - logprior: -7.0841e-01
Epoch 4/10
19/19 - 3s - loss: 235.2965 - loglik: -2.3457e+02 - logprior: -7.2767e-01
Epoch 5/10
19/19 - 3s - loss: 234.4845 - loglik: -2.3368e+02 - logprior: -8.0608e-01
Epoch 6/10
19/19 - 3s - loss: 233.9916 - loglik: -2.3317e+02 - logprior: -8.2230e-01
Epoch 7/10
19/19 - 3s - loss: 234.3839 - loglik: -2.3360e+02 - logprior: -7.8584e-01
Fitted a model with MAP estimate = -233.8473
Time for alignment: 78.7109
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 321.4612 - loglik: -3.1839e+02 - logprior: -3.0729e+00
Epoch 2/10
19/19 - 2s - loss: 279.3284 - loglik: -2.7800e+02 - logprior: -1.3272e+00
Epoch 3/10
19/19 - 2s - loss: 256.7011 - loglik: -2.5506e+02 - logprior: -1.6449e+00
Epoch 4/10
19/19 - 2s - loss: 250.4735 - loglik: -2.4885e+02 - logprior: -1.6209e+00
Epoch 5/10
19/19 - 2s - loss: 246.9923 - loglik: -2.4535e+02 - logprior: -1.6422e+00
Epoch 6/10
19/19 - 2s - loss: 245.9165 - loglik: -2.4430e+02 - logprior: -1.6155e+00
Epoch 7/10
19/19 - 2s - loss: 245.4310 - loglik: -2.4385e+02 - logprior: -1.5817e+00
Epoch 8/10
19/19 - 2s - loss: 245.1903 - loglik: -2.4363e+02 - logprior: -1.5633e+00
Epoch 9/10
19/19 - 2s - loss: 245.5442 - loglik: -2.4400e+02 - logprior: -1.5478e+00
Fitted a model with MAP estimate = -244.9135
expansions: [(17, 1), (18, 2), (19, 3), (20, 1), (22, 1), (23, 1), (29, 1), (33, 2), (38, 1), (40, 1), (42, 1), (44, 1), (48, 1), (49, 1), (59, 1), (64, 2), (65, 2), (70, 1), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 250.5993 - loglik: -2.4671e+02 - logprior: -3.8938e+00
Epoch 2/2
19/19 - 3s - loss: 239.3925 - loglik: -2.3749e+02 - logprior: -1.9059e+00
Fitted a model with MAP estimate = -237.4505
expansions: [(0, 2)]
discards: [ 0 15 16 42 82]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 239.6455 - loglik: -2.3672e+02 - logprior: -2.9215e+00
Epoch 2/2
19/19 - 3s - loss: 235.8636 - loglik: -2.3479e+02 - logprior: -1.0737e+00
Fitted a model with MAP estimate = -235.0211
expansions: [(22, 1)]
discards: [ 0 18]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 241.2102 - loglik: -2.3748e+02 - logprior: -3.7311e+00
Epoch 2/10
19/19 - 3s - loss: 236.7090 - loglik: -2.3550e+02 - logprior: -1.2110e+00
Epoch 3/10
19/19 - 3s - loss: 235.1944 - loglik: -2.3418e+02 - logprior: -1.0153e+00
Epoch 4/10
19/19 - 3s - loss: 233.7964 - loglik: -2.3282e+02 - logprior: -9.7784e-01
Epoch 5/10
19/19 - 3s - loss: 233.2357 - loglik: -2.3229e+02 - logprior: -9.4935e-01
Epoch 6/10
19/19 - 3s - loss: 233.3309 - loglik: -2.3240e+02 - logprior: -9.3052e-01
Fitted a model with MAP estimate = -232.9894
Time for alignment: 79.4535
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 321.4899 - loglik: -3.1842e+02 - logprior: -3.0710e+00
Epoch 2/10
19/19 - 2s - loss: 280.6277 - loglik: -2.7930e+02 - logprior: -1.3268e+00
Epoch 3/10
19/19 - 2s - loss: 256.6107 - loglik: -2.5493e+02 - logprior: -1.6778e+00
Epoch 4/10
19/19 - 2s - loss: 248.4754 - loglik: -2.4682e+02 - logprior: -1.6576e+00
Epoch 5/10
19/19 - 2s - loss: 246.3640 - loglik: -2.4471e+02 - logprior: -1.6494e+00
Epoch 6/10
19/19 - 2s - loss: 245.2635 - loglik: -2.4365e+02 - logprior: -1.6157e+00
Epoch 7/10
19/19 - 2s - loss: 244.7004 - loglik: -2.4312e+02 - logprior: -1.5768e+00
Epoch 8/10
19/19 - 2s - loss: 243.7138 - loglik: -2.4217e+02 - logprior: -1.5421e+00
Epoch 9/10
19/19 - 2s - loss: 243.6722 - loglik: -2.4214e+02 - logprior: -1.5300e+00
Epoch 10/10
19/19 - 2s - loss: 243.6383 - loglik: -2.4211e+02 - logprior: -1.5250e+00
Fitted a model with MAP estimate = -243.1997
expansions: [(17, 1), (18, 3), (19, 4), (21, 2), (29, 2), (31, 2), (32, 1), (40, 1), (42, 1), (43, 1), (53, 1), (56, 1), (63, 1), (64, 1), (66, 2), (69, 1), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 250.2274 - loglik: -2.4631e+02 - logprior: -3.9154e+00
Epoch 2/2
19/19 - 3s - loss: 238.9167 - loglik: -2.3696e+02 - logprior: -1.9546e+00
Fitted a model with MAP estimate = -236.9548
expansions: [(0, 2)]
discards: [ 0 15 16 22 23 28 39 43 87]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 240.0192 - loglik: -2.3712e+02 - logprior: -2.8970e+00
Epoch 2/2
19/19 - 3s - loss: 236.2217 - loglik: -2.3517e+02 - logprior: -1.0513e+00
Fitted a model with MAP estimate = -235.3856
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 241.6361 - loglik: -2.3794e+02 - logprior: -3.6943e+00
Epoch 2/10
19/19 - 3s - loss: 236.8978 - loglik: -2.3572e+02 - logprior: -1.1792e+00
Epoch 3/10
19/19 - 3s - loss: 235.2999 - loglik: -2.3431e+02 - logprior: -9.9461e-01
Epoch 4/10
19/19 - 3s - loss: 234.7051 - loglik: -2.3373e+02 - logprior: -9.7793e-01
Epoch 5/10
19/19 - 3s - loss: 234.1897 - loglik: -2.3324e+02 - logprior: -9.5259e-01
Epoch 6/10
19/19 - 3s - loss: 233.8277 - loglik: -2.3290e+02 - logprior: -9.2849e-01
Epoch 7/10
19/19 - 3s - loss: 233.4702 - loglik: -2.3256e+02 - logprior: -9.0573e-01
Epoch 8/10
19/19 - 3s - loss: 233.6092 - loglik: -2.3272e+02 - logprior: -8.9045e-01
Fitted a model with MAP estimate = -233.2688
Time for alignment: 87.9037
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 321.4823 - loglik: -3.1841e+02 - logprior: -3.0748e+00
Epoch 2/10
19/19 - 2s - loss: 278.9350 - loglik: -2.7760e+02 - logprior: -1.3380e+00
Epoch 3/10
19/19 - 2s - loss: 255.2595 - loglik: -2.5363e+02 - logprior: -1.6307e+00
Epoch 4/10
19/19 - 2s - loss: 248.6363 - loglik: -2.4703e+02 - logprior: -1.6110e+00
Epoch 5/10
19/19 - 2s - loss: 246.5313 - loglik: -2.4492e+02 - logprior: -1.6065e+00
Epoch 6/10
19/19 - 2s - loss: 245.0661 - loglik: -2.4352e+02 - logprior: -1.5508e+00
Epoch 7/10
19/19 - 2s - loss: 244.9927 - loglik: -2.4346e+02 - logprior: -1.5327e+00
Epoch 8/10
19/19 - 2s - loss: 245.0804 - loglik: -2.4357e+02 - logprior: -1.5144e+00
Fitted a model with MAP estimate = -244.5386
expansions: [(17, 1), (18, 2), (19, 3), (20, 1), (22, 1), (23, 1), (29, 2), (31, 2), (32, 1), (34, 1), (40, 1), (44, 1), (48, 1), (49, 1), (63, 1), (64, 2), (65, 2), (69, 2), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 251.2081 - loglik: -2.4733e+02 - logprior: -3.8734e+00
Epoch 2/2
19/19 - 3s - loss: 239.4601 - loglik: -2.3763e+02 - logprior: -1.8312e+00
Fitted a model with MAP estimate = -237.7491
expansions: [(0, 2)]
discards: [ 0 15 16 38 42 83 87 92]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 240.5354 - loglik: -2.3763e+02 - logprior: -2.9092e+00
Epoch 2/2
19/19 - 3s - loss: 236.3132 - loglik: -2.3528e+02 - logprior: -1.0299e+00
Fitted a model with MAP estimate = -235.6054
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 241.8836 - loglik: -2.3825e+02 - logprior: -3.6309e+00
Epoch 2/10
19/19 - 3s - loss: 237.5674 - loglik: -2.3658e+02 - logprior: -9.9183e-01
Epoch 3/10
19/19 - 3s - loss: 235.4144 - loglik: -2.3474e+02 - logprior: -6.7088e-01
Epoch 4/10
19/19 - 3s - loss: 234.7785 - loglik: -2.3411e+02 - logprior: -6.7094e-01
Epoch 5/10
19/19 - 3s - loss: 234.5446 - loglik: -2.3384e+02 - logprior: -7.0696e-01
Epoch 6/10
19/19 - 3s - loss: 233.6868 - loglik: -2.3293e+02 - logprior: -7.5672e-01
Epoch 7/10
19/19 - 3s - loss: 233.4438 - loglik: -2.3267e+02 - logprior: -7.7107e-01
Epoch 8/10
19/19 - 3s - loss: 234.0298 - loglik: -2.3326e+02 - logprior: -7.7409e-01
Fitted a model with MAP estimate = -233.3640
Time for alignment: 84.7718
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 321.4421 - loglik: -3.1837e+02 - logprior: -3.0768e+00
Epoch 2/10
19/19 - 2s - loss: 277.7437 - loglik: -2.7639e+02 - logprior: -1.3512e+00
Epoch 3/10
19/19 - 2s - loss: 254.2124 - loglik: -2.5251e+02 - logprior: -1.7056e+00
Epoch 4/10
19/19 - 2s - loss: 247.7307 - loglik: -2.4609e+02 - logprior: -1.6431e+00
Epoch 5/10
19/19 - 2s - loss: 245.6337 - loglik: -2.4394e+02 - logprior: -1.6921e+00
Epoch 6/10
19/19 - 2s - loss: 244.7678 - loglik: -2.4307e+02 - logprior: -1.6957e+00
Epoch 7/10
19/19 - 2s - loss: 244.8244 - loglik: -2.4316e+02 - logprior: -1.6619e+00
Fitted a model with MAP estimate = -244.3019
expansions: [(17, 1), (18, 2), (19, 3), (20, 1), (22, 1), (23, 1), (29, 2), (31, 1), (32, 1), (40, 1), (43, 1), (44, 1), (54, 1), (56, 1), (63, 1), (64, 1), (66, 2), (70, 1), (71, 1), (74, 1), (76, 1), (79, 1), (81, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 249.6831 - loglik: -2.4573e+02 - logprior: -3.9539e+00
Epoch 2/2
19/19 - 3s - loss: 239.3476 - loglik: -2.3742e+02 - logprior: -1.9245e+00
Fitted a model with MAP estimate = -237.7225
expansions: [(0, 2)]
discards: [ 0 15 16 37 85]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 240.3274 - loglik: -2.3740e+02 - logprior: -2.9321e+00
Epoch 2/2
19/19 - 3s - loss: 236.6405 - loglik: -2.3553e+02 - logprior: -1.1066e+00
Fitted a model with MAP estimate = -235.8557
expansions: [(22, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 242.3252 - loglik: -2.3857e+02 - logprior: -3.7516e+00
Epoch 2/10
19/19 - 3s - loss: 237.3888 - loglik: -2.3617e+02 - logprior: -1.2178e+00
Epoch 3/10
19/19 - 3s - loss: 235.4510 - loglik: -2.3443e+02 - logprior: -1.0174e+00
Epoch 4/10
19/19 - 3s - loss: 234.9890 - loglik: -2.3400e+02 - logprior: -9.8700e-01
Epoch 5/10
19/19 - 3s - loss: 234.1264 - loglik: -2.3317e+02 - logprior: -9.5328e-01
Epoch 6/10
19/19 - 3s - loss: 234.1415 - loglik: -2.3321e+02 - logprior: -9.3022e-01
Fitted a model with MAP estimate = -233.7600
Time for alignment: 75.4714
Computed alignments with likelihoods: ['-233.8473', '-232.9894', '-233.2688', '-233.3640', '-233.7600']
Best model has likelihood: -232.9894  (prior= -0.9090 )
time for generating output: 0.1498
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF05746.projection.fasta
SP score = 0.8486997635933806
