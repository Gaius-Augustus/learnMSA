Training of 3 independent models on file PF00079.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b41bf1eb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4af5a23670>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5907e20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b186fbcd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f497ff5bb80>, <__main__.SimpleDirichletPrior object at 0x7f4af5a23bb0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 42s - loss: 770.6315 - loglik: -7.6817e+02 - logprior: -2.0227e+00
Epoch 2/10
39/39 - 41s - loss: 641.2032 - loglik: -6.3701e+02 - logprior: -2.7671e+00
Epoch 3/10
39/39 - 41s - loss: 630.1921 - loglik: -6.2586e+02 - logprior: -2.7596e+00
Epoch 4/10
39/39 - 42s - loss: 626.7583 - loglik: -6.2268e+02 - logprior: -2.7685e+00
Epoch 5/10
39/39 - 43s - loss: 625.3323 - loglik: -6.2132e+02 - logprior: -2.8222e+00
Epoch 6/10
39/39 - 44s - loss: 624.5997 - loglik: -6.2067e+02 - logprior: -2.8455e+00
Epoch 7/10
39/39 - 45s - loss: 623.7079 - loglik: -6.1976e+02 - logprior: -2.8955e+00
Epoch 8/10
39/39 - 46s - loss: 623.7625 - loglik: -6.1990e+02 - logprior: -2.8992e+00
Fitted a model with MAP estimate = -621.6702
expansions: [(13, 1), (14, 1), (15, 1), (46, 1), (52, 5), (53, 1), (55, 1), (62, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (77, 1), (78, 1), (83, 1), (84, 1), (85, 1), (86, 1), (91, 1), (95, 1), (98, 1), (100, 1), (103, 1), (113, 1), (115, 1), (120, 1), (121, 1), (139, 1), (140, 1), (143, 1), (146, 1), (148, 1), (158, 1), (159, 1), (160, 1), (162, 1), (163, 1), (165, 1), (166, 1), (172, 1), (185, 1), (188, 1), (192, 1), (194, 1), (195, 1), (196, 1), (198, 1), (199, 2), (200, 1), (201, 1), (213, 1), (214, 1), (217, 1), (219, 1), (223, 1), (228, 1), (233, 1), (235, 2), (237, 1), (262, 1), (263, 1), (264, 1), (265, 3), (267, 1), (268, 1), (277, 1), (278, 2), (279, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 72s - loss: 627.5670 - loglik: -6.2489e+02 - logprior: -2.3649e+00
Epoch 2/2
39/39 - 70s - loss: 602.9661 - loglik: -6.0047e+02 - logprior: -1.3131e+00
Fitted a model with MAP estimate = -596.4531
expansions: [(16, 1), (146, 1)]
discards: [ 58  59  60 250]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 74s - loss: 612.2850 - loglik: -6.1007e+02 - logprior: -1.9288e+00
Epoch 2/2
39/39 - 69s - loss: 601.5585 - loglik: -5.9971e+02 - logprior: -1.0077e+00
Fitted a model with MAP estimate = -595.8627
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 70s - loss: 611.2617 - loglik: -6.0927e+02 - logprior: -1.6910e+00
Epoch 2/10
39/39 - 66s - loss: 601.7698 - loglik: -5.9992e+02 - logprior: -6.4420e-01
Epoch 3/10
39/39 - 65s - loss: 596.2794 - loglik: -5.9398e+02 - logprior: -6.4890e-01
Epoch 4/10
39/39 - 67s - loss: 594.4741 - loglik: -5.9224e+02 - logprior: -5.8044e-01
Epoch 5/10
39/39 - 74s - loss: 593.4990 - loglik: -5.9149e+02 - logprior: -4.5410e-01
Epoch 6/10
39/39 - 73s - loss: 592.9990 - loglik: -5.9123e+02 - logprior: -3.5717e-01
Epoch 7/10
39/39 - 73s - loss: 591.4144 - loglik: -5.8996e+02 - logprior: -1.4320e-01
Epoch 8/10
39/39 - 77s - loss: 591.1778 - loglik: -5.8992e+02 - logprior: -4.2375e-02
Epoch 9/10
39/39 - 83s - loss: 590.7744 - loglik: -5.8959e+02 - logprior: -4.1276e-02
Epoch 10/10
39/39 - 79s - loss: 590.7301 - loglik: -5.8982e+02 - logprior: 0.1609
Fitted a model with MAP estimate = -587.9987
Time for alignment: 1609.6869
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 56s - loss: 771.6373 - loglik: -7.6918e+02 - logprior: -2.0189e+00
Epoch 2/10
39/39 - 55s - loss: 642.9276 - loglik: -6.3907e+02 - logprior: -2.8503e+00
Epoch 3/10
39/39 - 55s - loss: 631.2731 - loglik: -6.2705e+02 - logprior: -2.8935e+00
Epoch 4/10
39/39 - 57s - loss: 628.5859 - loglik: -6.2441e+02 - logprior: -2.8545e+00
Epoch 5/10
39/39 - 57s - loss: 627.7404 - loglik: -6.2357e+02 - logprior: -2.8601e+00
Epoch 6/10
39/39 - 54s - loss: 625.7546 - loglik: -6.2165e+02 - logprior: -2.8710e+00
Epoch 7/10
39/39 - 52s - loss: 626.4471 - loglik: -6.2242e+02 - logprior: -2.8933e+00
Fitted a model with MAP estimate = -623.6847
expansions: [(9, 1), (12, 1), (14, 1), (15, 1), (34, 1), (49, 1), (51, 4), (55, 1), (61, 2), (62, 1), (63, 1), (65, 1), (67, 1), (68, 1), (77, 1), (78, 1), (81, 1), (83, 1), (84, 1), (85, 1), (90, 1), (91, 1), (98, 1), (99, 1), (100, 1), (106, 1), (108, 1), (117, 1), (119, 1), (121, 1), (134, 1), (140, 1), (143, 1), (146, 1), (157, 1), (158, 1), (159, 1), (160, 1), (162, 1), (163, 1), (165, 2), (185, 2), (188, 1), (191, 1), (193, 1), (194, 1), (195, 1), (197, 1), (200, 1), (201, 1), (213, 1), (214, 1), (217, 1), (220, 1), (229, 1), (234, 2), (235, 3), (263, 1), (264, 1), (265, 1), (266, 2), (267, 3), (268, 1), (277, 1), (279, 1), (280, 3)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 368 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 83s - loss: 629.6118 - loglik: -6.2686e+02 - logprior: -2.4371e+00
Epoch 2/2
39/39 - 68s - loss: 602.8502 - loglik: -6.0028e+02 - logprior: -1.4014e+00
Fitted a model with MAP estimate = -595.8900
expansions: [(208, 1)]
discards: [ 74 295 338]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 71s - loss: 612.6837 - loglik: -6.1041e+02 - logprior: -1.9901e+00
Epoch 2/2
39/39 - 72s - loss: 601.2715 - loglik: -5.9933e+02 - logprior: -1.0271e+00
Fitted a model with MAP estimate = -595.6953
expansions: [(144, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 367 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 84s - loss: 610.6845 - loglik: -6.0871e+02 - logprior: -1.6763e+00
Epoch 2/10
39/39 - 81s - loss: 600.8262 - loglik: -5.9894e+02 - logprior: -7.0654e-01
Epoch 3/10
39/39 - 79s - loss: 595.3645 - loglik: -5.9302e+02 - logprior: -6.9842e-01
Epoch 4/10
39/39 - 83s - loss: 593.5235 - loglik: -5.9127e+02 - logprior: -5.6960e-01
Epoch 5/10
39/39 - 84s - loss: 592.0970 - loglik: -5.9007e+02 - logprior: -4.1617e-01
Epoch 6/10
39/39 - 79s - loss: 592.2751 - loglik: -5.9050e+02 - logprior: -3.1421e-01
Fitted a model with MAP estimate = -588.9983
Time for alignment: 1430.7114
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 771.6095 - loglik: -7.6915e+02 - logprior: -2.0127e+00
Epoch 2/10
39/39 - 47s - loss: 642.4513 - loglik: -6.3794e+02 - logprior: -2.6424e+00
Epoch 3/10
39/39 - 46s - loss: 631.6998 - loglik: -6.2725e+02 - logprior: -2.6534e+00
Epoch 4/10
39/39 - 46s - loss: 628.0339 - loglik: -6.2383e+02 - logprior: -2.7310e+00
Epoch 5/10
39/39 - 45s - loss: 626.7206 - loglik: -6.2268e+02 - logprior: -2.7531e+00
Epoch 6/10
39/39 - 46s - loss: 624.7598 - loglik: -6.2080e+02 - logprior: -2.7855e+00
Epoch 7/10
39/39 - 46s - loss: 625.0782 - loglik: -6.2117e+02 - logprior: -2.8061e+00
Fitted a model with MAP estimate = -622.9839
expansions: [(12, 2), (13, 1), (14, 1), (34, 1), (46, 1), (51, 4), (55, 1), (61, 2), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (76, 1), (77, 1), (83, 1), (84, 1), (85, 1), (87, 1), (89, 1), (90, 1), (95, 1), (98, 1), (99, 1), (101, 1), (111, 1), (117, 1), (118, 1), (119, 1), (139, 1), (142, 1), (145, 1), (147, 1), (157, 1), (158, 1), (161, 1), (162, 2), (164, 2), (173, 1), (184, 1), (187, 1), (191, 1), (193, 1), (194, 1), (195, 1), (197, 1), (198, 2), (199, 1), (200, 1), (204, 1), (212, 1), (216, 1), (219, 1), (228, 1), (235, 2), (236, 2), (263, 1), (264, 1), (265, 1), (266, 2), (267, 3), (268, 1), (277, 1), (279, 1), (280, 3)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 369 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 68s - loss: 629.1845 - loglik: -6.2652e+02 - logprior: -2.3519e+00
Epoch 2/2
39/39 - 65s - loss: 602.5096 - loglik: -6.0003e+02 - logprior: -1.3874e+00
Fitted a model with MAP estimate = -595.9300
expansions: [(147, 1)]
discards: [ 58  74 251 299 339]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 67s - loss: 612.6953 - loglik: -6.1044e+02 - logprior: -1.9538e+00
Epoch 2/2
39/39 - 64s - loss: 601.9281 - loglik: -5.9985e+02 - logprior: -1.0152e+00
Fitted a model with MAP estimate = -596.0356
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 71s - loss: 611.0076 - loglik: -6.0900e+02 - logprior: -1.7201e+00
Epoch 2/10
39/39 - 71s - loss: 601.0019 - loglik: -5.9929e+02 - logprior: -7.5572e-01
Epoch 3/10
39/39 - 74s - loss: 595.8354 - loglik: -5.9376e+02 - logprior: -6.9713e-01
Epoch 4/10
39/39 - 72s - loss: 594.1583 - loglik: -5.9208e+02 - logprior: -5.4609e-01
Epoch 5/10
39/39 - 73s - loss: 593.9503 - loglik: -5.9195e+02 - logprior: -4.8715e-01
Epoch 6/10
39/39 - 75s - loss: 592.0578 - loglik: -5.9032e+02 - logprior: -3.2687e-01
Epoch 7/10
39/39 - 72s - loss: 592.7416 - loglik: -5.9117e+02 - logprior: -2.4421e-01
Fitted a model with MAP estimate = -589.5871
Time for alignment: 1328.5291
Computed alignments with likelihoods: ['-587.9987', '-588.9983', '-589.5871']
Best model has likelihood: -587.9987
time for generating output: 0.3256
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00079.projection.fasta
SP score = 0.9242919389978214
Training of 3 independent models on file PF01381.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4af5907e20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b186fbcd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b41bf1eb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4af5a23670>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4800075b80>, <__main__.SimpleDirichletPrior object at 0x7f497c8117c0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.1473 - loglik: -1.4685e+02 - logprior: -3.2689e+00
Epoch 2/10
19/19 - 1s - loss: 129.4527 - loglik: -1.2764e+02 - logprior: -1.4761e+00
Epoch 3/10
19/19 - 1s - loss: 118.2841 - loglik: -1.1617e+02 - logprior: -1.7310e+00
Epoch 4/10
19/19 - 1s - loss: 116.0688 - loglik: -1.1412e+02 - logprior: -1.6257e+00
Epoch 5/10
19/19 - 1s - loss: 115.6417 - loglik: -1.1378e+02 - logprior: -1.5965e+00
Epoch 6/10
19/19 - 1s - loss: 115.3004 - loglik: -1.1350e+02 - logprior: -1.5577e+00
Epoch 7/10
19/19 - 1s - loss: 115.1537 - loglik: -1.1338e+02 - logprior: -1.5457e+00
Epoch 8/10
19/19 - 1s - loss: 115.0141 - loglik: -1.1326e+02 - logprior: -1.5408e+00
Epoch 9/10
19/19 - 1s - loss: 114.9994 - loglik: -1.1326e+02 - logprior: -1.5367e+00
Epoch 10/10
19/19 - 1s - loss: 114.8545 - loglik: -1.1313e+02 - logprior: -1.5326e+00
Fitted a model with MAP estimate = -114.4672
expansions: [(6, 1), (7, 1), (11, 1), (15, 1), (19, 2), (23, 2), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 123.5375 - loglik: -1.1920e+02 - logprior: -4.2488e+00
Epoch 2/2
19/19 - 1s - loss: 113.7102 - loglik: -1.1112e+02 - logprior: -2.2284e+00
Fitted a model with MAP estimate = -111.6324
expansions: [(0, 2)]
discards: [ 0 22 28 35 42]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 113.7930 - loglik: -1.1061e+02 - logprior: -3.1146e+00
Epoch 2/2
19/19 - 1s - loss: 110.1183 - loglik: -1.0849e+02 - logprior: -1.3187e+00
Fitted a model with MAP estimate = -108.8667
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.3551 - loglik: -1.1143e+02 - logprior: -3.8663e+00
Epoch 2/10
19/19 - 1s - loss: 110.6360 - loglik: -1.0888e+02 - logprior: -1.5232e+00
Epoch 3/10
19/19 - 1s - loss: 109.4687 - loglik: -1.0766e+02 - logprior: -1.3788e+00
Epoch 4/10
19/19 - 1s - loss: 108.8076 - loglik: -1.0703e+02 - logprior: -1.3440e+00
Epoch 5/10
19/19 - 1s - loss: 108.6307 - loglik: -1.0692e+02 - logprior: -1.3308e+00
Epoch 6/10
19/19 - 1s - loss: 108.3361 - loglik: -1.0668e+02 - logprior: -1.3174e+00
Epoch 7/10
19/19 - 1s - loss: 108.0725 - loglik: -1.0646e+02 - logprior: -1.3005e+00
Epoch 8/10
19/19 - 1s - loss: 107.9570 - loglik: -1.0637e+02 - logprior: -1.2898e+00
Epoch 9/10
19/19 - 1s - loss: 107.8009 - loglik: -1.0624e+02 - logprior: -1.2772e+00
Epoch 10/10
19/19 - 1s - loss: 107.6656 - loglik: -1.0613e+02 - logprior: -1.2718e+00
Fitted a model with MAP estimate = -107.2745
Time for alignment: 55.0765
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 150.0480 - loglik: -1.4676e+02 - logprior: -3.2688e+00
Epoch 2/10
19/19 - 1s - loss: 127.4038 - loglik: -1.2564e+02 - logprior: -1.4618e+00
Epoch 3/10
19/19 - 1s - loss: 118.8300 - loglik: -1.1662e+02 - logprior: -1.7142e+00
Epoch 4/10
19/19 - 1s - loss: 117.0329 - loglik: -1.1512e+02 - logprior: -1.6002e+00
Epoch 5/10
19/19 - 1s - loss: 116.6337 - loglik: -1.1481e+02 - logprior: -1.5746e+00
Epoch 6/10
19/19 - 1s - loss: 116.4904 - loglik: -1.1471e+02 - logprior: -1.5500e+00
Epoch 7/10
19/19 - 1s - loss: 116.0805 - loglik: -1.1434e+02 - logprior: -1.5479e+00
Epoch 8/10
19/19 - 1s - loss: 115.9048 - loglik: -1.1419e+02 - logprior: -1.5447e+00
Epoch 9/10
19/19 - 1s - loss: 116.0491 - loglik: -1.1433e+02 - logprior: -1.5331e+00
Fitted a model with MAP estimate = -115.4853
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (19, 2), (20, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 122.5600 - loglik: -1.1824e+02 - logprior: -4.2419e+00
Epoch 2/2
19/19 - 1s - loss: 113.5393 - loglik: -1.1108e+02 - logprior: -2.1831e+00
Fitted a model with MAP estimate = -111.5397
expansions: [(0, 2)]
discards: [ 0 23 34 41]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.7185 - loglik: -1.1053e+02 - logprior: -3.1168e+00
Epoch 2/2
19/19 - 1s - loss: 110.1837 - loglik: -1.0856e+02 - logprior: -1.3203e+00
Fitted a model with MAP estimate = -108.9245
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.2760 - loglik: -1.1138e+02 - logprior: -3.8412e+00
Epoch 2/10
19/19 - 1s - loss: 110.6489 - loglik: -1.0896e+02 - logprior: -1.5118e+00
Epoch 3/10
19/19 - 1s - loss: 109.8055 - loglik: -1.0811e+02 - logprior: -1.3667e+00
Epoch 4/10
19/19 - 1s - loss: 109.0591 - loglik: -1.0733e+02 - logprior: -1.3409e+00
Epoch 5/10
19/19 - 1s - loss: 108.7090 - loglik: -1.0702e+02 - logprior: -1.3234e+00
Epoch 6/10
19/19 - 1s - loss: 108.2818 - loglik: -1.0663e+02 - logprior: -1.3191e+00
Epoch 7/10
19/19 - 1s - loss: 108.1254 - loglik: -1.0652e+02 - logprior: -1.3032e+00
Epoch 8/10
19/19 - 1s - loss: 107.9570 - loglik: -1.0637e+02 - logprior: -1.2924e+00
Epoch 9/10
19/19 - 1s - loss: 107.7812 - loglik: -1.0622e+02 - logprior: -1.2772e+00
Epoch 10/10
19/19 - 1s - loss: 107.6672 - loglik: -1.0612e+02 - logprior: -1.2693e+00
Fitted a model with MAP estimate = -107.2928
Time for alignment: 54.1857
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.0361 - loglik: -1.4675e+02 - logprior: -3.2670e+00
Epoch 2/10
19/19 - 1s - loss: 128.4892 - loglik: -1.2677e+02 - logprior: -1.4504e+00
Epoch 3/10
19/19 - 1s - loss: 120.7692 - loglik: -1.1863e+02 - logprior: -1.6472e+00
Epoch 4/10
19/19 - 1s - loss: 118.8691 - loglik: -1.1694e+02 - logprior: -1.5378e+00
Epoch 5/10
19/19 - 1s - loss: 118.3220 - loglik: -1.1650e+02 - logprior: -1.5079e+00
Epoch 6/10
19/19 - 1s - loss: 117.8192 - loglik: -1.1605e+02 - logprior: -1.4793e+00
Epoch 7/10
19/19 - 1s - loss: 117.6033 - loglik: -1.1589e+02 - logprior: -1.4739e+00
Epoch 8/10
19/19 - 1s - loss: 117.4913 - loglik: -1.1581e+02 - logprior: -1.4622e+00
Epoch 9/10
19/19 - 1s - loss: 117.3477 - loglik: -1.1568e+02 - logprior: -1.4559e+00
Epoch 10/10
19/19 - 1s - loss: 117.2804 - loglik: -1.1562e+02 - logprior: -1.4504e+00
Fitted a model with MAP estimate = -116.8527
expansions: [(6, 2), (7, 2), (16, 1), (19, 2), (28, 2), (29, 2), (30, 2), (31, 2), (32, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 125.0690 - loglik: -1.2074e+02 - logprior: -4.2405e+00
Epoch 2/2
19/19 - 1s - loss: 114.0670 - loglik: -1.1136e+02 - logprior: -2.3047e+00
Fitted a model with MAP estimate = -111.6487
expansions: [(0, 2)]
discards: [ 0  8 23 35 37 41 46]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.8862 - loglik: -1.1070e+02 - logprior: -3.1168e+00
Epoch 2/2
19/19 - 1s - loss: 110.1234 - loglik: -1.0850e+02 - logprior: -1.3179e+00
Fitted a model with MAP estimate = -108.8124
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.4853 - loglik: -1.1157e+02 - logprior: -3.8569e+00
Epoch 2/10
19/19 - 1s - loss: 110.6727 - loglik: -1.0894e+02 - logprior: -1.5178e+00
Epoch 3/10
19/19 - 1s - loss: 109.5104 - loglik: -1.0773e+02 - logprior: -1.3742e+00
Epoch 4/10
19/19 - 1s - loss: 109.0164 - loglik: -1.0724e+02 - logprior: -1.3431e+00
Epoch 5/10
19/19 - 1s - loss: 108.5417 - loglik: -1.0684e+02 - logprior: -1.3246e+00
Epoch 6/10
19/19 - 1s - loss: 108.3926 - loglik: -1.0674e+02 - logprior: -1.3195e+00
Epoch 7/10
19/19 - 1s - loss: 108.0758 - loglik: -1.0647e+02 - logprior: -1.2993e+00
Epoch 8/10
19/19 - 1s - loss: 107.9251 - loglik: -1.0634e+02 - logprior: -1.2890e+00
Epoch 9/10
19/19 - 1s - loss: 107.7789 - loglik: -1.0622e+02 - logprior: -1.2740e+00
Epoch 10/10
19/19 - 1s - loss: 107.7350 - loglik: -1.0620e+02 - logprior: -1.2698e+00
Fitted a model with MAP estimate = -107.2699
Time for alignment: 54.4795
Computed alignments with likelihoods: ['-107.2745', '-107.2928', '-107.2699']
Best model has likelihood: -107.2699
time for generating output: 0.1173
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01381.projection.fasta
SP score = 0.6702533643224011
Training of 3 independent models on file PF02878.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b1837ff70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b2966c880>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afed76dc0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f498221e100>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f497dddcd90>, <__main__.SimpleDirichletPrior object at 0x7f4b0733b640>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 360.1431 - loglik: -3.5689e+02 - logprior: -3.1925e+00
Epoch 2/10
19/19 - 4s - loss: 302.5779 - loglik: -3.0100e+02 - logprior: -1.4208e+00
Epoch 3/10
19/19 - 4s - loss: 275.8292 - loglik: -2.7369e+02 - logprior: -1.7593e+00
Epoch 4/10
19/19 - 4s - loss: 270.2330 - loglik: -2.6791e+02 - logprior: -1.7643e+00
Epoch 5/10
19/19 - 4s - loss: 268.5499 - loglik: -2.6632e+02 - logprior: -1.7109e+00
Epoch 6/10
19/19 - 4s - loss: 267.4007 - loglik: -2.6525e+02 - logprior: -1.6651e+00
Epoch 7/10
19/19 - 4s - loss: 266.9028 - loglik: -2.6481e+02 - logprior: -1.6416e+00
Epoch 8/10
19/19 - 4s - loss: 266.5928 - loglik: -2.6454e+02 - logprior: -1.6179e+00
Epoch 9/10
19/19 - 4s - loss: 266.1612 - loglik: -2.6413e+02 - logprior: -1.6177e+00
Epoch 10/10
19/19 - 4s - loss: 265.6490 - loglik: -2.6364e+02 - logprior: -1.6095e+00
Fitted a model with MAP estimate = -265.4206
expansions: [(12, 2), (13, 2), (14, 2), (16, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (52, 1), (56, 2), (57, 2), (70, 1), (71, 2), (93, 1), (96, 1), (101, 1), (105, 1), (108, 1), (109, 1), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 273.4413 - loglik: -2.6910e+02 - logprior: -4.2447e+00
Epoch 2/2
19/19 - 5s - loss: 259.8525 - loglik: -2.5713e+02 - logprior: -2.3051e+00
Fitted a model with MAP estimate = -257.1008
expansions: [(0, 2)]
discards: [  0  17  38  74  76  93 140]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 261.8237 - loglik: -2.5870e+02 - logprior: -3.0380e+00
Epoch 2/2
19/19 - 5s - loss: 256.9513 - loglik: -2.5532e+02 - logprior: -1.2660e+00
Fitted a model with MAP estimate = -255.2465
expansions: []
discards: [ 0 16]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 262.9557 - loglik: -2.5890e+02 - logprior: -3.9776e+00
Epoch 2/10
19/19 - 5s - loss: 258.2297 - loglik: -2.5630e+02 - logprior: -1.5853e+00
Epoch 3/10
19/19 - 5s - loss: 255.7109 - loglik: -2.5409e+02 - logprior: -1.0616e+00
Epoch 4/10
19/19 - 5s - loss: 254.5473 - loglik: -2.5281e+02 - logprior: -1.0997e+00
Epoch 5/10
19/19 - 5s - loss: 254.0833 - loglik: -2.5245e+02 - logprior: -1.0361e+00
Epoch 6/10
19/19 - 5s - loss: 253.6796 - loglik: -2.5216e+02 - logprior: -9.8808e-01
Epoch 7/10
19/19 - 5s - loss: 252.8428 - loglik: -2.5140e+02 - logprior: -9.3769e-01
Epoch 8/10
19/19 - 5s - loss: 252.7340 - loglik: -2.5135e+02 - logprior: -9.1054e-01
Epoch 9/10
19/19 - 5s - loss: 252.7661 - loglik: -2.5144e+02 - logprior: -8.7428e-01
Fitted a model with MAP estimate = -252.0546
Time for alignment: 148.3948
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 360.3709 - loglik: -3.5712e+02 - logprior: -3.1875e+00
Epoch 2/10
19/19 - 4s - loss: 304.6281 - loglik: -3.0299e+02 - logprior: -1.4072e+00
Epoch 3/10
19/19 - 4s - loss: 277.0339 - loglik: -2.7451e+02 - logprior: -1.6666e+00
Epoch 4/10
19/19 - 4s - loss: 271.0469 - loglik: -2.6851e+02 - logprior: -1.6673e+00
Epoch 5/10
19/19 - 4s - loss: 268.2264 - loglik: -2.6589e+02 - logprior: -1.6401e+00
Epoch 6/10
19/19 - 4s - loss: 266.9402 - loglik: -2.6474e+02 - logprior: -1.5988e+00
Epoch 7/10
19/19 - 4s - loss: 266.8121 - loglik: -2.6467e+02 - logprior: -1.6188e+00
Epoch 8/10
19/19 - 4s - loss: 265.9367 - loglik: -2.6382e+02 - logprior: -1.6172e+00
Epoch 9/10
19/19 - 4s - loss: 265.8235 - loglik: -2.6371e+02 - logprior: -1.6147e+00
Epoch 10/10
19/19 - 4s - loss: 265.0582 - loglik: -2.6298e+02 - logprior: -1.6187e+00
Fitted a model with MAP estimate = -264.9036
expansions: [(12, 1), (14, 3), (16, 1), (17, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (93, 1), (97, 1), (101, 1), (107, 1), (108, 1), (109, 1), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 273.5724 - loglik: -2.6927e+02 - logprior: -4.2080e+00
Epoch 2/2
19/19 - 5s - loss: 259.7473 - loglik: -2.5714e+02 - logprior: -2.2029e+00
Fitted a model with MAP estimate = -256.9321
expansions: [(0, 3)]
discards: [  0  16  37  75 137]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 261.4975 - loglik: -2.5840e+02 - logprior: -3.0115e+00
Epoch 2/2
19/19 - 5s - loss: 256.8190 - loglik: -2.5513e+02 - logprior: -1.2884e+00
Fitted a model with MAP estimate = -254.9407
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 262.6289 - loglik: -2.5857e+02 - logprior: -3.9806e+00
Epoch 2/10
19/19 - 5s - loss: 258.4524 - loglik: -2.5640e+02 - logprior: -1.7485e+00
Epoch 3/10
19/19 - 5s - loss: 255.5463 - loglik: -2.5401e+02 - logprior: -1.0532e+00
Epoch 4/10
19/19 - 5s - loss: 254.3006 - loglik: -2.5274e+02 - logprior: -9.7756e-01
Epoch 5/10
19/19 - 5s - loss: 254.0741 - loglik: -2.5252e+02 - logprior: -9.7932e-01
Epoch 6/10
19/19 - 5s - loss: 252.9249 - loglik: -2.5147e+02 - logprior: -9.1758e-01
Epoch 7/10
19/19 - 5s - loss: 253.1707 - loglik: -2.5182e+02 - logprior: -8.6115e-01
Fitted a model with MAP estimate = -252.2956
Time for alignment: 137.9480
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 360.2224 - loglik: -3.5697e+02 - logprior: -3.1906e+00
Epoch 2/10
19/19 - 4s - loss: 303.2706 - loglik: -3.0170e+02 - logprior: -1.4203e+00
Epoch 3/10
19/19 - 4s - loss: 274.6503 - loglik: -2.7247e+02 - logprior: -1.7711e+00
Epoch 4/10
19/19 - 4s - loss: 269.5191 - loglik: -2.6714e+02 - logprior: -1.7970e+00
Epoch 5/10
19/19 - 4s - loss: 268.2747 - loglik: -2.6598e+02 - logprior: -1.7194e+00
Epoch 6/10
19/19 - 4s - loss: 266.5402 - loglik: -2.6432e+02 - logprior: -1.6894e+00
Epoch 7/10
19/19 - 4s - loss: 266.4908 - loglik: -2.6434e+02 - logprior: -1.6552e+00
Epoch 8/10
19/19 - 4s - loss: 265.6130 - loglik: -2.6348e+02 - logprior: -1.6502e+00
Epoch 9/10
19/19 - 4s - loss: 265.6846 - loglik: -2.6359e+02 - logprior: -1.6342e+00
Fitted a model with MAP estimate = -264.9920
expansions: [(12, 1), (14, 3), (16, 1), (17, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 2), (70, 1), (71, 1), (93, 1), (96, 1), (101, 1), (105, 1), (108, 1), (109, 1), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 274.1015 - loglik: -2.6978e+02 - logprior: -4.2214e+00
Epoch 2/2
19/19 - 5s - loss: 259.6274 - loglik: -2.5696e+02 - logprior: -2.2290e+00
Fitted a model with MAP estimate = -257.0834
expansions: [(0, 3)]
discards: [  0  16  37  75  88 138]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 261.7523 - loglik: -2.5865e+02 - logprior: -3.0189e+00
Epoch 2/2
19/19 - 5s - loss: 256.9457 - loglik: -2.5527e+02 - logprior: -1.2719e+00
Fitted a model with MAP estimate = -255.0153
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 262.4620 - loglik: -2.5840e+02 - logprior: -3.9803e+00
Epoch 2/10
19/19 - 5s - loss: 257.9195 - loglik: -2.5585e+02 - logprior: -1.6806e+00
Epoch 3/10
19/19 - 5s - loss: 255.4489 - loglik: -2.5382e+02 - logprior: -1.0448e+00
Epoch 4/10
19/19 - 5s - loss: 254.5465 - loglik: -2.5292e+02 - logprior: -1.0159e+00
Epoch 5/10
19/19 - 5s - loss: 253.8956 - loglik: -2.5233e+02 - logprior: -1.0021e+00
Epoch 6/10
19/19 - 5s - loss: 252.8404 - loglik: -2.5138e+02 - logprior: -9.4108e-01
Epoch 7/10
19/19 - 5s - loss: 253.9556 - loglik: -2.5258e+02 - logprior: -8.9153e-01
Fitted a model with MAP estimate = -252.3854
Time for alignment: 134.7560
Computed alignments with likelihoods: ['-252.0546', '-252.2956', '-252.3854']
Best model has likelihood: -252.0546
time for generating output: 0.1844
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02878.projection.fasta
SP score = 0.8397350993377484
Training of 3 independent models on file PF00970.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f47de594bb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f47d413f460>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d415c580>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f47d416f4c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f473b445610>, <__main__.SimpleDirichletPrior object at 0x7f473b3c5400>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 274.6926 - loglik: -2.7143e+02 - logprior: -3.1981e+00
Epoch 2/10
19/19 - 3s - loss: 236.7997 - loglik: -2.3496e+02 - logprior: -1.4122e+00
Epoch 3/10
19/19 - 3s - loss: 220.9135 - loglik: -2.1853e+02 - logprior: -1.6562e+00
Epoch 4/10
19/19 - 3s - loss: 217.0838 - loglik: -2.1491e+02 - logprior: -1.6614e+00
Epoch 5/10
19/19 - 3s - loss: 216.1204 - loglik: -2.1412e+02 - logprior: -1.6344e+00
Epoch 6/10
19/19 - 3s - loss: 215.3726 - loglik: -2.1344e+02 - logprior: -1.6202e+00
Epoch 7/10
19/19 - 3s - loss: 215.1237 - loglik: -2.1322e+02 - logprior: -1.5831e+00
Epoch 8/10
19/19 - 3s - loss: 215.0543 - loglik: -2.1316e+02 - logprior: -1.5768e+00
Epoch 9/10
19/19 - 3s - loss: 214.5766 - loglik: -2.1267e+02 - logprior: -1.5868e+00
Epoch 10/10
19/19 - 3s - loss: 214.7142 - loglik: -2.1280e+02 - logprior: -1.5821e+00
Fitted a model with MAP estimate = -213.8213
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (18, 2), (21, 1), (28, 1), (29, 1), (33, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 220.1306 - loglik: -2.1588e+02 - logprior: -4.1587e+00
Epoch 2/2
19/19 - 4s - loss: 207.6153 - loglik: -2.0573e+02 - logprior: -1.5196e+00
Fitted a model with MAP estimate = -205.0691
expansions: []
discards: [ 0 73 78 81 83 94]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 212.3732 - loglik: -2.0813e+02 - logprior: -4.1648e+00
Epoch 2/2
19/19 - 4s - loss: 207.6124 - loglik: -2.0565e+02 - logprior: -1.6646e+00
Fitted a model with MAP estimate = -205.4207
expansions: [(0, 2), (24, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 209.2377 - loglik: -2.0610e+02 - logprior: -3.0557e+00
Epoch 2/10
19/19 - 4s - loss: 205.7729 - loglik: -2.0423e+02 - logprior: -1.2667e+00
Epoch 3/10
19/19 - 4s - loss: 204.7719 - loglik: -2.0305e+02 - logprior: -1.2750e+00
Epoch 4/10
19/19 - 4s - loss: 204.0002 - loglik: -2.0227e+02 - logprior: -1.2449e+00
Epoch 5/10
19/19 - 4s - loss: 203.3672 - loglik: -2.0173e+02 - logprior: -1.2108e+00
Epoch 6/10
19/19 - 4s - loss: 203.5662 - loglik: -2.0201e+02 - logprior: -1.1909e+00
Fitted a model with MAP estimate = -202.7546
Time for alignment: 100.3942
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 274.9050 - loglik: -2.7164e+02 - logprior: -3.2040e+00
Epoch 2/10
19/19 - 3s - loss: 236.6097 - loglik: -2.3494e+02 - logprior: -1.4274e+00
Epoch 3/10
19/19 - 3s - loss: 220.0868 - loglik: -2.1797e+02 - logprior: -1.7173e+00
Epoch 4/10
19/19 - 3s - loss: 216.2765 - loglik: -2.1410e+02 - logprior: -1.7330e+00
Epoch 5/10
19/19 - 3s - loss: 215.1054 - loglik: -2.1303e+02 - logprior: -1.6610e+00
Epoch 6/10
19/19 - 3s - loss: 214.8719 - loglik: -2.1287e+02 - logprior: -1.6349e+00
Epoch 7/10
19/19 - 3s - loss: 214.1724 - loglik: -2.1221e+02 - logprior: -1.6233e+00
Epoch 8/10
19/19 - 3s - loss: 214.3180 - loglik: -2.1240e+02 - logprior: -1.5990e+00
Fitted a model with MAP estimate = -213.3111
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (17, 1), (18, 1), (21, 1), (28, 1), (29, 1), (35, 1), (46, 1), (59, 2), (61, 2), (63, 3), (70, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 218.9448 - loglik: -2.1478e+02 - logprior: -4.0758e+00
Epoch 2/2
19/19 - 4s - loss: 207.0719 - loglik: -2.0520e+02 - logprior: -1.5131e+00
Fitted a model with MAP estimate = -205.0234
expansions: []
discards: [ 0 73 77 93]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 211.4540 - loglik: -2.0719e+02 - logprior: -4.1835e+00
Epoch 2/2
19/19 - 4s - loss: 206.9985 - loglik: -2.0500e+02 - logprior: -1.6825e+00
Fitted a model with MAP estimate = -204.9804
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 208.6602 - loglik: -2.0552e+02 - logprior: -3.0613e+00
Epoch 2/10
19/19 - 4s - loss: 205.7536 - loglik: -2.0422e+02 - logprior: -1.2742e+00
Epoch 3/10
19/19 - 4s - loss: 204.5995 - loglik: -2.0300e+02 - logprior: -1.2778e+00
Epoch 4/10
19/19 - 4s - loss: 203.6512 - loglik: -2.0200e+02 - logprior: -1.2502e+00
Epoch 5/10
19/19 - 4s - loss: 203.3002 - loglik: -2.0169e+02 - logprior: -1.2133e+00
Epoch 6/10
19/19 - 4s - loss: 203.0277 - loglik: -2.0147e+02 - logprior: -1.1930e+00
Epoch 7/10
19/19 - 4s - loss: 203.2067 - loglik: -2.0170e+02 - logprior: -1.1727e+00
Fitted a model with MAP estimate = -202.3903
Time for alignment: 97.0537
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 274.6919 - loglik: -2.7143e+02 - logprior: -3.1980e+00
Epoch 2/10
19/19 - 3s - loss: 236.2710 - loglik: -2.3454e+02 - logprior: -1.4236e+00
Epoch 3/10
19/19 - 3s - loss: 220.2787 - loglik: -2.1804e+02 - logprior: -1.6634e+00
Epoch 4/10
19/19 - 3s - loss: 216.4938 - loglik: -2.1431e+02 - logprior: -1.6857e+00
Epoch 5/10
19/19 - 3s - loss: 215.5899 - loglik: -2.1356e+02 - logprior: -1.6118e+00
Epoch 6/10
19/19 - 3s - loss: 214.9678 - loglik: -2.1300e+02 - logprior: -1.5722e+00
Epoch 7/10
19/19 - 3s - loss: 214.5802 - loglik: -2.1269e+02 - logprior: -1.5490e+00
Epoch 8/10
19/19 - 3s - loss: 214.4286 - loglik: -2.1257e+02 - logprior: -1.5491e+00
Epoch 9/10
19/19 - 3s - loss: 214.2245 - loglik: -2.1235e+02 - logprior: -1.5657e+00
Epoch 10/10
19/19 - 3s - loss: 214.0403 - loglik: -2.1219e+02 - logprior: -1.5646e+00
Fitted a model with MAP estimate = -213.2819
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (17, 1), (18, 2), (21, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (74, 1), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 219.3750 - loglik: -2.1516e+02 - logprior: -4.1269e+00
Epoch 2/2
19/19 - 4s - loss: 207.2195 - loglik: -2.0533e+02 - logprior: -1.5225e+00
Fitted a model with MAP estimate = -204.8152
expansions: [(25, 1)]
discards: [ 0 45 74 79 82 84]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 211.9973 - loglik: -2.0777e+02 - logprior: -4.1474e+00
Epoch 2/2
19/19 - 4s - loss: 207.2652 - loglik: -2.0531e+02 - logprior: -1.6333e+00
Fitted a model with MAP estimate = -205.0015
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 208.7886 - loglik: -2.0568e+02 - logprior: -3.0318e+00
Epoch 2/10
19/19 - 4s - loss: 205.6416 - loglik: -2.0411e+02 - logprior: -1.2497e+00
Epoch 3/10
19/19 - 4s - loss: 204.4883 - loglik: -2.0280e+02 - logprior: -1.2574e+00
Epoch 4/10
19/19 - 4s - loss: 203.5334 - loglik: -2.0185e+02 - logprior: -1.2166e+00
Epoch 5/10
19/19 - 4s - loss: 203.5631 - loglik: -2.0195e+02 - logprior: -1.1867e+00
Fitted a model with MAP estimate = -202.6813
Time for alignment: 97.5531
Computed alignments with likelihoods: ['-202.7546', '-202.3903', '-202.6813']
Best model has likelihood: -202.3903
time for generating output: 0.2127
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00970.projection.fasta
SP score = 0.6707253769071048
Training of 3 independent models on file PF00313.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b20f65070>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4982034be0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4800266b50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f47b4574df0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f475c19dcd0>, <__main__.SimpleDirichletPrior object at 0x7f47ac79e340>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.4613 - loglik: -1.7314e+02 - logprior: -3.3039e+00
Epoch 2/10
19/19 - 2s - loss: 134.1673 - loglik: -1.3260e+02 - logprior: -1.5397e+00
Epoch 3/10
19/19 - 2s - loss: 117.1869 - loglik: -1.1546e+02 - logprior: -1.5581e+00
Epoch 4/10
19/19 - 2s - loss: 114.2933 - loglik: -1.1247e+02 - logprior: -1.6123e+00
Epoch 5/10
19/19 - 2s - loss: 113.4465 - loglik: -1.1173e+02 - logprior: -1.5458e+00
Epoch 6/10
19/19 - 2s - loss: 113.1744 - loglik: -1.1148e+02 - logprior: -1.5115e+00
Epoch 7/10
19/19 - 2s - loss: 112.6438 - loglik: -1.1097e+02 - logprior: -1.4959e+00
Epoch 8/10
19/19 - 2s - loss: 112.8438 - loglik: -1.1118e+02 - logprior: -1.4894e+00
Fitted a model with MAP estimate = -112.3790
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (36, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 114.5084 - loglik: -1.1020e+02 - logprior: -4.2517e+00
Epoch 2/2
19/19 - 2s - loss: 104.7352 - loglik: -1.0320e+02 - logprior: -1.4353e+00
Fitted a model with MAP estimate = -103.3977
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 109.3594 - loglik: -1.0515e+02 - logprior: -4.1724e+00
Epoch 2/2
19/19 - 2s - loss: 105.0177 - loglik: -1.0325e+02 - logprior: -1.6530e+00
Fitted a model with MAP estimate = -103.8584
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 107.6137 - loglik: -1.0427e+02 - logprior: -3.3057e+00
Epoch 2/10
19/19 - 2s - loss: 104.4569 - loglik: -1.0282e+02 - logprior: -1.5160e+00
Epoch 3/10
19/19 - 2s - loss: 103.6745 - loglik: -1.0204e+02 - logprior: -1.4445e+00
Epoch 4/10
19/19 - 2s - loss: 102.6060 - loglik: -1.0095e+02 - logprior: -1.4053e+00
Epoch 5/10
19/19 - 2s - loss: 102.1425 - loglik: -1.0052e+02 - logprior: -1.3696e+00
Epoch 6/10
19/19 - 2s - loss: 101.7979 - loglik: -1.0020e+02 - logprior: -1.3505e+00
Epoch 7/10
19/19 - 2s - loss: 101.8665 - loglik: -1.0030e+02 - logprior: -1.3280e+00
Fitted a model with MAP estimate = -101.2686
Time for alignment: 55.6086
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 176.5930 - loglik: -1.7328e+02 - logprior: -3.2992e+00
Epoch 2/10
19/19 - 2s - loss: 133.9782 - loglik: -1.3240e+02 - logprior: -1.5529e+00
Epoch 3/10
19/19 - 2s - loss: 117.2048 - loglik: -1.1546e+02 - logprior: -1.5592e+00
Epoch 4/10
19/19 - 2s - loss: 114.6460 - loglik: -1.1281e+02 - logprior: -1.5823e+00
Epoch 5/10
19/19 - 2s - loss: 113.4982 - loglik: -1.1176e+02 - logprior: -1.5448e+00
Epoch 6/10
19/19 - 2s - loss: 113.0207 - loglik: -1.1135e+02 - logprior: -1.5117e+00
Epoch 7/10
19/19 - 1s - loss: 112.7051 - loglik: -1.1103e+02 - logprior: -1.5000e+00
Epoch 8/10
19/19 - 2s - loss: 112.7748 - loglik: -1.1112e+02 - logprior: -1.4844e+00
Fitted a model with MAP estimate = -112.3393
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (36, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 114.4123 - loglik: -1.1011e+02 - logprior: -4.2503e+00
Epoch 2/2
19/19 - 2s - loss: 104.8651 - loglik: -1.0333e+02 - logprior: -1.4379e+00
Fitted a model with MAP estimate = -103.3907
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 109.4032 - loglik: -1.0521e+02 - logprior: -4.1573e+00
Epoch 2/2
19/19 - 2s - loss: 105.2202 - loglik: -1.0344e+02 - logprior: -1.6639e+00
Fitted a model with MAP estimate = -103.8455
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.5522 - loglik: -1.0451e+02 - logprior: -3.0135e+00
Epoch 2/10
19/19 - 2s - loss: 104.1152 - loglik: -1.0283e+02 - logprior: -1.1751e+00
Epoch 3/10
19/19 - 2s - loss: 103.1584 - loglik: -1.0158e+02 - logprior: -1.4027e+00
Epoch 4/10
19/19 - 2s - loss: 102.4262 - loglik: -1.0092e+02 - logprior: -1.2695e+00
Epoch 5/10
19/19 - 2s - loss: 101.8461 - loglik: -1.0037e+02 - logprior: -1.2343e+00
Epoch 6/10
19/19 - 2s - loss: 101.4113 - loglik: -9.9953e+01 - logprior: -1.2260e+00
Epoch 7/10
19/19 - 2s - loss: 101.6855 - loglik: -1.0026e+02 - logprior: -1.2046e+00
Fitted a model with MAP estimate = -101.0165
Time for alignment: 54.9902
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 176.4037 - loglik: -1.7309e+02 - logprior: -3.3038e+00
Epoch 2/10
19/19 - 2s - loss: 133.4777 - loglik: -1.3191e+02 - logprior: -1.5399e+00
Epoch 3/10
19/19 - 2s - loss: 117.2044 - loglik: -1.1550e+02 - logprior: -1.5604e+00
Epoch 4/10
19/19 - 2s - loss: 114.2354 - loglik: -1.1243e+02 - logprior: -1.6049e+00
Epoch 5/10
19/19 - 2s - loss: 113.7973 - loglik: -1.1208e+02 - logprior: -1.5412e+00
Epoch 6/10
19/19 - 2s - loss: 113.4644 - loglik: -1.1178e+02 - logprior: -1.5107e+00
Epoch 7/10
19/19 - 2s - loss: 112.8605 - loglik: -1.1119e+02 - logprior: -1.4995e+00
Epoch 8/10
19/19 - 2s - loss: 113.0767 - loglik: -1.1142e+02 - logprior: -1.4872e+00
Fitted a model with MAP estimate = -112.6151
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 114.3203 - loglik: -1.1001e+02 - logprior: -4.2576e+00
Epoch 2/2
19/19 - 2s - loss: 104.6933 - loglik: -1.0317e+02 - logprior: -1.4259e+00
Fitted a model with MAP estimate = -103.2413
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 109.4015 - loglik: -1.0518e+02 - logprior: -4.1929e+00
Epoch 2/2
19/19 - 2s - loss: 105.0020 - loglik: -1.0323e+02 - logprior: -1.6637e+00
Fitted a model with MAP estimate = -103.7364
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.4441 - loglik: -1.0438e+02 - logprior: -3.0264e+00
Epoch 2/10
19/19 - 2s - loss: 104.0635 - loglik: -1.0277e+02 - logprior: -1.1767e+00
Epoch 3/10
19/19 - 2s - loss: 103.1695 - loglik: -1.0159e+02 - logprior: -1.3973e+00
Epoch 4/10
19/19 - 2s - loss: 101.8966 - loglik: -1.0039e+02 - logprior: -1.2741e+00
Epoch 5/10
19/19 - 2s - loss: 102.0188 - loglik: -1.0055e+02 - logprior: -1.2328e+00
Fitted a model with MAP estimate = -101.3495
Time for alignment: 52.4008
Computed alignments with likelihoods: ['-101.2686', '-101.0165', '-101.3495']
Best model has likelihood: -101.0165
time for generating output: 0.1264
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00313.projection.fasta
SP score = 0.8080495356037152
Training of 3 independent models on file PF00009.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f47b406c970>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4739791220>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47397910d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4739783d00>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f47385f7d90>, <__main__.SimpleDirichletPrior object at 0x7f4afe80afa0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 171 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 618.0944 - loglik: -6.1515e+02 - logprior: -2.8556e+00
Epoch 2/10
19/19 - 16s - loss: 527.4233 - loglik: -5.2582e+02 - logprior: -1.4303e+00
Epoch 3/10
19/19 - 17s - loss: 488.9273 - loglik: -4.8689e+02 - logprior: -1.7577e+00
Epoch 4/10
19/19 - 17s - loss: 476.7812 - loglik: -4.7440e+02 - logprior: -1.7885e+00
Epoch 5/10
19/19 - 17s - loss: 472.8753 - loglik: -4.7051e+02 - logprior: -1.7554e+00
Epoch 6/10
19/19 - 18s - loss: 471.6869 - loglik: -4.6938e+02 - logprior: -1.6945e+00
Epoch 7/10
19/19 - 18s - loss: 469.2927 - loglik: -4.6702e+02 - logprior: -1.6573e+00
Epoch 8/10
19/19 - 18s - loss: 470.4706 - loglik: -4.6823e+02 - logprior: -1.6434e+00
Fitted a model with MAP estimate = -467.4975
expansions: [(20, 1), (57, 1), (76, 3), (80, 1), (81, 2), (82, 1), (102, 1), (105, 1), (106, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 22s - loss: 479.7228 - loglik: -4.7629e+02 - logprior: -3.3442e+00
Epoch 2/2
19/19 - 19s - loss: 467.3586 - loglik: -4.6541e+02 - logprior: -1.5641e+00
Fitted a model with MAP estimate = -463.2698
expansions: [(39, 1)]
discards: [ 0 79 87]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 21s - loss: 473.9340 - loglik: -4.6959e+02 - logprior: -4.2580e+00
Epoch 2/2
19/19 - 19s - loss: 469.2877 - loglik: -4.6647e+02 - logprior: -2.3754e+00
Fitted a model with MAP estimate = -464.6329
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10036 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 22s - loss: 468.9125 - loglik: -4.6559e+02 - logprior: -3.2358e+00
Epoch 2/10
19/19 - 18s - loss: 464.8717 - loglik: -4.6305e+02 - logprior: -1.4097e+00
Epoch 3/10
19/19 - 19s - loss: 462.8341 - loglik: -4.6073e+02 - logprior: -1.3845e+00
Epoch 4/10
19/19 - 19s - loss: 458.8385 - loglik: -4.5664e+02 - logprior: -1.3114e+00
Epoch 5/10
19/19 - 20s - loss: 460.1451 - loglik: -4.5804e+02 - logprior: -1.2548e+00
Fitted a model with MAP estimate = -457.8613
Time for alignment: 399.4085
Fitting a model of length 171 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 616.8798 - loglik: -6.1395e+02 - logprior: -2.8430e+00
Epoch 2/10
19/19 - 18s - loss: 525.3206 - loglik: -5.2374e+02 - logprior: -1.4226e+00
Epoch 3/10
19/19 - 18s - loss: 492.2185 - loglik: -4.8997e+02 - logprior: -1.6477e+00
Epoch 4/10
19/19 - 18s - loss: 481.2426 - loglik: -4.7868e+02 - logprior: -1.5764e+00
Epoch 5/10
19/19 - 18s - loss: 477.9676 - loglik: -4.7547e+02 - logprior: -1.6338e+00
Epoch 6/10
19/19 - 18s - loss: 473.2249 - loglik: -4.7082e+02 - logprior: -1.6305e+00
Epoch 7/10
19/19 - 18s - loss: 470.8341 - loglik: -4.6845e+02 - logprior: -1.6611e+00
Epoch 8/10
19/19 - 18s - loss: 470.0490 - loglik: -4.6768e+02 - logprior: -1.6925e+00
Epoch 9/10
19/19 - 18s - loss: 467.6424 - loglik: -4.6532e+02 - logprior: -1.6815e+00
Epoch 10/10
19/19 - 18s - loss: 468.5808 - loglik: -4.6632e+02 - logprior: -1.6651e+00
Fitted a model with MAP estimate = -465.6640
expansions: [(20, 1), (30, 4), (57, 1), (76, 2), (78, 1), (93, 1), (100, 1), (103, 1), (106, 1), (137, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 186 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 23s - loss: 478.2806 - loglik: -4.7489e+02 - logprior: -3.3023e+00
Epoch 2/2
19/19 - 19s - loss: 466.7688 - loglik: -4.6491e+02 - logprior: -1.4684e+00
Fitted a model with MAP estimate = -461.6416
expansions: []
discards: [ 0 33]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 22s - loss: 473.2348 - loglik: -4.6896e+02 - logprior: -4.1891e+00
Epoch 2/2
19/19 - 20s - loss: 466.7090 - loglik: -4.6396e+02 - logprior: -2.3415e+00
Fitted a model with MAP estimate = -462.7828
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10036 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 23s - loss: 468.1574 - loglik: -4.6492e+02 - logprior: -3.1553e+00
Epoch 2/10
19/19 - 20s - loss: 462.4491 - loglik: -4.6070e+02 - logprior: -1.3445e+00
Epoch 3/10
19/19 - 20s - loss: 459.5421 - loglik: -4.5749e+02 - logprior: -1.3130e+00
Epoch 4/10
19/19 - 20s - loss: 457.7843 - loglik: -4.5561e+02 - logprior: -1.2659e+00
Epoch 5/10
19/19 - 20s - loss: 458.3172 - loglik: -4.5621e+02 - logprior: -1.2156e+00
Fitted a model with MAP estimate = -455.7859
Time for alignment: 445.4062
Fitting a model of length 171 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 615.9841 - loglik: -6.1304e+02 - logprior: -2.8582e+00
Epoch 2/10
19/19 - 17s - loss: 518.1174 - loglik: -5.1652e+02 - logprior: -1.4550e+00
Epoch 3/10
19/19 - 18s - loss: 484.3552 - loglik: -4.8240e+02 - logprior: -1.6867e+00
Epoch 4/10
19/19 - 18s - loss: 473.6718 - loglik: -4.7141e+02 - logprior: -1.6782e+00
Epoch 5/10
19/19 - 18s - loss: 469.6124 - loglik: -4.6731e+02 - logprior: -1.6508e+00
Epoch 6/10
19/19 - 17s - loss: 468.5926 - loglik: -4.6635e+02 - logprior: -1.5987e+00
Epoch 7/10
19/19 - 18s - loss: 469.8392 - loglik: -4.6762e+02 - logprior: -1.5728e+00
Fitted a model with MAP estimate = -465.3935
expansions: [(25, 1), (30, 4), (37, 1), (57, 1), (76, 2), (78, 1), (102, 1), (104, 1), (107, 1), (139, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 24s - loss: 475.9269 - loglik: -4.7262e+02 - logprior: -3.2133e+00
Epoch 2/2
19/19 - 20s - loss: 463.9478 - loglik: -4.6222e+02 - logprior: -1.3527e+00
Fitted a model with MAP estimate = -459.8768
expansions: [(151, 2)]
discards: [ 0 33]
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 24s - loss: 470.6132 - loglik: -4.6644e+02 - logprior: -4.0860e+00
Epoch 2/2
19/19 - 22s - loss: 463.9980 - loglik: -4.6133e+02 - logprior: -2.2135e+00
Fitted a model with MAP estimate = -460.4348
expansions: [(0, 2)]
discards: [  0 150]
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10036 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 24s - loss: 465.8828 - loglik: -4.6279e+02 - logprior: -3.0036e+00
Epoch 2/10
19/19 - 21s - loss: 460.2088 - loglik: -4.5857e+02 - logprior: -1.2163e+00
Epoch 3/10
19/19 - 20s - loss: 458.0701 - loglik: -4.5610e+02 - logprior: -1.2275e+00
Epoch 4/10
19/19 - 20s - loss: 455.4307 - loglik: -4.5334e+02 - logprior: -1.1875e+00
Epoch 5/10
19/19 - 21s - loss: 456.2747 - loglik: -4.5426e+02 - logprior: -1.1498e+00
Fitted a model with MAP estimate = -453.9696
Time for alignment: 401.5370
Computed alignments with likelihoods: ['-457.8613', '-455.7859', '-453.9696']
Best model has likelihood: -453.9696
time for generating output: 0.4606
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00009.projection.fasta
SP score = 0.7825928555414393
Training of 3 independent models on file PF14604.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4ab215cb50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f475c7c7f10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b444a490>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f497ded1940>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f497deee970>, <__main__.SimpleDirichletPrior object at 0x7f4190687b20>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.8194 - loglik: -1.3953e+02 - logprior: -3.2829e+00
Epoch 2/10
19/19 - 1s - loss: 113.7796 - loglik: -1.1224e+02 - logprior: -1.4988e+00
Epoch 3/10
19/19 - 1s - loss: 104.6463 - loglik: -1.0276e+02 - logprior: -1.7015e+00
Epoch 4/10
19/19 - 1s - loss: 102.3866 - loglik: -1.0060e+02 - logprior: -1.5571e+00
Epoch 5/10
19/19 - 1s - loss: 101.5711 - loglik: -9.9840e+01 - logprior: -1.5367e+00
Epoch 6/10
19/19 - 1s - loss: 101.2617 - loglik: -9.9560e+01 - logprior: -1.5174e+00
Epoch 7/10
19/19 - 1s - loss: 101.0681 - loglik: -9.9380e+01 - logprior: -1.5005e+00
Epoch 8/10
19/19 - 1s - loss: 101.1068 - loglik: -9.9424e+01 - logprior: -1.4950e+00
Fitted a model with MAP estimate = -100.5332
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (20, 2), (21, 3), (28, 2), (29, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 108.6615 - loglik: -1.0429e+02 - logprior: -4.2984e+00
Epoch 2/2
19/19 - 1s - loss: 99.8781 - loglik: -9.7432e+01 - logprior: -2.2894e+00
Fitted a model with MAP estimate = -97.9234
expansions: [(0, 1)]
discards: [ 0  8 17 27 29 41 45]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 101.2399 - loglik: -9.7886e+01 - logprior: -3.3143e+00
Epoch 2/2
19/19 - 1s - loss: 96.9552 - loglik: -9.5258e+01 - logprior: -1.5896e+00
Fitted a model with MAP estimate = -96.2120
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 99.8879 - loglik: -9.6499e+01 - logprior: -3.3551e+00
Epoch 2/10
19/19 - 1s - loss: 96.9642 - loglik: -9.5306e+01 - logprior: -1.5523e+00
Epoch 3/10
19/19 - 1s - loss: 96.3579 - loglik: -9.4709e+01 - logprior: -1.4728e+00
Epoch 4/10
19/19 - 1s - loss: 96.0892 - loglik: -9.4441e+01 - logprior: -1.4337e+00
Epoch 5/10
19/19 - 1s - loss: 95.9155 - loglik: -9.4304e+01 - logprior: -1.3974e+00
Epoch 6/10
19/19 - 1s - loss: 95.7226 - loglik: -9.4138e+01 - logprior: -1.3845e+00
Epoch 7/10
19/19 - 1s - loss: 95.6322 - loglik: -9.4069e+01 - logprior: -1.3637e+00
Epoch 8/10
19/19 - 1s - loss: 95.2863 - loglik: -9.3739e+01 - logprior: -1.3505e+00
Epoch 9/10
19/19 - 1s - loss: 95.5023 - loglik: -9.3960e+01 - logprior: -1.3419e+00
Fitted a model with MAP estimate = -95.1577
Time for alignment: 53.6152
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.8166 - loglik: -1.3953e+02 - logprior: -3.2781e+00
Epoch 2/10
19/19 - 1s - loss: 113.6602 - loglik: -1.1211e+02 - logprior: -1.4920e+00
Epoch 3/10
19/19 - 1s - loss: 103.6352 - loglik: -1.0173e+02 - logprior: -1.6865e+00
Epoch 4/10
19/19 - 1s - loss: 101.5907 - loglik: -9.9786e+01 - logprior: -1.5731e+00
Epoch 5/10
19/19 - 1s - loss: 101.1111 - loglik: -9.9348e+01 - logprior: -1.5599e+00
Epoch 6/10
19/19 - 1s - loss: 100.7988 - loglik: -9.9033e+01 - logprior: -1.5445e+00
Epoch 7/10
19/19 - 1s - loss: 100.4695 - loglik: -9.8702e+01 - logprior: -1.5345e+00
Epoch 8/10
19/19 - 1s - loss: 100.4953 - loglik: -9.8727e+01 - logprior: -1.5335e+00
Fitted a model with MAP estimate = -100.0051
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (27, 2), (28, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 108.1802 - loglik: -1.0382e+02 - logprior: -4.2915e+00
Epoch 2/2
19/19 - 1s - loss: 99.8234 - loglik: -9.7412e+01 - logprior: -2.2503e+00
Fitted a model with MAP estimate = -97.7678
expansions: [(0, 1)]
discards: [ 0  8 17 26 27 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.3351 - loglik: -9.8002e+01 - logprior: -3.2926e+00
Epoch 2/2
19/19 - 1s - loss: 97.1886 - loglik: -9.5517e+01 - logprior: -1.5601e+00
Fitted a model with MAP estimate = -96.4957
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 99.6796 - loglik: -9.6293e+01 - logprior: -3.3533e+00
Epoch 2/10
19/19 - 1s - loss: 97.0139 - loglik: -9.5360e+01 - logprior: -1.5492e+00
Epoch 3/10
19/19 - 1s - loss: 96.3066 - loglik: -9.4658e+01 - logprior: -1.4673e+00
Epoch 4/10
19/19 - 1s - loss: 95.9976 - loglik: -9.4350e+01 - logprior: -1.4229e+00
Epoch 5/10
19/19 - 1s - loss: 95.9733 - loglik: -9.4360e+01 - logprior: -1.3923e+00
Epoch 6/10
19/19 - 1s - loss: 95.6174 - loglik: -9.4032e+01 - logprior: -1.3729e+00
Epoch 7/10
19/19 - 1s - loss: 95.6638 - loglik: -9.4096e+01 - logprior: -1.3561e+00
Fitted a model with MAP estimate = -95.2295
Time for alignment: 50.3823
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.8504 - loglik: -1.3956e+02 - logprior: -3.2799e+00
Epoch 2/10
19/19 - 1s - loss: 114.0592 - loglik: -1.1251e+02 - logprior: -1.4900e+00
Epoch 3/10
19/19 - 1s - loss: 103.9813 - loglik: -1.0200e+02 - logprior: -1.6862e+00
Epoch 4/10
19/19 - 1s - loss: 101.7038 - loglik: -9.9901e+01 - logprior: -1.5548e+00
Epoch 5/10
19/19 - 1s - loss: 101.0931 - loglik: -9.9317e+01 - logprior: -1.5622e+00
Epoch 6/10
19/19 - 1s - loss: 100.6671 - loglik: -9.8904e+01 - logprior: -1.5495e+00
Epoch 7/10
19/19 - 1s - loss: 100.5025 - loglik: -9.8713e+01 - logprior: -1.5362e+00
Epoch 8/10
19/19 - 1s - loss: 100.3702 - loglik: -9.8579e+01 - logprior: -1.5374e+00
Epoch 9/10
19/19 - 1s - loss: 100.1572 - loglik: -9.8379e+01 - logprior: -1.5315e+00
Epoch 10/10
19/19 - 1s - loss: 100.2104 - loglik: -9.8434e+01 - logprior: -1.5278e+00
Fitted a model with MAP estimate = -99.7720
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (27, 2), (28, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 108.5741 - loglik: -1.0421e+02 - logprior: -4.2891e+00
Epoch 2/2
19/19 - 1s - loss: 99.9642 - loglik: -9.7471e+01 - logprior: -2.2774e+00
Fitted a model with MAP estimate = -97.9347
expansions: [(0, 1)]
discards: [ 0  8 17 26 27 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.3229 - loglik: -9.7979e+01 - logprior: -3.2916e+00
Epoch 2/2
19/19 - 1s - loss: 97.2312 - loglik: -9.5554e+01 - logprior: -1.5615e+00
Fitted a model with MAP estimate = -96.4785
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.6886 - loglik: -9.6306e+01 - logprior: -3.3496e+00
Epoch 2/10
19/19 - 1s - loss: 97.0191 - loglik: -9.5361e+01 - logprior: -1.5505e+00
Epoch 3/10
19/19 - 1s - loss: 96.3312 - loglik: -9.4681e+01 - logprior: -1.4683e+00
Epoch 4/10
19/19 - 1s - loss: 96.0236 - loglik: -9.4377e+01 - logprior: -1.4214e+00
Epoch 5/10
19/19 - 1s - loss: 95.6983 - loglik: -9.4084e+01 - logprior: -1.3915e+00
Epoch 6/10
19/19 - 1s - loss: 95.7299 - loglik: -9.4142e+01 - logprior: -1.3768e+00
Fitted a model with MAP estimate = -95.3176
Time for alignment: 50.7442
Computed alignments with likelihoods: ['-95.1577', '-95.2295', '-95.3176']
Best model has likelihood: -95.1577
time for generating output: 0.1190
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF14604.projection.fasta
SP score = 0.7773695811903012
Training of 3 independent models on file PF00625.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4ab26a0130>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4ab24b2970>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5adde80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f47d43a6f10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b209476a0>, <__main__.SimpleDirichletPrior object at 0x7f47ac420a30>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 458.2940 - loglik: -4.5518e+02 - logprior: -3.0813e+00
Epoch 2/10
19/19 - 7s - loss: 379.4044 - loglik: -3.7771e+02 - logprior: -1.6116e+00
Epoch 3/10
19/19 - 7s - loss: 347.8292 - loglik: -3.4547e+02 - logprior: -2.1240e+00
Epoch 4/10
19/19 - 7s - loss: 341.6829 - loglik: -3.3910e+02 - logprior: -2.2246e+00
Epoch 5/10
19/19 - 8s - loss: 339.3451 - loglik: -3.3683e+02 - logprior: -2.1453e+00
Epoch 6/10
19/19 - 8s - loss: 339.3565 - loglik: -3.3691e+02 - logprior: -2.0808e+00
Fitted a model with MAP estimate = -334.7614
expansions: [(0, 2), (14, 1), (16, 2), (17, 1), (18, 2), (21, 1), (22, 1), (23, 1), (43, 2), (44, 1), (48, 1), (49, 1), (50, 1), (51, 1), (58, 1), (66, 1), (69, 1), (70, 1), (80, 2), (87, 2), (88, 1), (89, 1), (99, 1), (100, 1), (112, 1), (113, 1), (114, 2), (115, 1), (116, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 186 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 336.8332 - loglik: -3.3276e+02 - logprior: -3.9921e+00
Epoch 2/2
19/19 - 11s - loss: 317.6855 - loglik: -3.1595e+02 - logprior: -1.4871e+00
Fitted a model with MAP estimate = -311.3573
expansions: [(25, 1)]
discards: [  0  20  54 102]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 322.3568 - loglik: -3.1817e+02 - logprior: -4.1172e+00
Epoch 2/2
19/19 - 11s - loss: 316.5277 - loglik: -3.1430e+02 - logprior: -1.9936e+00
Fitted a model with MAP estimate = -311.0671
expansions: [(0, 2)]
discards: [  0 110]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10140 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 316.1150 - loglik: -3.1299e+02 - logprior: -3.0618e+00
Epoch 2/10
19/19 - 11s - loss: 311.9110 - loglik: -3.1046e+02 - logprior: -1.2175e+00
Epoch 3/10
19/19 - 11s - loss: 310.3318 - loglik: -3.0885e+02 - logprior: -1.1893e+00
Epoch 4/10
19/19 - 11s - loss: 308.2030 - loglik: -3.0665e+02 - logprior: -1.1700e+00
Epoch 5/10
19/19 - 11s - loss: 307.1922 - loglik: -3.0562e+02 - logprior: -1.1285e+00
Epoch 6/10
19/19 - 11s - loss: 307.2453 - loglik: -3.0569e+02 - logprior: -1.0920e+00
Fitted a model with MAP estimate = -305.8029
Time for alignment: 218.7770
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 458.2969 - loglik: -4.5518e+02 - logprior: -3.0784e+00
Epoch 2/10
19/19 - 8s - loss: 380.4315 - loglik: -3.7873e+02 - logprior: -1.6029e+00
Epoch 3/10
19/19 - 8s - loss: 349.1813 - loglik: -3.4670e+02 - logprior: -2.0749e+00
Epoch 4/10
19/19 - 8s - loss: 342.0477 - loglik: -3.3934e+02 - logprior: -2.1550e+00
Epoch 5/10
19/19 - 8s - loss: 339.8959 - loglik: -3.3727e+02 - logprior: -2.1124e+00
Epoch 6/10
19/19 - 8s - loss: 338.6270 - loglik: -3.3603e+02 - logprior: -2.0904e+00
Epoch 7/10
19/19 - 8s - loss: 337.4524 - loglik: -3.3491e+02 - logprior: -2.0751e+00
Epoch 8/10
19/19 - 8s - loss: 337.1713 - loglik: -3.3463e+02 - logprior: -2.0924e+00
Epoch 9/10
19/19 - 8s - loss: 336.4847 - loglik: -3.3396e+02 - logprior: -2.0983e+00
Epoch 10/10
19/19 - 8s - loss: 336.4597 - loglik: -3.3394e+02 - logprior: -2.1031e+00
Fitted a model with MAP estimate = -332.5665
expansions: [(0, 2), (14, 1), (16, 1), (17, 1), (18, 2), (19, 1), (20, 1), (22, 1), (30, 1), (43, 2), (44, 1), (48, 1), (49, 1), (50, 1), (51, 1), (58, 1), (65, 1), (66, 1), (68, 1), (71, 2), (87, 2), (88, 1), (89, 1), (99, 1), (100, 1), (112, 1), (113, 1), (114, 2), (115, 1), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 339.1442 - loglik: -3.3489e+02 - logprior: -4.1619e+00
Epoch 2/2
19/19 - 10s - loss: 317.7262 - loglik: -3.1592e+02 - logprior: -1.5181e+00
Fitted a model with MAP estimate = -311.3643
expansions: []
discards: [ 0 93]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 321.9152 - loglik: -3.1761e+02 - logprior: -4.2284e+00
Epoch 2/2
19/19 - 11s - loss: 316.6552 - loglik: -3.1438e+02 - logprior: -2.0102e+00
Fitted a model with MAP estimate = -311.0675
expansions: [(0, 2)]
discards: [  0 111]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10140 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 316.1820 - loglik: -3.1301e+02 - logprior: -3.1014e+00
Epoch 2/10
19/19 - 11s - loss: 312.3188 - loglik: -3.1081e+02 - logprior: -1.2400e+00
Epoch 3/10
19/19 - 12s - loss: 309.8537 - loglik: -3.0820e+02 - logprior: -1.2286e+00
Epoch 4/10
19/19 - 11s - loss: 308.4641 - loglik: -3.0673e+02 - logprior: -1.1886e+00
Epoch 5/10
19/19 - 12s - loss: 307.7574 - loglik: -3.0603e+02 - logprior: -1.1646e+00
Epoch 6/10
19/19 - 11s - loss: 305.2786 - loglik: -3.0361e+02 - logprior: -1.1349e+00
Epoch 7/10
19/19 - 11s - loss: 306.8654 - loglik: -3.0525e+02 - logprior: -1.1067e+00
Fitted a model with MAP estimate = -305.2352
Time for alignment: 262.0197
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 458.4307 - loglik: -4.5531e+02 - logprior: -3.0858e+00
Epoch 2/10
19/19 - 8s - loss: 380.7797 - loglik: -3.7908e+02 - logprior: -1.6223e+00
Epoch 3/10
19/19 - 8s - loss: 346.8590 - loglik: -3.4448e+02 - logprior: -2.1088e+00
Epoch 4/10
19/19 - 8s - loss: 340.3350 - loglik: -3.3772e+02 - logprior: -2.2406e+00
Epoch 5/10
19/19 - 8s - loss: 337.2113 - loglik: -3.3463e+02 - logprior: -2.1708e+00
Epoch 6/10
19/19 - 8s - loss: 336.8136 - loglik: -3.3429e+02 - logprior: -2.1190e+00
Epoch 7/10
19/19 - 8s - loss: 335.9972 - loglik: -3.3351e+02 - logprior: -2.1068e+00
Epoch 8/10
19/19 - 8s - loss: 335.7457 - loglik: -3.3331e+02 - logprior: -2.0785e+00
Epoch 9/10
19/19 - 8s - loss: 335.4899 - loglik: -3.3307e+02 - logprior: -2.0731e+00
Epoch 10/10
19/19 - 8s - loss: 335.3763 - loglik: -3.3297e+02 - logprior: -2.0638e+00
Fitted a model with MAP estimate = -331.6992
expansions: [(0, 2), (14, 1), (16, 1), (17, 1), (18, 2), (19, 1), (21, 1), (22, 1), (36, 1), (43, 2), (44, 1), (48, 1), (50, 1), (52, 1), (57, 1), (66, 1), (67, 1), (69, 1), (70, 1), (71, 1), (80, 1), (87, 2), (88, 1), (89, 1), (99, 1), (113, 1), (114, 2), (115, 1), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 335.8731 - loglik: -3.3159e+02 - logprior: -4.1974e+00
Epoch 2/2
19/19 - 11s - loss: 317.1734 - loglik: -3.1536e+02 - logprior: -1.4994e+00
Fitted a model with MAP estimate = -310.8438
expansions: []
discards: [ 0 54]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 321.5132 - loglik: -3.1716e+02 - logprior: -4.2764e+00
Epoch 2/2
19/19 - 11s - loss: 316.1985 - loglik: -3.1391e+02 - logprior: -2.0067e+00
Fitted a model with MAP estimate = -310.7827
expansions: [(0, 2)]
discards: [  0 111]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10140 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 316.0279 - loglik: -3.1288e+02 - logprior: -3.0716e+00
Epoch 2/10
19/19 - 10s - loss: 311.7417 - loglik: -3.1020e+02 - logprior: -1.2290e+00
Epoch 3/10
19/19 - 10s - loss: 309.7877 - loglik: -3.0811e+02 - logprior: -1.1969e+00
Epoch 4/10
19/19 - 11s - loss: 308.2444 - loglik: -3.0654e+02 - logprior: -1.1772e+00
Epoch 5/10
19/19 - 11s - loss: 306.8556 - loglik: -3.0521e+02 - logprior: -1.1469e+00
Epoch 6/10
19/19 - 11s - loss: 306.4304 - loglik: -3.0484e+02 - logprior: -1.1174e+00
Epoch 7/10
19/19 - 11s - loss: 306.4089 - loglik: -3.0486e+02 - logprior: -1.0901e+00
Epoch 8/10
19/19 - 11s - loss: 305.6469 - loglik: -3.0414e+02 - logprior: -1.0793e+00
Epoch 9/10
19/19 - 11s - loss: 305.6638 - loglik: -3.0420e+02 - logprior: -1.0383e+00
Fitted a model with MAP estimate = -304.7299
Time for alignment: 285.8180
Computed alignments with likelihoods: ['-305.8029', '-305.2352', '-304.7299']
Best model has likelihood: -304.7299
time for generating output: 0.3903
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00625.projection.fasta
SP score = 0.2607456296561939
Training of 3 independent models on file PF00476.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f497f20feb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f47389ba040>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f41793af2e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f47ac3c0610>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4739f105b0>, <__main__.SimpleDirichletPrior object at 0x7f41903a37f0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 904.7838 - loglik: -9.0276e+02 - logprior: -1.8831e+00
Epoch 2/10
39/39 - 61s - loss: 719.2364 - loglik: -7.1563e+02 - logprior: -2.6796e+00
Epoch 3/10
39/39 - 64s - loss: 700.6797 - loglik: -6.9641e+02 - logprior: -2.9124e+00
Epoch 4/10
39/39 - 60s - loss: 695.4588 - loglik: -6.9128e+02 - logprior: -2.8604e+00
Epoch 5/10
39/39 - 58s - loss: 693.7474 - loglik: -6.8970e+02 - logprior: -2.8602e+00
Epoch 6/10
39/39 - 58s - loss: 692.3536 - loglik: -6.8840e+02 - logprior: -2.8858e+00
Epoch 7/10
39/39 - 59s - loss: 692.1110 - loglik: -6.8820e+02 - logprior: -2.9457e+00
Epoch 8/10
39/39 - 58s - loss: 691.3765 - loglik: -6.8758e+02 - logprior: -2.8873e+00
Epoch 9/10
39/39 - 55s - loss: 691.3078 - loglik: -6.8757e+02 - logprior: -2.8872e+00
Epoch 10/10
39/39 - 56s - loss: 690.8751 - loglik: -6.8718e+02 - logprior: -2.8858e+00
Fitted a model with MAP estimate = -689.0922
expansions: [(0, 4), (36, 1), (42, 1), (45, 1), (60, 1), (62, 2), (66, 1), (72, 1), (73, 1), (79, 1), (83, 2), (84, 3), (90, 1), (91, 1), (103, 1), (112, 1), (113, 1), (118, 1), (122, 2), (125, 2), (127, 1), (144, 1), (146, 1), (149, 1), (151, 1), (155, 1), (161, 2), (162, 2), (163, 1), (165, 1), (166, 1), (173, 1), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (228, 1), (229, 1), (232, 3), (233, 1), (237, 1), (256, 1), (257, 4), (258, 1), (259, 1), (260, 1), (261, 1), (279, 2), (280, 1), (281, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 386 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 86s - loss: 684.6248 - loglik: -6.8114e+02 - logprior: -3.2543e+00
Epoch 2/2
39/39 - 84s - loss: 654.1766 - loglik: -6.5158e+02 - logprior: -1.9617e+00
Fitted a model with MAP estimate = -646.8841
expansions: []
discards: [  0 100 147 152 199 232 319 350 365 366]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 85s - loss: 665.3093 - loglik: -6.6227e+02 - logprior: -2.8218e+00
Epoch 2/2
39/39 - 83s - loss: 653.1752 - loglik: -6.5102e+02 - logprior: -1.2418e+00
Fitted a model with MAP estimate = -646.4841
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 84s - loss: 662.9803 - loglik: -6.6069e+02 - logprior: -2.0547e+00
Epoch 2/10
39/39 - 79s - loss: 652.8235 - loglik: -6.5086e+02 - logprior: -9.4103e-01
Epoch 3/10
39/39 - 80s - loss: 646.4446 - loglik: -6.4414e+02 - logprior: -8.1688e-01
Epoch 4/10
39/39 - 82s - loss: 643.1720 - loglik: -6.4086e+02 - logprior: -8.0830e-01
Epoch 5/10
39/39 - 89s - loss: 641.2142 - loglik: -6.3923e+02 - logprior: -6.2626e-01
Epoch 6/10
39/39 - 86s - loss: 640.6452 - loglik: -6.3893e+02 - logprior: -4.7247e-01
Epoch 7/10
39/39 - 85s - loss: 640.0234 - loglik: -6.3862e+02 - logprior: -2.7805e-01
Epoch 8/10
39/39 - 84s - loss: 639.2496 - loglik: -6.3813e+02 - logprior: -7.5047e-02
Epoch 9/10
39/39 - 83s - loss: 638.2236 - loglik: -6.3727e+02 - logprior: 0.0356
Epoch 10/10
39/39 - 81s - loss: 637.3978 - loglik: -6.3675e+02 - logprior: 0.2952
Fitted a model with MAP estimate = -636.5107
Time for alignment: 2131.7779
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 906.8621 - loglik: -9.0469e+02 - logprior: -1.9233e+00
Epoch 2/10
39/39 - 57s - loss: 721.1700 - loglik: -7.1674e+02 - logprior: -2.5991e+00
Epoch 3/10
39/39 - 56s - loss: 703.0655 - loglik: -6.9863e+02 - logprior: -2.7297e+00
Epoch 4/10
39/39 - 56s - loss: 697.9343 - loglik: -6.9375e+02 - logprior: -2.8124e+00
Epoch 5/10
39/39 - 60s - loss: 695.7468 - loglik: -6.9177e+02 - logprior: -2.8053e+00
Epoch 6/10
39/39 - 62s - loss: 694.6354 - loglik: -6.9075e+02 - logprior: -2.8096e+00
Epoch 7/10
39/39 - 62s - loss: 693.4841 - loglik: -6.8974e+02 - logprior: -2.7539e+00
Epoch 8/10
39/39 - 53s - loss: 693.5172 - loglik: -6.8988e+02 - logprior: -2.6990e+00
Fitted a model with MAP estimate = -691.2333
expansions: [(0, 4), (24, 1), (35, 1), (45, 1), (55, 1), (59, 1), (61, 2), (65, 1), (71, 1), (79, 1), (80, 1), (82, 2), (83, 1), (84, 1), (90, 1), (91, 1), (92, 1), (112, 3), (122, 2), (124, 3), (140, 1), (144, 1), (146, 1), (149, 1), (155, 1), (158, 1), (161, 1), (162, 1), (163, 1), (164, 2), (165, 1), (186, 1), (188, 1), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (227, 1), (228, 1), (230, 5), (256, 4), (258, 2), (259, 2), (260, 1), (279, 2), (280, 1), (281, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 385 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 73s - loss: 685.1735 - loglik: -6.8169e+02 - logprior: -3.2228e+00
Epoch 2/2
39/39 - 70s - loss: 654.0295 - loglik: -6.5140e+02 - logprior: -1.9971e+00
Fitted a model with MAP estimate = -647.5974
expansions: []
discards: [  0   2 101 147 152 285 322 349 364 365]
Re-initialized the encoder parameters.
Fitting a model of length 375 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 72s - loss: 665.5560 - loglik: -6.6276e+02 - logprior: -2.5562e+00
Epoch 2/2
39/39 - 69s - loss: 653.8940 - loglik: -6.5175e+02 - logprior: -1.2199e+00
Fitted a model with MAP estimate = -646.9214
expansions: [(0, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 379 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 82s - loss: 663.9582 - loglik: -6.6110e+02 - logprior: -2.6182e+00
Epoch 2/10
39/39 - 88s - loss: 652.6328 - loglik: -6.5078e+02 - logprior: -1.0088e+00
Epoch 3/10
39/39 - 90s - loss: 646.6154 - loglik: -6.4440e+02 - logprior: -9.0599e-01
Epoch 4/10
39/39 - 93s - loss: 643.1212 - loglik: -6.4097e+02 - logprior: -7.0744e-01
Epoch 5/10
39/39 - 88s - loss: 640.7551 - loglik: -6.3889e+02 - logprior: -5.1470e-01
Epoch 6/10
39/39 - 70s - loss: 640.3846 - loglik: -6.3855e+02 - logprior: -5.9497e-01
Epoch 7/10
39/39 - 65s - loss: 639.6030 - loglik: -6.3813e+02 - logprior: -3.4484e-01
Epoch 8/10
39/39 - 64s - loss: 638.1245 - loglik: -6.3690e+02 - logprior: -1.7603e-01
Epoch 9/10
39/39 - 63s - loss: 638.7504 - loglik: -6.3793e+02 - logprior: 0.1542
Fitted a model with MAP estimate = -636.3861
Time for alignment: 1747.9608
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 905.8660 - loglik: -9.0377e+02 - logprior: -1.9127e+00
Epoch 2/10
39/39 - 45s - loss: 721.3985 - loglik: -7.1726e+02 - logprior: -2.5396e+00
Epoch 3/10
39/39 - 44s - loss: 703.5695 - loglik: -6.9931e+02 - logprior: -2.5972e+00
Epoch 4/10
39/39 - 44s - loss: 698.0494 - loglik: -6.9404e+02 - logprior: -2.6641e+00
Epoch 5/10
39/39 - 43s - loss: 695.8169 - loglik: -6.9190e+02 - logprior: -2.8290e+00
Epoch 6/10
39/39 - 43s - loss: 695.5135 - loglik: -6.9177e+02 - logprior: -2.7369e+00
Epoch 7/10
39/39 - 43s - loss: 693.9242 - loglik: -6.9032e+02 - logprior: -2.6830e+00
Epoch 8/10
39/39 - 43s - loss: 693.2152 - loglik: -6.8956e+02 - logprior: -2.7675e+00
Epoch 9/10
39/39 - 42s - loss: 692.9890 - loglik: -6.8940e+02 - logprior: -2.7387e+00
Epoch 10/10
39/39 - 43s - loss: 693.3572 - loglik: -6.8989e+02 - logprior: -2.6470e+00
Fitted a model with MAP estimate = -690.9020
expansions: [(0, 4), (41, 1), (45, 2), (60, 1), (62, 2), (66, 1), (72, 1), (73, 1), (79, 1), (80, 1), (83, 1), (84, 3), (90, 1), (91, 1), (92, 1), (112, 1), (113, 1), (118, 1), (122, 2), (124, 3), (142, 1), (144, 1), (146, 1), (149, 1), (155, 1), (158, 1), (160, 1), (162, 1), (163, 1), (164, 2), (165, 1), (186, 1), (188, 1), (189, 1), (190, 1), (193, 1), (205, 1), (207, 2), (208, 2), (209, 3), (227, 1), (228, 1), (229, 1), (230, 4), (256, 4), (258, 2), (259, 2), (260, 1), (279, 2), (280, 1), (281, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 387 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 64s - loss: 684.7677 - loglik: -6.8128e+02 - logprior: -3.2269e+00
Epoch 2/2
39/39 - 61s - loss: 653.7490 - loglik: -6.5100e+02 - logprior: -1.9427e+00
Fitted a model with MAP estimate = -646.2961
expansions: []
discards: [  0 100 147 152 258 259 289 325 350 366 367]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 62s - loss: 665.8104 - loglik: -6.6275e+02 - logprior: -2.8278e+00
Epoch 2/2
39/39 - 58s - loss: 654.0400 - loglik: -6.5206e+02 - logprior: -1.1112e+00
Fitted a model with MAP estimate = -647.3797
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 61s - loss: 663.3501 - loglik: -6.6115e+02 - logprior: -1.9567e+00
Epoch 2/10
39/39 - 58s - loss: 653.4081 - loglik: -6.5159e+02 - logprior: -7.7343e-01
Epoch 3/10
39/39 - 58s - loss: 646.9159 - loglik: -6.4467e+02 - logprior: -7.5074e-01
Epoch 4/10
39/39 - 58s - loss: 644.3148 - loglik: -6.4217e+02 - logprior: -6.5922e-01
Epoch 5/10
39/39 - 58s - loss: 641.8076 - loglik: -6.3978e+02 - logprior: -6.7886e-01
Epoch 6/10
39/39 - 58s - loss: 641.2183 - loglik: -6.3953e+02 - logprior: -4.9490e-01
Epoch 7/10
39/39 - 58s - loss: 640.7234 - loglik: -6.3923e+02 - logprior: -4.0371e-01
Epoch 8/10
39/39 - 58s - loss: 638.9906 - loglik: -6.3787e+02 - logprior: -1.1072e-01
Epoch 9/10
39/39 - 58s - loss: 639.5291 - loglik: -6.3861e+02 - logprior: 0.0403
Fitted a model with MAP estimate = -637.4758
Time for alignment: 1452.2985
Computed alignments with likelihoods: ['-636.5107', '-636.3861', '-637.4758']
Best model has likelihood: -636.3861
time for generating output: 0.4272
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00476.projection.fasta
SP score = 0.9433676559660812
Training of 3 independent models on file PF02836.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f475c1d7fd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f475c7aa640>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a2f3a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f47d42339a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b06fbaa30>, <__main__.SimpleDirichletPrior object at 0x7f473b259880>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 212 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 691.0581 - loglik: -6.8803e+02 - logprior: -2.9399e+00
Epoch 2/10
19/19 - 16s - loss: 602.0408 - loglik: -6.0025e+02 - logprior: -1.4894e+00
Epoch 3/10
19/19 - 16s - loss: 556.9665 - loglik: -5.5423e+02 - logprior: -1.9445e+00
Epoch 4/10
19/19 - 16s - loss: 546.7943 - loglik: -5.4316e+02 - logprior: -2.1969e+00
Epoch 5/10
19/19 - 16s - loss: 540.5702 - loglik: -5.3669e+02 - logprior: -2.3857e+00
Epoch 6/10
19/19 - 16s - loss: 539.9706 - loglik: -5.3600e+02 - logprior: -2.4423e+00
Epoch 7/10
19/19 - 16s - loss: 536.7075 - loglik: -5.3281e+02 - logprior: -2.4730e+00
Epoch 8/10
19/19 - 16s - loss: 536.0355 - loglik: -5.3225e+02 - logprior: -2.4962e+00
Epoch 9/10
19/19 - 16s - loss: 535.7475 - loglik: -5.3203e+02 - logprior: -2.5194e+00
Epoch 10/10
19/19 - 16s - loss: 534.9683 - loglik: -5.3129e+02 - logprior: -2.5311e+00
Fitted a model with MAP estimate = -533.0792
expansions: [(4, 1), (6, 1), (33, 1), (49, 1), (78, 1), (81, 9), (84, 1), (87, 2), (88, 1), (89, 1), (113, 1), (114, 9), (115, 3), (116, 1), (123, 1), (126, 5), (127, 1), (128, 1), (129, 1), (134, 1), (138, 1), (139, 1), (141, 9), (142, 1), (145, 1), (146, 1), (150, 1), (154, 1), (157, 1), (160, 6), (161, 1), (171, 1), (172, 5)]
discards: [  0 162 163 164 165 166 167 168 174 175 176 177 178 179 180 181 182 183
 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201
 202 203 204 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 22s - loss: 584.2817 - loglik: -5.7986e+02 - logprior: -4.3215e+00
Epoch 2/2
19/19 - 18s - loss: 552.3187 - loglik: -5.4950e+02 - logprior: -2.3291e+00
Fitted a model with MAP estimate = -545.6675
expansions: [(4, 1), (239, 18)]
discards: [  0   1  94  95 138 139 140 141 142 156 157 158 162 163 182 183 184 205
 206 207 208 209 210 211 212 213 216 217 218 219 220 221 222 223 224 225
 227 236 237]
Re-initialized the encoder parameters.
Fitting a model of length 219 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 20s - loss: 566.5773 - loglik: -5.6237e+02 - logprior: -4.1183e+00
Epoch 2/2
19/19 - 17s - loss: 553.2379 - loglik: -5.5053e+02 - logprior: -2.2265e+00
Fitted a model with MAP estimate = -548.1044
expansions: [(0, 2), (2, 1), (156, 2), (199, 8), (219, 3)]
discards: [  0 189 190 191 192 193 205 206 207 208 209 210 211 212 213 214 215 216
 217 218]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 19s - loss: 559.8542 - loglik: -5.5644e+02 - logprior: -3.3230e+00
Epoch 2/10
19/19 - 16s - loss: 550.1912 - loglik: -5.4830e+02 - logprior: -1.4038e+00
Epoch 3/10
19/19 - 16s - loss: 547.1899 - loglik: -5.4507e+02 - logprior: -1.2996e+00
Epoch 4/10
19/19 - 16s - loss: 543.2714 - loglik: -5.4088e+02 - logprior: -1.2517e+00
Epoch 5/10
19/19 - 16s - loss: 541.7500 - loglik: -5.3911e+02 - logprior: -1.2331e+00
Epoch 6/10
19/19 - 16s - loss: 538.5569 - loglik: -5.3575e+02 - logprior: -1.2719e+00
Epoch 7/10
19/19 - 16s - loss: 539.2258 - loglik: -5.3638e+02 - logprior: -1.2698e+00
Fitted a model with MAP estimate = -535.6971
Time for alignment: 446.1901
Fitting a model of length 212 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 690.7193 - loglik: -6.8770e+02 - logprior: -2.9291e+00
Epoch 2/10
19/19 - 16s - loss: 599.5937 - loglik: -5.9786e+02 - logprior: -1.4699e+00
Epoch 3/10
19/19 - 16s - loss: 555.6323 - loglik: -5.5331e+02 - logprior: -1.9886e+00
Epoch 4/10
19/19 - 16s - loss: 544.2032 - loglik: -5.4127e+02 - logprior: -2.2162e+00
Epoch 5/10
19/19 - 16s - loss: 541.6138 - loglik: -5.3844e+02 - logprior: -2.2657e+00
Epoch 6/10
19/19 - 16s - loss: 538.3051 - loglik: -5.3502e+02 - logprior: -2.2775e+00
Epoch 7/10
19/19 - 16s - loss: 537.5347 - loglik: -5.3419e+02 - logprior: -2.2900e+00
Epoch 8/10
19/19 - 16s - loss: 536.1871 - loglik: -5.3286e+02 - logprior: -2.2776e+00
Epoch 9/10
19/19 - 16s - loss: 536.5879 - loglik: -5.3324e+02 - logprior: -2.2974e+00
Fitted a model with MAP estimate = -534.2429
expansions: [(4, 1), (6, 1), (31, 1), (40, 1), (82, 9), (91, 1), (118, 1), (119, 9), (120, 3), (121, 1), (128, 1), (129, 2), (131, 8), (136, 1), (138, 1), (141, 2), (142, 9), (145, 2), (146, 2), (150, 1), (152, 2), (155, 1), (158, 1), (159, 3), (160, 1), (170, 1), (171, 6), (172, 1), (182, 1)]
discards: [  0 161 162 163 164 165 166 167 168 174 175 176 177 178 179 180 185 186
 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204
 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 22s - loss: 578.7925 - loglik: -5.7438e+02 - logprior: -4.3168e+00
Epoch 2/2
19/19 - 19s - loss: 550.9047 - loglik: -5.4812e+02 - logprior: -2.2919e+00
Fitted a model with MAP estimate = -543.6896
expansions: [(4, 2), (91, 1), (143, 1), (243, 29)]
discards: [  0   1 135 136 137 138 139 163 164 165 186 187 198 199 204 205 206 207
 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225
 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242]
Re-initialized the encoder parameters.
Fitting a model of length 223 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 20s - loss: 567.3096 - loglik: -5.6325e+02 - logprior: -3.9695e+00
Epoch 2/2
19/19 - 17s - loss: 552.5396 - loglik: -5.5023e+02 - logprior: -1.8417e+00
Fitted a model with MAP estimate = -546.2017
expansions: [(190, 1), (223, 23)]
discards: [  0   1  81  82  83  84  86  87 194 195 196 197 198 199 200 201 202 203
 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221
 222]
Re-initialized the encoder parameters.
Fitting a model of length 210 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 19s - loss: 569.4822 - loglik: -5.6562e+02 - logprior: -3.7748e+00
Epoch 2/10
19/19 - 16s - loss: 555.1847 - loglik: -5.5331e+02 - logprior: -1.4016e+00
Epoch 3/10
19/19 - 16s - loss: 549.3893 - loglik: -5.4740e+02 - logprior: -1.1064e+00
Epoch 4/10
19/19 - 16s - loss: 545.0784 - loglik: -5.4274e+02 - logprior: -1.1864e+00
Epoch 5/10
19/19 - 16s - loss: 542.3094 - loglik: -5.3978e+02 - logprior: -1.1484e+00
Epoch 6/10
19/19 - 16s - loss: 539.2834 - loglik: -5.3666e+02 - logprior: -1.1393e+00
Epoch 7/10
19/19 - 16s - loss: 539.4412 - loglik: -5.3681e+02 - logprior: -1.1604e+00
Fitted a model with MAP estimate = -536.7452
Time for alignment: 428.3224
Fitting a model of length 212 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 692.2347 - loglik: -6.8921e+02 - logprior: -2.9382e+00
Epoch 2/10
19/19 - 16s - loss: 600.6666 - loglik: -5.9890e+02 - logprior: -1.4562e+00
Epoch 3/10
19/19 - 16s - loss: 556.2105 - loglik: -5.5374e+02 - logprior: -1.8738e+00
Epoch 4/10
19/19 - 16s - loss: 546.6025 - loglik: -5.4343e+02 - logprior: -1.9794e+00
Epoch 5/10
19/19 - 16s - loss: 541.9246 - loglik: -5.3860e+02 - logprior: -2.0172e+00
Epoch 6/10
19/19 - 16s - loss: 539.5424 - loglik: -5.3612e+02 - logprior: -2.0680e+00
Epoch 7/10
19/19 - 16s - loss: 538.5368 - loglik: -5.3501e+02 - logprior: -2.1634e+00
Epoch 8/10
19/19 - 16s - loss: 536.1584 - loglik: -5.3253e+02 - logprior: -2.2313e+00
Epoch 9/10
19/19 - 16s - loss: 536.3118 - loglik: -5.3274e+02 - logprior: -2.2586e+00
Fitted a model with MAP estimate = -533.5138
expansions: [(4, 1), (6, 1), (31, 1), (37, 1), (82, 9), (117, 1), (118, 12), (119, 3), (120, 2), (126, 1), (128, 2), (129, 7), (130, 2), (135, 1), (137, 1), (140, 2), (141, 10), (142, 1), (145, 1), (146, 1), (154, 2), (155, 1), (156, 2), (157, 2), (158, 4), (159, 1), (168, 1), (169, 1), (170, 1), (171, 4), (173, 1), (182, 1)]
discards: [  0 148 174 175 176 177 178 179 180 183 184 185 186 187 188 189 190 191
 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209
 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 255 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 24s - loss: 574.8851 - loglik: -5.7042e+02 - logprior: -4.3652e+00
Epoch 2/2
19/19 - 21s - loss: 543.7668 - loglik: -5.4102e+02 - logprior: -2.2597e+00
Fitted a model with MAP estimate = -536.0240
expansions: [(144, 1), (255, 15)]
discards: [  0   1 138 139 140 141 142 157 158 160 190 191 192 193 208 209 210 211
 212 213 214 241 242 246 247 248 249 250 251 252 253 254]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 23s - loss: 556.2693 - loglik: -5.5214e+02 - logprior: -4.0342e+00
Epoch 2/2
19/19 - 20s - loss: 543.0831 - loglik: -5.4065e+02 - logprior: -1.9615e+00
Fitted a model with MAP estimate = -537.5346
expansions: [(0, 2), (2, 2), (89, 2), (195, 5), (196, 1), (239, 5)]
discards: [  0  79  80  81  82  83  84  85 135 157 224 225 226 227 228 229 230 231
 232 233 234 235 236 237 238]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 22s - loss: 550.3420 - loglik: -5.4698e+02 - logprior: -3.2664e+00
Epoch 2/10
19/19 - 19s - loss: 539.8616 - loglik: -5.3784e+02 - logprior: -1.5390e+00
Epoch 3/10
19/19 - 19s - loss: 536.6577 - loglik: -5.3446e+02 - logprior: -1.3933e+00
Epoch 4/10
19/19 - 19s - loss: 530.9985 - loglik: -5.2854e+02 - logprior: -1.3870e+00
Epoch 5/10
19/19 - 19s - loss: 531.9676 - loglik: -5.2927e+02 - logprior: -1.3764e+00
Fitted a model with MAP estimate = -527.8396
Time for alignment: 430.6736
Computed alignments with likelihoods: ['-533.0792', '-534.2429', '-527.8396']
Best model has likelihood: -527.8396
time for generating output: 0.3470
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02836.projection.fasta
SP score = 0.7910203052087275
Training of 3 independent models on file PF02777.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f47ac2e6a90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f473b4f1dc0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4179edb160>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4178424c10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f473916b1c0>, <__main__.SimpleDirichletPrior object at 0x7f497f53d1f0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 269.8612 - loglik: -2.6659e+02 - logprior: -3.2592e+00
Epoch 2/10
19/19 - 3s - loss: 199.9548 - loglik: -1.9844e+02 - logprior: -1.4743e+00
Epoch 3/10
19/19 - 3s - loss: 176.6849 - loglik: -1.7470e+02 - logprior: -1.8674e+00
Epoch 4/10
19/19 - 3s - loss: 172.9070 - loglik: -1.7082e+02 - logprior: -1.8410e+00
Epoch 5/10
19/19 - 3s - loss: 171.5760 - loglik: -1.6961e+02 - logprior: -1.7792e+00
Epoch 6/10
19/19 - 3s - loss: 170.7133 - loglik: -1.6875e+02 - logprior: -1.7776e+00
Epoch 7/10
19/19 - 3s - loss: 170.2034 - loglik: -1.6827e+02 - logprior: -1.7548e+00
Epoch 8/10
19/19 - 3s - loss: 169.8168 - loglik: -1.6790e+02 - logprior: -1.7481e+00
Epoch 9/10
19/19 - 3s - loss: 170.3332 - loglik: -1.6843e+02 - logprior: -1.7467e+00
Fitted a model with MAP estimate = -169.5744
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 5), (47, 1), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 169.4578 - loglik: -1.6526e+02 - logprior: -4.1599e+00
Epoch 2/2
19/19 - 4s - loss: 156.9212 - loglik: -1.5449e+02 - logprior: -2.2954e+00
Fitted a model with MAP estimate = -154.6541
expansions: [(0, 2)]
discards: [ 0 42 59 60]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 157.2223 - loglik: -1.5405e+02 - logprior: -3.1351e+00
Epoch 2/2
19/19 - 4s - loss: 152.5585 - loglik: -1.5106e+02 - logprior: -1.3534e+00
Fitted a model with MAP estimate = -151.3397
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 159.3145 - loglik: -1.5519e+02 - logprior: -4.0895e+00
Epoch 2/10
19/19 - 4s - loss: 153.6598 - loglik: -1.5177e+02 - logprior: -1.7457e+00
Epoch 3/10
19/19 - 4s - loss: 152.3449 - loglik: -1.5086e+02 - logprior: -1.2802e+00
Epoch 4/10
19/19 - 4s - loss: 150.9603 - loglik: -1.4946e+02 - logprior: -1.2497e+00
Epoch 5/10
19/19 - 4s - loss: 150.6078 - loglik: -1.4910e+02 - logprior: -1.2441e+00
Epoch 6/10
19/19 - 4s - loss: 150.1609 - loglik: -1.4869e+02 - logprior: -1.2268e+00
Epoch 7/10
19/19 - 4s - loss: 150.1273 - loglik: -1.4868e+02 - logprior: -1.2106e+00
Epoch 8/10
19/19 - 4s - loss: 150.3044 - loglik: -1.4889e+02 - logprior: -1.1859e+00
Fitted a model with MAP estimate = -149.4610
Time for alignment: 108.0404
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 269.9494 - loglik: -2.6668e+02 - logprior: -3.2546e+00
Epoch 2/10
19/19 - 3s - loss: 200.2145 - loglik: -1.9870e+02 - logprior: -1.4799e+00
Epoch 3/10
19/19 - 3s - loss: 176.4831 - loglik: -1.7446e+02 - logprior: -1.8999e+00
Epoch 4/10
19/19 - 3s - loss: 172.8091 - loglik: -1.7072e+02 - logprior: -1.8398e+00
Epoch 5/10
19/19 - 3s - loss: 171.5569 - loglik: -1.6957e+02 - logprior: -1.7782e+00
Epoch 6/10
19/19 - 3s - loss: 170.8838 - loglik: -1.6893e+02 - logprior: -1.7666e+00
Epoch 7/10
19/19 - 3s - loss: 170.4464 - loglik: -1.6850e+02 - logprior: -1.7538e+00
Epoch 8/10
19/19 - 3s - loss: 170.2785 - loglik: -1.6835e+02 - logprior: -1.7441e+00
Epoch 9/10
19/19 - 3s - loss: 169.9318 - loglik: -1.6802e+02 - logprior: -1.7460e+00
Epoch 10/10
19/19 - 3s - loss: 169.9675 - loglik: -1.6806e+02 - logprior: -1.7478e+00
Fitted a model with MAP estimate = -169.4711
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 5), (47, 1), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 169.4325 - loglik: -1.6524e+02 - logprior: -4.1525e+00
Epoch 2/2
19/19 - 4s - loss: 157.0065 - loglik: -1.5457e+02 - logprior: -2.2911e+00
Fitted a model with MAP estimate = -154.9425
expansions: [(0, 2)]
discards: [ 0 42 60 61]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 157.0187 - loglik: -1.5386e+02 - logprior: -3.1233e+00
Epoch 2/2
19/19 - 4s - loss: 152.5580 - loglik: -1.5107e+02 - logprior: -1.3452e+00
Fitted a model with MAP estimate = -151.3336
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 159.1707 - loglik: -1.5508e+02 - logprior: -4.0550e+00
Epoch 2/10
19/19 - 4s - loss: 153.6316 - loglik: -1.5182e+02 - logprior: -1.6683e+00
Epoch 3/10
19/19 - 4s - loss: 152.2802 - loglik: -1.5081e+02 - logprior: -1.2625e+00
Epoch 4/10
19/19 - 4s - loss: 150.9780 - loglik: -1.4948e+02 - logprior: -1.2487e+00
Epoch 5/10
19/19 - 4s - loss: 150.4131 - loglik: -1.4892e+02 - logprior: -1.2368e+00
Epoch 6/10
19/19 - 4s - loss: 150.0198 - loglik: -1.4855e+02 - logprior: -1.2175e+00
Epoch 7/10
19/19 - 4s - loss: 149.5845 - loglik: -1.4816e+02 - logprior: -1.1877e+00
Epoch 8/10
19/19 - 4s - loss: 149.7451 - loglik: -1.4834e+02 - logprior: -1.1691e+00
Fitted a model with MAP estimate = -149.2562
Time for alignment: 113.1352
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 270.0047 - loglik: -2.6674e+02 - logprior: -3.2586e+00
Epoch 2/10
19/19 - 3s - loss: 200.7339 - loglik: -1.9921e+02 - logprior: -1.4842e+00
Epoch 3/10
19/19 - 3s - loss: 176.4508 - loglik: -1.7446e+02 - logprior: -1.8807e+00
Epoch 4/10
19/19 - 3s - loss: 172.9404 - loglik: -1.7085e+02 - logprior: -1.8266e+00
Epoch 5/10
19/19 - 3s - loss: 171.6472 - loglik: -1.6964e+02 - logprior: -1.7858e+00
Epoch 6/10
19/19 - 3s - loss: 170.8836 - loglik: -1.6887e+02 - logprior: -1.7958e+00
Epoch 7/10
19/19 - 3s - loss: 170.3784 - loglik: -1.6840e+02 - logprior: -1.7863e+00
Epoch 8/10
19/19 - 3s - loss: 169.5414 - loglik: -1.6759e+02 - logprior: -1.7774e+00
Epoch 9/10
19/19 - 3s - loss: 170.0352 - loglik: -1.6809e+02 - logprior: -1.7692e+00
Fitted a model with MAP estimate = -169.4194
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 168.8718 - loglik: -1.6466e+02 - logprior: -4.1667e+00
Epoch 2/2
19/19 - 4s - loss: 156.2588 - loglik: -1.5382e+02 - logprior: -2.2903e+00
Fitted a model with MAP estimate = -154.0896
expansions: [(0, 2)]
discards: [ 0 42 61 62]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 156.4438 - loglik: -1.5328e+02 - logprior: -3.1187e+00
Epoch 2/2
19/19 - 4s - loss: 152.1181 - loglik: -1.5063e+02 - logprior: -1.3385e+00
Fitted a model with MAP estimate = -150.9088
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 158.7207 - loglik: -1.5463e+02 - logprior: -4.0467e+00
Epoch 2/10
19/19 - 4s - loss: 153.8156 - loglik: -1.5197e+02 - logprior: -1.7061e+00
Epoch 3/10
19/19 - 4s - loss: 151.4652 - loglik: -1.4998e+02 - logprior: -1.2685e+00
Epoch 4/10
19/19 - 4s - loss: 151.2644 - loglik: -1.4976e+02 - logprior: -1.2482e+00
Epoch 5/10
19/19 - 4s - loss: 149.9267 - loglik: -1.4844e+02 - logprior: -1.2256e+00
Epoch 6/10
19/19 - 4s - loss: 149.8694 - loglik: -1.4839e+02 - logprior: -1.2269e+00
Epoch 7/10
19/19 - 4s - loss: 150.1168 - loglik: -1.4868e+02 - logprior: -1.1990e+00
Fitted a model with MAP estimate = -149.3419
Time for alignment: 107.0069
Computed alignments with likelihoods: ['-149.4610', '-149.2562', '-149.3419']
Best model has likelihood: -149.2562
time for generating output: 0.1580
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02777.projection.fasta
SP score = 0.9313475177304964
Training of 3 independent models on file PF07654.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4738f38fa0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4af5854a00>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b730940>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b208a1b50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4178213fd0>, <__main__.SimpleDirichletPrior object at 0x7f47392c1c10>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 231.9185 - loglik: -2.2866e+02 - logprior: -3.2320e+00
Epoch 2/10
19/19 - 2s - loss: 196.2393 - loglik: -1.9473e+02 - logprior: -1.4188e+00
Epoch 3/10
19/19 - 2s - loss: 183.9938 - loglik: -1.8221e+02 - logprior: -1.5527e+00
Epoch 4/10
19/19 - 2s - loss: 181.5388 - loglik: -1.7973e+02 - logprior: -1.5259e+00
Epoch 5/10
19/19 - 2s - loss: 180.3089 - loglik: -1.7854e+02 - logprior: -1.4953e+00
Epoch 6/10
19/19 - 2s - loss: 179.5660 - loglik: -1.7785e+02 - logprior: -1.4714e+00
Epoch 7/10
19/19 - 2s - loss: 178.9223 - loglik: -1.7720e+02 - logprior: -1.4680e+00
Epoch 8/10
19/19 - 2s - loss: 179.0811 - loglik: -1.7737e+02 - logprior: -1.4605e+00
Fitted a model with MAP estimate = -178.5530
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (32, 1), (33, 1), (34, 1), (35, 1), (37, 1), (47, 1), (48, 1), (49, 1), (53, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 183.6374 - loglik: -1.7951e+02 - logprior: -4.0561e+00
Epoch 2/2
19/19 - 2s - loss: 175.8107 - loglik: -1.7337e+02 - logprior: -2.2543e+00
Fitted a model with MAP estimate = -173.7364
expansions: [(0, 2)]
discards: [ 0 65 72]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 177.3421 - loglik: -1.7423e+02 - logprior: -3.0524e+00
Epoch 2/2
19/19 - 2s - loss: 173.6501 - loglik: -1.7222e+02 - logprior: -1.2964e+00
Fitted a model with MAP estimate = -172.3773
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 178.0850 - loglik: -1.7427e+02 - logprior: -3.7786e+00
Epoch 2/10
19/19 - 2s - loss: 173.9589 - loglik: -1.7242e+02 - logprior: -1.4209e+00
Epoch 3/10
19/19 - 2s - loss: 172.4573 - loglik: -1.7094e+02 - logprior: -1.2852e+00
Epoch 4/10
19/19 - 2s - loss: 171.3711 - loglik: -1.6978e+02 - logprior: -1.2544e+00
Epoch 5/10
19/19 - 2s - loss: 171.1151 - loglik: -1.6954e+02 - logprior: -1.2249e+00
Epoch 6/10
19/19 - 2s - loss: 170.4601 - loglik: -1.6892e+02 - logprior: -1.2040e+00
Epoch 7/10
19/19 - 2s - loss: 170.3688 - loglik: -1.6886e+02 - logprior: -1.1907e+00
Epoch 8/10
19/19 - 2s - loss: 170.0981 - loglik: -1.6863e+02 - logprior: -1.1643e+00
Epoch 9/10
19/19 - 2s - loss: 170.3667 - loglik: -1.6892e+02 - logprior: -1.1540e+00
Fitted a model with MAP estimate = -169.7350
Time for alignment: 72.7555
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 232.0219 - loglik: -2.2876e+02 - logprior: -3.2357e+00
Epoch 2/10
19/19 - 2s - loss: 196.5808 - loglik: -1.9502e+02 - logprior: -1.4252e+00
Epoch 3/10
19/19 - 2s - loss: 184.0114 - loglik: -1.8205e+02 - logprior: -1.5087e+00
Epoch 4/10
19/19 - 2s - loss: 180.9786 - loglik: -1.7911e+02 - logprior: -1.5054e+00
Epoch 5/10
19/19 - 2s - loss: 179.7149 - loglik: -1.7794e+02 - logprior: -1.5051e+00
Epoch 6/10
19/19 - 2s - loss: 179.0965 - loglik: -1.7738e+02 - logprior: -1.4740e+00
Epoch 7/10
19/19 - 2s - loss: 178.7829 - loglik: -1.7709e+02 - logprior: -1.4523e+00
Epoch 8/10
19/19 - 2s - loss: 178.7296 - loglik: -1.7704e+02 - logprior: -1.4404e+00
Epoch 9/10
19/19 - 2s - loss: 178.7410 - loglik: -1.7705e+02 - logprior: -1.4367e+00
Fitted a model with MAP estimate = -178.2444
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (31, 1), (32, 1), (35, 2), (37, 1), (49, 1), (50, 2), (53, 2), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 184.0671 - loglik: -1.7992e+02 - logprior: -4.0671e+00
Epoch 2/2
19/19 - 2s - loss: 175.8535 - loglik: -1.7334e+02 - logprior: -2.3014e+00
Fitted a model with MAP estimate = -173.8327
expansions: [(0, 2)]
discards: [ 0 64 68 73]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 177.4957 - loglik: -1.7437e+02 - logprior: -3.0611e+00
Epoch 2/2
19/19 - 2s - loss: 173.8255 - loglik: -1.7239e+02 - logprior: -1.2951e+00
Fitted a model with MAP estimate = -172.3832
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 178.0315 - loglik: -1.7422e+02 - logprior: -3.7669e+00
Epoch 2/10
19/19 - 2s - loss: 174.1801 - loglik: -1.7266e+02 - logprior: -1.4038e+00
Epoch 3/10
19/19 - 2s - loss: 172.3102 - loglik: -1.7081e+02 - logprior: -1.2775e+00
Epoch 4/10
19/19 - 2s - loss: 171.3451 - loglik: -1.6976e+02 - logprior: -1.2524e+00
Epoch 5/10
19/19 - 2s - loss: 170.7941 - loglik: -1.6922e+02 - logprior: -1.2218e+00
Epoch 6/10
19/19 - 2s - loss: 170.8009 - loglik: -1.6927e+02 - logprior: -1.1975e+00
Fitted a model with MAP estimate = -170.0996
Time for alignment: 67.8093
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 232.2704 - loglik: -2.2902e+02 - logprior: -3.2303e+00
Epoch 2/10
19/19 - 2s - loss: 196.0764 - loglik: -1.9455e+02 - logprior: -1.4071e+00
Epoch 3/10
19/19 - 2s - loss: 185.0413 - loglik: -1.8308e+02 - logprior: -1.5247e+00
Epoch 4/10
19/19 - 2s - loss: 181.5687 - loglik: -1.7971e+02 - logprior: -1.4900e+00
Epoch 5/10
19/19 - 2s - loss: 180.4594 - loglik: -1.7864e+02 - logprior: -1.5053e+00
Epoch 6/10
19/19 - 2s - loss: 179.6132 - loglik: -1.7785e+02 - logprior: -1.4917e+00
Epoch 7/10
19/19 - 2s - loss: 179.4089 - loglik: -1.7764e+02 - logprior: -1.4818e+00
Epoch 8/10
19/19 - 2s - loss: 179.0789 - loglik: -1.7733e+02 - logprior: -1.4768e+00
Epoch 9/10
19/19 - 2s - loss: 178.9374 - loglik: -1.7720e+02 - logprior: -1.4686e+00
Epoch 10/10
19/19 - 2s - loss: 178.5971 - loglik: -1.7686e+02 - logprior: -1.4656e+00
Fitted a model with MAP estimate = -178.4816
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (31, 1), (32, 1), (34, 1), (35, 2), (47, 1), (49, 1), (50, 2), (53, 2), (55, 2), (57, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 185.0318 - loglik: -1.8087e+02 - logprior: -4.0858e+00
Epoch 2/2
19/19 - 2s - loss: 175.9224 - loglik: -1.7335e+02 - logprior: -2.3195e+00
Fitted a model with MAP estimate = -173.7790
expansions: [(0, 2)]
discards: [ 0 43 65 69 70]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 177.1689 - loglik: -1.7404e+02 - logprior: -3.0655e+00
Epoch 2/2
19/19 - 2s - loss: 173.1718 - loglik: -1.7173e+02 - logprior: -1.2873e+00
Fitted a model with MAP estimate = -171.6987
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 177.4392 - loglik: -1.7363e+02 - logprior: -3.7651e+00
Epoch 2/10
19/19 - 2s - loss: 173.3920 - loglik: -1.7187e+02 - logprior: -1.4028e+00
Epoch 3/10
19/19 - 2s - loss: 171.7859 - loglik: -1.7029e+02 - logprior: -1.2664e+00
Epoch 4/10
19/19 - 2s - loss: 170.9913 - loglik: -1.6942e+02 - logprior: -1.2393e+00
Epoch 5/10
19/19 - 2s - loss: 170.0534 - loglik: -1.6850e+02 - logprior: -1.2056e+00
Epoch 6/10
19/19 - 2s - loss: 170.0146 - loglik: -1.6850e+02 - logprior: -1.1853e+00
Epoch 7/10
19/19 - 2s - loss: 169.6491 - loglik: -1.6818e+02 - logprior: -1.1598e+00
Epoch 8/10
19/19 - 2s - loss: 169.4992 - loglik: -1.6805e+02 - logprior: -1.1469e+00
Epoch 9/10
19/19 - 2s - loss: 169.6359 - loglik: -1.6822e+02 - logprior: -1.1250e+00
Fitted a model with MAP estimate = -169.1491
Time for alignment: 76.2413
Computed alignments with likelihoods: ['-169.7350', '-170.0996', '-169.1491']
Best model has likelihood: -169.1491
time for generating output: 0.1476
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07654.projection.fasta
SP score = 0.7836734693877551
Training of 3 independent models on file PF04082.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f47d45a3880>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f473870fe80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473870f340>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4739512160>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f473a687e50>, <__main__.SimpleDirichletPrior object at 0x7f48000d47f0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 21s - loss: 665.4290 - loglik: -6.6238e+02 - logprior: -2.9331e+00
Epoch 2/10
19/19 - 19s - loss: 614.9604 - loglik: -6.1285e+02 - logprior: -1.2134e+00
Epoch 3/10
19/19 - 19s - loss: 587.0218 - loglik: -5.8358e+02 - logprior: -1.4661e+00
Epoch 4/10
19/19 - 20s - loss: 576.4605 - loglik: -5.7261e+02 - logprior: -1.8637e+00
Epoch 5/10
19/19 - 20s - loss: 570.1322 - loglik: -5.6621e+02 - logprior: -2.0114e+00
Epoch 6/10
19/19 - 21s - loss: 565.7131 - loglik: -5.6155e+02 - logprior: -2.0274e+00
Epoch 7/10
19/19 - 22s - loss: 564.5448 - loglik: -5.6049e+02 - logprior: -2.0650e+00
Epoch 8/10
19/19 - 22s - loss: 562.6799 - loglik: -5.5881e+02 - logprior: -2.0850e+00
Epoch 9/10
19/19 - 21s - loss: 562.8262 - loglik: -5.5918e+02 - logprior: -2.0743e+00
Fitted a model with MAP estimate = -559.7430
expansions: [(23, 1), (24, 6), (28, 1), (48, 5), (60, 1), (62, 2), (66, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 2), (105, 1), (106, 3), (107, 1), (109, 1), (111, 1), (122, 1), (125, 1), (128, 1), (135, 1), (139, 1), (152, 2), (159, 1), (161, 1), (163, 1), (165, 1), (168, 1), (169, 1), (170, 1), (174, 1), (176, 3), (179, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 30s - loss: 653.2018 - loglik: -6.4833e+02 - logprior: -4.7655e+00
Epoch 2/2
19/19 - 27s - loss: 597.3456 - loglik: -5.9401e+02 - logprior: -2.6079e+00
Fitted a model with MAP estimate = -583.3954
expansions: [(0, 3), (53, 2), (57, 1), (58, 2), (75, 1), (76, 1), (81, 1), (102, 1), (136, 1), (187, 3), (192, 2), (240, 8)]
discards: [  0  29  30  31  54  59  60  61  62  77  78 100 104 131 132 137 185 188
 189 190 195 196 218 223 224 227 228 229 239]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 32s - loss: 595.8504 - loglik: -5.9247e+02 - logprior: -3.2841e+00
Epoch 2/2
19/19 - 31s - loss: 581.9124 - loglik: -5.7990e+02 - logprior: -1.4267e+00
Fitted a model with MAP estimate = -574.1672
expansions: [(103, 1), (191, 2), (195, 1), (196, 1), (215, 2), (216, 2)]
discards: [  0   2  54  61  77  81  97 229 230 231 232 233 234 235 236]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 34s - loss: 592.2595 - loglik: -5.8826e+02 - logprior: -3.9001e+00
Epoch 2/10
19/19 - 32s - loss: 581.1069 - loglik: -5.7892e+02 - logprior: -1.5389e+00
Epoch 3/10
19/19 - 31s - loss: 570.6127 - loglik: -5.6820e+02 - logprior: -9.2991e-01
Epoch 4/10
19/19 - 30s - loss: 564.3706 - loglik: -5.6124e+02 - logprior: -9.1421e-01
Epoch 5/10
19/19 - 30s - loss: 555.8846 - loglik: -5.5211e+02 - logprior: -1.0535e+00
Epoch 6/10
19/19 - 31s - loss: 552.0490 - loglik: -5.4802e+02 - logprior: -1.1790e+00
Epoch 7/10
19/19 - 31s - loss: 548.6555 - loglik: -5.4477e+02 - logprior: -1.2118e+00
Epoch 8/10
19/19 - 31s - loss: 546.5610 - loglik: -5.4292e+02 - logprior: -1.2214e+00
Epoch 9/10
19/19 - 30s - loss: 545.3323 - loglik: -5.4185e+02 - logprior: -1.2259e+00
Epoch 10/10
19/19 - 32s - loss: 544.0474 - loglik: -5.4052e+02 - logprior: -1.2439e+00
Fitted a model with MAP estimate = -540.0654
Time for alignment: 732.7245
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 26s - loss: 666.4471 - loglik: -6.6340e+02 - logprior: -2.9339e+00
Epoch 2/10
19/19 - 23s - loss: 616.4749 - loglik: -6.1437e+02 - logprior: -1.2115e+00
Epoch 3/10
19/19 - 24s - loss: 587.0507 - loglik: -5.8352e+02 - logprior: -1.5439e+00
Epoch 4/10
19/19 - 24s - loss: 577.5391 - loglik: -5.7310e+02 - logprior: -1.7885e+00
Epoch 5/10
19/19 - 23s - loss: 568.3033 - loglik: -5.6346e+02 - logprior: -1.9300e+00
Epoch 6/10
19/19 - 23s - loss: 565.6825 - loglik: -5.6100e+02 - logprior: -2.0046e+00
Epoch 7/10
19/19 - 23s - loss: 562.5998 - loglik: -5.5828e+02 - logprior: -2.0400e+00
Epoch 8/10
19/19 - 23s - loss: 561.6245 - loglik: -5.5769e+02 - logprior: -2.0612e+00
Epoch 9/10
19/19 - 24s - loss: 561.5454 - loglik: -5.5783e+02 - logprior: -2.0789e+00
Epoch 10/10
19/19 - 23s - loss: 560.2469 - loglik: -5.5660e+02 - logprior: -2.1102e+00
Fitted a model with MAP estimate = -557.6263
expansions: [(23, 1), (24, 6), (28, 1), (49, 6), (60, 1), (62, 2), (63, 1), (65, 1), (81, 1), (82, 1), (83, 1), (84, 1), (97, 1), (104, 1), (105, 2), (106, 1), (108, 1), (110, 1), (124, 1), (125, 1), (127, 1), (134, 1), (156, 8), (161, 1), (163, 1), (165, 1), (168, 1), (169, 1), (170, 1), (176, 2), (179, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 35s - loss: 657.7917 - loglik: -6.5285e+02 - logprior: -4.8274e+00
Epoch 2/2
19/19 - 32s - loss: 599.5978 - loglik: -5.9607e+02 - logprior: -2.8055e+00
Fitted a model with MAP estimate = -585.2055
expansions: [(0, 4), (28, 1), (54, 1), (77, 1), (102, 1), (103, 2), (130, 2), (135, 1), (154, 1), (158, 1), (193, 2), (195, 4), (202, 1), (217, 1), (242, 10)]
discards: [  0  30  31  32  33  55  61  62  63  80  81 100 104 128 131 136 155 159
 177 178 179 180 181 182 183 184 185 186 187 188 196 197 198 199 200 203
 215 229 230 231]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 35s - loss: 598.4955 - loglik: -5.9505e+02 - logprior: -3.3476e+00
Epoch 2/2
19/19 - 31s - loss: 581.7844 - loglik: -5.7976e+02 - logprior: -1.3915e+00
Fitted a model with MAP estimate = -574.9595
expansions: [(30, 1), (73, 1), (177, 7), (181, 3), (183, 1), (213, 2)]
discards: [  1   2   3 100 224 225 226 227 228 229 230 231 232 233 234]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 34s - loss: 591.3503 - loglik: -5.8830e+02 - logprior: -2.9569e+00
Epoch 2/10
19/19 - 27s - loss: 579.6522 - loglik: -5.7811e+02 - logprior: -1.0107e+00
Epoch 3/10
19/19 - 28s - loss: 569.0219 - loglik: -5.6704e+02 - logprior: -1.1016e+00
Epoch 4/10
19/19 - 29s - loss: 561.6742 - loglik: -5.5936e+02 - logprior: -1.1266e+00
Epoch 5/10
19/19 - 31s - loss: 553.8943 - loglik: -5.5118e+02 - logprior: -1.1539e+00
Epoch 6/10
19/19 - 31s - loss: 550.3695 - loglik: -5.4733e+02 - logprior: -1.2190e+00
Epoch 7/10
19/19 - 30s - loss: 547.1746 - loglik: -5.4403e+02 - logprior: -1.2476e+00
Epoch 8/10
19/19 - 27s - loss: 546.8608 - loglik: -5.4371e+02 - logprior: -1.2610e+00
Epoch 9/10
19/19 - 26s - loss: 544.1868 - loglik: -5.4101e+02 - logprior: -1.2805e+00
Epoch 10/10
19/19 - 26s - loss: 544.4987 - loglik: -5.4124e+02 - logprior: -1.3027e+00
Fitted a model with MAP estimate = -539.9841
Time for alignment: 780.2076
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 23s - loss: 666.4064 - loglik: -6.6335e+02 - logprior: -2.9345e+00
Epoch 2/10
19/19 - 22s - loss: 615.1060 - loglik: -6.1300e+02 - logprior: -1.2097e+00
Epoch 3/10
19/19 - 22s - loss: 589.3122 - loglik: -5.8594e+02 - logprior: -1.3987e+00
Epoch 4/10
19/19 - 23s - loss: 579.1555 - loglik: -5.7520e+02 - logprior: -1.5673e+00
Epoch 5/10
19/19 - 22s - loss: 573.4175 - loglik: -5.6885e+02 - logprior: -1.6785e+00
Epoch 6/10
19/19 - 20s - loss: 568.6781 - loglik: -5.6395e+02 - logprior: -1.7513e+00
Epoch 7/10
19/19 - 20s - loss: 564.5532 - loglik: -5.6023e+02 - logprior: -1.8770e+00
Epoch 8/10
19/19 - 20s - loss: 562.6498 - loglik: -5.5882e+02 - logprior: -1.9197e+00
Epoch 9/10
19/19 - 20s - loss: 561.8435 - loglik: -5.5833e+02 - logprior: -1.9395e+00
Epoch 10/10
19/19 - 21s - loss: 562.2636 - loglik: -5.5890e+02 - logprior: -1.9432e+00
Fitted a model with MAP estimate = -558.7214
expansions: [(23, 1), (24, 6), (44, 1), (48, 5), (53, 1), (62, 2), (63, 1), (65, 1), (80, 1), (81, 2), (82, 2), (83, 3), (103, 1), (104, 2), (105, 1), (108, 1), (109, 1), (124, 1), (127, 1), (132, 1), (135, 1), (160, 1), (162, 1), (168, 1), (169, 1), (170, 1), (177, 3), (179, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 32s - loss: 656.7720 - loglik: -6.5193e+02 - logprior: -4.7330e+00
Epoch 2/2
19/19 - 31s - loss: 599.5251 - loglik: -5.9617e+02 - logprior: -2.6619e+00
Fitted a model with MAP estimate = -582.8618
expansions: [(0, 3), (27, 2), (58, 2), (76, 1), (131, 2), (179, 1)]
discards: [  0  29  30  31  32  33  51  52  53  54  59  60  61  79  98 100 101 129
 132 184 185 186 187 188 189 190 191 192 193 223 224 225 235]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 32s - loss: 598.0492 - loglik: -5.9473e+02 - logprior: -3.2213e+00
Epoch 2/2
19/19 - 28s - loss: 583.7528 - loglik: -5.8189e+02 - logprior: -1.2368e+00
Fitted a model with MAP estimate = -577.2958
expansions: [(27, 4), (51, 4), (69, 1), (93, 1), (175, 8), (177, 2), (178, 7), (198, 2), (214, 8)]
discards: [  0   2  73 170]
Re-initialized the encoder parameters.
Fitting a model of length 247 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 38s - loss: 593.6337 - loglik: -5.8968e+02 - logprior: -3.8624e+00
Epoch 2/10
19/19 - 33s - loss: 579.4099 - loglik: -5.7698e+02 - logprior: -1.8930e+00
Epoch 3/10
19/19 - 34s - loss: 568.9722 - loglik: -5.6654e+02 - logprior: -1.3248e+00
Epoch 4/10
19/19 - 34s - loss: 559.3663 - loglik: -5.5655e+02 - logprior: -1.3292e+00
Epoch 5/10
19/19 - 34s - loss: 552.5465 - loglik: -5.4937e+02 - logprior: -1.4380e+00
Epoch 6/10
19/19 - 34s - loss: 548.1776 - loglik: -5.4473e+02 - logprior: -1.4920e+00
Epoch 7/10
19/19 - 34s - loss: 546.0944 - loglik: -5.4256e+02 - logprior: -1.5133e+00
Epoch 8/10
19/19 - 34s - loss: 544.4579 - loglik: -5.4095e+02 - logprior: -1.5147e+00
Epoch 9/10
19/19 - 34s - loss: 544.5017 - loglik: -5.4100e+02 - logprior: -1.5199e+00
Fitted a model with MAP estimate = -540.3529
Time for alignment: 763.7647
Computed alignments with likelihoods: ['-540.0654', '-539.9841', '-540.3529']
Best model has likelihood: -539.9841
time for generating output: 0.2948
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF04082.projection.fasta
SP score = 0.6628228782287823
Training of 3 independent models on file PF00046.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b0764c5e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f47ac1113a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac111b20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b07355370>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4982358970>, <__main__.SimpleDirichletPrior object at 0x7f473879f520>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 154.1913 - loglik: -1.5088e+02 - logprior: -3.3000e+00
Epoch 2/10
19/19 - 1s - loss: 123.5250 - loglik: -1.2190e+02 - logprior: -1.5806e+00
Epoch 3/10
19/19 - 2s - loss: 110.3174 - loglik: -1.0835e+02 - logprior: -1.6693e+00
Epoch 4/10
19/19 - 2s - loss: 106.7637 - loglik: -1.0477e+02 - logprior: -1.7408e+00
Epoch 5/10
19/19 - 1s - loss: 105.8332 - loglik: -1.0397e+02 - logprior: -1.6849e+00
Epoch 6/10
19/19 - 2s - loss: 105.4229 - loglik: -1.0359e+02 - logprior: -1.6833e+00
Epoch 7/10
19/19 - 2s - loss: 105.1271 - loglik: -1.0333e+02 - logprior: -1.6487e+00
Epoch 8/10
19/19 - 1s - loss: 105.0826 - loglik: -1.0330e+02 - logprior: -1.6397e+00
Epoch 9/10
19/19 - 1s - loss: 104.9457 - loglik: -1.0317e+02 - logprior: -1.6316e+00
Epoch 10/10
19/19 - 2s - loss: 105.1384 - loglik: -1.0336e+02 - logprior: -1.6308e+00
Fitted a model with MAP estimate = -104.6288
expansions: [(3, 1), (5, 1), (7, 1), (10, 1), (14, 1), (16, 1), (21, 1), (24, 1), (27, 3), (28, 1), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 106.3796 - loglik: -1.0314e+02 - logprior: -3.1668e+00
Epoch 2/2
19/19 - 2s - loss: 99.4082 - loglik: -9.7871e+01 - logprior: -1.4019e+00
Fitted a model with MAP estimate = -98.4392
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 101.5391 - loglik: -9.8420e+01 - logprior: -3.0804e+00
Epoch 2/2
19/19 - 2s - loss: 99.0509 - loglik: -9.7614e+01 - logprior: -1.3287e+00
Fitted a model with MAP estimate = -98.1278
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 101.3224 - loglik: -9.8235e+01 - logprior: -3.0551e+00
Epoch 2/10
19/19 - 2s - loss: 98.7286 - loglik: -9.7310e+01 - logprior: -1.3122e+00
Epoch 3/10
19/19 - 2s - loss: 98.1446 - loglik: -9.6682e+01 - logprior: -1.2859e+00
Epoch 4/10
19/19 - 1s - loss: 97.5031 - loglik: -9.6031e+01 - logprior: -1.2390e+00
Epoch 5/10
19/19 - 2s - loss: 97.2671 - loglik: -9.5818e+01 - logprior: -1.2199e+00
Epoch 6/10
19/19 - 2s - loss: 96.8566 - loglik: -9.5450e+01 - logprior: -1.1992e+00
Epoch 7/10
19/19 - 2s - loss: 96.8616 - loglik: -9.5490e+01 - logprior: -1.1788e+00
Fitted a model with MAP estimate = -96.5193
Time for alignment: 57.0706
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 154.1433 - loglik: -1.5084e+02 - logprior: -3.3008e+00
Epoch 2/10
19/19 - 1s - loss: 124.1244 - loglik: -1.2252e+02 - logprior: -1.5692e+00
Epoch 3/10
19/19 - 1s - loss: 110.1352 - loglik: -1.0812e+02 - logprior: -1.6850e+00
Epoch 4/10
19/19 - 1s - loss: 107.0163 - loglik: -1.0499e+02 - logprior: -1.7435e+00
Epoch 5/10
19/19 - 1s - loss: 105.8146 - loglik: -1.0393e+02 - logprior: -1.6798e+00
Epoch 6/10
19/19 - 2s - loss: 105.6115 - loglik: -1.0376e+02 - logprior: -1.6873e+00
Epoch 7/10
19/19 - 2s - loss: 105.1686 - loglik: -1.0337e+02 - logprior: -1.6512e+00
Epoch 8/10
19/19 - 1s - loss: 105.0285 - loglik: -1.0325e+02 - logprior: -1.6415e+00
Epoch 9/10
19/19 - 2s - loss: 105.0659 - loglik: -1.0329e+02 - logprior: -1.6325e+00
Fitted a model with MAP estimate = -104.6853
expansions: [(3, 1), (5, 1), (7, 1), (10, 1), (14, 1), (16, 1), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 106.8915 - loglik: -1.0365e+02 - logprior: -3.1805e+00
Epoch 2/2
19/19 - 2s - loss: 99.5193 - loglik: -9.7955e+01 - logprior: -1.4417e+00
Fitted a model with MAP estimate = -98.4891
expansions: []
discards: [36 39]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 101.5793 - loglik: -9.8458e+01 - logprior: -3.0860e+00
Epoch 2/2
19/19 - 1s - loss: 99.0479 - loglik: -9.7613e+01 - logprior: -1.3333e+00
Fitted a model with MAP estimate = -98.1191
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 101.2949 - loglik: -9.8202e+01 - logprior: -3.0608e+00
Epoch 2/10
19/19 - 1s - loss: 98.7069 - loglik: -9.7289e+01 - logprior: -1.3133e+00
Epoch 3/10
19/19 - 1s - loss: 98.1350 - loglik: -9.6672e+01 - logprior: -1.2818e+00
Epoch 4/10
19/19 - 1s - loss: 97.4112 - loglik: -9.5937e+01 - logprior: -1.2453e+00
Epoch 5/10
19/19 - 2s - loss: 97.3469 - loglik: -9.5909e+01 - logprior: -1.2115e+00
Epoch 6/10
19/19 - 2s - loss: 96.8622 - loglik: -9.5461e+01 - logprior: -1.1979e+00
Epoch 7/10
19/19 - 1s - loss: 96.7645 - loglik: -9.5397e+01 - logprior: -1.1815e+00
Epoch 8/10
19/19 - 1s - loss: 96.8744 - loglik: -9.5530e+01 - logprior: -1.1664e+00
Fitted a model with MAP estimate = -96.4601
Time for alignment: 56.1456
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.2324 - loglik: -1.5093e+02 - logprior: -3.2965e+00
Epoch 2/10
19/19 - 1s - loss: 123.6712 - loglik: -1.2208e+02 - logprior: -1.5580e+00
Epoch 3/10
19/19 - 1s - loss: 111.4994 - loglik: -1.0965e+02 - logprior: -1.6697e+00
Epoch 4/10
19/19 - 1s - loss: 108.2926 - loglik: -1.0633e+02 - logprior: -1.7004e+00
Epoch 5/10
19/19 - 2s - loss: 106.8899 - loglik: -1.0504e+02 - logprior: -1.6388e+00
Epoch 6/10
19/19 - 1s - loss: 106.5600 - loglik: -1.0473e+02 - logprior: -1.6353e+00
Epoch 7/10
19/19 - 1s - loss: 106.3750 - loglik: -1.0459e+02 - logprior: -1.6068e+00
Epoch 8/10
19/19 - 1s - loss: 106.2544 - loglik: -1.0448e+02 - logprior: -1.5980e+00
Epoch 9/10
19/19 - 1s - loss: 106.0675 - loglik: -1.0431e+02 - logprior: -1.5940e+00
Epoch 10/10
19/19 - 1s - loss: 105.7151 - loglik: -1.0396e+02 - logprior: -1.5915e+00
Fitted a model with MAP estimate = -105.7418
expansions: [(3, 1), (5, 1), (7, 2), (9, 1), (16, 1), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 107.0096 - loglik: -1.0375e+02 - logprior: -3.1799e+00
Epoch 2/2
19/19 - 2s - loss: 99.6125 - loglik: -9.8029e+01 - logprior: -1.4356e+00
Fitted a model with MAP estimate = -98.5180
expansions: []
discards: [36 39]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.6462 - loglik: -9.8524e+01 - logprior: -3.0814e+00
Epoch 2/2
19/19 - 2s - loss: 98.9254 - loglik: -9.7487e+01 - logprior: -1.3333e+00
Fitted a model with MAP estimate = -98.1422
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 105.1736 - loglik: -1.0098e+02 - logprior: -4.1644e+00
Epoch 2/10
19/19 - 1s - loss: 101.8634 - loglik: -9.9445e+01 - logprior: -2.3161e+00
Epoch 3/10
19/19 - 2s - loss: 100.5181 - loglik: -9.8100e+01 - logprior: -2.2477e+00
Epoch 4/10
19/19 - 2s - loss: 98.8065 - loglik: -9.6907e+01 - logprior: -1.6804e+00
Epoch 5/10
19/19 - 2s - loss: 98.0041 - loglik: -9.6450e+01 - logprior: -1.3267e+00
Epoch 6/10
19/19 - 2s - loss: 97.7475 - loglik: -9.6227e+01 - logprior: -1.3184e+00
Epoch 7/10
19/19 - 2s - loss: 97.8112 - loglik: -9.6333e+01 - logprior: -1.2916e+00
Fitted a model with MAP estimate = -97.3283
Time for alignment: 56.1314
Computed alignments with likelihoods: ['-96.5193', '-96.4601', '-97.3283']
Best model has likelihood: -96.4601
time for generating output: 0.1323
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00046.projection.fasta
SP score = 0.9594907407407407
Training of 3 independent models on file PF01814.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f475c33efa0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f41901ce7c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f41901cedf0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f497e4f8f70>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4190663be0>, <__main__.SimpleDirichletPrior object at 0x7f47b4433ee0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 365.5593 - loglik: -3.6228e+02 - logprior: -3.1640e+00
Epoch 2/10
19/19 - 4s - loss: 335.6334 - loglik: -3.3355e+02 - logprior: -1.2064e+00
Epoch 3/10
19/19 - 4s - loss: 321.8711 - loglik: -3.1884e+02 - logprior: -1.4698e+00
Epoch 4/10
19/19 - 4s - loss: 315.6163 - loglik: -3.1288e+02 - logprior: -1.5297e+00
Epoch 5/10
19/19 - 4s - loss: 311.1220 - loglik: -3.0833e+02 - logprior: -1.6374e+00
Epoch 6/10
19/19 - 4s - loss: 309.5184 - loglik: -3.0665e+02 - logprior: -1.6542e+00
Epoch 7/10
19/19 - 5s - loss: 308.9055 - loglik: -3.0622e+02 - logprior: -1.6517e+00
Epoch 8/10
19/19 - 4s - loss: 308.7411 - loglik: -3.0616e+02 - logprior: -1.6440e+00
Epoch 9/10
19/19 - 5s - loss: 307.6591 - loglik: -3.0518e+02 - logprior: -1.6416e+00
Epoch 10/10
19/19 - 5s - loss: 307.4348 - loglik: -3.0505e+02 - logprior: -1.6396e+00
Fitted a model with MAP estimate = -306.3625
expansions: [(15, 1), (17, 1), (19, 1), (21, 4), (24, 4), (27, 1), (29, 1), (32, 1), (51, 1), (53, 3), (73, 1), (74, 1), (78, 1), (83, 1), (84, 3), (86, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 354.2898 - loglik: -3.5091e+02 - logprior: -3.2698e+00
Epoch 2/2
19/19 - 6s - loss: 322.1000 - loglik: -3.2003e+02 - logprior: -1.4196e+00
Fitted a model with MAP estimate = -313.9874
expansions: [(22, 1), (97, 2)]
discards: [ 27 105]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 322.0217 - loglik: -3.1884e+02 - logprior: -3.0829e+00
Epoch 2/2
19/19 - 6s - loss: 314.1711 - loglik: -3.1231e+02 - logprior: -1.2537e+00
Fitted a model with MAP estimate = -309.7325
expansions: [(2, 1), (33, 1)]
discards: [ 0 98 99]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 321.1536 - loglik: -3.1715e+02 - logprior: -3.9399e+00
Epoch 2/10
19/19 - 6s - loss: 313.4366 - loglik: -3.1086e+02 - logprior: -2.1529e+00
Epoch 3/10
19/19 - 6s - loss: 308.1373 - loglik: -3.0511e+02 - logprior: -2.0819e+00
Epoch 4/10
19/19 - 6s - loss: 303.8331 - loglik: -3.0098e+02 - logprior: -1.4872e+00
Epoch 5/10
19/19 - 6s - loss: 301.1076 - loglik: -2.9850e+02 - logprior: -1.1462e+00
Epoch 6/10
19/19 - 6s - loss: 299.4392 - loglik: -2.9691e+02 - logprior: -1.1471e+00
Epoch 7/10
19/19 - 6s - loss: 298.5760 - loglik: -2.9617e+02 - logprior: -1.1307e+00
Epoch 8/10
19/19 - 6s - loss: 298.0231 - loglik: -2.9575e+02 - logprior: -1.1148e+00
Epoch 9/10
19/19 - 6s - loss: 297.0097 - loglik: -2.9485e+02 - logprior: -1.1033e+00
Epoch 10/10
19/19 - 6s - loss: 296.7007 - loglik: -2.9463e+02 - logprior: -1.0949e+00
Fitted a model with MAP estimate = -295.4883
Time for alignment: 168.4903
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 365.4486 - loglik: -3.6217e+02 - logprior: -3.1602e+00
Epoch 2/10
19/19 - 5s - loss: 336.7352 - loglik: -3.3465e+02 - logprior: -1.2032e+00
Epoch 3/10
19/19 - 5s - loss: 322.0105 - loglik: -3.1886e+02 - logprior: -1.5191e+00
Epoch 4/10
19/19 - 5s - loss: 314.7306 - loglik: -3.1130e+02 - logprior: -1.5510e+00
Epoch 5/10
19/19 - 5s - loss: 311.8565 - loglik: -3.0846e+02 - logprior: -1.6050e+00
Epoch 6/10
19/19 - 5s - loss: 310.7030 - loglik: -3.0760e+02 - logprior: -1.6104e+00
Epoch 7/10
19/19 - 5s - loss: 308.8568 - loglik: -3.0602e+02 - logprior: -1.6302e+00
Epoch 8/10
19/19 - 5s - loss: 308.6843 - loglik: -3.0602e+02 - logprior: -1.6452e+00
Epoch 9/10
19/19 - 5s - loss: 307.4945 - loglik: -3.0492e+02 - logprior: -1.6839e+00
Epoch 10/10
19/19 - 5s - loss: 307.2576 - loglik: -3.0477e+02 - logprior: -1.6898e+00
Fitted a model with MAP estimate = -306.0388
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 4), (24, 2), (27, 1), (28, 1), (29, 1), (32, 1), (51, 1), (53, 3), (54, 2), (74, 1), (75, 4), (82, 1), (83, 1), (84, 3), (86, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 355.0421 - loglik: -3.5161e+02 - logprior: -3.3243e+00
Epoch 2/2
19/19 - 6s - loss: 323.2148 - loglik: -3.2111e+02 - logprior: -1.4876e+00
Fitted a model with MAP estimate = -314.7400
expansions: [(96, 1)]
discards: [ 27  28  29  65  98  99 100 101 102 110]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 322.4928 - loglik: -3.1931e+02 - logprior: -3.0864e+00
Epoch 2/2
19/19 - 5s - loss: 314.2142 - loglik: -3.1249e+02 - logprior: -1.2334e+00
Fitted a model with MAP estimate = -310.1123
expansions: [(2, 1), (24, 2), (61, 1), (91, 4)]
discards: [ 0 30 66]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 321.3005 - loglik: -3.1727e+02 - logprior: -3.9364e+00
Epoch 2/10
19/19 - 6s - loss: 312.9774 - loglik: -3.1015e+02 - logprior: -2.1810e+00
Epoch 3/10
19/19 - 6s - loss: 307.7966 - loglik: -3.0430e+02 - logprior: -2.0863e+00
Epoch 4/10
19/19 - 6s - loss: 303.4704 - loglik: -3.0020e+02 - logprior: -1.5821e+00
Epoch 5/10
19/19 - 6s - loss: 300.4758 - loglik: -2.9768e+02 - logprior: -1.2258e+00
Epoch 6/10
19/19 - 6s - loss: 298.9775 - loglik: -2.9635e+02 - logprior: -1.2176e+00
Epoch 7/10
19/19 - 6s - loss: 297.4229 - loglik: -2.9495e+02 - logprior: -1.2067e+00
Epoch 8/10
19/19 - 6s - loss: 297.6117 - loglik: -2.9526e+02 - logprior: -1.2130e+00
Fitted a model with MAP estimate = -295.5396
Time for alignment: 156.3761
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 365.5574 - loglik: -3.6228e+02 - logprior: -3.1629e+00
Epoch 2/10
19/19 - 4s - loss: 336.1320 - loglik: -3.3405e+02 - logprior: -1.2114e+00
Epoch 3/10
19/19 - 4s - loss: 322.2766 - loglik: -3.1906e+02 - logprior: -1.4879e+00
Epoch 4/10
19/19 - 4s - loss: 315.7803 - loglik: -3.1216e+02 - logprior: -1.4670e+00
Epoch 5/10
19/19 - 5s - loss: 311.2963 - loglik: -3.0797e+02 - logprior: -1.6074e+00
Epoch 6/10
19/19 - 5s - loss: 309.3009 - loglik: -3.0652e+02 - logprior: -1.6680e+00
Epoch 7/10
19/19 - 5s - loss: 308.2890 - loglik: -3.0563e+02 - logprior: -1.6816e+00
Epoch 8/10
19/19 - 4s - loss: 307.0466 - loglik: -3.0452e+02 - logprior: -1.6845e+00
Epoch 9/10
19/19 - 5s - loss: 307.6944 - loglik: -3.0524e+02 - logprior: -1.6812e+00
Fitted a model with MAP estimate = -306.1120
expansions: [(15, 1), (17, 1), (19, 1), (20, 1), (21, 4), (24, 2), (27, 1), (28, 1), (29, 1), (32, 1), (51, 1), (53, 3), (73, 1), (74, 1), (76, 3), (83, 1), (84, 3), (87, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 356.0205 - loglik: -3.5176e+02 - logprior: -4.1679e+00
Epoch 2/2
19/19 - 6s - loss: 322.3805 - loglik: -3.1952e+02 - logprior: -2.3012e+00
Fitted a model with MAP estimate = -313.6077
expansions: [(0, 2), (94, 1), (96, 1)]
discards: [  0  28  33  98 107]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 320.8935 - loglik: -3.1771e+02 - logprior: -3.0910e+00
Epoch 2/2
19/19 - 6s - loss: 312.5956 - loglik: -3.1079e+02 - logprior: -1.2411e+00
Fitted a model with MAP estimate = -308.1899
expansions: [(28, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 318.5335 - loglik: -3.1560e+02 - logprior: -2.8391e+00
Epoch 2/10
19/19 - 6s - loss: 311.4833 - loglik: -3.0977e+02 - logprior: -1.1164e+00
Epoch 3/10
19/19 - 6s - loss: 306.9803 - loglik: -3.0432e+02 - logprior: -1.2504e+00
Epoch 4/10
19/19 - 6s - loss: 303.6581 - loglik: -3.0055e+02 - logprior: -1.1600e+00
Epoch 5/10
19/19 - 6s - loss: 301.2773 - loglik: -2.9822e+02 - logprior: -1.1826e+00
Epoch 6/10
19/19 - 6s - loss: 298.9567 - loglik: -2.9618e+02 - logprior: -1.1906e+00
Epoch 7/10
19/19 - 6s - loss: 297.7050 - loglik: -2.9514e+02 - logprior: -1.1908e+00
Epoch 8/10
19/19 - 6s - loss: 296.6436 - loglik: -2.9428e+02 - logprior: -1.1757e+00
Epoch 9/10
19/19 - 6s - loss: 296.8751 - loglik: -2.9464e+02 - logprior: -1.1604e+00
Fitted a model with MAP estimate = -294.9001
Time for alignment: 157.6676
Computed alignments with likelihoods: ['-295.4883', '-295.5396', '-294.9001']
Best model has likelihood: -294.9001
time for generating output: 0.1625
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01814.projection.fasta
SP score = 0.9255631733594515
Training of 3 independent models on file PF00155.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4af5a05af0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f49823c7d00>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47390e0730>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f498203b640>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4ab2111fd0>, <__main__.SimpleDirichletPrior object at 0x7f497e7068e0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 917.6789 - loglik: -9.1524e+02 - logprior: -1.8969e+00
Epoch 2/10
39/39 - 43s - loss: 818.6849 - loglik: -8.1459e+02 - logprior: -2.0778e+00
Epoch 3/10
39/39 - 47s - loss: 802.5001 - loglik: -7.9832e+02 - logprior: -2.4455e+00
Epoch 4/10
39/39 - 46s - loss: 798.7191 - loglik: -7.9444e+02 - logprior: -2.4504e+00
Epoch 5/10
39/39 - 41s - loss: 796.8860 - loglik: -7.9263e+02 - logprior: -2.4546e+00
Epoch 6/10
39/39 - 43s - loss: 795.4847 - loglik: -7.9127e+02 - logprior: -2.4940e+00
Epoch 7/10
39/39 - 46s - loss: 794.4136 - loglik: -7.9026e+02 - logprior: -2.5222e+00
Epoch 8/10
39/39 - 48s - loss: 793.8030 - loglik: -7.8979e+02 - logprior: -2.5362e+00
Epoch 9/10
39/39 - 50s - loss: 792.8177 - loglik: -7.8893e+02 - logprior: -2.5586e+00
Epoch 10/10
39/39 - 51s - loss: 792.2903 - loglik: -7.8851e+02 - logprior: -2.5685e+00
Fitted a model with MAP estimate = -783.1127
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (23, 3), (24, 1), (25, 1), (30, 1), (40, 2), (41, 1), (42, 2), (44, 2), (55, 1), (58, 1), (61, 1), (78, 1), (79, 1), (80, 1), (81, 1), (86, 1), (87, 1), (93, 1), (97, 1), (98, 1), (103, 1), (119, 1), (130, 1), (145, 1), (148, 2), (153, 2), (154, 1), (155, 2), (166, 1), (181, 1), (184, 1), (186, 1), (187, 1), (188, 1), (189, 1), (206, 4), (207, 1), (208, 1), (209, 1), (210, 1), (219, 1), (220, 1), (226, 2), (227, 1), (229, 2), (240, 3), (242, 1), (245, 1), (250, 1), (261, 1), (262, 1), (263, 1), (269, 2), (270, 2), (271, 1), (272, 1)]
discards: [94]
Re-initialized the encoder parameters.
Fitting a model of length 357 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 79s - loss: 837.4703 - loglik: -8.3420e+02 - logprior: -2.8465e+00
Epoch 2/2
39/39 - 68s - loss: 782.9463 - loglik: -7.7997e+02 - logprior: -1.5140e+00
Fitted a model with MAP estimate = -765.9658
expansions: [(122, 2)]
discards: [  1  30 189 285 289 301 317 341 344]
Re-initialized the encoder parameters.
Fitting a model of length 350 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 58s - loss: 800.5635 - loglik: -7.9832e+02 - logprior: -1.9164e+00
Epoch 2/2
39/39 - 54s - loss: 779.4877 - loglik: -7.7695e+02 - logprior: -1.1371e+00
Fitted a model with MAP estimate = -763.7099
expansions: [(155, 2), (338, 1)]
discards: [120]
Re-initialized the encoder parameters.
Fitting a model of length 352 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 59s - loss: 787.9058 - loglik: -7.8588e+02 - logprior: -1.7301e+00
Epoch 2/10
39/39 - 56s - loss: 769.2247 - loglik: -7.6701e+02 - logprior: -9.6434e-01
Epoch 3/10
39/39 - 55s - loss: 761.9814 - loglik: -7.5911e+02 - logprior: -9.8372e-01
Epoch 4/10
39/39 - 55s - loss: 757.7978 - loglik: -7.5473e+02 - logprior: -8.9606e-01
Epoch 5/10
39/39 - 55s - loss: 756.5722 - loglik: -7.5350e+02 - logprior: -8.1433e-01
Epoch 6/10
39/39 - 55s - loss: 755.3405 - loglik: -7.5237e+02 - logprior: -7.8650e-01
Epoch 7/10
39/39 - 56s - loss: 753.9651 - loglik: -7.5127e+02 - logprior: -6.4313e-01
Epoch 8/10
39/39 - 55s - loss: 753.4827 - loglik: -7.5105e+02 - logprior: -5.4382e-01
Epoch 9/10
39/39 - 55s - loss: 751.5387 - loglik: -7.4935e+02 - logprior: -4.3279e-01
Epoch 10/10
39/39 - 55s - loss: 750.9351 - loglik: -7.4896e+02 - logprior: -3.4570e-01
Fitted a model with MAP estimate = -748.1867
Time for alignment: 1547.9578
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 916.1368 - loglik: -9.1370e+02 - logprior: -1.8934e+00
Epoch 2/10
39/39 - 39s - loss: 817.4893 - loglik: -8.1333e+02 - logprior: -2.0554e+00
Epoch 3/10
39/39 - 40s - loss: 803.1078 - loglik: -7.9819e+02 - logprior: -2.3160e+00
Epoch 4/10
39/39 - 39s - loss: 798.2239 - loglik: -7.9353e+02 - logprior: -2.4089e+00
Epoch 5/10
39/39 - 37s - loss: 796.0497 - loglik: -7.9153e+02 - logprior: -2.4958e+00
Epoch 6/10
39/39 - 37s - loss: 794.7108 - loglik: -7.9042e+02 - logprior: -2.5202e+00
Epoch 7/10
39/39 - 38s - loss: 794.1225 - loglik: -7.8999e+02 - logprior: -2.5356e+00
Epoch 8/10
39/39 - 38s - loss: 793.3640 - loglik: -7.8935e+02 - logprior: -2.6101e+00
Epoch 9/10
39/39 - 38s - loss: 792.4967 - loglik: -7.8865e+02 - logprior: -2.5608e+00
Epoch 10/10
39/39 - 38s - loss: 792.3065 - loglik: -7.8853e+02 - logprior: -2.6193e+00
Fitted a model with MAP estimate = -782.8192
expansions: [(0, 2), (15, 2), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (30, 1), (40, 1), (42, 1), (43, 2), (45, 2), (48, 2), (55, 1), (58, 1), (61, 1), (78, 1), (79, 1), (80, 1), (81, 1), (86, 1), (87, 1), (90, 1), (95, 1), (97, 1), (98, 1), (103, 1), (121, 1), (124, 2), (130, 1), (142, 1), (148, 2), (149, 1), (154, 1), (155, 2), (169, 1), (181, 1), (184, 1), (185, 1), (187, 1), (188, 1), (189, 1), (206, 4), (207, 1), (208, 1), (209, 1), (210, 1), (219, 1), (222, 1), (226, 2), (227, 1), (240, 3), (242, 1), (245, 1), (250, 1), (261, 1), (262, 1), (263, 1), (270, 3), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 357 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 59s - loss: 837.2691 - loglik: -8.3386e+02 - logprior: -2.9782e+00
Epoch 2/2
39/39 - 56s - loss: 780.5786 - loglik: -7.7735e+02 - logprior: -1.6179e+00
Fitted a model with MAP estimate = -763.1916
expansions: [(159, 1)]
discards: [  0  64 117 288 318 344]
Re-initialized the encoder parameters.
Fitting a model of length 352 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 57s - loss: 799.2311 - loglik: -7.9595e+02 - logprior: -2.9688e+00
Epoch 2/2
39/39 - 55s - loss: 777.5721 - loglik: -7.7525e+02 - logprior: -1.2599e+00
Fitted a model with MAP estimate = -762.2080
expansions: [(0, 2)]
discards: [  0 119]
Re-initialized the encoder parameters.
Fitting a model of length 352 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 786.7451 - loglik: -7.8470e+02 - logprior: -1.7238e+00
Epoch 2/10
39/39 - 55s - loss: 769.4158 - loglik: -7.6708e+02 - logprior: -8.7364e-01
Epoch 3/10
39/39 - 56s - loss: 761.0688 - loglik: -7.5805e+02 - logprior: -9.1798e-01
Epoch 4/10
39/39 - 57s - loss: 758.4301 - loglik: -7.5526e+02 - logprior: -8.3872e-01
Epoch 5/10
39/39 - 57s - loss: 756.5488 - loglik: -7.5341e+02 - logprior: -7.6187e-01
Epoch 6/10
39/39 - 59s - loss: 754.9995 - loglik: -7.5210e+02 - logprior: -6.5990e-01
Epoch 7/10
39/39 - 59s - loss: 753.8709 - loglik: -7.5119e+02 - logprior: -5.8634e-01
Epoch 8/10
39/39 - 59s - loss: 751.9427 - loglik: -7.4948e+02 - logprior: -5.4060e-01
Epoch 9/10
39/39 - 57s - loss: 752.6149 - loglik: -7.5044e+02 - logprior: -4.0554e-01
Fitted a model with MAP estimate = -748.6255
Time for alignment: 1366.6277
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 44s - loss: 917.6329 - loglik: -9.1519e+02 - logprior: -1.9073e+00
Epoch 2/10
39/39 - 41s - loss: 819.9756 - loglik: -8.1578e+02 - logprior: -2.0308e+00
Epoch 3/10
39/39 - 42s - loss: 804.4961 - loglik: -7.9929e+02 - logprior: -2.2781e+00
Epoch 4/10
39/39 - 41s - loss: 799.3427 - loglik: -7.9436e+02 - logprior: -2.3953e+00
Epoch 5/10
39/39 - 41s - loss: 796.0172 - loglik: -7.9134e+02 - logprior: -2.5360e+00
Epoch 6/10
39/39 - 41s - loss: 794.9694 - loglik: -7.9055e+02 - logprior: -2.5840e+00
Epoch 7/10
39/39 - 42s - loss: 793.9733 - loglik: -7.8974e+02 - logprior: -2.6105e+00
Epoch 8/10
39/39 - 40s - loss: 793.0999 - loglik: -7.8910e+02 - logprior: -2.5784e+00
Epoch 9/10
39/39 - 38s - loss: 792.5530 - loglik: -7.8864e+02 - logprior: -2.6472e+00
Epoch 10/10
39/39 - 38s - loss: 792.4667 - loglik: -7.8866e+02 - logprior: -2.6374e+00
Fitted a model with MAP estimate = -782.8866
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2), (26, 1), (33, 1), (40, 1), (42, 1), (43, 2), (45, 2), (56, 1), (59, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 1), (87, 1), (88, 1), (91, 1), (96, 1), (98, 1), (99, 1), (103, 1), (112, 1), (121, 1), (124, 2), (132, 2), (145, 1), (148, 2), (149, 1), (154, 1), (155, 2), (161, 1), (181, 1), (184, 1), (186, 1), (187, 1), (188, 1), (189, 1), (206, 4), (207, 1), (208, 1), (209, 1), (210, 1), (220, 1), (222, 1), (228, 2), (240, 2), (241, 1), (243, 1), (246, 1), (251, 1), (258, 1), (261, 1), (263, 1), (270, 2), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 356 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 56s - loss: 838.3842 - loglik: -8.3504e+02 - logprior: -2.9372e+00
Epoch 2/2
39/39 - 53s - loss: 780.6748 - loglik: -7.7773e+02 - logprior: -1.5934e+00
Fitted a model with MAP estimate = -763.9113
expansions: [(159, 1)]
discards: [  0   1  30 117 168 304 319]
Re-initialized the encoder parameters.
Fitting a model of length 350 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 57s - loss: 797.9955 - loglik: -7.9560e+02 - logprior: -2.0561e+00
Epoch 2/2
39/39 - 53s - loss: 777.9957 - loglik: -7.7553e+02 - logprior: -9.1711e-01
Fitted a model with MAP estimate = -762.0678
expansions: [(0, 2)]
discards: [118]
Re-initialized the encoder parameters.
Fitting a model of length 351 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 787.6653 - loglik: -7.8540e+02 - logprior: -1.9862e+00
Epoch 2/10
39/39 - 54s - loss: 769.3211 - loglik: -7.6733e+02 - logprior: -9.2381e-01
Epoch 3/10
39/39 - 56s - loss: 761.3185 - loglik: -7.5878e+02 - logprior: -8.8462e-01
Epoch 4/10
39/39 - 59s - loss: 758.0478 - loglik: -7.5525e+02 - logprior: -8.3228e-01
Epoch 5/10
39/39 - 60s - loss: 757.6334 - loglik: -7.5476e+02 - logprior: -7.5877e-01
Epoch 6/10
39/39 - 61s - loss: 754.9426 - loglik: -7.5219e+02 - logprior: -6.5514e-01
Epoch 7/10
39/39 - 63s - loss: 754.2189 - loglik: -7.5166e+02 - logprior: -5.4998e-01
Epoch 8/10
39/39 - 61s - loss: 752.6072 - loglik: -7.5025e+02 - logprior: -4.6771e-01
Epoch 9/10
39/39 - 67s - loss: 752.4540 - loglik: -7.5030e+02 - logprior: -3.9024e-01
Epoch 10/10
39/39 - 69s - loss: 751.2552 - loglik: -7.4931e+02 - logprior: -2.8276e-01
Fitted a model with MAP estimate = -748.3086
Time for alignment: 1464.8997
Computed alignments with likelihoods: ['-748.1867', '-748.6255', '-748.3086']
Best model has likelihood: -748.1867
time for generating output: 1.4640
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00155.projection.fasta
SP score = 0.4615111711086806
Training of 3 independent models on file PF00450.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f47392647f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4739255f40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f475c1c26d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f497eb5dd90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f498204b460>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f473a28c220>, <__main__.SimpleDirichletPrior object at 0x7f41793dc040>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 71s - loss: 891.8430 - loglik: -8.8958e+02 - logprior: -2.0278e+00
Epoch 2/10
39/39 - 67s - loss: 751.7888 - loglik: -7.4878e+02 - logprior: -2.4409e+00
Epoch 3/10
39/39 - 67s - loss: 739.6874 - loglik: -7.3599e+02 - logprior: -2.6405e+00
Epoch 4/10
39/39 - 68s - loss: 735.9106 - loglik: -7.3212e+02 - logprior: -2.6866e+00
Epoch 5/10
39/39 - 67s - loss: 734.4584 - loglik: -7.3063e+02 - logprior: -2.7185e+00
Epoch 6/10
39/39 - 72s - loss: 732.3682 - loglik: -7.2851e+02 - logprior: -2.7540e+00
Epoch 7/10
39/39 - 74s - loss: 733.1461 - loglik: -7.2933e+02 - logprior: -2.7488e+00
Fitted a model with MAP estimate = -730.0631
expansions: [(9, 1), (19, 1), (21, 1), (34, 1), (67, 2), (101, 1), (102, 1), (104, 2), (105, 1), (106, 1), (114, 1), (120, 1), (139, 2), (140, 1), (141, 1), (142, 3), (143, 2), (144, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (167, 1), (176, 1), (177, 4), (180, 1), (181, 6), (182, 1), (184, 1), (186, 1), (188, 2), (189, 2), (190, 1), (211, 2), (212, 2), (219, 2), (220, 1), (221, 1), (226, 1), (228, 1), (233, 3), (236, 1), (239, 1), (241, 2), (242, 2), (245, 1), (250, 1), (252, 1), (255, 1), (265, 1), (275, 1), (276, 1), (277, 1), (280, 1), (281, 1), (282, 1), (296, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  0   1 191 192 193 194 195 196]
Re-initialized the encoder parameters.
Fitting a model of length 404 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 99s - loss: 736.1925 - loglik: -7.3267e+02 - logprior: -3.2206e+00
Epoch 2/2
39/39 - 95s - loss: 713.7708 - loglik: -7.1067e+02 - logprior: -2.1121e+00
Fitted a model with MAP estimate = -706.8218
expansions: [(4, 1), (5, 1), (111, 1)]
discards: [  0   1 107 160 164 208 214 217 218 239 240 241 243 254 266 302]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 88s - loss: 723.1193 - loglik: -7.1995e+02 - logprior: -2.8847e+00
Epoch 2/2
39/39 - 75s - loss: 713.5496 - loglik: -7.1103e+02 - logprior: -1.6210e+00
Fitted a model with MAP estimate = -707.8460
expansions: [(4, 1), (5, 2), (164, 1), (214, 2), (245, 2)]
discards: [  0   1 210 211 233 247 248]
Re-initialized the encoder parameters.
Fitting a model of length 392 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 81s - loss: 720.7832 - loglik: -7.1794e+02 - logprior: -2.5759e+00
Epoch 2/10
39/39 - 77s - loss: 711.8146 - loglik: -7.0996e+02 - logprior: -1.0078e+00
Epoch 3/10
39/39 - 77s - loss: 708.7177 - loglik: -7.0656e+02 - logprior: -9.3508e-01
Epoch 4/10
39/39 - 76s - loss: 706.1823 - loglik: -7.0393e+02 - logprior: -9.5132e-01
Epoch 5/10
39/39 - 73s - loss: 705.3603 - loglik: -7.0337e+02 - logprior: -7.1310e-01
Epoch 6/10
39/39 - 76s - loss: 704.7042 - loglik: -7.0287e+02 - logprior: -5.8650e-01
Epoch 7/10
39/39 - 80s - loss: 702.7433 - loglik: -7.0103e+02 - logprior: -4.7052e-01
Epoch 8/10
39/39 - 81s - loss: 704.4629 - loglik: -7.0277e+02 - logprior: -3.9634e-01
Fitted a model with MAP estimate = -700.8956
Time for alignment: 1828.9237
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 63s - loss: 896.1478 - loglik: -8.9386e+02 - logprior: -2.0290e+00
Epoch 2/10
39/39 - 60s - loss: 755.6729 - loglik: -7.5236e+02 - logprior: -2.2958e+00
Epoch 3/10
39/39 - 61s - loss: 740.2283 - loglik: -7.3631e+02 - logprior: -2.4978e+00
Epoch 4/10
39/39 - 59s - loss: 736.5272 - loglik: -7.3262e+02 - logprior: -2.5595e+00
Epoch 5/10
39/39 - 58s - loss: 734.1611 - loglik: -7.3016e+02 - logprior: -2.7200e+00
Epoch 6/10
39/39 - 57s - loss: 731.9442 - loglik: -7.2804e+02 - logprior: -2.7004e+00
Epoch 7/10
39/39 - 57s - loss: 731.3596 - loglik: -7.2743e+02 - logprior: -2.7808e+00
Epoch 8/10
39/39 - 57s - loss: 730.9202 - loglik: -7.2704e+02 - logprior: -2.7882e+00
Epoch 9/10
39/39 - 57s - loss: 730.5891 - loglik: -7.2675e+02 - logprior: -2.7747e+00
Epoch 10/10
39/39 - 58s - loss: 730.5247 - loglik: -7.2673e+02 - logprior: -2.7448e+00
Fitted a model with MAP estimate = -727.7426
expansions: [(6, 1), (20, 2), (21, 1), (64, 1), (67, 1), (98, 1), (101, 1), (105, 1), (107, 1), (108, 1), (109, 1), (115, 1), (121, 1), (140, 2), (141, 1), (142, 1), (144, 2), (145, 3), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (168, 1), (177, 1), (178, 4), (181, 7), (182, 1), (184, 1), (185, 1), (188, 2), (189, 3), (190, 1), (200, 1), (211, 2), (216, 1), (217, 1), (218, 1), (219, 1), (233, 1), (234, 3), (236, 1), (241, 2), (245, 1), (253, 1), (257, 1), (278, 1), (279, 1), (280, 1), (283, 5), (284, 1), (289, 1), (296, 1), (297, 1), (307, 1), (313, 1), (317, 1), (319, 1), (321, 1)]
discards: [  1 191 192 193 194 195]
Re-initialized the encoder parameters.
Fitting a model of length 404 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 86s - loss: 738.3895 - loglik: -7.3543e+02 - logprior: -2.6490e+00
Epoch 2/2
39/39 - 82s - loss: 713.3140 - loglik: -7.1093e+02 - logprior: -1.4167e+00
Fitted a model with MAP estimate = -707.1276
expansions: [(234, 2), (235, 1), (298, 1)]
discards: [  5 161 209 218 237 257 286 348 349]
Re-initialized the encoder parameters.
Fitting a model of length 399 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 83s - loss: 721.3394 - loglik: -7.1896e+02 - logprior: -2.0866e+00
Epoch 2/2
39/39 - 91s - loss: 712.3227 - loglik: -7.1045e+02 - logprior: -9.3450e-01
Fitted a model with MAP estimate = -706.9043
expansions: [(5, 1), (231, 1), (232, 1), (233, 1)]
discards: [215 216]
Re-initialized the encoder parameters.
Fitting a model of length 401 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 96s - loss: 718.1615 - loglik: -7.1609e+02 - logprior: -1.7908e+00
Epoch 2/10
39/39 - 98s - loss: 710.7890 - loglik: -7.0929e+02 - logprior: -6.7127e-01
Epoch 3/10
39/39 - 88s - loss: 705.9402 - loglik: -7.0400e+02 - logprior: -7.2403e-01
Epoch 4/10
39/39 - 90s - loss: 704.3578 - loglik: -7.0250e+02 - logprior: -5.6554e-01
Epoch 5/10
39/39 - 94s - loss: 703.5201 - loglik: -7.0178e+02 - logprior: -4.7549e-01
Epoch 6/10
39/39 - 90s - loss: 703.1897 - loglik: -7.0159e+02 - logprior: -3.3967e-01
Epoch 7/10
39/39 - 105s - loss: 701.7047 - loglik: -7.0028e+02 - logprior: -1.5670e-01
Epoch 8/10
39/39 - 109s - loss: 700.8521 - loglik: -6.9946e+02 - logprior: -1.0127e-01
Epoch 9/10
39/39 - 109s - loss: 700.2886 - loglik: -6.9896e+02 - logprior: -2.3013e-03
Epoch 10/10
39/39 - 103s - loss: 698.6955 - loglik: -6.9737e+02 - logprior: 0.0212
Fitted a model with MAP estimate = -696.8385
Time for alignment: 2262.6685
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 74s - loss: 892.2285 - loglik: -8.8994e+02 - logprior: -2.0337e+00
Epoch 2/10
39/39 - 72s - loss: 754.5623 - loglik: -7.5110e+02 - logprior: -2.3133e+00
Epoch 3/10
39/39 - 66s - loss: 740.8188 - loglik: -7.3688e+02 - logprior: -2.6132e+00
Epoch 4/10
39/39 - 72s - loss: 735.9998 - loglik: -7.3202e+02 - logprior: -2.7272e+00
Epoch 5/10
39/39 - 66s - loss: 734.6235 - loglik: -7.3060e+02 - logprior: -2.8112e+00
Epoch 6/10
39/39 - 62s - loss: 733.2812 - loglik: -7.2927e+02 - logprior: -2.8719e+00
Epoch 7/10
39/39 - 66s - loss: 733.5735 - loglik: -7.2961e+02 - logprior: -2.8503e+00
Fitted a model with MAP estimate = -729.9003
expansions: [(9, 1), (19, 1), (21, 1), (34, 1), (67, 1), (69, 1), (77, 1), (100, 1), (104, 1), (106, 1), (107, 1), (108, 1), (121, 1), (140, 2), (141, 1), (142, 1), (144, 3), (145, 2), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (167, 1), (176, 2), (177, 4), (180, 2), (181, 1), (184, 1), (186, 1), (187, 1), (189, 1), (190, 2), (191, 1), (192, 1), (210, 1), (218, 2), (219, 1), (220, 1), (227, 1), (233, 3), (236, 1), (239, 1), (241, 2), (242, 2), (251, 1), (252, 2), (255, 1), (265, 1), (275, 1), (276, 1), (277, 1), (281, 2), (282, 2), (288, 1), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  0   1 198 199 200 201 202 203 204 205 206 207]
Re-initialized the encoder parameters.
Fitting a model of length 393 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 91s - loss: 736.4928 - loglik: -7.3278e+02 - logprior: -3.3922e+00
Epoch 2/2
39/39 - 101s - loss: 714.5049 - loglik: -7.1141e+02 - logprior: -2.2713e+00
Fitted a model with MAP estimate = -708.5280
expansions: [(4, 1), (5, 2)]
discards: [  0   1 205 253 287 288 289 337 338]
Re-initialized the encoder parameters.
Fitting a model of length 387 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 104s - loss: 723.6373 - loglik: -7.2048e+02 - logprior: -2.8791e+00
Epoch 2/2
39/39 - 99s - loss: 714.9180 - loglik: -7.1255e+02 - logprior: -1.4296e+00
Fitted a model with MAP estimate = -709.4837
expansions: [(4, 1), (5, 2), (286, 2)]
discards: [  0   1 214]
Re-initialized the encoder parameters.
Fitting a model of length 389 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 101s - loss: 721.0855 - loglik: -7.1830e+02 - logprior: -2.5082e+00
Epoch 2/10
39/39 - 91s - loss: 712.2762 - loglik: -7.1047e+02 - logprior: -9.8234e-01
Epoch 3/10
39/39 - 96s - loss: 708.7150 - loglik: -7.0658e+02 - logprior: -9.0460e-01
Epoch 4/10
39/39 - 84s - loss: 706.4095 - loglik: -7.0431e+02 - logprior: -7.8096e-01
Epoch 5/10
39/39 - 87s - loss: 707.1417 - loglik: -7.0502e+02 - logprior: -8.3101e-01
Fitted a model with MAP estimate = -703.6459
Time for alignment: 1743.3483
Computed alignments with likelihoods: ['-700.8956', '-696.8385', '-703.6459']
Best model has likelihood: -696.8385
time for generating output: 0.5072
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00450.projection.fasta
SP score = 0.7881889763779527
Training of 3 independent models on file PF07679.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f47d42e2250>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f41607cb970>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738ebf0d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f41904580d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f498204b460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b06fec910>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f47d4246bb0>, <__main__.SimpleDirichletPrior object at 0x7f473ac67af0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 252.5576 - loglik: -2.4929e+02 - logprior: -3.1841e+00
Epoch 2/10
19/19 - 2s - loss: 220.2022 - loglik: -2.1851e+02 - logprior: -1.3877e+00
Epoch 3/10
19/19 - 2s - loss: 205.8530 - loglik: -2.0365e+02 - logprior: -1.6900e+00
Epoch 4/10
19/19 - 2s - loss: 202.9958 - loglik: -2.0087e+02 - logprior: -1.6291e+00
Epoch 5/10
19/19 - 2s - loss: 201.9745 - loglik: -1.9991e+02 - logprior: -1.6369e+00
Epoch 6/10
19/19 - 2s - loss: 201.5839 - loglik: -1.9957e+02 - logprior: -1.6084e+00
Epoch 7/10
19/19 - 2s - loss: 201.2381 - loglik: -1.9927e+02 - logprior: -1.6160e+00
Epoch 8/10
19/19 - 2s - loss: 200.7285 - loglik: -1.9878e+02 - logprior: -1.6099e+00
Epoch 9/10
19/19 - 2s - loss: 200.6995 - loglik: -1.9873e+02 - logprior: -1.6048e+00
Epoch 10/10
19/19 - 2s - loss: 200.3574 - loglik: -1.9838e+02 - logprior: -1.6042e+00
Fitted a model with MAP estimate = -199.8173
expansions: [(8, 1), (9, 3), (10, 1), (13, 1), (14, 1), (17, 1), (18, 1), (34, 1), (39, 1), (41, 2), (43, 2), (48, 1), (49, 2), (60, 1), (62, 1), (64, 2), (65, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 211.9145 - loglik: -2.0775e+02 - logprior: -4.0847e+00
Epoch 2/2
19/19 - 3s - loss: 200.1355 - loglik: -1.9740e+02 - logprior: -2.3003e+00
Fitted a model with MAP estimate = -197.7506
expansions: [(0, 2)]
discards: [ 0 10 65 84]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 200.5943 - loglik: -1.9748e+02 - logprior: -3.0394e+00
Epoch 2/2
19/19 - 3s - loss: 196.6682 - loglik: -1.9506e+02 - logprior: -1.2704e+00
Fitted a model with MAP estimate = -195.2272
expansions: []
discards: [ 0 52]
Re-initialized the encoder parameters.
Fitting a model of length 89 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 201.9132 - loglik: -1.9798e+02 - logprior: -3.8555e+00
Epoch 2/10
19/19 - 3s - loss: 197.0778 - loglik: -1.9536e+02 - logprior: -1.4044e+00
Epoch 3/10
19/19 - 3s - loss: 195.6194 - loglik: -1.9389e+02 - logprior: -1.2476e+00
Epoch 4/10
19/19 - 3s - loss: 194.6710 - loglik: -1.9291e+02 - logprior: -1.2180e+00
Epoch 5/10
19/19 - 3s - loss: 194.1821 - loglik: -1.9245e+02 - logprior: -1.1981e+00
Epoch 6/10
19/19 - 3s - loss: 193.8324 - loglik: -1.9213e+02 - logprior: -1.1789e+00
Epoch 7/10
19/19 - 3s - loss: 193.5801 - loglik: -1.9191e+02 - logprior: -1.1663e+00
Epoch 8/10
19/19 - 3s - loss: 192.9012 - loglik: -1.9124e+02 - logprior: -1.1564e+00
Epoch 9/10
19/19 - 3s - loss: 192.7562 - loglik: -1.9109e+02 - logprior: -1.1440e+00
Epoch 10/10
19/19 - 3s - loss: 192.2257 - loglik: -1.9052e+02 - logprior: -1.1451e+00
Fitted a model with MAP estimate = -191.4134
Time for alignment: 91.7240
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 252.8090 - loglik: -2.4954e+02 - logprior: -3.1814e+00
Epoch 2/10
19/19 - 2s - loss: 221.7975 - loglik: -2.2017e+02 - logprior: -1.3401e+00
Epoch 3/10
19/19 - 2s - loss: 208.6814 - loglik: -2.0655e+02 - logprior: -1.6089e+00
Epoch 4/10
19/19 - 2s - loss: 205.8530 - loglik: -2.0387e+02 - logprior: -1.5787e+00
Epoch 5/10
19/19 - 2s - loss: 205.0917 - loglik: -2.0313e+02 - logprior: -1.5713e+00
Epoch 6/10
19/19 - 2s - loss: 204.7473 - loglik: -2.0281e+02 - logprior: -1.5416e+00
Epoch 7/10
19/19 - 2s - loss: 204.2777 - loglik: -2.0236e+02 - logprior: -1.5375e+00
Epoch 8/10
19/19 - 2s - loss: 204.0736 - loglik: -2.0217e+02 - logprior: -1.5287e+00
Epoch 9/10
19/19 - 2s - loss: 203.9679 - loglik: -2.0206e+02 - logprior: -1.5255e+00
Epoch 10/10
19/19 - 2s - loss: 203.8355 - loglik: -2.0191e+02 - logprior: -1.5274e+00
Fitted a model with MAP estimate = -203.1030
expansions: [(9, 1), (10, 1), (11, 1), (13, 1), (19, 3), (20, 1), (37, 2), (39, 1), (41, 2), (43, 2), (47, 1), (48, 2), (49, 2), (60, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 212.8579 - loglik: -2.0874e+02 - logprior: -4.0314e+00
Epoch 2/2
19/19 - 3s - loss: 200.2908 - loglik: -1.9757e+02 - logprior: -2.2985e+00
Fitted a model with MAP estimate = -197.9302
expansions: [(0, 2)]
discards: [ 0 51 66]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 200.7682 - loglik: -1.9766e+02 - logprior: -3.0384e+00
Epoch 2/2
19/19 - 3s - loss: 196.6889 - loglik: -1.9507e+02 - logprior: -1.2678e+00
Fitted a model with MAP estimate = -195.0748
expansions: [(55, 1)]
discards: [ 0 46]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 202.0231 - loglik: -1.9807e+02 - logprior: -3.8806e+00
Epoch 2/10
19/19 - 3s - loss: 197.0005 - loglik: -1.9528e+02 - logprior: -1.4029e+00
Epoch 3/10
19/19 - 3s - loss: 195.6920 - loglik: -1.9394e+02 - logprior: -1.2092e+00
Epoch 4/10
19/19 - 3s - loss: 194.8294 - loglik: -1.9305e+02 - logprior: -1.2074e+00
Epoch 5/10
19/19 - 3s - loss: 193.8427 - loglik: -1.9215e+02 - logprior: -1.1832e+00
Epoch 6/10
19/19 - 3s - loss: 193.5169 - loglik: -1.9184e+02 - logprior: -1.1709e+00
Epoch 7/10
19/19 - 3s - loss: 193.2189 - loglik: -1.9156e+02 - logprior: -1.1607e+00
Epoch 8/10
19/19 - 3s - loss: 193.0190 - loglik: -1.9136e+02 - logprior: -1.1426e+00
Epoch 9/10
19/19 - 3s - loss: 192.2675 - loglik: -1.9061e+02 - logprior: -1.1340e+00
Epoch 10/10
19/19 - 3s - loss: 191.8550 - loglik: -1.9016e+02 - logprior: -1.1348e+00
Fitted a model with MAP estimate = -191.1496
Time for alignment: 91.4789
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 252.2670 - loglik: -2.4900e+02 - logprior: -3.1852e+00
Epoch 2/10
19/19 - 2s - loss: 221.2739 - loglik: -2.1962e+02 - logprior: -1.3518e+00
Epoch 3/10
19/19 - 2s - loss: 209.0798 - loglik: -2.0700e+02 - logprior: -1.5494e+00
Epoch 4/10
19/19 - 2s - loss: 205.2356 - loglik: -2.0323e+02 - logprior: -1.5414e+00
Epoch 5/10
19/19 - 2s - loss: 203.6459 - loglik: -2.0171e+02 - logprior: -1.5820e+00
Epoch 6/10
19/19 - 2s - loss: 203.2083 - loglik: -2.0129e+02 - logprior: -1.5630e+00
Epoch 7/10
19/19 - 2s - loss: 202.7222 - loglik: -2.0079e+02 - logprior: -1.5516e+00
Epoch 8/10
19/19 - 2s - loss: 202.7529 - loglik: -2.0084e+02 - logprior: -1.5471e+00
Fitted a model with MAP estimate = -201.9179
expansions: [(8, 1), (9, 3), (10, 1), (12, 2), (13, 1), (35, 1), (40, 2), (41, 2), (43, 2), (48, 1), (49, 2), (58, 1), (59, 1), (60, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 210.3666 - loglik: -2.0621e+02 - logprior: -4.0730e+00
Epoch 2/2
19/19 - 3s - loss: 200.0227 - loglik: -1.9734e+02 - logprior: -2.2664e+00
Fitted a model with MAP estimate = -197.7046
expansions: [(0, 2)]
discards: [ 0 10 49 65]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 200.7023 - loglik: -1.9759e+02 - logprior: -3.0327e+00
Epoch 2/2
19/19 - 3s - loss: 196.6502 - loglik: -1.9504e+02 - logprior: -1.2722e+00
Fitted a model with MAP estimate = -195.0990
expansions: [(54, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 202.0299 - loglik: -1.9806e+02 - logprior: -3.8938e+00
Epoch 2/10
19/19 - 3s - loss: 197.0961 - loglik: -1.9544e+02 - logprior: -1.4194e+00
Epoch 3/10
19/19 - 3s - loss: 195.4786 - loglik: -1.9400e+02 - logprior: -1.2329e+00
Epoch 4/10
19/19 - 3s - loss: 194.5863 - loglik: -1.9303e+02 - logprior: -1.2154e+00
Epoch 5/10
19/19 - 3s - loss: 194.0111 - loglik: -1.9241e+02 - logprior: -1.1829e+00
Epoch 6/10
19/19 - 3s - loss: 193.5524 - loglik: -1.9191e+02 - logprior: -1.1764e+00
Epoch 7/10
19/19 - 3s - loss: 193.1140 - loglik: -1.9148e+02 - logprior: -1.1589e+00
Epoch 8/10
19/19 - 3s - loss: 193.1508 - loglik: -1.9151e+02 - logprior: -1.1466e+00
Fitted a model with MAP estimate = -192.0452
Time for alignment: 81.5825
Computed alignments with likelihoods: ['-191.4134', '-191.1496', '-192.0452']
Best model has likelihood: -191.1496
time for generating output: 0.1485
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07679.projection.fasta
SP score = 0.7713178294573644
Training of 3 independent models on file PF07686.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4168d75130>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4179a56100>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49822cf430>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f49822cfee0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f498204b460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b06fec910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49822cf9a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b07749400>, <__main__.SimpleDirichletPrior object at 0x7f4739fbde20>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 301.6815 - loglik: -2.9842e+02 - logprior: -3.1794e+00
Epoch 2/10
19/19 - 3s - loss: 271.3381 - loglik: -2.6954e+02 - logprior: -1.3900e+00
Epoch 3/10
19/19 - 3s - loss: 256.8498 - loglik: -2.5486e+02 - logprior: -1.5251e+00
Epoch 4/10
19/19 - 3s - loss: 254.0034 - loglik: -2.5189e+02 - logprior: -1.4440e+00
Epoch 5/10
19/19 - 3s - loss: 252.5541 - loglik: -2.5041e+02 - logprior: -1.4239e+00
Epoch 6/10
19/19 - 3s - loss: 251.4538 - loglik: -2.4933e+02 - logprior: -1.3954e+00
Epoch 7/10
19/19 - 3s - loss: 251.1683 - loglik: -2.4907e+02 - logprior: -1.3949e+00
Epoch 8/10
19/19 - 3s - loss: 250.5437 - loglik: -2.4850e+02 - logprior: -1.3872e+00
Epoch 9/10
19/19 - 3s - loss: 250.0681 - loglik: -2.4808e+02 - logprior: -1.3860e+00
Epoch 10/10
19/19 - 3s - loss: 250.0887 - loglik: -2.4816e+02 - logprior: -1.3765e+00
Fitted a model with MAP estimate = -248.7604
expansions: [(0, 2), (7, 1), (20, 3), (21, 2), (22, 1), (23, 1), (38, 1), (41, 3), (42, 1), (52, 1), (53, 2), (54, 1), (74, 1), (75, 2), (76, 2), (77, 4), (78, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 113 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 268.0416 - loglik: -2.6348e+02 - logprior: -4.4711e+00
Epoch 2/2
19/19 - 4s - loss: 254.5744 - loglik: -2.5261e+02 - logprior: -1.5674e+00
Fitted a model with MAP estimate = -250.9328
expansions: [(106, 1)]
discards: [ 1 28 70 96]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 255.9563 - loglik: -2.5281e+02 - logprior: -3.0695e+00
Epoch 2/2
19/19 - 4s - loss: 252.0100 - loglik: -2.5031e+02 - logprior: -1.3260e+00
Fitted a model with MAP estimate = -249.2109
expansions: [(52, 1)]
discards: [46 53]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 254.5645 - loglik: -2.5152e+02 - logprior: -2.9628e+00
Epoch 2/10
19/19 - 4s - loss: 251.2390 - loglik: -2.4963e+02 - logprior: -1.2622e+00
Epoch 3/10
19/19 - 4s - loss: 249.3887 - loglik: -2.4754e+02 - logprior: -1.1799e+00
Epoch 4/10
19/19 - 4s - loss: 247.4998 - loglik: -2.4548e+02 - logprior: -1.2045e+00
Epoch 5/10
19/19 - 4s - loss: 246.3771 - loglik: -2.4423e+02 - logprior: -1.1905e+00
Epoch 6/10
19/19 - 4s - loss: 245.1327 - loglik: -2.4297e+02 - logprior: -1.1712e+00
Epoch 7/10
19/19 - 4s - loss: 244.1356 - loglik: -2.4203e+02 - logprior: -1.1486e+00
Epoch 8/10
19/19 - 4s - loss: 243.2595 - loglik: -2.4120e+02 - logprior: -1.1460e+00
Epoch 9/10
19/19 - 4s - loss: 243.3742 - loglik: -2.4138e+02 - logprior: -1.1210e+00
Fitted a model with MAP estimate = -241.6099
Time for alignment: 113.9565
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 301.8222 - loglik: -2.9856e+02 - logprior: -3.1711e+00
Epoch 2/10
19/19 - 3s - loss: 274.2886 - loglik: -2.7243e+02 - logprior: -1.3723e+00
Epoch 3/10
19/19 - 3s - loss: 258.9512 - loglik: -2.5679e+02 - logprior: -1.4395e+00
Epoch 4/10
19/19 - 3s - loss: 255.1391 - loglik: -2.5296e+02 - logprior: -1.3571e+00
Epoch 5/10
19/19 - 3s - loss: 253.3524 - loglik: -2.5118e+02 - logprior: -1.3431e+00
Epoch 6/10
19/19 - 3s - loss: 252.1011 - loglik: -2.4992e+02 - logprior: -1.3831e+00
Epoch 7/10
19/19 - 3s - loss: 251.0217 - loglik: -2.4885e+02 - logprior: -1.3850e+00
Epoch 8/10
19/19 - 3s - loss: 250.7346 - loglik: -2.4867e+02 - logprior: -1.3854e+00
Epoch 9/10
19/19 - 3s - loss: 250.2281 - loglik: -2.4821e+02 - logprior: -1.3852e+00
Epoch 10/10
19/19 - 3s - loss: 249.8758 - loglik: -2.4789e+02 - logprior: -1.3873e+00
Fitted a model with MAP estimate = -248.7722
expansions: [(0, 2), (17, 1), (20, 3), (21, 2), (22, 1), (39, 2), (41, 3), (42, 1), (44, 2), (53, 1), (54, 1), (74, 1), (75, 2), (76, 1), (77, 4), (78, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 268.6782 - loglik: -2.6414e+02 - logprior: -4.4505e+00
Epoch 2/2
19/19 - 4s - loss: 254.8028 - loglik: -2.5277e+02 - logprior: -1.6002e+00
Fitted a model with MAP estimate = -250.7279
expansions: [(53, 1), (105, 1)]
discards: [ 1 28 55 59 97]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 256.1213 - loglik: -2.5298e+02 - logprior: -3.0639e+00
Epoch 2/2
19/19 - 3s - loss: 251.7448 - loglik: -2.5005e+02 - logprior: -1.3070e+00
Fitted a model with MAP estimate = -249.3266
expansions: [(53, 1)]
discards: [48]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 254.4409 - loglik: -2.5140e+02 - logprior: -2.9550e+00
Epoch 2/10
19/19 - 3s - loss: 250.7674 - loglik: -2.4912e+02 - logprior: -1.2887e+00
Epoch 3/10
19/19 - 3s - loss: 249.0661 - loglik: -2.4722e+02 - logprior: -1.2200e+00
Epoch 4/10
19/19 - 3s - loss: 247.0122 - loglik: -2.4496e+02 - logprior: -1.2212e+00
Epoch 5/10
19/19 - 3s - loss: 245.9700 - loglik: -2.4384e+02 - logprior: -1.1934e+00
Epoch 6/10
19/19 - 3s - loss: 245.1874 - loglik: -2.4304e+02 - logprior: -1.1784e+00
Epoch 7/10
19/19 - 3s - loss: 243.9545 - loglik: -2.4183e+02 - logprior: -1.1677e+00
Epoch 8/10
19/19 - 3s - loss: 243.5432 - loglik: -2.4146e+02 - logprior: -1.1587e+00
Epoch 9/10
19/19 - 3s - loss: 242.2192 - loglik: -2.4018e+02 - logprior: -1.1514e+00
Epoch 10/10
19/19 - 3s - loss: 242.5915 - loglik: -2.4055e+02 - logprior: -1.1450e+00
Fitted a model with MAP estimate = -240.7991
Time for alignment: 111.4082
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 301.8903 - loglik: -2.9863e+02 - logprior: -3.1722e+00
Epoch 2/10
19/19 - 3s - loss: 272.1940 - loglik: -2.7036e+02 - logprior: -1.3727e+00
Epoch 3/10
19/19 - 3s - loss: 258.2538 - loglik: -2.5606e+02 - logprior: -1.4277e+00
Epoch 4/10
19/19 - 3s - loss: 254.7354 - loglik: -2.5259e+02 - logprior: -1.3167e+00
Epoch 5/10
19/19 - 3s - loss: 253.0784 - loglik: -2.5091e+02 - logprior: -1.3479e+00
Epoch 6/10
19/19 - 3s - loss: 251.7245 - loglik: -2.4952e+02 - logprior: -1.3947e+00
Epoch 7/10
19/19 - 3s - loss: 250.9696 - loglik: -2.4883e+02 - logprior: -1.3857e+00
Epoch 8/10
19/19 - 3s - loss: 250.5069 - loglik: -2.4842e+02 - logprior: -1.3904e+00
Epoch 9/10
19/19 - 3s - loss: 249.9673 - loglik: -2.4797e+02 - logprior: -1.3764e+00
Epoch 10/10
19/19 - 3s - loss: 250.0995 - loglik: -2.4814e+02 - logprior: -1.3749e+00
Fitted a model with MAP estimate = -248.6276
expansions: [(0, 2), (21, 3), (22, 2), (23, 1), (24, 1), (39, 2), (41, 4), (42, 1), (44, 2), (53, 2), (54, 1), (74, 1), (75, 2), (76, 1), (77, 4), (78, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 114 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 268.6281 - loglik: -2.6410e+02 - logprior: -4.4422e+00
Epoch 2/2
19/19 - 3s - loss: 254.5905 - loglik: -2.5259e+02 - logprior: -1.5946e+00
Fitted a model with MAP estimate = -250.8203
expansions: []
discards: [  1  28  60  72  99 106]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 256.1676 - loglik: -2.5304e+02 - logprior: -3.0573e+00
Epoch 2/2
19/19 - 3s - loss: 251.8931 - loglik: -2.5017e+02 - logprior: -1.3377e+00
Fitted a model with MAP estimate = -249.5027
expansions: [(52, 1), (97, 2)]
discards: [ 0 48 54]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 255.6936 - loglik: -2.5206e+02 - logprior: -3.5532e+00
Epoch 2/10
19/19 - 3s - loss: 251.0403 - loglik: -2.4941e+02 - logprior: -1.2706e+00
Epoch 3/10
19/19 - 3s - loss: 249.0130 - loglik: -2.4725e+02 - logprior: -1.1458e+00
Epoch 4/10
19/19 - 3s - loss: 247.1345 - loglik: -2.4520e+02 - logprior: -1.1137e+00
Epoch 5/10
19/19 - 3s - loss: 246.0808 - loglik: -2.4407e+02 - logprior: -1.0823e+00
Epoch 6/10
19/19 - 3s - loss: 245.4215 - loglik: -2.4341e+02 - logprior: -1.0738e+00
Epoch 7/10
19/19 - 3s - loss: 243.9131 - loglik: -2.4191e+02 - logprior: -1.0579e+00
Epoch 8/10
19/19 - 3s - loss: 243.7366 - loglik: -2.4178e+02 - logprior: -1.0411e+00
Epoch 9/10
19/19 - 3s - loss: 242.7320 - loglik: -2.4080e+02 - logprior: -1.0446e+00
Epoch 10/10
19/19 - 3s - loss: 242.0350 - loglik: -2.4012e+02 - logprior: -1.0427e+00
Fitted a model with MAP estimate = -241.0215
Time for alignment: 108.1859
Computed alignments with likelihoods: ['-241.6099', '-240.7991', '-241.0215']
Best model has likelihood: -240.7991
time for generating output: 0.1767
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07686.projection.fasta
SP score = 0.8870606483276712
Training of 3 independent models on file PF00505.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b07123730>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f497d269220>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac2ad5e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f497e11b8b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f498204b460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b06fec910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49822cf9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f0ebe20>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f477c159640>, <__main__.SimpleDirichletPrior object at 0x7f497f01e700>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 186.6893 - loglik: -1.8340e+02 - logprior: -3.2854e+00
Epoch 2/10
19/19 - 2s - loss: 154.6870 - loglik: -1.5307e+02 - logprior: -1.5326e+00
Epoch 3/10
19/19 - 2s - loss: 142.3855 - loglik: -1.4032e+02 - logprior: -1.7334e+00
Epoch 4/10
19/19 - 2s - loss: 137.0057 - loglik: -1.3501e+02 - logprior: -1.7042e+00
Epoch 5/10
19/19 - 2s - loss: 135.5066 - loglik: -1.3351e+02 - logprior: -1.7448e+00
Epoch 6/10
19/19 - 2s - loss: 134.7938 - loglik: -1.3288e+02 - logprior: -1.7235e+00
Epoch 7/10
19/19 - 2s - loss: 134.9175 - loglik: -1.3305e+02 - logprior: -1.7216e+00
Fitted a model with MAP estimate = -134.2403
expansions: [(12, 1), (14, 1), (16, 1), (17, 1), (18, 1), (21, 2), (23, 1), (30, 1), (33, 1), (35, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 139.7023 - loglik: -1.3540e+02 - logprior: -4.2329e+00
Epoch 2/2
19/19 - 2s - loss: 130.5106 - loglik: -1.2819e+02 - logprior: -2.1535e+00
Fitted a model with MAP estimate = -128.3274
expansions: [(0, 2)]
discards: [ 0 25]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 130.6264 - loglik: -1.2744e+02 - logprior: -3.1352e+00
Epoch 2/2
19/19 - 2s - loss: 127.1115 - loglik: -1.2564e+02 - logprior: -1.3105e+00
Fitted a model with MAP estimate = -125.7473
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 132.9401 - loglik: -1.2897e+02 - logprior: -3.9256e+00
Epoch 2/10
19/19 - 2s - loss: 127.9217 - loglik: -1.2624e+02 - logprior: -1.5094e+00
Epoch 3/10
19/19 - 2s - loss: 126.0627 - loglik: -1.2441e+02 - logprior: -1.3392e+00
Epoch 4/10
19/19 - 2s - loss: 125.2558 - loglik: -1.2361e+02 - logprior: -1.3207e+00
Epoch 5/10
19/19 - 2s - loss: 124.7274 - loglik: -1.2315e+02 - logprior: -1.3012e+00
Epoch 6/10
19/19 - 2s - loss: 124.4239 - loglik: -1.2289e+02 - logprior: -1.2950e+00
Epoch 7/10
19/19 - 2s - loss: 124.3848 - loglik: -1.2292e+02 - logprior: -1.2566e+00
Epoch 8/10
19/19 - 2s - loss: 124.1545 - loglik: -1.2270e+02 - logprior: -1.2558e+00
Epoch 9/10
19/19 - 2s - loss: 123.9877 - loglik: -1.2257e+02 - logprior: -1.2350e+00
Epoch 10/10
19/19 - 2s - loss: 123.8912 - loglik: -1.2249e+02 - logprior: -1.2200e+00
Fitted a model with MAP estimate = -123.7211
Time for alignment: 61.0833
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 186.5817 - loglik: -1.8328e+02 - logprior: -3.2904e+00
Epoch 2/10
19/19 - 2s - loss: 153.3932 - loglik: -1.5155e+02 - logprior: -1.7602e+00
Epoch 3/10
19/19 - 2s - loss: 140.3272 - loglik: -1.3831e+02 - logprior: -1.7347e+00
Epoch 4/10
19/19 - 2s - loss: 136.4606 - loglik: -1.3441e+02 - logprior: -1.8266e+00
Epoch 5/10
19/19 - 2s - loss: 135.3118 - loglik: -1.3334e+02 - logprior: -1.7914e+00
Epoch 6/10
19/19 - 2s - loss: 134.9777 - loglik: -1.3304e+02 - logprior: -1.7705e+00
Epoch 7/10
19/19 - 2s - loss: 134.7232 - loglik: -1.3282e+02 - logprior: -1.7461e+00
Epoch 8/10
19/19 - 2s - loss: 134.3992 - loglik: -1.3251e+02 - logprior: -1.7429e+00
Epoch 9/10
19/19 - 2s - loss: 134.6545 - loglik: -1.3279e+02 - logprior: -1.7310e+00
Fitted a model with MAP estimate = -134.1413
expansions: [(14, 1), (16, 1), (17, 1), (18, 1), (21, 1), (23, 1), (30, 1), (33, 1), (35, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 136.0240 - loglik: -1.3250e+02 - logprior: -3.4456e+00
Epoch 2/2
19/19 - 2s - loss: 128.2148 - loglik: -1.2654e+02 - logprior: -1.4911e+00
Fitted a model with MAP estimate = -126.8563
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 130.9708 - loglik: -1.2759e+02 - logprior: -3.3192e+00
Epoch 2/10
19/19 - 2s - loss: 127.6803 - loglik: -1.2607e+02 - logprior: -1.4657e+00
Epoch 3/10
19/19 - 2s - loss: 126.3265 - loglik: -1.2475e+02 - logprior: -1.3745e+00
Epoch 4/10
19/19 - 2s - loss: 125.6135 - loglik: -1.2398e+02 - logprior: -1.3449e+00
Epoch 5/10
19/19 - 2s - loss: 124.9480 - loglik: -1.2334e+02 - logprior: -1.3244e+00
Epoch 6/10
19/19 - 2s - loss: 124.6341 - loglik: -1.2307e+02 - logprior: -1.3083e+00
Epoch 7/10
19/19 - 2s - loss: 124.7898 - loglik: -1.2328e+02 - logprior: -1.2881e+00
Fitted a model with MAP estimate = -124.1932
Time for alignment: 49.8659
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 186.7249 - loglik: -1.8343e+02 - logprior: -3.2914e+00
Epoch 2/10
19/19 - 2s - loss: 154.3332 - loglik: -1.5251e+02 - logprior: -1.7547e+00
Epoch 3/10
19/19 - 2s - loss: 140.4447 - loglik: -1.3828e+02 - logprior: -1.7539e+00
Epoch 4/10
19/19 - 2s - loss: 136.6958 - loglik: -1.3457e+02 - logprior: -1.8134e+00
Epoch 5/10
19/19 - 2s - loss: 135.6648 - loglik: -1.3367e+02 - logprior: -1.7639e+00
Epoch 6/10
19/19 - 2s - loss: 135.0506 - loglik: -1.3312e+02 - logprior: -1.7719e+00
Epoch 7/10
19/19 - 2s - loss: 134.6370 - loglik: -1.3275e+02 - logprior: -1.7546e+00
Epoch 8/10
19/19 - 2s - loss: 134.9126 - loglik: -1.3303e+02 - logprior: -1.7443e+00
Fitted a model with MAP estimate = -134.3562
expansions: [(14, 1), (16, 1), (17, 1), (18, 1), (21, 1), (23, 1), (30, 1), (33, 1), (34, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 136.1163 - loglik: -1.3260e+02 - logprior: -3.4403e+00
Epoch 2/2
19/19 - 2s - loss: 128.2341 - loglik: -1.2656e+02 - logprior: -1.4953e+00
Fitted a model with MAP estimate = -126.8799
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 131.0080 - loglik: -1.2763e+02 - logprior: -3.3209e+00
Epoch 2/10
19/19 - 2s - loss: 127.7108 - loglik: -1.2608e+02 - logprior: -1.4758e+00
Epoch 3/10
19/19 - 2s - loss: 126.4719 - loglik: -1.2488e+02 - logprior: -1.3723e+00
Epoch 4/10
19/19 - 2s - loss: 125.4821 - loglik: -1.2385e+02 - logprior: -1.3454e+00
Epoch 5/10
19/19 - 2s - loss: 124.8771 - loglik: -1.2328e+02 - logprior: -1.3216e+00
Epoch 6/10
19/19 - 2s - loss: 124.8782 - loglik: -1.2332e+02 - logprior: -1.3077e+00
Fitted a model with MAP estimate = -124.3066
Time for alignment: 44.6650
Computed alignments with likelihoods: ['-123.7211', '-124.1932', '-124.3066']
Best model has likelihood: -123.7211
time for generating output: 0.1228
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00505.projection.fasta
SP score = 0.9383510769992572
Training of 3 independent models on file PF13393.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f473a07ad00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b0716ca30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4169dbeaf0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4169dbefd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f498204b460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b06fec910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49822cf9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f0ebe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47380993a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f497f947070>, <__main__.SimpleDirichletPrior object at 0x7f4161505550>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 27s - loss: 788.1709 - loglik: -7.8527e+02 - logprior: -2.7924e+00
Epoch 2/10
19/19 - 25s - loss: 675.8139 - loglik: -6.7382e+02 - logprior: -1.3860e+00
Epoch 3/10
19/19 - 26s - loss: 616.9783 - loglik: -6.1398e+02 - logprior: -2.3218e+00
Epoch 4/10
19/19 - 26s - loss: 604.0578 - loglik: -6.0063e+02 - logprior: -2.5085e+00
Epoch 5/10
19/19 - 26s - loss: 598.9582 - loglik: -5.9546e+02 - logprior: -2.5452e+00
Epoch 6/10
19/19 - 25s - loss: 596.1369 - loglik: -5.9269e+02 - logprior: -2.5181e+00
Epoch 7/10
19/19 - 25s - loss: 595.3101 - loglik: -5.9195e+02 - logprior: -2.4895e+00
Epoch 8/10
19/19 - 25s - loss: 594.8867 - loglik: -5.9156e+02 - logprior: -2.4896e+00
Epoch 9/10
19/19 - 25s - loss: 594.3306 - loglik: -5.9104e+02 - logprior: -2.4834e+00
Epoch 10/10
19/19 - 25s - loss: 594.1703 - loglik: -5.9090e+02 - logprior: -2.4886e+00
Fitted a model with MAP estimate = -591.7140
expansions: [(10, 1), (14, 1), (15, 1), (16, 1), (17, 2), (19, 1), (32, 2), (34, 1), (35, 2), (42, 1), (44, 1), (45, 1), (46, 1), (51, 1), (67, 1), (68, 1), (69, 1), (71, 1), (77, 1), (78, 1), (93, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (122, 1), (132, 1), (136, 1), (137, 3), (138, 1), (148, 1), (150, 1), (154, 3), (163, 1), (164, 1), (165, 1), (169, 1), (178, 1), (180, 1), (184, 1), (186, 1), (193, 3), (194, 1), (197, 1), (211, 1), (212, 3), (214, 3), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 43s - loss: 593.3199 - loglik: -5.8972e+02 - logprior: -3.2704e+00
Epoch 2/2
39/39 - 38s - loss: 568.0578 - loglik: -5.6453e+02 - logprior: -2.3156e+00
Fitted a model with MAP estimate = -560.1266
expansions: [(0, 3), (150, 1)]
discards: [  0  39  45 171 172 274]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 41s - loss: 575.6364 - loglik: -5.7315e+02 - logprior: -2.2105e+00
Epoch 2/2
39/39 - 38s - loss: 564.6871 - loglik: -5.6269e+02 - logprior: -1.2218e+00
Fitted a model with MAP estimate = -558.7089
expansions: []
discards: [  0   1 193]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 576.5181 - loglik: -5.7346e+02 - logprior: -2.7615e+00
Epoch 2/10
39/39 - 38s - loss: 564.9695 - loglik: -5.6271e+02 - logprior: -1.1369e+00
Epoch 3/10
39/39 - 39s - loss: 559.3538 - loglik: -5.5714e+02 - logprior: -8.5515e-01
Epoch 4/10
39/39 - 39s - loss: 557.0386 - loglik: -5.5505e+02 - logprior: -7.7147e-01
Epoch 5/10
39/39 - 39s - loss: 556.3246 - loglik: -5.5464e+02 - logprior: -6.0645e-01
Epoch 6/10
39/39 - 40s - loss: 555.7945 - loglik: -5.5432e+02 - logprior: -4.7319e-01
Epoch 7/10
39/39 - 41s - loss: 554.7192 - loglik: -5.5343e+02 - logprior: -3.6288e-01
Epoch 8/10
39/39 - 42s - loss: 554.6522 - loglik: -5.5355e+02 - logprior: -2.2614e-01
Epoch 9/10
39/39 - 41s - loss: 554.2930 - loglik: -5.5335e+02 - logprior: -8.7786e-02
Epoch 10/10
39/39 - 42s - loss: 553.2903 - loglik: -5.5252e+02 - logprior: 0.0438
Fitted a model with MAP estimate = -552.3492
Time for alignment: 976.7621
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 30s - loss: 789.0239 - loglik: -7.8612e+02 - logprior: -2.7938e+00
Epoch 2/10
19/19 - 27s - loss: 677.5163 - loglik: -6.7553e+02 - logprior: -1.3579e+00
Epoch 3/10
19/19 - 28s - loss: 622.9430 - loglik: -6.1940e+02 - logprior: -2.1818e+00
Epoch 4/10
19/19 - 28s - loss: 606.3576 - loglik: -6.0202e+02 - logprior: -2.3890e+00
Epoch 5/10
19/19 - 28s - loss: 600.4623 - loglik: -5.9646e+02 - logprior: -2.4815e+00
Epoch 6/10
19/19 - 28s - loss: 597.1967 - loglik: -5.9347e+02 - logprior: -2.5633e+00
Epoch 7/10
19/19 - 28s - loss: 594.4882 - loglik: -5.9100e+02 - logprior: -2.5944e+00
Epoch 8/10
19/19 - 29s - loss: 593.8803 - loglik: -5.9046e+02 - logprior: -2.5994e+00
Epoch 9/10
19/19 - 30s - loss: 592.9021 - loglik: -5.8956e+02 - logprior: -2.6018e+00
Epoch 10/10
19/19 - 30s - loss: 592.1566 - loglik: -5.8886e+02 - logprior: -2.6030e+00
Fitted a model with MAP estimate = -590.5451
expansions: [(10, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (33, 1), (34, 1), (37, 1), (44, 1), (45, 1), (46, 1), (51, 1), (67, 1), (68, 1), (69, 1), (70, 2), (72, 1), (76, 1), (93, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (123, 1), (132, 2), (134, 1), (135, 1), (137, 1), (138, 1), (140, 1), (146, 1), (147, 1), (154, 3), (163, 1), (164, 1), (165, 1), (169, 1), (174, 2), (175, 1), (184, 1), (185, 1), (186, 1), (192, 2), (193, 1), (194, 1), (197, 1), (211, 1), (212, 3), (214, 3), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 311 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 594.8519 - loglik: -5.9131e+02 - logprior: -3.2314e+00
Epoch 2/2
39/39 - 44s - loss: 570.6993 - loglik: -5.6728e+02 - logprior: -2.2780e+00
Fitted a model with MAP estimate = -562.9347
expansions: [(0, 3), (148, 1)]
discards: [  0  86 162 245 275]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 575.2957 - loglik: -5.7279e+02 - logprior: -2.2292e+00
Epoch 2/2
39/39 - 46s - loss: 564.8168 - loglik: -5.6279e+02 - logprior: -1.2658e+00
Fitted a model with MAP estimate = -558.8822
expansions: [(198, 4)]
discards: [  0   1 174 194]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 47s - loss: 576.1514 - loglik: -5.7308e+02 - logprior: -2.7801e+00
Epoch 2/10
39/39 - 45s - loss: 562.9536 - loglik: -5.6062e+02 - logprior: -1.2119e+00
Epoch 3/10
39/39 - 44s - loss: 557.5853 - loglik: -5.5521e+02 - logprior: -9.9104e-01
Epoch 4/10
39/39 - 44s - loss: 554.6869 - loglik: -5.5258e+02 - logprior: -8.4778e-01
Epoch 5/10
39/39 - 44s - loss: 554.5060 - loglik: -5.5269e+02 - logprior: -7.0279e-01
Epoch 6/10
39/39 - 43s - loss: 553.1796 - loglik: -5.5155e+02 - logprior: -6.0934e-01
Epoch 7/10
39/39 - 44s - loss: 552.5828 - loglik: -5.5118e+02 - logprior: -4.6231e-01
Epoch 8/10
39/39 - 45s - loss: 552.4761 - loglik: -5.5126e+02 - logprior: -3.3995e-01
Epoch 9/10
39/39 - 47s - loss: 551.7513 - loglik: -5.5071e+02 - logprior: -2.1985e-01
Epoch 10/10
39/39 - 47s - loss: 551.9824 - loglik: -5.5105e+02 - logprior: -1.4928e-01
Fitted a model with MAP estimate = -550.5231
Time for alignment: 1117.9616
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 30s - loss: 788.2224 - loglik: -7.8532e+02 - logprior: -2.7929e+00
Epoch 2/10
19/19 - 29s - loss: 676.1981 - loglik: -6.7422e+02 - logprior: -1.3585e+00
Epoch 3/10
19/19 - 30s - loss: 621.0710 - loglik: -6.1771e+02 - logprior: -2.1923e+00
Epoch 4/10
19/19 - 30s - loss: 604.4442 - loglik: -6.0042e+02 - logprior: -2.3618e+00
Epoch 5/10
19/19 - 30s - loss: 599.9819 - loglik: -5.9617e+02 - logprior: -2.3936e+00
Epoch 6/10
19/19 - 30s - loss: 595.8591 - loglik: -5.9228e+02 - logprior: -2.3393e+00
Epoch 7/10
19/19 - 30s - loss: 595.1330 - loglik: -5.9174e+02 - logprior: -2.3777e+00
Epoch 8/10
19/19 - 28s - loss: 592.9426 - loglik: -5.8962e+02 - logprior: -2.4071e+00
Epoch 9/10
19/19 - 29s - loss: 593.1823 - loglik: -5.8990e+02 - logprior: -2.4181e+00
Fitted a model with MAP estimate = -590.7849
expansions: [(10, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (32, 2), (34, 1), (35, 2), (37, 1), (41, 1), (45, 1), (46, 1), (51, 1), (67, 1), (70, 3), (76, 1), (93, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (128, 1), (132, 2), (134, 1), (135, 1), (137, 2), (148, 1), (149, 1), (154, 3), (163, 1), (164, 6), (174, 2), (175, 1), (177, 1), (180, 1), (183, 1), (185, 1), (192, 2), (193, 1), (194, 1), (197, 1), (211, 1), (212, 3), (214, 2), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 314 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 47s - loss: 594.8098 - loglik: -5.9127e+02 - logprior: -3.2364e+00
Epoch 2/2
39/39 - 44s - loss: 569.7265 - loglik: -5.6619e+02 - logprior: -2.3177e+00
Fitted a model with MAP estimate = -561.1436
expansions: [(0, 3), (149, 1), (199, 2), (209, 2)]
discards: [  0  39  45  87 163 172 194 211]
Re-initialized the encoder parameters.
Fitting a model of length 314 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 47s - loss: 576.3389 - loglik: -5.7382e+02 - logprior: -2.2280e+00
Epoch 2/2
39/39 - 46s - loss: 563.6808 - loglik: -5.6143e+02 - logprior: -1.2977e+00
Fitted a model with MAP estimate = -557.4195
expansions: [(87, 2), (175, 1), (210, 1)]
discards: [  0   1 248]
Re-initialized the encoder parameters.
Fitting a model of length 315 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 45s - loss: 575.5165 - loglik: -5.7248e+02 - logprior: -2.7535e+00
Epoch 2/10
39/39 - 40s - loss: 561.9453 - loglik: -5.5973e+02 - logprior: -1.1623e+00
Epoch 3/10
39/39 - 40s - loss: 556.0858 - loglik: -5.5375e+02 - logprior: -9.7763e-01
Epoch 4/10
39/39 - 40s - loss: 553.8273 - loglik: -5.5171e+02 - logprior: -8.3601e-01
Epoch 5/10
39/39 - 39s - loss: 552.7419 - loglik: -5.5090e+02 - logprior: -6.7305e-01
Epoch 6/10
39/39 - 39s - loss: 551.7518 - loglik: -5.5013e+02 - logprior: -5.5610e-01
Epoch 7/10
39/39 - 38s - loss: 551.6799 - loglik: -5.5029e+02 - logprior: -4.1334e-01
Epoch 8/10
39/39 - 38s - loss: 550.2746 - loglik: -5.4908e+02 - logprior: -2.9368e-01
Epoch 9/10
39/39 - 38s - loss: 550.3696 - loglik: -5.4933e+02 - logprior: -1.6929e-01
Fitted a model with MAP estimate = -548.7083
Time for alignment: 999.0543
Computed alignments with likelihoods: ['-552.3492', '-550.5231', '-548.7083']
Best model has likelihood: -548.7083
time for generating output: 0.3855
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13393.projection.fasta
SP score = 0.39988680250366226
Training of 3 independent models on file PF00202.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f47b45ad190>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f416029b700>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4168228ac0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f41690623d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f498204b460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b06fec910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49822cf9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f0ebe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47380993a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66520>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4190687cd0>, <__main__.SimpleDirichletPrior object at 0x7f4ab260eb20>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 53s - loss: 972.4252 - loglik: -9.7011e+02 - logprior: -1.8859e+00
Epoch 2/10
39/39 - 52s - loss: 806.0567 - loglik: -8.0226e+02 - logprior: -2.1305e+00
Epoch 3/10
39/39 - 52s - loss: 792.6450 - loglik: -7.8841e+02 - logprior: -2.3225e+00
Epoch 4/10
39/39 - 52s - loss: 789.3344 - loglik: -7.8524e+02 - logprior: -2.3029e+00
Epoch 5/10
39/39 - 53s - loss: 787.6578 - loglik: -7.8369e+02 - logprior: -2.3403e+00
Epoch 6/10
39/39 - 53s - loss: 786.7377 - loglik: -7.8286e+02 - logprior: -2.4336e+00
Epoch 7/10
39/39 - 54s - loss: 785.4124 - loglik: -7.8172e+02 - logprior: -2.3984e+00
Epoch 8/10
39/39 - 53s - loss: 785.2140 - loglik: -7.8160e+02 - logprior: -2.4279e+00
Epoch 9/10
39/39 - 53s - loss: 784.7286 - loglik: -7.8112e+02 - logprior: -2.5045e+00
Epoch 10/10
39/39 - 52s - loss: 784.3034 - loglik: -7.8073e+02 - logprior: -2.5211e+00
Fitted a model with MAP estimate = -774.4243
expansions: [(0, 3), (16, 1), (48, 1), (51, 1), (53, 1), (57, 2), (61, 1), (69, 1), (70, 1), (73, 1), (99, 1), (120, 2), (121, 2), (122, 1), (146, 3), (148, 1), (163, 1), (170, 1), (173, 1), (175, 1), (176, 1), (193, 2), (194, 1), (195, 1), (196, 1), (199, 1), (219, 1), (220, 1), (221, 1), (222, 3), (246, 2), (247, 1), (250, 1), (252, 1), (254, 1), (255, 1), (256, 2), (257, 1), (258, 1), (262, 1), (264, 1), (284, 7), (286, 1), (287, 1), (288, 1), (296, 1), (297, 1), (298, 3), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 393 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 69s - loss: 792.5638 - loglik: -7.8908e+02 - logprior: -3.1616e+00
Epoch 2/2
39/39 - 66s - loss: 754.7424 - loglik: -7.5196e+02 - logprior: -1.7584e+00
Fitted a model with MAP estimate = -740.8329
expansions: [(263, 2)]
discards: [  2  65 139 224 288 363 387]
Re-initialized the encoder parameters.
Fitting a model of length 388 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 70s - loss: 764.9820 - loglik: -7.6277e+02 - logprior: -1.8824e+00
Epoch 2/2
39/39 - 68s - loss: 752.9019 - loglik: -7.5084e+02 - logprior: -9.8416e-01
Fitted a model with MAP estimate = -739.8729
expansions: [(341, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 390 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 71s - loss: 755.5510 - loglik: -7.5350e+02 - logprior: -1.7502e+00
Epoch 2/10
39/39 - 68s - loss: 743.7560 - loglik: -7.4207e+02 - logprior: -7.0426e-01
Epoch 3/10
39/39 - 68s - loss: 739.7867 - loglik: -7.3776e+02 - logprior: -6.6246e-01
Epoch 4/10
39/39 - 68s - loss: 737.7518 - loglik: -7.3569e+02 - logprior: -5.3075e-01
Epoch 5/10
39/39 - 68s - loss: 736.9123 - loglik: -7.3495e+02 - logprior: -4.3606e-01
Epoch 6/10
39/39 - 68s - loss: 735.9536 - loglik: -7.3421e+02 - logprior: -2.7757e-01
Epoch 7/10
39/39 - 68s - loss: 734.9924 - loglik: -7.3345e+02 - logprior: -1.4649e-01
Epoch 8/10
39/39 - 69s - loss: 735.1948 - loglik: -7.3387e+02 - logprior: 7.0792e-04
Fitted a model with MAP estimate = -732.0519
Time for alignment: 1652.7170
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 55s - loss: 971.1584 - loglik: -9.6886e+02 - logprior: -1.8868e+00
Epoch 2/10
39/39 - 52s - loss: 804.8965 - loglik: -8.0178e+02 - logprior: -2.1923e+00
Epoch 3/10
39/39 - 53s - loss: 791.3828 - loglik: -7.8776e+02 - logprior: -2.2745e+00
Epoch 4/10
39/39 - 56s - loss: 787.7900 - loglik: -7.8412e+02 - logprior: -2.2653e+00
Epoch 5/10
39/39 - 59s - loss: 786.2712 - loglik: -7.8263e+02 - logprior: -2.2943e+00
Epoch 6/10
39/39 - 59s - loss: 785.3206 - loglik: -7.8174e+02 - logprior: -2.3085e+00
Epoch 7/10
39/39 - 60s - loss: 784.4128 - loglik: -7.8085e+02 - logprior: -2.3457e+00
Epoch 8/10
39/39 - 58s - loss: 783.7256 - loglik: -7.8018e+02 - logprior: -2.3914e+00
Epoch 9/10
39/39 - 59s - loss: 783.5021 - loglik: -7.7995e+02 - logprior: -2.4479e+00
Epoch 10/10
39/39 - 59s - loss: 783.2230 - loglik: -7.7975e+02 - logprior: -2.4241e+00
Fitted a model with MAP estimate = -773.2753
expansions: [(0, 3), (20, 1), (23, 1), (47, 1), (51, 1), (52, 1), (56, 2), (63, 1), (69, 1), (70, 1), (74, 2), (104, 1), (119, 2), (120, 1), (122, 1), (131, 1), (146, 3), (147, 2), (170, 1), (173, 1), (175, 1), (176, 1), (193, 2), (195, 1), (196, 1), (205, 1), (219, 1), (220, 1), (222, 1), (223, 1), (224, 3), (247, 1), (251, 1), (253, 1), (255, 1), (256, 1), (257, 1), (260, 1), (262, 2), (263, 1), (264, 1), (266, 2), (285, 7), (286, 3), (287, 1), (288, 1), (295, 3), (296, 1), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 396 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 94s - loss: 789.9622 - loglik: -7.8658e+02 - logprior: -3.0473e+00
Epoch 2/2
39/39 - 92s - loss: 753.7922 - loglik: -7.5088e+02 - logprior: -1.6896e+00
Fitted a model with MAP estimate = -739.1234
expansions: [(348, 2)]
discards: [ 65  87 136 170 319 390]
Re-initialized the encoder parameters.
Fitting a model of length 392 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 84s - loss: 764.4842 - loglik: -7.6210e+02 - logprior: -2.0686e+00
Epoch 2/2
39/39 - 83s - loss: 751.7014 - loglik: -7.4972e+02 - logprior: -1.0331e+00
Fitted a model with MAP estimate = -738.6560
expansions: []
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 92s - loss: 754.6461 - loglik: -7.5270e+02 - logprior: -1.6381e+00
Epoch 2/10
39/39 - 91s - loss: 744.0082 - loglik: -7.4209e+02 - logprior: -7.4920e-01
Epoch 3/10
39/39 - 85s - loss: 738.8926 - loglik: -7.3658e+02 - logprior: -6.8420e-01
Epoch 4/10
39/39 - 83s - loss: 737.6454 - loglik: -7.3540e+02 - logprior: -5.7003e-01
Epoch 5/10
39/39 - 82s - loss: 736.5667 - loglik: -7.3452e+02 - logprior: -4.4147e-01
Epoch 6/10
39/39 - 88s - loss: 735.8062 - loglik: -7.3393e+02 - logprior: -3.7274e-01
Epoch 7/10
39/39 - 77s - loss: 734.9972 - loglik: -7.3346e+02 - logprior: -1.4344e-01
Epoch 8/10
39/39 - 68s - loss: 733.4307 - loglik: -7.3205e+02 - logprior: -6.3119e-02
Epoch 9/10
39/39 - 69s - loss: 734.9119 - loglik: -7.3381e+02 - logprior: 0.1186
Fitted a model with MAP estimate = -731.3547
Time for alignment: 2095.6973
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 55s - loss: 969.7147 - loglik: -9.6741e+02 - logprior: -1.8799e+00
Epoch 2/10
39/39 - 54s - loss: 805.5309 - loglik: -8.0203e+02 - logprior: -2.0787e+00
Epoch 3/10
39/39 - 54s - loss: 791.4496 - loglik: -7.8726e+02 - logprior: -2.2175e+00
Epoch 4/10
39/39 - 53s - loss: 788.2117 - loglik: -7.8418e+02 - logprior: -2.2169e+00
Epoch 5/10
39/39 - 54s - loss: 785.8568 - loglik: -7.8191e+02 - logprior: -2.3510e+00
Epoch 6/10
39/39 - 54s - loss: 784.2870 - loglik: -7.8042e+02 - logprior: -2.4439e+00
Epoch 7/10
39/39 - 54s - loss: 783.6935 - loglik: -7.7999e+02 - logprior: -2.4069e+00
Epoch 8/10
39/39 - 53s - loss: 783.4149 - loglik: -7.7981e+02 - logprior: -2.4288e+00
Epoch 9/10
39/39 - 53s - loss: 783.0289 - loglik: -7.7953e+02 - logprior: -2.4346e+00
Epoch 10/10
39/39 - 52s - loss: 782.3873 - loglik: -7.7893e+02 - logprior: -2.4644e+00
Fitted a model with MAP estimate = -772.8196
expansions: [(0, 3), (20, 1), (44, 1), (51, 1), (53, 1), (57, 2), (64, 1), (71, 1), (74, 1), (100, 1), (121, 2), (122, 2), (123, 1), (132, 1), (146, 3), (148, 1), (163, 1), (170, 1), (174, 1), (175, 1), (176, 1), (177, 1), (192, 2), (195, 1), (196, 1), (199, 1), (219, 1), (220, 1), (222, 1), (223, 1), (224, 3), (247, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 2), (257, 1), (258, 1), (264, 1), (266, 2), (285, 7), (286, 1), (288, 1), (289, 1), (297, 4), (298, 2), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 395 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 72s - loss: 791.6332 - loglik: -7.8822e+02 - logprior: -3.0771e+00
Epoch 2/2
39/39 - 69s - loss: 754.0988 - loglik: -7.5115e+02 - logprior: -1.7147e+00
Fitted a model with MAP estimate = -739.3178
expansions: [(346, 3)]
discards: [ 65 138 139 222 318 364 389]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 73s - loss: 764.8109 - loglik: -7.6252e+02 - logprior: -1.9830e+00
Epoch 2/2
39/39 - 69s - loss: 751.9789 - loglik: -7.4996e+02 - logprior: -1.0184e+00
Fitted a model with MAP estimate = -738.8895
expansions: [(137, 1)]
discards: [  4 333 334 341]
Re-initialized the encoder parameters.
Fitting a model of length 388 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 69s - loss: 755.1942 - loglik: -7.5327e+02 - logprior: -1.6131e+00
Epoch 2/10
39/39 - 68s - loss: 744.8857 - loglik: -7.4299e+02 - logprior: -6.8376e-01
Epoch 3/10
39/39 - 72s - loss: 739.8837 - loglik: -7.3749e+02 - logprior: -6.9195e-01
Epoch 4/10
39/39 - 75s - loss: 738.8215 - loglik: -7.3646e+02 - logprior: -6.1909e-01
Epoch 5/10
39/39 - 77s - loss: 737.9349 - loglik: -7.3581e+02 - logprior: -4.9386e-01
Epoch 6/10
39/39 - 81s - loss: 735.0494 - loglik: -7.3315e+02 - logprior: -4.0224e-01
Epoch 7/10
39/39 - 85s - loss: 735.8514 - loglik: -7.3422e+02 - logprior: -2.3057e-01
Fitted a model with MAP estimate = -733.1591
Time for alignment: 1664.9496
Computed alignments with likelihoods: ['-732.0519', '-731.3547', '-733.1591']
Best model has likelihood: -731.3547
time for generating output: 1.4538
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00202.projection.fasta
SP score = 0.3548681686713525
Training of 3 independent models on file PF00018.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f473ae96be0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f414c7243a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4179d54040>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f497ca8a5b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f498204b460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b06fec910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49822cf9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f0ebe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47380993a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497e9c63a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4168e98580>, <__main__.SimpleDirichletPrior object at 0x7f4168e674c0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 133.8303 - loglik: -1.3054e+02 - logprior: -3.2852e+00
Epoch 2/10
19/19 - 1s - loss: 109.2304 - loglik: -1.0769e+02 - logprior: -1.4653e+00
Epoch 3/10
19/19 - 1s - loss: 100.7391 - loglik: -9.8910e+01 - logprior: -1.6407e+00
Epoch 4/10
19/19 - 1s - loss: 98.7302 - loglik: -9.7010e+01 - logprior: -1.5681e+00
Epoch 5/10
19/19 - 1s - loss: 98.2924 - loglik: -9.6645e+01 - logprior: -1.5259e+00
Epoch 6/10
19/19 - 1s - loss: 98.1399 - loglik: -9.6518e+01 - logprior: -1.5136e+00
Epoch 7/10
19/19 - 1s - loss: 97.8987 - loglik: -9.6295e+01 - logprior: -1.4937e+00
Epoch 8/10
19/19 - 1s - loss: 97.8787 - loglik: -9.6282e+01 - logprior: -1.4835e+00
Epoch 9/10
19/19 - 1s - loss: 97.7423 - loglik: -9.6150e+01 - logprior: -1.4771e+00
Epoch 10/10
19/19 - 1s - loss: 97.7828 - loglik: -9.6195e+01 - logprior: -1.4724e+00
Fitted a model with MAP estimate = -97.5432
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.6032 - loglik: -9.8248e+01 - logprior: -4.2819e+00
Epoch 2/2
19/19 - 1s - loss: 95.1887 - loglik: -9.2781e+01 - logprior: -2.2447e+00
Fitted a model with MAP estimate = -93.6235
expansions: [(0, 1)]
discards: [ 0  9 12 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 96.6573 - loglik: -9.3411e+01 - logprior: -3.2063e+00
Epoch 2/2
19/19 - 1s - loss: 93.4696 - loglik: -9.1812e+01 - logprior: -1.5640e+00
Fitted a model with MAP estimate = -92.7047
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 95.7866 - loglik: -9.2442e+01 - logprior: -3.3142e+00
Epoch 2/10
19/19 - 1s - loss: 93.1728 - loglik: -9.1519e+01 - logprior: -1.5574e+00
Epoch 3/10
19/19 - 1s - loss: 92.6080 - loglik: -9.0976e+01 - logprior: -1.4792e+00
Epoch 4/10
19/19 - 1s - loss: 92.3197 - loglik: -9.0717e+01 - logprior: -1.4335e+00
Epoch 5/10
19/19 - 1s - loss: 92.1073 - loglik: -9.0549e+01 - logprior: -1.4002e+00
Epoch 6/10
19/19 - 1s - loss: 92.2023 - loglik: -9.0662e+01 - logprior: -1.3884e+00
Fitted a model with MAP estimate = -91.8328
Time for alignment: 48.6288
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 133.7356 - loglik: -1.3044e+02 - logprior: -3.2843e+00
Epoch 2/10
19/19 - 1s - loss: 108.7938 - loglik: -1.0726e+02 - logprior: -1.4660e+00
Epoch 3/10
19/19 - 1s - loss: 100.7196 - loglik: -9.8886e+01 - logprior: -1.6335e+00
Epoch 4/10
19/19 - 1s - loss: 98.8197 - loglik: -9.7122e+01 - logprior: -1.5464e+00
Epoch 5/10
19/19 - 1s - loss: 98.2376 - loglik: -9.6602e+01 - logprior: -1.5116e+00
Epoch 6/10
19/19 - 1s - loss: 98.1095 - loglik: -9.6495e+01 - logprior: -1.5007e+00
Epoch 7/10
19/19 - 1s - loss: 97.9492 - loglik: -9.6351e+01 - logprior: -1.4855e+00
Epoch 8/10
19/19 - 1s - loss: 97.8459 - loglik: -9.6249e+01 - logprior: -1.4844e+00
Epoch 9/10
19/19 - 1s - loss: 97.9781 - loglik: -9.6394e+01 - logprior: -1.4764e+00
Fitted a model with MAP estimate = -97.5721
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.4769 - loglik: -9.8121e+01 - logprior: -4.2821e+00
Epoch 2/2
19/19 - 1s - loss: 95.2892 - loglik: -9.2898e+01 - logprior: -2.2342e+00
Fitted a model with MAP estimate = -93.6176
expansions: [(0, 2)]
discards: [ 0  9 12 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 95.6257 - loglik: -9.2416e+01 - logprior: -3.1699e+00
Epoch 2/2
19/19 - 1s - loss: 92.5561 - loglik: -9.1086e+01 - logprior: -1.3724e+00
Fitted a model with MAP estimate = -91.9296
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 96.9055 - loglik: -9.3215e+01 - logprior: -3.6606e+00
Epoch 2/10
19/19 - 1s - loss: 93.2946 - loglik: -9.1634e+01 - logprior: -1.5617e+00
Epoch 3/10
19/19 - 1s - loss: 92.6631 - loglik: -9.1028e+01 - logprior: -1.4798e+00
Epoch 4/10
19/19 - 1s - loss: 92.4096 - loglik: -9.0812e+01 - logprior: -1.4301e+00
Epoch 5/10
19/19 - 1s - loss: 92.1839 - loglik: -9.0622e+01 - logprior: -1.4024e+00
Epoch 6/10
19/19 - 1s - loss: 92.0293 - loglik: -9.0490e+01 - logprior: -1.3879e+00
Epoch 7/10
19/19 - 1s - loss: 92.1390 - loglik: -9.0620e+01 - logprior: -1.3725e+00
Fitted a model with MAP estimate = -91.7784
Time for alignment: 48.7392
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.7862 - loglik: -1.3049e+02 - logprior: -3.2847e+00
Epoch 2/10
19/19 - 1s - loss: 109.4580 - loglik: -1.0792e+02 - logprior: -1.4643e+00
Epoch 3/10
19/19 - 1s - loss: 100.8676 - loglik: -9.9021e+01 - logprior: -1.6494e+00
Epoch 4/10
19/19 - 1s - loss: 98.6400 - loglik: -9.6909e+01 - logprior: -1.5656e+00
Epoch 5/10
19/19 - 1s - loss: 97.9589 - loglik: -9.6297e+01 - logprior: -1.5363e+00
Epoch 6/10
19/19 - 1s - loss: 97.8202 - loglik: -9.6173e+01 - logprior: -1.5254e+00
Epoch 7/10
19/19 - 1s - loss: 97.7081 - loglik: -9.6081e+01 - logprior: -1.5098e+00
Epoch 8/10
19/19 - 1s - loss: 97.5935 - loglik: -9.5977e+01 - logprior: -1.4981e+00
Epoch 9/10
19/19 - 1s - loss: 97.5628 - loglik: -9.5947e+01 - logprior: -1.4904e+00
Epoch 10/10
19/19 - 1s - loss: 97.4547 - loglik: -9.5839e+01 - logprior: -1.4872e+00
Fitted a model with MAP estimate = -97.2568
expansions: [(7, 1), (8, 3), (9, 2), (13, 2), (18, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 103.3496 - loglik: -9.8974e+01 - logprior: -4.3062e+00
Epoch 2/2
19/19 - 1s - loss: 95.3816 - loglik: -9.2952e+01 - logprior: -2.2789e+00
Fitted a model with MAP estimate = -93.7326
expansions: [(0, 1)]
discards: [ 0  9 12 19 30 40 42]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 96.8262 - loglik: -9.3587e+01 - logprior: -3.2015e+00
Epoch 2/2
19/19 - 1s - loss: 93.5278 - loglik: -9.1871e+01 - logprior: -1.5634e+00
Fitted a model with MAP estimate = -92.7119
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 95.7811 - loglik: -9.2439e+01 - logprior: -3.3111e+00
Epoch 2/10
19/19 - 1s - loss: 93.1896 - loglik: -9.1534e+01 - logprior: -1.5559e+00
Epoch 3/10
19/19 - 1s - loss: 92.6265 - loglik: -9.0998e+01 - logprior: -1.4741e+00
Epoch 4/10
19/19 - 1s - loss: 92.3207 - loglik: -9.0722e+01 - logprior: -1.4288e+00
Epoch 5/10
19/19 - 1s - loss: 92.2178 - loglik: -9.0663e+01 - logprior: -1.3961e+00
Epoch 6/10
19/19 - 1s - loss: 92.1572 - loglik: -9.0628e+01 - logprior: -1.3772e+00
Epoch 7/10
19/19 - 1s - loss: 91.8220 - loglik: -9.0313e+01 - logprior: -1.3641e+00
Epoch 8/10
19/19 - 1s - loss: 92.0456 - loglik: -9.0549e+01 - logprior: -1.3497e+00
Fitted a model with MAP estimate = -91.7229
Time for alignment: 48.9430
Computed alignments with likelihoods: ['-91.8328', '-91.7784', '-91.7229']
Best model has likelihood: -91.7229
time for generating output: 0.1020
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00018.projection.fasta
SP score = 0.841124304824891
Training of 3 independent models on file PF00687.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4168df51f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f414c337220>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739e8ff10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f41901fa790>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f498204b460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b06fec910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49822cf9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f0ebe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47380993a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497e9c63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afeca48e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f47b43d3790>, <__main__.SimpleDirichletPrior object at 0x7f4158cd1670>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 525.6799 - loglik: -5.2268e+02 - logprior: -2.9945e+00
Epoch 2/10
19/19 - 10s - loss: 422.7438 - loglik: -4.2152e+02 - logprior: -1.2018e+00
Epoch 3/10
19/19 - 10s - loss: 377.5061 - loglik: -3.7530e+02 - logprior: -2.0013e+00
Epoch 4/10
19/19 - 11s - loss: 367.2745 - loglik: -3.6447e+02 - logprior: -2.3187e+00
Epoch 5/10
19/19 - 11s - loss: 360.8859 - loglik: -3.5825e+02 - logprior: -2.2299e+00
Epoch 6/10
19/19 - 11s - loss: 362.0530 - loglik: -3.5949e+02 - logprior: -2.1887e+00
Fitted a model with MAP estimate = -359.0694
expansions: [(4, 1), (5, 2), (9, 1), (12, 1), (13, 1), (20, 1), (22, 1), (27, 1), (34, 1), (39, 1), (40, 1), (41, 1), (42, 1), (58, 1), (59, 1), (61, 1), (63, 1), (66, 1), (74, 2), (93, 1), (97, 2), (100, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 1), (120, 1), (123, 1), (127, 1), (129, 1), (131, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 195 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 19s - loss: 354.5766 - loglik: -3.5144e+02 - logprior: -3.0536e+00
Epoch 2/2
19/19 - 16s - loss: 331.8409 - loglik: -3.3043e+02 - logprior: -1.1879e+00
Fitted a model with MAP estimate = -328.2587
expansions: []
discards: [ 93 119 170]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 20s - loss: 334.7957 - loglik: -3.3177e+02 - logprior: -2.9532e+00
Epoch 2/2
19/19 - 16s - loss: 330.1384 - loglik: -3.2890e+02 - logprior: -1.0157e+00
Fitted a model with MAP estimate = -327.5439
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 19s - loss: 333.7315 - loglik: -3.3072e+02 - logprior: -2.9368e+00
Epoch 2/10
19/19 - 16s - loss: 329.6656 - loglik: -3.2846e+02 - logprior: -9.6318e-01
Epoch 3/10
19/19 - 16s - loss: 326.7568 - loglik: -3.2552e+02 - logprior: -9.5597e-01
Epoch 4/10
19/19 - 16s - loss: 324.4599 - loglik: -3.2311e+02 - logprior: -1.0106e+00
Epoch 5/10
19/19 - 16s - loss: 322.9440 - loglik: -3.2163e+02 - logprior: -9.2765e-01
Epoch 6/10
19/19 - 16s - loss: 323.1111 - loglik: -3.2187e+02 - logprior: -8.7618e-01
Fitted a model with MAP estimate = -321.0695
Time for alignment: 304.1621
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 525.6841 - loglik: -5.2268e+02 - logprior: -2.9923e+00
Epoch 2/10
19/19 - 12s - loss: 423.5427 - loglik: -4.2232e+02 - logprior: -1.1982e+00
Epoch 3/10
19/19 - 11s - loss: 377.6369 - loglik: -3.7544e+02 - logprior: -2.0163e+00
Epoch 4/10
19/19 - 11s - loss: 364.7370 - loglik: -3.6196e+02 - logprior: -2.3192e+00
Epoch 5/10
19/19 - 11s - loss: 363.2996 - loglik: -3.6069e+02 - logprior: -2.2071e+00
Epoch 6/10
19/19 - 11s - loss: 361.7978 - loglik: -3.5925e+02 - logprior: -2.1606e+00
Epoch 7/10
19/19 - 12s - loss: 358.5071 - loglik: -3.5602e+02 - logprior: -2.1130e+00
Epoch 8/10
19/19 - 11s - loss: 358.3882 - loglik: -3.5596e+02 - logprior: -2.0928e+00
Epoch 9/10
19/19 - 11s - loss: 358.5063 - loglik: -3.5615e+02 - logprior: -2.0699e+00
Fitted a model with MAP estimate = -357.6458
expansions: [(4, 1), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (40, 2), (41, 1), (46, 1), (58, 1), (59, 1), (61, 1), (63, 1), (66, 1), (74, 1), (93, 1), (97, 2), (100, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 1), (120, 1), (127, 1), (129, 2), (130, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 193 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 18s - loss: 353.0029 - loglik: -3.4984e+02 - logprior: -3.1012e+00
Epoch 2/2
19/19 - 15s - loss: 331.4970 - loglik: -3.2995e+02 - logprior: -1.3487e+00
Fitted a model with MAP estimate = -328.3842
expansions: []
discards: [118]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 18s - loss: 335.0315 - loglik: -3.3201e+02 - logprior: -2.9545e+00
Epoch 2/2
19/19 - 16s - loss: 329.3655 - loglik: -3.2803e+02 - logprior: -1.1232e+00
Fitted a model with MAP estimate = -327.8029
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 19s - loss: 334.1223 - loglik: -3.3117e+02 - logprior: -2.8820e+00
Epoch 2/10
19/19 - 16s - loss: 329.9507 - loglik: -3.2865e+02 - logprior: -1.0702e+00
Epoch 3/10
19/19 - 16s - loss: 327.4073 - loglik: -3.2614e+02 - logprior: -9.8711e-01
Epoch 4/10
19/19 - 16s - loss: 324.4523 - loglik: -3.2314e+02 - logprior: -9.4455e-01
Epoch 5/10
19/19 - 14s - loss: 323.1493 - loglik: -3.2183e+02 - logprior: -9.1155e-01
Epoch 6/10
19/19 - 14s - loss: 323.0382 - loglik: -3.2180e+02 - logprior: -8.6222e-01
Epoch 7/10
19/19 - 14s - loss: 321.1877 - loglik: -3.2005e+02 - logprior: -7.8446e-01
Epoch 8/10
19/19 - 14s - loss: 321.8958 - loglik: -3.2083e+02 - logprior: -7.1954e-01
Fitted a model with MAP estimate = -319.3330
Time for alignment: 364.9655
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 525.6204 - loglik: -5.2263e+02 - logprior: -2.9881e+00
Epoch 2/10
19/19 - 10s - loss: 422.3456 - loglik: -4.2114e+02 - logprior: -1.1805e+00
Epoch 3/10
19/19 - 10s - loss: 377.1732 - loglik: -3.7503e+02 - logprior: -1.9129e+00
Epoch 4/10
19/19 - 10s - loss: 365.9032 - loglik: -3.6309e+02 - logprior: -2.2740e+00
Epoch 5/10
19/19 - 10s - loss: 362.7376 - loglik: -3.6014e+02 - logprior: -2.1859e+00
Epoch 6/10
19/19 - 10s - loss: 358.9436 - loglik: -3.5649e+02 - logprior: -2.1308e+00
Epoch 7/10
19/19 - 10s - loss: 361.0195 - loglik: -3.5866e+02 - logprior: -2.0660e+00
Fitted a model with MAP estimate = -358.8603
expansions: [(4, 1), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (40, 2), (41, 1), (52, 1), (58, 1), (59, 1), (61, 1), (63, 1), (66, 1), (74, 2), (98, 2), (99, 1), (100, 1), (102, 1), (103, 1), (108, 1), (110, 1), (112, 1), (116, 1), (120, 1), (123, 1), (129, 2), (130, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 194 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 18s - loss: 353.2907 - loglik: -3.5015e+02 - logprior: -3.0830e+00
Epoch 2/2
19/19 - 14s - loss: 331.7343 - loglik: -3.3020e+02 - logprior: -1.3474e+00
Fitted a model with MAP estimate = -328.2887
expansions: []
discards: [ 93 119]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 17s - loss: 335.2084 - loglik: -3.3220e+02 - logprior: -2.9531e+00
Epoch 2/2
19/19 - 14s - loss: 329.7607 - loglik: -3.2841e+02 - logprior: -1.1552e+00
Fitted a model with MAP estimate = -327.2256
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 17s - loss: 333.4320 - loglik: -3.3046e+02 - logprior: -2.8987e+00
Epoch 2/10
19/19 - 14s - loss: 328.9338 - loglik: -3.2766e+02 - logprior: -1.0778e+00
Epoch 3/10
19/19 - 14s - loss: 326.2788 - loglik: -3.2502e+02 - logprior: -1.0009e+00
Epoch 4/10
19/19 - 14s - loss: 324.5099 - loglik: -3.2318e+02 - logprior: -9.6670e-01
Epoch 5/10
19/19 - 14s - loss: 323.2221 - loglik: -3.2191e+02 - logprior: -9.1934e-01
Epoch 6/10
19/19 - 14s - loss: 322.1745 - loglik: -3.2094e+02 - logprior: -8.6326e-01
Epoch 7/10
19/19 - 14s - loss: 321.7797 - loglik: -3.2063e+02 - logprior: -8.0003e-01
Epoch 8/10
19/19 - 14s - loss: 317.8036 - loglik: -3.1673e+02 - logprior: -7.5036e-01
Epoch 9/10
19/19 - 14s - loss: 320.7782 - loglik: -3.1975e+02 - logprior: -6.7436e-01
Fitted a model with MAP estimate = -317.9333
Time for alignment: 325.8532
Computed alignments with likelihoods: ['-321.0695', '-319.3330', '-317.9333']
Best model has likelihood: -317.9333
time for generating output: 0.2183
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00687.projection.fasta
SP score = 0.7664823479370481
Training of 3 independent models on file PF03129.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f497e94ec10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4190612880>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f414c482f10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f414c337220>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f498204b460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b06fec910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49822cf9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f0ebe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47380993a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497e9c63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afeca48e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f417880a760>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f41782ec370>, <__main__.SimpleDirichletPrior object at 0x7f4169c14700>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 258.4019 - loglik: -2.5509e+02 - logprior: -3.2241e+00
Epoch 2/10
19/19 - 3s - loss: 227.0802 - loglik: -2.2516e+02 - logprior: -1.4292e+00
Epoch 3/10
19/19 - 2s - loss: 211.7401 - loglik: -2.0941e+02 - logprior: -1.6024e+00
Epoch 4/10
19/19 - 2s - loss: 207.6024 - loglik: -2.0564e+02 - logprior: -1.5698e+00
Epoch 5/10
19/19 - 3s - loss: 205.7971 - loglik: -2.0395e+02 - logprior: -1.5629e+00
Epoch 6/10
19/19 - 2s - loss: 205.4662 - loglik: -2.0369e+02 - logprior: -1.5014e+00
Epoch 7/10
19/19 - 3s - loss: 205.0559 - loglik: -2.0335e+02 - logprior: -1.4454e+00
Epoch 8/10
19/19 - 3s - loss: 204.3484 - loglik: -2.0266e+02 - logprior: -1.4372e+00
Epoch 9/10
19/19 - 2s - loss: 203.3823 - loglik: -2.0172e+02 - logprior: -1.4415e+00
Epoch 10/10
19/19 - 2s - loss: 203.3630 - loglik: -2.0175e+02 - logprior: -1.4298e+00
Fitted a model with MAP estimate = -202.6933
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (24, 1), (31, 1), (37, 2), (38, 1), (43, 1), (50, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 208.7378 - loglik: -2.0450e+02 - logprior: -4.1470e+00
Epoch 2/2
19/19 - 3s - loss: 197.0794 - loglik: -1.9459e+02 - logprior: -2.1179e+00
Fitted a model with MAP estimate = -194.0355
expansions: [(0, 2)]
discards: [ 0  7 45]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 197.4647 - loglik: -1.9439e+02 - logprior: -3.0050e+00
Epoch 2/2
19/19 - 3s - loss: 193.2636 - loglik: -1.9184e+02 - logprior: -1.1943e+00
Fitted a model with MAP estimate = -191.6495
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 198.5278 - loglik: -1.9490e+02 - logprior: -3.5681e+00
Epoch 2/10
19/19 - 3s - loss: 193.5746 - loglik: -1.9199e+02 - logprior: -1.3656e+00
Epoch 3/10
19/19 - 3s - loss: 192.2905 - loglik: -1.9061e+02 - logprior: -1.2660e+00
Epoch 4/10
19/19 - 3s - loss: 191.5223 - loglik: -1.8984e+02 - logprior: -1.2352e+00
Epoch 5/10
19/19 - 3s - loss: 191.2335 - loglik: -1.8965e+02 - logprior: -1.1996e+00
Epoch 6/10
19/19 - 3s - loss: 190.5923 - loglik: -1.8907e+02 - logprior: -1.1778e+00
Epoch 7/10
19/19 - 3s - loss: 190.9000 - loglik: -1.8943e+02 - logprior: -1.1595e+00
Fitted a model with MAP estimate = -190.2978
Time for alignment: 88.7803
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 257.9645 - loglik: -2.5466e+02 - logprior: -3.2286e+00
Epoch 2/10
19/19 - 3s - loss: 225.5650 - loglik: -2.2367e+02 - logprior: -1.4357e+00
Epoch 3/10
19/19 - 3s - loss: 211.8725 - loglik: -2.0966e+02 - logprior: -1.5588e+00
Epoch 4/10
19/19 - 2s - loss: 207.3499 - loglik: -2.0537e+02 - logprior: -1.5354e+00
Epoch 5/10
19/19 - 3s - loss: 205.8207 - loglik: -2.0381e+02 - logprior: -1.6503e+00
Epoch 6/10
19/19 - 2s - loss: 205.0743 - loglik: -2.0314e+02 - logprior: -1.6509e+00
Epoch 7/10
19/19 - 3s - loss: 204.5546 - loglik: -2.0268e+02 - logprior: -1.6376e+00
Epoch 8/10
19/19 - 2s - loss: 204.6364 - loglik: -2.0277e+02 - logprior: -1.6345e+00
Fitted a model with MAP estimate = -203.7452
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (18, 1), (31, 1), (36, 1), (37, 2), (38, 1), (50, 1), (55, 1), (57, 1), (64, 2), (68, 1), (69, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 209.1328 - loglik: -2.0487e+02 - logprior: -4.1753e+00
Epoch 2/2
19/19 - 3s - loss: 197.0600 - loglik: -1.9463e+02 - logprior: -2.1053e+00
Fitted a model with MAP estimate = -194.0369
expansions: [(0, 2)]
discards: [ 0  7 46 80]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 197.5522 - loglik: -1.9447e+02 - logprior: -3.0191e+00
Epoch 2/2
19/19 - 3s - loss: 193.2737 - loglik: -1.9184e+02 - logprior: -1.2080e+00
Fitted a model with MAP estimate = -191.6938
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 198.4926 - loglik: -1.9487e+02 - logprior: -3.5607e+00
Epoch 2/10
19/19 - 3s - loss: 193.8421 - loglik: -1.9227e+02 - logprior: -1.3612e+00
Epoch 3/10
19/19 - 3s - loss: 192.0399 - loglik: -1.9036e+02 - logprior: -1.2728e+00
Epoch 4/10
19/19 - 3s - loss: 191.7586 - loglik: -1.9006e+02 - logprior: -1.2442e+00
Epoch 5/10
19/19 - 3s - loss: 191.2213 - loglik: -1.8962e+02 - logprior: -1.2012e+00
Epoch 6/10
19/19 - 3s - loss: 190.8543 - loglik: -1.8932e+02 - logprior: -1.1866e+00
Epoch 7/10
19/19 - 3s - loss: 190.6934 - loglik: -1.8922e+02 - logprior: -1.1658e+00
Epoch 8/10
19/19 - 3s - loss: 190.7328 - loglik: -1.8930e+02 - logprior: -1.1457e+00
Fitted a model with MAP estimate = -190.1798
Time for alignment: 86.5722
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 258.0851 - loglik: -2.5478e+02 - logprior: -3.2260e+00
Epoch 2/10
19/19 - 2s - loss: 226.1378 - loglik: -2.2417e+02 - logprior: -1.4307e+00
Epoch 3/10
19/19 - 2s - loss: 211.0534 - loglik: -2.0877e+02 - logprior: -1.5726e+00
Epoch 4/10
19/19 - 2s - loss: 206.0824 - loglik: -2.0413e+02 - logprior: -1.5397e+00
Epoch 5/10
19/19 - 2s - loss: 204.4016 - loglik: -2.0266e+02 - logprior: -1.4769e+00
Epoch 6/10
19/19 - 3s - loss: 203.6657 - loglik: -2.0201e+02 - logprior: -1.4297e+00
Epoch 7/10
19/19 - 3s - loss: 203.3700 - loglik: -2.0173e+02 - logprior: -1.4135e+00
Epoch 8/10
19/19 - 2s - loss: 203.1004 - loglik: -2.0151e+02 - logprior: -1.4000e+00
Epoch 9/10
19/19 - 2s - loss: 202.9685 - loglik: -2.0139e+02 - logprior: -1.3966e+00
Epoch 10/10
19/19 - 2s - loss: 202.9127 - loglik: -2.0135e+02 - logprior: -1.3947e+00
Fitted a model with MAP estimate = -202.3210
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (24, 1), (30, 2), (37, 1), (39, 1), (48, 1), (50, 1), (55, 1), (63, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 207.5053 - loglik: -2.0330e+02 - logprior: -4.1197e+00
Epoch 2/2
19/19 - 3s - loss: 196.6437 - loglik: -1.9416e+02 - logprior: -2.0907e+00
Fitted a model with MAP estimate = -193.8362
expansions: [(0, 2)]
discards: [ 0  7 37]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 197.2865 - loglik: -1.9422e+02 - logprior: -3.0010e+00
Epoch 2/2
19/19 - 3s - loss: 193.3862 - loglik: -1.9195e+02 - logprior: -1.1923e+00
Fitted a model with MAP estimate = -191.7671
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 198.4656 - loglik: -1.9483e+02 - logprior: -3.5739e+00
Epoch 2/10
19/19 - 3s - loss: 193.6989 - loglik: -1.9208e+02 - logprior: -1.3561e+00
Epoch 3/10
19/19 - 3s - loss: 192.2391 - loglik: -1.9053e+02 - logprior: -1.2729e+00
Epoch 4/10
19/19 - 3s - loss: 191.3209 - loglik: -1.8965e+02 - logprior: -1.2291e+00
Epoch 5/10
19/19 - 3s - loss: 191.4571 - loglik: -1.8988e+02 - logprior: -1.2016e+00
Fitted a model with MAP estimate = -190.5851
Time for alignment: 81.4805
Computed alignments with likelihoods: ['-190.2978', '-190.1798', '-190.5851']
Best model has likelihood: -190.1798
time for generating output: 0.1680
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF03129.projection.fasta
SP score = 0.9744646241435138
Training of 3 independent models on file PF13378.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f41687e7280>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f497e0d4e50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4168ae1670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48000cc5e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f498204b460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b06fec910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49822cf9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f0ebe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47380993a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497e9c63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afeca48e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f417880a760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178369f10>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f497cc07430>, <__main__.SimpleDirichletPrior object at 0x7f4168507400>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 173 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 603.6781 - loglik: -6.0062e+02 - logprior: -2.9433e+00
Epoch 2/10
19/19 - 12s - loss: 529.8798 - loglik: -5.2790e+02 - logprior: -1.1886e+00
Epoch 3/10
19/19 - 12s - loss: 491.4056 - loglik: -4.8839e+02 - logprior: -1.7303e+00
Epoch 4/10
19/19 - 12s - loss: 480.2043 - loglik: -4.7697e+02 - logprior: -1.8719e+00
Epoch 5/10
19/19 - 12s - loss: 476.2865 - loglik: -4.7322e+02 - logprior: -1.8419e+00
Epoch 6/10
19/19 - 12s - loss: 474.8617 - loglik: -4.7204e+02 - logprior: -1.8038e+00
Epoch 7/10
19/19 - 12s - loss: 473.7855 - loglik: -4.7106e+02 - logprior: -1.8061e+00
Epoch 8/10
19/19 - 12s - loss: 472.8388 - loglik: -4.7008e+02 - logprior: -1.8469e+00
Epoch 9/10
19/19 - 12s - loss: 471.9675 - loglik: -4.6922e+02 - logprior: -1.8613e+00
Epoch 10/10
19/19 - 12s - loss: 471.6634 - loglik: -4.6896e+02 - logprior: -1.8742e+00
Fitted a model with MAP estimate = -468.1152
expansions: [(0, 3), (11, 1), (18, 4), (20, 1), (35, 2), (45, 2), (46, 1), (49, 1), (51, 1), (58, 1), (68, 1), (72, 1), (75, 1), (84, 1), (85, 1), (86, 3), (87, 2), (88, 2), (102, 1), (107, 1), (108, 1), (116, 1), (125, 3), (139, 2), (140, 2), (141, 4), (148, 2), (149, 1), (151, 2), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 225 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 21s - loss: 492.6734 - loglik: -4.8810e+02 - logprior: -4.4722e+00
Epoch 2/2
19/19 - 17s - loss: 461.7185 - loglik: -4.5937e+02 - logprior: -1.8553e+00
Fitted a model with MAP estimate = -453.4372
expansions: []
discards: [ 25  45  57 109 112 115 179 180 181]
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 20s - loss: 464.4944 - loglik: -4.6118e+02 - logprior: -3.2360e+00
Epoch 2/2
19/19 - 18s - loss: 456.3390 - loglik: -4.5454e+02 - logprior: -1.3484e+00
Fitted a model with MAP estimate = -450.3314
expansions: []
discards: [183]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 10089 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 22s - loss: 459.9030 - loglik: -4.5667e+02 - logprior: -3.1392e+00
Epoch 2/10
19/19 - 19s - loss: 453.7635 - loglik: -4.5204e+02 - logprior: -1.2914e+00
Epoch 3/10
19/19 - 18s - loss: 449.5670 - loglik: -4.4769e+02 - logprior: -1.2166e+00
Epoch 4/10
19/19 - 18s - loss: 448.5375 - loglik: -4.4654e+02 - logprior: -1.1896e+00
Epoch 5/10
19/19 - 18s - loss: 446.7704 - loglik: -4.4471e+02 - logprior: -1.1568e+00
Epoch 6/10
19/19 - 17s - loss: 446.8178 - loglik: -4.4477e+02 - logprior: -1.1404e+00
Fitted a model with MAP estimate = -445.1111
Time for alignment: 390.1375
Fitting a model of length 173 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 603.4565 - loglik: -6.0039e+02 - logprior: -2.9459e+00
Epoch 2/10
19/19 - 12s - loss: 528.7112 - loglik: -5.2673e+02 - logprior: -1.2049e+00
Epoch 3/10
19/19 - 12s - loss: 488.4641 - loglik: -4.8543e+02 - logprior: -1.8140e+00
Epoch 4/10
19/19 - 12s - loss: 479.1870 - loglik: -4.7592e+02 - logprior: -1.8932e+00
Epoch 5/10
19/19 - 12s - loss: 475.7314 - loglik: -4.7260e+02 - logprior: -1.8499e+00
Epoch 6/10
19/19 - 12s - loss: 475.0404 - loglik: -4.7213e+02 - logprior: -1.8177e+00
Epoch 7/10
19/19 - 12s - loss: 473.6635 - loglik: -4.7089e+02 - logprior: -1.8119e+00
Epoch 8/10
19/19 - 12s - loss: 472.6575 - loglik: -4.6987e+02 - logprior: -1.8487e+00
Epoch 9/10
19/19 - 13s - loss: 471.9392 - loglik: -4.6910e+02 - logprior: -1.8663e+00
Epoch 10/10
19/19 - 12s - loss: 471.1159 - loglik: -4.6828e+02 - logprior: -1.8790e+00
Fitted a model with MAP estimate = -467.5736
expansions: [(0, 3), (11, 1), (20, 2), (21, 1), (34, 1), (36, 1), (45, 2), (46, 1), (49, 1), (51, 1), (65, 2), (69, 1), (76, 1), (85, 1), (86, 1), (87, 3), (88, 2), (89, 2), (103, 1), (108, 1), (109, 1), (113, 1), (116, 1), (125, 3), (132, 1), (142, 5), (147, 1), (149, 2), (150, 2), (151, 2), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 224 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 21s - loss: 494.1425 - loglik: -4.8960e+02 - logprior: -4.4426e+00
Epoch 2/2
19/19 - 17s - loss: 462.3504 - loglik: -4.6008e+02 - logprior: -1.8080e+00
Fitted a model with MAP estimate = -453.9492
expansions: []
discards: [ 25  55  80 108 111 114 179 180 196]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 19s - loss: 464.9234 - loglik: -4.6163e+02 - logprior: -3.2178e+00
Epoch 2/2
19/19 - 17s - loss: 456.3441 - loglik: -4.5458e+02 - logprior: -1.3387e+00
Fitted a model with MAP estimate = -450.6251
expansions: [(22, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10089 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 459.5314 - loglik: -4.5633e+02 - logprior: -3.1103e+00
Epoch 2/10
19/19 - 17s - loss: 453.3010 - loglik: -4.5160e+02 - logprior: -1.2697e+00
Epoch 3/10
19/19 - 17s - loss: 449.9726 - loglik: -4.4807e+02 - logprior: -1.2087e+00
Epoch 4/10
19/19 - 17s - loss: 447.7663 - loglik: -4.4570e+02 - logprior: -1.1992e+00
Epoch 5/10
19/19 - 17s - loss: 446.5465 - loglik: -4.4444e+02 - logprior: -1.1589e+00
Epoch 6/10
19/19 - 17s - loss: 446.6877 - loglik: -4.4461e+02 - logprior: -1.1504e+00
Fitted a model with MAP estimate = -444.9058
Time for alignment: 386.2406
Fitting a model of length 173 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 603.6396 - loglik: -6.0060e+02 - logprior: -2.9287e+00
Epoch 2/10
19/19 - 12s - loss: 528.3680 - loglik: -5.2642e+02 - logprior: -1.1696e+00
Epoch 3/10
19/19 - 12s - loss: 488.7198 - loglik: -4.8592e+02 - logprior: -1.7971e+00
Epoch 4/10
19/19 - 12s - loss: 478.3734 - loglik: -4.7551e+02 - logprior: -1.9236e+00
Epoch 5/10
19/19 - 12s - loss: 475.2459 - loglik: -4.7232e+02 - logprior: -1.9247e+00
Epoch 6/10
19/19 - 12s - loss: 474.2431 - loglik: -4.7136e+02 - logprior: -1.9205e+00
Epoch 7/10
19/19 - 12s - loss: 472.9944 - loglik: -4.7020e+02 - logprior: -1.8997e+00
Epoch 8/10
19/19 - 13s - loss: 472.5778 - loglik: -4.6985e+02 - logprior: -1.9010e+00
Epoch 9/10
19/19 - 13s - loss: 472.2336 - loglik: -4.6954e+02 - logprior: -1.9083e+00
Epoch 10/10
19/19 - 13s - loss: 471.6910 - loglik: -4.6900e+02 - logprior: -1.9177e+00
Fitted a model with MAP estimate = -468.3865
expansions: [(0, 3), (11, 1), (18, 4), (20, 1), (36, 1), (45, 2), (46, 1), (49, 1), (51, 1), (69, 1), (71, 1), (75, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (92, 1), (102, 1), (107, 1), (108, 1), (116, 1), (125, 3), (126, 1), (138, 1), (140, 1), (141, 2), (146, 2), (148, 2), (149, 1), (150, 2), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 222 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 21s - loss: 490.7643 - loglik: -4.8623e+02 - logprior: -4.4260e+00
Epoch 2/2
19/19 - 19s - loss: 460.8609 - loglik: -4.5864e+02 - logprior: -1.7186e+00
Fitted a model with MAP estimate = -452.5462
expansions: []
discards: [ 25  56  94 113 178]
Re-initialized the encoder parameters.
Fitting a model of length 217 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 22s - loss: 463.5832 - loglik: -4.6031e+02 - logprior: -3.1985e+00
Epoch 2/2
19/19 - 19s - loss: 455.8166 - loglik: -4.5407e+02 - logprior: -1.3472e+00
Fitted a model with MAP estimate = -450.2967
expansions: []
discards: [187]
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10089 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 459.6761 - loglik: -4.5646e+02 - logprior: -3.1177e+00
Epoch 2/10
19/19 - 18s - loss: 453.7227 - loglik: -4.5195e+02 - logprior: -1.2546e+00
Epoch 3/10
19/19 - 18s - loss: 449.8936 - loglik: -4.4771e+02 - logprior: -1.1816e+00
Epoch 4/10
19/19 - 17s - loss: 447.7806 - loglik: -4.4550e+02 - logprior: -1.1692e+00
Epoch 5/10
19/19 - 18s - loss: 447.3070 - loglik: -4.4508e+02 - logprior: -1.1550e+00
Epoch 6/10
19/19 - 18s - loss: 445.6808 - loglik: -4.4357e+02 - logprior: -1.1347e+00
Epoch 7/10
19/19 - 18s - loss: 445.3676 - loglik: -4.4337e+02 - logprior: -1.0958e+00
Epoch 8/10
19/19 - 17s - loss: 445.4261 - loglik: -4.4350e+02 - logprior: -1.0748e+00
Fitted a model with MAP estimate = -444.0742
Time for alignment: 435.0223
Computed alignments with likelihoods: ['-445.1111', '-444.9058', '-444.0742']
Best model has likelihood: -444.0742
time for generating output: 0.3216
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13378.projection.fasta
SP score = 0.6990715432213641
Training of 3 independent models on file PF00084.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4738c70ca0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f477c4ee1f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b420f4ac0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b0714fc70>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f498204b460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b06fec910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49822cf9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f0ebe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47380993a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497e9c63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afeca48e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f417880a760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178369f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42a2c40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f475c065940>, <__main__.SimpleDirichletPrior object at 0x7f473afa35b0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 165.0325 - loglik: -1.6173e+02 - logprior: -3.2636e+00
Epoch 2/10
19/19 - 2s - loss: 137.6078 - loglik: -1.3612e+02 - logprior: -1.4686e+00
Epoch 3/10
19/19 - 1s - loss: 128.2890 - loglik: -1.2656e+02 - logprior: -1.6111e+00
Epoch 4/10
19/19 - 2s - loss: 126.4161 - loglik: -1.2461e+02 - logprior: -1.4865e+00
Epoch 5/10
19/19 - 2s - loss: 125.8285 - loglik: -1.2407e+02 - logprior: -1.4772e+00
Epoch 6/10
19/19 - 1s - loss: 125.6570 - loglik: -1.2393e+02 - logprior: -1.4494e+00
Epoch 7/10
19/19 - 2s - loss: 125.3347 - loglik: -1.2361e+02 - logprior: -1.4388e+00
Epoch 8/10
19/19 - 2s - loss: 125.0969 - loglik: -1.2338e+02 - logprior: -1.4321e+00
Epoch 9/10
19/19 - 1s - loss: 125.2921 - loglik: -1.2359e+02 - logprior: -1.4252e+00
Fitted a model with MAP estimate = -124.7779
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (26, 1), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 134.0266 - loglik: -1.2974e+02 - logprior: -4.2065e+00
Epoch 2/2
19/19 - 2s - loss: 126.9012 - loglik: -1.2437e+02 - logprior: -2.3465e+00
Fitted a model with MAP estimate = -124.0528
expansions: []
discards: [12 13 41 42 51]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 126.6740 - loglik: -1.2316e+02 - logprior: -3.4612e+00
Epoch 2/2
19/19 - 2s - loss: 122.9329 - loglik: -1.2125e+02 - logprior: -1.5422e+00
Fitted a model with MAP estimate = -122.2041
expansions: []
discards: [14]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 125.8772 - loglik: -1.2253e+02 - logprior: -3.3142e+00
Epoch 2/10
19/19 - 2s - loss: 122.9788 - loglik: -1.2131e+02 - logprior: -1.5271e+00
Epoch 3/10
19/19 - 1s - loss: 122.2450 - loglik: -1.2055e+02 - logprior: -1.4552e+00
Epoch 4/10
19/19 - 1s - loss: 121.9258 - loglik: -1.2021e+02 - logprior: -1.4000e+00
Epoch 5/10
19/19 - 2s - loss: 121.5771 - loglik: -1.1987e+02 - logprior: -1.3654e+00
Epoch 6/10
19/19 - 2s - loss: 121.3943 - loglik: -1.1971e+02 - logprior: -1.3512e+00
Epoch 7/10
19/19 - 2s - loss: 121.2764 - loglik: -1.1961e+02 - logprior: -1.3377e+00
Epoch 8/10
19/19 - 1s - loss: 120.8870 - loglik: -1.1925e+02 - logprior: -1.3179e+00
Epoch 9/10
19/19 - 2s - loss: 120.9798 - loglik: -1.1934e+02 - logprior: -1.3111e+00
Fitted a model with MAP estimate = -120.4764
Time for alignment: 59.7017
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 165.0371 - loglik: -1.6174e+02 - logprior: -3.2617e+00
Epoch 2/10
19/19 - 2s - loss: 137.8303 - loglik: -1.3635e+02 - logprior: -1.4676e+00
Epoch 3/10
19/19 - 2s - loss: 128.6014 - loglik: -1.2681e+02 - logprior: -1.6000e+00
Epoch 4/10
19/19 - 1s - loss: 126.7331 - loglik: -1.2493e+02 - logprior: -1.4621e+00
Epoch 5/10
19/19 - 1s - loss: 125.9519 - loglik: -1.2423e+02 - logprior: -1.4548e+00
Epoch 6/10
19/19 - 2s - loss: 125.7477 - loglik: -1.2404e+02 - logprior: -1.4334e+00
Epoch 7/10
19/19 - 2s - loss: 125.3652 - loglik: -1.2365e+02 - logprior: -1.4388e+00
Epoch 8/10
19/19 - 2s - loss: 125.3766 - loglik: -1.2369e+02 - logprior: -1.4272e+00
Fitted a model with MAP estimate = -124.8653
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (26, 1), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 133.8917 - loglik: -1.2961e+02 - logprior: -4.2088e+00
Epoch 2/2
19/19 - 1s - loss: 126.7238 - loglik: -1.2419e+02 - logprior: -2.3465e+00
Fitted a model with MAP estimate = -123.8761
expansions: []
discards: [13 14 41 42 51]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 126.5557 - loglik: -1.2304e+02 - logprior: -3.4679e+00
Epoch 2/2
19/19 - 2s - loss: 122.9725 - loglik: -1.2129e+02 - logprior: -1.5407e+00
Fitted a model with MAP estimate = -122.1838
expansions: []
discards: [14 15]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 125.9273 - loglik: -1.2258e+02 - logprior: -3.3093e+00
Epoch 2/10
19/19 - 2s - loss: 123.1014 - loglik: -1.2145e+02 - logprior: -1.5151e+00
Epoch 3/10
19/19 - 2s - loss: 122.3874 - loglik: -1.2072e+02 - logprior: -1.4347e+00
Epoch 4/10
19/19 - 1s - loss: 121.9360 - loglik: -1.2022e+02 - logprior: -1.3957e+00
Epoch 5/10
19/19 - 2s - loss: 121.6432 - loglik: -1.1995e+02 - logprior: -1.3587e+00
Epoch 6/10
19/19 - 2s - loss: 121.5363 - loglik: -1.1986e+02 - logprior: -1.3410e+00
Epoch 7/10
19/19 - 2s - loss: 121.2545 - loglik: -1.1960e+02 - logprior: -1.3293e+00
Epoch 8/10
19/19 - 2s - loss: 121.1601 - loglik: -1.1953e+02 - logprior: -1.3110e+00
Epoch 9/10
19/19 - 2s - loss: 121.0918 - loglik: -1.1947e+02 - logprior: -1.3010e+00
Epoch 10/10
19/19 - 2s - loss: 120.7794 - loglik: -1.1915e+02 - logprior: -1.2931e+00
Fitted a model with MAP estimate = -120.4911
Time for alignment: 58.0954
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 165.0758 - loglik: -1.6178e+02 - logprior: -3.2624e+00
Epoch 2/10
19/19 - 1s - loss: 136.9450 - loglik: -1.3546e+02 - logprior: -1.4673e+00
Epoch 3/10
19/19 - 2s - loss: 128.5030 - loglik: -1.2670e+02 - logprior: -1.5906e+00
Epoch 4/10
19/19 - 1s - loss: 127.2172 - loglik: -1.2549e+02 - logprior: -1.4507e+00
Epoch 5/10
19/19 - 1s - loss: 126.5200 - loglik: -1.2485e+02 - logprior: -1.4410e+00
Epoch 6/10
19/19 - 1s - loss: 126.2636 - loglik: -1.2459e+02 - logprior: -1.4265e+00
Epoch 7/10
19/19 - 2s - loss: 126.1758 - loglik: -1.2450e+02 - logprior: -1.4186e+00
Epoch 8/10
19/19 - 2s - loss: 125.8637 - loglik: -1.2418e+02 - logprior: -1.4156e+00
Epoch 9/10
19/19 - 2s - loss: 125.6824 - loglik: -1.2401e+02 - logprior: -1.4199e+00
Epoch 10/10
19/19 - 2s - loss: 125.9649 - loglik: -1.2430e+02 - logprior: -1.4095e+00
Fitted a model with MAP estimate = -125.3498
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (18, 1), (30, 1), (34, 3), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.7442 - loglik: -1.2947e+02 - logprior: -4.1977e+00
Epoch 2/2
19/19 - 2s - loss: 126.7827 - loglik: -1.2426e+02 - logprior: -2.3182e+00
Fitted a model with MAP estimate = -124.1541
expansions: []
discards: [13 14 46 50]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 126.5770 - loglik: -1.2304e+02 - logprior: -3.4802e+00
Epoch 2/2
19/19 - 1s - loss: 122.9592 - loglik: -1.2128e+02 - logprior: -1.5377e+00
Fitted a model with MAP estimate = -122.1688
expansions: []
discards: [14 15]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 125.8910 - loglik: -1.2255e+02 - logprior: -3.3035e+00
Epoch 2/10
19/19 - 2s - loss: 123.1636 - loglik: -1.2151e+02 - logprior: -1.5102e+00
Epoch 3/10
19/19 - 2s - loss: 122.3521 - loglik: -1.2068e+02 - logprior: -1.4371e+00
Epoch 4/10
19/19 - 1s - loss: 121.9767 - loglik: -1.2028e+02 - logprior: -1.3857e+00
Epoch 5/10
19/19 - 2s - loss: 121.7735 - loglik: -1.2009e+02 - logprior: -1.3551e+00
Epoch 6/10
19/19 - 2s - loss: 121.4400 - loglik: -1.1977e+02 - logprior: -1.3404e+00
Epoch 7/10
19/19 - 1s - loss: 121.2806 - loglik: -1.1964e+02 - logprior: -1.3230e+00
Epoch 8/10
19/19 - 1s - loss: 121.2677 - loglik: -1.1964e+02 - logprior: -1.3104e+00
Epoch 9/10
19/19 - 1s - loss: 120.9396 - loglik: -1.1932e+02 - logprior: -1.2960e+00
Epoch 10/10
19/19 - 1s - loss: 121.1389 - loglik: -1.1952e+02 - logprior: -1.2850e+00
Fitted a model with MAP estimate = -120.5293
Time for alignment: 59.5586
Computed alignments with likelihoods: ['-120.4764', '-120.4911', '-120.5293']
Best model has likelihood: -120.4764
time for generating output: 0.1157
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00084.projection.fasta
SP score = 0.9006024096385542
Training of 3 independent models on file PF00232.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f497e5f8430>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b0714e160>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178e8ed30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4738a242e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f498204b460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b06fec910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49822cf9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f0ebe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47380993a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497e9c63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afeca48e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f417880a760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178369f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42a2c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f416941bd90>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4178a48520>, <__main__.SimpleDirichletPrior object at 0x7f497e2c18e0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 70s - loss: 898.2224 - loglik: -8.9613e+02 - logprior: -1.9960e+00
Epoch 2/10
39/39 - 71s - loss: 719.4241 - loglik: -7.1669e+02 - logprior: -2.0327e+00
Epoch 3/10
39/39 - 77s - loss: 705.1460 - loglik: -7.0197e+02 - logprior: -2.0687e+00
Epoch 4/10
39/39 - 76s - loss: 702.2457 - loglik: -6.9916e+02 - logprior: -2.0292e+00
Epoch 5/10
39/39 - 71s - loss: 700.5861 - loglik: -6.9749e+02 - logprior: -2.1255e+00
Epoch 6/10
39/39 - 66s - loss: 699.5497 - loglik: -6.9645e+02 - logprior: -2.1964e+00
Epoch 7/10
39/39 - 64s - loss: 698.5165 - loglik: -6.9539e+02 - logprior: -2.2392e+00
Epoch 8/10
39/39 - 64s - loss: 698.4529 - loglik: -6.9533e+02 - logprior: -2.2678e+00
Epoch 9/10
39/39 - 65s - loss: 697.5403 - loglik: -6.9443e+02 - logprior: -2.2831e+00
Epoch 10/10
39/39 - 65s - loss: 698.5057 - loglik: -6.9538e+02 - logprior: -2.3022e+00
Fitted a model with MAP estimate = -695.9554
expansions: [(0, 3), (37, 1), (38, 1), (42, 1), (105, 1), (133, 1), (163, 1), (164, 1), (165, 1), (170, 1), (175, 8), (176, 1), (177, 1), (189, 1), (190, 1), (191, 6), (192, 1), (195, 1), (196, 1), (197, 1), (198, 2), (199, 1), (200, 2), (201, 2), (203, 1), (207, 1), (210, 1), (214, 2), (217, 1), (221, 3), (222, 4), (223, 2), (225, 1), (226, 3), (227, 1), (228, 1), (229, 2), (230, 1), (241, 1), (243, 1), (244, 2), (245, 3), (246, 2), (248, 2), (249, 6), (251, 1), (253, 1), (254, 1), (256, 1), (286, 1), (288, 1), (289, 1), (290, 2), (292, 1), (303, 1), (305, 4), (307, 2), (326, 1), (330, 1), (342, 1), (352, 1), (355, 6)]
discards: [  2   3 126]
Re-initialized the encoder parameters.
Fitting a model of length 461 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 101s - loss: 680.9100 - loglik: -6.7760e+02 - logprior: -3.0229e+00
Epoch 2/2
39/39 - 96s - loss: 655.8245 - loglik: -6.5328e+02 - logprior: -1.6831e+00
Fitted a model with MAP estimate = -650.2016
expansions: [(398, 1)]
discards: [  2 187 188 189 190 237 268 289 309 317 318 319 320 321 402 459]
Re-initialized the encoder parameters.
Fitting a model of length 446 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 98s - loss: 664.5217 - loglik: -6.6192e+02 - logprior: -2.3242e+00
Epoch 2/2
39/39 - 92s - loss: 656.8954 - loglik: -6.5475e+02 - logprior: -1.2660e+00
Fitted a model with MAP estimate = -652.0438
expansions: [(304, 2)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 447 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 104s - loss: 662.7515 - loglik: -6.6052e+02 - logprior: -1.9443e+00
Epoch 2/10
39/39 - 117s - loss: 655.3066 - loglik: -6.5345e+02 - logprior: -1.0023e+00
Epoch 3/10
39/39 - 125s - loss: 652.1802 - loglik: -6.5014e+02 - logprior: -8.6594e-01
Epoch 4/10
39/39 - 126s - loss: 650.9817 - loglik: -6.4890e+02 - logprior: -8.0788e-01
Epoch 5/10
39/39 - 110s - loss: 649.6143 - loglik: -6.4770e+02 - logprior: -6.6114e-01
Epoch 6/10
39/39 - 107s - loss: 648.9502 - loglik: -6.4717e+02 - logprior: -5.8064e-01
Epoch 7/10
39/39 - 117s - loss: 649.1038 - loglik: -6.4764e+02 - logprior: -2.5836e-01
Fitted a model with MAP estimate = -646.2700
Time for alignment: 2301.1619
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 69s - loss: 903.6686 - loglik: -9.0150e+02 - logprior: -2.0691e+00
Epoch 2/10
39/39 - 67s - loss: 720.5933 - loglik: -7.1787e+02 - logprior: -2.2521e+00
Epoch 3/10
39/39 - 78s - loss: 705.2903 - loglik: -7.0221e+02 - logprior: -2.2368e+00
Epoch 4/10
39/39 - 76s - loss: 702.7201 - loglik: -6.9958e+02 - logprior: -2.1611e+00
Epoch 5/10
39/39 - 65s - loss: 700.8174 - loglik: -6.9757e+02 - logprior: -2.2349e+00
Epoch 6/10
39/39 - 66s - loss: 699.6180 - loglik: -6.9633e+02 - logprior: -2.2567e+00
Epoch 7/10
39/39 - 74s - loss: 699.2422 - loglik: -6.9595e+02 - logprior: -2.2827e+00
Epoch 8/10
39/39 - 83s - loss: 698.3160 - loglik: -6.9503e+02 - logprior: -2.2884e+00
Epoch 9/10
39/39 - 86s - loss: 697.8163 - loglik: -6.9451e+02 - logprior: -2.3172e+00
Epoch 10/10
39/39 - 87s - loss: 698.6011 - loglik: -6.9529e+02 - logprior: -2.3418e+00
Fitted a model with MAP estimate = -695.8104
expansions: [(0, 3), (37, 1), (38, 1), (88, 1), (134, 1), (148, 1), (164, 1), (165, 1), (177, 1), (178, 7), (180, 1), (181, 1), (192, 2), (193, 6), (194, 1), (195, 1), (196, 1), (197, 1), (198, 3), (199, 1), (200, 1), (201, 1), (203, 1), (204, 1), (206, 1), (209, 1), (211, 1), (212, 1), (214, 1), (218, 1), (223, 1), (224, 8), (226, 1), (227, 2), (228, 2), (229, 5), (242, 1), (243, 2), (244, 2), (246, 3), (247, 2), (248, 5), (250, 1), (252, 1), (253, 1), (255, 1), (268, 1), (269, 1), (270, 1), (285, 1), (287, 1), (288, 2), (290, 1), (291, 1), (300, 1), (302, 1), (303, 1), (304, 1), (306, 1), (326, 1), (329, 1), (346, 1), (355, 5)]
discards: [  2   3 127]
Re-initialized the encoder parameters.
Fitting a model of length 458 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 129s - loss: 682.3243 - loglik: -6.7895e+02 - logprior: -3.0450e+00
Epoch 2/2
39/39 - 127s - loss: 655.7432 - loglik: -6.5292e+02 - logprior: -1.7864e+00
Fitted a model with MAP estimate = -649.7148
expansions: [(214, 1), (231, 1), (281, 1), (308, 1)]
discards: [  2   3 187 188 189 190 218 219 220 267]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 129s - loss: 664.6924 - loglik: -6.6205e+02 - logprior: -2.3517e+00
Epoch 2/2
39/39 - 107s - loss: 655.4888 - loglik: -6.5322e+02 - logprior: -1.3547e+00
Fitted a model with MAP estimate = -651.0208
expansions: [(304, 1)]
discards: [306 310 311 315 316 317 318 319]
Re-initialized the encoder parameters.
Fitting a model of length 445 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 100s - loss: 663.9005 - loglik: -6.6145e+02 - logprior: -2.1669e+00
Epoch 2/10
39/39 - 105s - loss: 655.7631 - loglik: -6.5393e+02 - logprior: -9.6465e-01
Epoch 3/10
39/39 - 117s - loss: 652.5060 - loglik: -6.5040e+02 - logprior: -9.4309e-01
Epoch 4/10
39/39 - 99s - loss: 651.5135 - loglik: -6.4930e+02 - logprior: -9.8029e-01
Epoch 5/10
39/39 - 103s - loss: 650.6462 - loglik: -6.4866e+02 - logprior: -7.6415e-01
Epoch 6/10
39/39 - 122s - loss: 649.7138 - loglik: -6.4807e+02 - logprior: -4.7389e-01
Epoch 7/10
39/39 - 126s - loss: 648.6578 - loglik: -6.4715e+02 - logprior: -3.7999e-01
Epoch 8/10
39/39 - 124s - loss: 647.7228 - loglik: -6.4633e+02 - logprior: -2.8224e-01
Epoch 9/10
39/39 - 112s - loss: 648.6849 - loglik: -6.4750e+02 - logprior: -1.0009e-01
Fitted a model with MAP estimate = -646.0262
Time for alignment: 2822.8038
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 87s - loss: 902.7683 - loglik: -9.0061e+02 - logprior: -2.0548e+00
Epoch 2/10
39/39 - 87s - loss: 717.3243 - loglik: -7.1456e+02 - logprior: -2.2812e+00
Epoch 3/10
39/39 - 87s - loss: 703.5310 - loglik: -7.0044e+02 - logprior: -2.2982e+00
Epoch 4/10
39/39 - 85s - loss: 700.4324 - loglik: -6.9732e+02 - logprior: -2.2497e+00
Epoch 5/10
39/39 - 82s - loss: 698.8718 - loglik: -6.9571e+02 - logprior: -2.2616e+00
Epoch 6/10
39/39 - 69s - loss: 697.9616 - loglik: -6.9482e+02 - logprior: -2.2843e+00
Epoch 7/10
39/39 - 65s - loss: 697.7625 - loglik: -6.9463e+02 - logprior: -2.3054e+00
Epoch 8/10
39/39 - 65s - loss: 696.2075 - loglik: -6.9307e+02 - logprior: -2.3236e+00
Epoch 9/10
39/39 - 73s - loss: 697.3988 - loglik: -6.9425e+02 - logprior: -2.3383e+00
Fitted a model with MAP estimate = -694.9228
expansions: [(0, 3), (43, 1), (45, 1), (105, 1), (133, 1), (142, 1), (161, 1), (162, 1), (163, 2), (168, 1), (172, 8), (173, 1), (174, 1), (175, 1), (183, 1), (185, 1), (186, 1), (187, 6), (188, 1), (191, 1), (192, 3), (193, 1), (195, 1), (196, 1), (198, 1), (199, 1), (202, 1), (204, 1), (206, 1), (207, 1), (214, 1), (219, 1), (220, 6), (223, 1), (224, 3), (225, 1), (226, 1), (227, 2), (228, 1), (239, 1), (243, 1), (244, 2), (245, 3), (247, 2), (248, 5), (250, 1), (252, 1), (253, 1), (266, 1), (270, 1), (271, 1), (284, 1), (286, 1), (288, 2), (291, 1), (302, 1), (304, 2), (305, 2), (307, 1), (326, 2), (329, 1), (341, 1), (355, 6)]
discards: [  1 126]
Re-initialized the encoder parameters.
Fitting a model of length 458 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 104s - loss: 677.0695 - loglik: -6.7378e+02 - logprior: -2.9828e+00
Epoch 2/2
39/39 - 113s - loss: 653.6505 - loglik: -6.5106e+02 - logprior: -1.6161e+00
Fitted a model with MAP estimate = -648.3904
expansions: [(215, 1)]
discards: [  2   3 186 187 287 319 456]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 127s - loss: 662.2970 - loglik: -6.5979e+02 - logprior: -2.2156e+00
Epoch 2/2
39/39 - 112s - loss: 653.3488 - loglik: -6.5117e+02 - logprior: -1.2399e+00
Fitted a model with MAP estimate = -649.1719
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 452 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 109s - loss: 660.0635 - loglik: -6.5765e+02 - logprior: -2.1367e+00
Epoch 2/10
39/39 - 112s - loss: 652.8305 - loglik: -6.5115e+02 - logprior: -8.4505e-01
Epoch 3/10
39/39 - 107s - loss: 649.1693 - loglik: -6.4718e+02 - logprior: -8.3737e-01
Epoch 4/10
39/39 - 110s - loss: 648.1441 - loglik: -6.4611e+02 - logprior: -7.9366e-01
Epoch 5/10
39/39 - 101s - loss: 647.7170 - loglik: -6.4600e+02 - logprior: -4.8491e-01
Epoch 6/10
39/39 - 97s - loss: 646.1974 - loglik: -6.4465e+02 - logprior: -3.5602e-01
Epoch 7/10
39/39 - 98s - loss: 645.6506 - loglik: -6.4427e+02 - logprior: -2.3573e-01
Epoch 8/10
39/39 - 100s - loss: 644.7740 - loglik: -6.4356e+02 - logprior: -1.0416e-01
Epoch 9/10
39/39 - 101s - loss: 644.6315 - loglik: -6.4360e+02 - logprior: 0.0745
Epoch 10/10
39/39 - 103s - loss: 644.2547 - loglik: -6.4334e+02 - logprior: 0.1874
Fitted a model with MAP estimate = -641.9646
Time for alignment: 2714.8802
Computed alignments with likelihoods: ['-646.2700', '-646.0262', '-641.9646']
Best model has likelihood: -641.9646
time for generating output: 0.4996
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00232.projection.fasta
SP score = 0.8610060093478745
Training of 3 independent models on file PF13522.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f473b730250>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f477c4b7940>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4179eb0f70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b2967de80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f498204b460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b06fec910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49822cf9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f0ebe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47380993a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497e9c63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afeca48e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f417880a760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178369f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42a2c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f416941bd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f78a3d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f473858e790>, <__main__.SimpleDirichletPrior object at 0x7f47de5943a0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 408.8022 - loglik: -4.0568e+02 - logprior: -3.1065e+00
Epoch 2/10
19/19 - 5s - loss: 331.1724 - loglik: -3.2990e+02 - logprior: -1.1996e+00
Epoch 3/10
19/19 - 5s - loss: 298.6871 - loglik: -2.9660e+02 - logprior: -1.4267e+00
Epoch 4/10
19/19 - 5s - loss: 293.0979 - loglik: -2.9099e+02 - logprior: -1.3806e+00
Epoch 5/10
19/19 - 5s - loss: 291.4445 - loglik: -2.8946e+02 - logprior: -1.3756e+00
Epoch 6/10
19/19 - 5s - loss: 289.9820 - loglik: -2.8800e+02 - logprior: -1.3951e+00
Epoch 7/10
19/19 - 5s - loss: 289.1392 - loglik: -2.8723e+02 - logprior: -1.3780e+00
Epoch 8/10
19/19 - 5s - loss: 288.9424 - loglik: -2.8706e+02 - logprior: -1.3717e+00
Epoch 9/10
19/19 - 5s - loss: 288.3971 - loglik: -2.8651e+02 - logprior: -1.3703e+00
Epoch 10/10
19/19 - 5s - loss: 288.8474 - loglik: -2.8699e+02 - logprior: -1.3655e+00
Fitted a model with MAP estimate = -287.5578
expansions: [(0, 6), (7, 3), (8, 2), (10, 2), (27, 1), (33, 1), (35, 1), (37, 2), (52, 1), (54, 2), (58, 1), (59, 1), (73, 1), (75, 3), (76, 1), (83, 1), (85, 1), (96, 2), (97, 2), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 159 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 293.7030 - loglik: -2.8927e+02 - logprior: -4.3519e+00
Epoch 2/2
19/19 - 7s - loss: 277.7432 - loglik: -2.7593e+02 - logprior: -1.5079e+00
Fitted a model with MAP estimate = -274.6727
expansions: [(0, 4)]
discards: [  1   2   3   4   5  21  53  73 100 101 129 148]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 283.7294 - loglik: -2.7950e+02 - logprior: -4.1550e+00
Epoch 2/2
19/19 - 7s - loss: 277.4525 - loglik: -2.7575e+02 - logprior: -1.4082e+00
Fitted a model with MAP estimate = -275.0700
expansions: [(0, 5)]
discards: [0 1 2 4]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 281.1052 - loglik: -2.7772e+02 - logprior: -3.3205e+00
Epoch 2/10
19/19 - 7s - loss: 276.0796 - loglik: -2.7454e+02 - logprior: -1.2414e+00
Epoch 3/10
19/19 - 7s - loss: 274.3596 - loglik: -2.7270e+02 - logprior: -1.0908e+00
Epoch 4/10
19/19 - 7s - loss: 272.4602 - loglik: -2.7071e+02 - logprior: -1.0304e+00
Epoch 5/10
19/19 - 7s - loss: 272.2613 - loglik: -2.7056e+02 - logprior: -9.6054e-01
Epoch 6/10
19/19 - 7s - loss: 271.7201 - loglik: -2.7010e+02 - logprior: -9.1314e-01
Epoch 7/10
19/19 - 7s - loss: 270.8468 - loglik: -2.6931e+02 - logprior: -8.7218e-01
Epoch 8/10
19/19 - 7s - loss: 270.6032 - loglik: -2.6912e+02 - logprior: -8.4310e-01
Epoch 9/10
19/19 - 7s - loss: 271.2877 - loglik: -2.6988e+02 - logprior: -8.0650e-01
Fitted a model with MAP estimate = -269.8854
Time for alignment: 194.1611
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 409.3789 - loglik: -4.0626e+02 - logprior: -3.1076e+00
Epoch 2/10
19/19 - 5s - loss: 326.5840 - loglik: -3.2532e+02 - logprior: -1.1936e+00
Epoch 3/10
19/19 - 5s - loss: 296.6706 - loglik: -2.9466e+02 - logprior: -1.4466e+00
Epoch 4/10
19/19 - 5s - loss: 291.8952 - loglik: -2.8972e+02 - logprior: -1.4148e+00
Epoch 5/10
19/19 - 5s - loss: 289.2285 - loglik: -2.8720e+02 - logprior: -1.4130e+00
Epoch 6/10
19/19 - 5s - loss: 289.0060 - loglik: -2.8699e+02 - logprior: -1.4248e+00
Epoch 7/10
19/19 - 5s - loss: 287.4822 - loglik: -2.8554e+02 - logprior: -1.3963e+00
Epoch 8/10
19/19 - 5s - loss: 286.9989 - loglik: -2.8509e+02 - logprior: -1.3896e+00
Epoch 9/10
19/19 - 5s - loss: 287.8002 - loglik: -2.8590e+02 - logprior: -1.3880e+00
Fitted a model with MAP estimate = -286.2012
expansions: [(0, 6), (7, 3), (8, 2), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (55, 2), (56, 2), (58, 2), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (97, 1), (98, 1), (100, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 159 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 292.7636 - loglik: -2.8838e+02 - logprior: -4.3103e+00
Epoch 2/2
19/19 - 7s - loss: 278.0899 - loglik: -2.7625e+02 - logprior: -1.4974e+00
Fitted a model with MAP estimate = -274.7110
expansions: [(0, 4)]
discards: [  1   2   3   4   5  21  74  75  79 101 102 148]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 283.8483 - loglik: -2.7964e+02 - logprior: -4.1393e+00
Epoch 2/2
19/19 - 7s - loss: 277.5279 - loglik: -2.7579e+02 - logprior: -1.3951e+00
Fitted a model with MAP estimate = -274.8615
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 281.0553 - loglik: -2.7768e+02 - logprior: -3.3052e+00
Epoch 2/10
19/19 - 7s - loss: 276.2454 - loglik: -2.7474e+02 - logprior: -1.1466e+00
Epoch 3/10
19/19 - 7s - loss: 274.2621 - loglik: -2.7261e+02 - logprior: -1.0065e+00
Epoch 4/10
19/19 - 7s - loss: 273.3723 - loglik: -2.7165e+02 - logprior: -9.5885e-01
Epoch 5/10
19/19 - 7s - loss: 272.3368 - loglik: -2.7069e+02 - logprior: -9.0990e-01
Epoch 6/10
19/19 - 7s - loss: 271.1814 - loglik: -2.6962e+02 - logprior: -8.7597e-01
Epoch 7/10
19/19 - 7s - loss: 271.2806 - loglik: -2.6979e+02 - logprior: -8.3754e-01
Fitted a model with MAP estimate = -270.2863
Time for alignment: 174.2312
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 409.0905 - loglik: -4.0596e+02 - logprior: -3.1143e+00
Epoch 2/10
19/19 - 5s - loss: 324.8913 - loglik: -3.2360e+02 - logprior: -1.2068e+00
Epoch 3/10
19/19 - 5s - loss: 296.9012 - loglik: -2.9481e+02 - logprior: -1.4791e+00
Epoch 4/10
19/19 - 5s - loss: 292.8398 - loglik: -2.9077e+02 - logprior: -1.4206e+00
Epoch 5/10
19/19 - 5s - loss: 291.0336 - loglik: -2.8908e+02 - logprior: -1.3671e+00
Epoch 6/10
19/19 - 5s - loss: 290.6129 - loglik: -2.8873e+02 - logprior: -1.3332e+00
Epoch 7/10
19/19 - 5s - loss: 288.9047 - loglik: -2.8702e+02 - logprior: -1.3646e+00
Epoch 8/10
19/19 - 5s - loss: 288.6568 - loglik: -2.8678e+02 - logprior: -1.3770e+00
Epoch 9/10
19/19 - 5s - loss: 287.7664 - loglik: -2.8588e+02 - logprior: -1.3817e+00
Epoch 10/10
19/19 - 5s - loss: 287.9122 - loglik: -2.8606e+02 - logprior: -1.3843e+00
Fitted a model with MAP estimate = -286.8655
expansions: [(0, 6), (7, 3), (8, 2), (10, 2), (34, 1), (35, 1), (37, 1), (53, 1), (55, 2), (59, 1), (60, 1), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (97, 1), (98, 1), (100, 1), (105, 1), (106, 1), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 156 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 292.7678 - loglik: -2.8835e+02 - logprior: -4.3368e+00
Epoch 2/2
19/19 - 7s - loss: 278.1594 - loglik: -2.7639e+02 - logprior: -1.4398e+00
Fitted a model with MAP estimate = -275.5174
expansions: [(0, 4)]
discards: [  1   2   3   4   5  21  72  99 100]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 284.0143 - loglik: -2.7979e+02 - logprior: -4.1540e+00
Epoch 2/2
19/19 - 7s - loss: 278.9460 - loglik: -2.7720e+02 - logprior: -1.4058e+00
Fitted a model with MAP estimate = -276.2024
expansions: [(0, 5)]
discards: [0 1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 281.8487 - loglik: -2.7834e+02 - logprior: -3.4342e+00
Epoch 2/10
19/19 - 7s - loss: 277.3321 - loglik: -2.7572e+02 - logprior: -1.2670e+00
Epoch 3/10
19/19 - 7s - loss: 274.5138 - loglik: -2.7281e+02 - logprior: -1.1037e+00
Epoch 4/10
19/19 - 7s - loss: 273.8611 - loglik: -2.7213e+02 - logprior: -1.0209e+00
Epoch 5/10
19/19 - 7s - loss: 272.6852 - loglik: -2.7103e+02 - logprior: -9.6308e-01
Epoch 6/10
19/19 - 7s - loss: 272.8962 - loglik: -2.7136e+02 - logprior: -8.8041e-01
Fitted a model with MAP estimate = -271.3749
Time for alignment: 171.1638
Computed alignments with likelihoods: ['-269.8854', '-270.2863', '-271.3749']
Best model has likelihood: -269.8854
time for generating output: 0.1861
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13522.projection.fasta
SP score = 0.6781144073363639
Training of 3 independent models on file PF00037.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f47b47c0bb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f414c0b0220>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f414c0b00d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f497fa8a5b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f498204b460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b06fec910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49822cf9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f0ebe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47380993a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497e9c63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afeca48e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f417880a760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178369f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42a2c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f416941bd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f78a3d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fa8adc0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f473a05fa30>, <__main__.SimpleDirichletPrior object at 0x7f47b474b6d0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 69.0601 - loglik: -6.5662e+01 - logprior: -3.3844e+00
Epoch 2/10
19/19 - 1s - loss: 52.3327 - loglik: -5.0835e+01 - logprior: -1.4878e+00
Epoch 3/10
19/19 - 1s - loss: 45.7310 - loglik: -4.4144e+01 - logprior: -1.5616e+00
Epoch 4/10
19/19 - 1s - loss: 44.1409 - loglik: -4.2407e+01 - logprior: -1.5866e+00
Epoch 5/10
19/19 - 1s - loss: 43.8120 - loglik: -4.2147e+01 - logprior: -1.5518e+00
Epoch 6/10
19/19 - 1s - loss: 43.6046 - loglik: -4.1928e+01 - logprior: -1.5428e+00
Epoch 7/10
19/19 - 1s - loss: 43.4839 - loglik: -4.1810e+01 - logprior: -1.5373e+00
Epoch 8/10
19/19 - 1s - loss: 43.4132 - loglik: -4.1739e+01 - logprior: -1.5287e+00
Epoch 9/10
19/19 - 1s - loss: 43.3851 - loglik: -4.1710e+01 - logprior: -1.5234e+00
Epoch 10/10
19/19 - 1s - loss: 43.3330 - loglik: -4.1661e+01 - logprior: -1.5192e+00
Fitted a model with MAP estimate = -43.1419
expansions: [(0, 1), (3, 1), (4, 1), (9, 1), (11, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 47.1700 - loglik: -4.2356e+01 - logprior: -4.7673e+00
Epoch 2/2
19/19 - 1s - loss: 42.0631 - loglik: -4.0564e+01 - logprior: -1.4057e+00
Fitted a model with MAP estimate = -41.0651
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 44.2475 - loglik: -4.0832e+01 - logprior: -3.3743e+00
Epoch 2/2
19/19 - 1s - loss: 41.6683 - loglik: -3.9983e+01 - logprior: -1.5865e+00
Fitted a model with MAP estimate = -41.1295
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 43.9023 - loglik: -4.0535e+01 - logprior: -3.3254e+00
Epoch 2/10
19/19 - 1s - loss: 41.6306 - loglik: -3.9980e+01 - logprior: -1.5553e+00
Epoch 3/10
19/19 - 1s - loss: 40.9936 - loglik: -3.9366e+01 - logprior: -1.4721e+00
Epoch 4/10
19/19 - 1s - loss: 40.8114 - loglik: -3.9185e+01 - logprior: -1.4284e+00
Epoch 5/10
19/19 - 1s - loss: 40.5645 - loglik: -3.8946e+01 - logprior: -1.4053e+00
Epoch 6/10
19/19 - 1s - loss: 40.4366 - loglik: -3.8831e+01 - logprior: -1.3889e+00
Epoch 7/10
19/19 - 1s - loss: 40.3810 - loglik: -3.8772e+01 - logprior: -1.3777e+00
Epoch 8/10
19/19 - 1s - loss: 40.1815 - loglik: -3.8587e+01 - logprior: -1.3695e+00
Epoch 9/10
19/19 - 1s - loss: 40.1659 - loglik: -3.8582e+01 - logprior: -1.3619e+00
Epoch 10/10
19/19 - 1s - loss: 40.1706 - loglik: -3.8590e+01 - logprior: -1.3550e+00
Fitted a model with MAP estimate = -39.8352
Time for alignment: 36.4422
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 68.8347 - loglik: -6.5437e+01 - logprior: -3.3838e+00
Epoch 2/10
19/19 - 1s - loss: 52.1858 - loglik: -5.0694e+01 - logprior: -1.4823e+00
Epoch 3/10
19/19 - 1s - loss: 46.4827 - loglik: -4.4886e+01 - logprior: -1.5385e+00
Epoch 4/10
19/19 - 1s - loss: 44.6020 - loglik: -4.2850e+01 - logprior: -1.5801e+00
Epoch 5/10
19/19 - 1s - loss: 44.2056 - loglik: -4.2512e+01 - logprior: -1.5611e+00
Epoch 6/10
19/19 - 1s - loss: 43.9504 - loglik: -4.2239e+01 - logprior: -1.5586e+00
Epoch 7/10
19/19 - 1s - loss: 43.8587 - loglik: -4.2156e+01 - logprior: -1.5458e+00
Epoch 8/10
19/19 - 1s - loss: 43.7592 - loglik: -4.2051e+01 - logprior: -1.5367e+00
Epoch 9/10
19/19 - 1s - loss: 43.7406 - loglik: -4.2036e+01 - logprior: -1.5339e+00
Epoch 10/10
19/19 - 1s - loss: 43.6605 - loglik: -4.1957e+01 - logprior: -1.5293e+00
Fitted a model with MAP estimate = -43.4700
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 47.4488 - loglik: -4.2615e+01 - logprior: -4.7920e+00
Epoch 2/2
19/19 - 1s - loss: 42.0790 - loglik: -4.0578e+01 - logprior: -1.3974e+00
Fitted a model with MAP estimate = -41.0833
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 44.2526 - loglik: -4.0856e+01 - logprior: -3.3634e+00
Epoch 2/2
19/19 - 1s - loss: 41.6146 - loglik: -3.9916e+01 - logprior: -1.5891e+00
Fitted a model with MAP estimate = -41.0877
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 43.9082 - loglik: -4.0552e+01 - logprior: -3.3241e+00
Epoch 2/10
19/19 - 1s - loss: 41.5899 - loglik: -3.9928e+01 - logprior: -1.5573e+00
Epoch 3/10
19/19 - 1s - loss: 41.0350 - loglik: -3.9407e+01 - logprior: -1.4749e+00
Epoch 4/10
19/19 - 1s - loss: 40.7203 - loglik: -3.9092e+01 - logprior: -1.4306e+00
Epoch 5/10
19/19 - 1s - loss: 40.5098 - loglik: -3.8899e+01 - logprior: -1.4066e+00
Epoch 6/10
19/19 - 1s - loss: 40.3906 - loglik: -3.8792e+01 - logprior: -1.3938e+00
Epoch 7/10
19/19 - 1s - loss: 40.2622 - loglik: -3.8670e+01 - logprior: -1.3785e+00
Epoch 8/10
19/19 - 1s - loss: 40.2027 - loglik: -3.8614e+01 - logprior: -1.3699e+00
Epoch 9/10
19/19 - 1s - loss: 40.0830 - loglik: -3.8504e+01 - logprior: -1.3616e+00
Epoch 10/10
19/19 - 1s - loss: 40.1004 - loglik: -3.8517e+01 - logprior: -1.3572e+00
Fitted a model with MAP estimate = -39.8153
Time for alignment: 35.9493
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 69.0275 - loglik: -6.5621e+01 - logprior: -3.3924e+00
Epoch 2/10
19/19 - 1s - loss: 51.9603 - loglik: -5.0481e+01 - logprior: -1.4706e+00
Epoch 3/10
19/19 - 1s - loss: 46.3232 - loglik: -4.4755e+01 - logprior: -1.5020e+00
Epoch 4/10
19/19 - 1s - loss: 44.5875 - loglik: -4.2852e+01 - logprior: -1.5787e+00
Epoch 5/10
19/19 - 1s - loss: 44.2503 - loglik: -4.2560e+01 - logprior: -1.5553e+00
Epoch 6/10
19/19 - 1s - loss: 44.0132 - loglik: -4.2309e+01 - logprior: -1.5447e+00
Epoch 7/10
19/19 - 1s - loss: 43.9505 - loglik: -4.2244e+01 - logprior: -1.5421e+00
Epoch 8/10
19/19 - 1s - loss: 43.8039 - loglik: -4.2084e+01 - logprior: -1.5382e+00
Epoch 9/10
19/19 - 1s - loss: 43.6747 - loglik: -4.1978e+01 - logprior: -1.5308e+00
Epoch 10/10
19/19 - 1s - loss: 43.7073 - loglik: -4.2000e+01 - logprior: -1.5312e+00
Fitted a model with MAP estimate = -43.4834
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 47.4214 - loglik: -4.2586e+01 - logprior: -4.7936e+00
Epoch 2/2
19/19 - 1s - loss: 42.1471 - loglik: -4.0642e+01 - logprior: -1.4009e+00
Fitted a model with MAP estimate = -41.1019
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 44.2233 - loglik: -4.0821e+01 - logprior: -3.3701e+00
Epoch 2/2
19/19 - 1s - loss: 41.6431 - loglik: -3.9947e+01 - logprior: -1.5865e+00
Fitted a model with MAP estimate = -41.0910
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 43.9252 - loglik: -4.0567e+01 - logprior: -3.3252e+00
Epoch 2/10
19/19 - 1s - loss: 41.5510 - loglik: -3.9887e+01 - logprior: -1.5591e+00
Epoch 3/10
19/19 - 1s - loss: 40.9805 - loglik: -3.9345e+01 - logprior: -1.4804e+00
Epoch 4/10
19/19 - 1s - loss: 40.7173 - loglik: -3.9087e+01 - logprior: -1.4337e+00
Epoch 5/10
19/19 - 1s - loss: 40.4985 - loglik: -3.8881e+01 - logprior: -1.4101e+00
Epoch 6/10
19/19 - 1s - loss: 40.3356 - loglik: -3.8733e+01 - logprior: -1.3961e+00
Epoch 7/10
19/19 - 1s - loss: 40.2811 - loglik: -3.8681e+01 - logprior: -1.3854e+00
Epoch 8/10
19/19 - 1s - loss: 40.1487 - loglik: -3.8558e+01 - logprior: -1.3732e+00
Epoch 9/10
19/19 - 1s - loss: 40.0818 - loglik: -3.8494e+01 - logprior: -1.3668e+00
Epoch 10/10
19/19 - 1s - loss: 40.0864 - loglik: -3.8505e+01 - logprior: -1.3582e+00
Fitted a model with MAP estimate = -39.7852
Time for alignment: 34.9382
Computed alignments with likelihoods: ['-39.8352', '-39.8153', '-39.7852']
Best model has likelihood: -39.7852
time for generating output: 0.0919
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00037.projection.fasta
SP score = 0.8900481540930979
Training of 3 independent models on file PF00224.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f414c169fd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f413de4f8b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f413dce6d00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f413de68100>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f498204b460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b06fec910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49822cf9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f0ebe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47380993a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497e9c63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afeca48e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f417880a760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178369f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42a2c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f416941bd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f78a3d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fa8adc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f414c310460>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4145a972b0>, <__main__.SimpleDirichletPrior object at 0x7f49821308e0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 735.8676 - loglik: -7.3382e+02 - logprior: -2.0148e+00
Epoch 2/10
39/39 - 33s - loss: 527.7253 - loglik: -5.2501e+02 - logprior: -2.2291e+00
Epoch 3/10
39/39 - 33s - loss: 515.7953 - loglik: -5.1304e+02 - logprior: -2.2274e+00
Epoch 4/10
39/39 - 33s - loss: 512.8871 - loglik: -5.1017e+02 - logprior: -2.2064e+00
Epoch 5/10
39/39 - 33s - loss: 511.9516 - loglik: -5.0929e+02 - logprior: -2.2073e+00
Epoch 6/10
39/39 - 34s - loss: 511.3818 - loglik: -5.0874e+02 - logprior: -2.2121e+00
Epoch 7/10
39/39 - 35s - loss: 510.7831 - loglik: -5.0815e+02 - logprior: -2.2207e+00
Epoch 8/10
39/39 - 35s - loss: 510.7146 - loglik: -5.0808e+02 - logprior: -2.2294e+00
Epoch 9/10
39/39 - 35s - loss: 510.6917 - loglik: -5.0804e+02 - logprior: -2.2473e+00
Epoch 10/10
39/39 - 35s - loss: 510.3120 - loglik: -5.0766e+02 - logprior: -2.2458e+00
Fitted a model with MAP estimate = -509.1873
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (38, 1), (40, 1), (44, 1), (45, 1), (46, 2), (48, 1), (65, 1), (69, 1), (71, 1), (73, 1), (79, 2), (80, 1), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (105, 1), (109, 1), (110, 2), (112, 2), (113, 1), (133, 1), (134, 1), (142, 1), (145, 2), (148, 2), (159, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (179, 1), (182, 1), (183, 1), (189, 1), (209, 1), (212, 1), (214, 2), (216, 2), (227, 1), (229, 3), (230, 1), (243, 1), (258, 1), (264, 1), (266, 1), (269, 1), (270, 1), (271, 1), (272, 2), (273, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 54s - loss: 482.9949 - loglik: -4.8007e+02 - logprior: -2.7370e+00
Epoch 2/2
39/39 - 52s - loss: 463.2537 - loglik: -4.6129e+02 - logprior: -1.4714e+00
Fitted a model with MAP estimate = -459.9109
expansions: []
discards: [  0   2  57  98 138 142 181 216]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 473.4619 - loglik: -4.7047e+02 - logprior: -2.8089e+00
Epoch 2/2
39/39 - 50s - loss: 464.7070 - loglik: -4.6348e+02 - logprior: -8.2403e-01
Fitted a model with MAP estimate = -460.9125
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 53s - loss: 468.3479 - loglik: -4.6623e+02 - logprior: -1.9378e+00
Epoch 2/10
39/39 - 50s - loss: 462.1877 - loglik: -4.6110e+02 - logprior: -6.9042e-01
Epoch 3/10
39/39 - 50s - loss: 459.6643 - loglik: -4.5853e+02 - logprior: -6.0876e-01
Epoch 4/10
39/39 - 48s - loss: 458.2833 - loglik: -4.5724e+02 - logprior: -4.4558e-01
Epoch 5/10
39/39 - 45s - loss: 457.5340 - loglik: -4.5656e+02 - logprior: -3.7597e-01
Epoch 6/10
39/39 - 44s - loss: 456.8738 - loglik: -4.5608e+02 - logprior: -2.1156e-01
Epoch 7/10
39/39 - 44s - loss: 456.4670 - loglik: -4.5581e+02 - logprior: -8.1136e-02
Epoch 8/10
39/39 - 45s - loss: 456.2689 - loglik: -4.5568e+02 - logprior: -1.4516e-02
Epoch 9/10
39/39 - 45s - loss: 456.1490 - loglik: -4.5586e+02 - logprior: 0.2883
Epoch 10/10
39/39 - 45s - loss: 455.8745 - loglik: -4.5557e+02 - logprior: 0.2595
Fitted a model with MAP estimate = -454.7256
Time for alignment: 1253.9264
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 735.5104 - loglik: -7.3351e+02 - logprior: -1.9850e+00
Epoch 2/10
39/39 - 34s - loss: 526.8931 - loglik: -5.2421e+02 - logprior: -2.2518e+00
Epoch 3/10
39/39 - 34s - loss: 515.0760 - loglik: -5.1232e+02 - logprior: -2.2611e+00
Epoch 4/10
39/39 - 34s - loss: 512.4212 - loglik: -5.0969e+02 - logprior: -2.2523e+00
Epoch 5/10
39/39 - 34s - loss: 510.9765 - loglik: -5.0827e+02 - logprior: -2.2555e+00
Epoch 6/10
39/39 - 34s - loss: 510.4786 - loglik: -5.0776e+02 - logprior: -2.2878e+00
Epoch 7/10
39/39 - 34s - loss: 510.4127 - loglik: -5.0769e+02 - logprior: -2.3058e+00
Epoch 8/10
39/39 - 34s - loss: 509.1986 - loglik: -5.0649e+02 - logprior: -2.3090e+00
Epoch 9/10
39/39 - 34s - loss: 509.7513 - loglik: -5.0703e+02 - logprior: -2.3164e+00
Fitted a model with MAP estimate = -508.1506
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (38, 1), (39, 1), (44, 1), (45, 1), (46, 2), (48, 1), (65, 1), (69, 1), (71, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (96, 1), (109, 1), (110, 1), (111, 1), (113, 1), (120, 1), (132, 1), (133, 1), (145, 2), (148, 1), (149, 1), (159, 1), (163, 1), (167, 1), (170, 1), (171, 3), (172, 1), (181, 1), (182, 1), (188, 1), (189, 1), (209, 1), (212, 1), (214, 1), (215, 1), (216, 1), (217, 1), (227, 1), (229, 2), (231, 1), (244, 1), (257, 1), (258, 1), (264, 1), (267, 1), (269, 2), (271, 1), (272, 2), (273, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 50s - loss: 484.0693 - loglik: -4.8108e+02 - logprior: -2.7930e+00
Epoch 2/2
39/39 - 47s - loss: 463.9658 - loglik: -4.6179e+02 - logprior: -1.6884e+00
Fitted a model with MAP estimate = -460.4938
expansions: []
discards: [  0   2  57  98 179 214 346]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 50s - loss: 473.5589 - loglik: -4.7061e+02 - logprior: -2.7651e+00
Epoch 2/2
39/39 - 46s - loss: 465.1204 - loglik: -4.6376e+02 - logprior: -9.5536e-01
Fitted a model with MAP estimate = -461.6636
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 468.5226 - loglik: -4.6641e+02 - logprior: -1.9475e+00
Epoch 2/10
39/39 - 45s - loss: 462.1668 - loglik: -4.6111e+02 - logprior: -6.6615e-01
Epoch 3/10
39/39 - 45s - loss: 460.0778 - loglik: -4.5881e+02 - logprior: -7.5103e-01
Epoch 4/10
39/39 - 46s - loss: 458.3060 - loglik: -4.5734e+02 - logprior: -3.8541e-01
Epoch 5/10
39/39 - 47s - loss: 457.9397 - loglik: -4.5681e+02 - logprior: -5.5125e-01
Epoch 6/10
39/39 - 48s - loss: 457.1588 - loglik: -4.5647e+02 - logprior: -1.1813e-01
Epoch 7/10
39/39 - 48s - loss: 457.2898 - loglik: -4.5655e+02 - logprior: -1.7885e-01
Fitted a model with MAP estimate = -455.8425
Time for alignment: 1048.8238
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 740.9681 - loglik: -7.3899e+02 - logprior: -1.9692e+00
Epoch 2/10
39/39 - 36s - loss: 534.1080 - loglik: -5.3164e+02 - logprior: -2.2411e+00
Epoch 3/10
39/39 - 37s - loss: 521.5406 - loglik: -5.1897e+02 - logprior: -2.1826e+00
Epoch 4/10
39/39 - 37s - loss: 518.4734 - loglik: -5.1596e+02 - logprior: -2.1045e+00
Epoch 5/10
39/39 - 37s - loss: 517.0042 - loglik: -5.1447e+02 - logprior: -2.1071e+00
Epoch 6/10
39/39 - 38s - loss: 516.2013 - loglik: -5.1369e+02 - logprior: -2.1114e+00
Epoch 7/10
39/39 - 37s - loss: 516.1377 - loglik: -5.1365e+02 - logprior: -2.1082e+00
Epoch 8/10
39/39 - 36s - loss: 515.6812 - loglik: -5.1318e+02 - logprior: -2.1256e+00
Epoch 9/10
39/39 - 37s - loss: 515.7465 - loglik: -5.1325e+02 - logprior: -2.1321e+00
Fitted a model with MAP estimate = -514.4782
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (43, 1), (44, 1), (45, 2), (47, 1), (64, 1), (68, 1), (70, 1), (77, 1), (79, 2), (82, 1), (83, 1), (84, 3), (90, 1), (96, 1), (109, 1), (110, 1), (111, 1), (113, 1), (132, 3), (142, 1), (145, 2), (147, 1), (148, 1), (159, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (179, 1), (182, 1), (188, 1), (189, 1), (206, 1), (209, 1), (214, 2), (216, 2), (217, 1), (229, 2), (231, 1), (244, 1), (257, 1), (258, 1), (264, 1), (267, 1), (269, 1), (272, 3), (273, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 58s - loss: 488.7792 - loglik: -4.8580e+02 - logprior: -2.8095e+00
Epoch 2/2
39/39 - 53s - loss: 468.9782 - loglik: -4.6699e+02 - logprior: -1.5363e+00
Fitted a model with MAP estimate = -465.1238
expansions: []
discards: [  0   2  57  98 108 165 181 216]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 59s - loss: 478.3295 - loglik: -4.7535e+02 - logprior: -2.8213e+00
Epoch 2/2
39/39 - 57s - loss: 469.7652 - loglik: -4.6859e+02 - logprior: -7.8091e-01
Fitted a model with MAP estimate = -466.0183
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 58s - loss: 473.4157 - loglik: -4.7124e+02 - logprior: -2.0197e+00
Epoch 2/10
39/39 - 54s - loss: 467.3946 - loglik: -4.6628e+02 - logprior: -7.3857e-01
Epoch 3/10
39/39 - 54s - loss: 465.0186 - loglik: -4.6386e+02 - logprior: -6.4296e-01
Epoch 4/10
39/39 - 53s - loss: 463.3869 - loglik: -4.6226e+02 - logprior: -5.4794e-01
Epoch 5/10
39/39 - 51s - loss: 462.5903 - loglik: -4.6168e+02 - logprior: -3.2997e-01
Epoch 6/10
39/39 - 53s - loss: 462.3641 - loglik: -4.6154e+02 - logprior: -2.6020e-01
Epoch 7/10
39/39 - 52s - loss: 462.1418 - loglik: -4.6150e+02 - logprior: -8.6850e-02
Epoch 8/10
39/39 - 54s - loss: 461.5323 - loglik: -4.6097e+02 - logprior: -2.1415e-02
Epoch 9/10
39/39 - 53s - loss: 461.6104 - loglik: -4.6130e+02 - logprior: 0.2316
Fitted a model with MAP estimate = -460.5827
Time for alignment: 1304.0724
Computed alignments with likelihoods: ['-454.7256', '-455.8425', '-460.5827']
Best model has likelihood: -454.7256
time for generating output: 0.2814
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00224.projection.fasta
SP score = 0.9231178504921521
Training of 3 independent models on file PF13365.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f497e103fa0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f413c17f280>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f413c17f6d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f413c195d90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f498204b460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b06fec910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49822cf9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f0ebe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47380993a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497e9c63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afeca48e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f417880a760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178369f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42a2c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f416941bd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f78a3d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fa8adc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f414c310460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f413c195f10>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4168f90760>, <__main__.SimpleDirichletPrior object at 0x7f47d41d5700>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 397.9082 - loglik: -3.9469e+02 - logprior: -3.2039e+00
Epoch 2/10
19/19 - 7s - loss: 326.5069 - loglik: -3.2496e+02 - logprior: -1.4560e+00
Epoch 3/10
19/19 - 7s - loss: 302.0836 - loglik: -2.9981e+02 - logprior: -1.9773e+00
Epoch 4/10
19/19 - 7s - loss: 296.1463 - loglik: -2.9375e+02 - logprior: -1.8262e+00
Epoch 5/10
19/19 - 7s - loss: 294.6333 - loglik: -2.9218e+02 - logprior: -1.7715e+00
Epoch 6/10
19/19 - 7s - loss: 294.4050 - loglik: -2.9197e+02 - logprior: -1.7302e+00
Epoch 7/10
19/19 - 7s - loss: 292.5887 - loglik: -2.9020e+02 - logprior: -1.7090e+00
Epoch 8/10
19/19 - 7s - loss: 292.5934 - loglik: -2.9025e+02 - logprior: -1.6965e+00
Fitted a model with MAP estimate = -290.7725
expansions: [(7, 2), (22, 1), (23, 1), (24, 1), (25, 2), (28, 1), (29, 2), (36, 1), (37, 2), (46, 3), (49, 2), (50, 2), (57, 1), (71, 1), (84, 2), (85, 3), (86, 6), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 147 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 308.6190 - loglik: -3.0415e+02 - logprior: -4.3792e+00
Epoch 2/2
19/19 - 9s - loss: 295.3772 - loglik: -2.9229e+02 - logprior: -2.7204e+00
Fitted a model with MAP estimate = -290.6159
expansions: [(0, 2)]
discards: [  0  29  36  47  59  65  67 106 139]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 295.5644 - loglik: -2.9211e+02 - logprior: -3.3652e+00
Epoch 2/2
19/19 - 9s - loss: 289.9312 - loglik: -2.8792e+02 - logprior: -1.5536e+00
Fitted a model with MAP estimate = -285.8113
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10065 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 297.2341 - loglik: -2.9291e+02 - logprior: -4.2375e+00
Epoch 2/10
19/19 - 9s - loss: 290.2149 - loglik: -2.8799e+02 - logprior: -1.8342e+00
Epoch 3/10
19/19 - 9s - loss: 286.6664 - loglik: -2.8444e+02 - logprior: -1.5765e+00
Epoch 4/10
19/19 - 9s - loss: 283.3177 - loglik: -2.8099e+02 - logprior: -1.4835e+00
Epoch 5/10
19/19 - 9s - loss: 282.3368 - loglik: -2.8001e+02 - logprior: -1.3594e+00
Epoch 6/10
19/19 - 9s - loss: 280.8421 - loglik: -2.7861e+02 - logprior: -1.3352e+00
Epoch 7/10
19/19 - 9s - loss: 281.1281 - loglik: -2.7905e+02 - logprior: -1.2763e+00
Fitted a model with MAP estimate = -279.2427
Time for alignment: 205.8115
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 397.6228 - loglik: -3.9441e+02 - logprior: -3.1992e+00
Epoch 2/10
19/19 - 7s - loss: 326.2660 - loglik: -3.2473e+02 - logprior: -1.4221e+00
Epoch 3/10
19/19 - 7s - loss: 302.7234 - loglik: -3.0024e+02 - logprior: -1.7992e+00
Epoch 4/10
19/19 - 7s - loss: 297.0602 - loglik: -2.9434e+02 - logprior: -1.7164e+00
Epoch 5/10
19/19 - 7s - loss: 295.4185 - loglik: -2.9273e+02 - logprior: -1.6705e+00
Epoch 6/10
19/19 - 7s - loss: 293.8207 - loglik: -2.9131e+02 - logprior: -1.6623e+00
Epoch 7/10
19/19 - 7s - loss: 291.5388 - loglik: -2.8913e+02 - logprior: -1.6524e+00
Epoch 8/10
19/19 - 7s - loss: 293.3327 - loglik: -2.9101e+02 - logprior: -1.6420e+00
Fitted a model with MAP estimate = -290.4637
expansions: [(7, 2), (21, 3), (22, 1), (25, 2), (28, 1), (29, 2), (37, 1), (45, 1), (46, 3), (51, 2), (56, 1), (57, 1), (71, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 145 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 307.8031 - loglik: -3.0334e+02 - logprior: -4.3695e+00
Epoch 2/2
19/19 - 9s - loss: 294.8798 - loglik: -2.9176e+02 - logprior: -2.7056e+00
Fitted a model with MAP estimate = -290.3520
expansions: [(0, 2)]
discards: [  0  23  30  37  59  66 105 137]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 295.8004 - loglik: -2.9233e+02 - logprior: -3.3840e+00
Epoch 2/2
19/19 - 9s - loss: 289.6681 - loglik: -2.8768e+02 - logprior: -1.5482e+00
Fitted a model with MAP estimate = -286.0080
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10065 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 297.6596 - loglik: -2.9334e+02 - logprior: -4.2378e+00
Epoch 2/10
19/19 - 9s - loss: 289.9884 - loglik: -2.8771e+02 - logprior: -1.8645e+00
Epoch 3/10
19/19 - 8s - loss: 286.2815 - loglik: -2.8398e+02 - logprior: -1.6014e+00
Epoch 4/10
19/19 - 9s - loss: 284.0029 - loglik: -2.8158e+02 - logprior: -1.4740e+00
Epoch 5/10
19/19 - 9s - loss: 281.7278 - loglik: -2.7932e+02 - logprior: -1.3844e+00
Epoch 6/10
19/19 - 8s - loss: 281.3334 - loglik: -2.7911e+02 - logprior: -1.3267e+00
Epoch 7/10
19/19 - 9s - loss: 281.1749 - loglik: -2.7909e+02 - logprior: -1.2906e+00
Epoch 8/10
19/19 - 9s - loss: 280.6640 - loglik: -2.7868e+02 - logprior: -1.2440e+00
Epoch 9/10
19/19 - 9s - loss: 278.8741 - loglik: -2.7695e+02 - logprior: -1.2177e+00
Epoch 10/10
19/19 - 9s - loss: 279.5805 - loglik: -2.7771e+02 - logprior: -1.1835e+00
Fitted a model with MAP estimate = -278.7446
Time for alignment: 228.4935
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 397.4204 - loglik: -3.9422e+02 - logprior: -3.1949e+00
Epoch 2/10
19/19 - 7s - loss: 327.6876 - loglik: -3.2614e+02 - logprior: -1.4287e+00
Epoch 3/10
19/19 - 7s - loss: 303.4779 - loglik: -3.0104e+02 - logprior: -1.8206e+00
Epoch 4/10
19/19 - 7s - loss: 296.6457 - loglik: -2.9419e+02 - logprior: -1.7679e+00
Epoch 5/10
19/19 - 7s - loss: 295.6524 - loglik: -2.9309e+02 - logprior: -1.7463e+00
Epoch 6/10
19/19 - 7s - loss: 293.4514 - loglik: -2.9101e+02 - logprior: -1.7042e+00
Epoch 7/10
19/19 - 7s - loss: 293.7118 - loglik: -2.9140e+02 - logprior: -1.6797e+00
Fitted a model with MAP estimate = -291.4590
expansions: [(7, 2), (22, 1), (23, 1), (24, 1), (25, 2), (28, 1), (29, 2), (37, 2), (45, 2), (46, 3), (49, 2), (50, 2), (57, 1), (71, 3), (84, 5), (85, 2), (86, 3), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 308.8471 - loglik: -3.0440e+02 - logprior: -4.3581e+00
Epoch 2/2
19/19 - 10s - loss: 294.6560 - loglik: -2.9162e+02 - logprior: -2.6653e+00
Fitted a model with MAP estimate = -290.6405
expansions: [(0, 2)]
discards: [  0  29  36  47  58  59  60  66  68  93 141]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 295.8568 - loglik: -2.9244e+02 - logprior: -3.3314e+00
Epoch 2/2
19/19 - 9s - loss: 289.8380 - loglik: -2.8791e+02 - logprior: -1.5505e+00
Fitted a model with MAP estimate = -286.1084
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10065 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 297.8661 - loglik: -2.9352e+02 - logprior: -4.2583e+00
Epoch 2/10
19/19 - 8s - loss: 289.2514 - loglik: -2.8702e+02 - logprior: -1.8544e+00
Epoch 3/10
19/19 - 8s - loss: 286.7021 - loglik: -2.8443e+02 - logprior: -1.6167e+00
Epoch 4/10
19/19 - 8s - loss: 283.6463 - loglik: -2.8120e+02 - logprior: -1.5091e+00
Epoch 5/10
19/19 - 8s - loss: 281.7473 - loglik: -2.7932e+02 - logprior: -1.3993e+00
Epoch 6/10
19/19 - 8s - loss: 281.5278 - loglik: -2.7929e+02 - logprior: -1.3293e+00
Epoch 7/10
19/19 - 9s - loss: 280.7375 - loglik: -2.7863e+02 - logprior: -1.3117e+00
Epoch 8/10
19/19 - 9s - loss: 279.2241 - loglik: -2.7722e+02 - logprior: -1.2701e+00
Epoch 9/10
19/19 - 9s - loss: 279.7830 - loglik: -2.7786e+02 - logprior: -1.2254e+00
Fitted a model with MAP estimate = -278.8388
Time for alignment: 209.8863
Computed alignments with likelihoods: ['-279.2427', '-278.7446', '-278.8388']
Best model has likelihood: -278.7446
time for generating output: 0.2777
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13365.projection.fasta
SP score = 0.37680424834031
Training of 3 independent models on file PF00078.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4190439850>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b06ff9c70>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b06ff9df0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4ab2424be0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f498204b460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b06fec910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49822cf9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f0ebe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47380993a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497e9c63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afeca48e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f417880a760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178369f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42a2c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f416941bd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f78a3d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fa8adc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f414c310460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f413c195f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab2424f40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f47b46454f0>, <__main__.SimpleDirichletPrior object at 0x7f4168ee50a0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 488.9049 - loglik: -4.8575e+02 - logprior: -3.0442e+00
Epoch 2/10
19/19 - 9s - loss: 449.1132 - loglik: -4.4708e+02 - logprior: -1.2922e+00
Epoch 3/10
19/19 - 9s - loss: 426.8279 - loglik: -4.2381e+02 - logprior: -1.5931e+00
Epoch 4/10
19/19 - 9s - loss: 421.2287 - loglik: -4.1758e+02 - logprior: -1.6628e+00
Epoch 5/10
19/19 - 9s - loss: 418.0167 - loglik: -4.1459e+02 - logprior: -1.6907e+00
Epoch 6/10
19/19 - 9s - loss: 416.3725 - loglik: -4.1324e+02 - logprior: -1.7778e+00
Epoch 7/10
19/19 - 9s - loss: 415.1228 - loglik: -4.1224e+02 - logprior: -1.8242e+00
Epoch 8/10
19/19 - 9s - loss: 414.7140 - loglik: -4.1197e+02 - logprior: -1.8305e+00
Epoch 9/10
19/19 - 9s - loss: 412.2357 - loglik: -4.0958e+02 - logprior: -1.8239e+00
Epoch 10/10
19/19 - 9s - loss: 415.2771 - loglik: -4.1264e+02 - logprior: -1.8391e+00
Fitted a model with MAP estimate = -412.1076
expansions: [(20, 1), (21, 1), (32, 3), (33, 2), (46, 1), (47, 1), (48, 1), (49, 2), (58, 8), (76, 2), (77, 4), (78, 1), (94, 1), (102, 1), (107, 1), (108, 1), (117, 1), (118, 9)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 442.8206 - loglik: -4.3916e+02 - logprior: -3.5642e+00
Epoch 2/2
19/19 - 13s - loss: 418.1866 - loglik: -4.1604e+02 - logprior: -1.6520e+00
Fitted a model with MAP estimate = -411.1875
expansions: [(38, 1)]
discards: [ 55  74  75  76  77  99 150 151 152 153 154 155 156]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 421.7493 - loglik: -4.1841e+02 - logprior: -3.2590e+00
Epoch 2/2
19/19 - 11s - loss: 414.1521 - loglik: -4.1216e+02 - logprior: -1.4836e+00
Fitted a model with MAP estimate = -409.8313
expansions: []
discards: [69 70 71 72 73]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 421.7751 - loglik: -4.1861e+02 - logprior: -3.0811e+00
Epoch 2/10
19/19 - 11s - loss: 415.0384 - loglik: -4.1322e+02 - logprior: -1.3053e+00
Epoch 3/10
19/19 - 11s - loss: 410.9720 - loglik: -4.0854e+02 - logprior: -1.2911e+00
Epoch 4/10
19/19 - 12s - loss: 409.2210 - loglik: -4.0639e+02 - logprior: -1.3459e+00
Epoch 5/10
19/19 - 11s - loss: 405.2860 - loglik: -4.0239e+02 - logprior: -1.3532e+00
Epoch 6/10
19/19 - 12s - loss: 404.8817 - loglik: -4.0214e+02 - logprior: -1.3377e+00
Epoch 7/10
19/19 - 12s - loss: 404.7835 - loglik: -4.0226e+02 - logprior: -1.3182e+00
Epoch 8/10
19/19 - 12s - loss: 402.7152 - loglik: -4.0034e+02 - logprior: -1.3094e+00
Epoch 9/10
19/19 - 12s - loss: 403.1881 - loglik: -4.0092e+02 - logprior: -1.2998e+00
Fitted a model with MAP estimate = -401.7479
Time for alignment: 306.4337
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 488.3720 - loglik: -4.8522e+02 - logprior: -3.0437e+00
Epoch 2/10
19/19 - 10s - loss: 449.2308 - loglik: -4.4717e+02 - logprior: -1.3136e+00
Epoch 3/10
19/19 - 10s - loss: 425.7654 - loglik: -4.2248e+02 - logprior: -1.6754e+00
Epoch 4/10
19/19 - 10s - loss: 421.9339 - loglik: -4.1818e+02 - logprior: -1.6911e+00
Epoch 5/10
19/19 - 10s - loss: 418.3501 - loglik: -4.1487e+02 - logprior: -1.7589e+00
Epoch 6/10
19/19 - 10s - loss: 416.8042 - loglik: -4.1366e+02 - logprior: -1.8037e+00
Epoch 7/10
19/19 - 10s - loss: 414.2785 - loglik: -4.1136e+02 - logprior: -1.8200e+00
Epoch 8/10
19/19 - 10s - loss: 416.5081 - loglik: -4.1374e+02 - logprior: -1.8205e+00
Fitted a model with MAP estimate = -413.3261
expansions: [(19, 2), (20, 1), (31, 16), (32, 1), (33, 1), (48, 1), (49, 1), (50, 2), (76, 2), (77, 4), (78, 1), (94, 1), (106, 1), (107, 1), (108, 2), (109, 1), (112, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 166 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 17s - loss: 443.1899 - loglik: -4.3954e+02 - logprior: -3.5611e+00
Epoch 2/2
19/19 - 12s - loss: 416.9628 - loglik: -4.1482e+02 - logprior: -1.6312e+00
Fitted a model with MAP estimate = -410.7815
expansions: []
discards: [ 35  36  37  38  39  40  41  42  43  44  45  46  69 104]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 422.2289 - loglik: -4.1895e+02 - logprior: -3.1962e+00
Epoch 2/2
19/19 - 11s - loss: 415.7260 - loglik: -4.1383e+02 - logprior: -1.4065e+00
Fitted a model with MAP estimate = -411.8562
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 420.1053 - loglik: -4.1692e+02 - logprior: -3.1033e+00
Epoch 2/10
19/19 - 10s - loss: 414.1818 - loglik: -4.1239e+02 - logprior: -1.2841e+00
Epoch 3/10
19/19 - 10s - loss: 412.4689 - loglik: -4.0999e+02 - logprior: -1.2659e+00
Epoch 4/10
19/19 - 10s - loss: 408.0923 - loglik: -4.0503e+02 - logprior: -1.3100e+00
Epoch 5/10
19/19 - 10s - loss: 406.0343 - loglik: -4.0296e+02 - logprior: -1.3383e+00
Epoch 6/10
19/19 - 10s - loss: 405.0745 - loglik: -4.0225e+02 - logprior: -1.3489e+00
Epoch 7/10
19/19 - 10s - loss: 404.4427 - loglik: -4.0185e+02 - logprior: -1.3423e+00
Epoch 8/10
19/19 - 10s - loss: 401.0848 - loglik: -3.9869e+02 - logprior: -1.3276e+00
Epoch 9/10
19/19 - 10s - loss: 404.4968 - loglik: -4.0222e+02 - logprior: -1.3158e+00
Fitted a model with MAP estimate = -401.3405
Time for alignment: 274.7496
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 488.8746 - loglik: -4.8572e+02 - logprior: -3.0410e+00
Epoch 2/10
19/19 - 9s - loss: 447.4913 - loglik: -4.4549e+02 - logprior: -1.2725e+00
Epoch 3/10
19/19 - 9s - loss: 427.1954 - loglik: -4.2460e+02 - logprior: -1.6410e+00
Epoch 4/10
19/19 - 9s - loss: 421.5580 - loglik: -4.1875e+02 - logprior: -1.7431e+00
Epoch 5/10
19/19 - 9s - loss: 417.8254 - loglik: -4.1478e+02 - logprior: -1.7710e+00
Epoch 6/10
19/19 - 9s - loss: 416.7237 - loglik: -4.1379e+02 - logprior: -1.7643e+00
Epoch 7/10
19/19 - 9s - loss: 416.4390 - loglik: -4.1366e+02 - logprior: -1.7671e+00
Epoch 8/10
19/19 - 9s - loss: 417.6663 - loglik: -4.1503e+02 - logprior: -1.7518e+00
Fitted a model with MAP estimate = -414.2032
expansions: [(19, 2), (20, 1), (31, 4), (32, 3), (47, 1), (48, 1), (49, 2), (55, 5), (75, 2), (76, 2), (78, 1), (94, 1), (98, 1), (107, 4), (108, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 158 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 439.7242 - loglik: -4.3617e+02 - logprior: -3.4584e+00
Epoch 2/2
19/19 - 11s - loss: 417.6508 - loglik: -4.1558e+02 - logprior: -1.5510e+00
Fitted a model with MAP estimate = -412.0469
expansions: [(91, 2)]
discards: [ 35  57  62  63  64  65  70 100]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 422.4718 - loglik: -4.1922e+02 - logprior: -3.1666e+00
Epoch 2/2
19/19 - 11s - loss: 415.0305 - loglik: -4.1310e+02 - logprior: -1.4188e+00
Fitted a model with MAP estimate = -411.1550
expansions: [(64, 4)]
discards: [84 85]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 419.8018 - loglik: -4.1663e+02 - logprior: -3.0861e+00
Epoch 2/10
19/19 - 11s - loss: 413.5750 - loglik: -4.1173e+02 - logprior: -1.3200e+00
Epoch 3/10
19/19 - 11s - loss: 408.8879 - loglik: -4.0636e+02 - logprior: -1.2901e+00
Epoch 4/10
19/19 - 11s - loss: 405.8393 - loglik: -4.0277e+02 - logprior: -1.3349e+00
Epoch 5/10
19/19 - 11s - loss: 405.1238 - loglik: -4.0207e+02 - logprior: -1.3679e+00
Epoch 6/10
19/19 - 11s - loss: 401.9458 - loglik: -3.9918e+02 - logprior: -1.3544e+00
Epoch 7/10
19/19 - 11s - loss: 402.1085 - loglik: -3.9958e+02 - logprior: -1.3443e+00
Fitted a model with MAP estimate = -400.3950
Time for alignment: 254.0677
Computed alignments with likelihoods: ['-401.7479', '-401.3405', '-400.3950']
Best model has likelihood: -400.3950
time for generating output: 0.1846
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00078.projection.fasta
SP score = 0.7768035073734556
Training of 3 independent models on file PF00150.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f417901baf0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4aa533bb50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497e579f40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b41dd4ee0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f498204b460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b06fec910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49822cf9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f0ebe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47380993a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497e9c63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afeca48e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f417880a760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178369f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42a2c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f416941bd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f78a3d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fa8adc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f414c310460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f413c195f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab2424f40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b183dc1c0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f414c0feca0>, <__main__.SimpleDirichletPrior object at 0x7f497f979280>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 33s - loss: 793.4362 - loglik: -7.9117e+02 - logprior: -1.7787e+00
Epoch 2/10
39/39 - 31s - loss: 721.6901 - loglik: -7.1869e+02 - logprior: -1.3041e+00
Epoch 3/10
39/39 - 30s - loss: 709.7394 - loglik: -7.0549e+02 - logprior: -1.5680e+00
Epoch 4/10
39/39 - 31s - loss: 701.9398 - loglik: -6.9714e+02 - logprior: -1.7253e+00
Epoch 5/10
39/39 - 31s - loss: 697.0046 - loglik: -6.9209e+02 - logprior: -1.8421e+00
Epoch 6/10
39/39 - 32s - loss: 693.9663 - loglik: -6.8956e+02 - logprior: -1.8874e+00
Epoch 7/10
39/39 - 32s - loss: 692.2726 - loglik: -6.8812e+02 - logprior: -1.9283e+00
Epoch 8/10
39/39 - 33s - loss: 690.2205 - loglik: -6.8605e+02 - logprior: -1.9635e+00
Epoch 9/10
39/39 - 33s - loss: 688.6298 - loglik: -6.8436e+02 - logprior: -1.9988e+00
Epoch 10/10
39/39 - 31s - loss: 687.2119 - loglik: -6.8315e+02 - logprior: -2.0123e+00
Fitted a model with MAP estimate = -684.1271
expansions: [(0, 3), (9, 1), (10, 1), (23, 3), (42, 2), (49, 1), (50, 1), (51, 1), (84, 5), (85, 4), (94, 1), (97, 2), (103, 1), (114, 4), (115, 2), (118, 1), (132, 1), (150, 5), (172, 2), (173, 3), (175, 3), (179, 1), (206, 7), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 285 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 764.4123 - loglik: -7.6034e+02 - logprior: -3.7087e+00
Epoch 2/2
39/39 - 45s - loss: 705.7128 - loglik: -7.0219e+02 - logprior: -1.9638e+00
Fitted a model with MAP estimate = -693.5467
expansions: [(105, 2), (116, 1), (187, 1), (195, 1), (215, 1), (251, 1)]
discards: [  0   2   3   4  15  16  30  52  53 106 117 121 144 145 146 188 189 190
 191 192 193 217 222 249 256 257]
Re-initialized the encoder parameters.
Fitting a model of length 266 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 718.3057 - loglik: -7.1504e+02 - logprior: -2.9475e+00
Epoch 2/2
39/39 - 38s - loss: 700.9010 - loglik: -6.9851e+02 - logprior: -1.0527e+00
Fitted a model with MAP estimate = -693.0182
expansions: [(0, 4), (44, 2), (95, 1), (97, 1), (174, 1), (200, 1), (237, 3)]
discards: [ 99 100 238]
Re-initialized the encoder parameters.
Fitting a model of length 276 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 44s - loss: 709.5791 - loglik: -7.0713e+02 - logprior: -2.1287e+00
Epoch 2/10
39/39 - 40s - loss: 696.6583 - loglik: -6.9445e+02 - logprior: -9.4164e-01
Epoch 3/10
39/39 - 42s - loss: 690.5191 - loglik: -6.8756e+02 - logprior: -9.6962e-01
Epoch 4/10
39/39 - 43s - loss: 685.9752 - loglik: -6.8259e+02 - logprior: -9.5282e-01
Epoch 5/10
39/39 - 42s - loss: 682.7430 - loglik: -6.7925e+02 - logprior: -9.2418e-01
Epoch 6/10
39/39 - 43s - loss: 679.3426 - loglik: -6.7603e+02 - logprior: -8.4683e-01
Epoch 7/10
39/39 - 43s - loss: 677.2343 - loglik: -6.7422e+02 - logprior: -7.8240e-01
Epoch 8/10
39/39 - 41s - loss: 675.5168 - loglik: -6.7264e+02 - logprior: -7.1201e-01
Epoch 9/10
39/39 - 41s - loss: 673.4244 - loglik: -6.7040e+02 - logprior: -6.6796e-01
Epoch 10/10
39/39 - 40s - loss: 670.4649 - loglik: -6.6722e+02 - logprior: -6.4834e-01
Fitted a model with MAP estimate = -666.7810
Time for alignment: 1071.3258
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 791.7233 - loglik: -7.8946e+02 - logprior: -1.7778e+00
Epoch 2/10
39/39 - 30s - loss: 720.8475 - loglik: -7.1809e+02 - logprior: -1.2920e+00
Epoch 3/10
39/39 - 31s - loss: 706.8489 - loglik: -7.0315e+02 - logprior: -1.6170e+00
Epoch 4/10
39/39 - 31s - loss: 701.1149 - loglik: -6.9682e+02 - logprior: -1.6877e+00
Epoch 5/10
39/39 - 32s - loss: 697.1938 - loglik: -6.9283e+02 - logprior: -1.7834e+00
Epoch 6/10
39/39 - 33s - loss: 694.2570 - loglik: -6.9017e+02 - logprior: -1.8472e+00
Epoch 7/10
39/39 - 31s - loss: 692.7705 - loglik: -6.8884e+02 - logprior: -1.8791e+00
Epoch 8/10
39/39 - 30s - loss: 690.7802 - loglik: -6.8673e+02 - logprior: -1.9269e+00
Epoch 9/10
39/39 - 29s - loss: 688.9747 - loglik: -6.8486e+02 - logprior: -1.9243e+00
Epoch 10/10
39/39 - 29s - loss: 687.6216 - loglik: -6.8362e+02 - logprior: -1.9266e+00
Fitted a model with MAP estimate = -684.5033
expansions: [(0, 4), (41, 1), (42, 2), (49, 1), (52, 1), (61, 1), (86, 1), (87, 15), (88, 1), (104, 1), (115, 4), (116, 2), (117, 1), (121, 1), (154, 1), (175, 5), (176, 2), (179, 1), (207, 7), (208, 1), (214, 1)]
discards: [157]
Re-initialized the encoder parameters.
Fitting a model of length 282 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 40s - loss: 766.7988 - loglik: -7.6280e+02 - logprior: -3.6642e+00
Epoch 2/2
39/39 - 37s - loss: 706.9459 - loglik: -7.0362e+02 - logprior: -1.9587e+00
Fitted a model with MAP estimate = -694.8646
expansions: [(0, 5), (1, 1), (2, 1), (3, 2), (48, 1), (148, 1), (157, 1), (189, 2), (220, 1), (253, 1), (255, 2), (256, 1)]
discards: [  4   5   6   7   8   9  10  11  12  87  88  89  90  91  92 113 114 115
 116 150 151 152 153 154 190 218 222 245 246 247 248]
Re-initialized the encoder parameters.
Fitting a model of length 270 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 41s - loss: 718.8564 - loglik: -7.1531e+02 - logprior: -3.2369e+00
Epoch 2/2
39/39 - 36s - loss: 701.8932 - loglik: -6.9928e+02 - logprior: -1.3045e+00
Fitted a model with MAP estimate = -694.2042
expansions: [(90, 1), (202, 1)]
discards: [  0   9  47 207 209 210 240]
Re-initialized the encoder parameters.
Fitting a model of length 265 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 39s - loss: 711.7238 - loglik: -7.0856e+02 - logprior: -2.8469e+00
Epoch 2/10
39/39 - 36s - loss: 698.6669 - loglik: -6.9640e+02 - logprior: -1.1377e+00
Epoch 3/10
39/39 - 35s - loss: 692.4902 - loglik: -6.8968e+02 - logprior: -1.0081e+00
Epoch 4/10
39/39 - 35s - loss: 688.0793 - loglik: -6.8479e+02 - logprior: -9.7010e-01
Epoch 5/10
39/39 - 35s - loss: 684.1020 - loglik: -6.8063e+02 - logprior: -9.2784e-01
Epoch 6/10
39/39 - 34s - loss: 681.1959 - loglik: -6.7780e+02 - logprior: -8.8048e-01
Epoch 7/10
39/39 - 34s - loss: 678.9811 - loglik: -6.7587e+02 - logprior: -8.3029e-01
Epoch 8/10
39/39 - 34s - loss: 676.4912 - loglik: -6.7348e+02 - logprior: -7.5323e-01
Epoch 9/10
39/39 - 34s - loss: 673.7549 - loglik: -6.7053e+02 - logprior: -7.3217e-01
Epoch 10/10
39/39 - 34s - loss: 672.1182 - loglik: -6.6886e+02 - logprior: -6.9641e-01
Fitted a model with MAP estimate = -668.1913
Time for alignment: 955.1289
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 30s - loss: 790.8445 - loglik: -7.8858e+02 - logprior: -1.7823e+00
Epoch 2/10
39/39 - 27s - loss: 720.5325 - loglik: -7.1784e+02 - logprior: -1.3015e+00
Epoch 3/10
39/39 - 27s - loss: 706.8243 - loglik: -7.0314e+02 - logprior: -1.7116e+00
Epoch 4/10
39/39 - 27s - loss: 701.3287 - loglik: -6.9699e+02 - logprior: -1.7699e+00
Epoch 5/10
39/39 - 27s - loss: 697.2761 - loglik: -6.9280e+02 - logprior: -1.8544e+00
Epoch 6/10
39/39 - 27s - loss: 694.7780 - loglik: -6.9051e+02 - logprior: -1.8978e+00
Epoch 7/10
39/39 - 27s - loss: 692.6480 - loglik: -6.8857e+02 - logprior: -1.9468e+00
Epoch 8/10
39/39 - 27s - loss: 690.5734 - loglik: -6.8630e+02 - logprior: -2.0131e+00
Epoch 9/10
39/39 - 27s - loss: 688.1520 - loglik: -6.8372e+02 - logprior: -2.0656e+00
Epoch 10/10
39/39 - 27s - loss: 686.6344 - loglik: -6.8241e+02 - logprior: -2.0681e+00
Fitted a model with MAP estimate = -683.5932
expansions: [(0, 3), (9, 1), (10, 1), (23, 1), (42, 1), (43, 1), (47, 2), (50, 1), (52, 1), (81, 1), (86, 1), (87, 15), (103, 1), (114, 3), (117, 1), (121, 1), (125, 1), (154, 1), (157, 1), (173, 5), (176, 2), (179, 1), (180, 1), (206, 6), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 283 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 766.5040 - loglik: -7.6251e+02 - logprior: -3.6350e+00
Epoch 2/2
39/39 - 35s - loss: 708.7130 - loglik: -7.0505e+02 - logprior: -2.1701e+00
Fitted a model with MAP estimate = -696.1136
expansions: [(0, 3), (28, 1), (60, 1), (100, 1), (108, 4), (109, 1), (110, 1), (145, 2), (146, 1), (192, 4), (193, 1), (214, 1), (215, 2), (217, 1), (257, 1)]
discards: [  0   1   2   3   4  15  16  29  51  57  61  89  91 115 116 117 118 119
 120 121 122 147 148 151 152 190 221 225 226 227 228 229 230 231 232 233
 251]
Re-initialized the encoder parameters.
Fitting a model of length 271 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 716.9688 - loglik: -7.1433e+02 - logprior: -2.3119e+00
Epoch 2/2
39/39 - 35s - loss: 700.6391 - loglik: -6.9812e+02 - logprior: -1.1623e+00
Fitted a model with MAP estimate = -692.7865
expansions: [(0, 3), (12, 1), (13, 1), (86, 1), (209, 1)]
discards: [  1 102 141 180 222]
Re-initialized the encoder parameters.
Fitting a model of length 273 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 708.6781 - loglik: -7.0592e+02 - logprior: -2.4529e+00
Epoch 2/10
39/39 - 36s - loss: 696.9182 - loglik: -6.9459e+02 - logprior: -1.0682e+00
Epoch 3/10
39/39 - 37s - loss: 690.7090 - loglik: -6.8762e+02 - logprior: -1.1010e+00
Epoch 4/10
39/39 - 36s - loss: 686.1407 - loglik: -6.8261e+02 - logprior: -1.0812e+00
Epoch 5/10
39/39 - 36s - loss: 682.5739 - loglik: -6.7879e+02 - logprior: -1.0664e+00
Epoch 6/10
39/39 - 37s - loss: 678.9952 - loglik: -6.7519e+02 - logprior: -1.1444e+00
Epoch 7/10
39/39 - 38s - loss: 677.0908 - loglik: -6.7350e+02 - logprior: -1.1981e+00
Epoch 8/10
39/39 - 39s - loss: 674.4581 - loglik: -6.7118e+02 - logprior: -9.4055e-01
Epoch 9/10
39/39 - 41s - loss: 672.0322 - loglik: -6.6860e+02 - logprior: -9.0667e-01
Epoch 10/10
39/39 - 41s - loss: 670.3132 - loglik: -6.6692e+02 - logprior: -8.9584e-01
Fitted a model with MAP estimate = -666.6074
Time for alignment: 943.6632
Computed alignments with likelihoods: ['-666.7810', '-668.1913', '-666.6074']
Best model has likelihood: -666.6074
time for generating output: 0.4550
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00150.projection.fasta
SP score = 0.5970949720670391
Training of 3 independent models on file PF13561.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4aa52f6640>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4aa531c700>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afecc14c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4738c86d90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f498204b460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b06fec910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49822cf9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f0ebe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47380993a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497e9c63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afeca48e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f417880a760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178369f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42a2c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f416941bd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f78a3d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fa8adc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f414c310460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f413c195f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab2424f40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b183dc1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1813b2e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f47b407a580>, <__main__.SimpleDirichletPrior object at 0x7f4161693b50>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 189 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 17s - loss: 623.4793 - loglik: -6.2045e+02 - logprior: -2.9228e+00
Epoch 2/10
19/19 - 15s - loss: 529.0938 - loglik: -5.2709e+02 - logprior: -1.4082e+00
Epoch 3/10
19/19 - 16s - loss: 481.6379 - loglik: -4.7883e+02 - logprior: -2.1933e+00
Epoch 4/10
19/19 - 16s - loss: 472.0826 - loglik: -4.6890e+02 - logprior: -2.3685e+00
Epoch 5/10
19/19 - 15s - loss: 470.3085 - loglik: -4.6708e+02 - logprior: -2.3421e+00
Epoch 6/10
19/19 - 16s - loss: 467.8000 - loglik: -4.6464e+02 - logprior: -2.2897e+00
Epoch 7/10
19/19 - 16s - loss: 466.9624 - loglik: -4.6382e+02 - logprior: -2.2953e+00
Epoch 8/10
19/19 - 16s - loss: 466.2014 - loglik: -4.6311e+02 - logprior: -2.2719e+00
Epoch 9/10
19/19 - 16s - loss: 466.7558 - loglik: -4.6372e+02 - logprior: -2.2664e+00
Fitted a model with MAP estimate = -462.0697
expansions: [(12, 1), (15, 1), (22, 1), (23, 2), (24, 1), (29, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (45, 1), (46, 1), (48, 1), (49, 1), (52, 1), (66, 1), (67, 2), (69, 1), (72, 1), (83, 1), (86, 1), (87, 1), (92, 1), (94, 1), (108, 1), (110, 2), (111, 2), (112, 1), (122, 1), (128, 1), (149, 2), (150, 1), (151, 4), (153, 1), (155, 1), (156, 1), (157, 3), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 22s - loss: 483.9678 - loglik: -4.7975e+02 - logprior: -4.1170e+00
Epoch 2/2
19/19 - 20s - loss: 456.0443 - loglik: -4.5325e+02 - logprior: -2.3376e+00
Fitted a model with MAP estimate = -448.3513
expansions: [(3, 1)]
discards: [  0  26 139 142 199]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 460.7948 - loglik: -4.5684e+02 - logprior: -3.8792e+00
Epoch 2/2
19/19 - 21s - loss: 451.9656 - loglik: -4.5000e+02 - logprior: -1.5787e+00
Fitted a model with MAP estimate = -446.0521
expansions: [(4, 1), (195, 1)]
discards: [  0  83 197]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 10077 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 21s - loss: 456.7777 - loglik: -4.5283e+02 - logprior: -3.8621e+00
Epoch 2/10
19/19 - 19s - loss: 449.0312 - loglik: -4.4658e+02 - logprior: -2.0187e+00
Epoch 3/10
19/19 - 20s - loss: 445.3468 - loglik: -4.4268e+02 - logprior: -1.8952e+00
Epoch 4/10
19/19 - 21s - loss: 443.3343 - loglik: -4.4111e+02 - logprior: -1.2076e+00
Epoch 5/10
19/19 - 21s - loss: 441.1417 - loglik: -4.3891e+02 - logprior: -1.0474e+00
Epoch 6/10
19/19 - 20s - loss: 439.2282 - loglik: -4.3693e+02 - logprior: -1.0457e+00
Epoch 7/10
19/19 - 19s - loss: 439.2107 - loglik: -4.3694e+02 - logprior: -1.0322e+00
Epoch 8/10
19/19 - 18s - loss: 438.6312 - loglik: -4.3645e+02 - logprior: -1.0003e+00
Epoch 9/10
19/19 - 18s - loss: 437.3035 - loglik: -4.3521e+02 - logprior: -9.7678e-01
Epoch 10/10
19/19 - 19s - loss: 437.2148 - loglik: -4.3519e+02 - logprior: -9.6356e-01
Fitted a model with MAP estimate = -435.5940
Time for alignment: 526.4954
Fitting a model of length 189 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 18s - loss: 623.8033 - loglik: -6.2077e+02 - logprior: -2.9229e+00
Epoch 2/10
19/19 - 14s - loss: 527.3083 - loglik: -5.2525e+02 - logprior: -1.4005e+00
Epoch 3/10
19/19 - 15s - loss: 482.8541 - loglik: -4.7930e+02 - logprior: -2.0907e+00
Epoch 4/10
19/19 - 15s - loss: 473.7995 - loglik: -4.7017e+02 - logprior: -2.2647e+00
Epoch 5/10
19/19 - 15s - loss: 469.6096 - loglik: -4.6614e+02 - logprior: -2.3382e+00
Epoch 6/10
19/19 - 15s - loss: 467.7085 - loglik: -4.6444e+02 - logprior: -2.3388e+00
Epoch 7/10
19/19 - 16s - loss: 466.9128 - loglik: -4.6373e+02 - logprior: -2.2938e+00
Epoch 8/10
19/19 - 16s - loss: 465.9734 - loglik: -4.6286e+02 - logprior: -2.2781e+00
Epoch 9/10
19/19 - 16s - loss: 464.9398 - loglik: -4.6186e+02 - logprior: -2.2687e+00
Epoch 10/10
19/19 - 16s - loss: 465.1331 - loglik: -4.6208e+02 - logprior: -2.2784e+00
Fitted a model with MAP estimate = -461.3260
expansions: [(12, 1), (15, 1), (22, 1), (23, 2), (24, 1), (27, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (47, 1), (48, 1), (66, 1), (67, 2), (69, 1), (72, 1), (83, 1), (88, 1), (89, 1), (92, 1), (95, 1), (108, 1), (110, 2), (111, 2), (112, 1), (126, 1), (128, 1), (149, 2), (150, 1), (151, 4), (153, 1), (154, 1), (155, 1), (156, 1), (157, 2), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 25s - loss: 485.7995 - loglik: -4.8159e+02 - logprior: -4.1187e+00
Epoch 2/2
19/19 - 24s - loss: 455.8138 - loglik: -4.5305e+02 - logprior: -2.3095e+00
Fitted a model with MAP estimate = -448.0792
expansions: [(3, 1)]
discards: [  0  26 139 142 189 190 200]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 460.7830 - loglik: -4.5683e+02 - logprior: -3.8809e+00
Epoch 2/2
19/19 - 23s - loss: 451.7400 - loglik: -4.4973e+02 - logprior: -1.5893e+00
Fitted a model with MAP estimate = -445.9045
expansions: [(4, 1)]
discards: [ 0 83]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10077 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 26s - loss: 456.5726 - loglik: -4.5261e+02 - logprior: -3.8767e+00
Epoch 2/10
19/19 - 19s - loss: 449.3028 - loglik: -4.4680e+02 - logprior: -2.0885e+00
Epoch 3/10
19/19 - 19s - loss: 445.9734 - loglik: -4.4333e+02 - logprior: -1.9267e+00
Epoch 4/10
19/19 - 20s - loss: 442.3974 - loglik: -4.4023e+02 - logprior: -1.2147e+00
Epoch 5/10
19/19 - 20s - loss: 441.0201 - loglik: -4.3886e+02 - logprior: -1.0343e+00
Epoch 6/10
19/19 - 21s - loss: 439.8514 - loglik: -4.3762e+02 - logprior: -1.0203e+00
Epoch 7/10
19/19 - 23s - loss: 439.4099 - loglik: -4.3720e+02 - logprior: -9.9535e-01
Epoch 8/10
19/19 - 21s - loss: 438.7263 - loglik: -4.3658e+02 - logprior: -9.6503e-01
Epoch 9/10
19/19 - 19s - loss: 437.1527 - loglik: -4.3507e+02 - logprior: -9.5620e-01
Epoch 10/10
19/19 - 18s - loss: 437.7246 - loglik: -4.3576e+02 - logprior: -9.1204e-01
Fitted a model with MAP estimate = -435.7983
Time for alignment: 569.4654
Fitting a model of length 189 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 16s - loss: 623.7227 - loglik: -6.2069e+02 - logprior: -2.9240e+00
Epoch 2/10
19/19 - 14s - loss: 525.8942 - loglik: -5.2389e+02 - logprior: -1.4069e+00
Epoch 3/10
19/19 - 14s - loss: 480.8004 - loglik: -4.7762e+02 - logprior: -2.1632e+00
Epoch 4/10
19/19 - 14s - loss: 472.2561 - loglik: -4.6875e+02 - logprior: -2.2944e+00
Epoch 5/10
19/19 - 15s - loss: 469.6248 - loglik: -4.6625e+02 - logprior: -2.2766e+00
Epoch 6/10
19/19 - 15s - loss: 467.7956 - loglik: -4.6454e+02 - logprior: -2.2411e+00
Epoch 7/10
19/19 - 15s - loss: 467.1638 - loglik: -4.6398e+02 - logprior: -2.2342e+00
Epoch 8/10
19/19 - 14s - loss: 465.5331 - loglik: -4.6237e+02 - logprior: -2.2362e+00
Epoch 9/10
19/19 - 14s - loss: 465.4590 - loglik: -4.6235e+02 - logprior: -2.2281e+00
Epoch 10/10
19/19 - 13s - loss: 465.1210 - loglik: -4.6205e+02 - logprior: -2.2388e+00
Fitted a model with MAP estimate = -461.1400
expansions: [(12, 1), (15, 1), (18, 1), (23, 2), (24, 1), (27, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 2), (37, 1), (42, 1), (45, 1), (46, 1), (48, 1), (66, 1), (67, 2), (69, 1), (77, 1), (83, 1), (86, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (133, 1), (149, 2), (150, 1), (151, 4), (153, 1), (154, 1), (155, 1), (156, 1), (157, 2), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 22s - loss: 485.3192 - loglik: -4.8109e+02 - logprior: -4.1345e+00
Epoch 2/2
19/19 - 21s - loss: 456.1951 - loglik: -4.5342e+02 - logprior: -2.3603e+00
Fitted a model with MAP estimate = -448.2478
expansions: [(3, 1)]
discards: [  0  26  85 139 143 189 200]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 25s - loss: 460.5843 - loglik: -4.5662e+02 - logprior: -3.8916e+00
Epoch 2/2
19/19 - 22s - loss: 452.3521 - loglik: -4.5031e+02 - logprior: -1.6135e+00
Fitted a model with MAP estimate = -446.0816
expansions: [(4, 1), (194, 1)]
discards: [  0 191]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10077 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 26s - loss: 456.5514 - loglik: -4.5261e+02 - logprior: -3.8502e+00
Epoch 2/10
19/19 - 24s - loss: 449.4844 - loglik: -4.4704e+02 - logprior: -2.0321e+00
Epoch 3/10
19/19 - 22s - loss: 445.8654 - loglik: -4.4325e+02 - logprior: -1.9069e+00
Epoch 4/10
19/19 - 23s - loss: 442.6016 - loglik: -4.4044e+02 - logprior: -1.2194e+00
Epoch 5/10
19/19 - 23s - loss: 441.9094 - loglik: -4.3976e+02 - logprior: -1.0305e+00
Epoch 6/10
19/19 - 23s - loss: 438.8678 - loglik: -4.3662e+02 - logprior: -1.0334e+00
Epoch 7/10
19/19 - 22s - loss: 439.3007 - loglik: -4.3707e+02 - logprior: -1.0080e+00
Fitted a model with MAP estimate = -437.2174
Time for alignment: 501.7209
Computed alignments with likelihoods: ['-435.5940', '-435.7983', '-437.2174']
Best model has likelihood: -435.5940
time for generating output: 0.4109
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13561.projection.fasta
SP score = 0.7411031233690822
Training of 3 independent models on file PF05746.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f48e82b0af0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f48e82b0e80>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0df0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0d30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0cd0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0eb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f48e82b0f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0fa0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0ac0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0a60>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0760>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0610>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0550>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b04f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0490>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f48e82b03d0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f41610d6f10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4178ff93d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4169d1b790>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f47ac59c190>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f48e825bd30> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48e82b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b424c7b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1857ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fdc2a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20a6a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afe94e250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1810b520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab242db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b294b5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b20d0e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b186fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47b4233c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab21964f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d416f520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4af5ae3a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4739783be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497ca94bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b207ea430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ac3c0370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42339d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178424e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47382150d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f473b2f4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4190663610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497d134cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f498204b460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b06fec910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49822cf9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f0ebe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47380993a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4738a66520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497e9c63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4afeca48e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f417880a760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4178369f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47d42a2c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f416941bd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497f78a3d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f497fa8adc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f414c310460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f413c195f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4ab2424f40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b183dc1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b1813b2e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f414c0fef10>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f4b607bde50> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f48e8349040>, <function make_default_emission_matrix at 0x7f48e8349040>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f414c0fe580>, <__main__.SimpleDirichletPrior object at 0x7f4739878460>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 321.2829 - loglik: -3.1795e+02 - logprior: -3.2332e+00
Epoch 2/10
19/19 - 4s - loss: 276.4055 - loglik: -2.7422e+02 - logprior: -1.5375e+00
Epoch 3/10
19/19 - 4s - loss: 253.9408 - loglik: -2.5087e+02 - logprior: -1.9913e+00
Epoch 4/10
19/19 - 4s - loss: 247.8952 - loglik: -2.4519e+02 - logprior: -1.8966e+00
Epoch 5/10
19/19 - 4s - loss: 244.1456 - loglik: -2.4160e+02 - logprior: -1.9127e+00
Epoch 6/10
19/19 - 4s - loss: 243.2117 - loglik: -2.4087e+02 - logprior: -1.9138e+00
Epoch 7/10
19/19 - 4s - loss: 242.7550 - loglik: -2.4049e+02 - logprior: -1.8973e+00
Epoch 8/10
19/19 - 4s - loss: 241.8976 - loglik: -2.3967e+02 - logprior: -1.8816e+00
Epoch 9/10
19/19 - 4s - loss: 242.2338 - loglik: -2.4004e+02 - logprior: -1.8798e+00
Fitted a model with MAP estimate = -241.3099
expansions: [(17, 2), (19, 2), (20, 4), (22, 2), (23, 1), (27, 1), (28, 2), (32, 2), (34, 1), (40, 1), (44, 1), (48, 1), (49, 1), (59, 1), (62, 1), (63, 1), (65, 2), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 251.6710 - loglik: -2.4733e+02 - logprior: -4.2567e+00
Epoch 2/2
19/19 - 5s - loss: 236.6673 - loglik: -2.3393e+02 - logprior: -2.3176e+00
Fitted a model with MAP estimate = -233.5768
expansions: [(0, 2)]
discards: [ 0 15 16 17 31 40 45 88]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 238.5398 - loglik: -2.3536e+02 - logprior: -3.0961e+00
Epoch 2/2
19/19 - 5s - loss: 233.5123 - loglik: -2.3190e+02 - logprior: -1.2541e+00
Fitted a model with MAP estimate = -231.5111
expansions: []
discards: [ 0 21 22]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 240.7594 - loglik: -2.3678e+02 - logprior: -3.8954e+00
Epoch 2/10
19/19 - 5s - loss: 235.2334 - loglik: -2.3345e+02 - logprior: -1.4530e+00
Epoch 3/10
19/19 - 5s - loss: 232.2089 - loglik: -2.3035e+02 - logprior: -1.3430e+00
Epoch 4/10
19/19 - 5s - loss: 231.0556 - loglik: -2.2911e+02 - logprior: -1.3204e+00
Epoch 5/10
19/19 - 5s - loss: 230.5065 - loglik: -2.2860e+02 - logprior: -1.3031e+00
Epoch 6/10
19/19 - 5s - loss: 230.1993 - loglik: -2.2840e+02 - logprior: -1.2661e+00
Epoch 7/10
19/19 - 5s - loss: 229.6936 - loglik: -2.2796e+02 - logprior: -1.2500e+00
Epoch 8/10
19/19 - 5s - loss: 229.8733 - loglik: -2.2822e+02 - logprior: -1.2155e+00
Fitted a model with MAP estimate = -229.0124
Time for alignment: 135.2920
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 320.7555 - loglik: -3.1743e+02 - logprior: -3.2370e+00
Epoch 2/10
19/19 - 4s - loss: 275.6967 - loglik: -2.7351e+02 - logprior: -1.5585e+00
Epoch 3/10
19/19 - 4s - loss: 252.8816 - loglik: -2.4987e+02 - logprior: -1.9490e+00
Epoch 4/10
19/19 - 4s - loss: 245.9372 - loglik: -2.4322e+02 - logprior: -1.9226e+00
Epoch 5/10
19/19 - 4s - loss: 243.7664 - loglik: -2.4127e+02 - logprior: -1.9127e+00
Epoch 6/10
19/19 - 4s - loss: 242.9390 - loglik: -2.4065e+02 - logprior: -1.8729e+00
Epoch 7/10
19/19 - 4s - loss: 242.6536 - loglik: -2.4046e+02 - logprior: -1.8366e+00
Epoch 8/10
19/19 - 4s - loss: 241.9232 - loglik: -2.3976e+02 - logprior: -1.8203e+00
Epoch 9/10
19/19 - 4s - loss: 242.2545 - loglik: -2.4012e+02 - logprior: -1.8159e+00
Fitted a model with MAP estimate = -241.4618
expansions: [(17, 2), (20, 4), (23, 2), (27, 1), (28, 2), (32, 2), (34, 1), (40, 1), (44, 1), (47, 1), (49, 1), (59, 1), (62, 1), (63, 1), (69, 1), (71, 2), (76, 1), (79, 1), (81, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 251.6913 - loglik: -2.4738e+02 - logprior: -4.2272e+00
Epoch 2/2
19/19 - 5s - loss: 238.1403 - loglik: -2.3543e+02 - logprior: -2.2629e+00
Fitted a model with MAP estimate = -234.8303
expansions: [(0, 2)]
discards: [ 0 37 42]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 237.7541 - loglik: -2.3457e+02 - logprior: -3.0953e+00
Epoch 2/2
19/19 - 4s - loss: 232.7296 - loglik: -2.3108e+02 - logprior: -1.2763e+00
Fitted a model with MAP estimate = -230.8984
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 239.9267 - loglik: -2.3592e+02 - logprior: -3.9288e+00
Epoch 2/10
19/19 - 5s - loss: 234.2603 - loglik: -2.3248e+02 - logprior: -1.4909e+00
Epoch 3/10
19/19 - 5s - loss: 231.3834 - loglik: -2.2959e+02 - logprior: -1.3433e+00
Epoch 4/10
19/19 - 5s - loss: 230.2477 - loglik: -2.2836e+02 - logprior: -1.3061e+00
Epoch 5/10
19/19 - 5s - loss: 229.7302 - loglik: -2.2787e+02 - logprior: -1.2671e+00
Epoch 6/10
19/19 - 5s - loss: 229.2732 - loglik: -2.2749e+02 - logprior: -1.2441e+00
Epoch 7/10
19/19 - 5s - loss: 228.9028 - loglik: -2.2719e+02 - logprior: -1.2181e+00
Epoch 8/10
19/19 - 5s - loss: 229.1922 - loglik: -2.2755e+02 - logprior: -1.1914e+00
Fitted a model with MAP estimate = -228.2646
Time for alignment: 128.5694
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 320.8639 - loglik: -3.1753e+02 - logprior: -3.2376e+00
Epoch 2/10
19/19 - 4s - loss: 275.0869 - loglik: -2.7293e+02 - logprior: -1.5544e+00
Epoch 3/10
19/19 - 4s - loss: 250.4812 - loglik: -2.4753e+02 - logprior: -2.0001e+00
Epoch 4/10
19/19 - 4s - loss: 245.0621 - loglik: -2.4235e+02 - logprior: -1.9424e+00
Epoch 5/10
19/19 - 4s - loss: 243.4306 - loglik: -2.4101e+02 - logprior: -1.9063e+00
Epoch 6/10
19/19 - 4s - loss: 242.7835 - loglik: -2.4048e+02 - logprior: -1.8501e+00
Epoch 7/10
19/19 - 4s - loss: 242.3469 - loglik: -2.4012e+02 - logprior: -1.8342e+00
Epoch 8/10
19/19 - 4s - loss: 241.5828 - loglik: -2.3941e+02 - logprior: -1.8355e+00
Epoch 9/10
19/19 - 4s - loss: 241.8337 - loglik: -2.3968e+02 - logprior: -1.8278e+00
Fitted a model with MAP estimate = -241.0194
expansions: [(17, 2), (20, 3), (23, 1), (24, 1), (28, 2), (32, 2), (34, 1), (40, 1), (44, 1), (48, 1), (49, 1), (55, 1), (62, 1), (63, 1), (69, 2), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 252.3212 - loglik: -2.4801e+02 - logprior: -4.2318e+00
Epoch 2/2
19/19 - 5s - loss: 238.3246 - loglik: -2.3558e+02 - logprior: -2.3056e+00
Fitted a model with MAP estimate = -235.2643
expansions: [(0, 2), (21, 2), (22, 1)]
discards: [ 0 27 35 40 87]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 237.9420 - loglik: -2.3475e+02 - logprior: -3.1068e+00
Epoch 2/2
19/19 - 5s - loss: 232.9387 - loglik: -2.3129e+02 - logprior: -1.2676e+00
Fitted a model with MAP estimate = -230.8515
expansions: []
discards: [ 0 17]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 240.3732 - loglik: -2.3641e+02 - logprior: -3.8793e+00
Epoch 2/10
19/19 - 4s - loss: 234.4984 - loglik: -2.3269e+02 - logprior: -1.4686e+00
Epoch 3/10
19/19 - 4s - loss: 231.5029 - loglik: -2.2961e+02 - logprior: -1.3502e+00
Epoch 4/10
19/19 - 5s - loss: 230.6997 - loglik: -2.2877e+02 - logprior: -1.3079e+00
Epoch 5/10
19/19 - 5s - loss: 230.0693 - loglik: -2.2818e+02 - logprior: -1.2925e+00
Epoch 6/10
19/19 - 5s - loss: 229.5555 - loglik: -2.2777e+02 - logprior: -1.2479e+00
Epoch 7/10
19/19 - 5s - loss: 229.6616 - loglik: -2.2795e+02 - logprior: -1.2256e+00
Fitted a model with MAP estimate = -228.7335
Time for alignment: 123.6838
Computed alignments with likelihoods: ['-229.0124', '-228.2646', '-228.7335']
Best model has likelihood: -228.2646
time for generating output: 0.2043
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF05746.projection.fasta
SP score = 0.8603624901497242
