Training of 3 independent models on file PF00079.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd40b562e80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd3e90d7fd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd42baff760>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd3e978fe50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd3e978feb0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd3e9e4c9a0>, <__main__.SimpleDirichletPrior object at 0x7fd3e97b6910>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 40s - loss: 769.9880 - loglik: -7.6780e+02 - logprior: -1.7490e+00
Epoch 2/10
39/39 - 38s - loss: 642.5294 - loglik: -6.3914e+02 - logprior: -2.3620e+00
Epoch 3/10
39/39 - 39s - loss: 630.2119 - loglik: -6.2637e+02 - logprior: -2.4674e+00
Epoch 4/10
39/39 - 39s - loss: 628.0591 - loglik: -6.2436e+02 - logprior: -2.3705e+00
Epoch 5/10
39/39 - 40s - loss: 625.6075 - loglik: -6.2195e+02 - logprior: -2.3833e+00
Epoch 6/10
39/39 - 41s - loss: 625.2637 - loglik: -6.2162e+02 - logprior: -2.4470e+00
Epoch 7/10
39/39 - 41s - loss: 624.0526 - loglik: -6.2046e+02 - logprior: -2.4732e+00
Epoch 8/10
39/39 - 42s - loss: 624.0517 - loglik: -6.2053e+02 - logprior: -2.4817e+00
Epoch 9/10
39/39 - 42s - loss: 623.2702 - loglik: -6.1979e+02 - logprior: -2.4952e+00
Epoch 10/10
39/39 - 42s - loss: 623.3019 - loglik: -6.1990e+02 - logprior: -2.4952e+00
Fitted a model with MAP estimate = -621.5472
expansions: [(9, 1), (12, 1), (14, 1), (15, 1), (46, 1), (53, 3), (56, 1), (62, 1), (63, 1), (64, 1), (66, 1), (68, 1), (69, 1), (78, 1), (79, 1), (82, 1), (84, 1), (85, 1), (86, 1), (91, 1), (92, 1), (95, 1), (98, 1), (100, 1), (103, 1), (113, 1), (118, 1), (121, 1), (122, 1), (136, 1), (141, 1), (144, 1), (147, 1), (158, 1), (159, 1), (162, 1), (164, 1), (165, 1), (167, 1), (168, 1), (174, 1), (175, 1), (186, 1), (189, 1), (193, 1), (195, 1), (196, 1), (197, 1), (199, 1), (200, 2), (201, 1), (202, 1), (206, 1), (215, 1), (218, 1), (220, 1), (230, 1), (234, 1), (235, 1), (236, 2), (238, 1), (263, 1), (264, 1), (265, 1), (266, 3), (267, 2), (268, 1), (279, 2), (280, 3)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 64s - loss: 628.3481 - loglik: -6.2613e+02 - logprior: -1.9666e+00
Epoch 2/2
39/39 - 61s - loss: 602.5049 - loglik: -6.0079e+02 - logprior: -8.6215e-01
Fitted a model with MAP estimate = -596.6736
expansions: []
discards: [250]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 64s - loss: 611.1563 - loglik: -6.0937e+02 - logprior: -1.5495e+00
Epoch 2/2
39/39 - 60s - loss: 600.9062 - loglik: -5.9939e+02 - logprior: -5.2033e-01
Fitted a model with MAP estimate = -595.6425
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 366 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 63s - loss: 610.4329 - loglik: -6.0889e+02 - logprior: -1.3073e+00
Epoch 2/10
39/39 - 59s - loss: 601.1343 - loglik: -5.9993e+02 - logprior: -2.6680e-01
Epoch 3/10
39/39 - 60s - loss: 596.5102 - loglik: -5.9467e+02 - logprior: -1.9651e-01
Epoch 4/10
39/39 - 61s - loss: 593.7507 - loglik: -5.9191e+02 - logprior: -7.2023e-02
Epoch 5/10
39/39 - 63s - loss: 593.5919 - loglik: -5.9200e+02 - logprior: 0.0787
Epoch 6/10
39/39 - 64s - loss: 593.0314 - loglik: -5.9170e+02 - logprior: 0.1967
Epoch 7/10
39/39 - 67s - loss: 591.4683 - loglik: -5.9038e+02 - logprior: 0.3383
Epoch 8/10
39/39 - 67s - loss: 591.6627 - loglik: -5.9077e+02 - logprior: 0.4171
Fitted a model with MAP estimate = -588.8363
Time for alignment: 1384.5628
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 770.0815 - loglik: -7.6790e+02 - logprior: -1.7416e+00
Epoch 2/10
39/39 - 45s - loss: 641.0183 - loglik: -6.3691e+02 - logprior: -2.4068e+00
Epoch 3/10
39/39 - 45s - loss: 629.9940 - loglik: -6.2582e+02 - logprior: -2.4068e+00
Epoch 4/10
39/39 - 45s - loss: 626.8584 - loglik: -6.2307e+02 - logprior: -2.3939e+00
Epoch 5/10
39/39 - 47s - loss: 625.8782 - loglik: -6.2218e+02 - logprior: -2.4147e+00
Epoch 6/10
39/39 - 46s - loss: 624.7936 - loglik: -6.2114e+02 - logprior: -2.4634e+00
Epoch 7/10
39/39 - 46s - loss: 624.1583 - loglik: -6.2058e+02 - logprior: -2.4809e+00
Epoch 8/10
39/39 - 46s - loss: 623.7309 - loglik: -6.2025e+02 - logprior: -2.4920e+00
Epoch 9/10
39/39 - 45s - loss: 624.0278 - loglik: -6.2059e+02 - logprior: -2.5006e+00
Fitted a model with MAP estimate = -621.8126
expansions: [(13, 1), (14, 1), (15, 1), (46, 1), (52, 5), (55, 1), (61, 1), (62, 1), (63, 1), (65, 1), (67, 1), (68, 1), (75, 1), (76, 1), (77, 1), (83, 1), (84, 1), (85, 1), (89, 1), (90, 1), (94, 1), (97, 1), (98, 1), (99, 1), (105, 1), (111, 1), (116, 1), (118, 1), (120, 1), (133, 1), (139, 1), (141, 1), (142, 1), (157, 1), (158, 1), (162, 1), (163, 1), (165, 1), (166, 1), (174, 1), (185, 2), (188, 1), (191, 1), (193, 1), (194, 1), (195, 1), (197, 1), (198, 2), (199, 1), (213, 1), (214, 1), (217, 1), (219, 1), (223, 1), (228, 1), (235, 2), (236, 2), (237, 1), (241, 1), (261, 1), (262, 1), (263, 1), (265, 2), (267, 1), (268, 1), (277, 1), (278, 2), (279, 1), (280, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 68s - loss: 628.5905 - loglik: -6.2640e+02 - logprior: -1.9529e+00
Epoch 2/2
39/39 - 62s - loss: 603.5765 - loglik: -6.0192e+02 - logprior: -9.3472e-01
Fitted a model with MAP estimate = -598.2919
expansions: [(16, 1)]
discards: [ 58 249 297]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 62s - loss: 611.4556 - loglik: -6.0966e+02 - logprior: -1.5440e+00
Epoch 2/2
39/39 - 61s - loss: 601.6843 - loglik: -6.0007e+02 - logprior: -5.7129e-01
Fitted a model with MAP estimate = -595.9943
expansions: [(143, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 366 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 68s - loss: 610.8458 - loglik: -6.0932e+02 - logprior: -1.2921e+00
Epoch 2/10
39/39 - 65s - loss: 601.2756 - loglik: -6.0005e+02 - logprior: -2.7915e-01
Epoch 3/10
39/39 - 65s - loss: 596.1912 - loglik: -5.9437e+02 - logprior: -1.7600e-01
Epoch 4/10
39/39 - 66s - loss: 594.5058 - loglik: -5.9271e+02 - logprior: -3.4589e-02
Epoch 5/10
39/39 - 66s - loss: 593.2172 - loglik: -5.9157e+02 - logprior: 0.0239
Epoch 6/10
39/39 - 67s - loss: 592.8937 - loglik: -5.9150e+02 - logprior: 0.1311
Epoch 7/10
39/39 - 64s - loss: 592.0704 - loglik: -5.9096e+02 - logprior: 0.3001
Epoch 8/10
39/39 - 59s - loss: 591.1377 - loglik: -5.9027e+02 - logprior: 0.4199
Epoch 9/10
39/39 - 59s - loss: 590.7964 - loglik: -5.9017e+02 - logprior: 0.5783
Epoch 10/10
39/39 - 59s - loss: 589.7034 - loglik: -5.8930e+02 - logprior: 0.7079
Fitted a model with MAP estimate = -588.0567
Time for alignment: 1531.2657
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 44s - loss: 771.2242 - loglik: -7.6905e+02 - logprior: -1.7255e+00
Epoch 2/10
39/39 - 41s - loss: 642.0156 - loglik: -6.3796e+02 - logprior: -2.3299e+00
Epoch 3/10
39/39 - 41s - loss: 630.1225 - loglik: -6.2601e+02 - logprior: -2.3427e+00
Epoch 4/10
39/39 - 41s - loss: 627.8001 - loglik: -6.2408e+02 - logprior: -2.3407e+00
Epoch 5/10
39/39 - 41s - loss: 626.0329 - loglik: -6.2243e+02 - logprior: -2.3408e+00
Epoch 6/10
39/39 - 41s - loss: 625.0352 - loglik: -6.2151e+02 - logprior: -2.3580e+00
Epoch 7/10
39/39 - 41s - loss: 624.7931 - loglik: -6.2135e+02 - logprior: -2.3643e+00
Epoch 8/10
39/39 - 41s - loss: 624.1754 - loglik: -6.2080e+02 - logprior: -2.3806e+00
Epoch 9/10
39/39 - 42s - loss: 624.1917 - loglik: -6.2088e+02 - logprior: -2.3954e+00
Fitted a model with MAP estimate = -622.3712
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (46, 1), (52, 4), (57, 1), (62, 2), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (78, 1), (79, 1), (80, 1), (83, 1), (84, 1), (85, 1), (86, 1), (90, 1), (91, 1), (94, 1), (97, 1), (99, 1), (102, 1), (106, 1), (117, 1), (119, 1), (121, 1), (135, 1), (140, 1), (143, 1), (146, 1), (157, 1), (158, 1), (159, 1), (163, 2), (164, 1), (166, 2), (174, 1), (185, 1), (188, 1), (192, 1), (194, 1), (195, 1), (196, 2), (197, 1), (198, 1), (199, 1), (213, 1), (214, 1), (216, 1), (217, 1), (223, 1), (228, 1), (234, 1), (235, 2), (237, 1), (262, 1), (263, 1), (264, 1), (265, 2), (266, 3), (267, 1), (278, 2), (279, 1), (280, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 368 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 64s - loss: 628.9372 - loglik: -6.2674e+02 - logprior: -1.9647e+00
Epoch 2/2
39/39 - 60s - loss: 604.1388 - loglik: -6.0242e+02 - logprior: -8.9880e-01
Fitted a model with MAP estimate = -598.0005
expansions: [(210, 1)]
discards: [ 58  73 206 337 338]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 65s - loss: 611.4214 - loglik: -6.0962e+02 - logprior: -1.5385e+00
Epoch 2/2
39/39 - 61s - loss: 601.3878 - loglik: -5.9981e+02 - logprior: -5.5010e-01
Fitted a model with MAP estimate = -595.9522
expansions: [(143, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 66s - loss: 610.8721 - loglik: -6.0934e+02 - logprior: -1.2932e+00
Epoch 2/10
39/39 - 61s - loss: 600.9633 - loglik: -5.9974e+02 - logprior: -2.5517e-01
Epoch 3/10
39/39 - 62s - loss: 596.3826 - loglik: -5.9455e+02 - logprior: -1.4436e-01
Epoch 4/10
39/39 - 60s - loss: 594.1492 - loglik: -5.9230e+02 - logprior: -4.3295e-02
Epoch 5/10
39/39 - 59s - loss: 592.9601 - loglik: -5.9136e+02 - logprior: 0.0991
Epoch 6/10
39/39 - 60s - loss: 592.8408 - loglik: -5.9144e+02 - logprior: 0.1434
Epoch 7/10
39/39 - 60s - loss: 591.6330 - loglik: -5.9051e+02 - logprior: 0.2914
Epoch 8/10
39/39 - 59s - loss: 591.2654 - loglik: -5.9042e+02 - logprior: 0.4543
Epoch 9/10
39/39 - 59s - loss: 590.1015 - loglik: -5.8949e+02 - logprior: 0.5846
Epoch 10/10
39/39 - 59s - loss: 589.4792 - loglik: -5.8902e+02 - logprior: 0.6497
Fitted a model with MAP estimate = -587.7297
Time for alignment: 1448.8545
Computed alignments with likelihoods: ['-588.8363', '-588.0567', '-587.7297']
Best model has likelihood: -587.7297
time for generating output: 0.3503
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00079.projection.fasta
SP score = 0.9264705882352942
Training of 3 independent models on file PF01381.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd42baff760>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd3e978fe50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd40b562e80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd3e90d7fd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd3f2068f40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd2116f6dc0>, <__main__.SimpleDirichletPrior object at 0x7fd3cf963fa0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 150.0445 - loglik: -1.4679e+02 - logprior: -3.2285e+00
Epoch 2/10
19/19 - 1s - loss: 126.9653 - loglik: -1.2526e+02 - logprior: -1.4386e+00
Epoch 3/10
19/19 - 1s - loss: 118.3997 - loglik: -1.1625e+02 - logprior: -1.6793e+00
Epoch 4/10
19/19 - 1s - loss: 117.0355 - loglik: -1.1517e+02 - logprior: -1.5491e+00
Epoch 5/10
19/19 - 1s - loss: 116.5143 - loglik: -1.1473e+02 - logprior: -1.5273e+00
Epoch 6/10
19/19 - 1s - loss: 116.3000 - loglik: -1.1456e+02 - logprior: -1.4972e+00
Epoch 7/10
19/19 - 1s - loss: 116.1070 - loglik: -1.1439e+02 - logprior: -1.4848e+00
Epoch 8/10
19/19 - 1s - loss: 116.0598 - loglik: -1.1437e+02 - logprior: -1.4769e+00
Epoch 9/10
19/19 - 1s - loss: 115.9558 - loglik: -1.1427e+02 - logprior: -1.4743e+00
Epoch 10/10
19/19 - 1s - loss: 115.8031 - loglik: -1.1414e+02 - logprior: -1.4656e+00
Fitted a model with MAP estimate = -115.4537
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (19, 2), (20, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 118.6184 - loglik: -1.1533e+02 - logprior: -3.2117e+00
Epoch 2/2
19/19 - 1s - loss: 110.5601 - loglik: -1.0893e+02 - logprior: -1.3768e+00
Fitted a model with MAP estimate = -109.2788
expansions: []
discards: [ 0 24 35 42]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 117.1537 - loglik: -1.1289e+02 - logprior: -4.2075e+00
Epoch 2/2
19/19 - 1s - loss: 113.2575 - loglik: -1.1072e+02 - logprior: -2.2521e+00
Fitted a model with MAP estimate = -111.9388
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 113.2303 - loglik: -1.1013e+02 - logprior: -3.0359e+00
Epoch 2/10
19/19 - 1s - loss: 109.9424 - loglik: -1.0840e+02 - logprior: -1.2387e+00
Epoch 3/10
19/19 - 1s - loss: 109.0244 - loglik: -1.0735e+02 - logprior: -1.1981e+00
Epoch 4/10
19/19 - 1s - loss: 108.4607 - loglik: -1.0687e+02 - logprior: -1.1632e+00
Epoch 5/10
19/19 - 1s - loss: 108.1478 - loglik: -1.0664e+02 - logprior: -1.1295e+00
Epoch 6/10
19/19 - 1s - loss: 107.8771 - loglik: -1.0641e+02 - logprior: -1.1140e+00
Epoch 7/10
19/19 - 1s - loss: 107.6916 - loglik: -1.0626e+02 - logprior: -1.1014e+00
Epoch 8/10
19/19 - 1s - loss: 107.4307 - loglik: -1.0602e+02 - logprior: -1.0909e+00
Epoch 9/10
19/19 - 1s - loss: 107.2548 - loglik: -1.0587e+02 - logprior: -1.0763e+00
Epoch 10/10
19/19 - 1s - loss: 107.2739 - loglik: -1.0590e+02 - logprior: -1.0702e+00
Fitted a model with MAP estimate = -106.7329
Time for alignment: 54.0597
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 150.0115 - loglik: -1.4676e+02 - logprior: -3.2270e+00
Epoch 2/10
19/19 - 1s - loss: 128.9682 - loglik: -1.2725e+02 - logprior: -1.4409e+00
Epoch 3/10
19/19 - 1s - loss: 119.3467 - loglik: -1.1717e+02 - logprior: -1.6936e+00
Epoch 4/10
19/19 - 1s - loss: 117.2148 - loglik: -1.1531e+02 - logprior: -1.5602e+00
Epoch 5/10
19/19 - 1s - loss: 116.6005 - loglik: -1.1480e+02 - logprior: -1.5293e+00
Epoch 6/10
19/19 - 1s - loss: 116.3924 - loglik: -1.1465e+02 - logprior: -1.5005e+00
Epoch 7/10
19/19 - 1s - loss: 116.2248 - loglik: -1.1452e+02 - logprior: -1.4816e+00
Epoch 8/10
19/19 - 1s - loss: 116.0337 - loglik: -1.1435e+02 - logprior: -1.4747e+00
Epoch 9/10
19/19 - 1s - loss: 115.9768 - loglik: -1.1431e+02 - logprior: -1.4672e+00
Epoch 10/10
19/19 - 1s - loss: 115.8996 - loglik: -1.1423e+02 - logprior: -1.4650e+00
Fitted a model with MAP estimate = -115.5199
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (19, 2), (26, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 122.6033 - loglik: -1.1835e+02 - logprior: -4.1782e+00
Epoch 2/2
19/19 - 1s - loss: 113.5015 - loglik: -1.1109e+02 - logprior: -2.1255e+00
Fitted a model with MAP estimate = -111.4916
expansions: [(0, 2)]
discards: [ 0 22 34 41]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 113.5947 - loglik: -1.1048e+02 - logprior: -3.0566e+00
Epoch 2/2
19/19 - 1s - loss: 110.0672 - loglik: -1.0850e+02 - logprior: -1.2588e+00
Fitted a model with MAP estimate = -108.7915
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.2225 - loglik: -1.1139e+02 - logprior: -3.7867e+00
Epoch 2/10
19/19 - 1s - loss: 110.5926 - loglik: -1.0898e+02 - logprior: -1.4570e+00
Epoch 3/10
19/19 - 1s - loss: 109.6090 - loglik: -1.0801e+02 - logprior: -1.2971e+00
Epoch 4/10
19/19 - 1s - loss: 108.9881 - loglik: -1.0733e+02 - logprior: -1.2697e+00
Epoch 5/10
19/19 - 1s - loss: 108.7640 - loglik: -1.0713e+02 - logprior: -1.2512e+00
Epoch 6/10
19/19 - 1s - loss: 108.3977 - loglik: -1.0680e+02 - logprior: -1.2416e+00
Epoch 7/10
19/19 - 1s - loss: 108.3157 - loglik: -1.0675e+02 - logprior: -1.2306e+00
Epoch 8/10
19/19 - 1s - loss: 108.0056 - loglik: -1.0647e+02 - logprior: -1.2162e+00
Epoch 9/10
19/19 - 1s - loss: 107.9281 - loglik: -1.0642e+02 - logprior: -1.2020e+00
Epoch 10/10
19/19 - 1s - loss: 107.7336 - loglik: -1.0623e+02 - logprior: -1.1997e+00
Fitted a model with MAP estimate = -107.2979
Time for alignment: 53.4248
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 149.8094 - loglik: -1.4656e+02 - logprior: -3.2253e+00
Epoch 2/10
19/19 - 1s - loss: 127.2325 - loglik: -1.2549e+02 - logprior: -1.4133e+00
Epoch 3/10
19/19 - 1s - loss: 119.5925 - loglik: -1.1753e+02 - logprior: -1.6280e+00
Epoch 4/10
19/19 - 1s - loss: 117.5497 - loglik: -1.1575e+02 - logprior: -1.5254e+00
Epoch 5/10
19/19 - 1s - loss: 117.0680 - loglik: -1.1533e+02 - logprior: -1.5147e+00
Epoch 6/10
19/19 - 1s - loss: 116.7326 - loglik: -1.1503e+02 - logprior: -1.4895e+00
Epoch 7/10
19/19 - 1s - loss: 116.5706 - loglik: -1.1488e+02 - logprior: -1.4794e+00
Epoch 8/10
19/19 - 1s - loss: 116.6202 - loglik: -1.1495e+02 - logprior: -1.4708e+00
Fitted a model with MAP estimate = -116.0629
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (18, 2), (19, 2), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 122.7473 - loglik: -1.1851e+02 - logprior: -4.1784e+00
Epoch 2/2
19/19 - 1s - loss: 113.5081 - loglik: -1.1112e+02 - logprior: -2.1582e+00
Fitted a model with MAP estimate = -111.4824
expansions: [(0, 2)]
discards: [ 0 22 24 35 42]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 113.6779 - loglik: -1.1056e+02 - logprior: -3.0610e+00
Epoch 2/2
19/19 - 1s - loss: 110.0350 - loglik: -1.0859e+02 - logprior: -1.2603e+00
Fitted a model with MAP estimate = -108.9813
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.1652 - loglik: -1.1134e+02 - logprior: -3.7826e+00
Epoch 2/10
19/19 - 1s - loss: 110.4667 - loglik: -1.0885e+02 - logprior: -1.4538e+00
Epoch 3/10
19/19 - 1s - loss: 109.5779 - loglik: -1.0796e+02 - logprior: -1.3055e+00
Epoch 4/10
19/19 - 1s - loss: 109.2161 - loglik: -1.0756e+02 - logprior: -1.2686e+00
Epoch 5/10
19/19 - 1s - loss: 108.6212 - loglik: -1.0697e+02 - logprior: -1.2574e+00
Epoch 6/10
19/19 - 1s - loss: 108.3913 - loglik: -1.0679e+02 - logprior: -1.2442e+00
Epoch 7/10
19/19 - 1s - loss: 108.1994 - loglik: -1.0663e+02 - logprior: -1.2363e+00
Epoch 8/10
19/19 - 1s - loss: 108.1078 - loglik: -1.0657e+02 - logprior: -1.2177e+00
Epoch 9/10
19/19 - 1s - loss: 107.7646 - loglik: -1.0624e+02 - logprior: -1.2117e+00
Epoch 10/10
19/19 - 1s - loss: 107.7749 - loglik: -1.0627e+02 - logprior: -1.2040e+00
Fitted a model with MAP estimate = -107.3035
Time for alignment: 50.3636
Computed alignments with likelihoods: ['-106.7329', '-107.2979', '-107.3035']
Best model has likelihood: -106.7329
time for generating output: 0.1203
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01381.projection.fasta
SP score = 0.6646868095607013
Training of 3 independent models on file PF02878.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd24d5fe220>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd3e934f190>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd222e67850>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd255e5a6d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd244351af0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd3e935ba60>, <__main__.SimpleDirichletPrior object at 0x7fd2228a2820>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 360.0997 - loglik: -3.5696e+02 - logprior: -3.0839e+00
Epoch 2/10
19/19 - 4s - loss: 303.6052 - loglik: -3.0206e+02 - logprior: -1.2639e+00
Epoch 3/10
19/19 - 4s - loss: 276.6708 - loglik: -2.7423e+02 - logprior: -1.4851e+00
Epoch 4/10
19/19 - 4s - loss: 270.8547 - loglik: -2.6855e+02 - logprior: -1.4509e+00
Epoch 5/10
19/19 - 4s - loss: 268.5877 - loglik: -2.6646e+02 - logprior: -1.4065e+00
Epoch 6/10
19/19 - 4s - loss: 267.6209 - loglik: -2.6559e+02 - logprior: -1.3941e+00
Epoch 7/10
19/19 - 4s - loss: 267.1846 - loglik: -2.6525e+02 - logprior: -1.3792e+00
Epoch 8/10
19/19 - 4s - loss: 266.8517 - loglik: -2.6495e+02 - logprior: -1.3622e+00
Epoch 9/10
19/19 - 4s - loss: 266.1959 - loglik: -2.6430e+02 - logprior: -1.3713e+00
Epoch 10/10
19/19 - 4s - loss: 266.0999 - loglik: -2.6422e+02 - logprior: -1.3712e+00
Fitted a model with MAP estimate = -265.4545
expansions: [(12, 2), (13, 4), (16, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (56, 1), (69, 1), (70, 1), (71, 1), (93, 1), (97, 1), (101, 1), (107, 1), (108, 1), (109, 1), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 273.6058 - loglik: -2.6950e+02 - logprior: -4.0086e+00
Epoch 2/2
19/19 - 5s - loss: 259.8721 - loglik: -2.5739e+02 - logprior: -2.0639e+00
Fitted a model with MAP estimate = -256.7230
expansions: [(0, 3)]
discards: [  0  13  17  38 137]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 261.6553 - loglik: -2.5870e+02 - logprior: -2.8681e+00
Epoch 2/2
19/19 - 5s - loss: 256.9376 - loglik: -2.5542e+02 - logprior: -1.1218e+00
Fitted a model with MAP estimate = -255.0660
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 262.3035 - loglik: -2.5837e+02 - logprior: -3.8410e+00
Epoch 2/10
19/19 - 4s - loss: 258.1400 - loglik: -2.5613e+02 - logprior: -1.5894e+00
Epoch 3/10
19/19 - 4s - loss: 255.2607 - loglik: -2.5371e+02 - logprior: -8.8438e-01
Epoch 4/10
19/19 - 5s - loss: 254.8344 - loglik: -2.5334e+02 - logprior: -8.1900e-01
Epoch 5/10
19/19 - 5s - loss: 253.6973 - loglik: -2.5229e+02 - logprior: -8.0724e-01
Epoch 6/10
19/19 - 5s - loss: 253.8942 - loglik: -2.5261e+02 - logprior: -7.2990e-01
Fitted a model with MAP estimate = -252.6335
Time for alignment: 121.9525
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 360.3502 - loglik: -3.5721e+02 - logprior: -3.0789e+00
Epoch 2/10
19/19 - 4s - loss: 301.4185 - loglik: -2.9985e+02 - logprior: -1.3024e+00
Epoch 3/10
19/19 - 4s - loss: 274.9586 - loglik: -2.7245e+02 - logprior: -1.5687e+00
Epoch 4/10
19/19 - 4s - loss: 269.4101 - loglik: -2.6697e+02 - logprior: -1.5732e+00
Epoch 5/10
19/19 - 4s - loss: 267.9812 - loglik: -2.6578e+02 - logprior: -1.4948e+00
Epoch 6/10
19/19 - 4s - loss: 265.8085 - loglik: -2.6376e+02 - logprior: -1.4495e+00
Epoch 7/10
19/19 - 4s - loss: 266.5256 - loglik: -2.6455e+02 - logprior: -1.4550e+00
Fitted a model with MAP estimate = -265.0367
expansions: [(12, 1), (14, 3), (16, 1), (17, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (97, 1), (102, 1), (103, 1), (107, 1), (108, 1), (109, 1), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 271.5615 - loglik: -2.6744e+02 - logprior: -4.0210e+00
Epoch 2/2
19/19 - 5s - loss: 259.0304 - loglik: -2.5659e+02 - logprior: -2.0262e+00
Fitted a model with MAP estimate = -256.2722
expansions: [(0, 3)]
discards: [  0  16  37  75 137]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 260.9159 - loglik: -2.5795e+02 - logprior: -2.8794e+00
Epoch 2/2
19/19 - 5s - loss: 256.2483 - loglik: -2.5471e+02 - logprior: -1.1317e+00
Fitted a model with MAP estimate = -254.5242
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 261.9889 - loglik: -2.5805e+02 - logprior: -3.8574e+00
Epoch 2/10
19/19 - 5s - loss: 257.4489 - loglik: -2.5561e+02 - logprior: -1.5535e+00
Epoch 3/10
19/19 - 5s - loss: 255.2917 - loglik: -2.5397e+02 - logprior: -8.9634e-01
Epoch 4/10
19/19 - 5s - loss: 254.4365 - loglik: -2.5306e+02 - logprior: -8.3036e-01
Epoch 5/10
19/19 - 5s - loss: 253.2306 - loglik: -2.5183e+02 - logprior: -8.1771e-01
Epoch 6/10
19/19 - 5s - loss: 252.7874 - loglik: -2.5147e+02 - logprior: -7.5464e-01
Epoch 7/10
19/19 - 5s - loss: 253.4239 - loglik: -2.5217e+02 - logprior: -7.2576e-01
Fitted a model with MAP estimate = -252.0764
Time for alignment: 117.3475
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 360.3479 - loglik: -3.5721e+02 - logprior: -3.0826e+00
Epoch 2/10
19/19 - 4s - loss: 303.1300 - loglik: -3.0156e+02 - logprior: -1.3404e+00
Epoch 3/10
19/19 - 4s - loss: 276.2661 - loglik: -2.7387e+02 - logprior: -1.6394e+00
Epoch 4/10
19/19 - 4s - loss: 271.0554 - loglik: -2.6869e+02 - logprior: -1.5831e+00
Epoch 5/10
19/19 - 4s - loss: 269.7289 - loglik: -2.6750e+02 - logprior: -1.5248e+00
Epoch 6/10
19/19 - 4s - loss: 268.8701 - loglik: -2.6675e+02 - logprior: -1.4864e+00
Epoch 7/10
19/19 - 4s - loss: 267.1764 - loglik: -2.6512e+02 - logprior: -1.4758e+00
Epoch 8/10
19/19 - 4s - loss: 267.0940 - loglik: -2.6507e+02 - logprior: -1.4905e+00
Epoch 9/10
19/19 - 4s - loss: 266.4466 - loglik: -2.6443e+02 - logprior: -1.4948e+00
Epoch 10/10
19/19 - 4s - loss: 266.4711 - loglik: -2.6447e+02 - logprior: -1.4879e+00
Fitted a model with MAP estimate = -265.5581
expansions: [(12, 1), (14, 3), (16, 1), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (41, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (93, 1), (97, 1), (101, 1), (105, 1), (108, 1), (109, 1), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 274.0379 - loglik: -2.6989e+02 - logprior: -4.0489e+00
Epoch 2/2
19/19 - 5s - loss: 259.4733 - loglik: -2.5699e+02 - logprior: -2.0672e+00
Fitted a model with MAP estimate = -256.6452
expansions: [(0, 2)]
discards: [  0  16  37  75 137]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 261.0016 - loglik: -2.5807e+02 - logprior: -2.8480e+00
Epoch 2/2
19/19 - 5s - loss: 256.6929 - loglik: -2.5521e+02 - logprior: -1.0913e+00
Fitted a model with MAP estimate = -254.9235
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 262.0507 - loglik: -2.5815e+02 - logprior: -3.8156e+00
Epoch 2/10
19/19 - 5s - loss: 257.4652 - loglik: -2.5570e+02 - logprior: -1.4169e+00
Epoch 3/10
19/19 - 5s - loss: 255.5625 - loglik: -2.5425e+02 - logprior: -8.7508e-01
Epoch 4/10
19/19 - 5s - loss: 254.2507 - loglik: -2.5282e+02 - logprior: -8.9653e-01
Epoch 5/10
19/19 - 5s - loss: 253.3573 - loglik: -2.5193e+02 - logprior: -8.4587e-01
Epoch 6/10
19/19 - 5s - loss: 253.4046 - loglik: -2.5204e+02 - logprior: -7.8593e-01
Fitted a model with MAP estimate = -252.2657
Time for alignment: 125.4329
Computed alignments with likelihoods: ['-252.6335', '-252.0764', '-252.2657']
Best model has likelihood: -252.0764
time for generating output: 0.1906
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02878.projection.fasta
SP score = 0.8278145695364238
Training of 3 independent models on file PF00970.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd233ad9a30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd3cf987070>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2338e2af0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd3cfaa9310>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd3cfaa91f0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd3e98acc70>, <__main__.SimpleDirichletPrior object at 0x7fd070569e20>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 274.6588 - loglik: -2.7147e+02 - logprior: -3.1271e+00
Epoch 2/10
19/19 - 3s - loss: 235.7267 - loglik: -2.3405e+02 - logprior: -1.3254e+00
Epoch 3/10
19/19 - 3s - loss: 219.9456 - loglik: -2.1775e+02 - logprior: -1.5663e+00
Epoch 4/10
19/19 - 3s - loss: 216.6942 - loglik: -2.1465e+02 - logprior: -1.5769e+00
Epoch 5/10
19/19 - 3s - loss: 215.3212 - loglik: -2.1340e+02 - logprior: -1.5238e+00
Epoch 6/10
19/19 - 3s - loss: 215.2049 - loglik: -2.1338e+02 - logprior: -1.4837e+00
Epoch 7/10
19/19 - 3s - loss: 214.6086 - loglik: -2.1281e+02 - logprior: -1.4635e+00
Epoch 8/10
19/19 - 3s - loss: 214.3792 - loglik: -2.1259e+02 - logprior: -1.4573e+00
Epoch 9/10
19/19 - 3s - loss: 214.6465 - loglik: -2.1286e+02 - logprior: -1.4566e+00
Fitted a model with MAP estimate = -213.6078
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (16, 1), (17, 1), (18, 2), (27, 1), (28, 1), (29, 1), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 219.0831 - loglik: -2.1501e+02 - logprior: -3.9950e+00
Epoch 2/2
19/19 - 4s - loss: 206.7564 - loglik: -2.0509e+02 - logprior: -1.3748e+00
Fitted a model with MAP estimate = -204.5087
expansions: []
discards: [ 0 25 75 80 83 85 96]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 212.0623 - loglik: -2.0796e+02 - logprior: -4.0331e+00
Epoch 2/2
19/19 - 3s - loss: 207.1960 - loglik: -2.0545e+02 - logprior: -1.4945e+00
Fitted a model with MAP estimate = -205.2566
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 209.3867 - loglik: -2.0631e+02 - logprior: -3.0001e+00
Epoch 2/10
19/19 - 3s - loss: 206.2432 - loglik: -2.0477e+02 - logprior: -1.1697e+00
Epoch 3/10
19/19 - 3s - loss: 204.9497 - loglik: -2.0332e+02 - logprior: -1.1013e+00
Epoch 4/10
19/19 - 3s - loss: 204.3623 - loglik: -2.0278e+02 - logprior: -1.0616e+00
Epoch 5/10
19/19 - 3s - loss: 203.7863 - loglik: -2.0228e+02 - logprior: -1.0491e+00
Epoch 6/10
19/19 - 3s - loss: 203.6185 - loglik: -2.0221e+02 - logprior: -1.0025e+00
Epoch 7/10
19/19 - 3s - loss: 203.1882 - loglik: -2.0181e+02 - logprior: -1.0068e+00
Epoch 8/10
19/19 - 3s - loss: 203.2820 - loglik: -2.0194e+02 - logprior: -9.8522e-01
Fitted a model with MAP estimate = -202.6549
Time for alignment: 99.7088
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 274.6931 - loglik: -2.7151e+02 - logprior: -3.1262e+00
Epoch 2/10
19/19 - 3s - loss: 236.6033 - loglik: -2.3496e+02 - logprior: -1.3154e+00
Epoch 3/10
19/19 - 3s - loss: 220.6797 - loglik: -2.1856e+02 - logprior: -1.5863e+00
Epoch 4/10
19/19 - 3s - loss: 216.9074 - loglik: -2.1480e+02 - logprior: -1.6226e+00
Epoch 5/10
19/19 - 3s - loss: 215.4230 - loglik: -2.1347e+02 - logprior: -1.5410e+00
Epoch 6/10
19/19 - 3s - loss: 215.4620 - loglik: -2.1360e+02 - logprior: -1.4858e+00
Fitted a model with MAP estimate = -214.3448
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (18, 2), (21, 1), (28, 1), (29, 1), (35, 1), (46, 1), (59, 1), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 218.2958 - loglik: -2.1437e+02 - logprior: -3.8805e+00
Epoch 2/2
19/19 - 4s - loss: 206.8882 - loglik: -2.0533e+02 - logprior: -1.3879e+00
Fitted a model with MAP estimate = -204.8132
expansions: []
discards: [ 0 77 80 82 93]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 212.1120 - loglik: -2.0799e+02 - logprior: -4.0628e+00
Epoch 2/2
19/19 - 3s - loss: 207.1003 - loglik: -2.0534e+02 - logprior: -1.5820e+00
Fitted a model with MAP estimate = -205.3632
expansions: [(0, 2), (24, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 208.9364 - loglik: -2.0593e+02 - logprior: -2.9540e+00
Epoch 2/10
19/19 - 3s - loss: 205.5606 - loglik: -2.0421e+02 - logprior: -1.1688e+00
Epoch 3/10
19/19 - 3s - loss: 204.5205 - loglik: -2.0303e+02 - logprior: -1.1527e+00
Epoch 4/10
19/19 - 3s - loss: 203.8542 - loglik: -2.0232e+02 - logprior: -1.0972e+00
Epoch 5/10
19/19 - 3s - loss: 203.8215 - loglik: -2.0231e+02 - logprior: -1.0617e+00
Epoch 6/10
19/19 - 3s - loss: 202.9717 - loglik: -2.0152e+02 - logprior: -1.0448e+00
Epoch 7/10
19/19 - 3s - loss: 202.8415 - loglik: -2.0144e+02 - logprior: -1.0222e+00
Epoch 8/10
19/19 - 3s - loss: 203.1410 - loglik: -2.0177e+02 - logprior: -1.0077e+00
Fitted a model with MAP estimate = -202.4083
Time for alignment: 90.6278
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 274.5807 - loglik: -2.7140e+02 - logprior: -3.1237e+00
Epoch 2/10
19/19 - 3s - loss: 235.6387 - loglik: -2.3391e+02 - logprior: -1.3376e+00
Epoch 3/10
19/19 - 3s - loss: 220.0154 - loglik: -2.1769e+02 - logprior: -1.5795e+00
Epoch 4/10
19/19 - 3s - loss: 216.4816 - loglik: -2.1442e+02 - logprior: -1.5652e+00
Epoch 5/10
19/19 - 3s - loss: 215.4224 - loglik: -2.1349e+02 - logprior: -1.5307e+00
Epoch 6/10
19/19 - 3s - loss: 215.1146 - loglik: -2.1329e+02 - logprior: -1.4642e+00
Epoch 7/10
19/19 - 3s - loss: 214.7769 - loglik: -2.1298e+02 - logprior: -1.4450e+00
Epoch 8/10
19/19 - 3s - loss: 214.5575 - loglik: -2.1277e+02 - logprior: -1.4440e+00
Epoch 9/10
19/19 - 3s - loss: 214.2337 - loglik: -2.1247e+02 - logprior: -1.4327e+00
Epoch 10/10
19/19 - 3s - loss: 214.3835 - loglik: -2.1263e+02 - logprior: -1.4323e+00
Fitted a model with MAP estimate = -213.4946
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (18, 2), (21, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 1), (70, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 219.3165 - loglik: -2.1520e+02 - logprior: -4.0428e+00
Epoch 2/2
19/19 - 3s - loss: 207.0138 - loglik: -2.0528e+02 - logprior: -1.4143e+00
Fitted a model with MAP estimate = -204.8238
expansions: []
discards: [ 0 45 74 81 94]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 211.7358 - loglik: -2.0759e+02 - logprior: -4.0738e+00
Epoch 2/2
19/19 - 3s - loss: 206.9395 - loglik: -2.0507e+02 - logprior: -1.5663e+00
Fitted a model with MAP estimate = -204.9523
expansions: [(0, 2), (24, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 208.7180 - loglik: -2.0569e+02 - logprior: -2.9569e+00
Epoch 2/10
19/19 - 3s - loss: 205.3224 - loglik: -2.0392e+02 - logprior: -1.1642e+00
Epoch 3/10
19/19 - 3s - loss: 204.1909 - loglik: -2.0271e+02 - logprior: -1.1532e+00
Epoch 4/10
19/19 - 3s - loss: 203.5798 - loglik: -2.0204e+02 - logprior: -1.1054e+00
Epoch 5/10
19/19 - 3s - loss: 203.2348 - loglik: -2.0172e+02 - logprior: -1.0705e+00
Epoch 6/10
19/19 - 3s - loss: 202.7003 - loglik: -2.0124e+02 - logprior: -1.0521e+00
Epoch 7/10
19/19 - 3s - loss: 202.8045 - loglik: -2.0140e+02 - logprior: -1.0288e+00
Fitted a model with MAP estimate = -202.1757
Time for alignment: 98.9087
Computed alignments with likelihoods: ['-202.6549', '-202.4083', '-202.1757']
Best model has likelihood: -202.1757
time for generating output: 0.2145
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00970.projection.fasta
SP score = 0.6617202311095062
Training of 3 independent models on file PF00313.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd1efc5de50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd208a43910>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd22257cdc0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd3c6937d60>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd222d070d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd1f8749640>, <__main__.SimpleDirichletPrior object at 0x7fd24ce899d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 176.3229 - loglik: -1.7305e+02 - logprior: -3.2555e+00
Epoch 2/10
19/19 - 2s - loss: 132.5795 - loglik: -1.3107e+02 - logprior: -1.4796e+00
Epoch 3/10
19/19 - 2s - loss: 117.4168 - loglik: -1.1577e+02 - logprior: -1.4876e+00
Epoch 4/10
19/19 - 1s - loss: 114.4414 - loglik: -1.1267e+02 - logprior: -1.5247e+00
Epoch 5/10
19/19 - 2s - loss: 113.8171 - loglik: -1.1215e+02 - logprior: -1.4716e+00
Epoch 6/10
19/19 - 2s - loss: 112.9941 - loglik: -1.1137e+02 - logprior: -1.4335e+00
Epoch 7/10
19/19 - 2s - loss: 113.2025 - loglik: -1.1159e+02 - logprior: -1.4192e+00
Fitted a model with MAP estimate = -112.6848
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.1591 - loglik: -1.0997e+02 - logprior: -4.1394e+00
Epoch 2/2
19/19 - 2s - loss: 104.7210 - loglik: -1.0326e+02 - logprior: -1.3584e+00
Fitted a model with MAP estimate = -103.2958
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 109.4454 - loglik: -1.0528e+02 - logprior: -4.1150e+00
Epoch 2/2
19/19 - 1s - loss: 105.0416 - loglik: -1.0332e+02 - logprior: -1.5866e+00
Fitted a model with MAP estimate = -103.8017
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 107.5535 - loglik: -1.0455e+02 - logprior: -2.9522e+00
Epoch 2/10
19/19 - 2s - loss: 104.0716 - loglik: -1.0284e+02 - logprior: -1.0965e+00
Epoch 3/10
19/19 - 2s - loss: 103.1227 - loglik: -1.0160e+02 - logprior: -1.3182e+00
Epoch 4/10
19/19 - 2s - loss: 102.4471 - loglik: -1.0100e+02 - logprior: -1.1887e+00
Epoch 5/10
19/19 - 2s - loss: 102.0544 - loglik: -1.0064e+02 - logprior: -1.1544e+00
Epoch 6/10
19/19 - 2s - loss: 101.3757 - loglik: -9.9986e+01 - logprior: -1.1460e+00
Epoch 7/10
19/19 - 2s - loss: 101.4604 - loglik: -1.0010e+02 - logprior: -1.1225e+00
Fitted a model with MAP estimate = -101.0528
Time for alignment: 52.7241
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.4829 - loglik: -1.7322e+02 - logprior: -3.2509e+00
Epoch 2/10
19/19 - 2s - loss: 134.4339 - loglik: -1.3291e+02 - logprior: -1.4928e+00
Epoch 3/10
19/19 - 1s - loss: 117.0952 - loglik: -1.1540e+02 - logprior: -1.4988e+00
Epoch 4/10
19/19 - 1s - loss: 114.5271 - loglik: -1.1273e+02 - logprior: -1.5196e+00
Epoch 5/10
19/19 - 1s - loss: 113.5321 - loglik: -1.1186e+02 - logprior: -1.4749e+00
Epoch 6/10
19/19 - 1s - loss: 113.2432 - loglik: -1.1162e+02 - logprior: -1.4445e+00
Epoch 7/10
19/19 - 2s - loss: 112.6795 - loglik: -1.1107e+02 - logprior: -1.4285e+00
Epoch 8/10
19/19 - 1s - loss: 112.8149 - loglik: -1.1120e+02 - logprior: -1.4208e+00
Fitted a model with MAP estimate = -112.3652
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (36, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 114.3568 - loglik: -1.1013e+02 - logprior: -4.1694e+00
Epoch 2/2
19/19 - 2s - loss: 104.6858 - loglik: -1.0321e+02 - logprior: -1.3603e+00
Fitted a model with MAP estimate = -103.3189
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 109.3351 - loglik: -1.0518e+02 - logprior: -4.1039e+00
Epoch 2/2
19/19 - 2s - loss: 105.1002 - loglik: -1.0339e+02 - logprior: -1.5794e+00
Fitted a model with MAP estimate = -103.7976
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 107.6524 - loglik: -1.0437e+02 - logprior: -3.2296e+00
Epoch 2/10
19/19 - 2s - loss: 104.4909 - loglik: -1.0290e+02 - logprior: -1.4499e+00
Epoch 3/10
19/19 - 2s - loss: 103.4197 - loglik: -1.0184e+02 - logprior: -1.3584e+00
Epoch 4/10
19/19 - 2s - loss: 102.7685 - loglik: -1.0118e+02 - logprior: -1.3202e+00
Epoch 5/10
19/19 - 2s - loss: 102.2924 - loglik: -1.0073e+02 - logprior: -1.2874e+00
Epoch 6/10
19/19 - 2s - loss: 102.0322 - loglik: -1.0051e+02 - logprior: -1.2668e+00
Epoch 7/10
19/19 - 1s - loss: 101.7823 - loglik: -1.0028e+02 - logprior: -1.2453e+00
Epoch 8/10
19/19 - 2s - loss: 101.6057 - loglik: -1.0011e+02 - logprior: -1.2305e+00
Epoch 9/10
19/19 - 2s - loss: 101.4618 - loglik: -9.9981e+01 - logprior: -1.2135e+00
Epoch 10/10
19/19 - 2s - loss: 101.2713 - loglik: -9.9799e+01 - logprior: -1.1995e+00
Fitted a model with MAP estimate = -100.9396
Time for alignment: 58.9654
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.2475 - loglik: -1.7298e+02 - logprior: -3.2533e+00
Epoch 2/10
19/19 - 1s - loss: 133.5561 - loglik: -1.3204e+02 - logprior: -1.4902e+00
Epoch 3/10
19/19 - 1s - loss: 117.2213 - loglik: -1.1555e+02 - logprior: -1.5049e+00
Epoch 4/10
19/19 - 1s - loss: 114.3600 - loglik: -1.1262e+02 - logprior: -1.5306e+00
Epoch 5/10
19/19 - 1s - loss: 113.5624 - loglik: -1.1190e+02 - logprior: -1.4757e+00
Epoch 6/10
19/19 - 1s - loss: 113.2852 - loglik: -1.1166e+02 - logprior: -1.4439e+00
Epoch 7/10
19/19 - 1s - loss: 112.6187 - loglik: -1.1101e+02 - logprior: -1.4281e+00
Epoch 8/10
19/19 - 2s - loss: 112.5645 - loglik: -1.1095e+02 - logprior: -1.4197e+00
Epoch 9/10
19/19 - 2s - loss: 112.5339 - loglik: -1.1093e+02 - logprior: -1.4135e+00
Epoch 10/10
19/19 - 2s - loss: 112.6226 - loglik: -1.1101e+02 - logprior: -1.4097e+00
Fitted a model with MAP estimate = -112.1833
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (36, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.3815 - loglik: -1.1008e+02 - logprior: -4.2414e+00
Epoch 2/2
19/19 - 2s - loss: 105.1232 - loglik: -1.0363e+02 - logprior: -1.3564e+00
Fitted a model with MAP estimate = -103.3327
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 109.3088 - loglik: -1.0516e+02 - logprior: -4.0968e+00
Epoch 2/2
19/19 - 2s - loss: 105.0762 - loglik: -1.0337e+02 - logprior: -1.5737e+00
Fitted a model with MAP estimate = -103.8074
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 107.5275 - loglik: -1.0425e+02 - logprior: -3.2297e+00
Epoch 2/10
19/19 - 2s - loss: 104.6331 - loglik: -1.0304e+02 - logprior: -1.4483e+00
Epoch 3/10
19/19 - 1s - loss: 103.4138 - loglik: -1.0184e+02 - logprior: -1.3555e+00
Epoch 4/10
19/19 - 1s - loss: 102.6443 - loglik: -1.0105e+02 - logprior: -1.3193e+00
Epoch 5/10
19/19 - 2s - loss: 102.4266 - loglik: -1.0087e+02 - logprior: -1.2869e+00
Epoch 6/10
19/19 - 1s - loss: 101.9729 - loglik: -1.0044e+02 - logprior: -1.2676e+00
Epoch 7/10
19/19 - 2s - loss: 101.7530 - loglik: -1.0025e+02 - logprior: -1.2484e+00
Epoch 8/10
19/19 - 2s - loss: 101.5343 - loglik: -1.0004e+02 - logprior: -1.2327e+00
Epoch 9/10
19/19 - 2s - loss: 101.4865 - loglik: -1.0001e+02 - logprior: -1.2094e+00
Epoch 10/10
19/19 - 2s - loss: 101.0426 - loglik: -9.9568e+01 - logprior: -1.1974e+00
Fitted a model with MAP estimate = -100.9616
Time for alignment: 61.0451
Computed alignments with likelihoods: ['-101.0528', '-100.9396', '-100.9616']
Best model has likelihood: -100.9396
time for generating output: 0.1215
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00313.projection.fasta
SP score = 0.8080495356037152
Training of 3 independent models on file PF00009.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd24caef7c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd068332580>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd0683321c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd24cb72490>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd24cb724c0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd1e7076f10>, <__main__.SimpleDirichletPrior object at 0x7fd25589db80>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 569.2427 - loglik: -5.6713e+02 - logprior: -1.9667e+00
Epoch 2/10
39/39 - 18s - loss: 480.8574 - loglik: -4.7862e+02 - logprior: -1.5088e+00
Epoch 3/10
39/39 - 18s - loss: 470.7901 - loglik: -4.6843e+02 - logprior: -1.4694e+00
Epoch 4/10
39/39 - 18s - loss: 468.7159 - loglik: -4.6651e+02 - logprior: -1.4144e+00
Epoch 5/10
39/39 - 18s - loss: 467.5603 - loglik: -4.6544e+02 - logprior: -1.4127e+00
Epoch 6/10
39/39 - 18s - loss: 467.3505 - loglik: -4.6524e+02 - logprior: -1.4115e+00
Epoch 7/10
39/39 - 18s - loss: 466.8848 - loglik: -4.6480e+02 - logprior: -1.4090e+00
Epoch 8/10
39/39 - 18s - loss: 466.4532 - loglik: -4.6437e+02 - logprior: -1.4098e+00
Epoch 9/10
39/39 - 18s - loss: 466.5430 - loglik: -4.6447e+02 - logprior: -1.4144e+00
Fitted a model with MAP estimate = -463.8572
expansions: [(25, 1), (30, 4), (55, 1), (56, 1), (57, 1), (75, 2), (77, 1), (80, 1), (99, 1), (105, 1), (106, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 186 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 472.0336 - loglik: -4.6958e+02 - logprior: -2.1834e+00
Epoch 2/2
39/39 - 20s - loss: 462.6148 - loglik: -4.6081e+02 - logprior: -1.2183e+00
Fitted a model with MAP estimate = -458.2897
expansions: []
discards: [ 0 33 83]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 470.8667 - loglik: -4.6763e+02 - logprior: -3.0086e+00
Epoch 2/2
39/39 - 20s - loss: 464.5270 - loglik: -4.6241e+02 - logprior: -1.5711e+00
Fitted a model with MAP estimate = -459.8309
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 465.8396 - loglik: -4.6368e+02 - logprior: -1.9500e+00
Epoch 2/10
39/39 - 20s - loss: 461.2038 - loglik: -4.5976e+02 - logprior: -9.1631e-01
Epoch 3/10
39/39 - 20s - loss: 458.5913 - loglik: -4.5696e+02 - logprior: -8.1586e-01
Epoch 4/10
39/39 - 20s - loss: 457.8545 - loglik: -4.5626e+02 - logprior: -6.7829e-01
Epoch 5/10
39/39 - 20s - loss: 456.6910 - loglik: -4.5524e+02 - logprior: -5.7764e-01
Epoch 6/10
39/39 - 20s - loss: 456.1651 - loglik: -4.5488e+02 - logprior: -4.8732e-01
Epoch 7/10
39/39 - 20s - loss: 455.9118 - loglik: -4.5474e+02 - logprior: -4.1103e-01
Epoch 8/10
39/39 - 20s - loss: 455.7173 - loglik: -4.5467e+02 - logprior: -3.2861e-01
Epoch 9/10
39/39 - 20s - loss: 455.6852 - loglik: -4.5475e+02 - logprior: -2.5132e-01
Epoch 10/10
39/39 - 20s - loss: 454.3038 - loglik: -4.5346e+02 - logprior: -1.6536e-01
Fitted a model with MAP estimate = -454.2001
Time for alignment: 538.6722
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 566.2325 - loglik: -5.6412e+02 - logprior: -1.9925e+00
Epoch 2/10
39/39 - 19s - loss: 481.4296 - loglik: -4.7908e+02 - logprior: -1.4541e+00
Epoch 3/10
39/39 - 18s - loss: 472.9540 - loglik: -4.7057e+02 - logprior: -1.4055e+00
Epoch 4/10
39/39 - 19s - loss: 470.5489 - loglik: -4.6821e+02 - logprior: -1.4020e+00
Epoch 5/10
39/39 - 19s - loss: 469.5539 - loglik: -4.6733e+02 - logprior: -1.3926e+00
Epoch 6/10
39/39 - 19s - loss: 468.9373 - loglik: -4.6678e+02 - logprior: -1.3927e+00
Epoch 7/10
39/39 - 19s - loss: 469.3082 - loglik: -4.6719e+02 - logprior: -1.3898e+00
Fitted a model with MAP estimate = -466.1742
expansions: [(25, 1), (30, 3), (54, 1), (56, 1), (75, 3), (102, 1), (103, 1), (106, 1), (107, 1), (135, 1), (138, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 472.6613 - loglik: -4.7026e+02 - logprior: -2.1434e+00
Epoch 2/2
39/39 - 20s - loss: 462.8063 - loglik: -4.6105e+02 - logprior: -1.1849e+00
Fitted a model with MAP estimate = -458.6042
expansions: [(152, 1)]
discards: [ 0 35 82]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 471.1432 - loglik: -4.6797e+02 - logprior: -2.9471e+00
Epoch 2/2
39/39 - 20s - loss: 465.2006 - loglik: -4.6325e+02 - logprior: -1.4045e+00
Fitted a model with MAP estimate = -459.9444
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 466.8696 - loglik: -4.6493e+02 - logprior: -1.7350e+00
Epoch 2/10
39/39 - 20s - loss: 462.0074 - loglik: -4.6058e+02 - logprior: -8.9808e-01
Epoch 3/10
39/39 - 20s - loss: 459.1844 - loglik: -4.5757e+02 - logprior: -7.9567e-01
Epoch 4/10
39/39 - 20s - loss: 458.4651 - loglik: -4.5684e+02 - logprior: -7.1953e-01
Epoch 5/10
39/39 - 21s - loss: 457.9197 - loglik: -4.5645e+02 - logprior: -6.3249e-01
Epoch 6/10
39/39 - 20s - loss: 457.0130 - loglik: -4.5570e+02 - logprior: -5.5142e-01
Epoch 7/10
39/39 - 20s - loss: 455.8396 - loglik: -4.5467e+02 - logprior: -4.6489e-01
Epoch 8/10
39/39 - 20s - loss: 457.7213 - loglik: -4.5667e+02 - logprior: -3.7958e-01
Fitted a model with MAP estimate = -455.5099
Time for alignment: 470.8690
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 567.3855 - loglik: -5.6525e+02 - logprior: -1.9962e+00
Epoch 2/10
39/39 - 19s - loss: 476.9019 - loglik: -4.7472e+02 - logprior: -1.5110e+00
Epoch 3/10
39/39 - 19s - loss: 469.0214 - loglik: -4.6671e+02 - logprior: -1.4062e+00
Epoch 4/10
39/39 - 19s - loss: 467.4303 - loglik: -4.6520e+02 - logprior: -1.3633e+00
Epoch 5/10
39/39 - 18s - loss: 466.5866 - loglik: -4.6441e+02 - logprior: -1.3644e+00
Epoch 6/10
39/39 - 18s - loss: 465.4091 - loglik: -4.6328e+02 - logprior: -1.3639e+00
Epoch 7/10
39/39 - 19s - loss: 465.5637 - loglik: -4.6348e+02 - logprior: -1.3708e+00
Fitted a model with MAP estimate = -462.6876
expansions: [(25, 1), (30, 1), (31, 1), (34, 1), (61, 1), (62, 1), (75, 1), (81, 1), (101, 1), (102, 1), (105, 1), (106, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 471.6307 - loglik: -4.6926e+02 - logprior: -2.1207e+00
Epoch 2/2
39/39 - 20s - loss: 463.0200 - loglik: -4.6135e+02 - logprior: -1.1235e+00
Fitted a model with MAP estimate = -458.9583
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 470.4035 - loglik: -4.6721e+02 - logprior: -2.9729e+00
Epoch 2/2
39/39 - 20s - loss: 464.4687 - loglik: -4.6246e+02 - logprior: -1.4763e+00
Fitted a model with MAP estimate = -459.5929
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 466.0278 - loglik: -4.6392e+02 - logprior: -1.8920e+00
Epoch 2/10
39/39 - 20s - loss: 460.9256 - loglik: -4.5956e+02 - logprior: -8.5200e-01
Epoch 3/10
39/39 - 20s - loss: 458.7372 - loglik: -4.5719e+02 - logprior: -7.2241e-01
Epoch 4/10
39/39 - 21s - loss: 457.2367 - loglik: -4.5570e+02 - logprior: -6.2280e-01
Epoch 5/10
39/39 - 20s - loss: 456.7930 - loglik: -4.5540e+02 - logprior: -5.4307e-01
Epoch 6/10
39/39 - 20s - loss: 456.2861 - loglik: -4.5506e+02 - logprior: -4.5410e-01
Epoch 7/10
39/39 - 20s - loss: 455.5524 - loglik: -4.5445e+02 - logprior: -3.7786e-01
Epoch 8/10
39/39 - 20s - loss: 455.0551 - loglik: -4.5407e+02 - logprior: -3.0192e-01
Epoch 9/10
39/39 - 20s - loss: 455.2679 - loglik: -4.5439e+02 - logprior: -2.2281e-01
Fitted a model with MAP estimate = -454.3395
Time for alignment: 487.1622
Computed alignments with likelihoods: ['-454.2001', '-455.5099', '-454.3395']
Best model has likelihood: -454.2001
time for generating output: 0.2949
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00009.projection.fasta
SP score = 0.7474590837755026
Training of 3 independent models on file PF14604.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd068246970>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd40b42b160>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd3cfeefb50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd0301bbf10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd0301bbd90>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd4281de3a0>, <__main__.SimpleDirichletPrior object at 0x7fd40b27adf0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.7467 - loglik: -1.3950e+02 - logprior: -3.2391e+00
Epoch 2/10
19/19 - 1s - loss: 114.2413 - loglik: -1.1273e+02 - logprior: -1.4532e+00
Epoch 3/10
19/19 - 1s - loss: 103.9759 - loglik: -1.0210e+02 - logprior: -1.6439e+00
Epoch 4/10
19/19 - 1s - loss: 101.7452 - loglik: -1.0001e+02 - logprior: -1.5153e+00
Epoch 5/10
19/19 - 1s - loss: 101.2890 - loglik: -9.9615e+01 - logprior: -1.5021e+00
Epoch 6/10
19/19 - 1s - loss: 100.7320 - loglik: -9.9057e+01 - logprior: -1.4873e+00
Epoch 7/10
19/19 - 1s - loss: 100.7377 - loglik: -9.9080e+01 - logprior: -1.4728e+00
Fitted a model with MAP estimate = -100.3683
expansions: [(6, 1), (7, 2), (8, 1), (13, 1), (20, 2), (21, 3), (26, 1), (28, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.3904 - loglik: -1.0312e+02 - logprior: -4.2063e+00
Epoch 2/2
19/19 - 1s - loss: 99.6323 - loglik: -9.7294e+01 - logprior: -2.1648e+00
Fitted a model with MAP estimate = -97.6314
expansions: [(0, 1)]
discards: [ 0  8 25 28 43]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 100.5460 - loglik: -9.7239e+01 - logprior: -3.2559e+00
Epoch 2/2
19/19 - 1s - loss: 96.7246 - loglik: -9.5069e+01 - logprior: -1.5228e+00
Fitted a model with MAP estimate = -96.0227
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.8118 - loglik: -9.6472e+01 - logprior: -3.2941e+00
Epoch 2/10
19/19 - 1s - loss: 96.9957 - loglik: -9.5388e+01 - logprior: -1.4835e+00
Epoch 3/10
19/19 - 1s - loss: 96.5140 - loglik: -9.4917e+01 - logprior: -1.4003e+00
Epoch 4/10
19/19 - 1s - loss: 96.0649 - loglik: -9.4490e+01 - logprior: -1.3551e+00
Epoch 5/10
19/19 - 1s - loss: 95.9572 - loglik: -9.4414e+01 - logprior: -1.3246e+00
Epoch 6/10
19/19 - 1s - loss: 95.7023 - loglik: -9.4186e+01 - logprior: -1.3085e+00
Epoch 7/10
19/19 - 1s - loss: 95.7297 - loglik: -9.4230e+01 - logprior: -1.2921e+00
Fitted a model with MAP estimate = -95.3691
Time for alignment: 47.1680
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 142.7596 - loglik: -1.3951e+02 - logprior: -3.2391e+00
Epoch 2/10
19/19 - 1s - loss: 114.3491 - loglik: -1.1284e+02 - logprior: -1.4503e+00
Epoch 3/10
19/19 - 1s - loss: 104.2690 - loglik: -1.0236e+02 - logprior: -1.6346e+00
Epoch 4/10
19/19 - 1s - loss: 101.7809 - loglik: -1.0002e+02 - logprior: -1.5147e+00
Epoch 5/10
19/19 - 1s - loss: 101.2924 - loglik: -9.9582e+01 - logprior: -1.5066e+00
Epoch 6/10
19/19 - 1s - loss: 100.8376 - loglik: -9.9129e+01 - logprior: -1.4873e+00
Epoch 7/10
19/19 - 1s - loss: 100.6965 - loglik: -9.9003e+01 - logprior: -1.4737e+00
Epoch 8/10
19/19 - 1s - loss: 100.7546 - loglik: -9.9053e+01 - logprior: -1.4642e+00
Fitted a model with MAP estimate = -100.3002
expansions: [(6, 1), (7, 2), (8, 1), (13, 1), (20, 2), (21, 3), (26, 1), (27, 2), (28, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 107.3941 - loglik: -1.0311e+02 - logprior: -4.2155e+00
Epoch 2/2
19/19 - 1s - loss: 99.7880 - loglik: -9.7402e+01 - logprior: -2.1694e+00
Fitted a model with MAP estimate = -97.7137
expansions: [(0, 1)]
discards: [ 0  8 25 28 38 40]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.0363 - loglik: -9.7743e+01 - logprior: -3.2370e+00
Epoch 2/2
19/19 - 1s - loss: 97.2675 - loglik: -9.5629e+01 - logprior: -1.5030e+00
Fitted a model with MAP estimate = -96.4915
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 99.6758 - loglik: -9.6339e+01 - logprior: -3.2899e+00
Epoch 2/10
19/19 - 1s - loss: 97.0119 - loglik: -9.5392e+01 - logprior: -1.4905e+00
Epoch 3/10
19/19 - 1s - loss: 96.4265 - loglik: -9.4823e+01 - logprior: -1.4019e+00
Epoch 4/10
19/19 - 1s - loss: 96.0969 - loglik: -9.4509e+01 - logprior: -1.3583e+00
Epoch 5/10
19/19 - 1s - loss: 95.9439 - loglik: -9.4391e+01 - logprior: -1.3263e+00
Epoch 6/10
19/19 - 1s - loss: 95.6765 - loglik: -9.4139e+01 - logprior: -1.3147e+00
Epoch 7/10
19/19 - 1s - loss: 95.5891 - loglik: -9.4078e+01 - logprior: -1.2927e+00
Epoch 8/10
19/19 - 1s - loss: 95.5073 - loglik: -9.4003e+01 - logprior: -1.2818e+00
Epoch 9/10
19/19 - 1s - loss: 95.4660 - loglik: -9.3966e+01 - logprior: -1.2715e+00
Epoch 10/10
19/19 - 1s - loss: 95.3653 - loglik: -9.3870e+01 - logprior: -1.2580e+00
Fitted a model with MAP estimate = -95.0822
Time for alignment: 53.0696
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.6926 - loglik: -1.3944e+02 - logprior: -3.2421e+00
Epoch 2/10
19/19 - 1s - loss: 113.7071 - loglik: -1.1219e+02 - logprior: -1.4578e+00
Epoch 3/10
19/19 - 1s - loss: 103.7224 - loglik: -1.0184e+02 - logprior: -1.6458e+00
Epoch 4/10
19/19 - 1s - loss: 101.7070 - loglik: -9.9949e+01 - logprior: -1.5208e+00
Epoch 5/10
19/19 - 1s - loss: 100.9408 - loglik: -9.9263e+01 - logprior: -1.5061e+00
Epoch 6/10
19/19 - 1s - loss: 100.5827 - loglik: -9.8910e+01 - logprior: -1.4901e+00
Epoch 7/10
19/19 - 1s - loss: 100.4996 - loglik: -9.8836e+01 - logprior: -1.4773e+00
Epoch 8/10
19/19 - 1s - loss: 100.2721 - loglik: -9.8604e+01 - logprior: -1.4697e+00
Epoch 9/10
19/19 - 1s - loss: 100.3309 - loglik: -9.8667e+01 - logprior: -1.4636e+00
Fitted a model with MAP estimate = -99.9457
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (28, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 108.2320 - loglik: -1.0394e+02 - logprior: -4.2187e+00
Epoch 2/2
19/19 - 1s - loss: 99.7678 - loglik: -9.7345e+01 - logprior: -2.2156e+00
Fitted a model with MAP estimate = -97.8134
expansions: [(0, 1)]
discards: [ 0  8 17 26 27 44]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.7787 - loglik: -9.7467e+01 - logprior: -3.2564e+00
Epoch 2/2
19/19 - 1s - loss: 96.7555 - loglik: -9.5105e+01 - logprior: -1.5210e+00
Fitted a model with MAP estimate = -96.0292
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.7830 - loglik: -9.6448e+01 - logprior: -3.2883e+00
Epoch 2/10
19/19 - 1s - loss: 97.0445 - loglik: -9.5430e+01 - logprior: -1.4887e+00
Epoch 3/10
19/19 - 1s - loss: 96.4003 - loglik: -9.4801e+01 - logprior: -1.4049e+00
Epoch 4/10
19/19 - 1s - loss: 96.1388 - loglik: -9.4562e+01 - logprior: -1.3553e+00
Epoch 5/10
19/19 - 1s - loss: 95.8090 - loglik: -9.4271e+01 - logprior: -1.3244e+00
Epoch 6/10
19/19 - 1s - loss: 95.8369 - loglik: -9.4325e+01 - logprior: -1.3055e+00
Fitted a model with MAP estimate = -95.4273
Time for alignment: 47.8853
Computed alignments with likelihoods: ['-95.3691', '-95.0822', '-95.4273']
Best model has likelihood: -95.0822
time for generating output: 0.1161
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF14604.projection.fasta
SP score = 0.7729610580455547
Training of 3 independent models on file PF00625.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd070757370>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd211546220>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2443f1580>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd3e9848a00>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd208f5e130>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd222d74ca0>, <__main__.SimpleDirichletPrior object at 0x7fd3d86e2d30>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 458.5199 - loglik: -4.5554e+02 - logprior: -2.9447e+00
Epoch 2/10
19/19 - 6s - loss: 381.3452 - loglik: -3.7973e+02 - logprior: -1.4834e+00
Epoch 3/10
19/19 - 6s - loss: 348.3163 - loglik: -3.4575e+02 - logprior: -1.9793e+00
Epoch 4/10
19/19 - 6s - loss: 341.9850 - loglik: -3.3939e+02 - logprior: -2.1061e+00
Epoch 5/10
19/19 - 6s - loss: 340.4882 - loglik: -3.3806e+02 - logprior: -1.9913e+00
Epoch 6/10
19/19 - 6s - loss: 338.6580 - loglik: -3.3635e+02 - logprior: -1.9325e+00
Epoch 7/10
19/19 - 6s - loss: 338.1838 - loglik: -3.3588e+02 - logprior: -1.9317e+00
Epoch 8/10
19/19 - 7s - loss: 338.9111 - loglik: -3.3663e+02 - logprior: -1.9308e+00
Fitted a model with MAP estimate = -334.2942
expansions: [(0, 2), (11, 1), (14, 1), (15, 1), (16, 1), (17, 2), (18, 1), (22, 1), (36, 1), (43, 2), (44, 1), (48, 1), (49, 1), (50, 1), (51, 2), (56, 1), (57, 1), (65, 1), (68, 1), (69, 1), (79, 1), (87, 1), (88, 1), (89, 1), (99, 1), (100, 1), (105, 1), (113, 2), (114, 2), (115, 1), (116, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 328.2539 - loglik: -3.2549e+02 - logprior: -2.5702e+00
Epoch 2/2
39/39 - 11s - loss: 314.8429 - loglik: -3.1330e+02 - logprior: -1.1678e+00
Fitted a model with MAP estimate = -309.1060
expansions: []
discards: [ 54  68 114 144]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 319.0502 - loglik: -3.1692e+02 - logprior: -1.9736e+00
Epoch 2/2
39/39 - 11s - loss: 314.5322 - loglik: -3.1322e+02 - logprior: -9.8370e-01
Fitted a model with MAP estimate = -309.3561
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 318.0402 - loglik: -3.1516e+02 - logprior: -2.7138e+00
Epoch 2/10
39/39 - 11s - loss: 312.5354 - loglik: -3.1100e+02 - logprior: -1.1731e+00
Epoch 3/10
39/39 - 11s - loss: 309.6818 - loglik: -3.0841e+02 - logprior: -7.3091e-01
Epoch 4/10
39/39 - 11s - loss: 308.0261 - loglik: -3.0676e+02 - logprior: -6.9153e-01
Epoch 5/10
39/39 - 11s - loss: 307.5036 - loglik: -3.0632e+02 - logprior: -6.2819e-01
Epoch 6/10
39/39 - 11s - loss: 307.1449 - loglik: -3.0607e+02 - logprior: -5.6177e-01
Epoch 7/10
39/39 - 11s - loss: 306.7018 - loglik: -3.0572e+02 - logprior: -4.8279e-01
Epoch 8/10
39/39 - 11s - loss: 306.6493 - loglik: -3.0575e+02 - logprior: -4.1614e-01
Epoch 9/10
39/39 - 11s - loss: 305.7486 - loglik: -3.0494e+02 - logprior: -3.3644e-01
Epoch 10/10
39/39 - 11s - loss: 305.5500 - loglik: -3.0483e+02 - logprior: -2.6292e-01
Fitted a model with MAP estimate = -305.0008
Time for alignment: 271.7217
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 457.5595 - loglik: -4.5458e+02 - logprior: -2.9507e+00
Epoch 2/10
19/19 - 7s - loss: 380.4810 - loglik: -3.7893e+02 - logprior: -1.4861e+00
Epoch 3/10
19/19 - 7s - loss: 347.4646 - loglik: -3.4518e+02 - logprior: -1.9515e+00
Epoch 4/10
19/19 - 7s - loss: 340.7170 - loglik: -3.3820e+02 - logprior: -2.0730e+00
Epoch 5/10
19/19 - 7s - loss: 337.5124 - loglik: -3.3509e+02 - logprior: -1.9599e+00
Epoch 6/10
19/19 - 7s - loss: 336.3047 - loglik: -3.3393e+02 - logprior: -1.9157e+00
Epoch 7/10
19/19 - 7s - loss: 336.5107 - loglik: -3.3420e+02 - logprior: -1.8843e+00
Fitted a model with MAP estimate = -332.2216
expansions: [(0, 2), (14, 1), (16, 2), (17, 1), (18, 2), (19, 2), (21, 1), (23, 1), (36, 1), (43, 2), (44, 1), (48, 1), (50, 1), (52, 1), (59, 1), (66, 2), (67, 1), (69, 1), (70, 1), (71, 1), (80, 1), (87, 2), (88, 1), (89, 1), (99, 1), (103, 1), (111, 1), (113, 1), (114, 2), (115, 1), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 190 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 326.7530 - loglik: -3.2401e+02 - logprior: -2.5594e+00
Epoch 2/2
39/39 - 12s - loss: 313.3121 - loglik: -3.1175e+02 - logprior: -1.1959e+00
Fitted a model with MAP estimate = -307.4422
expansions: []
discards: [ 0 20 27 57 87]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 319.6344 - loglik: -3.1650e+02 - logprior: -2.9761e+00
Epoch 2/2
39/39 - 11s - loss: 313.5168 - loglik: -3.1204e+02 - logprior: -1.1335e+00
Fitted a model with MAP estimate = -308.0535
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 314.2541 - loglik: -3.1220e+02 - logprior: -1.9067e+00
Epoch 2/10
39/39 - 11s - loss: 309.9353 - loglik: -3.0876e+02 - logprior: -8.4614e-01
Epoch 3/10
39/39 - 11s - loss: 307.7824 - loglik: -3.0649e+02 - logprior: -7.6643e-01
Epoch 4/10
39/39 - 11s - loss: 306.7034 - loglik: -3.0541e+02 - logprior: -7.0937e-01
Epoch 5/10
39/39 - 11s - loss: 305.8330 - loglik: -3.0464e+02 - logprior: -6.3251e-01
Epoch 6/10
39/39 - 11s - loss: 305.3366 - loglik: -3.0427e+02 - logprior: -5.5345e-01
Epoch 7/10
39/39 - 11s - loss: 304.6148 - loglik: -3.0364e+02 - logprior: -4.7904e-01
Epoch 8/10
39/39 - 11s - loss: 304.5475 - loglik: -3.0364e+02 - logprior: -4.2483e-01
Epoch 9/10
39/39 - 11s - loss: 304.6116 - loglik: -3.0381e+02 - logprior: -3.4075e-01
Fitted a model with MAP estimate = -303.4893
Time for alignment: 257.9898
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 458.1832 - loglik: -4.5519e+02 - logprior: -2.9558e+00
Epoch 2/10
19/19 - 6s - loss: 380.7736 - loglik: -3.7919e+02 - logprior: -1.5054e+00
Epoch 3/10
19/19 - 6s - loss: 346.6148 - loglik: -3.4429e+02 - logprior: -1.9596e+00
Epoch 4/10
19/19 - 6s - loss: 339.5397 - loglik: -3.3704e+02 - logprior: -2.0566e+00
Epoch 5/10
19/19 - 7s - loss: 336.6396 - loglik: -3.3418e+02 - logprior: -1.9931e+00
Epoch 6/10
19/19 - 6s - loss: 335.2975 - loglik: -3.3287e+02 - logprior: -1.9550e+00
Epoch 7/10
19/19 - 6s - loss: 334.6711 - loglik: -3.3227e+02 - logprior: -1.9392e+00
Epoch 8/10
19/19 - 7s - loss: 334.5826 - loglik: -3.3220e+02 - logprior: -1.9267e+00
Epoch 9/10
19/19 - 7s - loss: 333.4968 - loglik: -3.3111e+02 - logprior: -1.9362e+00
Epoch 10/10
19/19 - 7s - loss: 333.8631 - loglik: -3.3149e+02 - logprior: -1.9330e+00
Fitted a model with MAP estimate = -330.0027
expansions: [(0, 2), (14, 1), (15, 2), (16, 2), (17, 1), (18, 1), (21, 1), (22, 1), (36, 1), (43, 2), (44, 1), (48, 1), (50, 1), (52, 1), (57, 1), (58, 1), (65, 2), (66, 1), (68, 1), (71, 1), (80, 1), (87, 1), (88, 1), (89, 2), (99, 1), (100, 1), (111, 1), (113, 1), (114, 1), (115, 1), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 188 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 325.3150 - loglik: -3.2255e+02 - logprior: -2.5665e+00
Epoch 2/2
39/39 - 11s - loss: 311.5046 - loglik: -3.0993e+02 - logprior: -1.1804e+00
Fitted a model with MAP estimate = -306.2696
expansions: []
discards: [  0  19  55  86 117]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 318.2824 - loglik: -3.1518e+02 - logprior: -2.9488e+00
Epoch 2/2
39/39 - 11s - loss: 311.9518 - loglik: -3.1057e+02 - logprior: -1.0329e+00
Fitted a model with MAP estimate = -306.2430
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 312.3981 - loglik: -3.1044e+02 - logprior: -1.8105e+00
Epoch 2/10
39/39 - 11s - loss: 308.3620 - loglik: -3.0727e+02 - logprior: -7.3692e-01
Epoch 3/10
39/39 - 11s - loss: 305.9549 - loglik: -3.0474e+02 - logprior: -6.7173e-01
Epoch 4/10
39/39 - 11s - loss: 304.7086 - loglik: -3.0349e+02 - logprior: -6.0944e-01
Epoch 5/10
39/39 - 11s - loss: 304.2124 - loglik: -3.0309e+02 - logprior: -5.3470e-01
Epoch 6/10
39/39 - 11s - loss: 303.5255 - loglik: -3.0253e+02 - logprior: -4.5263e-01
Epoch 7/10
39/39 - 11s - loss: 303.1290 - loglik: -3.0222e+02 - logprior: -3.8666e-01
Epoch 8/10
39/39 - 11s - loss: 302.8791 - loglik: -3.0206e+02 - logprior: -3.1346e-01
Epoch 9/10
39/39 - 11s - loss: 302.0267 - loglik: -3.0129e+02 - logprior: -2.5130e-01
Epoch 10/10
39/39 - 11s - loss: 302.5327 - loglik: -3.0190e+02 - logprior: -1.6842e-01
Fitted a model with MAP estimate = -301.4485
Time for alignment: 284.2280
Computed alignments with likelihoods: ['-305.0008', '-303.4893', '-301.4485']
Best model has likelihood: -301.4485
time for generating output: 0.3617
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00625.projection.fasta
SP score = 0.4122774913477386
Training of 3 independent models on file PF00476.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd0685b5460>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd068562ee0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd068562fa0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd068562eb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd068562d00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd1f85e2ca0>, <__main__.SimpleDirichletPrior object at 0x7fd24cc0c850>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 901.9614 - loglik: -9.0023e+02 - logprior: -1.5795e+00
Epoch 2/10
39/39 - 50s - loss: 717.6888 - loglik: -7.1422e+02 - logprior: -2.1933e+00
Epoch 3/10
39/39 - 51s - loss: 699.7193 - loglik: -6.9568e+02 - logprior: -2.3230e+00
Epoch 4/10
39/39 - 51s - loss: 695.4924 - loglik: -6.9169e+02 - logprior: -2.3186e+00
Epoch 5/10
39/39 - 51s - loss: 693.0156 - loglik: -6.8932e+02 - logprior: -2.4223e+00
Epoch 6/10
39/39 - 49s - loss: 691.7896 - loglik: -6.8823e+02 - logprior: -2.4393e+00
Epoch 7/10
39/39 - 48s - loss: 691.1525 - loglik: -6.8772e+02 - logprior: -2.4216e+00
Epoch 8/10
39/39 - 49s - loss: 690.6041 - loglik: -6.8723e+02 - logprior: -2.4489e+00
Epoch 9/10
39/39 - 48s - loss: 690.5609 - loglik: -6.8722e+02 - logprior: -2.4715e+00
Epoch 10/10
39/39 - 49s - loss: 690.3740 - loglik: -6.8707e+02 - logprior: -2.4740e+00
Fitted a model with MAP estimate = -688.4279
expansions: [(0, 4), (41, 1), (46, 1), (61, 1), (63, 2), (67, 1), (73, 1), (81, 1), (82, 1), (84, 2), (85, 3), (91, 1), (92, 1), (93, 1), (113, 1), (114, 1), (119, 1), (123, 2), (125, 3), (143, 1), (145, 1), (147, 1), (150, 1), (156, 1), (159, 1), (162, 1), (163, 1), (164, 1), (165, 2), (166, 1), (187, 1), (189, 2), (190, 1), (191, 1), (194, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (229, 1), (230, 1), (231, 3), (232, 1), (256, 1), (257, 1), (258, 1), (259, 2), (260, 2), (261, 3), (279, 2), (280, 1), (281, 1), (282, 1), (284, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 386 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 78s - loss: 684.4856 - loglik: -6.8140e+02 - logprior: -2.7874e+00
Epoch 2/2
39/39 - 79s - loss: 653.8124 - loglik: -6.5156e+02 - logprior: -1.5119e+00
Fitted a model with MAP estimate = -646.8966
expansions: []
discards: [  0   2 147 152 231 324 325 350 365 366]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 75s - loss: 664.4639 - loglik: -6.6210e+02 - logprior: -2.1217e+00
Epoch 2/2
39/39 - 73s - loss: 654.2532 - loglik: -6.5302e+02 - logprior: -5.7580e-01
Fitted a model with MAP estimate = -647.8469
expansions: [(1, 1)]
discards: [99]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 73s - loss: 662.0144 - loglik: -6.6021e+02 - logprior: -1.5680e+00
Epoch 2/10
39/39 - 70s - loss: 653.0798 - loglik: -6.5190e+02 - logprior: -5.4153e-01
Epoch 3/10
39/39 - 68s - loss: 646.4523 - loglik: -6.4506e+02 - logprior: -3.7943e-01
Epoch 4/10
39/39 - 73s - loss: 643.5175 - loglik: -6.4190e+02 - logprior: -3.0504e-01
Epoch 5/10
39/39 - 76s - loss: 641.6308 - loglik: -6.4023e+02 - logprior: -3.5243e-02
Epoch 6/10
39/39 - 73s - loss: 639.5537 - loglik: -6.3850e+02 - logprior: 0.2354
Epoch 7/10
39/39 - 64s - loss: 639.5635 - loglik: -6.3879e+02 - logprior: 0.4022
Fitted a model with MAP estimate = -637.8308
Time for alignment: 1630.9189
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 906.4212 - loglik: -9.0459e+02 - logprior: -1.6349e+00
Epoch 2/10
39/39 - 43s - loss: 721.1885 - loglik: -7.1745e+02 - logprior: -2.2375e+00
Epoch 3/10
39/39 - 43s - loss: 703.4923 - loglik: -6.9970e+02 - logprior: -2.2551e+00
Epoch 4/10
39/39 - 43s - loss: 699.0312 - loglik: -6.9537e+02 - logprior: -2.2414e+00
Epoch 5/10
39/39 - 43s - loss: 696.4115 - loglik: -6.9293e+02 - logprior: -2.2412e+00
Epoch 6/10
39/39 - 43s - loss: 695.2598 - loglik: -6.9173e+02 - logprior: -2.4020e+00
Epoch 7/10
39/39 - 43s - loss: 694.4291 - loglik: -6.9107e+02 - logprior: -2.3432e+00
Epoch 8/10
39/39 - 43s - loss: 694.4239 - loglik: -6.9111e+02 - logprior: -2.3739e+00
Epoch 9/10
39/39 - 43s - loss: 693.8307 - loglik: -6.9069e+02 - logprior: -2.2557e+00
Epoch 10/10
39/39 - 43s - loss: 693.5467 - loglik: -6.9046e+02 - logprior: -2.2560e+00
Fitted a model with MAP estimate = -691.5093
expansions: [(0, 4), (25, 1), (42, 1), (45, 1), (60, 1), (62, 2), (64, 1), (65, 1), (71, 1), (72, 1), (80, 1), (82, 2), (83, 1), (84, 1), (90, 1), (91, 1), (102, 1), (112, 1), (113, 1), (118, 1), (122, 2), (125, 2), (141, 1), (144, 1), (146, 1), (149, 1), (151, 1), (155, 1), (158, 1), (161, 1), (162, 1), (163, 1), (165, 2), (168, 1), (186, 1), (188, 2), (189, 1), (190, 1), (205, 1), (206, 1), (207, 2), (208, 2), (209, 3), (227, 1), (230, 6), (256, 4), (258, 2), (259, 2), (260, 1), (276, 1), (278, 2), (279, 1), (281, 1), (284, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 388 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 69s - loss: 685.2155 - loglik: -6.8208e+02 - logprior: -2.8443e+00
Epoch 2/2
39/39 - 70s - loss: 653.6823 - loglik: -6.5137e+02 - logprior: -1.5938e+00
Fitted a model with MAP estimate = -646.4121
expansions: []
discards: [  0   2 101 147 152 231 259 260 288 325 351 367 368]
Re-initialized the encoder parameters.
Fitting a model of length 375 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 74s - loss: 664.8276 - loglik: -6.6254e+02 - logprior: -2.0429e+00
Epoch 2/2
39/39 - 68s - loss: 654.4467 - loglik: -6.5319e+02 - logprior: -6.1011e-01
Fitted a model with MAP estimate = -648.1847
expansions: [(1, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 70s - loss: 661.2979 - loglik: -6.5957e+02 - logprior: -1.5006e+00
Epoch 2/10
39/39 - 74s - loss: 652.4189 - loglik: -6.5139e+02 - logprior: -4.0576e-01
Epoch 3/10
39/39 - 69s - loss: 646.8333 - loglik: -6.4539e+02 - logprior: -4.5398e-01
Epoch 4/10
39/39 - 68s - loss: 643.1823 - loglik: -6.4180e+02 - logprior: -1.0562e-01
Epoch 5/10
39/39 - 75s - loss: 640.9663 - loglik: -6.3970e+02 - logprior: 0.0790
Epoch 6/10
39/39 - 78s - loss: 639.8878 - loglik: -6.3885e+02 - logprior: 0.2301
Epoch 7/10
39/39 - 78s - loss: 639.9225 - loglik: -6.3916e+02 - logprior: 0.4163
Fitted a model with MAP estimate = -637.6464
Time for alignment: 1552.4205
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 54s - loss: 902.8542 - loglik: -9.0111e+02 - logprior: -1.5893e+00
Epoch 2/10
39/39 - 52s - loss: 716.4889 - loglik: -7.1329e+02 - logprior: -2.3733e+00
Epoch 3/10
39/39 - 52s - loss: 700.0003 - loglik: -6.9635e+02 - logprior: -2.4415e+00
Epoch 4/10
39/39 - 53s - loss: 695.8488 - loglik: -6.9221e+02 - logprior: -2.3798e+00
Epoch 5/10
39/39 - 53s - loss: 694.1430 - loglik: -6.9061e+02 - logprior: -2.3307e+00
Epoch 6/10
39/39 - 53s - loss: 692.5992 - loglik: -6.8913e+02 - logprior: -2.3400e+00
Epoch 7/10
39/39 - 50s - loss: 692.4246 - loglik: -6.8902e+02 - logprior: -2.3906e+00
Epoch 8/10
39/39 - 49s - loss: 691.4813 - loglik: -6.8810e+02 - logprior: -2.4429e+00
Epoch 9/10
39/39 - 52s - loss: 692.1157 - loglik: -6.8890e+02 - logprior: -2.3425e+00
Fitted a model with MAP estimate = -689.5769
expansions: [(0, 4), (36, 1), (42, 1), (45, 1), (59, 1), (62, 2), (66, 1), (72, 1), (73, 1), (79, 1), (80, 1), (82, 2), (83, 1), (84, 1), (90, 1), (91, 1), (92, 1), (102, 1), (111, 1), (112, 1), (117, 1), (121, 2), (124, 2), (140, 1), (144, 1), (145, 1), (148, 1), (150, 1), (158, 1), (160, 2), (161, 2), (162, 1), (164, 1), (165, 1), (186, 1), (188, 2), (189, 1), (190, 1), (193, 1), (206, 1), (207, 2), (208, 2), (209, 3), (227, 1), (228, 1), (231, 4), (233, 1), (256, 1), (257, 1), (258, 1), (259, 2), (260, 2), (261, 3), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 389 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 73s - loss: 684.6003 - loglik: -6.8152e+02 - logprior: -2.8007e+00
Epoch 2/2
39/39 - 78s - loss: 653.9157 - loglik: -6.5166e+02 - logprior: -1.5646e+00
Fitted a model with MAP estimate = -646.6662
expansions: []
discards: [  0   2 147 152 199 232 260 261 290 327 329 368 369]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 76s - loss: 664.7401 - loglik: -6.6234e+02 - logprior: -2.1602e+00
Epoch 2/2
39/39 - 78s - loss: 653.9101 - loglik: -6.5259e+02 - logprior: -6.8874e-01
Fitted a model with MAP estimate = -647.9202
expansions: [(1, 1)]
discards: [99]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 77s - loss: 662.1068 - loglik: -6.6028e+02 - logprior: -1.5996e+00
Epoch 2/10
39/39 - 76s - loss: 652.5925 - loglik: -6.5137e+02 - logprior: -5.9394e-01
Epoch 3/10
39/39 - 77s - loss: 646.2108 - loglik: -6.4482e+02 - logprior: -3.7560e-01
Epoch 4/10
39/39 - 76s - loss: 643.2164 - loglik: -6.4178e+02 - logprior: -1.5310e-01
Epoch 5/10
39/39 - 71s - loss: 641.1617 - loglik: -6.3987e+02 - logprior: 0.0569
Epoch 6/10
39/39 - 74s - loss: 640.2811 - loglik: -6.3889e+02 - logprior: -1.1261e-01
Epoch 7/10
39/39 - 73s - loss: 639.6331 - loglik: -6.3861e+02 - logprior: 0.1555
Epoch 8/10
39/39 - 68s - loss: 639.3594 - loglik: -6.3869e+02 - logprior: 0.4152
Epoch 9/10
39/39 - 75s - loss: 637.7732 - loglik: -6.3736e+02 - logprior: 0.5959
Epoch 10/10
39/39 - 69s - loss: 637.3312 - loglik: -6.3732e+02 - logprior: 0.9264
Fitted a model with MAP estimate = -636.3050
Time for alignment: 1850.1511
Computed alignments with likelihoods: ['-637.8308', '-637.6464', '-636.3050']
Best model has likelihood: -636.3050
time for generating output: 0.4298
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00476.projection.fasta
SP score = 0.9467595396729255
Training of 3 independent models on file PF02836.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd0687e1d00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd1efb83f10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fca5c2cefd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd1e754bca0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd0687d8460>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd02825a670>, <__main__.SimpleDirichletPrior object at 0x7fd02966e340>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 645.3600 - loglik: -6.4314e+02 - logprior: -1.9983e+00
Epoch 2/10
39/39 - 20s - loss: 554.2728 - loglik: -5.5188e+02 - logprior: -1.7139e+00
Epoch 3/10
39/39 - 20s - loss: 543.7756 - loglik: -5.4056e+02 - logprior: -1.8867e+00
Epoch 4/10
39/39 - 20s - loss: 541.2195 - loglik: -5.3794e+02 - logprior: -1.9089e+00
Epoch 5/10
39/39 - 21s - loss: 539.7175 - loglik: -5.3642e+02 - logprior: -1.9430e+00
Epoch 6/10
39/39 - 21s - loss: 538.3420 - loglik: -5.3507e+02 - logprior: -1.9697e+00
Epoch 7/10
39/39 - 21s - loss: 537.6545 - loglik: -5.3439e+02 - logprior: -1.9877e+00
Epoch 8/10
39/39 - 21s - loss: 537.1742 - loglik: -5.3393e+02 - logprior: -2.0216e+00
Epoch 9/10
39/39 - 22s - loss: 536.0593 - loglik: -5.3286e+02 - logprior: -2.0320e+00
Epoch 10/10
39/39 - 22s - loss: 536.0641 - loglik: -5.3292e+02 - logprior: -2.0430e+00
Fitted a model with MAP estimate = -534.1516
expansions: [(4, 1), (6, 1), (33, 1), (49, 1), (82, 8), (90, 1), (91, 2), (93, 2), (117, 1), (118, 8), (119, 3), (120, 1), (121, 2), (129, 1), (131, 4), (132, 2), (135, 1), (136, 1), (142, 2), (143, 10), (146, 2), (147, 2), (153, 1), (156, 1), (159, 5), (160, 1), (169, 1), (170, 1), (171, 4), (173, 1)]
discards: [  0 162 163 164 165 166 175 176 184 185 186 187 188 189 190 191 192 193
 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 248 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 566.3940 - loglik: -5.6308e+02 - logprior: -3.0682e+00
Epoch 2/2
39/39 - 27s - loss: 543.6218 - loglik: -5.4113e+02 - logprior: -1.6780e+00
Fitted a model with MAP estimate = -538.2736
expansions: [(3, 1), (4, 1), (248, 24)]
discards: [  0   1  81  82  83  84  93  94  95  96 137 138 139 140 146 165 167 189
 190 200 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222
 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240
 241 242 243 244 245 246 247]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 564.4144 - loglik: -5.6139e+02 - logprior: -2.7337e+00
Epoch 2/2
39/39 - 22s - loss: 549.6884 - loglik: -5.4720e+02 - logprior: -1.3914e+00
Fitted a model with MAP estimate = -543.8245
expansions: [(4, 1), (83, 1), (84, 6), (146, 1), (188, 2), (195, 1), (199, 8), (200, 1), (201, 1), (202, 1), (213, 18)]
discards: [  0   1  88 174 203 204 205 206 207 208 209 210 211 212]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 548.4954 - loglik: -5.4572e+02 - logprior: -2.5305e+00
Epoch 2/10
39/39 - 26s - loss: 534.8329 - loglik: -5.3330e+02 - logprior: -9.0601e-01
Epoch 3/10
39/39 - 26s - loss: 529.6248 - loglik: -5.2818e+02 - logprior: -4.6626e-01
Epoch 4/10
39/39 - 26s - loss: 527.1983 - loglik: -5.2541e+02 - logprior: -4.9419e-01
Epoch 5/10
39/39 - 25s - loss: 525.9140 - loglik: -5.2401e+02 - logprior: -4.5917e-01
Epoch 6/10
39/39 - 26s - loss: 524.5748 - loglik: -5.2263e+02 - logprior: -4.3619e-01
Epoch 7/10
39/39 - 25s - loss: 523.6665 - loglik: -5.2172e+02 - logprior: -4.0317e-01
Epoch 8/10
39/39 - 26s - loss: 522.4694 - loglik: -5.2057e+02 - logprior: -3.7652e-01
Epoch 9/10
39/39 - 25s - loss: 521.0107 - loglik: -5.1922e+02 - logprior: -3.3863e-01
Epoch 10/10
39/39 - 24s - loss: 520.8027 - loglik: -5.1912e+02 - logprior: -2.9139e-01
Fitted a model with MAP estimate = -518.8162
Time for alignment: 682.5210
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 644.3508 - loglik: -6.4211e+02 - logprior: -2.0086e+00
Epoch 2/10
39/39 - 19s - loss: 549.6583 - loglik: -5.4677e+02 - logprior: -1.8897e+00
Epoch 3/10
39/39 - 19s - loss: 540.8222 - loglik: -5.3744e+02 - logprior: -1.9515e+00
Epoch 4/10
39/39 - 19s - loss: 538.3032 - loglik: -5.3493e+02 - logprior: -1.9447e+00
Epoch 5/10
39/39 - 19s - loss: 536.7012 - loglik: -5.3337e+02 - logprior: -1.9702e+00
Epoch 6/10
39/39 - 19s - loss: 535.2244 - loglik: -5.3190e+02 - logprior: -2.0006e+00
Epoch 7/10
39/39 - 19s - loss: 534.7982 - loglik: -5.3149e+02 - logprior: -2.0228e+00
Epoch 8/10
39/39 - 19s - loss: 533.9603 - loglik: -5.3066e+02 - logprior: -2.0472e+00
Epoch 9/10
39/39 - 19s - loss: 533.6507 - loglik: -5.3037e+02 - logprior: -2.0638e+00
Epoch 10/10
39/39 - 19s - loss: 532.9523 - loglik: -5.2968e+02 - logprior: -2.0792e+00
Fitted a model with MAP estimate = -531.0901
expansions: [(4, 1), (6, 1), (33, 1), (40, 1), (82, 9), (119, 1), (120, 5), (121, 9), (128, 1), (129, 1), (131, 8), (134, 1), (135, 1), (137, 1), (138, 1), (139, 1), (140, 2), (141, 9), (142, 1), (147, 1), (155, 1), (156, 1), (157, 1), (160, 4), (170, 1), (171, 1), (172, 4), (173, 3), (174, 1), (184, 1)]
discards: [  0 149 161 162 163 164 165 166 167 176 177 178 179 180 181 182 185 186
 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204
 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 569.3409 - loglik: -5.6615e+02 - logprior: -2.9580e+00
Epoch 2/2
39/39 - 23s - loss: 541.2742 - loglik: -5.3890e+02 - logprior: -1.6405e+00
Fitted a model with MAP estimate = -535.4296
expansions: [(217, 8), (218, 1), (243, 9)]
discards: [  0  79  80  81  82  83  84  93 134 135 136 137 138 158 159 181 182 184
 194 205 206 207 208 209 221 222 223 224 225 226 227 228 233]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 554.6637 - loglik: -5.5141e+02 - logprior: -2.9722e+00
Epoch 2/2
39/39 - 21s - loss: 539.6597 - loglik: -5.3712e+02 - logprior: -1.4380e+00
Fitted a model with MAP estimate = -533.5639
expansions: [(0, 2), (187, 3), (196, 1), (197, 1), (206, 1), (209, 7), (228, 3)]
discards: [  0 148 166 167 185 214 215 216 217 221 222 223 224 225 226 227]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 546.3459 - loglik: -5.4426e+02 - logprior: -1.8311e+00
Epoch 2/10
39/39 - 21s - loss: 536.6611 - loglik: -5.3537e+02 - logprior: -6.3963e-01
Epoch 3/10
39/39 - 21s - loss: 532.9716 - loglik: -5.3143e+02 - logprior: -5.7699e-01
Epoch 4/10
39/39 - 21s - loss: 530.7579 - loglik: -5.2894e+02 - logprior: -5.3456e-01
Epoch 5/10
39/39 - 21s - loss: 529.8192 - loglik: -5.2787e+02 - logprior: -4.8905e-01
Epoch 6/10
39/39 - 21s - loss: 528.2479 - loglik: -5.2632e+02 - logprior: -4.2187e-01
Epoch 7/10
39/39 - 21s - loss: 526.7388 - loglik: -5.2490e+02 - logprior: -3.5052e-01
Epoch 8/10
39/39 - 21s - loss: 526.6028 - loglik: -5.2487e+02 - logprior: -2.9193e-01
Epoch 9/10
39/39 - 21s - loss: 525.6079 - loglik: -5.2402e+02 - logprior: -2.3589e-01
Epoch 10/10
39/39 - 21s - loss: 525.2250 - loglik: -5.2378e+02 - logprior: -1.6964e-01
Fitted a model with MAP estimate = -523.3500
Time for alignment: 600.9344
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 644.6173 - loglik: -6.4240e+02 - logprior: -1.9909e+00
Epoch 2/10
39/39 - 19s - loss: 552.8984 - loglik: -5.4997e+02 - logprior: -1.7480e+00
Epoch 3/10
39/39 - 19s - loss: 543.6818 - loglik: -5.4041e+02 - logprior: -1.8510e+00
Epoch 4/10
39/39 - 19s - loss: 539.9891 - loglik: -5.3654e+02 - logprior: -1.9752e+00
Epoch 5/10
39/39 - 19s - loss: 538.4073 - loglik: -5.3498e+02 - logprior: -2.0185e+00
Epoch 6/10
39/39 - 19s - loss: 537.2934 - loglik: -5.3391e+02 - logprior: -2.0375e+00
Epoch 7/10
39/39 - 19s - loss: 536.3535 - loglik: -5.3301e+02 - logprior: -2.0591e+00
Epoch 8/10
39/39 - 19s - loss: 535.7636 - loglik: -5.3246e+02 - logprior: -2.0834e+00
Epoch 9/10
39/39 - 19s - loss: 535.3320 - loglik: -5.3205e+02 - logprior: -2.0927e+00
Epoch 10/10
39/39 - 19s - loss: 534.3218 - loglik: -5.3106e+02 - logprior: -2.1102e+00
Fitted a model with MAP estimate = -532.8118
expansions: [(4, 1), (6, 1), (33, 1), (40, 1), (82, 8), (89, 1), (90, 2), (92, 2), (114, 1), (115, 1), (116, 9), (117, 3), (118, 1), (126, 1), (129, 5), (130, 2), (133, 1), (134, 1), (137, 1), (139, 1), (141, 9), (142, 1), (144, 1), (146, 1), (155, 1), (158, 7), (159, 1), (170, 1), (171, 6)]
discards: [  0 148 149 150 151 160 161 162 168 172 176 177 178 179 180 181 182 183
 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201
 202 203 204 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 572.5456 - loglik: -5.6928e+02 - logprior: -3.0181e+00
Epoch 2/2
39/39 - 22s - loss: 547.6141 - loglik: -5.4512e+02 - logprior: -1.7210e+00
Fitted a model with MAP estimate = -540.8884
expansions: [(144, 1), (220, 2), (225, 1), (226, 13), (238, 9)]
discards: [  0   1  81  82  83  84 136 137 138 139 140 181 182 183 213 214 231 232]
Re-initialized the encoder parameters.
Fitting a model of length 246 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 547.5440 - loglik: -5.4435e+02 - logprior: -2.9149e+00
Epoch 2/2
39/39 - 23s - loss: 532.5800 - loglik: -5.3008e+02 - logprior: -1.4138e+00
Fitted a model with MAP estimate = -527.3730
expansions: [(0, 2), (2, 1), (200, 1), (205, 1), (217, 6), (246, 3)]
discards: [  0  80  81  82 154 155 218 219 220 221 222 223 224 225 226 227 228 229
 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 550.2007 - loglik: -5.4813e+02 - logprior: -1.8106e+00
Epoch 2/10
39/39 - 20s - loss: 540.5184 - loglik: -5.3892e+02 - logprior: -6.2492e-01
Epoch 3/10
39/39 - 20s - loss: 536.6366 - loglik: -5.3440e+02 - logprior: -5.5007e-01
Epoch 4/10
39/39 - 20s - loss: 533.9888 - loglik: -5.3169e+02 - logprior: -4.9241e-01
Epoch 5/10
39/39 - 20s - loss: 533.0690 - loglik: -5.3086e+02 - logprior: -4.4077e-01
Epoch 6/10
39/39 - 20s - loss: 530.8983 - loglik: -5.2883e+02 - logprior: -3.8495e-01
Epoch 7/10
39/39 - 20s - loss: 530.8439 - loglik: -5.2892e+02 - logprior: -3.2260e-01
Epoch 8/10
39/39 - 20s - loss: 528.5873 - loglik: -5.2683e+02 - logprior: -2.7087e-01
Epoch 9/10
39/39 - 20s - loss: 528.5309 - loglik: -5.2693e+02 - logprior: -2.1079e-01
Epoch 10/10
39/39 - 20s - loss: 527.9069 - loglik: -5.2643e+02 - logprior: -1.5686e-01
Fitted a model with MAP estimate = -525.8528
Time for alignment: 595.1048
Computed alignments with likelihoods: ['-518.8162', '-523.3500', '-525.8528']
Best model has likelihood: -518.8162
time for generating output: 0.3491
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02836.projection.fasta
SP score = 0.8388195232690124
Training of 3 independent models on file PF02777.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd1e754b850>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd029038f70>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd029124c70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd029124c40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd029124f10>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd029118ee0>, <__main__.SimpleDirichletPrior object at 0x7fd24d2b1d90>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 269.9670 - loglik: -2.6678e+02 - logprior: -3.1784e+00
Epoch 2/10
19/19 - 3s - loss: 200.8742 - loglik: -1.9945e+02 - logprior: -1.3869e+00
Epoch 3/10
19/19 - 3s - loss: 176.7635 - loglik: -1.7487e+02 - logprior: -1.7789e+00
Epoch 4/10
19/19 - 3s - loss: 171.7020 - loglik: -1.6968e+02 - logprior: -1.7809e+00
Epoch 5/10
19/19 - 3s - loss: 170.3197 - loglik: -1.6841e+02 - logprior: -1.7061e+00
Epoch 6/10
19/19 - 3s - loss: 170.2057 - loglik: -1.6834e+02 - logprior: -1.6877e+00
Epoch 7/10
19/19 - 3s - loss: 169.8425 - loglik: -1.6800e+02 - logprior: -1.6554e+00
Epoch 8/10
19/19 - 3s - loss: 169.3535 - loglik: -1.6752e+02 - logprior: -1.6469e+00
Epoch 9/10
19/19 - 3s - loss: 169.1553 - loglik: -1.6734e+02 - logprior: -1.6410e+00
Epoch 10/10
19/19 - 3s - loss: 169.4504 - loglik: -1.6762e+02 - logprior: -1.6471e+00
Fitted a model with MAP estimate = -168.7947
expansions: [(4, 1), (6, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 5), (47, 1), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 165.2363 - loglik: -1.6213e+02 - logprior: -3.0531e+00
Epoch 2/2
19/19 - 3s - loss: 153.7871 - loglik: -1.5232e+02 - logprior: -1.3013e+00
Fitted a model with MAP estimate = -152.1809
expansions: []
discards: [42 59 60]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 156.4633 - loglik: -1.5339e+02 - logprior: -3.0224e+00
Epoch 2/2
19/19 - 3s - loss: 152.9735 - loglik: -1.5155e+02 - logprior: -1.2308e+00
Fitted a model with MAP estimate = -151.7959
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 159.4884 - loglik: -1.5541e+02 - logprior: -4.0283e+00
Epoch 2/10
19/19 - 3s - loss: 154.8854 - loglik: -1.5237e+02 - logprior: -2.3304e+00
Epoch 3/10
19/19 - 3s - loss: 153.2751 - loglik: -1.5084e+02 - logprior: -2.1852e+00
Epoch 4/10
19/19 - 3s - loss: 152.1878 - loglik: -1.5009e+02 - logprior: -1.8131e+00
Epoch 5/10
19/19 - 3s - loss: 151.4779 - loglik: -1.5002e+02 - logprior: -1.1695e+00
Epoch 6/10
19/19 - 3s - loss: 150.7984 - loglik: -1.4936e+02 - logprior: -1.1734e+00
Epoch 7/10
19/19 - 3s - loss: 150.6648 - loglik: -1.4927e+02 - logprior: -1.1347e+00
Epoch 8/10
19/19 - 3s - loss: 150.6358 - loglik: -1.4927e+02 - logprior: -1.1101e+00
Epoch 9/10
19/19 - 3s - loss: 151.0797 - loglik: -1.4975e+02 - logprior: -1.0849e+00
Fitted a model with MAP estimate = -150.0682
Time for alignment: 107.4010
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 269.9193 - loglik: -2.6673e+02 - logprior: -3.1817e+00
Epoch 2/10
19/19 - 3s - loss: 200.8626 - loglik: -1.9944e+02 - logprior: -1.3908e+00
Epoch 3/10
19/19 - 3s - loss: 176.6248 - loglik: -1.7473e+02 - logprior: -1.7897e+00
Epoch 4/10
19/19 - 3s - loss: 172.9823 - loglik: -1.7098e+02 - logprior: -1.7485e+00
Epoch 5/10
19/19 - 3s - loss: 171.7607 - loglik: -1.6986e+02 - logprior: -1.6829e+00
Epoch 6/10
19/19 - 3s - loss: 171.1484 - loglik: -1.6927e+02 - logprior: -1.6714e+00
Epoch 7/10
19/19 - 3s - loss: 170.1972 - loglik: -1.6832e+02 - logprior: -1.6668e+00
Epoch 8/10
19/19 - 3s - loss: 170.2704 - loglik: -1.6839e+02 - logprior: -1.6702e+00
Fitted a model with MAP estimate = -169.6583
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 168.8707 - loglik: -1.6477e+02 - logprior: -4.0364e+00
Epoch 2/2
19/19 - 3s - loss: 156.6382 - loglik: -1.5428e+02 - logprior: -2.1660e+00
Fitted a model with MAP estimate = -154.3017
expansions: [(0, 2)]
discards: [ 0 42 61 62]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 156.5991 - loglik: -1.5354e+02 - logprior: -3.0070e+00
Epoch 2/2
19/19 - 3s - loss: 152.3627 - loglik: -1.5095e+02 - logprior: -1.2168e+00
Fitted a model with MAP estimate = -151.0770
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 158.8541 - loglik: -1.5487e+02 - logprior: -3.9282e+00
Epoch 2/10
19/19 - 3s - loss: 153.6108 - loglik: -1.5188e+02 - logprior: -1.5411e+00
Epoch 3/10
19/19 - 3s - loss: 151.7179 - loglik: -1.5032e+02 - logprior: -1.1430e+00
Epoch 4/10
19/19 - 3s - loss: 150.8235 - loglik: -1.4941e+02 - logprior: -1.1287e+00
Epoch 5/10
19/19 - 3s - loss: 150.3264 - loglik: -1.4892e+02 - logprior: -1.1140e+00
Epoch 6/10
19/19 - 3s - loss: 150.1215 - loglik: -1.4875e+02 - logprior: -1.1037e+00
Epoch 7/10
19/19 - 3s - loss: 149.9467 - loglik: -1.4862e+02 - logprior: -1.0670e+00
Epoch 8/10
19/19 - 3s - loss: 149.4783 - loglik: -1.4818e+02 - logprior: -1.0517e+00
Epoch 9/10
19/19 - 3s - loss: 149.5060 - loglik: -1.4823e+02 - logprior: -1.0267e+00
Fitted a model with MAP estimate = -149.1148
Time for alignment: 99.9143
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 269.9557 - loglik: -2.6676e+02 - logprior: -3.1813e+00
Epoch 2/10
19/19 - 3s - loss: 199.6977 - loglik: -1.9826e+02 - logprior: -1.3988e+00
Epoch 3/10
19/19 - 3s - loss: 176.3324 - loglik: -1.7444e+02 - logprior: -1.7812e+00
Epoch 4/10
19/19 - 3s - loss: 172.4420 - loglik: -1.7046e+02 - logprior: -1.7483e+00
Epoch 5/10
19/19 - 3s - loss: 171.4862 - loglik: -1.6959e+02 - logprior: -1.6901e+00
Epoch 6/10
19/19 - 3s - loss: 170.9576 - loglik: -1.6905e+02 - logprior: -1.6964e+00
Epoch 7/10
19/19 - 3s - loss: 170.4053 - loglik: -1.6849e+02 - logprior: -1.6948e+00
Epoch 8/10
19/19 - 3s - loss: 169.7112 - loglik: -1.6781e+02 - logprior: -1.6875e+00
Epoch 9/10
19/19 - 3s - loss: 170.1758 - loglik: -1.6828e+02 - logprior: -1.6870e+00
Fitted a model with MAP estimate = -169.3250
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 2), (71, 2), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 169.6977 - loglik: -1.6558e+02 - logprior: -4.0593e+00
Epoch 2/2
19/19 - 3s - loss: 156.5218 - loglik: -1.5414e+02 - logprior: -2.1844e+00
Fitted a model with MAP estimate = -154.2832
expansions: [(0, 2)]
discards: [ 0 42 61 62]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 156.5931 - loglik: -1.5351e+02 - logprior: -3.0252e+00
Epoch 2/2
19/19 - 3s - loss: 152.0114 - loglik: -1.5059e+02 - logprior: -1.2354e+00
Fitted a model with MAP estimate = -150.7311
expansions: []
discards: [ 0 91]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 158.8972 - loglik: -1.5491e+02 - logprior: -3.9371e+00
Epoch 2/10
19/19 - 3s - loss: 153.6447 - loglik: -1.5188e+02 - logprior: -1.5740e+00
Epoch 3/10
19/19 - 3s - loss: 151.6748 - loglik: -1.5028e+02 - logprior: -1.1475e+00
Epoch 4/10
19/19 - 3s - loss: 150.6519 - loglik: -1.4924e+02 - logprior: -1.1302e+00
Epoch 5/10
19/19 - 3s - loss: 150.5846 - loglik: -1.4920e+02 - logprior: -1.0968e+00
Epoch 6/10
19/19 - 3s - loss: 150.1203 - loglik: -1.4874e+02 - logprior: -1.1043e+00
Epoch 7/10
19/19 - 3s - loss: 149.7207 - loglik: -1.4838e+02 - logprior: -1.0875e+00
Epoch 8/10
19/19 - 3s - loss: 149.2015 - loglik: -1.4790e+02 - logprior: -1.0456e+00
Epoch 9/10
19/19 - 3s - loss: 149.9656 - loglik: -1.4868e+02 - logprior: -1.0334e+00
Fitted a model with MAP estimate = -149.0747
Time for alignment: 102.8585
Computed alignments with likelihoods: ['-150.0682', '-149.1148', '-149.0747']
Best model has likelihood: -149.0747
time for generating output: 0.1521
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02777.projection.fasta
SP score = 0.9313475177304964
Training of 3 independent models on file PF07654.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd3d8398a90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd24cc8ca90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd24cc8c550>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd233b93c10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd233b93760>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd3fac96b20>, <__main__.SimpleDirichletPrior object at 0x7fd029740e80>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 232.1144 - loglik: -2.2892e+02 - logprior: -3.1730e+00
Epoch 2/10
19/19 - 2s - loss: 196.2588 - loglik: -1.9480e+02 - logprior: -1.3643e+00
Epoch 3/10
19/19 - 2s - loss: 183.5579 - loglik: -1.8175e+02 - logprior: -1.4874e+00
Epoch 4/10
19/19 - 2s - loss: 180.9545 - loglik: -1.7919e+02 - logprior: -1.4508e+00
Epoch 5/10
19/19 - 2s - loss: 179.6643 - loglik: -1.7796e+02 - logprior: -1.4276e+00
Epoch 6/10
19/19 - 2s - loss: 179.3922 - loglik: -1.7773e+02 - logprior: -1.4000e+00
Epoch 7/10
19/19 - 2s - loss: 179.2094 - loglik: -1.7757e+02 - logprior: -1.3840e+00
Epoch 8/10
19/19 - 2s - loss: 178.7583 - loglik: -1.7712e+02 - logprior: -1.3840e+00
Epoch 9/10
19/19 - 2s - loss: 178.8468 - loglik: -1.7721e+02 - logprior: -1.3786e+00
Fitted a model with MAP estimate = -178.2665
expansions: [(7, 2), (8, 2), (9, 3), (21, 1), (22, 1), (31, 1), (34, 1), (35, 2), (37, 1), (46, 1), (48, 1), (53, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 183.6736 - loglik: -1.7964e+02 - logprior: -3.9728e+00
Epoch 2/2
19/19 - 2s - loss: 175.8790 - loglik: -1.7346e+02 - logprior: -2.1953e+00
Fitted a model with MAP estimate = -173.9040
expansions: [(0, 2)]
discards: [ 0 43 72]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 177.2471 - loglik: -1.7421e+02 - logprior: -2.9710e+00
Epoch 2/2
19/19 - 2s - loss: 173.6685 - loglik: -1.7230e+02 - logprior: -1.2046e+00
Fitted a model with MAP estimate = -172.3288
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 178.1752 - loglik: -1.7444e+02 - logprior: -3.6844e+00
Epoch 2/10
19/19 - 2s - loss: 173.8449 - loglik: -1.7238e+02 - logprior: -1.3141e+00
Epoch 3/10
19/19 - 2s - loss: 172.4378 - loglik: -1.7096e+02 - logprior: -1.1739e+00
Epoch 4/10
19/19 - 2s - loss: 171.7364 - loglik: -1.7020e+02 - logprior: -1.1478e+00
Epoch 5/10
19/19 - 2s - loss: 170.7316 - loglik: -1.6924e+02 - logprior: -1.1162e+00
Epoch 6/10
19/19 - 2s - loss: 170.8498 - loglik: -1.6939e+02 - logprior: -1.0982e+00
Fitted a model with MAP estimate = -170.1290
Time for alignment: 64.7615
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 231.9726 - loglik: -2.2878e+02 - logprior: -3.1710e+00
Epoch 2/10
19/19 - 2s - loss: 196.2052 - loglik: -1.9473e+02 - logprior: -1.3547e+00
Epoch 3/10
19/19 - 2s - loss: 183.8993 - loglik: -1.8206e+02 - logprior: -1.4493e+00
Epoch 4/10
19/19 - 2s - loss: 181.6428 - loglik: -1.7990e+02 - logprior: -1.4168e+00
Epoch 5/10
19/19 - 2s - loss: 179.9615 - loglik: -1.7830e+02 - logprior: -1.3988e+00
Epoch 6/10
19/19 - 2s - loss: 179.7370 - loglik: -1.7811e+02 - logprior: -1.3918e+00
Epoch 7/10
19/19 - 2s - loss: 179.2837 - loglik: -1.7769e+02 - logprior: -1.3821e+00
Epoch 8/10
19/19 - 2s - loss: 179.3254 - loglik: -1.7774e+02 - logprior: -1.3676e+00
Fitted a model with MAP estimate = -178.8621
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (31, 1), (32, 1), (34, 1), (35, 2), (38, 1), (46, 1), (48, 1), (53, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 183.5885 - loglik: -1.7956e+02 - logprior: -3.9674e+00
Epoch 2/2
19/19 - 2s - loss: 176.0079 - loglik: -1.7363e+02 - logprior: -2.1900e+00
Fitted a model with MAP estimate = -174.0621
expansions: [(0, 2)]
discards: [ 0 43 72]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 177.3296 - loglik: -1.7431e+02 - logprior: -2.9680e+00
Epoch 2/2
19/19 - 2s - loss: 173.5558 - loglik: -1.7218e+02 - logprior: -1.2154e+00
Fitted a model with MAP estimate = -172.3971
expansions: [(8, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 178.2548 - loglik: -1.7442e+02 - logprior: -3.7830e+00
Epoch 2/10
19/19 - 2s - loss: 173.9129 - loglik: -1.7239e+02 - logprior: -1.3726e+00
Epoch 3/10
19/19 - 2s - loss: 172.5450 - loglik: -1.7108e+02 - logprior: -1.1620e+00
Epoch 4/10
19/19 - 2s - loss: 171.3838 - loglik: -1.6985e+02 - logprior: -1.1328e+00
Epoch 5/10
19/19 - 2s - loss: 170.8809 - loglik: -1.6939e+02 - logprior: -1.1036e+00
Epoch 6/10
19/19 - 2s - loss: 170.7919 - loglik: -1.6935e+02 - logprior: -1.0811e+00
Epoch 7/10
19/19 - 2s - loss: 170.0617 - loglik: -1.6865e+02 - logprior: -1.0665e+00
Epoch 8/10
19/19 - 2s - loss: 170.3195 - loglik: -1.6894e+02 - logprior: -1.0491e+00
Fitted a model with MAP estimate = -169.7196
Time for alignment: 64.6695
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 231.9628 - loglik: -2.2877e+02 - logprior: -3.1712e+00
Epoch 2/10
19/19 - 2s - loss: 195.9796 - loglik: -1.9451e+02 - logprior: -1.3516e+00
Epoch 3/10
19/19 - 2s - loss: 183.7733 - loglik: -1.8194e+02 - logprior: -1.4593e+00
Epoch 4/10
19/19 - 2s - loss: 181.1475 - loglik: -1.7940e+02 - logprior: -1.4270e+00
Epoch 5/10
19/19 - 2s - loss: 180.0867 - loglik: -1.7838e+02 - logprior: -1.4123e+00
Epoch 6/10
19/19 - 2s - loss: 178.9515 - loglik: -1.7730e+02 - logprior: -1.3950e+00
Epoch 7/10
19/19 - 2s - loss: 178.7451 - loglik: -1.7711e+02 - logprior: -1.3891e+00
Epoch 8/10
19/19 - 2s - loss: 178.7328 - loglik: -1.7710e+02 - logprior: -1.3818e+00
Epoch 9/10
19/19 - 2s - loss: 178.5729 - loglik: -1.7695e+02 - logprior: -1.3715e+00
Epoch 10/10
19/19 - 2s - loss: 178.7454 - loglik: -1.7712e+02 - logprior: -1.3754e+00
Fitted a model with MAP estimate = -178.0959
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (32, 1), (33, 1), (34, 2), (35, 1), (37, 1), (46, 1), (48, 1), (49, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 183.6873 - loglik: -1.7965e+02 - logprior: -3.9702e+00
Epoch 2/2
19/19 - 2s - loss: 175.8238 - loglik: -1.7340e+02 - logprior: -2.1980e+00
Fitted a model with MAP estimate = -173.5783
expansions: [(0, 2)]
discards: [ 0 43 72]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 176.6254 - loglik: -1.7361e+02 - logprior: -2.9567e+00
Epoch 2/2
19/19 - 2s - loss: 172.7887 - loglik: -1.7143e+02 - logprior: -1.1976e+00
Fitted a model with MAP estimate = -171.5936
expansions: [(8, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 177.3697 - loglik: -1.7356e+02 - logprior: -3.7568e+00
Epoch 2/10
19/19 - 2s - loss: 173.1407 - loglik: -1.7163e+02 - logprior: -1.3550e+00
Epoch 3/10
19/19 - 2s - loss: 171.8132 - loglik: -1.7036e+02 - logprior: -1.1483e+00
Epoch 4/10
19/19 - 2s - loss: 170.5855 - loglik: -1.6907e+02 - logprior: -1.1140e+00
Epoch 5/10
19/19 - 2s - loss: 169.9339 - loglik: -1.6846e+02 - logprior: -1.0901e+00
Epoch 6/10
19/19 - 2s - loss: 169.8245 - loglik: -1.6840e+02 - logprior: -1.0652e+00
Epoch 7/10
19/19 - 2s - loss: 169.5445 - loglik: -1.6815e+02 - logprior: -1.0527e+00
Epoch 8/10
19/19 - 2s - loss: 169.4651 - loglik: -1.6811e+02 - logprior: -1.0315e+00
Epoch 9/10
19/19 - 2s - loss: 169.5027 - loglik: -1.6816e+02 - logprior: -1.0183e+00
Fitted a model with MAP estimate = -168.8265
Time for alignment: 70.9063
Computed alignments with likelihoods: ['-170.1290', '-169.7196', '-168.8265']
Best model has likelihood: -168.8265
time for generating output: 0.1456
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07654.projection.fasta
SP score = 0.8326530612244898
Training of 3 independent models on file PF04082.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd40b522370>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd0282441c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd028297d30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd029749f70>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd029749430>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd028261e80>, <__main__.SimpleDirichletPrior object at 0x7fd3d83745b0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 641.7377 - loglik: -6.3934e+02 - logprior: -1.8670e+00
Epoch 2/10
39/39 - 19s - loss: 586.0009 - loglik: -5.8209e+02 - logprior: -1.3467e+00
Epoch 3/10
39/39 - 19s - loss: 575.2864 - loglik: -5.7040e+02 - logprior: -1.4912e+00
Epoch 4/10
39/39 - 20s - loss: 569.1541 - loglik: -5.6439e+02 - logprior: -1.5854e+00
Epoch 5/10
39/39 - 20s - loss: 565.7295 - loglik: -5.6133e+02 - logprior: -1.6297e+00
Epoch 6/10
39/39 - 20s - loss: 563.7415 - loglik: -5.5985e+02 - logprior: -1.6642e+00
Epoch 7/10
39/39 - 20s - loss: 562.1475 - loglik: -5.5857e+02 - logprior: -1.6770e+00
Epoch 8/10
39/39 - 20s - loss: 561.1921 - loglik: -5.5781e+02 - logprior: -1.6899e+00
Epoch 9/10
39/39 - 20s - loss: 559.6960 - loglik: -5.5637e+02 - logprior: -1.6968e+00
Epoch 10/10
39/39 - 20s - loss: 559.1656 - loglik: -5.5569e+02 - logprior: -1.7157e+00
Fitted a model with MAP estimate = -556.2247
expansions: [(22, 1), (24, 5), (44, 2), (48, 5), (60, 1), (62, 2), (63, 1), (65, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 2), (104, 2), (105, 3), (108, 1), (109, 1), (120, 1), (123, 1), (126, 1), (133, 1), (149, 7), (162, 1), (164, 1), (167, 1), (169, 1), (174, 1), (175, 3)]
discards: [  0 189]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 637.2960 - loglik: -6.3343e+02 - logprior: -3.3967e+00
Epoch 2/2
39/39 - 25s - loss: 580.4754 - loglik: -5.7667e+02 - logprior: -1.7063e+00
Fitted a model with MAP estimate = -565.9051
expansions: [(24, 1), (75, 1), (106, 2), (107, 1), (129, 1), (137, 1), (177, 2), (184, 5), (189, 1), (201, 1), (238, 1), (240, 3)]
discards: [ 25  59  60  79  80  97  99 104 133 134 135 190 191 192 193 194 195 196
 236]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 595.4876 - loglik: -5.9328e+02 - logprior: -1.8406e+00
Epoch 2/2
39/39 - 25s - loss: 571.7454 - loglik: -5.6940e+02 - logprior: -7.0142e-01
Fitted a model with MAP estimate = -562.0268
expansions: [(126, 1), (241, 2)]
discards: [103 127 174 237 238 239 240]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 591.1887 - loglik: -5.8919e+02 - logprior: -1.6294e+00
Epoch 2/10
39/39 - 25s - loss: 572.2894 - loglik: -5.6983e+02 - logprior: -3.5836e-01
Epoch 3/10
39/39 - 25s - loss: 561.5236 - loglik: -5.5774e+02 - logprior: -4.7663e-01
Epoch 4/10
39/39 - 25s - loss: 555.3098 - loglik: -5.5098e+02 - logprior: -5.2348e-01
Epoch 5/10
39/39 - 25s - loss: 551.4786 - loglik: -5.4739e+02 - logprior: -5.2723e-01
Epoch 6/10
39/39 - 25s - loss: 549.0494 - loglik: -5.4562e+02 - logprior: -4.8974e-01
Epoch 7/10
39/39 - 24s - loss: 546.6702 - loglik: -5.4378e+02 - logprior: -4.2661e-01
Epoch 8/10
39/39 - 25s - loss: 545.2748 - loglik: -5.4272e+02 - logprior: -3.7676e-01
Epoch 9/10
39/39 - 25s - loss: 543.6865 - loglik: -5.4124e+02 - logprior: -3.3547e-01
Epoch 10/10
39/39 - 25s - loss: 541.6636 - loglik: -5.3877e+02 - logprior: -3.0723e-01
Fitted a model with MAP estimate = -536.4637
Time for alignment: 658.5339
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 642.0474 - loglik: -6.3965e+02 - logprior: -1.8674e+00
Epoch 2/10
39/39 - 20s - loss: 585.9987 - loglik: -5.8225e+02 - logprior: -1.3290e+00
Epoch 3/10
39/39 - 20s - loss: 574.3417 - loglik: -5.7000e+02 - logprior: -1.5426e+00
Epoch 4/10
39/39 - 19s - loss: 569.1003 - loglik: -5.6446e+02 - logprior: -1.5693e+00
Epoch 5/10
39/39 - 19s - loss: 565.8372 - loglik: -5.6140e+02 - logprior: -1.6289e+00
Epoch 6/10
39/39 - 19s - loss: 563.6370 - loglik: -5.5968e+02 - logprior: -1.6608e+00
Epoch 7/10
39/39 - 19s - loss: 562.6341 - loglik: -5.5904e+02 - logprior: -1.6966e+00
Epoch 8/10
39/39 - 19s - loss: 561.4528 - loglik: -5.5808e+02 - logprior: -1.7035e+00
Epoch 9/10
39/39 - 19s - loss: 560.6490 - loglik: -5.5738e+02 - logprior: -1.7163e+00
Epoch 10/10
39/39 - 19s - loss: 559.3972 - loglik: -5.5611e+02 - logprior: -1.7270e+00
Fitted a model with MAP estimate = -557.1410
expansions: [(22, 1), (24, 6), (27, 1), (44, 2), (48, 5), (59, 1), (61, 2), (62, 1), (64, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 2), (103, 1), (104, 3), (105, 1), (109, 1), (120, 1), (123, 1), (126, 1), (133, 1), (155, 4), (160, 1), (162, 1), (164, 1), (167, 1), (169, 1), (179, 1)]
discards: [  0 189]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 637.6331 - loglik: -6.3370e+02 - logprior: -3.4782e+00
Epoch 2/2
39/39 - 23s - loss: 579.9442 - loglik: -5.7598e+02 - logprior: -1.7565e+00
Fitted a model with MAP estimate = -564.5859
expansions: [(76, 1), (77, 1), (102, 1), (107, 1), (108, 1), (131, 1), (178, 1), (185, 1), (213, 1), (236, 4)]
discards: [ 25  26  61  80  81  98 100 103 105 129 133 193]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 595.5457 - loglik: -5.9334e+02 - logprior: -1.8505e+00
Epoch 2/2
39/39 - 24s - loss: 572.6338 - loglik: -5.7052e+02 - logprior: -5.3427e-01
Fitted a model with MAP estimate = -563.2903
expansions: [(25, 3), (30, 1), (181, 1), (187, 1), (189, 1)]
discards: [ 54 190 232 233 234]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 590.2491 - loglik: -5.8834e+02 - logprior: -1.5541e+00
Epoch 2/10
39/39 - 24s - loss: 570.8187 - loglik: -5.6873e+02 - logprior: -3.2494e-01
Epoch 3/10
39/39 - 24s - loss: 561.0585 - loglik: -5.5757e+02 - logprior: -4.0836e-01
Epoch 4/10
39/39 - 24s - loss: 554.4086 - loglik: -5.5024e+02 - logprior: -4.7637e-01
Epoch 5/10
39/39 - 24s - loss: 550.1195 - loglik: -5.4613e+02 - logprior: -4.9900e-01
Epoch 6/10
39/39 - 24s - loss: 547.9144 - loglik: -5.4458e+02 - logprior: -4.5654e-01
Epoch 7/10
39/39 - 24s - loss: 546.0526 - loglik: -5.4324e+02 - logprior: -4.1344e-01
Epoch 8/10
39/39 - 24s - loss: 544.6035 - loglik: -5.4217e+02 - logprior: -3.3890e-01
Epoch 9/10
39/39 - 24s - loss: 543.5383 - loglik: -5.4120e+02 - logprior: -3.0376e-01
Epoch 10/10
39/39 - 24s - loss: 541.3168 - loglik: -5.3849e+02 - logprior: -2.8059e-01
Fitted a model with MAP estimate = -536.1743
Time for alignment: 638.2963
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 641.8687 - loglik: -6.3947e+02 - logprior: -1.8714e+00
Epoch 2/10
39/39 - 20s - loss: 585.9673 - loglik: -5.8220e+02 - logprior: -1.3815e+00
Epoch 3/10
39/39 - 20s - loss: 574.2284 - loglik: -5.6985e+02 - logprior: -1.5801e+00
Epoch 4/10
39/39 - 20s - loss: 569.6999 - loglik: -5.6505e+02 - logprior: -1.6139e+00
Epoch 5/10
39/39 - 20s - loss: 566.1857 - loglik: -5.6176e+02 - logprior: -1.6517e+00
Epoch 6/10
39/39 - 20s - loss: 564.2054 - loglik: -5.6024e+02 - logprior: -1.6807e+00
Epoch 7/10
39/39 - 20s - loss: 562.6519 - loglik: -5.5908e+02 - logprior: -1.7146e+00
Epoch 8/10
39/39 - 20s - loss: 562.0398 - loglik: -5.5873e+02 - logprior: -1.7046e+00
Epoch 9/10
39/39 - 20s - loss: 560.9644 - loglik: -5.5773e+02 - logprior: -1.7103e+00
Epoch 10/10
39/39 - 20s - loss: 560.1305 - loglik: -5.5678e+02 - logprior: -1.7229e+00
Fitted a model with MAP estimate = -557.2768
expansions: [(23, 1), (24, 6), (44, 2), (49, 5), (60, 1), (62, 2), (63, 1), (65, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 2), (96, 1), (103, 2), (104, 5), (107, 1), (108, 1), (119, 1), (122, 1), (125, 1), (132, 1), (162, 1), (166, 1), (167, 1), (169, 1), (174, 1), (175, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 635.0392 - loglik: -6.3181e+02 - logprior: -2.7774e+00
Epoch 2/2
39/39 - 25s - loss: 578.5124 - loglik: -5.7510e+02 - logprior: -1.2663e+00
Fitted a model with MAP estimate = -565.3537
expansions: [(33, 1), (77, 1), (107, 2), (108, 1), (185, 1), (194, 2), (198, 1), (199, 1)]
discards: [ 29  30  31  61  80  81 101 102 105 128 129 134]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 594.5102 - loglik: -5.9225e+02 - logprior: -1.9144e+00
Epoch 2/2
39/39 - 25s - loss: 572.5521 - loglik: -5.7028e+02 - logprior: -7.8599e-01
Fitted a model with MAP estimate = -563.1884
expansions: [(25, 4), (27, 1), (99, 1), (182, 1)]
discards: [ 94  97 189 218]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 589.9610 - loglik: -5.8794e+02 - logprior: -1.6447e+00
Epoch 2/10
39/39 - 27s - loss: 571.3348 - loglik: -5.6882e+02 - logprior: -4.7668e-01
Epoch 3/10
39/39 - 27s - loss: 561.6884 - loglik: -5.5777e+02 - logprior: -5.6456e-01
Epoch 4/10
39/39 - 26s - loss: 555.3356 - loglik: -5.5091e+02 - logprior: -5.8815e-01
Epoch 5/10
39/39 - 26s - loss: 551.3648 - loglik: -5.4723e+02 - logprior: -5.8571e-01
Epoch 6/10
39/39 - 26s - loss: 548.7279 - loglik: -5.4522e+02 - logprior: -5.4984e-01
Epoch 7/10
39/39 - 26s - loss: 546.9094 - loglik: -5.4395e+02 - logprior: -5.0521e-01
Epoch 8/10
39/39 - 26s - loss: 545.3236 - loglik: -5.4276e+02 - logprior: -4.4450e-01
Epoch 9/10
39/39 - 26s - loss: 544.2230 - loglik: -5.4184e+02 - logprior: -3.8462e-01
Epoch 10/10
39/39 - 26s - loss: 541.9513 - loglik: -5.3921e+02 - logprior: -3.6757e-01
Fitted a model with MAP estimate = -537.8020
Time for alignment: 676.6944
Computed alignments with likelihoods: ['-536.4637', '-536.1743', '-537.8020']
Best model has likelihood: -536.1743
time for generating output: 0.3045
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF04082.projection.fasta
SP score = 0.6185424354243543
Training of 3 independent models on file PF00046.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fca45609ac0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd3d8366df0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd3d8366e80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd0681eb9a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd24484f280>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd1f811afa0>, <__main__.SimpleDirichletPrior object at 0x7fca3c08e310>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.1356 - loglik: -1.5087e+02 - logprior: -3.2588e+00
Epoch 2/10
19/19 - 1s - loss: 123.8906 - loglik: -1.2231e+02 - logprior: -1.5406e+00
Epoch 3/10
19/19 - 1s - loss: 110.2099 - loglik: -1.0823e+02 - logprior: -1.6293e+00
Epoch 4/10
19/19 - 1s - loss: 106.8741 - loglik: -1.0491e+02 - logprior: -1.6942e+00
Epoch 5/10
19/19 - 1s - loss: 105.9597 - loglik: -1.0414e+02 - logprior: -1.6057e+00
Epoch 6/10
19/19 - 1s - loss: 105.1969 - loglik: -1.0340e+02 - logprior: -1.6261e+00
Epoch 7/10
19/19 - 1s - loss: 105.2149 - loglik: -1.0347e+02 - logprior: -1.5880e+00
Fitted a model with MAP estimate = -104.8528
expansions: [(3, 1), (5, 1), (7, 1), (10, 1), (14, 1), (16, 1), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 106.5439 - loglik: -1.0337e+02 - logprior: -3.1172e+00
Epoch 2/2
19/19 - 1s - loss: 99.4836 - loglik: -9.7980e+01 - logprior: -1.3686e+00
Fitted a model with MAP estimate = -98.4355
expansions: []
discards: [36 39]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.5202 - loglik: -9.8446e+01 - logprior: -3.0269e+00
Epoch 2/2
19/19 - 1s - loss: 98.9020 - loglik: -9.7504e+01 - logprior: -1.2677e+00
Fitted a model with MAP estimate = -98.0981
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 101.1871 - loglik: -9.8140e+01 - logprior: -3.0008e+00
Epoch 2/10
19/19 - 1s - loss: 98.7579 - loglik: -9.7377e+01 - logprior: -1.2462e+00
Epoch 3/10
19/19 - 2s - loss: 98.0961 - loglik: -9.6682e+01 - logprior: -1.2049e+00
Epoch 4/10
19/19 - 1s - loss: 97.6143 - loglik: -9.6198e+01 - logprior: -1.1760e+00
Epoch 5/10
19/19 - 1s - loss: 97.1834 - loglik: -9.5810e+01 - logprior: -1.1409e+00
Epoch 6/10
19/19 - 1s - loss: 96.8718 - loglik: -9.5533e+01 - logprior: -1.1288e+00
Epoch 7/10
19/19 - 1s - loss: 96.9924 - loglik: -9.5680e+01 - logprior: -1.1105e+00
Fitted a model with MAP estimate = -96.5713
Time for alignment: 50.4874
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.2057 - loglik: -1.5095e+02 - logprior: -3.2530e+00
Epoch 2/10
19/19 - 1s - loss: 123.3939 - loglik: -1.2183e+02 - logprior: -1.5259e+00
Epoch 3/10
19/19 - 1s - loss: 110.6917 - loglik: -1.0875e+02 - logprior: -1.6258e+00
Epoch 4/10
19/19 - 1s - loss: 107.3498 - loglik: -1.0540e+02 - logprior: -1.6834e+00
Epoch 5/10
19/19 - 1s - loss: 106.3943 - loglik: -1.0457e+02 - logprior: -1.6165e+00
Epoch 6/10
19/19 - 1s - loss: 105.8399 - loglik: -1.0403e+02 - logprior: -1.6234e+00
Epoch 7/10
19/19 - 1s - loss: 105.8465 - loglik: -1.0410e+02 - logprior: -1.5792e+00
Fitted a model with MAP estimate = -105.3875
expansions: [(3, 1), (5, 1), (7, 1), (10, 1), (16, 1), (17, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 107.3647 - loglik: -1.0417e+02 - logprior: -3.1414e+00
Epoch 2/2
19/19 - 1s - loss: 99.5104 - loglik: -9.7971e+01 - logprior: -1.3984e+00
Fitted a model with MAP estimate = -98.4526
expansions: []
discards: [22 37 40]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.6529 - loglik: -9.8575e+01 - logprior: -3.0299e+00
Epoch 2/2
19/19 - 1s - loss: 98.9501 - loglik: -9.7555e+01 - logprior: -1.2647e+00
Fitted a model with MAP estimate = -98.1035
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 101.2236 - loglik: -9.8175e+01 - logprior: -3.0014e+00
Epoch 2/10
19/19 - 1s - loss: 98.8321 - loglik: -9.7467e+01 - logprior: -1.2384e+00
Epoch 3/10
19/19 - 1s - loss: 98.0208 - loglik: -9.6603e+01 - logprior: -1.2146e+00
Epoch 4/10
19/19 - 1s - loss: 97.4986 - loglik: -9.6087e+01 - logprior: -1.1716e+00
Epoch 5/10
19/19 - 1s - loss: 97.4015 - loglik: -9.6032e+01 - logprior: -1.1381e+00
Epoch 6/10
19/19 - 1s - loss: 97.0318 - loglik: -9.5690e+01 - logprior: -1.1320e+00
Epoch 7/10
19/19 - 1s - loss: 96.6054 - loglik: -9.5296e+01 - logprior: -1.1107e+00
Epoch 8/10
19/19 - 1s - loss: 96.7562 - loglik: -9.5472e+01 - logprior: -1.0978e+00
Fitted a model with MAP estimate = -96.4782
Time for alignment: 50.8217
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.1154 - loglik: -1.5085e+02 - logprior: -3.2555e+00
Epoch 2/10
19/19 - 1s - loss: 124.1461 - loglik: -1.2259e+02 - logprior: -1.5180e+00
Epoch 3/10
19/19 - 1s - loss: 110.6028 - loglik: -1.0857e+02 - logprior: -1.6502e+00
Epoch 4/10
19/19 - 1s - loss: 107.6343 - loglik: -1.0565e+02 - logprior: -1.6782e+00
Epoch 5/10
19/19 - 1s - loss: 106.4846 - loglik: -1.0464e+02 - logprior: -1.6134e+00
Epoch 6/10
19/19 - 1s - loss: 105.9503 - loglik: -1.0414e+02 - logprior: -1.6181e+00
Epoch 7/10
19/19 - 1s - loss: 105.6354 - loglik: -1.0387e+02 - logprior: -1.5826e+00
Epoch 8/10
19/19 - 1s - loss: 105.6846 - loglik: -1.0393e+02 - logprior: -1.5755e+00
Fitted a model with MAP estimate = -105.2790
expansions: [(3, 1), (5, 1), (7, 1), (10, 1), (16, 1), (17, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 107.3662 - loglik: -1.0417e+02 - logprior: -3.1377e+00
Epoch 2/2
19/19 - 1s - loss: 99.6062 - loglik: -9.8057e+01 - logprior: -1.4044e+00
Fitted a model with MAP estimate = -98.4633
expansions: []
discards: [22 37 40]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.6476 - loglik: -9.8574e+01 - logprior: -3.0284e+00
Epoch 2/2
19/19 - 1s - loss: 98.8503 - loglik: -9.7458e+01 - logprior: -1.2657e+00
Fitted a model with MAP estimate = -98.1154
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 101.2239 - loglik: -9.8181e+01 - logprior: -2.9954e+00
Epoch 2/10
19/19 - 1s - loss: 98.6744 - loglik: -9.7299e+01 - logprior: -1.2426e+00
Epoch 3/10
19/19 - 1s - loss: 98.0772 - loglik: -9.6656e+01 - logprior: -1.2098e+00
Epoch 4/10
19/19 - 1s - loss: 97.6718 - loglik: -9.6262e+01 - logprior: -1.1733e+00
Epoch 5/10
19/19 - 1s - loss: 97.1069 - loglik: -9.5739e+01 - logprior: -1.1365e+00
Epoch 6/10
19/19 - 2s - loss: 96.9089 - loglik: -9.5565e+01 - logprior: -1.1311e+00
Epoch 7/10
19/19 - 1s - loss: 97.0198 - loglik: -9.5713e+01 - logprior: -1.1090e+00
Fitted a model with MAP estimate = -96.5561
Time for alignment: 49.7286
Computed alignments with likelihoods: ['-96.5713', '-96.4782', '-96.5561']
Best model has likelihood: -96.4782
time for generating output: 0.1113
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00046.projection.fasta
SP score = 0.9594907407407407
Training of 3 independent models on file PF01814.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd3d83fd760>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd2560827f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1ddb04640>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd1ef967e80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1f831a130>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd233f95040>, <__main__.SimpleDirichletPrior object at 0x7fd030356d90>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 365.4228 - loglik: -3.6225e+02 - logprior: -3.0608e+00
Epoch 2/10
19/19 - 4s - loss: 336.1872 - loglik: -3.3420e+02 - logprior: -1.1030e+00
Epoch 3/10
19/19 - 4s - loss: 321.2939 - loglik: -3.1825e+02 - logprior: -1.4097e+00
Epoch 4/10
19/19 - 4s - loss: 314.6760 - loglik: -3.1160e+02 - logprior: -1.3943e+00
Epoch 5/10
19/19 - 4s - loss: 312.0193 - loglik: -3.0898e+02 - logprior: -1.4259e+00
Epoch 6/10
19/19 - 4s - loss: 309.1054 - loglik: -3.0626e+02 - logprior: -1.4514e+00
Epoch 7/10
19/19 - 4s - loss: 309.2722 - loglik: -3.0664e+02 - logprior: -1.4591e+00
Fitted a model with MAP estimate = -307.1756
expansions: [(15, 1), (18, 1), (19, 1), (21, 5), (23, 4), (24, 1), (25, 1), (28, 1), (31, 1), (53, 3), (73, 1), (74, 1), (82, 1), (83, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 348.6195 - loglik: -3.4459e+02 - logprior: -3.9264e+00
Epoch 2/2
19/19 - 5s - loss: 320.5846 - loglik: -3.1781e+02 - logprior: -2.1204e+00
Fitted a model with MAP estimate = -313.5947
expansions: [(0, 2), (22, 1)]
discards: [  0 106]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 319.8303 - loglik: -3.1682e+02 - logprior: -2.9085e+00
Epoch 2/2
19/19 - 5s - loss: 312.5687 - loglik: -3.1089e+02 - logprior: -1.1071e+00
Fitted a model with MAP estimate = -308.7680
expansions: []
discards: [ 1 38]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 317.9391 - loglik: -3.1516e+02 - logprior: -2.7055e+00
Epoch 2/10
19/19 - 5s - loss: 311.6619 - loglik: -3.1016e+02 - logprior: -1.0255e+00
Epoch 3/10
19/19 - 5s - loss: 306.8505 - loglik: -3.0463e+02 - logprior: -1.1244e+00
Epoch 4/10
19/19 - 5s - loss: 303.0437 - loglik: -3.0041e+02 - logprior: -1.0306e+00
Epoch 5/10
19/19 - 5s - loss: 301.2854 - loglik: -2.9854e+02 - logprior: -1.0449e+00
Epoch 6/10
19/19 - 5s - loss: 299.5016 - loglik: -2.9694e+02 - logprior: -1.0198e+00
Epoch 7/10
19/19 - 5s - loss: 299.0046 - loglik: -2.9663e+02 - logprior: -1.0067e+00
Epoch 8/10
19/19 - 5s - loss: 297.4219 - loglik: -2.9523e+02 - logprior: -9.7891e-01
Epoch 9/10
19/19 - 5s - loss: 297.4315 - loglik: -2.9537e+02 - logprior: -9.5886e-01
Fitted a model with MAP estimate = -295.8851
Time for alignment: 131.3331
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 365.5240 - loglik: -3.6235e+02 - logprior: -3.0597e+00
Epoch 2/10
19/19 - 4s - loss: 335.3097 - loglik: -3.3333e+02 - logprior: -1.1081e+00
Epoch 3/10
19/19 - 4s - loss: 321.8928 - loglik: -3.1877e+02 - logprior: -1.3930e+00
Epoch 4/10
19/19 - 4s - loss: 315.6752 - loglik: -3.1215e+02 - logprior: -1.3532e+00
Epoch 5/10
19/19 - 4s - loss: 311.7153 - loglik: -3.0842e+02 - logprior: -1.3804e+00
Epoch 6/10
19/19 - 4s - loss: 309.5645 - loglik: -3.0668e+02 - logprior: -1.4253e+00
Epoch 7/10
19/19 - 4s - loss: 308.1617 - loglik: -3.0560e+02 - logprior: -1.4346e+00
Epoch 8/10
19/19 - 4s - loss: 307.5719 - loglik: -3.0512e+02 - logprior: -1.4386e+00
Epoch 9/10
19/19 - 4s - loss: 307.0979 - loglik: -3.0475e+02 - logprior: -1.4434e+00
Epoch 10/10
19/19 - 4s - loss: 306.6843 - loglik: -3.0438e+02 - logprior: -1.4456e+00
Fitted a model with MAP estimate = -305.6484
expansions: [(18, 1), (20, 2), (22, 4), (24, 4), (27, 1), (29, 1), (32, 1), (51, 1), (53, 3), (73, 1), (74, 1), (82, 1), (83, 1), (84, 3), (87, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 356.5153 - loglik: -3.5328e+02 - logprior: -3.1293e+00
Epoch 2/2
19/19 - 5s - loss: 323.1273 - loglik: -3.2119e+02 - logprior: -1.3103e+00
Fitted a model with MAP estimate = -315.2027
expansions: [(98, 3)]
discards: [ 34  65 105]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 322.2872 - loglik: -3.1924e+02 - logprior: -2.9527e+00
Epoch 2/2
19/19 - 5s - loss: 313.8357 - loglik: -3.1236e+02 - logprior: -1.0968e+00
Fitted a model with MAP estimate = -309.9884
expansions: [(2, 1), (27, 2), (30, 1), (64, 1)]
discards: [ 0 33 96 97 98]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 321.4333 - loglik: -3.1756e+02 - logprior: -3.8003e+00
Epoch 2/10
19/19 - 5s - loss: 313.7245 - loglik: -3.1120e+02 - logprior: -2.0129e+00
Epoch 3/10
19/19 - 5s - loss: 308.3611 - loglik: -3.0525e+02 - logprior: -1.8817e+00
Epoch 4/10
19/19 - 5s - loss: 304.1807 - loglik: -3.0109e+02 - logprior: -1.3098e+00
Epoch 5/10
19/19 - 5s - loss: 301.6357 - loglik: -2.9888e+02 - logprior: -9.5887e-01
Epoch 6/10
19/19 - 5s - loss: 300.1476 - loglik: -2.9759e+02 - logprior: -9.4790e-01
Epoch 7/10
19/19 - 5s - loss: 299.0207 - loglik: -2.9668e+02 - logprior: -9.4130e-01
Epoch 8/10
19/19 - 5s - loss: 298.0980 - loglik: -2.9589e+02 - logprior: -9.5156e-01
Epoch 9/10
19/19 - 5s - loss: 297.7592 - loglik: -2.9573e+02 - logprior: -9.1457e-01
Epoch 10/10
19/19 - 5s - loss: 297.5796 - loglik: -2.9565e+02 - logprior: -9.0931e-01
Fitted a model with MAP estimate = -296.1196
Time for alignment: 150.0206
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 365.3329 - loglik: -3.6216e+02 - logprior: -3.0614e+00
Epoch 2/10
19/19 - 4s - loss: 336.0648 - loglik: -3.3409e+02 - logprior: -1.1002e+00
Epoch 3/10
19/19 - 4s - loss: 321.6612 - loglik: -3.1868e+02 - logprior: -1.3973e+00
Epoch 4/10
19/19 - 4s - loss: 314.4145 - loglik: -3.1130e+02 - logprior: -1.4277e+00
Epoch 5/10
19/19 - 4s - loss: 310.6656 - loglik: -3.0752e+02 - logprior: -1.4840e+00
Epoch 6/10
19/19 - 4s - loss: 309.8794 - loglik: -3.0698e+02 - logprior: -1.4768e+00
Epoch 7/10
19/19 - 4s - loss: 308.5786 - loglik: -3.0596e+02 - logprior: -1.4686e+00
Epoch 8/10
19/19 - 4s - loss: 308.0538 - loglik: -3.0557e+02 - logprior: -1.4727e+00
Epoch 9/10
19/19 - 4s - loss: 307.8375 - loglik: -3.0549e+02 - logprior: -1.4725e+00
Epoch 10/10
19/19 - 4s - loss: 307.1069 - loglik: -3.0485e+02 - logprior: -1.4655e+00
Fitted a model with MAP estimate = -306.1742
expansions: [(15, 1), (17, 1), (19, 1), (21, 5), (23, 2), (27, 1), (29, 1), (32, 1), (51, 1), (53, 4), (73, 1), (74, 1), (78, 1), (83, 1), (84, 3), (87, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 354.2801 - loglik: -3.5104e+02 - logprior: -3.1347e+00
Epoch 2/2
19/19 - 5s - loss: 321.9734 - loglik: -3.2001e+02 - logprior: -1.3214e+00
Fitted a model with MAP estimate = -314.6066
expansions: [(22, 1), (26, 1), (96, 1), (97, 1)]
discards: [ 28  32  33  69 105]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 321.8004 - loglik: -3.1879e+02 - logprior: -2.9183e+00
Epoch 2/2
19/19 - 5s - loss: 314.0455 - loglik: -3.1238e+02 - logprior: -1.1017e+00
Fitted a model with MAP estimate = -309.5814
expansions: [(2, 1), (31, 2), (93, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 320.5936 - loglik: -3.1672e+02 - logprior: -3.8082e+00
Epoch 2/10
19/19 - 5s - loss: 313.2698 - loglik: -3.1075e+02 - logprior: -2.0226e+00
Epoch 3/10
19/19 - 5s - loss: 308.0749 - loglik: -3.0496e+02 - logprior: -1.9041e+00
Epoch 4/10
19/19 - 5s - loss: 303.6090 - loglik: -3.0050e+02 - logprior: -1.3505e+00
Epoch 5/10
19/19 - 5s - loss: 301.1475 - loglik: -2.9837e+02 - logprior: -9.9148e-01
Epoch 6/10
19/19 - 5s - loss: 299.4644 - loglik: -2.9687e+02 - logprior: -9.7508e-01
Epoch 7/10
19/19 - 5s - loss: 298.4580 - loglik: -2.9606e+02 - logprior: -9.8016e-01
Epoch 8/10
19/19 - 5s - loss: 297.6773 - loglik: -2.9545e+02 - logprior: -9.5345e-01
Epoch 9/10
19/19 - 5s - loss: 296.9661 - loglik: -2.9487e+02 - logprior: -9.5413e-01
Epoch 10/10
19/19 - 5s - loss: 296.2107 - loglik: -2.9422e+02 - logprior: -9.4304e-01
Fitted a model with MAP estimate = -295.0483
Time for alignment: 149.6310
Computed alignments with likelihoods: ['-295.8851', '-296.1196', '-295.0483']
Best model has likelihood: -295.0483
time for generating output: 0.1693
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01814.projection.fasta
SP score = 0.910871694417238
Training of 3 independent models on file PF00155.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fca45773610>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd24ce72ee0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd24ce72c40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd222684c40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd3e97b5370>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd068585be0>, <__main__.SimpleDirichletPrior object at 0x7fd1f83686d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 39s - loss: 916.2405 - loglik: -9.1407e+02 - logprior: -1.6279e+00
Epoch 2/10
39/39 - 37s - loss: 817.7928 - loglik: -8.1392e+02 - logprior: -1.7887e+00
Epoch 3/10
39/39 - 37s - loss: 803.9525 - loglik: -7.9976e+02 - logprior: -1.9308e+00
Epoch 4/10
39/39 - 37s - loss: 800.0107 - loglik: -7.9597e+02 - logprior: -1.8949e+00
Epoch 5/10
39/39 - 37s - loss: 797.9216 - loglik: -7.9411e+02 - logprior: -1.9018e+00
Epoch 6/10
39/39 - 38s - loss: 796.4349 - loglik: -7.9277e+02 - logprior: -1.9677e+00
Epoch 7/10
39/39 - 40s - loss: 795.5054 - loglik: -7.9191e+02 - logprior: -1.9885e+00
Epoch 8/10
39/39 - 41s - loss: 794.4904 - loglik: -7.9102e+02 - logprior: -1.9691e+00
Epoch 9/10
39/39 - 41s - loss: 794.1478 - loglik: -7.9071e+02 - logprior: -2.0187e+00
Epoch 10/10
39/39 - 40s - loss: 793.6366 - loglik: -7.9031e+02 - logprior: -2.0380e+00
Fitted a model with MAP estimate = -784.0319
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2), (30, 1), (43, 2), (44, 3), (56, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 1), (86, 1), (88, 1), (95, 2), (101, 1), (103, 1), (121, 1), (124, 2), (130, 1), (142, 1), (144, 1), (149, 1), (154, 1), (155, 3), (157, 2), (181, 1), (183, 1), (185, 2), (186, 1), (187, 1), (188, 1), (205, 4), (206, 1), (207, 1), (208, 1), (212, 1), (219, 1), (222, 1), (227, 2), (239, 2), (240, 1), (242, 1), (245, 1), (250, 1), (261, 1), (262, 1), (263, 1), (270, 1), (271, 2), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 353 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 51s - loss: 837.0361 - loglik: -8.3408e+02 - logprior: -2.5327e+00
Epoch 2/2
39/39 - 48s - loss: 782.0737 - loglik: -7.7933e+02 - logprior: -1.1964e+00
Fitted a model with MAP estimate = -765.1292
expansions: [(53, 1), (54, 1), (155, 1), (278, 1), (341, 1)]
discards: [  0   1  30  58 191 197 300 315]
Re-initialized the encoder parameters.
Fitting a model of length 350 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 51s - loss: 798.4630 - loglik: -7.9632e+02 - logprior: -1.8356e+00
Epoch 2/2
39/39 - 48s - loss: 777.1290 - loglik: -7.7525e+02 - logprior: -6.7774e-01
Fitted a model with MAP estimate = -762.0442
expansions: [(0, 2), (180, 1)]
discards: [278 335]
Re-initialized the encoder parameters.
Fitting a model of length 351 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 51s - loss: 787.5015 - loglik: -7.8565e+02 - logprior: -1.5955e+00
Epoch 2/10
39/39 - 48s - loss: 768.3936 - loglik: -7.6668e+02 - logprior: -4.4213e-01
Epoch 3/10
39/39 - 46s - loss: 762.1902 - loglik: -7.5961e+02 - logprior: -3.7450e-01
Epoch 4/10
39/39 - 47s - loss: 759.6756 - loglik: -7.5684e+02 - logprior: -3.2974e-01
Epoch 5/10
39/39 - 47s - loss: 756.9911 - loglik: -7.5420e+02 - logprior: -2.3541e-01
Epoch 6/10
39/39 - 48s - loss: 756.2219 - loglik: -7.5370e+02 - logprior: -1.2412e-01
Epoch 7/10
39/39 - 49s - loss: 754.3333 - loglik: -7.5206e+02 - logprior: -4.0371e-02
Epoch 8/10
39/39 - 49s - loss: 753.3963 - loglik: -7.5150e+02 - logprior: 0.1027
Epoch 9/10
39/39 - 49s - loss: 752.0560 - loglik: -7.5042e+02 - logprior: 0.1833
Epoch 10/10
39/39 - 50s - loss: 751.8183 - loglik: -7.5043e+02 - logprior: 0.2613
Fitted a model with MAP estimate = -748.7318
Time for alignment: 1287.8288
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 39s - loss: 917.1909 - loglik: -9.1502e+02 - logprior: -1.6280e+00
Epoch 2/10
39/39 - 35s - loss: 816.9642 - loglik: -8.1302e+02 - logprior: -1.8874e+00
Epoch 3/10
39/39 - 35s - loss: 802.4796 - loglik: -7.9820e+02 - logprior: -1.9443e+00
Epoch 4/10
39/39 - 34s - loss: 799.5378 - loglik: -7.9543e+02 - logprior: -1.9138e+00
Epoch 5/10
39/39 - 34s - loss: 797.6277 - loglik: -7.9370e+02 - logprior: -1.9528e+00
Epoch 6/10
39/39 - 34s - loss: 796.4528 - loglik: -7.9271e+02 - logprior: -1.9856e+00
Epoch 7/10
39/39 - 34s - loss: 795.5862 - loglik: -7.9195e+02 - logprior: -1.9930e+00
Epoch 8/10
39/39 - 34s - loss: 794.7133 - loglik: -7.9117e+02 - logprior: -2.0297e+00
Epoch 9/10
39/39 - 34s - loss: 794.0842 - loglik: -7.9065e+02 - logprior: -2.0104e+00
Epoch 10/10
39/39 - 34s - loss: 793.4564 - loglik: -7.9010e+02 - logprior: -2.0483e+00
Fitted a model with MAP estimate = -783.9775
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (24, 1), (25, 3), (37, 1), (43, 3), (44, 2), (48, 2), (56, 1), (58, 1), (61, 1), (78, 1), (79, 1), (80, 1), (81, 1), (85, 1), (87, 1), (94, 3), (102, 1), (119, 1), (124, 2), (132, 2), (145, 1), (148, 1), (149, 1), (153, 2), (154, 1), (155, 2), (166, 1), (181, 1), (184, 1), (185, 1), (187, 1), (188, 1), (189, 1), (206, 4), (207, 1), (208, 1), (209, 1), (218, 1), (220, 1), (227, 2), (228, 2), (240, 2), (242, 1), (245, 1), (255, 1), (261, 1), (262, 1), (263, 1), (269, 2), (270, 2), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 357 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 838.2667 - loglik: -8.3529e+02 - logprior: -2.5565e+00
Epoch 2/2
39/39 - 49s - loss: 781.4326 - loglik: -7.7870e+02 - logprior: -1.1682e+00
Fitted a model with MAP estimate = -764.6970
expansions: [(55, 1), (157, 1), (305, 1)]
discards: [  0  63 120 166 192 288 312 341]
Re-initialized the encoder parameters.
Fitting a model of length 352 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 800.0082 - loglik: -7.9716e+02 - logprior: -2.5519e+00
Epoch 2/2
39/39 - 49s - loss: 777.9114 - loglik: -7.7591e+02 - logprior: -8.0839e-01
Fitted a model with MAP estimate = -762.4363
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 353 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 53s - loss: 787.3170 - loglik: -7.8572e+02 - logprior: -1.3226e+00
Epoch 2/10
39/39 - 47s - loss: 769.0464 - loglik: -7.6734e+02 - logprior: -4.3079e-01
Epoch 3/10
39/39 - 48s - loss: 762.2650 - loglik: -7.5962e+02 - logprior: -3.8278e-01
Epoch 4/10
39/39 - 48s - loss: 758.2956 - loglik: -7.5545e+02 - logprior: -2.9127e-01
Epoch 5/10
39/39 - 50s - loss: 757.0084 - loglik: -7.5421e+02 - logprior: -2.2795e-01
Epoch 6/10
39/39 - 53s - loss: 756.1120 - loglik: -7.5359e+02 - logprior: -1.1980e-01
Epoch 7/10
39/39 - 55s - loss: 753.6871 - loglik: -7.5141e+02 - logprior: -7.3331e-02
Epoch 8/10
39/39 - 57s - loss: 754.0692 - loglik: -7.5215e+02 - logprior: 0.0543
Fitted a model with MAP estimate = -750.1060
Time for alignment: 1180.6135
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 42s - loss: 916.8184 - loglik: -9.1466e+02 - logprior: -1.6224e+00
Epoch 2/10
39/39 - 38s - loss: 820.3846 - loglik: -8.1660e+02 - logprior: -1.7452e+00
Epoch 3/10
39/39 - 37s - loss: 804.4216 - loglik: -8.0025e+02 - logprior: -1.9875e+00
Epoch 4/10
39/39 - 37s - loss: 800.0592 - loglik: -7.9590e+02 - logprior: -1.9676e+00
Epoch 5/10
39/39 - 38s - loss: 798.1677 - loglik: -7.9416e+02 - logprior: -1.9930e+00
Epoch 6/10
39/39 - 39s - loss: 796.7508 - loglik: -7.9290e+02 - logprior: -2.0289e+00
Epoch 7/10
39/39 - 37s - loss: 795.9062 - loglik: -7.9219e+02 - logprior: -2.0363e+00
Epoch 8/10
39/39 - 36s - loss: 794.9913 - loglik: -7.9141e+02 - logprior: -2.0501e+00
Epoch 9/10
39/39 - 36s - loss: 794.3855 - loglik: -7.9088e+02 - logprior: -2.1061e+00
Epoch 10/10
39/39 - 37s - loss: 793.7316 - loglik: -7.9036e+02 - logprior: -2.1015e+00
Fitted a model with MAP estimate = -784.4882
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2), (26, 1), (30, 1), (40, 2), (41, 1), (42, 1), (44, 2), (56, 1), (57, 1), (58, 1), (61, 1), (78, 1), (80, 1), (81, 1), (86, 1), (91, 2), (92, 1), (93, 2), (94, 1), (100, 1), (103, 1), (121, 1), (124, 2), (132, 2), (145, 1), (150, 1), (153, 2), (156, 2), (162, 1), (181, 1), (184, 1), (186, 2), (187, 1), (188, 2), (189, 1), (206, 4), (207, 1), (208, 1), (209, 1), (211, 1), (219, 1), (220, 1), (226, 1), (228, 1), (240, 3), (242, 1), (258, 1), (261, 1), (262, 1), (263, 1), (270, 3), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 358 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 57s - loss: 835.6947 - loglik: -8.3277e+02 - logprior: -2.5051e+00
Epoch 2/2
39/39 - 53s - loss: 780.1118 - loglik: -7.7739e+02 - logprior: -1.1162e+00
Fitted a model with MAP estimate = -763.2286
expansions: [(160, 1), (257, 1)]
discards: [  0   1  30 117 121 169 194 235 260 345]
Re-initialized the encoder parameters.
Fitting a model of length 350 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 62s - loss: 797.9245 - loglik: -7.9594e+02 - logprior: -1.6884e+00
Epoch 2/2
39/39 - 60s - loss: 777.0604 - loglik: -7.7524e+02 - logprior: -6.3420e-01
Fitted a model with MAP estimate = -762.0371
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 352 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 60s - loss: 786.2999 - loglik: -7.8438e+02 - logprior: -1.6621e+00
Epoch 2/10
39/39 - 58s - loss: 768.7374 - loglik: -7.6700e+02 - logprior: -4.6681e-01
Epoch 3/10
39/39 - 58s - loss: 761.0406 - loglik: -7.5843e+02 - logprior: -4.0348e-01
Epoch 4/10
39/39 - 56s - loss: 758.9536 - loglik: -7.5611e+02 - logprior: -3.1328e-01
Epoch 5/10
39/39 - 53s - loss: 756.2440 - loglik: -7.5343e+02 - logprior: -2.5946e-01
Epoch 6/10
39/39 - 56s - loss: 755.4229 - loglik: -7.5290e+02 - logprior: -1.3021e-01
Epoch 7/10
39/39 - 55s - loss: 753.3646 - loglik: -7.5112e+02 - logprior: -4.7535e-02
Epoch 8/10
39/39 - 51s - loss: 753.5618 - loglik: -7.5164e+02 - logprior: 0.0691
Fitted a model with MAP estimate = -749.5873
Time for alignment: 1329.5289
Computed alignments with likelihoods: ['-748.7318', '-750.1060', '-749.5873']
Best model has likelihood: -748.7318
time for generating output: 1.0750
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00155.projection.fasta
SP score = 0.46823669271696894
Training of 3 independent models on file PF00450.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2112bec10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd1f811a910>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1f811ab80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd0685d5d00>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd24ced22e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fca4464da30>, <__main__.SimpleDirichletPrior object at 0x7fd2445f96a0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 59s - loss: 891.4844 - loglik: -8.8954e+02 - logprior: -1.6990e+00
Epoch 2/10
39/39 - 59s - loss: 753.7648 - loglik: -7.5075e+02 - logprior: -1.8251e+00
Epoch 3/10
39/39 - 56s - loss: 741.1323 - loglik: -7.3776e+02 - logprior: -2.0358e+00
Epoch 4/10
39/39 - 56s - loss: 736.9692 - loglik: -7.3358e+02 - logprior: -2.0934e+00
Epoch 5/10
39/39 - 60s - loss: 735.3190 - loglik: -7.3201e+02 - logprior: -2.0954e+00
Epoch 6/10
39/39 - 62s - loss: 734.6185 - loglik: -7.3133e+02 - logprior: -2.1281e+00
Epoch 7/10
39/39 - 64s - loss: 734.4560 - loglik: -7.3123e+02 - logprior: -2.1271e+00
Epoch 8/10
39/39 - 64s - loss: 732.6461 - loglik: -7.2934e+02 - logprior: -2.1969e+00
Epoch 9/10
39/39 - 63s - loss: 733.1622 - loglik: -7.2995e+02 - logprior: -2.1826e+00
Fitted a model with MAP estimate = -730.1131
expansions: [(9, 1), (19, 1), (21, 1), (31, 1), (67, 1), (102, 2), (105, 1), (107, 1), (110, 1), (122, 1), (142, 2), (143, 1), (144, 1), (145, 3), (146, 2), (147, 1), (161, 3), (162, 1), (163, 1), (164, 1), (169, 1), (177, 2), (178, 4), (181, 1), (182, 5), (184, 1), (185, 1), (188, 1), (189, 2), (190, 2), (191, 1), (202, 1), (203, 2), (211, 2), (213, 1), (218, 2), (219, 1), (220, 1), (228, 1), (233, 1), (236, 1), (240, 1), (242, 1), (244, 2), (246, 1), (249, 1), (250, 2), (256, 1), (277, 1), (278, 1), (279, 1), (282, 1), (283, 5), (288, 1), (289, 2), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  0   1 193 194 195 196 197]
Re-initialized the encoder parameters.
Fitting a model of length 407 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 96s - loss: 737.3857 - loglik: -7.3416e+02 - logprior: -2.9718e+00
Epoch 2/2
39/39 - 91s - loss: 713.9834 - loglik: -7.1140e+02 - logprior: -1.8635e+00
Fitted a model with MAP estimate = -708.0648
expansions: [(4, 2), (232, 1), (233, 2), (234, 2), (248, 1)]
discards: [  0   1 160 164 208 213 214 259 266 301 351 352 353 362]
Re-initialized the encoder parameters.
Fitting a model of length 401 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 90s - loss: 722.4495 - loglik: -7.1974e+02 - logprior: -2.4398e+00
Epoch 2/2
39/39 - 90s - loss: 712.1416 - loglik: -7.1005e+02 - logprior: -1.1734e+00
Fitted a model with MAP estimate = -706.8940
expansions: [(4, 1), (5, 1), (6, 1), (164, 1), (210, 1), (246, 1)]
discards: [  0   1 235 236 237 238 239 240 241 242 249 250 251 252 253 300]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 84s - loss: 724.0229 - loglik: -7.2166e+02 - logprior: -2.1115e+00
Epoch 2/10
39/39 - 74s - loss: 713.7120 - loglik: -7.1227e+02 - logprior: -5.7770e-01
Epoch 3/10
39/39 - 76s - loss: 710.3135 - loglik: -7.0839e+02 - logprior: -4.6909e-01
Epoch 4/10
39/39 - 79s - loss: 708.0126 - loglik: -7.0606e+02 - logprior: -3.6653e-01
Epoch 5/10
39/39 - 74s - loss: 707.1643 - loglik: -7.0547e+02 - logprior: -2.0483e-01
Epoch 6/10
39/39 - 81s - loss: 706.4593 - loglik: -7.0497e+02 - logprior: -7.0109e-02
Epoch 7/10
39/39 - 87s - loss: 705.3852 - loglik: -7.0397e+02 - logprior: -8.2020e-03
Epoch 8/10
39/39 - 88s - loss: 704.3627 - loglik: -7.0311e+02 - logprior: 0.1524
Epoch 9/10
39/39 - 82s - loss: 703.7612 - loglik: -7.0249e+02 - logprior: 0.1501
Epoch 10/10
39/39 - 69s - loss: 703.5657 - loglik: -7.0246e+02 - logprior: 0.3781
Fitted a model with MAP estimate = -700.3726
Time for alignment: 2072.5340
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 54s - loss: 892.8344 - loglik: -8.9089e+02 - logprior: -1.7015e+00
Epoch 2/10
39/39 - 51s - loss: 753.5120 - loglik: -7.5082e+02 - logprior: -1.9329e+00
Epoch 3/10
39/39 - 53s - loss: 739.6102 - loglik: -7.3625e+02 - logprior: -2.1236e+00
Epoch 4/10
39/39 - 53s - loss: 736.5651 - loglik: -7.3330e+02 - logprior: -2.1071e+00
Epoch 5/10
39/39 - 54s - loss: 735.0327 - loglik: -7.3174e+02 - logprior: -2.1774e+00
Epoch 6/10
39/39 - 53s - loss: 735.4368 - loglik: -7.3209e+02 - logprior: -2.2262e+00
Fitted a model with MAP estimate = -731.6038
expansions: [(9, 1), (19, 2), (20, 1), (34, 1), (66, 1), (68, 1), (100, 2), (104, 2), (105, 1), (112, 1), (121, 1), (140, 1), (142, 1), (143, 1), (145, 5), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (168, 1), (177, 1), (178, 5), (181, 5), (182, 2), (183, 1), (188, 1), (189, 1), (190, 1), (191, 2), (204, 2), (212, 2), (220, 2), (221, 1), (222, 1), (230, 1), (235, 1), (239, 1), (242, 1), (244, 2), (245, 2), (248, 1), (253, 1), (254, 2), (257, 1), (280, 1), (281, 1), (284, 1), (285, 5), (289, 1), (290, 2), (292, 1), (315, 1), (316, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1 192 193 194 195 196 208 209]
Re-initialized the encoder parameters.
Fitting a model of length 404 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 76s - loss: 734.4956 - loglik: -7.3210e+02 - logprior: -2.1368e+00
Epoch 2/2
39/39 - 71s - loss: 713.9727 - loglik: -7.1224e+02 - logprior: -1.0416e+00
Fitted a model with MAP estimate = -708.6134
expansions: [(235, 5), (250, 1), (251, 1)]
discards: [106 208 209 218 219 236 237 244 263 297 298 299 350 351 359]
Re-initialized the encoder parameters.
Fitting a model of length 396 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 72s - loss: 722.7256 - loglik: -7.2083e+02 - logprior: -1.6274e+00
Epoch 2/2
39/39 - 70s - loss: 713.3470 - loglik: -7.1183e+02 - logprior: -6.0310e-01
Fitted a model with MAP estimate = -707.6940
expansions: [(5, 1), (232, 3), (295, 2)]
discards: [236 237 238 239 240 241]
Re-initialized the encoder parameters.
Fitting a model of length 396 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 75s - loss: 718.5140 - loglik: -7.1680e+02 - logprior: -1.4647e+00
Epoch 2/10
39/39 - 73s - loss: 709.5771 - loglik: -7.0845e+02 - logprior: -2.5890e-01
Epoch 3/10
39/39 - 76s - loss: 706.7953 - loglik: -7.0521e+02 - logprior: -2.1908e-01
Epoch 4/10
39/39 - 75s - loss: 704.9190 - loglik: -7.0340e+02 - logprior: -1.3739e-01
Epoch 5/10
39/39 - 75s - loss: 703.9971 - loglik: -7.0257e+02 - logprior: -1.0712e-01
Epoch 6/10
39/39 - 76s - loss: 702.9894 - loglik: -7.0179e+02 - logprior: 0.0522
Epoch 7/10
39/39 - 78s - loss: 702.1195 - loglik: -7.0119e+02 - logprior: 0.2694
Epoch 8/10
39/39 - 81s - loss: 701.2216 - loglik: -7.0036e+02 - logprior: 0.3485
Epoch 9/10
39/39 - 81s - loss: 701.5685 - loglik: -7.0077e+02 - logprior: 0.3835
Fitted a model with MAP estimate = -698.8941
Time for alignment: 1582.0825
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 59s - loss: 893.8959 - loglik: -8.9195e+02 - logprior: -1.6980e+00
Epoch 2/10
39/39 - 58s - loss: 754.7921 - loglik: -7.5240e+02 - logprior: -1.8236e+00
Epoch 3/10
39/39 - 57s - loss: 741.4142 - loglik: -7.3822e+02 - logprior: -2.0732e+00
Epoch 4/10
39/39 - 56s - loss: 738.4961 - loglik: -7.3532e+02 - logprior: -2.0168e+00
Epoch 5/10
39/39 - 56s - loss: 736.8197 - loglik: -7.3361e+02 - logprior: -2.0462e+00
Epoch 6/10
39/39 - 56s - loss: 735.4795 - loglik: -7.3227e+02 - logprior: -2.0670e+00
Epoch 7/10
39/39 - 55s - loss: 735.0450 - loglik: -7.3182e+02 - logprior: -2.1073e+00
Epoch 8/10
39/39 - 58s - loss: 735.2557 - loglik: -7.3205e+02 - logprior: -2.1043e+00
Fitted a model with MAP estimate = -731.8943
expansions: [(6, 1), (20, 2), (21, 1), (31, 1), (66, 1), (97, 1), (100, 2), (103, 1), (105, 1), (106, 1), (114, 1), (119, 1), (120, 1), (138, 2), (139, 1), (140, 1), (142, 2), (143, 3), (158, 1), (160, 2), (161, 1), (162, 1), (175, 4), (176, 1), (177, 1), (180, 1), (181, 3), (182, 1), (184, 1), (185, 1), (188, 2), (189, 3), (190, 1), (203, 2), (219, 2), (220, 1), (221, 1), (226, 1), (234, 1), (235, 3), (237, 1), (240, 1), (242, 2), (243, 2), (249, 1), (250, 2), (256, 1), (277, 1), (278, 3), (283, 4), (285, 1), (296, 1), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1 191 192 193 194 195 196]
Re-initialized the encoder parameters.
Fitting a model of length 404 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 84s - loss: 734.3503 - loglik: -7.3203e+02 - logprior: -2.0479e+00
Epoch 2/2
39/39 - 82s - loss: 713.2935 - loglik: -7.1168e+02 - logprior: -9.1963e-01
Fitted a model with MAP estimate = -707.7046
expansions: [(214, 2), (233, 2), (234, 1), (238, 1)]
discards: [  5 160 204 205 244 263 284 299 349 350]
Re-initialized the encoder parameters.
Fitting a model of length 400 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 80s - loss: 719.9935 - loglik: -7.1809e+02 - logprior: -1.6348e+00
Epoch 2/2
39/39 - 80s - loss: 710.9154 - loglik: -7.0945e+02 - logprior: -5.4835e-01
Fitted a model with MAP estimate = -706.0319
expansions: [(231, 1)]
discards: [210 211 235 236 237 238 239]
Re-initialized the encoder parameters.
Fitting a model of length 394 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 75s - loss: 719.8423 - loglik: -7.1823e+02 - logprior: -1.3600e+00
Epoch 2/10
39/39 - 70s - loss: 710.9870 - loglik: -7.0992e+02 - logprior: -1.9985e-01
Epoch 3/10
39/39 - 69s - loss: 706.9227 - loglik: -7.0542e+02 - logprior: -1.3600e-01
Epoch 4/10
39/39 - 68s - loss: 705.3397 - loglik: -7.0389e+02 - logprior: -3.9437e-02
Epoch 5/10
39/39 - 67s - loss: 705.6143 - loglik: -7.0423e+02 - logprior: -3.3441e-02
Fitted a model with MAP estimate = -702.2276
Time for alignment: 1475.1780
Computed alignments with likelihoods: ['-700.3726', '-698.8941', '-702.2276']
Best model has likelihood: -698.8941
time for generating output: 0.4913
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00450.projection.fasta
SP score = 0.8169059749884205
Training of 3 independent models on file PF07679.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd24ceeed30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd222c7b9d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd3c6c24070>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd0680b4550>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd0680b4a90>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd1e6122580>, <__main__.SimpleDirichletPrior object at 0x7fca3db8f280>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 252.5219 - loglik: -2.4932e+02 - logprior: -3.1194e+00
Epoch 2/10
19/19 - 2s - loss: 220.8674 - loglik: -2.1930e+02 - logprior: -1.3077e+00
Epoch 3/10
19/19 - 2s - loss: 207.7289 - loglik: -2.0568e+02 - logprior: -1.5344e+00
Epoch 4/10
19/19 - 2s - loss: 205.3431 - loglik: -2.0345e+02 - logprior: -1.4600e+00
Epoch 5/10
19/19 - 2s - loss: 204.2769 - loglik: -2.0244e+02 - logprior: -1.4609e+00
Epoch 6/10
19/19 - 2s - loss: 203.8172 - loglik: -2.0203e+02 - logprior: -1.4570e+00
Epoch 7/10
19/19 - 2s - loss: 203.6161 - loglik: -2.0182e+02 - logprior: -1.4465e+00
Epoch 8/10
19/19 - 2s - loss: 203.1231 - loglik: -2.0132e+02 - logprior: -1.4425e+00
Epoch 9/10
19/19 - 2s - loss: 202.9728 - loglik: -2.0116e+02 - logprior: -1.4400e+00
Epoch 10/10
19/19 - 2s - loss: 203.1322 - loglik: -2.0132e+02 - logprior: -1.4376e+00
Fitted a model with MAP estimate = -202.2565
expansions: [(8, 1), (9, 3), (10, 1), (13, 1), (14, 1), (15, 1), (35, 1), (37, 2), (39, 1), (41, 2), (43, 2), (47, 1), (48, 2), (49, 2), (60, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 210.7138 - loglik: -2.0665e+02 - logprior: -3.9739e+00
Epoch 2/2
19/19 - 2s - loss: 199.8062 - loglik: -1.9727e+02 - logprior: -2.2041e+00
Fitted a model with MAP estimate = -197.6835
expansions: [(0, 2)]
discards: [ 0 10 45 53 67]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 200.6087 - loglik: -1.9761e+02 - logprior: -2.9387e+00
Epoch 2/2
19/19 - 2s - loss: 196.6425 - loglik: -1.9515e+02 - logprior: -1.1662e+00
Fitted a model with MAP estimate = -195.1631
expansions: [(54, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 201.9005 - loglik: -1.9804e+02 - logprior: -3.7939e+00
Epoch 2/10
19/19 - 2s - loss: 196.9860 - loglik: -1.9541e+02 - logprior: -1.3257e+00
Epoch 3/10
19/19 - 2s - loss: 195.3959 - loglik: -1.9395e+02 - logprior: -1.1056e+00
Epoch 4/10
19/19 - 2s - loss: 194.6456 - loglik: -1.9311e+02 - logprior: -1.0906e+00
Epoch 5/10
19/19 - 2s - loss: 194.1892 - loglik: -1.9263e+02 - logprior: -1.0625e+00
Epoch 6/10
19/19 - 2s - loss: 193.7794 - loglik: -1.9222e+02 - logprior: -1.0455e+00
Epoch 7/10
19/19 - 2s - loss: 193.2202 - loglik: -1.9168e+02 - logprior: -1.0360e+00
Epoch 8/10
19/19 - 2s - loss: 193.2215 - loglik: -1.9169e+02 - logprior: -1.0167e+00
Fitted a model with MAP estimate = -192.2391
Time for alignment: 79.3775
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 252.7074 - loglik: -2.4951e+02 - logprior: -3.1159e+00
Epoch 2/10
19/19 - 2s - loss: 222.7729 - loglik: -2.2113e+02 - logprior: -1.3205e+00
Epoch 3/10
19/19 - 2s - loss: 208.6792 - loglik: -2.0642e+02 - logprior: -1.6091e+00
Epoch 4/10
19/19 - 2s - loss: 204.0778 - loglik: -2.0200e+02 - logprior: -1.5729e+00
Epoch 5/10
19/19 - 2s - loss: 202.5984 - loglik: -2.0063e+02 - logprior: -1.6063e+00
Epoch 6/10
19/19 - 2s - loss: 202.3139 - loglik: -2.0037e+02 - logprior: -1.5810e+00
Epoch 7/10
19/19 - 2s - loss: 201.8595 - loglik: -1.9992e+02 - logprior: -1.5709e+00
Epoch 8/10
19/19 - 2s - loss: 201.6681 - loglik: -1.9975e+02 - logprior: -1.5569e+00
Epoch 9/10
19/19 - 2s - loss: 201.5961 - loglik: -1.9967e+02 - logprior: -1.5551e+00
Epoch 10/10
19/19 - 2s - loss: 201.5816 - loglik: -1.9966e+02 - logprior: -1.5549e+00
Fitted a model with MAP estimate = -200.8422
expansions: [(9, 1), (10, 1), (11, 1), (13, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (39, 1), (41, 2), (43, 2), (48, 1), (49, 2), (50, 1), (58, 1), (59, 1), (60, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 210.6154 - loglik: -2.0661e+02 - logprior: -3.9255e+00
Epoch 2/2
19/19 - 2s - loss: 199.7535 - loglik: -1.9723e+02 - logprior: -2.1765e+00
Fitted a model with MAP estimate = -197.5808
expansions: [(0, 2)]
discards: [ 0 51 64]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 200.2131 - loglik: -1.9721e+02 - logprior: -2.9366e+00
Epoch 2/2
19/19 - 2s - loss: 196.3559 - loglik: -1.9486e+02 - logprior: -1.1696e+00
Fitted a model with MAP estimate = -194.8761
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 201.6066 - loglik: -1.9780e+02 - logprior: -3.7426e+00
Epoch 2/10
19/19 - 2s - loss: 196.8079 - loglik: -1.9530e+02 - logprior: -1.2961e+00
Epoch 3/10
19/19 - 2s - loss: 195.2838 - loglik: -1.9383e+02 - logprior: -1.1223e+00
Epoch 4/10
19/19 - 2s - loss: 194.6551 - loglik: -1.9312e+02 - logprior: -1.0802e+00
Epoch 5/10
19/19 - 2s - loss: 194.1379 - loglik: -1.9257e+02 - logprior: -1.0623e+00
Epoch 6/10
19/19 - 2s - loss: 193.8380 - loglik: -1.9227e+02 - logprior: -1.0505e+00
Epoch 7/10
19/19 - 2s - loss: 193.2733 - loglik: -1.9173e+02 - logprior: -1.0286e+00
Epoch 8/10
19/19 - 2s - loss: 193.1703 - loglik: -1.9164e+02 - logprior: -1.0191e+00
Epoch 9/10
19/19 - 2s - loss: 192.4885 - loglik: -1.9095e+02 - logprior: -1.0075e+00
Epoch 10/10
19/19 - 2s - loss: 192.2911 - loglik: -1.9074e+02 - logprior: -1.0008e+00
Fitted a model with MAP estimate = -191.4408
Time for alignment: 83.8704
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 252.5993 - loglik: -2.4940e+02 - logprior: -3.1166e+00
Epoch 2/10
19/19 - 2s - loss: 221.2614 - loglik: -2.1965e+02 - logprior: -1.3175e+00
Epoch 3/10
19/19 - 2s - loss: 208.2320 - loglik: -2.0624e+02 - logprior: -1.5236e+00
Epoch 4/10
19/19 - 2s - loss: 205.8685 - loglik: -2.0399e+02 - logprior: -1.4019e+00
Epoch 5/10
19/19 - 2s - loss: 205.2919 - loglik: -2.0346e+02 - logprior: -1.3932e+00
Epoch 6/10
19/19 - 2s - loss: 204.8609 - loglik: -2.0308e+02 - logprior: -1.3615e+00
Epoch 7/10
19/19 - 2s - loss: 204.4496 - loglik: -2.0269e+02 - logprior: -1.3547e+00
Epoch 8/10
19/19 - 2s - loss: 204.0760 - loglik: -2.0233e+02 - logprior: -1.3525e+00
Epoch 9/10
19/19 - 2s - loss: 204.0544 - loglik: -2.0231e+02 - logprior: -1.3462e+00
Epoch 10/10
19/19 - 2s - loss: 203.8297 - loglik: -2.0208e+02 - logprior: -1.3452e+00
Fitted a model with MAP estimate = -203.1766
expansions: [(8, 1), (9, 2), (10, 3), (12, 1), (19, 1), (24, 1), (34, 1), (39, 1), (41, 2), (43, 2), (48, 1), (49, 2), (60, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 212.4161 - loglik: -2.0838e+02 - logprior: -3.9433e+00
Epoch 2/2
19/19 - 2s - loss: 200.1655 - loglik: -1.9761e+02 - logprior: -2.1912e+00
Fitted a model with MAP estimate = -197.7659
expansions: [(0, 2)]
discards: [ 0 13 65]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 200.5668 - loglik: -1.9755e+02 - logprior: -2.9543e+00
Epoch 2/2
19/19 - 2s - loss: 196.8426 - loglik: -1.9534e+02 - logprior: -1.1668e+00
Fitted a model with MAP estimate = -195.1484
expansions: [(55, 1)]
discards: [ 0 52]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 201.9116 - loglik: -1.9806e+02 - logprior: -3.7834e+00
Epoch 2/10
19/19 - 2s - loss: 196.7560 - loglik: -1.9511e+02 - logprior: -1.3251e+00
Epoch 3/10
19/19 - 2s - loss: 195.4239 - loglik: -1.9374e+02 - logprior: -1.1200e+00
Epoch 4/10
19/19 - 2s - loss: 194.7166 - loglik: -1.9305e+02 - logprior: -1.0832e+00
Epoch 5/10
19/19 - 2s - loss: 194.2196 - loglik: -1.9261e+02 - logprior: -1.0640e+00
Epoch 6/10
19/19 - 2s - loss: 193.7009 - loglik: -1.9214e+02 - logprior: -1.0372e+00
Epoch 7/10
19/19 - 2s - loss: 193.3420 - loglik: -1.9179e+02 - logprior: -1.0404e+00
Epoch 8/10
19/19 - 2s - loss: 192.8433 - loglik: -1.9131e+02 - logprior: -1.0167e+00
Epoch 9/10
19/19 - 2s - loss: 192.8942 - loglik: -1.9135e+02 - logprior: -1.0079e+00
Fitted a model with MAP estimate = -191.8446
Time for alignment: 81.0005
Computed alignments with likelihoods: ['-192.2391', '-191.4408', '-191.8446']
Best model has likelihood: -191.4408
time for generating output: 0.1499
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07679.projection.fasta
SP score = 0.7743632336655593
Training of 3 independent models on file PF07686.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd3e978f730>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd3e96823a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fca34646fa0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fca296aef10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fca296aed90>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd02902dd90>, <__main__.SimpleDirichletPrior object at 0x7fca5c02ffa0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 301.6702 - loglik: -2.9849e+02 - logprior: -3.0917e+00
Epoch 2/10
19/19 - 3s - loss: 272.6742 - loglik: -2.7094e+02 - logprior: -1.2848e+00
Epoch 3/10
19/19 - 2s - loss: 258.4388 - loglik: -2.5642e+02 - logprior: -1.3684e+00
Epoch 4/10
19/19 - 3s - loss: 254.3755 - loglik: -2.5229e+02 - logprior: -1.2846e+00
Epoch 5/10
19/19 - 3s - loss: 253.0006 - loglik: -2.5094e+02 - logprior: -1.2621e+00
Epoch 6/10
19/19 - 3s - loss: 252.2832 - loglik: -2.5024e+02 - logprior: -1.2555e+00
Epoch 7/10
19/19 - 3s - loss: 251.5538 - loglik: -2.4954e+02 - logprior: -1.2485e+00
Epoch 8/10
19/19 - 3s - loss: 250.7260 - loglik: -2.4874e+02 - logprior: -1.2561e+00
Epoch 9/10
19/19 - 3s - loss: 250.8300 - loglik: -2.4890e+02 - logprior: -1.2479e+00
Fitted a model with MAP estimate = -249.1232
expansions: [(0, 2), (17, 1), (20, 2), (21, 2), (22, 1), (23, 1), (24, 1), (39, 2), (41, 3), (42, 1), (44, 2), (51, 1), (52, 1), (74, 1), (75, 2), (76, 2), (77, 4), (78, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 114 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 266.9921 - loglik: -2.6263e+02 - logprior: -4.2746e+00
Epoch 2/2
19/19 - 3s - loss: 254.2357 - loglik: -2.5249e+02 - logprior: -1.4163e+00
Fitted a model with MAP estimate = -250.7794
expansions: [(54, 1)]
discards: [ 1 24 28 56 60 97]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 255.7705 - loglik: -2.5276e+02 - logprior: -2.9521e+00
Epoch 2/2
19/19 - 3s - loss: 252.0713 - loglik: -2.5055e+02 - logprior: -1.2150e+00
Fitted a model with MAP estimate = -249.4934
expansions: [(53, 1)]
discards: [95]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 254.3965 - loglik: -2.5142e+02 - logprior: -2.9087e+00
Epoch 2/10
19/19 - 3s - loss: 250.6259 - loglik: -2.4915e+02 - logprior: -1.1292e+00
Epoch 3/10
19/19 - 3s - loss: 248.9231 - loglik: -2.4723e+02 - logprior: -1.0256e+00
Epoch 4/10
19/19 - 3s - loss: 247.1441 - loglik: -2.4528e+02 - logprior: -9.7539e-01
Epoch 5/10
19/19 - 3s - loss: 246.7212 - loglik: -2.4479e+02 - logprior: -9.8259e-01
Epoch 6/10
19/19 - 3s - loss: 245.2250 - loglik: -2.4329e+02 - logprior: -9.6661e-01
Epoch 7/10
19/19 - 3s - loss: 244.6205 - loglik: -2.4268e+02 - logprior: -9.5789e-01
Epoch 8/10
19/19 - 3s - loss: 243.4517 - loglik: -2.4153e+02 - logprior: -9.5114e-01
Epoch 9/10
19/19 - 3s - loss: 243.5795 - loglik: -2.4170e+02 - logprior: -9.4151e-01
Fitted a model with MAP estimate = -241.7793
Time for alignment: 97.1384
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 301.7584 - loglik: -2.9858e+02 - logprior: -3.0929e+00
Epoch 2/10
19/19 - 2s - loss: 270.2829 - loglik: -2.6852e+02 - logprior: -1.2981e+00
Epoch 3/10
19/19 - 3s - loss: 258.0338 - loglik: -2.5588e+02 - logprior: -1.3347e+00
Epoch 4/10
19/19 - 3s - loss: 254.6758 - loglik: -2.5266e+02 - logprior: -1.2301e+00
Epoch 5/10
19/19 - 3s - loss: 253.1149 - loglik: -2.5106e+02 - logprior: -1.2494e+00
Epoch 6/10
19/19 - 2s - loss: 251.9196 - loglik: -2.4984e+02 - logprior: -1.2476e+00
Epoch 7/10
19/19 - 2s - loss: 251.4996 - loglik: -2.4944e+02 - logprior: -1.2443e+00
Epoch 8/10
19/19 - 3s - loss: 250.6571 - loglik: -2.4865e+02 - logprior: -1.2443e+00
Epoch 9/10
19/19 - 3s - loss: 250.3957 - loglik: -2.4846e+02 - logprior: -1.2452e+00
Epoch 10/10
19/19 - 3s - loss: 250.1745 - loglik: -2.4828e+02 - logprior: -1.2461e+00
Fitted a model with MAP estimate = -248.8408
expansions: [(0, 2), (17, 1), (20, 3), (21, 2), (22, 1), (23, 1), (38, 2), (41, 3), (42, 1), (44, 2), (53, 1), (54, 1), (74, 1), (75, 2), (76, 1), (77, 4), (78, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 113 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 267.6781 - loglik: -2.6327e+02 - logprior: -4.3274e+00
Epoch 2/2
19/19 - 3s - loss: 254.5270 - loglik: -2.5275e+02 - logprior: -1.4687e+00
Fitted a model with MAP estimate = -250.7913
expansions: [(106, 1)]
discards: [ 1 28 56 60 98]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 255.7040 - loglik: -2.5269e+02 - logprior: -2.9518e+00
Epoch 2/2
19/19 - 3s - loss: 251.4434 - loglik: -2.4991e+02 - logprior: -1.2138e+00
Fitted a model with MAP estimate = -248.9492
expansions: [(52, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 254.0892 - loglik: -2.5110e+02 - logprior: -2.9252e+00
Epoch 2/10
19/19 - 3s - loss: 250.5895 - loglik: -2.4915e+02 - logprior: -1.1396e+00
Epoch 3/10
19/19 - 3s - loss: 248.9435 - loglik: -2.4724e+02 - logprior: -1.0157e+00
Epoch 4/10
19/19 - 3s - loss: 247.0650 - loglik: -2.4512e+02 - logprior: -9.9032e-01
Epoch 5/10
19/19 - 3s - loss: 246.2267 - loglik: -2.4426e+02 - logprior: -9.6511e-01
Epoch 6/10
19/19 - 3s - loss: 245.0923 - loglik: -2.4315e+02 - logprior: -9.3756e-01
Epoch 7/10
19/19 - 3s - loss: 243.9762 - loglik: -2.4204e+02 - logprior: -9.2776e-01
Epoch 8/10
19/19 - 3s - loss: 243.4784 - loglik: -2.4158e+02 - logprior: -9.0564e-01
Epoch 9/10
19/19 - 3s - loss: 242.5441 - loglik: -2.4067e+02 - logprior: -9.0678e-01
Epoch 10/10
19/19 - 3s - loss: 242.0361 - loglik: -2.4020e+02 - logprior: -8.8528e-01
Fitted a model with MAP estimate = -240.7524
Time for alignment: 102.4345
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 301.7064 - loglik: -2.9853e+02 - logprior: -3.0881e+00
Epoch 2/10
19/19 - 3s - loss: 273.4662 - loglik: -2.7165e+02 - logprior: -1.2984e+00
Epoch 3/10
19/19 - 2s - loss: 258.9584 - loglik: -2.5675e+02 - logprior: -1.3280e+00
Epoch 4/10
19/19 - 2s - loss: 255.1241 - loglik: -2.5315e+02 - logprior: -1.2010e+00
Epoch 5/10
19/19 - 3s - loss: 253.4429 - loglik: -2.5140e+02 - logprior: -1.2180e+00
Epoch 6/10
19/19 - 2s - loss: 252.0614 - loglik: -2.4996e+02 - logprior: -1.2196e+00
Epoch 7/10
19/19 - 3s - loss: 251.1869 - loglik: -2.4911e+02 - logprior: -1.2247e+00
Epoch 8/10
19/19 - 3s - loss: 250.8849 - loglik: -2.4887e+02 - logprior: -1.2207e+00
Epoch 9/10
19/19 - 3s - loss: 250.1405 - loglik: -2.4820e+02 - logprior: -1.2285e+00
Epoch 10/10
19/19 - 2s - loss: 250.1430 - loglik: -2.4824e+02 - logprior: -1.2336e+00
Fitted a model with MAP estimate = -248.7234
expansions: [(0, 2), (20, 4), (21, 2), (22, 1), (23, 1), (38, 2), (41, 3), (42, 1), (44, 2), (51, 1), (52, 1), (53, 1), (75, 2), (76, 1), (77, 4), (78, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 113 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 269.1554 - loglik: -2.6476e+02 - logprior: -4.3119e+00
Epoch 2/2
19/19 - 3s - loss: 254.7911 - loglik: -2.5301e+02 - logprior: -1.4642e+00
Fitted a model with MAP estimate = -251.1611
expansions: [(106, 1)]
discards: [ 1 28 56 60 98]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 255.9012 - loglik: -2.5290e+02 - logprior: -2.9431e+00
Epoch 2/2
19/19 - 3s - loss: 251.4682 - loglik: -2.4997e+02 - logprior: -1.1795e+00
Fitted a model with MAP estimate = -249.0848
expansions: [(52, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 253.9534 - loglik: -2.5104e+02 - logprior: -2.8450e+00
Epoch 2/10
19/19 - 3s - loss: 250.5750 - loglik: -2.4911e+02 - logprior: -1.1506e+00
Epoch 3/10
19/19 - 3s - loss: 248.6645 - loglik: -2.4689e+02 - logprior: -1.0579e+00
Epoch 4/10
19/19 - 3s - loss: 247.6584 - loglik: -2.4568e+02 - logprior: -1.0395e+00
Epoch 5/10
19/19 - 3s - loss: 245.9462 - loglik: -2.4397e+02 - logprior: -1.0163e+00
Epoch 6/10
19/19 - 3s - loss: 245.2171 - loglik: -2.4325e+02 - logprior: -1.0000e+00
Epoch 7/10
19/19 - 3s - loss: 244.4471 - loglik: -2.4249e+02 - logprior: -9.8153e-01
Epoch 8/10
19/19 - 3s - loss: 243.8020 - loglik: -2.4187e+02 - logprior: -9.6986e-01
Epoch 9/10
19/19 - 3s - loss: 242.8586 - loglik: -2.4097e+02 - logprior: -9.5734e-01
Epoch 10/10
19/19 - 3s - loss: 242.6635 - loglik: -2.4081e+02 - logprior: -9.4336e-01
Fitted a model with MAP estimate = -241.2044
Time for alignment: 102.7851
Computed alignments with likelihoods: ['-241.7793', '-240.7524', '-241.2044']
Best model has likelihood: -240.7524
time for generating output: 0.1750
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07686.projection.fasta
SP score = 0.880810033394489
Training of 3 independent models on file PF00505.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd219c5f970>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fca440e0a60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fca440e0b50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fca440e0b80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd24422a700>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd028d04040>, <__main__.SimpleDirichletPrior object at 0x7fd3cfdb0100>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 186.4987 - loglik: -1.8326e+02 - logprior: -3.2341e+00
Epoch 2/10
19/19 - 2s - loss: 155.5079 - loglik: -1.5379e+02 - logprior: -1.6667e+00
Epoch 3/10
19/19 - 2s - loss: 142.3767 - loglik: -1.4045e+02 - logprior: -1.6386e+00
Epoch 4/10
19/19 - 2s - loss: 138.7945 - loglik: -1.3679e+02 - logprior: -1.7091e+00
Epoch 5/10
19/19 - 2s - loss: 137.4198 - loglik: -1.3550e+02 - logprior: -1.6841e+00
Epoch 6/10
19/19 - 2s - loss: 136.8683 - loglik: -1.3501e+02 - logprior: -1.6663e+00
Epoch 7/10
19/19 - 2s - loss: 136.4326 - loglik: -1.3461e+02 - logprior: -1.6443e+00
Epoch 8/10
19/19 - 2s - loss: 136.3655 - loglik: -1.3456e+02 - logprior: -1.6386e+00
Epoch 9/10
19/19 - 2s - loss: 136.3127 - loglik: -1.3452e+02 - logprior: -1.6338e+00
Epoch 10/10
19/19 - 2s - loss: 136.1675 - loglik: -1.3438e+02 - logprior: -1.6289e+00
Fitted a model with MAP estimate = -135.9012
expansions: [(14, 1), (16, 1), (17, 1), (18, 1), (21, 1), (22, 3), (23, 1), (34, 1), (35, 1), (40, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 136.9056 - loglik: -1.3345e+02 - logprior: -3.3820e+00
Epoch 2/2
19/19 - 2s - loss: 128.1067 - loglik: -1.2645e+02 - logprior: -1.4542e+00
Fitted a model with MAP estimate = -126.6537
expansions: []
discards: [28]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 131.0774 - loglik: -1.2777e+02 - logprior: -3.2500e+00
Epoch 2/2
19/19 - 2s - loss: 127.7743 - loglik: -1.2622e+02 - logprior: -1.4038e+00
Fitted a model with MAP estimate = -126.4991
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 130.8126 - loglik: -1.2754e+02 - logprior: -3.2268e+00
Epoch 2/10
19/19 - 2s - loss: 127.5782 - loglik: -1.2606e+02 - logprior: -1.3727e+00
Epoch 3/10
19/19 - 2s - loss: 126.3416 - loglik: -1.2480e+02 - logprior: -1.2539e+00
Epoch 4/10
19/19 - 2s - loss: 125.4518 - loglik: -1.2388e+02 - logprior: -1.2266e+00
Epoch 5/10
19/19 - 2s - loss: 124.9955 - loglik: -1.2348e+02 - logprior: -1.2005e+00
Epoch 6/10
19/19 - 2s - loss: 124.8352 - loglik: -1.2338e+02 - logprior: -1.1850e+00
Epoch 7/10
19/19 - 2s - loss: 124.7117 - loglik: -1.2330e+02 - logprior: -1.1649e+00
Epoch 8/10
19/19 - 2s - loss: 124.3213 - loglik: -1.2294e+02 - logprior: -1.1522e+00
Epoch 9/10
19/19 - 2s - loss: 124.2411 - loglik: -1.2289e+02 - logprior: -1.1389e+00
Epoch 10/10
19/19 - 2s - loss: 124.1125 - loglik: -1.2278e+02 - logprior: -1.1267e+00
Fitted a model with MAP estimate = -123.8947
Time for alignment: 64.0109
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 186.4141 - loglik: -1.8317e+02 - logprior: -3.2349e+00
Epoch 2/10
19/19 - 2s - loss: 154.5800 - loglik: -1.5286e+02 - logprior: -1.6470e+00
Epoch 3/10
19/19 - 2s - loss: 142.3243 - loglik: -1.4031e+02 - logprior: -1.6797e+00
Epoch 4/10
19/19 - 2s - loss: 138.3432 - loglik: -1.3630e+02 - logprior: -1.7049e+00
Epoch 5/10
19/19 - 2s - loss: 137.0515 - loglik: -1.3513e+02 - logprior: -1.6883e+00
Epoch 6/10
19/19 - 2s - loss: 136.5225 - loglik: -1.3468e+02 - logprior: -1.6718e+00
Epoch 7/10
19/19 - 2s - loss: 136.4631 - loglik: -1.3465e+02 - logprior: -1.6428e+00
Epoch 8/10
19/19 - 2s - loss: 136.1386 - loglik: -1.3435e+02 - logprior: -1.6307e+00
Epoch 9/10
19/19 - 2s - loss: 136.3453 - loglik: -1.3456e+02 - logprior: -1.6274e+00
Fitted a model with MAP estimate = -135.8017
expansions: [(14, 1), (16, 1), (17, 1), (18, 1), (21, 3), (22, 2), (23, 1), (34, 1), (38, 1), (40, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 137.0066 - loglik: -1.3353e+02 - logprior: -3.4036e+00
Epoch 2/2
19/19 - 2s - loss: 128.2457 - loglik: -1.2657e+02 - logprior: -1.4783e+00
Fitted a model with MAP estimate = -126.6581
expansions: []
discards: [26 29]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 131.3490 - loglik: -1.2803e+02 - logprior: -3.2589e+00
Epoch 2/2
19/19 - 2s - loss: 127.7738 - loglik: -1.2623e+02 - logprior: -1.3983e+00
Fitted a model with MAP estimate = -126.5495
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 130.9577 - loglik: -1.2768e+02 - logprior: -3.2308e+00
Epoch 2/10
19/19 - 2s - loss: 127.5940 - loglik: -1.2609e+02 - logprior: -1.3668e+00
Epoch 3/10
19/19 - 2s - loss: 126.3569 - loglik: -1.2482e+02 - logprior: -1.2539e+00
Epoch 4/10
19/19 - 2s - loss: 125.4515 - loglik: -1.2387e+02 - logprior: -1.2295e+00
Epoch 5/10
19/19 - 2s - loss: 125.0211 - loglik: -1.2351e+02 - logprior: -1.1989e+00
Epoch 6/10
19/19 - 2s - loss: 124.6646 - loglik: -1.2320e+02 - logprior: -1.1819e+00
Epoch 7/10
19/19 - 2s - loss: 124.8048 - loglik: -1.2340e+02 - logprior: -1.1658e+00
Fitted a model with MAP estimate = -124.1431
Time for alignment: 57.6490
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 186.5991 - loglik: -1.8336e+02 - logprior: -3.2341e+00
Epoch 2/10
19/19 - 2s - loss: 155.0408 - loglik: -1.5348e+02 - logprior: -1.4748e+00
Epoch 3/10
19/19 - 2s - loss: 141.3413 - loglik: -1.3926e+02 - logprior: -1.6938e+00
Epoch 4/10
19/19 - 2s - loss: 138.1680 - loglik: -1.3622e+02 - logprior: -1.6174e+00
Epoch 5/10
19/19 - 2s - loss: 137.0770 - loglik: -1.3520e+02 - logprior: -1.6524e+00
Epoch 6/10
19/19 - 2s - loss: 137.0299 - loglik: -1.3521e+02 - logprior: -1.6299e+00
Epoch 7/10
19/19 - 2s - loss: 136.3651 - loglik: -1.3456e+02 - logprior: -1.6227e+00
Epoch 8/10
19/19 - 1s - loss: 136.4798 - loglik: -1.3470e+02 - logprior: -1.6132e+00
Fitted a model with MAP estimate = -136.0204
expansions: [(12, 1), (14, 1), (16, 1), (17, 1), (18, 1), (21, 2), (22, 2), (23, 1), (34, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 140.3017 - loglik: -1.3606e+02 - logprior: -4.1641e+00
Epoch 2/2
19/19 - 2s - loss: 130.5758 - loglik: -1.2826e+02 - logprior: -2.1070e+00
Fitted a model with MAP estimate = -128.3352
expansions: [(0, 2)]
discards: [ 0 28]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 130.5708 - loglik: -1.2745e+02 - logprior: -3.0629e+00
Epoch 2/2
19/19 - 2s - loss: 126.9480 - loglik: -1.2557e+02 - logprior: -1.2324e+00
Fitted a model with MAP estimate = -125.6723
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 132.8430 - loglik: -1.2894e+02 - logprior: -3.8574e+00
Epoch 2/10
19/19 - 2s - loss: 127.6437 - loglik: -1.2605e+02 - logprior: -1.4445e+00
Epoch 3/10
19/19 - 2s - loss: 126.2346 - loglik: -1.2471e+02 - logprior: -1.2433e+00
Epoch 4/10
19/19 - 2s - loss: 125.4561 - loglik: -1.2388e+02 - logprior: -1.2273e+00
Epoch 5/10
19/19 - 2s - loss: 124.6257 - loglik: -1.2310e+02 - logprior: -1.2153e+00
Epoch 6/10
19/19 - 2s - loss: 124.6894 - loglik: -1.2322e+02 - logprior: -1.1934e+00
Fitted a model with MAP estimate = -124.0449
Time for alignment: 54.3021
Computed alignments with likelihoods: ['-123.8947', '-124.1431', '-124.0449']
Best model has likelihood: -123.8947
time for generating output: 0.1331
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00505.projection.fasta
SP score = 0.9088883386976975
Training of 3 independent models on file PF13393.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fca357a95b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd1efa34040>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1efea7610>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fca455e7400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd222845d30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd3c69b0640>, <__main__.SimpleDirichletPrior object at 0x7fca444f3df0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 729.6338 - loglik: -7.2738e+02 - logprior: -1.8176e+00
Epoch 2/10
39/39 - 24s - loss: 616.2587 - loglik: -6.1313e+02 - logprior: -1.9375e+00
Epoch 3/10
39/39 - 24s - loss: 601.6387 - loglik: -5.9833e+02 - logprior: -2.0647e+00
Epoch 4/10
39/39 - 24s - loss: 598.3696 - loglik: -5.9525e+02 - logprior: -2.0543e+00
Epoch 5/10
39/39 - 24s - loss: 595.9720 - loglik: -5.9289e+02 - logprior: -2.1039e+00
Epoch 6/10
39/39 - 24s - loss: 594.5002 - loglik: -5.9145e+02 - logprior: -2.1445e+00
Epoch 7/10
39/39 - 24s - loss: 593.7293 - loglik: -5.9071e+02 - logprior: -2.1572e+00
Epoch 8/10
39/39 - 24s - loss: 593.3730 - loglik: -5.9040e+02 - logprior: -2.1728e+00
Epoch 9/10
39/39 - 25s - loss: 592.4946 - loglik: -5.8957e+02 - logprior: -2.1645e+00
Epoch 10/10
39/39 - 25s - loss: 592.1629 - loglik: -5.8928e+02 - logprior: -2.1698e+00
Fitted a model with MAP estimate = -590.7070
expansions: [(10, 1), (11, 1), (13, 1), (14, 3), (15, 1), (33, 1), (34, 1), (42, 1), (44, 1), (45, 1), (46, 1), (51, 1), (52, 1), (66, 1), (67, 1), (68, 1), (70, 1), (76, 1), (77, 1), (112, 1), (114, 1), (115, 1), (117, 1), (120, 4), (121, 1), (123, 1), (131, 1), (132, 1), (135, 1), (136, 2), (137, 1), (149, 1), (154, 1), (155, 7), (163, 2), (164, 3), (174, 1), (175, 2), (177, 1), (180, 1), (183, 1), (184, 1), (185, 1), (192, 1), (193, 1), (196, 1), (210, 1), (211, 4), (213, 2), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 314 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 592.4985 - loglik: -5.8940e+02 - logprior: -2.8488e+00
Epoch 2/2
39/39 - 35s - loss: 567.9056 - loglik: -5.6522e+02 - logprior: -1.8255e+00
Fitted a model with MAP estimate = -561.2260
expansions: [(0, 3), (209, 1), (250, 1)]
discards: [  0 226 275]
Re-initialized the encoder parameters.
Fitting a model of length 316 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 39s - loss: 572.2700 - loglik: -5.7024e+02 - logprior: -1.7680e+00
Epoch 2/2
39/39 - 35s - loss: 561.7747 - loglik: -5.5998e+02 - logprior: -7.7550e-01
Fitted a model with MAP estimate = -555.5673
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 314 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 39s - loss: 573.7261 - loglik: -5.7112e+02 - logprior: -2.3750e+00
Epoch 2/10
39/39 - 35s - loss: 562.3693 - loglik: -5.6112e+02 - logprior: -6.6837e-01
Epoch 3/10
39/39 - 35s - loss: 556.3932 - loglik: -5.5515e+02 - logprior: -3.4134e-01
Epoch 4/10
39/39 - 35s - loss: 554.1199 - loglik: -5.5273e+02 - logprior: -2.6286e-01
Epoch 5/10
39/39 - 35s - loss: 553.6501 - loglik: -5.5240e+02 - logprior: -9.8525e-02
Epoch 6/10
39/39 - 35s - loss: 551.9409 - loglik: -5.5081e+02 - logprior: -3.7185e-02
Epoch 7/10
39/39 - 35s - loss: 552.5389 - loglik: -5.5165e+02 - logprior: 0.1551
Fitted a model with MAP estimate = -550.2713
Time for alignment: 790.6822
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 732.2013 - loglik: -7.2993e+02 - logprior: -1.8170e+00
Epoch 2/10
39/39 - 25s - loss: 617.3125 - loglik: -6.1419e+02 - logprior: -1.8226e+00
Epoch 3/10
39/39 - 25s - loss: 602.1913 - loglik: -5.9872e+02 - logprior: -1.9824e+00
Epoch 4/10
39/39 - 25s - loss: 598.5565 - loglik: -5.9537e+02 - logprior: -2.0167e+00
Epoch 5/10
39/39 - 25s - loss: 596.9933 - loglik: -5.9396e+02 - logprior: -2.0095e+00
Epoch 6/10
39/39 - 25s - loss: 596.5076 - loglik: -5.9356e+02 - logprior: -2.0140e+00
Epoch 7/10
39/39 - 26s - loss: 595.7047 - loglik: -5.9283e+02 - logprior: -2.0144e+00
Epoch 8/10
39/39 - 26s - loss: 595.3939 - loglik: -5.9258e+02 - logprior: -2.0211e+00
Epoch 9/10
39/39 - 26s - loss: 595.2100 - loglik: -5.9242e+02 - logprior: -2.0298e+00
Epoch 10/10
39/39 - 26s - loss: 595.0522 - loglik: -5.9229e+02 - logprior: -2.0210e+00
Fitted a model with MAP estimate = -592.8258
expansions: [(10, 1), (14, 1), (15, 1), (16, 1), (17, 4), (18, 1), (21, 1), (34, 1), (46, 1), (47, 1), (52, 1), (53, 1), (67, 1), (68, 1), (69, 2), (71, 1), (72, 1), (78, 1), (93, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (123, 1), (132, 2), (135, 1), (138, 3), (148, 2), (149, 1), (154, 3), (163, 1), (164, 5), (174, 2), (175, 1), (184, 1), (185, 2), (192, 2), (193, 1), (194, 1), (197, 1), (211, 1), (212, 3), (214, 3), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 314 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 43s - loss: 594.1295 - loglik: -5.9103e+02 - logprior: -2.8561e+00
Epoch 2/2
39/39 - 39s - loss: 568.2779 - loglik: -5.6538e+02 - logprior: -1.9114e+00
Fitted a model with MAP estimate = -561.2905
expansions: [(0, 3), (146, 2), (199, 1), (210, 2)]
discards: [  0  86 163 186 248 278]
Re-initialized the encoder parameters.
Fitting a model of length 316 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 43s - loss: 572.9672 - loglik: -5.7082e+02 - logprior: -1.8830e+00
Epoch 2/2
39/39 - 39s - loss: 561.6744 - loglik: -5.5983e+02 - logprior: -8.6811e-01
Fitted a model with MAP estimate = -555.3428
expansions: [(214, 1)]
discards: [  0   1 176]
Re-initialized the encoder parameters.
Fitting a model of length 314 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 573.6990 - loglik: -5.7109e+02 - logprior: -2.3754e+00
Epoch 2/10
39/39 - 43s - loss: 562.3672 - loglik: -5.6106e+02 - logprior: -7.0282e-01
Epoch 3/10
39/39 - 41s - loss: 556.5829 - loglik: -5.5531e+02 - logprior: -3.6619e-01
Epoch 4/10
39/39 - 41s - loss: 554.1633 - loglik: -5.5278e+02 - logprior: -2.7895e-01
Epoch 5/10
39/39 - 42s - loss: 553.4302 - loglik: -5.5210e+02 - logprior: -1.8541e-01
Epoch 6/10
39/39 - 42s - loss: 552.6815 - loglik: -5.5160e+02 - logprior: 8.5069e-04
Epoch 7/10
39/39 - 42s - loss: 551.9442 - loglik: -5.5104e+02 - logprior: 0.1334
Epoch 8/10
39/39 - 40s - loss: 551.7244 - loglik: -5.5101e+02 - logprior: 0.2670
Epoch 9/10
39/39 - 40s - loss: 550.2958 - loglik: -5.4976e+02 - logprior: 0.4099
Epoch 10/10
39/39 - 40s - loss: 551.3937 - loglik: -5.5103e+02 - logprior: 0.5494
Fitted a model with MAP estimate = -549.1557
Time for alignment: 1015.3875
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 731.0822 - loglik: -7.2881e+02 - logprior: -1.8290e+00
Epoch 2/10
39/39 - 27s - loss: 616.2108 - loglik: -6.1257e+02 - logprior: -2.0281e+00
Epoch 3/10
39/39 - 27s - loss: 603.5565 - loglik: -5.9997e+02 - logprior: -2.0979e+00
Epoch 4/10
39/39 - 27s - loss: 597.7142 - loglik: -5.9438e+02 - logprior: -2.1294e+00
Epoch 5/10
39/39 - 27s - loss: 596.0043 - loglik: -5.9285e+02 - logprior: -2.1331e+00
Epoch 6/10
39/39 - 27s - loss: 594.8657 - loglik: -5.9181e+02 - logprior: -2.1377e+00
Epoch 7/10
39/39 - 28s - loss: 594.4001 - loglik: -5.9141e+02 - logprior: -2.1375e+00
Epoch 8/10
39/39 - 28s - loss: 593.6667 - loglik: -5.9074e+02 - logprior: -2.1500e+00
Epoch 9/10
39/39 - 29s - loss: 593.6818 - loglik: -5.9080e+02 - logprior: -2.1574e+00
Fitted a model with MAP estimate = -591.4930
expansions: [(12, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (34, 1), (35, 1), (38, 1), (42, 1), (46, 1), (47, 1), (52, 1), (53, 1), (67, 1), (68, 1), (69, 1), (71, 1), (77, 1), (78, 1), (102, 2), (115, 1), (116, 1), (117, 1), (121, 5), (131, 1), (132, 1), (136, 3), (137, 1), (147, 1), (154, 3), (163, 1), (164, 1), (165, 1), (166, 1), (175, 1), (177, 1), (180, 1), (183, 1), (185, 1), (192, 3), (193, 1), (196, 1), (211, 1), (212, 3), (213, 2), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 43s - loss: 595.6385 - loglik: -5.9250e+02 - logprior: -2.9040e+00
Epoch 2/2
39/39 - 40s - loss: 574.0360 - loglik: -5.7144e+02 - logprior: -1.8172e+00
Fitted a model with MAP estimate = -567.9421
expansions: [(0, 3), (148, 1), (217, 1)]
discards: [  0 122 191]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 43s - loss: 578.8745 - loglik: -5.7685e+02 - logprior: -1.7721e+00
Epoch 2/2
39/39 - 39s - loss: 569.0636 - loglik: -5.6730e+02 - logprior: -7.4902e-01
Fitted a model with MAP estimate = -562.6091
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 580.2748 - loglik: -5.7770e+02 - logprior: -2.3510e+00
Epoch 2/10
39/39 - 38s - loss: 569.4701 - loglik: -5.6824e+02 - logprior: -6.6050e-01
Epoch 3/10
39/39 - 38s - loss: 563.6700 - loglik: -5.6243e+02 - logprior: -3.3761e-01
Epoch 4/10
39/39 - 37s - loss: 560.9501 - loglik: -5.5963e+02 - logprior: -2.1272e-01
Epoch 5/10
39/39 - 39s - loss: 560.4948 - loglik: -5.5932e+02 - logprior: -4.1352e-02
Epoch 6/10
39/39 - 40s - loss: 559.3013 - loglik: -5.5828e+02 - logprior: 0.0704
Epoch 7/10
39/39 - 40s - loss: 559.4604 - loglik: -5.5859e+02 - logprior: 0.1694
Fitted a model with MAP estimate = -557.3043
Time for alignment: 866.1831
Computed alignments with likelihoods: ['-550.2713', '-549.1557', '-557.3043']
Best model has likelihood: -549.1557
time for generating output: 0.3696
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13393.projection.fasta
SP score = 0.4051804501265148
Training of 3 independent models on file PF00202.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd028186c10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fca35337460>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fca353374f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd1f83c9910>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd24d25fbb0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd21a0507f0>, <__main__.SimpleDirichletPrior object at 0x7fd3e973f310>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 51s - loss: 973.9548 - loglik: -9.7196e+02 - logprior: -1.5585e+00
Epoch 2/10
39/39 - 45s - loss: 807.7310 - loglik: -8.0440e+02 - logprior: -1.5992e+00
Epoch 3/10
39/39 - 46s - loss: 793.6368 - loglik: -7.8979e+02 - logprior: -1.7498e+00
Epoch 4/10
39/39 - 47s - loss: 789.7513 - loglik: -7.8611e+02 - logprior: -1.8169e+00
Epoch 5/10
39/39 - 46s - loss: 787.4541 - loglik: -7.8399e+02 - logprior: -1.8558e+00
Epoch 6/10
39/39 - 47s - loss: 785.8911 - loglik: -7.8252e+02 - logprior: -1.8841e+00
Epoch 7/10
39/39 - 48s - loss: 785.1011 - loglik: -7.8181e+02 - logprior: -1.8956e+00
Epoch 8/10
39/39 - 48s - loss: 784.2785 - loglik: -7.8111e+02 - logprior: -1.8812e+00
Epoch 9/10
39/39 - 47s - loss: 784.4003 - loglik: -7.8131e+02 - logprior: -1.8971e+00
Fitted a model with MAP estimate = -774.2253
expansions: [(0, 3), (20, 1), (52, 1), (54, 1), (58, 2), (64, 1), (70, 1), (74, 1), (75, 1), (100, 3), (120, 2), (122, 1), (123, 1), (132, 1), (146, 3), (148, 1), (163, 1), (170, 1), (174, 1), (175, 1), (176, 1), (188, 1), (192, 2), (195, 1), (196, 1), (199, 1), (219, 1), (220, 1), (221, 1), (223, 1), (224, 3), (247, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 2), (257, 1), (258, 1), (262, 1), (264, 1), (285, 6), (286, 4), (287, 1), (288, 1), (295, 3), (296, 2), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 396 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 69s - loss: 790.9694 - loglik: -7.8809e+02 - logprior: -2.6369e+00
Epoch 2/2
39/39 - 65s - loss: 755.5596 - loglik: -7.5357e+02 - logprior: -1.1617e+00
Fitted a model with MAP estimate = -741.9628
expansions: [(345, 1)]
discards: [ 65  84 113 136 223 362 363 390]
Re-initialized the encoder parameters.
Fitting a model of length 389 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 64s - loss: 763.8982 - loglik: -7.6202e+02 - logprior: -1.5867e+00
Epoch 2/2
39/39 - 60s - loss: 753.3199 - loglik: -7.5180e+02 - logprior: -4.9704e-01
Fitted a model with MAP estimate = -740.5521
expansions: []
discards: [  4 333]
Re-initialized the encoder parameters.
Fitting a model of length 387 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 65s - loss: 755.8956 - loglik: -7.5440e+02 - logprior: -1.2211e+00
Epoch 2/10
39/39 - 61s - loss: 746.2652 - loglik: -7.4499e+02 - logprior: -2.8112e-01
Epoch 3/10
39/39 - 62s - loss: 742.1159 - loglik: -7.4027e+02 - logprior: -1.7785e-01
Epoch 4/10
39/39 - 61s - loss: 739.4482 - loglik: -7.3746e+02 - logprior: -1.2487e-01
Epoch 5/10
39/39 - 60s - loss: 739.4413 - loglik: -7.3762e+02 - logprior: -9.2297e-03
Epoch 6/10
39/39 - 59s - loss: 736.8231 - loglik: -7.3529e+02 - logprior: 0.1525
Epoch 7/10
39/39 - 59s - loss: 737.3438 - loglik: -7.3601e+02 - logprior: 0.2599
Fitted a model with MAP estimate = -733.7813
Time for alignment: 1405.2951
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 970.2705 - loglik: -9.6827e+02 - logprior: -1.5697e+00
Epoch 2/10
39/39 - 47s - loss: 808.4993 - loglik: -8.0585e+02 - logprior: -1.4985e+00
Epoch 3/10
39/39 - 48s - loss: 793.9608 - loglik: -7.9031e+02 - logprior: -1.7175e+00
Epoch 4/10
39/39 - 48s - loss: 789.4566 - loglik: -7.8591e+02 - logprior: -1.7389e+00
Epoch 5/10
39/39 - 49s - loss: 787.8736 - loglik: -7.8440e+02 - logprior: -1.8157e+00
Epoch 6/10
39/39 - 49s - loss: 786.7008 - loglik: -7.8335e+02 - logprior: -1.8559e+00
Epoch 7/10
39/39 - 50s - loss: 785.3558 - loglik: -7.8209e+02 - logprior: -1.8870e+00
Epoch 8/10
39/39 - 53s - loss: 784.7927 - loglik: -7.8154e+02 - logprior: -1.9990e+00
Epoch 9/10
39/39 - 52s - loss: 784.5847 - loglik: -7.8126e+02 - logprior: -2.1458e+00
Epoch 10/10
39/39 - 52s - loss: 784.1509 - loglik: -7.8100e+02 - logprior: -2.0605e+00
Fitted a model with MAP estimate = -774.1859
expansions: [(0, 3), (20, 1), (48, 1), (52, 1), (53, 1), (57, 2), (70, 1), (71, 1), (74, 1), (100, 1), (105, 1), (121, 1), (122, 1), (123, 1), (144, 1), (147, 3), (148, 2), (167, 1), (171, 1), (176, 1), (177, 1), (186, 1), (193, 2), (196, 1), (197, 3), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 3), (247, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 2), (257, 1), (258, 1), (262, 1), (264, 1), (265, 2), (284, 7), (286, 1), (287, 1), (288, 1), (296, 2), (297, 1), (298, 3), (308, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 397 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 76s - loss: 791.5007 - loglik: -7.8849e+02 - logprior: -2.7595e+00
Epoch 2/2
39/39 - 73s - loss: 754.0679 - loglik: -7.5186e+02 - logprior: -1.3835e+00
Fitted a model with MAP estimate = -740.9426
expansions: [(347, 2)]
discards: [ 65 222 229 319 365 366 391]
Re-initialized the encoder parameters.
Fitting a model of length 392 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 76s - loss: 762.5998 - loglik: -7.6072e+02 - logprior: -1.5968e+00
Epoch 2/2
39/39 - 72s - loss: 750.9708 - loglik: -7.4948e+02 - logprior: -4.6526e-01
Fitted a model with MAP estimate = -738.1586
expansions: []
discards: [  4 167 335]
Re-initialized the encoder parameters.
Fitting a model of length 389 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 68s - loss: 754.3951 - loglik: -7.5291e+02 - logprior: -1.2208e+00
Epoch 2/10
39/39 - 60s - loss: 744.1423 - loglik: -7.4297e+02 - logprior: -1.8653e-01
Epoch 3/10
39/39 - 61s - loss: 739.5948 - loglik: -7.3781e+02 - logprior: -1.1483e-01
Epoch 4/10
39/39 - 61s - loss: 738.7948 - loglik: -7.3691e+02 - logprior: -3.8070e-02
Epoch 5/10
39/39 - 62s - loss: 736.6988 - loglik: -7.3499e+02 - logprior: 0.0750
Epoch 6/10
39/39 - 62s - loss: 736.5381 - loglik: -7.3514e+02 - logprior: 0.2387
Epoch 7/10
39/39 - 63s - loss: 735.5782 - loglik: -7.3438e+02 - logprior: 0.3486
Epoch 8/10
39/39 - 66s - loss: 734.1490 - loglik: -7.3322e+02 - logprior: 0.5090
Epoch 9/10
39/39 - 68s - loss: 733.8514 - loglik: -7.3315e+02 - logprior: 0.6457
Epoch 10/10
39/39 - 68s - loss: 733.8190 - loglik: -7.3333e+02 - logprior: 0.7584
Fitted a model with MAP estimate = -731.0385
Time for alignment: 1805.0689
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 971.4103 - loglik: -9.6941e+02 - logprior: -1.5838e+00
Epoch 2/10
39/39 - 49s - loss: 804.7962 - loglik: -8.0184e+02 - logprior: -1.7043e+00
Epoch 3/10
39/39 - 50s - loss: 791.6261 - loglik: -7.8812e+02 - logprior: -1.8063e+00
Epoch 4/10
39/39 - 49s - loss: 787.6900 - loglik: -7.8418e+02 - logprior: -1.8562e+00
Epoch 5/10
39/39 - 50s - loss: 785.5019 - loglik: -7.8209e+02 - logprior: -1.8823e+00
Epoch 6/10
39/39 - 52s - loss: 784.5004 - loglik: -7.8120e+02 - logprior: -1.8960e+00
Epoch 7/10
39/39 - 51s - loss: 783.9151 - loglik: -7.8071e+02 - logprior: -1.9034e+00
Epoch 8/10
39/39 - 51s - loss: 782.8467 - loglik: -7.7971e+02 - logprior: -1.9246e+00
Epoch 9/10
39/39 - 51s - loss: 783.1574 - loglik: -7.7998e+02 - logprior: -2.0453e+00
Fitted a model with MAP estimate = -773.1231
expansions: [(0, 3), (15, 1), (19, 1), (47, 1), (50, 2), (51, 1), (55, 1), (68, 1), (69, 1), (72, 2), (73, 2), (119, 2), (120, 1), (122, 1), (131, 1), (146, 3), (147, 2), (170, 1), (173, 1), (175, 1), (176, 1), (193, 2), (195, 1), (196, 1), (218, 1), (219, 1), (220, 1), (222, 1), (223, 1), (224, 3), (248, 1), (249, 1), (251, 1), (252, 1), (255, 1), (256, 1), (257, 1), (260, 1), (262, 3), (263, 1), (264, 1), (266, 2), (285, 7), (286, 3), (287, 1), (288, 1), (291, 1), (295, 1), (296, 1), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 397 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 74s - loss: 787.4924 - loglik: -7.8450e+02 - logprior: -2.7334e+00
Epoch 2/2
39/39 - 71s - loss: 754.8031 - loglik: -7.5265e+02 - logprior: -1.3232e+00
Fitted a model with MAP estimate = -741.6111
expansions: [(350, 2)]
discards: [  2  85 136 312 321 391]
Re-initialized the encoder parameters.
Fitting a model of length 393 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 77s - loss: 762.3666 - loglik: -7.6052e+02 - logprior: -1.5501e+00
Epoch 2/2
39/39 - 74s - loss: 751.4711 - loglik: -7.4986e+02 - logprior: -5.8651e-01
Fitted a model with MAP estimate = -738.8377
expansions: [(366, 2)]
discards: [167 336 338 346]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 73s - loss: 755.1256 - loglik: -7.5364e+02 - logprior: -1.2216e+00
Epoch 2/10
39/39 - 69s - loss: 743.8704 - loglik: -7.4260e+02 - logprior: -2.9190e-01
Epoch 3/10
39/39 - 70s - loss: 739.7161 - loglik: -7.3784e+02 - logprior: -2.2932e-01
Epoch 4/10
39/39 - 69s - loss: 737.8289 - loglik: -7.3582e+02 - logprior: -1.7477e-01
Epoch 5/10
39/39 - 70s - loss: 737.9868 - loglik: -7.3617e+02 - logprior: -2.3278e-03
Fitted a model with MAP estimate = -734.2998
Time for alignment: 1467.2367
Computed alignments with likelihoods: ['-733.7813', '-731.0385', '-734.2998']
Best model has likelihood: -731.0385
time for generating output: 1.2313
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00202.projection.fasta
SP score = 0.33588210290055004
Training of 3 independent models on file PF00018.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fca285d1670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd0285ca160>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1e617a1c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fca3ded2f70>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd222e366a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fca352a14f0>, <__main__.SimpleDirichletPrior object at 0x7fd1e6f08af0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.7632 - loglik: -1.3051e+02 - logprior: -3.2501e+00
Epoch 2/10
19/19 - 1s - loss: 109.2849 - loglik: -1.0779e+02 - logprior: -1.4304e+00
Epoch 3/10
19/19 - 1s - loss: 101.5779 - loglik: -9.9806e+01 - logprior: -1.5790e+00
Epoch 4/10
19/19 - 1s - loss: 99.0613 - loglik: -9.7398e+01 - logprior: -1.5101e+00
Epoch 5/10
19/19 - 1s - loss: 98.3059 - loglik: -9.6713e+01 - logprior: -1.4747e+00
Epoch 6/10
19/19 - 1s - loss: 98.0636 - loglik: -9.6492e+01 - logprior: -1.4576e+00
Epoch 7/10
19/19 - 1s - loss: 97.9069 - loglik: -9.6354e+01 - logprior: -1.4417e+00
Epoch 8/10
19/19 - 1s - loss: 97.9057 - loglik: -9.6364e+01 - logprior: -1.4319e+00
Epoch 9/10
19/19 - 1s - loss: 97.8711 - loglik: -9.6338e+01 - logprior: -1.4245e+00
Epoch 10/10
19/19 - 1s - loss: 97.7634 - loglik: -9.6228e+01 - logprior: -1.4194e+00
Fitted a model with MAP estimate = -97.5624
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.4621 - loglik: -9.8177e+01 - logprior: -4.2269e+00
Epoch 2/2
19/19 - 1s - loss: 95.1360 - loglik: -9.2789e+01 - logprior: -2.1832e+00
Fitted a model with MAP estimate = -93.5856
expansions: [(0, 2)]
discards: [ 0  9 12 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 95.5887 - loglik: -9.2429e+01 - logprior: -3.1196e+00
Epoch 2/2
19/19 - 1s - loss: 92.5416 - loglik: -9.1100e+01 - logprior: -1.3186e+00
Fitted a model with MAP estimate = -91.9089
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 96.9107 - loglik: -9.3274e+01 - logprior: -3.5962e+00
Epoch 2/10
19/19 - 1s - loss: 93.2027 - loglik: -9.1581e+01 - logprior: -1.5007e+00
Epoch 3/10
19/19 - 1s - loss: 92.6664 - loglik: -9.1083e+01 - logprior: -1.4168e+00
Epoch 4/10
19/19 - 1s - loss: 92.4339 - loglik: -9.0886e+01 - logprior: -1.3728e+00
Epoch 5/10
19/19 - 1s - loss: 92.2554 - loglik: -9.0752e+01 - logprior: -1.3435e+00
Epoch 6/10
19/19 - 1s - loss: 92.1054 - loglik: -9.0628e+01 - logprior: -1.3250e+00
Epoch 7/10
19/19 - 1s - loss: 92.0475 - loglik: -9.0585e+01 - logprior: -1.3123e+00
Epoch 8/10
19/19 - 1s - loss: 91.8190 - loglik: -9.0370e+01 - logprior: -1.2998e+00
Epoch 9/10
19/19 - 1s - loss: 92.1484 - loglik: -9.0707e+01 - logprior: -1.2880e+00
Fitted a model with MAP estimate = -91.6965
Time for alignment: 50.1535
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 133.7406 - loglik: -1.3049e+02 - logprior: -3.2439e+00
Epoch 2/10
19/19 - 1s - loss: 108.7109 - loglik: -1.0722e+02 - logprior: -1.4224e+00
Epoch 3/10
19/19 - 1s - loss: 100.9152 - loglik: -9.9130e+01 - logprior: -1.6049e+00
Epoch 4/10
19/19 - 1s - loss: 98.8171 - loglik: -9.7177e+01 - logprior: -1.4867e+00
Epoch 5/10
19/19 - 1s - loss: 98.3024 - loglik: -9.6717e+01 - logprior: -1.4701e+00
Epoch 6/10
19/19 - 1s - loss: 97.9830 - loglik: -9.6416e+01 - logprior: -1.4544e+00
Epoch 7/10
19/19 - 1s - loss: 97.9841 - loglik: -9.6437e+01 - logprior: -1.4377e+00
Fitted a model with MAP estimate = -97.7053
expansions: [(7, 1), (8, 3), (9, 2), (13, 2), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 103.1361 - loglik: -9.8858e+01 - logprior: -4.2443e+00
Epoch 2/2
19/19 - 1s - loss: 95.2199 - loglik: -9.2900e+01 - logprior: -2.1705e+00
Fitted a model with MAP estimate = -93.5602
expansions: [(0, 2)]
discards: [ 0  9 12 19 30 40 42]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 95.8975 - loglik: -9.2733e+01 - logprior: -3.1247e+00
Epoch 2/2
19/19 - 1s - loss: 92.5771 - loglik: -9.1130e+01 - logprior: -1.3209e+00
Fitted a model with MAP estimate = -91.9210
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 96.8527 - loglik: -9.3220e+01 - logprior: -3.5921e+00
Epoch 2/10
19/19 - 1s - loss: 93.2494 - loglik: -9.1625e+01 - logprior: -1.5036e+00
Epoch 3/10
19/19 - 1s - loss: 92.7099 - loglik: -9.1123e+01 - logprior: -1.4181e+00
Epoch 4/10
19/19 - 1s - loss: 92.4180 - loglik: -9.0867e+01 - logprior: -1.3745e+00
Epoch 5/10
19/19 - 1s - loss: 92.2434 - loglik: -9.0738e+01 - logprior: -1.3444e+00
Epoch 6/10
19/19 - 1s - loss: 92.1522 - loglik: -9.0666e+01 - logprior: -1.3311e+00
Epoch 7/10
19/19 - 1s - loss: 92.0449 - loglik: -9.0583e+01 - logprior: -1.3135e+00
Epoch 8/10
19/19 - 1s - loss: 91.8646 - loglik: -9.0410e+01 - logprior: -1.3034e+00
Epoch 9/10
19/19 - 1s - loss: 91.9213 - loglik: -9.0480e+01 - logprior: -1.2889e+00
Fitted a model with MAP estimate = -91.6876
Time for alignment: 47.3398
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.6468 - loglik: -1.3039e+02 - logprior: -3.2477e+00
Epoch 2/10
19/19 - 1s - loss: 109.1971 - loglik: -1.0771e+02 - logprior: -1.4336e+00
Epoch 3/10
19/19 - 1s - loss: 101.3721 - loglik: -9.9525e+01 - logprior: -1.6168e+00
Epoch 4/10
19/19 - 1s - loss: 99.2040 - loglik: -9.7559e+01 - logprior: -1.4848e+00
Epoch 5/10
19/19 - 1s - loss: 98.4336 - loglik: -9.6832e+01 - logprior: -1.4619e+00
Epoch 6/10
19/19 - 1s - loss: 98.0318 - loglik: -9.6468e+01 - logprior: -1.4448e+00
Epoch 7/10
19/19 - 1s - loss: 97.9942 - loglik: -9.6452e+01 - logprior: -1.4271e+00
Epoch 8/10
19/19 - 1s - loss: 97.9100 - loglik: -9.6384e+01 - logprior: -1.4143e+00
Epoch 9/10
19/19 - 1s - loss: 97.9125 - loglik: -9.6391e+01 - logprior: -1.4070e+00
Fitted a model with MAP estimate = -97.6513
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (23, 2), (28, 2), (29, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.5972 - loglik: -9.8311e+01 - logprior: -4.2345e+00
Epoch 2/2
19/19 - 1s - loss: 95.2336 - loglik: -9.2876e+01 - logprior: -2.1990e+00
Fitted a model with MAP estimate = -93.5507
expansions: [(0, 1)]
discards: [ 0  9 12 29 32 40 42]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 96.6334 - loglik: -9.3443e+01 - logprior: -3.1495e+00
Epoch 2/2
19/19 - 1s - loss: 93.5288 - loglik: -9.1889e+01 - logprior: -1.5168e+00
Fitted a model with MAP estimate = -92.7057
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 95.7124 - loglik: -9.2410e+01 - logprior: -3.2608e+00
Epoch 2/10
19/19 - 1s - loss: 93.2290 - loglik: -9.1599e+01 - logprior: -1.5050e+00
Epoch 3/10
19/19 - 1s - loss: 92.6209 - loglik: -9.1032e+01 - logprior: -1.4219e+00
Epoch 4/10
19/19 - 1s - loss: 92.4165 - loglik: -9.0865e+01 - logprior: -1.3738e+00
Epoch 5/10
19/19 - 1s - loss: 92.2063 - loglik: -9.0701e+01 - logprior: -1.3411e+00
Epoch 6/10
19/19 - 1s - loss: 92.0499 - loglik: -9.0569e+01 - logprior: -1.3254e+00
Epoch 7/10
19/19 - 1s - loss: 92.0584 - loglik: -9.0598e+01 - logprior: -1.3086e+00
Fitted a model with MAP estimate = -91.7831
Time for alignment: 47.2669
Computed alignments with likelihoods: ['-91.6965', '-91.6876', '-91.7831']
Best model has likelihood: -91.6876
time for generating output: 0.1110
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00018.projection.fasta
SP score = 0.8421764617465805
Training of 3 independent models on file PF00687.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fca3c95cbb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd255aae310>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd3e977dbe0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd029a01cd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd03037bf70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd1efe55490>, <__main__.SimpleDirichletPrior object at 0x7fd3d860f550>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 472.0935 - loglik: -4.7016e+02 - logprior: -1.9219e+00
Epoch 2/10
39/39 - 11s - loss: 372.6261 - loglik: -3.7042e+02 - logprior: -1.9138e+00
Epoch 3/10
39/39 - 11s - loss: 363.4758 - loglik: -3.6111e+02 - logprior: -1.9545e+00
Epoch 4/10
39/39 - 12s - loss: 360.7618 - loglik: -3.5852e+02 - logprior: -1.8854e+00
Epoch 5/10
39/39 - 12s - loss: 359.3253 - loglik: -3.5714e+02 - logprior: -1.8607e+00
Epoch 6/10
39/39 - 12s - loss: 358.8990 - loglik: -3.5674e+02 - logprior: -1.8539e+00
Epoch 7/10
39/39 - 12s - loss: 358.0587 - loglik: -3.5591e+02 - logprior: -1.8510e+00
Epoch 8/10
39/39 - 11s - loss: 357.9302 - loglik: -3.5581e+02 - logprior: -1.8453e+00
Epoch 9/10
39/39 - 11s - loss: 356.9947 - loglik: -3.5490e+02 - logprior: -1.8384e+00
Epoch 10/10
39/39 - 12s - loss: 357.3694 - loglik: -3.5523e+02 - logprior: -1.7973e+00
Fitted a model with MAP estimate = -355.3547
expansions: [(2, 1), (3, 3), (4, 2), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (39, 1), (40, 2), (41, 1), (42, 2), (58, 1), (59, 1), (60, 1), (63, 1), (66, 2), (74, 2), (93, 1), (97, 2), (99, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 1), (123, 1), (127, 1), (129, 2), (130, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 199 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 343.3124 - loglik: -3.4109e+02 - logprior: -2.0793e+00
Epoch 2/2
39/39 - 15s - loss: 328.0362 - loglik: -3.2667e+02 - logprior: -1.0978e+00
Fitted a model with MAP estimate = -325.0385
expansions: []
discards: [  2   5  53  58  88  98 124]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 333.4217 - loglik: -3.3146e+02 - logprior: -1.8281e+00
Epoch 2/2
39/39 - 15s - loss: 328.7398 - loglik: -3.2767e+02 - logprior: -7.7639e-01
Fitted a model with MAP estimate = -326.4526
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 332.6752 - loglik: -3.3089e+02 - logprior: -1.6387e+00
Epoch 2/10
39/39 - 15s - loss: 328.5098 - loglik: -3.2760e+02 - logprior: -6.0448e-01
Epoch 3/10
39/39 - 15s - loss: 325.8295 - loglik: -3.2493e+02 - logprior: -4.9083e-01
Epoch 4/10
39/39 - 15s - loss: 323.1606 - loglik: -3.2234e+02 - logprior: -3.7070e-01
Epoch 5/10
39/39 - 15s - loss: 322.3351 - loglik: -3.2170e+02 - logprior: -2.3506e-01
Epoch 6/10
39/39 - 15s - loss: 320.6762 - loglik: -3.2016e+02 - logprior: -1.2516e-01
Epoch 7/10
39/39 - 15s - loss: 318.9834 - loglik: -3.1852e+02 - logprior: -1.6052e-02
Epoch 8/10
39/39 - 15s - loss: 317.8204 - loglik: -3.1744e+02 - logprior: 0.0873
Epoch 9/10
39/39 - 15s - loss: 317.9169 - loglik: -3.1767e+02 - logprior: 0.1847
Fitted a model with MAP estimate = -316.8268
Time for alignment: 393.9452
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 472.2810 - loglik: -4.7033e+02 - logprior: -1.9297e+00
Epoch 2/10
39/39 - 12s - loss: 372.6607 - loglik: -3.7043e+02 - logprior: -1.9781e+00
Epoch 3/10
39/39 - 12s - loss: 364.0954 - loglik: -3.6172e+02 - logprior: -2.0018e+00
Epoch 4/10
39/39 - 12s - loss: 361.3938 - loglik: -3.5911e+02 - logprior: -1.9376e+00
Epoch 5/10
39/39 - 12s - loss: 360.2678 - loglik: -3.5802e+02 - logprior: -1.8971e+00
Epoch 6/10
39/39 - 12s - loss: 359.1127 - loglik: -3.5691e+02 - logprior: -1.8812e+00
Epoch 7/10
39/39 - 12s - loss: 359.0806 - loglik: -3.5689e+02 - logprior: -1.8782e+00
Epoch 8/10
39/39 - 12s - loss: 358.2456 - loglik: -3.5606e+02 - logprior: -1.8526e+00
Epoch 9/10
39/39 - 12s - loss: 356.9944 - loglik: -3.5477e+02 - logprior: -1.8265e+00
Epoch 10/10
39/39 - 12s - loss: 356.2602 - loglik: -3.5402e+02 - logprior: -1.8367e+00
Fitted a model with MAP estimate = -355.4370
expansions: [(4, 1), (5, 2), (8, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (34, 1), (39, 1), (40, 2), (41, 1), (42, 2), (54, 1), (58, 1), (60, 1), (63, 1), (66, 1), (74, 1), (93, 1), (97, 2), (100, 1), (102, 1), (103, 1), (108, 1), (113, 1), (114, 1), (116, 1), (120, 1), (123, 1), (129, 2), (130, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 195 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 343.8492 - loglik: -3.4167e+02 - logprior: -2.0434e+00
Epoch 2/2
39/39 - 15s - loss: 329.4717 - loglik: -3.2817e+02 - logprior: -1.0412e+00
Fitted a model with MAP estimate = -325.8934
expansions: []
discards: [ 51  56 120]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 332.7315 - loglik: -3.3076e+02 - logprior: -1.8206e+00
Epoch 2/2
39/39 - 15s - loss: 328.4020 - loglik: -3.2733e+02 - logprior: -7.9065e-01
Fitted a model with MAP estimate = -325.8313
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 332.4680 - loglik: -3.3066e+02 - logprior: -1.6675e+00
Epoch 2/10
39/39 - 15s - loss: 328.1088 - loglik: -3.2720e+02 - logprior: -6.1615e-01
Epoch 3/10
39/39 - 15s - loss: 325.5659 - loglik: -3.2464e+02 - logprior: -5.0972e-01
Epoch 4/10
39/39 - 15s - loss: 322.6745 - loglik: -3.2186e+02 - logprior: -3.7692e-01
Epoch 5/10
39/39 - 15s - loss: 322.0313 - loglik: -3.2138e+02 - logprior: -2.4825e-01
Epoch 6/10
39/39 - 15s - loss: 320.6038 - loglik: -3.2005e+02 - logprior: -1.2405e-01
Epoch 7/10
39/39 - 15s - loss: 318.4737 - loglik: -3.1794e+02 - logprior: -2.9380e-02
Epoch 8/10
39/39 - 15s - loss: 317.5621 - loglik: -3.1716e+02 - logprior: 0.0737
Epoch 9/10
39/39 - 15s - loss: 317.1425 - loglik: -3.1687e+02 - logprior: 0.1679
Epoch 10/10
39/39 - 15s - loss: 317.0680 - loglik: -3.1690e+02 - logprior: 0.2597
Fitted a model with MAP estimate = -316.2358
Time for alignment: 411.5787
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 472.8628 - loglik: -4.7092e+02 - logprior: -1.9230e+00
Epoch 2/10
39/39 - 12s - loss: 369.9361 - loglik: -3.6777e+02 - logprior: -1.9445e+00
Epoch 3/10
39/39 - 12s - loss: 362.3763 - loglik: -3.6006e+02 - logprior: -1.9760e+00
Epoch 4/10
39/39 - 12s - loss: 359.2573 - loglik: -3.5702e+02 - logprior: -1.9047e+00
Epoch 5/10
39/39 - 12s - loss: 358.4556 - loglik: -3.5626e+02 - logprior: -1.8736e+00
Epoch 6/10
39/39 - 12s - loss: 357.4446 - loglik: -3.5528e+02 - logprior: -1.8411e+00
Epoch 7/10
39/39 - 12s - loss: 357.0081 - loglik: -3.5487e+02 - logprior: -1.8219e+00
Epoch 8/10
39/39 - 12s - loss: 355.9923 - loglik: -3.5382e+02 - logprior: -1.8159e+00
Epoch 9/10
39/39 - 12s - loss: 354.9482 - loglik: -3.5274e+02 - logprior: -1.7969e+00
Epoch 10/10
39/39 - 12s - loss: 354.9644 - loglik: -3.5278e+02 - logprior: -1.7888e+00
Fitted a model with MAP estimate = -353.8964
expansions: [(4, 2), (5, 2), (12, 2), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (41, 2), (42, 1), (43, 2), (52, 1), (58, 1), (59, 1), (61, 2), (63, 2), (67, 1), (75, 1), (98, 2), (99, 1), (100, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 1), (120, 1), (127, 1), (129, 2), (132, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 199 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 342.5184 - loglik: -3.4046e+02 - logprior: -1.9321e+00
Epoch 2/2
39/39 - 16s - loss: 327.6788 - loglik: -3.2649e+02 - logprior: -9.3530e-01
Fitted a model with MAP estimate = -324.8184
expansions: []
discards: [  5  52  57  80  84 123 174]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 332.5771 - loglik: -3.3078e+02 - logprior: -1.6527e+00
Epoch 2/2
39/39 - 15s - loss: 327.9985 - loglik: -3.2712e+02 - logprior: -6.0671e-01
Fitted a model with MAP estimate = -325.4189
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 331.7979 - loglik: -3.3019e+02 - logprior: -1.4660e+00
Epoch 2/10
39/39 - 15s - loss: 327.6842 - loglik: -3.2685e+02 - logprior: -5.3601e-01
Epoch 3/10
39/39 - 15s - loss: 325.0915 - loglik: -3.2423e+02 - logprior: -4.5580e-01
Epoch 4/10
39/39 - 15s - loss: 322.0888 - loglik: -3.2128e+02 - logprior: -3.5314e-01
Epoch 5/10
39/39 - 15s - loss: 320.8580 - loglik: -3.2021e+02 - logprior: -2.5054e-01
Epoch 6/10
39/39 - 15s - loss: 319.5804 - loglik: -3.1903e+02 - logprior: -1.3500e-01
Epoch 7/10
39/39 - 15s - loss: 317.2973 - loglik: -3.1679e+02 - logprior: -1.2921e-02
Epoch 8/10
39/39 - 15s - loss: 317.4994 - loglik: -3.1712e+02 - logprior: 0.0973
Fitted a model with MAP estimate = -316.0334
Time for alignment: 381.5069
Computed alignments with likelihoods: ['-316.8268', '-316.2358', '-316.0334']
Best model has likelihood: -316.0334
time for generating output: 0.2108
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00687.projection.fasta
SP score = 0.7698851552530838
Training of 3 independent models on file PF03129.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fca29fb59a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd03066f070>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd03066f280>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd028a980a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd028a98d90>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd1e6af5d30>, <__main__.SimpleDirichletPrior object at 0x7fca34f7aca0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 258.1663 - loglik: -2.5492e+02 - logprior: -3.1585e+00
Epoch 2/10
19/19 - 2s - loss: 226.2515 - loglik: -2.2449e+02 - logprior: -1.3750e+00
Epoch 3/10
19/19 - 2s - loss: 212.9320 - loglik: -2.1085e+02 - logprior: -1.5080e+00
Epoch 4/10
19/19 - 2s - loss: 209.2727 - loglik: -2.0742e+02 - logprior: -1.4367e+00
Epoch 5/10
19/19 - 2s - loss: 208.3256 - loglik: -2.0661e+02 - logprior: -1.4084e+00
Epoch 6/10
19/19 - 2s - loss: 207.8685 - loglik: -2.0624e+02 - logprior: -1.3539e+00
Epoch 7/10
19/19 - 2s - loss: 206.4165 - loglik: -2.0483e+02 - logprior: -1.3500e+00
Epoch 8/10
19/19 - 2s - loss: 205.6073 - loglik: -2.0407e+02 - logprior: -1.3212e+00
Epoch 9/10
19/19 - 2s - loss: 205.7351 - loglik: -2.0423e+02 - logprior: -1.3045e+00
Fitted a model with MAP estimate = -204.8684
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (24, 1), (29, 1), (36, 1), (37, 2), (38, 1), (50, 2), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 211.3245 - loglik: -2.0716e+02 - logprior: -4.0755e+00
Epoch 2/2
19/19 - 3s - loss: 200.7067 - loglik: -1.9830e+02 - logprior: -2.0680e+00
Fitted a model with MAP estimate = -197.5398
expansions: [(0, 2)]
discards: [ 0  7 46 62]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 200.9597 - loglik: -1.9796e+02 - logprior: -2.9328e+00
Epoch 2/2
19/19 - 3s - loss: 196.8960 - loglik: -1.9549e+02 - logprior: -1.1289e+00
Fitted a model with MAP estimate = -195.1871
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 202.1573 - loglik: -1.9861e+02 - logprior: -3.5015e+00
Epoch 2/10
19/19 - 3s - loss: 197.1043 - loglik: -1.9564e+02 - logprior: -1.3050e+00
Epoch 3/10
19/19 - 2s - loss: 196.0404 - loglik: -1.9457e+02 - logprior: -1.1783e+00
Epoch 4/10
19/19 - 2s - loss: 195.1343 - loglik: -1.9359e+02 - logprior: -1.1474e+00
Epoch 5/10
19/19 - 2s - loss: 194.8791 - loglik: -1.9337e+02 - logprior: -1.1135e+00
Epoch 6/10
19/19 - 2s - loss: 194.5655 - loglik: -1.9312e+02 - logprior: -1.0800e+00
Epoch 7/10
19/19 - 3s - loss: 194.3185 - loglik: -1.9292e+02 - logprior: -1.0697e+00
Epoch 8/10
19/19 - 2s - loss: 194.2154 - loglik: -1.9286e+02 - logprior: -1.0509e+00
Epoch 9/10
19/19 - 3s - loss: 194.0804 - loglik: -1.9277e+02 - logprior: -1.0250e+00
Epoch 10/10
19/19 - 3s - loss: 193.8273 - loglik: -1.9254e+02 - logprior: -1.0122e+00
Fitted a model with MAP estimate = -193.5626
Time for alignment: 88.0515
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 257.8981 - loglik: -2.5466e+02 - logprior: -3.1603e+00
Epoch 2/10
19/19 - 2s - loss: 225.8769 - loglik: -2.2405e+02 - logprior: -1.3528e+00
Epoch 3/10
19/19 - 2s - loss: 213.2285 - loglik: -2.1111e+02 - logprior: -1.4503e+00
Epoch 4/10
19/19 - 2s - loss: 209.7408 - loglik: -2.0797e+02 - logprior: -1.3887e+00
Epoch 5/10
19/19 - 2s - loss: 207.6526 - loglik: -2.0596e+02 - logprior: -1.3905e+00
Epoch 6/10
19/19 - 2s - loss: 206.8522 - loglik: -2.0532e+02 - logprior: -1.2950e+00
Epoch 7/10
19/19 - 2s - loss: 206.4980 - loglik: -2.0501e+02 - logprior: -1.2710e+00
Epoch 8/10
19/19 - 2s - loss: 206.4870 - loglik: -2.0503e+02 - logprior: -1.2549e+00
Epoch 9/10
19/19 - 2s - loss: 206.5986 - loglik: -2.0515e+02 - logprior: -1.2480e+00
Fitted a model with MAP estimate = -205.6464
expansions: [(7, 1), (8, 2), (9, 4), (12, 1), (15, 1), (27, 1), (30, 2), (37, 1), (38, 1), (50, 2), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 211.3777 - loglik: -2.0722e+02 - logprior: -4.0621e+00
Epoch 2/2
19/19 - 3s - loss: 200.4207 - loglik: -1.9796e+02 - logprior: -2.0470e+00
Fitted a model with MAP estimate = -197.2685
expansions: [(0, 2)]
discards: [ 0  8  9 12 39 63]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 201.3324 - loglik: -1.9833e+02 - logprior: -2.9368e+00
Epoch 2/2
19/19 - 3s - loss: 197.1727 - loglik: -1.9579e+02 - logprior: -1.1341e+00
Fitted a model with MAP estimate = -194.8247
expansions: [(53, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 201.3628 - loglik: -1.9757e+02 - logprior: -3.7271e+00
Epoch 2/10
19/19 - 3s - loss: 195.2887 - loglik: -1.9368e+02 - logprior: -1.3275e+00
Epoch 3/10
19/19 - 3s - loss: 193.1880 - loglik: -1.9153e+02 - logprior: -1.1726e+00
Epoch 4/10
19/19 - 3s - loss: 192.5321 - loglik: -1.9093e+02 - logprior: -1.1271e+00
Epoch 5/10
19/19 - 3s - loss: 192.4411 - loglik: -1.9094e+02 - logprior: -1.1050e+00
Epoch 6/10
19/19 - 2s - loss: 191.7254 - loglik: -1.9031e+02 - logprior: -1.0708e+00
Epoch 7/10
19/19 - 3s - loss: 191.7638 - loglik: -1.9039e+02 - logprior: -1.0561e+00
Fitted a model with MAP estimate = -191.2357
Time for alignment: 80.3868
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 257.8562 - loglik: -2.5462e+02 - logprior: -3.1597e+00
Epoch 2/10
19/19 - 2s - loss: 225.5900 - loglik: -2.2389e+02 - logprior: -1.3573e+00
Epoch 3/10
19/19 - 2s - loss: 213.0959 - loglik: -2.1109e+02 - logprior: -1.4593e+00
Epoch 4/10
19/19 - 2s - loss: 209.3047 - loglik: -2.0757e+02 - logprior: -1.3650e+00
Epoch 5/10
19/19 - 2s - loss: 207.8081 - loglik: -2.0618e+02 - logprior: -1.3572e+00
Epoch 6/10
19/19 - 2s - loss: 206.7626 - loglik: -2.0523e+02 - logprior: -1.3183e+00
Epoch 7/10
19/19 - 2s - loss: 205.9380 - loglik: -2.0444e+02 - logprior: -1.2876e+00
Epoch 8/10
19/19 - 2s - loss: 204.6760 - loglik: -2.0319e+02 - logprior: -1.2850e+00
Epoch 9/10
19/19 - 2s - loss: 204.5333 - loglik: -2.0309e+02 - logprior: -1.2622e+00
Epoch 10/10
19/19 - 2s - loss: 204.1907 - loglik: -2.0278e+02 - logprior: -1.2428e+00
Fitted a model with MAP estimate = -203.7231
expansions: [(7, 1), (8, 2), (9, 4), (12, 1), (15, 1), (24, 1), (26, 1), (37, 2), (38, 2), (50, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 207.6671 - loglik: -2.0354e+02 - logprior: -4.0376e+00
Epoch 2/2
19/19 - 3s - loss: 196.6427 - loglik: -1.9425e+02 - logprior: -2.0375e+00
Fitted a model with MAP estimate = -193.6722
expansions: [(0, 2)]
discards: [ 0  8  9 12 47]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 197.3509 - loglik: -1.9438e+02 - logprior: -2.9102e+00
Epoch 2/2
19/19 - 3s - loss: 193.2699 - loglik: -1.9190e+02 - logprior: -1.1038e+00
Fitted a model with MAP estimate = -191.5629
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 198.5102 - loglik: -1.9498e+02 - logprior: -3.4803e+00
Epoch 2/10
19/19 - 3s - loss: 193.6714 - loglik: -1.9224e+02 - logprior: -1.2661e+00
Epoch 3/10
19/19 - 3s - loss: 192.3002 - loglik: -1.9085e+02 - logprior: -1.1541e+00
Epoch 4/10
19/19 - 3s - loss: 191.7458 - loglik: -1.9024e+02 - logprior: -1.1121e+00
Epoch 5/10
19/19 - 3s - loss: 191.2722 - loglik: -1.8980e+02 - logprior: -1.0830e+00
Epoch 6/10
19/19 - 3s - loss: 191.0192 - loglik: -1.8960e+02 - logprior: -1.0643e+00
Epoch 7/10
19/19 - 3s - loss: 190.7950 - loglik: -1.8944e+02 - logprior: -1.0381e+00
Epoch 8/10
19/19 - 3s - loss: 190.7253 - loglik: -1.8941e+02 - logprior: -1.0116e+00
Epoch 9/10
19/19 - 3s - loss: 190.5553 - loglik: -1.8927e+02 - logprior: -9.9939e-01
Epoch 10/10
19/19 - 3s - loss: 190.4462 - loglik: -1.8919e+02 - logprior: -9.8122e-01
Fitted a model with MAP estimate = -190.0311
Time for alignment: 90.3708
Computed alignments with likelihoods: ['-193.5626', '-191.2357', '-190.0311']
Best model has likelihood: -190.0311
time for generating output: 0.1620
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF03129.projection.fasta
SP score = 0.9736512978692387
Training of 3 independent models on file PF13378.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd029850f70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd3c6cd89a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fca35bb55e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd208de3f10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd02998b400>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd0280eb7f0>, <__main__.SimpleDirichletPrior object at 0x7fca35b6c370>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 566.7238 - loglik: -5.6434e+02 - logprior: -1.8826e+00
Epoch 2/10
39/39 - 13s - loss: 487.4320 - loglik: -4.8457e+02 - logprior: -1.5957e+00
Epoch 3/10
39/39 - 13s - loss: 476.1299 - loglik: -4.7331e+02 - logprior: -1.6485e+00
Epoch 4/10
39/39 - 13s - loss: 474.2086 - loglik: -4.7161e+02 - logprior: -1.5995e+00
Epoch 5/10
39/39 - 14s - loss: 473.3224 - loglik: -4.7083e+02 - logprior: -1.6000e+00
Epoch 6/10
39/39 - 13s - loss: 472.8383 - loglik: -4.7043e+02 - logprior: -1.5966e+00
Epoch 7/10
39/39 - 13s - loss: 472.5477 - loglik: -4.7016e+02 - logprior: -1.5998e+00
Epoch 8/10
39/39 - 13s - loss: 472.0291 - loglik: -4.6967e+02 - logprior: -1.6045e+00
Epoch 9/10
39/39 - 14s - loss: 471.8385 - loglik: -4.6945e+02 - logprior: -1.6185e+00
Epoch 10/10
39/39 - 13s - loss: 471.5292 - loglik: -4.6916e+02 - logprior: -1.6270e+00
Fitted a model with MAP estimate = -467.9935
expansions: [(0, 3), (19, 2), (20, 2), (21, 1), (31, 1), (35, 2), (45, 2), (46, 1), (49, 1), (51, 1), (65, 1), (68, 1), (72, 1), (75, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (103, 1), (104, 1), (107, 1), (108, 1), (116, 1), (125, 1), (126, 1), (127, 1), (132, 1), (141, 2), (142, 1), (146, 1), (149, 2), (150, 2), (151, 3), (153, 1), (159, 1), (161, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 222 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 475.0106 - loglik: -4.7190e+02 - logprior: -2.8578e+00
Epoch 2/2
39/39 - 17s - loss: 454.4602 - loglik: -4.5262e+02 - logprior: -1.1856e+00
Fitted a model with MAP estimate = -448.4608
expansions: [(186, 1)]
discards: [ 23  26  45  57 113 190 192]
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 461.7409 - loglik: -4.5955e+02 - logprior: -1.9587e+00
Epoch 2/2
39/39 - 18s - loss: 453.3925 - loglik: -4.5153e+02 - logprior: -8.8298e-01
Fitted a model with MAP estimate = -447.3291
expansions: []
discards: [173]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 458.7351 - loglik: -4.5673e+02 - logprior: -1.7927e+00
Epoch 2/10
39/39 - 17s - loss: 451.4755 - loglik: -4.4994e+02 - logprior: -7.1126e-01
Epoch 3/10
39/39 - 17s - loss: 448.4277 - loglik: -4.4652e+02 - logprior: -6.4605e-01
Epoch 4/10
39/39 - 17s - loss: 447.2293 - loglik: -4.4539e+02 - logprior: -6.0156e-01
Epoch 5/10
39/39 - 16s - loss: 446.6317 - loglik: -4.4500e+02 - logprior: -5.2121e-01
Epoch 6/10
39/39 - 16s - loss: 446.6355 - loglik: -4.4518e+02 - logprior: -4.4956e-01
Fitted a model with MAP estimate = -444.6193
Time for alignment: 398.9858
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 567.2991 - loglik: -5.6491e+02 - logprior: -1.8899e+00
Epoch 2/10
39/39 - 13s - loss: 489.5281 - loglik: -4.8671e+02 - logprior: -1.5582e+00
Epoch 3/10
39/39 - 12s - loss: 477.0412 - loglik: -4.7420e+02 - logprior: -1.6559e+00
Epoch 4/10
39/39 - 13s - loss: 474.0149 - loglik: -4.7135e+02 - logprior: -1.6448e+00
Epoch 5/10
39/39 - 13s - loss: 473.1270 - loglik: -4.7059e+02 - logprior: -1.6385e+00
Epoch 6/10
39/39 - 13s - loss: 472.5786 - loglik: -4.7013e+02 - logprior: -1.6408e+00
Epoch 7/10
39/39 - 13s - loss: 472.2183 - loglik: -4.6983e+02 - logprior: -1.6437e+00
Epoch 8/10
39/39 - 13s - loss: 471.7263 - loglik: -4.6939e+02 - logprior: -1.6435e+00
Epoch 9/10
39/39 - 13s - loss: 471.4932 - loglik: -4.6914e+02 - logprior: -1.6459e+00
Epoch 10/10
39/39 - 13s - loss: 471.2625 - loglik: -4.6890e+02 - logprior: -1.6438e+00
Fitted a model with MAP estimate = -467.7398
expansions: [(0, 3), (11, 1), (18, 4), (20, 1), (35, 2), (45, 2), (46, 1), (49, 1), (50, 1), (58, 1), (68, 1), (72, 1), (75, 1), (84, 1), (85, 1), (86, 2), (87, 1), (89, 1), (92, 1), (102, 1), (107, 1), (108, 1), (116, 1), (125, 3), (129, 1), (131, 1), (140, 2), (141, 2), (149, 2), (150, 2), (151, 2), (153, 1), (159, 1), (161, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 222 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 475.5500 - loglik: -4.7230e+02 - logprior: -2.8749e+00
Epoch 2/2
39/39 - 17s - loss: 455.1524 - loglik: -4.5289e+02 - logprior: -1.1860e+00
Fitted a model with MAP estimate = -448.2120
expansions: [(187, 1)]
discards: [ 25  45  57 109 178 192]
Re-initialized the encoder parameters.
Fitting a model of length 217 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 462.3293 - loglik: -4.6015e+02 - logprior: -1.9372e+00
Epoch 2/2
39/39 - 17s - loss: 453.2203 - loglik: -4.5136e+02 - logprior: -9.0389e-01
Fitted a model with MAP estimate = -447.2696
expansions: []
discards: [175]
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 458.6194 - loglik: -4.5661e+02 - logprior: -1.7778e+00
Epoch 2/10
39/39 - 17s - loss: 451.0502 - loglik: -4.4939e+02 - logprior: -7.1890e-01
Epoch 3/10
39/39 - 17s - loss: 448.3732 - loglik: -4.4641e+02 - logprior: -6.3541e-01
Epoch 4/10
39/39 - 17s - loss: 447.2735 - loglik: -4.4546e+02 - logprior: -5.7344e-01
Epoch 5/10
39/39 - 17s - loss: 446.3032 - loglik: -4.4469e+02 - logprior: -4.9012e-01
Epoch 6/10
39/39 - 17s - loss: 446.3698 - loglik: -4.4494e+02 - logprior: -4.1953e-01
Fitted a model with MAP estimate = -444.5638
Time for alignment: 389.9921
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 565.7167 - loglik: -5.6335e+02 - logprior: -1.8717e+00
Epoch 2/10
39/39 - 13s - loss: 485.8216 - loglik: -4.8280e+02 - logprior: -1.6424e+00
Epoch 3/10
39/39 - 13s - loss: 476.3051 - loglik: -4.7331e+02 - logprior: -1.6327e+00
Epoch 4/10
39/39 - 13s - loss: 474.3029 - loglik: -4.7153e+02 - logprior: -1.5851e+00
Epoch 5/10
39/39 - 13s - loss: 473.2986 - loglik: -4.7065e+02 - logprior: -1.5882e+00
Epoch 6/10
39/39 - 13s - loss: 472.5778 - loglik: -4.7001e+02 - logprior: -1.6070e+00
Epoch 7/10
39/39 - 13s - loss: 472.0477 - loglik: -4.6952e+02 - logprior: -1.6196e+00
Epoch 8/10
39/39 - 13s - loss: 471.6807 - loglik: -4.6915e+02 - logprior: -1.6233e+00
Epoch 9/10
39/39 - 13s - loss: 471.1449 - loglik: -4.6859e+02 - logprior: -1.6368e+00
Epoch 10/10
39/39 - 13s - loss: 470.7356 - loglik: -4.6816e+02 - logprior: -1.6555e+00
Fitted a model with MAP estimate = -466.9942
expansions: [(0, 3), (11, 1), (20, 1), (21, 1), (26, 1), (35, 2), (45, 2), (46, 1), (49, 1), (50, 1), (58, 1), (68, 1), (72, 1), (75, 1), (84, 1), (85, 1), (86, 2), (87, 1), (89, 1), (103, 1), (108, 1), (109, 1), (113, 1), (117, 1), (125, 3), (136, 1), (146, 5), (150, 2), (151, 4), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 220 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 479.4249 - loglik: -4.7628e+02 - logprior: -2.8679e+00
Epoch 2/2
39/39 - 17s - loss: 455.4864 - loglik: -4.5370e+02 - logprior: -1.1527e+00
Fitted a model with MAP estimate = -449.2622
expansions: [(22, 1)]
discards: [ 43  55 107 182 183]
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 462.1754 - loglik: -4.6000e+02 - logprior: -1.9376e+00
Epoch 2/2
39/39 - 18s - loss: 453.6624 - loglik: -4.5173e+02 - logprior: -9.0981e-01
Fitted a model with MAP estimate = -447.6270
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 459.0079 - loglik: -4.5698e+02 - logprior: -1.8036e+00
Epoch 2/10
39/39 - 18s - loss: 451.3197 - loglik: -4.4969e+02 - logprior: -7.4866e-01
Epoch 3/10
39/39 - 18s - loss: 448.4992 - loglik: -4.4652e+02 - logprior: -6.8803e-01
Epoch 4/10
39/39 - 18s - loss: 447.8324 - loglik: -4.4596e+02 - logprior: -6.2455e-01
Epoch 5/10
39/39 - 18s - loss: 446.6641 - loglik: -4.4500e+02 - logprior: -5.3876e-01
Epoch 6/10
39/39 - 18s - loss: 446.1209 - loglik: -4.4461e+02 - logprior: -4.7068e-01
Epoch 7/10
39/39 - 18s - loss: 446.3336 - loglik: -4.4496e+02 - logprior: -3.9408e-01
Fitted a model with MAP estimate = -444.3205
Time for alignment: 421.6886
Computed alignments with likelihoods: ['-444.6193', '-444.5638', '-444.3205']
Best model has likelihood: -444.3205
time for generating output: 0.3186
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13378.projection.fasta
SP score = 0.7207810900716479
Training of 3 independent models on file PF00084.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fca457df2e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fca3d8c02e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1dd9bf730>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fca2840cdc0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fca2840cd30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd1e7838f10>, <__main__.SimpleDirichletPrior object at 0x7fca45958eb0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 164.9364 - loglik: -1.6168e+02 - logprior: -3.2196e+00
Epoch 2/10
19/19 - 1s - loss: 136.5254 - loglik: -1.3508e+02 - logprior: -1.4273e+00
Epoch 3/10
19/19 - 1s - loss: 128.1971 - loglik: -1.2654e+02 - logprior: -1.5344e+00
Epoch 4/10
19/19 - 1s - loss: 126.5175 - loglik: -1.2481e+02 - logprior: -1.4092e+00
Epoch 5/10
19/19 - 1s - loss: 126.2602 - loglik: -1.2459e+02 - logprior: -1.4095e+00
Epoch 6/10
19/19 - 1s - loss: 125.7128 - loglik: -1.2406e+02 - logprior: -1.3798e+00
Epoch 7/10
19/19 - 1s - loss: 125.7148 - loglik: -1.2404e+02 - logprior: -1.3703e+00
Fitted a model with MAP estimate = -125.2681
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (18, 1), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 133.7175 - loglik: -1.2954e+02 - logprior: -4.1375e+00
Epoch 2/2
19/19 - 2s - loss: 126.5914 - loglik: -1.2413e+02 - logprior: -2.2724e+00
Fitted a model with MAP estimate = -123.6598
expansions: []
discards: [12 13 41 42 51]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 126.4484 - loglik: -1.2301e+02 - logprior: -3.3881e+00
Epoch 2/2
19/19 - 1s - loss: 122.9953 - loglik: -1.2135e+02 - logprior: -1.4755e+00
Fitted a model with MAP estimate = -122.1905
expansions: []
discards: [14]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 125.7825 - loglik: -1.2248e+02 - logprior: -3.2548e+00
Epoch 2/10
19/19 - 1s - loss: 123.0395 - loglik: -1.2141e+02 - logprior: -1.4682e+00
Epoch 3/10
19/19 - 1s - loss: 122.4098 - loglik: -1.2077e+02 - logprior: -1.3752e+00
Epoch 4/10
19/19 - 1s - loss: 121.9124 - loglik: -1.2026e+02 - logprior: -1.3268e+00
Epoch 5/10
19/19 - 1s - loss: 121.6023 - loglik: -1.1998e+02 - logprior: -1.2915e+00
Epoch 6/10
19/19 - 1s - loss: 121.4739 - loglik: -1.1985e+02 - logprior: -1.2776e+00
Epoch 7/10
19/19 - 1s - loss: 121.4352 - loglik: -1.1983e+02 - logprior: -1.2577e+00
Epoch 8/10
19/19 - 1s - loss: 121.1395 - loglik: -1.1955e+02 - logprior: -1.2452e+00
Epoch 9/10
19/19 - 1s - loss: 121.0210 - loglik: -1.1943e+02 - logprior: -1.2290e+00
Epoch 10/10
19/19 - 1s - loss: 121.0288 - loglik: -1.1945e+02 - logprior: -1.2203e+00
Fitted a model with MAP estimate = -120.5022
Time for alignment: 56.3546
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 164.9374 - loglik: -1.6168e+02 - logprior: -3.2214e+00
Epoch 2/10
19/19 - 1s - loss: 136.2062 - loglik: -1.3477e+02 - logprior: -1.4197e+00
Epoch 3/10
19/19 - 1s - loss: 128.3751 - loglik: -1.2663e+02 - logprior: -1.5343e+00
Epoch 4/10
19/19 - 1s - loss: 126.9129 - loglik: -1.2526e+02 - logprior: -1.3950e+00
Epoch 5/10
19/19 - 1s - loss: 126.2886 - loglik: -1.2465e+02 - logprior: -1.3951e+00
Epoch 6/10
19/19 - 1s - loss: 125.8714 - loglik: -1.2422e+02 - logprior: -1.3831e+00
Epoch 7/10
19/19 - 1s - loss: 125.6653 - loglik: -1.2402e+02 - logprior: -1.3692e+00
Epoch 8/10
19/19 - 1s - loss: 125.6895 - loglik: -1.2403e+02 - logprior: -1.3631e+00
Fitted a model with MAP estimate = -125.1772
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (16, 1), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 133.8898 - loglik: -1.2968e+02 - logprior: -4.1345e+00
Epoch 2/2
19/19 - 1s - loss: 126.7760 - loglik: -1.2429e+02 - logprior: -2.2686e+00
Fitted a model with MAP estimate = -123.8178
expansions: []
discards: [13 14 16 17 41 42 51]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 126.8414 - loglik: -1.2340e+02 - logprior: -3.3812e+00
Epoch 2/2
19/19 - 1s - loss: 123.1920 - loglik: -1.2156e+02 - logprior: -1.4666e+00
Fitted a model with MAP estimate = -122.4118
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.7706 - loglik: -1.2247e+02 - logprior: -3.2448e+00
Epoch 2/10
19/19 - 1s - loss: 123.0648 - loglik: -1.2145e+02 - logprior: -1.4528e+00
Epoch 3/10
19/19 - 1s - loss: 122.3875 - loglik: -1.2075e+02 - logprior: -1.3642e+00
Epoch 4/10
19/19 - 1s - loss: 122.1003 - loglik: -1.2046e+02 - logprior: -1.3125e+00
Epoch 5/10
19/19 - 1s - loss: 121.7505 - loglik: -1.2014e+02 - logprior: -1.2773e+00
Epoch 6/10
19/19 - 1s - loss: 121.7066 - loglik: -1.2010e+02 - logprior: -1.2649e+00
Epoch 7/10
19/19 - 1s - loss: 121.2248 - loglik: -1.1964e+02 - logprior: -1.2508e+00
Epoch 8/10
19/19 - 1s - loss: 121.3526 - loglik: -1.1977e+02 - logprior: -1.2387e+00
Fitted a model with MAP estimate = -120.8283
Time for alignment: 53.7304
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 165.0536 - loglik: -1.6180e+02 - logprior: -3.2204e+00
Epoch 2/10
19/19 - 1s - loss: 138.2089 - loglik: -1.3677e+02 - logprior: -1.4221e+00
Epoch 3/10
19/19 - 1s - loss: 129.7923 - loglik: -1.2818e+02 - logprior: -1.5250e+00
Epoch 4/10
19/19 - 1s - loss: 127.8845 - loglik: -1.2621e+02 - logprior: -1.3910e+00
Epoch 5/10
19/19 - 1s - loss: 126.8819 - loglik: -1.2523e+02 - logprior: -1.3975e+00
Epoch 6/10
19/19 - 1s - loss: 126.3518 - loglik: -1.2471e+02 - logprior: -1.3769e+00
Epoch 7/10
19/19 - 1s - loss: 126.0390 - loglik: -1.2439e+02 - logprior: -1.3660e+00
Epoch 8/10
19/19 - 1s - loss: 126.0574 - loglik: -1.2441e+02 - logprior: -1.3534e+00
Fitted a model with MAP estimate = -125.5932
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (22, 2), (30, 2), (34, 3), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 134.5135 - loglik: -1.3029e+02 - logprior: -4.1553e+00
Epoch 2/2
19/19 - 1s - loss: 126.7907 - loglik: -1.2428e+02 - logprior: -2.2980e+00
Fitted a model with MAP estimate = -123.8689
expansions: []
discards: [13 14 32 42 48 52]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 126.5900 - loglik: -1.2313e+02 - logprior: -3.3949e+00
Epoch 2/2
19/19 - 1s - loss: 123.0185 - loglik: -1.2137e+02 - logprior: -1.4741e+00
Fitted a model with MAP estimate = -122.1682
expansions: []
discards: [14 15]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.8591 - loglik: -1.2256e+02 - logprior: -3.2473e+00
Epoch 2/10
19/19 - 2s - loss: 123.0115 - loglik: -1.2140e+02 - logprior: -1.4468e+00
Epoch 3/10
19/19 - 1s - loss: 122.5004 - loglik: -1.2087e+02 - logprior: -1.3595e+00
Epoch 4/10
19/19 - 1s - loss: 122.0154 - loglik: -1.2037e+02 - logprior: -1.3143e+00
Epoch 5/10
19/19 - 1s - loss: 121.7521 - loglik: -1.2015e+02 - logprior: -1.2787e+00
Epoch 6/10
19/19 - 1s - loss: 121.6135 - loglik: -1.2002e+02 - logprior: -1.2598e+00
Epoch 7/10
19/19 - 2s - loss: 121.4810 - loglik: -1.1990e+02 - logprior: -1.2401e+00
Epoch 8/10
19/19 - 1s - loss: 121.2112 - loglik: -1.1963e+02 - logprior: -1.2305e+00
Epoch 9/10
19/19 - 1s - loss: 121.3395 - loglik: -1.1977e+02 - logprior: -1.2200e+00
Fitted a model with MAP estimate = -120.6995
Time for alignment: 54.3234
Computed alignments with likelihoods: ['-120.5022', '-120.8283', '-120.6995']
Best model has likelihood: -120.5022
time for generating output: 0.1153
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00084.projection.fasta
SP score = 0.8885542168674698
Training of 3 independent models on file PF00232.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fca20643850>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fca206a54c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd02970bd30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fca206a0fd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2113c4130>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fca45b17820>, <__main__.SimpleDirichletPrior object at 0x7fd1e644adc0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 69s - loss: 900.2172 - loglik: -8.9844e+02 - logprior: -1.6671e+00
Epoch 2/10
39/39 - 71s - loss: 713.9067 - loglik: -7.1177e+02 - logprior: -1.6791e+00
Epoch 3/10
39/39 - 73s - loss: 699.0427 - loglik: -6.9630e+02 - logprior: -1.8091e+00
Epoch 4/10
39/39 - 73s - loss: 695.4824 - loglik: -6.9280e+02 - logprior: -1.7402e+00
Epoch 5/10
39/39 - 68s - loss: 695.3625 - loglik: -6.9267e+02 - logprior: -1.7643e+00
Epoch 6/10
39/39 - 62s - loss: 693.5280 - loglik: -6.9081e+02 - logprior: -1.8043e+00
Epoch 7/10
39/39 - 59s - loss: 692.4509 - loglik: -6.8970e+02 - logprior: -1.8321e+00
Epoch 8/10
39/39 - 59s - loss: 692.8785 - loglik: -6.9012e+02 - logprior: -1.8502e+00
Fitted a model with MAP estimate = -690.5302
expansions: [(0, 3), (38, 1), (42, 1), (131, 1), (133, 1), (135, 1), (162, 1), (164, 1), (175, 8), (176, 1), (177, 1), (187, 1), (189, 1), (190, 1), (191, 4), (192, 1), (193, 1), (196, 1), (197, 1), (198, 1), (199, 1), (201, 1), (202, 2), (203, 2), (204, 1), (207, 1), (209, 1), (211, 1), (215, 1), (218, 1), (223, 3), (224, 4), (225, 2), (227, 1), (228, 3), (229, 1), (230, 1), (231, 2), (232, 1), (243, 1), (245, 1), (246, 2), (247, 2), (248, 4), (249, 2), (250, 5), (251, 2), (252, 1), (254, 1), (256, 1), (269, 1), (286, 1), (287, 2), (288, 1), (289, 2), (291, 1), (302, 1), (304, 1), (305, 1), (306, 1), (308, 1), (313, 1), (326, 1), (330, 1), (342, 1), (349, 1), (355, 6)]
discards: [  2 127]
Re-initialized the encoder parameters.
Fitting a model of length 461 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 104s - loss: 677.7184 - loglik: -6.7481e+02 - logprior: -2.6349e+00
Epoch 2/2
39/39 - 92s - loss: 654.4200 - loglik: -6.5247e+02 - logprior: -1.2564e+00
Fitted a model with MAP estimate = -649.5236
expansions: [(313, 1)]
discards: [  2 186 187 213 237 266 267 289 314 315 316 317 318 319 459]
Re-initialized the encoder parameters.
Fitting a model of length 447 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 112s - loss: 664.4419 - loglik: -6.6235e+02 - logprior: -1.8226e+00
Epoch 2/2
39/39 - 111s - loss: 656.5085 - loglik: -6.5470e+02 - logprior: -8.5944e-01
Fitted a model with MAP estimate = -652.0326
expansions: [(261, 1), (302, 1), (303, 2), (304, 1)]
discards: [  2 305 306]
Re-initialized the encoder parameters.
Fitting a model of length 449 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 111s - loss: 662.2657 - loglik: -6.6067e+02 - logprior: -1.3283e+00
Epoch 2/10
39/39 - 109s - loss: 655.5391 - loglik: -6.5408e+02 - logprior: -5.4775e-01
Epoch 3/10
39/39 - 105s - loss: 652.1311 - loglik: -6.5053e+02 - logprior: -2.5385e-01
Epoch 4/10
39/39 - 101s - loss: 650.0347 - loglik: -6.4860e+02 - logprior: -4.1285e-02
Epoch 5/10
39/39 - 110s - loss: 650.2437 - loglik: -6.4892e+02 - logprior: -2.3943e-03
Fitted a model with MAP estimate = -646.9215
Time for alignment: 1973.3577
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 73s - loss: 899.7214 - loglik: -8.9784e+02 - logprior: -1.7591e+00
Epoch 2/10
39/39 - 72s - loss: 716.9644 - loglik: -7.1424e+02 - logprior: -1.9029e+00
Epoch 3/10
39/39 - 71s - loss: 704.3588 - loglik: -7.0107e+02 - logprior: -2.1809e+00
Epoch 4/10
39/39 - 65s - loss: 700.8110 - loglik: -6.9745e+02 - logprior: -2.2094e+00
Epoch 5/10
39/39 - 62s - loss: 698.7741 - loglik: -6.9545e+02 - logprior: -2.2305e+00
Epoch 6/10
39/39 - 66s - loss: 697.6238 - loglik: -6.9441e+02 - logprior: -2.1767e+00
Epoch 7/10
39/39 - 72s - loss: 697.3411 - loglik: -6.9414e+02 - logprior: -2.2030e+00
Epoch 8/10
39/39 - 65s - loss: 697.0894 - loglik: -6.9377e+02 - logprior: -2.3755e+00
Epoch 9/10
39/39 - 60s - loss: 696.0976 - loglik: -6.9280e+02 - logprior: -2.3814e+00
Epoch 10/10
39/39 - 60s - loss: 695.8922 - loglik: -6.9258e+02 - logprior: -2.4169e+00
Fitted a model with MAP estimate = -694.4315
expansions: [(0, 3), (39, 1), (43, 1), (132, 1), (135, 1), (147, 1), (164, 1), (178, 8), (179, 1), (180, 1), (190, 1), (192, 2), (193, 7), (194, 1), (197, 1), (198, 1), (199, 1), (200, 2), (201, 1), (202, 2), (203, 2), (204, 1), (209, 1), (212, 1), (216, 2), (219, 2), (224, 1), (225, 6), (228, 1), (229, 3), (230, 1), (231, 1), (232, 2), (233, 1), (244, 1), (245, 1), (247, 2), (248, 6), (249, 2), (250, 5), (251, 2), (252, 2), (253, 1), (266, 1), (268, 1), (270, 1), (284, 1), (286, 1), (287, 1), (288, 2), (290, 1), (291, 1), (300, 1), (302, 1), (303, 1), (304, 1), (306, 1), (326, 1), (329, 1), (341, 1), (348, 1), (351, 1), (355, 3)]
discards: [  2   3 128 352 353]
Re-initialized the encoder parameters.
Fitting a model of length 458 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 105s - loss: 679.6320 - loglik: -6.7684e+02 - logprior: -2.5120e+00
Epoch 2/2
39/39 - 92s - loss: 655.8861 - loglik: -6.5417e+02 - logprior: -9.9384e-01
Fitted a model with MAP estimate = -650.9523
expansions: [(217, 1), (312, 2), (313, 1), (320, 1), (321, 1), (453, 1), (455, 1)]
discards: [  2   3   4 187 188 189 190 213 214 238 261 290 310 323 324 325 326]
Re-initialized the encoder parameters.
Fitting a model of length 449 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 111s - loss: 664.9099 - loglik: -6.6282e+02 - logprior: -1.8089e+00
Epoch 2/2
39/39 - 104s - loss: 656.1108 - loglik: -6.5459e+02 - logprior: -5.6088e-01
Fitted a model with MAP estimate = -651.4938
expansions: [(0, 3), (313, 1)]
discards: [305 306 309 310 315]
Re-initialized the encoder parameters.
Fitting a model of length 448 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 98s - loss: 663.9299 - loglik: -6.6174e+02 - logprior: -1.9256e+00
Epoch 2/10
39/39 - 92s - loss: 655.4489 - loglik: -6.5411e+02 - logprior: -4.5376e-01
Epoch 3/10
39/39 - 93s - loss: 652.5468 - loglik: -6.5077e+02 - logprior: -4.2120e-01
Epoch 4/10
39/39 - 91s - loss: 651.1626 - loglik: -6.4948e+02 - logprior: -2.8118e-01
Epoch 5/10
39/39 - 92s - loss: 649.5609 - loglik: -6.4812e+02 - logprior: -1.0014e-01
Epoch 6/10
39/39 - 88s - loss: 649.0511 - loglik: -6.4765e+02 - logprior: -1.4160e-01
Epoch 7/10
39/39 - 86s - loss: 648.1448 - loglik: -6.4712e+02 - logprior: 0.1858
Epoch 8/10
39/39 - 87s - loss: 647.8207 - loglik: -6.4712e+02 - logprior: 0.4570
Epoch 9/10
39/39 - 88s - loss: 646.9149 - loglik: -6.4627e+02 - logprior: 0.4965
Epoch 10/10
39/39 - 90s - loss: 646.3961 - loglik: -6.4595e+02 - logprior: 0.6811
Fitted a model with MAP estimate = -644.8180
Time for alignment: 2426.5681
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 63s - loss: 904.8534 - loglik: -9.0299e+02 - logprior: -1.7188e+00
Epoch 2/10
39/39 - 60s - loss: 718.0379 - loglik: -7.1527e+02 - logprior: -1.6959e+00
Epoch 3/10
39/39 - 60s - loss: 703.5663 - loglik: -7.0066e+02 - logprior: -1.7368e+00
Epoch 4/10
39/39 - 61s - loss: 700.2950 - loglik: -6.9745e+02 - logprior: -1.6931e+00
Epoch 5/10
39/39 - 61s - loss: 698.2769 - loglik: -6.9548e+02 - logprior: -1.7165e+00
Epoch 6/10
39/39 - 61s - loss: 696.4694 - loglik: -6.9372e+02 - logprior: -1.7417e+00
Epoch 7/10
39/39 - 60s - loss: 696.5660 - loglik: -6.9385e+02 - logprior: -1.7614e+00
Fitted a model with MAP estimate = -694.2961
expansions: [(0, 3), (37, 1), (43, 1), (70, 1), (131, 1), (134, 1), (146, 1), (163, 1), (164, 1), (170, 1), (175, 1), (176, 6), (177, 1), (178, 1), (191, 2), (192, 7), (193, 1), (196, 1), (197, 1), (198, 1), (199, 2), (200, 1), (201, 1), (203, 1), (204, 2), (209, 1), (211, 1), (215, 1), (217, 1), (218, 1), (223, 1), (224, 8), (226, 1), (227, 2), (228, 3), (229, 2), (242, 1), (244, 1), (245, 2), (246, 2), (247, 3), (249, 2), (250, 6), (251, 2), (252, 1), (254, 1), (270, 1), (287, 1), (288, 2), (289, 1), (290, 2), (292, 1), (293, 1), (302, 1), (304, 2), (305, 2), (307, 1), (327, 1), (330, 1), (339, 1), (349, 1), (355, 6)]
discards: [  2   3 127]
Re-initialized the encoder parameters.
Fitting a model of length 461 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 93s - loss: 678.2242 - loglik: -6.7533e+02 - logprior: -2.6125e+00
Epoch 2/2
39/39 - 87s - loss: 654.2123 - loglik: -6.5227e+02 - logprior: -1.2499e+00
Fitted a model with MAP estimate = -649.5574
expansions: [(217, 1), (281, 1)]
discards: [  2   3 187 188 189 190 214 215 273 287 308 315 316 317 318 328 459]
Re-initialized the encoder parameters.
Fitting a model of length 446 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 89s - loss: 664.1789 - loglik: -6.6216e+02 - logprior: -1.7479e+00
Epoch 2/2
39/39 - 87s - loss: 655.7884 - loglik: -6.5391e+02 - logprior: -9.2803e-01
Fitted a model with MAP estimate = -651.1653
expansions: [(303, 2), (304, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 449 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 95s - loss: 661.1803 - loglik: -6.5936e+02 - logprior: -1.5597e+00
Epoch 2/10
39/39 - 92s - loss: 654.9918 - loglik: -6.5359e+02 - logprior: -5.2109e-01
Epoch 3/10
39/39 - 95s - loss: 651.8624 - loglik: -6.5009e+02 - logprior: -4.1854e-01
Epoch 4/10
39/39 - 92s - loss: 649.6979 - loglik: -6.4802e+02 - logprior: -2.7671e-01
Epoch 5/10
39/39 - 87s - loss: 650.2116 - loglik: -6.4881e+02 - logprior: -6.5208e-02
Fitted a model with MAP estimate = -646.8260
Time for alignment: 1649.7899
Computed alignments with likelihoods: ['-646.9215', '-644.8180', '-646.8260']
Best model has likelihood: -644.8180
time for generating output: 0.5084
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00232.projection.fasta
SP score = 0.8176051635878032
Training of 3 independent models on file PF13522.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd2112b8490>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fca5c4f5040>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1e71d43a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fca28286fa0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fca28286a30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd3d833b970>, <__main__.SimpleDirichletPrior object at 0x7fd3c6b541f0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 408.9968 - loglik: -4.0598e+02 - logprior: -2.9973e+00
Epoch 2/10
19/19 - 4s - loss: 325.3688 - loglik: -3.2417e+02 - logprior: -1.0852e+00
Epoch 3/10
19/19 - 5s - loss: 295.3077 - loglik: -2.9337e+02 - logprior: -1.3198e+00
Epoch 4/10
19/19 - 5s - loss: 290.6909 - loglik: -2.8875e+02 - logprior: -1.2977e+00
Epoch 5/10
19/19 - 4s - loss: 289.7081 - loglik: -2.8779e+02 - logprior: -1.2652e+00
Epoch 6/10
19/19 - 5s - loss: 288.5380 - loglik: -2.8668e+02 - logprior: -1.2367e+00
Epoch 7/10
19/19 - 5s - loss: 288.4315 - loglik: -2.8663e+02 - logprior: -1.2167e+00
Epoch 8/10
19/19 - 5s - loss: 287.3475 - loglik: -2.8558e+02 - logprior: -1.2030e+00
Epoch 9/10
19/19 - 5s - loss: 287.6301 - loglik: -2.8588e+02 - logprior: -1.2004e+00
Fitted a model with MAP estimate = -286.5975
expansions: [(0, 6), (7, 3), (8, 2), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (53, 1), (55, 1), (58, 1), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (97, 1), (98, 1), (99, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 156 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 291.7071 - loglik: -2.8752e+02 - logprior: -4.1009e+00
Epoch 2/2
19/19 - 6s - loss: 277.6635 - loglik: -2.7605e+02 - logprior: -1.2479e+00
Fitted a model with MAP estimate = -274.8574
expansions: [(0, 4)]
discards: [  1   2   3   4   5  21  98  99 145]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 283.5167 - loglik: -2.7949e+02 - logprior: -3.9476e+00
Epoch 2/2
19/19 - 6s - loss: 277.6445 - loglik: -2.7611e+02 - logprior: -1.2116e+00
Fitted a model with MAP estimate = -274.9693
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 281.0868 - loglik: -2.7788e+02 - logprior: -3.1339e+00
Epoch 2/10
19/19 - 6s - loss: 275.7684 - loglik: -2.7449e+02 - logprior: -9.7764e-01
Epoch 3/10
19/19 - 6s - loss: 274.5531 - loglik: -2.7328e+02 - logprior: -8.0983e-01
Epoch 4/10
19/19 - 6s - loss: 273.0778 - loglik: -2.7169e+02 - logprior: -7.5246e-01
Epoch 5/10
19/19 - 6s - loss: 272.3250 - loglik: -2.7089e+02 - logprior: -7.1149e-01
Epoch 6/10
19/19 - 6s - loss: 271.7902 - loglik: -2.7038e+02 - logprior: -6.7419e-01
Epoch 7/10
19/19 - 6s - loss: 271.7169 - loglik: -2.7037e+02 - logprior: -6.4651e-01
Epoch 8/10
19/19 - 6s - loss: 271.0717 - loglik: -2.6980e+02 - logprior: -6.0608e-01
Epoch 9/10
19/19 - 6s - loss: 270.4511 - loglik: -2.6922e+02 - logprior: -5.8621e-01
Epoch 10/10
19/19 - 6s - loss: 271.2412 - loglik: -2.7005e+02 - logprior: -5.5354e-01
Fitted a model with MAP estimate = -269.9594
Time for alignment: 171.7647
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 409.2266 - loglik: -4.0621e+02 - logprior: -2.9970e+00
Epoch 2/10
19/19 - 5s - loss: 326.0232 - loglik: -3.2477e+02 - logprior: -1.1210e+00
Epoch 3/10
19/19 - 5s - loss: 296.4742 - loglik: -2.9461e+02 - logprior: -1.3500e+00
Epoch 4/10
19/19 - 5s - loss: 291.7490 - loglik: -2.8986e+02 - logprior: -1.2851e+00
Epoch 5/10
19/19 - 5s - loss: 290.2769 - loglik: -2.8844e+02 - logprior: -1.2452e+00
Epoch 6/10
19/19 - 5s - loss: 289.0906 - loglik: -2.8729e+02 - logprior: -1.2210e+00
Epoch 7/10
19/19 - 5s - loss: 288.9440 - loglik: -2.8719e+02 - logprior: -1.2020e+00
Epoch 8/10
19/19 - 5s - loss: 288.3141 - loglik: -2.8656e+02 - logprior: -1.1946e+00
Epoch 9/10
19/19 - 5s - loss: 287.9045 - loglik: -2.8615e+02 - logprior: -1.1927e+00
Epoch 10/10
19/19 - 5s - loss: 287.9488 - loglik: -2.8619e+02 - logprior: -1.2052e+00
Fitted a model with MAP estimate = -286.9165
expansions: [(0, 6), (7, 3), (8, 2), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (53, 1), (55, 1), (58, 1), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (96, 1), (97, 1), (99, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 156 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 292.1302 - loglik: -2.8790e+02 - logprior: -4.1434e+00
Epoch 2/2
19/19 - 6s - loss: 277.7370 - loglik: -2.7612e+02 - logprior: -1.2554e+00
Fitted a model with MAP estimate = -274.8447
expansions: [(0, 4)]
discards: [  1   2   3   4   5  21  98 145]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 283.6231 - loglik: -2.7957e+02 - logprior: -3.9747e+00
Epoch 2/2
19/19 - 6s - loss: 277.6331 - loglik: -2.7609e+02 - logprior: -1.2463e+00
Fitted a model with MAP estimate = -275.2461
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 281.4258 - loglik: -2.7820e+02 - logprior: -3.1489e+00
Epoch 2/10
19/19 - 6s - loss: 276.4884 - loglik: -2.7518e+02 - logprior: -1.0115e+00
Epoch 3/10
19/19 - 6s - loss: 274.7331 - loglik: -2.7343e+02 - logprior: -8.2016e-01
Epoch 4/10
19/19 - 6s - loss: 273.2188 - loglik: -2.7181e+02 - logprior: -7.6086e-01
Epoch 5/10
19/19 - 6s - loss: 272.9304 - loglik: -2.7149e+02 - logprior: -6.9853e-01
Epoch 6/10
19/19 - 6s - loss: 271.7220 - loglik: -2.7032e+02 - logprior: -6.6353e-01
Epoch 7/10
19/19 - 6s - loss: 272.0112 - loglik: -2.7066e+02 - logprior: -6.3557e-01
Fitted a model with MAP estimate = -270.7828
Time for alignment: 161.4355
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 409.3209 - loglik: -4.0632e+02 - logprior: -2.9912e+00
Epoch 2/10
19/19 - 5s - loss: 327.1131 - loglik: -3.2599e+02 - logprior: -1.0720e+00
Epoch 3/10
19/19 - 5s - loss: 296.2852 - loglik: -2.9455e+02 - logprior: -1.3249e+00
Epoch 4/10
19/19 - 5s - loss: 291.4546 - loglik: -2.8959e+02 - logprior: -1.2723e+00
Epoch 5/10
19/19 - 5s - loss: 289.5976 - loglik: -2.8781e+02 - logprior: -1.2244e+00
Epoch 6/10
19/19 - 5s - loss: 289.1971 - loglik: -2.8742e+02 - logprior: -1.1898e+00
Epoch 7/10
19/19 - 5s - loss: 289.0011 - loglik: -2.8727e+02 - logprior: -1.1630e+00
Epoch 8/10
19/19 - 5s - loss: 288.2815 - loglik: -2.8655e+02 - logprior: -1.1535e+00
Epoch 9/10
19/19 - 5s - loss: 287.3620 - loglik: -2.8564e+02 - logprior: -1.1534e+00
Epoch 10/10
19/19 - 5s - loss: 287.9514 - loglik: -2.8624e+02 - logprior: -1.1537e+00
Fitted a model with MAP estimate = -286.7598
expansions: [(0, 6), (7, 2), (8, 2), (9, 2), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (52, 1), (54, 2), (58, 1), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (96, 2), (97, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 158 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 291.8906 - loglik: -2.8767e+02 - logprior: -4.1322e+00
Epoch 2/2
19/19 - 6s - loss: 277.4328 - loglik: -2.7579e+02 - logprior: -1.2724e+00
Fitted a model with MAP estimate = -274.4227
expansions: [(0, 4)]
discards: [  1   2   3   4   5  18  22  73 100 147]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 283.2520 - loglik: -2.7922e+02 - logprior: -3.9555e+00
Epoch 2/2
19/19 - 6s - loss: 277.4035 - loglik: -2.7588e+02 - logprior: -1.2206e+00
Fitted a model with MAP estimate = -275.0156
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 281.0038 - loglik: -2.7780e+02 - logprior: -3.1251e+00
Epoch 2/10
19/19 - 6s - loss: 275.8909 - loglik: -2.7463e+02 - logprior: -9.7854e-01
Epoch 3/10
19/19 - 6s - loss: 274.4604 - loglik: -2.7320e+02 - logprior: -7.9546e-01
Epoch 4/10
19/19 - 6s - loss: 273.1158 - loglik: -2.7175e+02 - logprior: -7.3191e-01
Epoch 5/10
19/19 - 6s - loss: 271.6069 - loglik: -2.7020e+02 - logprior: -6.8291e-01
Epoch 6/10
19/19 - 6s - loss: 272.3289 - loglik: -2.7095e+02 - logprior: -6.4384e-01
Fitted a model with MAP estimate = -270.6555
Time for alignment: 154.9520
Computed alignments with likelihoods: ['-269.9594', '-270.7828', '-270.6555']
Best model has likelihood: -269.9594
time for generating output: 0.1825
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13522.projection.fasta
SP score = 0.6426938539559083
Training of 3 independent models on file PF00037.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd028d3f8e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd3fa92c700>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd2221ae4f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fca35a06160>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fca5c59d4c0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd255a0a9d0>, <__main__.SimpleDirichletPrior object at 0x7fca440feb20>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 68.9149 - loglik: -6.5526e+01 - logprior: -3.3744e+00
Epoch 2/10
19/19 - 1s - loss: 51.5283 - loglik: -5.0085e+01 - logprior: -1.4350e+00
Epoch 3/10
19/19 - 1s - loss: 46.3101 - loglik: -4.4920e+01 - logprior: -1.3718e+00
Epoch 4/10
19/19 - 1s - loss: 44.7913 - loglik: -4.3527e+01 - logprior: -1.2585e+00
Epoch 5/10
19/19 - 1s - loss: 44.4186 - loglik: -4.3076e+01 - logprior: -1.2237e+00
Epoch 6/10
19/19 - 1s - loss: 44.1226 - loglik: -4.2801e+01 - logprior: -1.2144e+00
Epoch 7/10
19/19 - 1s - loss: 44.0914 - loglik: -4.2760e+01 - logprior: -1.2000e+00
Epoch 8/10
19/19 - 1s - loss: 44.0083 - loglik: -4.2673e+01 - logprior: -1.1930e+00
Epoch 9/10
19/19 - 1s - loss: 43.9162 - loglik: -4.2585e+01 - logprior: -1.1847e+00
Epoch 10/10
19/19 - 1s - loss: 43.9392 - loglik: -4.2594e+01 - logprior: -1.1807e+00
Fitted a model with MAP estimate = -43.7343
expansions: [(0, 2), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 47.4195 - loglik: -4.2680e+01 - logprior: -4.6871e+00
Epoch 2/2
19/19 - 1s - loss: 42.1652 - loglik: -4.0406e+01 - logprior: -1.6798e+00
Fitted a model with MAP estimate = -41.4675
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 43.8977 - loglik: -4.0675e+01 - logprior: -3.1776e+00
Epoch 2/10
19/19 - 1s - loss: 41.7818 - loglik: -4.0237e+01 - logprior: -1.4524e+00
Epoch 3/10
19/19 - 1s - loss: 41.2452 - loglik: -3.9635e+01 - logprior: -1.4571e+00
Epoch 4/10
19/19 - 1s - loss: 40.8655 - loglik: -3.9251e+01 - logprior: -1.4193e+00
Epoch 5/10
19/19 - 1s - loss: 40.6191 - loglik: -3.9028e+01 - logprior: -1.3856e+00
Epoch 6/10
19/19 - 1s - loss: 40.4838 - loglik: -3.8901e+01 - logprior: -1.3721e+00
Epoch 7/10
19/19 - 1s - loss: 40.3736 - loglik: -3.8798e+01 - logprior: -1.3606e+00
Epoch 8/10
19/19 - 1s - loss: 40.3720 - loglik: -3.8788e+01 - logprior: -1.3537e+00
Epoch 9/10
19/19 - 1s - loss: 40.2244 - loglik: -3.8637e+01 - logprior: -1.3455e+00
Epoch 10/10
19/19 - 1s - loss: 40.1746 - loglik: -3.8588e+01 - logprior: -1.3392e+00
Fitted a model with MAP estimate = -39.9091
Time for alignment: 33.2907
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 68.9425 - loglik: -6.5554e+01 - logprior: -3.3745e+00
Epoch 2/10
19/19 - 1s - loss: 51.7416 - loglik: -5.0262e+01 - logprior: -1.4709e+00
Epoch 3/10
19/19 - 1s - loss: 45.8904 - loglik: -4.4261e+01 - logprior: -1.5592e+00
Epoch 4/10
19/19 - 1s - loss: 44.4971 - loglik: -4.2783e+01 - logprior: -1.5570e+00
Epoch 5/10
19/19 - 1s - loss: 44.2471 - loglik: -4.2593e+01 - logprior: -1.5300e+00
Epoch 6/10
19/19 - 1s - loss: 43.9728 - loglik: -4.2295e+01 - logprior: -1.5231e+00
Epoch 7/10
19/19 - 1s - loss: 43.9366 - loglik: -4.2266e+01 - logprior: -1.5122e+00
Epoch 8/10
19/19 - 1s - loss: 43.8362 - loglik: -4.2157e+01 - logprior: -1.5081e+00
Epoch 9/10
19/19 - 1s - loss: 43.8013 - loglik: -4.2123e+01 - logprior: -1.5015e+00
Epoch 10/10
19/19 - 1s - loss: 43.7599 - loglik: -4.2073e+01 - logprior: -1.4982e+00
Fitted a model with MAP estimate = -43.5471
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 47.3038 - loglik: -4.2515e+01 - logprior: -4.7341e+00
Epoch 2/2
19/19 - 1s - loss: 42.0857 - loglik: -4.0622e+01 - logprior: -1.3652e+00
Fitted a model with MAP estimate = -41.1247
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 44.2309 - loglik: -4.0844e+01 - logprior: -3.3397e+00
Epoch 2/2
19/19 - 1s - loss: 41.6437 - loglik: -3.9993e+01 - logprior: -1.5492e+00
Fitted a model with MAP estimate = -41.1656
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 43.8661 - loglik: -4.0527e+01 - logprior: -3.2935e+00
Epoch 2/10
19/19 - 1s - loss: 41.5528 - loglik: -3.9917e+01 - logprior: -1.5322e+00
Epoch 3/10
19/19 - 1s - loss: 41.0856 - loglik: -3.9491e+01 - logprior: -1.4427e+00
Epoch 4/10
19/19 - 1s - loss: 40.7836 - loglik: -3.9196e+01 - logprior: -1.3978e+00
Epoch 5/10
19/19 - 1s - loss: 40.6500 - loglik: -3.9072e+01 - logprior: -1.3721e+00
Epoch 6/10
19/19 - 1s - loss: 40.4037 - loglik: -3.8831e+01 - logprior: -1.3602e+00
Epoch 7/10
19/19 - 1s - loss: 40.3695 - loglik: -3.8807e+01 - logprior: -1.3469e+00
Epoch 8/10
19/19 - 1s - loss: 40.2846 - loglik: -3.8712e+01 - logprior: -1.3407e+00
Epoch 9/10
19/19 - 1s - loss: 40.2136 - loglik: -3.8642e+01 - logprior: -1.3284e+00
Epoch 10/10
19/19 - 1s - loss: 40.1342 - loglik: -3.8564e+01 - logprior: -1.3243e+00
Fitted a model with MAP estimate = -39.8711
Time for alignment: 39.1907
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 68.9384 - loglik: -6.5552e+01 - logprior: -3.3720e+00
Epoch 2/10
19/19 - 1s - loss: 51.7419 - loglik: -5.0286e+01 - logprior: -1.4471e+00
Epoch 3/10
19/19 - 1s - loss: 46.3784 - loglik: -4.4811e+01 - logprior: -1.4594e+00
Epoch 4/10
19/19 - 1s - loss: 44.7571 - loglik: -4.3100e+01 - logprior: -1.5331e+00
Epoch 5/10
19/19 - 1s - loss: 44.2335 - loglik: -4.2591e+01 - logprior: -1.5188e+00
Epoch 6/10
19/19 - 1s - loss: 44.1088 - loglik: -4.2466e+01 - logprior: -1.5062e+00
Epoch 7/10
19/19 - 1s - loss: 43.9460 - loglik: -4.2295e+01 - logprior: -1.5019e+00
Epoch 8/10
19/19 - 1s - loss: 43.9121 - loglik: -4.2249e+01 - logprior: -1.5031e+00
Epoch 9/10
19/19 - 1s - loss: 43.7709 - loglik: -4.2099e+01 - logprior: -1.4995e+00
Epoch 10/10
19/19 - 1s - loss: 43.7738 - loglik: -4.2096e+01 - logprior: -1.4960e+00
Fitted a model with MAP estimate = -43.5465
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 47.3191 - loglik: -4.2499e+01 - logprior: -4.7654e+00
Epoch 2/2
19/19 - 1s - loss: 42.1343 - loglik: -4.0666e+01 - logprior: -1.3755e+00
Fitted a model with MAP estimate = -41.1427
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 44.1934 - loglik: -4.0812e+01 - logprior: -3.3375e+00
Epoch 2/2
19/19 - 1s - loss: 41.6764 - loglik: -4.0026e+01 - logprior: -1.5514e+00
Fitted a model with MAP estimate = -41.1636
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 43.8892 - loglik: -4.0556e+01 - logprior: -3.2870e+00
Epoch 2/10
19/19 - 1s - loss: 41.5899 - loglik: -3.9964e+01 - logprior: -1.5257e+00
Epoch 3/10
19/19 - 1s - loss: 41.0909 - loglik: -3.9500e+01 - logprior: -1.4406e+00
Epoch 4/10
19/19 - 1s - loss: 40.8364 - loglik: -3.9250e+01 - logprior: -1.3958e+00
Epoch 5/10
19/19 - 1s - loss: 40.6153 - loglik: -3.9044e+01 - logprior: -1.3672e+00
Epoch 6/10
19/19 - 1s - loss: 40.4244 - loglik: -3.8855e+01 - logprior: -1.3570e+00
Epoch 7/10
19/19 - 1s - loss: 40.4082 - loglik: -3.8845e+01 - logprior: -1.3449e+00
Epoch 8/10
19/19 - 1s - loss: 40.2877 - loglik: -3.8720e+01 - logprior: -1.3346e+00
Epoch 9/10
19/19 - 1s - loss: 40.2803 - loglik: -3.8718e+01 - logprior: -1.3221e+00
Epoch 10/10
19/19 - 1s - loss: 40.1700 - loglik: -3.8603e+01 - logprior: -1.3184e+00
Fitted a model with MAP estimate = -39.8881
Time for alignment: 37.9497
Computed alignments with likelihoods: ['-39.9091', '-39.8711', '-39.8881']
Best model has likelihood: -39.8711
time for generating output: 0.0880
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00037.projection.fasta
SP score = 0.8980738362760835
Training of 3 independent models on file PF00224.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd255a279d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd1f817d460>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fca3c7433a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fca45875400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd0283e9400>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd21151f8b0>, <__main__.SimpleDirichletPrior object at 0x7fd1e6915e80>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 31s - loss: 732.9218 - loglik: -7.3119e+02 - logprior: -1.7184e+00
Epoch 2/10
39/39 - 28s - loss: 526.8414 - loglik: -5.2457e+02 - logprior: -1.9408e+00
Epoch 3/10
39/39 - 29s - loss: 514.5278 - loglik: -5.1208e+02 - logprior: -1.9453e+00
Epoch 4/10
39/39 - 29s - loss: 512.4368 - loglik: -5.1006e+02 - logprior: -1.8307e+00
Epoch 5/10
39/39 - 29s - loss: 510.7020 - loglik: -5.0834e+02 - logprior: -1.8277e+00
Epoch 6/10
39/39 - 29s - loss: 510.2445 - loglik: -5.0791e+02 - logprior: -1.8300e+00
Epoch 7/10
39/39 - 29s - loss: 509.8995 - loglik: -5.0757e+02 - logprior: -1.8392e+00
Epoch 8/10
39/39 - 29s - loss: 509.6295 - loglik: -5.0732e+02 - logprior: -1.8462e+00
Epoch 9/10
39/39 - 29s - loss: 509.7590 - loglik: -5.0745e+02 - logprior: -1.8539e+00
Fitted a model with MAP estimate = -508.2317
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (38, 1), (40, 1), (44, 1), (45, 1), (46, 2), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 2), (80, 1), (82, 1), (83, 1), (84, 1), (90, 1), (93, 1), (105, 1), (111, 1), (112, 1), (114, 1), (121, 1), (133, 1), (134, 1), (136, 1), (145, 2), (148, 1), (149, 1), (159, 1), (163, 1), (169, 1), (170, 1), (172, 2), (173, 1), (180, 1), (183, 1), (184, 1), (188, 1), (207, 1), (213, 1), (215, 2), (217, 2), (228, 1), (230, 3), (231, 1), (244, 1), (259, 1), (265, 1), (267, 1), (269, 1), (270, 1), (272, 3), (273, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 347 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 45s - loss: 482.9318 - loglik: -4.8042e+02 - logprior: -2.3009e+00
Epoch 2/2
39/39 - 42s - loss: 463.6049 - loglik: -4.6210e+02 - logprior: -9.5612e-01
Fitted a model with MAP estimate = -460.2423
expansions: []
discards: [  0   1  57  98 179 214]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 45s - loss: 470.4580 - loglik: -4.6845e+02 - logprior: -1.8096e+00
Epoch 2/2
39/39 - 43s - loss: 464.0352 - loglik: -4.6322e+02 - logprior: -2.9257e-01
Fitted a model with MAP estimate = -461.1307
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 468.1384 - loglik: -4.6618e+02 - logprior: -1.7609e+00
Epoch 2/10
39/39 - 45s - loss: 462.1169 - loglik: -4.6133e+02 - logprior: -2.8857e-01
Epoch 3/10
39/39 - 44s - loss: 459.6797 - loglik: -4.5880e+02 - logprior: -2.1384e-01
Epoch 4/10
39/39 - 46s - loss: 458.3133 - loglik: -4.5756e+02 - logprior: -4.5440e-02
Epoch 5/10
39/39 - 44s - loss: 457.6436 - loglik: -4.5696e+02 - logprior: -1.9264e-02
Epoch 6/10
39/39 - 45s - loss: 456.6913 - loglik: -4.5630e+02 - logprior: 0.2271
Epoch 7/10
39/39 - 47s - loss: 457.1944 - loglik: -4.5689e+02 - logprior: 0.2875
Fitted a model with MAP estimate = -455.7925
Time for alignment: 967.6291
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 35s - loss: 736.3225 - loglik: -7.3461e+02 - logprior: -1.7010e+00
Epoch 2/10
39/39 - 34s - loss: 529.5944 - loglik: -5.2738e+02 - logprior: -1.8973e+00
Epoch 3/10
39/39 - 33s - loss: 514.8105 - loglik: -5.1232e+02 - logprior: -1.9562e+00
Epoch 4/10
39/39 - 33s - loss: 512.1945 - loglik: -5.0982e+02 - logprior: -1.8626e+00
Epoch 5/10
39/39 - 34s - loss: 511.4117 - loglik: -5.0906e+02 - logprior: -1.8420e+00
Epoch 6/10
39/39 - 34s - loss: 510.7220 - loglik: -5.0840e+02 - logprior: -1.8404e+00
Epoch 7/10
39/39 - 34s - loss: 510.0734 - loglik: -5.0776e+02 - logprior: -1.8480e+00
Epoch 8/10
39/39 - 34s - loss: 510.2494 - loglik: -5.0793e+02 - logprior: -1.8627e+00
Fitted a model with MAP estimate = -508.8891
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (43, 1), (44, 1), (45, 2), (65, 1), (67, 2), (68, 1), (70, 1), (77, 1), (79, 3), (81, 1), (82, 1), (83, 1), (90, 1), (92, 1), (108, 1), (110, 1), (111, 1), (113, 1), (120, 1), (132, 1), (133, 1), (142, 1), (147, 2), (148, 2), (158, 1), (166, 1), (168, 1), (169, 1), (171, 2), (172, 1), (181, 1), (182, 1), (188, 1), (189, 1), (209, 1), (212, 1), (214, 1), (215, 1), (216, 1), (217, 1), (230, 3), (231, 1), (244, 1), (259, 1), (265, 1), (267, 1), (269, 1), (270, 1), (272, 3), (273, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 50s - loss: 483.5365 - loglik: -4.8097e+02 - logprior: -2.3689e+00
Epoch 2/2
39/39 - 46s - loss: 463.8488 - loglik: -4.6232e+02 - logprior: -1.0041e+00
Fitted a model with MAP estimate = -460.0461
expansions: []
discards: [  0   2  57  83 100 186 215]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 473.1966 - loglik: -4.7055e+02 - logprior: -2.4513e+00
Epoch 2/2
39/39 - 47s - loss: 464.7464 - loglik: -4.6381e+02 - logprior: -4.3161e-01
Fitted a model with MAP estimate = -460.9076
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 468.3457 - loglik: -4.6654e+02 - logprior: -1.6060e+00
Epoch 2/10
39/39 - 50s - loss: 462.1363 - loglik: -4.6134e+02 - logprior: -2.9311e-01
Epoch 3/10
39/39 - 50s - loss: 459.7712 - loglik: -4.5878e+02 - logprior: -3.3673e-01
Epoch 4/10
39/39 - 46s - loss: 458.1322 - loglik: -4.5742e+02 - logprior: -1.1051e-03
Epoch 5/10
39/39 - 46s - loss: 457.8526 - loglik: -4.5701e+02 - logprior: -1.7200e-01
Epoch 6/10
39/39 - 46s - loss: 457.0641 - loglik: -4.5673e+02 - logprior: 0.2918
Epoch 7/10
39/39 - 47s - loss: 456.8701 - loglik: -4.5643e+02 - logprior: 0.1457
Epoch 8/10
39/39 - 48s - loss: 456.2979 - loglik: -4.5632e+02 - logprior: 0.5953
Epoch 9/10
39/39 - 46s - loss: 455.8583 - loglik: -4.5576e+02 - logprior: 0.4602
Epoch 10/10
39/39 - 48s - loss: 455.8598 - loglik: -4.5620e+02 - logprior: 0.8858
Fitted a model with MAP estimate = -454.9837
Time for alignment: 1181.5597
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 735.8499 - loglik: -7.3410e+02 - logprior: -1.7280e+00
Epoch 2/10
39/39 - 34s - loss: 529.5455 - loglik: -5.2729e+02 - logprior: -1.9036e+00
Epoch 3/10
39/39 - 30s - loss: 517.0598 - loglik: -5.1473e+02 - logprior: -1.8572e+00
Epoch 4/10
39/39 - 29s - loss: 514.2005 - loglik: -5.1195e+02 - logprior: -1.7723e+00
Epoch 5/10
39/39 - 29s - loss: 513.3123 - loglik: -5.1108e+02 - logprior: -1.7479e+00
Epoch 6/10
39/39 - 29s - loss: 512.3525 - loglik: -5.1011e+02 - logprior: -1.7573e+00
Epoch 7/10
39/39 - 29s - loss: 512.1184 - loglik: -5.0986e+02 - logprior: -1.7784e+00
Epoch 8/10
39/39 - 29s - loss: 511.5319 - loglik: -5.0928e+02 - logprior: -1.7877e+00
Epoch 9/10
39/39 - 29s - loss: 512.0486 - loglik: -5.0980e+02 - logprior: -1.7906e+00
Fitted a model with MAP estimate = -510.3412
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (18, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 2), (80, 1), (82, 3), (83, 1), (90, 1), (92, 1), (108, 1), (110, 1), (111, 1), (113, 1), (120, 1), (132, 1), (133, 1), (142, 1), (147, 2), (148, 2), (163, 1), (166, 1), (168, 1), (169, 1), (171, 2), (172, 1), (179, 1), (182, 1), (183, 1), (189, 1), (206, 1), (209, 1), (214, 2), (216, 2), (230, 3), (231, 1), (244, 1), (257, 1), (258, 1), (264, 1), (267, 1), (269, 2), (271, 1), (272, 2), (273, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 482.0850 - loglik: -4.7966e+02 - logprior: -2.2217e+00
Epoch 2/2
39/39 - 48s - loss: 463.7406 - loglik: -4.6215e+02 - logprior: -1.0426e+00
Fitted a model with MAP estimate = -459.9183
expansions: []
discards: [  0  97 184 213]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 471.2575 - loglik: -4.6833e+02 - logprior: -2.7283e+00
Epoch 2/2
39/39 - 46s - loss: 463.9448 - loglik: -4.6277e+02 - logprior: -6.6473e-01
Fitted a model with MAP estimate = -460.4137
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 53s - loss: 467.7338 - loglik: -4.6601e+02 - logprior: -1.5298e+00
Epoch 2/10
39/39 - 52s - loss: 461.9893 - loglik: -4.6112e+02 - logprior: -3.5234e-01
Epoch 3/10
39/39 - 49s - loss: 459.6121 - loglik: -4.5889e+02 - logprior: -4.9696e-02
Epoch 4/10
39/39 - 50s - loss: 458.5324 - loglik: -4.5773e+02 - logprior: -9.5140e-02
Epoch 5/10
39/39 - 52s - loss: 457.3592 - loglik: -4.5686e+02 - logprior: 0.1759
Epoch 6/10
39/39 - 52s - loss: 456.8402 - loglik: -4.5641e+02 - logprior: 0.1899
Epoch 7/10
39/39 - 49s - loss: 457.0345 - loglik: -4.5688e+02 - logprior: 0.4305
Fitted a model with MAP estimate = -455.4361
Time for alignment: 1046.8988
Computed alignments with likelihoods: ['-455.7925', '-454.9837', '-455.4361']
Best model has likelihood: -454.9837
time for generating output: 0.2979
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00224.projection.fasta
SP score = 0.9197924980047885
Training of 3 independent models on file PF13365.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd028b4fc70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fca5c2e0460>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd028254be0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd233aca850>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd3e9cef280>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd3e9cefb50>, <__main__.SimpleDirichletPrior object at 0x7fd24ce72880>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 397.6485 - loglik: -3.9454e+02 - logprior: -3.0929e+00
Epoch 2/10
19/19 - 6s - loss: 327.5604 - loglik: -3.2612e+02 - logprior: -1.3213e+00
Epoch 3/10
19/19 - 6s - loss: 302.9604 - loglik: -3.0050e+02 - logprior: -1.7219e+00
Epoch 4/10
19/19 - 6s - loss: 297.1615 - loglik: -2.9445e+02 - logprior: -1.5600e+00
Epoch 5/10
19/19 - 6s - loss: 296.3275 - loglik: -2.9378e+02 - logprior: -1.5236e+00
Epoch 6/10
19/19 - 6s - loss: 294.0659 - loglik: -2.9167e+02 - logprior: -1.5026e+00
Epoch 7/10
19/19 - 6s - loss: 293.0541 - loglik: -2.9074e+02 - logprior: -1.5142e+00
Epoch 8/10
19/19 - 6s - loss: 293.0258 - loglik: -2.9081e+02 - logprior: -1.5033e+00
Epoch 9/10
19/19 - 6s - loss: 291.4227 - loglik: -2.8929e+02 - logprior: -1.4990e+00
Epoch 10/10
19/19 - 6s - loss: 293.5338 - loglik: -2.9142e+02 - logprior: -1.5019e+00
Fitted a model with MAP estimate = -290.5267
expansions: [(7, 2), (22, 1), (23, 1), (24, 1), (25, 2), (29, 2), (35, 1), (37, 2), (45, 2), (46, 3), (49, 2), (50, 2), (55, 1), (70, 1), (71, 2), (84, 4), (85, 2), (86, 3), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 302.9713 - loglik: -2.9942e+02 - logprior: -3.3368e+00
Epoch 2/2
39/39 - 10s - loss: 290.1520 - loglik: -2.8782e+02 - logprior: -1.7271e+00
Fitted a model with MAP estimate = -284.5388
expansions: []
discards: [ 29  35  47  58  59  60  66  68  93 140]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 294.0967 - loglik: -2.9151e+02 - logprior: -2.4109e+00
Epoch 2/2
39/39 - 9s - loss: 288.8967 - loglik: -2.8706e+02 - logprior: -1.3701e+00
Fitted a model with MAP estimate = -284.6718
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 292.7059 - loglik: -2.9023e+02 - logprior: -2.3175e+00
Epoch 2/10
39/39 - 9s - loss: 287.6089 - loglik: -2.8589e+02 - logprior: -1.2699e+00
Epoch 3/10
39/39 - 9s - loss: 284.3857 - loglik: -2.8244e+02 - logprior: -1.1205e+00
Epoch 4/10
39/39 - 9s - loss: 282.3299 - loglik: -2.8035e+02 - logprior: -9.7255e-01
Epoch 5/10
39/39 - 9s - loss: 281.1974 - loglik: -2.7941e+02 - logprior: -9.0011e-01
Epoch 6/10
39/39 - 9s - loss: 280.3275 - loglik: -2.7873e+02 - logprior: -8.2372e-01
Epoch 7/10
39/39 - 10s - loss: 279.9402 - loglik: -2.7846e+02 - logprior: -7.6285e-01
Epoch 8/10
39/39 - 9s - loss: 279.3605 - loglik: -2.7796e+02 - logprior: -7.1449e-01
Epoch 9/10
39/39 - 10s - loss: 280.0027 - loglik: -2.7867e+02 - logprior: -6.5602e-01
Fitted a model with MAP estimate = -278.5115
Time for alignment: 244.0822
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 397.8827 - loglik: -3.9478e+02 - logprior: -3.0899e+00
Epoch 2/10
19/19 - 6s - loss: 329.1311 - loglik: -3.2769e+02 - logprior: -1.3215e+00
Epoch 3/10
19/19 - 6s - loss: 301.5876 - loglik: -2.9935e+02 - logprior: -1.7781e+00
Epoch 4/10
19/19 - 6s - loss: 296.6006 - loglik: -2.9414e+02 - logprior: -1.6359e+00
Epoch 5/10
19/19 - 6s - loss: 294.1419 - loglik: -2.9168e+02 - logprior: -1.5721e+00
Epoch 6/10
19/19 - 6s - loss: 293.3905 - loglik: -2.9103e+02 - logprior: -1.5250e+00
Epoch 7/10
19/19 - 6s - loss: 291.6229 - loglik: -2.8935e+02 - logprior: -1.5046e+00
Epoch 8/10
19/19 - 6s - loss: 292.9684 - loglik: -2.9075e+02 - logprior: -1.5016e+00
Fitted a model with MAP estimate = -290.0957
expansions: [(7, 2), (22, 1), (23, 1), (24, 1), (25, 2), (28, 1), (29, 2), (37, 2), (45, 2), (46, 3), (49, 2), (50, 2), (55, 1), (60, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 147 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 302.5997 - loglik: -2.9903e+02 - logprior: -3.3674e+00
Epoch 2/2
39/39 - 10s - loss: 290.0839 - loglik: -2.8774e+02 - logprior: -1.8093e+00
Fitted a model with MAP estimate = -284.7594
expansions: []
discards: [ 29  36  47  58  60  66  68 107 139]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 293.9947 - loglik: -2.9133e+02 - logprior: -2.4958e+00
Epoch 2/2
39/39 - 10s - loss: 288.9999 - loglik: -2.8711e+02 - logprior: -1.4228e+00
Fitted a model with MAP estimate = -284.6553
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 292.4832 - loglik: -2.8996e+02 - logprior: -2.3699e+00
Epoch 2/10
39/39 - 10s - loss: 287.8349 - loglik: -2.8607e+02 - logprior: -1.3162e+00
Epoch 3/10
39/39 - 9s - loss: 284.5511 - loglik: -2.8256e+02 - logprior: -1.1450e+00
Epoch 4/10
39/39 - 10s - loss: 282.4251 - loglik: -2.8042e+02 - logprior: -9.9398e-01
Epoch 5/10
39/39 - 9s - loss: 280.5171 - loglik: -2.7871e+02 - logprior: -9.0340e-01
Epoch 6/10
39/39 - 10s - loss: 280.3350 - loglik: -2.7870e+02 - logprior: -8.4672e-01
Epoch 7/10
39/39 - 9s - loss: 280.3155 - loglik: -2.7880e+02 - logprior: -7.8425e-01
Epoch 8/10
39/39 - 9s - loss: 279.3301 - loglik: -2.7790e+02 - logprior: -7.2744e-01
Epoch 9/10
39/39 - 9s - loss: 279.0927 - loglik: -2.7774e+02 - logprior: -6.7057e-01
Epoch 10/10
39/39 - 10s - loss: 279.8451 - loglik: -2.7857e+02 - logprior: -6.1212e-01
Fitted a model with MAP estimate = -278.2565
Time for alignment: 242.1051
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 397.6841 - loglik: -3.9458e+02 - logprior: -3.0953e+00
Epoch 2/10
19/19 - 6s - loss: 326.5098 - loglik: -3.2508e+02 - logprior: -1.3247e+00
Epoch 3/10
19/19 - 6s - loss: 302.0862 - loglik: -2.9987e+02 - logprior: -1.7679e+00
Epoch 4/10
19/19 - 6s - loss: 296.4260 - loglik: -2.9402e+02 - logprior: -1.6185e+00
Epoch 5/10
19/19 - 6s - loss: 294.5717 - loglik: -2.9212e+02 - logprior: -1.5566e+00
Epoch 6/10
19/19 - 6s - loss: 292.9210 - loglik: -2.9057e+02 - logprior: -1.5084e+00
Epoch 7/10
19/19 - 6s - loss: 294.0284 - loglik: -2.9177e+02 - logprior: -1.4971e+00
Fitted a model with MAP estimate = -290.7865
expansions: [(7, 2), (21, 3), (22, 1), (25, 1), (28, 1), (29, 2), (34, 1), (36, 2), (46, 3), (49, 2), (50, 2), (55, 1), (56, 1), (84, 2), (85, 3), (86, 6), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 147 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 302.3087 - loglik: -2.9877e+02 - logprior: -3.3474e+00
Epoch 2/2
39/39 - 10s - loss: 289.9091 - loglik: -2.8761e+02 - logprior: -1.8003e+00
Fitted a model with MAP estimate = -284.7517
expansions: []
discards: [ 23  36  47  59  65  67 106 139]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 293.9362 - loglik: -2.9127e+02 - logprior: -2.5025e+00
Epoch 2/2
39/39 - 9s - loss: 288.7674 - loglik: -2.8688e+02 - logprior: -1.4228e+00
Fitted a model with MAP estimate = -284.5499
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 292.3938 - loglik: -2.8985e+02 - logprior: -2.3895e+00
Epoch 2/10
39/39 - 10s - loss: 287.9185 - loglik: -2.8613e+02 - logprior: -1.3407e+00
Epoch 3/10
39/39 - 9s - loss: 284.2269 - loglik: -2.8224e+02 - logprior: -1.1561e+00
Epoch 4/10
39/39 - 10s - loss: 281.9488 - loglik: -2.7995e+02 - logprior: -9.8993e-01
Epoch 5/10
39/39 - 10s - loss: 280.7839 - loglik: -2.7897e+02 - logprior: -9.1405e-01
Epoch 6/10
39/39 - 10s - loss: 280.6414 - loglik: -2.7901e+02 - logprior: -8.5420e-01
Epoch 7/10
39/39 - 10s - loss: 279.8103 - loglik: -2.7830e+02 - logprior: -7.8706e-01
Epoch 8/10
39/39 - 10s - loss: 279.0462 - loglik: -2.7761e+02 - logprior: -7.3180e-01
Epoch 9/10
39/39 - 10s - loss: 279.4889 - loglik: -2.7813e+02 - logprior: -6.8467e-01
Fitted a model with MAP estimate = -278.2960
Time for alignment: 225.7216
Computed alignments with likelihoods: ['-278.5115', '-278.2565', '-278.2960']
Best model has likelihood: -278.2565
time for generating output: 0.2607
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13365.projection.fasta
SP score = 0.40947079266036873
Training of 3 independent models on file PF00078.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd1e6c06a90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fca29134fa0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd3cfcc1ac0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd3f20d9df0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd3f20d9ca0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fca296aea30>, <__main__.SimpleDirichletPrior object at 0x7fca204f3280>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 468.8943 - loglik: -4.6644e+02 - logprior: -2.0135e+00
Epoch 2/10
39/39 - 11s - loss: 426.1237 - loglik: -4.2292e+02 - logprior: -1.4712e+00
Epoch 3/10
39/39 - 11s - loss: 419.0980 - loglik: -4.1565e+02 - logprior: -1.5178e+00
Epoch 4/10
39/39 - 11s - loss: 415.2502 - loglik: -4.1220e+02 - logprior: -1.5304e+00
Epoch 5/10
39/39 - 11s - loss: 413.8112 - loglik: -4.1096e+02 - logprior: -1.5318e+00
Epoch 6/10
39/39 - 11s - loss: 412.6470 - loglik: -4.1004e+02 - logprior: -1.5325e+00
Epoch 7/10
39/39 - 11s - loss: 412.4113 - loglik: -4.0996e+02 - logprior: -1.5350e+00
Epoch 8/10
39/39 - 10s - loss: 411.8497 - loglik: -4.0950e+02 - logprior: -1.5344e+00
Epoch 9/10
39/39 - 11s - loss: 411.7838 - loglik: -4.0949e+02 - logprior: -1.5255e+00
Epoch 10/10
39/39 - 11s - loss: 411.5195 - loglik: -4.0928e+02 - logprior: -1.5308e+00
Fitted a model with MAP estimate = -410.3666
expansions: [(30, 2), (31, 1), (45, 1), (47, 1), (49, 1), (58, 1), (59, 5), (74, 1), (76, 1), (79, 1), (80, 9), (107, 1), (108, 3), (110, 1), (112, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 158 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 431.0168 - loglik: -4.2841e+02 - logprior: -2.3326e+00
Epoch 2/2
39/39 - 12s - loss: 410.0131 - loglik: -4.0784e+02 - logprior: -1.2522e+00
Fitted a model with MAP estimate = -405.1601
expansions: []
discards: [ 67  68  97  98  99 100 101 102]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 417.3208 - loglik: -4.1504e+02 - logprior: -2.0497e+00
Epoch 2/2
39/39 - 12s - loss: 409.8493 - loglik: -4.0761e+02 - logprior: -1.0490e+00
Fitted a model with MAP estimate = -405.0256
expansions: [(95, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 416.3354 - loglik: -4.1418e+02 - logprior: -1.9521e+00
Epoch 2/10
39/39 - 12s - loss: 410.1751 - loglik: -4.0858e+02 - logprior: -8.9513e-01
Epoch 3/10
39/39 - 12s - loss: 406.0893 - loglik: -4.0419e+02 - logprior: -9.0777e-01
Epoch 4/10
39/39 - 12s - loss: 403.2819 - loglik: -4.0108e+02 - logprior: -8.5628e-01
Epoch 5/10
39/39 - 12s - loss: 401.6859 - loglik: -3.9948e+02 - logprior: -8.1619e-01
Epoch 6/10
39/39 - 12s - loss: 400.9038 - loglik: -3.9887e+02 - logprior: -7.7889e-01
Epoch 7/10
39/39 - 12s - loss: 399.8899 - loglik: -3.9802e+02 - logprior: -7.3041e-01
Epoch 8/10
39/39 - 12s - loss: 400.3041 - loglik: -3.9861e+02 - logprior: -6.8911e-01
Fitted a model with MAP estimate = -398.4250
Time for alignment: 320.1694
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 468.5389 - loglik: -4.6607e+02 - logprior: -2.0091e+00
Epoch 2/10
39/39 - 11s - loss: 429.0898 - loglik: -4.2648e+02 - logprior: -1.3948e+00
Epoch 3/10
39/39 - 11s - loss: 422.0033 - loglik: -4.1880e+02 - logprior: -1.4905e+00
Epoch 4/10
39/39 - 11s - loss: 419.5111 - loglik: -4.1653e+02 - logprior: -1.4708e+00
Epoch 5/10
39/39 - 11s - loss: 418.8367 - loglik: -4.1610e+02 - logprior: -1.4715e+00
Epoch 6/10
39/39 - 11s - loss: 417.3025 - loglik: -4.1479e+02 - logprior: -1.4677e+00
Epoch 7/10
39/39 - 10s - loss: 417.8372 - loglik: -4.1543e+02 - logprior: -1.4686e+00
Fitted a model with MAP estimate = -415.8298
expansions: [(19, 2), (33, 6), (34, 3), (35, 1), (46, 1), (47, 3), (48, 2), (57, 11), (72, 1), (74, 2), (75, 3), (76, 1), (78, 5), (107, 3), (112, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 428.7304 - loglik: -4.2618e+02 - logprior: -2.2897e+00
Epoch 2/2
39/39 - 14s - loss: 409.3412 - loglik: -4.0696e+02 - logprior: -1.3130e+00
Fitted a model with MAP estimate = -403.1577
expansions: [(24, 1), (104, 1)]
discards: [ 20  21  37  39  60  76  77  78  79  80  81  82 101 113 114 115 116 117]
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 419.2996 - loglik: -4.1703e+02 - logprior: -2.0285e+00
Epoch 2/2
39/39 - 12s - loss: 410.8747 - loglik: -4.0865e+02 - logprior: -1.0184e+00
Fitted a model with MAP estimate = -405.3827
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 417.9337 - loglik: -4.1587e+02 - logprior: -1.8580e+00
Epoch 2/10
39/39 - 12s - loss: 409.9880 - loglik: -4.0808e+02 - logprior: -8.2210e-01
Epoch 3/10
39/39 - 12s - loss: 405.2745 - loglik: -4.0253e+02 - logprior: -8.0329e-01
Epoch 4/10
39/39 - 12s - loss: 403.3387 - loglik: -4.0071e+02 - logprior: -7.7390e-01
Epoch 5/10
39/39 - 12s - loss: 401.1277 - loglik: -3.9885e+02 - logprior: -7.3497e-01
Epoch 6/10
39/39 - 12s - loss: 401.0150 - loglik: -3.9902e+02 - logprior: -6.9022e-01
Epoch 7/10
39/39 - 12s - loss: 400.3746 - loglik: -3.9859e+02 - logprior: -6.5634e-01
Epoch 8/10
39/39 - 12s - loss: 399.5824 - loglik: -3.9797e+02 - logprior: -6.1122e-01
Epoch 9/10
39/39 - 12s - loss: 398.6136 - loglik: -3.9715e+02 - logprior: -5.6727e-01
Epoch 10/10
39/39 - 12s - loss: 399.3281 - loglik: -3.9797e+02 - logprior: -5.1559e-01
Fitted a model with MAP estimate = -397.7396
Time for alignment: 317.4973
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 468.4624 - loglik: -4.6600e+02 - logprior: -2.0155e+00
Epoch 2/10
39/39 - 10s - loss: 427.5556 - loglik: -4.2459e+02 - logprior: -1.4252e+00
Epoch 3/10
39/39 - 10s - loss: 420.4051 - loglik: -4.1687e+02 - logprior: -1.5470e+00
Epoch 4/10
39/39 - 10s - loss: 417.1675 - loglik: -4.1410e+02 - logprior: -1.5681e+00
Epoch 5/10
39/39 - 10s - loss: 416.0541 - loglik: -4.1322e+02 - logprior: -1.5903e+00
Epoch 6/10
39/39 - 11s - loss: 414.5347 - loglik: -4.1189e+02 - logprior: -1.6196e+00
Epoch 7/10
39/39 - 11s - loss: 414.1301 - loglik: -4.1161e+02 - logprior: -1.6328e+00
Epoch 8/10
39/39 - 10s - loss: 413.7724 - loglik: -4.1134e+02 - logprior: -1.6397e+00
Epoch 9/10
39/39 - 11s - loss: 413.5620 - loglik: -4.1117e+02 - logprior: -1.6538e+00
Epoch 10/10
39/39 - 11s - loss: 413.2974 - loglik: -4.1097e+02 - logprior: -1.6499e+00
Fitted a model with MAP estimate = -412.2691
expansions: [(19, 2), (20, 1), (31, 1), (32, 2), (33, 3), (48, 1), (49, 1), (50, 2), (54, 1), (55, 2), (56, 1), (58, 1), (74, 2), (76, 1), (79, 1), (80, 9), (94, 1), (98, 1), (107, 4), (108, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 165 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 430.4531 - loglik: -4.2786e+02 - logprior: -2.3184e+00
Epoch 2/2
39/39 - 12s - loss: 411.0703 - loglik: -4.0872e+02 - logprior: -1.3168e+00
Fitted a model with MAP estimate = -405.4344
expansions: [(71, 13)]
discards: [ 35  57  62  63  64  65  72  73  93 103 104 105 106 107 108]
Re-initialized the encoder parameters.
Fitting a model of length 163 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 420.8255 - loglik: -4.1852e+02 - logprior: -2.0745e+00
Epoch 2/2
39/39 - 12s - loss: 409.2611 - loglik: -4.0692e+02 - logprior: -1.1618e+00
Fitted a model with MAP estimate = -403.6053
expansions: []
discards: [69 70 71 72 73 74 76 77 78]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 418.9354 - loglik: -4.1685e+02 - logprior: -1.8762e+00
Epoch 2/10
39/39 - 12s - loss: 410.8004 - loglik: -4.0885e+02 - logprior: -8.5506e-01
Epoch 3/10
39/39 - 11s - loss: 406.2530 - loglik: -4.0361e+02 - logprior: -7.8475e-01
Epoch 4/10
39/39 - 11s - loss: 403.7993 - loglik: -4.0125e+02 - logprior: -7.6088e-01
Epoch 5/10
39/39 - 11s - loss: 402.6015 - loglik: -4.0042e+02 - logprior: -7.1039e-01
Epoch 6/10
39/39 - 11s - loss: 402.0143 - loglik: -4.0011e+02 - logprior: -6.5838e-01
Epoch 7/10
39/39 - 11s - loss: 400.9853 - loglik: -3.9929e+02 - logprior: -6.1952e-01
Epoch 8/10
39/39 - 11s - loss: 400.7290 - loglik: -3.9919e+02 - logprior: -5.8865e-01
Epoch 9/10
39/39 - 11s - loss: 400.3712 - loglik: -3.9896e+02 - logprior: -5.3344e-01
Epoch 10/10
39/39 - 12s - loss: 400.0810 - loglik: -3.9879e+02 - logprior: -4.9223e-01
Fitted a model with MAP estimate = -398.9588
Time for alignment: 340.6444
Computed alignments with likelihoods: ['-398.4250', '-397.7396', '-398.9588']
Best model has likelihood: -397.7396
time for generating output: 0.2027
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00078.projection.fasta
SP score = 0.8923874053407732
Training of 3 independent models on file PF00150.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fca3d1419a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fca3cf90850>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1f84cf7c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fca3d08dd00>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fca29a29fd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fca3d081f70>, <__main__.SimpleDirichletPrior object at 0x7fd030773910>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 792.0092 - loglik: -7.8997e+02 - logprior: -1.5480e+00
Epoch 2/10
39/39 - 24s - loss: 720.7904 - loglik: -7.1835e+02 - logprior: -9.4603e-01
Epoch 3/10
39/39 - 24s - loss: 707.3297 - loglik: -7.0384e+02 - logprior: -1.2216e+00
Epoch 4/10
39/39 - 24s - loss: 702.0132 - loglik: -6.9814e+02 - logprior: -1.2175e+00
Epoch 5/10
39/39 - 24s - loss: 698.0499 - loglik: -6.9405e+02 - logprior: -1.2729e+00
Epoch 6/10
39/39 - 25s - loss: 695.5797 - loglik: -6.9174e+02 - logprior: -1.3265e+00
Epoch 7/10
39/39 - 25s - loss: 692.8054 - loglik: -6.8917e+02 - logprior: -1.3971e+00
Epoch 8/10
39/39 - 24s - loss: 691.1760 - loglik: -6.8770e+02 - logprior: -1.4412e+00
Epoch 9/10
39/39 - 25s - loss: 689.9354 - loglik: -6.8644e+02 - logprior: -1.4592e+00
Epoch 10/10
39/39 - 25s - loss: 688.1717 - loglik: -6.8459e+02 - logprior: -1.4696e+00
Fitted a model with MAP estimate = -684.4183
expansions: [(0, 3), (9, 1), (10, 1), (23, 3), (43, 2), (51, 2), (52, 1), (74, 1), (85, 1), (87, 18), (101, 1), (114, 3), (125, 1), (159, 2), (174, 4), (180, 2), (181, 1), (202, 1), (206, 6), (208, 1), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 285 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 762.9472 - loglik: -7.5938e+02 - logprior: -3.2856e+00
Epoch 2/2
39/39 - 33s - loss: 704.9152 - loglik: -7.0246e+02 - logprior: -1.5496e+00
Fitted a model with MAP estimate = -695.0981
expansions: [(51, 1), (149, 2), (150, 2), (185, 1), (198, 2), (217, 4), (257, 2)]
discards: [  2   3   4  15  16  30  52  53  62  92  93  94  95  96  97  98 100 110
 111 151 152 153 191 192 193 194 195 196 218 219 227 228 229 230 231 232
 233 234 248 259 260]
Re-initialized the encoder parameters.
Fitting a model of length 258 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 714.4459 - loglik: -7.1248e+02 - logprior: -1.7085e+00
Epoch 2/2
39/39 - 29s - loss: 700.5842 - loglik: -6.9855e+02 - logprior: -7.6296e-01
Fitted a model with MAP estimate = -693.4939
expansions: [(0, 3), (89, 2), (172, 1), (176, 4), (179, 2), (209, 1)]
discards: [ 93 134]
Re-initialized the encoder parameters.
Fitting a model of length 269 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 709.8693 - loglik: -7.0693e+02 - logprior: -2.6918e+00
Epoch 2/10
39/39 - 34s - loss: 697.6150 - loglik: -6.9569e+02 - logprior: -7.3007e-01
Epoch 3/10
39/39 - 35s - loss: 691.4061 - loglik: -6.8850e+02 - logprior: -6.4822e-01
Epoch 4/10
39/39 - 36s - loss: 687.9778 - loglik: -6.8469e+02 - logprior: -6.1341e-01
Epoch 5/10
39/39 - 36s - loss: 684.0242 - loglik: -6.8067e+02 - logprior: -5.6679e-01
Epoch 6/10
39/39 - 36s - loss: 681.2711 - loglik: -6.7802e+02 - logprior: -5.1349e-01
Epoch 7/10
39/39 - 37s - loss: 678.9273 - loglik: -6.7578e+02 - logprior: -6.7776e-01
Epoch 8/10
39/39 - 34s - loss: 677.2440 - loglik: -6.7464e+02 - logprior: -4.1417e-01
Epoch 9/10
39/39 - 33s - loss: 675.8730 - loglik: -6.7322e+02 - logprior: -5.3085e-01
Epoch 10/10
39/39 - 34s - loss: 673.3394 - loglik: -6.7068e+02 - logprior: -2.4002e-01
Fitted a model with MAP estimate = -669.1038
Time for alignment: 862.0594
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 32s - loss: 792.5173 - loglik: -7.9047e+02 - logprior: -1.5556e+00
Epoch 2/10
39/39 - 26s - loss: 720.0651 - loglik: -7.1721e+02 - logprior: -8.7520e-01
Epoch 3/10
39/39 - 26s - loss: 707.9543 - loglik: -7.0400e+02 - logprior: -1.0941e+00
Epoch 4/10
39/39 - 26s - loss: 701.8203 - loglik: -6.9772e+02 - logprior: -1.1752e+00
Epoch 5/10
39/39 - 27s - loss: 697.1249 - loglik: -6.9303e+02 - logprior: -1.2378e+00
Epoch 6/10
39/39 - 28s - loss: 694.4250 - loglik: -6.9060e+02 - logprior: -1.2811e+00
Epoch 7/10
39/39 - 27s - loss: 692.3597 - loglik: -6.8884e+02 - logprior: -1.3274e+00
Epoch 8/10
39/39 - 26s - loss: 690.8542 - loglik: -6.8752e+02 - logprior: -1.3467e+00
Epoch 9/10
39/39 - 26s - loss: 689.5237 - loglik: -6.8611e+02 - logprior: -1.4113e+00
Epoch 10/10
39/39 - 27s - loss: 687.7822 - loglik: -6.8421e+02 - logprior: -1.4253e+00
Fitted a model with MAP estimate = -684.1271
expansions: [(0, 4), (40, 1), (42, 2), (43, 1), (51, 1), (52, 3), (84, 1), (85, 1), (86, 1), (87, 14), (95, 1), (102, 1), (115, 4), (116, 2), (117, 1), (120, 1), (154, 1), (157, 1), (172, 2), (173, 4), (176, 2), (179, 1), (205, 1), (207, 7), (208, 1), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 44s - loss: 762.8074 - loglik: -7.5938e+02 - logprior: -3.1357e+00
Epoch 2/2
39/39 - 42s - loss: 705.7119 - loglik: -7.0340e+02 - logprior: -1.3917e+00
Fitted a model with MAP estimate = -695.4545
expansions: [(0, 6), (92, 1), (100, 2), (192, 1), (219, 1), (220, 1), (221, 1)]
discards: [  0   1   2  45  47  51  61  97 116 117 118 119 120 121 152 155 156 194
 198 210 211 212 225 226 227 228 259 260]
Re-initialized the encoder parameters.
Fitting a model of length 274 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 41s - loss: 711.7212 - loglik: -7.0948e+02 - logprior: -1.9802e+00
Epoch 2/2
39/39 - 39s - loss: 697.9810 - loglik: -6.9598e+02 - logprior: -7.1856e-01
Fitted a model with MAP estimate = -690.6291
expansions: [(0, 4), (7, 1), (49, 2), (204, 1), (206, 3)]
discards: [  0   1   2   3   4   5  10  11  12  13  14  15  16  18  19 101 246]
Re-initialized the encoder parameters.
Fitting a model of length 268 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 709.6777 - loglik: -7.0785e+02 - logprior: -1.5877e+00
Epoch 2/10
39/39 - 38s - loss: 698.0141 - loglik: -6.9637e+02 - logprior: -4.6619e-01
Epoch 3/10
39/39 - 37s - loss: 692.3952 - loglik: -6.8972e+02 - logprior: -4.4943e-01
Epoch 4/10
39/39 - 35s - loss: 688.2648 - loglik: -6.8523e+02 - logprior: -3.9715e-01
Epoch 5/10
39/39 - 34s - loss: 684.9865 - loglik: -6.8188e+02 - logprior: -3.3302e-01
Epoch 6/10
39/39 - 35s - loss: 681.9718 - loglik: -6.7895e+02 - logprior: -2.8470e-01
Epoch 7/10
39/39 - 36s - loss: 679.0560 - loglik: -6.7630e+02 - logprior: -2.1635e-01
Epoch 8/10
39/39 - 33s - loss: 677.4510 - loglik: -6.7501e+02 - logprior: -1.4755e-01
Epoch 9/10
39/39 - 33s - loss: 675.5052 - loglik: -6.7312e+02 - logprior: -7.4895e-02
Epoch 10/10
39/39 - 34s - loss: 672.8249 - loglik: -6.7032e+02 - logprior: -5.9661e-03
Fitted a model with MAP estimate = -668.9827
Time for alignment: 962.1870
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 30s - loss: 792.3782 - loglik: -7.9033e+02 - logprior: -1.5566e+00
Epoch 2/10
39/39 - 27s - loss: 722.3483 - loglik: -7.1995e+02 - logprior: -9.0979e-01
Epoch 3/10
39/39 - 26s - loss: 708.7164 - loglik: -7.0541e+02 - logprior: -1.2278e+00
Epoch 4/10
39/39 - 26s - loss: 703.3654 - loglik: -6.9945e+02 - logprior: -1.2598e+00
Epoch 5/10
39/39 - 27s - loss: 699.3457 - loglik: -6.9525e+02 - logprior: -1.3126e+00
Epoch 6/10
39/39 - 28s - loss: 696.3806 - loglik: -6.9245e+02 - logprior: -1.3638e+00
Epoch 7/10
39/39 - 29s - loss: 694.0850 - loglik: -6.9043e+02 - logprior: -1.4123e+00
Epoch 8/10
39/39 - 29s - loss: 692.2659 - loglik: -6.8885e+02 - logprior: -1.4364e+00
Epoch 9/10
39/39 - 30s - loss: 691.1700 - loglik: -6.8786e+02 - logprior: -1.4446e+00
Epoch 10/10
39/39 - 30s - loss: 689.8539 - loglik: -6.8647e+02 - logprior: -1.4628e+00
Fitted a model with MAP estimate = -686.0804
expansions: [(0, 3), (9, 1), (10, 1), (23, 3), (42, 1), (43, 1), (50, 1), (52, 2), (57, 1), (84, 15), (87, 1), (103, 1), (114, 4), (115, 2), (116, 1), (120, 1), (148, 1), (152, 1), (171, 3), (174, 2), (206, 7), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 283 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 44s - loss: 761.4714 - loglik: -7.5792e+02 - logprior: -3.2829e+00
Epoch 2/2
39/39 - 40s - loss: 705.6201 - loglik: -7.0327e+02 - logprior: -1.4731e+00
Fitted a model with MAP estimate = -696.0715
expansions: [(51, 1), (60, 1), (158, 1), (214, 1), (218, 3), (249, 1)]
discards: [  2   4  15  16  30  52  53  61 113 114 154 155 156 215 244 245]
Re-initialized the encoder parameters.
Fitting a model of length 275 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 43s - loss: 710.9789 - loglik: -7.0895e+02 - logprior: -1.7679e+00
Epoch 2/2
39/39 - 38s - loss: 698.1662 - loglik: -6.9615e+02 - logprior: -7.3360e-01
Fitted a model with MAP estimate = -690.9635
expansions: [(0, 3), (12, 1), (13, 1), (182, 1)]
discards: [  2  98 146 183 208 209]
Re-initialized the encoder parameters.
Fitting a model of length 275 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 42s - loss: 708.5012 - loglik: -7.0568e+02 - logprior: -2.5809e+00
Epoch 2/10
39/39 - 37s - loss: 696.4839 - loglik: -6.9471e+02 - logprior: -5.9255e-01
Epoch 3/10
39/39 - 34s - loss: 690.4058 - loglik: -6.8764e+02 - logprior: -5.2664e-01
Epoch 4/10
39/39 - 34s - loss: 686.5181 - loglik: -6.8339e+02 - logprior: -4.6711e-01
Epoch 5/10
39/39 - 36s - loss: 683.0048 - loglik: -6.7957e+02 - logprior: -6.0935e-01
Epoch 6/10
39/39 - 36s - loss: 679.9619 - loglik: -6.7664e+02 - logprior: -4.8281e-01
Epoch 7/10
39/39 - 34s - loss: 676.7008 - loglik: -6.7382e+02 - logprior: -3.1271e-01
Epoch 8/10
39/39 - 34s - loss: 675.4566 - loglik: -6.7287e+02 - logprior: -2.3363e-01
Epoch 9/10
39/39 - 36s - loss: 673.3152 - loglik: -6.7042e+02 - logprior: -4.6746e-01
Epoch 10/10
39/39 - 37s - loss: 670.9838 - loglik: -6.6819e+02 - logprior: -1.8774e-01
Fitted a model with MAP estimate = -666.3368
Time for alignment: 967.6106
Computed alignments with likelihoods: ['-669.1038', '-668.9827', '-666.3368']
Best model has likelihood: -666.3368
time for generating output: 0.4524
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00150.projection.fasta
SP score = 0.5962011173184357
Training of 3 independent models on file PF13561.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd0297b3700>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd3d8244be0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fca346ce2b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd219dcaa90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fca45fc30a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd219f86fd0>, <__main__.SimpleDirichletPrior object at 0x7fd0706b0b50>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 574.5345 - loglik: -5.7213e+02 - logprior: -1.9610e+00
Epoch 2/10
39/39 - 15s - loss: 478.8581 - loglik: -4.7589e+02 - logprior: -1.9476e+00
Epoch 3/10
39/39 - 15s - loss: 470.5031 - loglik: -4.6732e+02 - logprior: -2.0201e+00
Epoch 4/10
39/39 - 15s - loss: 468.6961 - loglik: -4.6570e+02 - logprior: -1.9402e+00
Epoch 5/10
39/39 - 16s - loss: 467.6704 - loglik: -4.6475e+02 - logprior: -1.9291e+00
Epoch 6/10
39/39 - 16s - loss: 466.9744 - loglik: -4.6411e+02 - logprior: -1.9382e+00
Epoch 7/10
39/39 - 15s - loss: 466.4545 - loglik: -4.6362e+02 - logprior: -1.9481e+00
Epoch 8/10
39/39 - 16s - loss: 466.0998 - loglik: -4.6331e+02 - logprior: -1.9524e+00
Epoch 9/10
39/39 - 15s - loss: 465.8792 - loglik: -4.6313e+02 - logprior: -1.9599e+00
Epoch 10/10
39/39 - 15s - loss: 465.2130 - loglik: -4.6249e+02 - logprior: -1.9659e+00
Fitted a model with MAP estimate = -461.6776
expansions: [(8, 1), (11, 1), (22, 1), (23, 2), (24, 1), (31, 1), (33, 3), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (47, 1), (48, 1), (66, 1), (67, 2), (69, 1), (72, 1), (83, 1), (86, 1), (87, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (128, 1), (149, 2), (150, 1), (151, 4), (153, 1), (154, 1), (155, 1), (156, 1), (157, 2), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 471.2842 - loglik: -4.6815e+02 - logprior: -2.8880e+00
Epoch 2/2
39/39 - 22s - loss: 450.3811 - loglik: -4.4802e+02 - logprior: -1.6723e+00
Fitted a model with MAP estimate = -443.5394
expansions: [(3, 1), (199, 1)]
discards: [  0  26  84 139 143 187 196 200]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 457.5739 - loglik: -4.5471e+02 - logprior: -2.6329e+00
Epoch 2/2
39/39 - 18s - loss: 448.6562 - loglik: -4.4678e+02 - logprior: -9.9959e-01
Fitted a model with MAP estimate = -442.3537
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 454.6935 - loglik: -4.5197e+02 - logprior: -2.5059e+00
Epoch 2/10
39/39 - 18s - loss: 446.7303 - loglik: -4.4456e+02 - logprior: -1.3410e+00
Epoch 3/10
39/39 - 18s - loss: 443.2751 - loglik: -4.4140e+02 - logprior: -5.0797e-01
Epoch 4/10
39/39 - 18s - loss: 441.8674 - loglik: -4.3995e+02 - logprior: -4.4084e-01
Epoch 5/10
39/39 - 18s - loss: 440.0927 - loglik: -4.3826e+02 - logprior: -3.6797e-01
Epoch 6/10
39/39 - 18s - loss: 439.4500 - loglik: -4.3776e+02 - logprior: -2.9036e-01
Epoch 7/10
39/39 - 18s - loss: 439.0712 - loglik: -4.3752e+02 - logprior: -2.1769e-01
Epoch 8/10
39/39 - 18s - loss: 437.8749 - loglik: -4.3649e+02 - logprior: -1.5051e-01
Epoch 9/10
39/39 - 17s - loss: 437.5897 - loglik: -4.3636e+02 - logprior: -5.9895e-02
Epoch 10/10
39/39 - 17s - loss: 436.3197 - loglik: -4.3520e+02 - logprior: 0.0031
Fitted a model with MAP estimate = -434.7310
Time for alignment: 521.4070
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 573.5908 - loglik: -5.7121e+02 - logprior: -1.9558e+00
Epoch 2/10
39/39 - 14s - loss: 479.0237 - loglik: -4.7638e+02 - logprior: -1.8806e+00
Epoch 3/10
39/39 - 14s - loss: 470.5136 - loglik: -4.6754e+02 - logprior: -1.9920e+00
Epoch 4/10
39/39 - 14s - loss: 468.3141 - loglik: -4.6540e+02 - logprior: -1.9347e+00
Epoch 5/10
39/39 - 14s - loss: 467.4241 - loglik: -4.6458e+02 - logprior: -1.9201e+00
Epoch 6/10
39/39 - 14s - loss: 466.7934 - loglik: -4.6399e+02 - logprior: -1.9181e+00
Epoch 7/10
39/39 - 14s - loss: 466.5126 - loglik: -4.6373e+02 - logprior: -1.9294e+00
Epoch 8/10
39/39 - 14s - loss: 465.8199 - loglik: -4.6305e+02 - logprior: -1.9405e+00
Epoch 9/10
39/39 - 14s - loss: 465.6739 - loglik: -4.6294e+02 - logprior: -1.9517e+00
Epoch 10/10
39/39 - 14s - loss: 465.1663 - loglik: -4.6249e+02 - logprior: -1.9445e+00
Fitted a model with MAP estimate = -461.4930
expansions: [(8, 1), (11, 1), (18, 1), (23, 2), (24, 1), (29, 1), (31, 1), (32, 1), (33, 2), (35, 1), (36, 2), (37, 1), (42, 1), (44, 1), (45, 1), (48, 1), (65, 1), (66, 2), (69, 1), (72, 1), (83, 1), (86, 1), (87, 1), (92, 1), (94, 1), (108, 1), (110, 2), (111, 2), (112, 1), (126, 1), (128, 1), (149, 5), (150, 3), (152, 1), (154, 1), (155, 1), (156, 1), (161, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 470.1842 - loglik: -4.6701e+02 - logprior: -2.9142e+00
Epoch 2/2
39/39 - 18s - loss: 449.8497 - loglik: -4.4738e+02 - logprior: -1.6553e+00
Fitted a model with MAP estimate = -442.6669
expansions: [(3, 1), (199, 1)]
discards: [  0  26  42  84 140 143 191 192 200]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 458.0327 - loglik: -4.5520e+02 - logprior: -2.6191e+00
Epoch 2/2
39/39 - 17s - loss: 448.8041 - loglik: -4.4694e+02 - logprior: -1.0175e+00
Fitted a model with MAP estimate = -442.4384
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 454.6670 - loglik: -4.5195e+02 - logprior: -2.5085e+00
Epoch 2/10
39/39 - 17s - loss: 446.9983 - loglik: -4.4483e+02 - logprior: -1.3642e+00
Epoch 3/10
39/39 - 17s - loss: 443.2604 - loglik: -4.4139e+02 - logprior: -5.1110e-01
Epoch 4/10
39/39 - 17s - loss: 441.6601 - loglik: -4.3975e+02 - logprior: -4.4249e-01
Epoch 5/10
39/39 - 17s - loss: 440.7788 - loglik: -4.3895e+02 - logprior: -3.6941e-01
Epoch 6/10
39/39 - 17s - loss: 439.6124 - loglik: -4.3793e+02 - logprior: -2.8940e-01
Epoch 7/10
39/39 - 17s - loss: 439.1888 - loglik: -4.3764e+02 - logprior: -2.2157e-01
Epoch 8/10
39/39 - 17s - loss: 437.9838 - loglik: -4.3661e+02 - logprior: -1.5368e-01
Epoch 9/10
39/39 - 17s - loss: 437.0802 - loglik: -4.3586e+02 - logprior: -6.6627e-02
Epoch 10/10
39/39 - 17s - loss: 437.5480 - loglik: -4.3645e+02 - logprior: 0.0051
Fitted a model with MAP estimate = -435.0635
Time for alignment: 484.2304
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 573.8449 - loglik: -5.7146e+02 - logprior: -1.9589e+00
Epoch 2/10
39/39 - 14s - loss: 476.9046 - loglik: -4.7378e+02 - logprior: -2.0090e+00
Epoch 3/10
39/39 - 14s - loss: 469.9756 - loglik: -4.6679e+02 - logprior: -2.0108e+00
Epoch 4/10
39/39 - 14s - loss: 468.3123 - loglik: -4.6532e+02 - logprior: -1.9091e+00
Epoch 5/10
39/39 - 14s - loss: 467.1989 - loglik: -4.6428e+02 - logprior: -1.9043e+00
Epoch 6/10
39/39 - 14s - loss: 466.5478 - loglik: -4.6368e+02 - logprior: -1.9110e+00
Epoch 7/10
39/39 - 14s - loss: 466.1422 - loglik: -4.6330e+02 - logprior: -1.9180e+00
Epoch 8/10
39/39 - 14s - loss: 465.5771 - loglik: -4.6276e+02 - logprior: -1.9295e+00
Epoch 9/10
39/39 - 14s - loss: 465.6073 - loglik: -4.6285e+02 - logprior: -1.9309e+00
Fitted a model with MAP estimate = -461.4646
expansions: [(8, 1), (11, 1), (22, 1), (23, 2), (24, 1), (27, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 2), (37, 1), (44, 1), (45, 1), (46, 1), (48, 1), (66, 1), (67, 2), (69, 1), (72, 1), (83, 1), (86, 1), (87, 1), (88, 1), (93, 1), (96, 1), (108, 1), (109, 2), (111, 2), (121, 1), (128, 1), (149, 2), (150, 1), (151, 4), (153, 1), (155, 1), (156, 1), (157, 2), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 471.4037 - loglik: -4.6825e+02 - logprior: -2.9147e+00
Epoch 2/2
39/39 - 18s - loss: 450.2481 - loglik: -4.4802e+02 - logprior: -1.5999e+00
Fitted a model with MAP estimate = -443.5377
expansions: [(3, 1), (198, 1)]
discards: [  0  26  84 139 143 199]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 457.5609 - loglik: -4.5471e+02 - logprior: -2.6416e+00
Epoch 2/2
39/39 - 17s - loss: 448.9011 - loglik: -4.4701e+02 - logprior: -1.0350e+00
Fitted a model with MAP estimate = -442.2654
expansions: [(3, 1)]
discards: [  0 181]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 454.6121 - loglik: -4.5189e+02 - logprior: -2.5137e+00
Epoch 2/10
39/39 - 17s - loss: 446.7652 - loglik: -4.4461e+02 - logprior: -1.3603e+00
Epoch 3/10
39/39 - 17s - loss: 443.0259 - loglik: -4.4116e+02 - logprior: -5.1852e-01
Epoch 4/10
39/39 - 17s - loss: 441.5585 - loglik: -4.3962e+02 - logprior: -4.5440e-01
Epoch 5/10
39/39 - 17s - loss: 440.3449 - loglik: -4.3845e+02 - logprior: -3.8437e-01
Epoch 6/10
39/39 - 17s - loss: 439.3943 - loglik: -4.3763e+02 - logprior: -3.1508e-01
Epoch 7/10
39/39 - 17s - loss: 438.7072 - loglik: -4.3710e+02 - logprior: -2.3937e-01
Epoch 8/10
39/39 - 17s - loss: 437.5900 - loglik: -4.3617e+02 - logprior: -1.6264e-01
Epoch 9/10
39/39 - 18s - loss: 436.5661 - loglik: -4.3528e+02 - logprior: -9.1799e-02
Epoch 10/10
39/39 - 18s - loss: 436.4221 - loglik: -4.3527e+02 - logprior: -2.1618e-02
Fitted a model with MAP estimate = -434.4649
Time for alignment: 470.6217
Computed alignments with likelihoods: ['-434.7310', '-435.0635', '-434.4649']
Best model has likelihood: -434.4649
time for generating output: 0.3760
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13561.projection.fasta
SP score = 0.7363418231172255
Training of 3 independent models on file PF05746.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd14c448a60>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd14c448c70>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c4480d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e62b0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6cd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd14c3e6f70>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6670>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6100>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6940>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6640>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e68e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6ca0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6f40>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6c40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd14c3e6c70> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd3d86e2a00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd1e6d01940>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd1e6d01610>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd22244d850>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd14c339ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd14c3e6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fca450aa130>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd4281e5dc0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd2c81d4430>, <function make_default_emission_matrix at 0x7fd2c81d4430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fca44f90310>, <__main__.SimpleDirichletPrior object at 0x7fca29396130>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 321.0481 - loglik: -3.1781e+02 - logprior: -3.1476e+00
Epoch 2/10
19/19 - 3s - loss: 275.1597 - loglik: -2.7308e+02 - logprior: -1.4491e+00
Epoch 3/10
19/19 - 3s - loss: 250.8393 - loglik: -2.4789e+02 - logprior: -1.8631e+00
Epoch 4/10
19/19 - 3s - loss: 245.6127 - loglik: -2.4311e+02 - logprior: -1.7663e+00
Epoch 5/10
19/19 - 3s - loss: 243.4910 - loglik: -2.4122e+02 - logprior: -1.7331e+00
Epoch 6/10
19/19 - 3s - loss: 242.8160 - loglik: -2.4069e+02 - logprior: -1.7135e+00
Epoch 7/10
19/19 - 3s - loss: 241.9312 - loglik: -2.3987e+02 - logprior: -1.6967e+00
Epoch 8/10
19/19 - 3s - loss: 241.4013 - loglik: -2.3936e+02 - logprior: -1.7036e+00
Epoch 9/10
19/19 - 3s - loss: 241.2193 - loglik: -2.3920e+02 - logprior: -1.7007e+00
Epoch 10/10
19/19 - 3s - loss: 241.1181 - loglik: -2.3911e+02 - logprior: -1.7033e+00
Fitted a model with MAP estimate = -240.5106
expansions: [(17, 2), (18, 4), (19, 2), (21, 2), (24, 1), (28, 2), (32, 2), (40, 1), (44, 1), (48, 1), (50, 1), (56, 1), (63, 1), (64, 1), (66, 2), (68, 1), (71, 1), (76, 1), (77, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 251.0734 - loglik: -2.4692e+02 - logprior: -4.0805e+00
Epoch 2/2
19/19 - 4s - loss: 237.4393 - loglik: -2.3491e+02 - logprior: -2.1832e+00
Fitted a model with MAP estimate = -234.2174
expansions: [(0, 2)]
discards: [ 0 16 17 18 28 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 238.6092 - loglik: -2.3555e+02 - logprior: -2.9911e+00
Epoch 2/2
19/19 - 4s - loss: 233.7176 - loglik: -2.3224e+02 - logprior: -1.1529e+00
Fitted a model with MAP estimate = -231.6205
expansions: []
discards: [ 0 82]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 240.7578 - loglik: -2.3686e+02 - logprior: -3.8241e+00
Epoch 2/10
19/19 - 4s - loss: 234.9770 - loglik: -2.3340e+02 - logprior: -1.3436e+00
Epoch 3/10
19/19 - 4s - loss: 232.4800 - loglik: -2.3094e+02 - logprior: -1.1825e+00
Epoch 4/10
19/19 - 4s - loss: 231.2517 - loglik: -2.2959e+02 - logprior: -1.1445e+00
Epoch 5/10
19/19 - 4s - loss: 230.7250 - loglik: -2.2904e+02 - logprior: -1.1157e+00
Epoch 6/10
19/19 - 4s - loss: 230.0657 - loglik: -2.2841e+02 - logprior: -1.0996e+00
Epoch 7/10
19/19 - 4s - loss: 229.6370 - loglik: -2.2804e+02 - logprior: -1.0762e+00
Epoch 8/10
19/19 - 4s - loss: 229.8089 - loglik: -2.2826e+02 - logprior: -1.0612e+00
Fitted a model with MAP estimate = -228.8992
Time for alignment: 121.5146
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 320.8871 - loglik: -3.1765e+02 - logprior: -3.1470e+00
Epoch 2/10
19/19 - 3s - loss: 275.3798 - loglik: -2.7328e+02 - logprior: -1.4480e+00
Epoch 3/10
19/19 - 3s - loss: 251.3389 - loglik: -2.4828e+02 - logprior: -1.8498e+00
Epoch 4/10
19/19 - 3s - loss: 245.2310 - loglik: -2.4260e+02 - logprior: -1.7879e+00
Epoch 5/10
19/19 - 3s - loss: 242.5952 - loglik: -2.4021e+02 - logprior: -1.8003e+00
Epoch 6/10
19/19 - 3s - loss: 241.9936 - loglik: -2.3978e+02 - logprior: -1.7697e+00
Epoch 7/10
19/19 - 3s - loss: 241.6449 - loglik: -2.3950e+02 - logprior: -1.7360e+00
Epoch 8/10
19/19 - 3s - loss: 241.0658 - loglik: -2.3896e+02 - logprior: -1.7193e+00
Epoch 9/10
19/19 - 3s - loss: 241.3779 - loglik: -2.3930e+02 - logprior: -1.7047e+00
Fitted a model with MAP estimate = -240.4478
expansions: [(17, 2), (19, 4), (20, 2), (22, 1), (24, 1), (28, 2), (32, 2), (34, 1), (40, 1), (43, 1), (48, 1), (49, 1), (56, 1), (62, 1), (66, 2), (70, 1), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 251.3321 - loglik: -2.4719e+02 - logprior: -4.0697e+00
Epoch 2/2
19/19 - 4s - loss: 237.2578 - loglik: -2.3476e+02 - logprior: -2.1340e+00
Fitted a model with MAP estimate = -234.3082
expansions: [(0, 2)]
discards: [ 0 38 43 86]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 237.4527 - loglik: -2.3440e+02 - logprior: -2.9853e+00
Epoch 2/2
19/19 - 4s - loss: 232.3895 - loglik: -2.3090e+02 - logprior: -1.1738e+00
Fitted a model with MAP estimate = -230.4804
expansions: [(24, 1)]
discards: [ 0 17 18 19]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 240.9188 - loglik: -2.3702e+02 - logprior: -3.8302e+00
Epoch 2/10
19/19 - 4s - loss: 235.0426 - loglik: -2.3344e+02 - logprior: -1.3722e+00
Epoch 3/10
19/19 - 4s - loss: 232.3269 - loglik: -2.3076e+02 - logprior: -1.2058e+00
Epoch 4/10
19/19 - 4s - loss: 231.4432 - loglik: -2.2976e+02 - logprior: -1.1638e+00
Epoch 5/10
19/19 - 4s - loss: 230.4553 - loglik: -2.2875e+02 - logprior: -1.1302e+00
Epoch 6/10
19/19 - 4s - loss: 230.1548 - loglik: -2.2849e+02 - logprior: -1.1068e+00
Epoch 7/10
19/19 - 4s - loss: 230.0131 - loglik: -2.2840e+02 - logprior: -1.0845e+00
Epoch 8/10
19/19 - 4s - loss: 229.5166 - loglik: -2.2798e+02 - logprior: -1.0532e+00
Epoch 9/10
19/19 - 4s - loss: 229.8881 - loglik: -2.2840e+02 - logprior: -1.0322e+00
Fitted a model with MAP estimate = -228.8956
Time for alignment: 120.2303
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 320.8890 - loglik: -3.1765e+02 - logprior: -3.1465e+00
Epoch 2/10
19/19 - 3s - loss: 276.0302 - loglik: -2.7398e+02 - logprior: -1.4444e+00
Epoch 3/10
19/19 - 3s - loss: 251.1800 - loglik: -2.4848e+02 - logprior: -1.8562e+00
Epoch 4/10
19/19 - 3s - loss: 244.8092 - loglik: -2.4225e+02 - logprior: -1.8163e+00
Epoch 5/10
19/19 - 3s - loss: 243.4553 - loglik: -2.4114e+02 - logprior: -1.8084e+00
Epoch 6/10
19/19 - 3s - loss: 242.4849 - loglik: -2.4027e+02 - logprior: -1.7647e+00
Epoch 7/10
19/19 - 3s - loss: 242.0537 - loglik: -2.3991e+02 - logprior: -1.7432e+00
Epoch 8/10
19/19 - 3s - loss: 242.2627 - loglik: -2.4015e+02 - logprior: -1.7300e+00
Fitted a model with MAP estimate = -241.2161
expansions: [(17, 2), (20, 4), (27, 1), (28, 2), (32, 2), (34, 1), (40, 1), (44, 1), (48, 1), (49, 1), (56, 1), (62, 1), (63, 1), (65, 2), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 251.4848 - loglik: -2.4732e+02 - logprior: -4.0847e+00
Epoch 2/2
19/19 - 4s - loss: 237.7638 - loglik: -2.3530e+02 - logprior: -2.1448e+00
Fitted a model with MAP estimate = -234.6763
expansions: [(0, 2), (21, 1), (23, 2)]
discards: [ 0 35 40 83]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 237.5668 - loglik: -2.3450e+02 - logprior: -3.0012e+00
Epoch 2/2
19/19 - 4s - loss: 232.5760 - loglik: -2.3106e+02 - logprior: -1.1642e+00
Fitted a model with MAP estimate = -230.4127
expansions: []
discards: [ 0 17 18 25]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 240.6459 - loglik: -2.3685e+02 - logprior: -3.7279e+00
Epoch 2/10
19/19 - 4s - loss: 234.7283 - loglik: -2.3314e+02 - logprior: -1.3393e+00
Epoch 3/10
19/19 - 4s - loss: 231.9082 - loglik: -2.3034e+02 - logprior: -1.2142e+00
Epoch 4/10
19/19 - 4s - loss: 231.3837 - loglik: -2.2970e+02 - logprior: -1.1759e+00
Epoch 5/10
19/19 - 4s - loss: 230.2419 - loglik: -2.2855e+02 - logprior: -1.1358e+00
Epoch 6/10
19/19 - 4s - loss: 229.8955 - loglik: -2.2824e+02 - logprior: -1.1126e+00
Epoch 7/10
19/19 - 4s - loss: 229.8550 - loglik: -2.2826e+02 - logprior: -1.0852e+00
Epoch 8/10
19/19 - 4s - loss: 229.2621 - loglik: -2.2772e+02 - logprior: -1.0650e+00
Epoch 9/10
19/19 - 4s - loss: 229.0176 - loglik: -2.2754e+02 - logprior: -1.0355e+00
Epoch 10/10
19/19 - 4s - loss: 229.8092 - loglik: -2.2837e+02 - logprior: -1.0171e+00
Fitted a model with MAP estimate = -228.6060
Time for alignment: 122.0273
Computed alignments with likelihoods: ['-228.8992', '-228.8956', '-228.6060']
Best model has likelihood: -228.6060
time for generating output: 0.1827
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF05746.projection.fasta
SP score = 0.8583136327817179
