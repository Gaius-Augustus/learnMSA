Training of 3 independent models on file PF00079.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96ac1df850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96ac1e4d00>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 768.9656 - loglik: -7.6741e+02 - logprior: -1.5564e+00
Epoch 2/10
39/39 - 30s - loss: 642.5462 - loglik: -6.4068e+02 - logprior: -1.8706e+00
Epoch 3/10
39/39 - 31s - loss: 631.6571 - loglik: -6.2986e+02 - logprior: -1.8008e+00
Epoch 4/10
39/39 - 31s - loss: 628.1663 - loglik: -6.2644e+02 - logprior: -1.7278e+00
Epoch 5/10
39/39 - 31s - loss: 626.4229 - loglik: -6.2468e+02 - logprior: -1.7386e+00
Epoch 6/10
39/39 - 31s - loss: 625.4448 - loglik: -6.2369e+02 - logprior: -1.7520e+00
Epoch 7/10
39/39 - 31s - loss: 624.0214 - loglik: -6.2225e+02 - logprior: -1.7705e+00
Epoch 8/10
39/39 - 31s - loss: 623.3831 - loglik: -6.2159e+02 - logprior: -1.7932e+00
Epoch 9/10
39/39 - 31s - loss: 623.2733 - loglik: -6.2148e+02 - logprior: -1.7945e+00
Epoch 10/10
39/39 - 31s - loss: 623.1809 - loglik: -6.2138e+02 - logprior: -1.8059e+00
Fitted a model with MAP estimate = -622.5171
expansions: [(13, 1), (14, 1), (15, 1), (46, 1), (53, 3), (54, 2), (55, 1), (61, 2), (62, 1), (63, 1), (64, 1), (66, 1), (67, 1), (76, 1), (77, 1), (80, 1), (82, 1), (83, 1), (84, 1), (90, 1), (94, 1), (97, 1), (98, 1), (99, 1), (104, 1), (105, 1), (116, 1), (118, 1), (120, 1), (134, 1), (139, 1), (142, 1), (145, 1), (148, 1), (157, 1), (158, 1), (162, 2), (163, 2), (165, 1), (171, 1), (184, 1), (187, 1), (191, 1), (193, 1), (194, 1), (195, 1), (197, 1), (198, 2), (199, 1), (200, 1), (212, 1), (213, 1), (216, 1), (218, 1), (228, 1), (232, 1), (234, 3), (241, 1), (261, 1), (262, 2), (263, 1), (266, 3), (267, 1), (278, 2), (279, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 628.4789 - loglik: -6.2695e+02 - logprior: -1.5308e+00
Epoch 2/2
39/39 - 46s - loss: 605.1187 - loglik: -6.0479e+02 - logprior: -3.3030e-01
Fitted a model with MAP estimate = -601.4579
expansions: [(16, 1), (79, 1)]
discards: [ 57  73 251 336 337]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 47s - loss: 610.8859 - loglik: -6.0984e+02 - logprior: -1.0490e+00
Epoch 2/2
39/39 - 44s - loss: 603.8577 - loglik: -6.0390e+02 - logprior: 0.0452
Fitted a model with MAP estimate = -600.8822
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 364 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 610.0532 - loglik: -6.0921e+02 - logprior: -8.4278e-01
Epoch 2/10
39/39 - 45s - loss: 603.3387 - loglik: -6.0360e+02 - logprior: 0.2645
Epoch 3/10
39/39 - 45s - loss: 599.8200 - loglik: -6.0032e+02 - logprior: 0.4980
Epoch 4/10
39/39 - 44s - loss: 597.2283 - loglik: -5.9786e+02 - logprior: 0.6303
Epoch 5/10
39/39 - 45s - loss: 594.8720 - loglik: -5.9559e+02 - logprior: 0.7191
Epoch 6/10
39/39 - 45s - loss: 592.8646 - loglik: -5.9371e+02 - logprior: 0.8420
Epoch 7/10
39/39 - 45s - loss: 590.7847 - loglik: -5.9175e+02 - logprior: 0.9654
Epoch 8/10
39/39 - 44s - loss: 590.0019 - loglik: -5.9107e+02 - logprior: 1.0643
Epoch 9/10
39/39 - 45s - loss: 589.7994 - loglik: -5.9102e+02 - logprior: 1.2167
Epoch 10/10
39/39 - 44s - loss: 589.0270 - loglik: -5.9036e+02 - logprior: 1.3329
Fitted a model with MAP estimate = -588.4075
Time for alignment: 1136.9068
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 767.1982 - loglik: -7.6564e+02 - logprior: -1.5540e+00
Epoch 2/10
39/39 - 31s - loss: 641.1909 - loglik: -6.3933e+02 - logprior: -1.8604e+00
Epoch 3/10
39/39 - 31s - loss: 630.1169 - loglik: -6.2827e+02 - logprior: -1.8481e+00
Epoch 4/10
39/39 - 31s - loss: 627.2714 - loglik: -6.2549e+02 - logprior: -1.7858e+00
Epoch 5/10
39/39 - 31s - loss: 625.5912 - loglik: -6.2378e+02 - logprior: -1.8064e+00
Epoch 6/10
39/39 - 31s - loss: 624.0491 - loglik: -6.2219e+02 - logprior: -1.8578e+00
Epoch 7/10
39/39 - 31s - loss: 622.7558 - loglik: -6.2088e+02 - logprior: -1.8768e+00
Epoch 8/10
39/39 - 31s - loss: 622.5077 - loglik: -6.2064e+02 - logprior: -1.8725e+00
Epoch 9/10
39/39 - 31s - loss: 621.9795 - loglik: -6.2009e+02 - logprior: -1.8885e+00
Epoch 10/10
39/39 - 31s - loss: 621.7217 - loglik: -6.1984e+02 - logprior: -1.8849e+00
Fitted a model with MAP estimate = -621.1995
expansions: [(13, 1), (14, 2), (15, 1), (50, 1), (52, 3), (56, 1), (62, 1), (64, 2), (65, 1), (66, 1), (68, 1), (69, 1), (78, 1), (79, 1), (82, 1), (84, 1), (85, 1), (86, 1), (88, 1), (91, 1), (95, 1), (98, 1), (99, 1), (100, 1), (102, 1), (112, 1), (117, 1), (119, 1), (121, 1), (134, 1), (140, 1), (143, 1), (145, 1), (149, 1), (158, 1), (159, 1), (160, 1), (162, 1), (163, 1), (165, 1), (166, 1), (174, 1), (185, 1), (188, 1), (192, 1), (194, 1), (195, 1), (196, 1), (198, 1), (199, 2), (200, 1), (201, 1), (213, 1), (214, 1), (217, 1), (220, 1), (229, 1), (233, 1), (236, 2), (247, 1), (263, 1), (264, 3), (265, 2), (266, 3), (267, 1), (278, 2), (279, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 368 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 627.9896 - loglik: -6.2652e+02 - logprior: -1.4702e+00
Epoch 2/2
39/39 - 45s - loss: 604.5282 - loglik: -6.0426e+02 - logprior: -2.7008e-01
Fitted a model with MAP estimate = -601.0944
expansions: []
discards: [ 76 252 338]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 610.4486 - loglik: -6.0936e+02 - logprior: -1.0897e+00
Epoch 2/2
39/39 - 45s - loss: 603.4872 - loglik: -6.0357e+02 - logprior: 0.0848
Fitted a model with MAP estimate = -600.5968
expansions: []
discards: [335]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 609.5074 - loglik: -6.0868e+02 - logprior: -8.2364e-01
Epoch 2/10
39/39 - 44s - loss: 603.4392 - loglik: -6.0378e+02 - logprior: 0.3380
Epoch 3/10
39/39 - 44s - loss: 599.6119 - loglik: -6.0012e+02 - logprior: 0.5043
Epoch 4/10
39/39 - 45s - loss: 597.8032 - loglik: -5.9842e+02 - logprior: 0.6156
Epoch 5/10
39/39 - 44s - loss: 594.1614 - loglik: -5.9487e+02 - logprior: 0.7109
Epoch 6/10
39/39 - 45s - loss: 592.9000 - loglik: -5.9376e+02 - logprior: 0.8628
Epoch 7/10
39/39 - 45s - loss: 591.0375 - loglik: -5.9198e+02 - logprior: 0.9457
Epoch 8/10
39/39 - 45s - loss: 590.0239 - loglik: -5.9114e+02 - logprior: 1.1132
Epoch 9/10
39/39 - 44s - loss: 589.3226 - loglik: -5.9054e+02 - logprior: 1.2133
Epoch 10/10
39/39 - 44s - loss: 589.2861 - loglik: -5.9069e+02 - logprior: 1.4078
Fitted a model with MAP estimate = -588.3041
Time for alignment: 1134.8761
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 767.2399 - loglik: -7.6571e+02 - logprior: -1.5338e+00
Epoch 2/10
39/39 - 31s - loss: 640.8783 - loglik: -6.3901e+02 - logprior: -1.8685e+00
Epoch 3/10
39/39 - 31s - loss: 630.5075 - loglik: -6.2869e+02 - logprior: -1.8208e+00
Epoch 4/10
39/39 - 31s - loss: 628.5029 - loglik: -6.2671e+02 - logprior: -1.7882e+00
Epoch 5/10
39/39 - 31s - loss: 625.8127 - loglik: -6.2403e+02 - logprior: -1.7855e+00
Epoch 6/10
39/39 - 31s - loss: 624.3535 - loglik: -6.2252e+02 - logprior: -1.8294e+00
Epoch 7/10
39/39 - 31s - loss: 622.9016 - loglik: -6.2103e+02 - logprior: -1.8691e+00
Epoch 8/10
39/39 - 31s - loss: 623.0970 - loglik: -6.2121e+02 - logprior: -1.8859e+00
Fitted a model with MAP estimate = -621.6694
expansions: [(13, 1), (14, 1), (15, 1), (46, 1), (52, 3), (56, 1), (62, 1), (64, 2), (65, 1), (66, 1), (68, 1), (69, 1), (78, 1), (79, 1), (85, 1), (86, 1), (87, 1), (89, 1), (91, 1), (92, 1), (95, 1), (98, 1), (99, 1), (100, 1), (106, 1), (118, 1), (119, 1), (120, 1), (121, 1), (135, 1), (140, 1), (143, 1), (146, 1), (157, 1), (158, 1), (159, 1), (160, 1), (162, 1), (163, 1), (165, 1), (166, 1), (174, 1), (183, 1), (184, 1), (188, 1), (194, 1), (195, 1), (196, 1), (198, 1), (199, 2), (200, 1), (201, 1), (213, 1), (214, 1), (216, 1), (217, 1), (219, 1), (228, 1), (234, 2), (235, 2), (237, 1), (262, 1), (263, 1), (264, 1), (267, 3), (268, 1), (277, 1), (278, 2), (279, 1), (280, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 627.2222 - loglik: -6.2570e+02 - logprior: -1.5216e+00
Epoch 2/2
39/39 - 45s - loss: 604.7973 - loglik: -6.0442e+02 - logprior: -3.8109e-01
Fitted a model with MAP estimate = -601.4866
expansions: [(16, 1), (336, 1)]
discards: [ 55  75 250 296]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 47s - loss: 610.5118 - loglik: -6.0940e+02 - logprior: -1.1074e+00
Epoch 2/2
39/39 - 44s - loss: 603.7833 - loglik: -6.0383e+02 - logprior: 0.0418
Fitted a model with MAP estimate = -600.9134
expansions: [(56, 2)]
discards: [334 335]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 610.0093 - loglik: -6.0902e+02 - logprior: -9.8686e-01
Epoch 2/10
39/39 - 45s - loss: 603.1109 - loglik: -6.0344e+02 - logprior: 0.3256
Epoch 3/10
39/39 - 45s - loss: 600.2350 - loglik: -6.0070e+02 - logprior: 0.4674
Epoch 4/10
39/39 - 45s - loss: 597.0750 - loglik: -5.9768e+02 - logprior: 0.6041
Epoch 5/10
39/39 - 45s - loss: 594.6666 - loglik: -5.9539e+02 - logprior: 0.7222
Epoch 6/10
39/39 - 45s - loss: 592.5757 - loglik: -5.9343e+02 - logprior: 0.8526
Epoch 7/10
39/39 - 45s - loss: 590.8400 - loglik: -5.9184e+02 - logprior: 1.0044
Epoch 8/10
39/39 - 45s - loss: 589.7155 - loglik: -5.9080e+02 - logprior: 1.0857
Epoch 9/10
39/39 - 45s - loss: 589.6882 - loglik: -5.9083e+02 - logprior: 1.1466
Epoch 10/10
39/39 - 44s - loss: 589.0936 - loglik: -5.9041e+02 - logprior: 1.3127
Fitted a model with MAP estimate = -588.0889
Time for alignment: 1072.9616
Computed alignments with likelihoods: ['-588.4075', '-588.3041', '-588.0889']
Best model has likelihood: -588.0889
SP score = 0.9357
Training of 3 independent models on file PF01381.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9691347310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f969089f2e0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.0495 - loglik: -1.4685e+02 - logprior: -3.1951e+00
Epoch 2/10
19/19 - 1s - loss: 128.9530 - loglik: -1.2759e+02 - logprior: -1.3673e+00
Epoch 3/10
19/19 - 1s - loss: 118.9503 - loglik: -1.1738e+02 - logprior: -1.5740e+00
Epoch 4/10
19/19 - 1s - loss: 116.7128 - loglik: -1.1526e+02 - logprior: -1.4507e+00
Epoch 5/10
19/19 - 1s - loss: 116.1040 - loglik: -1.1468e+02 - logprior: -1.4248e+00
Epoch 6/10
19/19 - 1s - loss: 115.8167 - loglik: -1.1442e+02 - logprior: -1.3933e+00
Epoch 7/10
19/19 - 1s - loss: 115.6114 - loglik: -1.1423e+02 - logprior: -1.3835e+00
Epoch 8/10
19/19 - 1s - loss: 115.6294 - loglik: -1.1426e+02 - logprior: -1.3738e+00
Fitted a model with MAP estimate = -115.3052
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (19, 2), (28, 2), (29, 2), (30, 2), (31, 2), (32, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 123.3488 - loglik: -1.1923e+02 - logprior: -4.1230e+00
Epoch 2/2
19/19 - 1s - loss: 113.3985 - loglik: -1.1131e+02 - logprior: -2.0891e+00
Fitted a model with MAP estimate = -111.3984
expansions: [(0, 2)]
discards: [ 0 23 34 36 40 46]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.0104 - loglik: -1.1101e+02 - logprior: -2.9977e+00
Epoch 2/2
19/19 - 1s - loss: 109.8349 - loglik: -1.0865e+02 - logprior: -1.1890e+00
Fitted a model with MAP estimate = -108.8752
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.1310 - loglik: -1.1139e+02 - logprior: -3.7371e+00
Epoch 2/10
19/19 - 1s - loss: 110.4126 - loglik: -1.0903e+02 - logprior: -1.3825e+00
Epoch 3/10
19/19 - 1s - loss: 109.1094 - loglik: -1.0790e+02 - logprior: -1.2102e+00
Epoch 4/10
19/19 - 1s - loss: 108.1775 - loglik: -1.0701e+02 - logprior: -1.1683e+00
Epoch 5/10
19/19 - 1s - loss: 107.6840 - loglik: -1.0653e+02 - logprior: -1.1492e+00
Epoch 6/10
19/19 - 1s - loss: 107.1476 - loglik: -1.0601e+02 - logprior: -1.1385e+00
Epoch 7/10
19/19 - 1s - loss: 106.8866 - loglik: -1.0576e+02 - logprior: -1.1233e+00
Epoch 8/10
19/19 - 1s - loss: 106.7270 - loglik: -1.0561e+02 - logprior: -1.1122e+00
Epoch 9/10
19/19 - 1s - loss: 106.6599 - loglik: -1.0556e+02 - logprior: -1.0973e+00
Epoch 10/10
19/19 - 1s - loss: 106.3841 - loglik: -1.0529e+02 - logprior: -1.0908e+00
Fitted a model with MAP estimate = -106.3150
Time for alignment: 40.7140
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 149.8796 - loglik: -1.4669e+02 - logprior: -3.1925e+00
Epoch 2/10
19/19 - 1s - loss: 127.4697 - loglik: -1.2611e+02 - logprior: -1.3564e+00
Epoch 3/10
19/19 - 1s - loss: 118.5811 - loglik: -1.1704e+02 - logprior: -1.5362e+00
Epoch 4/10
19/19 - 1s - loss: 116.4598 - loglik: -1.1504e+02 - logprior: -1.4176e+00
Epoch 5/10
19/19 - 1s - loss: 115.7971 - loglik: -1.1440e+02 - logprior: -1.3973e+00
Epoch 6/10
19/19 - 1s - loss: 115.4563 - loglik: -1.1409e+02 - logprior: -1.3689e+00
Epoch 7/10
19/19 - 1s - loss: 115.3343 - loglik: -1.1398e+02 - logprior: -1.3551e+00
Epoch 8/10
19/19 - 1s - loss: 115.1928 - loglik: -1.1385e+02 - logprior: -1.3462e+00
Epoch 9/10
19/19 - 1s - loss: 115.0699 - loglik: -1.1373e+02 - logprior: -1.3420e+00
Epoch 10/10
19/19 - 1s - loss: 115.0276 - loglik: -1.1369e+02 - logprior: -1.3394e+00
Fitted a model with MAP estimate = -114.7766
expansions: [(6, 3), (10, 1), (19, 2), (20, 1), (27, 2), (28, 2), (29, 2), (30, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 124.8611 - loglik: -1.2075e+02 - logprior: -4.1081e+00
Epoch 2/2
19/19 - 1s - loss: 113.5761 - loglik: -1.1144e+02 - logprior: -2.1388e+00
Fitted a model with MAP estimate = -111.6079
expansions: [(0, 2)]
discards: [ 0 23 34 36 40 45]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 113.7050 - loglik: -1.1071e+02 - logprior: -2.9971e+00
Epoch 2/2
19/19 - 1s - loss: 109.7833 - loglik: -1.0859e+02 - logprior: -1.1894e+00
Fitted a model with MAP estimate = -108.8684
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.2473 - loglik: -1.1150e+02 - logprior: -3.7448e+00
Epoch 2/10
19/19 - 1s - loss: 110.4073 - loglik: -1.0902e+02 - logprior: -1.3874e+00
Epoch 3/10
19/19 - 1s - loss: 109.1051 - loglik: -1.0790e+02 - logprior: -1.2058e+00
Epoch 4/10
19/19 - 1s - loss: 108.1643 - loglik: -1.0700e+02 - logprior: -1.1686e+00
Epoch 5/10
19/19 - 1s - loss: 107.6500 - loglik: -1.0650e+02 - logprior: -1.1508e+00
Epoch 6/10
19/19 - 1s - loss: 107.2242 - loglik: -1.0609e+02 - logprior: -1.1384e+00
Epoch 7/10
19/19 - 1s - loss: 106.9675 - loglik: -1.0584e+02 - logprior: -1.1243e+00
Epoch 8/10
19/19 - 1s - loss: 106.7318 - loglik: -1.0562e+02 - logprior: -1.1095e+00
Epoch 9/10
19/19 - 1s - loss: 106.5466 - loglik: -1.0545e+02 - logprior: -1.0982e+00
Epoch 10/10
19/19 - 1s - loss: 106.3716 - loglik: -1.0528e+02 - logprior: -1.0884e+00
Fitted a model with MAP estimate = -106.3216
Time for alignment: 41.7031
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 149.8344 - loglik: -1.4664e+02 - logprior: -3.1980e+00
Epoch 2/10
19/19 - 1s - loss: 127.3516 - loglik: -1.2596e+02 - logprior: -1.3899e+00
Epoch 3/10
19/19 - 1s - loss: 118.7255 - loglik: -1.1714e+02 - logprior: -1.5836e+00
Epoch 4/10
19/19 - 1s - loss: 116.6753 - loglik: -1.1522e+02 - logprior: -1.4595e+00
Epoch 5/10
19/19 - 1s - loss: 116.0765 - loglik: -1.1463e+02 - logprior: -1.4417e+00
Epoch 6/10
19/19 - 1s - loss: 115.6809 - loglik: -1.1426e+02 - logprior: -1.4169e+00
Epoch 7/10
19/19 - 1s - loss: 115.5539 - loglik: -1.1415e+02 - logprior: -1.4018e+00
Epoch 8/10
19/19 - 1s - loss: 115.4904 - loglik: -1.1409e+02 - logprior: -1.3964e+00
Epoch 9/10
19/19 - 1s - loss: 115.4082 - loglik: -1.1402e+02 - logprior: -1.3907e+00
Epoch 10/10
19/19 - 1s - loss: 115.4015 - loglik: -1.1401e+02 - logprior: -1.3869e+00
Fitted a model with MAP estimate = -115.1004
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (18, 2), (19, 2), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 123.4365 - loglik: -1.1932e+02 - logprior: -4.1183e+00
Epoch 2/2
19/19 - 1s - loss: 113.3834 - loglik: -1.1128e+02 - logprior: -2.1005e+00
Fitted a model with MAP estimate = -111.4818
expansions: [(0, 2)]
discards: [ 0 22 24 35 42]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.6087 - loglik: -1.1061e+02 - logprior: -2.9995e+00
Epoch 2/2
19/19 - 1s - loss: 109.7748 - loglik: -1.0858e+02 - logprior: -1.1901e+00
Fitted a model with MAP estimate = -108.8779
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.1487 - loglik: -1.1140e+02 - logprior: -3.7507e+00
Epoch 2/10
19/19 - 1s - loss: 110.4066 - loglik: -1.0902e+02 - logprior: -1.3826e+00
Epoch 3/10
19/19 - 1s - loss: 109.0831 - loglik: -1.0787e+02 - logprior: -1.2102e+00
Epoch 4/10
19/19 - 1s - loss: 108.2534 - loglik: -1.0709e+02 - logprior: -1.1659e+00
Epoch 5/10
19/19 - 1s - loss: 107.5655 - loglik: -1.0642e+02 - logprior: -1.1480e+00
Epoch 6/10
19/19 - 1s - loss: 107.2430 - loglik: -1.0610e+02 - logprior: -1.1399e+00
Epoch 7/10
19/19 - 1s - loss: 106.8489 - loglik: -1.0573e+02 - logprior: -1.1178e+00
Epoch 8/10
19/19 - 1s - loss: 106.8256 - loglik: -1.0572e+02 - logprior: -1.1101e+00
Epoch 9/10
19/19 - 1s - loss: 106.5983 - loglik: -1.0550e+02 - logprior: -1.0957e+00
Epoch 10/10
19/19 - 1s - loss: 106.3715 - loglik: -1.0529e+02 - logprior: -1.0805e+00
Fitted a model with MAP estimate = -106.3193
Time for alignment: 41.6776
Computed alignments with likelihoods: ['-106.3150', '-106.3216', '-106.3193']
Best model has likelihood: -106.3150
SP score = 0.6734
Training of 3 independent models on file PF02878.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f969095dbb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9638e764f0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 359.7047 - loglik: -3.5669e+02 - logprior: -3.0145e+00
Epoch 2/10
19/19 - 3s - loss: 301.8764 - loglik: -3.0068e+02 - logprior: -1.1985e+00
Epoch 3/10
19/19 - 3s - loss: 275.8827 - loglik: -2.7458e+02 - logprior: -1.3076e+00
Epoch 4/10
19/19 - 3s - loss: 270.2343 - loglik: -2.6894e+02 - logprior: -1.2969e+00
Epoch 5/10
19/19 - 3s - loss: 267.5302 - loglik: -2.6631e+02 - logprior: -1.2190e+00
Epoch 6/10
19/19 - 3s - loss: 266.2905 - loglik: -2.6509e+02 - logprior: -1.1999e+00
Epoch 7/10
19/19 - 3s - loss: 264.8299 - loglik: -2.6362e+02 - logprior: -1.2084e+00
Epoch 8/10
19/19 - 3s - loss: 264.9221 - loglik: -2.6374e+02 - logprior: -1.1812e+00
Fitted a model with MAP estimate = -264.3895
expansions: [(12, 1), (14, 3), (16, 1), (17, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (93, 1), (97, 1), (101, 1), (106, 1), (108, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 274.4525 - loglik: -2.7060e+02 - logprior: -3.8520e+00
Epoch 2/2
19/19 - 3s - loss: 259.5776 - loglik: -2.5774e+02 - logprior: -1.8339e+00
Fitted a model with MAP estimate = -257.0545
expansions: [(0, 3)]
discards: [  0  16  37  75 137]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 260.9019 - loglik: -2.5823e+02 - logprior: -2.6681e+00
Epoch 2/2
19/19 - 3s - loss: 256.3850 - loglik: -2.5545e+02 - logprior: -9.3433e-01
Fitted a model with MAP estimate = -255.2472
expansions: []
discards: [  0   1   2 137]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 263.2012 - loglik: -2.5959e+02 - logprior: -3.6082e+00
Epoch 2/10
19/19 - 3s - loss: 259.0475 - loglik: -2.5753e+02 - logprior: -1.5186e+00
Epoch 3/10
19/19 - 3s - loss: 256.5528 - loglik: -2.5585e+02 - logprior: -7.0036e-01
Epoch 4/10
19/19 - 3s - loss: 254.8908 - loglik: -2.5443e+02 - logprior: -4.5636e-01
Epoch 5/10
19/19 - 3s - loss: 253.6307 - loglik: -2.5320e+02 - logprior: -4.3410e-01
Epoch 6/10
19/19 - 3s - loss: 253.4353 - loglik: -2.5303e+02 - logprior: -4.0488e-01
Epoch 7/10
19/19 - 3s - loss: 252.2137 - loglik: -2.5185e+02 - logprior: -3.6054e-01
Epoch 8/10
19/19 - 3s - loss: 251.9296 - loglik: -2.5162e+02 - logprior: -3.1348e-01
Epoch 9/10
19/19 - 3s - loss: 252.3093 - loglik: -2.5206e+02 - logprior: -2.5261e-01
Fitted a model with MAP estimate = -251.3634
Time for alignment: 96.9134
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 359.6075 - loglik: -3.5660e+02 - logprior: -3.0067e+00
Epoch 2/10
19/19 - 3s - loss: 302.8702 - loglik: -3.0164e+02 - logprior: -1.2257e+00
Epoch 3/10
19/19 - 3s - loss: 275.7044 - loglik: -2.7434e+02 - logprior: -1.3598e+00
Epoch 4/10
19/19 - 3s - loss: 269.1852 - loglik: -2.6784e+02 - logprior: -1.3410e+00
Epoch 5/10
19/19 - 3s - loss: 267.2642 - loglik: -2.6599e+02 - logprior: -1.2763e+00
Epoch 6/10
19/19 - 3s - loss: 265.7233 - loglik: -2.6447e+02 - logprior: -1.2518e+00
Epoch 7/10
19/19 - 3s - loss: 264.7873 - loglik: -2.6355e+02 - logprior: -1.2390e+00
Epoch 8/10
19/19 - 3s - loss: 263.9120 - loglik: -2.6268e+02 - logprior: -1.2361e+00
Epoch 9/10
19/19 - 3s - loss: 264.1869 - loglik: -2.6296e+02 - logprior: -1.2272e+00
Fitted a model with MAP estimate = -263.6307
expansions: [(12, 1), (14, 3), (16, 1), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 2), (93, 1), (96, 1), (101, 1), (107, 1), (108, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 275.3793 - loglik: -2.7149e+02 - logprior: -3.8888e+00
Epoch 2/2
19/19 - 3s - loss: 260.1063 - loglik: -2.5820e+02 - logprior: -1.9018e+00
Fitted a model with MAP estimate = -257.3100
expansions: [(0, 2)]
discards: [  0  16  37  75  91 138 139]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 261.0184 - loglik: -2.5834e+02 - logprior: -2.6778e+00
Epoch 2/2
19/19 - 3s - loss: 256.4689 - loglik: -2.5556e+02 - logprior: -9.1165e-01
Fitted a model with MAP estimate = -255.3983
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 259.4835 - loglik: -2.5692e+02 - logprior: -2.5664e+00
Epoch 2/10
19/19 - 3s - loss: 256.2908 - loglik: -2.5543e+02 - logprior: -8.6082e-01
Epoch 3/10
19/19 - 3s - loss: 254.8190 - loglik: -2.5412e+02 - logprior: -6.9780e-01
Epoch 4/10
19/19 - 3s - loss: 253.8928 - loglik: -2.5326e+02 - logprior: -6.3649e-01
Epoch 5/10
19/19 - 3s - loss: 252.8595 - loglik: -2.5225e+02 - logprior: -6.1173e-01
Epoch 6/10
19/19 - 3s - loss: 252.5461 - loglik: -2.5196e+02 - logprior: -5.8145e-01
Epoch 7/10
19/19 - 3s - loss: 251.5362 - loglik: -2.5097e+02 - logprior: -5.6200e-01
Epoch 8/10
19/19 - 3s - loss: 251.1945 - loglik: -2.5065e+02 - logprior: -5.4810e-01
Epoch 9/10
19/19 - 3s - loss: 251.3623 - loglik: -2.5084e+02 - logprior: -5.2039e-01
Fitted a model with MAP estimate = -250.6501
Time for alignment: 100.2351
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 359.8061 - loglik: -3.5680e+02 - logprior: -3.0050e+00
Epoch 2/10
19/19 - 3s - loss: 299.4756 - loglik: -2.9830e+02 - logprior: -1.1716e+00
Epoch 3/10
19/19 - 3s - loss: 275.3781 - loglik: -2.7408e+02 - logprior: -1.2955e+00
Epoch 4/10
19/19 - 3s - loss: 270.2308 - loglik: -2.6897e+02 - logprior: -1.2580e+00
Epoch 5/10
19/19 - 3s - loss: 269.1019 - loglik: -2.6791e+02 - logprior: -1.1907e+00
Epoch 6/10
19/19 - 3s - loss: 266.7559 - loglik: -2.6554e+02 - logprior: -1.2120e+00
Epoch 7/10
19/19 - 3s - loss: 266.5558 - loglik: -2.6536e+02 - logprior: -1.1935e+00
Epoch 8/10
19/19 - 3s - loss: 265.5992 - loglik: -2.6441e+02 - logprior: -1.1858e+00
Epoch 9/10
19/19 - 3s - loss: 265.1609 - loglik: -2.6398e+02 - logprior: -1.1825e+00
Epoch 10/10
19/19 - 3s - loss: 265.4297 - loglik: -2.6425e+02 - logprior: -1.1815e+00
Fitted a model with MAP estimate = -265.0447
expansions: [(12, 1), (14, 4), (16, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (94, 1), (97, 1), (101, 1), (107, 1), (108, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 275.8598 - loglik: -2.7199e+02 - logprior: -3.8713e+00
Epoch 2/2
19/19 - 3s - loss: 259.9303 - loglik: -2.5805e+02 - logprior: -1.8803e+00
Fitted a model with MAP estimate = -257.4296
expansions: [(0, 3)]
discards: [  0  16  37  75 137]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 261.3784 - loglik: -2.5869e+02 - logprior: -2.6880e+00
Epoch 2/2
19/19 - 3s - loss: 256.6439 - loglik: -2.5571e+02 - logprior: -9.3196e-01
Fitted a model with MAP estimate = -255.5691
expansions: []
discards: [  0   2 137]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 262.5236 - loglik: -2.5885e+02 - logprior: -3.6686e+00
Epoch 2/10
19/19 - 3s - loss: 257.9125 - loglik: -2.5655e+02 - logprior: -1.3612e+00
Epoch 3/10
19/19 - 3s - loss: 256.1179 - loglik: -2.5548e+02 - logprior: -6.3825e-01
Epoch 4/10
19/19 - 3s - loss: 254.4466 - loglik: -2.5386e+02 - logprior: -5.8226e-01
Epoch 5/10
19/19 - 3s - loss: 253.3823 - loglik: -2.5286e+02 - logprior: -5.2467e-01
Epoch 6/10
19/19 - 3s - loss: 252.3796 - loglik: -2.5191e+02 - logprior: -4.6912e-01
Epoch 7/10
19/19 - 3s - loss: 253.4622 - loglik: -2.5303e+02 - logprior: -4.3282e-01
Fitted a model with MAP estimate = -251.9114
Time for alignment: 95.4591
Computed alignments with likelihoods: ['-251.3634', '-250.6501', '-251.9114']
Best model has likelihood: -250.6501
SP score = 0.8464
Training of 3 independent models on file PF00970.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9638cb2430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9638bff640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 274.2815 - loglik: -2.7120e+02 - logprior: -3.0792e+00
Epoch 2/10
19/19 - 2s - loss: 235.9106 - loglik: -2.3463e+02 - logprior: -1.2789e+00
Epoch 3/10
19/19 - 2s - loss: 219.7907 - loglik: -2.1835e+02 - logprior: -1.4357e+00
Epoch 4/10
19/19 - 2s - loss: 214.8078 - loglik: -2.1338e+02 - logprior: -1.4308e+00
Epoch 5/10
19/19 - 2s - loss: 213.1561 - loglik: -2.1180e+02 - logprior: -1.3515e+00
Epoch 6/10
19/19 - 2s - loss: 212.7831 - loglik: -2.1148e+02 - logprior: -1.2991e+00
Epoch 7/10
19/19 - 2s - loss: 212.2786 - loglik: -2.1099e+02 - logprior: -1.2848e+00
Epoch 8/10
19/19 - 2s - loss: 211.6435 - loglik: -2.1038e+02 - logprior: -1.2674e+00
Epoch 9/10
19/19 - 2s - loss: 211.4034 - loglik: -2.1013e+02 - logprior: -1.2772e+00
Epoch 10/10
19/19 - 2s - loss: 211.0184 - loglik: -2.0975e+02 - logprior: -1.2702e+00
Fitted a model with MAP estimate = -210.4925
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (17, 1), (18, 2), (21, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (63, 3), (64, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 224.7778 - loglik: -2.2094e+02 - logprior: -3.8414e+00
Epoch 2/2
19/19 - 2s - loss: 206.9538 - loglik: -2.0569e+02 - logprior: -1.2675e+00
Fitted a model with MAP estimate = -204.8170
expansions: [(25, 1)]
discards: [ 0 45 75 79 94]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 210.9609 - loglik: -2.0703e+02 - logprior: -3.9356e+00
Epoch 2/2
19/19 - 2s - loss: 206.1770 - loglik: -2.0475e+02 - logprior: -1.4303e+00
Fitted a model with MAP estimate = -204.5051
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 209.2852 - loglik: -2.0623e+02 - logprior: -3.0564e+00
Epoch 2/10
19/19 - 2s - loss: 205.5229 - loglik: -2.0456e+02 - logprior: -9.6127e-01
Epoch 3/10
19/19 - 2s - loss: 204.2122 - loglik: -2.0337e+02 - logprior: -8.4356e-01
Epoch 4/10
19/19 - 2s - loss: 203.5729 - loglik: -2.0275e+02 - logprior: -8.1877e-01
Epoch 5/10
19/19 - 2s - loss: 202.6570 - loglik: -2.0189e+02 - logprior: -7.7165e-01
Epoch 6/10
19/19 - 2s - loss: 202.2147 - loglik: -2.0146e+02 - logprior: -7.5252e-01
Epoch 7/10
19/19 - 2s - loss: 201.4319 - loglik: -2.0071e+02 - logprior: -7.2574e-01
Epoch 8/10
19/19 - 2s - loss: 201.5563 - loglik: -2.0084e+02 - logprior: -7.1855e-01
Fitted a model with MAP estimate = -200.9089
Time for alignment: 74.3072
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 274.4971 - loglik: -2.7142e+02 - logprior: -3.0784e+00
Epoch 2/10
19/19 - 2s - loss: 236.7247 - loglik: -2.3545e+02 - logprior: -1.2720e+00
Epoch 3/10
19/19 - 2s - loss: 219.9040 - loglik: -2.1846e+02 - logprior: -1.4451e+00
Epoch 4/10
19/19 - 2s - loss: 215.2871 - loglik: -2.1385e+02 - logprior: -1.4327e+00
Epoch 5/10
19/19 - 2s - loss: 213.7078 - loglik: -2.1234e+02 - logprior: -1.3688e+00
Epoch 6/10
19/19 - 2s - loss: 212.9491 - loglik: -2.1162e+02 - logprior: -1.3251e+00
Epoch 7/10
19/19 - 2s - loss: 212.3673 - loglik: -2.1104e+02 - logprior: -1.3229e+00
Epoch 8/10
19/19 - 2s - loss: 212.0367 - loglik: -2.1073e+02 - logprior: -1.3017e+00
Epoch 9/10
19/19 - 2s - loss: 211.7286 - loglik: -2.1041e+02 - logprior: -1.3152e+00
Epoch 10/10
19/19 - 2s - loss: 211.4562 - loglik: -2.1015e+02 - logprior: -1.3050e+00
Fitted a model with MAP estimate = -210.7290
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (16, 1), (17, 1), (18, 1), (21, 1), (28, 1), (29, 1), (35, 1), (46, 1), (54, 1), (61, 2), (63, 3), (70, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 223.7034 - loglik: -2.1985e+02 - logprior: -3.8530e+00
Epoch 2/2
19/19 - 2s - loss: 207.1766 - loglik: -2.0593e+02 - logprior: -1.2441e+00
Fitted a model with MAP estimate = -204.9723
expansions: []
discards: [ 0 78 93]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 210.9817 - loglik: -2.0702e+02 - logprior: -3.9649e+00
Epoch 2/2
19/19 - 2s - loss: 206.5087 - loglik: -2.0508e+02 - logprior: -1.4297e+00
Fitted a model with MAP estimate = -204.7316
expansions: []
discards: [ 0 22]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 209.7511 - loglik: -2.0667e+02 - logprior: -3.0832e+00
Epoch 2/10
19/19 - 2s - loss: 205.9418 - loglik: -2.0494e+02 - logprior: -1.0037e+00
Epoch 3/10
19/19 - 2s - loss: 204.7634 - loglik: -2.0387e+02 - logprior: -8.9587e-01
Epoch 4/10
19/19 - 2s - loss: 203.7939 - loglik: -2.0294e+02 - logprior: -8.5365e-01
Epoch 5/10
19/19 - 2s - loss: 203.1190 - loglik: -2.0230e+02 - logprior: -8.1694e-01
Epoch 6/10
19/19 - 2s - loss: 202.6178 - loglik: -2.0183e+02 - logprior: -7.9077e-01
Epoch 7/10
19/19 - 2s - loss: 202.2009 - loglik: -2.0143e+02 - logprior: -7.6759e-01
Epoch 8/10
19/19 - 2s - loss: 201.8282 - loglik: -2.0108e+02 - logprior: -7.4901e-01
Epoch 9/10
19/19 - 2s - loss: 201.4261 - loglik: -2.0070e+02 - logprior: -7.2356e-01
Epoch 10/10
19/19 - 2s - loss: 201.0322 - loglik: -2.0032e+02 - logprior: -7.1708e-01
Fitted a model with MAP estimate = -200.6314
Time for alignment: 78.3794
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 274.3897 - loglik: -2.7132e+02 - logprior: -3.0661e+00
Epoch 2/10
19/19 - 2s - loss: 234.7540 - loglik: -2.3351e+02 - logprior: -1.2474e+00
Epoch 3/10
19/19 - 2s - loss: 219.0144 - loglik: -2.1759e+02 - logprior: -1.4245e+00
Epoch 4/10
19/19 - 2s - loss: 214.7744 - loglik: -2.1332e+02 - logprior: -1.4504e+00
Epoch 5/10
19/19 - 2s - loss: 213.2867 - loglik: -2.1194e+02 - logprior: -1.3460e+00
Epoch 6/10
19/19 - 2s - loss: 212.7527 - loglik: -2.1145e+02 - logprior: -1.3062e+00
Epoch 7/10
19/19 - 2s - loss: 212.2345 - loglik: -2.1094e+02 - logprior: -1.2953e+00
Epoch 8/10
19/19 - 2s - loss: 211.6310 - loglik: -2.1035e+02 - logprior: -1.2805e+00
Epoch 9/10
19/19 - 2s - loss: 211.2644 - loglik: -2.0998e+02 - logprior: -1.2804e+00
Epoch 10/10
19/19 - 2s - loss: 210.9312 - loglik: -2.0966e+02 - logprior: -1.2717e+00
Fitted a model with MAP estimate = -210.3633
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (17, 1), (18, 2), (29, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (63, 3), (70, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 224.1798 - loglik: -2.2035e+02 - logprior: -3.8308e+00
Epoch 2/2
19/19 - 2s - loss: 207.0662 - loglik: -2.0579e+02 - logprior: -1.2784e+00
Fitted a model with MAP estimate = -204.7115
expansions: []
discards: [ 0 46 76 80 95]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 211.0711 - loglik: -2.0713e+02 - logprior: -3.9422e+00
Epoch 2/2
19/19 - 2s - loss: 206.2392 - loglik: -2.0479e+02 - logprior: -1.4447e+00
Fitted a model with MAP estimate = -204.6011
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 208.1494 - loglik: -2.0532e+02 - logprior: -2.8291e+00
Epoch 2/10
19/19 - 2s - loss: 204.7657 - loglik: -2.0374e+02 - logprior: -1.0303e+00
Epoch 3/10
19/19 - 2s - loss: 203.7327 - loglik: -2.0276e+02 - logprior: -9.7217e-01
Epoch 4/10
19/19 - 2s - loss: 202.8629 - loglik: -2.0194e+02 - logprior: -9.2361e-01
Epoch 5/10
19/19 - 2s - loss: 202.3627 - loglik: -2.0149e+02 - logprior: -8.7671e-01
Epoch 6/10
19/19 - 2s - loss: 201.2558 - loglik: -2.0040e+02 - logprior: -8.5509e-01
Epoch 7/10
19/19 - 2s - loss: 201.0039 - loglik: -2.0017e+02 - logprior: -8.3262e-01
Epoch 8/10
19/19 - 2s - loss: 200.6141 - loglik: -1.9979e+02 - logprior: -8.2049e-01
Epoch 9/10
19/19 - 2s - loss: 200.2898 - loglik: -1.9949e+02 - logprior: -7.9621e-01
Epoch 10/10
19/19 - 2s - loss: 199.6245 - loglik: -1.9884e+02 - logprior: -7.8382e-01
Fitted a model with MAP estimate = -199.3483
Time for alignment: 79.0275
Computed alignments with likelihoods: ['-200.9089', '-200.6314', '-199.3483']
Best model has likelihood: -199.3483
SP score = 0.6675
Training of 3 independent models on file PF00313.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96911dc190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96631601c0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.4114 - loglik: -1.7319e+02 - logprior: -3.2199e+00
Epoch 2/10
19/19 - 1s - loss: 133.2967 - loglik: -1.3186e+02 - logprior: -1.4320e+00
Epoch 3/10
19/19 - 1s - loss: 117.7343 - loglik: -1.1634e+02 - logprior: -1.3902e+00
Epoch 4/10
19/19 - 1s - loss: 114.7493 - loglik: -1.1333e+02 - logprior: -1.4189e+00
Epoch 5/10
19/19 - 1s - loss: 113.9614 - loglik: -1.1259e+02 - logprior: -1.3712e+00
Epoch 6/10
19/19 - 1s - loss: 113.3506 - loglik: -1.1201e+02 - logprior: -1.3436e+00
Epoch 7/10
19/19 - 1s - loss: 113.1469 - loglik: -1.1181e+02 - logprior: -1.3340e+00
Epoch 8/10
19/19 - 1s - loss: 112.9042 - loglik: -1.1158e+02 - logprior: -1.3216e+00
Epoch 9/10
19/19 - 1s - loss: 112.6182 - loglik: -1.1130e+02 - logprior: -1.3181e+00
Epoch 10/10
19/19 - 1s - loss: 112.9924 - loglik: -1.1168e+02 - logprior: -1.3122e+00
Fitted a model with MAP estimate = -112.6396
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (40, 3), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 114.6771 - loglik: -1.1048e+02 - logprior: -4.1999e+00
Epoch 2/2
19/19 - 1s - loss: 104.6497 - loglik: -1.0335e+02 - logprior: -1.3008e+00
Fitted a model with MAP estimate = -103.5425
expansions: []
discards: [ 0 38 54]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.4030 - loglik: -1.0541e+02 - logprior: -3.9899e+00
Epoch 2/2
19/19 - 1s - loss: 104.9530 - loglik: -1.0353e+02 - logprior: -1.4237e+00
Fitted a model with MAP estimate = -104.1601
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.5591 - loglik: -1.0446e+02 - logprior: -3.0975e+00
Epoch 2/10
19/19 - 1s - loss: 104.4723 - loglik: -1.0311e+02 - logprior: -1.3618e+00
Epoch 3/10
19/19 - 1s - loss: 103.6066 - loglik: -1.0234e+02 - logprior: -1.2669e+00
Epoch 4/10
19/19 - 1s - loss: 102.9971 - loglik: -1.0178e+02 - logprior: -1.2179e+00
Epoch 5/10
19/19 - 1s - loss: 102.4354 - loglik: -1.0125e+02 - logprior: -1.1819e+00
Epoch 6/10
19/19 - 1s - loss: 102.0900 - loglik: -1.0092e+02 - logprior: -1.1676e+00
Epoch 7/10
19/19 - 1s - loss: 101.6709 - loglik: -1.0052e+02 - logprior: -1.1508e+00
Epoch 8/10
19/19 - 1s - loss: 101.6938 - loglik: -1.0056e+02 - logprior: -1.1363e+00
Fitted a model with MAP estimate = -101.4417
Time for alignment: 46.3578
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.2774 - loglik: -1.7306e+02 - logprior: -3.2181e+00
Epoch 2/10
19/19 - 1s - loss: 133.0227 - loglik: -1.3158e+02 - logprior: -1.4430e+00
Epoch 3/10
19/19 - 1s - loss: 117.5754 - loglik: -1.1616e+02 - logprior: -1.4202e+00
Epoch 4/10
19/19 - 1s - loss: 114.8580 - loglik: -1.1342e+02 - logprior: -1.4340e+00
Epoch 5/10
19/19 - 1s - loss: 113.8623 - loglik: -1.1247e+02 - logprior: -1.3963e+00
Epoch 6/10
19/19 - 1s - loss: 113.1860 - loglik: -1.1182e+02 - logprior: -1.3696e+00
Epoch 7/10
19/19 - 1s - loss: 113.4356 - loglik: -1.1208e+02 - logprior: -1.3577e+00
Fitted a model with MAP estimate = -112.9279
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (27, 1), (28, 2), (29, 1), (30, 1), (31, 1), (34, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.0197 - loglik: -1.0994e+02 - logprior: -4.0764e+00
Epoch 2/2
19/19 - 1s - loss: 104.5587 - loglik: -1.0331e+02 - logprior: -1.2468e+00
Fitted a model with MAP estimate = -103.4517
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.2644 - loglik: -1.0525e+02 - logprior: -4.0177e+00
Epoch 2/2
19/19 - 1s - loss: 104.8783 - loglik: -1.0339e+02 - logprior: -1.4917e+00
Fitted a model with MAP estimate = -104.0084
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.4794 - loglik: -1.0432e+02 - logprior: -3.1633e+00
Epoch 2/10
19/19 - 1s - loss: 104.4902 - loglik: -1.0312e+02 - logprior: -1.3688e+00
Epoch 3/10
19/19 - 1s - loss: 103.3991 - loglik: -1.0214e+02 - logprior: -1.2635e+00
Epoch 4/10
19/19 - 1s - loss: 102.8350 - loglik: -1.0162e+02 - logprior: -1.2199e+00
Epoch 5/10
19/19 - 1s - loss: 102.4945 - loglik: -1.0130e+02 - logprior: -1.1899e+00
Epoch 6/10
19/19 - 1s - loss: 101.9719 - loglik: -1.0080e+02 - logprior: -1.1710e+00
Epoch 7/10
19/19 - 1s - loss: 101.9011 - loglik: -1.0075e+02 - logprior: -1.1530e+00
Epoch 8/10
19/19 - 1s - loss: 101.6312 - loglik: -1.0049e+02 - logprior: -1.1363e+00
Epoch 9/10
19/19 - 1s - loss: 101.3887 - loglik: -1.0027e+02 - logprior: -1.1215e+00
Epoch 10/10
19/19 - 1s - loss: 101.2615 - loglik: -1.0015e+02 - logprior: -1.1094e+00
Fitted a model with MAP estimate = -101.2871
Time for alignment: 44.1931
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.4048 - loglik: -1.7319e+02 - logprior: -3.2179e+00
Epoch 2/10
19/19 - 1s - loss: 134.4419 - loglik: -1.3298e+02 - logprior: -1.4645e+00
Epoch 3/10
19/19 - 1s - loss: 117.3933 - loglik: -1.1597e+02 - logprior: -1.4279e+00
Epoch 4/10
19/19 - 1s - loss: 114.3550 - loglik: -1.1292e+02 - logprior: -1.4374e+00
Epoch 5/10
19/19 - 1s - loss: 113.5403 - loglik: -1.1215e+02 - logprior: -1.3952e+00
Epoch 6/10
19/19 - 1s - loss: 113.0564 - loglik: -1.1169e+02 - logprior: -1.3679e+00
Epoch 7/10
19/19 - 1s - loss: 112.5725 - loglik: -1.1122e+02 - logprior: -1.3575e+00
Epoch 8/10
19/19 - 1s - loss: 112.3673 - loglik: -1.1103e+02 - logprior: -1.3416e+00
Epoch 9/10
19/19 - 1s - loss: 112.2792 - loglik: -1.1094e+02 - logprior: -1.3374e+00
Epoch 10/10
19/19 - 1s - loss: 112.4018 - loglik: -1.1107e+02 - logprior: -1.3309e+00
Fitted a model with MAP estimate = -112.1424
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (36, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.3633 - loglik: -1.1018e+02 - logprior: -4.1850e+00
Epoch 2/2
19/19 - 1s - loss: 104.5853 - loglik: -1.0332e+02 - logprior: -1.2681e+00
Fitted a model with MAP estimate = -103.5105
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.2726 - loglik: -1.0528e+02 - logprior: -3.9968e+00
Epoch 2/2
19/19 - 1s - loss: 104.9591 - loglik: -1.0349e+02 - logprior: -1.4728e+00
Fitted a model with MAP estimate = -104.0554
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.5201 - loglik: -1.0436e+02 - logprior: -3.1647e+00
Epoch 2/10
19/19 - 1s - loss: 104.3187 - loglik: -1.0295e+02 - logprior: -1.3672e+00
Epoch 3/10
19/19 - 1s - loss: 103.6510 - loglik: -1.0239e+02 - logprior: -1.2608e+00
Epoch 4/10
19/19 - 1s - loss: 102.9660 - loglik: -1.0175e+02 - logprior: -1.2151e+00
Epoch 5/10
19/19 - 1s - loss: 102.6015 - loglik: -1.0141e+02 - logprior: -1.1888e+00
Epoch 6/10
19/19 - 1s - loss: 101.8790 - loglik: -1.0071e+02 - logprior: -1.1657e+00
Epoch 7/10
19/19 - 1s - loss: 101.9009 - loglik: -1.0075e+02 - logprior: -1.1545e+00
Fitted a model with MAP estimate = -101.5498
Time for alignment: 44.0413
Computed alignments with likelihoods: ['-101.4417', '-101.2871', '-101.5498']
Best model has likelihood: -101.2871
SP score = 0.8220
Training of 3 independent models on file PF00009.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f961639d250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9638f8f760>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 171 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 616.1854 - loglik: -6.1361e+02 - logprior: -2.5746e+00
Epoch 2/10
19/19 - 10s - loss: 518.2394 - loglik: -5.1709e+02 - logprior: -1.1531e+00
Epoch 3/10
19/19 - 10s - loss: 481.0917 - loglik: -4.7982e+02 - logprior: -1.2754e+00
Epoch 4/10
19/19 - 10s - loss: 472.9258 - loglik: -4.7173e+02 - logprior: -1.1956e+00
Epoch 5/10
19/19 - 10s - loss: 468.6816 - loglik: -4.6752e+02 - logprior: -1.1566e+00
Epoch 6/10
19/19 - 10s - loss: 470.1472 - loglik: -4.6905e+02 - logprior: -1.1001e+00
Fitted a model with MAP estimate = -466.0763
expansions: [(25, 1), (30, 1), (31, 1), (32, 1), (56, 1), (75, 2), (101, 1), (102, 1), (103, 1), (105, 1), (106, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 474.0874 - loglik: -4.7129e+02 - logprior: -2.7940e+00
Epoch 2/2
19/19 - 11s - loss: 464.5003 - loglik: -4.6356e+02 - logprior: -9.3737e-01
Fitted a model with MAP estimate = -461.1307
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 471.3683 - loglik: -4.6765e+02 - logprior: -3.7227e+00
Epoch 2/2
19/19 - 11s - loss: 465.7651 - loglik: -4.6396e+02 - logprior: -1.8086e+00
Fitted a model with MAP estimate = -463.0383
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10036 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 466.3066 - loglik: -4.6360e+02 - logprior: -2.7065e+00
Epoch 2/10
19/19 - 11s - loss: 461.3558 - loglik: -4.6053e+02 - logprior: -8.2091e-01
Epoch 3/10
19/19 - 11s - loss: 461.1602 - loglik: -4.6048e+02 - logprior: -6.8516e-01
Epoch 4/10
19/19 - 11s - loss: 459.0304 - loglik: -4.5841e+02 - logprior: -6.2252e-01
Epoch 5/10
19/19 - 11s - loss: 457.4232 - loglik: -4.5686e+02 - logprior: -5.6680e-01
Epoch 6/10
19/19 - 11s - loss: 458.3456 - loglik: -4.5781e+02 - logprior: -5.3268e-01
Fitted a model with MAP estimate = -456.8293
Time for alignment: 238.7481
Fitting a model of length 171 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 616.1714 - loglik: -6.1360e+02 - logprior: -2.5741e+00
Epoch 2/10
19/19 - 10s - loss: 520.8784 - loglik: -5.1973e+02 - logprior: -1.1487e+00
Epoch 3/10
19/19 - 10s - loss: 485.1530 - loglik: -4.8395e+02 - logprior: -1.2005e+00
Epoch 4/10
19/19 - 10s - loss: 475.7016 - loglik: -4.7465e+02 - logprior: -1.0553e+00
Epoch 5/10
19/19 - 10s - loss: 472.6171 - loglik: -4.7161e+02 - logprior: -1.0092e+00
Epoch 6/10
19/19 - 10s - loss: 469.8318 - loglik: -4.6883e+02 - logprior: -1.0034e+00
Epoch 7/10
19/19 - 10s - loss: 469.4173 - loglik: -4.6841e+02 - logprior: -1.0034e+00
Epoch 8/10
19/19 - 10s - loss: 468.8316 - loglik: -4.6783e+02 - logprior: -1.0015e+00
Epoch 9/10
19/19 - 10s - loss: 467.0971 - loglik: -4.6609e+02 - logprior: -1.0084e+00
Epoch 10/10
19/19 - 10s - loss: 466.5717 - loglik: -4.6556e+02 - logprior: -1.0092e+00
Fitted a model with MAP estimate = -465.0604
expansions: [(25, 1), (30, 3), (56, 2), (57, 1), (75, 1), (81, 1), (100, 1), (101, 1), (102, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 484.1824 - loglik: -4.8047e+02 - logprior: -3.7093e+00
Epoch 2/2
19/19 - 11s - loss: 470.7931 - loglik: -4.6894e+02 - logprior: -1.8549e+00
Fitted a model with MAP estimate = -464.7881
expansions: [(0, 2), (152, 1)]
discards: [ 0 32 60 61]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 471.5443 - loglik: -4.6877e+02 - logprior: -2.7695e+00
Epoch 2/2
19/19 - 11s - loss: 465.6810 - loglik: -4.6477e+02 - logprior: -9.1148e-01
Fitted a model with MAP estimate = -462.2414
expansions: [(60, 1), (61, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10036 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 469.7960 - loglik: -4.6621e+02 - logprior: -3.5813e+00
Epoch 2/10
19/19 - 11s - loss: 463.7928 - loglik: -4.6270e+02 - logprior: -1.0909e+00
Epoch 3/10
19/19 - 11s - loss: 461.0863 - loglik: -4.6044e+02 - logprior: -6.5102e-01
Epoch 4/10
19/19 - 11s - loss: 458.7286 - loglik: -4.5811e+02 - logprior: -6.1519e-01
Epoch 5/10
19/19 - 11s - loss: 457.7097 - loglik: -4.5713e+02 - logprior: -5.7644e-01
Epoch 6/10
19/19 - 11s - loss: 458.3394 - loglik: -4.5779e+02 - logprior: -5.5004e-01
Fitted a model with MAP estimate = -457.1265
Time for alignment: 279.7871
Fitting a model of length 171 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 617.0383 - loglik: -6.1448e+02 - logprior: -2.5623e+00
Epoch 2/10
19/19 - 10s - loss: 520.1508 - loglik: -5.1907e+02 - logprior: -1.0792e+00
Epoch 3/10
19/19 - 10s - loss: 484.8647 - loglik: -4.8367e+02 - logprior: -1.1942e+00
Epoch 4/10
19/19 - 10s - loss: 475.7355 - loglik: -4.7461e+02 - logprior: -1.1255e+00
Epoch 5/10
19/19 - 10s - loss: 472.0827 - loglik: -4.7098e+02 - logprior: -1.0995e+00
Epoch 6/10
19/19 - 10s - loss: 473.3045 - loglik: -4.7222e+02 - logprior: -1.0829e+00
Fitted a model with MAP estimate = -467.9703
expansions: [(25, 1), (30, 4), (57, 1), (76, 2), (77, 1), (81, 1), (100, 1), (101, 1), (102, 1), (105, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 479.7345 - loglik: -4.7603e+02 - logprior: -3.7068e+00
Epoch 2/2
19/19 - 11s - loss: 467.0137 - loglik: -4.6525e+02 - logprior: -1.7611e+00
Fitted a model with MAP estimate = -463.4213
expansions: [(0, 2)]
discards: [ 0 31 32 33]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 470.1424 - loglik: -4.6736e+02 - logprior: -2.7830e+00
Epoch 2/2
19/19 - 11s - loss: 463.6906 - loglik: -4.6279e+02 - logprior: -8.9990e-01
Fitted a model with MAP estimate = -461.9516
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10036 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 470.3324 - loglik: -4.6677e+02 - logprior: -3.5585e+00
Epoch 2/10
19/19 - 11s - loss: 465.1162 - loglik: -4.6409e+02 - logprior: -1.0288e+00
Epoch 3/10
19/19 - 11s - loss: 461.8232 - loglik: -4.6116e+02 - logprior: -6.6725e-01
Epoch 4/10
19/19 - 11s - loss: 461.2885 - loglik: -4.6067e+02 - logprior: -6.1378e-01
Epoch 5/10
19/19 - 11s - loss: 459.7071 - loglik: -4.5912e+02 - logprior: -5.8830e-01
Epoch 6/10
19/19 - 11s - loss: 459.1163 - loglik: -4.5857e+02 - logprior: -5.4582e-01
Epoch 7/10
19/19 - 10s - loss: 458.5284 - loglik: -4.5801e+02 - logprior: -5.1876e-01
Epoch 8/10
19/19 - 11s - loss: 459.7629 - loglik: -4.5928e+02 - logprior: -4.8252e-01
Fitted a model with MAP estimate = -458.1724
Time for alignment: 258.9127
Computed alignments with likelihoods: ['-456.8293', '-457.1265', '-458.1724']
Best model has likelihood: -456.8293
SP score = 0.7760
Training of 3 independent models on file PF14604.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9691209580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96635176d0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.6879 - loglik: -1.3948e+02 - logprior: -3.2119e+00
Epoch 2/10
19/19 - 1s - loss: 113.2223 - loglik: -1.1179e+02 - logprior: -1.4278e+00
Epoch 3/10
19/19 - 1s - loss: 103.3463 - loglik: -1.0176e+02 - logprior: -1.5873e+00
Epoch 4/10
19/19 - 1s - loss: 101.0298 - loglik: -9.9591e+01 - logprior: -1.4391e+00
Epoch 5/10
19/19 - 1s - loss: 100.2836 - loglik: -9.8857e+01 - logprior: -1.4270e+00
Epoch 6/10
19/19 - 1s - loss: 100.0339 - loglik: -9.8623e+01 - logprior: -1.4106e+00
Epoch 7/10
19/19 - 1s - loss: 99.6745 - loglik: -9.8275e+01 - logprior: -1.3996e+00
Epoch 8/10
19/19 - 1s - loss: 99.6015 - loglik: -9.8210e+01 - logprior: -1.3914e+00
Epoch 9/10
19/19 - 1s - loss: 99.5254 - loglik: -9.8138e+01 - logprior: -1.3878e+00
Epoch 10/10
19/19 - 1s - loss: 99.3255 - loglik: -9.7943e+01 - logprior: -1.3828e+00
Fitted a model with MAP estimate = -99.3369
expansions: [(6, 2), (7, 2), (8, 1), (13, 2), (18, 1), (20, 1), (21, 2), (26, 1), (27, 2), (28, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 108.9900 - loglik: -1.0484e+02 - logprior: -4.1526e+00
Epoch 2/2
19/19 - 1s - loss: 99.8572 - loglik: -9.7703e+01 - logprior: -2.1540e+00
Fitted a model with MAP estimate = -97.9229
expansions: [(0, 1)]
discards: [ 0  5  8 18 28 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.1215 - loglik: -9.7934e+01 - logprior: -3.1875e+00
Epoch 2/2
19/19 - 1s - loss: 96.9015 - loglik: -9.5456e+01 - logprior: -1.4459e+00
Fitted a model with MAP estimate = -96.3874
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.5804 - loglik: -9.6337e+01 - logprior: -3.2436e+00
Epoch 2/10
19/19 - 1s - loss: 96.7528 - loglik: -9.5327e+01 - logprior: -1.4262e+00
Epoch 3/10
19/19 - 1s - loss: 96.1456 - loglik: -9.4809e+01 - logprior: -1.3361e+00
Epoch 4/10
19/19 - 1s - loss: 95.5240 - loglik: -9.4239e+01 - logprior: -1.2852e+00
Epoch 5/10
19/19 - 1s - loss: 95.1749 - loglik: -9.3926e+01 - logprior: -1.2491e+00
Epoch 6/10
19/19 - 1s - loss: 95.0955 - loglik: -9.3865e+01 - logprior: -1.2303e+00
Epoch 7/10
19/19 - 1s - loss: 94.7755 - loglik: -9.3558e+01 - logprior: -1.2177e+00
Epoch 8/10
19/19 - 1s - loss: 94.4283 - loglik: -9.3223e+01 - logprior: -1.2055e+00
Epoch 9/10
19/19 - 1s - loss: 94.6290 - loglik: -9.3440e+01 - logprior: -1.1890e+00
Fitted a model with MAP estimate = -94.4180
Time for alignment: 42.2684
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.6639 - loglik: -1.3945e+02 - logprior: -3.2117e+00
Epoch 2/10
19/19 - 1s - loss: 113.5474 - loglik: -1.1212e+02 - logprior: -1.4276e+00
Epoch 3/10
19/19 - 1s - loss: 103.2241 - loglik: -1.0164e+02 - logprior: -1.5841e+00
Epoch 4/10
19/19 - 1s - loss: 100.9733 - loglik: -9.9533e+01 - logprior: -1.4404e+00
Epoch 5/10
19/19 - 1s - loss: 100.2905 - loglik: -9.8860e+01 - logprior: -1.4302e+00
Epoch 6/10
19/19 - 1s - loss: 99.8129 - loglik: -9.8396e+01 - logprior: -1.4166e+00
Epoch 7/10
19/19 - 1s - loss: 99.5679 - loglik: -9.8166e+01 - logprior: -1.4020e+00
Epoch 8/10
19/19 - 1s - loss: 99.3911 - loglik: -9.7993e+01 - logprior: -1.3977e+00
Epoch 9/10
19/19 - 1s - loss: 99.2741 - loglik: -9.7881e+01 - logprior: -1.3927e+00
Epoch 10/10
19/19 - 1s - loss: 99.4308 - loglik: -9.8041e+01 - logprior: -1.3894e+00
Fitted a model with MAP estimate = -99.1931
expansions: [(6, 2), (7, 1), (8, 1), (13, 2), (18, 1), (20, 1), (21, 2), (26, 1), (28, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 108.5752 - loglik: -1.0443e+02 - logprior: -4.1425e+00
Epoch 2/2
19/19 - 1s - loss: 99.5320 - loglik: -9.7398e+01 - logprior: -2.1339e+00
Fitted a model with MAP estimate = -97.8211
expansions: [(0, 1)]
discards: [ 0  5 17 27 39 43]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.0694 - loglik: -9.7890e+01 - logprior: -3.1795e+00
Epoch 2/2
19/19 - 1s - loss: 96.9886 - loglik: -9.5544e+01 - logprior: -1.4449e+00
Fitted a model with MAP estimate = -96.4052
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.4936 - loglik: -9.6256e+01 - logprior: -3.2372e+00
Epoch 2/10
19/19 - 1s - loss: 96.8170 - loglik: -9.5388e+01 - logprior: -1.4287e+00
Epoch 3/10
19/19 - 1s - loss: 96.0820 - loglik: -9.4750e+01 - logprior: -1.3322e+00
Epoch 4/10
19/19 - 1s - loss: 95.6154 - loglik: -9.4336e+01 - logprior: -1.2794e+00
Epoch 5/10
19/19 - 1s - loss: 95.2122 - loglik: -9.3966e+01 - logprior: -1.2461e+00
Epoch 6/10
19/19 - 1s - loss: 95.0195 - loglik: -9.3788e+01 - logprior: -1.2314e+00
Epoch 7/10
19/19 - 1s - loss: 94.7802 - loglik: -9.3561e+01 - logprior: -1.2192e+00
Epoch 8/10
19/19 - 1s - loss: 94.5262 - loglik: -9.3322e+01 - logprior: -1.2042e+00
Epoch 9/10
19/19 - 1s - loss: 94.5590 - loglik: -9.3371e+01 - logprior: -1.1884e+00
Fitted a model with MAP estimate = -94.3828
Time for alignment: 42.1587
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.6675 - loglik: -1.3946e+02 - logprior: -3.2100e+00
Epoch 2/10
19/19 - 1s - loss: 113.5522 - loglik: -1.1213e+02 - logprior: -1.4249e+00
Epoch 3/10
19/19 - 1s - loss: 103.4272 - loglik: -1.0185e+02 - logprior: -1.5821e+00
Epoch 4/10
19/19 - 1s - loss: 101.0019 - loglik: -9.9564e+01 - logprior: -1.4379e+00
Epoch 5/10
19/19 - 1s - loss: 100.2199 - loglik: -9.8791e+01 - logprior: -1.4293e+00
Epoch 6/10
19/19 - 1s - loss: 99.9392 - loglik: -9.8525e+01 - logprior: -1.4142e+00
Epoch 7/10
19/19 - 1s - loss: 99.6153 - loglik: -9.8212e+01 - logprior: -1.4034e+00
Epoch 8/10
19/19 - 1s - loss: 99.3026 - loglik: -9.7907e+01 - logprior: -1.3960e+00
Epoch 9/10
19/19 - 1s - loss: 99.5232 - loglik: -9.8130e+01 - logprior: -1.3932e+00
Fitted a model with MAP estimate = -99.2624
expansions: [(6, 2), (7, 1), (8, 1), (13, 2), (18, 1), (20, 1), (21, 2), (26, 1), (28, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 108.4459 - loglik: -1.0430e+02 - logprior: -4.1435e+00
Epoch 2/2
19/19 - 1s - loss: 99.4772 - loglik: -9.7361e+01 - logprior: -2.1166e+00
Fitted a model with MAP estimate = -97.7479
expansions: [(0, 1)]
discards: [ 0  5 17 27 39 43]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.0178 - loglik: -9.7836e+01 - logprior: -3.1817e+00
Epoch 2/2
19/19 - 1s - loss: 97.0177 - loglik: -9.5572e+01 - logprior: -1.4452e+00
Fitted a model with MAP estimate = -96.3775
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.5161 - loglik: -9.6282e+01 - logprior: -3.2347e+00
Epoch 2/10
19/19 - 1s - loss: 96.7697 - loglik: -9.5338e+01 - logprior: -1.4312e+00
Epoch 3/10
19/19 - 1s - loss: 96.0638 - loglik: -9.4729e+01 - logprior: -1.3345e+00
Epoch 4/10
19/19 - 1s - loss: 95.7140 - loglik: -9.4434e+01 - logprior: -1.2804e+00
Epoch 5/10
19/19 - 1s - loss: 95.2670 - loglik: -9.4019e+01 - logprior: -1.2481e+00
Epoch 6/10
19/19 - 1s - loss: 94.9975 - loglik: -9.3767e+01 - logprior: -1.2306e+00
Epoch 7/10
19/19 - 1s - loss: 94.6672 - loglik: -9.3448e+01 - logprior: -1.2192e+00
Epoch 8/10
19/19 - 1s - loss: 94.5611 - loglik: -9.3356e+01 - logprior: -1.2046e+00
Epoch 9/10
19/19 - 1s - loss: 94.5474 - loglik: -9.3356e+01 - logprior: -1.1917e+00
Epoch 10/10
19/19 - 1s - loss: 94.3816 - loglik: -9.3204e+01 - logprior: -1.1779e+00
Fitted a model with MAP estimate = -94.3395
Time for alignment: 41.5319
Computed alignments with likelihoods: ['-94.4180', '-94.3828', '-94.3395']
Best model has likelihood: -94.3395
SP score = 0.7546
Training of 3 independent models on file PF00625.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95e2e34d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f961eac6940>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 457.5283 - loglik: -4.5468e+02 - logprior: -2.8465e+00
Epoch 2/10
19/19 - 4s - loss: 380.3318 - loglik: -3.7899e+02 - logprior: -1.3459e+00
Epoch 3/10
19/19 - 4s - loss: 348.5262 - loglik: -3.4688e+02 - logprior: -1.6414e+00
Epoch 4/10
19/19 - 4s - loss: 340.7093 - loglik: -3.3900e+02 - logprior: -1.7104e+00
Epoch 5/10
19/19 - 4s - loss: 337.8821 - loglik: -3.3619e+02 - logprior: -1.6950e+00
Epoch 6/10
19/19 - 4s - loss: 335.3539 - loglik: -3.3368e+02 - logprior: -1.6751e+00
Epoch 7/10
19/19 - 4s - loss: 334.1928 - loglik: -3.3251e+02 - logprior: -1.6833e+00
Epoch 8/10
19/19 - 4s - loss: 334.4095 - loglik: -3.3272e+02 - logprior: -1.6939e+00
Fitted a model with MAP estimate = -330.5244
expansions: [(0, 2), (14, 1), (16, 1), (17, 1), (18, 2), (19, 1), (21, 1), (22, 1), (36, 1), (43, 2), (44, 1), (48, 1), (49, 1), (50, 1), (51, 1), (58, 1), (65, 1), (66, 1), (68, 1), (71, 1), (80, 1), (88, 1), (89, 1), (90, 1), (100, 1), (101, 1), (113, 1), (114, 1), (115, 1), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 332.8701 - loglik: -3.2923e+02 - logprior: -3.6369e+00
Epoch 2/2
19/19 - 6s - loss: 314.6267 - loglik: -3.1360e+02 - logprior: -1.0316e+00
Fitted a model with MAP estimate = -310.0228
expansions: []
discards: [  0  54 114]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 320.9320 - loglik: -3.1713e+02 - logprior: -3.7995e+00
Epoch 2/2
19/19 - 6s - loss: 315.3331 - loglik: -3.1372e+02 - logprior: -1.6155e+00
Fitted a model with MAP estimate = -310.9415
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10140 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 314.8580 - loglik: -3.1218e+02 - logprior: -2.6771e+00
Epoch 2/10
19/19 - 6s - loss: 310.8904 - loglik: -3.1007e+02 - logprior: -8.1541e-01
Epoch 3/10
19/19 - 6s - loss: 309.2984 - loglik: -3.0858e+02 - logprior: -7.1893e-01
Epoch 4/10
19/19 - 6s - loss: 308.4547 - loglik: -3.0780e+02 - logprior: -6.5536e-01
Epoch 5/10
19/19 - 6s - loss: 307.2549 - loglik: -3.0664e+02 - logprior: -6.1442e-01
Epoch 6/10
19/19 - 6s - loss: 307.0034 - loglik: -3.0642e+02 - logprior: -5.8710e-01
Epoch 7/10
19/19 - 6s - loss: 305.5399 - loglik: -3.0498e+02 - logprior: -5.6369e-01
Epoch 8/10
19/19 - 6s - loss: 306.3034 - loglik: -3.0577e+02 - logprior: -5.3649e-01
Fitted a model with MAP estimate = -305.2133
Time for alignment: 154.0869
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 457.3453 - loglik: -4.5449e+02 - logprior: -2.8511e+00
Epoch 2/10
19/19 - 4s - loss: 378.8398 - loglik: -3.7748e+02 - logprior: -1.3564e+00
Epoch 3/10
19/19 - 4s - loss: 346.3859 - loglik: -3.4467e+02 - logprior: -1.7119e+00
Epoch 4/10
19/19 - 4s - loss: 339.8935 - loglik: -3.3809e+02 - logprior: -1.8008e+00
Epoch 5/10
19/19 - 4s - loss: 337.4243 - loglik: -3.3571e+02 - logprior: -1.7177e+00
Epoch 6/10
19/19 - 4s - loss: 336.0706 - loglik: -3.3437e+02 - logprior: -1.7013e+00
Epoch 7/10
19/19 - 4s - loss: 335.2798 - loglik: -3.3360e+02 - logprior: -1.6828e+00
Epoch 8/10
19/19 - 4s - loss: 335.1283 - loglik: -3.3346e+02 - logprior: -1.6634e+00
Epoch 9/10
19/19 - 4s - loss: 335.6415 - loglik: -3.3397e+02 - logprior: -1.6753e+00
Fitted a model with MAP estimate = -331.7703
expansions: [(0, 2), (14, 1), (16, 2), (17, 1), (18, 2), (19, 2), (21, 1), (23, 1), (36, 1), (38, 1), (44, 1), (48, 1), (50, 1), (52, 1), (57, 1), (66, 1), (67, 1), (69, 1), (70, 1), (71, 1), (80, 1), (87, 2), (88, 1), (89, 1), (99, 1), (100, 1), (111, 1), (113, 1), (114, 2), (115, 1), (116, 1), (122, 1), (123, 1), (135, 1), (136, 1), (137, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 333.6554 - loglik: -3.2996e+02 - logprior: -3.6980e+00
Epoch 2/2
19/19 - 6s - loss: 315.3092 - loglik: -3.1431e+02 - logprior: -1.0008e+00
Fitted a model with MAP estimate = -310.0191
expansions: []
discards: [  0  20  27 114]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 319.7791 - loglik: -3.1608e+02 - logprior: -3.6969e+00
Epoch 2/2
19/19 - 6s - loss: 314.6339 - loglik: -3.1351e+02 - logprior: -1.1282e+00
Fitted a model with MAP estimate = -310.1201
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10140 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 315.6687 - loglik: -3.1281e+02 - logprior: -2.8549e+00
Epoch 2/10
19/19 - 6s - loss: 311.2563 - loglik: -3.1052e+02 - logprior: -7.3686e-01
Epoch 3/10
19/19 - 6s - loss: 309.9945 - loglik: -3.0939e+02 - logprior: -6.0307e-01
Epoch 4/10
19/19 - 6s - loss: 309.1174 - loglik: -3.0856e+02 - logprior: -5.5577e-01
Epoch 5/10
19/19 - 6s - loss: 307.1796 - loglik: -3.0667e+02 - logprior: -5.0818e-01
Epoch 6/10
19/19 - 6s - loss: 308.2932 - loglik: -3.0781e+02 - logprior: -4.8380e-01
Fitted a model with MAP estimate = -306.5689
Time for alignment: 147.4608
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 457.4702 - loglik: -4.5462e+02 - logprior: -2.8534e+00
Epoch 2/10
19/19 - 4s - loss: 380.2164 - loglik: -3.7884e+02 - logprior: -1.3713e+00
Epoch 3/10
19/19 - 4s - loss: 348.1514 - loglik: -3.4642e+02 - logprior: -1.7331e+00
Epoch 4/10
19/19 - 4s - loss: 341.0338 - loglik: -3.3925e+02 - logprior: -1.7878e+00
Epoch 5/10
19/19 - 4s - loss: 338.0201 - loglik: -3.3631e+02 - logprior: -1.7102e+00
Epoch 6/10
19/19 - 4s - loss: 337.4430 - loglik: -3.3579e+02 - logprior: -1.6550e+00
Epoch 7/10
19/19 - 4s - loss: 336.9752 - loglik: -3.3533e+02 - logprior: -1.6450e+00
Epoch 8/10
19/19 - 4s - loss: 336.3908 - loglik: -3.3476e+02 - logprior: -1.6323e+00
Epoch 9/10
19/19 - 4s - loss: 336.6928 - loglik: -3.3506e+02 - logprior: -1.6344e+00
Fitted a model with MAP estimate = -332.9930
expansions: [(0, 2), (14, 1), (15, 2), (16, 1), (17, 1), (18, 1), (22, 1), (36, 1), (38, 1), (44, 1), (48, 1), (49, 1), (50, 1), (51, 1), (66, 4), (68, 1), (70, 1), (73, 1), (87, 1), (88, 1), (89, 2), (99, 1), (100, 1), (113, 2), (114, 2), (115, 1), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 186 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 334.3661 - loglik: -3.3069e+02 - logprior: -3.6752e+00
Epoch 2/2
19/19 - 6s - loss: 316.5218 - loglik: -3.1547e+02 - logprior: -1.0546e+00
Fitted a model with MAP estimate = -311.0108
expansions: []
discards: [  0  83 114 143]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 321.1245 - loglik: -3.1733e+02 - logprior: -3.7962e+00
Epoch 2/2
19/19 - 6s - loss: 315.9486 - loglik: -3.1434e+02 - logprior: -1.6131e+00
Fitted a model with MAP estimate = -310.5934
expansions: [(0, 2), (23, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10140 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 314.1568 - loglik: -3.1145e+02 - logprior: -2.7035e+00
Epoch 2/10
19/19 - 6s - loss: 309.9077 - loglik: -3.0907e+02 - logprior: -8.3616e-01
Epoch 3/10
19/19 - 6s - loss: 307.9020 - loglik: -3.0716e+02 - logprior: -7.4540e-01
Epoch 4/10
19/19 - 6s - loss: 306.9862 - loglik: -3.0630e+02 - logprior: -6.8724e-01
Epoch 5/10
19/19 - 6s - loss: 306.5303 - loglik: -3.0588e+02 - logprior: -6.4759e-01
Epoch 6/10
19/19 - 6s - loss: 304.9738 - loglik: -3.0435e+02 - logprior: -6.2270e-01
Epoch 7/10
19/19 - 6s - loss: 304.7631 - loglik: -3.0417e+02 - logprior: -5.9329e-01
Epoch 8/10
19/19 - 6s - loss: 304.6309 - loglik: -3.0406e+02 - logprior: -5.7056e-01
Epoch 9/10
19/19 - 6s - loss: 304.4580 - loglik: -3.0392e+02 - logprior: -5.3851e-01
Epoch 10/10
19/19 - 6s - loss: 303.0717 - loglik: -3.0255e+02 - logprior: -5.1691e-01
Fitted a model with MAP estimate = -303.5810
Time for alignment: 170.9754
Computed alignments with likelihoods: ['-305.2133', '-306.5689', '-303.5810']
Best model has likelihood: -303.5810
SP score = 0.3664
Training of 3 independent models on file PF00476.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f965208da60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95e396c7c0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 35s - loss: 900.4575 - loglik: -8.9907e+02 - logprior: -1.3859e+00
Epoch 2/10
39/39 - 32s - loss: 717.6398 - loglik: -7.1592e+02 - logprior: -1.7180e+00
Epoch 3/10
39/39 - 32s - loss: 701.5696 - loglik: -6.9983e+02 - logprior: -1.7420e+00
Epoch 4/10
39/39 - 32s - loss: 697.9398 - loglik: -6.9624e+02 - logprior: -1.6990e+00
Epoch 5/10
39/39 - 32s - loss: 695.9573 - loglik: -6.9422e+02 - logprior: -1.7399e+00
Epoch 6/10
39/39 - 32s - loss: 694.2388 - loglik: -6.9254e+02 - logprior: -1.7029e+00
Epoch 7/10
39/39 - 32s - loss: 693.9196 - loglik: -6.9225e+02 - logprior: -1.6736e+00
Epoch 8/10
39/39 - 32s - loss: 693.1268 - loglik: -6.9143e+02 - logprior: -1.7011e+00
Epoch 9/10
39/39 - 32s - loss: 693.4105 - loglik: -6.9167e+02 - logprior: -1.7431e+00
Fitted a model with MAP estimate = -692.1573
expansions: [(0, 15), (24, 1), (46, 1), (61, 1), (63, 2), (67, 1), (73, 1), (74, 1), (82, 1), (84, 2), (85, 1), (86, 1), (92, 1), (93, 1), (94, 1), (103, 1), (113, 1), (114, 1), (119, 1), (123, 2), (126, 2), (128, 1), (143, 1), (145, 1), (147, 1), (150, 1), (156, 1), (159, 1), (161, 2), (162, 2), (163, 1), (165, 1), (166, 1), (182, 2), (189, 1), (190, 1), (191, 1), (198, 1), (205, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (228, 1), (229, 1), (230, 1), (231, 4), (233, 1), (256, 1), (257, 1), (258, 1), (259, 2), (260, 2), (261, 3), (279, 2), (280, 1), (282, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 399 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 54s - loss: 685.1638 - loglik: -6.8294e+02 - logprior: -2.2231e+00
Epoch 2/2
39/39 - 50s - loss: 655.9042 - loglik: -6.5505e+02 - logprior: -8.5331e-01
Fitted a model with MAP estimate = -650.8555
expansions: []
discards: [  1   2   3   4   5   6   7   8   9  10  11  12 112 158 163 210 235 299
 336 338 363 378 379]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 663.5085 - loglik: -6.6215e+02 - logprior: -1.3543e+00
Epoch 2/2
39/39 - 46s - loss: 654.6405 - loglik: -6.5446e+02 - logprior: -1.8249e-01
Fitted a model with MAP estimate = -651.0442
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 661.4343 - loglik: -6.6023e+02 - logprior: -1.1994e+00
Epoch 2/10
39/39 - 46s - loss: 653.5530 - loglik: -6.5371e+02 - logprior: 0.1527
Epoch 3/10
39/39 - 46s - loss: 650.5231 - loglik: -6.5089e+02 - logprior: 0.3655
Epoch 4/10
39/39 - 46s - loss: 647.6586 - loglik: -6.4819e+02 - logprior: 0.5301
Epoch 5/10
39/39 - 46s - loss: 645.3652 - loglik: -6.4607e+02 - logprior: 0.7033
Epoch 6/10
39/39 - 46s - loss: 643.9586 - loglik: -6.4470e+02 - logprior: 0.7417
Epoch 7/10
39/39 - 46s - loss: 643.5269 - loglik: -6.4444e+02 - logprior: 0.9170
Epoch 8/10
39/39 - 46s - loss: 641.8273 - loglik: -6.4308e+02 - logprior: 1.2562
Epoch 9/10
39/39 - 46s - loss: 641.8277 - loglik: -6.4326e+02 - logprior: 1.4350
Fitted a model with MAP estimate = -641.4094
Time for alignment: 1134.5550
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 35s - loss: 900.4647 - loglik: -8.9902e+02 - logprior: -1.4466e+00
Epoch 2/10
39/39 - 32s - loss: 717.4825 - loglik: -7.1540e+02 - logprior: -2.0783e+00
Epoch 3/10
39/39 - 32s - loss: 701.6391 - loglik: -6.9956e+02 - logprior: -2.0803e+00
Epoch 4/10
39/39 - 32s - loss: 697.1711 - loglik: -6.9513e+02 - logprior: -2.0451e+00
Epoch 5/10
39/39 - 32s - loss: 694.3680 - loglik: -6.9232e+02 - logprior: -2.0449e+00
Epoch 6/10
39/39 - 32s - loss: 693.4307 - loglik: -6.9136e+02 - logprior: -2.0714e+00
Epoch 7/10
39/39 - 32s - loss: 692.6699 - loglik: -6.9060e+02 - logprior: -2.0670e+00
Epoch 8/10
39/39 - 32s - loss: 692.4424 - loglik: -6.9034e+02 - logprior: -2.1032e+00
Epoch 9/10
39/39 - 32s - loss: 691.6473 - loglik: -6.8946e+02 - logprior: -2.1875e+00
Epoch 10/10
39/39 - 32s - loss: 692.1238 - loglik: -6.9000e+02 - logprior: -2.1206e+00
Fitted a model with MAP estimate = -691.0653
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (25, 1), (45, 2), (60, 1), (62, 2), (66, 1), (72, 1), (73, 1), (79, 1), (83, 3), (84, 3), (90, 1), (91, 1), (102, 1), (112, 1), (113, 1), (118, 1), (122, 2), (125, 2), (127, 1), (142, 1), (144, 1), (146, 1), (149, 1), (155, 1), (158, 1), (160, 1), (162, 1), (163, 1), (165, 2), (168, 1), (186, 1), (188, 2), (189, 1), (190, 1), (193, 1), (205, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (229, 1), (232, 3), (233, 1), (237, 1), (256, 1), (257, 1), (258, 1), (259, 2), (260, 2), (261, 3), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 385 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 51s - loss: 679.9919 - loglik: -6.7823e+02 - logprior: -1.7634e+00
Epoch 2/2
39/39 - 48s - loss: 654.8488 - loglik: -6.5410e+02 - logprior: -7.5150e-01
Fitted a model with MAP estimate = -650.7450
expansions: []
discards: [ 97 100 147 152 231 323 325 364 365]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 662.8902 - loglik: -6.6154e+02 - logprior: -1.3514e+00
Epoch 2/2
39/39 - 46s - loss: 654.5897 - loglik: -6.5448e+02 - logprior: -1.1273e-01
Fitted a model with MAP estimate = -650.8649
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 661.1981 - loglik: -6.6018e+02 - logprior: -1.0155e+00
Epoch 2/10
39/39 - 46s - loss: 654.0139 - loglik: -6.5419e+02 - logprior: 0.1775
Epoch 3/10
39/39 - 46s - loss: 649.4859 - loglik: -6.4981e+02 - logprior: 0.3254
Epoch 4/10
39/39 - 46s - loss: 648.0015 - loglik: -6.4830e+02 - logprior: 0.3035
Epoch 5/10
39/39 - 46s - loss: 644.7399 - loglik: -6.4547e+02 - logprior: 0.7283
Epoch 6/10
39/39 - 46s - loss: 643.9686 - loglik: -6.4477e+02 - logprior: 0.8049
Epoch 7/10
39/39 - 46s - loss: 643.0777 - loglik: -6.4397e+02 - logprior: 0.8906
Epoch 8/10
39/39 - 46s - loss: 643.0482 - loglik: -6.4408e+02 - logprior: 1.0340
Epoch 9/10
39/39 - 46s - loss: 641.8477 - loglik: -6.4303e+02 - logprior: 1.1871
Epoch 10/10
39/39 - 46s - loss: 641.8520 - loglik: -6.4316e+02 - logprior: 1.3047
Fitted a model with MAP estimate = -641.1912
Time for alignment: 1203.8597
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 902.2744 - loglik: -9.0089e+02 - logprior: -1.3814e+00
Epoch 2/10
39/39 - 32s - loss: 719.9407 - loglik: -7.1822e+02 - logprior: -1.7232e+00
Epoch 3/10
39/39 - 32s - loss: 703.3124 - loglik: -7.0160e+02 - logprior: -1.7158e+00
Epoch 4/10
39/39 - 32s - loss: 699.3419 - loglik: -6.9769e+02 - logprior: -1.6569e+00
Epoch 5/10
39/39 - 32s - loss: 696.9990 - loglik: -6.9525e+02 - logprior: -1.7464e+00
Epoch 6/10
39/39 - 32s - loss: 695.7513 - loglik: -6.9407e+02 - logprior: -1.6814e+00
Epoch 7/10
39/39 - 32s - loss: 694.3439 - loglik: -6.9265e+02 - logprior: -1.6928e+00
Epoch 8/10
39/39 - 32s - loss: 694.3118 - loglik: -6.9238e+02 - logprior: -1.9277e+00
Epoch 9/10
39/39 - 32s - loss: 694.5814 - loglik: -6.9273e+02 - logprior: -1.8480e+00
Fitted a model with MAP estimate = -693.2306
expansions: [(0, 5), (41, 1), (46, 1), (55, 1), (60, 1), (62, 2), (66, 1), (73, 1), (80, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (93, 1), (103, 1), (112, 1), (113, 1), (118, 1), (122, 2), (125, 2), (141, 1), (144, 1), (146, 1), (149, 1), (151, 1), (155, 1), (158, 1), (161, 1), (162, 1), (163, 1), (165, 1), (166, 1), (173, 1), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (229, 1), (230, 1), (231, 3), (233, 1), (256, 1), (257, 1), (258, 1), (259, 2), (260, 2), (261, 3), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (287, 1), (289, 1), (296, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 384 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 50s - loss: 680.5771 - loglik: -6.7863e+02 - logprior: -1.9421e+00
Epoch 2/2
39/39 - 47s - loss: 655.3640 - loglik: -6.5481e+02 - logprior: -5.5776e-01
Fitted a model with MAP estimate = -651.2635
expansions: []
discards: [  1   2   3 148 153 232 324 326]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 663.2914 - loglik: -6.6213e+02 - logprior: -1.1626e+00
Epoch 2/2
39/39 - 46s - loss: 655.2085 - loglik: -6.5509e+02 - logprior: -1.2121e-01
Fitted a model with MAP estimate = -650.9441
expansions: [(1, 1)]
discards: [99]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 661.7566 - loglik: -6.6064e+02 - logprior: -1.1183e+00
Epoch 2/10
39/39 - 46s - loss: 653.5634 - loglik: -6.5377e+02 - logprior: 0.2083
Epoch 3/10
39/39 - 46s - loss: 650.1235 - loglik: -6.5055e+02 - logprior: 0.4287
Epoch 4/10
39/39 - 46s - loss: 647.2703 - loglik: -6.4788e+02 - logprior: 0.6129
Epoch 5/10
39/39 - 46s - loss: 644.9385 - loglik: -6.4571e+02 - logprior: 0.7762
Epoch 6/10
39/39 - 46s - loss: 644.3647 - loglik: -6.4509e+02 - logprior: 0.7245
Epoch 7/10
39/39 - 46s - loss: 642.3763 - loglik: -6.4332e+02 - logprior: 0.9451
Epoch 8/10
39/39 - 46s - loss: 642.1816 - loglik: -6.4328e+02 - logprior: 1.1015
Epoch 9/10
39/39 - 46s - loss: 641.6770 - loglik: -6.4297e+02 - logprior: 1.2949
Epoch 10/10
39/39 - 46s - loss: 642.1288 - loglik: -6.4354e+02 - logprior: 1.4110
Fitted a model with MAP estimate = -641.4823
Time for alignment: 1170.4851
Computed alignments with likelihoods: ['-641.4094', '-641.1912', '-641.4823']
Best model has likelihood: -641.1912
SP score = 0.9368
Training of 3 independent models on file PF02836.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9616434b80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9652636430>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 212 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 689.8757 - loglik: -6.8728e+02 - logprior: -2.5941e+00
Epoch 2/10
19/19 - 12s - loss: 599.4800 - loglik: -5.9838e+02 - logprior: -1.0962e+00
Epoch 3/10
19/19 - 12s - loss: 555.1152 - loglik: -5.5378e+02 - logprior: -1.3346e+00
Epoch 4/10
19/19 - 12s - loss: 542.7548 - loglik: -5.4127e+02 - logprior: -1.4862e+00
Epoch 5/10
19/19 - 12s - loss: 536.6086 - loglik: -5.3508e+02 - logprior: -1.5273e+00
Epoch 6/10
19/19 - 12s - loss: 536.1909 - loglik: -5.3466e+02 - logprior: -1.5349e+00
Epoch 7/10
19/19 - 12s - loss: 531.2586 - loglik: -5.2970e+02 - logprior: -1.5549e+00
Epoch 8/10
19/19 - 12s - loss: 530.7645 - loglik: -5.2920e+02 - logprior: -1.5604e+00
Epoch 9/10
19/19 - 12s - loss: 531.1948 - loglik: -5.2962e+02 - logprior: -1.5793e+00
Fitted a model with MAP estimate = -530.2887
expansions: [(4, 1), (6, 1), (33, 1), (82, 5), (90, 1), (91, 1), (94, 2), (118, 1), (119, 8), (120, 2), (121, 1), (122, 1), (129, 1), (130, 2), (132, 7), (135, 1), (136, 1), (138, 1), (139, 1), (140, 7), (141, 3), (142, 2), (144, 2), (145, 2), (154, 1), (156, 2), (157, 3), (158, 1), (169, 1), (170, 7), (171, 1)]
discards: [  0   1 147 159 160 161 162 163 164 165 166 167 173 174 175 176 177 178
 179 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202
 203 204 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 17s - loss: 594.0849 - loglik: -5.9032e+02 - logprior: -3.7642e+00
Epoch 2/2
19/19 - 14s - loss: 556.5174 - loglik: -5.5487e+02 - logprior: -1.6493e+00
Fitted a model with MAP estimate = -549.1343
expansions: [(0, 2), (85, 3), (86, 1), (214, 1), (238, 17)]
discards: [  0  80  81  82 133 138 139 140 141 160 176 183 184 189 193 194 201 202
 203 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236
 237]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 563.9211 - loglik: -5.6125e+02 - logprior: -2.6712e+00
Epoch 2/2
19/19 - 13s - loss: 549.1422 - loglik: -5.4826e+02 - logprior: -8.8137e-01
Fitted a model with MAP estimate = -544.3425
expansions: [(178, 1), (185, 1), (225, 17)]
discards: [  0  81  82  83 153 216 217 218 219 220 221 222 223 224]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 16s - loss: 554.0876 - loglik: -5.5067e+02 - logprior: -3.4146e+00
Epoch 2/10
19/19 - 13s - loss: 543.5858 - loglik: -5.4225e+02 - logprior: -1.3311e+00
Epoch 3/10
19/19 - 13s - loss: 539.0569 - loglik: -5.3828e+02 - logprior: -7.8131e-01
Epoch 4/10
19/19 - 13s - loss: 532.7793 - loglik: -5.3256e+02 - logprior: -2.2371e-01
Epoch 5/10
19/19 - 13s - loss: 532.5261 - loglik: -5.3232e+02 - logprior: -2.0805e-01
Epoch 6/10
19/19 - 13s - loss: 526.5303 - loglik: -5.2635e+02 - logprior: -1.7813e-01
Epoch 7/10
19/19 - 13s - loss: 524.6498 - loglik: -5.2448e+02 - logprior: -1.7265e-01
Epoch 8/10
19/19 - 13s - loss: 522.7889 - loglik: -5.2265e+02 - logprior: -1.4254e-01
Epoch 9/10
19/19 - 13s - loss: 520.2103 - loglik: -5.2007e+02 - logprior: -1.4525e-01
Epoch 10/10
19/19 - 13s - loss: 521.7490 - loglik: -5.2166e+02 - logprior: -9.3193e-02
Fitted a model with MAP estimate = -520.4637
Time for alignment: 378.7376
Fitting a model of length 212 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 690.3937 - loglik: -6.8780e+02 - logprior: -2.5916e+00
Epoch 2/10
19/19 - 12s - loss: 598.1149 - loglik: -5.9695e+02 - logprior: -1.1654e+00
Epoch 3/10
19/19 - 12s - loss: 553.1751 - loglik: -5.5172e+02 - logprior: -1.4585e+00
Epoch 4/10
19/19 - 12s - loss: 543.1917 - loglik: -5.4164e+02 - logprior: -1.5468e+00
Epoch 5/10
19/19 - 12s - loss: 536.6590 - loglik: -5.3511e+02 - logprior: -1.5477e+00
Epoch 6/10
19/19 - 12s - loss: 534.5218 - loglik: -5.3297e+02 - logprior: -1.5504e+00
Epoch 7/10
19/19 - 12s - loss: 532.1868 - loglik: -5.3061e+02 - logprior: -1.5785e+00
Epoch 8/10
19/19 - 12s - loss: 531.2758 - loglik: -5.2967e+02 - logprior: -1.6100e+00
Epoch 9/10
19/19 - 12s - loss: 529.3588 - loglik: -5.2774e+02 - logprior: -1.6174e+00
Epoch 10/10
19/19 - 12s - loss: 529.4968 - loglik: -5.2788e+02 - logprior: -1.6196e+00
Fitted a model with MAP estimate = -528.8671
expansions: [(4, 1), (6, 1), (33, 1), (40, 1), (77, 1), (81, 7), (88, 3), (89, 2), (91, 1), (115, 9), (116, 3), (117, 1), (118, 3), (127, 1), (129, 2), (130, 3), (134, 1), (137, 1), (138, 1), (140, 1), (141, 8), (145, 1), (147, 1), (152, 2), (155, 1), (156, 2), (157, 1), (160, 2), (161, 3), (171, 2), (172, 4), (174, 1)]
discards: [  0   1  78 163 164 165 166 167 168 169 175 176 177 178 179 180 181 182
 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200
 201 202 203 204 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 17s - loss: 598.6752 - loglik: -5.9495e+02 - logprior: -3.7265e+00
Epoch 2/2
19/19 - 14s - loss: 557.2518 - loglik: -5.5555e+02 - logprior: -1.7057e+00
Fitted a model with MAP estimate = -548.3205
expansions: [(97, 1), (221, 2), (222, 1), (234, 1), (237, 9)]
discards: [  0  79  80  81  90  91 101 137 138 139 140 143 147 163 178 181 182 203
 211 212 218 219 223 224]
Re-initialized the encoder parameters.
Fitting a model of length 227 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 556.2692 - loglik: -5.5271e+02 - logprior: -3.5613e+00
Epoch 2/2
19/19 - 13s - loss: 544.0466 - loglik: -5.4242e+02 - logprior: -1.6248e+00
Fitted a model with MAP estimate = -541.2195
expansions: [(0, 2), (2, 1), (81, 3), (167, 1), (194, 1), (198, 8), (201, 1)]
discards: [  0  84  85 147 187 203 204 218 219 220 221 222 223 224 225 226]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 16s - loss: 550.6951 - loglik: -5.4807e+02 - logprior: -2.6276e+00
Epoch 2/10
19/19 - 13s - loss: 541.1000 - loglik: -5.4030e+02 - logprior: -7.9884e-01
Epoch 3/10
19/19 - 13s - loss: 537.4197 - loglik: -5.3700e+02 - logprior: -4.2464e-01
Epoch 4/10
19/19 - 13s - loss: 533.5443 - loglik: -5.3322e+02 - logprior: -3.2044e-01
Epoch 5/10
19/19 - 13s - loss: 529.7270 - loglik: -5.2952e+02 - logprior: -2.0216e-01
Epoch 6/10
19/19 - 13s - loss: 526.2851 - loglik: -5.2611e+02 - logprior: -1.7035e-01
Epoch 7/10
19/19 - 13s - loss: 523.5797 - loglik: -5.2346e+02 - logprior: -1.2098e-01
Epoch 8/10
19/19 - 13s - loss: 520.7997 - loglik: -5.2068e+02 - logprior: -1.2090e-01
Epoch 9/10
19/19 - 13s - loss: 522.0472 - loglik: -5.2196e+02 - logprior: -8.5398e-02
Fitted a model with MAP estimate = -519.7324
Time for alignment: 375.6872
Fitting a model of length 212 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 690.5107 - loglik: -6.8791e+02 - logprior: -2.6022e+00
Epoch 2/10
19/19 - 11s - loss: 600.1128 - loglik: -5.9897e+02 - logprior: -1.1438e+00
Epoch 3/10
19/19 - 12s - loss: 553.9523 - loglik: -5.5261e+02 - logprior: -1.3466e+00
Epoch 4/10
19/19 - 12s - loss: 541.7974 - loglik: -5.4033e+02 - logprior: -1.4664e+00
Epoch 5/10
19/19 - 12s - loss: 536.3862 - loglik: -5.3488e+02 - logprior: -1.5059e+00
Epoch 6/10
19/19 - 12s - loss: 532.9343 - loglik: -5.3142e+02 - logprior: -1.5188e+00
Epoch 7/10
19/19 - 12s - loss: 531.6932 - loglik: -5.3016e+02 - logprior: -1.5286e+00
Epoch 8/10
19/19 - 12s - loss: 531.0477 - loglik: -5.2952e+02 - logprior: -1.5238e+00
Epoch 9/10
19/19 - 12s - loss: 529.7557 - loglik: -5.2821e+02 - logprior: -1.5478e+00
Epoch 10/10
19/19 - 12s - loss: 527.9793 - loglik: -5.2643e+02 - logprior: -1.5500e+00
Fitted a model with MAP estimate = -528.3757
expansions: [(4, 1), (6, 1), (33, 1), (52, 1), (86, 1), (118, 1), (119, 9), (120, 3), (121, 1), (122, 3), (130, 2), (132, 7), (135, 1), (136, 1), (139, 1), (140, 3), (141, 1), (142, 8), (143, 1), (148, 1), (157, 1), (159, 6), (160, 1), (169, 1), (170, 2), (171, 5), (173, 1), (184, 1)]
discards: [  0   1 150 151 152 153 162 163 164 165 166 167 177 178 179 180 186 187
 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205
 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 597.9734 - loglik: -5.9429e+02 - logprior: -3.6838e+00
Epoch 2/2
19/19 - 14s - loss: 558.0204 - loglik: -5.5640e+02 - logprior: -1.6238e+00
Fitted a model with MAP estimate = -550.8142
expansions: [(84, 6), (86, 3), (236, 20)]
discards: [  0 130 131 132 133 136 140 151 155 157 171 172 173 175 176 177 180 203
 204 205 210 211 212 215 216 217 224 225 227 228 229 230 231 232 233]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 561.1074 - loglik: -5.5748e+02 - logprior: -3.6304e+00
Epoch 2/2
19/19 - 13s - loss: 546.5392 - loglik: -5.4483e+02 - logprior: -1.7127e+00
Fitted a model with MAP estimate = -541.9643
expansions: [(0, 2), (2, 1), (85, 2), (194, 9), (195, 1), (202, 5), (230, 3)]
discards: [  0  79  80  81  82  83  92 204 205 206 207 208 209 210 211 212 213 214
 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229]
Re-initialized the encoder parameters.
Fitting a model of length 220 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 557.5176 - loglik: -5.5484e+02 - logprior: -2.6785e+00
Epoch 2/10
19/19 - 12s - loss: 546.3572 - loglik: -5.4548e+02 - logprior: -8.7933e-01
Epoch 3/10
19/19 - 12s - loss: 542.7532 - loglik: -5.4224e+02 - logprior: -5.1540e-01
Epoch 4/10
19/19 - 12s - loss: 538.9204 - loglik: -5.3850e+02 - logprior: -4.1573e-01
Epoch 5/10
19/19 - 12s - loss: 535.3246 - loglik: -5.3498e+02 - logprior: -3.4862e-01
Epoch 6/10
19/19 - 12s - loss: 531.5402 - loglik: -5.3121e+02 - logprior: -3.3043e-01
Epoch 7/10
19/19 - 12s - loss: 529.5453 - loglik: -5.2927e+02 - logprior: -2.7653e-01
Epoch 8/10
19/19 - 12s - loss: 528.8299 - loglik: -5.2856e+02 - logprior: -2.7298e-01
Epoch 9/10
19/19 - 12s - loss: 526.1778 - loglik: -5.2594e+02 - logprior: -2.3703e-01
Epoch 10/10
19/19 - 12s - loss: 526.9238 - loglik: -5.2670e+02 - logprior: -2.2853e-01
Fitted a model with MAP estimate = -524.5792
Time for alignment: 382.4602
Computed alignments with likelihoods: ['-520.4637', '-519.7324', '-524.5792']
Best model has likelihood: -519.7324
SP score = 0.9257
Training of 3 independent models on file PF02777.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9649d36820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9690812d00>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 269.7603 - loglik: -2.6663e+02 - logprior: -3.1284e+00
Epoch 2/10
19/19 - 2s - loss: 199.7646 - loglik: -1.9842e+02 - logprior: -1.3428e+00
Epoch 3/10
19/19 - 2s - loss: 175.5476 - loglik: -1.7382e+02 - logprior: -1.7233e+00
Epoch 4/10
19/19 - 2s - loss: 171.5350 - loglik: -1.6987e+02 - logprior: -1.6601e+00
Epoch 5/10
19/19 - 2s - loss: 170.4928 - loglik: -1.6891e+02 - logprior: -1.5843e+00
Epoch 6/10
19/19 - 2s - loss: 170.0184 - loglik: -1.6844e+02 - logprior: -1.5795e+00
Epoch 7/10
19/19 - 2s - loss: 169.1873 - loglik: -1.6763e+02 - logprior: -1.5603e+00
Epoch 8/10
19/19 - 2s - loss: 169.3487 - loglik: -1.6779e+02 - logprior: -1.5556e+00
Fitted a model with MAP estimate = -168.9847
expansions: [(4, 1), (6, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 165.0344 - loglik: -1.6207e+02 - logprior: -2.9606e+00
Epoch 2/2
19/19 - 2s - loss: 153.2854 - loglik: -1.5210e+02 - logprior: -1.1851e+00
Fitted a model with MAP estimate = -152.1541
expansions: []
discards: [42 61 62]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 155.9113 - loglik: -1.5300e+02 - logprior: -2.9106e+00
Epoch 2/2
19/19 - 2s - loss: 152.6391 - loglik: -1.5153e+02 - logprior: -1.1072e+00
Fitted a model with MAP estimate = -151.8016
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 160.1868 - loglik: -1.5638e+02 - logprior: -3.8076e+00
Epoch 2/10
19/19 - 2s - loss: 156.2578 - loglik: -1.5433e+02 - logprior: -1.9232e+00
Epoch 3/10
19/19 - 2s - loss: 155.7873 - loglik: -1.5401e+02 - logprior: -1.7757e+00
Epoch 4/10
19/19 - 2s - loss: 153.8686 - loglik: -1.5250e+02 - logprior: -1.3655e+00
Epoch 5/10
19/19 - 2s - loss: 153.5229 - loglik: -1.5273e+02 - logprior: -7.8917e-01
Epoch 6/10
19/19 - 2s - loss: 151.2933 - loglik: -1.5040e+02 - logprior: -8.9601e-01
Epoch 7/10
19/19 - 2s - loss: 150.4884 - loglik: -1.4956e+02 - logprior: -9.2584e-01
Epoch 8/10
19/19 - 2s - loss: 150.3721 - loglik: -1.4945e+02 - logprior: -9.2037e-01
Epoch 9/10
19/19 - 2s - loss: 150.0167 - loglik: -1.4910e+02 - logprior: -9.1807e-01
Epoch 10/10
19/19 - 2s - loss: 150.3700 - loglik: -1.4947e+02 - logprior: -9.0073e-01
Fitted a model with MAP estimate = -149.9223
Time for alignment: 76.6699
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 269.7520 - loglik: -2.6663e+02 - logprior: -3.1236e+00
Epoch 2/10
19/19 - 2s - loss: 200.3027 - loglik: -1.9896e+02 - logprior: -1.3461e+00
Epoch 3/10
19/19 - 2s - loss: 177.9761 - loglik: -1.7628e+02 - logprior: -1.6966e+00
Epoch 4/10
19/19 - 2s - loss: 172.7456 - loglik: -1.7112e+02 - logprior: -1.6227e+00
Epoch 5/10
19/19 - 2s - loss: 171.5646 - loglik: -1.6998e+02 - logprior: -1.5799e+00
Epoch 6/10
19/19 - 2s - loss: 170.6720 - loglik: -1.6909e+02 - logprior: -1.5819e+00
Epoch 7/10
19/19 - 2s - loss: 170.4615 - loglik: -1.6889e+02 - logprior: -1.5734e+00
Epoch 8/10
19/19 - 2s - loss: 169.6628 - loglik: -1.6810e+02 - logprior: -1.5648e+00
Epoch 9/10
19/19 - 2s - loss: 170.0993 - loglik: -1.6853e+02 - logprior: -1.5658e+00
Fitted a model with MAP estimate = -169.5409
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 2), (71, 2), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 169.8145 - loglik: -1.6587e+02 - logprior: -3.9457e+00
Epoch 2/2
19/19 - 2s - loss: 156.6537 - loglik: -1.5461e+02 - logprior: -2.0467e+00
Fitted a model with MAP estimate = -154.4286
expansions: [(0, 2)]
discards: [ 0 42 61 62]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 156.5383 - loglik: -1.5362e+02 - logprior: -2.9220e+00
Epoch 2/2
19/19 - 2s - loss: 152.0342 - loglik: -1.5092e+02 - logprior: -1.1141e+00
Fitted a model with MAP estimate = -151.2089
expansions: []
discards: [ 0 91]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 158.9897 - loglik: -1.5518e+02 - logprior: -3.8086e+00
Epoch 2/10
19/19 - 2s - loss: 153.4693 - loglik: -1.5205e+02 - logprior: -1.4165e+00
Epoch 3/10
19/19 - 2s - loss: 152.1651 - loglik: -1.5116e+02 - logprior: -1.0057e+00
Epoch 4/10
19/19 - 2s - loss: 151.0864 - loglik: -1.5011e+02 - logprior: -9.7151e-01
Epoch 5/10
19/19 - 2s - loss: 150.5299 - loglik: -1.4957e+02 - logprior: -9.6131e-01
Epoch 6/10
19/19 - 2s - loss: 150.0169 - loglik: -1.4907e+02 - logprior: -9.4589e-01
Epoch 7/10
19/19 - 2s - loss: 149.6883 - loglik: -1.4876e+02 - logprior: -9.2405e-01
Epoch 8/10
19/19 - 2s - loss: 149.9330 - loglik: -1.4902e+02 - logprior: -9.0987e-01
Fitted a model with MAP estimate = -149.4660
Time for alignment: 73.2888
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 269.8927 - loglik: -2.6677e+02 - logprior: -3.1223e+00
Epoch 2/10
19/19 - 2s - loss: 200.2854 - loglik: -1.9894e+02 - logprior: -1.3480e+00
Epoch 3/10
19/19 - 2s - loss: 176.5529 - loglik: -1.7484e+02 - logprior: -1.7121e+00
Epoch 4/10
19/19 - 2s - loss: 172.4284 - loglik: -1.7079e+02 - logprior: -1.6340e+00
Epoch 5/10
19/19 - 2s - loss: 171.4883 - loglik: -1.6991e+02 - logprior: -1.5820e+00
Epoch 6/10
19/19 - 2s - loss: 170.8073 - loglik: -1.6922e+02 - logprior: -1.5843e+00
Epoch 7/10
19/19 - 2s - loss: 170.3402 - loglik: -1.6877e+02 - logprior: -1.5741e+00
Epoch 8/10
19/19 - 2s - loss: 169.8627 - loglik: -1.6829e+02 - logprior: -1.5708e+00
Epoch 9/10
19/19 - 2s - loss: 169.6487 - loglik: -1.6808e+02 - logprior: -1.5673e+00
Epoch 10/10
19/19 - 2s - loss: 169.6175 - loglik: -1.6804e+02 - logprior: -1.5735e+00
Fitted a model with MAP estimate = -169.4009
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 2), (71, 2), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 169.6181 - loglik: -1.6566e+02 - logprior: -3.9586e+00
Epoch 2/2
19/19 - 2s - loss: 156.7626 - loglik: -1.5470e+02 - logprior: -2.0593e+00
Fitted a model with MAP estimate = -154.4467
expansions: [(0, 2)]
discards: [ 0 42 61 62]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 156.4847 - loglik: -1.5357e+02 - logprior: -2.9180e+00
Epoch 2/2
19/19 - 2s - loss: 152.0232 - loglik: -1.5091e+02 - logprior: -1.1136e+00
Fitted a model with MAP estimate = -151.2264
expansions: []
discards: [ 0 91]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 159.0018 - loglik: -1.5516e+02 - logprior: -3.8453e+00
Epoch 2/10
19/19 - 2s - loss: 153.7491 - loglik: -1.5227e+02 - logprior: -1.4761e+00
Epoch 3/10
19/19 - 2s - loss: 152.3155 - loglik: -1.5130e+02 - logprior: -1.0126e+00
Epoch 4/10
19/19 - 2s - loss: 151.1736 - loglik: -1.5020e+02 - logprior: -9.7508e-01
Epoch 5/10
19/19 - 2s - loss: 150.9929 - loglik: -1.5003e+02 - logprior: -9.6011e-01
Epoch 6/10
19/19 - 2s - loss: 150.2250 - loglik: -1.4928e+02 - logprior: -9.4728e-01
Epoch 7/10
19/19 - 2s - loss: 149.9663 - loglik: -1.4904e+02 - logprior: -9.2819e-01
Epoch 8/10
19/19 - 2s - loss: 150.1246 - loglik: -1.4920e+02 - logprior: -9.2235e-01
Fitted a model with MAP estimate = -149.6863
Time for alignment: 76.2782
Computed alignments with likelihoods: ['-149.9223', '-149.4660', '-149.6863']
Best model has likelihood: -149.4660
SP score = 0.9228
Training of 3 independent models on file PF07654.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f961ed16f70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96302c1d90>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 232.1303 - loglik: -2.2900e+02 - logprior: -3.1274e+00
Epoch 2/10
19/19 - 1s - loss: 196.3636 - loglik: -1.9505e+02 - logprior: -1.3104e+00
Epoch 3/10
19/19 - 1s - loss: 183.5939 - loglik: -1.8221e+02 - logprior: -1.3805e+00
Epoch 4/10
19/19 - 1s - loss: 180.3595 - loglik: -1.7902e+02 - logprior: -1.3382e+00
Epoch 5/10
19/19 - 1s - loss: 179.1992 - loglik: -1.7787e+02 - logprior: -1.3246e+00
Epoch 6/10
19/19 - 1s - loss: 178.5631 - loglik: -1.7727e+02 - logprior: -1.2951e+00
Epoch 7/10
19/19 - 1s - loss: 178.2500 - loglik: -1.7696e+02 - logprior: -1.2907e+00
Epoch 8/10
19/19 - 1s - loss: 177.8463 - loglik: -1.7656e+02 - logprior: -1.2868e+00
Epoch 9/10
19/19 - 1s - loss: 178.1539 - loglik: -1.7687e+02 - logprior: -1.2845e+00
Fitted a model with MAP estimate = -177.6805
expansions: [(7, 2), (8, 2), (9, 3), (21, 1), (22, 1), (23, 1), (32, 1), (34, 1), (35, 1), (49, 1), (50, 1), (53, 2), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 183.9007 - loglik: -1.8002e+02 - logprior: -3.8845e+00
Epoch 2/2
19/19 - 1s - loss: 175.7988 - loglik: -1.7373e+02 - logprior: -2.0668e+00
Fitted a model with MAP estimate = -174.0919
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 176.1011 - loglik: -1.7319e+02 - logprior: -2.9076e+00
Epoch 2/2
19/19 - 1s - loss: 172.7211 - loglik: -1.7158e+02 - logprior: -1.1400e+00
Fitted a model with MAP estimate = -171.7415
expansions: [(8, 1)]
discards: [ 0 68 69]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 177.3932 - loglik: -1.7371e+02 - logprior: -3.6861e+00
Epoch 2/10
19/19 - 1s - loss: 173.3480 - loglik: -1.7210e+02 - logprior: -1.2486e+00
Epoch 3/10
19/19 - 1s - loss: 171.7515 - loglik: -1.7072e+02 - logprior: -1.0274e+00
Epoch 4/10
19/19 - 1s - loss: 171.2335 - loglik: -1.7025e+02 - logprior: -9.8729e-01
Epoch 5/10
19/19 - 1s - loss: 170.1015 - loglik: -1.6914e+02 - logprior: -9.6390e-01
Epoch 6/10
19/19 - 1s - loss: 170.0808 - loglik: -1.6914e+02 - logprior: -9.4294e-01
Epoch 7/10
19/19 - 1s - loss: 169.1125 - loglik: -1.6817e+02 - logprior: -9.3974e-01
Epoch 8/10
19/19 - 1s - loss: 169.1891 - loglik: -1.6826e+02 - logprior: -9.2991e-01
Fitted a model with MAP estimate = -168.8797
Time for alignment: 53.2096
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 231.4187 - loglik: -2.2829e+02 - logprior: -3.1297e+00
Epoch 2/10
19/19 - 1s - loss: 194.7860 - loglik: -1.9349e+02 - logprior: -1.2997e+00
Epoch 3/10
19/19 - 1s - loss: 184.0107 - loglik: -1.8266e+02 - logprior: -1.3549e+00
Epoch 4/10
19/19 - 1s - loss: 181.0729 - loglik: -1.7977e+02 - logprior: -1.3018e+00
Epoch 5/10
19/19 - 1s - loss: 180.0730 - loglik: -1.7877e+02 - logprior: -1.3038e+00
Epoch 6/10
19/19 - 1s - loss: 179.2967 - loglik: -1.7801e+02 - logprior: -1.2866e+00
Epoch 7/10
19/19 - 1s - loss: 178.5021 - loglik: -1.7721e+02 - logprior: -1.2872e+00
Epoch 8/10
19/19 - 1s - loss: 178.6045 - loglik: -1.7732e+02 - logprior: -1.2806e+00
Fitted a model with MAP estimate = -178.3564
expansions: [(7, 2), (8, 3), (9, 2), (23, 1), (32, 2), (33, 1), (34, 1), (35, 1), (38, 1), (47, 1), (48, 1), (49, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 183.4065 - loglik: -1.7952e+02 - logprior: -3.8836e+00
Epoch 2/2
19/19 - 1s - loss: 175.5421 - loglik: -1.7346e+02 - logprior: -2.0779e+00
Fitted a model with MAP estimate = -173.6702
expansions: [(0, 2), (10, 1)]
discards: [ 0 40 72]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 176.3690 - loglik: -1.7351e+02 - logprior: -2.8612e+00
Epoch 2/2
19/19 - 1s - loss: 172.6011 - loglik: -1.7153e+02 - logprior: -1.0755e+00
Fitted a model with MAP estimate = -171.6904
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 176.8420 - loglik: -1.7326e+02 - logprior: -3.5774e+00
Epoch 2/10
19/19 - 1s - loss: 172.8704 - loglik: -1.7168e+02 - logprior: -1.1861e+00
Epoch 3/10
19/19 - 1s - loss: 171.5936 - loglik: -1.7058e+02 - logprior: -1.0113e+00
Epoch 4/10
19/19 - 1s - loss: 170.7464 - loglik: -1.6978e+02 - logprior: -9.7003e-01
Epoch 5/10
19/19 - 1s - loss: 169.8326 - loglik: -1.6889e+02 - logprior: -9.4578e-01
Epoch 6/10
19/19 - 1s - loss: 169.5203 - loglik: -1.6860e+02 - logprior: -9.2484e-01
Epoch 7/10
19/19 - 1s - loss: 168.9981 - loglik: -1.6808e+02 - logprior: -9.2103e-01
Epoch 8/10
19/19 - 1s - loss: 168.5910 - loglik: -1.6768e+02 - logprior: -9.0901e-01
Epoch 9/10
19/19 - 1s - loss: 168.4527 - loglik: -1.6755e+02 - logprior: -9.0082e-01
Epoch 10/10
19/19 - 1s - loss: 168.3967 - loglik: -1.6751e+02 - logprior: -8.8889e-01
Fitted a model with MAP estimate = -168.1881
Time for alignment: 54.1268
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 231.9975 - loglik: -2.2887e+02 - logprior: -3.1285e+00
Epoch 2/10
19/19 - 1s - loss: 195.2334 - loglik: -1.9393e+02 - logprior: -1.3014e+00
Epoch 3/10
19/19 - 1s - loss: 183.8624 - loglik: -1.8251e+02 - logprior: -1.3527e+00
Epoch 4/10
19/19 - 1s - loss: 181.1316 - loglik: -1.7981e+02 - logprior: -1.3189e+00
Epoch 5/10
19/19 - 1s - loss: 179.8758 - loglik: -1.7857e+02 - logprior: -1.3108e+00
Epoch 6/10
19/19 - 1s - loss: 179.4162 - loglik: -1.7813e+02 - logprior: -1.2864e+00
Epoch 7/10
19/19 - 1s - loss: 178.9037 - loglik: -1.7762e+02 - logprior: -1.2858e+00
Epoch 8/10
19/19 - 1s - loss: 178.3634 - loglik: -1.7708e+02 - logprior: -1.2788e+00
Epoch 9/10
19/19 - 1s - loss: 178.5513 - loglik: -1.7728e+02 - logprior: -1.2733e+00
Fitted a model with MAP estimate = -178.0964
expansions: [(7, 2), (8, 3), (9, 2), (23, 1), (31, 1), (32, 1), (34, 1), (35, 1), (37, 1), (46, 1), (48, 1), (52, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 183.6274 - loglik: -1.7975e+02 - logprior: -3.8775e+00
Epoch 2/2
19/19 - 1s - loss: 175.7001 - loglik: -1.7365e+02 - logprior: -2.0551e+00
Fitted a model with MAP estimate = -174.2210
expansions: [(0, 2)]
discards: [ 0 71]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 177.0123 - loglik: -1.7414e+02 - logprior: -2.8691e+00
Epoch 2/2
19/19 - 1s - loss: 173.5186 - loglik: -1.7241e+02 - logprior: -1.1118e+00
Fitted a model with MAP estimate = -172.7298
expansions: []
discards: [ 0 11]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 178.5350 - loglik: -1.7494e+02 - logprior: -3.5921e+00
Epoch 2/10
19/19 - 1s - loss: 174.3527 - loglik: -1.7314e+02 - logprior: -1.2132e+00
Epoch 3/10
19/19 - 1s - loss: 173.0618 - loglik: -1.7204e+02 - logprior: -1.0264e+00
Epoch 4/10
19/19 - 1s - loss: 172.0654 - loglik: -1.7107e+02 - logprior: -9.9233e-01
Epoch 5/10
19/19 - 1s - loss: 171.9335 - loglik: -1.7097e+02 - logprior: -9.6546e-01
Epoch 6/10
19/19 - 1s - loss: 170.8203 - loglik: -1.6987e+02 - logprior: -9.5072e-01
Epoch 7/10
19/19 - 1s - loss: 170.7721 - loglik: -1.6983e+02 - logprior: -9.3948e-01
Epoch 8/10
19/19 - 1s - loss: 170.3005 - loglik: -1.6940e+02 - logprior: -9.0156e-01
Epoch 9/10
19/19 - 1s - loss: 170.2265 - loglik: -1.6934e+02 - logprior: -8.8589e-01
Epoch 10/10
19/19 - 1s - loss: 169.7846 - loglik: -1.6891e+02 - logprior: -8.7328e-01
Fitted a model with MAP estimate = -169.8773
Time for alignment: 53.8087
Computed alignments with likelihoods: ['-168.8797', '-168.1881', '-169.8773']
Best model has likelihood: -168.1881
SP score = 0.7959
Training of 3 independent models on file PF04082.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f961ebf0e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96903e4d90>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 665.7407 - loglik: -6.6311e+02 - logprior: -2.6287e+00
Epoch 2/10
19/19 - 11s - loss: 616.3003 - loglik: -6.1540e+02 - logprior: -9.0164e-01
Epoch 3/10
19/19 - 11s - loss: 585.1398 - loglik: -5.8409e+02 - logprior: -1.0477e+00
Epoch 4/10
19/19 - 11s - loss: 570.4276 - loglik: -5.6931e+02 - logprior: -1.1191e+00
Epoch 5/10
19/19 - 11s - loss: 562.9949 - loglik: -5.6185e+02 - logprior: -1.1479e+00
Epoch 6/10
19/19 - 11s - loss: 558.6930 - loglik: -5.5751e+02 - logprior: -1.1810e+00
Epoch 7/10
19/19 - 11s - loss: 555.4189 - loglik: -5.5418e+02 - logprior: -1.2437e+00
Epoch 8/10
19/19 - 11s - loss: 553.0109 - loglik: -5.5174e+02 - logprior: -1.2682e+00
Epoch 9/10
19/19 - 12s - loss: 554.5814 - loglik: -5.5331e+02 - logprior: -1.2699e+00
Fitted a model with MAP estimate = -552.2143
expansions: [(23, 1), (24, 2), (50, 4), (60, 1), (63, 2), (66, 1), (82, 1), (83, 1), (84, 2), (85, 2), (97, 1), (105, 1), (106, 2), (109, 1), (110, 1), (121, 1), (124, 1), (127, 1), (134, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 218 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 17s - loss: 657.0249 - loglik: -6.5310e+02 - logprior: -3.9265e+00
Epoch 2/2
19/19 - 13s - loss: 605.2684 - loglik: -6.0324e+02 - logprior: -2.0263e+00
Fitted a model with MAP estimate = -588.2610
expansions: [(0, 4), (25, 3), (30, 2), (49, 2), (53, 1), (54, 2), (70, 1), (71, 1), (122, 1), (180, 2), (181, 1), (218, 13)]
discards: [  0  26  27  28  50  51  56  57  58  59  67  72  93  98 123]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 18s - loss: 599.3860 - loglik: -5.9656e+02 - logprior: -2.8215e+00
Epoch 2/2
19/19 - 15s - loss: 583.3836 - loglik: -5.8240e+02 - logprior: -9.8821e-01
Fitted a model with MAP estimate = -577.3206
expansions: [(27, 1), (33, 2), (34, 1), (98, 1), (179, 1), (188, 2), (190, 1), (205, 1)]
discards: [  1   2   3  28  29  30  62 222 223 224 225 226 227 228 229 230 231 232
 233 234 235]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 17s - loss: 591.9188 - loglik: -5.8946e+02 - logprior: -2.4622e+00
Epoch 2/10
19/19 - 14s - loss: 581.3071 - loglik: -5.8084e+02 - logprior: -4.6716e-01
Epoch 3/10
19/19 - 14s - loss: 572.0283 - loglik: -5.7179e+02 - logprior: -2.3485e-01
Epoch 4/10
19/19 - 14s - loss: 561.3088 - loglik: -5.6111e+02 - logprior: -1.9582e-01
Epoch 5/10
19/19 - 14s - loss: 551.5319 - loglik: -5.5130e+02 - logprior: -2.3568e-01
Epoch 6/10
19/19 - 14s - loss: 543.3307 - loglik: -5.4299e+02 - logprior: -3.3715e-01
Epoch 7/10
19/19 - 14s - loss: 539.1966 - loglik: -5.3877e+02 - logprior: -4.2909e-01
Epoch 8/10
19/19 - 14s - loss: 537.2072 - loglik: -5.3666e+02 - logprior: -5.4881e-01
Epoch 9/10
19/19 - 14s - loss: 536.2618 - loglik: -5.3565e+02 - logprior: -6.1228e-01
Epoch 10/10
19/19 - 14s - loss: 532.9127 - loglik: -5.3224e+02 - logprior: -6.7481e-01
Fitted a model with MAP estimate = -534.6035
Time for alignment: 389.2854
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 665.7146 - loglik: -6.6308e+02 - logprior: -2.6325e+00
Epoch 2/10
19/19 - 11s - loss: 616.3643 - loglik: -6.1547e+02 - logprior: -8.9512e-01
Epoch 3/10
19/19 - 11s - loss: 584.6956 - loglik: -5.8361e+02 - logprior: -1.0831e+00
Epoch 4/10
19/19 - 11s - loss: 570.4855 - loglik: -5.6931e+02 - logprior: -1.1766e+00
Epoch 5/10
19/19 - 11s - loss: 562.8671 - loglik: -5.6166e+02 - logprior: -1.2040e+00
Epoch 6/10
19/19 - 11s - loss: 558.2869 - loglik: -5.5705e+02 - logprior: -1.2324e+00
Epoch 7/10
19/19 - 11s - loss: 554.5363 - loglik: -5.5327e+02 - logprior: -1.2672e+00
Epoch 8/10
19/19 - 11s - loss: 553.3040 - loglik: -5.5202e+02 - logprior: -1.2843e+00
Epoch 9/10
19/19 - 11s - loss: 553.7043 - loglik: -5.5244e+02 - logprior: -1.2642e+00
Fitted a model with MAP estimate = -551.7960
expansions: [(23, 1), (24, 3), (59, 1), (61, 2), (62, 1), (64, 1), (80, 1), (81, 1), (83, 1), (84, 1), (97, 1), (104, 1), (105, 2), (110, 1), (121, 1), (124, 1), (127, 1), (134, 1), (138, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 658.0142 - loglik: -6.5407e+02 - logprior: -3.9443e+00
Epoch 2/2
19/19 - 13s - loss: 605.6152 - loglik: -6.0354e+02 - logprior: -2.0771e+00
Fitted a model with MAP estimate = -587.8755
expansions: [(0, 4), (25, 5), (26, 3), (48, 2), (49, 4), (93, 1), (120, 1), (175, 1), (176, 1), (177, 2), (179, 2), (180, 1), (214, 13)]
discards: [  0  27  28  29  30  50  51  52  94 118]
Re-initialized the encoder parameters.
Fitting a model of length 244 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 19s - loss: 598.0204 - loglik: -5.9525e+02 - logprior: -2.7700e+00
Epoch 2/2
19/19 - 16s - loss: 580.8940 - loglik: -5.8001e+02 - logprior: -8.8749e-01
Fitted a model with MAP estimate = -575.2043
expansions: [(34, 2), (76, 1), (181, 1), (183, 1), (198, 1), (216, 1)]
discards: [  1   2   3  29  30  31  77 230 231 232 233 234 235 236 237 238 239 240
 241 242 243]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 17s - loss: 590.5757 - loglik: -5.8816e+02 - logprior: -2.4168e+00
Epoch 2/10
19/19 - 15s - loss: 581.2394 - loglik: -5.8084e+02 - logprior: -4.0343e-01
Epoch 3/10
19/19 - 15s - loss: 571.1915 - loglik: -5.7102e+02 - logprior: -1.7554e-01
Epoch 4/10
19/19 - 15s - loss: 560.1332 - loglik: -5.6005e+02 - logprior: -8.8221e-02
Epoch 5/10
19/19 - 14s - loss: 549.7371 - loglik: -5.4958e+02 - logprior: -1.5347e-01
Epoch 6/10
19/19 - 14s - loss: 543.2734 - loglik: -5.4304e+02 - logprior: -2.2896e-01
Epoch 7/10
19/19 - 15s - loss: 538.4120 - loglik: -5.3805e+02 - logprior: -3.6474e-01
Epoch 8/10
19/19 - 15s - loss: 533.6337 - loglik: -5.3316e+02 - logprior: -4.7377e-01
Epoch 9/10
19/19 - 15s - loss: 533.6364 - loglik: -5.3308e+02 - logprior: -5.5288e-01
Fitted a model with MAP estimate = -533.4650
Time for alignment: 382.4739
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 665.6104 - loglik: -6.6298e+02 - logprior: -2.6273e+00
Epoch 2/10
19/19 - 11s - loss: 615.8276 - loglik: -6.1492e+02 - logprior: -9.1186e-01
Epoch 3/10
19/19 - 11s - loss: 585.7655 - loglik: -5.8468e+02 - logprior: -1.0847e+00
Epoch 4/10
19/19 - 11s - loss: 571.5160 - loglik: -5.7039e+02 - logprior: -1.1242e+00
Epoch 5/10
19/19 - 11s - loss: 561.7502 - loglik: -5.6059e+02 - logprior: -1.1569e+00
Epoch 6/10
19/19 - 11s - loss: 557.9348 - loglik: -5.5672e+02 - logprior: -1.2113e+00
Epoch 7/10
19/19 - 11s - loss: 555.4517 - loglik: -5.5419e+02 - logprior: -1.2632e+00
Epoch 8/10
19/19 - 11s - loss: 554.9087 - loglik: -5.5362e+02 - logprior: -1.2923e+00
Epoch 9/10
19/19 - 11s - loss: 552.8240 - loglik: -5.5153e+02 - logprior: -1.2980e+00
Epoch 10/10
19/19 - 11s - loss: 551.2328 - loglik: -5.4993e+02 - logprior: -1.3041e+00
Fitted a model with MAP estimate = -551.6293
expansions: [(22, 1), (24, 2), (59, 1), (61, 2), (62, 1), (64, 1), (80, 1), (81, 1), (82, 2), (83, 2), (85, 1), (103, 2), (104, 2), (109, 1), (120, 1), (123, 1), (126, 1), (133, 1), (134, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 17s - loss: 659.3706 - loglik: -6.5540e+02 - logprior: -3.9723e+00
Epoch 2/2
19/19 - 13s - loss: 607.7098 - loglik: -6.0573e+02 - logprior: -1.9837e+00
Fitted a model with MAP estimate = -588.6594
expansions: [(0, 4), (24, 1), (25, 3), (30, 2), (47, 7), (216, 16)]
discards: [  0  26  27  28  49  50  51  68  88  93 119 168 169 170 171 172 173 174]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 17s - loss: 601.4516 - loglik: -5.9872e+02 - logprior: -2.7309e+00
Epoch 2/2
19/19 - 15s - loss: 583.5381 - loglik: -5.8250e+02 - logprior: -1.0357e+00
Fitted a model with MAP estimate = -577.8652
expansions: [(34, 2), (35, 1), (55, 2), (75, 1), (98, 1), (173, 1), (176, 6), (177, 2), (179, 3), (180, 3), (196, 1), (197, 1), (199, 2), (200, 1)]
discards: [  1   2   3  29  30  31  76 214 215 216 217 218 219 220 221 222 223 224
 225 226 227 228 229 230]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 18s - loss: 591.6274 - loglik: -5.8925e+02 - logprior: -2.3760e+00
Epoch 2/10
19/19 - 15s - loss: 580.1592 - loglik: -5.7979e+02 - logprior: -3.6639e-01
Epoch 3/10
19/19 - 15s - loss: 570.8646 - loglik: -5.7068e+02 - logprior: -1.8832e-01
Epoch 4/10
19/19 - 15s - loss: 559.6013 - loglik: -5.5950e+02 - logprior: -1.0568e-01
Epoch 5/10
19/19 - 15s - loss: 549.2051 - loglik: -5.4907e+02 - logprior: -1.3336e-01
Epoch 6/10
19/19 - 15s - loss: 542.6112 - loglik: -5.4239e+02 - logprior: -2.2617e-01
Epoch 7/10
19/19 - 15s - loss: 535.8278 - loglik: -5.3547e+02 - logprior: -3.5768e-01
Epoch 8/10
19/19 - 15s - loss: 534.5979 - loglik: -5.3412e+02 - logprior: -4.8038e-01
Epoch 9/10
19/19 - 15s - loss: 532.2210 - loglik: -5.3168e+02 - logprior: -5.3624e-01
Epoch 10/10
19/19 - 15s - loss: 533.1307 - loglik: -5.3252e+02 - logprior: -6.0634e-01
Fitted a model with MAP estimate = -531.9705
Time for alignment: 407.3516
Computed alignments with likelihoods: ['-534.6035', '-533.4650', '-531.9705']
Best model has likelihood: -531.9705
SP score = 0.4400
Training of 3 independent models on file PF00046.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9630642220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f965230f7c0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.0277 - loglik: -1.5080e+02 - logprior: -3.2273e+00
Epoch 2/10
19/19 - 1s - loss: 123.9656 - loglik: -1.2246e+02 - logprior: -1.5034e+00
Epoch 3/10
19/19 - 1s - loss: 111.2206 - loglik: -1.0964e+02 - logprior: -1.5787e+00
Epoch 4/10
19/19 - 1s - loss: 107.4183 - loglik: -1.0576e+02 - logprior: -1.6592e+00
Epoch 5/10
19/19 - 1s - loss: 106.5648 - loglik: -1.0498e+02 - logprior: -1.5836e+00
Epoch 6/10
19/19 - 1s - loss: 105.9108 - loglik: -1.0433e+02 - logprior: -1.5851e+00
Epoch 7/10
19/19 - 1s - loss: 105.9490 - loglik: -1.0440e+02 - logprior: -1.5487e+00
Fitted a model with MAP estimate = -105.6137
expansions: [(3, 1), (5, 1), (7, 1), (10, 1), (16, 1), (17, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 1), (31, 1), (32, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 110.9143 - loglik: -1.0679e+02 - logprior: -4.1258e+00
Epoch 2/2
19/19 - 1s - loss: 101.1595 - loglik: -9.8961e+01 - logprior: -2.1982e+00
Fitted a model with MAP estimate = -99.7043
expansions: [(3, 1)]
discards: [ 0 21 38]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 104.4075 - loglik: -1.0046e+02 - logprior: -3.9494e+00
Epoch 2/2
19/19 - 1s - loss: 99.9124 - loglik: -9.8416e+01 - logprior: -1.4968e+00
Fitted a model with MAP estimate = -99.1115
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 104.7136 - loglik: -1.0029e+02 - logprior: -4.4262e+00
Epoch 2/10
19/19 - 1s - loss: 100.3097 - loglik: -9.8270e+01 - logprior: -2.0396e+00
Epoch 3/10
19/19 - 1s - loss: 98.7898 - loglik: -9.7410e+01 - logprior: -1.3794e+00
Epoch 4/10
19/19 - 1s - loss: 98.3195 - loglik: -9.7112e+01 - logprior: -1.2080e+00
Epoch 5/10
19/19 - 1s - loss: 97.7648 - loglik: -9.6594e+01 - logprior: -1.1707e+00
Epoch 6/10
19/19 - 1s - loss: 97.5516 - loglik: -9.6398e+01 - logprior: -1.1538e+00
Epoch 7/10
19/19 - 1s - loss: 97.3106 - loglik: -9.6168e+01 - logprior: -1.1430e+00
Epoch 8/10
19/19 - 1s - loss: 97.2409 - loglik: -9.6110e+01 - logprior: -1.1307e+00
Epoch 9/10
19/19 - 1s - loss: 96.9769 - loglik: -9.5866e+01 - logprior: -1.1112e+00
Epoch 10/10
19/19 - 1s - loss: 96.8219 - loglik: -9.5723e+01 - logprior: -1.0986e+00
Fitted a model with MAP estimate = -96.8838
Time for alignment: 42.1942
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 153.9917 - loglik: -1.5076e+02 - logprior: -3.2274e+00
Epoch 2/10
19/19 - 1s - loss: 124.7497 - loglik: -1.2323e+02 - logprior: -1.5175e+00
Epoch 3/10
19/19 - 1s - loss: 110.0533 - loglik: -1.0848e+02 - logprior: -1.5713e+00
Epoch 4/10
19/19 - 1s - loss: 105.8675 - loglik: -1.0420e+02 - logprior: -1.6671e+00
Epoch 5/10
19/19 - 1s - loss: 104.8829 - loglik: -1.0329e+02 - logprior: -1.5904e+00
Epoch 6/10
19/19 - 1s - loss: 104.4536 - loglik: -1.0286e+02 - logprior: -1.5904e+00
Epoch 7/10
19/19 - 1s - loss: 104.1737 - loglik: -1.0262e+02 - logprior: -1.5572e+00
Epoch 8/10
19/19 - 1s - loss: 104.2862 - loglik: -1.0274e+02 - logprior: -1.5511e+00
Fitted a model with MAP estimate = -103.9444
expansions: [(3, 1), (5, 1), (7, 1), (10, 1), (14, 1), (16, 2), (21, 1), (24, 1), (27, 1), (29, 2), (30, 1), (31, 1), (33, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 110.2817 - loglik: -1.0615e+02 - logprior: -4.1325e+00
Epoch 2/2
19/19 - 1s - loss: 101.0162 - loglik: -9.8823e+01 - logprior: -2.1927e+00
Fitted a model with MAP estimate = -99.7472
expansions: [(3, 1)]
discards: [ 0 21 38]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 104.5666 - loglik: -1.0060e+02 - logprior: -3.9640e+00
Epoch 2/2
19/19 - 1s - loss: 99.8789 - loglik: -9.8369e+01 - logprior: -1.5104e+00
Fitted a model with MAP estimate = -99.0847
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 104.7136 - loglik: -1.0027e+02 - logprior: -4.4477e+00
Epoch 2/10
19/19 - 1s - loss: 100.2435 - loglik: -9.8217e+01 - logprior: -2.0265e+00
Epoch 3/10
19/19 - 1s - loss: 98.8710 - loglik: -9.7496e+01 - logprior: -1.3748e+00
Epoch 4/10
19/19 - 1s - loss: 98.3975 - loglik: -9.7186e+01 - logprior: -1.2119e+00
Epoch 5/10
19/19 - 1s - loss: 97.8552 - loglik: -9.6678e+01 - logprior: -1.1768e+00
Epoch 6/10
19/19 - 1s - loss: 97.4080 - loglik: -9.6259e+01 - logprior: -1.1487e+00
Epoch 7/10
19/19 - 1s - loss: 97.2664 - loglik: -9.6127e+01 - logprior: -1.1392e+00
Epoch 8/10
19/19 - 1s - loss: 97.0571 - loglik: -9.5933e+01 - logprior: -1.1241e+00
Epoch 9/10
19/19 - 1s - loss: 97.1277 - loglik: -9.6011e+01 - logprior: -1.1170e+00
Fitted a model with MAP estimate = -96.9536
Time for alignment: 41.5456
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.0052 - loglik: -1.5078e+02 - logprior: -3.2277e+00
Epoch 2/10
19/19 - 1s - loss: 123.2167 - loglik: -1.2171e+02 - logprior: -1.5025e+00
Epoch 3/10
19/19 - 1s - loss: 110.2633 - loglik: -1.0870e+02 - logprior: -1.5587e+00
Epoch 4/10
19/19 - 1s - loss: 106.9130 - loglik: -1.0530e+02 - logprior: -1.6162e+00
Epoch 5/10
19/19 - 1s - loss: 106.2604 - loglik: -1.0473e+02 - logprior: -1.5305e+00
Epoch 6/10
19/19 - 1s - loss: 105.4632 - loglik: -1.0392e+02 - logprior: -1.5424e+00
Epoch 7/10
19/19 - 1s - loss: 105.3585 - loglik: -1.0385e+02 - logprior: -1.5073e+00
Epoch 8/10
19/19 - 1s - loss: 105.1338 - loglik: -1.0363e+02 - logprior: -1.5008e+00
Epoch 9/10
19/19 - 1s - loss: 105.2349 - loglik: -1.0373e+02 - logprior: -1.5045e+00
Fitted a model with MAP estimate = -104.9483
expansions: [(3, 1), (5, 1), (7, 1), (10, 1), (16, 1), (17, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 110.8319 - loglik: -1.0669e+02 - logprior: -4.1384e+00
Epoch 2/2
19/19 - 1s - loss: 101.1589 - loglik: -9.8889e+01 - logprior: -2.2702e+00
Fitted a model with MAP estimate = -99.7918
expansions: [(3, 1)]
discards: [ 0 21 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 104.4933 - loglik: -1.0053e+02 - logprior: -3.9590e+00
Epoch 2/2
19/19 - 1s - loss: 99.9369 - loglik: -9.8435e+01 - logprior: -1.5016e+00
Fitted a model with MAP estimate = -99.1028
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 104.8450 - loglik: -1.0042e+02 - logprior: -4.4295e+00
Epoch 2/10
19/19 - 1s - loss: 100.2815 - loglik: -9.8270e+01 - logprior: -2.0116e+00
Epoch 3/10
19/19 - 1s - loss: 98.8560 - loglik: -9.7490e+01 - logprior: -1.3660e+00
Epoch 4/10
19/19 - 1s - loss: 98.2853 - loglik: -9.7087e+01 - logprior: -1.1984e+00
Epoch 5/10
19/19 - 1s - loss: 97.8939 - loglik: -9.6730e+01 - logprior: -1.1639e+00
Epoch 6/10
19/19 - 1s - loss: 97.3723 - loglik: -9.6228e+01 - logprior: -1.1440e+00
Epoch 7/10
19/19 - 1s - loss: 97.3559 - loglik: -9.6221e+01 - logprior: -1.1344e+00
Epoch 8/10
19/19 - 1s - loss: 97.0786 - loglik: -9.5956e+01 - logprior: -1.1231e+00
Epoch 9/10
19/19 - 1s - loss: 97.2831 - loglik: -9.6182e+01 - logprior: -1.1016e+00
Fitted a model with MAP estimate = -96.9618
Time for alignment: 43.0160
Computed alignments with likelihoods: ['-96.8838', '-96.9536', '-96.9618']
Best model has likelihood: -96.8838
SP score = 0.9531
Training of 3 independent models on file PF01814.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96911a1490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f961f25cf40>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 365.6920 - loglik: -3.6270e+02 - logprior: -2.9926e+00
Epoch 2/10
19/19 - 3s - loss: 335.7377 - loglik: -3.3471e+02 - logprior: -1.0283e+00
Epoch 3/10
19/19 - 3s - loss: 320.3116 - loglik: -3.1905e+02 - logprior: -1.2629e+00
Epoch 4/10
19/19 - 3s - loss: 313.7934 - loglik: -3.1262e+02 - logprior: -1.1755e+00
Epoch 5/10
19/19 - 3s - loss: 309.4862 - loglik: -3.0828e+02 - logprior: -1.2098e+00
Epoch 6/10
19/19 - 3s - loss: 307.3775 - loglik: -3.0616e+02 - logprior: -1.2166e+00
Epoch 7/10
19/19 - 3s - loss: 306.7639 - loglik: -3.0555e+02 - logprior: -1.2093e+00
Epoch 8/10
19/19 - 3s - loss: 305.5735 - loglik: -3.0436e+02 - logprior: -1.2176e+00
Epoch 9/10
19/19 - 3s - loss: 305.0469 - loglik: -3.0383e+02 - logprior: -1.2163e+00
Epoch 10/10
19/19 - 3s - loss: 303.9120 - loglik: -3.0269e+02 - logprior: -1.2268e+00
Fitted a model with MAP estimate = -303.4562
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 4), (24, 3), (27, 1), (29, 1), (32, 1), (38, 1), (54, 3), (74, 1), (75, 4), (76, 1), (82, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 348.6723 - loglik: -3.4492e+02 - logprior: -3.7473e+00
Epoch 2/2
19/19 - 3s - loss: 322.3204 - loglik: -3.2028e+02 - logprior: -2.0379e+00
Fitted a model with MAP estimate = -315.4531
expansions: [(0, 2), (69, 1), (96, 1)]
discards: [  0  28  38  98 108]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 319.6270 - loglik: -3.1684e+02 - logprior: -2.7877e+00
Epoch 2/2
19/19 - 3s - loss: 312.4538 - loglik: -3.1148e+02 - logprior: -9.6934e-01
Fitted a model with MAP estimate = -309.7072
expansions: [(36, 1), (101, 2)]
discards: [ 1 26 27]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 317.7628 - loglik: -3.1519e+02 - logprior: -2.5706e+00
Epoch 2/10
19/19 - 3s - loss: 311.9057 - loglik: -3.1105e+02 - logprior: -8.5759e-01
Epoch 3/10
19/19 - 3s - loss: 308.3961 - loglik: -3.0751e+02 - logprior: -8.8173e-01
Epoch 4/10
19/19 - 3s - loss: 304.2357 - loglik: -3.0347e+02 - logprior: -7.6649e-01
Epoch 5/10
19/19 - 3s - loss: 300.3701 - loglik: -2.9961e+02 - logprior: -7.5805e-01
Epoch 6/10
19/19 - 3s - loss: 297.7804 - loglik: -2.9704e+02 - logprior: -7.3767e-01
Epoch 7/10
19/19 - 3s - loss: 296.3595 - loglik: -2.9563e+02 - logprior: -7.2675e-01
Epoch 8/10
19/19 - 3s - loss: 295.1805 - loglik: -2.9447e+02 - logprior: -7.1499e-01
Epoch 9/10
19/19 - 3s - loss: 293.6808 - loglik: -2.9298e+02 - logprior: -7.0079e-01
Epoch 10/10
19/19 - 3s - loss: 292.9396 - loglik: -2.9224e+02 - logprior: -6.9559e-01
Fitted a model with MAP estimate = -291.6033
Time for alignment: 105.7888
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 365.4893 - loglik: -3.6249e+02 - logprior: -2.9957e+00
Epoch 2/10
19/19 - 3s - loss: 334.5208 - loglik: -3.3348e+02 - logprior: -1.0435e+00
Epoch 3/10
19/19 - 3s - loss: 319.5892 - loglik: -3.1832e+02 - logprior: -1.2728e+00
Epoch 4/10
19/19 - 3s - loss: 313.3497 - loglik: -3.1219e+02 - logprior: -1.1625e+00
Epoch 5/10
19/19 - 3s - loss: 309.5982 - loglik: -3.0843e+02 - logprior: -1.1715e+00
Epoch 6/10
19/19 - 3s - loss: 307.4596 - loglik: -3.0630e+02 - logprior: -1.1556e+00
Epoch 7/10
19/19 - 3s - loss: 306.2166 - loglik: -3.0506e+02 - logprior: -1.1515e+00
Epoch 8/10
19/19 - 3s - loss: 305.6430 - loglik: -3.0448e+02 - logprior: -1.1636e+00
Epoch 9/10
19/19 - 3s - loss: 304.6055 - loglik: -3.0342e+02 - logprior: -1.1887e+00
Epoch 10/10
19/19 - 3s - loss: 304.3372 - loglik: -3.0314e+02 - logprior: -1.1951e+00
Fitted a model with MAP estimate = -303.3484
expansions: [(18, 2), (21, 1), (22, 5), (24, 3), (27, 1), (33, 1), (35, 2), (53, 3), (73, 1), (74, 1), (75, 1), (77, 3), (82, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 348.8257 - loglik: -3.4509e+02 - logprior: -3.7393e+00
Epoch 2/2
19/19 - 3s - loss: 321.7650 - loglik: -3.1972e+02 - logprior: -2.0486e+00
Fitted a model with MAP estimate = -315.0336
expansions: [(0, 2), (21, 1), (66, 1), (99, 1)]
discards: [  0  18  48 108]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 319.3849 - loglik: -3.1660e+02 - logprior: -2.7847e+00
Epoch 2/2
19/19 - 3s - loss: 312.1382 - loglik: -3.1117e+02 - logprior: -9.7106e-01
Fitted a model with MAP estimate = -309.5622
expansions: [(96, 2), (99, 1)]
discards: [  1  25  26  27 100 101]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 317.7846 - loglik: -3.1522e+02 - logprior: -2.5681e+00
Epoch 2/10
19/19 - 3s - loss: 311.9602 - loglik: -3.1110e+02 - logprior: -8.6366e-01
Epoch 3/10
19/19 - 3s - loss: 308.3959 - loglik: -3.0751e+02 - logprior: -8.8365e-01
Epoch 4/10
19/19 - 3s - loss: 304.0544 - loglik: -3.0329e+02 - logprior: -7.6837e-01
Epoch 5/10
19/19 - 3s - loss: 300.3826 - loglik: -2.9963e+02 - logprior: -7.5107e-01
Epoch 6/10
19/19 - 3s - loss: 297.5434 - loglik: -2.9681e+02 - logprior: -7.3465e-01
Epoch 7/10
19/19 - 3s - loss: 296.2626 - loglik: -2.9554e+02 - logprior: -7.2340e-01
Epoch 8/10
19/19 - 3s - loss: 294.7449 - loglik: -2.9403e+02 - logprior: -7.1779e-01
Epoch 9/10
19/19 - 3s - loss: 293.4767 - loglik: -2.9277e+02 - logprior: -7.0536e-01
Epoch 10/10
19/19 - 3s - loss: 291.9943 - loglik: -2.9129e+02 - logprior: -7.0529e-01
Fitted a model with MAP estimate = -290.6475
Time for alignment: 105.1755
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 365.5307 - loglik: -3.6254e+02 - logprior: -2.9905e+00
Epoch 2/10
19/19 - 3s - loss: 334.9925 - loglik: -3.3396e+02 - logprior: -1.0332e+00
Epoch 3/10
19/19 - 3s - loss: 320.9880 - loglik: -3.1972e+02 - logprior: -1.2637e+00
Epoch 4/10
19/19 - 3s - loss: 314.4215 - loglik: -3.1327e+02 - logprior: -1.1549e+00
Epoch 5/10
19/19 - 3s - loss: 310.0773 - loglik: -3.0890e+02 - logprior: -1.1733e+00
Epoch 6/10
19/19 - 3s - loss: 307.5314 - loglik: -3.0637e+02 - logprior: -1.1647e+00
Epoch 7/10
19/19 - 3s - loss: 306.3427 - loglik: -3.0518e+02 - logprior: -1.1627e+00
Epoch 8/10
19/19 - 3s - loss: 305.4490 - loglik: -3.0427e+02 - logprior: -1.1788e+00
Epoch 9/10
19/19 - 3s - loss: 305.4515 - loglik: -3.0427e+02 - logprior: -1.1864e+00
Fitted a model with MAP estimate = -304.8042
expansions: [(18, 1), (21, 1), (22, 5), (24, 4), (27, 1), (29, 1), (32, 1), (51, 1), (53, 3), (70, 1), (77, 1), (78, 1), (82, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 346.5195 - loglik: -3.4281e+02 - logprior: -3.7114e+00
Epoch 2/2
19/19 - 3s - loss: 320.7301 - loglik: -3.1866e+02 - logprior: -2.0652e+00
Fitted a model with MAP estimate = -314.7349
expansions: [(0, 2), (20, 1), (36, 1), (92, 1)]
discards: [  0  33  38  97 105]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 319.3873 - loglik: -3.1661e+02 - logprior: -2.7736e+00
Epoch 2/2
19/19 - 3s - loss: 312.3976 - loglik: -3.1147e+02 - logprior: -9.2614e-01
Fitted a model with MAP estimate = -309.9080
expansions: [(95, 3)]
discards: [ 1 25 26 27]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 317.8575 - loglik: -3.1531e+02 - logprior: -2.5496e+00
Epoch 2/10
19/19 - 3s - loss: 312.3125 - loglik: -3.1147e+02 - logprior: -8.4341e-01
Epoch 3/10
19/19 - 3s - loss: 308.4604 - loglik: -3.0759e+02 - logprior: -8.6782e-01
Epoch 4/10
19/19 - 3s - loss: 304.3771 - loglik: -3.0361e+02 - logprior: -7.6905e-01
Epoch 5/10
19/19 - 3s - loss: 301.0332 - loglik: -3.0028e+02 - logprior: -7.5734e-01
Epoch 6/10
19/19 - 3s - loss: 298.0238 - loglik: -2.9729e+02 - logprior: -7.3091e-01
Epoch 7/10
19/19 - 3s - loss: 296.7066 - loglik: -2.9598e+02 - logprior: -7.3108e-01
Epoch 8/10
19/19 - 3s - loss: 295.4161 - loglik: -2.9470e+02 - logprior: -7.1166e-01
Epoch 9/10
19/19 - 3s - loss: 294.6822 - loglik: -2.9398e+02 - logprior: -6.9816e-01
Epoch 10/10
19/19 - 3s - loss: 293.0645 - loglik: -2.9237e+02 - logprior: -6.8964e-01
Fitted a model with MAP estimate = -292.0360
Time for alignment: 101.8000
Computed alignments with likelihoods: ['-291.6033', '-290.6475', '-292.0360']
Best model has likelihood: -290.6475
SP score = 0.8149
Training of 3 independent models on file PF00155.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f966be29b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96498362b0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 916.3414 - loglik: -9.1490e+02 - logprior: -1.4435e+00
Epoch 2/10
39/39 - 25s - loss: 814.3190 - loglik: -8.1288e+02 - logprior: -1.4356e+00
Epoch 3/10
39/39 - 25s - loss: 796.5833 - loglik: -7.9516e+02 - logprior: -1.4226e+00
Epoch 4/10
39/39 - 25s - loss: 789.8927 - loglik: -7.8853e+02 - logprior: -1.3600e+00
Epoch 5/10
39/39 - 25s - loss: 785.6096 - loglik: -7.8420e+02 - logprior: -1.4097e+00
Epoch 6/10
39/39 - 25s - loss: 783.8125 - loglik: -7.8235e+02 - logprior: -1.4646e+00
Epoch 7/10
39/39 - 25s - loss: 782.0059 - loglik: -7.8051e+02 - logprior: -1.4941e+00
Epoch 8/10
39/39 - 25s - loss: 781.7069 - loglik: -7.8019e+02 - logprior: -1.5143e+00
Epoch 9/10
39/39 - 25s - loss: 780.9525 - loglik: -7.7942e+02 - logprior: -1.5326e+00
Epoch 10/10
39/39 - 25s - loss: 780.7479 - loglik: -7.7922e+02 - logprior: -1.5306e+00
Fitted a model with MAP estimate = -772.9551
expansions: [(0, 2), (15, 2), (20, 1), (21, 1), (23, 1), (24, 2), (25, 1), (30, 1), (40, 1), (42, 1), (43, 2), (44, 2), (45, 1), (55, 1), (58, 1), (61, 1), (68, 1), (79, 1), (80, 1), (81, 1), (85, 2), (87, 1), (92, 2), (93, 1), (102, 1), (119, 1), (122, 1), (128, 2), (130, 1), (142, 1), (144, 1), (147, 1), (149, 1), (154, 1), (155, 3), (161, 1), (181, 1), (184, 1), (185, 1), (187, 1), (188, 1), (189, 1), (205, 1), (206, 2), (207, 1), (208, 1), (209, 3), (210, 1), (219, 1), (220, 1), (226, 2), (228, 1), (237, 1), (239, 1), (240, 1), (242, 1), (245, 1), (261, 2), (270, 2), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 357 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 40s - loss: 838.0956 - loglik: -8.3614e+02 - logprior: -1.9514e+00
Epoch 2/2
39/39 - 37s - loss: 780.0886 - loglik: -7.7961e+02 - logprior: -4.7529e-01
Fitted a model with MAP estimate = -765.5782
expansions: [(124, 1), (335, 1)]
discards: [  0  32 111 122 197 266 267 291 344]
Re-initialized the encoder parameters.
Fitting a model of length 350 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 794.1115 - loglik: -7.9203e+02 - logprior: -2.0829e+00
Epoch 2/2
39/39 - 35s - loss: 778.1139 - loglik: -7.7795e+02 - logprior: -1.6879e-01
Fitted a model with MAP estimate = -763.8504
expansions: [(0, 2), (257, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 352 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 39s - loss: 783.3734 - loglik: -7.8250e+02 - logprior: -8.7579e-01
Epoch 2/10
39/39 - 35s - loss: 769.4263 - loglik: -7.6970e+02 - logprior: 0.2689
Epoch 3/10
39/39 - 35s - loss: 760.1487 - loglik: -7.6059e+02 - logprior: 0.4407
Epoch 4/10
39/39 - 36s - loss: 752.4487 - loglik: -7.5300e+02 - logprior: 0.5551
Epoch 5/10
39/39 - 36s - loss: 746.0805 - loglik: -7.4674e+02 - logprior: 0.6639
Epoch 6/10
39/39 - 36s - loss: 742.6151 - loglik: -7.4334e+02 - logprior: 0.7290
Epoch 7/10
39/39 - 36s - loss: 741.2145 - loglik: -7.4200e+02 - logprior: 0.7815
Epoch 8/10
39/39 - 35s - loss: 740.8610 - loglik: -7.4177e+02 - logprior: 0.9079
Epoch 9/10
39/39 - 36s - loss: 739.0386 - loglik: -7.4009e+02 - logprior: 1.0533
Epoch 10/10
39/39 - 36s - loss: 739.8504 - loglik: -7.4102e+02 - logprior: 1.1724
Fitted a model with MAP estimate = -738.3457
Time for alignment: 947.3291
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 915.8117 - loglik: -9.1438e+02 - logprior: -1.4290e+00
Epoch 2/10
39/39 - 25s - loss: 815.0258 - loglik: -8.1373e+02 - logprior: -1.2919e+00
Epoch 3/10
39/39 - 25s - loss: 796.7693 - loglik: -7.9546e+02 - logprior: -1.3142e+00
Epoch 4/10
39/39 - 25s - loss: 790.4626 - loglik: -7.8921e+02 - logprior: -1.2539e+00
Epoch 5/10
39/39 - 25s - loss: 785.9438 - loglik: -7.8460e+02 - logprior: -1.3401e+00
Epoch 6/10
39/39 - 25s - loss: 783.9357 - loglik: -7.8258e+02 - logprior: -1.3523e+00
Epoch 7/10
39/39 - 25s - loss: 783.6375 - loglik: -7.8222e+02 - logprior: -1.4176e+00
Epoch 8/10
39/39 - 25s - loss: 782.9243 - loglik: -7.8150e+02 - logprior: -1.4220e+00
Epoch 9/10
39/39 - 25s - loss: 782.2942 - loglik: -7.8089e+02 - logprior: -1.4015e+00
Epoch 10/10
39/39 - 25s - loss: 782.1145 - loglik: -7.8069e+02 - logprior: -1.4210e+00
Fitted a model with MAP estimate = -774.3402
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (23, 3), (24, 2), (25, 1), (30, 1), (40, 1), (42, 1), (43, 2), (44, 2), (48, 2), (56, 1), (59, 1), (61, 1), (78, 1), (79, 1), (80, 1), (81, 1), (92, 1), (93, 2), (94, 1), (101, 1), (103, 1), (119, 1), (130, 1), (145, 1), (148, 2), (153, 1), (156, 2), (182, 1), (185, 1), (186, 1), (188, 1), (189, 1), (190, 1), (201, 1), (205, 1), (206, 3), (207, 1), (208, 1), (209, 2), (220, 1), (221, 1), (223, 1), (226, 2), (228, 1), (240, 1), (241, 1), (243, 1), (245, 1), (258, 1), (261, 1), (263, 1), (269, 2), (270, 2), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 39s - loss: 839.5248 - loglik: -8.3756e+02 - logprior: -1.9602e+00
Epoch 2/2
39/39 - 36s - loss: 782.1398 - loglik: -7.8168e+02 - logprior: -4.5480e-01
Fitted a model with MAP estimate = -767.8428
expansions: [(125, 1), (196, 1), (330, 1)]
discards: [  0  30  33  34  66 185 255 256 260 261 287 339]
Re-initialized the encoder parameters.
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 797.1219 - loglik: -7.9490e+02 - logprior: -2.2179e+00
Epoch 2/2
39/39 - 35s - loss: 780.9705 - loglik: -7.8078e+02 - logprior: -1.8642e-01
Fitted a model with MAP estimate = -766.9060
expansions: [(0, 2), (180, 1), (254, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 785.4651 - loglik: -7.8457e+02 - logprior: -8.9648e-01
Epoch 2/10
39/39 - 35s - loss: 771.4457 - loglik: -7.7169e+02 - logprior: 0.2455
Epoch 3/10
39/39 - 35s - loss: 763.5728 - loglik: -7.6400e+02 - logprior: 0.4227
Epoch 4/10
39/39 - 35s - loss: 754.6341 - loglik: -7.5512e+02 - logprior: 0.4861
Epoch 5/10
39/39 - 35s - loss: 749.3077 - loglik: -7.4993e+02 - logprior: 0.6255
Epoch 6/10
39/39 - 35s - loss: 746.3929 - loglik: -7.4705e+02 - logprior: 0.6538
Epoch 7/10
39/39 - 35s - loss: 744.1929 - loglik: -7.4498e+02 - logprior: 0.7871
Epoch 8/10
39/39 - 35s - loss: 743.5206 - loglik: -7.4436e+02 - logprior: 0.8354
Epoch 9/10
39/39 - 35s - loss: 743.0989 - loglik: -7.4403e+02 - logprior: 0.9272
Epoch 10/10
39/39 - 35s - loss: 742.1138 - loglik: -7.4320e+02 - logprior: 1.0894
Fitted a model with MAP estimate = -741.2316
Time for alignment: 942.6566
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 916.4113 - loglik: -9.1499e+02 - logprior: -1.4239e+00
Epoch 2/10
39/39 - 25s - loss: 814.8113 - loglik: -8.1349e+02 - logprior: -1.3163e+00
Epoch 3/10
39/39 - 25s - loss: 797.4689 - loglik: -7.9611e+02 - logprior: -1.3561e+00
Epoch 4/10
39/39 - 25s - loss: 791.0848 - loglik: -7.8977e+02 - logprior: -1.3157e+00
Epoch 5/10
39/39 - 25s - loss: 786.9929 - loglik: -7.8563e+02 - logprior: -1.3623e+00
Epoch 6/10
39/39 - 25s - loss: 785.1400 - loglik: -7.8372e+02 - logprior: -1.4226e+00
Epoch 7/10
39/39 - 25s - loss: 783.9213 - loglik: -7.8238e+02 - logprior: -1.5369e+00
Epoch 8/10
39/39 - 25s - loss: 782.8871 - loglik: -7.8143e+02 - logprior: -1.4619e+00
Epoch 9/10
39/39 - 25s - loss: 782.7454 - loglik: -7.8129e+02 - logprior: -1.4602e+00
Epoch 10/10
39/39 - 25s - loss: 782.1714 - loglik: -7.8068e+02 - logprior: -1.4920e+00
Fitted a model with MAP estimate = -774.3644
expansions: [(0, 2), (15, 2), (20, 1), (24, 1), (25, 2), (26, 2), (30, 1), (40, 1), (42, 1), (43, 2), (44, 2), (56, 1), (59, 1), (60, 1), (61, 1), (78, 1), (79, 1), (80, 1), (81, 1), (86, 1), (87, 1), (92, 1), (94, 1), (101, 1), (103, 2), (119, 1), (130, 1), (142, 1), (144, 1), (147, 1), (149, 1), (152, 1), (155, 2), (168, 1), (169, 1), (184, 1), (185, 2), (188, 2), (189, 1), (201, 1), (207, 1), (208, 1), (209, 2), (210, 1), (221, 1), (222, 1), (226, 2), (227, 2), (239, 2), (240, 1), (242, 1), (255, 1), (261, 2), (270, 2), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 352 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 39s - loss: 843.5639 - loglik: -8.4158e+02 - logprior: -1.9875e+00
Epoch 2/2
39/39 - 36s - loss: 783.9938 - loglik: -7.8352e+02 - logprior: -4.7486e-01
Fitted a model with MAP estimate = -769.5253
expansions: [(157, 2), (254, 1), (330, 1)]
discards: [  0  32  33 133 229 258 285 301 339]
Re-initialized the encoder parameters.
Fitting a model of length 347 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 37s - loss: 796.5406 - loglik: -7.9432e+02 - logprior: -2.2253e+00
Epoch 2/2
39/39 - 35s - loss: 779.7463 - loglik: -7.7955e+02 - logprior: -2.0015e-01
Fitted a model with MAP estimate = -765.6444
expansions: [(0, 2), (155, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 784.7729 - loglik: -7.8393e+02 - logprior: -8.4234e-01
Epoch 2/10
39/39 - 35s - loss: 771.1606 - loglik: -7.7137e+02 - logprior: 0.2080
Epoch 3/10
39/39 - 35s - loss: 761.9179 - loglik: -7.6229e+02 - logprior: 0.3724
Epoch 4/10
39/39 - 35s - loss: 753.6865 - loglik: -7.5421e+02 - logprior: 0.5187
Epoch 5/10
39/39 - 35s - loss: 747.0063 - loglik: -7.4755e+02 - logprior: 0.5450
Epoch 6/10
39/39 - 35s - loss: 744.1295 - loglik: -7.4480e+02 - logprior: 0.6700
Epoch 7/10
39/39 - 35s - loss: 741.7377 - loglik: -7.4244e+02 - logprior: 0.6997
Epoch 8/10
39/39 - 35s - loss: 741.5113 - loglik: -7.4235e+02 - logprior: 0.8429
Epoch 9/10
39/39 - 36s - loss: 740.4293 - loglik: -7.4139e+02 - logprior: 0.9627
Epoch 10/10
39/39 - 35s - loss: 740.2081 - loglik: -7.4125e+02 - logprior: 1.0467
Fitted a model with MAP estimate = -739.1017
Time for alignment: 940.6363
Computed alignments with likelihoods: ['-738.3457', '-741.2316', '-739.1017']
Best model has likelihood: -738.3457
SP score = 0.4670
Training of 3 independent models on file PF00450.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95e3c689a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f961eb9d4f0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 42s - loss: 889.1309 - loglik: -8.8767e+02 - logprior: -1.4612e+00
Epoch 2/10
39/39 - 38s - loss: 752.3582 - loglik: -7.5104e+02 - logprior: -1.3202e+00
Epoch 3/10
39/39 - 38s - loss: 740.2201 - loglik: -7.3872e+02 - logprior: -1.5016e+00
Epoch 4/10
39/39 - 38s - loss: 736.1667 - loglik: -7.3467e+02 - logprior: -1.4985e+00
Epoch 5/10
39/39 - 38s - loss: 736.5369 - loglik: -7.3499e+02 - logprior: -1.5428e+00
Fitted a model with MAP estimate = -734.3328
expansions: [(9, 1), (19, 1), (22, 1), (31, 1), (66, 1), (68, 1), (100, 1), (103, 2), (104, 1), (114, 1), (120, 1), (139, 2), (140, 1), (141, 1), (143, 3), (144, 2), (159, 2), (160, 1), (161, 1), (162, 1), (167, 1), (175, 2), (176, 4), (179, 1), (180, 5), (181, 1), (183, 1), (184, 1), (186, 1), (187, 1), (188, 3), (189, 1), (200, 2), (209, 2), (217, 2), (218, 1), (219, 1), (233, 1), (234, 3), (236, 1), (242, 1), (244, 2), (246, 1), (250, 1), (251, 1), (257, 1), (277, 2), (278, 2), (279, 1), (282, 1), (283, 1), (284, 3), (289, 1), (296, 1), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  0   1 190 191 192 193 194]
Re-initialized the encoder parameters.
Fitting a model of length 405 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 57s - loss: 732.7751 - loglik: -7.3039e+02 - logprior: -2.3803e+00
Epoch 2/2
39/39 - 54s - loss: 714.3171 - loglik: -7.1320e+02 - logprior: -1.1131e+00
Fitted a model with MAP estimate = -709.9715
expansions: [(4, 1), (5, 1), (109, 1), (230, 1), (231, 2)]
discards: [  0   1 204 214 232 233 234 239 248 249 262 284 300 301 343 351 352]
Re-initialized the encoder parameters.
Fitting a model of length 394 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 54s - loss: 726.8136 - loglik: -7.2485e+02 - logprior: -1.9632e+00
Epoch 2/2
39/39 - 52s - loss: 717.5707 - loglik: -7.1721e+02 - logprior: -3.6129e-01
Fitted a model with MAP estimate = -712.6645
expansions: [(4, 2), (229, 2), (230, 3), (245, 1), (246, 1)]
discards: [  0   1   2 233 234 235 236 237 238]
Re-initialized the encoder parameters.
Fitting a model of length 394 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 55s - loss: 723.7419 - loglik: -7.2195e+02 - logprior: -1.7931e+00
Epoch 2/10
39/39 - 52s - loss: 714.4658 - loglik: -7.1394e+02 - logprior: -5.2942e-01
Epoch 3/10
39/39 - 52s - loss: 709.8851 - loglik: -7.1009e+02 - logprior: 0.2009
Epoch 4/10
39/39 - 52s - loss: 706.9371 - loglik: -7.0732e+02 - logprior: 0.3878
Epoch 5/10
39/39 - 52s - loss: 705.3864 - loglik: -7.0592e+02 - logprior: 0.5362
Epoch 6/10
39/39 - 52s - loss: 705.0417 - loglik: -7.0569e+02 - logprior: 0.6480
Epoch 7/10
39/39 - 52s - loss: 704.0945 - loglik: -7.0486e+02 - logprior: 0.7637
Epoch 8/10
39/39 - 52s - loss: 704.1238 - loglik: -7.0502e+02 - logprior: 0.8989
Fitted a model with MAP estimate = -703.0384
Time for alignment: 1063.5895
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 888.0941 - loglik: -8.8662e+02 - logprior: -1.4727e+00
Epoch 2/10
39/39 - 38s - loss: 749.9207 - loglik: -7.4855e+02 - logprior: -1.3712e+00
Epoch 3/10
39/39 - 38s - loss: 737.4426 - loglik: -7.3593e+02 - logprior: -1.5103e+00
Epoch 4/10
39/39 - 38s - loss: 735.1208 - loglik: -7.3362e+02 - logprior: -1.5016e+00
Epoch 5/10
39/39 - 38s - loss: 731.9929 - loglik: -7.3039e+02 - logprior: -1.5986e+00
Epoch 6/10
39/39 - 38s - loss: 731.4118 - loglik: -7.2981e+02 - logprior: -1.6001e+00
Epoch 7/10
39/39 - 38s - loss: 731.7389 - loglik: -7.3013e+02 - logprior: -1.6112e+00
Fitted a model with MAP estimate = -729.9278
expansions: [(9, 1), (19, 1), (22, 1), (31, 1), (69, 1), (101, 1), (104, 1), (105, 1), (106, 1), (109, 1), (122, 1), (141, 1), (143, 1), (144, 1), (145, 3), (146, 3), (161, 2), (162, 1), (163, 1), (164, 1), (168, 1), (175, 1), (177, 1), (178, 5), (181, 1), (182, 5), (183, 1), (185, 1), (187, 1), (188, 1), (189, 1), (190, 2), (191, 2), (203, 2), (211, 1), (219, 2), (220, 1), (221, 1), (235, 3), (242, 1), (244, 2), (245, 2), (247, 1), (248, 1), (255, 1), (258, 1), (268, 1), (278, 1), (279, 1), (280, 1), (281, 1), (282, 1), (283, 5), (288, 1), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1 192 193 194 195 196]
Re-initialized the encoder parameters.
Fitting a model of length 405 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 57s - loss: 732.9845 - loglik: -7.3140e+02 - logprior: -1.5844e+00
Epoch 2/2
39/39 - 54s - loss: 711.9873 - loglik: -7.1161e+02 - logprior: -3.7334e-01
Fitted a model with MAP estimate = -708.4449
expansions: [(70, 1), (233, 2), (234, 2), (246, 1)]
discards: [160 207 208 217 238 239 240 241 242 243 244 251 252 265 299 300 350 351]
Re-initialized the encoder parameters.
Fitting a model of length 393 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 54s - loss: 722.1594 - loglik: -7.2091e+02 - logprior: -1.2515e+00
Epoch 2/2
39/39 - 51s - loss: 713.2393 - loglik: -7.1322e+02 - logprior: -1.8712e-02
Fitted a model with MAP estimate = -709.9987
expansions: [(5, 1), (233, 1)]
discards: [240 241 242 243 244 245 246]
Re-initialized the encoder parameters.
Fitting a model of length 388 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 53s - loss: 719.5744 - loglik: -7.1861e+02 - logprior: -9.6272e-01
Epoch 2/10
39/39 - 50s - loss: 712.7914 - loglik: -7.1317e+02 - logprior: 0.3741
Epoch 3/10
39/39 - 51s - loss: 710.2026 - loglik: -7.1073e+02 - logprior: 0.5237
Epoch 4/10
39/39 - 51s - loss: 707.6060 - loglik: -7.0822e+02 - logprior: 0.6128
Epoch 5/10
39/39 - 50s - loss: 705.8772 - loglik: -7.0667e+02 - logprior: 0.7923
Epoch 6/10
39/39 - 51s - loss: 705.7407 - loglik: -7.0664e+02 - logprior: 0.9009
Epoch 7/10
39/39 - 50s - loss: 705.4141 - loglik: -7.0642e+02 - logprior: 1.0109
Epoch 8/10
39/39 - 51s - loss: 703.6102 - loglik: -7.0477e+02 - logprior: 1.1574
Epoch 9/10
39/39 - 50s - loss: 704.4218 - loglik: -7.0575e+02 - logprior: 1.3245
Fitted a model with MAP estimate = -703.2845
Time for alignment: 1178.6835
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 892.2742 - loglik: -8.9079e+02 - logprior: -1.4836e+00
Epoch 2/10
39/39 - 38s - loss: 752.1362 - loglik: -7.5060e+02 - logprior: -1.5375e+00
Epoch 3/10
39/39 - 38s - loss: 739.6839 - loglik: -7.3809e+02 - logprior: -1.5950e+00
Epoch 4/10
39/39 - 38s - loss: 736.9089 - loglik: -7.3542e+02 - logprior: -1.4854e+00
Epoch 5/10
39/39 - 38s - loss: 735.9258 - loglik: -7.3441e+02 - logprior: -1.5157e+00
Epoch 6/10
39/39 - 38s - loss: 735.2864 - loglik: -7.3377e+02 - logprior: -1.5189e+00
Epoch 7/10
39/39 - 38s - loss: 735.2339 - loglik: -7.3368e+02 - logprior: -1.5521e+00
Epoch 8/10
39/39 - 38s - loss: 733.5157 - loglik: -7.3194e+02 - logprior: -1.5715e+00
Epoch 9/10
39/39 - 38s - loss: 734.1370 - loglik: -7.3261e+02 - logprior: -1.5305e+00
Fitted a model with MAP estimate = -733.3388
expansions: [(9, 1), (19, 2), (20, 1), (33, 1), (60, 1), (62, 1), (64, 1), (75, 1), (98, 1), (101, 2), (102, 1), (108, 1), (111, 1), (116, 1), (117, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 3), (139, 1), (140, 1), (155, 1), (157, 3), (158, 2), (163, 1), (172, 1), (173, 5), (176, 1), (177, 5), (179, 1), (181, 1), (183, 1), (184, 2), (185, 2), (186, 1), (199, 2), (208, 1), (217, 1), (218, 1), (219, 1), (224, 1), (232, 1), (233, 2), (236, 1), (239, 1), (241, 2), (242, 2), (244, 3), (255, 1), (276, 1), (278, 1), (282, 3), (283, 1), (284, 1), (288, 1), (289, 2), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1 188 189 190 191 192]
Re-initialized the encoder parameters.
Fitting a model of length 409 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 58s - loss: 733.9354 - loglik: -7.3232e+02 - logprior: -1.6166e+00
Epoch 2/2
39/39 - 55s - loss: 712.3563 - loglik: -7.1194e+02 - logprior: -4.1393e-01
Fitted a model with MAP estimate = -708.5875
expansions: [(111, 1)]
discards: [160 185 208 209 218 233 234 253 254 288 302 352 353 364]
Re-initialized the encoder parameters.
Fitting a model of length 396 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 55s - loss: 722.2469 - loglik: -7.2109e+02 - logprior: -1.1534e+00
Epoch 2/2
39/39 - 52s - loss: 714.8555 - loglik: -7.1494e+02 - logprior: 0.0820
Fitted a model with MAP estimate = -711.4018
expansions: [(246, 1), (247, 1)]
discards: [293]
Re-initialized the encoder parameters.
Fitting a model of length 397 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 56s - loss: 719.1960 - loglik: -7.1829e+02 - logprior: -9.1009e-01
Epoch 2/10
39/39 - 52s - loss: 712.9302 - loglik: -7.1332e+02 - logprior: 0.3857
Epoch 3/10
39/39 - 52s - loss: 709.6061 - loglik: -7.1012e+02 - logprior: 0.5180
Epoch 4/10
39/39 - 52s - loss: 708.6341 - loglik: -7.0935e+02 - logprior: 0.7129
Epoch 5/10
39/39 - 52s - loss: 706.1576 - loglik: -7.0695e+02 - logprior: 0.7965
Epoch 6/10
39/39 - 52s - loss: 705.4388 - loglik: -7.0639e+02 - logprior: 0.9477
Epoch 7/10
39/39 - 52s - loss: 703.7139 - loglik: -7.0479e+02 - logprior: 1.0783
Epoch 8/10
39/39 - 52s - loss: 705.2412 - loglik: -7.0649e+02 - logprior: 1.2519
Fitted a model with MAP estimate = -703.5427
Time for alignment: 1223.2024
Computed alignments with likelihoods: ['-703.0384', '-703.2845', '-703.5427']
Best model has likelihood: -703.0384
SP score = 0.8203
Training of 3 independent models on file PF07679.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9616160d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f961620b130>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 252.9212 - loglik: -2.4986e+02 - logprior: -3.0635e+00
Epoch 2/10
19/19 - 2s - loss: 221.7296 - loglik: -2.2050e+02 - logprior: -1.2302e+00
Epoch 3/10
19/19 - 2s - loss: 207.3053 - loglik: -2.0586e+02 - logprior: -1.4421e+00
Epoch 4/10
19/19 - 2s - loss: 203.7704 - loglik: -2.0240e+02 - logprior: -1.3714e+00
Epoch 5/10
19/19 - 2s - loss: 202.5522 - loglik: -2.0119e+02 - logprior: -1.3664e+00
Epoch 6/10
19/19 - 1s - loss: 202.2325 - loglik: -2.0090e+02 - logprior: -1.3374e+00
Epoch 7/10
19/19 - 1s - loss: 201.8848 - loglik: -2.0056e+02 - logprior: -1.3224e+00
Epoch 8/10
19/19 - 2s - loss: 201.5821 - loglik: -2.0027e+02 - logprior: -1.3144e+00
Epoch 9/10
19/19 - 1s - loss: 201.4477 - loglik: -2.0014e+02 - logprior: -1.3124e+00
Epoch 10/10
19/19 - 2s - loss: 201.0071 - loglik: -1.9970e+02 - logprior: -1.3079e+00
Fitted a model with MAP estimate = -200.8315
expansions: [(10, 2), (11, 1), (13, 2), (14, 1), (17, 1), (18, 1), (34, 1), (39, 2), (40, 2), (43, 2), (48, 1), (49, 2), (58, 1), (59, 1), (60, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 211.1657 - loglik: -2.0734e+02 - logprior: -3.8297e+00
Epoch 2/2
19/19 - 2s - loss: 199.8581 - loglik: -1.9781e+02 - logprior: -2.0475e+00
Fitted a model with MAP estimate = -197.9021
expansions: [(0, 2)]
discards: [ 0 48 65]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 200.1970 - loglik: -1.9734e+02 - logprior: -2.8530e+00
Epoch 2/2
19/19 - 2s - loss: 196.4696 - loglik: -1.9541e+02 - logprior: -1.0578e+00
Fitted a model with MAP estimate = -195.3841
expansions: []
discards: [ 0 51]
Re-initialized the encoder parameters.
Fitting a model of length 89 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 201.7219 - loglik: -1.9805e+02 - logprior: -3.6693e+00
Epoch 2/10
19/19 - 2s - loss: 196.8353 - loglik: -1.9563e+02 - logprior: -1.2086e+00
Epoch 3/10
19/19 - 2s - loss: 195.3308 - loglik: -1.9435e+02 - logprior: -9.7928e-01
Epoch 4/10
19/19 - 2s - loss: 194.4005 - loglik: -1.9348e+02 - logprior: -9.2213e-01
Epoch 5/10
19/19 - 2s - loss: 193.7300 - loglik: -1.9283e+02 - logprior: -8.9924e-01
Epoch 6/10
19/19 - 2s - loss: 193.0006 - loglik: -1.9211e+02 - logprior: -8.8767e-01
Epoch 7/10
19/19 - 2s - loss: 192.2586 - loglik: -1.9139e+02 - logprior: -8.6395e-01
Epoch 8/10
19/19 - 2s - loss: 191.7953 - loglik: -1.9095e+02 - logprior: -8.4927e-01
Epoch 9/10
19/19 - 2s - loss: 191.3693 - loglik: -1.9053e+02 - logprior: -8.4079e-01
Epoch 10/10
19/19 - 2s - loss: 190.8319 - loglik: -1.9000e+02 - logprior: -8.3481e-01
Fitted a model with MAP estimate = -190.5270
Time for alignment: 63.8898
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 252.6122 - loglik: -2.4954e+02 - logprior: -3.0697e+00
Epoch 2/10
19/19 - 2s - loss: 220.8265 - loglik: -2.1955e+02 - logprior: -1.2811e+00
Epoch 3/10
19/19 - 2s - loss: 205.9587 - loglik: -2.0444e+02 - logprior: -1.5154e+00
Epoch 4/10
19/19 - 2s - loss: 202.5050 - loglik: -2.0109e+02 - logprior: -1.4148e+00
Epoch 5/10
19/19 - 2s - loss: 201.5645 - loglik: -2.0015e+02 - logprior: -1.4166e+00
Epoch 6/10
19/19 - 1s - loss: 200.7936 - loglik: -1.9941e+02 - logprior: -1.3868e+00
Epoch 7/10
19/19 - 1s - loss: 200.4582 - loglik: -1.9908e+02 - logprior: -1.3765e+00
Epoch 8/10
19/19 - 1s - loss: 200.1470 - loglik: -1.9878e+02 - logprior: -1.3656e+00
Epoch 9/10
19/19 - 2s - loss: 200.0213 - loglik: -1.9866e+02 - logprior: -1.3616e+00
Epoch 10/10
19/19 - 2s - loss: 199.4746 - loglik: -1.9812e+02 - logprior: -1.3550e+00
Fitted a model with MAP estimate = -199.3572
expansions: [(9, 1), (10, 1), (11, 1), (13, 2), (14, 1), (19, 1), (20, 1), (37, 2), (39, 2), (41, 2), (43, 2), (48, 1), (49, 2), (60, 1), (62, 1), (64, 2), (65, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 212.5079 - loglik: -2.0863e+02 - logprior: -3.8737e+00
Epoch 2/2
19/19 - 2s - loss: 199.9336 - loglik: -1.9786e+02 - logprior: -2.0783e+00
Fitted a model with MAP estimate = -197.9245
expansions: [(0, 2)]
discards: [ 0 45 49 52 66 85]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 200.4046 - loglik: -1.9756e+02 - logprior: -2.8484e+00
Epoch 2/2
19/19 - 2s - loss: 196.4778 - loglik: -1.9542e+02 - logprior: -1.0561e+00
Fitted a model with MAP estimate = -195.4436
expansions: [(54, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 201.6168 - loglik: -1.9793e+02 - logprior: -3.6852e+00
Epoch 2/10
19/19 - 2s - loss: 196.6094 - loglik: -1.9539e+02 - logprior: -1.2209e+00
Epoch 3/10
19/19 - 2s - loss: 195.2887 - loglik: -1.9433e+02 - logprior: -9.6356e-01
Epoch 4/10
19/19 - 2s - loss: 194.1112 - loglik: -1.9319e+02 - logprior: -9.1656e-01
Epoch 5/10
19/19 - 2s - loss: 193.4187 - loglik: -1.9253e+02 - logprior: -8.9291e-01
Epoch 6/10
19/19 - 2s - loss: 192.7668 - loglik: -1.9189e+02 - logprior: -8.7983e-01
Epoch 7/10
19/19 - 2s - loss: 192.2248 - loglik: -1.9137e+02 - logprior: -8.5653e-01
Epoch 8/10
19/19 - 2s - loss: 191.3717 - loglik: -1.9052e+02 - logprior: -8.4908e-01
Epoch 9/10
19/19 - 2s - loss: 191.1473 - loglik: -1.9031e+02 - logprior: -8.3428e-01
Epoch 10/10
19/19 - 2s - loss: 190.4130 - loglik: -1.8959e+02 - logprior: -8.1887e-01
Fitted a model with MAP estimate = -190.2756
Time for alignment: 63.5597
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 252.5138 - loglik: -2.4945e+02 - logprior: -3.0675e+00
Epoch 2/10
19/19 - 1s - loss: 220.1460 - loglik: -2.1886e+02 - logprior: -1.2813e+00
Epoch 3/10
19/19 - 2s - loss: 205.2954 - loglik: -2.0382e+02 - logprior: -1.4736e+00
Epoch 4/10
19/19 - 2s - loss: 202.4176 - loglik: -2.0106e+02 - logprior: -1.3610e+00
Epoch 5/10
19/19 - 2s - loss: 201.3129 - loglik: -1.9995e+02 - logprior: -1.3582e+00
Epoch 6/10
19/19 - 2s - loss: 200.9520 - loglik: -1.9962e+02 - logprior: -1.3314e+00
Epoch 7/10
19/19 - 2s - loss: 200.5230 - loglik: -1.9921e+02 - logprior: -1.3169e+00
Epoch 8/10
19/19 - 2s - loss: 200.3311 - loglik: -1.9902e+02 - logprior: -1.3064e+00
Epoch 9/10
19/19 - 2s - loss: 199.6275 - loglik: -1.9832e+02 - logprior: -1.3067e+00
Epoch 10/10
19/19 - 2s - loss: 199.7538 - loglik: -1.9846e+02 - logprior: -1.2966e+00
Fitted a model with MAP estimate = -199.4407
expansions: [(8, 1), (9, 3), (10, 1), (13, 1), (14, 1), (19, 1), (20, 1), (34, 1), (39, 2), (40, 2), (42, 2), (49, 2), (58, 1), (59, 1), (60, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 211.1578 - loglik: -2.0731e+02 - logprior: -3.8472e+00
Epoch 2/2
19/19 - 2s - loss: 199.8764 - loglik: -1.9782e+02 - logprior: -2.0526e+00
Fitted a model with MAP estimate = -197.8748
expansions: [(0, 2)]
discards: [ 0 10 49 65]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 200.2950 - loglik: -1.9745e+02 - logprior: -2.8476e+00
Epoch 2/2
19/19 - 2s - loss: 196.4862 - loglik: -1.9543e+02 - logprior: -1.0593e+00
Fitted a model with MAP estimate = -195.4196
expansions: [(54, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 201.6390 - loglik: -1.9795e+02 - logprior: -3.6899e+00
Epoch 2/10
19/19 - 2s - loss: 196.4767 - loglik: -1.9524e+02 - logprior: -1.2319e+00
Epoch 3/10
19/19 - 2s - loss: 195.2365 - loglik: -1.9427e+02 - logprior: -9.6832e-01
Epoch 4/10
19/19 - 2s - loss: 194.3115 - loglik: -1.9339e+02 - logprior: -9.2391e-01
Epoch 5/10
19/19 - 2s - loss: 193.2854 - loglik: -1.9239e+02 - logprior: -8.9635e-01
Epoch 6/10
19/19 - 2s - loss: 192.7826 - loglik: -1.9191e+02 - logprior: -8.7301e-01
Epoch 7/10
19/19 - 2s - loss: 192.2007 - loglik: -1.9134e+02 - logprior: -8.6474e-01
Epoch 8/10
19/19 - 2s - loss: 191.3314 - loglik: -1.9048e+02 - logprior: -8.5119e-01
Epoch 9/10
19/19 - 2s - loss: 191.2318 - loglik: -1.9040e+02 - logprior: -8.3356e-01
Epoch 10/10
19/19 - 2s - loss: 190.6991 - loglik: -1.8987e+02 - logprior: -8.3136e-01
Fitted a model with MAP estimate = -190.2567
Time for alignment: 62.9390
Computed alignments with likelihoods: ['-190.5270', '-190.2756', '-190.2567']
Best model has likelihood: -190.2567
SP score = 0.8134
Training of 3 independent models on file PF07686.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96164ed760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9638825970>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 301.5563 - loglik: -2.9852e+02 - logprior: -3.0413e+00
Epoch 2/10
19/19 - 2s - loss: 271.8196 - loglik: -2.7059e+02 - logprior: -1.2342e+00
Epoch 3/10
19/19 - 2s - loss: 257.2047 - loglik: -2.5600e+02 - logprior: -1.2008e+00
Epoch 4/10
19/19 - 2s - loss: 254.1162 - loglik: -2.5302e+02 - logprior: -1.0941e+00
Epoch 5/10
19/19 - 2s - loss: 252.0013 - loglik: -2.5093e+02 - logprior: -1.0741e+00
Epoch 6/10
19/19 - 2s - loss: 251.0470 - loglik: -2.4999e+02 - logprior: -1.0606e+00
Epoch 7/10
19/19 - 2s - loss: 250.2846 - loglik: -2.4923e+02 - logprior: -1.0540e+00
Epoch 8/10
19/19 - 2s - loss: 249.9333 - loglik: -2.4889e+02 - logprior: -1.0471e+00
Epoch 9/10
19/19 - 2s - loss: 249.2547 - loglik: -2.4821e+02 - logprior: -1.0474e+00
Epoch 10/10
19/19 - 2s - loss: 249.2529 - loglik: -2.4821e+02 - logprior: -1.0396e+00
Fitted a model with MAP estimate = -248.3313
expansions: [(0, 2), (21, 3), (22, 2), (23, 1), (24, 1), (39, 2), (41, 3), (42, 1), (51, 2), (52, 1), (74, 1), (75, 3), (76, 2), (77, 2), (78, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 111 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 268.8219 - loglik: -2.6467e+02 - logprior: -4.1531e+00
Epoch 2/2
19/19 - 2s - loss: 254.2337 - loglik: -2.5296e+02 - logprior: -1.2759e+00
Fitted a model with MAP estimate = -250.8937
expansions: []
discards: [  1  55 100]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 255.3750 - loglik: -2.5256e+02 - logprior: -2.8178e+00
Epoch 2/2
19/19 - 2s - loss: 251.4871 - loglik: -2.5040e+02 - logprior: -1.0865e+00
Fitted a model with MAP estimate = -249.8595
expansions: [(50, 1), (52, 1)]
discards: [ 0 28]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 255.2818 - loglik: -2.5193e+02 - logprior: -3.3566e+00
Epoch 2/10
19/19 - 2s - loss: 250.7317 - loglik: -2.4965e+02 - logprior: -1.0802e+00
Epoch 3/10
19/19 - 2s - loss: 249.0960 - loglik: -2.4826e+02 - logprior: -8.3902e-01
Epoch 4/10
19/19 - 2s - loss: 247.7525 - loglik: -2.4696e+02 - logprior: -7.8812e-01
Epoch 5/10
19/19 - 2s - loss: 246.2484 - loglik: -2.4548e+02 - logprior: -7.6361e-01
Epoch 6/10
19/19 - 2s - loss: 244.9207 - loglik: -2.4418e+02 - logprior: -7.4081e-01
Epoch 7/10
19/19 - 2s - loss: 243.9417 - loglik: -2.4322e+02 - logprior: -7.2459e-01
Epoch 8/10
19/19 - 2s - loss: 243.2094 - loglik: -2.4251e+02 - logprior: -7.0252e-01
Epoch 9/10
19/19 - 2s - loss: 241.4201 - loglik: -2.4073e+02 - logprior: -6.9329e-01
Epoch 10/10
19/19 - 2s - loss: 240.1940 - loglik: -2.3950e+02 - logprior: -6.9190e-01
Fitted a model with MAP estimate = -238.9535
Time for alignment: 74.9066
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 301.3236 - loglik: -2.9828e+02 - logprior: -3.0411e+00
Epoch 2/10
19/19 - 2s - loss: 270.9811 - loglik: -2.6975e+02 - logprior: -1.2302e+00
Epoch 3/10
19/19 - 2s - loss: 257.5051 - loglik: -2.5630e+02 - logprior: -1.2008e+00
Epoch 4/10
19/19 - 2s - loss: 254.3240 - loglik: -2.5323e+02 - logprior: -1.0945e+00
Epoch 5/10
19/19 - 2s - loss: 252.2037 - loglik: -2.5112e+02 - logprior: -1.0795e+00
Epoch 6/10
19/19 - 2s - loss: 251.0081 - loglik: -2.4995e+02 - logprior: -1.0626e+00
Epoch 7/10
19/19 - 2s - loss: 250.7139 - loglik: -2.4966e+02 - logprior: -1.0587e+00
Epoch 8/10
19/19 - 2s - loss: 249.7593 - loglik: -2.4871e+02 - logprior: -1.0510e+00
Epoch 9/10
19/19 - 2s - loss: 249.8209 - loglik: -2.4878e+02 - logprior: -1.0456e+00
Fitted a model with MAP estimate = -248.7414
expansions: [(0, 2), (21, 3), (22, 2), (23, 1), (24, 1), (32, 1), (39, 1), (41, 2), (42, 1), (44, 2), (53, 1), (54, 1), (74, 1), (75, 2), (76, 2), (77, 3), (78, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 266.8650 - loglik: -2.6275e+02 - logprior: -4.1131e+00
Epoch 2/2
19/19 - 2s - loss: 253.6943 - loglik: -2.5242e+02 - logprior: -1.2772e+00
Fitted a model with MAP estimate = -250.7265
expansions: [(53, 1), (102, 1)]
discards: [ 1 28 54 58 95]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 255.3830 - loglik: -2.5257e+02 - logprior: -2.8141e+00
Epoch 2/2
19/19 - 2s - loss: 251.3944 - loglik: -2.5034e+02 - logprior: -1.0541e+00
Fitted a model with MAP estimate = -249.7174
expansions: []
discards: [94]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 253.9678 - loglik: -2.5122e+02 - logprior: -2.7436e+00
Epoch 2/10
19/19 - 2s - loss: 250.8707 - loglik: -2.4982e+02 - logprior: -1.0499e+00
Epoch 3/10
19/19 - 2s - loss: 249.0518 - loglik: -2.4814e+02 - logprior: -9.1165e-01
Epoch 4/10
19/19 - 2s - loss: 247.9501 - loglik: -2.4707e+02 - logprior: -8.8061e-01
Epoch 5/10
19/19 - 2s - loss: 246.3723 - loglik: -2.4551e+02 - logprior: -8.6127e-01
Epoch 6/10
19/19 - 2s - loss: 245.3418 - loglik: -2.4450e+02 - logprior: -8.3738e-01
Epoch 7/10
19/19 - 2s - loss: 243.9069 - loglik: -2.4309e+02 - logprior: -8.2157e-01
Epoch 8/10
19/19 - 2s - loss: 243.4737 - loglik: -2.4267e+02 - logprior: -8.0219e-01
Epoch 9/10
19/19 - 2s - loss: 241.8718 - loglik: -2.4107e+02 - logprior: -8.0003e-01
Epoch 10/10
19/19 - 2s - loss: 240.7576 - loglik: -2.3997e+02 - logprior: -7.9135e-01
Fitted a model with MAP estimate = -239.6707
Time for alignment: 73.5594
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 301.5280 - loglik: -2.9849e+02 - logprior: -3.0371e+00
Epoch 2/10
19/19 - 2s - loss: 274.0026 - loglik: -2.7276e+02 - logprior: -1.2393e+00
Epoch 3/10
19/19 - 2s - loss: 259.9302 - loglik: -2.5873e+02 - logprior: -1.2019e+00
Epoch 4/10
19/19 - 2s - loss: 255.4607 - loglik: -2.5439e+02 - logprior: -1.0661e+00
Epoch 5/10
19/19 - 2s - loss: 253.3907 - loglik: -2.5235e+02 - logprior: -1.0405e+00
Epoch 6/10
19/19 - 2s - loss: 251.7659 - loglik: -2.5072e+02 - logprior: -1.0498e+00
Epoch 7/10
19/19 - 2s - loss: 250.7001 - loglik: -2.4964e+02 - logprior: -1.0603e+00
Epoch 8/10
19/19 - 2s - loss: 250.1654 - loglik: -2.4911e+02 - logprior: -1.0540e+00
Epoch 9/10
19/19 - 2s - loss: 249.6952 - loglik: -2.4863e+02 - logprior: -1.0636e+00
Epoch 10/10
19/19 - 2s - loss: 249.3084 - loglik: -2.4825e+02 - logprior: -1.0573e+00
Fitted a model with MAP estimate = -248.2970
expansions: [(0, 2), (20, 3), (21, 2), (22, 1), (28, 1), (39, 2), (42, 2), (44, 1), (45, 1), (46, 1), (51, 2), (52, 2), (53, 1), (75, 3), (76, 2), (77, 2), (78, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 113 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 270.2714 - loglik: -2.6612e+02 - logprior: -4.1463e+00
Epoch 2/2
19/19 - 2s - loss: 254.7192 - loglik: -2.5340e+02 - logprior: -1.3147e+00
Fitted a model with MAP estimate = -251.0853
expansions: []
discards: [  1  28  48  53  71 102]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 255.5501 - loglik: -2.5275e+02 - logprior: -2.8040e+00
Epoch 2/2
19/19 - 2s - loss: 251.6355 - loglik: -2.5058e+02 - logprior: -1.0535e+00
Fitted a model with MAP estimate = -249.8180
expansions: [(22, 1), (100, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 253.9679 - loglik: -2.5125e+02 - logprior: -2.7137e+00
Epoch 2/10
19/19 - 2s - loss: 250.3702 - loglik: -2.4934e+02 - logprior: -1.0309e+00
Epoch 3/10
19/19 - 2s - loss: 249.0771 - loglik: -2.4819e+02 - logprior: -8.8562e-01
Epoch 4/10
19/19 - 2s - loss: 247.5306 - loglik: -2.4668e+02 - logprior: -8.5020e-01
Epoch 5/10
19/19 - 2s - loss: 246.5700 - loglik: -2.4575e+02 - logprior: -8.2216e-01
Epoch 6/10
19/19 - 2s - loss: 244.8449 - loglik: -2.4404e+02 - logprior: -8.0638e-01
Epoch 7/10
19/19 - 2s - loss: 244.0995 - loglik: -2.4332e+02 - logprior: -7.7861e-01
Epoch 8/10
19/19 - 2s - loss: 243.1814 - loglik: -2.4241e+02 - logprior: -7.6715e-01
Epoch 9/10
19/19 - 2s - loss: 242.1633 - loglik: -2.4141e+02 - logprior: -7.5773e-01
Epoch 10/10
19/19 - 2s - loss: 240.8104 - loglik: -2.4006e+02 - logprior: -7.5177e-01
Fitted a model with MAP estimate = -239.5558
Time for alignment: 75.4593
Computed alignments with likelihoods: ['-238.9535', '-239.6707', '-239.5558']
Best model has likelihood: -238.9535
SP score = 0.8808
Training of 3 independent models on file PF00505.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9649f91550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95e3344d00>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 186.5496 - loglik: -1.8335e+02 - logprior: -3.1969e+00
Epoch 2/10
19/19 - 1s - loss: 156.8649 - loglik: -1.5535e+02 - logprior: -1.5113e+00
Epoch 3/10
19/19 - 1s - loss: 141.3179 - loglik: -1.3961e+02 - logprior: -1.7049e+00
Epoch 4/10
19/19 - 1s - loss: 137.0013 - loglik: -1.3533e+02 - logprior: -1.6667e+00
Epoch 5/10
19/19 - 1s - loss: 135.5169 - loglik: -1.3389e+02 - logprior: -1.6314e+00
Epoch 6/10
19/19 - 1s - loss: 134.4768 - loglik: -1.3285e+02 - logprior: -1.6273e+00
Epoch 7/10
19/19 - 1s - loss: 134.2631 - loglik: -1.3266e+02 - logprior: -1.6039e+00
Epoch 8/10
19/19 - 1s - loss: 134.0169 - loglik: -1.3241e+02 - logprior: -1.6085e+00
Epoch 9/10
19/19 - 1s - loss: 133.5427 - loglik: -1.3194e+02 - logprior: -1.6012e+00
Epoch 10/10
19/19 - 1s - loss: 133.2303 - loglik: -1.3163e+02 - logprior: -1.6038e+00
Fitted a model with MAP estimate = -133.0551
expansions: [(14, 1), (16, 1), (17, 1), (21, 2), (22, 1), (23, 1), (30, 1), (33, 1), (35, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 137.0017 - loglik: -1.3369e+02 - logprior: -3.3133e+00
Epoch 2/2
19/19 - 1s - loss: 128.1155 - loglik: -1.2676e+02 - logprior: -1.3521e+00
Fitted a model with MAP estimate = -127.0815
expansions: []
discards: [25]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 130.7779 - loglik: -1.2761e+02 - logprior: -3.1719e+00
Epoch 2/2
19/19 - 1s - loss: 127.6798 - loglik: -1.2638e+02 - logprior: -1.2967e+00
Fitted a model with MAP estimate = -126.7791
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 130.3921 - loglik: -1.2725e+02 - logprior: -3.1464e+00
Epoch 2/10
19/19 - 1s - loss: 127.4507 - loglik: -1.2618e+02 - logprior: -1.2753e+00
Epoch 3/10
19/19 - 1s - loss: 126.5601 - loglik: -1.2543e+02 - logprior: -1.1318e+00
Epoch 4/10
19/19 - 1s - loss: 125.6373 - loglik: -1.2455e+02 - logprior: -1.0912e+00
Epoch 5/10
19/19 - 1s - loss: 125.0940 - loglik: -1.2401e+02 - logprior: -1.0842e+00
Epoch 6/10
19/19 - 1s - loss: 124.6560 - loglik: -1.2359e+02 - logprior: -1.0674e+00
Epoch 7/10
19/19 - 1s - loss: 123.9650 - loglik: -1.2290e+02 - logprior: -1.0603e+00
Epoch 8/10
19/19 - 1s - loss: 123.8255 - loglik: -1.2278e+02 - logprior: -1.0407e+00
Epoch 9/10
19/19 - 1s - loss: 123.7077 - loglik: -1.2267e+02 - logprior: -1.0352e+00
Epoch 10/10
19/19 - 1s - loss: 123.6162 - loglik: -1.2259e+02 - logprior: -1.0276e+00
Fitted a model with MAP estimate = -123.3217
Time for alignment: 49.1364
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 186.1847 - loglik: -1.8299e+02 - logprior: -3.1970e+00
Epoch 2/10
19/19 - 1s - loss: 154.5625 - loglik: -1.5312e+02 - logprior: -1.4405e+00
Epoch 3/10
19/19 - 1s - loss: 143.6631 - loglik: -1.4205e+02 - logprior: -1.6136e+00
Epoch 4/10
19/19 - 1s - loss: 139.0752 - loglik: -1.3757e+02 - logprior: -1.5081e+00
Epoch 5/10
19/19 - 1s - loss: 137.6687 - loglik: -1.3612e+02 - logprior: -1.5467e+00
Epoch 6/10
19/19 - 1s - loss: 136.9981 - loglik: -1.3548e+02 - logprior: -1.5177e+00
Epoch 7/10
19/19 - 1s - loss: 136.5381 - loglik: -1.3503e+02 - logprior: -1.5105e+00
Epoch 8/10
19/19 - 1s - loss: 136.3164 - loglik: -1.3483e+02 - logprior: -1.4906e+00
Epoch 9/10
19/19 - 1s - loss: 136.1877 - loglik: -1.3470e+02 - logprior: -1.4869e+00
Epoch 10/10
19/19 - 1s - loss: 136.0507 - loglik: -1.3457e+02 - logprior: -1.4826e+00
Fitted a model with MAP estimate = -135.8792
expansions: [(12, 1), (16, 2), (17, 2), (18, 1), (21, 1), (22, 2), (23, 1), (29, 1), (35, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 141.2999 - loglik: -1.3721e+02 - logprior: -4.0918e+00
Epoch 2/2
19/19 - 1s - loss: 131.1863 - loglik: -1.2912e+02 - logprior: -2.0621e+00
Fitted a model with MAP estimate = -129.2970
expansions: [(0, 2)]
discards: [ 0 21 28]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 130.7533 - loglik: -1.2777e+02 - logprior: -2.9836e+00
Epoch 2/2
19/19 - 1s - loss: 127.3140 - loglik: -1.2616e+02 - logprior: -1.1533e+00
Fitted a model with MAP estimate = -126.4375
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 133.0319 - loglik: -1.2925e+02 - logprior: -3.7791e+00
Epoch 2/10
19/19 - 1s - loss: 127.8902 - loglik: -1.2654e+02 - logprior: -1.3470e+00
Epoch 3/10
19/19 - 1s - loss: 126.9197 - loglik: -1.2579e+02 - logprior: -1.1293e+00
Epoch 4/10
19/19 - 1s - loss: 126.0310 - loglik: -1.2492e+02 - logprior: -1.1078e+00
Epoch 5/10
19/19 - 1s - loss: 125.2307 - loglik: -1.2414e+02 - logprior: -1.0887e+00
Epoch 6/10
19/19 - 1s - loss: 124.7527 - loglik: -1.2367e+02 - logprior: -1.0826e+00
Epoch 7/10
19/19 - 1s - loss: 124.4033 - loglik: -1.2334e+02 - logprior: -1.0647e+00
Epoch 8/10
19/19 - 1s - loss: 124.2178 - loglik: -1.2317e+02 - logprior: -1.0517e+00
Epoch 9/10
19/19 - 1s - loss: 123.9664 - loglik: -1.2292e+02 - logprior: -1.0495e+00
Epoch 10/10
19/19 - 1s - loss: 123.6849 - loglik: -1.2265e+02 - logprior: -1.0374e+00
Fitted a model with MAP estimate = -123.5640
Time for alignment: 49.7617
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 186.3576 - loglik: -1.8316e+02 - logprior: -3.1942e+00
Epoch 2/10
19/19 - 1s - loss: 155.9546 - loglik: -1.5453e+02 - logprior: -1.4210e+00
Epoch 3/10
19/19 - 1s - loss: 142.7448 - loglik: -1.4117e+02 - logprior: -1.5709e+00
Epoch 4/10
19/19 - 1s - loss: 139.5176 - loglik: -1.3805e+02 - logprior: -1.4665e+00
Epoch 5/10
19/19 - 1s - loss: 138.3887 - loglik: -1.3689e+02 - logprior: -1.4943e+00
Epoch 6/10
19/19 - 1s - loss: 137.9039 - loglik: -1.3644e+02 - logprior: -1.4652e+00
Epoch 7/10
19/19 - 1s - loss: 137.4080 - loglik: -1.3594e+02 - logprior: -1.4651e+00
Epoch 8/10
19/19 - 1s - loss: 137.1641 - loglik: -1.3570e+02 - logprior: -1.4619e+00
Epoch 9/10
19/19 - 1s - loss: 136.9403 - loglik: -1.3548e+02 - logprior: -1.4587e+00
Epoch 10/10
19/19 - 1s - loss: 136.8401 - loglik: -1.3538e+02 - logprior: -1.4600e+00
Fitted a model with MAP estimate = -136.5850
expansions: [(12, 1), (16, 1), (17, 1), (18, 1), (22, 5), (23, 1), (34, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 141.2322 - loglik: -1.3714e+02 - logprior: -4.0905e+00
Epoch 2/2
19/19 - 1s - loss: 130.8024 - loglik: -1.2876e+02 - logprior: -2.0423e+00
Fitted a model with MAP estimate = -129.0096
expansions: [(0, 2)]
discards: [ 0 27]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 130.4918 - loglik: -1.2751e+02 - logprior: -2.9832e+00
Epoch 2/2
19/19 - 1s - loss: 127.0690 - loglik: -1.2592e+02 - logprior: -1.1493e+00
Fitted a model with MAP estimate = -126.1793
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 132.6595 - loglik: -1.2888e+02 - logprior: -3.7747e+00
Epoch 2/10
19/19 - 1s - loss: 127.9469 - loglik: -1.2661e+02 - logprior: -1.3322e+00
Epoch 3/10
19/19 - 1s - loss: 126.5235 - loglik: -1.2539e+02 - logprior: -1.1312e+00
Epoch 4/10
19/19 - 1s - loss: 125.6375 - loglik: -1.2453e+02 - logprior: -1.1072e+00
Epoch 5/10
19/19 - 1s - loss: 125.2420 - loglik: -1.2416e+02 - logprior: -1.0825e+00
Epoch 6/10
19/19 - 1s - loss: 124.5332 - loglik: -1.2346e+02 - logprior: -1.0767e+00
Epoch 7/10
19/19 - 1s - loss: 124.1382 - loglik: -1.2307e+02 - logprior: -1.0696e+00
Epoch 8/10
19/19 - 1s - loss: 123.9146 - loglik: -1.2286e+02 - logprior: -1.0529e+00
Epoch 9/10
19/19 - 1s - loss: 123.7532 - loglik: -1.2271e+02 - logprior: -1.0397e+00
Epoch 10/10
19/19 - 1s - loss: 123.3834 - loglik: -1.2235e+02 - logprior: -1.0379e+00
Fitted a model with MAP estimate = -123.3842
Time for alignment: 48.8944
Computed alignments with likelihoods: ['-123.3217', '-123.5640', '-123.3842']
Best model has likelihood: -123.3217
SP score = 0.9424
Training of 3 independent models on file PF13393.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9638ca6f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95d9504190>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 19s - loss: 787.9776 - loglik: -7.8558e+02 - logprior: -2.4008e+00
Epoch 2/10
19/19 - 16s - loss: 675.5797 - loglik: -6.7465e+02 - logprior: -9.3034e-01
Epoch 3/10
19/19 - 16s - loss: 619.6216 - loglik: -6.1804e+02 - logprior: -1.5859e+00
Epoch 4/10
19/19 - 16s - loss: 606.5175 - loglik: -6.0480e+02 - logprior: -1.7154e+00
Epoch 5/10
19/19 - 16s - loss: 601.3325 - loglik: -5.9958e+02 - logprior: -1.7493e+00
Epoch 6/10
19/19 - 16s - loss: 597.1392 - loglik: -5.9539e+02 - logprior: -1.7489e+00
Epoch 7/10
19/19 - 16s - loss: 595.2642 - loglik: -5.9353e+02 - logprior: -1.7365e+00
Epoch 8/10
19/19 - 16s - loss: 593.5317 - loglik: -5.9180e+02 - logprior: -1.7313e+00
Epoch 9/10
19/19 - 16s - loss: 594.7944 - loglik: -5.9307e+02 - logprior: -1.7236e+00
Fitted a model with MAP estimate = -592.5552
expansions: [(11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (33, 1), (34, 1), (42, 2), (44, 2), (45, 1), (46, 1), (51, 1), (67, 1), (68, 1), (69, 2), (70, 1), (72, 1), (78, 1), (93, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (123, 1), (132, 1), (135, 2), (136, 2), (137, 2), (140, 1), (148, 1), (149, 1), (154, 1), (163, 1), (164, 2), (175, 1), (179, 1), (183, 1), (185, 1), (192, 3), (193, 1), (196, 1), (211, 1), (212, 3), (213, 1), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 593.6252 - loglik: -5.9110e+02 - logprior: -2.5284e+00
Epoch 2/2
39/39 - 25s - loss: 572.1622 - loglik: -5.7081e+02 - logprior: -1.3522e+00
Fitted a model with MAP estimate = -566.4142
expansions: [(147, 2), (208, 1)]
discards: [ 50  54  87 174]
Re-initialized the encoder parameters.
Fitting a model of length 306 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 575.5673 - loglik: -5.7404e+02 - logprior: -1.5296e+00
Epoch 2/2
39/39 - 25s - loss: 567.2423 - loglik: -5.6695e+02 - logprior: -2.8977e-01
Fitted a model with MAP estimate = -563.2996
expansions: [(211, 7), (218, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 315 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 30s - loss: 570.0770 - loglik: -5.6881e+02 - logprior: -1.2709e+00
Epoch 2/10
39/39 - 26s - loss: 561.8637 - loglik: -5.6174e+02 - logprior: -1.2289e-01
Epoch 3/10
39/39 - 26s - loss: 557.8572 - loglik: -5.5791e+02 - logprior: 0.0570
Epoch 4/10
39/39 - 26s - loss: 556.3586 - loglik: -5.5653e+02 - logprior: 0.1745
Epoch 5/10
39/39 - 26s - loss: 554.1357 - loglik: -5.5441e+02 - logprior: 0.2783
Epoch 6/10
39/39 - 26s - loss: 553.0490 - loglik: -5.5340e+02 - logprior: 0.3508
Epoch 7/10
39/39 - 26s - loss: 552.8134 - loglik: -5.5333e+02 - logprior: 0.5177
Epoch 8/10
39/39 - 26s - loss: 552.0945 - loglik: -5.5272e+02 - logprior: 0.6279
Epoch 9/10
39/39 - 26s - loss: 551.8557 - loglik: -5.5261e+02 - logprior: 0.7539
Epoch 10/10
39/39 - 26s - loss: 551.9597 - loglik: -5.5284e+02 - logprior: 0.8755
Fitted a model with MAP estimate = -550.6863
Time for alignment: 646.8934
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 19s - loss: 788.1728 - loglik: -7.8577e+02 - logprior: -2.3986e+00
Epoch 2/10
19/19 - 16s - loss: 671.5859 - loglik: -6.7067e+02 - logprior: -9.1868e-01
Epoch 3/10
19/19 - 16s - loss: 617.4482 - loglik: -6.1593e+02 - logprior: -1.5183e+00
Epoch 4/10
19/19 - 16s - loss: 603.6113 - loglik: -6.0201e+02 - logprior: -1.6041e+00
Epoch 5/10
19/19 - 16s - loss: 599.8958 - loglik: -5.9828e+02 - logprior: -1.6155e+00
Epoch 6/10
19/19 - 16s - loss: 597.2114 - loglik: -5.9557e+02 - logprior: -1.6431e+00
Epoch 7/10
19/19 - 16s - loss: 596.6504 - loglik: -5.9497e+02 - logprior: -1.6779e+00
Epoch 8/10
19/19 - 16s - loss: 593.6516 - loglik: -5.9193e+02 - logprior: -1.7173e+00
Epoch 9/10
19/19 - 16s - loss: 595.9193 - loglik: -5.9419e+02 - logprior: -1.7300e+00
Fitted a model with MAP estimate = -593.3488
expansions: [(10, 1), (11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (18, 1), (31, 1), (37, 1), (41, 1), (45, 1), (46, 1), (51, 1), (52, 1), (66, 1), (67, 1), (68, 1), (70, 1), (76, 1), (77, 1), (112, 1), (114, 1), (115, 1), (120, 4), (121, 1), (123, 1), (132, 1), (134, 1), (136, 2), (137, 1), (147, 1), (154, 3), (163, 2), (164, 3), (168, 1), (177, 1), (179, 1), (183, 1), (185, 1), (192, 3), (193, 1), (196, 1), (211, 1), (212, 3), (213, 1), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 306 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 592.3475 - loglik: -5.8983e+02 - logprior: -2.5192e+00
Epoch 2/2
39/39 - 25s - loss: 571.9170 - loglik: -5.7054e+02 - logprior: -1.3770e+00
Fitted a model with MAP estimate = -566.4602
expansions: [(0, 3), (145, 2), (195, 5), (221, 1)]
discards: [  0 203]
Re-initialized the encoder parameters.
Fitting a model of length 315 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 573.5300 - loglik: -5.7208e+02 - logprior: -1.4500e+00
Epoch 2/2
39/39 - 26s - loss: 564.8771 - loglik: -5.6449e+02 - logprior: -3.8513e-01
Fitted a model with MAP estimate = -561.3000
expansions: []
discards: [  0   1 147]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 574.1416 - loglik: -5.7215e+02 - logprior: -1.9925e+00
Epoch 2/10
39/39 - 26s - loss: 564.9052 - loglik: -5.6472e+02 - logprior: -1.8097e-01
Epoch 3/10
39/39 - 26s - loss: 560.6613 - loglik: -5.6080e+02 - logprior: 0.1387
Epoch 4/10
39/39 - 26s - loss: 558.3058 - loglik: -5.5855e+02 - logprior: 0.2472
Epoch 5/10
39/39 - 26s - loss: 556.4117 - loglik: -5.5676e+02 - logprior: 0.3524
Epoch 6/10
39/39 - 26s - loss: 555.7386 - loglik: -5.5621e+02 - logprior: 0.4746
Epoch 7/10
39/39 - 26s - loss: 554.7088 - loglik: -5.5529e+02 - logprior: 0.5827
Epoch 8/10
39/39 - 26s - loss: 554.9753 - loglik: -5.5564e+02 - logprior: 0.6641
Fitted a model with MAP estimate = -553.7471
Time for alignment: 594.4114
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 19s - loss: 788.5487 - loglik: -7.8615e+02 - logprior: -2.3997e+00
Epoch 2/10
19/19 - 16s - loss: 670.7958 - loglik: -6.6986e+02 - logprior: -9.3336e-01
Epoch 3/10
19/19 - 16s - loss: 615.9020 - loglik: -6.1437e+02 - logprior: -1.5365e+00
Epoch 4/10
19/19 - 16s - loss: 601.9899 - loglik: -6.0034e+02 - logprior: -1.6518e+00
Epoch 5/10
19/19 - 16s - loss: 595.5146 - loglik: -5.9383e+02 - logprior: -1.6803e+00
Epoch 6/10
19/19 - 16s - loss: 594.8885 - loglik: -5.9326e+02 - logprior: -1.6293e+00
Epoch 7/10
19/19 - 16s - loss: 593.6464 - loglik: -5.9203e+02 - logprior: -1.6187e+00
Epoch 8/10
19/19 - 16s - loss: 594.4937 - loglik: -5.9285e+02 - logprior: -1.6434e+00
Fitted a model with MAP estimate = -591.5181
expansions: [(11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (33, 1), (34, 1), (42, 2), (44, 2), (45, 1), (46, 1), (51, 1), (67, 1), (68, 1), (69, 2), (70, 1), (76, 1), (86, 1), (101, 1), (111, 1), (115, 1), (116, 1), (120, 5), (132, 1), (134, 1), (136, 2), (137, 1), (149, 1), (153, 3), (162, 3), (164, 3), (165, 1), (176, 1), (177, 1), (179, 1), (183, 1), (184, 1), (185, 1), (192, 1), (193, 1), (196, 1), (210, 1), (211, 3), (213, 3), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 309 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 591.6201 - loglik: -5.8914e+02 - logprior: -2.4817e+00
Epoch 2/2
39/39 - 25s - loss: 571.8731 - loglik: -5.7052e+02 - logprior: -1.3502e+00
Fitted a model with MAP estimate = -565.8232
expansions: [(149, 1), (196, 2), (205, 2), (245, 1)]
discards: [ 50  54 273]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 573.8909 - loglik: -5.7236e+02 - logprior: -1.5261e+00
Epoch 2/2
39/39 - 26s - loss: 565.2855 - loglik: -5.6498e+02 - logprior: -3.0543e-01
Fitted a model with MAP estimate = -561.4432
expansions: [(207, 1)]
discards: [189]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 571.0249 - loglik: -5.6978e+02 - logprior: -1.2423e+00
Epoch 2/10
39/39 - 26s - loss: 564.0760 - loglik: -5.6402e+02 - logprior: -5.1187e-02
Epoch 3/10
39/39 - 26s - loss: 561.0131 - loglik: -5.6114e+02 - logprior: 0.1233
Epoch 4/10
39/39 - 26s - loss: 557.7928 - loglik: -5.5804e+02 - logprior: 0.2489
Epoch 5/10
39/39 - 26s - loss: 556.7996 - loglik: -5.5715e+02 - logprior: 0.3485
Epoch 6/10
39/39 - 26s - loss: 555.8825 - loglik: -5.5634e+02 - logprior: 0.4582
Epoch 7/10
39/39 - 26s - loss: 555.3981 - loglik: -5.5596e+02 - logprior: 0.5647
Epoch 8/10
39/39 - 26s - loss: 554.4703 - loglik: -5.5517e+02 - logprior: 0.6996
Epoch 9/10
39/39 - 26s - loss: 553.7700 - loglik: -5.5459e+02 - logprior: 0.8174
Epoch 10/10
39/39 - 26s - loss: 553.7656 - loglik: -5.5471e+02 - logprior: 0.9429
Fitted a model with MAP estimate = -552.8494
Time for alignment: 631.8713
Computed alignments with likelihoods: ['-550.6863', '-553.7471', '-552.8494']
Best model has likelihood: -550.6863
SP score = 0.4032
Training of 3 independent models on file PF00202.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9690d762e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9545d46700>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 968.6227 - loglik: -9.6729e+02 - logprior: -1.3323e+00
Epoch 2/10
39/39 - 33s - loss: 801.0365 - loglik: -8.0000e+02 - logprior: -1.0404e+00
Epoch 3/10
39/39 - 33s - loss: 785.2421 - loglik: -7.8415e+02 - logprior: -1.0960e+00
Epoch 4/10
39/39 - 33s - loss: 781.4264 - loglik: -7.8031e+02 - logprior: -1.1127e+00
Epoch 5/10
39/39 - 33s - loss: 779.6517 - loglik: -7.7854e+02 - logprior: -1.1163e+00
Epoch 6/10
39/39 - 33s - loss: 779.3126 - loglik: -7.7815e+02 - logprior: -1.1596e+00
Epoch 7/10
39/39 - 33s - loss: 778.5536 - loglik: -7.7741e+02 - logprior: -1.1468e+00
Epoch 8/10
39/39 - 33s - loss: 778.1253 - loglik: -7.7691e+02 - logprior: -1.2201e+00
Epoch 9/10
39/39 - 33s - loss: 777.7151 - loglik: -7.7656e+02 - logprior: -1.1591e+00
Epoch 10/10
39/39 - 33s - loss: 777.1565 - loglik: -7.7600e+02 - logprior: -1.1577e+00
Fitted a model with MAP estimate = -768.6699
expansions: [(0, 3), (19, 1), (47, 1), (51, 1), (52, 1), (56, 2), (69, 1), (70, 1), (74, 2), (99, 1), (120, 2), (121, 2), (122, 2), (143, 1), (145, 1), (146, 2), (147, 1), (148, 1), (164, 1), (170, 1), (173, 1), (175, 1), (176, 1), (193, 2), (194, 1), (195, 1), (196, 1), (218, 1), (219, 1), (220, 1), (222, 1), (223, 1), (224, 3), (247, 1), (248, 1), (252, 1), (254, 1), (255, 1), (257, 1), (263, 1), (264, 3), (265, 2), (286, 4), (287, 6), (288, 3), (296, 2), (297, 1), (298, 3), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 400 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 794.9789 - loglik: -7.9289e+02 - logprior: -2.0861e+00
Epoch 2/2
39/39 - 48s - loss: 752.1380 - loglik: -7.5164e+02 - logprior: -4.9488e-01
Fitted a model with MAP estimate = -739.7172
expansions: [(83, 1), (348, 1)]
discards: [ 64  85 138 139 167 171 225 319 368 394]
Re-initialized the encoder parameters.
Fitting a model of length 392 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 760.9133 - loglik: -7.5997e+02 - logprior: -9.4568e-01
Epoch 2/2
39/39 - 46s - loss: 749.5573 - loglik: -7.4976e+02 - logprior: 0.2002
Fitted a model with MAP estimate = -738.1976
expansions: [(0, 2)]
discards: [  1 260]
Re-initialized the encoder parameters.
Fitting a model of length 392 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 753.1512 - loglik: -7.5159e+02 - logprior: -1.5629e+00
Epoch 2/10
39/39 - 46s - loss: 741.8295 - loglik: -7.4226e+02 - logprior: 0.4342
Epoch 3/10
39/39 - 46s - loss: 737.0384 - loglik: -7.3771e+02 - logprior: 0.6683
Epoch 4/10
39/39 - 46s - loss: 732.4928 - loglik: -7.3334e+02 - logprior: 0.8450
Epoch 5/10
39/39 - 46s - loss: 730.6246 - loglik: -7.3162e+02 - logprior: 0.9930
Epoch 6/10
39/39 - 46s - loss: 729.7357 - loglik: -7.3084e+02 - logprior: 1.1053
Epoch 7/10
39/39 - 46s - loss: 727.4073 - loglik: -7.2869e+02 - logprior: 1.2851
Epoch 8/10
39/39 - 46s - loss: 726.9974 - loglik: -7.2839e+02 - logprior: 1.3962
Epoch 9/10
39/39 - 46s - loss: 727.6639 - loglik: -7.2925e+02 - logprior: 1.5845
Fitted a model with MAP estimate = -725.4582
Time for alignment: 1192.7323
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 969.3427 - loglik: -9.6799e+02 - logprior: -1.3533e+00
Epoch 2/10
39/39 - 33s - loss: 802.0150 - loglik: -8.0091e+02 - logprior: -1.1034e+00
Epoch 3/10
39/39 - 33s - loss: 785.8402 - loglik: -7.8466e+02 - logprior: -1.1822e+00
Epoch 4/10
39/39 - 33s - loss: 781.3834 - loglik: -7.8018e+02 - logprior: -1.1995e+00
Epoch 5/10
39/39 - 33s - loss: 780.1513 - loglik: -7.7891e+02 - logprior: -1.2378e+00
Epoch 6/10
39/39 - 33s - loss: 779.1212 - loglik: -7.7789e+02 - logprior: -1.2271e+00
Epoch 7/10
39/39 - 33s - loss: 778.8070 - loglik: -7.7757e+02 - logprior: -1.2358e+00
Epoch 8/10
39/39 - 33s - loss: 778.5142 - loglik: -7.7721e+02 - logprior: -1.3027e+00
Epoch 9/10
39/39 - 33s - loss: 777.8009 - loglik: -7.7655e+02 - logprior: -1.2468e+00
Epoch 10/10
39/39 - 33s - loss: 777.4705 - loglik: -7.7620e+02 - logprior: -1.2678e+00
Fitted a model with MAP estimate = -768.8671
expansions: [(0, 3), (5, 1), (44, 1), (51, 1), (53, 1), (57, 2), (70, 1), (71, 1), (74, 1), (75, 2), (105, 1), (120, 1), (122, 1), (123, 1), (132, 1), (146, 3), (147, 1), (148, 1), (164, 1), (170, 1), (173, 1), (175, 1), (176, 1), (193, 2), (194, 1), (195, 1), (196, 1), (218, 1), (219, 1), (220, 1), (222, 1), (223, 1), (224, 3), (226, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (257, 1), (258, 1), (259, 1), (261, 1), (263, 1), (264, 1), (285, 6), (286, 3), (287, 1), (288, 1), (296, 4), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 394 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 50s - loss: 792.0964 - loglik: -7.8993e+02 - logprior: -2.1663e+00
Epoch 2/2
39/39 - 46s - loss: 753.4893 - loglik: -7.5291e+02 - logprior: -5.8090e-01
Fitted a model with MAP estimate = -741.3570
expansions: [(346, 2)]
discards: [  4  65  87 169 223 388]
Re-initialized the encoder parameters.
Fitting a model of length 390 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 762.2328 - loglik: -7.6125e+02 - logprior: -9.8388e-01
Epoch 2/2
39/39 - 46s - loss: 751.6589 - loglik: -7.5179e+02 - logprior: 0.1303
Fitted a model with MAP estimate = -740.1886
expansions: [(0, 2)]
discards: [  1 259 333 336 337]
Re-initialized the encoder parameters.
Fitting a model of length 387 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 755.5928 - loglik: -7.5408e+02 - logprior: -1.5084e+00
Epoch 2/10
39/39 - 45s - loss: 745.0351 - loglik: -7.4540e+02 - logprior: 0.3677
Epoch 3/10
39/39 - 46s - loss: 738.5920 - loglik: -7.3917e+02 - logprior: 0.5739
Epoch 4/10
39/39 - 45s - loss: 735.5523 - loglik: -7.3625e+02 - logprior: 0.6996
Epoch 5/10
39/39 - 45s - loss: 733.9276 - loglik: -7.3481e+02 - logprior: 0.8777
Epoch 6/10
39/39 - 45s - loss: 731.2347 - loglik: -7.3227e+02 - logprior: 1.0390
Epoch 7/10
39/39 - 46s - loss: 730.7945 - loglik: -7.3199e+02 - logprior: 1.1958
Epoch 8/10
39/39 - 45s - loss: 729.1217 - loglik: -7.3047e+02 - logprior: 1.3460
Epoch 9/10
39/39 - 46s - loss: 729.9725 - loglik: -7.3143e+02 - logprior: 1.4616
Fitted a model with MAP estimate = -727.9071
Time for alignment: 1176.4801
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 968.6366 - loglik: -9.6729e+02 - logprior: -1.3493e+00
Epoch 2/10
39/39 - 33s - loss: 803.0922 - loglik: -8.0200e+02 - logprior: -1.0942e+00
Epoch 3/10
39/39 - 33s - loss: 786.9878 - loglik: -7.8584e+02 - logprior: -1.1480e+00
Epoch 4/10
39/39 - 33s - loss: 781.9747 - loglik: -7.8081e+02 - logprior: -1.1619e+00
Epoch 5/10
39/39 - 33s - loss: 780.4762 - loglik: -7.7930e+02 - logprior: -1.1754e+00
Epoch 6/10
39/39 - 33s - loss: 779.8823 - loglik: -7.7869e+02 - logprior: -1.1903e+00
Epoch 7/10
39/39 - 33s - loss: 779.6009 - loglik: -7.7843e+02 - logprior: -1.1685e+00
Epoch 8/10
39/39 - 33s - loss: 778.9391 - loglik: -7.7775e+02 - logprior: -1.1853e+00
Epoch 9/10
39/39 - 33s - loss: 778.6570 - loglik: -7.7747e+02 - logprior: -1.1824e+00
Epoch 10/10
39/39 - 33s - loss: 778.0402 - loglik: -7.7681e+02 - logprior: -1.2285e+00
Fitted a model with MAP estimate = -769.4928
expansions: [(0, 3), (15, 1), (34, 1), (51, 1), (53, 1), (57, 2), (63, 1), (69, 1), (70, 1), (75, 2), (105, 1), (120, 1), (121, 2), (123, 1), (132, 1), (146, 3), (147, 1), (148, 1), (163, 1), (170, 1), (173, 1), (175, 1), (176, 1), (193, 2), (196, 1), (197, 3), (218, 1), (219, 1), (220, 1), (223, 2), (224, 2), (247, 1), (249, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (262, 3), (263, 2), (264, 2), (285, 6), (286, 4), (287, 1), (288, 1), (296, 2), (297, 1), (298, 3), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 401 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 51s - loss: 794.3318 - loglik: -7.9225e+02 - logprior: -2.0859e+00
Epoch 2/2
39/39 - 48s - loss: 752.8303 - loglik: -7.5224e+02 - logprior: -5.8831e-01
Fitted a model with MAP estimate = -740.0827
expansions: [(349, 1)]
discards: [  1  65  87 138 170 224 231 315 317 369 395]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 762.1374 - loglik: -7.6098e+02 - logprior: -1.1589e+00
Epoch 2/2
39/39 - 46s - loss: 750.8014 - loglik: -7.5089e+02 - logprior: 0.0927
Fitted a model with MAP estimate = -738.8793
expansions: [(0, 2), (258, 1)]
discards: [  3 333]
Re-initialized the encoder parameters.
Fitting a model of length 392 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 753.8749 - loglik: -7.5235e+02 - logprior: -1.5223e+00
Epoch 2/10
39/39 - 46s - loss: 742.7973 - loglik: -7.4320e+02 - logprior: 0.4059
Epoch 3/10
39/39 - 46s - loss: 738.0457 - loglik: -7.3868e+02 - logprior: 0.6345
Epoch 4/10
39/39 - 46s - loss: 733.0342 - loglik: -7.3380e+02 - logprior: 0.7632
Epoch 5/10
39/39 - 46s - loss: 732.1459 - loglik: -7.3308e+02 - logprior: 0.9346
Epoch 6/10
39/39 - 47s - loss: 729.1771 - loglik: -7.3026e+02 - logprior: 1.0847
Epoch 7/10
39/39 - 46s - loss: 730.1276 - loglik: -7.3135e+02 - logprior: 1.2191
Fitted a model with MAP estimate = -728.3496
Time for alignment: 1100.6853
Computed alignments with likelihoods: ['-725.4582', '-727.9071', '-728.3496']
Best model has likelihood: -725.4582
SP score = 0.4713
Training of 3 independent models on file PF00018.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95e39a4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95d93afdf0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.5639 - loglik: -1.3034e+02 - logprior: -3.2192e+00
Epoch 2/10
19/19 - 1s - loss: 108.2568 - loglik: -1.0686e+02 - logprior: -1.3969e+00
Epoch 3/10
19/19 - 1s - loss: 100.5266 - loglik: -9.8973e+01 - logprior: -1.5532e+00
Epoch 4/10
19/19 - 1s - loss: 98.2325 - loglik: -9.6806e+01 - logprior: -1.4263e+00
Epoch 5/10
19/19 - 1s - loss: 97.6233 - loglik: -9.6214e+01 - logprior: -1.4095e+00
Epoch 6/10
19/19 - 1s - loss: 97.3378 - loglik: -9.5944e+01 - logprior: -1.3940e+00
Epoch 7/10
19/19 - 1s - loss: 97.1902 - loglik: -9.5813e+01 - logprior: -1.3775e+00
Epoch 8/10
19/19 - 1s - loss: 97.1831 - loglik: -9.5816e+01 - logprior: -1.3676e+00
Epoch 9/10
19/19 - 1s - loss: 97.1579 - loglik: -9.5796e+01 - logprior: -1.3623e+00
Epoch 10/10
19/19 - 1s - loss: 97.0411 - loglik: -9.5685e+01 - logprior: -1.3565e+00
Fitted a model with MAP estimate = -96.9526
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.5712 - loglik: -9.8403e+01 - logprior: -4.1683e+00
Epoch 2/2
19/19 - 1s - loss: 94.9108 - loglik: -9.2783e+01 - logprior: -2.1277e+00
Fitted a model with MAP estimate = -93.4996
expansions: [(0, 2)]
discards: [ 0  9 12 29 41]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 95.3736 - loglik: -9.2291e+01 - logprior: -3.0828e+00
Epoch 2/2
19/19 - 1s - loss: 92.3131 - loglik: -9.1039e+01 - logprior: -1.2744e+00
Fitted a model with MAP estimate = -91.7003
expansions: []
discards: [ 0 37]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 96.8873 - loglik: -9.3333e+01 - logprior: -3.5540e+00
Epoch 2/10
19/19 - 1s - loss: 93.1140 - loglik: -9.1669e+01 - logprior: -1.4449e+00
Epoch 3/10
19/19 - 1s - loss: 92.3144 - loglik: -9.0966e+01 - logprior: -1.3484e+00
Epoch 4/10
19/19 - 1s - loss: 91.9441 - loglik: -9.0640e+01 - logprior: -1.3043e+00
Epoch 5/10
19/19 - 1s - loss: 91.6311 - loglik: -9.0357e+01 - logprior: -1.2742e+00
Epoch 6/10
19/19 - 1s - loss: 91.4396 - loglik: -9.0182e+01 - logprior: -1.2581e+00
Epoch 7/10
19/19 - 1s - loss: 91.4971 - loglik: -9.0255e+01 - logprior: -1.2417e+00
Fitted a model with MAP estimate = -91.2330
Time for alignment: 38.9224
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.5469 - loglik: -1.3033e+02 - logprior: -3.2212e+00
Epoch 2/10
19/19 - 1s - loss: 108.4153 - loglik: -1.0701e+02 - logprior: -1.4006e+00
Epoch 3/10
19/19 - 1s - loss: 100.6000 - loglik: -9.9048e+01 - logprior: -1.5520e+00
Epoch 4/10
19/19 - 1s - loss: 98.2631 - loglik: -9.6837e+01 - logprior: -1.4264e+00
Epoch 5/10
19/19 - 1s - loss: 97.4646 - loglik: -9.6056e+01 - logprior: -1.4085e+00
Epoch 6/10
19/19 - 1s - loss: 97.4502 - loglik: -9.6057e+01 - logprior: -1.3931e+00
Epoch 7/10
19/19 - 1s - loss: 97.2899 - loglik: -9.5914e+01 - logprior: -1.3759e+00
Epoch 8/10
19/19 - 1s - loss: 97.1551 - loglik: -9.5786e+01 - logprior: -1.3688e+00
Epoch 9/10
19/19 - 1s - loss: 97.0788 - loglik: -9.5718e+01 - logprior: -1.3610e+00
Epoch 10/10
19/19 - 1s - loss: 97.1109 - loglik: -9.5754e+01 - logprior: -1.3568e+00
Fitted a model with MAP estimate = -96.9423
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.6167 - loglik: -9.8449e+01 - logprior: -4.1678e+00
Epoch 2/2
19/19 - 1s - loss: 94.9294 - loglik: -9.2799e+01 - logprior: -2.1307e+00
Fitted a model with MAP estimate = -93.5026
expansions: [(0, 2)]
discards: [ 0  9 12 29 41]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 95.4071 - loglik: -9.2328e+01 - logprior: -3.0795e+00
Epoch 2/2
19/19 - 1s - loss: 92.1746 - loglik: -9.0901e+01 - logprior: -1.2733e+00
Fitted a model with MAP estimate = -91.7181
expansions: []
discards: [ 0 37]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 96.8866 - loglik: -9.3327e+01 - logprior: -3.5597e+00
Epoch 2/10
19/19 - 1s - loss: 93.0873 - loglik: -9.1643e+01 - logprior: -1.4440e+00
Epoch 3/10
19/19 - 1s - loss: 92.3230 - loglik: -9.0973e+01 - logprior: -1.3498e+00
Epoch 4/10
19/19 - 1s - loss: 91.9506 - loglik: -9.0648e+01 - logprior: -1.3022e+00
Epoch 5/10
19/19 - 1s - loss: 91.6883 - loglik: -9.0415e+01 - logprior: -1.2734e+00
Epoch 6/10
19/19 - 1s - loss: 91.4636 - loglik: -9.0205e+01 - logprior: -1.2584e+00
Epoch 7/10
19/19 - 1s - loss: 91.2505 - loglik: -9.0007e+01 - logprior: -1.2434e+00
Epoch 8/10
19/19 - 1s - loss: 91.3345 - loglik: -9.0102e+01 - logprior: -1.2326e+00
Fitted a model with MAP estimate = -91.1756
Time for alignment: 38.4228
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.5716 - loglik: -1.3035e+02 - logprior: -3.2211e+00
Epoch 2/10
19/19 - 1s - loss: 108.2988 - loglik: -1.0690e+02 - logprior: -1.4022e+00
Epoch 3/10
19/19 - 1s - loss: 99.9078 - loglik: -9.8341e+01 - logprior: -1.5663e+00
Epoch 4/10
19/19 - 1s - loss: 98.1963 - loglik: -9.6764e+01 - logprior: -1.4322e+00
Epoch 5/10
19/19 - 1s - loss: 97.5698 - loglik: -9.6160e+01 - logprior: -1.4093e+00
Epoch 6/10
19/19 - 1s - loss: 97.4206 - loglik: -9.6029e+01 - logprior: -1.3916e+00
Epoch 7/10
19/19 - 1s - loss: 97.1191 - loglik: -9.5743e+01 - logprior: -1.3762e+00
Epoch 8/10
19/19 - 1s - loss: 97.1497 - loglik: -9.5782e+01 - logprior: -1.3680e+00
Fitted a model with MAP estimate = -97.0178
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.4576 - loglik: -9.8294e+01 - logprior: -4.1634e+00
Epoch 2/2
19/19 - 1s - loss: 94.9490 - loglik: -9.2855e+01 - logprior: -2.0940e+00
Fitted a model with MAP estimate = -93.4411
expansions: [(0, 2)]
discards: [ 0  9 12 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 95.4652 - loglik: -9.2397e+01 - logprior: -3.0678e+00
Epoch 2/2
19/19 - 1s - loss: 92.3748 - loglik: -9.1112e+01 - logprior: -1.2624e+00
Fitted a model with MAP estimate = -91.7884
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 96.8212 - loglik: -9.3243e+01 - logprior: -3.5784e+00
Epoch 2/10
19/19 - 1s - loss: 92.9799 - loglik: -9.1536e+01 - logprior: -1.4441e+00
Epoch 3/10
19/19 - 1s - loss: 92.3374 - loglik: -9.0988e+01 - logprior: -1.3497e+00
Epoch 4/10
19/19 - 1s - loss: 91.9882 - loglik: -9.0683e+01 - logprior: -1.3056e+00
Epoch 5/10
19/19 - 1s - loss: 91.6698 - loglik: -9.0395e+01 - logprior: -1.2743e+00
Epoch 6/10
19/19 - 1s - loss: 91.6026 - loglik: -9.0342e+01 - logprior: -1.2607e+00
Epoch 7/10
19/19 - 1s - loss: 91.3720 - loglik: -9.0125e+01 - logprior: -1.2466e+00
Epoch 8/10
19/19 - 1s - loss: 91.1380 - loglik: -8.9908e+01 - logprior: -1.2301e+00
Epoch 9/10
19/19 - 1s - loss: 91.1959 - loglik: -8.9975e+01 - logprior: -1.2208e+00
Fitted a model with MAP estimate = -91.0974
Time for alignment: 37.2734
Computed alignments with likelihoods: ['-91.2330', '-91.1756', '-91.0974']
Best model has likelihood: -91.0974
SP score = 0.8414
Training of 3 independent models on file PF00687.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9638ecba30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96526e6820>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 525.1791 - loglik: -5.2243e+02 - logprior: -2.7473e+00
Epoch 2/10
19/19 - 6s - loss: 423.4677 - loglik: -4.2253e+02 - logprior: -9.4033e-01
Epoch 3/10
19/19 - 6s - loss: 375.2725 - loglik: -3.7366e+02 - logprior: -1.6132e+00
Epoch 4/10
19/19 - 6s - loss: 363.9698 - loglik: -3.6199e+02 - logprior: -1.9787e+00
Epoch 5/10
19/19 - 6s - loss: 359.1912 - loglik: -3.5729e+02 - logprior: -1.8977e+00
Epoch 6/10
19/19 - 6s - loss: 358.2738 - loglik: -3.5642e+02 - logprior: -1.8563e+00
Epoch 7/10
19/19 - 6s - loss: 357.6074 - loglik: -3.5580e+02 - logprior: -1.8059e+00
Epoch 8/10
19/19 - 6s - loss: 356.6506 - loglik: -3.5489e+02 - logprior: -1.7655e+00
Epoch 9/10
19/19 - 6s - loss: 355.5938 - loglik: -3.5384e+02 - logprior: -1.7526e+00
Epoch 10/10
19/19 - 6s - loss: 355.9347 - loglik: -3.5418e+02 - logprior: -1.7526e+00
Fitted a model with MAP estimate = -355.6540
expansions: [(4, 1), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (37, 1), (40, 1), (41, 1), (42, 1), (58, 1), (59, 1), (61, 1), (63, 2), (66, 1), (75, 1), (93, 1), (97, 2), (100, 1), (102, 1), (103, 1), (108, 1), (113, 1), (114, 1), (116, 1), (120, 1), (123, 1), (127, 1), (129, 1), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 195 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 351.4601 - loglik: -3.4876e+02 - logprior: -2.6975e+00
Epoch 2/2
19/19 - 9s - loss: 330.1314 - loglik: -3.2918e+02 - logprior: -9.5238e-01
Fitted a model with MAP estimate = -327.4160
expansions: []
discards: [ 80 119 170]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 333.8179 - loglik: -3.3119e+02 - logprior: -2.6244e+00
Epoch 2/2
19/19 - 9s - loss: 328.6247 - loglik: -3.2784e+02 - logprior: -7.8409e-01
Fitted a model with MAP estimate = -327.3429
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 333.2527 - loglik: -3.3069e+02 - logprior: -2.5671e+00
Epoch 2/10
19/19 - 9s - loss: 328.0705 - loglik: -3.2739e+02 - logprior: -6.7967e-01
Epoch 3/10
19/19 - 9s - loss: 327.1660 - loglik: -3.2660e+02 - logprior: -5.6124e-01
Epoch 4/10
19/19 - 9s - loss: 324.6785 - loglik: -3.2418e+02 - logprior: -4.9931e-01
Epoch 5/10
19/19 - 9s - loss: 324.3372 - loglik: -3.2389e+02 - logprior: -4.5120e-01
Epoch 6/10
19/19 - 9s - loss: 324.1855 - loglik: -3.2379e+02 - logprior: -4.0012e-01
Epoch 7/10
19/19 - 9s - loss: 321.0992 - loglik: -3.2079e+02 - logprior: -3.0950e-01
Epoch 8/10
19/19 - 8s - loss: 321.0834 - loglik: -3.2083e+02 - logprior: -2.5661e-01
Epoch 9/10
19/19 - 9s - loss: 320.7314 - loglik: -3.2052e+02 - logprior: -2.1029e-01
Epoch 10/10
19/19 - 8s - loss: 321.5200 - loglik: -3.2135e+02 - logprior: -1.7231e-01
Fitted a model with MAP estimate = -320.2391
Time for alignment: 245.2792
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 525.4264 - loglik: -5.2268e+02 - logprior: -2.7460e+00
Epoch 2/10
19/19 - 6s - loss: 421.2165 - loglik: -4.2028e+02 - logprior: -9.3834e-01
Epoch 3/10
19/19 - 6s - loss: 375.8328 - loglik: -3.7422e+02 - logprior: -1.6124e+00
Epoch 4/10
19/19 - 6s - loss: 368.6032 - loglik: -3.6675e+02 - logprior: -1.8561e+00
Epoch 5/10
19/19 - 6s - loss: 362.5061 - loglik: -3.6073e+02 - logprior: -1.7765e+00
Epoch 6/10
19/19 - 6s - loss: 361.9871 - loglik: -3.6024e+02 - logprior: -1.7480e+00
Epoch 7/10
19/19 - 6s - loss: 361.8421 - loglik: -3.6013e+02 - logprior: -1.7128e+00
Epoch 8/10
19/19 - 6s - loss: 358.9867 - loglik: -3.5729e+02 - logprior: -1.6947e+00
Epoch 9/10
19/19 - 6s - loss: 360.0652 - loglik: -3.5838e+02 - logprior: -1.6857e+00
Fitted a model with MAP estimate = -359.1544
expansions: [(4, 1), (5, 2), (8, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (34, 1), (39, 1), (40, 1), (41, 1), (42, 1), (54, 1), (58, 1), (60, 1), (63, 2), (66, 1), (72, 1), (74, 1), (97, 2), (99, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 2), (127, 1), (129, 2), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 195 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 352.5571 - loglik: -3.4988e+02 - logprior: -2.6792e+00
Epoch 2/2
19/19 - 9s - loss: 331.1691 - loglik: -3.3026e+02 - logprior: -9.1140e-01
Fitted a model with MAP estimate = -328.2201
expansions: []
discards: [ 80 119 170]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 334.6549 - loglik: -3.3208e+02 - logprior: -2.5770e+00
Epoch 2/2
19/19 - 9s - loss: 329.3482 - loglik: -3.2858e+02 - logprior: -7.6641e-01
Fitted a model with MAP estimate = -327.9693
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 333.4394 - loglik: -3.3092e+02 - logprior: -2.5204e+00
Epoch 2/10
19/19 - 9s - loss: 328.4747 - loglik: -3.2781e+02 - logprior: -6.6829e-01
Epoch 3/10
19/19 - 9s - loss: 327.6366 - loglik: -3.2711e+02 - logprior: -5.3135e-01
Epoch 4/10
19/19 - 9s - loss: 326.7262 - loglik: -3.2624e+02 - logprior: -4.8382e-01
Epoch 5/10
19/19 - 9s - loss: 325.0019 - loglik: -3.2456e+02 - logprior: -4.4594e-01
Epoch 6/10
19/19 - 9s - loss: 321.1635 - loglik: -3.2077e+02 - logprior: -3.9012e-01
Epoch 7/10
19/19 - 9s - loss: 324.2008 - loglik: -3.2389e+02 - logprior: -3.1401e-01
Fitted a model with MAP estimate = -322.2380
Time for alignment: 214.0252
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 525.5413 - loglik: -5.2280e+02 - logprior: -2.7364e+00
Epoch 2/10
19/19 - 6s - loss: 422.0320 - loglik: -4.2108e+02 - logprior: -9.5096e-01
Epoch 3/10
19/19 - 6s - loss: 377.9866 - loglik: -3.7639e+02 - logprior: -1.5956e+00
Epoch 4/10
19/19 - 6s - loss: 365.9304 - loglik: -3.6402e+02 - logprior: -1.9138e+00
Epoch 5/10
19/19 - 6s - loss: 360.5766 - loglik: -3.5873e+02 - logprior: -1.8515e+00
Epoch 6/10
19/19 - 6s - loss: 361.0746 - loglik: -3.5927e+02 - logprior: -1.8082e+00
Fitted a model with MAP estimate = -359.3239
expansions: [(4, 1), (5, 2), (8, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (39, 1), (40, 1), (41, 1), (42, 1), (58, 1), (59, 1), (60, 1), (63, 1), (66, 1), (74, 1), (80, 1), (97, 2), (100, 1), (102, 1), (103, 1), (108, 1), (110, 1), (112, 1), (113, 1), (116, 1), (123, 1), (127, 1), (129, 1), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 194 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 352.8028 - loglik: -3.5018e+02 - logprior: -2.6195e+00
Epoch 2/2
19/19 - 9s - loss: 330.2069 - loglik: -3.2953e+02 - logprior: -6.7745e-01
Fitted a model with MAP estimate = -327.9637
expansions: []
discards: [118 169]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 333.1692 - loglik: -3.3065e+02 - logprior: -2.5172e+00
Epoch 2/2
19/19 - 9s - loss: 328.9884 - loglik: -3.2849e+02 - logprior: -5.0208e-01
Fitted a model with MAP estimate = -327.3572
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 332.3559 - loglik: -3.2991e+02 - logprior: -2.4505e+00
Epoch 2/10
19/19 - 9s - loss: 328.1515 - loglik: -3.2770e+02 - logprior: -4.4953e-01
Epoch 3/10
19/19 - 9s - loss: 326.8546 - loglik: -3.2646e+02 - logprior: -3.9099e-01
Epoch 4/10
19/19 - 9s - loss: 324.4023 - loglik: -3.2406e+02 - logprior: -3.3884e-01
Epoch 5/10
19/19 - 9s - loss: 324.1635 - loglik: -3.2387e+02 - logprior: -2.9327e-01
Epoch 6/10
19/19 - 9s - loss: 322.5227 - loglik: -3.2225e+02 - logprior: -2.7171e-01
Epoch 7/10
19/19 - 9s - loss: 323.1948 - loglik: -3.2293e+02 - logprior: -2.6198e-01
Fitted a model with MAP estimate = -321.1306
Time for alignment: 194.2115
Computed alignments with likelihoods: ['-320.2391', '-322.2380', '-321.1306']
Best model has likelihood: -320.2391
SP score = 0.7746
Training of 3 independent models on file PF03129.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95e233a850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95e34c0f40>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 257.3746 - loglik: -2.5426e+02 - logprior: -3.1101e+00
Epoch 2/10
19/19 - 2s - loss: 225.1788 - loglik: -2.2391e+02 - logprior: -1.2688e+00
Epoch 3/10
19/19 - 2s - loss: 212.5123 - loglik: -2.1121e+02 - logprior: -1.3010e+00
Epoch 4/10
19/19 - 2s - loss: 209.1042 - loglik: -2.0786e+02 - logprior: -1.2423e+00
Epoch 5/10
19/19 - 2s - loss: 206.2452 - loglik: -2.0495e+02 - logprior: -1.2990e+00
Epoch 6/10
19/19 - 2s - loss: 204.9602 - loglik: -2.0371e+02 - logprior: -1.2511e+00
Epoch 7/10
19/19 - 2s - loss: 203.6562 - loglik: -2.0244e+02 - logprior: -1.2129e+00
Epoch 8/10
19/19 - 2s - loss: 202.4716 - loglik: -2.0126e+02 - logprior: -1.2085e+00
Epoch 9/10
19/19 - 2s - loss: 201.9700 - loglik: -2.0079e+02 - logprior: -1.1804e+00
Epoch 10/10
19/19 - 2s - loss: 201.6101 - loglik: -2.0045e+02 - logprior: -1.1609e+00
Fitted a model with MAP estimate = -201.0309
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (24, 1), (26, 1), (37, 2), (38, 2), (54, 2), (55, 1), (57, 1), (64, 2), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 212.0026 - loglik: -2.0804e+02 - logprior: -3.9669e+00
Epoch 2/2
19/19 - 2s - loss: 196.8827 - loglik: -1.9490e+02 - logprior: -1.9788e+00
Fitted a model with MAP estimate = -194.3794
expansions: [(0, 2)]
discards: [ 0  7 46 66 81]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 197.4293 - loglik: -1.9461e+02 - logprior: -2.8236e+00
Epoch 2/2
19/19 - 2s - loss: 193.0738 - loglik: -1.9207e+02 - logprior: -1.0000e+00
Fitted a model with MAP estimate = -191.7805
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 198.2034 - loglik: -1.9481e+02 - logprior: -3.3922e+00
Epoch 2/10
19/19 - 2s - loss: 193.5642 - loglik: -1.9241e+02 - logprior: -1.1530e+00
Epoch 3/10
19/19 - 2s - loss: 191.8856 - loglik: -1.9088e+02 - logprior: -1.0082e+00
Epoch 4/10
19/19 - 2s - loss: 190.8956 - loglik: -1.8993e+02 - logprior: -9.6132e-01
Epoch 5/10
19/19 - 2s - loss: 190.2089 - loglik: -1.8928e+02 - logprior: -9.3008e-01
Epoch 6/10
19/19 - 2s - loss: 189.4733 - loglik: -1.8857e+02 - logprior: -9.0475e-01
Epoch 7/10
19/19 - 2s - loss: 189.3972 - loglik: -1.8851e+02 - logprior: -8.8285e-01
Epoch 8/10
19/19 - 2s - loss: 189.0455 - loglik: -1.8818e+02 - logprior: -8.6816e-01
Epoch 9/10
19/19 - 2s - loss: 188.1120 - loglik: -1.8726e+02 - logprior: -8.4811e-01
Epoch 10/10
19/19 - 2s - loss: 188.8376 - loglik: -1.8800e+02 - logprior: -8.3778e-01
Fitted a model with MAP estimate = -188.2105
Time for alignment: 67.9965
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 257.8297 - loglik: -2.5472e+02 - logprior: -3.1099e+00
Epoch 2/10
19/19 - 2s - loss: 224.9774 - loglik: -2.2369e+02 - logprior: -1.2892e+00
Epoch 3/10
19/19 - 2s - loss: 210.6965 - loglik: -2.0934e+02 - logprior: -1.3547e+00
Epoch 4/10
19/19 - 2s - loss: 205.6654 - loglik: -2.0439e+02 - logprior: -1.2799e+00
Epoch 5/10
19/19 - 2s - loss: 203.8207 - loglik: -2.0256e+02 - logprior: -1.2647e+00
Epoch 6/10
19/19 - 2s - loss: 201.7791 - loglik: -2.0053e+02 - logprior: -1.2500e+00
Epoch 7/10
19/19 - 2s - loss: 201.7692 - loglik: -2.0058e+02 - logprior: -1.1930e+00
Epoch 8/10
19/19 - 2s - loss: 201.5665 - loglik: -2.0039e+02 - logprior: -1.1722e+00
Epoch 9/10
19/19 - 2s - loss: 200.3961 - loglik: -1.9923e+02 - logprior: -1.1613e+00
Epoch 10/10
19/19 - 2s - loss: 200.9727 - loglik: -1.9981e+02 - logprior: -1.1594e+00
Fitted a model with MAP estimate = -200.3474
expansions: [(7, 2), (8, 2), (9, 4), (12, 1), (15, 1), (22, 1), (30, 2), (36, 1), (37, 2), (38, 1), (54, 2), (55, 1), (57, 1), (64, 2), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 212.3992 - loglik: -2.0843e+02 - logprior: -3.9648e+00
Epoch 2/2
19/19 - 2s - loss: 196.9467 - loglik: -1.9494e+02 - logprior: -2.0036e+00
Fitted a model with MAP estimate = -194.0650
expansions: [(0, 2)]
discards: [ 0  7  9 10 13 41 50 70 85]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 197.8942 - loglik: -1.9507e+02 - logprior: -2.8252e+00
Epoch 2/2
19/19 - 2s - loss: 193.1019 - loglik: -1.9209e+02 - logprior: -1.0070e+00
Fitted a model with MAP estimate = -191.7362
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 198.2938 - loglik: -1.9488e+02 - logprior: -3.4182e+00
Epoch 2/10
19/19 - 2s - loss: 193.3819 - loglik: -1.9222e+02 - logprior: -1.1624e+00
Epoch 3/10
19/19 - 2s - loss: 191.8073 - loglik: -1.9080e+02 - logprior: -1.0098e+00
Epoch 4/10
19/19 - 2s - loss: 190.9741 - loglik: -1.9001e+02 - logprior: -9.6120e-01
Epoch 5/10
19/19 - 2s - loss: 190.1220 - loglik: -1.8920e+02 - logprior: -9.2506e-01
Epoch 6/10
19/19 - 2s - loss: 189.4404 - loglik: -1.8854e+02 - logprior: -9.0440e-01
Epoch 7/10
19/19 - 2s - loss: 189.1890 - loglik: -1.8830e+02 - logprior: -8.8580e-01
Epoch 8/10
19/19 - 2s - loss: 188.8849 - loglik: -1.8802e+02 - logprior: -8.6571e-01
Epoch 9/10
19/19 - 2s - loss: 188.6878 - loglik: -1.8784e+02 - logprior: -8.4747e-01
Epoch 10/10
19/19 - 2s - loss: 188.5116 - loglik: -1.8768e+02 - logprior: -8.3427e-01
Fitted a model with MAP estimate = -188.1702
Time for alignment: 66.1610
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 257.8045 - loglik: -2.5470e+02 - logprior: -3.1092e+00
Epoch 2/10
19/19 - 2s - loss: 225.1456 - loglik: -2.2385e+02 - logprior: -1.2968e+00
Epoch 3/10
19/19 - 2s - loss: 212.5081 - loglik: -2.1116e+02 - logprior: -1.3442e+00
Epoch 4/10
19/19 - 2s - loss: 207.5557 - loglik: -2.0626e+02 - logprior: -1.2969e+00
Epoch 5/10
19/19 - 2s - loss: 205.2501 - loglik: -2.0402e+02 - logprior: -1.2296e+00
Epoch 6/10
19/19 - 2s - loss: 204.1199 - loglik: -2.0294e+02 - logprior: -1.1759e+00
Epoch 7/10
19/19 - 2s - loss: 203.3019 - loglik: -2.0215e+02 - logprior: -1.1524e+00
Epoch 8/10
19/19 - 2s - loss: 202.0963 - loglik: -2.0095e+02 - logprior: -1.1428e+00
Epoch 9/10
19/19 - 2s - loss: 202.5221 - loglik: -2.0137e+02 - logprior: -1.1473e+00
Fitted a model with MAP estimate = -201.7494
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (25, 1), (30, 2), (37, 2), (38, 2), (50, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 209.8167 - loglik: -2.0588e+02 - logprior: -3.9405e+00
Epoch 2/2
19/19 - 2s - loss: 196.5101 - loglik: -1.9460e+02 - logprior: -1.9093e+00
Fitted a model with MAP estimate = -194.1257
expansions: [(0, 2)]
discards: [ 0  7 37 46]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 197.0106 - loglik: -1.9420e+02 - logprior: -2.8142e+00
Epoch 2/2
19/19 - 2s - loss: 192.9637 - loglik: -1.9197e+02 - logprior: -9.9386e-01
Fitted a model with MAP estimate = -191.6772
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 198.1267 - loglik: -1.9474e+02 - logprior: -3.3892e+00
Epoch 2/10
19/19 - 2s - loss: 193.3967 - loglik: -1.9224e+02 - logprior: -1.1522e+00
Epoch 3/10
19/19 - 2s - loss: 191.8586 - loglik: -1.9086e+02 - logprior: -1.0024e+00
Epoch 4/10
19/19 - 2s - loss: 190.9178 - loglik: -1.8996e+02 - logprior: -9.5841e-01
Epoch 5/10
19/19 - 2s - loss: 189.9114 - loglik: -1.8899e+02 - logprior: -9.1992e-01
Epoch 6/10
19/19 - 2s - loss: 189.9528 - loglik: -1.8904e+02 - logprior: -9.0818e-01
Fitted a model with MAP estimate = -189.1993
Time for alignment: 57.7482
Computed alignments with likelihoods: ['-188.2105', '-188.1702', '-189.1993']
Best model has likelihood: -188.1702
SP score = 0.9717
Training of 3 independent models on file PF13378.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9649c2c6d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9691115d60>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 173 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 603.3933 - loglik: -6.0075e+02 - logprior: -2.6452e+00
Epoch 2/10
19/19 - 7s - loss: 529.0696 - loglik: -5.2819e+02 - logprior: -8.8254e-01
Epoch 3/10
19/19 - 7s - loss: 486.1806 - loglik: -4.8482e+02 - logprior: -1.3614e+00
Epoch 4/10
19/19 - 7s - loss: 473.2037 - loglik: -4.7181e+02 - logprior: -1.3957e+00
Epoch 5/10
19/19 - 7s - loss: 467.8911 - loglik: -4.6651e+02 - logprior: -1.3764e+00
Epoch 6/10
19/19 - 7s - loss: 465.7484 - loglik: -4.6439e+02 - logprior: -1.3595e+00
Epoch 7/10
19/19 - 7s - loss: 464.2432 - loglik: -4.6292e+02 - logprior: -1.3282e+00
Epoch 8/10
19/19 - 7s - loss: 463.5649 - loglik: -4.6223e+02 - logprior: -1.3318e+00
Epoch 9/10
19/19 - 7s - loss: 462.9480 - loglik: -4.6161e+02 - logprior: -1.3403e+00
Epoch 10/10
19/19 - 7s - loss: 462.8797 - loglik: -4.6154e+02 - logprior: -1.3388e+00
Fitted a model with MAP estimate = -460.2822
expansions: [(0, 3), (11, 1), (18, 2), (20, 1), (26, 1), (35, 2), (45, 2), (46, 1), (49, 1), (51, 1), (58, 1), (68, 1), (72, 1), (75, 1), (85, 1), (86, 2), (87, 1), (88, 1), (89, 2), (103, 1), (108, 1), (109, 1), (113, 1), (116, 1), (124, 1), (125, 2), (132, 1), (141, 3), (142, 2), (149, 2), (150, 2), (151, 3), (153, 1), (159, 1), (161, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 223 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 502.3066 - loglik: -4.9847e+02 - logprior: -3.8401e+00
Epoch 2/2
19/19 - 10s - loss: 462.3043 - loglik: -4.6102e+02 - logprior: -1.2863e+00
Fitted a model with MAP estimate = -452.9854
expansions: [(187, 1)]
discards: [ 44  56 105 114 178 179 192 193]
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 463.6294 - loglik: -4.6089e+02 - logprior: -2.7437e+00
Epoch 2/2
19/19 - 9s - loss: 455.8992 - loglik: -4.5508e+02 - logprior: -8.1435e-01
Fitted a model with MAP estimate = -450.6005
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10089 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 459.0448 - loglik: -4.5641e+02 - logprior: -2.6392e+00
Epoch 2/10
19/19 - 9s - loss: 453.0035 - loglik: -4.5226e+02 - logprior: -7.4588e-01
Epoch 3/10
19/19 - 9s - loss: 448.4643 - loglik: -4.4793e+02 - logprior: -5.3192e-01
Epoch 4/10
19/19 - 9s - loss: 444.1284 - loglik: -4.4366e+02 - logprior: -4.6390e-01
Epoch 5/10
19/19 - 9s - loss: 441.2344 - loglik: -4.4082e+02 - logprior: -4.1096e-01
Epoch 6/10
19/19 - 9s - loss: 438.2201 - loglik: -4.3783e+02 - logprior: -3.9353e-01
Epoch 7/10
19/19 - 9s - loss: 435.6779 - loglik: -4.3531e+02 - logprior: -3.6485e-01
Epoch 8/10
19/19 - 9s - loss: 435.1606 - loglik: -4.3481e+02 - logprior: -3.4921e-01
Epoch 9/10
19/19 - 9s - loss: 434.1891 - loglik: -4.3385e+02 - logprior: -3.4188e-01
Epoch 10/10
19/19 - 9s - loss: 434.1512 - loglik: -4.3385e+02 - logprior: -2.9829e-01
Fitted a model with MAP estimate = -433.6385
Time for alignment: 273.3331
Fitting a model of length 173 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 603.1741 - loglik: -6.0051e+02 - logprior: -2.6654e+00
Epoch 2/10
19/19 - 7s - loss: 527.5652 - loglik: -5.2667e+02 - logprior: -8.9132e-01
Epoch 3/10
19/19 - 7s - loss: 487.1699 - loglik: -4.8586e+02 - logprior: -1.3123e+00
Epoch 4/10
19/19 - 7s - loss: 476.0742 - loglik: -4.7478e+02 - logprior: -1.2943e+00
Epoch 5/10
19/19 - 7s - loss: 469.6691 - loglik: -4.6837e+02 - logprior: -1.2989e+00
Epoch 6/10
19/19 - 7s - loss: 466.1772 - loglik: -4.6485e+02 - logprior: -1.3285e+00
Epoch 7/10
19/19 - 7s - loss: 464.7428 - loglik: -4.6344e+02 - logprior: -1.3072e+00
Epoch 8/10
19/19 - 7s - loss: 464.3059 - loglik: -4.6300e+02 - logprior: -1.3053e+00
Epoch 9/10
19/19 - 7s - loss: 463.0632 - loglik: -4.6175e+02 - logprior: -1.3128e+00
Epoch 10/10
19/19 - 7s - loss: 463.0832 - loglik: -4.6177e+02 - logprior: -1.3149e+00
Fitted a model with MAP estimate = -460.6011
expansions: [(0, 3), (11, 1), (18, 2), (20, 1), (31, 1), (35, 2), (45, 2), (46, 1), (49, 1), (51, 1), (65, 1), (69, 1), (75, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (103, 1), (108, 1), (109, 1), (113, 1), (116, 1), (125, 1), (126, 1), (127, 1), (132, 1), (141, 1), (142, 3), (146, 2), (149, 2), (150, 2), (151, 3), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 224 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 503.1022 - loglik: -4.9928e+02 - logprior: -3.8248e+00
Epoch 2/2
19/19 - 10s - loss: 462.2188 - loglik: -4.6095e+02 - logprior: -1.2708e+00
Fitted a model with MAP estimate = -452.8312
expansions: []
discards: [ 44  56  94 114 176 177 193 195]
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 463.6592 - loglik: -4.6091e+02 - logprior: -2.7454e+00
Epoch 2/2
19/19 - 9s - loss: 455.5998 - loglik: -4.5477e+02 - logprior: -8.2981e-01
Fitted a model with MAP estimate = -450.5937
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10089 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 459.0874 - loglik: -4.5644e+02 - logprior: -2.6482e+00
Epoch 2/10
19/19 - 9s - loss: 452.7367 - loglik: -4.5200e+02 - logprior: -7.3962e-01
Epoch 3/10
19/19 - 9s - loss: 448.2066 - loglik: -4.4768e+02 - logprior: -5.2395e-01
Epoch 4/10
19/19 - 9s - loss: 444.7673 - loglik: -4.4432e+02 - logprior: -4.5181e-01
Epoch 5/10
19/19 - 9s - loss: 441.2345 - loglik: -4.4083e+02 - logprior: -4.0864e-01
Epoch 6/10
19/19 - 9s - loss: 438.4294 - loglik: -4.3804e+02 - logprior: -3.8649e-01
Epoch 7/10
19/19 - 9s - loss: 436.1166 - loglik: -4.3577e+02 - logprior: -3.4586e-01
Epoch 8/10
19/19 - 9s - loss: 435.3492 - loglik: -4.3501e+02 - logprior: -3.4231e-01
Epoch 9/10
19/19 - 9s - loss: 434.6817 - loglik: -4.3436e+02 - logprior: -3.2181e-01
Epoch 10/10
19/19 - 9s - loss: 434.6299 - loglik: -4.3433e+02 - logprior: -2.9713e-01
Fitted a model with MAP estimate = -433.7218
Time for alignment: 274.3783
Fitting a model of length 173 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 603.2760 - loglik: -6.0063e+02 - logprior: -2.6485e+00
Epoch 2/10
19/19 - 7s - loss: 527.7549 - loglik: -5.2687e+02 - logprior: -8.8903e-01
Epoch 3/10
19/19 - 7s - loss: 486.3852 - loglik: -4.8506e+02 - logprior: -1.3248e+00
Epoch 4/10
19/19 - 7s - loss: 473.5233 - loglik: -4.7218e+02 - logprior: -1.3388e+00
Epoch 5/10
19/19 - 7s - loss: 468.2499 - loglik: -4.6697e+02 - logprior: -1.2839e+00
Epoch 6/10
19/19 - 7s - loss: 466.4039 - loglik: -4.6513e+02 - logprior: -1.2733e+00
Epoch 7/10
19/19 - 7s - loss: 464.6006 - loglik: -4.6335e+02 - logprior: -1.2472e+00
Epoch 8/10
19/19 - 7s - loss: 464.0496 - loglik: -4.6282e+02 - logprior: -1.2297e+00
Epoch 9/10
19/19 - 7s - loss: 464.0144 - loglik: -4.6278e+02 - logprior: -1.2336e+00
Epoch 10/10
19/19 - 7s - loss: 463.6911 - loglik: -4.6247e+02 - logprior: -1.2225e+00
Fitted a model with MAP estimate = -461.0744
expansions: [(0, 3), (11, 1), (18, 3), (20, 2), (35, 2), (45, 2), (46, 1), (49, 1), (51, 1), (65, 1), (68, 1), (75, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (92, 1), (102, 1), (107, 1), (108, 1), (116, 1), (124, 4), (131, 1), (134, 1), (140, 2), (145, 3), (148, 2), (149, 1), (150, 2), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 224 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 501.4391 - loglik: -4.9760e+02 - logprior: -3.8430e+00
Epoch 2/2
19/19 - 10s - loss: 462.0823 - loglik: -4.6083e+02 - logprior: -1.2514e+00
Fitted a model with MAP estimate = -453.1742
expansions: []
discards: [ 27  45  57  95 115]
Re-initialized the encoder parameters.
Fitting a model of length 219 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 463.6587 - loglik: -4.6086e+02 - logprior: -2.8009e+00
Epoch 2/2
19/19 - 10s - loss: 455.6490 - loglik: -4.5475e+02 - logprior: -8.9701e-01
Fitted a model with MAP estimate = -450.7009
expansions: []
discards: [182]
Re-initialized the encoder parameters.
Fitting a model of length 218 on 10089 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 459.6447 - loglik: -4.5695e+02 - logprior: -2.6985e+00
Epoch 2/10
19/19 - 9s - loss: 453.0416 - loglik: -4.5224e+02 - logprior: -8.0415e-01
Epoch 3/10
19/19 - 9s - loss: 449.0916 - loglik: -4.4851e+02 - logprior: -5.8045e-01
Epoch 4/10
19/19 - 9s - loss: 444.9817 - loglik: -4.4449e+02 - logprior: -4.8848e-01
Epoch 5/10
19/19 - 9s - loss: 441.9172 - loglik: -4.4147e+02 - logprior: -4.5051e-01
Epoch 6/10
19/19 - 9s - loss: 439.5030 - loglik: -4.3908e+02 - logprior: -4.2535e-01
Epoch 7/10
19/19 - 9s - loss: 437.0833 - loglik: -4.3668e+02 - logprior: -4.0045e-01
Epoch 8/10
19/19 - 10s - loss: 435.5178 - loglik: -4.3515e+02 - logprior: -3.6672e-01
Epoch 9/10
19/19 - 9s - loss: 435.5170 - loglik: -4.3517e+02 - logprior: -3.4306e-01
Epoch 10/10
19/19 - 9s - loss: 434.9874 - loglik: -4.3467e+02 - logprior: -3.1844e-01
Fitted a model with MAP estimate = -434.5464
Time for alignment: 275.9045
Computed alignments with likelihoods: ['-433.6385', '-433.7218', '-434.5464']
Best model has likelihood: -433.6385
SP score = 0.7192
Training of 3 independent models on file PF00084.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96637380d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f961f56af40>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 164.8454 - loglik: -1.6165e+02 - logprior: -3.1909e+00
Epoch 2/10
19/19 - 1s - loss: 135.6735 - loglik: -1.3428e+02 - logprior: -1.3902e+00
Epoch 3/10
19/19 - 1s - loss: 127.9994 - loglik: -1.2652e+02 - logprior: -1.4769e+00
Epoch 4/10
19/19 - 1s - loss: 126.1009 - loglik: -1.2479e+02 - logprior: -1.3129e+00
Epoch 5/10
19/19 - 1s - loss: 125.2314 - loglik: -1.2391e+02 - logprior: -1.3187e+00
Epoch 6/10
19/19 - 1s - loss: 124.8830 - loglik: -1.2359e+02 - logprior: -1.2963e+00
Epoch 7/10
19/19 - 1s - loss: 124.4534 - loglik: -1.2317e+02 - logprior: -1.2860e+00
Epoch 8/10
19/19 - 1s - loss: 124.3169 - loglik: -1.2304e+02 - logprior: -1.2793e+00
Epoch 9/10
19/19 - 1s - loss: 124.2223 - loglik: -1.2294e+02 - logprior: -1.2800e+00
Epoch 10/10
19/19 - 1s - loss: 123.8309 - loglik: -1.2256e+02 - logprior: -1.2726e+00
Fitted a model with MAP estimate = -123.5873
expansions: [(11, 1), (12, 3), (13, 3), (14, 2), (28, 2), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 135.8803 - loglik: -1.3182e+02 - logprior: -4.0623e+00
Epoch 2/2
19/19 - 1s - loss: 126.8596 - loglik: -1.2461e+02 - logprior: -2.2481e+00
Fitted a model with MAP estimate = -124.1165
expansions: []
discards: [12 13 16 36 41 42 51]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 127.0100 - loglik: -1.2366e+02 - logprior: -3.3455e+00
Epoch 2/2
19/19 - 1s - loss: 122.9493 - loglik: -1.2155e+02 - logprior: -1.3975e+00
Fitted a model with MAP estimate = -122.3422
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 125.6953 - loglik: -1.2251e+02 - logprior: -3.1877e+00
Epoch 2/10
19/19 - 1s - loss: 122.8737 - loglik: -1.2149e+02 - logprior: -1.3798e+00
Epoch 3/10
19/19 - 1s - loss: 122.0239 - loglik: -1.2074e+02 - logprior: -1.2831e+00
Epoch 4/10
19/19 - 1s - loss: 121.6252 - loglik: -1.2040e+02 - logprior: -1.2296e+00
Epoch 5/10
19/19 - 1s - loss: 121.0195 - loglik: -1.1983e+02 - logprior: -1.1939e+00
Epoch 6/10
19/19 - 1s - loss: 120.8486 - loglik: -1.1967e+02 - logprior: -1.1779e+00
Epoch 7/10
19/19 - 1s - loss: 120.3950 - loglik: -1.1923e+02 - logprior: -1.1660e+00
Epoch 8/10
19/19 - 1s - loss: 119.9853 - loglik: -1.1884e+02 - logprior: -1.1446e+00
Epoch 9/10
19/19 - 1s - loss: 119.9442 - loglik: -1.1880e+02 - logprior: -1.1421e+00
Epoch 10/10
19/19 - 1s - loss: 118.6250 - loglik: -1.1749e+02 - logprior: -1.1345e+00
Fitted a model with MAP estimate = -118.0135
Time for alignment: 47.4449
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 164.8365 - loglik: -1.6165e+02 - logprior: -3.1877e+00
Epoch 2/10
19/19 - 1s - loss: 136.2115 - loglik: -1.3483e+02 - logprior: -1.3830e+00
Epoch 3/10
19/19 - 1s - loss: 128.4235 - loglik: -1.2695e+02 - logprior: -1.4747e+00
Epoch 4/10
19/19 - 1s - loss: 126.3358 - loglik: -1.2502e+02 - logprior: -1.3152e+00
Epoch 5/10
19/19 - 1s - loss: 125.7514 - loglik: -1.2444e+02 - logprior: -1.3117e+00
Epoch 6/10
19/19 - 1s - loss: 125.2883 - loglik: -1.2400e+02 - logprior: -1.2908e+00
Epoch 7/10
19/19 - 1s - loss: 125.0037 - loglik: -1.2372e+02 - logprior: -1.2799e+00
Epoch 8/10
19/19 - 1s - loss: 124.8795 - loglik: -1.2361e+02 - logprior: -1.2714e+00
Epoch 9/10
19/19 - 1s - loss: 124.5779 - loglik: -1.2331e+02 - logprior: -1.2670e+00
Epoch 10/10
19/19 - 1s - loss: 124.4829 - loglik: -1.2322e+02 - logprior: -1.2641e+00
Fitted a model with MAP estimate = -124.3347
expansions: [(11, 5), (12, 3), (13, 1), (18, 1), (30, 2), (34, 3), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 134.7360 - loglik: -1.3067e+02 - logprior: -4.0696e+00
Epoch 2/2
19/19 - 1s - loss: 126.6634 - loglik: -1.2444e+02 - logprior: -2.2242e+00
Fitted a model with MAP estimate = -123.9985
expansions: []
discards: [12 13 16 40 46 50]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.7254 - loglik: -1.2339e+02 - logprior: -3.3344e+00
Epoch 2/2
19/19 - 1s - loss: 122.9366 - loglik: -1.2154e+02 - logprior: -1.3971e+00
Fitted a model with MAP estimate = -122.3386
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.6177 - loglik: -1.2243e+02 - logprior: -3.1873e+00
Epoch 2/10
19/19 - 1s - loss: 122.8834 - loglik: -1.2150e+02 - logprior: -1.3830e+00
Epoch 3/10
19/19 - 1s - loss: 122.2154 - loglik: -1.2093e+02 - logprior: -1.2840e+00
Epoch 4/10
19/19 - 1s - loss: 121.3570 - loglik: -1.2012e+02 - logprior: -1.2325e+00
Epoch 5/10
19/19 - 1s - loss: 121.2285 - loglik: -1.2003e+02 - logprior: -1.1972e+00
Epoch 6/10
19/19 - 1s - loss: 120.7896 - loglik: -1.1961e+02 - logprior: -1.1813e+00
Epoch 7/10
19/19 - 1s - loss: 120.2929 - loglik: -1.1913e+02 - logprior: -1.1640e+00
Epoch 8/10
19/19 - 1s - loss: 120.1036 - loglik: -1.1895e+02 - logprior: -1.1520e+00
Epoch 9/10
19/19 - 1s - loss: 119.8999 - loglik: -1.1876e+02 - logprior: -1.1373e+00
Epoch 10/10
19/19 - 1s - loss: 118.8444 - loglik: -1.1771e+02 - logprior: -1.1337e+00
Fitted a model with MAP estimate = -118.1486
Time for alignment: 45.0273
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 164.8692 - loglik: -1.6168e+02 - logprior: -3.1891e+00
Epoch 2/10
19/19 - 1s - loss: 136.1652 - loglik: -1.3478e+02 - logprior: -1.3901e+00
Epoch 3/10
19/19 - 1s - loss: 127.9392 - loglik: -1.2645e+02 - logprior: -1.4888e+00
Epoch 4/10
19/19 - 1s - loss: 126.2159 - loglik: -1.2488e+02 - logprior: -1.3343e+00
Epoch 5/10
19/19 - 1s - loss: 125.4149 - loglik: -1.2409e+02 - logprior: -1.3281e+00
Epoch 6/10
19/19 - 1s - loss: 125.1498 - loglik: -1.2384e+02 - logprior: -1.3076e+00
Epoch 7/10
19/19 - 1s - loss: 124.5548 - loglik: -1.2326e+02 - logprior: -1.2968e+00
Epoch 8/10
19/19 - 1s - loss: 124.5621 - loglik: -1.2327e+02 - logprior: -1.2898e+00
Fitted a model with MAP estimate = -124.2660
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (18, 1), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 134.3500 - loglik: -1.3029e+02 - logprior: -4.0616e+00
Epoch 2/2
19/19 - 1s - loss: 126.4291 - loglik: -1.2422e+02 - logprior: -2.2116e+00
Fitted a model with MAP estimate = -123.6974
expansions: []
discards: [13 14 41 42 51]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.4274 - loglik: -1.2308e+02 - logprior: -3.3459e+00
Epoch 2/2
19/19 - 1s - loss: 122.7512 - loglik: -1.2134e+02 - logprior: -1.4122e+00
Fitted a model with MAP estimate = -122.1066
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.4551 - loglik: -1.2226e+02 - logprior: -3.1929e+00
Epoch 2/10
19/19 - 1s - loss: 122.5711 - loglik: -1.2118e+02 - logprior: -1.3878e+00
Epoch 3/10
19/19 - 1s - loss: 121.6525 - loglik: -1.2036e+02 - logprior: -1.2899e+00
Epoch 4/10
19/19 - 1s - loss: 121.2404 - loglik: -1.2000e+02 - logprior: -1.2371e+00
Epoch 5/10
19/19 - 1s - loss: 120.7483 - loglik: -1.1955e+02 - logprior: -1.2023e+00
Epoch 6/10
19/19 - 1s - loss: 120.1621 - loglik: -1.1898e+02 - logprior: -1.1839e+00
Epoch 7/10
19/19 - 1s - loss: 120.0416 - loglik: -1.1887e+02 - logprior: -1.1722e+00
Epoch 8/10
19/19 - 1s - loss: 119.8145 - loglik: -1.1866e+02 - logprior: -1.1583e+00
Epoch 9/10
19/19 - 1s - loss: 119.3502 - loglik: -1.1820e+02 - logprior: -1.1452e+00
Epoch 10/10
19/19 - 1s - loss: 118.3533 - loglik: -1.1721e+02 - logprior: -1.1398e+00
Fitted a model with MAP estimate = -117.5961
Time for alignment: 44.0960
Computed alignments with likelihoods: ['-118.0135', '-118.1486', '-117.5961']
Best model has likelihood: -117.5961
SP score = 0.8705
Training of 3 independent models on file PF00232.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9638f59f70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9545f40670>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 899.0000 - loglik: -8.9754e+02 - logprior: -1.4612e+00
Epoch 2/10
39/39 - 42s - loss: 717.5226 - loglik: -7.1628e+02 - logprior: -1.2459e+00
Epoch 3/10
39/39 - 42s - loss: 703.6134 - loglik: -7.0219e+02 - logprior: -1.4242e+00
Epoch 4/10
39/39 - 42s - loss: 700.2615 - loglik: -6.9883e+02 - logprior: -1.4340e+00
Epoch 5/10
39/39 - 42s - loss: 698.5873 - loglik: -6.9712e+02 - logprior: -1.4721e+00
Epoch 6/10
39/39 - 42s - loss: 698.5003 - loglik: -6.9707e+02 - logprior: -1.4339e+00
Epoch 7/10
39/39 - 42s - loss: 697.2666 - loglik: -6.9557e+02 - logprior: -1.7012e+00
Epoch 8/10
39/39 - 42s - loss: 697.3644 - loglik: -6.9574e+02 - logprior: -1.6225e+00
Fitted a model with MAP estimate = -696.3528
expansions: [(0, 3), (37, 1), (41, 1), (42, 1), (134, 1), (146, 1), (163, 1), (164, 1), (170, 1), (174, 8), (175, 1), (176, 1), (177, 1), (185, 1), (187, 1), (189, 6), (190, 1), (193, 1), (194, 1), (195, 1), (196, 2), (197, 1), (198, 2), (199, 2), (201, 1), (203, 1), (208, 1), (213, 1), (216, 2), (220, 2), (221, 7), (224, 1), (225, 2), (226, 4), (239, 1), (241, 1), (242, 2), (243, 2), (244, 2), (246, 2), (247, 6), (249, 1), (252, 1), (268, 1), (269, 1), (270, 1), (285, 2), (286, 2), (287, 2), (289, 1), (300, 1), (302, 1), (303, 1), (304, 1), (306, 1), (325, 1), (329, 1), (341, 1), (351, 2), (352, 2), (355, 3)]
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 458 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 67s - loss: 681.2260 - loglik: -6.7901e+02 - logprior: -2.2131e+00
Epoch 2/2
39/39 - 64s - loss: 656.9603 - loglik: -6.5608e+02 - logprior: -8.8491e-01
Fitted a model with MAP estimate = -652.6946
expansions: [(211, 1), (213, 1), (281, 1), (283, 1), (320, 2), (321, 1), (451, 1)]
discards: [185 186 235 258 266 312 325 326 327 369 452 453 454 455 456 457]
Re-initialized the encoder parameters.
Fitting a model of length 450 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 65s - loss: 663.3246 - loglik: -6.6211e+02 - logprior: -1.2101e+00
Epoch 2/2
39/39 - 62s - loss: 654.9698 - loglik: -6.5516e+02 - logprior: 0.1945
Fitted a model with MAP estimate = -652.2174
expansions: [(308, 1), (309, 1), (449, 1), (450, 4)]
discards: [  2   3 310 311 312 313 314 315]
Re-initialized the encoder parameters.
Fitting a model of length 449 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 66s - loss: 661.6389 - loglik: -6.6034e+02 - logprior: -1.3022e+00
Epoch 2/10
39/39 - 62s - loss: 654.0180 - loglik: -6.5434e+02 - logprior: 0.3210
Epoch 3/10
39/39 - 62s - loss: 650.4007 - loglik: -6.5111e+02 - logprior: 0.7044
Epoch 4/10
39/39 - 62s - loss: 648.4716 - loglik: -6.4926e+02 - logprior: 0.7908
Epoch 5/10
39/39 - 62s - loss: 647.9280 - loglik: -6.4861e+02 - logprior: 0.6832
Epoch 6/10
39/39 - 62s - loss: 645.0294 - loglik: -6.4609e+02 - logprior: 1.0593
Epoch 7/10
39/39 - 62s - loss: 646.3049 - loglik: -6.4715e+02 - logprior: 0.8476
Fitted a model with MAP estimate = -644.9634
Time for alignment: 1326.5788
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 898.5294 - loglik: -8.9703e+02 - logprior: -1.5035e+00
Epoch 2/10
39/39 - 42s - loss: 717.2386 - loglik: -7.1604e+02 - logprior: -1.2015e+00
Epoch 3/10
39/39 - 42s - loss: 703.9029 - loglik: -7.0272e+02 - logprior: -1.1780e+00
Epoch 4/10
39/39 - 42s - loss: 700.4181 - loglik: -6.9945e+02 - logprior: -9.6984e-01
Epoch 5/10
39/39 - 42s - loss: 698.5752 - loglik: -6.9762e+02 - logprior: -9.5941e-01
Epoch 6/10
39/39 - 42s - loss: 699.0978 - loglik: -6.9815e+02 - logprior: -9.4475e-01
Fitted a model with MAP estimate = -697.2332
expansions: [(0, 3), (37, 1), (43, 1), (134, 1), (144, 1), (163, 1), (165, 1), (176, 8), (177, 1), (178, 1), (179, 1), (190, 2), (191, 7), (192, 1), (195, 1), (196, 3), (197, 1), (199, 1), (200, 2), (201, 2), (202, 1), (205, 1), (207, 1), (209, 3), (216, 1), (220, 2), (221, 5), (222, 2), (225, 1), (226, 2), (227, 4), (228, 1), (239, 1), (241, 1), (242, 1), (243, 3), (244, 2), (246, 2), (247, 1), (248, 4), (249, 1), (252, 1), (253, 1), (266, 1), (268, 1), (269, 1), (284, 1), (285, 2), (286, 3), (288, 1), (289, 1), (301, 1), (302, 1), (303, 1), (326, 1), (329, 1), (341, 1), (355, 6)]
discards: [2 3]
Re-initialized the encoder parameters.
Fitting a model of length 458 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 67s - loss: 678.0387 - loglik: -6.7584e+02 - logprior: -2.1965e+00
Epoch 2/2
39/39 - 64s - loss: 655.5901 - loglik: -6.5477e+02 - logprior: -8.2178e-01
Fitted a model with MAP estimate = -651.8362
expansions: [(216, 1), (283, 1), (285, 1), (374, 1)]
discards: [  2 186 187 213 214 237 267 274 275 276 277 278 314 315 316 317 318 319
 456]
Re-initialized the encoder parameters.
Fitting a model of length 443 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 64s - loss: 667.0634 - loglik: -6.6570e+02 - logprior: -1.3585e+00
Epoch 2/2
39/39 - 60s - loss: 659.3936 - loglik: -6.5909e+02 - logprior: -2.9899e-01
Fitted a model with MAP estimate = -655.7820
expansions: [(264, 4), (301, 2), (302, 1), (304, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 451 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 65s - loss: 661.2212 - loglik: -6.6012e+02 - logprior: -1.0989e+00
Epoch 2/10
39/39 - 62s - loss: 654.8486 - loglik: -6.5502e+02 - logprior: 0.1670
Epoch 3/10
39/39 - 63s - loss: 651.0928 - loglik: -6.5144e+02 - logprior: 0.3466
Epoch 4/10
39/39 - 62s - loss: 650.0844 - loglik: -6.5062e+02 - logprior: 0.5396
Epoch 5/10
39/39 - 62s - loss: 648.2325 - loglik: -6.4887e+02 - logprior: 0.6361
Epoch 6/10
39/39 - 62s - loss: 648.0530 - loglik: -6.4870e+02 - logprior: 0.6439
Epoch 7/10
39/39 - 62s - loss: 646.4923 - loglik: -6.4740e+02 - logprior: 0.9117
Epoch 8/10
39/39 - 62s - loss: 646.4468 - loglik: -6.4763e+02 - logprior: 1.1857
Epoch 9/10
39/39 - 62s - loss: 646.6351 - loglik: -6.4785e+02 - logprior: 1.2178
Fitted a model with MAP estimate = -645.3176
Time for alignment: 1361.8467
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 45s - loss: 902.0850 - loglik: -9.0065e+02 - logprior: -1.4354e+00
Epoch 2/10
39/39 - 42s - loss: 715.7592 - loglik: -7.1452e+02 - logprior: -1.2420e+00
Epoch 3/10
39/39 - 42s - loss: 700.9661 - loglik: -6.9969e+02 - logprior: -1.2778e+00
Epoch 4/10
39/39 - 42s - loss: 697.7526 - loglik: -6.9663e+02 - logprior: -1.1225e+00
Epoch 5/10
39/39 - 42s - loss: 696.5208 - loglik: -6.9541e+02 - logprior: -1.1152e+00
Epoch 6/10
39/39 - 42s - loss: 695.6414 - loglik: -6.9453e+02 - logprior: -1.1158e+00
Epoch 7/10
39/39 - 42s - loss: 694.6988 - loglik: -6.9359e+02 - logprior: -1.1120e+00
Epoch 8/10
39/39 - 42s - loss: 695.2649 - loglik: -6.9415e+02 - logprior: -1.1178e+00
Fitted a model with MAP estimate = -693.6356
expansions: [(0, 3), (38, 1), (134, 1), (147, 1), (162, 1), (163, 1), (164, 1), (176, 8), (177, 1), (178, 1), (188, 1), (190, 2), (191, 5), (192, 1), (193, 1), (196, 1), (197, 1), (198, 1), (199, 2), (200, 1), (201, 2), (202, 2), (204, 1), (208, 1), (209, 1), (210, 1), (211, 1), (223, 3), (224, 7), (227, 1), (228, 2), (229, 3), (230, 2), (231, 1), (242, 1), (244, 1), (245, 2), (246, 3), (247, 2), (249, 2), (250, 6), (251, 2), (252, 1), (254, 1), (270, 1), (271, 1), (272, 1), (285, 1), (287, 1), (288, 2), (289, 2), (291, 2), (300, 1), (302, 1), (303, 1), (304, 1), (316, 1), (326, 1), (329, 1), (349, 1), (355, 6)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 463 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 68s - loss: 678.3057 - loglik: -6.7610e+02 - logprior: -2.2104e+00
Epoch 2/2
39/39 - 65s - loss: 654.4859 - loglik: -6.5376e+02 - logprior: -7.2701e-01
Fitted a model with MAP estimate = -650.8157
expansions: [(216, 1), (283, 1)]
discards: [  2   3 187 188 189 190 213 214 238 268 269 289 310 318 319 320 321 322
 330 377 461]
Re-initialized the encoder parameters.
Fitting a model of length 444 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 63s - loss: 664.6313 - loglik: -6.6336e+02 - logprior: -1.2706e+00
Epoch 2/2
39/39 - 61s - loss: 656.8146 - loglik: -6.5682e+02 - logprior: 0.0092
Fitted a model with MAP estimate = -654.2531
expansions: [(303, 2), (365, 1)]
discards: [362]
Re-initialized the encoder parameters.
Fitting a model of length 446 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 65s - loss: 661.9852 - loglik: -6.6099e+02 - logprior: -9.9884e-01
Epoch 2/10
39/39 - 61s - loss: 655.5052 - loglik: -6.5588e+02 - logprior: 0.3750
Epoch 3/10
39/39 - 61s - loss: 653.3257 - loglik: -6.5389e+02 - logprior: 0.5676
Epoch 4/10
39/39 - 61s - loss: 651.0530 - loglik: -6.5168e+02 - logprior: 0.6257
Epoch 5/10
39/39 - 61s - loss: 648.8320 - loglik: -6.4984e+02 - logprior: 1.0034
Epoch 6/10
39/39 - 61s - loss: 648.7000 - loglik: -6.4981e+02 - logprior: 1.1061
Epoch 7/10
39/39 - 61s - loss: 647.9034 - loglik: -6.4918e+02 - logprior: 1.2720
Epoch 8/10
39/39 - 61s - loss: 647.5140 - loglik: -6.4884e+02 - logprior: 1.3276
Epoch 9/10
39/39 - 61s - loss: 648.1266 - loglik: -6.4974e+02 - logprior: 1.6102
Fitted a model with MAP estimate = -646.9073
Time for alignment: 1443.9898
Computed alignments with likelihoods: ['-644.9634', '-645.3176', '-646.9073']
Best model has likelihood: -644.9634
SP score = 0.8583
Training of 3 independent models on file PF13522.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f962fc9a0a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9545b9ddf0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 408.3205 - loglik: -4.0540e+02 - logprior: -2.9175e+00
Epoch 2/10
19/19 - 3s - loss: 324.4748 - loglik: -3.2350e+02 - logprior: -9.7770e-01
Epoch 3/10
19/19 - 3s - loss: 295.3140 - loglik: -2.9419e+02 - logprior: -1.1282e+00
Epoch 4/10
19/19 - 3s - loss: 290.7153 - loglik: -2.8966e+02 - logprior: -1.0596e+00
Epoch 5/10
19/19 - 3s - loss: 288.5645 - loglik: -2.8754e+02 - logprior: -1.0247e+00
Epoch 6/10
19/19 - 3s - loss: 286.9714 - loglik: -2.8597e+02 - logprior: -9.9778e-01
Epoch 7/10
19/19 - 3s - loss: 285.4941 - loglik: -2.8452e+02 - logprior: -9.7745e-01
Epoch 8/10
19/19 - 3s - loss: 286.6215 - loglik: -2.8566e+02 - logprior: -9.5875e-01
Fitted a model with MAP estimate = -285.4404
expansions: [(0, 7), (7, 3), (8, 2), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (55, 2), (56, 2), (58, 1), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (97, 1), (98, 1), (100, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 159 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 290.7372 - loglik: -2.8689e+02 - logprior: -3.8490e+00
Epoch 2/2
19/19 - 4s - loss: 276.7542 - loglik: -2.7570e+02 - logprior: -1.0571e+00
Fitted a model with MAP estimate = -274.1581
expansions: [(0, 4)]
discards: [  1   2   3   4   6  22  75  76 102 148]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 282.6182 - loglik: -2.7882e+02 - logprior: -3.7954e+00
Epoch 2/2
19/19 - 4s - loss: 276.7038 - loglik: -2.7558e+02 - logprior: -1.1200e+00
Fitted a model with MAP estimate = -274.8241
expansions: [(0, 4)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 280.0452 - loglik: -2.7704e+02 - logprior: -3.0050e+00
Epoch 2/10
19/19 - 4s - loss: 275.5428 - loglik: -2.7476e+02 - logprior: -7.8679e-01
Epoch 3/10
19/19 - 4s - loss: 273.6325 - loglik: -2.7300e+02 - logprior: -6.3051e-01
Epoch 4/10
19/19 - 4s - loss: 272.7473 - loglik: -2.7220e+02 - logprior: -5.5058e-01
Epoch 5/10
19/19 - 4s - loss: 271.3895 - loglik: -2.7090e+02 - logprior: -4.9371e-01
Epoch 6/10
19/19 - 4s - loss: 271.5748 - loglik: -2.7113e+02 - logprior: -4.4702e-01
Fitted a model with MAP estimate = -270.7562
Time for alignment: 108.0793
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 408.5389 - loglik: -4.0563e+02 - logprior: -2.9116e+00
Epoch 2/10
19/19 - 3s - loss: 327.7980 - loglik: -3.2681e+02 - logprior: -9.8543e-01
Epoch 3/10
19/19 - 3s - loss: 297.4531 - loglik: -2.9632e+02 - logprior: -1.1309e+00
Epoch 4/10
19/19 - 3s - loss: 290.4242 - loglik: -2.8934e+02 - logprior: -1.0848e+00
Epoch 5/10
19/19 - 3s - loss: 289.4868 - loglik: -2.8843e+02 - logprior: -1.0571e+00
Epoch 6/10
19/19 - 3s - loss: 288.3718 - loglik: -2.8735e+02 - logprior: -1.0255e+00
Epoch 7/10
19/19 - 3s - loss: 287.2491 - loglik: -2.8626e+02 - logprior: -9.8945e-01
Epoch 8/10
19/19 - 3s - loss: 287.2721 - loglik: -2.8631e+02 - logprior: -9.6555e-01
Fitted a model with MAP estimate = -286.7720
expansions: [(0, 7), (7, 3), (8, 2), (10, 2), (27, 1), (36, 1), (37, 1), (49, 1), (52, 1), (55, 1), (58, 1), (59, 1), (73, 1), (75, 3), (76, 1), (78, 1), (85, 1), (96, 2), (97, 1), (99, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 291.0388 - loglik: -2.8720e+02 - logprior: -3.8398e+00
Epoch 2/2
19/19 - 4s - loss: 277.0852 - loglik: -2.7599e+02 - logprior: -1.0946e+00
Fitted a model with MAP estimate = -274.9155
expansions: [(0, 4)]
discards: [  1   2   3   4   6  22  99 100 146]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 283.5271 - loglik: -2.7969e+02 - logprior: -3.8352e+00
Epoch 2/2
19/19 - 4s - loss: 277.6882 - loglik: -2.7654e+02 - logprior: -1.1472e+00
Fitted a model with MAP estimate = -275.7514
expansions: [(0, 4)]
discards: [0 1 2 4]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 280.5937 - loglik: -2.7754e+02 - logprior: -3.0522e+00
Epoch 2/10
19/19 - 4s - loss: 276.0775 - loglik: -2.7522e+02 - logprior: -8.5631e-01
Epoch 3/10
19/19 - 4s - loss: 274.6897 - loglik: -2.7403e+02 - logprior: -6.6085e-01
Epoch 4/10
19/19 - 4s - loss: 273.4833 - loglik: -2.7290e+02 - logprior: -5.8796e-01
Epoch 5/10
19/19 - 4s - loss: 273.2006 - loglik: -2.7268e+02 - logprior: -5.2420e-01
Epoch 6/10
19/19 - 4s - loss: 271.5789 - loglik: -2.7110e+02 - logprior: -4.8269e-01
Epoch 7/10
19/19 - 4s - loss: 271.7876 - loglik: -2.7134e+02 - logprior: -4.5195e-01
Fitted a model with MAP estimate = -271.1883
Time for alignment: 111.5232
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 408.3762 - loglik: -4.0546e+02 - logprior: -2.9124e+00
Epoch 2/10
19/19 - 3s - loss: 322.8287 - loglik: -3.2184e+02 - logprior: -9.9228e-01
Epoch 3/10
19/19 - 3s - loss: 294.0336 - loglik: -2.9288e+02 - logprior: -1.1492e+00
Epoch 4/10
19/19 - 3s - loss: 288.9388 - loglik: -2.8782e+02 - logprior: -1.1162e+00
Epoch 5/10
19/19 - 3s - loss: 287.3257 - loglik: -2.8625e+02 - logprior: -1.0717e+00
Epoch 6/10
19/19 - 3s - loss: 287.0721 - loglik: -2.8604e+02 - logprior: -1.0360e+00
Epoch 7/10
19/19 - 3s - loss: 286.1979 - loglik: -2.8519e+02 - logprior: -1.0106e+00
Epoch 8/10
19/19 - 3s - loss: 285.8477 - loglik: -2.8485e+02 - logprior: -1.0019e+00
Epoch 9/10
19/19 - 3s - loss: 285.5144 - loglik: -2.8452e+02 - logprior: -9.9517e-01
Epoch 10/10
19/19 - 3s - loss: 286.3341 - loglik: -2.8534e+02 - logprior: -9.9291e-01
Fitted a model with MAP estimate = -285.0598
expansions: [(0, 7), (7, 3), (8, 2), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (53, 1), (55, 1), (58, 1), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (97, 1), (98, 1), (100, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 290.3108 - loglik: -2.8641e+02 - logprior: -3.9010e+00
Epoch 2/2
19/19 - 4s - loss: 276.4714 - loglik: -2.7544e+02 - logprior: -1.0353e+00
Fitted a model with MAP estimate = -274.0508
expansions: [(0, 4)]
discards: [  1   2   3   4   6  22  99 100 146]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 282.6942 - loglik: -2.7892e+02 - logprior: -3.7765e+00
Epoch 2/2
19/19 - 4s - loss: 276.8800 - loglik: -2.7578e+02 - logprior: -1.1049e+00
Fitted a model with MAP estimate = -274.9652
expansions: [(0, 4)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 280.1217 - loglik: -2.7713e+02 - logprior: -2.9916e+00
Epoch 2/10
19/19 - 4s - loss: 275.3479 - loglik: -2.7456e+02 - logprior: -7.8449e-01
Epoch 3/10
19/19 - 4s - loss: 273.9154 - loglik: -2.7330e+02 - logprior: -6.1957e-01
Epoch 4/10
19/19 - 4s - loss: 272.7901 - loglik: -2.7223e+02 - logprior: -5.5628e-01
Epoch 5/10
19/19 - 4s - loss: 272.0284 - loglik: -2.7155e+02 - logprior: -4.7949e-01
Epoch 6/10
19/19 - 4s - loss: 270.7416 - loglik: -2.7030e+02 - logprior: -4.4623e-01
Epoch 7/10
19/19 - 4s - loss: 271.1631 - loglik: -2.7075e+02 - logprior: -4.0891e-01
Fitted a model with MAP estimate = -270.3737
Time for alignment: 117.6459
Computed alignments with likelihoods: ['-270.7562', '-271.1883', '-270.3737']
Best model has likelihood: -270.3737
SP score = 0.6515
Training of 3 independent models on file PF00037.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f966b967e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95e21447c0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 68.9180 - loglik: -6.5553e+01 - logprior: -3.3652e+00
Epoch 2/10
19/19 - 0s - loss: 51.6776 - loglik: -5.0222e+01 - logprior: -1.4556e+00
Epoch 3/10
19/19 - 0s - loss: 45.6627 - loglik: -4.4131e+01 - logprior: -1.5313e+00
Epoch 4/10
19/19 - 1s - loss: 44.2723 - loglik: -4.2744e+01 - logprior: -1.5280e+00
Epoch 5/10
19/19 - 0s - loss: 43.8967 - loglik: -4.2394e+01 - logprior: -1.5022e+00
Epoch 6/10
19/19 - 0s - loss: 43.5872 - loglik: -4.2094e+01 - logprior: -1.4928e+00
Epoch 7/10
19/19 - 0s - loss: 43.5046 - loglik: -4.2016e+01 - logprior: -1.4891e+00
Epoch 8/10
19/19 - 1s - loss: 43.3542 - loglik: -4.1870e+01 - logprior: -1.4838e+00
Epoch 9/10
19/19 - 1s - loss: 43.2414 - loglik: -4.1762e+01 - logprior: -1.4790e+00
Epoch 10/10
19/19 - 1s - loss: 43.1721 - loglik: -4.1692e+01 - logprior: -1.4797e+00
Fitted a model with MAP estimate = -43.1351
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 47.3285 - loglik: -4.2599e+01 - logprior: -4.7293e+00
Epoch 2/2
19/19 - 0s - loss: 41.9171 - loglik: -4.0560e+01 - logprior: -1.3572e+00
Fitted a model with MAP estimate = -40.9965
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 44.1576 - loglik: -4.0846e+01 - logprior: -3.3114e+00
Epoch 2/2
19/19 - 0s - loss: 41.5350 - loglik: -4.0016e+01 - logprior: -1.5189e+00
Fitted a model with MAP estimate = -41.0850
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 43.8249 - loglik: -4.0569e+01 - logprior: -3.2560e+00
Epoch 2/10
19/19 - 0s - loss: 41.4435 - loglik: -3.9948e+01 - logprior: -1.4958e+00
Epoch 3/10
19/19 - 0s - loss: 40.8146 - loglik: -3.9406e+01 - logprior: -1.4090e+00
Epoch 4/10
19/19 - 0s - loss: 40.4651 - loglik: -3.9106e+01 - logprior: -1.3590e+00
Epoch 5/10
19/19 - 0s - loss: 40.1506 - loglik: -3.8819e+01 - logprior: -1.3319e+00
Epoch 6/10
19/19 - 0s - loss: 39.8598 - loglik: -3.8541e+01 - logprior: -1.3185e+00
Epoch 7/10
19/19 - 0s - loss: 39.6816 - loglik: -3.8370e+01 - logprior: -1.3115e+00
Epoch 8/10
19/19 - 0s - loss: 39.6304 - loglik: -3.8332e+01 - logprior: -1.2986e+00
Epoch 9/10
19/19 - 0s - loss: 39.4409 - loglik: -3.8147e+01 - logprior: -1.2937e+00
Epoch 10/10
19/19 - 1s - loss: 39.3817 - loglik: -3.8095e+01 - logprior: -1.2871e+00
Fitted a model with MAP estimate = -39.3136
Time for alignment: 29.9789
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 68.8737 - loglik: -6.5518e+01 - logprior: -3.3559e+00
Epoch 2/10
19/19 - 0s - loss: 51.9945 - loglik: -5.0558e+01 - logprior: -1.4368e+00
Epoch 3/10
19/19 - 0s - loss: 46.5122 - loglik: -4.5102e+01 - logprior: -1.4104e+00
Epoch 4/10
19/19 - 0s - loss: 44.9971 - loglik: -4.3594e+01 - logprior: -1.4033e+00
Epoch 5/10
19/19 - 0s - loss: 44.4959 - loglik: -4.3021e+01 - logprior: -1.4748e+00
Epoch 6/10
19/19 - 0s - loss: 43.9878 - loglik: -4.2511e+01 - logprior: -1.4769e+00
Epoch 7/10
19/19 - 0s - loss: 43.8433 - loglik: -4.2366e+01 - logprior: -1.4776e+00
Epoch 8/10
19/19 - 0s - loss: 43.6978 - loglik: -4.2228e+01 - logprior: -1.4703e+00
Epoch 9/10
19/19 - 0s - loss: 43.4907 - loglik: -4.2022e+01 - logprior: -1.4690e+00
Epoch 10/10
19/19 - 0s - loss: 43.4172 - loglik: -4.1950e+01 - logprior: -1.4670e+00
Fitted a model with MAP estimate = -43.3220
expansions: [(0, 1), (2, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 47.5472 - loglik: -4.2840e+01 - logprior: -4.7069e+00
Epoch 2/2
19/19 - 0s - loss: 41.9330 - loglik: -4.0546e+01 - logprior: -1.3874e+00
Fitted a model with MAP estimate = -40.9949
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 44.1340 - loglik: -4.0830e+01 - logprior: -3.3036e+00
Epoch 2/2
19/19 - 0s - loss: 41.5024 - loglik: -3.9983e+01 - logprior: -1.5192e+00
Fitted a model with MAP estimate = -41.0696
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 43.7801 - loglik: -4.0527e+01 - logprior: -3.2530e+00
Epoch 2/10
19/19 - 0s - loss: 41.4692 - loglik: -3.9970e+01 - logprior: -1.4997e+00
Epoch 3/10
19/19 - 0s - loss: 40.7904 - loglik: -3.9383e+01 - logprior: -1.4076e+00
Epoch 4/10
19/19 - 0s - loss: 40.4930 - loglik: -3.9134e+01 - logprior: -1.3587e+00
Epoch 5/10
19/19 - 0s - loss: 40.0789 - loglik: -3.8750e+01 - logprior: -1.3285e+00
Epoch 6/10
19/19 - 1s - loss: 39.9335 - loglik: -3.8611e+01 - logprior: -1.3221e+00
Epoch 7/10
19/19 - 0s - loss: 39.6612 - loglik: -3.8353e+01 - logprior: -1.3080e+00
Epoch 8/10
19/19 - 0s - loss: 39.6221 - loglik: -3.8325e+01 - logprior: -1.2971e+00
Epoch 9/10
19/19 - 1s - loss: 39.3389 - loglik: -3.8045e+01 - logprior: -1.2944e+00
Epoch 10/10
19/19 - 0s - loss: 39.4228 - loglik: -3.8137e+01 - logprior: -1.2859e+00
Fitted a model with MAP estimate = -39.3044
Time for alignment: 28.2533
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 68.9475 - loglik: -6.5597e+01 - logprior: -3.3509e+00
Epoch 2/10
19/19 - 0s - loss: 51.7385 - loglik: -5.0307e+01 - logprior: -1.4313e+00
Epoch 3/10
19/19 - 0s - loss: 46.1791 - loglik: -4.4734e+01 - logprior: -1.4451e+00
Epoch 4/10
19/19 - 0s - loss: 44.5162 - loglik: -4.3014e+01 - logprior: -1.5022e+00
Epoch 5/10
19/19 - 0s - loss: 43.9436 - loglik: -4.2445e+01 - logprior: -1.4983e+00
Epoch 6/10
19/19 - 0s - loss: 43.6809 - loglik: -4.2192e+01 - logprior: -1.4887e+00
Epoch 7/10
19/19 - 0s - loss: 43.4531 - loglik: -4.1965e+01 - logprior: -1.4883e+00
Epoch 8/10
19/19 - 0s - loss: 43.3408 - loglik: -4.1860e+01 - logprior: -1.4809e+00
Epoch 9/10
19/19 - 1s - loss: 43.3132 - loglik: -4.1835e+01 - logprior: -1.4785e+00
Epoch 10/10
19/19 - 0s - loss: 43.1819 - loglik: -4.1707e+01 - logprior: -1.4750e+00
Fitted a model with MAP estimate = -43.1315
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 47.2891 - loglik: -4.2549e+01 - logprior: -4.7403e+00
Epoch 2/2
19/19 - 0s - loss: 41.9472 - loglik: -4.0580e+01 - logprior: -1.3672e+00
Fitted a model with MAP estimate = -41.0226
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 44.1831 - loglik: -4.0873e+01 - logprior: -3.3105e+00
Epoch 2/2
19/19 - 1s - loss: 41.4511 - loglik: -3.9934e+01 - logprior: -1.5170e+00
Fitted a model with MAP estimate = -41.0843
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 43.8161 - loglik: -4.0561e+01 - logprior: -3.2554e+00
Epoch 2/10
19/19 - 0s - loss: 41.4056 - loglik: -3.9908e+01 - logprior: -1.4972e+00
Epoch 3/10
19/19 - 0s - loss: 40.8307 - loglik: -3.9420e+01 - logprior: -1.4107e+00
Epoch 4/10
19/19 - 0s - loss: 40.4829 - loglik: -3.9125e+01 - logprior: -1.3581e+00
Epoch 5/10
19/19 - 0s - loss: 40.1742 - loglik: -3.8845e+01 - logprior: -1.3296e+00
Epoch 6/10
19/19 - 0s - loss: 39.8736 - loglik: -3.8552e+01 - logprior: -1.3219e+00
Epoch 7/10
19/19 - 0s - loss: 39.6949 - loglik: -3.8388e+01 - logprior: -1.3073e+00
Epoch 8/10
19/19 - 0s - loss: 39.5268 - loglik: -3.8225e+01 - logprior: -1.3022e+00
Epoch 9/10
19/19 - 0s - loss: 39.3791 - loglik: -3.8089e+01 - logprior: -1.2905e+00
Epoch 10/10
19/19 - 0s - loss: 39.4581 - loglik: -3.8175e+01 - logprior: -1.2835e+00
Fitted a model with MAP estimate = -39.3071
Time for alignment: 28.7883
Computed alignments with likelihoods: ['-39.3136', '-39.3044', '-39.3071']
Best model has likelihood: -39.3044
SP score = 0.8523
Training of 3 independent models on file PF00224.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f954647b2e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95e2b4cdf0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 734.5344 - loglik: -7.3300e+02 - logprior: -1.5303e+00
Epoch 2/10
39/39 - 21s - loss: 524.6132 - loglik: -5.2299e+02 - logprior: -1.6201e+00
Epoch 3/10
39/39 - 21s - loss: 512.9904 - loglik: -5.1142e+02 - logprior: -1.5671e+00
Epoch 4/10
39/39 - 21s - loss: 510.7527 - loglik: -5.0927e+02 - logprior: -1.4836e+00
Epoch 5/10
39/39 - 21s - loss: 509.9816 - loglik: -5.0851e+02 - logprior: -1.4705e+00
Epoch 6/10
39/39 - 21s - loss: 509.1606 - loglik: -5.0768e+02 - logprior: -1.4772e+00
Epoch 7/10
39/39 - 21s - loss: 508.6612 - loglik: -5.0717e+02 - logprior: -1.4872e+00
Epoch 8/10
39/39 - 21s - loss: 508.6827 - loglik: -5.0719e+02 - logprior: -1.4928e+00
Fitted a model with MAP estimate = -507.7697
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (96, 1), (109, 1), (110, 1), (111, 1), (113, 1), (120, 1), (132, 1), (133, 1), (145, 2), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (179, 1), (182, 1), (188, 1), (189, 1), (206, 1), (212, 1), (214, 1), (215, 1), (216, 1), (217, 1), (227, 1), (230, 2), (231, 1), (244, 1), (257, 1), (258, 1), (267, 1), (269, 1), (270, 1), (272, 3), (273, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 482.8030 - loglik: -4.8092e+02 - logprior: -1.8834e+00
Epoch 2/2
39/39 - 30s - loss: 464.0112 - loglik: -4.6347e+02 - logprior: -5.4529e-01
Fitted a model with MAP estimate = -461.4801
expansions: []
discards: [  0  97 178 213]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 471.1921 - loglik: -4.6888e+02 - logprior: -2.3163e+00
Epoch 2/2
39/39 - 30s - loss: 464.6183 - loglik: -4.6441e+02 - logprior: -2.0684e-01
Fitted a model with MAP estimate = -462.0094
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 33s - loss: 467.7997 - loglik: -4.6679e+02 - logprior: -1.0065e+00
Epoch 2/10
39/39 - 30s - loss: 462.7099 - loglik: -4.6291e+02 - logprior: 0.2028
Epoch 3/10
39/39 - 30s - loss: 460.8918 - loglik: -4.6110e+02 - logprior: 0.2044
Epoch 4/10
39/39 - 30s - loss: 459.9173 - loglik: -4.6044e+02 - logprior: 0.5210
Epoch 5/10
39/39 - 30s - loss: 458.6030 - loglik: -4.5908e+02 - logprior: 0.4740
Epoch 6/10
39/39 - 30s - loss: 458.5770 - loglik: -4.5936e+02 - logprior: 0.7875
Epoch 7/10
39/39 - 30s - loss: 457.5640 - loglik: -4.5836e+02 - logprior: 0.7985
Epoch 8/10
39/39 - 30s - loss: 457.8050 - loglik: -4.5885e+02 - logprior: 1.0421
Fitted a model with MAP estimate = -457.6948
Time for alignment: 705.8209
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 732.5013 - loglik: -7.3097e+02 - logprior: -1.5288e+00
Epoch 2/10
39/39 - 21s - loss: 523.7390 - loglik: -5.2213e+02 - logprior: -1.6117e+00
Epoch 3/10
39/39 - 21s - loss: 512.7975 - loglik: -5.1123e+02 - logprior: -1.5719e+00
Epoch 4/10
39/39 - 21s - loss: 510.4887 - loglik: -5.0900e+02 - logprior: -1.4932e+00
Epoch 5/10
39/39 - 21s - loss: 509.7337 - loglik: -5.0824e+02 - logprior: -1.4899e+00
Epoch 6/10
39/39 - 21s - loss: 509.1005 - loglik: -5.0760e+02 - logprior: -1.5032e+00
Epoch 7/10
39/39 - 21s - loss: 508.5776 - loglik: -5.0707e+02 - logprior: -1.5097e+00
Epoch 8/10
39/39 - 21s - loss: 508.5325 - loglik: -5.0701e+02 - logprior: -1.5268e+00
Epoch 9/10
39/39 - 21s - loss: 508.8620 - loglik: -5.0734e+02 - logprior: -1.5257e+00
Fitted a model with MAP estimate = -507.6871
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (105, 1), (109, 1), (110, 2), (112, 2), (113, 1), (133, 1), (134, 1), (142, 1), (145, 2), (147, 1), (148, 1), (159, 1), (163, 1), (167, 1), (170, 1), (171, 3), (172, 1), (181, 1), (182, 1), (188, 1), (205, 1), (209, 1), (212, 1), (214, 1), (215, 1), (216, 1), (217, 1), (228, 1), (230, 2), (231, 1), (244, 1), (259, 1), (265, 1), (267, 1), (269, 1), (270, 1), (272, 3), (273, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 482.7907 - loglik: -4.8091e+02 - logprior: -1.8761e+00
Epoch 2/2
39/39 - 30s - loss: 463.5916 - loglik: -4.6310e+02 - logprior: -4.9363e-01
Fitted a model with MAP estimate = -461.2675
expansions: [(0, 2)]
discards: [  0   1  97 137 141 180 215]
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 469.0807 - loglik: -4.6780e+02 - logprior: -1.2815e+00
Epoch 2/2
39/39 - 30s - loss: 463.1684 - loglik: -4.6298e+02 - logprior: -1.9312e-01
Fitted a model with MAP estimate = -461.3544
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 33s - loss: 471.7558 - loglik: -4.7000e+02 - logprior: -1.7597e+00
Epoch 2/10
39/39 - 30s - loss: 464.5544 - loglik: -4.6474e+02 - logprior: 0.1887
Epoch 3/10
39/39 - 30s - loss: 462.0805 - loglik: -4.6256e+02 - logprior: 0.4829
Epoch 4/10
39/39 - 30s - loss: 460.7597 - loglik: -4.6125e+02 - logprior: 0.4951
Epoch 5/10
39/39 - 30s - loss: 459.3698 - loglik: -4.6031e+02 - logprior: 0.9389
Epoch 6/10
39/39 - 30s - loss: 459.3664 - loglik: -4.6011e+02 - logprior: 0.7471
Epoch 7/10
39/39 - 30s - loss: 458.8719 - loglik: -4.5995e+02 - logprior: 1.0805
Epoch 8/10
39/39 - 30s - loss: 458.4076 - loglik: -4.5973e+02 - logprior: 1.3175
Epoch 9/10
39/39 - 30s - loss: 458.8666 - loglik: -4.6006e+02 - logprior: 1.1918
Fitted a model with MAP estimate = -457.9140
Time for alignment: 756.0373
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 737.1016 - loglik: -7.3556e+02 - logprior: -1.5453e+00
Epoch 2/10
39/39 - 21s - loss: 534.7331 - loglik: -5.3320e+02 - logprior: -1.5333e+00
Epoch 3/10
39/39 - 21s - loss: 520.8604 - loglik: -5.1937e+02 - logprior: -1.4924e+00
Epoch 4/10
39/39 - 21s - loss: 518.0454 - loglik: -5.1655e+02 - logprior: -1.4966e+00
Epoch 5/10
39/39 - 21s - loss: 515.7778 - loglik: -5.1427e+02 - logprior: -1.5091e+00
Epoch 6/10
39/39 - 21s - loss: 515.3500 - loglik: -5.1383e+02 - logprior: -1.5219e+00
Epoch 7/10
39/39 - 21s - loss: 515.4410 - loglik: -5.1392e+02 - logprior: -1.5225e+00
Fitted a model with MAP estimate = -514.3110
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (65, 2), (69, 1), (71, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (96, 1), (109, 1), (110, 1), (111, 1), (113, 1), (132, 3), (133, 1), (142, 1), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (170, 1), (171, 3), (172, 1), (179, 1), (182, 1), (188, 1), (189, 1), (209, 1), (212, 1), (214, 1), (215, 1), (216, 1), (217, 1), (227, 1), (229, 1), (230, 1), (231, 1), (244, 1), (246, 1), (258, 1), (264, 1), (267, 1), (269, 1), (272, 3), (273, 5)]
discards: [52]
Re-initialized the encoder parameters.
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 489.0518 - loglik: -4.8713e+02 - logprior: -1.9217e+00
Epoch 2/2
39/39 - 30s - loss: 469.5073 - loglik: -4.6896e+02 - logprior: -5.4821e-01
Fitted a model with MAP estimate = -466.8263
expansions: []
discards: [  0  77  97 163 213]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 476.7709 - loglik: -4.7439e+02 - logprior: -2.3838e+00
Epoch 2/2
39/39 - 30s - loss: 470.5879 - loglik: -4.7029e+02 - logprior: -2.9498e-01
Fitted a model with MAP estimate = -467.6558
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 33s - loss: 473.5095 - loglik: -4.7242e+02 - logprior: -1.0872e+00
Epoch 2/10
39/39 - 30s - loss: 468.2918 - loglik: -4.6840e+02 - logprior: 0.1128
Epoch 3/10
39/39 - 30s - loss: 466.2088 - loglik: -4.6658e+02 - logprior: 0.3760
Epoch 4/10
39/39 - 30s - loss: 465.6166 - loglik: -4.6597e+02 - logprior: 0.3531
Epoch 5/10
39/39 - 30s - loss: 464.3915 - loglik: -4.6506e+02 - logprior: 0.6698
Epoch 6/10
39/39 - 30s - loss: 463.9108 - loglik: -4.6446e+02 - logprior: 0.5503
Epoch 7/10
39/39 - 30s - loss: 463.1472 - loglik: -4.6412e+02 - logprior: 0.9761
Epoch 8/10
39/39 - 30s - loss: 463.7181 - loglik: -4.6445e+02 - logprior: 0.7287
Fitted a model with MAP estimate = -463.0310
Time for alignment: 683.7878
Computed alignments with likelihoods: ['-457.6948', '-457.9140', '-463.0310']
Best model has likelihood: -457.6948
SP score = 0.9147
Training of 3 independent models on file PF13365.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9649e5b880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95f42aadc0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 397.6755 - loglik: -3.9466e+02 - logprior: -3.0111e+00
Epoch 2/10
19/19 - 4s - loss: 325.8660 - loglik: -3.2463e+02 - logprior: -1.2398e+00
Epoch 3/10
19/19 - 4s - loss: 303.1433 - loglik: -3.0154e+02 - logprior: -1.5984e+00
Epoch 4/10
19/19 - 4s - loss: 298.6808 - loglik: -2.9726e+02 - logprior: -1.4238e+00
Epoch 5/10
19/19 - 4s - loss: 296.9691 - loglik: -2.9559e+02 - logprior: -1.3830e+00
Epoch 6/10
19/19 - 4s - loss: 296.3133 - loglik: -2.9496e+02 - logprior: -1.3544e+00
Epoch 7/10
19/19 - 4s - loss: 295.6146 - loglik: -2.9428e+02 - logprior: -1.3343e+00
Epoch 8/10
19/19 - 4s - loss: 293.6680 - loglik: -2.9235e+02 - logprior: -1.3168e+00
Epoch 9/10
19/19 - 4s - loss: 295.9814 - loglik: -2.9468e+02 - logprior: -1.3045e+00
Fitted a model with MAP estimate = -293.0704
expansions: [(7, 2), (21, 3), (22, 1), (25, 2), (29, 2), (35, 1), (37, 2), (45, 2), (46, 3), (49, 1), (50, 2), (57, 1), (71, 1), (84, 2), (85, 3), (86, 6), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 307.5268 - loglik: -3.0348e+02 - logprior: -4.0496e+00
Epoch 2/2
19/19 - 5s - loss: 295.0777 - loglik: -2.9270e+02 - logprior: -2.3776e+00
Fitted a model with MAP estimate = -291.5047
expansions: [(0, 2)]
discards: [  0  23  30  36  48  59  60  61  68 107 140]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 295.6355 - loglik: -2.9256e+02 - logprior: -3.0804e+00
Epoch 2/2
19/19 - 5s - loss: 289.8802 - loglik: -2.8868e+02 - logprior: -1.2047e+00
Fitted a model with MAP estimate = -287.3083
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10065 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 296.6542 - loglik: -2.9273e+02 - logprior: -3.9208e+00
Epoch 2/10
19/19 - 5s - loss: 290.5849 - loglik: -2.8907e+02 - logprior: -1.5124e+00
Epoch 3/10
19/19 - 5s - loss: 287.0783 - loglik: -2.8592e+02 - logprior: -1.1613e+00
Epoch 4/10
19/19 - 5s - loss: 286.5097 - loglik: -2.8545e+02 - logprior: -1.0614e+00
Epoch 5/10
19/19 - 5s - loss: 284.2498 - loglik: -2.8325e+02 - logprior: -1.0011e+00
Epoch 6/10
19/19 - 5s - loss: 284.2077 - loglik: -2.8326e+02 - logprior: -9.5171e-01
Epoch 7/10
19/19 - 5s - loss: 282.9522 - loglik: -2.8205e+02 - logprior: -9.0107e-01
Epoch 8/10
19/19 - 5s - loss: 281.2644 - loglik: -2.8042e+02 - logprior: -8.4846e-01
Epoch 9/10
19/19 - 5s - loss: 282.1679 - loglik: -2.8137e+02 - logprior: -7.9467e-01
Fitted a model with MAP estimate = -281.2676
Time for alignment: 140.3053
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 397.8276 - loglik: -3.9481e+02 - logprior: -3.0159e+00
Epoch 2/10
19/19 - 4s - loss: 326.2934 - loglik: -3.2507e+02 - logprior: -1.2246e+00
Epoch 3/10
19/19 - 4s - loss: 303.4601 - loglik: -3.0186e+02 - logprior: -1.6018e+00
Epoch 4/10
19/19 - 4s - loss: 298.9495 - loglik: -2.9752e+02 - logprior: -1.4294e+00
Epoch 5/10
19/19 - 4s - loss: 297.4392 - loglik: -2.9605e+02 - logprior: -1.3843e+00
Epoch 6/10
19/19 - 4s - loss: 295.8124 - loglik: -2.9446e+02 - logprior: -1.3532e+00
Epoch 7/10
19/19 - 4s - loss: 295.3808 - loglik: -2.9405e+02 - logprior: -1.3314e+00
Epoch 8/10
19/19 - 4s - loss: 294.8598 - loglik: -2.9354e+02 - logprior: -1.3206e+00
Epoch 9/10
19/19 - 4s - loss: 293.7179 - loglik: -2.9241e+02 - logprior: -1.3084e+00
Epoch 10/10
19/19 - 4s - loss: 293.9382 - loglik: -2.9264e+02 - logprior: -1.2985e+00
Fitted a model with MAP estimate = -292.8886
expansions: [(7, 2), (21, 3), (22, 1), (25, 1), (29, 2), (35, 1), (37, 2), (45, 2), (46, 3), (49, 1), (50, 2), (57, 1), (71, 1), (84, 2), (85, 3), (86, 6), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 147 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 307.5460 - loglik: -3.0350e+02 - logprior: -4.0505e+00
Epoch 2/2
19/19 - 5s - loss: 295.0258 - loglik: -2.9265e+02 - logprior: -2.3763e+00
Fitted a model with MAP estimate = -291.5296
expansions: [(0, 2)]
discards: [  0  23  35  47  58  59  67 106 139]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 295.0216 - loglik: -2.9193e+02 - logprior: -3.0941e+00
Epoch 2/2
19/19 - 5s - loss: 289.4908 - loglik: -2.8828e+02 - logprior: -1.2071e+00
Fitted a model with MAP estimate = -287.1593
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10065 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 296.5401 - loglik: -2.9262e+02 - logprior: -3.9237e+00
Epoch 2/10
19/19 - 5s - loss: 290.4461 - loglik: -2.8896e+02 - logprior: -1.4889e+00
Epoch 3/10
19/19 - 5s - loss: 287.3220 - loglik: -2.8615e+02 - logprior: -1.1717e+00
Epoch 4/10
19/19 - 5s - loss: 285.2143 - loglik: -2.8414e+02 - logprior: -1.0696e+00
Epoch 5/10
19/19 - 5s - loss: 284.8704 - loglik: -2.8388e+02 - logprior: -9.9357e-01
Epoch 6/10
19/19 - 5s - loss: 284.5452 - loglik: -2.8360e+02 - logprior: -9.4255e-01
Epoch 7/10
19/19 - 5s - loss: 282.2584 - loglik: -2.8136e+02 - logprior: -8.9876e-01
Epoch 8/10
19/19 - 5s - loss: 281.2278 - loglik: -2.8039e+02 - logprior: -8.3604e-01
Epoch 9/10
19/19 - 5s - loss: 282.7995 - loglik: -2.8202e+02 - logprior: -7.7858e-01
Fitted a model with MAP estimate = -281.0835
Time for alignment: 144.7379
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 397.3100 - loglik: -3.9429e+02 - logprior: -3.0207e+00
Epoch 2/10
19/19 - 4s - loss: 327.1971 - loglik: -3.2597e+02 - logprior: -1.2307e+00
Epoch 3/10
19/19 - 4s - loss: 303.7452 - loglik: -3.0217e+02 - logprior: -1.5777e+00
Epoch 4/10
19/19 - 4s - loss: 299.1576 - loglik: -2.9774e+02 - logprior: -1.4220e+00
Epoch 5/10
19/19 - 4s - loss: 297.2969 - loglik: -2.9592e+02 - logprior: -1.3779e+00
Epoch 6/10
19/19 - 4s - loss: 295.7855 - loglik: -2.9443e+02 - logprior: -1.3508e+00
Epoch 7/10
19/19 - 4s - loss: 295.7837 - loglik: -2.9446e+02 - logprior: -1.3279e+00
Epoch 8/10
19/19 - 4s - loss: 293.2439 - loglik: -2.9193e+02 - logprior: -1.3119e+00
Epoch 9/10
19/19 - 4s - loss: 294.0776 - loglik: -2.9277e+02 - logprior: -1.3035e+00
Fitted a model with MAP estimate = -292.8512
expansions: [(7, 2), (21, 3), (22, 1), (25, 2), (28, 1), (29, 2), (37, 2), (45, 2), (46, 3), (49, 1), (50, 2), (57, 1), (71, 1), (84, 2), (85, 3), (86, 6), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 307.2821 - loglik: -3.0323e+02 - logprior: -4.0565e+00
Epoch 2/2
19/19 - 5s - loss: 295.4641 - loglik: -2.9308e+02 - logprior: -2.3888e+00
Fitted a model with MAP estimate = -291.4891
expansions: [(0, 2)]
discards: [  0  23  30  37  48  59  61  68 107 140]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 295.4167 - loglik: -2.9235e+02 - logprior: -3.0684e+00
Epoch 2/2
19/19 - 5s - loss: 289.4673 - loglik: -2.8826e+02 - logprior: -1.2072e+00
Fitted a model with MAP estimate = -287.1220
expansions: []
discards: [ 0 56]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10065 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 297.1706 - loglik: -2.9326e+02 - logprior: -3.9097e+00
Epoch 2/10
19/19 - 5s - loss: 289.6910 - loglik: -2.8818e+02 - logprior: -1.5112e+00
Epoch 3/10
19/19 - 5s - loss: 288.1506 - loglik: -2.8697e+02 - logprior: -1.1814e+00
Epoch 4/10
19/19 - 5s - loss: 285.3666 - loglik: -2.8430e+02 - logprior: -1.0689e+00
Epoch 5/10
19/19 - 5s - loss: 284.8507 - loglik: -2.8384e+02 - logprior: -1.0069e+00
Epoch 6/10
19/19 - 5s - loss: 283.8445 - loglik: -2.8289e+02 - logprior: -9.5491e-01
Epoch 7/10
19/19 - 5s - loss: 282.8877 - loglik: -2.8199e+02 - logprior: -8.9736e-01
Epoch 8/10
19/19 - 5s - loss: 281.5806 - loglik: -2.8075e+02 - logprior: -8.2710e-01
Epoch 9/10
19/19 - 5s - loss: 281.7256 - loglik: -2.8093e+02 - logprior: -7.9805e-01
Fitted a model with MAP estimate = -281.2291
Time for alignment: 140.2307
Computed alignments with likelihoods: ['-281.2676', '-281.0835', '-281.2291']
Best model has likelihood: -281.0835
SP score = 0.3102
Training of 3 independent models on file PF00078.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95e36d24f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95d9042bb0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 487.9009 - loglik: -4.8506e+02 - logprior: -2.8372e+00
Epoch 2/10
19/19 - 5s - loss: 445.9845 - loglik: -4.4488e+02 - logprior: -1.1039e+00
Epoch 3/10
19/19 - 5s - loss: 421.9921 - loglik: -4.2066e+02 - logprior: -1.3368e+00
Epoch 4/10
19/19 - 5s - loss: 417.0144 - loglik: -4.1571e+02 - logprior: -1.3066e+00
Epoch 5/10
19/19 - 5s - loss: 414.7163 - loglik: -4.1341e+02 - logprior: -1.3098e+00
Epoch 6/10
19/19 - 5s - loss: 411.3648 - loglik: -4.1005e+02 - logprior: -1.3152e+00
Epoch 7/10
19/19 - 5s - loss: 411.2701 - loglik: -4.0995e+02 - logprior: -1.3206e+00
Epoch 8/10
19/19 - 5s - loss: 409.5699 - loglik: -4.0825e+02 - logprior: -1.3236e+00
Epoch 9/10
19/19 - 5s - loss: 409.1426 - loglik: -4.0780e+02 - logprior: -1.3470e+00
Epoch 10/10
19/19 - 5s - loss: 409.2664 - loglik: -4.0792e+02 - logprior: -1.3466e+00
Fitted a model with MAP estimate = -408.7772
expansions: [(19, 1), (21, 1), (30, 1), (31, 15), (32, 2), (46, 1), (47, 1), (48, 1), (49, 2), (58, 3), (59, 3), (74, 1), (76, 1), (79, 1), (80, 9), (94, 1), (97, 1), (101, 1), (106, 3), (108, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 440.8082 - loglik: -4.3781e+02 - logprior: -2.9971e+00
Epoch 2/2
19/19 - 8s - loss: 412.0409 - loglik: -4.1087e+02 - logprior: -1.1737e+00
Fitted a model with MAP estimate = -405.8716
expansions: []
discards: [ 34  35  36  37  38  39  40  41  42  43  44  66  83  84 112]
Re-initialized the encoder parameters.
Fitting a model of length 162 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 416.6894 - loglik: -4.1389e+02 - logprior: -2.7954e+00
Epoch 2/2
19/19 - 7s - loss: 409.6604 - loglik: -4.0868e+02 - logprior: -9.8057e-01
Fitted a model with MAP estimate = -407.5696
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 162 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 414.7610 - loglik: -4.1205e+02 - logprior: -2.7063e+00
Epoch 2/10
19/19 - 7s - loss: 409.6262 - loglik: -4.0879e+02 - logprior: -8.3640e-01
Epoch 3/10
19/19 - 7s - loss: 405.7274 - loglik: -4.0513e+02 - logprior: -5.9956e-01
Epoch 4/10
19/19 - 7s - loss: 401.4269 - loglik: -4.0089e+02 - logprior: -5.3934e-01
Epoch 5/10
19/19 - 7s - loss: 400.5222 - loglik: -4.0000e+02 - logprior: -5.2293e-01
Epoch 6/10
19/19 - 7s - loss: 397.6555 - loglik: -3.9713e+02 - logprior: -5.3047e-01
Epoch 7/10
19/19 - 7s - loss: 393.8758 - loglik: -3.9335e+02 - logprior: -5.2169e-01
Epoch 8/10
19/19 - 7s - loss: 391.9139 - loglik: -3.9138e+02 - logprior: -5.3428e-01
Epoch 9/10
19/19 - 7s - loss: 393.8634 - loglik: -3.9334e+02 - logprior: -5.2731e-01
Fitted a model with MAP estimate = -392.6864
Time for alignment: 195.9845
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 488.5271 - loglik: -4.8568e+02 - logprior: -2.8422e+00
Epoch 2/10
19/19 - 5s - loss: 446.2242 - loglik: -4.4512e+02 - logprior: -1.1038e+00
Epoch 3/10
19/19 - 5s - loss: 424.8176 - loglik: -4.2352e+02 - logprior: -1.3022e+00
Epoch 4/10
19/19 - 5s - loss: 419.8777 - loglik: -4.1862e+02 - logprior: -1.2591e+00
Epoch 5/10
19/19 - 5s - loss: 416.4499 - loglik: -4.1517e+02 - logprior: -1.2802e+00
Epoch 6/10
19/19 - 5s - loss: 413.2622 - loglik: -4.1196e+02 - logprior: -1.2977e+00
Epoch 7/10
19/19 - 5s - loss: 413.3597 - loglik: -4.1206e+02 - logprior: -1.2956e+00
Fitted a model with MAP estimate = -412.6574
expansions: [(19, 2), (20, 1), (31, 2), (32, 1), (33, 2), (34, 1), (47, 1), (48, 3), (50, 1), (76, 1), (79, 1), (80, 8), (94, 1), (97, 1), (107, 3), (109, 1), (112, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 158 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 437.7664 - loglik: -4.3483e+02 - logprior: -2.9342e+00
Epoch 2/2
19/19 - 7s - loss: 418.0581 - loglik: -4.1703e+02 - logprior: -1.0255e+00
Fitted a model with MAP estimate = -413.9438
expansions: []
discards: [39 56 90 91 92 94]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 421.5231 - loglik: -4.1873e+02 - logprior: -2.7884e+00
Epoch 2/2
19/19 - 6s - loss: 415.6133 - loglik: -4.1463e+02 - logprior: -9.8584e-01
Fitted a model with MAP estimate = -413.0300
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 420.1335 - loglik: -4.1742e+02 - logprior: -2.7127e+00
Epoch 2/10
19/19 - 6s - loss: 413.9076 - loglik: -4.1305e+02 - logprior: -8.6047e-01
Epoch 3/10
19/19 - 6s - loss: 412.1842 - loglik: -4.1155e+02 - logprior: -6.3640e-01
Epoch 4/10
19/19 - 6s - loss: 409.2849 - loglik: -4.0872e+02 - logprior: -5.6793e-01
Epoch 5/10
19/19 - 6s - loss: 404.9473 - loglik: -4.0438e+02 - logprior: -5.6315e-01
Epoch 6/10
19/19 - 6s - loss: 403.6468 - loglik: -4.0310e+02 - logprior: -5.4972e-01
Epoch 7/10
19/19 - 6s - loss: 402.2111 - loglik: -4.0166e+02 - logprior: -5.5550e-01
Epoch 8/10
19/19 - 6s - loss: 399.8186 - loglik: -3.9926e+02 - logprior: -5.5854e-01
Epoch 9/10
19/19 - 6s - loss: 399.6272 - loglik: -3.9908e+02 - logprior: -5.4911e-01
Epoch 10/10
19/19 - 6s - loss: 401.4780 - loglik: -4.0092e+02 - logprior: -5.5757e-01
Fitted a model with MAP estimate = -399.7637
Time for alignment: 174.2235
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 489.1023 - loglik: -4.8627e+02 - logprior: -2.8276e+00
Epoch 2/10
19/19 - 5s - loss: 447.8568 - loglik: -4.4678e+02 - logprior: -1.0742e+00
Epoch 3/10
19/19 - 5s - loss: 424.1419 - loglik: -4.2280e+02 - logprior: -1.3426e+00
Epoch 4/10
19/19 - 5s - loss: 417.8315 - loglik: -4.1650e+02 - logprior: -1.3336e+00
Epoch 5/10
19/19 - 5s - loss: 415.3680 - loglik: -4.1401e+02 - logprior: -1.3604e+00
Epoch 6/10
19/19 - 5s - loss: 411.9230 - loglik: -4.1056e+02 - logprior: -1.3666e+00
Epoch 7/10
19/19 - 5s - loss: 411.3061 - loglik: -4.0993e+02 - logprior: -1.3783e+00
Epoch 8/10
19/19 - 5s - loss: 410.0166 - loglik: -4.0865e+02 - logprior: -1.3628e+00
Epoch 9/10
19/19 - 5s - loss: 410.0883 - loglik: -4.0871e+02 - logprior: -1.3783e+00
Fitted a model with MAP estimate = -410.0039
expansions: [(19, 2), (20, 1), (31, 1), (32, 2), (33, 3), (47, 1), (48, 3), (49, 2), (58, 3), (59, 1), (60, 2), (74, 1), (75, 1), (76, 1), (78, 2), (79, 8), (106, 1), (107, 2), (108, 1), (109, 1), (112, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 439.3190 - loglik: -4.3634e+02 - logprior: -2.9836e+00
Epoch 2/2
19/19 - 7s - loss: 414.2410 - loglik: -4.1313e+02 - logprior: -1.1120e+00
Fitted a model with MAP estimate = -409.1206
expansions: []
discards: [ 35  41  56  73  74  75  77  99 100 101 103]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 417.9981 - loglik: -4.1523e+02 - logprior: -2.7726e+00
Epoch 2/2
19/19 - 7s - loss: 410.7558 - loglik: -4.0981e+02 - logprior: -9.4220e-01
Fitted a model with MAP estimate = -408.4167
expansions: [(69, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 414.9193 - loglik: -4.1225e+02 - logprior: -2.6655e+00
Epoch 2/10
19/19 - 7s - loss: 409.7870 - loglik: -4.0895e+02 - logprior: -8.3373e-01
Epoch 3/10
19/19 - 7s - loss: 406.7374 - loglik: -4.0608e+02 - logprior: -6.6017e-01
Epoch 4/10
19/19 - 7s - loss: 403.1235 - loglik: -4.0247e+02 - logprior: -6.5248e-01
Epoch 5/10
19/19 - 7s - loss: 401.1169 - loglik: -4.0047e+02 - logprior: -6.4850e-01
Epoch 6/10
19/19 - 7s - loss: 397.8251 - loglik: -3.9716e+02 - logprior: -6.6391e-01
Epoch 7/10
19/19 - 7s - loss: 396.6076 - loglik: -3.9594e+02 - logprior: -6.7098e-01
Epoch 8/10
19/19 - 7s - loss: 394.5627 - loglik: -3.9390e+02 - logprior: -6.6524e-01
Epoch 9/10
19/19 - 7s - loss: 394.3531 - loglik: -3.9369e+02 - logprior: -6.6666e-01
Epoch 10/10
19/19 - 7s - loss: 395.8082 - loglik: -3.9515e+02 - logprior: -6.5722e-01
Fitted a model with MAP estimate = -393.6863
Time for alignment: 191.2390
Computed alignments with likelihoods: ['-392.6864', '-399.7637', '-393.6863']
Best model has likelihood: -392.6864
SP score = 0.9322
Training of 3 independent models on file PF00150.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96388ac2b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f961e964cd0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 791.8154 - loglik: -7.9040e+02 - logprior: -1.4155e+00
Epoch 2/10
39/39 - 18s - loss: 717.8431 - loglik: -7.1712e+02 - logprior: -7.2049e-01
Epoch 3/10
39/39 - 18s - loss: 701.6528 - loglik: -7.0085e+02 - logprior: -8.0048e-01
Epoch 4/10
39/39 - 18s - loss: 690.4982 - loglik: -6.8970e+02 - logprior: -7.9909e-01
Epoch 5/10
39/39 - 18s - loss: 683.1281 - loglik: -6.8229e+02 - logprior: -8.3924e-01
Epoch 6/10
39/39 - 18s - loss: 679.9122 - loglik: -6.7904e+02 - logprior: -8.7291e-01
Epoch 7/10
39/39 - 18s - loss: 677.5780 - loglik: -6.7670e+02 - logprior: -8.7694e-01
Epoch 8/10
39/39 - 18s - loss: 675.1929 - loglik: -6.7442e+02 - logprior: -7.7517e-01
Epoch 9/10
39/39 - 18s - loss: 673.8696 - loglik: -6.7311e+02 - logprior: -7.6431e-01
Epoch 10/10
39/39 - 18s - loss: 672.5739 - loglik: -6.7175e+02 - logprior: -8.2187e-01
Fitted a model with MAP estimate = -671.6128
expansions: [(0, 5), (12, 1), (24, 1), (40, 1), (43, 1), (49, 1), (50, 1), (51, 1), (80, 1), (84, 3), (86, 6), (87, 1), (96, 1), (116, 1), (117, 1), (118, 4), (119, 1), (126, 1), (173, 1), (206, 5), (208, 1), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 269 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 797.5138 - loglik: -7.9468e+02 - logprior: -2.8350e+00
Epoch 2/2
39/39 - 22s - loss: 717.6012 - loglik: -7.1645e+02 - logprior: -1.1488e+00
Fitted a model with MAP estimate = -700.8435
expansions: [(0, 6), (1, 4), (2, 1), (48, 1), (50, 2), (51, 4), (103, 2), (104, 2), (106, 1), (139, 2), (140, 2), (185, 1), (186, 2), (190, 3), (208, 1), (210, 4), (211, 1), (212, 1), (242, 1), (243, 1), (244, 5)]
discards: [  5   6   7   8   9  10  11  12  13  14  18  19  20  21  22  23  45  46
  52  86  87  88  89  90  91  92  93  94  95 100 141 142 143 147 234 235
 236 237 238 239 240]
Re-initialized the encoder parameters.
Fitting a model of length 275 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 719.9889 - loglik: -7.1785e+02 - logprior: -2.1408e+00
Epoch 2/2
39/39 - 23s - loss: 700.9036 - loglik: -7.0063e+02 - logprior: -2.7522e-01
Fitted a model with MAP estimate = -694.7325
expansions: [(0, 4), (7, 1), (14, 1), (93, 1), (97, 3), (98, 4), (99, 2)]
discards: [  2  47  48  49  83  89 133 186 187 249 250]
Re-initialized the encoder parameters.
Fitting a model of length 280 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 709.7170 - loglik: -7.0740e+02 - logprior: -2.3220e+00
Epoch 2/10
39/39 - 24s - loss: 696.2134 - loglik: -6.9600e+02 - logprior: -2.1371e-01
Epoch 3/10
39/39 - 24s - loss: 689.3802 - loglik: -6.8945e+02 - logprior: 0.0709
Epoch 4/10
39/39 - 24s - loss: 679.9544 - loglik: -6.8016e+02 - logprior: 0.2083
Epoch 5/10
39/39 - 24s - loss: 668.9042 - loglik: -6.6917e+02 - logprior: 0.2634
Epoch 6/10
39/39 - 24s - loss: 662.3780 - loglik: -6.6266e+02 - logprior: 0.2786
Epoch 7/10
39/39 - 24s - loss: 655.1413 - loglik: -6.5555e+02 - logprior: 0.4066
Epoch 8/10
39/39 - 24s - loss: 651.4239 - loglik: -6.5189e+02 - logprior: 0.4617
Epoch 9/10
39/39 - 24s - loss: 650.0142 - loglik: -6.5055e+02 - logprior: 0.5314
Epoch 10/10
39/39 - 24s - loss: 649.5049 - loglik: -6.5011e+02 - logprior: 0.6010
Fitted a model with MAP estimate = -648.7441
Time for alignment: 635.8009
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 790.6672 - loglik: -7.8925e+02 - logprior: -1.4186e+00
Epoch 2/10
39/39 - 18s - loss: 716.7220 - loglik: -7.1601e+02 - logprior: -7.1444e-01
Epoch 3/10
39/39 - 18s - loss: 702.6266 - loglik: -7.0185e+02 - logprior: -7.7250e-01
Epoch 4/10
39/39 - 18s - loss: 691.2886 - loglik: -6.9055e+02 - logprior: -7.3979e-01
Epoch 5/10
39/39 - 18s - loss: 683.9077 - loglik: -6.8312e+02 - logprior: -7.9267e-01
Epoch 6/10
39/39 - 18s - loss: 680.7076 - loglik: -6.7986e+02 - logprior: -8.4473e-01
Epoch 7/10
39/39 - 18s - loss: 678.3549 - loglik: -6.7749e+02 - logprior: -8.6952e-01
Epoch 8/10
39/39 - 18s - loss: 675.6207 - loglik: -6.7480e+02 - logprior: -8.2490e-01
Epoch 9/10
39/39 - 18s - loss: 674.0400 - loglik: -6.7323e+02 - logprior: -8.1293e-01
Epoch 10/10
39/39 - 18s - loss: 672.9733 - loglik: -6.7217e+02 - logprior: -8.0824e-01
Fitted a model with MAP estimate = -671.7331
expansions: [(0, 5), (9, 1), (24, 1), (42, 2), (43, 1), (51, 1), (59, 1), (62, 1), (90, 8), (91, 1), (92, 1), (97, 1), (117, 1), (118, 1), (121, 1), (205, 5), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 262 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 799.1093 - loglik: -7.9629e+02 - logprior: -2.8230e+00
Epoch 2/2
39/39 - 22s - loss: 717.7019 - loglik: -7.1650e+02 - logprior: -1.2006e+00
Fitted a model with MAP estimate = -701.2051
expansions: [(0, 7), (3, 2), (4, 3), (106, 2), (108, 4), (109, 3), (110, 3), (140, 3), (147, 1), (180, 1), (181, 2), (203, 1), (205, 7), (206, 1), (228, 2), (235, 1), (237, 2), (238, 2)]
discards: [  5   6  15  16  17  18  19  20  21  22  23  24  53  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 111 142 148 199 229
 230 231 232 233]
Re-initialized the encoder parameters.
Fitting a model of length 269 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 719.6043 - loglik: -7.1748e+02 - logprior: -2.1288e+00
Epoch 2/2
39/39 - 22s - loss: 701.1844 - loglik: -7.0094e+02 - logprior: -2.4433e-01
Fitted a model with MAP estimate = -695.0969
expansions: [(0, 5), (15, 1), (85, 1), (96, 5), (100, 2), (196, 1)]
discards: [  4   5   6  17  18  19 178 203 204]
Re-initialized the encoder parameters.
Fitting a model of length 275 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 708.5974 - loglik: -7.0657e+02 - logprior: -2.0282e+00
Epoch 2/10
39/39 - 23s - loss: 696.1267 - loglik: -6.9601e+02 - logprior: -1.1586e-01
Epoch 3/10
39/39 - 23s - loss: 688.7852 - loglik: -6.8889e+02 - logprior: 0.1055
Epoch 4/10
39/39 - 23s - loss: 679.1595 - loglik: -6.7939e+02 - logprior: 0.2335
Epoch 5/10
39/39 - 23s - loss: 668.4979 - loglik: -6.6879e+02 - logprior: 0.2877
Epoch 6/10
39/39 - 23s - loss: 662.9658 - loglik: -6.6331e+02 - logprior: 0.3476
Epoch 7/10
39/39 - 23s - loss: 659.7155 - loglik: -6.6008e+02 - logprior: 0.3622
Epoch 8/10
39/39 - 23s - loss: 654.7788 - loglik: -6.5519e+02 - logprior: 0.4153
Epoch 9/10
39/39 - 23s - loss: 652.6222 - loglik: -6.5317e+02 - logprior: 0.5456
Epoch 10/10
39/39 - 23s - loss: 651.1564 - loglik: -6.5178e+02 - logprior: 0.6239
Fitted a model with MAP estimate = -650.5115
Time for alignment: 622.3448
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 792.4234 - loglik: -7.9101e+02 - logprior: -1.4160e+00
Epoch 2/10
39/39 - 18s - loss: 719.5713 - loglik: -7.1890e+02 - logprior: -6.6990e-01
Epoch 3/10
39/39 - 18s - loss: 703.0746 - loglik: -7.0230e+02 - logprior: -7.7103e-01
Epoch 4/10
39/39 - 18s - loss: 691.6402 - loglik: -6.9092e+02 - logprior: -7.2501e-01
Epoch 5/10
39/39 - 18s - loss: 684.6891 - loglik: -6.8395e+02 - logprior: -7.4388e-01
Epoch 6/10
39/39 - 18s - loss: 681.2552 - loglik: -6.8050e+02 - logprior: -7.5597e-01
Epoch 7/10
39/39 - 18s - loss: 678.6694 - loglik: -6.7787e+02 - logprior: -7.9743e-01
Epoch 8/10
39/39 - 18s - loss: 676.8171 - loglik: -6.7598e+02 - logprior: -8.3321e-01
Epoch 9/10
39/39 - 18s - loss: 675.7514 - loglik: -6.7487e+02 - logprior: -8.7863e-01
Epoch 10/10
39/39 - 18s - loss: 675.2405 - loglik: -6.7435e+02 - logprior: -8.9390e-01
Fitted a model with MAP estimate = -674.1381
expansions: [(0, 4), (23, 1), (43, 2), (51, 1), (52, 3), (80, 1), (85, 1), (95, 1), (98, 1), (102, 1), (116, 2), (117, 1), (120, 1), (123, 1), (134, 2), (158, 4), (175, 9), (206, 6), (208, 1), (214, 1)]
discards: [ 91 139]
Re-initialized the encoder parameters.
Fitting a model of length 271 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 783.0060 - loglik: -7.8013e+02 - logprior: -2.8810e+00
Epoch 2/2
39/39 - 23s - loss: 713.3796 - loglik: -7.1218e+02 - logprior: -1.2037e+00
Fitted a model with MAP estimate = -701.8759
expansions: [(0, 4), (96, 2), (97, 3), (99, 2), (101, 3), (102, 3), (131, 2), (132, 2), (134, 2), (174, 2), (235, 2), (236, 1), (237, 7), (238, 2)]
discards: [  2   3  48  49  60  88  89 135 136 137 155 178 182 195 196 197 198 199
 209 210 211 212 239 240 241 242 243 244 246 247 248 249 250]
Re-initialized the encoder parameters.
Fitting a model of length 275 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 714.2268 - loglik: -7.1181e+02 - logprior: -2.4177e+00
Epoch 2/2
39/39 - 23s - loss: 698.6462 - loglik: -6.9824e+02 - logprior: -4.0769e-01
Fitted a model with MAP estimate = -693.0704
expansions: [(50, 1), (94, 1), (189, 2), (210, 2), (211, 6)]
discards: [  1   2   3   4  82  83  84  85  86 148 191 192 240 253]
Re-initialized the encoder parameters.
Fitting a model of length 273 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 708.6111 - loglik: -7.0745e+02 - logprior: -1.1607e+00
Epoch 2/10
39/39 - 23s - loss: 698.1393 - loglik: -6.9810e+02 - logprior: -4.0852e-02
Epoch 3/10
39/39 - 23s - loss: 691.8239 - loglik: -6.9192e+02 - logprior: 0.0976
Epoch 4/10
39/39 - 23s - loss: 682.4070 - loglik: -6.8262e+02 - logprior: 0.2173
Epoch 5/10
39/39 - 23s - loss: 671.3519 - loglik: -6.7162e+02 - logprior: 0.2685
Epoch 6/10
39/39 - 23s - loss: 665.1368 - loglik: -6.6543e+02 - logprior: 0.2963
Epoch 7/10
39/39 - 23s - loss: 663.4542 - loglik: -6.6379e+02 - logprior: 0.3359
Epoch 8/10
39/39 - 23s - loss: 660.8209 - loglik: -6.6120e+02 - logprior: 0.3811
Epoch 9/10
39/39 - 23s - loss: 659.0084 - loglik: -6.5943e+02 - logprior: 0.4259
Epoch 10/10
39/39 - 23s - loss: 656.9019 - loglik: -6.5739e+02 - logprior: 0.4861
Fitted a model with MAP estimate = -656.1682
Time for alignment: 627.4278
Computed alignments with likelihoods: ['-648.7441', '-650.5115', '-656.1682']
Best model has likelihood: -648.7441
SP score = 0.3457
Training of 3 independent models on file PF13561.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95d97006a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f966bdcd670>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 189 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 622.9242 - loglik: -6.2030e+02 - logprior: -2.6210e+00
Epoch 2/10
19/19 - 8s - loss: 523.5217 - loglik: -5.2246e+02 - logprior: -1.0635e+00
Epoch 3/10
19/19 - 8s - loss: 478.4039 - loglik: -4.7680e+02 - logprior: -1.6007e+00
Epoch 4/10
19/19 - 8s - loss: 469.8228 - loglik: -4.6813e+02 - logprior: -1.6893e+00
Epoch 5/10
19/19 - 8s - loss: 464.9608 - loglik: -4.6326e+02 - logprior: -1.7032e+00
Epoch 6/10
19/19 - 8s - loss: 463.3854 - loglik: -4.6173e+02 - logprior: -1.6558e+00
Epoch 7/10
19/19 - 8s - loss: 462.2530 - loglik: -4.6062e+02 - logprior: -1.6323e+00
Epoch 8/10
19/19 - 8s - loss: 461.5228 - loglik: -4.5989e+02 - logprior: -1.6296e+00
Epoch 9/10
19/19 - 8s - loss: 461.2227 - loglik: -4.5960e+02 - logprior: -1.6223e+00
Epoch 10/10
19/19 - 8s - loss: 460.8546 - loglik: -4.5923e+02 - logprior: -1.6226e+00
Fitted a model with MAP estimate = -458.2192
expansions: [(12, 1), (15, 1), (18, 1), (23, 2), (24, 1), (27, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (48, 1), (50, 1), (66, 2), (67, 2), (71, 1), (77, 1), (87, 1), (88, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (111, 2), (112, 1), (126, 1), (128, 1), (149, 2), (150, 1), (151, 3), (153, 1), (154, 1), (155, 2), (156, 4), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 245 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 484.2553 - loglik: -4.8076e+02 - logprior: -3.4951e+00
Epoch 2/2
19/19 - 11s - loss: 454.7467 - loglik: -4.5298e+02 - logprior: -1.7632e+00
Fitted a model with MAP estimate = -447.6455
expansions: [(3, 1)]
discards: [  0  26  85  86 140 143 196 201 203]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 459.3926 - loglik: -4.5607e+02 - logprior: -3.3226e+00
Epoch 2/2
19/19 - 11s - loss: 451.1403 - loglik: -4.5013e+02 - logprior: -1.0076e+00
Fitted a model with MAP estimate = -445.6863
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10077 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 455.9195 - loglik: -4.5259e+02 - logprior: -3.3303e+00
Epoch 2/10
19/19 - 11s - loss: 448.2196 - loglik: -4.4675e+02 - logprior: -1.4682e+00
Epoch 3/10
19/19 - 11s - loss: 445.1194 - loglik: -4.4390e+02 - logprior: -1.2160e+00
Epoch 4/10
19/19 - 11s - loss: 441.7241 - loglik: -4.4122e+02 - logprior: -5.0471e-01
Epoch 5/10
19/19 - 11s - loss: 437.7269 - loglik: -4.3742e+02 - logprior: -3.0302e-01
Epoch 6/10
19/19 - 11s - loss: 435.8848 - loglik: -4.3561e+02 - logprior: -2.7456e-01
Epoch 7/10
19/19 - 11s - loss: 434.9753 - loglik: -4.3472e+02 - logprior: -2.5126e-01
Epoch 8/10
19/19 - 11s - loss: 432.9838 - loglik: -4.3276e+02 - logprior: -2.1919e-01
Epoch 9/10
19/19 - 11s - loss: 433.2887 - loglik: -4.3311e+02 - logprior: -1.7977e-01
Fitted a model with MAP estimate = -432.8965
Time for alignment: 309.1195
Fitting a model of length 189 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 623.1498 - loglik: -6.2053e+02 - logprior: -2.6191e+00
Epoch 2/10
19/19 - 8s - loss: 523.8531 - loglik: -5.2279e+02 - logprior: -1.0648e+00
Epoch 3/10
19/19 - 8s - loss: 478.0535 - loglik: -4.7646e+02 - logprior: -1.5971e+00
Epoch 4/10
19/19 - 8s - loss: 467.8686 - loglik: -4.6617e+02 - logprior: -1.6961e+00
Epoch 5/10
19/19 - 8s - loss: 464.8347 - loglik: -4.6314e+02 - logprior: -1.6986e+00
Epoch 6/10
19/19 - 8s - loss: 461.9310 - loglik: -4.6029e+02 - logprior: -1.6455e+00
Epoch 7/10
19/19 - 8s - loss: 461.7283 - loglik: -4.6009e+02 - logprior: -1.6347e+00
Epoch 8/10
19/19 - 8s - loss: 460.6582 - loglik: -4.5904e+02 - logprior: -1.6195e+00
Epoch 9/10
19/19 - 8s - loss: 460.1728 - loglik: -4.5855e+02 - logprior: -1.6184e+00
Epoch 10/10
19/19 - 8s - loss: 460.9376 - loglik: -4.5932e+02 - logprior: -1.6166e+00
Fitted a model with MAP estimate = -457.4900
expansions: [(12, 1), (15, 1), (22, 1), (25, 1), (28, 1), (29, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (49, 1), (66, 1), (67, 2), (70, 1), (71, 1), (77, 1), (83, 1), (88, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (133, 1), (149, 2), (150, 1), (151, 3), (153, 1), (154, 1), (155, 2), (156, 4), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 483.5423 - loglik: -4.8005e+02 - logprior: -3.4903e+00
Epoch 2/2
19/19 - 11s - loss: 455.0038 - loglik: -4.5323e+02 - logprior: -1.7714e+00
Fitted a model with MAP estimate = -447.6987
expansions: [(3, 1)]
discards: [  0  82 138 141 194 199 201]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 458.9033 - loglik: -4.5560e+02 - logprior: -3.3049e+00
Epoch 2/2
19/19 - 11s - loss: 451.3744 - loglik: -4.5044e+02 - logprior: -9.3658e-01
Fitted a model with MAP estimate = -445.6339
expansions: [(3, 1)]
discards: [  0 183]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10077 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 455.8676 - loglik: -4.5255e+02 - logprior: -3.3141e+00
Epoch 2/10
19/19 - 11s - loss: 448.7701 - loglik: -4.4729e+02 - logprior: -1.4793e+00
Epoch 3/10
19/19 - 11s - loss: 445.0782 - loglik: -4.4383e+02 - logprior: -1.2509e+00
Epoch 4/10
19/19 - 11s - loss: 440.9330 - loglik: -4.4039e+02 - logprior: -5.4472e-01
Epoch 5/10
19/19 - 11s - loss: 438.2523 - loglik: -4.3795e+02 - logprior: -3.0304e-01
Epoch 6/10
19/19 - 11s - loss: 436.1370 - loglik: -4.3586e+02 - logprior: -2.8088e-01
Epoch 7/10
19/19 - 11s - loss: 434.0941 - loglik: -4.3384e+02 - logprior: -2.5406e-01
Epoch 8/10
19/19 - 11s - loss: 433.7951 - loglik: -4.3356e+02 - logprior: -2.3034e-01
Epoch 9/10
19/19 - 11s - loss: 433.3368 - loglik: -4.3315e+02 - logprior: -1.8993e-01
Epoch 10/10
19/19 - 11s - loss: 433.6551 - loglik: -4.3349e+02 - logprior: -1.6246e-01
Fitted a model with MAP estimate = -432.7863
Time for alignment: 316.5377
Fitting a model of length 189 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 622.7849 - loglik: -6.2017e+02 - logprior: -2.6189e+00
Epoch 2/10
19/19 - 8s - loss: 524.2101 - loglik: -5.2314e+02 - logprior: -1.0685e+00
Epoch 3/10
19/19 - 8s - loss: 479.2336 - loglik: -4.7761e+02 - logprior: -1.6198e+00
Epoch 4/10
19/19 - 8s - loss: 468.3900 - loglik: -4.6666e+02 - logprior: -1.7285e+00
Epoch 5/10
19/19 - 8s - loss: 464.7583 - loglik: -4.6304e+02 - logprior: -1.7226e+00
Epoch 6/10
19/19 - 8s - loss: 462.5442 - loglik: -4.6089e+02 - logprior: -1.6498e+00
Epoch 7/10
19/19 - 8s - loss: 461.5863 - loglik: -4.5996e+02 - logprior: -1.6222e+00
Epoch 8/10
19/19 - 8s - loss: 461.1477 - loglik: -4.5954e+02 - logprior: -1.6036e+00
Epoch 9/10
19/19 - 8s - loss: 461.0708 - loglik: -4.5947e+02 - logprior: -1.5963e+00
Epoch 10/10
19/19 - 8s - loss: 460.2674 - loglik: -4.5867e+02 - logprior: -1.5969e+00
Fitted a model with MAP estimate = -457.6861
expansions: [(8, 1), (11, 1), (18, 1), (23, 2), (24, 1), (31, 1), (33, 3), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (48, 1), (50, 1), (66, 2), (67, 2), (69, 1), (72, 1), (83, 1), (86, 1), (87, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (128, 1), (149, 2), (150, 1), (151, 4), (153, 1), (154, 1), (155, 2), (156, 2), (157, 2), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 246 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 484.6554 - loglik: -4.8114e+02 - logprior: -3.5193e+00
Epoch 2/2
19/19 - 12s - loss: 454.5179 - loglik: -4.5274e+02 - logprior: -1.7811e+00
Fitted a model with MAP estimate = -447.6596
expansions: [(3, 1)]
discards: [  0  26  84  86 140 143 190 191 192 197 202 204]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 459.6439 - loglik: -4.5632e+02 - logprior: -3.3273e+00
Epoch 2/2
19/19 - 11s - loss: 451.1312 - loglik: -4.5018e+02 - logprior: -9.4873e-01
Fitted a model with MAP estimate = -445.8251
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10077 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 456.1972 - loglik: -4.5288e+02 - logprior: -3.3129e+00
Epoch 2/10
19/19 - 11s - loss: 448.6053 - loglik: -4.4713e+02 - logprior: -1.4723e+00
Epoch 3/10
19/19 - 11s - loss: 445.4000 - loglik: -4.4416e+02 - logprior: -1.2409e+00
Epoch 4/10
19/19 - 11s - loss: 441.3100 - loglik: -4.4076e+02 - logprior: -5.5120e-01
Epoch 5/10
19/19 - 11s - loss: 438.5416 - loglik: -4.3824e+02 - logprior: -2.9688e-01
Epoch 6/10
19/19 - 11s - loss: 436.1577 - loglik: -4.3589e+02 - logprior: -2.6860e-01
Epoch 7/10
19/19 - 11s - loss: 434.8092 - loglik: -4.3456e+02 - logprior: -2.4621e-01
Epoch 8/10
19/19 - 11s - loss: 433.5455 - loglik: -4.3332e+02 - logprior: -2.2122e-01
Epoch 9/10
19/19 - 11s - loss: 434.2646 - loglik: -4.3408e+02 - logprior: -1.8684e-01
Fitted a model with MAP estimate = -433.2627
Time for alignment: 306.5109
Computed alignments with likelihoods: ['-432.8965', '-432.7863', '-433.2627']
Best model has likelihood: -432.7863
SP score = 0.7458
Training of 3 independent models on file PF05746.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f966bb94940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96165fe8b0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : True
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 320.9704 - loglik: -3.1789e+02 - logprior: -3.0810e+00
Epoch 2/10
19/19 - 2s - loss: 277.1284 - loglik: -2.7577e+02 - logprior: -1.3574e+00
Epoch 3/10
19/19 - 2s - loss: 254.2170 - loglik: -2.5251e+02 - logprior: -1.7099e+00
Epoch 4/10
19/19 - 2s - loss: 247.9548 - loglik: -2.4634e+02 - logprior: -1.6136e+00
Epoch 5/10
19/19 - 2s - loss: 244.7805 - loglik: -2.4318e+02 - logprior: -1.6028e+00
Epoch 6/10
19/19 - 2s - loss: 243.4170 - loglik: -2.4183e+02 - logprior: -1.5837e+00
Epoch 7/10
19/19 - 2s - loss: 242.7449 - loglik: -2.4119e+02 - logprior: -1.5590e+00
Epoch 8/10
19/19 - 2s - loss: 242.3228 - loglik: -2.4077e+02 - logprior: -1.5487e+00
Epoch 9/10
19/19 - 2s - loss: 241.8271 - loglik: -2.4027e+02 - logprior: -1.5585e+00
Epoch 10/10
19/19 - 2s - loss: 241.7907 - loglik: -2.4023e+02 - logprior: -1.5600e+00
Fitted a model with MAP estimate = -241.4623
expansions: [(17, 2), (20, 4), (23, 2), (27, 1), (28, 2), (32, 2), (40, 1), (43, 1), (44, 1), (48, 1), (56, 1), (59, 2), (62, 1), (63, 1), (69, 2), (71, 1), (76, 1), (77, 1), (79, 1), (85, 1), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 252.4591 - loglik: -2.4851e+02 - logprior: -3.9456e+00
Epoch 2/2
19/19 - 3s - loss: 237.4665 - loglik: -2.3543e+02 - logprior: -2.0319e+00
Fitted a model with MAP estimate = -234.8116
expansions: [(0, 2)]
discards: [ 0 37 42 76 90]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 237.1486 - loglik: -2.3430e+02 - logprior: -2.8501e+00
Epoch 2/2
19/19 - 3s - loss: 232.6705 - loglik: -2.3167e+02 - logprior: -9.9780e-01
Fitted a model with MAP estimate = -231.2858
expansions: []
discards: [ 0 16 17 18]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 240.7876 - loglik: -2.3714e+02 - logprior: -3.6485e+00
Epoch 2/10
19/19 - 3s - loss: 235.1113 - loglik: -2.3392e+02 - logprior: -1.1868e+00
Epoch 3/10
19/19 - 3s - loss: 232.2974 - loglik: -2.3130e+02 - logprior: -9.9801e-01
Epoch 4/10
19/19 - 3s - loss: 230.8187 - loglik: -2.2985e+02 - logprior: -9.6584e-01
Epoch 5/10
19/19 - 3s - loss: 229.9631 - loglik: -2.2903e+02 - logprior: -9.3391e-01
Epoch 6/10
19/19 - 3s - loss: 229.1147 - loglik: -2.2820e+02 - logprior: -9.1817e-01
Epoch 7/10
19/19 - 3s - loss: 229.1932 - loglik: -2.2830e+02 - logprior: -8.9019e-01
Fitted a model with MAP estimate = -228.6490
Time for alignment: 85.3388
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 320.8554 - loglik: -3.1777e+02 - logprior: -3.0837e+00
Epoch 2/10
19/19 - 2s - loss: 277.3000 - loglik: -2.7596e+02 - logprior: -1.3411e+00
Epoch 3/10
19/19 - 2s - loss: 252.8611 - loglik: -2.5116e+02 - logprior: -1.7057e+00
Epoch 4/10
19/19 - 2s - loss: 244.7895 - loglik: -2.4317e+02 - logprior: -1.6175e+00
Epoch 5/10
19/19 - 2s - loss: 242.6342 - loglik: -2.4101e+02 - logprior: -1.6255e+00
Epoch 6/10
19/19 - 2s - loss: 241.6289 - loglik: -2.4005e+02 - logprior: -1.5821e+00
Epoch 7/10
19/19 - 2s - loss: 241.7755 - loglik: -2.4021e+02 - logprior: -1.5666e+00
Fitted a model with MAP estimate = -241.1067
expansions: [(17, 1), (18, 3), (19, 3), (22, 1), (24, 1), (28, 2), (32, 1), (34, 1), (40, 1), (48, 2), (49, 2), (55, 1), (62, 1), (63, 1), (68, 2), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 250.1448 - loglik: -2.4619e+02 - logprior: -3.9567e+00
Epoch 2/2
19/19 - 3s - loss: 237.3003 - loglik: -2.3529e+02 - logprior: -2.0054e+00
Fitted a model with MAP estimate = -234.6751
expansions: [(0, 2)]
discards: [ 0 15 16 37 61 89]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 238.3113 - loglik: -2.3545e+02 - logprior: -2.8579e+00
Epoch 2/2
19/19 - 3s - loss: 233.4523 - loglik: -2.3243e+02 - logprior: -1.0239e+00
Fitted a model with MAP estimate = -232.2272
expansions: []
discards: [ 0 21]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 240.6158 - loglik: -2.3695e+02 - logprior: -3.6611e+00
Epoch 2/10
19/19 - 3s - loss: 235.1939 - loglik: -2.3399e+02 - logprior: -1.2077e+00
Epoch 3/10
19/19 - 3s - loss: 232.0978 - loglik: -2.3107e+02 - logprior: -1.0238e+00
Epoch 4/10
19/19 - 3s - loss: 231.1756 - loglik: -2.3018e+02 - logprior: -9.9264e-01
Epoch 5/10
19/19 - 3s - loss: 229.7818 - loglik: -2.2882e+02 - logprior: -9.6093e-01
Epoch 6/10
19/19 - 3s - loss: 229.3346 - loglik: -2.2840e+02 - logprior: -9.3714e-01
Epoch 7/10
19/19 - 3s - loss: 228.8756 - loglik: -2.2797e+02 - logprior: -9.0933e-01
Epoch 8/10
19/19 - 3s - loss: 228.5957 - loglik: -2.2771e+02 - logprior: -8.8780e-01
Epoch 9/10
19/19 - 3s - loss: 228.2603 - loglik: -2.2740e+02 - logprior: -8.6484e-01
Epoch 10/10
19/19 - 3s - loss: 228.7507 - loglik: -2.2791e+02 - logprior: -8.4533e-01
Fitted a model with MAP estimate = -228.3235
Time for alignment: 85.0254
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 320.5014 - loglik: -3.1742e+02 - logprior: -3.0839e+00
Epoch 2/10
19/19 - 2s - loss: 275.6028 - loglik: -2.7424e+02 - logprior: -1.3675e+00
Epoch 3/10
19/19 - 2s - loss: 250.9061 - loglik: -2.4921e+02 - logprior: -1.6963e+00
Epoch 4/10
19/19 - 2s - loss: 243.6562 - loglik: -2.4201e+02 - logprior: -1.6501e+00
Epoch 5/10
19/19 - 2s - loss: 241.8557 - loglik: -2.4020e+02 - logprior: -1.6575e+00
Epoch 6/10
19/19 - 2s - loss: 241.1707 - loglik: -2.3955e+02 - logprior: -1.6219e+00
Epoch 7/10
19/19 - 2s - loss: 240.7321 - loglik: -2.3914e+02 - logprior: -1.5945e+00
Epoch 8/10
19/19 - 2s - loss: 240.0865 - loglik: -2.3850e+02 - logprior: -1.5880e+00
Epoch 9/10
19/19 - 2s - loss: 240.6700 - loglik: -2.3909e+02 - logprior: -1.5813e+00
Fitted a model with MAP estimate = -239.8316
expansions: [(17, 1), (18, 3), (19, 2), (20, 2), (27, 1), (28, 2), (32, 2), (34, 1), (40, 1), (43, 1), (47, 1), (49, 1), (56, 1), (62, 1), (63, 1), (69, 2), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 250.4204 - loglik: -2.4647e+02 - logprior: -3.9530e+00
Epoch 2/2
19/19 - 3s - loss: 237.3772 - loglik: -2.3534e+02 - logprior: -2.0412e+00
Fitted a model with MAP estimate = -234.7603
expansions: [(0, 2)]
discards: [ 0 15 16 37 42 89]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 238.2359 - loglik: -2.3538e+02 - logprior: -2.8565e+00
Epoch 2/2
19/19 - 3s - loss: 233.6179 - loglik: -2.3261e+02 - logprior: -1.0117e+00
Fitted a model with MAP estimate = -232.1509
expansions: []
discards: [ 0 22]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 240.5978 - loglik: -2.3693e+02 - logprior: -3.6698e+00
Epoch 2/10
19/19 - 3s - loss: 235.3230 - loglik: -2.3412e+02 - logprior: -1.2012e+00
Epoch 3/10
19/19 - 3s - loss: 232.1952 - loglik: -2.3117e+02 - logprior: -1.0216e+00
Epoch 4/10
19/19 - 3s - loss: 230.7965 - loglik: -2.2982e+02 - logprior: -9.7954e-01
Epoch 5/10
19/19 - 3s - loss: 229.8535 - loglik: -2.2891e+02 - logprior: -9.4574e-01
Epoch 6/10
19/19 - 3s - loss: 229.3682 - loglik: -2.2845e+02 - logprior: -9.1865e-01
Epoch 7/10
19/19 - 3s - loss: 228.7790 - loglik: -2.2789e+02 - logprior: -8.9126e-01
Epoch 8/10
19/19 - 3s - loss: 228.8907 - loglik: -2.2801e+02 - logprior: -8.7795e-01
Fitted a model with MAP estimate = -228.4469
Time for alignment: 85.3597
Computed alignments with likelihoods: ['-228.6490', '-228.3235', '-228.4469']
Best model has likelihood: -228.3235
SP score = 0.8444
