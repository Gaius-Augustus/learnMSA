Training of 5 independent models on file cys.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5cb06ea0a0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5cb06ea460>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5cda9057c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5cda905fa0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5cda905c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5cda905df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5cda905550>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5cda9054f0>, <__main__.SimpleDirichletPrior object at 0x7f5cc0debaf0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 1131.8671 - loglik: -1.1256e+03 - logprior: -6.2741e+00
Epoch 2/10
25/25 - 7s - loss: 1018.5831 - loglik: -1.0168e+03 - logprior: -1.8091e+00
Epoch 3/10
25/25 - 7s - loss: 998.5023 - loglik: -9.9659e+02 - logprior: -1.9108e+00
Epoch 4/10
25/25 - 7s - loss: 994.9824 - loglik: -9.9312e+02 - logprior: -1.8605e+00
Epoch 5/10
25/25 - 7s - loss: 995.6771 - loglik: -9.9368e+02 - logprior: -1.9980e+00
Fitted a model with MAP estimate = -993.5658
expansions: [(9, 3), (10, 1), (12, 1), (13, 1), (16, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (47, 3), (58, 1), (60, 1), (62, 2), (76, 1), (81, 2), (82, 2), (83, 2), (90, 1), (93, 1), (96, 2), (102, 1), (113, 2), (115, 1), (122, 1), (124, 1), (126, 3), (137, 1), (138, 2), (139, 1), (160, 1), (162, 2), (168, 2), (169, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 220 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 994.1127 - loglik: -9.8687e+02 - logprior: -7.2444e+00
Epoch 2/2
25/25 - 9s - loss: 978.3118 - loglik: -9.7593e+02 - logprior: -2.3768e+00
Fitted a model with MAP estimate = -975.9758
expansions: [(0, 3)]
discards: [  0   9  60  61  80 101 106 143 177 206]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 985.0546 - loglik: -9.8074e+02 - logprior: -4.3115e+00
Epoch 2/2
25/25 - 8s - loss: 977.2106 - loglik: -9.7682e+02 - logprior: -3.9510e-01
Fitted a model with MAP estimate = -975.5755
expansions: [(60, 2)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 12s - loss: 986.7938 - loglik: -9.8050e+02 - logprior: -6.2940e+00
Epoch 2/10
25/25 - 8s - loss: 977.1965 - loglik: -9.7597e+02 - logprior: -1.2294e+00
Epoch 3/10
25/25 - 8s - loss: 972.6085 - loglik: -9.7304e+02 - logprior: 0.4277
Epoch 4/10
25/25 - 8s - loss: 974.0718 - loglik: -9.7456e+02 - logprior: 0.4888
Fitted a model with MAP estimate = -971.9992
Time for alignment: 148.2076
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 1129.8235 - loglik: -1.1236e+03 - logprior: -6.2663e+00
Epoch 2/10
25/25 - 7s - loss: 1022.2386 - loglik: -1.0204e+03 - logprior: -1.8723e+00
Epoch 3/10
25/25 - 7s - loss: 1001.7796 - loglik: -9.9984e+02 - logprior: -1.9371e+00
Epoch 4/10
25/25 - 7s - loss: 996.2139 - loglik: -9.9436e+02 - logprior: -1.8572e+00
Epoch 5/10
25/25 - 7s - loss: 995.5856 - loglik: -9.9360e+02 - logprior: -1.9850e+00
Epoch 6/10
25/25 - 7s - loss: 995.5466 - loglik: -9.9343e+02 - logprior: -2.1124e+00
Epoch 7/10
25/25 - 7s - loss: 997.2150 - loglik: -9.9503e+02 - logprior: -2.1831e+00
Fitted a model with MAP estimate = -994.3175
expansions: [(9, 3), (10, 1), (12, 1), (13, 1), (31, 1), (32, 2), (33, 2), (35, 2), (36, 1), (47, 2), (57, 1), (58, 1), (60, 1), (62, 2), (76, 1), (80, 1), (81, 1), (82, 1), (83, 2), (90, 1), (93, 2), (96, 1), (98, 1), (101, 1), (112, 2), (124, 2), (126, 3), (137, 1), (138, 2), (139, 1), (150, 1), (159, 1), (167, 1), (168, 1), (169, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 220 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 995.2572 - loglik: -9.8795e+02 - logprior: -7.3024e+00
Epoch 2/2
25/25 - 9s - loss: 978.4791 - loglik: -9.7594e+02 - logprior: -2.5425e+00
Fitted a model with MAP estimate = -976.4770
expansions: [(0, 3)]
discards: [  0   9  42  46  62  82 107 144 178]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 985.0666 - loglik: -9.8073e+02 - logprior: -4.3414e+00
Epoch 2/2
25/25 - 9s - loss: 975.3189 - loglik: -9.7492e+02 - logprior: -4.0128e-01
Fitted a model with MAP estimate = -975.0392
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 12s - loss: 982.8262 - loglik: -9.7656e+02 - logprior: -6.2633e+00
Epoch 2/10
25/25 - 8s - loss: 981.2176 - loglik: -9.8003e+02 - logprior: -1.1860e+00
Epoch 3/10
25/25 - 8s - loss: 973.5075 - loglik: -9.7391e+02 - logprior: 0.4060
Epoch 4/10
25/25 - 8s - loss: 974.7912 - loglik: -9.7527e+02 - logprior: 0.4818
Fitted a model with MAP estimate = -972.2332
Time for alignment: 163.6171
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 1128.1814 - loglik: -1.1219e+03 - logprior: -6.2368e+00
Epoch 2/10
25/25 - 7s - loss: 1023.7463 - loglik: -1.0219e+03 - logprior: -1.8076e+00
Epoch 3/10
25/25 - 7s - loss: 1000.2925 - loglik: -9.9835e+02 - logprior: -1.9401e+00
Epoch 4/10
25/25 - 7s - loss: 998.2090 - loglik: -9.9637e+02 - logprior: -1.8379e+00
Epoch 5/10
25/25 - 7s - loss: 993.5939 - loglik: -9.9163e+02 - logprior: -1.9686e+00
Epoch 6/10
25/25 - 7s - loss: 998.0583 - loglik: -9.9603e+02 - logprior: -2.0279e+00
Fitted a model with MAP estimate = -995.0865
expansions: [(9, 3), (10, 1), (12, 1), (13, 1), (31, 1), (32, 2), (33, 2), (35, 2), (36, 1), (47, 3), (58, 1), (60, 1), (62, 2), (74, 1), (80, 1), (81, 1), (82, 1), (83, 2), (90, 2), (92, 1), (95, 1), (98, 1), (101, 1), (112, 2), (114, 1), (121, 1), (123, 1), (125, 3), (136, 1), (139, 2), (159, 1), (163, 1), (167, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 219 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 994.7308 - loglik: -9.8749e+02 - logprior: -7.2441e+00
Epoch 2/2
25/25 - 9s - loss: 978.5174 - loglik: -9.7612e+02 - logprior: -2.3940e+00
Fitted a model with MAP estimate = -976.4042
expansions: [(0, 3)]
discards: [  0   9  42  62  63  82 144]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 982.5477 - loglik: -9.7821e+02 - logprior: -4.3334e+00
Epoch 2/2
25/25 - 8s - loss: 978.9138 - loglik: -9.7850e+02 - logprior: -4.1849e-01
Fitted a model with MAP estimate = -975.1726
expansions: [(61, 2)]
discards: [  0   2  45 105 158]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 988.2391 - loglik: -9.8197e+02 - logprior: -6.2736e+00
Epoch 2/10
25/25 - 8s - loss: 975.6675 - loglik: -9.7446e+02 - logprior: -1.2063e+00
Epoch 3/10
25/25 - 8s - loss: 974.3496 - loglik: -9.7478e+02 - logprior: 0.4305
Epoch 4/10
25/25 - 8s - loss: 973.8315 - loglik: -9.7434e+02 - logprior: 0.5043
Epoch 5/10
25/25 - 8s - loss: 976.6728 - loglik: -9.7714e+02 - logprior: 0.4643
Fitted a model with MAP estimate = -971.3995
Time for alignment: 164.7780
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 1130.3351 - loglik: -1.1241e+03 - logprior: -6.2353e+00
Epoch 2/10
25/25 - 7s - loss: 1018.8072 - loglik: -1.0170e+03 - logprior: -1.7598e+00
Epoch 3/10
25/25 - 7s - loss: 999.1329 - loglik: -9.9730e+02 - logprior: -1.8291e+00
Epoch 4/10
25/25 - 7s - loss: 998.6212 - loglik: -9.9687e+02 - logprior: -1.7516e+00
Epoch 5/10
25/25 - 7s - loss: 994.7456 - loglik: -9.9291e+02 - logprior: -1.8390e+00
Epoch 6/10
25/25 - 7s - loss: 999.5895 - loglik: -9.9768e+02 - logprior: -1.9142e+00
Fitted a model with MAP estimate = -995.7897
expansions: [(9, 3), (10, 1), (11, 2), (31, 3), (32, 2), (34, 2), (47, 3), (56, 1), (57, 1), (59, 1), (61, 2), (75, 1), (80, 1), (82, 1), (83, 2), (90, 2), (92, 1), (95, 1), (98, 1), (101, 1), (112, 2), (114, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (150, 1), (159, 1), (167, 1), (168, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 219 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 992.8480 - loglik: -9.8556e+02 - logprior: -7.2833e+00
Epoch 2/2
25/25 - 9s - loss: 981.6379 - loglik: -9.7917e+02 - logprior: -2.4656e+00
Fitted a model with MAP estimate = -976.0551
expansions: [(0, 3)]
discards: [  0   9  44  61  81 143 177]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 982.1161 - loglik: -9.7778e+02 - logprior: -4.3386e+00
Epoch 2/2
25/25 - 8s - loss: 978.2316 - loglik: -9.7781e+02 - logprior: -4.2137e-01
Fitted a model with MAP estimate = -974.7789
expansions: []
discards: [  0   2 105]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 983.9962 - loglik: -9.7772e+02 - logprior: -6.2773e+00
Epoch 2/10
25/25 - 8s - loss: 980.3975 - loglik: -9.7919e+02 - logprior: -1.2107e+00
Epoch 3/10
25/25 - 8s - loss: 974.0619 - loglik: -9.7449e+02 - logprior: 0.4272
Epoch 4/10
25/25 - 8s - loss: 973.8344 - loglik: -9.7430e+02 - logprior: 0.4705
Epoch 5/10
25/25 - 8s - loss: 970.3458 - loglik: -9.7080e+02 - logprior: 0.4517
Epoch 6/10
25/25 - 8s - loss: 974.7261 - loglik: -9.7519e+02 - logprior: 0.4613
Fitted a model with MAP estimate = -970.8819
Time for alignment: 173.6826
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 1128.5940 - loglik: -1.1223e+03 - logprior: -6.2656e+00
Epoch 2/10
25/25 - 7s - loss: 1020.8181 - loglik: -1.0188e+03 - logprior: -2.0311e+00
Epoch 3/10
25/25 - 7s - loss: 995.8153 - loglik: -9.9368e+02 - logprior: -2.1361e+00
Epoch 4/10
25/25 - 7s - loss: 995.8697 - loglik: -9.9384e+02 - logprior: -2.0290e+00
Fitted a model with MAP estimate = -993.5746
expansions: [(9, 3), (10, 1), (12, 1), (13, 1), (16, 1), (31, 1), (32, 1), (33, 1), (35, 2), (37, 1), (48, 1), (50, 1), (58, 1), (60, 1), (62, 2), (76, 1), (81, 2), (82, 1), (83, 1), (86, 1), (90, 2), (92, 1), (95, 1), (98, 1), (101, 1), (112, 2), (114, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (149, 1), (159, 1), (161, 2), (166, 1), (167, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 219 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 994.1102 - loglik: -9.8687e+02 - logprior: -7.2404e+00
Epoch 2/2
25/25 - 9s - loss: 979.7901 - loglik: -9.7748e+02 - logprior: -2.3132e+00
Fitted a model with MAP estimate = -975.7358
expansions: [(0, 2)]
discards: [  0   9  44  80 101 142 176 205]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 983.1808 - loglik: -9.7888e+02 - logprior: -4.3058e+00
Epoch 2/2
25/25 - 8s - loss: 976.5031 - loglik: -9.7611e+02 - logprior: -3.9333e-01
Fitted a model with MAP estimate = -975.0164
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 984.3209 - loglik: -9.7803e+02 - logprior: -6.2935e+00
Epoch 2/10
25/25 - 8s - loss: 980.0477 - loglik: -9.7908e+02 - logprior: -9.7023e-01
Epoch 3/10
25/25 - 8s - loss: 976.1694 - loglik: -9.7655e+02 - logprior: 0.3827
Epoch 4/10
25/25 - 8s - loss: 970.9373 - loglik: -9.7136e+02 - logprior: 0.4231
Epoch 5/10
25/25 - 8s - loss: 971.0633 - loglik: -9.7149e+02 - logprior: 0.4225
Fitted a model with MAP estimate = -971.4308
Time for alignment: 151.1563
Computed alignments with likelihoods: ['-971.9992', '-972.2332', '-971.3995', '-970.8819', '-971.4308']
Best model has likelihood: -970.8819  (prior= 0.5396 )
time for generating output: 0.2833
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cys.projection.fasta
SP score = 0.9273205975998041
Training of 5 independent models on file DMRL_synthase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5cda9057c0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5cda905880>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5cb06ea0a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5cb06ea460>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5d10a951f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5ca7e15730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5ca7076d90>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5ca7d4adf0>, <__main__.SimpleDirichletPrior object at 0x7f5afed38250>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 809.6382 - loglik: -7.8579e+02 - logprior: -2.3847e+01
Epoch 2/10
10/10 - 2s - loss: 740.7056 - loglik: -7.3527e+02 - logprior: -5.4352e+00
Epoch 3/10
10/10 - 2s - loss: 689.6779 - loglik: -6.8693e+02 - logprior: -2.7439e+00
Epoch 4/10
10/10 - 2s - loss: 656.4931 - loglik: -6.5428e+02 - logprior: -2.2106e+00
Epoch 5/10
10/10 - 2s - loss: 644.4748 - loglik: -6.4266e+02 - logprior: -1.8189e+00
Epoch 6/10
10/10 - 2s - loss: 638.8300 - loglik: -6.3722e+02 - logprior: -1.6057e+00
Epoch 7/10
10/10 - 2s - loss: 634.9500 - loglik: -6.3343e+02 - logprior: -1.5151e+00
Epoch 8/10
10/10 - 2s - loss: 633.9954 - loglik: -6.3262e+02 - logprior: -1.3709e+00
Epoch 9/10
10/10 - 2s - loss: 633.5275 - loglik: -6.3226e+02 - logprior: -1.2662e+00
Epoch 10/10
10/10 - 2s - loss: 632.7347 - loglik: -6.3157e+02 - logprior: -1.1685e+00
Fitted a model with MAP estimate = -632.8558
expansions: [(0, 4), (13, 1), (14, 1), (15, 1), (20, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 1), (78, 1), (81, 1), (98, 2), (114, 3), (115, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 657.0856 - loglik: -6.2951e+02 - logprior: -2.7574e+01
Epoch 2/2
10/10 - 2s - loss: 619.4253 - loglik: -6.1176e+02 - logprior: -7.6698e+00
Fitted a model with MAP estimate = -612.5554
expansions: []
discards: [ 59  62 122 142]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 627.2064 - loglik: -6.0868e+02 - logprior: -1.8523e+01
Epoch 2/2
10/10 - 2s - loss: 609.7814 - loglik: -6.0586e+02 - logprior: -3.9258e+00
Fitted a model with MAP estimate = -607.2971
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 624.2765 - loglik: -6.0692e+02 - logprior: -1.7358e+01
Epoch 2/10
10/10 - 2s - loss: 608.8281 - loglik: -6.0530e+02 - logprior: -3.5250e+00
Epoch 3/10
10/10 - 2s - loss: 605.8449 - loglik: -6.0495e+02 - logprior: -8.9520e-01
Epoch 4/10
10/10 - 2s - loss: 604.5389 - loglik: -6.0454e+02 - logprior: 0.0059
Epoch 5/10
10/10 - 2s - loss: 603.0865 - loglik: -6.0353e+02 - logprior: 0.4446
Epoch 6/10
10/10 - 2s - loss: 602.7928 - loglik: -6.0356e+02 - logprior: 0.7664
Epoch 7/10
10/10 - 2s - loss: 602.2245 - loglik: -6.0327e+02 - logprior: 1.0437
Epoch 8/10
10/10 - 2s - loss: 602.7082 - loglik: -6.0393e+02 - logprior: 1.2168
Fitted a model with MAP estimate = -602.0341
Time for alignment: 63.6002
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 809.4089 - loglik: -7.8556e+02 - logprior: -2.3845e+01
Epoch 2/10
10/10 - 2s - loss: 741.8598 - loglik: -7.3643e+02 - logprior: -5.4345e+00
Epoch 3/10
10/10 - 2s - loss: 688.6986 - loglik: -6.8592e+02 - logprior: -2.7749e+00
Epoch 4/10
10/10 - 2s - loss: 658.5187 - loglik: -6.5621e+02 - logprior: -2.3068e+00
Epoch 5/10
10/10 - 2s - loss: 646.7836 - loglik: -6.4487e+02 - logprior: -1.9137e+00
Epoch 6/10
10/10 - 2s - loss: 641.3962 - loglik: -6.3973e+02 - logprior: -1.6622e+00
Epoch 7/10
10/10 - 2s - loss: 639.8942 - loglik: -6.3830e+02 - logprior: -1.5914e+00
Epoch 8/10
10/10 - 2s - loss: 638.2359 - loglik: -6.3673e+02 - logprior: -1.5068e+00
Epoch 9/10
10/10 - 2s - loss: 638.0936 - loglik: -6.3663e+02 - logprior: -1.4629e+00
Epoch 10/10
10/10 - 2s - loss: 637.1993 - loglik: -6.3569e+02 - logprior: -1.5087e+00
Fitted a model with MAP estimate = -637.0034
expansions: [(0, 4), (13, 1), (14, 1), (15, 1), (20, 1), (26, 1), (27, 1), (30, 1), (31, 1), (46, 3), (47, 2), (71, 1), (72, 1), (75, 1), (76, 1), (77, 2), (78, 1), (98, 2), (104, 3), (105, 3), (106, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 149 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 660.6553 - loglik: -6.3305e+02 - logprior: -2.7607e+01
Epoch 2/2
10/10 - 2s - loss: 620.1568 - loglik: -6.1238e+02 - logprior: -7.7738e+00
Fitted a model with MAP estimate = -612.9350
expansions: []
discards: [ 59  62 122 135 138]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 627.3082 - loglik: -6.0876e+02 - logprior: -1.8544e+01
Epoch 2/2
10/10 - 2s - loss: 609.8886 - loglik: -6.0595e+02 - logprior: -3.9393e+00
Fitted a model with MAP estimate = -607.3552
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 624.2670 - loglik: -6.0689e+02 - logprior: -1.7377e+01
Epoch 2/10
10/10 - 2s - loss: 608.5438 - loglik: -6.0498e+02 - logprior: -3.5600e+00
Epoch 3/10
10/10 - 2s - loss: 606.0891 - loglik: -6.0518e+02 - logprior: -9.0896e-01
Epoch 4/10
10/10 - 2s - loss: 604.1217 - loglik: -6.0411e+02 - logprior: -1.4905e-02
Epoch 5/10
10/10 - 2s - loss: 603.5725 - loglik: -6.0400e+02 - logprior: 0.4244
Epoch 6/10
10/10 - 2s - loss: 602.3402 - loglik: -6.0309e+02 - logprior: 0.7485
Epoch 7/10
10/10 - 2s - loss: 603.1441 - loglik: -6.0416e+02 - logprior: 1.0170
Fitted a model with MAP estimate = -602.2819
Time for alignment: 60.2460
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 809.5137 - loglik: -7.8567e+02 - logprior: -2.3846e+01
Epoch 2/10
10/10 - 2s - loss: 741.5170 - loglik: -7.3609e+02 - logprior: -5.4314e+00
Epoch 3/10
10/10 - 2s - loss: 687.0937 - loglik: -6.8436e+02 - logprior: -2.7332e+00
Epoch 4/10
10/10 - 2s - loss: 656.3623 - loglik: -6.5419e+02 - logprior: -2.1692e+00
Epoch 5/10
10/10 - 2s - loss: 644.8763 - loglik: -6.4319e+02 - logprior: -1.6847e+00
Epoch 6/10
10/10 - 2s - loss: 639.6747 - loglik: -6.3833e+02 - logprior: -1.3473e+00
Epoch 7/10
10/10 - 2s - loss: 636.0197 - loglik: -6.3485e+02 - logprior: -1.1697e+00
Epoch 8/10
10/10 - 2s - loss: 634.8716 - loglik: -6.3388e+02 - logprior: -9.8877e-01
Epoch 9/10
10/10 - 2s - loss: 634.3245 - loglik: -6.3343e+02 - logprior: -8.9372e-01
Epoch 10/10
10/10 - 2s - loss: 634.4136 - loglik: -6.3360e+02 - logprior: -8.1755e-01
Fitted a model with MAP estimate = -633.6843
expansions: [(0, 4), (13, 3), (22, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (76, 1), (77, 1), (78, 3), (81, 1), (105, 1), (114, 3), (115, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 655.4578 - loglik: -6.2790e+02 - logprior: -2.7555e+01
Epoch 2/2
10/10 - 2s - loss: 619.1426 - loglik: -6.1153e+02 - logprior: -7.6153e+00
Fitted a model with MAP estimate = -612.4098
expansions: []
discards: [ 59  62 141]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 626.3907 - loglik: -6.0787e+02 - logprior: -1.8518e+01
Epoch 2/2
10/10 - 2s - loss: 609.5762 - loglik: -6.0565e+02 - logprior: -3.9223e+00
Fitted a model with MAP estimate = -607.2822
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 624.0529 - loglik: -6.0668e+02 - logprior: -1.7371e+01
Epoch 2/10
10/10 - 2s - loss: 608.7283 - loglik: -6.0519e+02 - logprior: -3.5397e+00
Epoch 3/10
10/10 - 2s - loss: 605.8533 - loglik: -6.0496e+02 - logprior: -8.9701e-01
Epoch 4/10
10/10 - 2s - loss: 604.3981 - loglik: -6.0440e+02 - logprior: -1.0124e-03
Epoch 5/10
10/10 - 2s - loss: 603.7316 - loglik: -6.0417e+02 - logprior: 0.4382
Epoch 6/10
10/10 - 2s - loss: 602.3408 - loglik: -6.0311e+02 - logprior: 0.7680
Epoch 7/10
10/10 - 2s - loss: 602.6749 - loglik: -6.0371e+02 - logprior: 1.0389
Fitted a model with MAP estimate = -602.2425
Time for alignment: 60.7322
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 808.8720 - loglik: -7.8503e+02 - logprior: -2.3845e+01
Epoch 2/10
10/10 - 2s - loss: 741.5999 - loglik: -7.3617e+02 - logprior: -5.4316e+00
Epoch 3/10
10/10 - 2s - loss: 687.5262 - loglik: -6.8479e+02 - logprior: -2.7358e+00
Epoch 4/10
10/10 - 2s - loss: 656.7572 - loglik: -6.5454e+02 - logprior: -2.2195e+00
Epoch 5/10
10/10 - 2s - loss: 643.7532 - loglik: -6.4187e+02 - logprior: -1.8811e+00
Epoch 6/10
10/10 - 2s - loss: 637.8489 - loglik: -6.3610e+02 - logprior: -1.7473e+00
Epoch 7/10
10/10 - 2s - loss: 635.2308 - loglik: -6.3364e+02 - logprior: -1.5934e+00
Epoch 8/10
10/10 - 2s - loss: 634.8820 - loglik: -6.3354e+02 - logprior: -1.3445e+00
Epoch 9/10
10/10 - 2s - loss: 632.8433 - loglik: -6.3164e+02 - logprior: -1.2005e+00
Epoch 10/10
10/10 - 2s - loss: 632.7317 - loglik: -6.3160e+02 - logprior: -1.1280e+00
Fitted a model with MAP estimate = -632.6278
expansions: [(0, 4), (13, 1), (14, 1), (15, 1), (20, 1), (26, 1), (27, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 1), (78, 1), (81, 1), (105, 1), (114, 3), (115, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 656.0436 - loglik: -6.2849e+02 - logprior: -2.7558e+01
Epoch 2/2
10/10 - 2s - loss: 619.8805 - loglik: -6.1226e+02 - logprior: -7.6226e+00
Fitted a model with MAP estimate = -612.5166
expansions: []
discards: [ 59  62 140]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 626.6544 - loglik: -6.0812e+02 - logprior: -1.8533e+01
Epoch 2/2
10/10 - 2s - loss: 609.7143 - loglik: -6.0578e+02 - logprior: -3.9334e+00
Fitted a model with MAP estimate = -607.2971
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 623.4270 - loglik: -6.0606e+02 - logprior: -1.7370e+01
Epoch 2/10
10/10 - 2s - loss: 609.5615 - loglik: -6.0602e+02 - logprior: -3.5380e+00
Epoch 3/10
10/10 - 2s - loss: 605.7520 - loglik: -6.0484e+02 - logprior: -9.1114e-01
Epoch 4/10
10/10 - 2s - loss: 604.0936 - loglik: -6.0408e+02 - logprior: -8.6492e-03
Epoch 5/10
10/10 - 2s - loss: 603.4653 - loglik: -6.0389e+02 - logprior: 0.4232
Epoch 6/10
10/10 - 2s - loss: 602.8712 - loglik: -6.0362e+02 - logprior: 0.7531
Epoch 7/10
10/10 - 2s - loss: 602.9220 - loglik: -6.0394e+02 - logprior: 1.0180
Fitted a model with MAP estimate = -602.3158
Time for alignment: 60.9607
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 809.1954 - loglik: -7.8535e+02 - logprior: -2.3845e+01
Epoch 2/10
10/10 - 2s - loss: 741.2558 - loglik: -7.3582e+02 - logprior: -5.4338e+00
Epoch 3/10
10/10 - 2s - loss: 686.1551 - loglik: -6.8336e+02 - logprior: -2.7972e+00
Epoch 4/10
10/10 - 2s - loss: 653.5414 - loglik: -6.5112e+02 - logprior: -2.4185e+00
Epoch 5/10
10/10 - 2s - loss: 642.4512 - loglik: -6.4043e+02 - logprior: -2.0233e+00
Epoch 6/10
10/10 - 2s - loss: 635.6252 - loglik: -6.3392e+02 - logprior: -1.7021e+00
Epoch 7/10
10/10 - 2s - loss: 634.6206 - loglik: -6.3309e+02 - logprior: -1.5272e+00
Epoch 8/10
10/10 - 2s - loss: 633.4042 - loglik: -6.3208e+02 - logprior: -1.3208e+00
Epoch 9/10
10/10 - 2s - loss: 632.8722 - loglik: -6.3167e+02 - logprior: -1.2038e+00
Epoch 10/10
10/10 - 2s - loss: 632.2502 - loglik: -6.3113e+02 - logprior: -1.1200e+00
Fitted a model with MAP estimate = -632.2507
expansions: [(0, 4), (13, 1), (14, 1), (19, 1), (20, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 1), (78, 1), (81, 1), (98, 2), (114, 3), (115, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 655.8677 - loglik: -6.2831e+02 - logprior: -2.7558e+01
Epoch 2/2
10/10 - 2s - loss: 619.5766 - loglik: -6.1191e+02 - logprior: -7.6687e+00
Fitted a model with MAP estimate = -612.4806
expansions: []
discards: [ 59  62 122 142]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 626.5506 - loglik: -6.0802e+02 - logprior: -1.8526e+01
Epoch 2/2
10/10 - 2s - loss: 609.8990 - loglik: -6.0597e+02 - logprior: -3.9279e+00
Fitted a model with MAP estimate = -607.2748
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 623.9365 - loglik: -6.0658e+02 - logprior: -1.7360e+01
Epoch 2/10
10/10 - 2s - loss: 608.8314 - loglik: -6.0530e+02 - logprior: -3.5335e+00
Epoch 3/10
10/10 - 2s - loss: 606.0315 - loglik: -6.0513e+02 - logprior: -8.9891e-01
Epoch 4/10
10/10 - 2s - loss: 604.0323 - loglik: -6.0404e+02 - logprior: 0.0058
Epoch 5/10
10/10 - 2s - loss: 603.0068 - loglik: -6.0345e+02 - logprior: 0.4444
Epoch 6/10
10/10 - 2s - loss: 603.5072 - loglik: -6.0427e+02 - logprior: 0.7640
Fitted a model with MAP estimate = -602.6202
Time for alignment: 58.8027
Computed alignments with likelihoods: ['-602.0341', '-602.2819', '-602.2425', '-602.3158', '-602.6202']
Best model has likelihood: -602.0341  (prior= 1.2831 )
time for generating output: 0.2148
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DMRL_synthase.projection.fasta
SP score = 0.9119453924914676
Training of 5 independent models on file Stap_Strp_toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5984595940>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5ca75b5d60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f598c0c0640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f598c0c0a90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b0796d040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f59642bd040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b13b7cc10>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5cb06ea0a0>, <__main__.SimpleDirichletPrior object at 0x7f5b08f78730>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 581.9310 - loglik: -5.0588e+02 - logprior: -7.6051e+01
Epoch 2/10
10/10 - 2s - loss: 495.5539 - loglik: -4.7669e+02 - logprior: -1.8868e+01
Epoch 3/10
10/10 - 2s - loss: 461.6242 - loglik: -4.5369e+02 - logprior: -7.9350e+00
Epoch 4/10
10/10 - 2s - loss: 447.6301 - loglik: -4.4356e+02 - logprior: -4.0734e+00
Epoch 5/10
10/10 - 2s - loss: 442.3764 - loglik: -4.4044e+02 - logprior: -1.9343e+00
Epoch 6/10
10/10 - 2s - loss: 439.9404 - loglik: -4.3914e+02 - logprior: -7.9622e-01
Epoch 7/10
10/10 - 2s - loss: 438.7441 - loglik: -4.3861e+02 - logprior: -1.3179e-01
Epoch 8/10
10/10 - 2s - loss: 438.0667 - loglik: -4.3834e+02 - logprior: 0.2731
Epoch 9/10
10/10 - 2s - loss: 437.6749 - loglik: -4.3822e+02 - logprior: 0.5461
Epoch 10/10
10/10 - 2s - loss: 437.4009 - loglik: -4.3815e+02 - logprior: 0.7484
Fitted a model with MAP estimate = -437.2540
expansions: [(9, 3), (26, 2), (27, 3), (37, 1), (39, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 517.2562 - loglik: -4.3874e+02 - logprior: -7.8515e+01
Epoch 2/2
10/10 - 2s - loss: 461.9046 - loglik: -4.3070e+02 - logprior: -3.1203e+01
Fitted a model with MAP estimate = -451.3767
expansions: [(17, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 498.9448 - loglik: -4.2543e+02 - logprior: -7.3515e+01
Epoch 2/2
10/10 - 2s - loss: 443.1003 - loglik: -4.2188e+02 - logprior: -2.1217e+01
Fitted a model with MAP estimate = -431.5905
expansions: [(16, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 482.6923 - loglik: -4.2018e+02 - logprior: -6.2515e+01
Epoch 2/10
10/10 - 2s - loss: 434.1572 - loglik: -4.1916e+02 - logprior: -1.4997e+01
Epoch 3/10
10/10 - 2s - loss: 424.2626 - loglik: -4.1912e+02 - logprior: -5.1435e+00
Epoch 4/10
10/10 - 2s - loss: 420.2398 - loglik: -4.1914e+02 - logprior: -1.1023e+00
Epoch 5/10
10/10 - 2s - loss: 418.1298 - loglik: -4.1919e+02 - logprior: 1.0552
Epoch 6/10
10/10 - 2s - loss: 416.9463 - loglik: -4.1924e+02 - logprior: 2.2986
Epoch 7/10
10/10 - 2s - loss: 416.2638 - loglik: -4.1937e+02 - logprior: 3.1046
Epoch 8/10
10/10 - 2s - loss: 415.8018 - loglik: -4.1954e+02 - logprior: 3.7369
Epoch 9/10
10/10 - 2s - loss: 415.4403 - loglik: -4.1969e+02 - logprior: 4.2513
Epoch 10/10
10/10 - 2s - loss: 415.1366 - loglik: -4.1980e+02 - logprior: 4.6604
Fitted a model with MAP estimate = -414.9609
Time for alignment: 65.8366
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 581.9145 - loglik: -5.0586e+02 - logprior: -7.6052e+01
Epoch 2/10
10/10 - 2s - loss: 495.4403 - loglik: -4.7657e+02 - logprior: -1.8872e+01
Epoch 3/10
10/10 - 2s - loss: 461.4251 - loglik: -4.5350e+02 - logprior: -7.9283e+00
Epoch 4/10
10/10 - 2s - loss: 447.7443 - loglik: -4.4373e+02 - logprior: -4.0133e+00
Epoch 5/10
10/10 - 2s - loss: 442.6698 - loglik: -4.4076e+02 - logprior: -1.9102e+00
Epoch 6/10
10/10 - 2s - loss: 440.0391 - loglik: -4.3928e+02 - logprior: -7.5554e-01
Epoch 7/10
10/10 - 2s - loss: 438.3406 - loglik: -4.3823e+02 - logprior: -1.1303e-01
Epoch 8/10
10/10 - 2s - loss: 437.1148 - loglik: -4.3742e+02 - logprior: 0.3025
Epoch 9/10
10/10 - 2s - loss: 436.3073 - loglik: -4.3688e+02 - logprior: 0.5736
Epoch 10/10
10/10 - 2s - loss: 435.3832 - loglik: -4.3613e+02 - logprior: 0.7489
Fitted a model with MAP estimate = -434.7538
expansions: [(14, 2), (24, 1), (26, 2), (37, 1), (46, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 512.7037 - loglik: -4.3439e+02 - logprior: -7.8314e+01
Epoch 2/2
10/10 - 2s - loss: 458.0118 - loglik: -4.2688e+02 - logprior: -3.1130e+01
Fitted a model with MAP estimate = -448.4849
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 501.1466 - loglik: -4.2580e+02 - logprior: -7.5348e+01
Epoch 2/2
10/10 - 2s - loss: 449.5531 - loglik: -4.2400e+02 - logprior: -2.5557e+01
Fitted a model with MAP estimate = -438.2048
expansions: [(0, 3), (5, 2), (7, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 485.0809 - loglik: -4.2348e+02 - logprior: -6.1596e+01
Epoch 2/10
10/10 - 2s - loss: 435.1371 - loglik: -4.2021e+02 - logprior: -1.4924e+01
Epoch 3/10
10/10 - 2s - loss: 424.6868 - loglik: -4.1946e+02 - logprior: -5.2315e+00
Epoch 4/10
10/10 - 2s - loss: 420.6477 - loglik: -4.1941e+02 - logprior: -1.2333e+00
Epoch 5/10
10/10 - 2s - loss: 418.4304 - loglik: -4.1945e+02 - logprior: 1.0222
Epoch 6/10
10/10 - 2s - loss: 417.2864 - loglik: -4.1965e+02 - logprior: 2.3641
Epoch 7/10
10/10 - 2s - loss: 416.5803 - loglik: -4.1977e+02 - logprior: 3.1946
Epoch 8/10
10/10 - 2s - loss: 416.1110 - loglik: -4.1986e+02 - logprior: 3.7516
Epoch 9/10
10/10 - 2s - loss: 415.7498 - loglik: -4.1990e+02 - logprior: 4.1536
Epoch 10/10
10/10 - 2s - loss: 415.4557 - loglik: -4.1994e+02 - logprior: 4.4877
Fitted a model with MAP estimate = -415.2840
Time for alignment: 65.8636
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 581.9100 - loglik: -5.0586e+02 - logprior: -7.6051e+01
Epoch 2/10
10/10 - 2s - loss: 495.2645 - loglik: -4.7639e+02 - logprior: -1.8874e+01
Epoch 3/10
10/10 - 2s - loss: 460.9459 - loglik: -4.5300e+02 - logprior: -7.9417e+00
Epoch 4/10
10/10 - 2s - loss: 446.9962 - loglik: -4.4293e+02 - logprior: -4.0682e+00
Epoch 5/10
10/10 - 2s - loss: 441.5394 - loglik: -4.3963e+02 - logprior: -1.9105e+00
Epoch 6/10
10/10 - 2s - loss: 438.1173 - loglik: -4.3733e+02 - logprior: -7.8483e-01
Epoch 7/10
10/10 - 2s - loss: 436.2487 - loglik: -4.3612e+02 - logprior: -1.3302e-01
Epoch 8/10
10/10 - 2s - loss: 434.6424 - loglik: -4.3489e+02 - logprior: 0.2429
Epoch 9/10
10/10 - 2s - loss: 433.3399 - loglik: -4.3386e+02 - logprior: 0.5203
Epoch 10/10
10/10 - 2s - loss: 432.7748 - loglik: -4.3349e+02 - logprior: 0.7137
Fitted a model with MAP estimate = -432.5925
expansions: [(9, 3), (26, 2), (37, 1), (39, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 511.9147 - loglik: -4.3362e+02 - logprior: -7.8300e+01
Epoch 2/2
10/10 - 2s - loss: 457.7850 - loglik: -4.2664e+02 - logprior: -3.1141e+01
Fitted a model with MAP estimate = -447.9881
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 496.2336 - loglik: -4.2349e+02 - logprior: -7.2740e+01
Epoch 2/10
10/10 - 2s - loss: 441.8386 - loglik: -4.2150e+02 - logprior: -2.0342e+01
Epoch 3/10
10/10 - 2s - loss: 427.5756 - loglik: -4.2084e+02 - logprior: -6.7376e+00
Epoch 4/10
10/10 - 2s - loss: 422.7664 - loglik: -4.2078e+02 - logprior: -1.9887e+00
Epoch 5/10
10/10 - 2s - loss: 420.5502 - loglik: -4.2085e+02 - logprior: 0.3009
Epoch 6/10
10/10 - 2s - loss: 419.3729 - loglik: -4.2093e+02 - logprior: 1.5565
Epoch 7/10
10/10 - 2s - loss: 418.6681 - loglik: -4.2101e+02 - logprior: 2.3462
Epoch 8/10
10/10 - 2s - loss: 418.0353 - loglik: -4.2101e+02 - logprior: 2.9743
Epoch 9/10
10/10 - 2s - loss: 417.5822 - loglik: -4.2105e+02 - logprior: 3.4670
Epoch 10/10
10/10 - 2s - loss: 417.2595 - loglik: -4.2111e+02 - logprior: 3.8463
Fitted a model with MAP estimate = -417.0863
Time for alignment: 57.1149
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 581.9492 - loglik: -5.0590e+02 - logprior: -7.6051e+01
Epoch 2/10
10/10 - 2s - loss: 495.4808 - loglik: -4.7661e+02 - logprior: -1.8868e+01
Epoch 3/10
10/10 - 2s - loss: 461.3463 - loglik: -4.5339e+02 - logprior: -7.9529e+00
Epoch 4/10
10/10 - 2s - loss: 446.8566 - loglik: -4.4270e+02 - logprior: -4.1518e+00
Epoch 5/10
10/10 - 2s - loss: 439.8320 - loglik: -4.3772e+02 - logprior: -2.1144e+00
Epoch 6/10
10/10 - 2s - loss: 436.0348 - loglik: -4.3498e+02 - logprior: -1.0553e+00
Epoch 7/10
10/10 - 2s - loss: 433.5974 - loglik: -4.3314e+02 - logprior: -4.6037e-01
Epoch 8/10
10/10 - 2s - loss: 432.2379 - loglik: -4.3212e+02 - logprior: -1.2029e-01
Epoch 9/10
10/10 - 2s - loss: 431.6234 - loglik: -4.3177e+02 - logprior: 0.1446
Epoch 10/10
10/10 - 2s - loss: 430.9926 - loglik: -4.3132e+02 - logprior: 0.3300
Fitted a model with MAP estimate = -430.6986
expansions: [(9, 3), (14, 1), (15, 1), (25, 1), (26, 2), (37, 1), (52, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 509.7905 - loglik: -4.3125e+02 - logprior: -7.8539e+01
Epoch 2/2
10/10 - 2s - loss: 455.2914 - loglik: -4.2416e+02 - logprior: -3.1129e+01
Fitted a model with MAP estimate = -445.8185
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 494.6584 - loglik: -4.2179e+02 - logprior: -7.2872e+01
Epoch 2/10
10/10 - 2s - loss: 440.4724 - loglik: -4.1994e+02 - logprior: -2.0531e+01
Epoch 3/10
10/10 - 2s - loss: 426.1454 - loglik: -4.1939e+02 - logprior: -6.7602e+00
Epoch 4/10
10/10 - 2s - loss: 421.3782 - loglik: -4.1942e+02 - logprior: -1.9609e+00
Epoch 5/10
10/10 - 2s - loss: 419.1190 - loglik: -4.1945e+02 - logprior: 0.3355
Epoch 6/10
10/10 - 2s - loss: 417.8738 - loglik: -4.1949e+02 - logprior: 1.6133
Epoch 7/10
10/10 - 2s - loss: 417.0618 - loglik: -4.1948e+02 - logprior: 2.4228
Epoch 8/10
10/10 - 2s - loss: 416.5560 - loglik: -4.1956e+02 - logprior: 3.0015
Epoch 9/10
10/10 - 2s - loss: 416.1837 - loglik: -4.1967e+02 - logprior: 3.4875
Epoch 10/10
10/10 - 2s - loss: 415.8831 - loglik: -4.1977e+02 - logprior: 3.8869
Fitted a model with MAP estimate = -415.7045
Time for alignment: 57.4322
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 581.9265 - loglik: -5.0588e+02 - logprior: -7.6051e+01
Epoch 2/10
10/10 - 2s - loss: 495.4149 - loglik: -4.7654e+02 - logprior: -1.8873e+01
Epoch 3/10
10/10 - 2s - loss: 461.5160 - loglik: -4.5358e+02 - logprior: -7.9405e+00
Epoch 4/10
10/10 - 2s - loss: 446.9768 - loglik: -4.4285e+02 - logprior: -4.1312e+00
Epoch 5/10
10/10 - 2s - loss: 441.1628 - loglik: -4.3906e+02 - logprior: -2.1075e+00
Epoch 6/10
10/10 - 2s - loss: 437.8657 - loglik: -4.3686e+02 - logprior: -1.0043e+00
Epoch 7/10
10/10 - 2s - loss: 435.7777 - loglik: -4.3540e+02 - logprior: -3.7356e-01
Epoch 8/10
10/10 - 2s - loss: 434.7238 - loglik: -4.3473e+02 - logprior: 0.0034
Epoch 9/10
10/10 - 2s - loss: 434.1501 - loglik: -4.3442e+02 - logprior: 0.2666
Epoch 10/10
10/10 - 2s - loss: 433.1391 - loglik: -4.3358e+02 - logprior: 0.4373
Fitted a model with MAP estimate = -432.3530
expansions: [(9, 3), (14, 1), (15, 1), (25, 1), (26, 2), (27, 2), (37, 1), (39, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 510.5590 - loglik: -4.3195e+02 - logprior: -7.8605e+01
Epoch 2/2
10/10 - 2s - loss: 454.7868 - loglik: -4.2366e+02 - logprior: -3.1123e+01
Fitted a model with MAP estimate = -445.3796
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 494.3190 - loglik: -4.2117e+02 - logprior: -7.3146e+01
Epoch 2/10
10/10 - 2s - loss: 440.3826 - loglik: -4.1941e+02 - logprior: -2.0973e+01
Epoch 3/10
10/10 - 2s - loss: 425.7013 - loglik: -4.1890e+02 - logprior: -6.7969e+00
Epoch 4/10
10/10 - 2s - loss: 420.8365 - loglik: -4.1896e+02 - logprior: -1.8791e+00
Epoch 5/10
10/10 - 2s - loss: 418.4753 - loglik: -4.1891e+02 - logprior: 0.4323
Epoch 6/10
10/10 - 2s - loss: 417.1807 - loglik: -4.1885e+02 - logprior: 1.6655
Epoch 7/10
10/10 - 2s - loss: 416.4104 - loglik: -4.1886e+02 - logprior: 2.4537
Epoch 8/10
10/10 - 2s - loss: 415.7095 - loglik: -4.1880e+02 - logprior: 3.0886
Epoch 9/10
10/10 - 2s - loss: 415.2548 - loglik: -4.1887e+02 - logprior: 3.6102
Epoch 10/10
10/10 - 2s - loss: 414.9360 - loglik: -4.1894e+02 - logprior: 4.0087
Fitted a model with MAP estimate = -414.7468
Time for alignment: 57.5851
Computed alignments with likelihoods: ['-414.9609', '-415.2840', '-417.0863', '-415.7045', '-414.7468']
Best model has likelihood: -414.7468  (prior= 4.1899 )
time for generating output: 0.2521
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Stap_Strp_toxin.projection.fasta
SP score = 0.38405557393317896
Training of 5 independent models on file ghf5.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5ca7ad7910>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b08d749a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5ca6ff8ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b13e16250>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b0837a190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5c842cf2e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5c8436e2e0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5b08374700>, <__main__.SimpleDirichletPrior object at 0x7f5cf3cfe9d0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 13s - loss: 1531.8567 - loglik: -1.5205e+03 - logprior: -1.1329e+01
Epoch 2/10
20/20 - 10s - loss: 1456.0503 - loglik: -1.4560e+03 - logprior: -3.4215e-02
Epoch 3/10
20/20 - 10s - loss: 1429.4390 - loglik: -1.4296e+03 - logprior: 0.1935
Epoch 4/10
20/20 - 10s - loss: 1422.9285 - loglik: -1.4233e+03 - logprior: 0.3833
Epoch 5/10
20/20 - 10s - loss: 1418.1873 - loglik: -1.4185e+03 - logprior: 0.2875
Epoch 6/10
20/20 - 10s - loss: 1418.4530 - loglik: -1.4186e+03 - logprior: 0.0999
Fitted a model with MAP estimate = -1416.5998
expansions: [(0, 3), (18, 5), (45, 1), (52, 1), (54, 2), (82, 3), (86, 13), (87, 1), (94, 1), (116, 1), (118, 1), (121, 1), (150, 1), (153, 2), (168, 1), (169, 10), (198, 4), (207, 1), (208, 1)]
discards: [217 218]
Re-initialized the encoder parameters.
Fitting a model of length 276 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 16s - loss: 1435.0168 - loglik: -1.4231e+03 - logprior: -1.1876e+01
Epoch 2/2
20/20 - 13s - loss: 1413.2175 - loglik: -1.4128e+03 - logprior: -4.1460e-01
Fitted a model with MAP estimate = -1408.7520
expansions: [(0, 3)]
discards: [  1   2   3   4   5   6  96  97 104 105 205 206 207 208 209 210]
Re-initialized the encoder parameters.
Fitting a model of length 263 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 17s - loss: 1427.1855 - loglik: -1.4153e+03 - logprior: -1.1894e+01
Epoch 2/2
20/20 - 12s - loss: 1409.0786 - loglik: -1.4087e+03 - logprior: -4.2468e-01
Fitted a model with MAP estimate = -1408.8125
expansions: [(0, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 266 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 16s - loss: 1420.4509 - loglik: -1.4121e+03 - logprior: -8.3135e+00
Epoch 2/10
20/20 - 12s - loss: 1411.3020 - loglik: -1.4120e+03 - logprior: 0.6922
Epoch 3/10
20/20 - 12s - loss: 1409.0983 - loglik: -1.4108e+03 - logprior: 1.7212
Epoch 4/10
20/20 - 12s - loss: 1401.7888 - loglik: -1.4039e+03 - logprior: 2.0999
Epoch 5/10
20/20 - 12s - loss: 1403.7390 - loglik: -1.4060e+03 - logprior: 2.2411
Fitted a model with MAP estimate = -1400.7854
Time for alignment: 222.0147
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 13s - loss: 1530.9164 - loglik: -1.5196e+03 - logprior: -1.1324e+01
Epoch 2/10
20/20 - 10s - loss: 1455.4594 - loglik: -1.4554e+03 - logprior: -4.2506e-02
Epoch 3/10
20/20 - 10s - loss: 1425.1007 - loglik: -1.4253e+03 - logprior: 0.1783
Epoch 4/10
20/20 - 10s - loss: 1417.8054 - loglik: -1.4183e+03 - logprior: 0.4789
Epoch 5/10
20/20 - 10s - loss: 1423.5100 - loglik: -1.4239e+03 - logprior: 0.3977
Fitted a model with MAP estimate = -1416.3804
expansions: [(0, 3), (46, 1), (53, 4), (82, 4), (92, 3), (93, 1), (110, 3), (115, 2), (117, 1), (153, 2), (180, 1), (198, 3), (207, 1), (208, 1)]
discards: [  0 217 218]
Re-initialized the encoder parameters.
Fitting a model of length 252 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 15s - loss: 1430.9819 - loglik: -1.4226e+03 - logprior: -8.4295e+00
Epoch 2/2
20/20 - 11s - loss: 1414.3728 - loglik: -1.4144e+03 - logprior: 0.0050
Fitted a model with MAP estimate = -1411.5129
expansions: [(0, 4), (93, 3), (95, 3)]
discards: [  1   2   3   4 104 125 126 127 133]
Re-initialized the encoder parameters.
Fitting a model of length 253 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 15s - loss: 1427.7767 - loglik: -1.4177e+03 - logprior: -1.0066e+01
Epoch 2/2
20/20 - 11s - loss: 1408.6191 - loglik: -1.4087e+03 - logprior: 0.0957
Fitted a model with MAP estimate = -1409.2108
expansions: [(0, 3)]
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 252 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 15s - loss: 1421.9426 - loglik: -1.4106e+03 - logprior: -1.1322e+01
Epoch 2/10
20/20 - 11s - loss: 1412.2305 - loglik: -1.4123e+03 - logprior: 0.0554
Epoch 3/10
20/20 - 12s - loss: 1410.2776 - loglik: -1.4119e+03 - logprior: 1.5832
Epoch 4/10
20/20 - 12s - loss: 1406.2916 - loglik: -1.4084e+03 - logprior: 2.0798
Epoch 5/10
20/20 - 11s - loss: 1402.7849 - loglik: -1.4051e+03 - logprior: 2.2773
Epoch 6/10
20/20 - 12s - loss: 1404.5289 - loglik: -1.4068e+03 - logprior: 2.2867
Fitted a model with MAP estimate = -1402.4930
Time for alignment: 213.0039
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 13s - loss: 1530.0773 - loglik: -1.5188e+03 - logprior: -1.1323e+01
Epoch 2/10
20/20 - 10s - loss: 1458.1132 - loglik: -1.4581e+03 - logprior: -4.1054e-02
Epoch 3/10
20/20 - 10s - loss: 1433.9108 - loglik: -1.4340e+03 - logprior: 0.0663
Epoch 4/10
20/20 - 10s - loss: 1414.5432 - loglik: -1.4148e+03 - logprior: 0.2548
Epoch 5/10
20/20 - 10s - loss: 1410.7307 - loglik: -1.4108e+03 - logprior: 0.0831
Epoch 6/10
20/20 - 10s - loss: 1421.6725 - loglik: -1.4215e+03 - logprior: -1.4439e-01
Fitted a model with MAP estimate = -1413.5335
expansions: [(0, 3), (25, 1), (45, 1), (52, 1), (54, 2), (74, 1), (86, 9), (92, 1), (94, 1), (100, 1), (112, 2), (115, 2), (117, 1), (154, 1), (172, 10), (175, 1), (176, 1), (198, 4), (207, 1), (208, 1)]
discards: [  0 217 218]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 16s - loss: 1431.1680 - loglik: -1.4225e+03 - logprior: -8.6736e+00
Epoch 2/2
20/20 - 12s - loss: 1411.8357 - loglik: -1.4119e+03 - logprior: 0.0405
Fitted a model with MAP estimate = -1408.0395
expansions: [(0, 3), (99, 1)]
discards: [  1   2   3 137 198 199 200 201 202 203 233]
Re-initialized the encoder parameters.
Fitting a model of length 260 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 15s - loss: 1420.7019 - loglik: -1.4106e+03 - logprior: -1.0068e+01
Epoch 2/2
20/20 - 12s - loss: 1412.7877 - loglik: -1.4129e+03 - logprior: 0.1215
Fitted a model with MAP estimate = -1406.5515
expansions: [(0, 3)]
discards: [ 1  2  3  5 94]
Re-initialized the encoder parameters.
Fitting a model of length 258 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 16s - loss: 1421.3811 - loglik: -1.4101e+03 - logprior: -1.1267e+01
Epoch 2/10
20/20 - 12s - loss: 1409.7358 - loglik: -1.4098e+03 - logprior: 0.0421
Epoch 3/10
20/20 - 12s - loss: 1406.2910 - loglik: -1.4079e+03 - logprior: 1.6111
Epoch 4/10
20/20 - 12s - loss: 1401.7893 - loglik: -1.4039e+03 - logprior: 2.1131
Epoch 5/10
20/20 - 12s - loss: 1403.4008 - loglik: -1.4057e+03 - logprior: 2.3343
Fitted a model with MAP estimate = -1401.0129
Time for alignment: 217.2919
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 13s - loss: 1531.2546 - loglik: -1.5199e+03 - logprior: -1.1325e+01
Epoch 2/10
20/20 - 10s - loss: 1456.9883 - loglik: -1.4569e+03 - logprior: -9.1065e-02
Epoch 3/10
20/20 - 10s - loss: 1427.5803 - loglik: -1.4277e+03 - logprior: 0.1599
Epoch 4/10
20/20 - 10s - loss: 1422.3380 - loglik: -1.4226e+03 - logprior: 0.2982
Epoch 5/10
20/20 - 10s - loss: 1413.5509 - loglik: -1.4136e+03 - logprior: 0.0940
Epoch 6/10
20/20 - 10s - loss: 1416.6865 - loglik: -1.4165e+03 - logprior: -1.6351e-01
Fitted a model with MAP estimate = -1413.9168
expansions: [(0, 3), (18, 1), (25, 3), (45, 1), (52, 2), (53, 1), (82, 2), (86, 12), (93, 1), (94, 1), (98, 1), (112, 2), (115, 2), (117, 1), (150, 1), (153, 2), (199, 4), (207, 1), (208, 1)]
discards: [217 218]
Re-initialized the encoder parameters.
Fitting a model of length 265 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 16s - loss: 1433.8326 - loglik: -1.4219e+03 - logprior: -1.1924e+01
Epoch 2/2
20/20 - 12s - loss: 1409.7572 - loglik: -1.4093e+03 - logprior: -4.6588e-01
Fitted a model with MAP estimate = -1408.1307
expansions: [(0, 3)]
discards: [  1   2   3   4   5   6  30  95  96  97 145 187 235 236]
Re-initialized the encoder parameters.
Fitting a model of length 254 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 15s - loss: 1424.9562 - loglik: -1.4130e+03 - logprior: -1.1915e+01
Epoch 2/2
20/20 - 11s - loss: 1410.1108 - loglik: -1.4097e+03 - logprior: -3.9791e-01
Fitted a model with MAP estimate = -1407.4050
expansions: [(0, 4), (220, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 16s - loss: 1418.3365 - loglik: -1.4100e+03 - logprior: -8.3011e+00
Epoch 2/10
20/20 - 12s - loss: 1408.0001 - loglik: -1.4087e+03 - logprior: 0.7376
Epoch 3/10
20/20 - 12s - loss: 1404.1649 - loglik: -1.4060e+03 - logprior: 1.7889
Epoch 4/10
20/20 - 12s - loss: 1405.6760 - loglik: -1.4078e+03 - logprior: 2.1734
Fitted a model with MAP estimate = -1402.0584
Time for alignment: 203.8173
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 13s - loss: 1529.9714 - loglik: -1.5186e+03 - logprior: -1.1322e+01
Epoch 2/10
20/20 - 10s - loss: 1463.9744 - loglik: -1.4640e+03 - logprior: 0.0076
Epoch 3/10
20/20 - 10s - loss: 1421.6580 - loglik: -1.4220e+03 - logprior: 0.2975
Epoch 4/10
20/20 - 10s - loss: 1423.6351 - loglik: -1.4242e+03 - logprior: 0.5433
Fitted a model with MAP estimate = -1419.7744
expansions: [(0, 3), (45, 1), (52, 4), (81, 1), (85, 10), (86, 2), (93, 1), (115, 2), (117, 1), (146, 1), (198, 3), (206, 2)]
discards: [  1 217 218]
Re-initialized the encoder parameters.
Fitting a model of length 253 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 15s - loss: 1433.8914 - loglik: -1.4226e+03 - logprior: -1.1300e+01
Epoch 2/2
20/20 - 12s - loss: 1416.7302 - loglik: -1.4163e+03 - logprior: -4.6459e-01
Fitted a model with MAP estimate = -1413.7391
expansions: [(0, 4), (133, 2)]
discards: [  2   3   4   5 136 235]
Re-initialized the encoder parameters.
Fitting a model of length 253 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 1428.8533 - loglik: -1.4168e+03 - logprior: -1.2020e+01
Epoch 2/2
20/20 - 12s - loss: 1410.2571 - loglik: -1.4097e+03 - logprior: -5.3705e-01
Fitted a model with MAP estimate = -1408.1221
expansions: [(0, 5), (98, 1), (175, 1), (219, 2)]
discards: [5]
Re-initialized the encoder parameters.
Fitting a model of length 261 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 15s - loss: 1421.6688 - loglik: -1.4103e+03 - logprior: -1.1327e+01
Epoch 2/10
20/20 - 12s - loss: 1409.2056 - loglik: -1.4084e+03 - logprior: -8.4371e-01
Epoch 3/10
20/20 - 12s - loss: 1404.7852 - loglik: -1.4063e+03 - logprior: 1.4916
Epoch 4/10
20/20 - 12s - loss: 1405.6190 - loglik: -1.4077e+03 - logprior: 2.0761
Fitted a model with MAP estimate = -1401.0994
Time for alignment: 181.9300
Computed alignments with likelihoods: ['-1400.7854', '-1402.4930', '-1401.0129', '-1402.0584', '-1401.0994']
Best model has likelihood: -1400.7854  (prior= 2.2398 )
time for generating output: 0.3753
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf5.projection.fasta
SP score = 0.637233259749816
Training of 5 independent models on file myb_DNA-binding.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f59003a6a60>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5cc08c78b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5afee6b850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b0a4b96a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5940567d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5940571640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f59844e0700>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5b1c53a670>, <__main__.SimpleDirichletPrior object at 0x7f531d52d040>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 274.2548 - loglik: -2.7086e+02 - logprior: -3.3987e+00
Epoch 2/10
19/19 - 2s - loss: 253.5141 - loglik: -2.5208e+02 - logprior: -1.4311e+00
Epoch 3/10
19/19 - 2s - loss: 245.6075 - loglik: -2.4437e+02 - logprior: -1.2417e+00
Epoch 4/10
19/19 - 2s - loss: 243.3904 - loglik: -2.4220e+02 - logprior: -1.1871e+00
Epoch 5/10
19/19 - 2s - loss: 242.6234 - loglik: -2.4143e+02 - logprior: -1.1894e+00
Epoch 6/10
19/19 - 2s - loss: 242.1228 - loglik: -2.4094e+02 - logprior: -1.1831e+00
Epoch 7/10
19/19 - 2s - loss: 241.8647 - loglik: -2.4067e+02 - logprior: -1.1937e+00
Epoch 8/10
19/19 - 2s - loss: 241.6667 - loglik: -2.4046e+02 - logprior: -1.2116e+00
Epoch 9/10
19/19 - 2s - loss: 241.9053 - loglik: -2.4068e+02 - logprior: -1.2288e+00
Fitted a model with MAP estimate = -238.4101
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (20, 1), (23, 2), (29, 1), (30, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 242.8581 - loglik: -2.3943e+02 - logprior: -3.4285e+00
Epoch 2/2
19/19 - 1s - loss: 236.6237 - loglik: -2.3531e+02 - logprior: -1.3163e+00
Fitted a model with MAP estimate = -232.7646
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 235.2582 - loglik: -2.3204e+02 - logprior: -3.2153e+00
Epoch 2/10
19/19 - 1s - loss: 231.3286 - loglik: -2.2994e+02 - logprior: -1.3912e+00
Epoch 3/10
19/19 - 1s - loss: 230.5408 - loglik: -2.2927e+02 - logprior: -1.2755e+00
Epoch 4/10
19/19 - 1s - loss: 229.4603 - loglik: -2.2820e+02 - logprior: -1.2618e+00
Epoch 5/10
19/19 - 1s - loss: 229.2827 - loglik: -2.2803e+02 - logprior: -1.2509e+00
Epoch 6/10
19/19 - 1s - loss: 229.4724 - loglik: -2.2821e+02 - logprior: -1.2666e+00
Fitted a model with MAP estimate = -229.0374
Time for alignment: 44.3664
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 274.2957 - loglik: -2.7090e+02 - logprior: -3.3999e+00
Epoch 2/10
19/19 - 2s - loss: 253.5421 - loglik: -2.5209e+02 - logprior: -1.4484e+00
Epoch 3/10
19/19 - 2s - loss: 246.2814 - loglik: -2.4506e+02 - logprior: -1.2216e+00
Epoch 4/10
19/19 - 2s - loss: 243.7172 - loglik: -2.4254e+02 - logprior: -1.1804e+00
Epoch 5/10
19/19 - 2s - loss: 242.5093 - loglik: -2.4132e+02 - logprior: -1.1936e+00
Epoch 6/10
19/19 - 2s - loss: 242.3734 - loglik: -2.4117e+02 - logprior: -1.1985e+00
Epoch 7/10
19/19 - 2s - loss: 241.9505 - loglik: -2.4076e+02 - logprior: -1.1951e+00
Epoch 8/10
19/19 - 2s - loss: 241.6789 - loglik: -2.4046e+02 - logprior: -1.2197e+00
Epoch 9/10
19/19 - 2s - loss: 241.7937 - loglik: -2.4056e+02 - logprior: -1.2369e+00
Fitted a model with MAP estimate = -238.3406
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (20, 1), (23, 2), (28, 2), (30, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 242.1234 - loglik: -2.3869e+02 - logprior: -3.4350e+00
Epoch 2/2
19/19 - 1s - loss: 234.4417 - loglik: -2.3301e+02 - logprior: -1.4365e+00
Fitted a model with MAP estimate = -230.2416
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 232.3049 - loglik: -2.2905e+02 - logprior: -3.2549e+00
Epoch 2/10
19/19 - 2s - loss: 229.3639 - loglik: -2.2795e+02 - logprior: -1.4104e+00
Epoch 3/10
19/19 - 1s - loss: 228.7141 - loglik: -2.2744e+02 - logprior: -1.2778e+00
Epoch 4/10
19/19 - 1s - loss: 228.5936 - loglik: -2.2734e+02 - logprior: -1.2525e+00
Epoch 5/10
19/19 - 1s - loss: 228.3559 - loglik: -2.2711e+02 - logprior: -1.2476e+00
Epoch 6/10
19/19 - 1s - loss: 228.0648 - loglik: -2.2681e+02 - logprior: -1.2572e+00
Epoch 7/10
19/19 - 2s - loss: 227.5454 - loglik: -2.2627e+02 - logprior: -1.2769e+00
Epoch 8/10
19/19 - 1s - loss: 227.7852 - loglik: -2.2647e+02 - logprior: -1.3141e+00
Fitted a model with MAP estimate = -227.5365
Time for alignment: 46.2458
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 274.3867 - loglik: -2.7099e+02 - logprior: -3.3996e+00
Epoch 2/10
19/19 - 2s - loss: 253.2055 - loglik: -2.5167e+02 - logprior: -1.5399e+00
Epoch 3/10
19/19 - 2s - loss: 244.9842 - loglik: -2.4346e+02 - logprior: -1.5211e+00
Epoch 4/10
19/19 - 2s - loss: 242.0447 - loglik: -2.4051e+02 - logprior: -1.5371e+00
Epoch 5/10
19/19 - 2s - loss: 240.9311 - loglik: -2.3941e+02 - logprior: -1.5217e+00
Epoch 6/10
19/19 - 2s - loss: 240.2289 - loglik: -2.3867e+02 - logprior: -1.5581e+00
Epoch 7/10
19/19 - 2s - loss: 240.0955 - loglik: -2.3853e+02 - logprior: -1.5688e+00
Epoch 8/10
19/19 - 2s - loss: 240.2548 - loglik: -2.3865e+02 - logprior: -1.6069e+00
Fitted a model with MAP estimate = -236.8447
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (20, 1), (23, 2), (28, 1), (29, 1), (30, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 239.2525 - loglik: -2.3575e+02 - logprior: -3.5060e+00
Epoch 2/2
19/19 - 2s - loss: 232.8361 - loglik: -2.3141e+02 - logprior: -1.4259e+00
Fitted a model with MAP estimate = -229.3896
expansions: []
discards: [29]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 235.3705 - loglik: -2.3205e+02 - logprior: -3.3244e+00
Epoch 2/2
19/19 - 1s - loss: 232.7354 - loglik: -2.3134e+02 - logprior: -1.3951e+00
Fitted a model with MAP estimate = -229.3778
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 232.0066 - loglik: -2.2878e+02 - logprior: -3.2257e+00
Epoch 2/10
19/19 - 1s - loss: 229.4855 - loglik: -2.2813e+02 - logprior: -1.3602e+00
Epoch 3/10
19/19 - 2s - loss: 229.3713 - loglik: -2.2812e+02 - logprior: -1.2465e+00
Epoch 4/10
19/19 - 2s - loss: 228.8674 - loglik: -2.2766e+02 - logprior: -1.2092e+00
Epoch 5/10
19/19 - 1s - loss: 228.2683 - loglik: -2.2706e+02 - logprior: -1.2079e+00
Epoch 6/10
19/19 - 2s - loss: 228.2691 - loglik: -2.2705e+02 - logprior: -1.2199e+00
Fitted a model with MAP estimate = -228.1160
Time for alignment: 51.7002
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 274.3495 - loglik: -2.7095e+02 - logprior: -3.3973e+00
Epoch 2/10
19/19 - 2s - loss: 253.8397 - loglik: -2.5240e+02 - logprior: -1.4402e+00
Epoch 3/10
19/19 - 2s - loss: 245.9686 - loglik: -2.4473e+02 - logprior: -1.2343e+00
Epoch 4/10
19/19 - 2s - loss: 243.7556 - loglik: -2.4258e+02 - logprior: -1.1764e+00
Epoch 5/10
19/19 - 2s - loss: 242.8286 - loglik: -2.4165e+02 - logprior: -1.1773e+00
Epoch 6/10
19/19 - 2s - loss: 242.1065 - loglik: -2.4092e+02 - logprior: -1.1906e+00
Epoch 7/10
19/19 - 2s - loss: 241.9629 - loglik: -2.4077e+02 - logprior: -1.1955e+00
Epoch 8/10
19/19 - 2s - loss: 241.6499 - loglik: -2.4044e+02 - logprior: -1.2124e+00
Epoch 9/10
19/19 - 2s - loss: 241.8646 - loglik: -2.4064e+02 - logprior: -1.2268e+00
Fitted a model with MAP estimate = -238.4211
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (20, 1), (23, 2), (29, 1), (30, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 242.9316 - loglik: -2.3951e+02 - logprior: -3.4240e+00
Epoch 2/2
19/19 - 2s - loss: 236.5417 - loglik: -2.3522e+02 - logprior: -1.3173e+00
Fitted a model with MAP estimate = -232.7523
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 235.1754 - loglik: -2.3196e+02 - logprior: -3.2169e+00
Epoch 2/10
19/19 - 2s - loss: 231.4551 - loglik: -2.3005e+02 - logprior: -1.4002e+00
Epoch 3/10
19/19 - 1s - loss: 230.2063 - loglik: -2.2892e+02 - logprior: -1.2831e+00
Epoch 4/10
19/19 - 1s - loss: 229.6383 - loglik: -2.2837e+02 - logprior: -1.2648e+00
Epoch 5/10
19/19 - 2s - loss: 229.3800 - loglik: -2.2813e+02 - logprior: -1.2508e+00
Epoch 6/10
19/19 - 1s - loss: 229.4683 - loglik: -2.2821e+02 - logprior: -1.2591e+00
Fitted a model with MAP estimate = -229.0614
Time for alignment: 43.8904
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 274.2429 - loglik: -2.7084e+02 - logprior: -3.3986e+00
Epoch 2/10
19/19 - 2s - loss: 252.8864 - loglik: -2.5153e+02 - logprior: -1.3581e+00
Epoch 3/10
19/19 - 2s - loss: 246.3896 - loglik: -2.4499e+02 - logprior: -1.3973e+00
Epoch 4/10
19/19 - 2s - loss: 244.6677 - loglik: -2.4335e+02 - logprior: -1.3225e+00
Epoch 5/10
19/19 - 2s - loss: 244.1507 - loglik: -2.4286e+02 - logprior: -1.2955e+00
Epoch 6/10
19/19 - 2s - loss: 244.0323 - loglik: -2.4271e+02 - logprior: -1.3247e+00
Epoch 7/10
19/19 - 2s - loss: 243.7325 - loglik: -2.4240e+02 - logprior: -1.3369e+00
Epoch 8/10
19/19 - 2s - loss: 243.6395 - loglik: -2.4228e+02 - logprior: -1.3557e+00
Epoch 9/10
19/19 - 2s - loss: 243.4179 - loglik: -2.4203e+02 - logprior: -1.3889e+00
Epoch 10/10
19/19 - 2s - loss: 243.5115 - loglik: -2.4211e+02 - logprior: -1.4032e+00
Fitted a model with MAP estimate = -240.1201
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (23, 2), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 247.5727 - loglik: -2.4330e+02 - logprior: -4.2719e+00
Epoch 2/2
19/19 - 2s - loss: 240.5675 - loglik: -2.3866e+02 - logprior: -1.9073e+00
Fitted a model with MAP estimate = -235.5708
expansions: [(0, 1), (36, 2)]
discards: [ 0 13]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 238.6156 - loglik: -2.3538e+02 - logprior: -3.2362e+00
Epoch 2/2
19/19 - 2s - loss: 233.7493 - loglik: -2.3233e+02 - logprior: -1.4156e+00
Fitted a model with MAP estimate = -229.8212
expansions: []
discards: [29]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 232.7928 - loglik: -2.2962e+02 - logprior: -3.1697e+00
Epoch 2/10
19/19 - 2s - loss: 230.1947 - loglik: -2.2882e+02 - logprior: -1.3780e+00
Epoch 3/10
19/19 - 1s - loss: 229.6653 - loglik: -2.2843e+02 - logprior: -1.2393e+00
Epoch 4/10
19/19 - 2s - loss: 228.5814 - loglik: -2.2737e+02 - logprior: -1.2158e+00
Epoch 5/10
19/19 - 1s - loss: 228.3627 - loglik: -2.2715e+02 - logprior: -1.2134e+00
Epoch 6/10
19/19 - 1s - loss: 228.4924 - loglik: -2.2728e+02 - logprior: -1.2140e+00
Fitted a model with MAP estimate = -227.9744
Time for alignment: 55.4363
Computed alignments with likelihoods: ['-229.0374', '-227.5365', '-228.1160', '-229.0614', '-227.9744']
Best model has likelihood: -227.5365  (prior= -1.3167 )
time for generating output: 0.1096
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/myb_DNA-binding.projection.fasta
SP score = 0.9519038076152304
Training of 5 independent models on file ghf10.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5cc0f615e0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f531d53ddf0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5c842193d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531d4c1fa0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531d6cd310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f594027eb80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531d531f10>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5334b5e490>, <__main__.SimpleDirichletPrior object at 0x7f5334362dc0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 10s - loss: 1307.1863 - loglik: -1.2809e+03 - logprior: -2.6314e+01
Epoch 2/10
15/15 - 7s - loss: 1217.7689 - loglik: -1.2164e+03 - logprior: -1.3709e+00
Epoch 3/10
15/15 - 7s - loss: 1147.9908 - loglik: -1.1479e+03 - logprior: -4.4762e-02
Epoch 4/10
15/15 - 7s - loss: 1128.7563 - loglik: -1.1284e+03 - logprior: -3.4645e-01
Epoch 5/10
15/15 - 7s - loss: 1116.8019 - loglik: -1.1162e+03 - logprior: -5.6088e-01
Epoch 6/10
15/15 - 7s - loss: 1110.3375 - loglik: -1.1098e+03 - logprior: -5.3816e-01
Epoch 7/10
15/15 - 7s - loss: 1117.1350 - loglik: -1.1168e+03 - logprior: -3.6512e-01
Fitted a model with MAP estimate = -1113.3339
expansions: [(24, 1), (25, 4), (40, 1), (42, 2), (43, 1), (44, 1), (45, 2), (47, 1), (48, 2), (49, 2), (50, 3), (64, 3), (69, 1), (70, 1), (71, 2), (74, 1), (76, 1), (92, 1), (98, 1), (99, 2), (101, 3), (107, 1), (129, 2), (130, 1), (131, 1), (132, 2), (133, 2), (136, 1), (156, 1), (157, 4), (159, 2), (174, 8), (183, 4)]
discards: [  1   2   6 223]
Re-initialized the encoder parameters.
Fitting a model of length 286 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 1126.2557 - loglik: -1.1075e+03 - logprior: -1.8778e+01
Epoch 2/2
15/15 - 10s - loss: 1080.3710 - loglik: -1.0803e+03 - logprior: -8.8747e-02
Fitted a model with MAP estimate = -1078.5138
expansions: [(0, 3), (56, 1), (88, 1)]
discards: [ 22 127 173 228 229 230 231]
Re-initialized the encoder parameters.
Fitting a model of length 284 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 1104.2349 - loglik: -1.0796e+03 - logprior: -2.4652e+01
Epoch 2/2
15/15 - 10s - loss: 1081.8762 - loglik: -1.0800e+03 - logprior: -1.8786e+00
Fitted a model with MAP estimate = -1074.6258
expansions: [(64, 1), (204, 2), (243, 1)]
discards: [  0   1   2 166]
Re-initialized the encoder parameters.
Fitting a model of length 284 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 13s - loss: 1098.2053 - loglik: -1.0809e+03 - logprior: -1.7256e+01
Epoch 2/10
15/15 - 10s - loss: 1071.6219 - loglik: -1.0735e+03 - logprior: 1.8923
Epoch 3/10
15/15 - 9s - loss: 1069.6786 - loglik: -1.0744e+03 - logprior: 4.6933
Epoch 4/10
15/15 - 9s - loss: 1070.2225 - loglik: -1.0758e+03 - logprior: 5.5837
Fitted a model with MAP estimate = -1068.1826
Time for alignment: 159.6173
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 11s - loss: 1306.2397 - loglik: -1.2799e+03 - logprior: -2.6323e+01
Epoch 2/10
15/15 - 7s - loss: 1209.2892 - loglik: -1.2079e+03 - logprior: -1.4142e+00
Epoch 3/10
15/15 - 7s - loss: 1152.4335 - loglik: -1.1524e+03 - logprior: -1.5740e-02
Epoch 4/10
15/15 - 7s - loss: 1121.2054 - loglik: -1.1207e+03 - logprior: -5.2914e-01
Epoch 5/10
15/15 - 7s - loss: 1115.4100 - loglik: -1.1146e+03 - logprior: -8.5194e-01
Epoch 6/10
15/15 - 7s - loss: 1109.0625 - loglik: -1.1083e+03 - logprior: -7.5947e-01
Epoch 7/10
15/15 - 7s - loss: 1116.0242 - loglik: -1.1155e+03 - logprior: -5.0388e-01
Fitted a model with MAP estimate = -1110.7698
expansions: [(18, 1), (19, 1), (20, 2), (21, 4), (24, 1), (48, 2), (49, 1), (51, 2), (52, 1), (53, 2), (65, 1), (67, 2), (68, 3), (69, 1), (71, 1), (73, 1), (76, 1), (79, 1), (95, 1), (101, 1), (102, 1), (103, 3), (110, 1), (132, 2), (133, 1), (134, 1), (135, 2), (136, 2), (137, 1), (138, 1), (140, 1), (157, 1), (158, 5), (159, 2), (174, 7)]
discards: [  1   2 223]
Re-initialized the encoder parameters.
Fitting a model of length 283 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 1116.8804 - loglik: -1.0983e+03 - logprior: -1.8535e+01
Epoch 2/2
15/15 - 9s - loss: 1089.9021 - loglik: -1.0901e+03 - logprior: 0.2478
Fitted a model with MAP estimate = -1079.8430
expansions: []
discards: [ 83 175 232 233 234]
Re-initialized the encoder parameters.
Fitting a model of length 278 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 1106.3673 - loglik: -1.0908e+03 - logprior: -1.5561e+01
Epoch 2/2
15/15 - 9s - loss: 1073.6011 - loglik: -1.0748e+03 - logprior: 1.1681
Fitted a model with MAP estimate = -1077.8585
expansions: [(0, 3), (23, 2), (202, 1)]
discards: [164]
Re-initialized the encoder parameters.
Fitting a model of length 283 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 13s - loss: 1106.9584 - loglik: -1.0832e+03 - logprior: -2.3726e+01
Epoch 2/10
15/15 - 9s - loss: 1077.6886 - loglik: -1.0767e+03 - logprior: -9.3930e-01
Epoch 3/10
15/15 - 9s - loss: 1078.5271 - loglik: -1.0825e+03 - logprior: 4.0012
Fitted a model with MAP estimate = -1072.5131
Time for alignment: 148.7253
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 11s - loss: 1308.7765 - loglik: -1.2825e+03 - logprior: -2.6297e+01
Epoch 2/10
15/15 - 7s - loss: 1217.5062 - loglik: -1.2161e+03 - logprior: -1.4441e+00
Epoch 3/10
15/15 - 7s - loss: 1153.8351 - loglik: -1.1536e+03 - logprior: -2.7004e-01
Epoch 4/10
15/15 - 7s - loss: 1133.0399 - loglik: -1.1325e+03 - logprior: -5.7956e-01
Epoch 5/10
15/15 - 7s - loss: 1112.3026 - loglik: -1.1113e+03 - logprior: -9.6924e-01
Epoch 6/10
15/15 - 7s - loss: 1120.6080 - loglik: -1.1197e+03 - logprior: -9.4327e-01
Fitted a model with MAP estimate = -1114.8709
expansions: [(23, 1), (24, 2), (25, 2), (28, 1), (37, 3), (39, 1), (41, 1), (49, 1), (50, 1), (52, 2), (53, 1), (54, 1), (65, 1), (68, 4), (69, 2), (70, 1), (71, 1), (72, 1), (73, 1), (76, 2), (94, 1), (101, 1), (102, 1), (103, 3), (105, 2), (132, 2), (133, 2), (134, 4), (135, 2), (137, 1), (139, 1), (156, 1), (157, 4), (158, 3), (173, 8), (197, 1)]
discards: [  1   2 203 223]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 1120.2611 - loglik: -1.1014e+03 - logprior: -1.8818e+01
Epoch 2/2
15/15 - 10s - loss: 1083.2573 - loglik: -1.0834e+03 - logprior: 0.1764
Fitted a model with MAP estimate = -1077.0913
expansions: [(246, 2), (247, 2)]
discards: [ 89 139 171 233 234 235 236]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 1096.8153 - loglik: -1.0813e+03 - logprior: -1.5494e+01
Epoch 2/2
15/15 - 9s - loss: 1076.2463 - loglik: -1.0777e+03 - logprior: 1.4038
Fitted a model with MAP estimate = -1072.3376
expansions: [(0, 3), (204, 1)]
discards: [166]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 13s - loss: 1100.3811 - loglik: -1.0766e+03 - logprior: -2.3750e+01
Epoch 2/10
15/15 - 10s - loss: 1076.4408 - loglik: -1.0755e+03 - logprior: -9.2272e-01
Epoch 3/10
15/15 - 10s - loss: 1065.4802 - loglik: -1.0697e+03 - logprior: 4.1718
Epoch 4/10
15/15 - 10s - loss: 1070.2834 - loglik: -1.0761e+03 - logprior: 5.7916
Fitted a model with MAP estimate = -1067.3089
Time for alignment: 153.2466
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 11s - loss: 1312.1193 - loglik: -1.2858e+03 - logprior: -2.6292e+01
Epoch 2/10
15/15 - 7s - loss: 1210.6038 - loglik: -1.2093e+03 - logprior: -1.3269e+00
Epoch 3/10
15/15 - 7s - loss: 1159.5212 - loglik: -1.1597e+03 - logprior: 0.2167
Epoch 4/10
15/15 - 7s - loss: 1138.8551 - loglik: -1.1390e+03 - logprior: 0.1168
Epoch 5/10
15/15 - 7s - loss: 1128.5757 - loglik: -1.1282e+03 - logprior: -3.4617e-01
Epoch 6/10
15/15 - 7s - loss: 1112.9843 - loglik: -1.1130e+03 - logprior: -2.9274e-02
Epoch 7/10
15/15 - 7s - loss: 1120.8776 - loglik: -1.1210e+03 - logprior: 0.1617
Fitted a model with MAP estimate = -1118.9946
expansions: [(19, 1), (45, 2), (47, 1), (53, 2), (55, 3), (56, 2), (57, 3), (60, 1), (70, 3), (71, 5), (72, 1), (74, 2), (77, 1), (86, 1), (94, 1), (101, 4), (103, 2), (105, 2), (132, 2), (133, 2), (134, 4), (135, 2), (137, 1), (139, 1), (156, 1), (157, 6), (173, 8), (183, 3), (195, 1)]
discards: [  1   2   6  22 203 223]
Re-initialized the encoder parameters.
Fitting a model of length 287 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 1129.7156 - loglik: -1.1108e+03 - logprior: -1.8953e+01
Epoch 2/2
15/15 - 10s - loss: 1086.5929 - loglik: -1.0864e+03 - logprior: -2.2066e-01
Fitted a model with MAP estimate = -1083.2847
expansions: [(58, 1), (59, 1), (93, 2), (204, 2), (205, 1), (244, 1), (246, 1)]
discards: [ 20  21  83  84 127 136 168 230 231 232 233]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 1101.4966 - loglik: -1.0857e+03 - logprior: -1.5801e+01
Epoch 2/2
15/15 - 9s - loss: 1077.5084 - loglik: -1.0787e+03 - logprior: 1.1530
Fitted a model with MAP estimate = -1073.8325
expansions: [(0, 3), (62, 1)]
discards: [163 201]
Re-initialized the encoder parameters.
Fitting a model of length 287 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 13s - loss: 1096.3612 - loglik: -1.0726e+03 - logprior: -2.3738e+01
Epoch 2/10
15/15 - 10s - loss: 1082.5387 - loglik: -1.0816e+03 - logprior: -9.2663e-01
Epoch 3/10
15/15 - 10s - loss: 1071.2711 - loglik: -1.0754e+03 - logprior: 4.1592
Epoch 4/10
15/15 - 10s - loss: 1073.4030 - loglik: -1.0791e+03 - logprior: 5.6901
Fitted a model with MAP estimate = -1068.4157
Time for alignment: 160.7433
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 11s - loss: 1307.7922 - loglik: -1.2815e+03 - logprior: -2.6317e+01
Epoch 2/10
15/15 - 7s - loss: 1219.1346 - loglik: -1.2179e+03 - logprior: -1.2289e+00
Epoch 3/10
15/15 - 7s - loss: 1150.5210 - loglik: -1.1509e+03 - logprior: 0.3630
Epoch 4/10
15/15 - 7s - loss: 1141.0577 - loglik: -1.1412e+03 - logprior: 0.1874
Epoch 5/10
15/15 - 7s - loss: 1122.2328 - loglik: -1.1220e+03 - logprior: -1.8355e-01
Epoch 6/10
15/15 - 7s - loss: 1122.0151 - loglik: -1.1220e+03 - logprior: -5.1013e-02
Epoch 7/10
15/15 - 7s - loss: 1116.5764 - loglik: -1.1168e+03 - logprior: 0.2247
Epoch 8/10
15/15 - 7s - loss: 1119.1531 - loglik: -1.1196e+03 - logprior: 0.4274
Fitted a model with MAP estimate = -1116.6727
expansions: [(29, 1), (45, 1), (58, 1), (64, 1), (77, 3), (78, 1), (79, 15), (80, 3), (84, 1), (85, 1), (100, 1), (101, 1), (103, 3), (121, 2), (132, 2), (133, 1), (134, 7), (136, 1), (156, 1), (157, 6), (173, 7), (183, 6)]
discards: [  1   2   6 196 197 198 223]
Re-initialized the encoder parameters.
Fitting a model of length 284 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 1124.4788 - loglik: -1.1058e+03 - logprior: -1.8662e+01
Epoch 2/2
15/15 - 9s - loss: 1082.3411 - loglik: -1.0825e+03 - logprior: 0.1224
Fitted a model with MAP estimate = -1081.4326
expansions: [(0, 3), (90, 2), (92, 2), (203, 1)]
discards: [131 151 230 243]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 1108.0476 - loglik: -1.0835e+03 - logprior: -2.4528e+01
Epoch 2/2
15/15 - 10s - loss: 1085.7231 - loglik: -1.0840e+03 - logprior: -1.7677e+00
Fitted a model with MAP estimate = -1076.3304
expansions: [(207, 2)]
discards: [  0   1   2  26 169 234 235]
Re-initialized the encoder parameters.
Fitting a model of length 283 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 13s - loss: 1099.2192 - loglik: -1.0821e+03 - logprior: -1.7138e+01
Epoch 2/10
15/15 - 9s - loss: 1076.6779 - loglik: -1.0785e+03 - logprior: 1.8160
Epoch 3/10
15/15 - 9s - loss: 1079.0909 - loglik: -1.0837e+03 - logprior: 4.6344
Fitted a model with MAP estimate = -1072.7812
Time for alignment: 157.1531
Computed alignments with likelihoods: ['-1068.1826', '-1072.5131', '-1067.3089', '-1068.4157', '-1072.7812']
Best model has likelihood: -1067.3089  (prior= 6.1255 )
time for generating output: 0.3574
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf10.projection.fasta
SP score = 0.9258510279743849
Training of 5 independent models on file il8.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b13e8ea00>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f59204aaca0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5afee67f70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b12eb5dc0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f590044e160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5334665d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5ca73c94f0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5b136b2610>, <__main__.SimpleDirichletPrior object at 0x7f5b09f4d790>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 419.6828 - loglik: -3.7471e+02 - logprior: -4.4975e+01
Epoch 2/10
10/10 - 1s - loss: 369.8286 - loglik: -3.5791e+02 - logprior: -1.1923e+01
Epoch 3/10
10/10 - 1s - loss: 350.2422 - loglik: -3.4445e+02 - logprior: -5.7895e+00
Epoch 4/10
10/10 - 1s - loss: 340.5770 - loglik: -3.3690e+02 - logprior: -3.6806e+00
Epoch 5/10
10/10 - 1s - loss: 336.6081 - loglik: -3.3389e+02 - logprior: -2.7177e+00
Epoch 6/10
10/10 - 1s - loss: 335.1098 - loglik: -3.3290e+02 - logprior: -2.2075e+00
Epoch 7/10
10/10 - 1s - loss: 334.4839 - loglik: -3.3272e+02 - logprior: -1.7686e+00
Epoch 8/10
10/10 - 1s - loss: 333.8755 - loglik: -3.3247e+02 - logprior: -1.4066e+00
Epoch 9/10
10/10 - 1s - loss: 333.6125 - loglik: -3.3239e+02 - logprior: -1.2205e+00
Epoch 10/10
10/10 - 1s - loss: 333.3317 - loglik: -3.3220e+02 - logprior: -1.1292e+00
Fitted a model with MAP estimate = -333.3167
expansions: [(0, 3), (16, 1), (17, 1), (18, 3), (20, 1), (24, 1), (28, 1), (38, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 384.9682 - loglik: -3.2935e+02 - logprior: -5.5619e+01
Epoch 2/2
10/10 - 1s - loss: 340.7819 - loglik: -3.2396e+02 - logprior: -1.6826e+01
Fitted a model with MAP estimate = -333.1246
expansions: []
discards: [ 0 24]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 371.2881 - loglik: -3.2414e+02 - logprior: -4.7146e+01
Epoch 2/2
10/10 - 1s - loss: 342.0860 - loglik: -3.2378e+02 - logprior: -1.8306e+01
Fitted a model with MAP estimate = -337.1110
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 367.1218 - loglik: -3.2341e+02 - logprior: -4.3712e+01
Epoch 2/10
10/10 - 1s - loss: 335.0149 - loglik: -3.2250e+02 - logprior: -1.2511e+01
Epoch 3/10
10/10 - 1s - loss: 326.8859 - loglik: -3.2224e+02 - logprior: -4.6481e+00
Epoch 4/10
10/10 - 1s - loss: 323.9833 - loglik: -3.2191e+02 - logprior: -2.0706e+00
Epoch 5/10
10/10 - 1s - loss: 322.8299 - loglik: -3.2184e+02 - logprior: -9.9090e-01
Epoch 6/10
10/10 - 1s - loss: 322.0921 - loglik: -3.2172e+02 - logprior: -3.7094e-01
Epoch 7/10
10/10 - 1s - loss: 321.5348 - loglik: -3.2172e+02 - logprior: 0.1829
Epoch 8/10
10/10 - 1s - loss: 321.4848 - loglik: -3.2205e+02 - logprior: 0.5630
Epoch 9/10
10/10 - 1s - loss: 320.9998 - loglik: -3.2175e+02 - logprior: 0.7522
Epoch 10/10
10/10 - 1s - loss: 321.2202 - loglik: -3.2212e+02 - logprior: 0.9005
Fitted a model with MAP estimate = -320.9587
Time for alignment: 35.5725
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 419.6915 - loglik: -3.7471e+02 - logprior: -4.4977e+01
Epoch 2/10
10/10 - 1s - loss: 370.0628 - loglik: -3.5814e+02 - logprior: -1.1920e+01
Epoch 3/10
10/10 - 1s - loss: 350.1341 - loglik: -3.4438e+02 - logprior: -5.7574e+00
Epoch 4/10
10/10 - 1s - loss: 341.5002 - loglik: -3.3785e+02 - logprior: -3.6506e+00
Epoch 5/10
10/10 - 1s - loss: 337.7585 - loglik: -3.3506e+02 - logprior: -2.7003e+00
Epoch 6/10
10/10 - 1s - loss: 336.0322 - loglik: -3.3384e+02 - logprior: -2.1938e+00
Epoch 7/10
10/10 - 1s - loss: 335.1055 - loglik: -3.3334e+02 - logprior: -1.7615e+00
Epoch 8/10
10/10 - 1s - loss: 334.5515 - loglik: -3.3318e+02 - logprior: -1.3713e+00
Epoch 9/10
10/10 - 1s - loss: 334.4214 - loglik: -3.3324e+02 - logprior: -1.1836e+00
Epoch 10/10
10/10 - 1s - loss: 334.0266 - loglik: -3.3292e+02 - logprior: -1.1020e+00
Fitted a model with MAP estimate = -333.9815
expansions: [(0, 3), (16, 1), (17, 1), (18, 3), (20, 1), (21, 1), (24, 1), (38, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 385.3288 - loglik: -3.2971e+02 - logprior: -5.5616e+01
Epoch 2/2
10/10 - 1s - loss: 340.9036 - loglik: -3.2407e+02 - logprior: -1.6836e+01
Fitted a model with MAP estimate = -333.0702
expansions: []
discards: [ 0 24]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 371.0580 - loglik: -3.2388e+02 - logprior: -4.7176e+01
Epoch 2/2
10/10 - 1s - loss: 341.7156 - loglik: -3.2338e+02 - logprior: -1.8338e+01
Fitted a model with MAP estimate = -336.8579
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 366.7444 - loglik: -3.2301e+02 - logprior: -4.3735e+01
Epoch 2/10
10/10 - 1s - loss: 335.0021 - loglik: -3.2249e+02 - logprior: -1.2513e+01
Epoch 3/10
10/10 - 1s - loss: 327.0720 - loglik: -3.2241e+02 - logprior: -4.6638e+00
Epoch 4/10
10/10 - 1s - loss: 323.7975 - loglik: -3.2172e+02 - logprior: -2.0751e+00
Epoch 5/10
10/10 - 1s - loss: 322.4777 - loglik: -3.2148e+02 - logprior: -1.0014e+00
Epoch 6/10
10/10 - 1s - loss: 321.9747 - loglik: -3.2158e+02 - logprior: -3.9511e-01
Epoch 7/10
10/10 - 1s - loss: 321.5249 - loglik: -3.2168e+02 - logprior: 0.1556
Epoch 8/10
10/10 - 1s - loss: 321.4028 - loglik: -3.2193e+02 - logprior: 0.5287
Epoch 9/10
10/10 - 1s - loss: 320.9948 - loglik: -3.2171e+02 - logprior: 0.7176
Epoch 10/10
10/10 - 1s - loss: 321.0487 - loglik: -3.2191e+02 - logprior: 0.8657
Fitted a model with MAP estimate = -320.9035
Time for alignment: 35.1703
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 419.9192 - loglik: -3.7494e+02 - logprior: -4.4977e+01
Epoch 2/10
10/10 - 1s - loss: 369.7256 - loglik: -3.5780e+02 - logprior: -1.1922e+01
Epoch 3/10
10/10 - 1s - loss: 350.7911 - loglik: -3.4496e+02 - logprior: -5.8317e+00
Epoch 4/10
10/10 - 1s - loss: 341.0544 - loglik: -3.3716e+02 - logprior: -3.8967e+00
Epoch 5/10
10/10 - 1s - loss: 336.5197 - loglik: -3.3356e+02 - logprior: -2.9646e+00
Epoch 6/10
10/10 - 1s - loss: 334.4066 - loglik: -3.3190e+02 - logprior: -2.5098e+00
Epoch 7/10
10/10 - 1s - loss: 333.3529 - loglik: -3.3127e+02 - logprior: -2.0875e+00
Epoch 8/10
10/10 - 1s - loss: 333.0914 - loglik: -3.3135e+02 - logprior: -1.7445e+00
Epoch 9/10
10/10 - 1s - loss: 332.2412 - loglik: -3.3063e+02 - logprior: -1.6136e+00
Epoch 10/10
10/10 - 1s - loss: 332.1193 - loglik: -3.3053e+02 - logprior: -1.5869e+00
Fitted a model with MAP estimate = -332.0570
expansions: [(0, 2), (1, 1), (16, 1), (17, 1), (19, 1), (20, 2), (24, 1), (28, 1), (37, 1), (38, 1), (42, 1), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 384.4814 - loglik: -3.2889e+02 - logprior: -5.5596e+01
Epoch 2/2
10/10 - 1s - loss: 340.2404 - loglik: -3.2359e+02 - logprior: -1.6655e+01
Fitted a model with MAP estimate = -332.7570
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 370.6020 - loglik: -3.2347e+02 - logprior: -4.7133e+01
Epoch 2/2
10/10 - 1s - loss: 341.9244 - loglik: -3.2361e+02 - logprior: -1.8314e+01
Fitted a model with MAP estimate = -336.7247
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 365.5722 - loglik: -3.2298e+02 - logprior: -4.2592e+01
Epoch 2/10
10/10 - 1s - loss: 333.6391 - loglik: -3.2209e+02 - logprior: -1.1545e+01
Epoch 3/10
10/10 - 1s - loss: 326.6294 - loglik: -3.2218e+02 - logprior: -4.4467e+00
Epoch 4/10
10/10 - 1s - loss: 323.8448 - loglik: -3.2186e+02 - logprior: -1.9825e+00
Epoch 5/10
10/10 - 1s - loss: 322.6389 - loglik: -3.2172e+02 - logprior: -9.2136e-01
Epoch 6/10
10/10 - 1s - loss: 322.0353 - loglik: -3.2164e+02 - logprior: -4.0029e-01
Epoch 7/10
10/10 - 1s - loss: 321.4966 - loglik: -3.2162e+02 - logprior: 0.1239
Epoch 8/10
10/10 - 1s - loss: 321.0929 - loglik: -3.2163e+02 - logprior: 0.5417
Epoch 9/10
10/10 - 1s - loss: 321.4040 - loglik: -3.2216e+02 - logprior: 0.7512
Fitted a model with MAP estimate = -321.0175
Time for alignment: 33.5626
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 419.5749 - loglik: -3.7460e+02 - logprior: -4.4976e+01
Epoch 2/10
10/10 - 1s - loss: 370.2112 - loglik: -3.5829e+02 - logprior: -1.1925e+01
Epoch 3/10
10/10 - 1s - loss: 350.2623 - loglik: -3.4449e+02 - logprior: -5.7756e+00
Epoch 4/10
10/10 - 1s - loss: 340.8419 - loglik: -3.3714e+02 - logprior: -3.7004e+00
Epoch 5/10
10/10 - 1s - loss: 337.0086 - loglik: -3.3427e+02 - logprior: -2.7426e+00
Epoch 6/10
10/10 - 1s - loss: 335.6561 - loglik: -3.3345e+02 - logprior: -2.2110e+00
Epoch 7/10
10/10 - 1s - loss: 334.7649 - loglik: -3.3300e+02 - logprior: -1.7623e+00
Epoch 8/10
10/10 - 1s - loss: 333.5629 - loglik: -3.3213e+02 - logprior: -1.4301e+00
Epoch 9/10
10/10 - 1s - loss: 333.4233 - loglik: -3.3214e+02 - logprior: -1.2832e+00
Epoch 10/10
10/10 - 1s - loss: 332.9789 - loglik: -3.3177e+02 - logprior: -1.2106e+00
Fitted a model with MAP estimate = -332.9127
expansions: [(0, 2), (1, 1), (17, 1), (18, 3), (20, 1), (21, 1), (24, 1), (37, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 384.7213 - loglik: -3.2905e+02 - logprior: -5.5674e+01
Epoch 2/2
10/10 - 1s - loss: 341.0852 - loglik: -3.2432e+02 - logprior: -1.6766e+01
Fitted a model with MAP estimate = -333.2549
expansions: [(19, 1)]
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 371.3344 - loglik: -3.2406e+02 - logprior: -4.7278e+01
Epoch 2/2
10/10 - 1s - loss: 342.1048 - loglik: -3.2379e+02 - logprior: -1.8310e+01
Fitted a model with MAP estimate = -336.9513
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 365.5665 - loglik: -3.2303e+02 - logprior: -4.2535e+01
Epoch 2/10
10/10 - 1s - loss: 334.2635 - loglik: -3.2277e+02 - logprior: -1.1498e+01
Epoch 3/10
10/10 - 1s - loss: 326.9749 - loglik: -3.2256e+02 - logprior: -4.4133e+00
Epoch 4/10
10/10 - 1s - loss: 323.8065 - loglik: -3.2183e+02 - logprior: -1.9744e+00
Epoch 5/10
10/10 - 1s - loss: 322.3842 - loglik: -3.2146e+02 - logprior: -9.2807e-01
Epoch 6/10
10/10 - 1s - loss: 321.8022 - loglik: -3.2142e+02 - logprior: -3.7732e-01
Epoch 7/10
10/10 - 1s - loss: 321.5879 - loglik: -3.2176e+02 - logprior: 0.1764
Epoch 8/10
10/10 - 1s - loss: 321.2099 - loglik: -3.2179e+02 - logprior: 0.5823
Epoch 9/10
10/10 - 1s - loss: 320.9516 - loglik: -3.2173e+02 - logprior: 0.7797
Epoch 10/10
10/10 - 1s - loss: 320.7899 - loglik: -3.2170e+02 - logprior: 0.9139
Fitted a model with MAP estimate = -320.7785
Time for alignment: 35.2498
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 419.6753 - loglik: -3.7470e+02 - logprior: -4.4977e+01
Epoch 2/10
10/10 - 1s - loss: 370.2825 - loglik: -3.5836e+02 - logprior: -1.1920e+01
Epoch 3/10
10/10 - 1s - loss: 350.4636 - loglik: -3.4469e+02 - logprior: -5.7736e+00
Epoch 4/10
10/10 - 1s - loss: 340.3862 - loglik: -3.3670e+02 - logprior: -3.6833e+00
Epoch 5/10
10/10 - 1s - loss: 336.4501 - loglik: -3.3374e+02 - logprior: -2.7051e+00
Epoch 6/10
10/10 - 1s - loss: 334.9084 - loglik: -3.3272e+02 - logprior: -2.1928e+00
Epoch 7/10
10/10 - 1s - loss: 334.2777 - loglik: -3.3250e+02 - logprior: -1.7753e+00
Epoch 8/10
10/10 - 1s - loss: 333.7523 - loglik: -3.3235e+02 - logprior: -1.4059e+00
Epoch 9/10
10/10 - 1s - loss: 333.1393 - loglik: -3.3192e+02 - logprior: -1.2223e+00
Epoch 10/10
10/10 - 1s - loss: 333.0770 - loglik: -3.3193e+02 - logprior: -1.1468e+00
Fitted a model with MAP estimate = -333.1219
expansions: [(0, 3), (16, 1), (17, 1), (18, 3), (22, 1), (24, 1), (28, 1), (38, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 384.6789 - loglik: -3.2906e+02 - logprior: -5.5621e+01
Epoch 2/2
10/10 - 1s - loss: 340.6649 - loglik: -3.2383e+02 - logprior: -1.6831e+01
Fitted a model with MAP estimate = -332.7252
expansions: []
discards: [ 0 24]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 370.9526 - loglik: -3.2382e+02 - logprior: -4.7136e+01
Epoch 2/2
10/10 - 1s - loss: 341.5609 - loglik: -3.2328e+02 - logprior: -1.8282e+01
Fitted a model with MAP estimate = -336.7269
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 366.7527 - loglik: -3.2307e+02 - logprior: -4.3679e+01
Epoch 2/10
10/10 - 1s - loss: 334.8968 - loglik: -3.2245e+02 - logprior: -1.2451e+01
Epoch 3/10
10/10 - 1s - loss: 326.6015 - loglik: -3.2199e+02 - logprior: -4.6081e+00
Epoch 4/10
10/10 - 1s - loss: 323.7187 - loglik: -3.2170e+02 - logprior: -2.0187e+00
Epoch 5/10
10/10 - 1s - loss: 322.2816 - loglik: -3.2133e+02 - logprior: -9.4683e-01
Epoch 6/10
10/10 - 1s - loss: 321.8928 - loglik: -3.2155e+02 - logprior: -3.4192e-01
Epoch 7/10
10/10 - 1s - loss: 321.5236 - loglik: -3.2173e+02 - logprior: 0.2068
Epoch 8/10
10/10 - 1s - loss: 321.0766 - loglik: -3.2166e+02 - logprior: 0.5790
Epoch 9/10
10/10 - 1s - loss: 320.8105 - loglik: -3.2158e+02 - logprior: 0.7691
Epoch 10/10
10/10 - 1s - loss: 320.8834 - loglik: -3.2180e+02 - logprior: 0.9124
Fitted a model with MAP estimate = -320.7459
Time for alignment: 34.1510
Computed alignments with likelihoods: ['-320.9587', '-320.9035', '-321.0175', '-320.7785', '-320.7459']
Best model has likelihood: -320.7459  (prior= 0.9994 )
time for generating output: 0.1267
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/il8.projection.fasta
SP score = 0.902232269002543
Training of 5 independent models on file cytb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f59000fff40>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f531d2cccd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b134c9760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5aff8198b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5334975820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f59406ea9d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f59007695b0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5334ac8550>, <__main__.SimpleDirichletPrior object at 0x7f5b09596d30>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 492.1176 - loglik: -4.7772e+02 - logprior: -1.4402e+01
Epoch 2/10
11/11 - 2s - loss: 461.5222 - loglik: -4.5786e+02 - logprior: -3.6614e+00
Epoch 3/10
11/11 - 2s - loss: 440.6955 - loglik: -4.3847e+02 - logprior: -2.2260e+00
Epoch 4/10
11/11 - 2s - loss: 430.0382 - loglik: -4.2797e+02 - logprior: -2.0650e+00
Epoch 5/10
11/11 - 2s - loss: 426.9310 - loglik: -4.2495e+02 - logprior: -1.9778e+00
Epoch 6/10
11/11 - 2s - loss: 427.1758 - loglik: -4.2536e+02 - logprior: -1.8113e+00
Fitted a model with MAP estimate = -425.6568
expansions: [(11, 4), (12, 2), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 439.8968 - loglik: -4.2457e+02 - logprior: -1.5324e+01
Epoch 2/2
11/11 - 2s - loss: 424.5740 - loglik: -4.1817e+02 - logprior: -6.4089e+00
Fitted a model with MAP estimate = -421.8111
expansions: [(0, 2)]
discards: [ 0 15 43]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 427.9038 - loglik: -4.1606e+02 - logprior: -1.1841e+01
Epoch 2/2
11/11 - 2s - loss: 418.3962 - loglik: -4.1532e+02 - logprior: -3.0810e+00
Fitted a model with MAP estimate = -417.0575
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 431.4778 - loglik: -4.1727e+02 - logprior: -1.4210e+01
Epoch 2/10
11/11 - 2s - loss: 420.7227 - loglik: -4.1643e+02 - logprior: -4.2882e+00
Epoch 3/10
11/11 - 2s - loss: 416.5669 - loglik: -4.1446e+02 - logprior: -2.1070e+00
Epoch 4/10
11/11 - 2s - loss: 416.5752 - loglik: -4.1530e+02 - logprior: -1.2791e+00
Fitted a model with MAP estimate = -415.7648
Time for alignment: 46.2029
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 491.9047 - loglik: -4.7751e+02 - logprior: -1.4398e+01
Epoch 2/10
11/11 - 2s - loss: 462.0623 - loglik: -4.5841e+02 - logprior: -3.6537e+00
Epoch 3/10
11/11 - 2s - loss: 438.3295 - loglik: -4.3612e+02 - logprior: -2.2076e+00
Epoch 4/10
11/11 - 2s - loss: 431.7578 - loglik: -4.2967e+02 - logprior: -2.0875e+00
Epoch 5/10
11/11 - 2s - loss: 427.0416 - loglik: -4.2500e+02 - logprior: -2.0427e+00
Epoch 6/10
11/11 - 2s - loss: 425.8839 - loglik: -4.2401e+02 - logprior: -1.8706e+00
Epoch 7/10
11/11 - 2s - loss: 426.2421 - loglik: -4.2454e+02 - logprior: -1.7030e+00
Fitted a model with MAP estimate = -425.4290
expansions: [(8, 2), (10, 2), (12, 1), (24, 1), (26, 2), (32, 1), (35, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 439.5166 - loglik: -4.2422e+02 - logprior: -1.5296e+01
Epoch 2/2
11/11 - 2s - loss: 424.1237 - loglik: -4.1774e+02 - logprior: -6.3817e+00
Fitted a model with MAP estimate = -421.7464
expansions: [(0, 2)]
discards: [ 0 32]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 429.0408 - loglik: -4.1720e+02 - logprior: -1.1838e+01
Epoch 2/2
11/11 - 2s - loss: 417.7086 - loglik: -4.1460e+02 - logprior: -3.1119e+00
Fitted a model with MAP estimate = -417.1314
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 432.1696 - loglik: -4.1793e+02 - logprior: -1.4237e+01
Epoch 2/10
11/11 - 2s - loss: 418.4006 - loglik: -4.1409e+02 - logprior: -4.3071e+00
Epoch 3/10
11/11 - 2s - loss: 418.4034 - loglik: -4.1629e+02 - logprior: -2.1115e+00
Fitted a model with MAP estimate = -416.5981
Time for alignment: 46.3777
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 492.3055 - loglik: -4.7791e+02 - logprior: -1.4399e+01
Epoch 2/10
11/11 - 2s - loss: 461.3046 - loglik: -4.5764e+02 - logprior: -3.6628e+00
Epoch 3/10
11/11 - 2s - loss: 441.4473 - loglik: -4.3920e+02 - logprior: -2.2463e+00
Epoch 4/10
11/11 - 2s - loss: 430.5280 - loglik: -4.2840e+02 - logprior: -2.1238e+00
Epoch 5/10
11/11 - 2s - loss: 427.9233 - loglik: -4.2589e+02 - logprior: -2.0331e+00
Epoch 6/10
11/11 - 2s - loss: 426.6949 - loglik: -4.2482e+02 - logprior: -1.8750e+00
Epoch 7/10
11/11 - 2s - loss: 424.6863 - loglik: -4.2294e+02 - logprior: -1.7451e+00
Epoch 8/10
11/11 - 2s - loss: 425.4258 - loglik: -4.2373e+02 - logprior: -1.6971e+00
Fitted a model with MAP estimate = -424.9310
expansions: [(8, 2), (10, 2), (12, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 439.7800 - loglik: -4.2448e+02 - logprior: -1.5303e+01
Epoch 2/2
11/11 - 2s - loss: 421.8878 - loglik: -4.1548e+02 - logprior: -6.4126e+00
Fitted a model with MAP estimate = -421.6354
expansions: [(0, 2)]
discards: [ 0 42]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 426.7762 - loglik: -4.1493e+02 - logprior: -1.1844e+01
Epoch 2/2
11/11 - 2s - loss: 419.6567 - loglik: -4.1658e+02 - logprior: -3.0791e+00
Fitted a model with MAP estimate = -417.0749
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 431.2551 - loglik: -4.1701e+02 - logprior: -1.4246e+01
Epoch 2/10
11/11 - 2s - loss: 420.8076 - loglik: -4.1648e+02 - logprior: -4.3254e+00
Epoch 3/10
11/11 - 2s - loss: 415.7025 - loglik: -4.1358e+02 - logprior: -2.1180e+00
Epoch 4/10
11/11 - 2s - loss: 417.7325 - loglik: -4.1645e+02 - logprior: -1.2792e+00
Fitted a model with MAP estimate = -415.8894
Time for alignment: 48.4767
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 492.8217 - loglik: -4.7842e+02 - logprior: -1.4399e+01
Epoch 2/10
11/11 - 2s - loss: 461.5586 - loglik: -4.5790e+02 - logprior: -3.6609e+00
Epoch 3/10
11/11 - 2s - loss: 439.6700 - loglik: -4.3741e+02 - logprior: -2.2609e+00
Epoch 4/10
11/11 - 2s - loss: 429.5056 - loglik: -4.2735e+02 - logprior: -2.1532e+00
Epoch 5/10
11/11 - 2s - loss: 426.7799 - loglik: -4.2473e+02 - logprior: -2.0500e+00
Epoch 6/10
11/11 - 2s - loss: 427.2788 - loglik: -4.2540e+02 - logprior: -1.8759e+00
Fitted a model with MAP estimate = -425.4994
expansions: [(8, 2), (10, 2), (12, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 439.2165 - loglik: -4.2391e+02 - logprior: -1.5302e+01
Epoch 2/2
11/11 - 2s - loss: 423.8666 - loglik: -4.1750e+02 - logprior: -6.3637e+00
Fitted a model with MAP estimate = -421.6504
expansions: [(0, 2)]
discards: [ 0 42]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 428.5288 - loglik: -4.1669e+02 - logprior: -1.1834e+01
Epoch 2/2
11/11 - 2s - loss: 418.0916 - loglik: -4.1501e+02 - logprior: -3.0805e+00
Fitted a model with MAP estimate = -416.9943
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 431.6553 - loglik: -4.1742e+02 - logprior: -1.4233e+01
Epoch 2/10
11/11 - 2s - loss: 419.2153 - loglik: -4.1490e+02 - logprior: -4.3105e+00
Epoch 3/10
11/11 - 2s - loss: 418.2149 - loglik: -4.1610e+02 - logprior: -2.1171e+00
Epoch 4/10
11/11 - 2s - loss: 415.6841 - loglik: -4.1442e+02 - logprior: -1.2608e+00
Epoch 5/10
11/11 - 2s - loss: 415.2271 - loglik: -4.1419e+02 - logprior: -1.0324e+00
Epoch 6/10
11/11 - 2s - loss: 416.2460 - loglik: -4.1543e+02 - logprior: -8.1923e-01
Fitted a model with MAP estimate = -414.9303
Time for alignment: 48.5413
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 492.0198 - loglik: -4.7762e+02 - logprior: -1.4403e+01
Epoch 2/10
11/11 - 2s - loss: 461.6702 - loglik: -4.5800e+02 - logprior: -3.6670e+00
Epoch 3/10
11/11 - 2s - loss: 440.0985 - loglik: -4.3785e+02 - logprior: -2.2458e+00
Epoch 4/10
11/11 - 2s - loss: 430.4314 - loglik: -4.2829e+02 - logprior: -2.1367e+00
Epoch 5/10
11/11 - 2s - loss: 427.6333 - loglik: -4.2557e+02 - logprior: -2.0631e+00
Epoch 6/10
11/11 - 2s - loss: 426.1752 - loglik: -4.2427e+02 - logprior: -1.9016e+00
Epoch 7/10
11/11 - 2s - loss: 424.5022 - loglik: -4.2274e+02 - logprior: -1.7575e+00
Epoch 8/10
11/11 - 2s - loss: 425.1253 - loglik: -4.2340e+02 - logprior: -1.7258e+00
Fitted a model with MAP estimate = -424.8135
expansions: [(8, 3), (9, 1), (10, 1), (12, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 439.9875 - loglik: -4.2465e+02 - logprior: -1.5342e+01
Epoch 2/2
11/11 - 2s - loss: 424.4095 - loglik: -4.1794e+02 - logprior: -6.4651e+00
Fitted a model with MAP estimate = -422.1377
expansions: [(0, 2)]
discards: [ 0  8 43]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 428.2070 - loglik: -4.1637e+02 - logprior: -1.1833e+01
Epoch 2/2
11/11 - 2s - loss: 418.4110 - loglik: -4.1534e+02 - logprior: -3.0667e+00
Fitted a model with MAP estimate = -417.0675
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 431.9461 - loglik: -4.1776e+02 - logprior: -1.4185e+01
Epoch 2/10
11/11 - 2s - loss: 420.1273 - loglik: -4.1585e+02 - logprior: -4.2805e+00
Epoch 3/10
11/11 - 2s - loss: 416.9060 - loglik: -4.1482e+02 - logprior: -2.0861e+00
Epoch 4/10
11/11 - 2s - loss: 415.9641 - loglik: -4.1471e+02 - logprior: -1.2542e+00
Epoch 5/10
11/11 - 2s - loss: 414.9814 - loglik: -4.1396e+02 - logprior: -1.0169e+00
Epoch 6/10
11/11 - 2s - loss: 415.0454 - loglik: -4.1421e+02 - logprior: -8.3418e-01
Fitted a model with MAP estimate = -414.8790
Time for alignment: 51.7279
Computed alignments with likelihoods: ['-415.7648', '-416.5981', '-415.8894', '-414.9303', '-414.8790']
Best model has likelihood: -414.8790  (prior= -0.7857 )
time for generating output: 0.1394
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cytb.projection.fasta
SP score = 0.8234804329725229
Training of 5 independent models on file kringle.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5ca7aa63a0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5afe458d30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5964708c10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b09e6cd00>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f53349b8fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5334842910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b134c9760>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5c8463f3a0>, <__main__.SimpleDirichletPrior object at 0x7f5b13c1ad00>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 504.8581 - loglik: -4.6033e+02 - logprior: -4.4531e+01
Epoch 2/10
10/10 - 1s - loss: 444.8508 - loglik: -4.3323e+02 - logprior: -1.1626e+01
Epoch 3/10
10/10 - 1s - loss: 413.2704 - loglik: -4.0754e+02 - logprior: -5.7349e+00
Epoch 4/10
10/10 - 1s - loss: 391.8163 - loglik: -3.8791e+02 - logprior: -3.9019e+00
Epoch 5/10
10/10 - 1s - loss: 383.6657 - loglik: -3.8054e+02 - logprior: -3.1252e+00
Epoch 6/10
10/10 - 1s - loss: 381.6051 - loglik: -3.7899e+02 - logprior: -2.6149e+00
Epoch 7/10
10/10 - 1s - loss: 380.3828 - loglik: -3.7811e+02 - logprior: -2.2712e+00
Epoch 8/10
10/10 - 1s - loss: 379.9240 - loglik: -3.7784e+02 - logprior: -2.0844e+00
Epoch 9/10
10/10 - 1s - loss: 379.2973 - loglik: -3.7733e+02 - logprior: -1.9651e+00
Epoch 10/10
10/10 - 1s - loss: 378.6212 - loglik: -3.7671e+02 - logprior: -1.9094e+00
Fitted a model with MAP estimate = -378.7490
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 407.8130 - loglik: -3.6977e+02 - logprior: -3.8046e+01
Epoch 2/2
10/10 - 1s - loss: 373.7325 - loglik: -3.6390e+02 - logprior: -9.8347e+00
Fitted a model with MAP estimate = -368.6538
expansions: []
discards: [ 0 46 59 66]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 413.3657 - loglik: -3.6809e+02 - logprior: -4.5272e+01
Epoch 2/2
10/10 - 1s - loss: 384.9020 - loglik: -3.6642e+02 - logprior: -1.8486e+01
Fitted a model with MAP estimate = -381.2593
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 401.0591 - loglik: -3.6460e+02 - logprior: -3.6463e+01
Epoch 2/10
10/10 - 1s - loss: 372.2827 - loglik: -3.6324e+02 - logprior: -9.0457e+00
Epoch 3/10
10/10 - 1s - loss: 366.3221 - loglik: -3.6284e+02 - logprior: -3.4844e+00
Epoch 4/10
10/10 - 1s - loss: 364.3046 - loglik: -3.6303e+02 - logprior: -1.2750e+00
Epoch 5/10
10/10 - 1s - loss: 362.8903 - loglik: -3.6276e+02 - logprior: -1.2683e-01
Epoch 6/10
10/10 - 1s - loss: 362.8072 - loglik: -3.6329e+02 - logprior: 0.4877
Epoch 7/10
10/10 - 1s - loss: 362.2794 - loglik: -3.6311e+02 - logprior: 0.8338
Epoch 8/10
10/10 - 1s - loss: 362.2898 - loglik: -3.6337e+02 - logprior: 1.0771
Fitted a model with MAP estimate = -362.0604
Time for alignment: 37.2830
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 504.8416 - loglik: -4.6031e+02 - logprior: -4.4531e+01
Epoch 2/10
10/10 - 1s - loss: 445.2929 - loglik: -4.3367e+02 - logprior: -1.1628e+01
Epoch 3/10
10/10 - 1s - loss: 414.7249 - loglik: -4.0896e+02 - logprior: -5.7655e+00
Epoch 4/10
10/10 - 1s - loss: 392.7529 - loglik: -3.8881e+02 - logprior: -3.9415e+00
Epoch 5/10
10/10 - 1s - loss: 384.4147 - loglik: -3.8121e+02 - logprior: -3.2075e+00
Epoch 6/10
10/10 - 1s - loss: 381.5853 - loglik: -3.7889e+02 - logprior: -2.6957e+00
Epoch 7/10
10/10 - 1s - loss: 380.5600 - loglik: -3.7823e+02 - logprior: -2.3297e+00
Epoch 8/10
10/10 - 1s - loss: 379.3530 - loglik: -3.7722e+02 - logprior: -2.1280e+00
Epoch 9/10
10/10 - 1s - loss: 379.1560 - loglik: -3.7714e+02 - logprior: -2.0203e+00
Epoch 10/10
10/10 - 1s - loss: 379.0002 - loglik: -3.7706e+02 - logprior: -1.9361e+00
Fitted a model with MAP estimate = -378.7081
expansions: [(2, 1), (5, 2), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 408.9745 - loglik: -3.7094e+02 - logprior: -3.8038e+01
Epoch 2/2
10/10 - 1s - loss: 374.3223 - loglik: -3.6432e+02 - logprior: -1.0003e+01
Fitted a model with MAP estimate = -369.5956
expansions: []
discards: [45 58 65]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 399.4555 - loglik: -3.6351e+02 - logprior: -3.5948e+01
Epoch 2/2
10/10 - 1s - loss: 371.7227 - loglik: -3.6243e+02 - logprior: -9.2957e+00
Fitted a model with MAP estimate = -368.1885
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 411.9836 - loglik: -3.6698e+02 - logprior: -4.5002e+01
Epoch 2/10
10/10 - 1s - loss: 385.0849 - loglik: -3.6678e+02 - logprior: -1.8304e+01
Epoch 3/10
10/10 - 1s - loss: 378.1343 - loglik: -3.6531e+02 - logprior: -1.2828e+01
Epoch 4/10
10/10 - 1s - loss: 375.9511 - loglik: -3.6551e+02 - logprior: -1.0445e+01
Epoch 5/10
10/10 - 1s - loss: 372.5621 - loglik: -3.6510e+02 - logprior: -7.4630e+00
Epoch 6/10
10/10 - 1s - loss: 367.2517 - loglik: -3.6572e+02 - logprior: -1.5290e+00
Epoch 7/10
10/10 - 1s - loss: 365.0686 - loglik: -3.6564e+02 - logprior: 0.5739
Epoch 8/10
10/10 - 1s - loss: 365.1354 - loglik: -3.6608e+02 - logprior: 0.9407
Fitted a model with MAP estimate = -364.8477
Time for alignment: 36.8017
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 504.6855 - loglik: -4.6015e+02 - logprior: -4.4531e+01
Epoch 2/10
10/10 - 1s - loss: 445.3592 - loglik: -4.3373e+02 - logprior: -1.1628e+01
Epoch 3/10
10/10 - 1s - loss: 413.8318 - loglik: -4.0805e+02 - logprior: -5.7779e+00
Epoch 4/10
10/10 - 1s - loss: 392.6696 - loglik: -3.8873e+02 - logprior: -3.9393e+00
Epoch 5/10
10/10 - 1s - loss: 384.5306 - loglik: -3.8139e+02 - logprior: -3.1389e+00
Epoch 6/10
10/10 - 1s - loss: 381.7839 - loglik: -3.7912e+02 - logprior: -2.6649e+00
Epoch 7/10
10/10 - 1s - loss: 380.8840 - loglik: -3.7858e+02 - logprior: -2.3042e+00
Epoch 8/10
10/10 - 1s - loss: 379.5609 - loglik: -3.7747e+02 - logprior: -2.0923e+00
Epoch 9/10
10/10 - 1s - loss: 379.3213 - loglik: -3.7734e+02 - logprior: -1.9860e+00
Epoch 10/10
10/10 - 1s - loss: 378.9373 - loglik: -3.7703e+02 - logprior: -1.9100e+00
Fitted a model with MAP estimate = -378.7407
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 407.7638 - loglik: -3.6973e+02 - logprior: -3.8033e+01
Epoch 2/2
10/10 - 1s - loss: 373.4314 - loglik: -3.6360e+02 - logprior: -9.8298e+00
Fitted a model with MAP estimate = -368.6168
expansions: []
discards: [ 0 46 59 66]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 413.1625 - loglik: -3.6790e+02 - logprior: -4.5264e+01
Epoch 2/2
10/10 - 1s - loss: 385.2865 - loglik: -3.6681e+02 - logprior: -1.8477e+01
Fitted a model with MAP estimate = -381.2310
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 401.0019 - loglik: -3.6455e+02 - logprior: -3.6455e+01
Epoch 2/10
10/10 - 1s - loss: 372.3988 - loglik: -3.6335e+02 - logprior: -9.0460e+00
Epoch 3/10
10/10 - 1s - loss: 365.9920 - loglik: -3.6250e+02 - logprior: -3.4924e+00
Epoch 4/10
10/10 - 1s - loss: 364.5186 - loglik: -3.6324e+02 - logprior: -1.2750e+00
Epoch 5/10
10/10 - 1s - loss: 362.9474 - loglik: -3.6282e+02 - logprior: -1.2337e-01
Epoch 6/10
10/10 - 1s - loss: 362.9117 - loglik: -3.6340e+02 - logprior: 0.4848
Epoch 7/10
10/10 - 1s - loss: 362.2230 - loglik: -3.6306e+02 - logprior: 0.8359
Epoch 8/10
10/10 - 1s - loss: 362.1570 - loglik: -3.6323e+02 - logprior: 1.0750
Epoch 9/10
10/10 - 1s - loss: 362.2394 - loglik: -3.6352e+02 - logprior: 1.2761
Fitted a model with MAP estimate = -361.9305
Time for alignment: 38.3614
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 504.6641 - loglik: -4.6013e+02 - logprior: -4.4531e+01
Epoch 2/10
10/10 - 1s - loss: 445.6795 - loglik: -4.3405e+02 - logprior: -1.1630e+01
Epoch 3/10
10/10 - 1s - loss: 414.7143 - loglik: -4.0893e+02 - logprior: -5.7816e+00
Epoch 4/10
10/10 - 1s - loss: 393.3521 - loglik: -3.8941e+02 - logprior: -3.9410e+00
Epoch 5/10
10/10 - 1s - loss: 384.5181 - loglik: -3.8131e+02 - logprior: -3.2105e+00
Epoch 6/10
10/10 - 1s - loss: 381.6610 - loglik: -3.7898e+02 - logprior: -2.6834e+00
Epoch 7/10
10/10 - 1s - loss: 380.6128 - loglik: -3.7829e+02 - logprior: -2.3258e+00
Epoch 8/10
10/10 - 1s - loss: 379.5262 - loglik: -3.7739e+02 - logprior: -2.1360e+00
Epoch 9/10
10/10 - 1s - loss: 378.8568 - loglik: -3.7682e+02 - logprior: -2.0361e+00
Epoch 10/10
10/10 - 1s - loss: 378.9153 - loglik: -3.7697e+02 - logprior: -1.9452e+00
Fitted a model with MAP estimate = -378.7195
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 407.6208 - loglik: -3.6958e+02 - logprior: -3.8040e+01
Epoch 2/2
10/10 - 1s - loss: 373.6728 - loglik: -3.6384e+02 - logprior: -9.8291e+00
Fitted a model with MAP estimate = -368.6055
expansions: []
discards: [ 0 46 59 66]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 412.7753 - loglik: -3.6758e+02 - logprior: -4.5195e+01
Epoch 2/2
10/10 - 1s - loss: 385.5175 - loglik: -3.6705e+02 - logprior: -1.8463e+01
Fitted a model with MAP estimate = -381.1828
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 401.0899 - loglik: -3.6484e+02 - logprior: -3.6254e+01
Epoch 2/10
10/10 - 1s - loss: 372.1278 - loglik: -3.6313e+02 - logprior: -9.0022e+00
Epoch 3/10
10/10 - 1s - loss: 366.1465 - loglik: -3.6267e+02 - logprior: -3.4751e+00
Epoch 4/10
10/10 - 1s - loss: 364.1227 - loglik: -3.6285e+02 - logprior: -1.2688e+00
Epoch 5/10
10/10 - 1s - loss: 363.4221 - loglik: -3.6330e+02 - logprior: -1.2713e-01
Epoch 6/10
10/10 - 1s - loss: 362.5144 - loglik: -3.6300e+02 - logprior: 0.4812
Epoch 7/10
10/10 - 1s - loss: 362.3735 - loglik: -3.6320e+02 - logprior: 0.8281
Epoch 8/10
10/10 - 1s - loss: 362.0738 - loglik: -3.6315e+02 - logprior: 1.0733
Epoch 9/10
10/10 - 1s - loss: 361.9965 - loglik: -3.6328e+02 - logprior: 1.2814
Epoch 10/10
10/10 - 1s - loss: 361.9800 - loglik: -3.6343e+02 - logprior: 1.4483
Fitted a model with MAP estimate = -361.8022
Time for alignment: 38.6378
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 504.7935 - loglik: -4.6026e+02 - logprior: -4.4533e+01
Epoch 2/10
10/10 - 1s - loss: 445.2172 - loglik: -4.3359e+02 - logprior: -1.1632e+01
Epoch 3/10
10/10 - 1s - loss: 415.3709 - loglik: -4.0958e+02 - logprior: -5.7958e+00
Epoch 4/10
10/10 - 1s - loss: 393.5238 - loglik: -3.8955e+02 - logprior: -3.9784e+00
Epoch 5/10
10/10 - 1s - loss: 384.3347 - loglik: -3.8116e+02 - logprior: -3.1757e+00
Epoch 6/10
10/10 - 1s - loss: 381.6887 - loglik: -3.7897e+02 - logprior: -2.7145e+00
Epoch 7/10
10/10 - 1s - loss: 379.7618 - loglik: -3.7736e+02 - logprior: -2.4026e+00
Epoch 8/10
10/10 - 1s - loss: 378.9221 - loglik: -3.7676e+02 - logprior: -2.1602e+00
Epoch 9/10
10/10 - 1s - loss: 378.8749 - loglik: -3.7685e+02 - logprior: -2.0300e+00
Epoch 10/10
10/10 - 1s - loss: 378.2234 - loglik: -3.7628e+02 - logprior: -1.9425e+00
Fitted a model with MAP estimate = -378.1432
expansions: [(2, 1), (5, 2), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 1), (32, 2), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 408.0913 - loglik: -3.7005e+02 - logprior: -3.8044e+01
Epoch 2/2
10/10 - 1s - loss: 374.7700 - loglik: -3.6477e+02 - logprior: -9.9972e+00
Fitted a model with MAP estimate = -369.5984
expansions: []
discards: [58 65]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 398.9050 - loglik: -3.6292e+02 - logprior: -3.5987e+01
Epoch 2/2
10/10 - 1s - loss: 371.7068 - loglik: -3.6239e+02 - logprior: -9.3217e+00
Fitted a model with MAP estimate = -367.8615
expansions: [(3, 1)]
discards: [ 0 45]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 412.3086 - loglik: -3.6730e+02 - logprior: -4.5004e+01
Epoch 2/10
10/10 - 1s - loss: 384.5867 - loglik: -3.6628e+02 - logprior: -1.8306e+01
Epoch 3/10
10/10 - 1s - loss: 378.4405 - loglik: -3.6561e+02 - logprior: -1.2827e+01
Epoch 4/10
10/10 - 1s - loss: 375.9107 - loglik: -3.6546e+02 - logprior: -1.0454e+01
Epoch 5/10
10/10 - 1s - loss: 373.3408 - loglik: -3.6577e+02 - logprior: -7.5736e+00
Epoch 6/10
10/10 - 1s - loss: 367.1348 - loglik: -3.6549e+02 - logprior: -1.6469e+00
Epoch 7/10
10/10 - 1s - loss: 365.4748 - loglik: -3.6603e+02 - logprior: 0.5531
Epoch 8/10
10/10 - 1s - loss: 365.1033 - loglik: -3.6603e+02 - logprior: 0.9301
Epoch 9/10
10/10 - 1s - loss: 364.7002 - loglik: -3.6582e+02 - logprior: 1.1223
Epoch 10/10
10/10 - 1s - loss: 364.8967 - loglik: -3.6618e+02 - logprior: 1.2791
Fitted a model with MAP estimate = -364.6988
Time for alignment: 38.1161
Computed alignments with likelihoods: ['-362.0604', '-364.8477', '-361.9305', '-361.8022', '-364.6988']
Best model has likelihood: -361.8022  (prior= 1.5353 )
time for generating output: 0.1450
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kringle.projection.fasta
SP score = 0.8564831261101243
Training of 5 independent models on file LIM.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f592015d070>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5cc0f615e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b0a028a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5ca77e3130>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b0a719d30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b12a5fd30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f59647d2a90>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5900457100>, <__main__.SimpleDirichletPrior object at 0x7f5b0a04e1c0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 349.7590 - loglik: -3.4377e+02 - logprior: -5.9902e+00
Epoch 2/10
15/15 - 1s - loss: 322.1413 - loglik: -3.2032e+02 - logprior: -1.8216e+00
Epoch 3/10
15/15 - 1s - loss: 307.8875 - loglik: -3.0608e+02 - logprior: -1.8033e+00
Epoch 4/10
15/15 - 1s - loss: 304.3974 - loglik: -3.0266e+02 - logprior: -1.7395e+00
Epoch 5/10
15/15 - 1s - loss: 303.8023 - loglik: -3.0203e+02 - logprior: -1.7703e+00
Epoch 6/10
15/15 - 1s - loss: 303.2965 - loglik: -3.0139e+02 - logprior: -1.9027e+00
Epoch 7/10
15/15 - 1s - loss: 302.8930 - loglik: -3.0097e+02 - logprior: -1.9195e+00
Epoch 8/10
15/15 - 1s - loss: 302.9346 - loglik: -3.0101e+02 - logprior: -1.9280e+00
Fitted a model with MAP estimate = -302.7859
expansions: [(9, 2), (10, 3), (11, 2), (15, 1), (27, 1), (29, 2), (30, 2), (32, 2), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 6s - loss: 312.7213 - loglik: -3.0544e+02 - logprior: -7.2790e+00
Epoch 2/2
15/15 - 1s - loss: 304.0077 - loglik: -3.0060e+02 - logprior: -3.4041e+00
Fitted a model with MAP estimate = -301.7096
expansions: []
discards: [11 15 48]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 304.8708 - loglik: -2.9875e+02 - logprior: -6.1182e+00
Epoch 2/2
15/15 - 1s - loss: 298.6859 - loglik: -2.9662e+02 - logprior: -2.0680e+00
Fitted a model with MAP estimate = -298.0045
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 5s - loss: 302.5529 - loglik: -2.9703e+02 - logprior: -5.5275e+00
Epoch 2/10
15/15 - 1s - loss: 298.2081 - loglik: -2.9622e+02 - logprior: -1.9930e+00
Epoch 3/10
15/15 - 1s - loss: 297.7440 - loglik: -2.9618e+02 - logprior: -1.5595e+00
Epoch 4/10
15/15 - 1s - loss: 297.6209 - loglik: -2.9616e+02 - logprior: -1.4566e+00
Epoch 5/10
15/15 - 1s - loss: 297.1054 - loglik: -2.9565e+02 - logprior: -1.4596e+00
Epoch 6/10
15/15 - 1s - loss: 296.9098 - loglik: -2.9541e+02 - logprior: -1.4992e+00
Epoch 7/10
15/15 - 1s - loss: 296.4874 - loglik: -2.9495e+02 - logprior: -1.5362e+00
Epoch 8/10
15/15 - 1s - loss: 296.7914 - loglik: -2.9524e+02 - logprior: -1.5508e+00
Fitted a model with MAP estimate = -296.5272
Time for alignment: 43.1987
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 349.7767 - loglik: -3.4379e+02 - logprior: -5.9891e+00
Epoch 2/10
15/15 - 1s - loss: 321.3690 - loglik: -3.1955e+02 - logprior: -1.8160e+00
Epoch 3/10
15/15 - 1s - loss: 307.6469 - loglik: -3.0585e+02 - logprior: -1.7971e+00
Epoch 4/10
15/15 - 1s - loss: 304.7823 - loglik: -3.0304e+02 - logprior: -1.7419e+00
Epoch 5/10
15/15 - 1s - loss: 303.8765 - loglik: -3.0209e+02 - logprior: -1.7846e+00
Epoch 6/10
15/15 - 1s - loss: 303.4254 - loglik: -3.0152e+02 - logprior: -1.9022e+00
Epoch 7/10
15/15 - 1s - loss: 303.5401 - loglik: -3.0162e+02 - logprior: -1.9167e+00
Fitted a model with MAP estimate = -303.1927
expansions: [(9, 2), (10, 3), (11, 2), (12, 1), (28, 1), (29, 2), (30, 2), (32, 2), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 5s - loss: 312.4797 - loglik: -3.0522e+02 - logprior: -7.2641e+00
Epoch 2/2
15/15 - 1s - loss: 303.9550 - loglik: -3.0057e+02 - logprior: -3.3859e+00
Fitted a model with MAP estimate = -301.5188
expansions: []
discards: [11 15 48]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 304.4152 - loglik: -2.9833e+02 - logprior: -6.0808e+00
Epoch 2/2
15/15 - 1s - loss: 298.7016 - loglik: -2.9663e+02 - logprior: -2.0733e+00
Fitted a model with MAP estimate = -297.9803
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 5s - loss: 302.7340 - loglik: -2.9720e+02 - logprior: -5.5380e+00
Epoch 2/10
15/15 - 1s - loss: 298.5741 - loglik: -2.9657e+02 - logprior: -2.0005e+00
Epoch 3/10
15/15 - 1s - loss: 297.5907 - loglik: -2.9604e+02 - logprior: -1.5526e+00
Epoch 4/10
15/15 - 1s - loss: 297.6445 - loglik: -2.9619e+02 - logprior: -1.4568e+00
Fitted a model with MAP estimate = -297.2889
Time for alignment: 36.5260
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 349.9370 - loglik: -3.4395e+02 - logprior: -5.9905e+00
Epoch 2/10
15/15 - 1s - loss: 322.0025 - loglik: -3.2017e+02 - logprior: -1.8289e+00
Epoch 3/10
15/15 - 1s - loss: 307.7486 - loglik: -3.0594e+02 - logprior: -1.8067e+00
Epoch 4/10
15/15 - 1s - loss: 304.3746 - loglik: -3.0260e+02 - logprior: -1.7773e+00
Epoch 5/10
15/15 - 1s - loss: 303.4360 - loglik: -3.0161e+02 - logprior: -1.8256e+00
Epoch 6/10
15/15 - 1s - loss: 302.8943 - loglik: -3.0095e+02 - logprior: -1.9419e+00
Epoch 7/10
15/15 - 1s - loss: 302.9444 - loglik: -3.0098e+02 - logprior: -1.9632e+00
Fitted a model with MAP estimate = -302.6872
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (25, 2), (28, 1), (29, 2), (30, 2), (32, 3), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 313.9100 - loglik: -3.0660e+02 - logprior: -7.3062e+00
Epoch 2/2
15/15 - 1s - loss: 304.2148 - loglik: -3.0076e+02 - logprior: -3.4530e+00
Fitted a model with MAP estimate = -301.6875
expansions: []
discards: [12 15 31 46 50]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 304.8539 - loglik: -2.9878e+02 - logprior: -6.0744e+00
Epoch 2/2
15/15 - 1s - loss: 299.1305 - loglik: -2.9707e+02 - logprior: -2.0577e+00
Fitted a model with MAP estimate = -298.1061
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 5s - loss: 302.8929 - loglik: -2.9739e+02 - logprior: -5.5030e+00
Epoch 2/10
15/15 - 1s - loss: 298.5586 - loglik: -2.9660e+02 - logprior: -1.9540e+00
Epoch 3/10
15/15 - 1s - loss: 297.9973 - loglik: -2.9648e+02 - logprior: -1.5154e+00
Epoch 4/10
15/15 - 1s - loss: 297.5732 - loglik: -2.9616e+02 - logprior: -1.4170e+00
Epoch 5/10
15/15 - 1s - loss: 297.4521 - loglik: -2.9603e+02 - logprior: -1.4186e+00
Epoch 6/10
15/15 - 1s - loss: 297.0079 - loglik: -2.9555e+02 - logprior: -1.4597e+00
Epoch 7/10
15/15 - 1s - loss: 296.9128 - loglik: -2.9543e+02 - logprior: -1.4863e+00
Epoch 8/10
15/15 - 1s - loss: 297.0610 - loglik: -2.9553e+02 - logprior: -1.5267e+00
Fitted a model with MAP estimate = -296.7278
Time for alignment: 40.6758
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 349.8639 - loglik: -3.4388e+02 - logprior: -5.9888e+00
Epoch 2/10
15/15 - 1s - loss: 321.6435 - loglik: -3.1983e+02 - logprior: -1.8180e+00
Epoch 3/10
15/15 - 1s - loss: 308.8679 - loglik: -3.0708e+02 - logprior: -1.7855e+00
Epoch 4/10
15/15 - 1s - loss: 305.3706 - loglik: -3.0363e+02 - logprior: -1.7385e+00
Epoch 5/10
15/15 - 1s - loss: 303.8607 - loglik: -3.0208e+02 - logprior: -1.7802e+00
Epoch 6/10
15/15 - 1s - loss: 303.4632 - loglik: -3.0155e+02 - logprior: -1.9128e+00
Epoch 7/10
15/15 - 1s - loss: 302.8658 - loglik: -3.0092e+02 - logprior: -1.9427e+00
Epoch 8/10
15/15 - 1s - loss: 302.7364 - loglik: -3.0077e+02 - logprior: -1.9641e+00
Epoch 9/10
15/15 - 1s - loss: 302.5824 - loglik: -3.0058e+02 - logprior: -1.9977e+00
Epoch 10/10
15/15 - 1s - loss: 302.7832 - loglik: -3.0074e+02 - logprior: -2.0474e+00
Fitted a model with MAP estimate = -302.3320
expansions: [(9, 2), (10, 3), (11, 2), (24, 2), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 314.1333 - loglik: -3.0675e+02 - logprior: -7.3821e+00
Epoch 2/2
15/15 - 1s - loss: 304.5384 - loglik: -3.0102e+02 - logprior: -3.5152e+00
Fitted a model with MAP estimate = -302.3835
expansions: [(0, 2)]
discards: [ 0 11 15 31 46 50]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 304.9285 - loglik: -2.9969e+02 - logprior: -5.2348e+00
Epoch 2/2
15/15 - 1s - loss: 298.7015 - loglik: -2.9691e+02 - logprior: -1.7894e+00
Fitted a model with MAP estimate = -297.7170
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 305.9617 - loglik: -2.9929e+02 - logprior: -6.6697e+00
Epoch 2/10
15/15 - 1s - loss: 299.4681 - loglik: -2.9720e+02 - logprior: -2.2645e+00
Epoch 3/10
15/15 - 1s - loss: 297.6734 - loglik: -2.9610e+02 - logprior: -1.5702e+00
Epoch 4/10
15/15 - 1s - loss: 297.3285 - loglik: -2.9588e+02 - logprior: -1.4464e+00
Epoch 5/10
15/15 - 1s - loss: 297.3059 - loglik: -2.9584e+02 - logprior: -1.4662e+00
Epoch 6/10
15/15 - 1s - loss: 296.9509 - loglik: -2.9546e+02 - logprior: -1.4900e+00
Epoch 7/10
15/15 - 1s - loss: 296.7295 - loglik: -2.9519e+02 - logprior: -1.5354e+00
Epoch 8/10
15/15 - 1s - loss: 296.3950 - loglik: -2.9484e+02 - logprior: -1.5595e+00
Epoch 9/10
15/15 - 1s - loss: 296.8464 - loglik: -2.9528e+02 - logprior: -1.5653e+00
Fitted a model with MAP estimate = -296.4275
Time for alignment: 43.6074
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 349.9150 - loglik: -3.4392e+02 - logprior: -5.9908e+00
Epoch 2/10
15/15 - 1s - loss: 321.5099 - loglik: -3.1968e+02 - logprior: -1.8288e+00
Epoch 3/10
15/15 - 1s - loss: 308.0446 - loglik: -3.0625e+02 - logprior: -1.7967e+00
Epoch 4/10
15/15 - 1s - loss: 304.8216 - loglik: -3.0306e+02 - logprior: -1.7630e+00
Epoch 5/10
15/15 - 1s - loss: 303.3270 - loglik: -3.0150e+02 - logprior: -1.8229e+00
Epoch 6/10
15/15 - 1s - loss: 303.0375 - loglik: -3.0110e+02 - logprior: -1.9352e+00
Epoch 7/10
15/15 - 1s - loss: 302.9575 - loglik: -3.0099e+02 - logprior: -1.9721e+00
Epoch 8/10
15/15 - 1s - loss: 302.3295 - loglik: -3.0036e+02 - logprior: -1.9732e+00
Epoch 9/10
15/15 - 1s - loss: 302.7625 - loglik: -3.0077e+02 - logprior: -1.9974e+00
Fitted a model with MAP estimate = -302.5111
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (25, 2), (28, 1), (29, 2), (30, 2), (32, 3), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 5s - loss: 314.0629 - loglik: -3.0674e+02 - logprior: -7.3269e+00
Epoch 2/2
15/15 - 1s - loss: 304.0773 - loglik: -3.0059e+02 - logprior: -3.4870e+00
Fitted a model with MAP estimate = -302.0354
expansions: [(0, 2)]
discards: [ 0 13 15 31 46 50]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 304.9008 - loglik: -2.9969e+02 - logprior: -5.2111e+00
Epoch 2/2
15/15 - 1s - loss: 298.5557 - loglik: -2.9678e+02 - logprior: -1.7771e+00
Fitted a model with MAP estimate = -297.7565
expansions: []
discards: [ 0 37]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 306.6491 - loglik: -2.9998e+02 - logprior: -6.6724e+00
Epoch 2/10
15/15 - 1s - loss: 299.2652 - loglik: -2.9701e+02 - logprior: -2.2548e+00
Epoch 3/10
15/15 - 1s - loss: 297.9596 - loglik: -2.9642e+02 - logprior: -1.5399e+00
Epoch 4/10
15/15 - 1s - loss: 297.5616 - loglik: -2.9614e+02 - logprior: -1.4192e+00
Epoch 5/10
15/15 - 1s - loss: 297.4362 - loglik: -2.9601e+02 - logprior: -1.4247e+00
Epoch 6/10
15/15 - 1s - loss: 297.1872 - loglik: -2.9572e+02 - logprior: -1.4637e+00
Epoch 7/10
15/15 - 1s - loss: 296.8930 - loglik: -2.9539e+02 - logprior: -1.4995e+00
Epoch 8/10
15/15 - 1s - loss: 296.7710 - loglik: -2.9526e+02 - logprior: -1.5126e+00
Epoch 9/10
15/15 - 1s - loss: 296.8832 - loglik: -2.9535e+02 - logprior: -1.5352e+00
Fitted a model with MAP estimate = -296.6626
Time for alignment: 43.2249
Computed alignments with likelihoods: ['-296.5272', '-297.2889', '-296.7278', '-296.4275', '-296.6626']
Best model has likelihood: -296.4275  (prior= -1.6053 )
time for generating output: 0.1367
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/LIM.projection.fasta
SP score = 0.9790322580645161
Training of 5 independent models on file annexin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f59006068b0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5900637ee0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b07ff5d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b07ff5ca0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5334d4b070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5cc0dc20a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f59207b9b50>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5b0a483370>, <__main__.SimpleDirichletPrior object at 0x7f5afe979550>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 384.9572 - loglik: -3.6949e+02 - logprior: -1.5462e+01
Epoch 2/10
10/10 - 2s - loss: 355.9792 - loglik: -3.5165e+02 - logprior: -4.3298e+00
Epoch 3/10
10/10 - 2s - loss: 338.0069 - loglik: -3.3552e+02 - logprior: -2.4900e+00
Epoch 4/10
10/10 - 2s - loss: 328.9521 - loglik: -3.2691e+02 - logprior: -2.0415e+00
Epoch 5/10
10/10 - 2s - loss: 323.5015 - loglik: -3.2161e+02 - logprior: -1.8931e+00
Epoch 6/10
10/10 - 2s - loss: 321.2191 - loglik: -3.1944e+02 - logprior: -1.7763e+00
Epoch 7/10
10/10 - 2s - loss: 319.3444 - loglik: -3.1780e+02 - logprior: -1.5473e+00
Epoch 8/10
10/10 - 2s - loss: 318.9782 - loglik: -3.1751e+02 - logprior: -1.4647e+00
Epoch 9/10
10/10 - 2s - loss: 318.2337 - loglik: -3.1679e+02 - logprior: -1.4450e+00
Epoch 10/10
10/10 - 2s - loss: 317.8340 - loglik: -3.1642e+02 - logprior: -1.4125e+00
Fitted a model with MAP estimate = -318.0567
expansions: [(0, 5), (24, 1), (30, 1), (32, 1), (33, 1), (38, 3), (39, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 333.4518 - loglik: -3.1524e+02 - logprior: -1.8211e+01
Epoch 2/2
10/10 - 2s - loss: 314.1934 - loglik: -3.0867e+02 - logprior: -5.5260e+00
Fitted a model with MAP estimate = -309.6411
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 318.3414 - loglik: -3.0520e+02 - logprior: -1.3143e+01
Epoch 2/10
10/10 - 2s - loss: 308.4649 - loglik: -3.0486e+02 - logprior: -3.6016e+00
Epoch 3/10
10/10 - 2s - loss: 306.3709 - loglik: -3.0442e+02 - logprior: -1.9525e+00
Epoch 4/10
10/10 - 2s - loss: 305.3336 - loglik: -3.0381e+02 - logprior: -1.5269e+00
Epoch 5/10
10/10 - 2s - loss: 305.4159 - loglik: -3.0409e+02 - logprior: -1.3265e+00
Fitted a model with MAP estimate = -304.6672
Time for alignment: 51.1791
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 384.7565 - loglik: -3.6929e+02 - logprior: -1.5464e+01
Epoch 2/10
10/10 - 2s - loss: 355.7603 - loglik: -3.5142e+02 - logprior: -4.3416e+00
Epoch 3/10
10/10 - 2s - loss: 339.0717 - loglik: -3.3657e+02 - logprior: -2.5065e+00
Epoch 4/10
10/10 - 2s - loss: 327.0943 - loglik: -3.2498e+02 - logprior: -2.1162e+00
Epoch 5/10
10/10 - 2s - loss: 322.9770 - loglik: -3.2105e+02 - logprior: -1.9292e+00
Epoch 6/10
10/10 - 2s - loss: 318.7512 - loglik: -3.1697e+02 - logprior: -1.7772e+00
Epoch 7/10
10/10 - 2s - loss: 317.2984 - loglik: -3.1557e+02 - logprior: -1.7288e+00
Epoch 8/10
10/10 - 2s - loss: 316.5788 - loglik: -3.1490e+02 - logprior: -1.6756e+00
Epoch 9/10
10/10 - 2s - loss: 316.6309 - loglik: -3.1500e+02 - logprior: -1.6291e+00
Fitted a model with MAP estimate = -316.3456
expansions: [(0, 5), (24, 1), (30, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 1), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 333.3681 - loglik: -3.1520e+02 - logprior: -1.8167e+01
Epoch 2/2
10/10 - 2s - loss: 313.7767 - loglik: -3.0826e+02 - logprior: -5.5177e+00
Fitted a model with MAP estimate = -309.4701
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 318.8246 - loglik: -3.0568e+02 - logprior: -1.3140e+01
Epoch 2/10
10/10 - 2s - loss: 307.6407 - loglik: -3.0404e+02 - logprior: -3.6040e+00
Epoch 3/10
10/10 - 2s - loss: 306.4209 - loglik: -3.0447e+02 - logprior: -1.9525e+00
Epoch 4/10
10/10 - 2s - loss: 306.2626 - loglik: -3.0473e+02 - logprior: -1.5332e+00
Epoch 5/10
10/10 - 2s - loss: 304.3219 - loglik: -3.0300e+02 - logprior: -1.3268e+00
Epoch 6/10
10/10 - 2s - loss: 304.8627 - loglik: -3.0375e+02 - logprior: -1.1145e+00
Fitted a model with MAP estimate = -304.4103
Time for alignment: 50.4838
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 384.4966 - loglik: -3.6903e+02 - logprior: -1.5463e+01
Epoch 2/10
10/10 - 2s - loss: 356.0906 - loglik: -3.5176e+02 - logprior: -4.3325e+00
Epoch 3/10
10/10 - 2s - loss: 339.5733 - loglik: -3.3709e+02 - logprior: -2.4857e+00
Epoch 4/10
10/10 - 2s - loss: 326.3392 - loglik: -3.2425e+02 - logprior: -2.0894e+00
Epoch 5/10
10/10 - 2s - loss: 322.4967 - loglik: -3.2058e+02 - logprior: -1.9208e+00
Epoch 6/10
10/10 - 2s - loss: 317.9597 - loglik: -3.1617e+02 - logprior: -1.7852e+00
Epoch 7/10
10/10 - 2s - loss: 317.4961 - loglik: -3.1575e+02 - logprior: -1.7424e+00
Epoch 8/10
10/10 - 2s - loss: 316.4884 - loglik: -3.1482e+02 - logprior: -1.6731e+00
Epoch 9/10
10/10 - 2s - loss: 316.0742 - loglik: -3.1446e+02 - logprior: -1.6115e+00
Epoch 10/10
10/10 - 2s - loss: 316.9249 - loglik: -3.1535e+02 - logprior: -1.5764e+00
Fitted a model with MAP estimate = -316.3490
expansions: [(0, 5), (24, 1), (30, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 332.8598 - loglik: -3.1465e+02 - logprior: -1.8214e+01
Epoch 2/2
10/10 - 2s - loss: 313.5659 - loglik: -3.0803e+02 - logprior: -5.5352e+00
Fitted a model with MAP estimate = -309.4494
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 318.9528 - loglik: -3.0581e+02 - logprior: -1.3146e+01
Epoch 2/10
10/10 - 2s - loss: 307.8913 - loglik: -3.0429e+02 - logprior: -3.6034e+00
Epoch 3/10
10/10 - 2s - loss: 306.4471 - loglik: -3.0449e+02 - logprior: -1.9565e+00
Epoch 4/10
10/10 - 2s - loss: 305.7147 - loglik: -3.0418e+02 - logprior: -1.5301e+00
Epoch 5/10
10/10 - 2s - loss: 304.4099 - loglik: -3.0308e+02 - logprior: -1.3274e+00
Epoch 6/10
10/10 - 2s - loss: 304.0723 - loglik: -3.0296e+02 - logprior: -1.1162e+00
Epoch 7/10
10/10 - 2s - loss: 305.1069 - loglik: -3.0411e+02 - logprior: -1.0005e+00
Fitted a model with MAP estimate = -304.2493
Time for alignment: 53.8919
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 384.8344 - loglik: -3.6937e+02 - logprior: -1.5464e+01
Epoch 2/10
10/10 - 2s - loss: 356.1318 - loglik: -3.5179e+02 - logprior: -4.3432e+00
Epoch 3/10
10/10 - 2s - loss: 339.0009 - loglik: -3.3647e+02 - logprior: -2.5319e+00
Epoch 4/10
10/10 - 2s - loss: 328.6133 - loglik: -3.2645e+02 - logprior: -2.1597e+00
Epoch 5/10
10/10 - 2s - loss: 323.6953 - loglik: -3.2159e+02 - logprior: -2.1020e+00
Epoch 6/10
10/10 - 2s - loss: 322.9189 - loglik: -3.2075e+02 - logprior: -2.1647e+00
Epoch 7/10
10/10 - 1s - loss: 321.0168 - loglik: -3.1887e+02 - logprior: -2.1454e+00
Epoch 8/10
10/10 - 2s - loss: 320.6534 - loglik: -3.1862e+02 - logprior: -2.0356e+00
Epoch 9/10
10/10 - 2s - loss: 321.0601 - loglik: -3.1909e+02 - logprior: -1.9676e+00
Fitted a model with MAP estimate = -320.4300
expansions: [(4, 2), (5, 2), (7, 1), (8, 1), (24, 1), (33, 3), (35, 1), (38, 1), (39, 1), (40, 1), (41, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 333.1207 - loglik: -3.1678e+02 - logprior: -1.6344e+01
Epoch 2/2
10/10 - 2s - loss: 314.1035 - loglik: -3.0717e+02 - logprior: -6.9322e+00
Fitted a model with MAP estimate = -311.5165
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 317.9595 - loglik: -3.0526e+02 - logprior: -1.2701e+01
Epoch 2/2
10/10 - 2s - loss: 306.9840 - loglik: -3.0352e+02 - logprior: -3.4661e+00
Fitted a model with MAP estimate = -306.1298
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 320.9624 - loglik: -3.0590e+02 - logprior: -1.5060e+01
Epoch 2/10
10/10 - 2s - loss: 309.3799 - loglik: -3.0450e+02 - logprior: -4.8749e+00
Epoch 3/10
10/10 - 2s - loss: 306.8494 - loglik: -3.0434e+02 - logprior: -2.5067e+00
Epoch 4/10
10/10 - 2s - loss: 305.7484 - loglik: -3.0418e+02 - logprior: -1.5676e+00
Epoch 5/10
10/10 - 2s - loss: 304.5573 - loglik: -3.0327e+02 - logprior: -1.2911e+00
Epoch 6/10
10/10 - 2s - loss: 304.0024 - loglik: -3.0295e+02 - logprior: -1.0544e+00
Epoch 7/10
10/10 - 2s - loss: 304.3676 - loglik: -3.0339e+02 - logprior: -9.7686e-01
Fitted a model with MAP estimate = -303.8940
Time for alignment: 60.3208
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 383.5358 - loglik: -3.6807e+02 - logprior: -1.5465e+01
Epoch 2/10
10/10 - 2s - loss: 357.3176 - loglik: -3.5297e+02 - logprior: -4.3503e+00
Epoch 3/10
10/10 - 2s - loss: 339.7245 - loglik: -3.3718e+02 - logprior: -2.5426e+00
Epoch 4/10
10/10 - 2s - loss: 330.6407 - loglik: -3.2857e+02 - logprior: -2.0676e+00
Epoch 5/10
10/10 - 2s - loss: 324.4726 - loglik: -3.2262e+02 - logprior: -1.8524e+00
Epoch 6/10
10/10 - 2s - loss: 320.9412 - loglik: -3.1915e+02 - logprior: -1.7885e+00
Epoch 7/10
10/10 - 2s - loss: 319.8751 - loglik: -3.1825e+02 - logprior: -1.6235e+00
Epoch 8/10
10/10 - 2s - loss: 318.4804 - loglik: -3.1698e+02 - logprior: -1.5017e+00
Epoch 9/10
10/10 - 2s - loss: 318.3124 - loglik: -3.1684e+02 - logprior: -1.4716e+00
Epoch 10/10
10/10 - 2s - loss: 318.6454 - loglik: -3.1724e+02 - logprior: -1.4094e+00
Fitted a model with MAP estimate = -317.9663
expansions: [(0, 5), (28, 1), (33, 3), (35, 1), (37, 1), (38, 1), (39, 1), (47, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 333.5537 - loglik: -3.1532e+02 - logprior: -1.8236e+01
Epoch 2/2
10/10 - 2s - loss: 313.2588 - loglik: -3.0772e+02 - logprior: -5.5358e+00
Fitted a model with MAP estimate = -309.5907
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 317.7335 - loglik: -3.0459e+02 - logprior: -1.3147e+01
Epoch 2/10
10/10 - 2s - loss: 308.6158 - loglik: -3.0501e+02 - logprior: -3.6107e+00
Epoch 3/10
10/10 - 2s - loss: 307.0035 - loglik: -3.0505e+02 - logprior: -1.9525e+00
Epoch 4/10
10/10 - 2s - loss: 305.0709 - loglik: -3.0354e+02 - logprior: -1.5326e+00
Epoch 5/10
10/10 - 2s - loss: 304.7103 - loglik: -3.0338e+02 - logprior: -1.3271e+00
Epoch 6/10
10/10 - 2s - loss: 304.9332 - loglik: -3.0382e+02 - logprior: -1.1121e+00
Fitted a model with MAP estimate = -304.3978
Time for alignment: 51.8287
Computed alignments with likelihoods: ['-304.6672', '-304.4103', '-304.2493', '-303.8940', '-304.3978']
Best model has likelihood: -303.8940  (prior= -0.9371 )
time for generating output: 0.2853
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/annexin.projection.fasta
SP score = 0.9966151893378464
Training of 5 independent models on file hormone_rec.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f59000c6fd0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5aff7f2820>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5984068dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f53441089a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b13a2b430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f59000cff10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b07ff5d00>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5b07ff5ca0>, <__main__.SimpleDirichletPrior object at 0x7f5b1c61e190>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 1077.4923 - loglik: -1.0696e+03 - logprior: -7.8805e+00
Epoch 2/10
23/23 - 6s - loss: 1005.6172 - loglik: -1.0047e+03 - logprior: -8.8426e-01
Epoch 3/10
23/23 - 6s - loss: 987.1392 - loglik: -9.8642e+02 - logprior: -7.1814e-01
Epoch 4/10
23/23 - 6s - loss: 979.6885 - loglik: -9.7914e+02 - logprior: -5.4679e-01
Epoch 5/10
23/23 - 6s - loss: 974.2947 - loglik: -9.7385e+02 - logprior: -4.4817e-01
Epoch 6/10
23/23 - 6s - loss: 976.6185 - loglik: -9.7612e+02 - logprior: -4.9737e-01
Fitted a model with MAP estimate = -974.1052
expansions: [(0, 8), (9, 4), (19, 1), (42, 1), (52, 1), (56, 2), (57, 2), (66, 1), (71, 1), (72, 3), (76, 1), (79, 2), (80, 1), (83, 1), (108, 1), (112, 2), (113, 1), (115, 1), (118, 1), (120, 1), (127, 1), (130, 1), (148, 1), (149, 1), (150, 1), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 205 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 10s - loss: 989.5634 - loglik: -9.8094e+02 - logprior: -8.6214e+00
Epoch 2/2
23/23 - 7s - loss: 969.5812 - loglik: -9.6915e+02 - logprior: -4.2964e-01
Fitted a model with MAP estimate = -967.2132
expansions: [(0, 18)]
discards: [  1   2   3   4   5   6   7   8   9  72  94 100 107 143 200 201 202 203
 204]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 12s - loss: 984.6359 - loglik: -9.7601e+02 - logprior: -8.6236e+00
Epoch 2/2
23/23 - 7s - loss: 970.1797 - loglik: -9.6988e+02 - logprior: -3.0169e-01
Fitted a model with MAP estimate = -968.2801
expansions: [(0, 17), (204, 8)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 211 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 10s - loss: 978.6078 - loglik: -9.7187e+02 - logprior: -6.7393e+00
Epoch 2/10
23/23 - 7s - loss: 968.1332 - loglik: -9.6832e+02 - logprior: 0.1834
Epoch 3/10
23/23 - 7s - loss: 962.7299 - loglik: -9.6355e+02 - logprior: 0.8212
Epoch 4/10
23/23 - 7s - loss: 960.7623 - loglik: -9.6173e+02 - logprior: 0.9727
Epoch 5/10
23/23 - 7s - loss: 956.2370 - loglik: -9.5735e+02 - logprior: 1.1128
Epoch 6/10
23/23 - 7s - loss: 955.6733 - loglik: -9.5688e+02 - logprior: 1.2021
Epoch 7/10
23/23 - 7s - loss: 954.9043 - loglik: -9.5615e+02 - logprior: 1.2493
Epoch 8/10
23/23 - 7s - loss: 954.3774 - loglik: -9.5561e+02 - logprior: 1.2281
Epoch 9/10
23/23 - 7s - loss: 952.3203 - loglik: -9.5348e+02 - logprior: 1.1610
Epoch 10/10
23/23 - 7s - loss: 952.0859 - loglik: -9.5329e+02 - logprior: 1.2070
Fitted a model with MAP estimate = -952.3597
Time for alignment: 173.8967
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 10s - loss: 1076.3883 - loglik: -1.0685e+03 - logprior: -7.8521e+00
Epoch 2/10
23/23 - 6s - loss: 1008.9334 - loglik: -1.0081e+03 - logprior: -7.8424e-01
Epoch 3/10
23/23 - 6s - loss: 987.2513 - loglik: -9.8670e+02 - logprior: -5.4919e-01
Epoch 4/10
23/23 - 6s - loss: 980.7704 - loglik: -9.8031e+02 - logprior: -4.6532e-01
Epoch 5/10
23/23 - 6s - loss: 977.7583 - loglik: -9.7737e+02 - logprior: -3.9255e-01
Epoch 6/10
23/23 - 6s - loss: 976.3666 - loglik: -9.7592e+02 - logprior: -4.4624e-01
Epoch 7/10
23/23 - 6s - loss: 975.4613 - loglik: -9.7487e+02 - logprior: -5.8696e-01
Epoch 8/10
23/23 - 6s - loss: 972.9266 - loglik: -9.7221e+02 - logprior: -7.1538e-01
Epoch 9/10
23/23 - 6s - loss: 974.9073 - loglik: -9.7409e+02 - logprior: -8.1488e-01
Fitted a model with MAP estimate = -973.5718
expansions: [(0, 7), (8, 5), (15, 1), (30, 1), (38, 1), (41, 1), (56, 1), (57, 2), (58, 2), (69, 1), (70, 2), (71, 1), (79, 2), (80, 1), (83, 1), (86, 1), (110, 1), (112, 1), (115, 2), (120, 1), (123, 1), (126, 1), (149, 1), (150, 2), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 10s - loss: 988.2225 - loglik: -9.7938e+02 - logprior: -8.8427e+00
Epoch 2/2
23/23 - 7s - loss: 969.9741 - loglik: -9.6956e+02 - logprior: -4.1225e-01
Fitted a model with MAP estimate = -966.5398
expansions: [(0, 18)]
discards: [  1   2   3   4   5   6  15  17  77  93 107 199 200 201 202 203]
Re-initialized the encoder parameters.
Fitting a model of length 206 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 11s - loss: 982.5627 - loglik: -9.7422e+02 - logprior: -8.3443e+00
Epoch 2/2
23/23 - 7s - loss: 971.0414 - loglik: -9.7072e+02 - logprior: -3.2442e-01
Fitted a model with MAP estimate = -968.3709
expansions: [(0, 18), (28, 2), (206, 8)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 83]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 10s - loss: 979.2570 - loglik: -9.7258e+02 - logprior: -6.6771e+00
Epoch 2/10
23/23 - 7s - loss: 963.8314 - loglik: -9.6406e+02 - logprior: 0.2247
Epoch 3/10
23/23 - 7s - loss: 966.2463 - loglik: -9.6710e+02 - logprior: 0.8562
Fitted a model with MAP estimate = -960.9127
Time for alignment: 144.1310
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 1078.1200 - loglik: -1.0703e+03 - logprior: -7.8671e+00
Epoch 2/10
23/23 - 6s - loss: 1006.1313 - loglik: -1.0053e+03 - logprior: -8.1241e-01
Epoch 3/10
23/23 - 6s - loss: 984.9019 - loglik: -9.8444e+02 - logprior: -4.5761e-01
Epoch 4/10
23/23 - 6s - loss: 980.1756 - loglik: -9.7985e+02 - logprior: -3.2577e-01
Epoch 5/10
23/23 - 6s - loss: 976.8809 - loglik: -9.7662e+02 - logprior: -2.5914e-01
Epoch 6/10
23/23 - 6s - loss: 973.5198 - loglik: -9.7319e+02 - logprior: -3.3187e-01
Epoch 7/10
23/23 - 6s - loss: 975.4857 - loglik: -9.7503e+02 - logprior: -4.5726e-01
Fitted a model with MAP estimate = -973.8491
expansions: [(0, 8), (9, 4), (15, 1), (43, 1), (51, 1), (56, 2), (57, 1), (59, 2), (62, 1), (69, 1), (71, 2), (76, 1), (83, 1), (107, 1), (112, 2), (113, 1), (115, 1), (118, 1), (120, 1), (127, 1), (149, 2), (150, 1), (159, 6)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 202 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 10s - loss: 989.4783 - loglik: -9.8078e+02 - logprior: -8.6940e+00
Epoch 2/2
23/23 - 7s - loss: 973.4778 - loglik: -9.7294e+02 - logprior: -5.3916e-01
Fitted a model with MAP estimate = -968.4817
expansions: [(0, 18), (92, 1), (184, 1)]
discards: [  1   2   3   4   5   6   7   8   9  72  77  94 140 196 197 198 199 200
 201]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 10s - loss: 983.4820 - loglik: -9.7488e+02 - logprior: -8.6024e+00
Epoch 2/2
23/23 - 7s - loss: 970.8127 - loglik: -9.7042e+02 - logprior: -3.9185e-01
Fitted a model with MAP estimate = -968.4348
expansions: [(0, 16), (203, 8)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Re-initialized the encoder parameters.
Fitting a model of length 209 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 11s - loss: 980.4719 - loglik: -9.7176e+02 - logprior: -8.7133e+00
Epoch 2/10
23/23 - 7s - loss: 971.6168 - loglik: -9.7091e+02 - logprior: -7.0758e-01
Epoch 3/10
23/23 - 7s - loss: 965.2369 - loglik: -9.6616e+02 - logprior: 0.9202
Epoch 4/10
23/23 - 7s - loss: 959.1862 - loglik: -9.6026e+02 - logprior: 1.0746
Epoch 5/10
23/23 - 7s - loss: 960.1256 - loglik: -9.6130e+02 - logprior: 1.1752
Fitted a model with MAP estimate = -956.6130
Time for alignment: 143.1985
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 1080.6281 - loglik: -1.0727e+03 - logprior: -7.9022e+00
Epoch 2/10
23/23 - 6s - loss: 1005.0532 - loglik: -1.0042e+03 - logprior: -8.5639e-01
Epoch 3/10
23/23 - 6s - loss: 984.7830 - loglik: -9.8415e+02 - logprior: -6.3487e-01
Epoch 4/10
23/23 - 6s - loss: 977.8117 - loglik: -9.7732e+02 - logprior: -4.9501e-01
Epoch 5/10
23/23 - 6s - loss: 977.0034 - loglik: -9.7660e+02 - logprior: -4.0767e-01
Epoch 6/10
23/23 - 6s - loss: 974.1567 - loglik: -9.7372e+02 - logprior: -4.3895e-01
Epoch 7/10
23/23 - 6s - loss: 973.7277 - loglik: -9.7317e+02 - logprior: -5.5432e-01
Epoch 8/10
23/23 - 6s - loss: 973.3840 - loglik: -9.7270e+02 - logprior: -6.7951e-01
Epoch 9/10
23/23 - 6s - loss: 971.6330 - loglik: -9.7086e+02 - logprior: -7.7582e-01
Epoch 10/10
23/23 - 6s - loss: 973.4918 - loglik: -9.7266e+02 - logprior: -8.2915e-01
Fitted a model with MAP estimate = -972.4742
expansions: [(0, 8), (9, 4), (31, 1), (43, 1), (51, 1), (52, 1), (57, 1), (59, 2), (66, 1), (70, 1), (71, 1), (72, 2), (76, 2), (77, 2), (86, 1), (110, 1), (112, 1), (119, 1), (120, 1), (121, 1), (123, 1), (126, 1), (149, 1), (150, 2), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 203 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 10s - loss: 988.7863 - loglik: -9.7998e+02 - logprior: -8.8053e+00
Epoch 2/2
23/23 - 7s - loss: 968.1766 - loglik: -9.6782e+02 - logprior: -3.6116e-01
Fitted a model with MAP estimate = -965.8362
expansions: [(0, 19)]
discards: [  1   2   3   4   5   6   7   8   9  10  94 102 198 199 200 201 202]
Re-initialized the encoder parameters.
Fitting a model of length 205 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 10s - loss: 981.5787 - loglik: -9.7303e+02 - logprior: -8.5484e+00
Epoch 2/2
23/23 - 7s - loss: 968.6027 - loglik: -9.6835e+02 - logprior: -2.5156e-01
Fitted a model with MAP estimate = -966.5363
expansions: [(0, 18), (205, 8)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 22 23 85]
Re-initialized the encoder parameters.
Fitting a model of length 208 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 10s - loss: 977.5294 - loglik: -9.7122e+02 - logprior: -6.3130e+00
Epoch 2/10
23/23 - 7s - loss: 970.1193 - loglik: -9.7051e+02 - logprior: 0.3933
Epoch 3/10
23/23 - 7s - loss: 961.9186 - loglik: -9.6297e+02 - logprior: 1.0537
Epoch 4/10
23/23 - 7s - loss: 960.1001 - loglik: -9.6124e+02 - logprior: 1.1360
Epoch 5/10
23/23 - 7s - loss: 957.8353 - loglik: -9.5916e+02 - logprior: 1.3294
Epoch 6/10
23/23 - 7s - loss: 953.1741 - loglik: -9.5461e+02 - logprior: 1.4358
Epoch 7/10
23/23 - 7s - loss: 951.7880 - loglik: -9.5327e+02 - logprior: 1.4826
Epoch 8/10
23/23 - 7s - loss: 956.9191 - loglik: -9.5837e+02 - logprior: 1.4485
Fitted a model with MAP estimate = -952.5135
Time for alignment: 181.3889
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 10s - loss: 1075.9043 - loglik: -1.0680e+03 - logprior: -7.8692e+00
Epoch 2/10
23/23 - 6s - loss: 1007.0963 - loglik: -1.0063e+03 - logprior: -7.5133e-01
Epoch 3/10
23/23 - 6s - loss: 986.0401 - loglik: -9.8547e+02 - logprior: -5.6749e-01
Epoch 4/10
23/23 - 6s - loss: 980.1367 - loglik: -9.7964e+02 - logprior: -4.9428e-01
Epoch 5/10
23/23 - 6s - loss: 973.6118 - loglik: -9.7314e+02 - logprior: -4.7162e-01
Epoch 6/10
23/23 - 6s - loss: 975.7117 - loglik: -9.7519e+02 - logprior: -5.1881e-01
Fitted a model with MAP estimate = -973.7653
expansions: [(0, 8), (9, 3), (15, 1), (31, 1), (51, 1), (57, 1), (58, 2), (59, 2), (66, 1), (70, 1), (71, 1), (72, 2), (79, 1), (81, 2), (83, 1), (86, 1), (112, 2), (113, 1), (119, 1), (120, 1), (121, 1), (122, 1), (126, 1), (149, 1), (150, 2), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 10s - loss: 987.7622 - loglik: -9.7914e+02 - logprior: -8.6263e+00
Epoch 2/2
23/23 - 7s - loss: 969.3045 - loglik: -9.6880e+02 - logprior: -5.0792e-01
Fitted a model with MAP estimate = -966.7605
expansions: [(0, 18)]
discards: [  1   2   3   4   5   6   7   8   9  10  73  76  94 106 142 199 200 201
 202 203]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 11s - loss: 983.3490 - loglik: -9.7464e+02 - logprior: -8.7103e+00
Epoch 2/2
23/23 - 7s - loss: 971.1163 - loglik: -9.7076e+02 - logprior: -3.6049e-01
Fitted a model with MAP estimate = -968.3861
expansions: [(0, 19), (81, 1), (202, 7)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 24]
Re-initialized the encoder parameters.
Fitting a model of length 210 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 10s - loss: 979.7684 - loglik: -9.7305e+02 - logprior: -6.7150e+00
Epoch 2/10
23/23 - 7s - loss: 965.9041 - loglik: -9.6606e+02 - logprior: 0.1544
Epoch 3/10
23/23 - 7s - loss: 967.0924 - loglik: -9.6785e+02 - logprior: 0.7554
Fitted a model with MAP estimate = -962.1188
Time for alignment: 123.6996
Computed alignments with likelihoods: ['-952.3597', '-960.9127', '-956.6130', '-952.5135', '-962.1188']
Best model has likelihood: -952.3597  (prior= 1.2062 )
time for generating output: 0.2502
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hormone_rec.projection.fasta
SP score = 0.6477843764275925
Training of 5 independent models on file Acetyltransf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f534413f880>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b09c0b430>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5ca7963cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b0a2a9b20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b09240f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f53349c0fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531c573490>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5afe4560d0>, <__main__.SimpleDirichletPrior object at 0x7f5c84417160>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 510.1583 - loglik: -5.0893e+02 - logprior: -1.2304e+00
Epoch 2/10
29/29 - 4s - loss: 485.0023 - loglik: -4.8419e+02 - logprior: -8.1004e-01
Epoch 3/10
29/29 - 4s - loss: 480.6315 - loglik: -4.7985e+02 - logprior: -7.8464e-01
Epoch 4/10
29/29 - 4s - loss: 479.6812 - loglik: -4.7889e+02 - logprior: -7.8782e-01
Epoch 5/10
29/29 - 4s - loss: 479.1374 - loglik: -4.7834e+02 - logprior: -7.9541e-01
Epoch 6/10
29/29 - 4s - loss: 477.4103 - loglik: -4.7660e+02 - logprior: -8.1293e-01
Epoch 7/10
29/29 - 4s - loss: 477.4069 - loglik: -4.7657e+02 - logprior: -8.3347e-01
Epoch 8/10
29/29 - 4s - loss: 477.1686 - loglik: -4.7632e+02 - logprior: -8.4839e-01
Epoch 9/10
29/29 - 4s - loss: 476.3510 - loglik: -4.7546e+02 - logprior: -8.8898e-01
Epoch 10/10
29/29 - 4s - loss: 475.9915 - loglik: -4.7508e+02 - logprior: -9.1204e-01
Fitted a model with MAP estimate = -443.7334
expansions: [(1, 1), (14, 3), (16, 2), (17, 2), (21, 1), (38, 1), (41, 1), (43, 2), (44, 2), (46, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 9s - loss: 481.2443 - loglik: -4.8004e+02 - logprior: -1.2007e+00
Epoch 2/2
29/29 - 4s - loss: 476.1407 - loglik: -4.7540e+02 - logprior: -7.3970e-01
Fitted a model with MAP estimate = -437.6744
expansions: [(3, 1)]
discards: [55 66]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 477.2227 - loglik: -4.7614e+02 - logprior: -1.0874e+00
Epoch 2/2
29/29 - 4s - loss: 475.5089 - loglik: -4.7482e+02 - logprior: -6.9045e-01
Fitted a model with MAP estimate = -437.2093
expansions: []
discards: [ 1 58]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 9s - loss: 437.5442 - loglik: -4.3693e+02 - logprior: -6.1031e-01
Epoch 2/10
42/42 - 5s - loss: 436.6039 - loglik: -4.3611e+02 - logprior: -4.9080e-01
Epoch 3/10
42/42 - 5s - loss: 435.8514 - loglik: -4.3537e+02 - logprior: -4.7647e-01
Epoch 4/10
42/42 - 5s - loss: 435.7153 - loglik: -4.3524e+02 - logprior: -4.7432e-01
Epoch 5/10
42/42 - 5s - loss: 435.1281 - loglik: -4.3465e+02 - logprior: -4.8260e-01
Epoch 6/10
42/42 - 5s - loss: 434.1919 - loglik: -4.3370e+02 - logprior: -4.8929e-01
Epoch 7/10
42/42 - 5s - loss: 434.7292 - loglik: -4.3424e+02 - logprior: -4.9323e-01
Fitted a model with MAP estimate = -432.9874
Time for alignment: 157.6780
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 510.2160 - loglik: -5.0899e+02 - logprior: -1.2296e+00
Epoch 2/10
29/29 - 4s - loss: 485.5819 - loglik: -4.8477e+02 - logprior: -8.1255e-01
Epoch 3/10
29/29 - 4s - loss: 480.0357 - loglik: -4.7926e+02 - logprior: -7.7983e-01
Epoch 4/10
29/29 - 4s - loss: 479.3408 - loglik: -4.7856e+02 - logprior: -7.8154e-01
Epoch 5/10
29/29 - 4s - loss: 480.5187 - loglik: -4.7973e+02 - logprior: -7.8404e-01
Fitted a model with MAP estimate = -441.3879
expansions: [(2, 2), (14, 2), (16, 2), (17, 2), (21, 1), (38, 1), (41, 1), (43, 1), (46, 2), (47, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 479.3594 - loglik: -4.7820e+02 - logprior: -1.1591e+00
Epoch 2/2
29/29 - 4s - loss: 475.9218 - loglik: -4.7519e+02 - logprior: -7.2803e-01
Fitted a model with MAP estimate = -437.3923
expansions: []
discards: [ 0 59 65]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 479.0609 - loglik: -4.7762e+02 - logprior: -1.4450e+00
Epoch 2/2
29/29 - 4s - loss: 476.7053 - loglik: -4.7588e+02 - logprior: -8.2339e-01
Fitted a model with MAP estimate = -438.5610
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 437.4256 - loglik: -4.3671e+02 - logprior: -7.1103e-01
Epoch 2/10
42/42 - 6s - loss: 437.1080 - loglik: -4.3663e+02 - logprior: -4.7348e-01
Epoch 3/10
42/42 - 5s - loss: 436.2307 - loglik: -4.3576e+02 - logprior: -4.7512e-01
Epoch 4/10
42/42 - 6s - loss: 436.0357 - loglik: -4.3556e+02 - logprior: -4.7741e-01
Epoch 5/10
42/42 - 5s - loss: 433.8476 - loglik: -4.3336e+02 - logprior: -4.8644e-01
Epoch 6/10
42/42 - 5s - loss: 435.0881 - loglik: -4.3460e+02 - logprior: -4.9106e-01
Fitted a model with MAP estimate = -433.7963
Time for alignment: 131.7062
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 510.5162 - loglik: -5.0929e+02 - logprior: -1.2298e+00
Epoch 2/10
29/29 - 4s - loss: 484.4699 - loglik: -4.8367e+02 - logprior: -8.0241e-01
Epoch 3/10
29/29 - 4s - loss: 481.2966 - loglik: -4.8051e+02 - logprior: -7.8171e-01
Epoch 4/10
29/29 - 4s - loss: 479.5648 - loglik: -4.7877e+02 - logprior: -7.9326e-01
Epoch 5/10
29/29 - 4s - loss: 479.1622 - loglik: -4.7836e+02 - logprior: -8.0665e-01
Epoch 6/10
29/29 - 4s - loss: 477.8999 - loglik: -4.7708e+02 - logprior: -8.1751e-01
Epoch 7/10
29/29 - 4s - loss: 477.1093 - loglik: -4.7627e+02 - logprior: -8.3476e-01
Epoch 8/10
29/29 - 4s - loss: 477.1529 - loglik: -4.7630e+02 - logprior: -8.5760e-01
Fitted a model with MAP estimate = -442.0730
expansions: [(1, 1), (14, 2), (16, 2), (17, 2), (21, 1), (38, 1), (41, 1), (43, 1), (44, 2), (45, 1), (46, 1), (47, 1), (48, 2), (50, 1), (52, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 479.6334 - loglik: -4.7845e+02 - logprior: -1.1836e+00
Epoch 2/2
29/29 - 4s - loss: 476.5943 - loglik: -4.7586e+02 - logprior: -7.3312e-01
Fitted a model with MAP estimate = -437.4821
expansions: [(3, 1), (16, 1)]
discards: [56 64]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 476.7150 - loglik: -4.7563e+02 - logprior: -1.0828e+00
Epoch 2/2
29/29 - 4s - loss: 475.6363 - loglik: -4.7496e+02 - logprior: -6.7549e-01
Fitted a model with MAP estimate = -437.4249
expansions: []
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 437.7150 - loglik: -4.3706e+02 - logprior: -6.5639e-01
Epoch 2/10
42/42 - 5s - loss: 436.9974 - loglik: -4.3644e+02 - logprior: -5.5954e-01
Epoch 3/10
42/42 - 5s - loss: 435.4485 - loglik: -4.3490e+02 - logprior: -5.4692e-01
Epoch 4/10
42/42 - 5s - loss: 435.7940 - loglik: -4.3524e+02 - logprior: -5.5238e-01
Fitted a model with MAP estimate = -435.1545
Time for alignment: 132.2398
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 509.2655 - loglik: -5.0803e+02 - logprior: -1.2325e+00
Epoch 2/10
29/29 - 4s - loss: 485.5906 - loglik: -4.8478e+02 - logprior: -8.0911e-01
Epoch 3/10
29/29 - 4s - loss: 480.9188 - loglik: -4.8014e+02 - logprior: -7.8180e-01
Epoch 4/10
29/29 - 4s - loss: 480.3100 - loglik: -4.7952e+02 - logprior: -7.8812e-01
Epoch 5/10
29/29 - 4s - loss: 478.6013 - loglik: -4.7781e+02 - logprior: -7.9017e-01
Epoch 6/10
29/29 - 4s - loss: 478.5660 - loglik: -4.7775e+02 - logprior: -8.2014e-01
Epoch 7/10
29/29 - 4s - loss: 477.1472 - loglik: -4.7630e+02 - logprior: -8.4319e-01
Epoch 8/10
29/29 - 4s - loss: 477.5417 - loglik: -4.7668e+02 - logprior: -8.6006e-01
Fitted a model with MAP estimate = -442.2674
expansions: [(1, 1), (2, 1), (14, 2), (16, 2), (17, 2), (21, 1), (38, 1), (41, 1), (43, 1), (46, 2), (47, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 479.5782 - loglik: -4.7840e+02 - logprior: -1.1818e+00
Epoch 2/2
29/29 - 4s - loss: 476.1116 - loglik: -4.7538e+02 - logprior: -7.2972e-01
Fitted a model with MAP estimate = -437.5028
expansions: []
discards: [ 1 59 65]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 477.1813 - loglik: -4.7617e+02 - logprior: -1.0126e+00
Epoch 2/2
29/29 - 4s - loss: 476.4524 - loglik: -4.7575e+02 - logprior: -7.0233e-01
Fitted a model with MAP estimate = -437.7733
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 9s - loss: 437.1823 - loglik: -4.3652e+02 - logprior: -6.6689e-01
Epoch 2/10
42/42 - 5s - loss: 436.7176 - loglik: -4.3624e+02 - logprior: -4.8095e-01
Epoch 3/10
42/42 - 5s - loss: 436.0757 - loglik: -4.3560e+02 - logprior: -4.7872e-01
Epoch 4/10
42/42 - 5s - loss: 435.1222 - loglik: -4.3465e+02 - logprior: -4.7543e-01
Epoch 5/10
42/42 - 5s - loss: 435.9359 - loglik: -4.3545e+02 - logprior: -4.8228e-01
Fitted a model with MAP estimate = -434.3176
Time for alignment: 137.4435
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 510.9553 - loglik: -5.0973e+02 - logprior: -1.2291e+00
Epoch 2/10
29/29 - 4s - loss: 485.1508 - loglik: -4.8434e+02 - logprior: -8.0936e-01
Epoch 3/10
29/29 - 4s - loss: 481.1776 - loglik: -4.8039e+02 - logprior: -7.9028e-01
Epoch 4/10
29/29 - 4s - loss: 479.8317 - loglik: -4.7903e+02 - logprior: -8.0653e-01
Epoch 5/10
29/29 - 4s - loss: 479.3191 - loglik: -4.7851e+02 - logprior: -8.0695e-01
Epoch 6/10
29/29 - 4s - loss: 478.5192 - loglik: -4.7771e+02 - logprior: -8.1378e-01
Epoch 7/10
29/29 - 4s - loss: 477.9190 - loglik: -4.7708e+02 - logprior: -8.4165e-01
Epoch 8/10
29/29 - 4s - loss: 476.9970 - loglik: -4.7614e+02 - logprior: -8.5758e-01
Epoch 9/10
29/29 - 4s - loss: 476.9465 - loglik: -4.7605e+02 - logprior: -8.9291e-01
Epoch 10/10
29/29 - 4s - loss: 476.8980 - loglik: -4.7598e+02 - logprior: -9.1789e-01
Fitted a model with MAP estimate = -444.4895
expansions: [(1, 1), (2, 1), (13, 1), (14, 3), (15, 1), (22, 1), (35, 2), (39, 1), (41, 1), (43, 2), (44, 2), (46, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 481.6846 - loglik: -4.8048e+02 - logprior: -1.2023e+00
Epoch 2/2
29/29 - 4s - loss: 475.7114 - loglik: -4.7496e+02 - logprior: -7.4784e-01
Fitted a model with MAP estimate = -437.3833
expansions: [(16, 1)]
discards: [ 1 44 56 67]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 477.5626 - loglik: -4.7655e+02 - logprior: -1.0139e+00
Epoch 2/2
29/29 - 4s - loss: 475.9045 - loglik: -4.7520e+02 - logprior: -7.0908e-01
Fitted a model with MAP estimate = -437.5683
expansions: []
discards: [16 57]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 437.0433 - loglik: -4.3637e+02 - logprior: -6.7140e-01
Epoch 2/10
42/42 - 5s - loss: 436.7592 - loglik: -4.3628e+02 - logprior: -4.8377e-01
Epoch 3/10
42/42 - 5s - loss: 435.3835 - loglik: -4.3490e+02 - logprior: -4.8007e-01
Epoch 4/10
42/42 - 5s - loss: 436.2240 - loglik: -4.3575e+02 - logprior: -4.7664e-01
Fitted a model with MAP estimate = -435.1442
Time for alignment: 138.8813
Computed alignments with likelihoods: ['-432.9874', '-433.7963', '-435.1545', '-434.3176', '-435.1442']
Best model has likelihood: -432.9874  (prior= -0.5168 )
time for generating output: 0.2252
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Acetyltransf.projection.fasta
SP score = 0.606030647553139
Training of 5 independent models on file phoslip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5cf3cf8d60>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b0a46cc10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531d172730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531c5ae2e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5afea5fcd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f590038af10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b0a77baf0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5cda9f1460>, <__main__.SimpleDirichletPrior object at 0x7f53351bd460>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 734.0130 - loglik: -6.8182e+02 - logprior: -5.2196e+01
Epoch 2/10
10/10 - 1s - loss: 650.5248 - loglik: -6.3817e+02 - logprior: -1.2356e+01
Epoch 3/10
10/10 - 1s - loss: 602.9880 - loglik: -5.9749e+02 - logprior: -5.5028e+00
Epoch 4/10
10/10 - 1s - loss: 573.1887 - loglik: -5.6956e+02 - logprior: -3.6241e+00
Epoch 5/10
10/10 - 1s - loss: 562.1851 - loglik: -5.5944e+02 - logprior: -2.7483e+00
Epoch 6/10
10/10 - 1s - loss: 558.3519 - loglik: -5.5619e+02 - logprior: -2.1593e+00
Epoch 7/10
10/10 - 1s - loss: 555.9155 - loglik: -5.5413e+02 - logprior: -1.7817e+00
Epoch 8/10
10/10 - 1s - loss: 555.3585 - loglik: -5.5383e+02 - logprior: -1.5266e+00
Epoch 9/10
10/10 - 1s - loss: 554.4998 - loglik: -5.5313e+02 - logprior: -1.3656e+00
Epoch 10/10
10/10 - 1s - loss: 554.0367 - loglik: -5.5277e+02 - logprior: -1.2643e+00
Fitted a model with MAP estimate = -553.9083
expansions: [(10, 2), (11, 2), (12, 2), (13, 2), (16, 1), (30, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (63, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 604.6216 - loglik: -5.5161e+02 - logprior: -5.3007e+01
Epoch 2/2
10/10 - 1s - loss: 558.9576 - loglik: -5.3835e+02 - logprior: -2.0612e+01
Fitted a model with MAP estimate = -551.4292
expansions: [(0, 3)]
discards: [  0   9  18 102 103]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 578.1312 - loglik: -5.3743e+02 - logprior: -4.0699e+01
Epoch 2/2
10/10 - 1s - loss: 543.7550 - loglik: -5.3437e+02 - logprior: -9.3881e+00
Fitted a model with MAP estimate = -538.3338
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 586.0219 - loglik: -5.3658e+02 - logprior: -4.9437e+01
Epoch 2/10
10/10 - 1s - loss: 549.2842 - loglik: -5.3384e+02 - logprior: -1.5440e+01
Epoch 3/10
10/10 - 1s - loss: 538.5872 - loglik: -5.3407e+02 - logprior: -4.5166e+00
Epoch 4/10
10/10 - 1s - loss: 533.2450 - loglik: -5.3263e+02 - logprior: -6.1258e-01
Epoch 5/10
10/10 - 1s - loss: 531.0751 - loglik: -5.3194e+02 - logprior: 0.8655
Epoch 6/10
10/10 - 1s - loss: 530.1457 - loglik: -5.3205e+02 - logprior: 1.9034
Epoch 7/10
10/10 - 1s - loss: 529.7628 - loglik: -5.3241e+02 - logprior: 2.6479
Epoch 8/10
10/10 - 1s - loss: 528.3474 - loglik: -5.3142e+02 - logprior: 3.0706
Epoch 9/10
10/10 - 1s - loss: 528.7658 - loglik: -5.3217e+02 - logprior: 3.3996
Fitted a model with MAP estimate = -528.3667
Time for alignment: 50.3219
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 734.2841 - loglik: -6.8209e+02 - logprior: -5.2197e+01
Epoch 2/10
10/10 - 1s - loss: 650.9935 - loglik: -6.3865e+02 - logprior: -1.2345e+01
Epoch 3/10
10/10 - 1s - loss: 601.2090 - loglik: -5.9585e+02 - logprior: -5.3566e+00
Epoch 4/10
10/10 - 1s - loss: 576.1032 - loglik: -5.7292e+02 - logprior: -3.1850e+00
Epoch 5/10
10/10 - 1s - loss: 563.0858 - loglik: -5.6090e+02 - logprior: -2.1849e+00
Epoch 6/10
10/10 - 1s - loss: 558.7338 - loglik: -5.5715e+02 - logprior: -1.5815e+00
Epoch 7/10
10/10 - 1s - loss: 554.7807 - loglik: -5.5358e+02 - logprior: -1.2000e+00
Epoch 8/10
10/10 - 1s - loss: 554.6405 - loglik: -5.5359e+02 - logprior: -1.0481e+00
Epoch 9/10
10/10 - 1s - loss: 554.0881 - loglik: -5.5315e+02 - logprior: -9.3706e-01
Epoch 10/10
10/10 - 1s - loss: 552.9315 - loglik: -5.5209e+02 - logprior: -8.3783e-01
Fitted a model with MAP estimate = -553.0821
expansions: [(10, 2), (11, 3), (12, 3), (13, 2), (14, 1), (44, 2), (45, 2), (51, 2), (52, 1), (63, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 603.8475 - loglik: -5.5090e+02 - logprior: -5.2945e+01
Epoch 2/2
10/10 - 1s - loss: 559.2711 - loglik: -5.3879e+02 - logprior: -2.0484e+01
Fitted a model with MAP estimate = -551.5510
expansions: [(0, 3)]
discards: [  0   9  13 102 103]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 578.7881 - loglik: -5.3814e+02 - logprior: -4.0645e+01
Epoch 2/2
10/10 - 1s - loss: 544.8418 - loglik: -5.3542e+02 - logprior: -9.4213e+00
Fitted a model with MAP estimate = -538.8884
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 586.5891 - loglik: -5.3718e+02 - logprior: -4.9408e+01
Epoch 2/10
10/10 - 1s - loss: 550.0218 - loglik: -5.3472e+02 - logprior: -1.5307e+01
Epoch 3/10
10/10 - 1s - loss: 538.4167 - loglik: -5.3398e+02 - logprior: -4.4325e+00
Epoch 4/10
10/10 - 1s - loss: 533.3370 - loglik: -5.3273e+02 - logprior: -6.1052e-01
Epoch 5/10
10/10 - 1s - loss: 531.5613 - loglik: -5.3244e+02 - logprior: 0.8837
Epoch 6/10
10/10 - 1s - loss: 530.1851 - loglik: -5.3212e+02 - logprior: 1.9383
Epoch 7/10
10/10 - 1s - loss: 529.6238 - loglik: -5.3230e+02 - logprior: 2.6804
Epoch 8/10
10/10 - 1s - loss: 528.9095 - loglik: -5.3202e+02 - logprior: 3.1090
Epoch 9/10
10/10 - 1s - loss: 528.9902 - loglik: -5.3244e+02 - logprior: 3.4538
Fitted a model with MAP estimate = -528.5427
Time for alignment: 49.0813
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 734.5000 - loglik: -6.8230e+02 - logprior: -5.2198e+01
Epoch 2/10
10/10 - 1s - loss: 651.2767 - loglik: -6.3892e+02 - logprior: -1.2362e+01
Epoch 3/10
10/10 - 1s - loss: 603.4102 - loglik: -5.9788e+02 - logprior: -5.5297e+00
Epoch 4/10
10/10 - 1s - loss: 576.1768 - loglik: -5.7260e+02 - logprior: -3.5727e+00
Epoch 5/10
10/10 - 1s - loss: 563.2003 - loglik: -5.6053e+02 - logprior: -2.6723e+00
Epoch 6/10
10/10 - 1s - loss: 558.8160 - loglik: -5.5669e+02 - logprior: -2.1230e+00
Epoch 7/10
10/10 - 1s - loss: 555.9095 - loglik: -5.5413e+02 - logprior: -1.7755e+00
Epoch 8/10
10/10 - 1s - loss: 554.9089 - loglik: -5.5339e+02 - logprior: -1.5215e+00
Epoch 9/10
10/10 - 1s - loss: 554.4736 - loglik: -5.5310e+02 - logprior: -1.3691e+00
Epoch 10/10
10/10 - 1s - loss: 554.4703 - loglik: -5.5319e+02 - logprior: -1.2764e+00
Fitted a model with MAP estimate = -553.9322
expansions: [(10, 2), (11, 2), (12, 2), (13, 1), (16, 1), (30, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (63, 1), (79, 1), (81, 2), (82, 2), (96, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 603.5051 - loglik: -5.5031e+02 - logprior: -5.3197e+01
Epoch 2/2
10/10 - 1s - loss: 557.3861 - loglik: -5.3666e+02 - logprior: -2.0725e+01
Fitted a model with MAP estimate = -548.4508
expansions: [(0, 3)]
discards: [  0   9 101 102 118 119 120 121 122]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 578.0727 - loglik: -5.3744e+02 - logprior: -4.0630e+01
Epoch 2/2
10/10 - 1s - loss: 543.5079 - loglik: -5.3417e+02 - logprior: -9.3423e+00
Fitted a model with MAP estimate = -538.5582
expansions: [(117, 6)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 586.4114 - loglik: -5.3614e+02 - logprior: -5.0275e+01
Epoch 2/10
10/10 - 1s - loss: 549.6026 - loglik: -5.3230e+02 - logprior: -1.7304e+01
Epoch 3/10
10/10 - 1s - loss: 535.2531 - loglik: -5.2932e+02 - logprior: -5.9350e+00
Epoch 4/10
10/10 - 1s - loss: 530.5773 - loglik: -5.2967e+02 - logprior: -9.0672e-01
Epoch 5/10
10/10 - 1s - loss: 527.3953 - loglik: -5.2818e+02 - logprior: 0.7800
Epoch 6/10
10/10 - 1s - loss: 526.0485 - loglik: -5.2782e+02 - logprior: 1.7762
Epoch 7/10
10/10 - 1s - loss: 525.7993 - loglik: -5.2829e+02 - logprior: 2.4956
Epoch 8/10
10/10 - 1s - loss: 524.9794 - loglik: -5.2795e+02 - logprior: 2.9663
Epoch 9/10
10/10 - 1s - loss: 525.3016 - loglik: -5.2856e+02 - logprior: 3.2623
Fitted a model with MAP estimate = -524.7431
Time for alignment: 48.6684
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 734.0526 - loglik: -6.8185e+02 - logprior: -5.2199e+01
Epoch 2/10
10/10 - 1s - loss: 650.9269 - loglik: -6.3858e+02 - logprior: -1.2352e+01
Epoch 3/10
10/10 - 1s - loss: 601.6543 - loglik: -5.9625e+02 - logprior: -5.4090e+00
Epoch 4/10
10/10 - 1s - loss: 576.5862 - loglik: -5.7333e+02 - logprior: -3.2591e+00
Epoch 5/10
10/10 - 1s - loss: 568.4822 - loglik: -5.6628e+02 - logprior: -2.2032e+00
Epoch 6/10
10/10 - 1s - loss: 563.2955 - loglik: -5.6175e+02 - logprior: -1.5498e+00
Epoch 7/10
10/10 - 1s - loss: 557.6256 - loglik: -5.5638e+02 - logprior: -1.2420e+00
Epoch 8/10
10/10 - 1s - loss: 554.6971 - loglik: -5.5359e+02 - logprior: -1.1108e+00
Epoch 9/10
10/10 - 1s - loss: 553.7925 - loglik: -5.5269e+02 - logprior: -1.1000e+00
Epoch 10/10
10/10 - 1s - loss: 552.1472 - loglik: -5.5103e+02 - logprior: -1.1145e+00
Fitted a model with MAP estimate = -551.9404
expansions: [(10, 2), (11, 3), (12, 3), (13, 2), (14, 1), (44, 2), (45, 2), (50, 1), (51, 2), (56, 1), (58, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 603.5312 - loglik: -5.5046e+02 - logprior: -5.3075e+01
Epoch 2/2
10/10 - 1s - loss: 559.1422 - loglik: -5.3852e+02 - logprior: -2.0621e+01
Fitted a model with MAP estimate = -551.2800
expansions: [(0, 3)]
discards: [  0   9  13  65  66 103 104]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 580.7452 - loglik: -5.4000e+02 - logprior: -4.0744e+01
Epoch 2/2
10/10 - 1s - loss: 544.9222 - loglik: -5.3532e+02 - logprior: -9.6059e+00
Fitted a model with MAP estimate = -539.5148
expansions: [(65, 1)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 586.9919 - loglik: -5.3711e+02 - logprior: -4.9878e+01
Epoch 2/10
10/10 - 1s - loss: 551.5248 - loglik: -5.3519e+02 - logprior: -1.6339e+01
Epoch 3/10
10/10 - 1s - loss: 538.9042 - loglik: -5.3380e+02 - logprior: -5.1038e+00
Epoch 4/10
10/10 - 1s - loss: 534.6001 - loglik: -5.3387e+02 - logprior: -7.2872e-01
Epoch 5/10
10/10 - 1s - loss: 531.3995 - loglik: -5.3224e+02 - logprior: 0.8450
Epoch 6/10
10/10 - 1s - loss: 530.3144 - loglik: -5.3223e+02 - logprior: 1.9153
Epoch 7/10
10/10 - 1s - loss: 530.0300 - loglik: -5.3269e+02 - logprior: 2.6567
Epoch 8/10
10/10 - 1s - loss: 529.5464 - loglik: -5.3262e+02 - logprior: 3.0741
Epoch 9/10
10/10 - 1s - loss: 527.6680 - loglik: -5.3110e+02 - logprior: 3.4278
Epoch 10/10
10/10 - 1s - loss: 528.4969 - loglik: -5.3225e+02 - logprior: 3.7519
Fitted a model with MAP estimate = -528.1930
Time for alignment: 50.6968
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 733.8956 - loglik: -6.8170e+02 - logprior: -5.2199e+01
Epoch 2/10
10/10 - 1s - loss: 651.4642 - loglik: -6.3911e+02 - logprior: -1.2357e+01
Epoch 3/10
10/10 - 1s - loss: 602.0074 - loglik: -5.9661e+02 - logprior: -5.4005e+00
Epoch 4/10
10/10 - 1s - loss: 575.0863 - loglik: -5.7178e+02 - logprior: -3.3028e+00
Epoch 5/10
10/10 - 1s - loss: 563.6029 - loglik: -5.6098e+02 - logprior: -2.6272e+00
Epoch 6/10
10/10 - 1s - loss: 558.1523 - loglik: -5.5592e+02 - logprior: -2.2285e+00
Epoch 7/10
10/10 - 1s - loss: 556.1206 - loglik: -5.5427e+02 - logprior: -1.8537e+00
Epoch 8/10
10/10 - 1s - loss: 555.1512 - loglik: -5.5357e+02 - logprior: -1.5847e+00
Epoch 9/10
10/10 - 1s - loss: 554.3568 - loglik: -5.5291e+02 - logprior: -1.4422e+00
Epoch 10/10
10/10 - 1s - loss: 554.0603 - loglik: -5.5273e+02 - logprior: -1.3318e+00
Fitted a model with MAP estimate = -553.9225
expansions: [(10, 2), (11, 2), (12, 2), (13, 2), (16, 1), (30, 1), (31, 1), (44, 2), (45, 2), (51, 2), (52, 1), (68, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 604.2928 - loglik: -5.5127e+02 - logprior: -5.3020e+01
Epoch 2/2
10/10 - 1s - loss: 560.6483 - loglik: -5.4004e+02 - logprior: -2.0606e+01
Fitted a model with MAP estimate = -551.8874
expansions: [(0, 3)]
discards: [  0   9  18 102 103]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 578.8055 - loglik: -5.3805e+02 - logprior: -4.0759e+01
Epoch 2/2
10/10 - 1s - loss: 543.9609 - loglik: -5.3456e+02 - logprior: -9.4034e+00
Fitted a model with MAP estimate = -538.6002
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 585.7894 - loglik: -5.3640e+02 - logprior: -4.9392e+01
Epoch 2/10
10/10 - 1s - loss: 550.3783 - loglik: -5.3514e+02 - logprior: -1.5241e+01
Epoch 3/10
10/10 - 1s - loss: 537.7820 - loglik: -5.3339e+02 - logprior: -4.3948e+00
Epoch 4/10
10/10 - 1s - loss: 533.0464 - loglik: -5.3244e+02 - logprior: -6.0639e-01
Epoch 5/10
10/10 - 1s - loss: 531.5817 - loglik: -5.3245e+02 - logprior: 0.8710
Epoch 6/10
10/10 - 1s - loss: 530.6659 - loglik: -5.3260e+02 - logprior: 1.9324
Epoch 7/10
10/10 - 1s - loss: 528.7985 - loglik: -5.3147e+02 - logprior: 2.6731
Epoch 8/10
10/10 - 1s - loss: 529.0539 - loglik: -5.3215e+02 - logprior: 3.0913
Fitted a model with MAP estimate = -528.6655
Time for alignment: 47.2190
Computed alignments with likelihoods: ['-528.3667', '-528.5427', '-524.7431', '-528.1930', '-528.6655']
Best model has likelihood: -524.7431  (prior= 3.4124 )
time for generating output: 0.1731
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phoslip.projection.fasta
SP score = 0.9436494236872877
Training of 5 independent models on file cah.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5ca6ff84f0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b09ace4f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f533418b340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b083c48b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f594015d5e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5aff425730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f53344dc820>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5cc0dda7f0>, <__main__.SimpleDirichletPrior object at 0x7f531d6dabe0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 1264.3497 - loglik: -1.2346e+03 - logprior: -2.9751e+01
Epoch 2/10
14/14 - 4s - loss: 1165.7966 - loglik: -1.1628e+03 - logprior: -3.0150e+00
Epoch 3/10
14/14 - 4s - loss: 1110.4777 - loglik: -1.1095e+03 - logprior: -1.0183e+00
Epoch 4/10
14/14 - 4s - loss: 1094.6042 - loglik: -1.0940e+03 - logprior: -5.9237e-01
Epoch 5/10
14/14 - 4s - loss: 1084.5813 - loglik: -1.0842e+03 - logprior: -3.5893e-01
Epoch 6/10
14/14 - 4s - loss: 1085.7870 - loglik: -1.0856e+03 - logprior: -1.7278e-01
Fitted a model with MAP estimate = -1084.1680
expansions: [(14, 1), (15, 1), (28, 1), (30, 1), (31, 2), (32, 2), (36, 1), (41, 2), (42, 1), (51, 1), (52, 1), (53, 1), (54, 4), (64, 1), (80, 1), (90, 1), (110, 1), (114, 2), (115, 2), (116, 3), (119, 1), (120, 1), (121, 3), (131, 2), (161, 1), (164, 2), (166, 2), (167, 2), (168, 5), (180, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 1108.2892 - loglik: -1.0798e+03 - logprior: -2.8483e+01
Epoch 2/2
14/14 - 5s - loss: 1075.4430 - loglik: -1.0675e+03 - logprior: -7.9332e+00
Fitted a model with MAP estimate = -1070.9870
expansions: [(69, 2), (138, 1)]
discards: [  0  72  73  74 143 166 206 213]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 1102.2408 - loglik: -1.0765e+03 - logprior: -2.5753e+01
Epoch 2/2
14/14 - 5s - loss: 1065.1696 - loglik: -1.0593e+03 - logprior: -5.8663e+00
Fitted a model with MAP estimate = -1067.4982
expansions: [(0, 4), (77, 3)]
discards: [ 0 49 73]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 1084.6578 - loglik: -1.0661e+03 - logprior: -1.8608e+01
Epoch 2/10
14/14 - 5s - loss: 1063.5774 - loglik: -1.0639e+03 - logprior: 0.3369
Epoch 3/10
14/14 - 5s - loss: 1058.9417 - loglik: -1.0624e+03 - logprior: 3.4137
Epoch 4/10
14/14 - 5s - loss: 1055.5690 - loglik: -1.0602e+03 - logprior: 4.6070
Epoch 5/10
14/14 - 5s - loss: 1058.7103 - loglik: -1.0639e+03 - logprior: 5.1618
Fitted a model with MAP estimate = -1055.9895
Time for alignment: 97.3163
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 1263.7609 - loglik: -1.2340e+03 - logprior: -2.9747e+01
Epoch 2/10
14/14 - 4s - loss: 1168.9602 - loglik: -1.1660e+03 - logprior: -2.9987e+00
Epoch 3/10
14/14 - 4s - loss: 1106.0217 - loglik: -1.1050e+03 - logprior: -1.0489e+00
Epoch 4/10
14/14 - 4s - loss: 1092.7928 - loglik: -1.0924e+03 - logprior: -4.3866e-01
Epoch 5/10
14/14 - 4s - loss: 1087.2466 - loglik: -1.0871e+03 - logprior: -1.6102e-01
Epoch 6/10
14/14 - 4s - loss: 1083.7257 - loglik: -1.0838e+03 - logprior: 0.0343
Epoch 7/10
14/14 - 4s - loss: 1084.9330 - loglik: -1.0851e+03 - logprior: 0.1925
Fitted a model with MAP estimate = -1084.2379
expansions: [(15, 1), (16, 2), (28, 1), (30, 1), (31, 2), (32, 2), (40, 2), (41, 2), (42, 1), (51, 1), (52, 1), (53, 1), (54, 3), (65, 1), (80, 1), (90, 1), (110, 1), (112, 1), (113, 2), (114, 2), (115, 2), (118, 1), (119, 2), (120, 1), (130, 2), (165, 1), (166, 9), (167, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 1107.9054 - loglik: -1.0795e+03 - logprior: -2.8428e+01
Epoch 2/2
14/14 - 5s - loss: 1074.1404 - loglik: -1.0661e+03 - logprior: -7.9966e+00
Fitted a model with MAP estimate = -1069.7638
expansions: [(139, 1)]
discards: [  0  18  49  91 165 211]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 1096.2701 - loglik: -1.0706e+03 - logprior: -2.5715e+01
Epoch 2/2
14/14 - 5s - loss: 1071.0463 - loglik: -1.0650e+03 - logprior: -5.9999e+00
Fitted a model with MAP estimate = -1066.2331
expansions: [(0, 4), (68, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 1083.7113 - loglik: -1.0652e+03 - logprior: -1.8539e+01
Epoch 2/10
14/14 - 5s - loss: 1062.4408 - loglik: -1.0629e+03 - logprior: 0.4284
Epoch 3/10
14/14 - 5s - loss: 1054.0089 - loglik: -1.0575e+03 - logprior: 3.5365
Epoch 4/10
14/14 - 5s - loss: 1059.4552 - loglik: -1.0642e+03 - logprior: 4.7276
Fitted a model with MAP estimate = -1055.4960
Time for alignment: 95.4861
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 1265.0419 - loglik: -1.2353e+03 - logprior: -2.9767e+01
Epoch 2/10
14/14 - 4s - loss: 1166.0010 - loglik: -1.1630e+03 - logprior: -2.9974e+00
Epoch 3/10
14/14 - 4s - loss: 1114.5135 - loglik: -1.1135e+03 - logprior: -1.0199e+00
Epoch 4/10
14/14 - 4s - loss: 1090.2572 - loglik: -1.0897e+03 - logprior: -5.9855e-01
Epoch 5/10
14/14 - 4s - loss: 1084.3474 - loglik: -1.0840e+03 - logprior: -3.3526e-01
Epoch 6/10
14/14 - 4s - loss: 1081.7155 - loglik: -1.0816e+03 - logprior: -1.1625e-01
Epoch 7/10
14/14 - 4s - loss: 1080.6901 - loglik: -1.0808e+03 - logprior: 0.0837
Epoch 8/10
14/14 - 4s - loss: 1084.9404 - loglik: -1.0852e+03 - logprior: 0.2650
Fitted a model with MAP estimate = -1081.3326
expansions: [(15, 2), (16, 2), (28, 1), (30, 1), (31, 2), (32, 2), (36, 1), (40, 1), (41, 2), (42, 2), (50, 1), (51, 1), (54, 3), (61, 2), (90, 1), (91, 7), (112, 1), (113, 2), (114, 2), (115, 2), (118, 1), (119, 1), (120, 3), (154, 1), (157, 1), (163, 1), (166, 2), (167, 7)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 1108.3331 - loglik: -1.0797e+03 - logprior: -2.8655e+01
Epoch 2/2
14/14 - 5s - loss: 1070.4030 - loglik: -1.0624e+03 - logprior: -8.0480e+00
Fitted a model with MAP estimate = -1068.0900
expansions: [(0, 4), (120, 1), (146, 1)]
discards: [  0  18  19  50  55 116 212]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 1086.8368 - loglik: -1.0675e+03 - logprior: -1.9354e+01
Epoch 2/2
14/14 - 5s - loss: 1057.5272 - loglik: -1.0570e+03 - logprior: -5.4807e-01
Fitted a model with MAP estimate = -1057.7649
expansions: [(16, 1), (71, 2), (119, 1), (215, 2)]
discards: [  1   2   3  17  80 113 114 115]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 1084.4128 - loglik: -1.0660e+03 - logprior: -1.8449e+01
Epoch 2/10
14/14 - 5s - loss: 1058.4166 - loglik: -1.0587e+03 - logprior: 0.3041
Epoch 3/10
14/14 - 5s - loss: 1055.6245 - loglik: -1.0588e+03 - logprior: 3.1760
Epoch 4/10
14/14 - 5s - loss: 1052.6835 - loglik: -1.0572e+03 - logprior: 4.4694
Epoch 5/10
14/14 - 5s - loss: 1053.6239 - loglik: -1.0589e+03 - logprior: 5.2835
Fitted a model with MAP estimate = -1052.3371
Time for alignment: 105.3973
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 1260.1766 - loglik: -1.2304e+03 - logprior: -2.9744e+01
Epoch 2/10
14/14 - 4s - loss: 1171.2195 - loglik: -1.1682e+03 - logprior: -3.0311e+00
Epoch 3/10
14/14 - 4s - loss: 1110.6123 - loglik: -1.1096e+03 - logprior: -1.0394e+00
Epoch 4/10
14/14 - 4s - loss: 1090.8595 - loglik: -1.0904e+03 - logprior: -4.8366e-01
Epoch 5/10
14/14 - 4s - loss: 1086.0193 - loglik: -1.0859e+03 - logprior: -1.6651e-01
Epoch 6/10
14/14 - 4s - loss: 1085.0306 - loglik: -1.0851e+03 - logprior: 0.0350
Epoch 7/10
14/14 - 4s - loss: 1085.0272 - loglik: -1.0853e+03 - logprior: 0.2260
Epoch 8/10
14/14 - 4s - loss: 1083.2753 - loglik: -1.0837e+03 - logprior: 0.3791
Epoch 9/10
14/14 - 4s - loss: 1086.4832 - loglik: -1.0869e+03 - logprior: 0.3802
Fitted a model with MAP estimate = -1082.9850
expansions: [(14, 1), (15, 1), (16, 2), (28, 1), (30, 1), (31, 2), (32, 2), (36, 1), (40, 1), (41, 2), (42, 2), (50, 1), (51, 1), (52, 1), (53, 3), (64, 1), (80, 1), (90, 1), (113, 1), (114, 1), (115, 1), (116, 4), (118, 2), (119, 2), (120, 3), (130, 2), (154, 1), (163, 2), (165, 1), (166, 6), (167, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 1107.4537 - loglik: -1.0790e+03 - logprior: -2.8433e+01
Epoch 2/2
14/14 - 5s - loss: 1074.1068 - loglik: -1.0663e+03 - logprior: -7.7677e+00
Fitted a model with MAP estimate = -1070.5621
expansions: [(0, 4), (144, 1)]
discards: [  0  18  50  55 152 155 169]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 1084.4949 - loglik: -1.0655e+03 - logprior: -1.8974e+01
Epoch 2/2
14/14 - 5s - loss: 1062.8600 - loglik: -1.0630e+03 - logprior: 0.0930
Fitted a model with MAP estimate = -1060.2763
expansions: []
discards: [ 1  2  3 23]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 1083.8060 - loglik: -1.0656e+03 - logprior: -1.8183e+01
Epoch 2/10
14/14 - 5s - loss: 1060.8394 - loglik: -1.0613e+03 - logprior: 0.4535
Epoch 3/10
14/14 - 5s - loss: 1057.9377 - loglik: -1.0612e+03 - logprior: 3.2803
Epoch 4/10
14/14 - 5s - loss: 1058.4647 - loglik: -1.0630e+03 - logprior: 4.5625
Fitted a model with MAP estimate = -1056.1599
Time for alignment: 101.2602
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 1265.5934 - loglik: -1.2358e+03 - logprior: -2.9757e+01
Epoch 2/10
14/14 - 4s - loss: 1162.5632 - loglik: -1.1596e+03 - logprior: -2.9609e+00
Epoch 3/10
14/14 - 4s - loss: 1105.6565 - loglik: -1.1048e+03 - logprior: -9.0587e-01
Epoch 4/10
14/14 - 4s - loss: 1090.9440 - loglik: -1.0904e+03 - logprior: -5.6212e-01
Epoch 5/10
14/14 - 4s - loss: 1087.4519 - loglik: -1.0870e+03 - logprior: -4.1836e-01
Epoch 6/10
14/14 - 4s - loss: 1085.2444 - loglik: -1.0850e+03 - logprior: -2.5265e-01
Epoch 7/10
14/14 - 4s - loss: 1082.5691 - loglik: -1.0824e+03 - logprior: -1.8541e-01
Epoch 8/10
14/14 - 4s - loss: 1081.0315 - loglik: -1.0809e+03 - logprior: -9.5279e-02
Epoch 9/10
14/14 - 4s - loss: 1077.6586 - loglik: -1.0776e+03 - logprior: -8.6970e-03
Epoch 10/10
14/14 - 4s - loss: 1082.2106 - loglik: -1.0823e+03 - logprior: 0.0909
Fitted a model with MAP estimate = -1079.7022
expansions: [(14, 3), (16, 1), (28, 1), (29, 1), (30, 2), (31, 2), (39, 2), (40, 2), (41, 1), (43, 1), (49, 1), (50, 1), (51, 1), (52, 3), (79, 1), (89, 1), (112, 1), (113, 3), (114, 2), (115, 1), (116, 1), (118, 1), (119, 1), (120, 1), (125, 1), (158, 1), (164, 1), (166, 2), (167, 8)]
discards: [ 0 70]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 1106.5367 - loglik: -1.0783e+03 - logprior: -2.8248e+01
Epoch 2/2
14/14 - 5s - loss: 1074.0034 - loglik: -1.0663e+03 - logprior: -7.6749e+00
Fitted a model with MAP estimate = -1069.5947
expansions: [(0, 4)]
discards: [ 0 14 49 51]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 1086.0865 - loglik: -1.0671e+03 - logprior: -1.9011e+01
Epoch 2/2
14/14 - 5s - loss: 1061.9323 - loglik: -1.0621e+03 - logprior: 0.1727
Fitted a model with MAP estimate = -1060.4246
expansions: []
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 227 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 1081.6946 - loglik: -1.0636e+03 - logprior: -1.8129e+01
Epoch 2/10
14/14 - 5s - loss: 1061.6613 - loglik: -1.0622e+03 - logprior: 0.5579
Epoch 3/10
14/14 - 5s - loss: 1058.9191 - loglik: -1.0623e+03 - logprior: 3.3987
Epoch 4/10
14/14 - 5s - loss: 1060.0691 - loglik: -1.0647e+03 - logprior: 4.6442
Fitted a model with MAP estimate = -1056.2011
Time for alignment: 106.7002
Computed alignments with likelihoods: ['-1055.9895', '-1055.4960', '-1052.3371', '-1056.1599', '-1056.2011']
Best model has likelihood: -1052.3371  (prior= 5.5856 )
time for generating output: 0.2768
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cah.projection.fasta
SP score = 0.8778065630397237
Training of 5 independent models on file trfl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5ca7b6d400>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5334fb0940>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5334fb05e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f598c271bb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f598c271850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f598c271d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f598c271eb0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f533486d100>, <__main__.SimpleDirichletPrior object at 0x7f531df99eb0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 1472.2052 - loglik: -1.4128e+03 - logprior: -5.9356e+01
Epoch 2/10
11/11 - 8s - loss: 1344.1143 - loglik: -1.3376e+03 - logprior: -6.5162e+00
Epoch 3/10
11/11 - 9s - loss: 1254.1171 - loglik: -1.2544e+03 - logprior: 0.3081
Epoch 4/10
11/11 - 8s - loss: 1203.2323 - loglik: -1.2046e+03 - logprior: 1.3225
Epoch 5/10
11/11 - 9s - loss: 1192.1227 - loglik: -1.1938e+03 - logprior: 1.6694
Epoch 6/10
11/11 - 8s - loss: 1183.8315 - loglik: -1.1859e+03 - logprior: 2.1052
Epoch 7/10
11/11 - 8s - loss: 1175.5905 - loglik: -1.1782e+03 - logprior: 2.6458
Epoch 8/10
11/11 - 9s - loss: 1184.5151 - loglik: -1.1875e+03 - logprior: 2.9381
Fitted a model with MAP estimate = -1179.3754
expansions: [(22, 4), (33, 1), (36, 1), (42, 1), (49, 1), (64, 1), (65, 3), (66, 1), (79, 1), (80, 1), (81, 1), (92, 1), (94, 1), (104, 4), (105, 1), (106, 2), (109, 1), (162, 1), (163, 6), (169, 3), (181, 1), (182, 1), (183, 3), (199, 7), (200, 2), (201, 5), (220, 1), (222, 2), (224, 1), (225, 1)]
discards: [  0 209]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 15s - loss: 1234.6694 - loglik: -1.1826e+03 - logprior: -5.2026e+01
Epoch 2/2
11/11 - 10s - loss: 1163.6854 - loglik: -1.1501e+03 - logprior: -1.3572e+01
Fitted a model with MAP estimate = -1158.7496
expansions: [(0, 2), (220, 1), (242, 1), (248, 1)]
discards: [  0  75  76  77 123 124 128 201]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 16s - loss: 1189.1968 - loglik: -1.1541e+03 - logprior: -3.5146e+01
Epoch 2/2
11/11 - 12s - loss: 1143.4414 - loglik: -1.1436e+03 - logprior: 0.1227
Fitted a model with MAP estimate = -1140.0657
expansions: [(22, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 16s - loss: 1187.9907 - loglik: -1.1432e+03 - logprior: -4.4773e+01
Epoch 2/10
11/11 - 12s - loss: 1165.7841 - loglik: -1.1563e+03 - logprior: -9.4709e+00
Epoch 3/10
11/11 - 12s - loss: 1134.5374 - loglik: -1.1373e+03 - logprior: 2.7175
Epoch 4/10
11/11 - 12s - loss: 1130.2559 - loglik: -1.1405e+03 - logprior: 10.2401
Epoch 5/10
11/11 - 12s - loss: 1131.3501 - loglik: -1.1438e+03 - logprior: 12.4205
Fitted a model with MAP estimate = -1128.0765
Time for alignment: 208.4752
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 13s - loss: 1466.5287 - loglik: -1.4072e+03 - logprior: -5.9356e+01
Epoch 2/10
11/11 - 9s - loss: 1351.5221 - loglik: -1.3450e+03 - logprior: -6.5130e+00
Epoch 3/10
11/11 - 10s - loss: 1257.3875 - loglik: -1.2578e+03 - logprior: 0.4214
Epoch 4/10
11/11 - 9s - loss: 1208.7314 - loglik: -1.2104e+03 - logprior: 1.6221
Epoch 5/10
11/11 - 9s - loss: 1197.6320 - loglik: -1.1996e+03 - logprior: 1.9273
Epoch 6/10
11/11 - 9s - loss: 1186.2814 - loglik: -1.1886e+03 - logprior: 2.3426
Epoch 7/10
11/11 - 8s - loss: 1179.4636 - loglik: -1.1823e+03 - logprior: 2.8473
Epoch 8/10
11/11 - 10s - loss: 1184.0239 - loglik: -1.1872e+03 - logprior: 3.2082
Fitted a model with MAP estimate = -1181.6764
expansions: [(22, 3), (25, 1), (36, 1), (43, 1), (47, 1), (52, 2), (64, 1), (65, 3), (66, 1), (79, 3), (80, 2), (91, 2), (92, 2), (102, 3), (103, 1), (104, 1), (108, 1), (137, 1), (162, 1), (163, 5), (180, 1), (181, 5), (197, 3), (198, 3), (199, 2), (200, 5), (222, 2), (224, 1), (225, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 15s - loss: 1230.9353 - loglik: -1.1790e+03 - logprior: -5.1969e+01
Epoch 2/2
11/11 - 12s - loss: 1173.0209 - loglik: -1.1591e+03 - logprior: -1.3916e+01
Fitted a model with MAP estimate = -1161.2909
expansions: [(0, 2), (200, 1), (219, 1), (240, 1), (247, 1)]
discards: [  0  59  76  77  78  96 112 125 126]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 15s - loss: 1190.4445 - loglik: -1.1551e+03 - logprior: -3.5371e+01
Epoch 2/2
11/11 - 11s - loss: 1145.6989 - loglik: -1.1456e+03 - logprior: -1.0177e-01
Fitted a model with MAP estimate = -1141.8408
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 1197.2677 - loglik: -1.1524e+03 - logprior: -4.4827e+01
Epoch 2/10
11/11 - 13s - loss: 1156.3143 - loglik: -1.1470e+03 - logprior: -9.2826e+00
Epoch 3/10
11/11 - 11s - loss: 1141.0079 - loglik: -1.1443e+03 - logprior: 3.2837
Epoch 4/10
11/11 - 12s - loss: 1135.2074 - loglik: -1.1453e+03 - logprior: 10.1165
Epoch 5/10
11/11 - 12s - loss: 1125.3788 - loglik: -1.1376e+03 - logprior: 12.2367
Epoch 6/10
11/11 - 11s - loss: 1135.8530 - loglik: -1.1493e+03 - logprior: 13.4480
Fitted a model with MAP estimate = -1129.0262
Time for alignment: 225.7943
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 13s - loss: 1469.9020 - loglik: -1.4105e+03 - logprior: -5.9359e+01
Epoch 2/10
11/11 - 9s - loss: 1345.4618 - loglik: -1.3389e+03 - logprior: -6.5600e+00
Epoch 3/10
11/11 - 9s - loss: 1256.6626 - loglik: -1.2567e+03 - logprior: 0.0073
Epoch 4/10
11/11 - 9s - loss: 1203.2786 - loglik: -1.2044e+03 - logprior: 1.0931
Epoch 5/10
11/11 - 10s - loss: 1189.1816 - loglik: -1.1906e+03 - logprior: 1.3861
Epoch 6/10
11/11 - 9s - loss: 1184.5085 - loglik: -1.1865e+03 - logprior: 2.0191
Epoch 7/10
11/11 - 9s - loss: 1183.0782 - loglik: -1.1855e+03 - logprior: 2.4078
Epoch 8/10
11/11 - 9s - loss: 1179.2706 - loglik: -1.1820e+03 - logprior: 2.7259
Epoch 9/10
11/11 - 9s - loss: 1183.4873 - loglik: -1.1865e+03 - logprior: 3.0276
Fitted a model with MAP estimate = -1179.3106
expansions: [(19, 4), (21, 1), (22, 1), (24, 1), (32, 1), (35, 1), (41, 1), (45, 1), (50, 2), (62, 4), (63, 1), (76, 1), (77, 1), (78, 1), (80, 1), (102, 4), (103, 1), (104, 1), (161, 7), (179, 1), (180, 1), (181, 1), (182, 2), (196, 1), (197, 3), (198, 1), (199, 2), (200, 2), (202, 4), (223, 2), (224, 1), (225, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 15s - loss: 1229.8734 - loglik: -1.1786e+03 - logprior: -5.1279e+01
Epoch 2/2
11/11 - 12s - loss: 1166.6344 - loglik: -1.1535e+03 - logprior: -1.3153e+01
Fitted a model with MAP estimate = -1160.4748
expansions: [(0, 2), (198, 1), (254, 2), (276, 1)]
discards: [  0  21  61  77 125 126 296 297]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 15s - loss: 1191.6471 - loglik: -1.1564e+03 - logprior: -3.5211e+01
Epoch 2/2
11/11 - 11s - loss: 1147.7494 - loglik: -1.1479e+03 - logprior: 0.1851
Fitted a model with MAP estimate = -1143.9598
expansions: [(241, 1), (296, 1)]
discards: [  0  76 277]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 16s - loss: 1201.7065 - loglik: -1.1572e+03 - logprior: -4.4520e+01
Epoch 2/10
11/11 - 12s - loss: 1158.1945 - loglik: -1.1495e+03 - logprior: -8.6488e+00
Epoch 3/10
11/11 - 13s - loss: 1142.3717 - loglik: -1.1465e+03 - logprior: 4.1651
Epoch 4/10
11/11 - 12s - loss: 1138.1571 - loglik: -1.1484e+03 - logprior: 10.2889
Epoch 5/10
11/11 - 11s - loss: 1128.0020 - loglik: -1.1402e+03 - logprior: 12.2375
Epoch 6/10
11/11 - 10s - loss: 1135.5156 - loglik: -1.1489e+03 - logprior: 13.3416
Fitted a model with MAP estimate = -1130.5862
Time for alignment: 234.4691
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 1473.8110 - loglik: -1.4145e+03 - logprior: -5.9343e+01
Epoch 2/10
11/11 - 8s - loss: 1343.4017 - loglik: -1.3370e+03 - logprior: -6.4487e+00
Epoch 3/10
11/11 - 9s - loss: 1257.4426 - loglik: -1.2578e+03 - logprior: 0.3754
Epoch 4/10
11/11 - 9s - loss: 1207.5549 - loglik: -1.2093e+03 - logprior: 1.7537
Epoch 5/10
11/11 - 9s - loss: 1189.5546 - loglik: -1.1918e+03 - logprior: 2.2918
Epoch 6/10
11/11 - 9s - loss: 1188.7472 - loglik: -1.1915e+03 - logprior: 2.7184
Epoch 7/10
11/11 - 8s - loss: 1181.1229 - loglik: -1.1843e+03 - logprior: 3.1363
Epoch 8/10
11/11 - 9s - loss: 1182.8379 - loglik: -1.1863e+03 - logprior: 3.4842
Fitted a model with MAP estimate = -1181.7616
expansions: [(22, 3), (38, 1), (49, 1), (54, 2), (63, 1), (65, 4), (66, 1), (79, 1), (80, 2), (82, 1), (102, 1), (103, 1), (104, 6), (105, 1), (107, 1), (108, 1), (137, 1), (162, 1), (163, 5), (180, 1), (181, 5), (197, 5), (200, 2), (201, 5), (222, 2), (224, 1), (225, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 1231.3813 - loglik: -1.1799e+03 - logprior: -5.1432e+01
Epoch 2/2
11/11 - 13s - loss: 1175.8425 - loglik: -1.1625e+03 - logprior: -1.3328e+01
Fitted a model with MAP estimate = -1164.3641
expansions: [(0, 2), (218, 1), (245, 1)]
discards: [  0  59  75  93 124 125 126]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 15s - loss: 1195.2681 - loglik: -1.1602e+03 - logprior: -3.5101e+01
Epoch 2/2
11/11 - 12s - loss: 1151.4460 - loglik: -1.1513e+03 - logprior: -1.1217e-01
Fitted a model with MAP estimate = -1145.1453
expansions: [(236, 1)]
discards: [ 0 75]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 1197.1782 - loglik: -1.1524e+03 - logprior: -4.4799e+01
Epoch 2/10
11/11 - 13s - loss: 1157.6567 - loglik: -1.1492e+03 - logprior: -8.4863e+00
Epoch 3/10
11/11 - 12s - loss: 1148.4613 - loglik: -1.1530e+03 - logprior: 4.4889
Epoch 4/10
11/11 - 12s - loss: 1140.8051 - loglik: -1.1511e+03 - logprior: 10.3022
Epoch 5/10
11/11 - 11s - loss: 1130.9445 - loglik: -1.1431e+03 - logprior: 12.1827
Epoch 6/10
11/11 - 11s - loss: 1134.2694 - loglik: -1.1475e+03 - logprior: 13.2607
Fitted a model with MAP estimate = -1132.4489
Time for alignment: 222.0005
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 13s - loss: 1466.2291 - loglik: -1.4069e+03 - logprior: -5.9375e+01
Epoch 2/10
11/11 - 9s - loss: 1354.0579 - loglik: -1.3476e+03 - logprior: -6.4674e+00
Epoch 3/10
11/11 - 9s - loss: 1246.5400 - loglik: -1.2468e+03 - logprior: 0.2364
Epoch 4/10
11/11 - 8s - loss: 1207.6118 - loglik: -1.2088e+03 - logprior: 1.1445
Epoch 5/10
11/11 - 10s - loss: 1186.0386 - loglik: -1.1877e+03 - logprior: 1.6748
Epoch 6/10
11/11 - 9s - loss: 1177.2815 - loglik: -1.1796e+03 - logprior: 2.2862
Epoch 7/10
11/11 - 9s - loss: 1183.2720 - loglik: -1.1859e+03 - logprior: 2.6004
Fitted a model with MAP estimate = -1178.1037
expansions: [(19, 4), (24, 1), (36, 1), (42, 1), (49, 1), (51, 2), (60, 1), (62, 2), (63, 2), (64, 2), (78, 2), (89, 1), (102, 4), (103, 1), (104, 2), (121, 2), (162, 1), (164, 5), (181, 1), (182, 1), (183, 1), (184, 2), (196, 1), (197, 1), (198, 6), (200, 2), (201, 5), (219, 1), (222, 1), (224, 1), (225, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 1228.5236 - loglik: -1.1765e+03 - logprior: -5.2054e+01
Epoch 2/2
11/11 - 12s - loss: 1168.0215 - loglik: -1.1543e+03 - logprior: -1.3681e+01
Fitted a model with MAP estimate = -1159.6188
expansions: [(0, 2), (23, 2), (97, 1), (242, 1)]
discards: [  0  59  76  77 123 124 128 147 298]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 1189.0419 - loglik: -1.1534e+03 - logprior: -3.5647e+01
Epoch 2/2
11/11 - 12s - loss: 1145.4501 - loglik: -1.1453e+03 - logprior: -1.6624e-01
Fitted a model with MAP estimate = -1140.1022
expansions: [(245, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 15s - loss: 1190.0616 - loglik: -1.1452e+03 - logprior: -4.4908e+01
Epoch 2/10
11/11 - 13s - loss: 1155.7596 - loglik: -1.1461e+03 - logprior: -9.6109e+00
Epoch 3/10
11/11 - 11s - loss: 1146.7499 - loglik: -1.1492e+03 - logprior: 2.4553
Epoch 4/10
11/11 - 12s - loss: 1123.5233 - loglik: -1.1336e+03 - logprior: 10.0268
Epoch 5/10
11/11 - 12s - loss: 1133.9757 - loglik: -1.1462e+03 - logprior: 12.2574
Fitted a model with MAP estimate = -1128.0926
Time for alignment: 202.0894
Computed alignments with likelihoods: ['-1128.0765', '-1129.0262', '-1130.5862', '-1132.4489', '-1128.0926']
Best model has likelihood: -1128.0765  (prior= 13.2644 )
time for generating output: 0.5384
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/trfl.projection.fasta
SP score = 0.9160296502200602
Training of 5 independent models on file serpin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f531d25ff10>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5ca7a360d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f59404b3a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5aff2dd040>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5ca7597df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b0a136700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b0969dcd0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5b1357ddc0>, <__main__.SimpleDirichletPrior object at 0x7f530c479580>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 19s - loss: 1666.3201 - loglik: -1.6568e+03 - logprior: -9.5196e+00
Epoch 2/10
21/21 - 16s - loss: 1532.1616 - loglik: -1.5311e+03 - logprior: -1.0449e+00
Epoch 3/10
21/21 - 16s - loss: 1491.9427 - loglik: -1.4892e+03 - logprior: -2.7846e+00
Epoch 4/10
21/21 - 16s - loss: 1479.1923 - loglik: -1.4765e+03 - logprior: -2.7114e+00
Epoch 5/10
21/21 - 16s - loss: 1476.6143 - loglik: -1.4740e+03 - logprior: -2.6069e+00
Epoch 6/10
21/21 - 16s - loss: 1477.3423 - loglik: -1.4746e+03 - logprior: -2.7555e+00
Fitted a model with MAP estimate = -1475.5470
expansions: [(13, 1), (14, 2), (15, 1), (27, 1), (49, 1), (51, 2), (53, 2), (54, 1), (60, 2), (61, 2), (62, 1), (64, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (86, 1), (87, 1), (90, 1), (93, 1), (94, 1), (95, 1), (102, 1), (111, 1), (114, 1), (115, 1), (128, 1), (134, 1), (135, 1), (138, 1), (141, 1), (152, 1), (153, 1), (154, 1), (155, 1), (157, 1), (158, 1), (160, 1), (161, 2), (180, 1), (181, 1), (182, 2), (188, 1), (189, 1), (190, 1), (192, 1), (193, 2), (194, 1), (195, 1), (208, 1), (211, 1), (212, 1), (218, 1), (228, 1), (229, 1), (230, 2), (232, 1), (234, 1), (235, 1), (255, 1), (256, 1), (257, 1), (258, 2), (260, 2), (271, 1), (272, 2), (273, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 27s - loss: 1471.9292 - loglik: -1.4653e+03 - logprior: -6.6433e+00
Epoch 2/2
21/21 - 24s - loss: 1449.3527 - loglik: -1.4505e+03 - logprior: 1.1717
Fitted a model with MAP estimate = -1444.5501
expansions: [(76, 1), (145, 1)]
discards: [249 333]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 28s - loss: 1454.3585 - loglik: -1.4497e+03 - logprior: -4.6601e+00
Epoch 2/2
21/21 - 24s - loss: 1449.4053 - loglik: -1.4513e+03 - logprior: 1.9281
Fitted a model with MAP estimate = -1442.4223
expansions: []
discards: [60]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 28s - loss: 1455.8651 - loglik: -1.4517e+03 - logprior: -4.1235e+00
Epoch 2/10
21/21 - 24s - loss: 1443.0973 - loglik: -1.4455e+03 - logprior: 2.3668
Epoch 3/10
21/21 - 24s - loss: 1438.7838 - loglik: -1.4418e+03 - logprior: 3.0593
Epoch 4/10
21/21 - 24s - loss: 1441.1569 - loglik: -1.4446e+03 - logprior: 3.4280
Fitted a model with MAP estimate = -1437.8123
Time for alignment: 367.4442
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 20s - loss: 1659.7190 - loglik: -1.6502e+03 - logprior: -9.5238e+00
Epoch 2/10
21/21 - 16s - loss: 1539.7407 - loglik: -1.5388e+03 - logprior: -9.3179e-01
Epoch 3/10
21/21 - 16s - loss: 1492.7263 - loglik: -1.4905e+03 - logprior: -2.2492e+00
Epoch 4/10
21/21 - 16s - loss: 1469.4625 - loglik: -1.4672e+03 - logprior: -2.2513e+00
Epoch 5/10
21/21 - 16s - loss: 1481.4049 - loglik: -1.4793e+03 - logprior: -2.1269e+00
Fitted a model with MAP estimate = -1475.8225
expansions: [(13, 1), (14, 2), (15, 1), (53, 4), (54, 1), (56, 2), (61, 2), (62, 2), (64, 2), (66, 1), (75, 2), (76, 3), (79, 1), (80, 1), (81, 1), (85, 1), (86, 1), (93, 1), (94, 1), (95, 1), (102, 1), (107, 1), (110, 1), (112, 1), (114, 1), (116, 1), (135, 1), (137, 1), (138, 1), (154, 1), (155, 1), (156, 1), (158, 2), (159, 1), (160, 1), (161, 1), (180, 1), (181, 1), (183, 1), (190, 3), (191, 2), (192, 1), (193, 2), (194, 1), (195, 1), (207, 1), (211, 1), (212, 1), (214, 1), (223, 1), (227, 1), (228, 2), (229, 2), (236, 1), (256, 1), (257, 1), (258, 1), (259, 2), (260, 3), (261, 1), (272, 2), (273, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 28s - loss: 1477.1641 - loglik: -1.4675e+03 - logprior: -9.6290e+00
Epoch 2/2
21/21 - 24s - loss: 1454.1166 - loglik: -1.4519e+03 - logprior: -2.2306e+00
Fitted a model with MAP estimate = -1448.1757
expansions: [(0, 3), (76, 1), (90, 1)]
discards: [  0  58 248 294 335]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 27s - loss: 1459.5200 - loglik: -1.4548e+03 - logprior: -4.6949e+00
Epoch 2/2
21/21 - 24s - loss: 1437.7976 - loglik: -1.4397e+03 - logprior: 1.9107
Fitted a model with MAP estimate = -1443.0416
expansions: []
discards: [  1   3 336]
Re-initialized the encoder parameters.
Fitting a model of length 362 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 27s - loss: 1456.3206 - loglik: -1.4521e+03 - logprior: -4.2338e+00
Epoch 2/10
21/21 - 23s - loss: 1442.4846 - loglik: -1.4446e+03 - logprior: 2.1227
Epoch 3/10
21/21 - 24s - loss: 1443.1298 - loglik: -1.4460e+03 - logprior: 2.9144
Fitted a model with MAP estimate = -1440.7748
Time for alignment: 327.3000
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 21s - loss: 1661.1743 - loglik: -1.6517e+03 - logprior: -9.4925e+00
Epoch 2/10
21/21 - 16s - loss: 1534.6068 - loglik: -1.5336e+03 - logprior: -9.6980e-01
Epoch 3/10
21/21 - 16s - loss: 1489.8060 - loglik: -1.4872e+03 - logprior: -2.5775e+00
Epoch 4/10
21/21 - 16s - loss: 1480.1219 - loglik: -1.4775e+03 - logprior: -2.5787e+00
Epoch 5/10
21/21 - 16s - loss: 1471.3519 - loglik: -1.4688e+03 - logprior: -2.5079e+00
Epoch 6/10
21/21 - 16s - loss: 1477.2399 - loglik: -1.4746e+03 - logprior: -2.5918e+00
Fitted a model with MAP estimate = -1474.3263
expansions: [(13, 1), (14, 2), (15, 1), (43, 1), (52, 2), (54, 3), (55, 2), (60, 2), (61, 2), (62, 1), (64, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (79, 1), (80, 1), (81, 1), (83, 1), (85, 1), (86, 1), (89, 1), (92, 1), (94, 1), (102, 1), (107, 1), (110, 1), (112, 1), (113, 1), (114, 1), (115, 1), (128, 1), (133, 1), (135, 1), (136, 1), (150, 1), (151, 1), (152, 1), (153, 1), (155, 1), (156, 1), (158, 1), (159, 1), (168, 1), (178, 1), (181, 1), (188, 1), (189, 1), (190, 2), (191, 1), (192, 1), (193, 1), (209, 1), (212, 1), (214, 1), (218, 1), (223, 1), (227, 1), (230, 2), (232, 1), (255, 1), (256, 1), (257, 1), (258, 1), (259, 2), (260, 1), (261, 1), (262, 2), (272, 2), (273, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 27s - loss: 1471.2006 - loglik: -1.4646e+03 - logprior: -6.6160e+00
Epoch 2/2
21/21 - 24s - loss: 1448.7341 - loglik: -1.4500e+03 - logprior: 1.2245
Fitted a model with MAP estimate = -1444.7484
expansions: [(77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 28s - loss: 1455.8766 - loglik: -1.4509e+03 - logprior: -4.9929e+00
Epoch 2/2
21/21 - 24s - loss: 1442.3654 - loglik: -1.4440e+03 - logprior: 1.6145
Fitted a model with MAP estimate = -1442.8875
expansions: []
discards: [ 56 336]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 27s - loss: 1454.0844 - loglik: -1.4499e+03 - logprior: -4.1772e+00
Epoch 2/10
21/21 - 24s - loss: 1443.5028 - loglik: -1.4456e+03 - logprior: 2.1439
Epoch 3/10
21/21 - 24s - loss: 1441.8230 - loglik: -1.4449e+03 - logprior: 3.0716
Epoch 4/10
21/21 - 24s - loss: 1441.2826 - loglik: -1.4448e+03 - logprior: 3.5025
Epoch 5/10
21/21 - 24s - loss: 1437.7527 - loglik: -1.4414e+03 - logprior: 3.6563
Epoch 6/10
21/21 - 24s - loss: 1437.5369 - loglik: -1.4414e+03 - logprior: 3.8753
Epoch 7/10
21/21 - 24s - loss: 1436.6082 - loglik: -1.4406e+03 - logprior: 3.9701
Epoch 8/10
21/21 - 24s - loss: 1436.6373 - loglik: -1.4408e+03 - logprior: 4.1788
Fitted a model with MAP estimate = -1436.0372
Time for alignment: 464.2138
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 21s - loss: 1660.6595 - loglik: -1.6511e+03 - logprior: -9.5144e+00
Epoch 2/10
21/21 - 16s - loss: 1531.4847 - loglik: -1.5305e+03 - logprior: -9.5065e-01
Epoch 3/10
21/21 - 16s - loss: 1486.0652 - loglik: -1.4837e+03 - logprior: -2.3594e+00
Epoch 4/10
21/21 - 16s - loss: 1481.5963 - loglik: -1.4794e+03 - logprior: -2.2231e+00
Epoch 5/10
21/21 - 16s - loss: 1466.1102 - loglik: -1.4638e+03 - logprior: -2.3391e+00
Epoch 6/10
21/21 - 16s - loss: 1479.5027 - loglik: -1.4771e+03 - logprior: -2.3580e+00
Fitted a model with MAP estimate = -1473.6455
expansions: [(13, 1), (14, 2), (15, 1), (52, 2), (54, 3), (56, 1), (61, 2), (62, 3), (64, 3), (75, 1), (76, 1), (77, 1), (78, 1), (80, 1), (81, 1), (82, 1), (84, 1), (85, 2), (86, 1), (89, 1), (92, 1), (93, 1), (94, 1), (101, 1), (110, 1), (112, 1), (114, 1), (116, 1), (134, 1), (135, 1), (138, 1), (152, 1), (153, 1), (154, 1), (155, 1), (157, 1), (158, 1), (160, 1), (161, 1), (170, 1), (180, 1), (183, 1), (190, 1), (191, 1), (192, 2), (193, 1), (194, 1), (195, 1), (196, 1), (208, 1), (209, 1), (212, 1), (215, 1), (224, 1), (228, 1), (229, 2), (230, 2), (232, 1), (257, 1), (258, 3), (259, 2), (261, 2), (262, 1), (271, 1), (272, 2), (273, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 28s - loss: 1472.1008 - loglik: -1.4652e+03 - logprior: -6.8649e+00
Epoch 2/2
21/21 - 23s - loss: 1451.0719 - loglik: -1.4516e+03 - logprior: 0.5717
Fitted a model with MAP estimate = -1445.3985
expansions: [(19, 1)]
discards: [295 334]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 27s - loss: 1457.4030 - loglik: -1.4527e+03 - logprior: -4.6835e+00
Epoch 2/2
21/21 - 24s - loss: 1446.0267 - loglik: -1.4479e+03 - logprior: 1.8591
Fitted a model with MAP estimate = -1443.1794
expansions: [(187, 1)]
discards: [56]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 29s - loss: 1452.2162 - loglik: -1.4481e+03 - logprior: -4.1547e+00
Epoch 2/10
21/21 - 24s - loss: 1446.0211 - loglik: -1.4483e+03 - logprior: 2.2576
Epoch 3/10
21/21 - 24s - loss: 1440.7526 - loglik: -1.4438e+03 - logprior: 3.0562
Epoch 4/10
21/21 - 24s - loss: 1440.0885 - loglik: -1.4436e+03 - logprior: 3.4785
Epoch 5/10
21/21 - 23s - loss: 1441.1692 - loglik: -1.4448e+03 - logprior: 3.6802
Fitted a model with MAP estimate = -1437.0755
Time for alignment: 391.2858
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 19s - loss: 1664.3824 - loglik: -1.6548e+03 - logprior: -9.5467e+00
Epoch 2/10
21/21 - 16s - loss: 1535.4983 - loglik: -1.5346e+03 - logprior: -8.7292e-01
Epoch 3/10
21/21 - 16s - loss: 1485.6344 - loglik: -1.4833e+03 - logprior: -2.3707e+00
Epoch 4/10
21/21 - 16s - loss: 1482.0662 - loglik: -1.4798e+03 - logprior: -2.2321e+00
Epoch 5/10
21/21 - 16s - loss: 1479.1198 - loglik: -1.4770e+03 - logprior: -2.1463e+00
Epoch 6/10
21/21 - 16s - loss: 1468.9841 - loglik: -1.4668e+03 - logprior: -2.1754e+00
Epoch 7/10
21/21 - 16s - loss: 1480.4427 - loglik: -1.4781e+03 - logprior: -2.3437e+00
Fitted a model with MAP estimate = -1475.2625
expansions: [(13, 1), (14, 2), (15, 1), (46, 1), (52, 2), (54, 2), (56, 1), (61, 2), (62, 2), (63, 1), (65, 1), (76, 1), (77, 1), (80, 1), (81, 1), (82, 2), (83, 2), (87, 1), (88, 1), (91, 1), (94, 1), (95, 1), (96, 1), (103, 1), (112, 1), (114, 1), (116, 1), (118, 1), (137, 1), (139, 1), (140, 1), (143, 1), (147, 1), (155, 1), (156, 1), (157, 1), (159, 3), (160, 2), (161, 3), (180, 1), (183, 2), (189, 1), (190, 1), (191, 1), (193, 1), (194, 2), (195, 1), (196, 1), (209, 1), (212, 1), (213, 1), (214, 1), (219, 1), (223, 1), (229, 2), (230, 2), (232, 1), (257, 1), (258, 3), (259, 2), (261, 2), (262, 1), (271, 1), (272, 2), (273, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 368 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 28s - loss: 1467.7914 - loglik: -1.4610e+03 - logprior: -6.7679e+00
Epoch 2/2
21/21 - 24s - loss: 1450.9596 - loglik: -1.4521e+03 - logprior: 1.1823
Fitted a model with MAP estimate = -1444.3787
expansions: [(76, 1)]
discards: [ 60 205 251 297 336]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 27s - loss: 1454.9427 - loglik: -1.4504e+03 - logprior: -4.5317e+00
Epoch 2/2
21/21 - 23s - loss: 1441.5520 - loglik: -1.4436e+03 - logprior: 2.0186
Fitted a model with MAP estimate = -1442.7473
expansions: [(56, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 27s - loss: 1454.8975 - loglik: -1.4506e+03 - logprior: -4.2889e+00
Epoch 2/10
21/21 - 24s - loss: 1445.6244 - loglik: -1.4481e+03 - logprior: 2.4614
Epoch 3/10
21/21 - 24s - loss: 1438.0291 - loglik: -1.4412e+03 - logprior: 3.1277
Epoch 4/10
21/21 - 24s - loss: 1440.3263 - loglik: -1.4439e+03 - logprior: 3.5815
Fitted a model with MAP estimate = -1437.7775
Time for alignment: 382.0482
Computed alignments with likelihoods: ['-1437.8123', '-1440.7748', '-1436.0372', '-1437.0755', '-1437.7775']
Best model has likelihood: -1436.0372  (prior= 4.0988 )
time for generating output: 0.4351
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/serpin.projection.fasta
SP score = 0.952465834818776
Training of 5 independent models on file ghf13.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b098e39d0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b09b906a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530d024520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530d017d60>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5aff425970>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5cf3ab7fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5afea1dd30>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5b0a549910>, <__main__.SimpleDirichletPrior object at 0x7f5b1c6b5190>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 1750.2357 - loglik: -1.7480e+03 - logprior: -2.2580e+00
Epoch 2/10
39/39 - 22s - loss: 1649.4143 - loglik: -1.6481e+03 - logprior: -1.3389e+00
Epoch 3/10
39/39 - 22s - loss: 1637.7057 - loglik: -1.6362e+03 - logprior: -1.5235e+00
Epoch 4/10
39/39 - 22s - loss: 1635.1836 - loglik: -1.6336e+03 - logprior: -1.6266e+00
Epoch 5/10
39/39 - 22s - loss: 1632.8567 - loglik: -1.6311e+03 - logprior: -1.8063e+00
Epoch 6/10
39/39 - 22s - loss: 1632.1283 - loglik: -1.6301e+03 - logprior: -2.0666e+00
Epoch 7/10
39/39 - 22s - loss: 1631.1974 - loglik: -1.6289e+03 - logprior: -2.3465e+00
Epoch 8/10
39/39 - 22s - loss: 1631.3188 - loglik: -1.6287e+03 - logprior: -2.5813e+00
Fitted a model with MAP estimate = -1404.9725
expansions: [(14, 1), (41, 1), (53, 1), (81, 3), (82, 1), (83, 1), (91, 1), (92, 1), (93, 3), (95, 2), (100, 2), (103, 1), (105, 4), (106, 1), (110, 1), (116, 1), (118, 1), (120, 2), (121, 4), (122, 1), (145, 6), (150, 5), (152, 2), (153, 1), (167, 4), (168, 2), (169, 2), (171, 1), (172, 1), (177, 2), (178, 4), (179, 2), (180, 2), (185, 1), (186, 1), (187, 1), (190, 3), (191, 1), (192, 1), (206, 1), (207, 1), (224, 1), (244, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 324 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 37s - loss: 1625.9490 - loglik: -1.6227e+03 - logprior: -3.2275e+00
Epoch 2/2
39/39 - 33s - loss: 1608.8925 - loglik: -1.6072e+03 - logprior: -1.7091e+00
Fitted a model with MAP estimate = -1383.6427
expansions: [(0, 2), (193, 1), (216, 1), (324, 2)]
discards: [  0 108 117 125 126 127 128 129 130 131 149 177 178 179 180 181 196 206
 207 208 209 210 321 322 323]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 1617.6527 - loglik: -1.6157e+03 - logprior: -1.9604e+00
Epoch 2/2
39/39 - 30s - loss: 1611.7913 - loglik: -1.6111e+03 - logprior: -6.7526e-01
Fitted a model with MAP estimate = -1386.3445
expansions: [(124, 4), (193, 1)]
discards: [  0 105 115 201 243 303 304]
Re-initialized the encoder parameters.
Fitting a model of length 303 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 35s - loss: 1388.0122 - loglik: -1.3860e+03 - logprior: -2.0425e+00
Epoch 2/10
43/43 - 32s - loss: 1388.3606 - loglik: -1.3878e+03 - logprior: -5.6760e-01
Fitted a model with MAP estimate = -1381.7788
Time for alignment: 541.3364
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 1749.9587 - loglik: -1.7477e+03 - logprior: -2.2526e+00
Epoch 2/10
39/39 - 22s - loss: 1651.0759 - loglik: -1.6497e+03 - logprior: -1.3336e+00
Epoch 3/10
39/39 - 22s - loss: 1638.5896 - loglik: -1.6370e+03 - logprior: -1.5613e+00
Epoch 4/10
39/39 - 22s - loss: 1636.1389 - loglik: -1.6344e+03 - logprior: -1.6988e+00
Epoch 5/10
39/39 - 22s - loss: 1633.6791 - loglik: -1.6318e+03 - logprior: -1.8926e+00
Epoch 6/10
39/39 - 22s - loss: 1633.1040 - loglik: -1.6310e+03 - logprior: -2.1359e+00
Epoch 7/10
39/39 - 22s - loss: 1632.2280 - loglik: -1.6299e+03 - logprior: -2.3499e+00
Epoch 8/10
39/39 - 22s - loss: 1632.1224 - loglik: -1.6296e+03 - logprior: -2.5572e+00
Epoch 9/10
39/39 - 22s - loss: 1631.3031 - loglik: -1.6285e+03 - logprior: -2.8086e+00
Epoch 10/10
39/39 - 22s - loss: 1631.1968 - loglik: -1.6282e+03 - logprior: -3.0206e+00
Fitted a model with MAP estimate = -1405.2492
expansions: [(14, 1), (30, 1), (41, 1), (74, 1), (75, 1), (80, 6), (82, 1), (93, 5), (98, 1), (102, 1), (104, 1), (105, 3), (109, 1), (118, 1), (120, 2), (121, 4), (127, 2), (147, 1), (156, 1), (158, 1), (159, 2), (160, 6), (161, 2), (168, 3), (169, 5), (170, 3), (171, 2), (172, 1), (173, 1), (174, 1), (177, 3), (178, 2), (179, 1), (180, 1), (186, 1), (187, 1), (188, 2), (191, 2), (207, 1), (208, 1), (211, 1), (212, 4), (224, 1), (244, 2)]
discards: [  0 148 149]
Re-initialized the encoder parameters.
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 37s - loss: 1626.1768 - loglik: -1.6227e+03 - logprior: -3.4849e+00
Epoch 2/2
39/39 - 33s - loss: 1607.7874 - loglik: -1.6059e+03 - logprior: -1.8534e+00
Fitted a model with MAP estimate = -1382.5001
expansions: [(0, 2), (133, 3), (223, 1), (264, 1), (326, 2)]
discards: [  0 117 118 119 120 121 122 126 127 128 129 130 148 157 179 180 202 225
 256 287 288 289 324 325]
Re-initialized the encoder parameters.
Fitting a model of length 311 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 1616.6482 - loglik: -1.6146e+03 - logprior: -2.0390e+00
Epoch 2/2
39/39 - 30s - loss: 1609.5654 - loglik: -1.6088e+03 - logprior: -8.0104e-01
Fitted a model with MAP estimate = -1383.6220
expansions: [(117, 1), (119, 2), (120, 2), (125, 3), (191, 1)]
discards: [  0 166 167 168 309 310]
Re-initialized the encoder parameters.
Fitting a model of length 314 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 37s - loss: 1385.4329 - loglik: -1.3834e+03 - logprior: -2.0504e+00
Epoch 2/10
43/43 - 34s - loss: 1382.2352 - loglik: -1.3816e+03 - logprior: -6.3789e-01
Epoch 3/10
43/43 - 34s - loss: 1376.3710 - loglik: -1.3759e+03 - logprior: -4.3231e-01
Epoch 4/10
43/43 - 33s - loss: 1377.3208 - loglik: -1.3769e+03 - logprior: -4.1650e-01
Fitted a model with MAP estimate = -1374.2435
Time for alignment: 658.7930
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 1747.9113 - loglik: -1.7457e+03 - logprior: -2.2488e+00
Epoch 2/10
39/39 - 22s - loss: 1652.8392 - loglik: -1.6516e+03 - logprior: -1.2419e+00
Epoch 3/10
39/39 - 22s - loss: 1640.1443 - loglik: -1.6387e+03 - logprior: -1.4705e+00
Epoch 4/10
39/39 - 22s - loss: 1635.8781 - loglik: -1.6343e+03 - logprior: -1.6007e+00
Epoch 5/10
39/39 - 22s - loss: 1633.6104 - loglik: -1.6318e+03 - logprior: -1.7945e+00
Epoch 6/10
39/39 - 22s - loss: 1632.0925 - loglik: -1.6300e+03 - logprior: -2.0807e+00
Epoch 7/10
39/39 - 22s - loss: 1631.6381 - loglik: -1.6293e+03 - logprior: -2.3174e+00
Epoch 8/10
39/39 - 22s - loss: 1630.8019 - loglik: -1.6282e+03 - logprior: -2.5840e+00
Epoch 9/10
39/39 - 22s - loss: 1630.7040 - loglik: -1.6279e+03 - logprior: -2.7572e+00
Epoch 10/10
39/39 - 22s - loss: 1628.7529 - loglik: -1.6258e+03 - logprior: -2.9246e+00
Fitted a model with MAP estimate = -1404.9115
expansions: [(14, 1), (38, 1), (45, 1), (82, 3), (83, 2), (85, 1), (97, 2), (102, 3), (106, 1), (107, 2), (108, 3), (110, 1), (111, 1), (112, 3), (120, 1), (121, 5), (122, 1), (147, 1), (151, 1), (161, 4), (163, 1), (164, 1), (168, 2), (169, 7), (170, 3), (171, 5), (172, 1), (177, 2), (178, 3), (179, 1), (184, 2), (185, 1), (186, 1), (187, 1), (189, 3), (190, 1), (206, 1), (212, 4), (213, 2), (244, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 325 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 37s - loss: 1623.4196 - loglik: -1.6200e+03 - logprior: -3.4464e+00
Epoch 2/2
39/39 - 33s - loss: 1605.0868 - loglik: -1.6032e+03 - logprior: -1.8444e+00
Fitted a model with MAP estimate = -1380.1796
expansions: [(0, 2), (125, 1), (193, 1), (197, 1), (236, 1), (259, 1), (260, 1), (325, 2)]
discards: [  0  34 175 176 177 178 209 222 223 247 286 323 324]
Re-initialized the encoder parameters.
Fitting a model of length 322 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 1606.8453 - loglik: -1.6047e+03 - logprior: -2.0978e+00
Epoch 2/2
39/39 - 34s - loss: 1600.4139 - loglik: -1.5995e+03 - logprior: -8.7762e-01
Fitted a model with MAP estimate = -1377.3749
expansions: [(125, 1), (136, 1), (210, 1), (218, 1), (287, 1)]
discards: [  0 320 321]
Re-initialized the encoder parameters.
Fitting a model of length 324 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 41s - loss: 1382.3553 - loglik: -1.3802e+03 - logprior: -2.1348e+00
Epoch 2/10
43/43 - 36s - loss: 1372.4865 - loglik: -1.3719e+03 - logprior: -5.9421e-01
Epoch 3/10
43/43 - 36s - loss: 1375.3057 - loglik: -1.3749e+03 - logprior: -4.3293e-01
Fitted a model with MAP estimate = -1371.4785
Time for alignment: 643.4122
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 1747.2953 - loglik: -1.7450e+03 - logprior: -2.2630e+00
Epoch 2/10
39/39 - 22s - loss: 1649.1262 - loglik: -1.6479e+03 - logprior: -1.2642e+00
Epoch 3/10
39/39 - 22s - loss: 1638.5674 - loglik: -1.6371e+03 - logprior: -1.4485e+00
Epoch 4/10
39/39 - 22s - loss: 1634.3373 - loglik: -1.6328e+03 - logprior: -1.5459e+00
Epoch 5/10
39/39 - 22s - loss: 1632.1691 - loglik: -1.6305e+03 - logprior: -1.6976e+00
Epoch 6/10
39/39 - 22s - loss: 1631.5035 - loglik: -1.6296e+03 - logprior: -1.9384e+00
Epoch 7/10
39/39 - 22s - loss: 1631.0630 - loglik: -1.6288e+03 - logprior: -2.2169e+00
Epoch 8/10
39/39 - 22s - loss: 1629.5490 - loglik: -1.6271e+03 - logprior: -2.4416e+00
Epoch 9/10
39/39 - 22s - loss: 1630.3881 - loglik: -1.6277e+03 - logprior: -2.6699e+00
Fitted a model with MAP estimate = -1404.6081
expansions: [(20, 1), (42, 1), (54, 1), (82, 3), (83, 2), (84, 2), (94, 2), (96, 2), (101, 2), (104, 1), (106, 4), (107, 1), (109, 1), (110, 1), (119, 1), (120, 4), (121, 3), (149, 2), (150, 6), (151, 1), (160, 3), (161, 2), (162, 1), (169, 1), (171, 1), (178, 1), (179, 4), (180, 2), (181, 1), (185, 1), (186, 1), (188, 1), (191, 1), (192, 2), (193, 2), (207, 2), (208, 1), (212, 4), (224, 1), (244, 3)]
discards: [  0 167]
Re-initialized the encoder parameters.
Fitting a model of length 318 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 1625.5240 - loglik: -1.6223e+03 - logprior: -3.2399e+00
Epoch 2/2
39/39 - 31s - loss: 1608.9843 - loglik: -1.6073e+03 - logprior: -1.6511e+00
Fitted a model with MAP estimate = -1383.2872
expansions: [(0, 2), (134, 3), (230, 1), (318, 2)]
discards: [  0 108 115 125 126 127 128 129 130 131 148 185 278 279 280 315 316 317]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 1614.6094 - loglik: -1.6127e+03 - logprior: -1.8876e+00
Epoch 2/2
39/39 - 30s - loss: 1608.1935 - loglik: -1.6075e+03 - logprior: -6.7267e-01
Fitted a model with MAP estimate = -1383.3328
expansions: [(114, 1), (122, 2), (123, 1)]
discards: [  0 116 226 306 307]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 36s - loss: 1385.4335 - loglik: -1.3834e+03 - logprior: -2.0525e+00
Epoch 2/10
43/43 - 33s - loss: 1379.1201 - loglik: -1.3786e+03 - logprior: -5.1730e-01
Epoch 3/10
43/43 - 32s - loss: 1380.6565 - loglik: -1.3803e+03 - logprior: -3.5764e-01
Fitted a model with MAP estimate = -1376.4669
Time for alignment: 590.8791
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 1749.2296 - loglik: -1.7470e+03 - logprior: -2.2488e+00
Epoch 2/10
39/39 - 22s - loss: 1652.6792 - loglik: -1.6514e+03 - logprior: -1.3167e+00
Epoch 3/10
39/39 - 22s - loss: 1640.3954 - loglik: -1.6389e+03 - logprior: -1.4923e+00
Epoch 4/10
39/39 - 22s - loss: 1636.4196 - loglik: -1.6348e+03 - logprior: -1.5824e+00
Epoch 5/10
39/39 - 22s - loss: 1635.1841 - loglik: -1.6334e+03 - logprior: -1.7914e+00
Epoch 6/10
39/39 - 22s - loss: 1633.8674 - loglik: -1.6318e+03 - logprior: -2.0479e+00
Epoch 7/10
39/39 - 22s - loss: 1634.3044 - loglik: -1.6320e+03 - logprior: -2.3082e+00
Fitted a model with MAP estimate = -1406.9765
expansions: [(14, 1), (41, 1), (43, 1), (56, 1), (81, 3), (82, 2), (83, 2), (94, 3), (96, 2), (106, 2), (107, 1), (110, 1), (119, 1), (121, 2), (122, 3), (127, 1), (149, 1), (151, 5), (152, 1), (169, 1), (170, 6), (171, 4), (172, 2), (173, 1), (174, 1), (179, 1), (180, 1), (186, 4), (187, 5), (188, 1), (191, 2), (192, 2), (193, 2), (208, 1), (209, 1), (213, 5), (224, 1), (244, 3)]
discards: [  0  34 161 162 163 164]
Re-initialized the encoder parameters.
Fitting a model of length 316 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 1630.3431 - loglik: -1.6270e+03 - logprior: -3.3078e+00
Epoch 2/2
39/39 - 31s - loss: 1613.3741 - loglik: -1.6116e+03 - logprior: -1.7848e+00
Fitted a model with MAP estimate = -1387.1450
expansions: [(0, 2), (121, 1), (201, 1), (202, 1), (316, 2)]
discards: [  0  85  94  95 122 123 124 125 126 127 192 193 194 195 196 233 234 236
 247 276 277 278 313 314 315]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 1621.0347 - loglik: -1.6190e+03 - logprior: -1.9874e+00
Epoch 2/2
39/39 - 29s - loss: 1615.1827 - loglik: -1.6145e+03 - logprior: -7.3111e-01
Fitted a model with MAP estimate = -1388.7382
expansions: [(94, 2), (118, 2), (119, 2)]
discards: [  0 116 201 202 239 296 297]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 34s - loss: 1392.3965 - loglik: -1.3903e+03 - logprior: -2.0858e+00
Epoch 2/10
43/43 - 32s - loss: 1387.8124 - loglik: -1.3872e+03 - logprior: -5.8074e-01
Epoch 3/10
43/43 - 31s - loss: 1386.4689 - loglik: -1.3861e+03 - logprior: -3.8583e-01
Epoch 4/10
43/43 - 31s - loss: 1376.1411 - loglik: -1.3757e+03 - logprior: -3.9618e-01
Epoch 5/10
43/43 - 31s - loss: 1382.6965 - loglik: -1.3823e+03 - logprior: -4.1259e-01
Fitted a model with MAP estimate = -1379.7491
Time for alignment: 600.1577
Computed alignments with likelihoods: ['-1381.7788', '-1374.2435', '-1371.4785', '-1376.4669', '-1379.7491']
Best model has likelihood: -1371.4785  (prior= -0.3898 )
time for generating output: 0.4723
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf13.projection.fasta
SP score = 0.5192108561774483
Training of 5 independent models on file cryst.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5afe52b940>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b09918a30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b130f49a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5cf3cf8a60>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f53349a5fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5afe53dee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b1c16b880>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5b0991acd0>, <__main__.SimpleDirichletPrior object at 0x7f530c208220>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 522.6241 - loglik: -4.8066e+02 - logprior: -4.1967e+01
Epoch 2/10
10/10 - 2s - loss: 469.0434 - loglik: -4.5813e+02 - logprior: -1.0911e+01
Epoch 3/10
10/10 - 2s - loss: 440.3763 - loglik: -4.3495e+02 - logprior: -5.4217e+00
Epoch 4/10
10/10 - 2s - loss: 424.1801 - loglik: -4.2051e+02 - logprior: -3.6722e+00
Epoch 5/10
10/10 - 2s - loss: 417.3945 - loglik: -4.1446e+02 - logprior: -2.9332e+00
Epoch 6/10
10/10 - 2s - loss: 414.7701 - loglik: -4.1220e+02 - logprior: -2.5684e+00
Epoch 7/10
10/10 - 2s - loss: 414.1078 - loglik: -4.1206e+02 - logprior: -2.0490e+00
Epoch 8/10
10/10 - 2s - loss: 412.8691 - loglik: -4.1119e+02 - logprior: -1.6786e+00
Epoch 9/10
10/10 - 2s - loss: 411.7633 - loglik: -4.1012e+02 - logprior: -1.6387e+00
Epoch 10/10
10/10 - 2s - loss: 412.3883 - loglik: -4.1068e+02 - logprior: -1.7105e+00
Fitted a model with MAP estimate = -411.7760
expansions: [(0, 2), (12, 1), (14, 2), (15, 1), (20, 1), (21, 1), (23, 2), (30, 1), (32, 1), (46, 1), (55, 3), (58, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 458.0121 - loglik: -4.0675e+02 - logprior: -5.1264e+01
Epoch 2/2
10/10 - 2s - loss: 413.9243 - loglik: -3.9859e+02 - logprior: -1.5334e+01
Fitted a model with MAP estimate = -405.6923
expansions: []
discards: [ 0 17 69]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 442.4459 - loglik: -3.9912e+02 - logprior: -4.3331e+01
Epoch 2/2
10/10 - 1s - loss: 415.1332 - loglik: -3.9829e+02 - logprior: -1.6840e+01
Fitted a model with MAP estimate = -410.2347
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 437.2313 - loglik: -3.9768e+02 - logprior: -3.9554e+01
Epoch 2/10
10/10 - 2s - loss: 407.1407 - loglik: -3.9637e+02 - logprior: -1.0771e+01
Epoch 3/10
10/10 - 2s - loss: 399.8379 - loglik: -3.9580e+02 - logprior: -4.0362e+00
Epoch 4/10
10/10 - 2s - loss: 396.5113 - loglik: -3.9458e+02 - logprior: -1.9265e+00
Epoch 5/10
10/10 - 2s - loss: 395.9040 - loglik: -3.9505e+02 - logprior: -8.5851e-01
Epoch 6/10
10/10 - 2s - loss: 394.9829 - loglik: -3.9496e+02 - logprior: -2.2776e-02
Epoch 7/10
10/10 - 2s - loss: 394.7878 - loglik: -3.9524e+02 - logprior: 0.4508
Epoch 8/10
10/10 - 2s - loss: 394.1973 - loglik: -3.9488e+02 - logprior: 0.6845
Epoch 9/10
10/10 - 2s - loss: 394.6467 - loglik: -3.9554e+02 - logprior: 0.8935
Fitted a model with MAP estimate = -394.1818
Time for alignment: 54.2980
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 522.5682 - loglik: -4.8060e+02 - logprior: -4.1971e+01
Epoch 2/10
10/10 - 2s - loss: 468.8555 - loglik: -4.5794e+02 - logprior: -1.0914e+01
Epoch 3/10
10/10 - 2s - loss: 440.8820 - loglik: -4.3548e+02 - logprior: -5.4010e+00
Epoch 4/10
10/10 - 1s - loss: 423.7773 - loglik: -4.2016e+02 - logprior: -3.6139e+00
Epoch 5/10
10/10 - 2s - loss: 417.6031 - loglik: -4.1468e+02 - logprior: -2.9231e+00
Epoch 6/10
10/10 - 2s - loss: 415.3066 - loglik: -4.1271e+02 - logprior: -2.5995e+00
Epoch 7/10
10/10 - 2s - loss: 413.5522 - loglik: -4.1147e+02 - logprior: -2.0861e+00
Epoch 8/10
10/10 - 2s - loss: 412.9904 - loglik: -4.1127e+02 - logprior: -1.7194e+00
Epoch 9/10
10/10 - 2s - loss: 412.7045 - loglik: -4.1103e+02 - logprior: -1.6723e+00
Epoch 10/10
10/10 - 2s - loss: 411.4612 - loglik: -4.0973e+02 - logprior: -1.7313e+00
Fitted a model with MAP estimate = -411.7785
expansions: [(0, 2), (12, 1), (14, 2), (15, 1), (20, 1), (21, 1), (23, 2), (30, 1), (32, 1), (46, 1), (55, 2), (58, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 457.8899 - loglik: -4.0669e+02 - logprior: -5.1197e+01
Epoch 2/2
10/10 - 2s - loss: 412.8769 - loglik: -3.9769e+02 - logprior: -1.5186e+01
Fitted a model with MAP estimate = -405.2859
expansions: []
discards: [ 0 17]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 442.7618 - loglik: -3.9946e+02 - logprior: -4.3302e+01
Epoch 2/2
10/10 - 2s - loss: 414.2873 - loglik: -3.9744e+02 - logprior: -1.6844e+01
Fitted a model with MAP estimate = -410.1850
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 437.1179 - loglik: -3.9757e+02 - logprior: -3.9544e+01
Epoch 2/10
10/10 - 2s - loss: 406.7631 - loglik: -3.9602e+02 - logprior: -1.0745e+01
Epoch 3/10
10/10 - 2s - loss: 399.8586 - loglik: -3.9585e+02 - logprior: -4.0112e+00
Epoch 4/10
10/10 - 2s - loss: 397.3002 - loglik: -3.9538e+02 - logprior: -1.9178e+00
Epoch 5/10
10/10 - 2s - loss: 395.0759 - loglik: -3.9423e+02 - logprior: -8.4359e-01
Epoch 6/10
10/10 - 2s - loss: 394.9564 - loglik: -3.9495e+02 - logprior: -4.1345e-03
Epoch 7/10
10/10 - 2s - loss: 394.6769 - loglik: -3.9514e+02 - logprior: 0.4663
Epoch 8/10
10/10 - 2s - loss: 394.7767 - loglik: -3.9548e+02 - logprior: 0.7030
Fitted a model with MAP estimate = -394.2778
Time for alignment: 52.6639
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 523.2502 - loglik: -4.8128e+02 - logprior: -4.1968e+01
Epoch 2/10
10/10 - 2s - loss: 468.4369 - loglik: -4.5753e+02 - logprior: -1.0908e+01
Epoch 3/10
10/10 - 2s - loss: 441.1817 - loglik: -4.3579e+02 - logprior: -5.3952e+00
Epoch 4/10
10/10 - 2s - loss: 424.6082 - loglik: -4.2094e+02 - logprior: -3.6683e+00
Epoch 5/10
10/10 - 2s - loss: 417.4796 - loglik: -4.1458e+02 - logprior: -2.8980e+00
Epoch 6/10
10/10 - 1s - loss: 415.6248 - loglik: -4.1306e+02 - logprior: -2.5635e+00
Epoch 7/10
10/10 - 1s - loss: 414.1993 - loglik: -4.1212e+02 - logprior: -2.0746e+00
Epoch 8/10
10/10 - 2s - loss: 413.3522 - loglik: -4.1165e+02 - logprior: -1.7068e+00
Epoch 9/10
10/10 - 2s - loss: 413.0729 - loglik: -4.1145e+02 - logprior: -1.6244e+00
Epoch 10/10
10/10 - 1s - loss: 411.9263 - loglik: -4.1025e+02 - logprior: -1.6798e+00
Fitted a model with MAP estimate = -412.1852
expansions: [(0, 2), (12, 1), (15, 2), (18, 1), (20, 2), (26, 1), (30, 1), (35, 2), (46, 1), (55, 3), (58, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 459.9744 - loglik: -4.0864e+02 - logprior: -5.1337e+01
Epoch 2/2
10/10 - 2s - loss: 415.5358 - loglik: -4.0003e+02 - logprior: -1.5502e+01
Fitted a model with MAP estimate = -407.5148
expansions: []
discards: [ 0 18 26 45 69]
Re-initialized the encoder parameters.
Fitting a model of length 80 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 444.6452 - loglik: -4.0104e+02 - logprior: -4.3609e+01
Epoch 2/2
10/10 - 2s - loss: 417.3330 - loglik: -4.0033e+02 - logprior: -1.7007e+01
Fitted a model with MAP estimate = -412.2487
expansions: [(30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 438.6541 - loglik: -3.9896e+02 - logprior: -3.9696e+01
Epoch 2/10
10/10 - 2s - loss: 409.4753 - loglik: -3.9862e+02 - logprior: -1.0859e+01
Epoch 3/10
10/10 - 2s - loss: 401.3402 - loglik: -3.9722e+02 - logprior: -4.1214e+00
Epoch 4/10
10/10 - 1s - loss: 398.1053 - loglik: -3.9608e+02 - logprior: -2.0248e+00
Epoch 5/10
10/10 - 2s - loss: 396.6913 - loglik: -3.9575e+02 - logprior: -9.3824e-01
Epoch 6/10
10/10 - 1s - loss: 396.2229 - loglik: -3.9613e+02 - logprior: -9.0742e-02
Epoch 7/10
10/10 - 2s - loss: 395.1225 - loglik: -3.9550e+02 - logprior: 0.3764
Epoch 8/10
10/10 - 2s - loss: 394.8537 - loglik: -3.9547e+02 - logprior: 0.6139
Epoch 9/10
10/10 - 2s - loss: 395.0836 - loglik: -3.9591e+02 - logprior: 0.8241
Fitted a model with MAP estimate = -394.7191
Time for alignment: 52.6804
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 523.3005 - loglik: -4.8133e+02 - logprior: -4.1969e+01
Epoch 2/10
10/10 - 1s - loss: 468.1961 - loglik: -4.5729e+02 - logprior: -1.0909e+01
Epoch 3/10
10/10 - 1s - loss: 441.6453 - loglik: -4.3626e+02 - logprior: -5.3879e+00
Epoch 4/10
10/10 - 2s - loss: 425.3488 - loglik: -4.2172e+02 - logprior: -3.6303e+00
Epoch 5/10
10/10 - 2s - loss: 418.8980 - loglik: -4.1608e+02 - logprior: -2.8159e+00
Epoch 6/10
10/10 - 2s - loss: 415.5985 - loglik: -4.1305e+02 - logprior: -2.5516e+00
Epoch 7/10
10/10 - 2s - loss: 413.7813 - loglik: -4.1168e+02 - logprior: -2.1050e+00
Epoch 8/10
10/10 - 2s - loss: 413.0641 - loglik: -4.1136e+02 - logprior: -1.7037e+00
Epoch 9/10
10/10 - 2s - loss: 412.6053 - loglik: -4.1101e+02 - logprior: -1.5935e+00
Epoch 10/10
10/10 - 2s - loss: 412.3651 - loglik: -4.1069e+02 - logprior: -1.6721e+00
Fitted a model with MAP estimate = -411.8570
expansions: [(0, 2), (13, 1), (15, 2), (18, 1), (20, 1), (21, 1), (23, 2), (30, 1), (32, 1), (46, 1), (55, 2), (58, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 457.9140 - loglik: -4.0668e+02 - logprior: -5.1232e+01
Epoch 2/2
10/10 - 2s - loss: 414.3747 - loglik: -3.9911e+02 - logprior: -1.5264e+01
Fitted a model with MAP estimate = -405.7088
expansions: []
discards: [ 0 18 28]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 442.7757 - loglik: -3.9936e+02 - logprior: -4.3420e+01
Epoch 2/2
10/10 - 2s - loss: 415.6544 - loglik: -3.9874e+02 - logprior: -1.6911e+01
Fitted a model with MAP estimate = -410.7079
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 437.3053 - loglik: -3.9767e+02 - logprior: -3.9632e+01
Epoch 2/10
10/10 - 2s - loss: 408.1605 - loglik: -3.9731e+02 - logprior: -1.0847e+01
Epoch 3/10
10/10 - 2s - loss: 400.0811 - loglik: -3.9596e+02 - logprior: -4.1208e+00
Epoch 4/10
10/10 - 2s - loss: 397.8209 - loglik: -3.9578e+02 - logprior: -2.0381e+00
Epoch 5/10
10/10 - 2s - loss: 396.1017 - loglik: -3.9513e+02 - logprior: -9.6823e-01
Epoch 6/10
10/10 - 2s - loss: 395.4973 - loglik: -3.9537e+02 - logprior: -1.2718e-01
Epoch 7/10
10/10 - 2s - loss: 395.2965 - loglik: -3.9564e+02 - logprior: 0.3442
Epoch 8/10
10/10 - 2s - loss: 395.2533 - loglik: -3.9583e+02 - logprior: 0.5779
Epoch 9/10
10/10 - 2s - loss: 394.9297 - loglik: -3.9572e+02 - logprior: 0.7867
Epoch 10/10
10/10 - 2s - loss: 394.3659 - loglik: -3.9533e+02 - logprior: 0.9649
Fitted a model with MAP estimate = -394.6614
Time for alignment: 56.0766
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 522.9226 - loglik: -4.8096e+02 - logprior: -4.1967e+01
Epoch 2/10
10/10 - 2s - loss: 468.8800 - loglik: -4.5797e+02 - logprior: -1.0905e+01
Epoch 3/10
10/10 - 1s - loss: 441.7144 - loglik: -4.3633e+02 - logprior: -5.3879e+00
Epoch 4/10
10/10 - 2s - loss: 425.2359 - loglik: -4.2157e+02 - logprior: -3.6662e+00
Epoch 5/10
10/10 - 2s - loss: 417.8864 - loglik: -4.1501e+02 - logprior: -2.8806e+00
Epoch 6/10
10/10 - 2s - loss: 415.4667 - loglik: -4.1289e+02 - logprior: -2.5750e+00
Epoch 7/10
10/10 - 2s - loss: 414.1312 - loglik: -4.1203e+02 - logprior: -2.1039e+00
Epoch 8/10
10/10 - 2s - loss: 413.6590 - loglik: -4.1198e+02 - logprior: -1.6761e+00
Epoch 9/10
10/10 - 2s - loss: 413.2316 - loglik: -4.1167e+02 - logprior: -1.5589e+00
Epoch 10/10
10/10 - 2s - loss: 412.7988 - loglik: -4.1120e+02 - logprior: -1.5978e+00
Fitted a model with MAP estimate = -412.6110
expansions: [(0, 2), (12, 1), (15, 2), (18, 1), (20, 2), (23, 1), (26, 1), (30, 1), (35, 2), (46, 1), (55, 3), (58, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 460.2303 - loglik: -4.0891e+02 - logprior: -5.1318e+01
Epoch 2/2
10/10 - 2s - loss: 415.4402 - loglik: -3.9996e+02 - logprior: -1.5479e+01
Fitted a model with MAP estimate = -407.4508
expansions: []
discards: [ 0 18 46 71]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 444.0638 - loglik: -4.0067e+02 - logprior: -4.3391e+01
Epoch 2/2
10/10 - 2s - loss: 416.7557 - loglik: -3.9986e+02 - logprior: -1.6893e+01
Fitted a model with MAP estimate = -411.6645
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 438.2860 - loglik: -3.9868e+02 - logprior: -3.9605e+01
Epoch 2/10
10/10 - 2s - loss: 408.1680 - loglik: -3.9736e+02 - logprior: -1.0805e+01
Epoch 3/10
10/10 - 2s - loss: 399.8903 - loglik: -3.9584e+02 - logprior: -4.0541e+00
Epoch 4/10
10/10 - 2s - loss: 397.5419 - loglik: -3.9557e+02 - logprior: -1.9679e+00
Epoch 5/10
10/10 - 2s - loss: 396.0666 - loglik: -3.9519e+02 - logprior: -8.7959e-01
Epoch 6/10
10/10 - 2s - loss: 395.5556 - loglik: -3.9552e+02 - logprior: -3.8003e-02
Epoch 7/10
10/10 - 2s - loss: 394.5712 - loglik: -3.9500e+02 - logprior: 0.4288
Epoch 8/10
10/10 - 2s - loss: 394.7628 - loglik: -3.9543e+02 - logprior: 0.6638
Fitted a model with MAP estimate = -394.6826
Time for alignment: 51.3148
Computed alignments with likelihoods: ['-394.1818', '-394.2778', '-394.7191', '-394.6614', '-394.6826']
Best model has likelihood: -394.1818  (prior= 0.9960 )
time for generating output: 0.2091
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cryst.projection.fasta
SP score = 0.8157671658660639
Training of 5 independent models on file Ald_Xan_dh_2.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b13697c40>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b1c5b92e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5334b5aa90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b135a0b50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531c6bce50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5940655250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b08d49d30>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5b1c1d53a0>, <__main__.SimpleDirichletPrior object at 0x7f59403600d0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 617.7214 - loglik: -6.0530e+02 - logprior: -1.2426e+01
Epoch 2/10
19/19 - 6s - loss: 554.8705 - loglik: -5.5250e+02 - logprior: -2.3730e+00
Epoch 3/10
19/19 - 7s - loss: 535.1037 - loglik: -5.3308e+02 - logprior: -2.0209e+00
Epoch 4/10
19/19 - 6s - loss: 531.0914 - loglik: -5.2923e+02 - logprior: -1.8564e+00
Epoch 5/10
19/19 - 6s - loss: 529.8566 - loglik: -5.2819e+02 - logprior: -1.6620e+00
Epoch 6/10
19/19 - 7s - loss: 526.9971 - loglik: -5.2530e+02 - logprior: -1.6954e+00
Epoch 7/10
19/19 - 7s - loss: 528.4883 - loglik: -5.2682e+02 - logprior: -1.6655e+00
Fitted a model with MAP estimate = -527.5405
expansions: [(10, 3), (11, 3), (15, 1), (26, 1), (29, 1), (33, 1), (47, 3), (48, 4), (49, 1), (57, 1), (62, 1), (71, 1), (72, 1), (73, 1), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 535.6473 - loglik: -5.2156e+02 - logprior: -1.4086e+01
Epoch 2/2
19/19 - 7s - loss: 517.7980 - loglik: -5.1340e+02 - logprior: -4.3978e+00
Fitted a model with MAP estimate = -513.8373
expansions: [(0, 2)]
discards: [ 0 14 58 60]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 522.1624 - loglik: -5.1274e+02 - logprior: -9.4250e+00
Epoch 2/2
19/19 - 6s - loss: 511.3780 - loglik: -5.1014e+02 - logprior: -1.2373e+00
Fitted a model with MAP estimate = -510.9280
expansions: []
discards: [ 0 13]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 525.1725 - loglik: -5.1393e+02 - logprior: -1.1247e+01
Epoch 2/10
19/19 - 7s - loss: 514.4896 - loglik: -5.1291e+02 - logprior: -1.5778e+00
Epoch 3/10
19/19 - 6s - loss: 510.1131 - loglik: -5.0988e+02 - logprior: -2.3792e-01
Epoch 4/10
19/19 - 6s - loss: 510.4709 - loglik: -5.1058e+02 - logprior: 0.1129
Fitted a model with MAP estimate = -510.5400
Time for alignment: 136.5726
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 616.6203 - loglik: -6.0419e+02 - logprior: -1.2432e+01
Epoch 2/10
19/19 - 6s - loss: 553.7974 - loglik: -5.5138e+02 - logprior: -2.4151e+00
Epoch 3/10
19/19 - 7s - loss: 532.0214 - loglik: -5.2995e+02 - logprior: -2.0667e+00
Epoch 4/10
19/19 - 7s - loss: 529.3011 - loglik: -5.2755e+02 - logprior: -1.7530e+00
Epoch 5/10
19/19 - 7s - loss: 531.6603 - loglik: -5.3010e+02 - logprior: -1.5554e+00
Fitted a model with MAP estimate = -528.7418
expansions: [(10, 3), (11, 2), (12, 1), (15, 1), (27, 1), (29, 1), (32, 1), (33, 1), (47, 2), (48, 4), (49, 2), (50, 2), (73, 4), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 535.8723 - loglik: -5.2178e+02 - logprior: -1.4087e+01
Epoch 2/2
19/19 - 6s - loss: 516.1657 - loglik: -5.1185e+02 - logprior: -4.3128e+00
Fitted a model with MAP estimate = -513.3954
expansions: [(0, 2)]
discards: [ 0 13 62 63 68]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 521.1758 - loglik: -5.1173e+02 - logprior: -9.4476e+00
Epoch 2/2
19/19 - 8s - loss: 511.6001 - loglik: -5.1035e+02 - logprior: -1.2542e+00
Fitted a model with MAP estimate = -510.4197
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 524.5806 - loglik: -5.1340e+02 - logprior: -1.1180e+01
Epoch 2/10
19/19 - 6s - loss: 511.2298 - loglik: -5.0965e+02 - logprior: -1.5804e+00
Epoch 3/10
19/19 - 7s - loss: 512.2568 - loglik: -5.1199e+02 - logprior: -2.6514e-01
Fitted a model with MAP estimate = -510.0295
Time for alignment: 117.0689
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 616.6382 - loglik: -6.0420e+02 - logprior: -1.2434e+01
Epoch 2/10
19/19 - 8s - loss: 552.6011 - loglik: -5.5022e+02 - logprior: -2.3834e+00
Epoch 3/10
19/19 - 7s - loss: 531.4955 - loglik: -5.2955e+02 - logprior: -1.9462e+00
Epoch 4/10
19/19 - 7s - loss: 530.5084 - loglik: -5.2881e+02 - logprior: -1.6956e+00
Epoch 5/10
19/19 - 7s - loss: 529.6083 - loglik: -5.2817e+02 - logprior: -1.4416e+00
Epoch 6/10
19/19 - 6s - loss: 527.9701 - loglik: -5.2654e+02 - logprior: -1.4346e+00
Epoch 7/10
19/19 - 7s - loss: 528.8410 - loglik: -5.2742e+02 - logprior: -1.4193e+00
Fitted a model with MAP estimate = -528.1565
expansions: [(7, 1), (9, 3), (10, 2), (11, 1), (26, 1), (29, 1), (30, 1), (48, 5), (49, 1), (57, 1), (72, 1), (73, 4), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 535.5933 - loglik: -5.2151e+02 - logprior: -1.4086e+01
Epoch 2/2
19/19 - 7s - loss: 518.2015 - loglik: -5.1374e+02 - logprior: -4.4588e+00
Fitted a model with MAP estimate = -514.1356
expansions: [(0, 2)]
discards: [ 0 13 14 91]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 521.9666 - loglik: -5.1252e+02 - logprior: -9.4477e+00
Epoch 2/2
19/19 - 6s - loss: 513.3023 - loglik: -5.1207e+02 - logprior: -1.2279e+00
Fitted a model with MAP estimate = -511.2838
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 524.7013 - loglik: -5.1340e+02 - logprior: -1.1305e+01
Epoch 2/10
19/19 - 8s - loss: 512.7000 - loglik: -5.1115e+02 - logprior: -1.5506e+00
Epoch 3/10
19/19 - 7s - loss: 511.0279 - loglik: -5.1076e+02 - logprior: -2.6856e-01
Epoch 4/10
19/19 - 7s - loss: 510.7198 - loglik: -5.1084e+02 - logprior: 0.1215
Epoch 5/10
19/19 - 7s - loss: 511.3048 - loglik: -5.1158e+02 - logprior: 0.2791
Fitted a model with MAP estimate = -510.0252
Time for alignment: 147.2021
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 615.6165 - loglik: -6.0318e+02 - logprior: -1.2437e+01
Epoch 2/10
19/19 - 7s - loss: 557.5292 - loglik: -5.5514e+02 - logprior: -2.3911e+00
Epoch 3/10
19/19 - 6s - loss: 532.5389 - loglik: -5.3043e+02 - logprior: -2.1069e+00
Epoch 4/10
19/19 - 8s - loss: 531.8391 - loglik: -5.2993e+02 - logprior: -1.9056e+00
Epoch 5/10
19/19 - 6s - loss: 529.3804 - loglik: -5.2762e+02 - logprior: -1.7647e+00
Epoch 6/10
19/19 - 7s - loss: 530.1116 - loglik: -5.2835e+02 - logprior: -1.7617e+00
Fitted a model with MAP estimate = -528.1913
expansions: [(10, 3), (11, 2), (12, 1), (15, 1), (27, 1), (29, 1), (43, 2), (47, 1), (48, 4), (49, 2), (50, 2), (62, 1), (71, 1), (72, 1), (73, 1), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 535.8164 - loglik: -5.2178e+02 - logprior: -1.4041e+01
Epoch 2/2
19/19 - 7s - loss: 517.6254 - loglik: -5.1325e+02 - logprior: -4.3775e+00
Fitted a model with MAP estimate = -513.7402
expansions: [(0, 2)]
discards: [ 0 13 14 61 62 67]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 521.7448 - loglik: -5.1233e+02 - logprior: -9.4188e+00
Epoch 2/2
19/19 - 7s - loss: 512.6741 - loglik: -5.1146e+02 - logprior: -1.2171e+00
Fitted a model with MAP estimate = -510.8940
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 525.2738 - loglik: -5.1405e+02 - logprior: -1.1224e+01
Epoch 2/10
19/19 - 6s - loss: 512.2817 - loglik: -5.1072e+02 - logprior: -1.5654e+00
Epoch 3/10
19/19 - 6s - loss: 510.9367 - loglik: -5.1068e+02 - logprior: -2.5309e-01
Epoch 4/10
19/19 - 5s - loss: 509.6562 - loglik: -5.0976e+02 - logprior: 0.1047
Epoch 5/10
19/19 - 6s - loss: 510.7738 - loglik: -5.1105e+02 - logprior: 0.2728
Fitted a model with MAP estimate = -509.8569
Time for alignment: 133.5256
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 616.6900 - loglik: -6.0425e+02 - logprior: -1.2437e+01
Epoch 2/10
19/19 - 7s - loss: 557.6151 - loglik: -5.5516e+02 - logprior: -2.4517e+00
Epoch 3/10
19/19 - 7s - loss: 534.1757 - loglik: -5.3200e+02 - logprior: -2.1730e+00
Epoch 4/10
19/19 - 6s - loss: 527.5937 - loglik: -5.2564e+02 - logprior: -1.9519e+00
Epoch 5/10
19/19 - 8s - loss: 532.0076 - loglik: -5.3028e+02 - logprior: -1.7293e+00
Fitted a model with MAP estimate = -528.8706
expansions: [(10, 3), (11, 2), (12, 2), (15, 1), (26, 1), (29, 1), (34, 1), (42, 2), (46, 1), (47, 3), (49, 1), (57, 1), (58, 1), (72, 4), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 536.5944 - loglik: -5.2254e+02 - logprior: -1.4056e+01
Epoch 2/2
19/19 - 7s - loss: 517.2697 - loglik: -5.1289e+02 - logprior: -4.3783e+00
Fitted a model with MAP estimate = -513.8097
expansions: [(0, 2)]
discards: [ 0 13 14 16 61]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 521.6481 - loglik: -5.1219e+02 - logprior: -9.4553e+00
Epoch 2/2
19/19 - 7s - loss: 512.3315 - loglik: -5.1107e+02 - logprior: -1.2587e+00
Fitted a model with MAP estimate = -510.9274
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 523.1921 - loglik: -5.1198e+02 - logprior: -1.1213e+01
Epoch 2/10
19/19 - 6s - loss: 513.7883 - loglik: -5.1222e+02 - logprior: -1.5671e+00
Epoch 3/10
19/19 - 6s - loss: 510.1857 - loglik: -5.0989e+02 - logprior: -2.9272e-01
Epoch 4/10
19/19 - 8s - loss: 511.7144 - loglik: -5.1181e+02 - logprior: 0.0946
Fitted a model with MAP estimate = -510.1950
Time for alignment: 123.5763
Computed alignments with likelihoods: ['-510.5400', '-510.0295', '-510.0252', '-509.8569', '-510.1950']
Best model has likelihood: -509.8569  (prior= 0.3303 )
time for generating output: 0.6362
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Ald_Xan_dh_2.projection.fasta
SP score = 0.21470146451370634
Training of 5 independent models on file ace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5afe475e80>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b12e82d60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531d44de50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5314a9c970>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b0a1be5e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f9755be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5cf3cd6d60>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f59206e1ac0>, <__main__.SimpleDirichletPrior object at 0x7f533458a880>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 49s - loss: 2345.5139 - loglik: -2.3423e+03 - logprior: -3.1792e+00
Epoch 2/10
49/49 - 46s - loss: 2209.6882 - loglik: -2.2097e+03 - logprior: -3.5167e-02
Epoch 3/10
49/49 - 46s - loss: 2208.9373 - loglik: -2.2089e+03 - logprior: 2.6557e-04
Epoch 4/10
49/49 - 46s - loss: 2199.0100 - loglik: -2.1988e+03 - logprior: -1.9632e-01
Epoch 5/10
49/49 - 46s - loss: 2193.1936 - loglik: -2.1927e+03 - logprior: -4.4413e-01
Epoch 6/10
49/49 - 46s - loss: 2203.0374 - loglik: -2.2024e+03 - logprior: -6.5838e-01
Fitted a model with MAP estimate = -2197.7917
expansions: [(0, 5), (131, 1), (139, 1), (181, 1), (209, 1), (211, 1), (212, 1), (226, 1), (228, 2), (229, 2), (230, 2), (231, 3), (233, 1), (238, 2), (239, 2), (240, 1), (249, 1), (252, 1), (253, 5), (254, 3), (255, 2), (274, 1), (275, 1), (276, 1), (277, 2), (294, 2), (295, 2), (296, 5), (297, 2), (298, 4), (300, 6), (301, 2), (309, 2), (310, 1), (311, 5), (312, 1), (314, 1), (320, 1), (321, 1), (322, 1), (344, 5), (345, 1), (376, 1), (377, 1), (393, 2), (394, 3), (399, 10), (400, 2), (404, 5)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 35]
Re-initialized the encoder parameters.
Fitting a model of length 499 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 65s - loss: 2190.4421 - loglik: -2.1874e+03 - logprior: -2.9939e+00
Epoch 2/2
49/49 - 62s - loss: 2172.6768 - loglik: -2.1750e+03 - logprior: 2.2805
Fitted a model with MAP estimate = -2170.6194
expansions: [(0, 6), (26, 3), (333, 1), (336, 1), (366, 1), (487, 1), (488, 2), (489, 1)]
discards: [  1   2   3   4   5   6   7   8   9 272 324 363 411 467 494 495 496 497
 498]
Re-initialized the encoder parameters.
Fitting a model of length 496 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 66s - loss: 2183.6826 - loglik: -2.1820e+03 - logprior: -1.6381e+00
Epoch 2/2
49/49 - 62s - loss: 2171.2732 - loglik: -2.1746e+03 - logprior: 3.3456
Fitted a model with MAP estimate = -2170.0186
expansions: [(0, 6), (491, 2)]
discards: [  0   1   2   3   4   5   6   7  23  24 322 492 493 494 495]
Re-initialized the encoder parameters.
Fitting a model of length 489 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 65s - loss: 2180.6409 - loglik: -2.1812e+03 - logprior: 0.6059
Epoch 2/10
49/49 - 61s - loss: 2173.6753 - loglik: -2.1784e+03 - logprior: 4.7701
Epoch 3/10
49/49 - 60s - loss: 2167.3013 - loglik: -2.1727e+03 - logprior: 5.3517
Epoch 4/10
49/49 - 61s - loss: 2177.8611 - loglik: -2.1834e+03 - logprior: 5.4979
Fitted a model with MAP estimate = -2166.8317
Time for alignment: 958.1518
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 49s - loss: 2349.9382 - loglik: -2.3466e+03 - logprior: -3.3696e+00
Epoch 2/10
49/49 - 46s - loss: 2209.0605 - loglik: -2.2088e+03 - logprior: -2.3691e-01
Epoch 3/10
49/49 - 46s - loss: 2201.0840 - loglik: -2.2006e+03 - logprior: -5.0188e-01
Epoch 4/10
49/49 - 46s - loss: 2211.0146 - loglik: -2.2101e+03 - logprior: -8.7134e-01
Fitted a model with MAP estimate = -2201.8857
expansions: [(0, 4), (140, 1), (214, 1), (230, 1), (233, 3), (234, 1), (235, 1), (236, 1), (247, 2), (249, 1), (252, 1), (259, 1), (262, 1), (263, 4), (264, 2), (283, 2), (284, 3), (285, 4), (301, 6), (302, 3), (303, 4), (304, 2), (305, 6), (306, 2), (307, 1), (308, 1), (311, 3), (312, 7), (313, 2), (314, 1), (315, 1), (317, 7), (318, 2), (319, 2), (320, 1), (332, 1), (333, 4), (336, 1), (349, 1), (350, 4), (351, 3), (352, 1), (353, 2), (382, 1), (383, 1), (384, 2), (389, 1), (390, 1)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14 397 398 399 400
 401 402 403]
Re-initialized the encoder parameters.
Fitting a model of length 491 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 64s - loss: 2194.6287 - loglik: -2.1918e+03 - logprior: -2.7927e+00
Epoch 2/2
49/49 - 61s - loss: 2172.3398 - loglik: -2.1748e+03 - logprior: 2.4769
Fitted a model with MAP estimate = -2172.9093
expansions: [(0, 6), (303, 1), (324, 3), (385, 1), (491, 6)]
discards: [  1   2   3   4   5   6   7   8  25 227 337 338 405 406 464]
Re-initialized the encoder parameters.
Fitting a model of length 493 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 65s - loss: 2180.7874 - loglik: -2.1789e+03 - logprior: -1.9208e+00
Epoch 2/2
49/49 - 61s - loss: 2176.6960 - loglik: -2.1796e+03 - logprior: 2.9282
Fitted a model with MAP estimate = -2170.2254
expansions: [(0, 7)]
discards: [  1   2   3   4   5   6 486 487 488 489 490 491 492]
Re-initialized the encoder parameters.
Fitting a model of length 487 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 63s - loss: 2181.5315 - loglik: -2.1807e+03 - logprior: -8.3922e-01
Epoch 2/10
49/49 - 61s - loss: 2175.1699 - loglik: -2.1797e+03 - logprior: 4.5195
Epoch 3/10
49/49 - 60s - loss: 2174.7283 - loglik: -2.1799e+03 - logprior: 5.1708
Epoch 4/10
49/49 - 60s - loss: 2168.3235 - loglik: -2.1737e+03 - logprior: 5.3951
Epoch 5/10
49/49 - 60s - loss: 2161.3206 - loglik: -2.1667e+03 - logprior: 5.4187
Epoch 6/10
49/49 - 60s - loss: 2177.1252 - loglik: -2.1828e+03 - logprior: 5.6328
Fitted a model with MAP estimate = -2165.7261
Time for alignment: 979.4340
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 51s - loss: 2351.8845 - loglik: -2.3486e+03 - logprior: -3.2807e+00
Epoch 2/10
49/49 - 46s - loss: 2206.2737 - loglik: -2.2061e+03 - logprior: -2.0918e-01
Epoch 3/10
49/49 - 46s - loss: 2199.8413 - loglik: -2.1995e+03 - logprior: -3.5583e-01
Epoch 4/10
49/49 - 46s - loss: 2207.1150 - loglik: -2.2066e+03 - logprior: -5.3778e-01
Fitted a model with MAP estimate = -2200.8030
expansions: [(0, 5), (110, 1), (138, 1), (142, 1), (184, 1), (194, 1), (211, 1), (213, 1), (214, 1), (221, 1), (227, 1), (229, 2), (230, 2), (231, 2), (232, 3), (234, 2), (239, 2), (241, 1), (249, 1), (250, 1), (251, 1), (252, 2), (253, 5), (254, 3), (255, 2), (271, 1), (272, 1), (274, 1), (275, 1), (276, 1), (277, 1), (293, 2), (294, 2), (295, 2), (296, 3), (297, 3), (298, 4), (300, 6), (301, 2), (309, 1), (310, 1), (312, 4), (313, 1), (314, 1), (323, 1), (324, 1), (325, 1), (327, 1), (331, 1), (345, 3), (347, 1), (365, 1), (366, 2), (394, 2), (395, 3), (399, 11), (404, 5)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 34 35 36]
Re-initialized the encoder parameters.
Fitting a model of length 499 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 66s - loss: 2189.3579 - loglik: -2.1866e+03 - logprior: -2.7139e+00
Epoch 2/2
49/49 - 62s - loss: 2180.3003 - loglik: -2.1824e+03 - logprior: 2.1438
Fitted a model with MAP estimate = -2170.4039
expansions: [(0, 7), (332, 1), (483, 1), (488, 1), (489, 3)]
discards: [  1   2   3   4   5   6  79 245 324 469 494 495 496 497 498]
Re-initialized the encoder parameters.
Fitting a model of length 497 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 67s - loss: 2185.1450 - loglik: -2.1833e+03 - logprior: -1.8440e+00
Epoch 2/2
49/49 - 62s - loss: 2168.7461 - loglik: -2.1718e+03 - logprior: 3.0788
Fitted a model with MAP estimate = -2169.7751
expansions: [(0, 6), (24, 3), (327, 1), (492, 2)]
discards: [  1   2   3   4   5   6   7 493 494 495 496]
Re-initialized the encoder parameters.
Fitting a model of length 498 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 65s - loss: 2182.4487 - loglik: -2.1816e+03 - logprior: -8.4493e-01
Epoch 2/10
49/49 - 62s - loss: 2172.4790 - loglik: -2.1769e+03 - logprior: 4.3816
Epoch 3/10
49/49 - 63s - loss: 2169.7786 - loglik: -2.1746e+03 - logprior: 4.8679
Epoch 4/10
49/49 - 62s - loss: 2165.2214 - loglik: -2.1704e+03 - logprior: 5.1935
Epoch 5/10
49/49 - 63s - loss: 2166.7961 - loglik: -2.1721e+03 - logprior: 5.3023
Fitted a model with MAP estimate = -2164.3986
Time for alignment: 938.3826
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 51s - loss: 2345.9622 - loglik: -2.3426e+03 - logprior: -3.3801e+00
Epoch 2/10
49/49 - 46s - loss: 2213.5752 - loglik: -2.2132e+03 - logprior: -3.4004e-01
Epoch 3/10
49/49 - 46s - loss: 2195.0020 - loglik: -2.1946e+03 - logprior: -4.5146e-01
Epoch 4/10
49/49 - 46s - loss: 2205.8848 - loglik: -2.2053e+03 - logprior: -6.2500e-01
Fitted a model with MAP estimate = -2200.6831
expansions: [(0, 5), (35, 3), (135, 1), (138, 1), (191, 1), (211, 1), (212, 1), (230, 4), (231, 3), (232, 1), (235, 2), (239, 2), (240, 2), (241, 2), (250, 1), (252, 2), (253, 6), (254, 3), (255, 2), (271, 1), (272, 1), (273, 2), (274, 1), (275, 1), (293, 1), (294, 2), (295, 1), (296, 3), (297, 2), (298, 4), (300, 8), (310, 2), (311, 1), (312, 4), (313, 1), (314, 1), (316, 1), (319, 1), (320, 2), (321, 2), (322, 2), (323, 2), (324, 1), (325, 2), (339, 1), (340, 1), (342, 1), (359, 3), (360, 2), (361, 3), (362, 1), (363, 2), (364, 1), (382, 2), (383, 3), (388, 2), (389, 2), (390, 1), (391, 4)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  17  18 399 400
 401 402 403]
Re-initialized the encoder parameters.
Fitting a model of length 504 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 67s - loss: 2189.2207 - loglik: -2.1860e+03 - logprior: -3.2173e+00
Epoch 2/2
49/49 - 63s - loss: 2184.6375 - loglik: -2.1869e+03 - logprior: 2.3090
Fitted a model with MAP estimate = -2171.7691
expansions: [(0, 6), (334, 1), (335, 1), (488, 1), (493, 1), (504, 5)]
discards: [  1   2   3   4   5   6   7  24  25 252 274 275 326 327 346 366 391 393
 443 444 474]
Re-initialized the encoder parameters.
Fitting a model of length 498 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 66s - loss: 2182.0283 - loglik: -2.1802e+03 - logprior: -1.7873e+00
Epoch 2/2
49/49 - 62s - loss: 2173.8311 - loglik: -2.1767e+03 - logprior: 2.9027
Fitted a model with MAP estimate = -2169.6857
expansions: [(0, 7)]
discards: [  1   2   3   4   5   6   7  23 320 490 491 492 493 494 495 496 497]
Re-initialized the encoder parameters.
Fitting a model of length 488 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 64s - loss: 2177.0757 - loglik: -2.1762e+03 - logprior: -8.2579e-01
Epoch 2/10
49/49 - 60s - loss: 2178.4180 - loglik: -2.1830e+03 - logprior: 4.5470
Fitted a model with MAP estimate = -2170.8233
Time for alignment: 749.3203
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 49s - loss: 2354.8733 - loglik: -2.3516e+03 - logprior: -3.2939e+00
Epoch 2/10
49/49 - 46s - loss: 2217.3289 - loglik: -2.2173e+03 - logprior: -5.2776e-02
Epoch 3/10
49/49 - 46s - loss: 2194.6304 - loglik: -2.1944e+03 - logprior: -2.1881e-01
Epoch 4/10
49/49 - 46s - loss: 2217.3455 - loglik: -2.2170e+03 - logprior: -3.5928e-01
Fitted a model with MAP estimate = -2202.9929
expansions: [(0, 5), (132, 1), (142, 1), (176, 1), (183, 1), (184, 1), (185, 1), (212, 1), (213, 1), (216, 1), (226, 1), (228, 4), (229, 3), (230, 3), (232, 2), (236, 2), (237, 3), (242, 1), (245, 1), (247, 2), (248, 3), (249, 4), (250, 2), (251, 1), (267, 1), (268, 1), (269, 2), (270, 1), (271, 1), (289, 1), (290, 2), (291, 2), (292, 4), (295, 3), (297, 7), (307, 1), (310, 3), (312, 1), (313, 1), (324, 1), (325, 1), (327, 1), (343, 2), (344, 5), (345, 1), (364, 1), (393, 6), (395, 11)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 34 35 36 89 90]
Re-initialized the encoder parameters.
Fitting a model of length 491 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 66s - loss: 2197.1870 - loglik: -2.1940e+03 - logprior: -3.1969e+00
Epoch 2/2
49/49 - 61s - loss: 2172.2854 - loglik: -2.1744e+03 - logprior: 2.0757
Fitted a model with MAP estimate = -2173.2759
expansions: [(0, 6), (227, 1), (248, 1), (478, 1), (482, 6)]
discards: [  1   2   3   4   5   6   7   8 233 270 405 486 487 488 489 490]
Re-initialized the encoder parameters.
Fitting a model of length 490 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 64s - loss: 2180.8293 - loglik: -2.1793e+03 - logprior: -1.5689e+00
Epoch 2/2
49/49 - 61s - loss: 2173.9773 - loglik: -2.1773e+03 - logprior: 3.3634
Fitted a model with MAP estimate = -2171.8050
expansions: [(0, 6), (481, 1), (483, 1), (484, 1), (490, 5)]
discards: [  1   2   3   4   5   6   7 320]
Re-initialized the encoder parameters.
Fitting a model of length 496 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 65s - loss: 2181.6909 - loglik: -2.1806e+03 - logprior: -1.1101e+00
Epoch 2/10
49/49 - 61s - loss: 2164.9849 - loglik: -2.1691e+03 - logprior: 4.1469
Epoch 3/10
49/49 - 62s - loss: 2177.1946 - loglik: -2.1818e+03 - logprior: 4.5869
Fitted a model with MAP estimate = -2165.7339
Time for alignment: 800.8238
Computed alignments with likelihoods: ['-2166.8317', '-2165.7261', '-2164.3986', '-2169.6857', '-2165.7339']
Best model has likelihood: -2164.3986  (prior= 5.5313 )
time for generating output: 0.5844
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ace.projection.fasta
SP score = 0.8234195785542812
Training of 5 independent models on file tRNA-synt_2b.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f530d200af0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5335037eb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f598c080970>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f590042cf40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531453c400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5ca7a1b580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f53144acca0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5940348310>, <__main__.SimpleDirichletPrior object at 0x7f5b13e117c0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 1008.4656 - loglik: -1.0062e+03 - logprior: -2.2633e+00
Epoch 2/10
39/39 - 11s - loss: 941.7260 - loglik: -9.4012e+02 - logprior: -1.6083e+00
Epoch 3/10
39/39 - 11s - loss: 935.1883 - loglik: -9.3351e+02 - logprior: -1.6792e+00
Epoch 4/10
39/39 - 11s - loss: 933.8347 - loglik: -9.3211e+02 - logprior: -1.7209e+00
Epoch 5/10
39/39 - 11s - loss: 932.3990 - loglik: -9.3059e+02 - logprior: -1.8103e+00
Epoch 6/10
39/39 - 11s - loss: 932.3939 - loglik: -9.3048e+02 - logprior: -1.9129e+00
Epoch 7/10
39/39 - 11s - loss: 931.9196 - loglik: -9.2995e+02 - logprior: -1.9740e+00
Epoch 8/10
39/39 - 12s - loss: 931.5569 - loglik: -9.2951e+02 - logprior: -2.0428e+00
Epoch 9/10
39/39 - 11s - loss: 932.2943 - loglik: -9.3021e+02 - logprior: -2.0868e+00
Fitted a model with MAP estimate = -920.2542
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (32, 1), (39, 3), (44, 1), (45, 1), (55, 2), (59, 1), (60, 1), (68, 1), (69, 2), (71, 2), (80, 1), (89, 2), (92, 1), (96, 1), (97, 1), (99, 1), (103, 2), (104, 2), (106, 1), (109, 1), (118, 1), (119, 1), (122, 1), (126, 2), (133, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 928.0334 - loglik: -9.2494e+02 - logprior: -3.0935e+00
Epoch 2/2
39/39 - 14s - loss: 914.9672 - loglik: -9.1391e+02 - logprior: -1.0555e+00
Fitted a model with MAP estimate = -901.1981
expansions: []
discards: [ 13  68  87  91 112 155 173]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 917.7349 - loglik: -9.1574e+02 - logprior: -1.9962e+00
Epoch 2/2
39/39 - 12s - loss: 913.4642 - loglik: -9.1271e+02 - logprior: -7.5120e-01
Fitted a model with MAP estimate = -901.1425
expansions: [(150, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 172 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 16s - loss: 902.6567 - loglik: -9.0101e+02 - logprior: -1.6434e+00
Epoch 2/10
41/41 - 13s - loss: 899.0142 - loglik: -8.9845e+02 - logprior: -5.6629e-01
Epoch 3/10
41/41 - 13s - loss: 897.4154 - loglik: -8.9691e+02 - logprior: -5.0226e-01
Epoch 4/10
41/41 - 13s - loss: 898.5314 - loglik: -8.9805e+02 - logprior: -4.7646e-01
Fitted a model with MAP estimate = -895.9880
Time for alignment: 290.6422
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 1008.9925 - loglik: -1.0067e+03 - logprior: -2.2604e+00
Epoch 2/10
39/39 - 11s - loss: 944.1581 - loglik: -9.4262e+02 - logprior: -1.5431e+00
Epoch 3/10
39/39 - 11s - loss: 936.0872 - loglik: -9.3448e+02 - logprior: -1.6037e+00
Epoch 4/10
39/39 - 11s - loss: 934.0765 - loglik: -9.3243e+02 - logprior: -1.6507e+00
Epoch 5/10
39/39 - 11s - loss: 932.9850 - loglik: -9.3125e+02 - logprior: -1.7301e+00
Epoch 6/10
39/39 - 12s - loss: 932.0700 - loglik: -9.3024e+02 - logprior: -1.8317e+00
Epoch 7/10
39/39 - 11s - loss: 932.0931 - loglik: -9.3020e+02 - logprior: -1.8976e+00
Fitted a model with MAP estimate = -921.2897
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 2), (33, 1), (40, 4), (42, 2), (43, 2), (54, 1), (56, 1), (58, 2), (59, 2), (69, 2), (71, 2), (80, 1), (89, 2), (92, 1), (93, 1), (95, 1), (99, 1), (103, 2), (104, 3), (106, 2), (118, 1), (122, 1), (126, 2), (131, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 929.2822 - loglik: -9.2615e+02 - logprior: -3.1350e+00
Epoch 2/2
39/39 - 13s - loss: 916.1131 - loglik: -9.1498e+02 - logprior: -1.1366e+00
Fitted a model with MAP estimate = -903.1524
expansions: []
discards: [ 13  29  51  52  56  78  80  92  96 117 138 145 170]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 918.1469 - loglik: -9.1613e+02 - logprior: -2.0207e+00
Epoch 2/2
39/39 - 13s - loss: 914.6340 - loglik: -9.1390e+02 - logprior: -7.3528e-01
Fitted a model with MAP estimate = -903.1021
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 169 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 18s - loss: 904.2013 - loglik: -9.0253e+02 - logprior: -1.6745e+00
Epoch 2/10
41/41 - 14s - loss: 901.7568 - loglik: -9.0119e+02 - logprior: -5.6368e-01
Epoch 3/10
41/41 - 14s - loss: 901.5019 - loglik: -9.0101e+02 - logprior: -4.9253e-01
Epoch 4/10
41/41 - 14s - loss: 899.2874 - loglik: -8.9881e+02 - logprior: -4.7809e-01
Epoch 5/10
41/41 - 14s - loss: 896.0149 - loglik: -8.9549e+02 - logprior: -5.2043e-01
Epoch 6/10
41/41 - 14s - loss: 898.4016 - loglik: -8.9784e+02 - logprior: -5.5774e-01
Fitted a model with MAP estimate = -896.4429
Time for alignment: 298.0393
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 1008.9651 - loglik: -1.0067e+03 - logprior: -2.2676e+00
Epoch 2/10
39/39 - 11s - loss: 942.9589 - loglik: -9.4136e+02 - logprior: -1.6012e+00
Epoch 3/10
39/39 - 11s - loss: 934.7515 - loglik: -9.3306e+02 - logprior: -1.6923e+00
Epoch 4/10
39/39 - 11s - loss: 932.1614 - loglik: -9.3040e+02 - logprior: -1.7656e+00
Epoch 5/10
39/39 - 11s - loss: 930.1217 - loglik: -9.2828e+02 - logprior: -1.8419e+00
Epoch 6/10
39/39 - 11s - loss: 930.0869 - loglik: -9.2810e+02 - logprior: -1.9873e+00
Epoch 7/10
39/39 - 11s - loss: 929.1645 - loglik: -9.2713e+02 - logprior: -2.0303e+00
Epoch 8/10
39/39 - 12s - loss: 929.0151 - loglik: -9.2692e+02 - logprior: -2.0917e+00
Epoch 9/10
39/39 - 12s - loss: 928.4841 - loglik: -9.2635e+02 - logprior: -2.1355e+00
Epoch 10/10
39/39 - 11s - loss: 928.3327 - loglik: -9.2615e+02 - logprior: -2.1781e+00
Fitted a model with MAP estimate = -917.9462
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 2), (26, 2), (32, 1), (39, 3), (44, 1), (45, 1), (55, 2), (57, 2), (58, 1), (68, 1), (69, 1), (71, 2), (80, 1), (89, 2), (92, 1), (93, 2), (100, 2), (102, 1), (103, 2), (104, 1), (105, 1), (107, 1), (109, 1), (118, 1), (120, 1), (124, 2), (126, 2), (131, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 925.8798 - loglik: -9.2271e+02 - logprior: -3.1672e+00
Epoch 2/2
39/39 - 13s - loss: 911.2970 - loglik: -9.1014e+02 - logprior: -1.1587e+00
Fitted a model with MAP estimate = -897.7559
expansions: []
discards: [ 13  29  35  51  72  76  94 115 122 131 166 167 170]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 914.4630 - loglik: -9.1247e+02 - logprior: -1.9900e+00
Epoch 2/2
39/39 - 13s - loss: 910.9744 - loglik: -9.1026e+02 - logprior: -7.1415e-01
Fitted a model with MAP estimate = -898.8172
expansions: [(158, 1)]
discards: [127]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 17s - loss: 901.1553 - loglik: -8.9954e+02 - logprior: -1.6201e+00
Epoch 2/10
41/41 - 14s - loss: 897.5061 - loglik: -8.9698e+02 - logprior: -5.2746e-01
Epoch 3/10
41/41 - 14s - loss: 895.8101 - loglik: -8.9534e+02 - logprior: -4.6608e-01
Epoch 4/10
41/41 - 14s - loss: 894.7278 - loglik: -8.9428e+02 - logprior: -4.4444e-01
Epoch 5/10
41/41 - 14s - loss: 894.9519 - loglik: -8.9449e+02 - logprior: -4.6670e-01
Fitted a model with MAP estimate = -893.6831
Time for alignment: 314.7876
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 1009.6299 - loglik: -1.0074e+03 - logprior: -2.2479e+00
Epoch 2/10
39/39 - 12s - loss: 941.9891 - loglik: -9.4035e+02 - logprior: -1.6428e+00
Epoch 3/10
39/39 - 12s - loss: 934.6349 - loglik: -9.3291e+02 - logprior: -1.7267e+00
Epoch 4/10
39/39 - 11s - loss: 932.2806 - loglik: -9.3049e+02 - logprior: -1.7883e+00
Epoch 5/10
39/39 - 12s - loss: 930.9418 - loglik: -9.2906e+02 - logprior: -1.8832e+00
Epoch 6/10
39/39 - 12s - loss: 930.8886 - loglik: -9.2892e+02 - logprior: -1.9705e+00
Epoch 7/10
39/39 - 11s - loss: 930.2281 - loglik: -9.2818e+02 - logprior: -2.0448e+00
Epoch 8/10
39/39 - 11s - loss: 929.7530 - loglik: -9.2765e+02 - logprior: -2.1067e+00
Epoch 9/10
39/39 - 11s - loss: 930.1774 - loglik: -9.2803e+02 - logprior: -2.1442e+00
Fitted a model with MAP estimate = -919.3100
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (27, 1), (39, 3), (40, 2), (45, 1), (55, 2), (59, 1), (60, 2), (69, 1), (70, 2), (71, 2), (80, 1), (89, 2), (92, 1), (93, 2), (95, 1), (99, 1), (103, 2), (104, 1), (105, 1), (107, 1), (109, 2), (118, 1), (119, 1), (122, 1), (125, 1), (133, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 927.5039 - loglik: -9.2435e+02 - logprior: -3.1517e+00
Epoch 2/2
39/39 - 13s - loss: 913.6938 - loglik: -9.1252e+02 - logprior: -1.1690e+00
Fitted a model with MAP estimate = -900.3216
expansions: []
discards: [ 13  50  51  52  69  77  91  93 114 121 136 147]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 916.7922 - loglik: -9.1479e+02 - logprior: -1.9989e+00
Epoch 2/2
39/39 - 13s - loss: 913.1447 - loglik: -9.1239e+02 - logprior: -7.5215e-01
Fitted a model with MAP estimate = -901.0931
expansions: [(49, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 171 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 17s - loss: 901.3998 - loglik: -8.9976e+02 - logprior: -1.6395e+00
Epoch 2/10
41/41 - 13s - loss: 899.1580 - loglik: -8.9860e+02 - logprior: -5.5581e-01
Epoch 3/10
41/41 - 13s - loss: 895.9612 - loglik: -8.9546e+02 - logprior: -5.0538e-01
Epoch 4/10
41/41 - 13s - loss: 896.5483 - loglik: -8.9606e+02 - logprior: -4.9049e-01
Fitted a model with MAP estimate = -894.7373
Time for alignment: 288.7962
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 1008.2214 - loglik: -1.0060e+03 - logprior: -2.2600e+00
Epoch 2/10
39/39 - 11s - loss: 938.6534 - loglik: -9.3698e+02 - logprior: -1.6689e+00
Epoch 3/10
39/39 - 11s - loss: 931.3850 - loglik: -9.2957e+02 - logprior: -1.8125e+00
Epoch 4/10
39/39 - 11s - loss: 930.1429 - loglik: -9.2831e+02 - logprior: -1.8374e+00
Epoch 5/10
39/39 - 11s - loss: 928.4304 - loglik: -9.2651e+02 - logprior: -1.9203e+00
Epoch 6/10
39/39 - 11s - loss: 928.1904 - loglik: -9.2615e+02 - logprior: -2.0411e+00
Epoch 7/10
39/39 - 11s - loss: 928.1628 - loglik: -9.2604e+02 - logprior: -2.1188e+00
Epoch 8/10
39/39 - 11s - loss: 927.8486 - loglik: -9.2566e+02 - logprior: -2.1846e+00
Epoch 9/10
39/39 - 11s - loss: 927.0889 - loglik: -9.2489e+02 - logprior: -2.2031e+00
Epoch 10/10
39/39 - 11s - loss: 928.0414 - loglik: -9.2580e+02 - logprior: -2.2401e+00
Fitted a model with MAP estimate = -916.6845
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (26, 1), (39, 1), (40, 1), (41, 2), (44, 1), (45, 1), (55, 2), (57, 2), (58, 1), (59, 2), (69, 2), (71, 2), (80, 1), (89, 2), (92, 1), (93, 2), (95, 1), (99, 1), (103, 1), (104, 1), (105, 1), (106, 1), (109, 1), (118, 1), (119, 1), (122, 1), (126, 2), (133, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 925.5836 - loglik: -9.2243e+02 - logprior: -3.1544e+00
Epoch 2/2
39/39 - 13s - loss: 912.2341 - loglik: -9.1106e+02 - logprior: -1.1769e+00
Fitted a model with MAP estimate = -898.6898
expansions: []
discards: [ 13  52  70  73  78  90  94 115 122 176]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 914.2616 - loglik: -9.1228e+02 - logprior: -1.9810e+00
Epoch 2/2
39/39 - 12s - loss: 910.9604 - loglik: -9.1027e+02 - logprior: -6.9237e-01
Fitted a model with MAP estimate = -899.0125
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 171 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 17s - loss: 900.9146 - loglik: -8.9930e+02 - logprior: -1.6183e+00
Epoch 2/10
41/41 - 13s - loss: 896.5588 - loglik: -8.9603e+02 - logprior: -5.3356e-01
Epoch 3/10
41/41 - 13s - loss: 896.9380 - loglik: -8.9645e+02 - logprior: -4.8919e-01
Fitted a model with MAP estimate = -895.4809
Time for alignment: 280.7696
Computed alignments with likelihoods: ['-895.9880', '-896.4429', '-893.6831', '-894.7373', '-895.4809']
Best model has likelihood: -893.6831  (prior= -0.5315 )
time for generating output: 0.3563
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tRNA-synt_2b.projection.fasta
SP score = 0.25225225225225223
Training of 5 independent models on file ChtBD.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5900d209d0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b07cdddc0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f59205e46a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5cb01fa700>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b0991a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5315fd75e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5334d33340>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f531cdd1f10>, <__main__.SimpleDirichletPrior object at 0x7f52f0db89a0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 297.2377 - loglik: -2.3551e+02 - logprior: -6.1727e+01
Epoch 2/10
10/10 - 1s - loss: 229.7677 - loglik: -2.1257e+02 - logprior: -1.7200e+01
Epoch 3/10
10/10 - 1s - loss: 205.2247 - loglik: -1.9673e+02 - logprior: -8.4954e+00
Epoch 4/10
10/10 - 1s - loss: 194.1954 - loglik: -1.8881e+02 - logprior: -5.3874e+00
Epoch 5/10
10/10 - 1s - loss: 189.4612 - loglik: -1.8567e+02 - logprior: -3.7890e+00
Epoch 6/10
10/10 - 1s - loss: 188.0392 - loglik: -1.8503e+02 - logprior: -3.0090e+00
Epoch 7/10
10/10 - 1s - loss: 187.0026 - loglik: -1.8448e+02 - logprior: -2.5216e+00
Epoch 8/10
10/10 - 1s - loss: 186.6860 - loglik: -1.8454e+02 - logprior: -2.1410e+00
Epoch 9/10
10/10 - 1s - loss: 186.4423 - loglik: -1.8457e+02 - logprior: -1.8725e+00
Epoch 10/10
10/10 - 1s - loss: 186.3660 - loglik: -1.8462e+02 - logprior: -1.7440e+00
Fitted a model with MAP estimate = -186.1519
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 262.4617 - loglik: -1.8263e+02 - logprior: -7.9831e+01
Epoch 2/2
10/10 - 1s - loss: 203.6196 - loglik: -1.7786e+02 - logprior: -2.5759e+01
Fitted a model with MAP estimate = -192.5442
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 241.5478 - loglik: -1.7565e+02 - logprior: -6.5897e+01
Epoch 2/2
10/10 - 1s - loss: 197.7829 - loglik: -1.7468e+02 - logprior: -2.3098e+01
Fitted a model with MAP estimate = -188.9435
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 241.3672 - loglik: -1.7564e+02 - logprior: -6.5730e+01
Epoch 2/10
10/10 - 1s - loss: 203.0750 - loglik: -1.7581e+02 - logprior: -2.7265e+01
Epoch 3/10
10/10 - 1s - loss: 191.8827 - loglik: -1.7590e+02 - logprior: -1.5985e+01
Epoch 4/10
10/10 - 1s - loss: 182.5697 - loglik: -1.7615e+02 - logprior: -6.4242e+00
Epoch 5/10
10/10 - 1s - loss: 179.0142 - loglik: -1.7619e+02 - logprior: -2.8276e+00
Epoch 6/10
10/10 - 1s - loss: 177.6805 - loglik: -1.7600e+02 - logprior: -1.6806e+00
Epoch 7/10
10/10 - 1s - loss: 177.3251 - loglik: -1.7622e+02 - logprior: -1.1085e+00
Epoch 8/10
10/10 - 1s - loss: 177.1469 - loglik: -1.7639e+02 - logprior: -7.5357e-01
Epoch 9/10
10/10 - 1s - loss: 176.6452 - loglik: -1.7614e+02 - logprior: -5.0335e-01
Epoch 10/10
10/10 - 1s - loss: 176.7636 - loglik: -1.7646e+02 - logprior: -2.9905e-01
Fitted a model with MAP estimate = -176.5148
Time for alignment: 30.5496
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 297.1272 - loglik: -2.3540e+02 - logprior: -6.1728e+01
Epoch 2/10
10/10 - 1s - loss: 229.9794 - loglik: -2.1277e+02 - logprior: -1.7205e+01
Epoch 3/10
10/10 - 1s - loss: 204.6572 - loglik: -1.9618e+02 - logprior: -8.4800e+00
Epoch 4/10
10/10 - 1s - loss: 193.9892 - loglik: -1.8869e+02 - logprior: -5.3011e+00
Epoch 5/10
10/10 - 1s - loss: 188.6002 - loglik: -1.8491e+02 - logprior: -3.6941e+00
Epoch 6/10
10/10 - 1s - loss: 186.4256 - loglik: -1.8351e+02 - logprior: -2.9121e+00
Epoch 7/10
10/10 - 1s - loss: 185.7445 - loglik: -1.8329e+02 - logprior: -2.4573e+00
Epoch 8/10
10/10 - 1s - loss: 185.3094 - loglik: -1.8321e+02 - logprior: -2.0996e+00
Epoch 9/10
10/10 - 1s - loss: 185.0402 - loglik: -1.8323e+02 - logprior: -1.8082e+00
Epoch 10/10
10/10 - 1s - loss: 184.8977 - loglik: -1.8329e+02 - logprior: -1.6107e+00
Fitted a model with MAP estimate = -184.7931
expansions: [(0, 4), (10, 2), (26, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 261.5042 - loglik: -1.8159e+02 - logprior: -7.9918e+01
Epoch 2/2
10/10 - 1s - loss: 203.3750 - loglik: -1.7759e+02 - logprior: -2.5786e+01
Fitted a model with MAP estimate = -192.2645
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 241.2851 - loglik: -1.7524e+02 - logprior: -6.6040e+01
Epoch 2/2
10/10 - 1s - loss: 197.6863 - loglik: -1.7455e+02 - logprior: -2.3136e+01
Fitted a model with MAP estimate = -188.7051
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 38 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 234.2275 - loglik: -1.7514e+02 - logprior: -5.9087e+01
Epoch 2/10
10/10 - 1s - loss: 192.3559 - loglik: -1.7568e+02 - logprior: -1.6681e+01
Epoch 3/10
10/10 - 1s - loss: 184.3590 - loglik: -1.7667e+02 - logprior: -7.6849e+00
Epoch 4/10
10/10 - 1s - loss: 181.3687 - loglik: -1.7702e+02 - logprior: -4.3485e+00
Epoch 5/10
10/10 - 1s - loss: 179.3915 - loglik: -1.7669e+02 - logprior: -2.7026e+00
Epoch 6/10
10/10 - 1s - loss: 178.5222 - loglik: -1.7671e+02 - logprior: -1.8124e+00
Epoch 7/10
10/10 - 1s - loss: 177.7204 - loglik: -1.7644e+02 - logprior: -1.2786e+00
Epoch 8/10
10/10 - 1s - loss: 177.6037 - loglik: -1.7670e+02 - logprior: -9.0026e-01
Epoch 9/10
10/10 - 1s - loss: 177.3489 - loglik: -1.7673e+02 - logprior: -6.2067e-01
Epoch 10/10
10/10 - 1s - loss: 177.1368 - loglik: -1.7671e+02 - logprior: -4.2313e-01
Fitted a model with MAP estimate = -177.0793
Time for alignment: 29.6390
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 297.3772 - loglik: -2.3565e+02 - logprior: -6.1728e+01
Epoch 2/10
10/10 - 1s - loss: 229.7041 - loglik: -2.1250e+02 - logprior: -1.7205e+01
Epoch 3/10
10/10 - 1s - loss: 204.5746 - loglik: -1.9609e+02 - logprior: -8.4814e+00
Epoch 4/10
10/10 - 1s - loss: 194.2187 - loglik: -1.8891e+02 - logprior: -5.3126e+00
Epoch 5/10
10/10 - 1s - loss: 189.9696 - loglik: -1.8623e+02 - logprior: -3.7386e+00
Epoch 6/10
10/10 - 1s - loss: 187.2193 - loglik: -1.8424e+02 - logprior: -2.9756e+00
Epoch 7/10
10/10 - 1s - loss: 186.3376 - loglik: -1.8382e+02 - logprior: -2.5163e+00
Epoch 8/10
10/10 - 1s - loss: 185.4607 - loglik: -1.8329e+02 - logprior: -2.1699e+00
Epoch 9/10
10/10 - 1s - loss: 185.3634 - loglik: -1.8349e+02 - logprior: -1.8738e+00
Epoch 10/10
10/10 - 1s - loss: 185.1187 - loglik: -1.8345e+02 - logprior: -1.6640e+00
Fitted a model with MAP estimate = -185.0675
expansions: [(0, 4), (10, 2), (21, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 261.4979 - loglik: -1.8161e+02 - logprior: -7.9892e+01
Epoch 2/2
10/10 - 1s - loss: 203.0202 - loglik: -1.7742e+02 - logprior: -2.5597e+01
Fitted a model with MAP estimate = -192.0081
expansions: [(0, 2)]
discards: [14]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 241.1524 - loglik: -1.7514e+02 - logprior: -6.6011e+01
Epoch 2/2
10/10 - 1s - loss: 197.5835 - loglik: -1.7446e+02 - logprior: -2.3127e+01
Fitted a model with MAP estimate = -188.7081
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 241.0792 - loglik: -1.7523e+02 - logprior: -6.5854e+01
Epoch 2/10
10/10 - 1s - loss: 203.0345 - loglik: -1.7575e+02 - logprior: -2.7282e+01
Epoch 3/10
10/10 - 1s - loss: 191.3256 - loglik: -1.7550e+02 - logprior: -1.5828e+01
Epoch 4/10
10/10 - 1s - loss: 182.3962 - loglik: -1.7600e+02 - logprior: -6.3919e+00
Epoch 5/10
10/10 - 1s - loss: 178.7390 - loglik: -1.7584e+02 - logprior: -2.8974e+00
Epoch 6/10
10/10 - 1s - loss: 177.5545 - loglik: -1.7577e+02 - logprior: -1.7861e+00
Epoch 7/10
10/10 - 1s - loss: 177.3501 - loglik: -1.7615e+02 - logprior: -1.2034e+00
Epoch 8/10
10/10 - 1s - loss: 176.8302 - loglik: -1.7599e+02 - logprior: -8.4328e-01
Epoch 9/10
10/10 - 1s - loss: 176.7168 - loglik: -1.7613e+02 - logprior: -5.8953e-01
Epoch 10/10
10/10 - 1s - loss: 176.4733 - loglik: -1.7608e+02 - logprior: -3.8861e-01
Fitted a model with MAP estimate = -176.4043
Time for alignment: 30.8900
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 297.2617 - loglik: -2.3553e+02 - logprior: -6.1728e+01
Epoch 2/10
10/10 - 1s - loss: 229.8403 - loglik: -2.1263e+02 - logprior: -1.7206e+01
Epoch 3/10
10/10 - 1s - loss: 204.5254 - loglik: -1.9603e+02 - logprior: -8.4917e+00
Epoch 4/10
10/10 - 1s - loss: 193.4438 - loglik: -1.8808e+02 - logprior: -5.3653e+00
Epoch 5/10
10/10 - 1s - loss: 189.3644 - loglik: -1.8557e+02 - logprior: -3.7920e+00
Epoch 6/10
10/10 - 1s - loss: 187.6843 - loglik: -1.8469e+02 - logprior: -2.9959e+00
Epoch 7/10
10/10 - 1s - loss: 187.1125 - loglik: -1.8460e+02 - logprior: -2.5134e+00
Epoch 8/10
10/10 - 1s - loss: 186.7067 - loglik: -1.8457e+02 - logprior: -2.1364e+00
Epoch 9/10
10/10 - 1s - loss: 186.5866 - loglik: -1.8472e+02 - logprior: -1.8684e+00
Epoch 10/10
10/10 - 1s - loss: 186.2092 - loglik: -1.8447e+02 - logprior: -1.7347e+00
Fitted a model with MAP estimate = -186.1413
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 262.4698 - loglik: -1.8265e+02 - logprior: -7.9818e+01
Epoch 2/2
10/10 - 1s - loss: 203.5928 - loglik: -1.7784e+02 - logprior: -2.5758e+01
Fitted a model with MAP estimate = -192.5430
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 241.4184 - loglik: -1.7552e+02 - logprior: -6.5902e+01
Epoch 2/2
10/10 - 1s - loss: 197.8419 - loglik: -1.7474e+02 - logprior: -2.3104e+01
Fitted a model with MAP estimate = -188.9136
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 241.2412 - loglik: -1.7551e+02 - logprior: -6.5733e+01
Epoch 2/10
10/10 - 1s - loss: 203.1275 - loglik: -1.7585e+02 - logprior: -2.7274e+01
Epoch 3/10
10/10 - 1s - loss: 191.9657 - loglik: -1.7598e+02 - logprior: -1.5988e+01
Epoch 4/10
10/10 - 1s - loss: 182.4302 - loglik: -1.7599e+02 - logprior: -6.4414e+00
Epoch 5/10
10/10 - 1s - loss: 179.0867 - loglik: -1.7627e+02 - logprior: -2.8173e+00
Epoch 6/10
10/10 - 1s - loss: 177.9553 - loglik: -1.7627e+02 - logprior: -1.6867e+00
Epoch 7/10
10/10 - 1s - loss: 177.2052 - loglik: -1.7609e+02 - logprior: -1.1133e+00
Epoch 8/10
10/10 - 1s - loss: 176.9885 - loglik: -1.7623e+02 - logprior: -7.5504e-01
Epoch 9/10
10/10 - 1s - loss: 176.7672 - loglik: -1.7626e+02 - logprior: -5.0445e-01
Epoch 10/10
10/10 - 1s - loss: 176.6266 - loglik: -1.7632e+02 - logprior: -3.0646e-01
Fitted a model with MAP estimate = -176.5243
Time for alignment: 29.0064
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 297.2991 - loglik: -2.3557e+02 - logprior: -6.1728e+01
Epoch 2/10
10/10 - 1s - loss: 229.6920 - loglik: -2.1249e+02 - logprior: -1.7207e+01
Epoch 3/10
10/10 - 1s - loss: 204.4933 - loglik: -1.9600e+02 - logprior: -8.4923e+00
Epoch 4/10
10/10 - 1s - loss: 193.6622 - loglik: -1.8830e+02 - logprior: -5.3595e+00
Epoch 5/10
10/10 - 1s - loss: 189.1821 - loglik: -1.8539e+02 - logprior: -3.7922e+00
Epoch 6/10
10/10 - 1s - loss: 187.7709 - loglik: -1.8477e+02 - logprior: -2.9995e+00
Epoch 7/10
10/10 - 1s - loss: 187.1077 - loglik: -1.8460e+02 - logprior: -2.5122e+00
Epoch 8/10
10/10 - 1s - loss: 186.6622 - loglik: -1.8453e+02 - logprior: -2.1328e+00
Epoch 9/10
10/10 - 1s - loss: 186.5912 - loglik: -1.8473e+02 - logprior: -1.8660e+00
Epoch 10/10
10/10 - 1s - loss: 186.2528 - loglik: -1.8452e+02 - logprior: -1.7336e+00
Fitted a model with MAP estimate = -186.1402
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 262.4993 - loglik: -1.8268e+02 - logprior: -7.9818e+01
Epoch 2/2
10/10 - 1s - loss: 203.5763 - loglik: -1.7782e+02 - logprior: -2.5759e+01
Fitted a model with MAP estimate = -192.5421
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 241.4126 - loglik: -1.7551e+02 - logprior: -6.5903e+01
Epoch 2/2
10/10 - 1s - loss: 197.7607 - loglik: -1.7465e+02 - logprior: -2.3108e+01
Fitted a model with MAP estimate = -188.9222
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 241.4388 - loglik: -1.7570e+02 - logprior: -6.5734e+01
Epoch 2/10
10/10 - 1s - loss: 203.0009 - loglik: -1.7572e+02 - logprior: -2.7277e+01
Epoch 3/10
10/10 - 1s - loss: 191.7636 - loglik: -1.7576e+02 - logprior: -1.6001e+01
Epoch 4/10
10/10 - 0s - loss: 182.6875 - loglik: -1.7624e+02 - logprior: -6.4524e+00
Epoch 5/10
10/10 - 1s - loss: 179.0394 - loglik: -1.7622e+02 - logprior: -2.8191e+00
Epoch 6/10
10/10 - 1s - loss: 177.7226 - loglik: -1.7603e+02 - logprior: -1.6880e+00
Epoch 7/10
10/10 - 1s - loss: 177.5645 - loglik: -1.7645e+02 - logprior: -1.1130e+00
Epoch 8/10
10/10 - 1s - loss: 176.7980 - loglik: -1.7604e+02 - logprior: -7.5559e-01
Epoch 9/10
10/10 - 1s - loss: 176.8246 - loglik: -1.7632e+02 - logprior: -5.0628e-01
Fitted a model with MAP estimate = -176.6873
Time for alignment: 28.0216
Computed alignments with likelihoods: ['-176.5148', '-177.0793', '-176.4043', '-176.5243', '-176.6873']
Best model has likelihood: -176.4043  (prior= -0.2959 )
time for generating output: 0.1000
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ChtBD.projection.fasta
SP score = 0.9589371980676329
Training of 5 independent models on file adh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f530050bbe0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f531d24e400>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5940400760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f53143ef370>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b08008e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5314659ee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531d854af0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f530c5a4fa0>, <__main__.SimpleDirichletPrior object at 0x7f5920162100>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 758.6443 - loglik: -7.5545e+02 - logprior: -3.1909e+00
Epoch 2/10
20/20 - 4s - loss: 714.7154 - loglik: -7.1345e+02 - logprior: -1.2676e+00
Epoch 3/10
20/20 - 4s - loss: 695.6724 - loglik: -6.9422e+02 - logprior: -1.4513e+00
Epoch 4/10
20/20 - 4s - loss: 690.1888 - loglik: -6.8879e+02 - logprior: -1.3944e+00
Epoch 5/10
20/20 - 4s - loss: 689.0339 - loglik: -6.8761e+02 - logprior: -1.4212e+00
Epoch 6/10
20/20 - 4s - loss: 687.8761 - loglik: -6.8648e+02 - logprior: -1.3983e+00
Epoch 7/10
20/20 - 4s - loss: 687.4799 - loglik: -6.8605e+02 - logprior: -1.4289e+00
Epoch 8/10
20/20 - 4s - loss: 687.1356 - loglik: -6.8565e+02 - logprior: -1.4865e+00
Epoch 9/10
20/20 - 4s - loss: 686.7004 - loglik: -6.8513e+02 - logprior: -1.5714e+00
Epoch 10/10
20/20 - 4s - loss: 686.7370 - loglik: -6.8508e+02 - logprior: -1.6562e+00
Fitted a model with MAP estimate = -644.4995
expansions: [(5, 1), (8, 2), (10, 1), (13, 2), (18, 1), (20, 1), (23, 2), (35, 1), (36, 2), (38, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 2), (64, 2), (76, 1), (78, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 688.0393 - loglik: -6.8528e+02 - logprior: -2.7559e+00
Epoch 2/2
40/40 - 7s - loss: 677.2852 - loglik: -6.7648e+02 - logprior: -8.0280e-01
Fitted a model with MAP estimate = -631.0731
expansions: [(138, 2)]
discards: [  8  30  46  74  82  87 103 125]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 12s - loss: 679.0285 - loglik: -6.7714e+02 - logprior: -1.8893e+00
Epoch 2/2
40/40 - 7s - loss: 675.7184 - loglik: -6.7504e+02 - logprior: -6.7396e-01
Fitted a model with MAP estimate = -631.1797
expansions: [(132, 2)]
discards: [130 131]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 12s - loss: 627.4935 - loglik: -6.2650e+02 - logprior: -9.9724e-01
Epoch 2/10
57/57 - 9s - loss: 627.0775 - loglik: -6.2648e+02 - logprior: -6.0177e-01
Epoch 3/10
57/57 - 9s - loss: 625.1628 - loglik: -6.2456e+02 - logprior: -6.0381e-01
Epoch 4/10
57/57 - 9s - loss: 623.3103 - loglik: -6.2270e+02 - logprior: -6.0785e-01
Epoch 5/10
57/57 - 9s - loss: 622.7496 - loglik: -6.2210e+02 - logprior: -6.4569e-01
Epoch 6/10
57/57 - 9s - loss: 622.4436 - loglik: -6.2175e+02 - logprior: -6.9770e-01
Epoch 7/10
57/57 - 9s - loss: 620.4073 - loglik: -6.1961e+02 - logprior: -7.9487e-01
Epoch 8/10
57/57 - 9s - loss: 621.2010 - loglik: -6.2035e+02 - logprior: -8.5300e-01
Fitted a model with MAP estimate = -619.9705
Time for alignment: 209.9355
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 9s - loss: 758.7672 - loglik: -7.5557e+02 - logprior: -3.1936e+00
Epoch 2/10
20/20 - 4s - loss: 713.0363 - loglik: -7.1177e+02 - logprior: -1.2687e+00
Epoch 3/10
20/20 - 4s - loss: 694.1855 - loglik: -6.9271e+02 - logprior: -1.4718e+00
Epoch 4/10
20/20 - 4s - loss: 690.3057 - loglik: -6.8892e+02 - logprior: -1.3877e+00
Epoch 5/10
20/20 - 4s - loss: 688.8511 - loglik: -6.8744e+02 - logprior: -1.4076e+00
Epoch 6/10
20/20 - 4s - loss: 688.5687 - loglik: -6.8717e+02 - logprior: -1.4000e+00
Epoch 7/10
20/20 - 4s - loss: 687.6642 - loglik: -6.8623e+02 - logprior: -1.4359e+00
Epoch 8/10
20/20 - 4s - loss: 687.7839 - loglik: -6.8627e+02 - logprior: -1.5163e+00
Fitted a model with MAP estimate = -643.9658
expansions: [(5, 1), (8, 2), (10, 1), (13, 2), (19, 1), (20, 1), (31, 1), (35, 1), (36, 2), (38, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 1), (62, 1), (76, 1), (78, 2), (79, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 687.1336 - loglik: -6.8446e+02 - logprior: -2.6781e+00
Epoch 2/2
40/40 - 7s - loss: 676.9125 - loglik: -6.7614e+02 - logprior: -7.6825e-01
Fitted a model with MAP estimate = -631.2594
expansions: [(137, 2)]
discards: [  8  45  73 101 107 124]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 678.6144 - loglik: -6.7668e+02 - logprior: -1.9344e+00
Epoch 2/2
40/40 - 7s - loss: 675.8949 - loglik: -6.7519e+02 - logprior: -7.0695e-01
Fitted a model with MAP estimate = -631.2073
expansions: [(133, 2)]
discards: [100 131 132]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 13s - loss: 628.3611 - loglik: -6.2736e+02 - logprior: -9.9639e-01
Epoch 2/10
57/57 - 9s - loss: 625.4922 - loglik: -6.2492e+02 - logprior: -5.7207e-01
Epoch 3/10
57/57 - 9s - loss: 625.7429 - loglik: -6.2516e+02 - logprior: -5.8102e-01
Fitted a model with MAP estimate = -623.9786
Time for alignment: 158.4861
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 758.5905 - loglik: -7.5540e+02 - logprior: -3.1941e+00
Epoch 2/10
20/20 - 4s - loss: 714.0013 - loglik: -7.1273e+02 - logprior: -1.2727e+00
Epoch 3/10
20/20 - 4s - loss: 695.0766 - loglik: -6.9360e+02 - logprior: -1.4727e+00
Epoch 4/10
20/20 - 4s - loss: 690.4415 - loglik: -6.8905e+02 - logprior: -1.3933e+00
Epoch 5/10
20/20 - 4s - loss: 689.4068 - loglik: -6.8801e+02 - logprior: -1.3976e+00
Epoch 6/10
20/20 - 4s - loss: 688.1349 - loglik: -6.8675e+02 - logprior: -1.3850e+00
Epoch 7/10
20/20 - 4s - loss: 687.9264 - loglik: -6.8651e+02 - logprior: -1.4150e+00
Epoch 8/10
20/20 - 4s - loss: 687.6507 - loglik: -6.8616e+02 - logprior: -1.4910e+00
Epoch 9/10
20/20 - 4s - loss: 687.3779 - loglik: -6.8581e+02 - logprior: -1.5691e+00
Epoch 10/10
20/20 - 4s - loss: 686.9853 - loglik: -6.8532e+02 - logprior: -1.6684e+00
Fitted a model with MAP estimate = -644.9457
expansions: [(5, 1), (8, 2), (10, 1), (13, 2), (19, 1), (20, 1), (23, 2), (35, 1), (36, 2), (38, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 1), (62, 1), (76, 1), (78, 2), (79, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 11s - loss: 687.8503 - loglik: -6.8511e+02 - logprior: -2.7435e+00
Epoch 2/2
40/40 - 7s - loss: 677.1151 - loglik: -6.7632e+02 - logprior: -7.9703e-01
Fitted a model with MAP estimate = -631.5449
expansions: [(138, 2)]
discards: [  8  30  46  74 101 105 108 125]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 679.0228 - loglik: -6.7713e+02 - logprior: -1.8902e+00
Epoch 2/2
40/40 - 6s - loss: 675.4210 - loglik: -6.7474e+02 - logprior: -6.7637e-01
Fitted a model with MAP estimate = -631.3674
expansions: [(132, 2)]
discards: [130 131]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 12s - loss: 628.6448 - loglik: -6.2765e+02 - logprior: -9.9754e-01
Epoch 2/10
57/57 - 9s - loss: 625.6096 - loglik: -6.2502e+02 - logprior: -5.8900e-01
Epoch 3/10
57/57 - 9s - loss: 625.0471 - loglik: -6.2446e+02 - logprior: -5.8642e-01
Epoch 4/10
57/57 - 9s - loss: 624.1444 - loglik: -6.2355e+02 - logprior: -5.9845e-01
Epoch 5/10
57/57 - 9s - loss: 622.9160 - loglik: -6.2229e+02 - logprior: -6.2741e-01
Epoch 6/10
57/57 - 9s - loss: 622.0201 - loglik: -6.2133e+02 - logprior: -6.8567e-01
Epoch 7/10
57/57 - 9s - loss: 620.9608 - loglik: -6.2018e+02 - logprior: -7.7909e-01
Epoch 8/10
57/57 - 9s - loss: 620.9650 - loglik: -6.2013e+02 - logprior: -8.3775e-01
Fitted a model with MAP estimate = -620.0560
Time for alignment: 207.3804
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 9s - loss: 758.7682 - loglik: -7.5558e+02 - logprior: -3.1909e+00
Epoch 2/10
20/20 - 4s - loss: 714.9261 - loglik: -7.1367e+02 - logprior: -1.2560e+00
Epoch 3/10
20/20 - 4s - loss: 695.3303 - loglik: -6.9387e+02 - logprior: -1.4628e+00
Epoch 4/10
20/20 - 4s - loss: 691.0017 - loglik: -6.8959e+02 - logprior: -1.4083e+00
Epoch 5/10
20/20 - 4s - loss: 689.6223 - loglik: -6.8819e+02 - logprior: -1.4309e+00
Epoch 6/10
20/20 - 4s - loss: 688.9534 - loglik: -6.8754e+02 - logprior: -1.4171e+00
Epoch 7/10
20/20 - 4s - loss: 688.5668 - loglik: -6.8713e+02 - logprior: -1.4413e+00
Epoch 8/10
20/20 - 4s - loss: 687.9720 - loglik: -6.8648e+02 - logprior: -1.4952e+00
Epoch 9/10
20/20 - 4s - loss: 688.0667 - loglik: -6.8647e+02 - logprior: -1.6012e+00
Fitted a model with MAP estimate = -645.0702
expansions: [(5, 1), (8, 2), (10, 1), (13, 2), (19, 1), (20, 1), (23, 2), (37, 2), (38, 2), (40, 1), (47, 1), (51, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 2), (62, 1), (76, 1), (78, 2), (79, 1), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 687.8821 - loglik: -6.8517e+02 - logprior: -2.7145e+00
Epoch 2/2
40/40 - 7s - loss: 676.8482 - loglik: -6.7606e+02 - logprior: -7.9046e-01
Fitted a model with MAP estimate = -631.0777
expansions: [(138, 2)]
discards: [  8  30  47  49  82 108]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 678.7440 - loglik: -6.7682e+02 - logprior: -1.9252e+00
Epoch 2/2
40/40 - 7s - loss: 675.5152 - loglik: -6.7481e+02 - logprior: -7.0625e-01
Fitted a model with MAP estimate = -630.9528
expansions: [(134, 2)]
discards: [ 98 119 132 133]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 13s - loss: 627.7609 - loglik: -6.2676e+02 - logprior: -9.9927e-01
Epoch 2/10
57/57 - 8s - loss: 626.9762 - loglik: -6.2639e+02 - logprior: -5.8771e-01
Epoch 3/10
57/57 - 9s - loss: 624.8710 - loglik: -6.2428e+02 - logprior: -5.9365e-01
Epoch 4/10
57/57 - 9s - loss: 624.4260 - loglik: -6.2383e+02 - logprior: -5.9806e-01
Epoch 5/10
57/57 - 9s - loss: 622.5201 - loglik: -6.2190e+02 - logprior: -6.2469e-01
Epoch 6/10
57/57 - 9s - loss: 621.9464 - loglik: -6.2126e+02 - logprior: -6.8444e-01
Epoch 7/10
57/57 - 9s - loss: 621.1373 - loglik: -6.2037e+02 - logprior: -7.7164e-01
Epoch 8/10
57/57 - 9s - loss: 620.9264 - loglik: -6.2008e+02 - logprior: -8.4387e-01
Epoch 9/10
57/57 - 9s - loss: 621.2652 - loglik: -6.2038e+02 - logprior: -8.8235e-01
Fitted a model with MAP estimate = -619.5316
Time for alignment: 215.8157
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 758.4382 - loglik: -7.5525e+02 - logprior: -3.1930e+00
Epoch 2/10
20/20 - 4s - loss: 714.5579 - loglik: -7.1330e+02 - logprior: -1.2615e+00
Epoch 3/10
20/20 - 4s - loss: 695.3384 - loglik: -6.9387e+02 - logprior: -1.4659e+00
Epoch 4/10
20/20 - 4s - loss: 691.3273 - loglik: -6.8994e+02 - logprior: -1.3833e+00
Epoch 5/10
20/20 - 4s - loss: 689.0146 - loglik: -6.8762e+02 - logprior: -1.3990e+00
Epoch 6/10
20/20 - 4s - loss: 688.4323 - loglik: -6.8706e+02 - logprior: -1.3758e+00
Epoch 7/10
20/20 - 4s - loss: 688.6089 - loglik: -6.8720e+02 - logprior: -1.4060e+00
Fitted a model with MAP estimate = -643.8992
expansions: [(5, 1), (8, 1), (10, 3), (12, 1), (18, 1), (20, 1), (23, 2), (35, 1), (36, 2), (38, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 1), (63, 1), (75, 1), (76, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 687.2897 - loglik: -6.8463e+02 - logprior: -2.6572e+00
Epoch 2/2
40/40 - 7s - loss: 677.4401 - loglik: -6.7668e+02 - logprior: -7.6365e-01
Fitted a model with MAP estimate = -631.4444
expansions: [(136, 2)]
discards: [ 12  30  46  74 106 123]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 11s - loss: 678.8735 - loglik: -6.7697e+02 - logprior: -1.9074e+00
Epoch 2/2
40/40 - 7s - loss: 675.9342 - loglik: -6.7524e+02 - logprior: -6.9813e-01
Fitted a model with MAP estimate = -631.5521
expansions: [(96, 1), (132, 2)]
discards: [130 131]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 12s - loss: 627.7813 - loglik: -6.2678e+02 - logprior: -1.0004e+00
Epoch 2/10
57/57 - 9s - loss: 626.2175 - loglik: -6.2563e+02 - logprior: -5.8834e-01
Epoch 3/10
57/57 - 9s - loss: 625.4641 - loglik: -6.2486e+02 - logprior: -6.0140e-01
Epoch 4/10
57/57 - 9s - loss: 623.1212 - loglik: -6.2252e+02 - logprior: -6.0486e-01
Epoch 5/10
57/57 - 9s - loss: 622.8530 - loglik: -6.2222e+02 - logprior: -6.3448e-01
Epoch 6/10
57/57 - 9s - loss: 621.3679 - loglik: -6.2069e+02 - logprior: -6.7642e-01
Epoch 7/10
57/57 - 9s - loss: 621.9416 - loglik: -6.2116e+02 - logprior: -7.7976e-01
Fitted a model with MAP estimate = -620.6299
Time for alignment: 187.3538
Computed alignments with likelihoods: ['-619.9705', '-623.9786', '-620.0560', '-619.5316', '-620.6299']
Best model has likelihood: -619.5316  (prior= -0.9668 )
time for generating output: 0.3660
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/adh.projection.fasta
SP score = 0.46818791946308724
Training of 5 independent models on file Sulfotransfer.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b0808b250>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5334510b20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f53145ce9d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f53145ce430>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f53145ce0a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5afe84b9d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f59203fa580>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f59203faf40>, <__main__.SimpleDirichletPrior object at 0x7f5300900b80>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 1383.3311 - loglik: -1.3703e+03 - logprior: -1.3002e+01
Epoch 2/10
19/19 - 7s - loss: 1311.4738 - loglik: -1.3106e+03 - logprior: -8.6357e-01
Epoch 3/10
19/19 - 7s - loss: 1284.1573 - loglik: -1.2836e+03 - logprior: -5.4494e-01
Epoch 4/10
19/19 - 7s - loss: 1277.4021 - loglik: -1.2773e+03 - logprior: -1.3036e-01
Epoch 5/10
19/19 - 7s - loss: 1273.4142 - loglik: -1.2733e+03 - logprior: -7.0964e-02
Epoch 6/10
19/19 - 7s - loss: 1271.6467 - loglik: -1.2715e+03 - logprior: -1.1647e-01
Epoch 7/10
19/19 - 7s - loss: 1272.2733 - loglik: -1.2719e+03 - logprior: -3.4146e-01
Fitted a model with MAP estimate = -1271.0321
expansions: [(0, 2), (25, 1), (28, 2), (30, 2), (41, 1), (45, 1), (65, 3), (66, 2), (90, 4), (103, 1), (118, 1), (119, 2), (120, 2), (135, 1), (148, 2), (150, 3), (152, 1), (162, 1), (165, 5), (183, 1), (184, 2), (198, 3)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 1296.8658 - loglik: -1.2829e+03 - logprior: -1.3955e+01
Epoch 2/2
19/19 - 9s - loss: 1273.7264 - loglik: -1.2728e+03 - logprior: -9.4123e-01
Fitted a model with MAP estimate = -1268.2085
expansions: [(176, 2), (223, 1), (224, 1)]
discards: [  0  34  35 138 139 173 183 184 185 186 205 206 237 238 239]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 1288.6843 - loglik: -1.2761e+03 - logprior: -1.2634e+01
Epoch 2/2
19/19 - 8s - loss: 1275.9895 - loglik: -1.2732e+03 - logprior: -2.7433e+00
Fitted a model with MAP estimate = -1268.9581
expansions: [(0, 2), (102, 1), (103, 3), (177, 3)]
discards: [  0 170]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 1278.8745 - loglik: -1.2705e+03 - logprior: -8.3704e+00
Epoch 2/10
19/19 - 9s - loss: 1267.9015 - loglik: -1.2685e+03 - logprior: 0.6322
Epoch 3/10
19/19 - 9s - loss: 1262.1718 - loglik: -1.2637e+03 - logprior: 1.5743
Epoch 4/10
19/19 - 9s - loss: 1259.5353 - loglik: -1.2617e+03 - logprior: 2.1454
Epoch 5/10
19/19 - 9s - loss: 1260.7903 - loglik: -1.2631e+03 - logprior: 2.3098
Fitted a model with MAP estimate = -1258.0507
Time for alignment: 167.7579
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 1383.5881 - loglik: -1.3706e+03 - logprior: -1.2970e+01
Epoch 2/10
19/19 - 7s - loss: 1317.7223 - loglik: -1.3168e+03 - logprior: -9.4581e-01
Epoch 3/10
19/19 - 7s - loss: 1288.3719 - loglik: -1.2877e+03 - logprior: -7.1749e-01
Epoch 4/10
19/19 - 7s - loss: 1276.4413 - loglik: -1.2758e+03 - logprior: -6.6618e-01
Epoch 5/10
19/19 - 7s - loss: 1277.6361 - loglik: -1.2771e+03 - logprior: -5.8472e-01
Fitted a model with MAP estimate = -1275.4157
expansions: [(18, 1), (25, 1), (28, 2), (30, 2), (64, 3), (65, 1), (90, 3), (93, 1), (99, 4), (103, 1), (104, 1), (120, 1), (135, 3), (136, 2), (149, 1), (150, 2), (157, 1), (162, 1), (166, 3), (182, 1), (183, 1), (184, 2)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 1289.9971 - loglik: -1.2801e+03 - logprior: -9.8603e+00
Epoch 2/2
19/19 - 9s - loss: 1269.4138 - loglik: -1.2690e+03 - logprior: -4.4015e-01
Fitted a model with MAP estimate = -1267.3662
expansions: [(221, 1), (222, 1)]
discards: [ 33  34 114 159 198 203 204]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 1280.3417 - loglik: -1.2718e+03 - logprior: -8.5297e+00
Epoch 2/2
19/19 - 8s - loss: 1268.8773 - loglik: -1.2692e+03 - logprior: 0.3041
Fitted a model with MAP estimate = -1266.2565
expansions: [(70, 1), (197, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 232 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 1278.9406 - loglik: -1.2707e+03 - logprior: -8.2259e+00
Epoch 2/10
19/19 - 9s - loss: 1266.4164 - loglik: -1.2671e+03 - logprior: 0.6749
Epoch 3/10
19/19 - 8s - loss: 1262.7911 - loglik: -1.2645e+03 - logprior: 1.6734
Epoch 4/10
19/19 - 8s - loss: 1263.0853 - loglik: -1.2652e+03 - logprior: 2.1534
Fitted a model with MAP estimate = -1260.4473
Time for alignment: 145.4908
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 1381.4379 - loglik: -1.3684e+03 - logprior: -1.3005e+01
Epoch 2/10
19/19 - 7s - loss: 1315.8480 - loglik: -1.3149e+03 - logprior: -9.4654e-01
Epoch 3/10
19/19 - 7s - loss: 1283.0583 - loglik: -1.2821e+03 - logprior: -9.4341e-01
Epoch 4/10
19/19 - 7s - loss: 1276.6078 - loglik: -1.2760e+03 - logprior: -6.4563e-01
Epoch 5/10
19/19 - 7s - loss: 1273.6727 - loglik: -1.2731e+03 - logprior: -5.6537e-01
Epoch 6/10
19/19 - 7s - loss: 1274.2349 - loglik: -1.2736e+03 - logprior: -6.1465e-01
Fitted a model with MAP estimate = -1271.6928
expansions: [(25, 1), (27, 1), (29, 3), (43, 4), (44, 1), (64, 1), (88, 1), (90, 3), (99, 4), (103, 1), (117, 2), (118, 1), (134, 1), (135, 1), (146, 3), (148, 2), (150, 1), (162, 1), (163, 1), (166, 5), (182, 1), (183, 1), (184, 2), (198, 3)]
discards: [170 171 172 173 174 175 176 177 178]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 1293.8296 - loglik: -1.2837e+03 - logprior: -1.0115e+01
Epoch 2/2
19/19 - 9s - loss: 1275.1595 - loglik: -1.2744e+03 - logprior: -7.9234e-01
Fitted a model with MAP estimate = -1269.1529
expansions: [(218, 1)]
discards: [116 173 176 193 199 200 201 202 203 204 231 232 233]
Re-initialized the encoder parameters.
Fitting a model of length 222 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 1284.7178 - loglik: -1.2760e+03 - logprior: -8.7294e+00
Epoch 2/2
19/19 - 8s - loss: 1270.8872 - loglik: -1.2709e+03 - logprior: -2.4300e-02
Fitted a model with MAP estimate = -1269.8772
expansions: [(141, 1), (193, 10), (204, 1)]
discards: [137]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 1281.1949 - loglik: -1.2728e+03 - logprior: -8.4343e+00
Epoch 2/10
19/19 - 8s - loss: 1268.0349 - loglik: -1.2685e+03 - logprior: 0.4641
Epoch 3/10
19/19 - 9s - loss: 1263.8490 - loglik: -1.2654e+03 - logprior: 1.5338
Epoch 4/10
19/19 - 9s - loss: 1260.8320 - loglik: -1.2628e+03 - logprior: 2.0086
Epoch 5/10
19/19 - 9s - loss: 1258.6787 - loglik: -1.2609e+03 - logprior: 2.2368
Epoch 6/10
19/19 - 8s - loss: 1263.6449 - loglik: -1.2659e+03 - logprior: 2.2658
Fitted a model with MAP estimate = -1257.8545
Time for alignment: 166.6390
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 1382.2946 - loglik: -1.3693e+03 - logprior: -1.2987e+01
Epoch 2/10
19/19 - 7s - loss: 1315.1082 - loglik: -1.3142e+03 - logprior: -8.6077e-01
Epoch 3/10
19/19 - 7s - loss: 1288.7152 - loglik: -1.2882e+03 - logprior: -5.5131e-01
Epoch 4/10
19/19 - 7s - loss: 1278.6968 - loglik: -1.2785e+03 - logprior: -1.6767e-01
Epoch 5/10
19/19 - 7s - loss: 1277.3853 - loglik: -1.2773e+03 - logprior: -1.3427e-01
Epoch 6/10
19/19 - 7s - loss: 1281.0641 - loglik: -1.2809e+03 - logprior: -1.7720e-01
Fitted a model with MAP estimate = -1276.1650
expansions: [(25, 1), (28, 2), (43, 3), (44, 3), (64, 4), (89, 3), (102, 1), (117, 1), (119, 1), (135, 1), (147, 3), (149, 3), (150, 1), (163, 2), (165, 6), (183, 1), (184, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 1295.2866 - loglik: -1.2855e+03 - logprior: -9.8123e+00
Epoch 2/2
19/19 - 9s - loss: 1275.4595 - loglik: -1.2751e+03 - logprior: -3.6858e-01
Fitted a model with MAP estimate = -1272.9370
expansions: [(50, 1), (222, 1)]
discards: [167 168 181 191 192 193 203 204]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 1287.4103 - loglik: -1.2787e+03 - logprior: -8.6725e+00
Epoch 2/2
19/19 - 8s - loss: 1273.9126 - loglik: -1.2739e+03 - logprior: -5.5523e-05
Fitted a model with MAP estimate = -1272.2430
expansions: [(0, 2), (211, 1)]
discards: [47 74]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 1289.4738 - loglik: -1.2764e+03 - logprior: -1.3059e+01
Epoch 2/10
19/19 - 8s - loss: 1273.7682 - loglik: -1.2736e+03 - logprior: -2.1191e-01
Epoch 3/10
19/19 - 8s - loss: 1272.0270 - loglik: -1.2735e+03 - logprior: 1.4819
Epoch 4/10
19/19 - 8s - loss: 1266.4915 - loglik: -1.2685e+03 - logprior: 2.0566
Epoch 5/10
19/19 - 8s - loss: 1265.2058 - loglik: -1.2674e+03 - logprior: 2.2226
Epoch 6/10
19/19 - 8s - loss: 1264.8317 - loglik: -1.2671e+03 - logprior: 2.2246
Epoch 7/10
19/19 - 8s - loss: 1263.1715 - loglik: -1.2655e+03 - logprior: 2.3351
Epoch 8/10
19/19 - 8s - loss: 1262.9395 - loglik: -1.2653e+03 - logprior: 2.3484
Epoch 9/10
19/19 - 8s - loss: 1259.5317 - loglik: -1.2618e+03 - logprior: 2.2416
Epoch 10/10
19/19 - 8s - loss: 1264.3505 - loglik: -1.2665e+03 - logprior: 2.1384
Fitted a model with MAP estimate = -1260.3431
Time for alignment: 202.0749
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 1383.2321 - loglik: -1.3702e+03 - logprior: -1.2992e+01
Epoch 2/10
19/19 - 7s - loss: 1315.0049 - loglik: -1.3142e+03 - logprior: -8.4965e-01
Epoch 3/10
19/19 - 7s - loss: 1283.9761 - loglik: -1.2833e+03 - logprior: -7.2500e-01
Epoch 4/10
19/19 - 7s - loss: 1275.5193 - loglik: -1.2751e+03 - logprior: -4.6403e-01
Epoch 5/10
19/19 - 7s - loss: 1274.3024 - loglik: -1.2739e+03 - logprior: -4.0718e-01
Epoch 6/10
19/19 - 7s - loss: 1272.0868 - loglik: -1.2716e+03 - logprior: -5.0025e-01
Epoch 7/10
19/19 - 7s - loss: 1271.6964 - loglik: -1.2709e+03 - logprior: -7.6361e-01
Epoch 8/10
19/19 - 7s - loss: 1269.5299 - loglik: -1.2685e+03 - logprior: -9.9449e-01
Epoch 9/10
19/19 - 7s - loss: 1268.5187 - loglik: -1.2672e+03 - logprior: -1.3012e+00
Epoch 10/10
19/19 - 7s - loss: 1269.8396 - loglik: -1.2683e+03 - logprior: -1.5756e+00
Fitted a model with MAP estimate = -1268.6312
expansions: [(18, 1), (25, 1), (29, 2), (44, 3), (45, 3), (65, 2), (66, 3), (88, 1), (90, 6), (103, 1), (104, 1), (117, 1), (119, 1), (136, 1), (148, 2), (150, 2), (151, 1), (153, 1), (163, 1), (165, 6), (183, 1), (184, 2)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 1296.4750 - loglik: -1.2861e+03 - logprior: -1.0383e+01
Epoch 2/2
19/19 - 9s - loss: 1272.6808 - loglik: -1.2725e+03 - logprior: -2.2896e-01
Fitted a model with MAP estimate = -1267.8229
expansions: [(49, 1), (51, 1), (52, 2), (107, 1)]
discards: [175 182 187 188 189 196 201 202 208 209 210 211 212 213 214 215]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 1288.7812 - loglik: -1.2802e+03 - logprior: -8.5392e+00
Epoch 2/2
19/19 - 8s - loss: 1274.3673 - loglik: -1.2747e+03 - logprior: 0.2975
Fitted a model with MAP estimate = -1271.2295
expansions: [(186, 1), (190, 2), (196, 1), (204, 1), (216, 1)]
discards: [44 79]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 1281.8868 - loglik: -1.2737e+03 - logprior: -8.1501e+00
Epoch 2/10
19/19 - 9s - loss: 1269.0331 - loglik: -1.2697e+03 - logprior: 0.6608
Epoch 3/10
19/19 - 9s - loss: 1267.5901 - loglik: -1.2693e+03 - logprior: 1.7011
Epoch 4/10
19/19 - 8s - loss: 1261.5869 - loglik: -1.2638e+03 - logprior: 2.2265
Epoch 5/10
19/19 - 9s - loss: 1263.0212 - loglik: -1.2655e+03 - logprior: 2.4370
Fitted a model with MAP estimate = -1260.3038
Time for alignment: 187.4615
Computed alignments with likelihoods: ['-1258.0507', '-1260.4473', '-1257.8545', '-1260.3431', '-1260.3038']
Best model has likelihood: -1257.8545  (prior= 2.2809 )
time for generating output: 0.3417
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Sulfotransfer.projection.fasta
SP score = 0.7427974947807933
Training of 5 independent models on file hom.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5cdaba0310>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f530c5e2d60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530c924fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5afe6fa700>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5cc08f0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530c97f5b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f59401c3880>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5335148ee0>, <__main__.SimpleDirichletPrior object at 0x7f533515fb80>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 320.3865 - loglik: -3.1696e+02 - logprior: -3.4309e+00
Epoch 2/10
19/19 - 1s - loss: 286.8666 - loglik: -2.8530e+02 - logprior: -1.5619e+00
Epoch 3/10
19/19 - 1s - loss: 274.4165 - loglik: -2.7281e+02 - logprior: -1.6072e+00
Epoch 4/10
19/19 - 1s - loss: 271.0197 - loglik: -2.6943e+02 - logprior: -1.5890e+00
Epoch 5/10
19/19 - 1s - loss: 269.7520 - loglik: -2.6818e+02 - logprior: -1.5740e+00
Epoch 6/10
19/19 - 1s - loss: 269.5466 - loglik: -2.6799e+02 - logprior: -1.5613e+00
Epoch 7/10
19/19 - 1s - loss: 269.0090 - loglik: -2.6743e+02 - logprior: -1.5778e+00
Epoch 8/10
19/19 - 1s - loss: 269.2588 - loglik: -2.6767e+02 - logprior: -1.5872e+00
Fitted a model with MAP estimate = -254.3755
expansions: [(5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 269.9662 - loglik: -2.6656e+02 - logprior: -3.4067e+00
Epoch 2/2
19/19 - 1s - loss: 261.5912 - loglik: -2.6016e+02 - logprior: -1.4338e+00
Fitted a model with MAP estimate = -246.3331
expansions: []
discards: [12 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 263.8088 - loglik: -2.6063e+02 - logprior: -3.1803e+00
Epoch 2/2
19/19 - 1s - loss: 260.7943 - loglik: -2.5950e+02 - logprior: -1.2912e+00
Fitted a model with MAP estimate = -246.1571
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 247.9942 - loglik: -2.4541e+02 - logprior: -2.5843e+00
Epoch 2/10
21/21 - 2s - loss: 245.2971 - loglik: -2.4402e+02 - logprior: -1.2811e+00
Epoch 3/10
21/21 - 2s - loss: 245.0376 - loglik: -2.4384e+02 - logprior: -1.1957e+00
Epoch 4/10
21/21 - 2s - loss: 244.5950 - loglik: -2.4344e+02 - logprior: -1.1580e+00
Epoch 5/10
21/21 - 2s - loss: 243.9716 - loglik: -2.4279e+02 - logprior: -1.1781e+00
Epoch 6/10
21/21 - 1s - loss: 243.7470 - loglik: -2.4256e+02 - logprior: -1.1864e+00
Epoch 7/10
21/21 - 2s - loss: 243.5818 - loglik: -2.4238e+02 - logprior: -1.2020e+00
Epoch 8/10
21/21 - 2s - loss: 242.7573 - loglik: -2.4154e+02 - logprior: -1.2211e+00
Epoch 9/10
21/21 - 2s - loss: 243.3630 - loglik: -2.4212e+02 - logprior: -1.2423e+00
Fitted a model with MAP estimate = -243.0946
Time for alignment: 56.2939
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 320.6019 - loglik: -3.1717e+02 - logprior: -3.4315e+00
Epoch 2/10
19/19 - 1s - loss: 287.8233 - loglik: -2.8624e+02 - logprior: -1.5798e+00
Epoch 3/10
19/19 - 1s - loss: 274.0232 - loglik: -2.7238e+02 - logprior: -1.6465e+00
Epoch 4/10
19/19 - 1s - loss: 270.3018 - loglik: -2.6865e+02 - logprior: -1.6509e+00
Epoch 5/10
19/19 - 1s - loss: 269.1206 - loglik: -2.6750e+02 - logprior: -1.6249e+00
Epoch 6/10
19/19 - 1s - loss: 268.8171 - loglik: -2.6721e+02 - logprior: -1.6077e+00
Epoch 7/10
19/19 - 1s - loss: 268.4746 - loglik: -2.6686e+02 - logprior: -1.6173e+00
Epoch 8/10
19/19 - 1s - loss: 268.3044 - loglik: -2.6668e+02 - logprior: -1.6282e+00
Epoch 9/10
19/19 - 1s - loss: 268.2978 - loglik: -2.6666e+02 - logprior: -1.6375e+00
Epoch 10/10
19/19 - 1s - loss: 268.1957 - loglik: -2.6655e+02 - logprior: -1.6448e+00
Fitted a model with MAP estimate = -253.6864
expansions: [(5, 1), (6, 1), (7, 1), (9, 1), (16, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 2), (31, 2), (32, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 62 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 271.3745 - loglik: -2.6794e+02 - logprior: -3.4314e+00
Epoch 2/2
19/19 - 1s - loss: 262.0189 - loglik: -2.6055e+02 - logprior: -1.4643e+00
Fitted a model with MAP estimate = -246.6446
expansions: []
discards: [20 38 42 44 47]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 263.9787 - loglik: -2.6080e+02 - logprior: -3.1775e+00
Epoch 2/2
19/19 - 1s - loss: 260.8822 - loglik: -2.5960e+02 - logprior: -1.2833e+00
Fitted a model with MAP estimate = -246.2312
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 247.9709 - loglik: -2.4539e+02 - logprior: -2.5818e+00
Epoch 2/10
21/21 - 1s - loss: 245.4385 - loglik: -2.4417e+02 - logprior: -1.2688e+00
Epoch 3/10
21/21 - 1s - loss: 244.9661 - loglik: -2.4379e+02 - logprior: -1.1740e+00
Epoch 4/10
21/21 - 1s - loss: 244.2069 - loglik: -2.4307e+02 - logprior: -1.1331e+00
Epoch 5/10
21/21 - 1s - loss: 243.4920 - loglik: -2.4235e+02 - logprior: -1.1388e+00
Epoch 6/10
21/21 - 1s - loss: 244.2385 - loglik: -2.4308e+02 - logprior: -1.1539e+00
Fitted a model with MAP estimate = -243.5021
Time for alignment: 55.0703
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 320.4687 - loglik: -3.1704e+02 - logprior: -3.4333e+00
Epoch 2/10
19/19 - 1s - loss: 286.9578 - loglik: -2.8539e+02 - logprior: -1.5648e+00
Epoch 3/10
19/19 - 1s - loss: 273.9083 - loglik: -2.7229e+02 - logprior: -1.6189e+00
Epoch 4/10
19/19 - 1s - loss: 271.1412 - loglik: -2.6956e+02 - logprior: -1.5763e+00
Epoch 5/10
19/19 - 1s - loss: 270.2988 - loglik: -2.6875e+02 - logprior: -1.5490e+00
Epoch 6/10
19/19 - 1s - loss: 269.7346 - loglik: -2.6820e+02 - logprior: -1.5392e+00
Epoch 7/10
19/19 - 1s - loss: 269.1771 - loglik: -2.6760e+02 - logprior: -1.5784e+00
Epoch 8/10
19/19 - 1s - loss: 268.9760 - loglik: -2.6739e+02 - logprior: -1.5883e+00
Epoch 9/10
19/19 - 1s - loss: 269.0509 - loglik: -2.6744e+02 - logprior: -1.6129e+00
Fitted a model with MAP estimate = -254.3084
expansions: [(5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 270.0533 - loglik: -2.6664e+02 - logprior: -3.4175e+00
Epoch 2/2
19/19 - 1s - loss: 261.5899 - loglik: -2.6016e+02 - logprior: -1.4317e+00
Fitted a model with MAP estimate = -246.3666
expansions: []
discards: [12 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 263.8639 - loglik: -2.6069e+02 - logprior: -3.1736e+00
Epoch 2/2
19/19 - 1s - loss: 260.6937 - loglik: -2.5941e+02 - logprior: -1.2844e+00
Fitted a model with MAP estimate = -246.1383
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 248.0235 - loglik: -2.4544e+02 - logprior: -2.5788e+00
Epoch 2/10
21/21 - 2s - loss: 245.4415 - loglik: -2.4418e+02 - logprior: -1.2647e+00
Epoch 3/10
21/21 - 2s - loss: 244.5639 - loglik: -2.4338e+02 - logprior: -1.1794e+00
Epoch 4/10
21/21 - 2s - loss: 244.7274 - loglik: -2.4359e+02 - logprior: -1.1350e+00
Fitted a model with MAP estimate = -244.0537
Time for alignment: 48.3021
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 320.3691 - loglik: -3.1693e+02 - logprior: -3.4353e+00
Epoch 2/10
19/19 - 1s - loss: 285.5099 - loglik: -2.8394e+02 - logprior: -1.5682e+00
Epoch 3/10
19/19 - 1s - loss: 273.4885 - loglik: -2.7187e+02 - logprior: -1.6184e+00
Epoch 4/10
19/19 - 1s - loss: 270.3622 - loglik: -2.6876e+02 - logprior: -1.6019e+00
Epoch 5/10
19/19 - 1s - loss: 269.8884 - loglik: -2.6832e+02 - logprior: -1.5716e+00
Epoch 6/10
19/19 - 1s - loss: 269.6877 - loglik: -2.6813e+02 - logprior: -1.5576e+00
Epoch 7/10
19/19 - 1s - loss: 268.8482 - loglik: -2.6728e+02 - logprior: -1.5732e+00
Epoch 8/10
19/19 - 1s - loss: 269.0676 - loglik: -2.6748e+02 - logprior: -1.5908e+00
Fitted a model with MAP estimate = -254.3704
expansions: [(5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 269.9209 - loglik: -2.6652e+02 - logprior: -3.3984e+00
Epoch 2/2
19/19 - 1s - loss: 261.6713 - loglik: -2.6024e+02 - logprior: -1.4319e+00
Fitted a model with MAP estimate = -246.4284
expansions: []
discards: [12 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 263.7840 - loglik: -2.6061e+02 - logprior: -3.1759e+00
Epoch 2/2
19/19 - 1s - loss: 260.7278 - loglik: -2.5944e+02 - logprior: -1.2841e+00
Fitted a model with MAP estimate = -246.1514
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 248.0438 - loglik: -2.4546e+02 - logprior: -2.5828e+00
Epoch 2/10
21/21 - 2s - loss: 245.3760 - loglik: -2.4411e+02 - logprior: -1.2670e+00
Epoch 3/10
21/21 - 1s - loss: 244.6951 - loglik: -2.4353e+02 - logprior: -1.1675e+00
Epoch 4/10
21/21 - 2s - loss: 244.7415 - loglik: -2.4361e+02 - logprior: -1.1329e+00
Fitted a model with MAP estimate = -244.0999
Time for alignment: 47.1803
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 320.4421 - loglik: -3.1701e+02 - logprior: -3.4352e+00
Epoch 2/10
19/19 - 1s - loss: 288.0279 - loglik: -2.8644e+02 - logprior: -1.5856e+00
Epoch 3/10
19/19 - 1s - loss: 273.3286 - loglik: -2.7170e+02 - logprior: -1.6316e+00
Epoch 4/10
19/19 - 1s - loss: 269.3466 - loglik: -2.6768e+02 - logprior: -1.6648e+00
Epoch 5/10
19/19 - 1s - loss: 268.5230 - loglik: -2.6689e+02 - logprior: -1.6339e+00
Epoch 6/10
19/19 - 1s - loss: 267.9950 - loglik: -2.6638e+02 - logprior: -1.6165e+00
Epoch 7/10
19/19 - 1s - loss: 267.7542 - loglik: -2.6613e+02 - logprior: -1.6205e+00
Epoch 8/10
19/19 - 1s - loss: 267.6161 - loglik: -2.6598e+02 - logprior: -1.6391e+00
Epoch 9/10
19/19 - 1s - loss: 267.6467 - loglik: -2.6600e+02 - logprior: -1.6445e+00
Fitted a model with MAP estimate = -253.0676
expansions: [(5, 1), (6, 1), (7, 1), (9, 1), (16, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 2), (31, 2), (33, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 270.3944 - loglik: -2.6697e+02 - logprior: -3.4231e+00
Epoch 2/2
19/19 - 1s - loss: 261.4948 - loglik: -2.6005e+02 - logprior: -1.4408e+00
Fitted a model with MAP estimate = -246.3230
expansions: []
discards: [20 38 42 44]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 263.8665 - loglik: -2.6069e+02 - logprior: -3.1752e+00
Epoch 2/2
19/19 - 1s - loss: 260.7717 - loglik: -2.5949e+02 - logprior: -1.2793e+00
Fitted a model with MAP estimate = -246.2048
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 247.8911 - loglik: -2.4530e+02 - logprior: -2.5931e+00
Epoch 2/10
21/21 - 2s - loss: 245.6677 - loglik: -2.4440e+02 - logprior: -1.2700e+00
Epoch 3/10
21/21 - 2s - loss: 244.8501 - loglik: -2.4367e+02 - logprior: -1.1777e+00
Epoch 4/10
21/21 - 1s - loss: 243.5047 - loglik: -2.4236e+02 - logprior: -1.1409e+00
Epoch 5/10
21/21 - 1s - loss: 244.3062 - loglik: -2.4317e+02 - logprior: -1.1385e+00
Fitted a model with MAP estimate = -243.7481
Time for alignment: 50.0682
Computed alignments with likelihoods: ['-243.0946', '-243.5021', '-244.0537', '-244.0999', '-243.7481']
Best model has likelihood: -243.0946  (prior= -1.2630 )
time for generating output: 0.1517
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hom.projection.fasta
SP score = 0.9452672247263362
Training of 5 independent models on file OTCace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52f8b89880>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52e8487a60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f116bdc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f0805b80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531dd461f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b0a1cf640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5aff5c85b0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52f02a6af0>, <__main__.SimpleDirichletPrior object at 0x7f5c842fd820>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 873.0891 - loglik: -8.6758e+02 - logprior: -5.5093e+00
Epoch 2/10
27/27 - 5s - loss: 790.1411 - loglik: -7.8809e+02 - logprior: -2.0482e+00
Epoch 3/10
27/27 - 5s - loss: 777.0734 - loglik: -7.7508e+02 - logprior: -1.9946e+00
Epoch 4/10
27/27 - 5s - loss: 774.8660 - loglik: -7.7285e+02 - logprior: -2.0198e+00
Epoch 5/10
27/27 - 5s - loss: 771.1824 - loglik: -7.6909e+02 - logprior: -2.0889e+00
Epoch 6/10
27/27 - 5s - loss: 770.1588 - loglik: -7.6798e+02 - logprior: -2.1802e+00
Epoch 7/10
27/27 - 5s - loss: 770.8070 - loglik: -7.6862e+02 - logprior: -2.1845e+00
Fitted a model with MAP estimate = -770.5815
expansions: [(0, 2), (9, 1), (10, 1), (21, 1), (24, 2), (26, 1), (35, 2), (36, 1), (37, 2), (38, 1), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (69, 1), (72, 1), (73, 1), (74, 1), (77, 1), (78, 1), (81, 1), (82, 2), (99, 1), (100, 1), (102, 1), (103, 2), (104, 2), (114, 1), (115, 1), (116, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 160 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 765.3976 - loglik: -7.5863e+02 - logprior: -6.7646e+00
Epoch 2/2
27/27 - 6s - loss: 748.2268 - loglik: -7.4685e+02 - logprior: -1.3813e+00
Fitted a model with MAP estimate = -745.6734
expansions: []
discards: [  0  29  48  49 108 135 137 153]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 10s - loss: 756.8743 - loglik: -7.5051e+02 - logprior: -6.3596e+00
Epoch 2/2
27/27 - 6s - loss: 750.0065 - loglik: -7.4870e+02 - logprior: -1.3044e+00
Fitted a model with MAP estimate = -748.1532
expansions: [(146, 1)]
discards: [11]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 753.5286 - loglik: -7.4932e+02 - logprior: -4.2116e+00
Epoch 2/10
27/27 - 6s - loss: 747.5320 - loglik: -7.4673e+02 - logprior: -8.0599e-01
Epoch 3/10
27/27 - 6s - loss: 746.9953 - loglik: -7.4647e+02 - logprior: -5.2448e-01
Epoch 4/10
27/27 - 6s - loss: 746.1832 - loglik: -7.4575e+02 - logprior: -4.3536e-01
Epoch 5/10
27/27 - 5s - loss: 745.1575 - loglik: -7.4482e+02 - logprior: -3.4129e-01
Epoch 6/10
27/27 - 6s - loss: 745.5238 - loglik: -7.4523e+02 - logprior: -2.9086e-01
Fitted a model with MAP estimate = -745.1523
Time for alignment: 134.7938
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 10s - loss: 873.1928 - loglik: -8.6770e+02 - logprior: -5.4956e+00
Epoch 2/10
27/27 - 5s - loss: 788.0695 - loglik: -7.8609e+02 - logprior: -1.9814e+00
Epoch 3/10
27/27 - 5s - loss: 771.7729 - loglik: -7.6969e+02 - logprior: -2.0826e+00
Epoch 4/10
27/27 - 5s - loss: 766.8070 - loglik: -7.6467e+02 - logprior: -2.1339e+00
Epoch 5/10
27/27 - 5s - loss: 766.4859 - loglik: -7.6429e+02 - logprior: -2.1913e+00
Epoch 6/10
27/27 - 5s - loss: 766.7388 - loglik: -7.6452e+02 - logprior: -2.2195e+00
Fitted a model with MAP estimate = -766.0443
expansions: [(0, 2), (19, 1), (21, 1), (24, 2), (26, 1), (36, 2), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 1), (67, 2), (68, 1), (69, 1), (71, 1), (72, 1), (73, 1), (78, 1), (82, 2), (101, 1), (102, 1), (103, 1), (104, 2), (105, 2), (106, 1), (108, 1), (114, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 162 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 761.8054 - loglik: -7.5513e+02 - logprior: -6.6733e+00
Epoch 2/2
27/27 - 6s - loss: 743.4399 - loglik: -7.4205e+02 - logprior: -1.3857e+00
Fitted a model with MAP estimate = -741.8294
expansions: [(42, 1)]
discards: [  0  28  47  87 110 137 139]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 752.8637 - loglik: -7.4660e+02 - logprior: -6.2634e+00
Epoch 2/2
27/27 - 6s - loss: 744.4358 - loglik: -7.4327e+02 - logprior: -1.1699e+00
Fitted a model with MAP estimate = -742.9229
expansions: []
discards: [44 45]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 10s - loss: 748.5995 - loglik: -7.4447e+02 - logprior: -4.1260e+00
Epoch 2/10
27/27 - 6s - loss: 746.0428 - loglik: -7.4534e+02 - logprior: -7.0150e-01
Epoch 3/10
27/27 - 6s - loss: 743.1328 - loglik: -7.4270e+02 - logprior: -4.2817e-01
Epoch 4/10
27/27 - 6s - loss: 743.0269 - loglik: -7.4268e+02 - logprior: -3.5112e-01
Epoch 5/10
27/27 - 6s - loss: 741.8696 - loglik: -7.4161e+02 - logprior: -2.5777e-01
Epoch 6/10
27/27 - 6s - loss: 741.9641 - loglik: -7.4177e+02 - logprior: -1.9636e-01
Fitted a model with MAP estimate = -741.7949
Time for alignment: 130.3451
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 8s - loss: 871.9617 - loglik: -8.6645e+02 - logprior: -5.5142e+00
Epoch 2/10
27/27 - 5s - loss: 787.2436 - loglik: -7.8532e+02 - logprior: -1.9249e+00
Epoch 3/10
27/27 - 5s - loss: 771.8271 - loglik: -7.6991e+02 - logprior: -1.9189e+00
Epoch 4/10
27/27 - 5s - loss: 770.3321 - loglik: -7.6837e+02 - logprior: -1.9645e+00
Epoch 5/10
27/27 - 5s - loss: 768.1006 - loglik: -7.6612e+02 - logprior: -1.9821e+00
Epoch 6/10
27/27 - 5s - loss: 767.1988 - loglik: -7.6519e+02 - logprior: -2.0074e+00
Epoch 7/10
27/27 - 5s - loss: 768.2692 - loglik: -7.6623e+02 - logprior: -2.0424e+00
Fitted a model with MAP estimate = -767.2786
expansions: [(0, 2), (9, 2), (18, 1), (25, 2), (26, 1), (36, 2), (37, 3), (38, 1), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 2), (67, 2), (68, 1), (69, 1), (71, 1), (72, 1), (73, 1), (77, 1), (78, 1), (81, 2), (93, 2), (102, 1), (103, 2), (104, 2), (105, 1), (114, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 164 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 761.1853 - loglik: -7.5444e+02 - logprior: -6.7484e+00
Epoch 2/2
27/27 - 6s - loss: 743.1686 - loglik: -7.4172e+02 - logprior: -1.4535e+00
Fitted a model with MAP estimate = -741.0660
expansions: [(43, 1)]
discards: [  0  12  30  47  48  67  89 112 139 141]
Re-initialized the encoder parameters.
Fitting a model of length 155 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 11s - loss: 754.1661 - loglik: -7.4788e+02 - logprior: -6.2860e+00
Epoch 2/2
27/27 - 6s - loss: 743.9707 - loglik: -7.4272e+02 - logprior: -1.2541e+00
Fitted a model with MAP estimate = -743.2902
expansions: []
discards: [44]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 748.6772 - loglik: -7.4454e+02 - logprior: -4.1414e+00
Epoch 2/10
27/27 - 6s - loss: 743.9473 - loglik: -7.4322e+02 - logprior: -7.2418e-01
Epoch 3/10
27/27 - 6s - loss: 743.3989 - loglik: -7.4296e+02 - logprior: -4.3726e-01
Epoch 4/10
27/27 - 6s - loss: 741.9340 - loglik: -7.4159e+02 - logprior: -3.4880e-01
Epoch 5/10
27/27 - 6s - loss: 742.0457 - loglik: -7.4180e+02 - logprior: -2.4141e-01
Fitted a model with MAP estimate = -741.5870
Time for alignment: 128.0770
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 8s - loss: 872.2097 - loglik: -8.6668e+02 - logprior: -5.5256e+00
Epoch 2/10
27/27 - 5s - loss: 788.8214 - loglik: -7.8680e+02 - logprior: -2.0171e+00
Epoch 3/10
27/27 - 5s - loss: 775.5743 - loglik: -7.7369e+02 - logprior: -1.8871e+00
Epoch 4/10
27/27 - 5s - loss: 769.8434 - loglik: -7.6797e+02 - logprior: -1.8781e+00
Epoch 5/10
27/27 - 5s - loss: 769.7963 - loglik: -7.6785e+02 - logprior: -1.9465e+00
Epoch 6/10
27/27 - 5s - loss: 767.3317 - loglik: -7.6534e+02 - logprior: -1.9926e+00
Epoch 7/10
27/27 - 5s - loss: 769.8519 - loglik: -7.6782e+02 - logprior: -2.0330e+00
Fitted a model with MAP estimate = -768.2105
expansions: [(0, 2), (9, 2), (18, 1), (21, 1), (24, 2), (25, 1), (36, 1), (37, 4), (38, 1), (39, 2), (43, 1), (45, 1), (68, 1), (69, 2), (71, 1), (72, 1), (73, 1), (78, 1), (82, 2), (101, 1), (103, 1), (104, 1), (105, 2), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 159 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 11s - loss: 764.5251 - loglik: -7.5782e+02 - logprior: -6.7097e+00
Epoch 2/2
27/27 - 6s - loss: 745.4473 - loglik: -7.4400e+02 - logprior: -1.4436e+00
Fitted a model with MAP estimate = -744.1468
expansions: [(43, 1), (130, 1)]
discards: [  0  12  30  48  54 109 136]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 754.1243 - loglik: -7.4781e+02 - logprior: -6.3184e+00
Epoch 2/2
27/27 - 6s - loss: 745.9809 - loglik: -7.4471e+02 - logprior: -1.2736e+00
Fitted a model with MAP estimate = -744.2329
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 749.3458 - loglik: -7.4512e+02 - logprior: -4.2285e+00
Epoch 2/10
27/27 - 6s - loss: 744.0051 - loglik: -7.4317e+02 - logprior: -8.3523e-01
Epoch 3/10
27/27 - 6s - loss: 742.8925 - loglik: -7.4235e+02 - logprior: -5.4630e-01
Epoch 4/10
27/27 - 6s - loss: 743.1270 - loglik: -7.4265e+02 - logprior: -4.7302e-01
Fitted a model with MAP estimate = -741.9302
Time for alignment: 121.8431
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 10s - loss: 874.6667 - loglik: -8.6915e+02 - logprior: -5.5136e+00
Epoch 2/10
27/27 - 5s - loss: 788.3553 - loglik: -7.8639e+02 - logprior: -1.9631e+00
Epoch 3/10
27/27 - 5s - loss: 771.6398 - loglik: -7.6969e+02 - logprior: -1.9489e+00
Epoch 4/10
27/27 - 5s - loss: 768.3793 - loglik: -7.6642e+02 - logprior: -1.9583e+00
Epoch 5/10
27/27 - 5s - loss: 767.6238 - loglik: -7.6566e+02 - logprior: -1.9634e+00
Epoch 6/10
27/27 - 5s - loss: 767.6362 - loglik: -7.6564e+02 - logprior: -1.9972e+00
Fitted a model with MAP estimate = -767.0125
expansions: [(0, 2), (19, 1), (21, 1), (25, 2), (26, 1), (35, 1), (36, 1), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 1), (67, 1), (68, 2), (69, 1), (71, 2), (73, 1), (78, 1), (81, 1), (94, 7), (102, 1), (103, 2), (104, 2), (105, 1), (115, 1), (116, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 165 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 762.6731 - loglik: -7.5599e+02 - logprior: -6.6804e+00
Epoch 2/2
27/27 - 6s - loss: 744.4398 - loglik: -7.4309e+02 - logprior: -1.3474e+00
Fitted a model with MAP estimate = -741.9937
expansions: []
discards: [  0  29  47  48  90 125 126 127 128 140 142]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 754.1669 - loglik: -7.4796e+02 - logprior: -6.2034e+00
Epoch 2/2
27/27 - 6s - loss: 745.6793 - loglik: -7.4461e+02 - logprior: -1.0674e+00
Fitted a model with MAP estimate = -743.9499
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 10s - loss: 749.2676 - loglik: -7.4523e+02 - logprior: -4.0355e+00
Epoch 2/10
27/27 - 6s - loss: 744.0460 - loglik: -7.4343e+02 - logprior: -6.1572e-01
Epoch 3/10
27/27 - 6s - loss: 743.0095 - loglik: -7.4267e+02 - logprior: -3.4137e-01
Epoch 4/10
27/27 - 6s - loss: 742.0410 - loglik: -7.4177e+02 - logprior: -2.6940e-01
Epoch 5/10
27/27 - 6s - loss: 742.3004 - loglik: -7.4212e+02 - logprior: -1.7839e-01
Fitted a model with MAP estimate = -741.6758
Time for alignment: 123.7692
Computed alignments with likelihoods: ['-745.1523', '-741.7949', '-741.0660', '-741.9302', '-741.6758']
Best model has likelihood: -741.0660  (prior= -1.2313 )
time for generating output: 0.3298
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/OTCace.projection.fasta
SP score = 0.5561461794019934
Training of 5 independent models on file lyase_1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b09919c10>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f53355207f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5315dc85b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5315dc8fd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b1346f070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f1e7e580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531c0ecc70>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5314a6bac0>, <__main__.SimpleDirichletPrior object at 0x7f531ccb1700>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 18s - loss: 1613.7095 - loglik: -1.6108e+03 - logprior: -2.9328e+00
Epoch 2/10
34/34 - 16s - loss: 1487.4769 - loglik: -1.4857e+03 - logprior: -1.8066e+00
Epoch 3/10
34/34 - 16s - loss: 1471.2385 - loglik: -1.4691e+03 - logprior: -2.1060e+00
Epoch 4/10
34/34 - 15s - loss: 1466.3657 - loglik: -1.4641e+03 - logprior: -2.2821e+00
Epoch 5/10
34/34 - 16s - loss: 1463.2642 - loglik: -1.4609e+03 - logprior: -2.3915e+00
Epoch 6/10
34/34 - 15s - loss: 1464.7562 - loglik: -1.4623e+03 - logprior: -2.5012e+00
Fitted a model with MAP estimate = -1463.6353
expansions: [(9, 1), (12, 2), (13, 1), (14, 2), (16, 2), (17, 4), (20, 1), (21, 2), (29, 2), (34, 1), (48, 1), (50, 2), (54, 4), (56, 1), (65, 1), (66, 1), (67, 1), (92, 1), (93, 1), (94, 2), (95, 2), (98, 1), (99, 1), (101, 1), (102, 1), (103, 1), (104, 1), (108, 1), (130, 1), (136, 1), (138, 1), (141, 1), (142, 2), (147, 1), (164, 2), (165, 3), (167, 1), (171, 1), (172, 1), (173, 1), (175, 1), (183, 1), (188, 1), (193, 1), (202, 2), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1), (230, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 25s - loss: 1451.3657 - loglik: -1.4476e+03 - logprior: -3.7829e+00
Epoch 2/2
34/34 - 21s - loss: 1428.4344 - loglik: -1.4268e+03 - logprior: -1.5994e+00
Fitted a model with MAP estimate = -1422.9787
expansions: [(18, 1), (74, 1), (264, 1)]
discards: [ 22  25  34  43  75  76  77  78  90 125 127 188 228]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 24s - loss: 1433.7780 - loglik: -1.4317e+03 - logprior: -2.1238e+00
Epoch 2/2
34/34 - 21s - loss: 1426.3750 - loglik: -1.4259e+03 - logprior: -4.5522e-01
Fitted a model with MAP estimate = -1426.0111
expansions: [(72, 1), (84, 1)]
discards: [16]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 27s - loss: 1432.3896 - loglik: -1.4304e+03 - logprior: -1.9953e+00
Epoch 2/10
34/34 - 20s - loss: 1426.0138 - loglik: -1.4259e+03 - logprior: -1.3921e-01
Epoch 3/10
34/34 - 21s - loss: 1426.5570 - loglik: -1.4265e+03 - logprior: -4.9987e-02
Fitted a model with MAP estimate = -1423.7054
Time for alignment: 357.7676
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 19s - loss: 1615.2925 - loglik: -1.6124e+03 - logprior: -2.9358e+00
Epoch 2/10
34/34 - 16s - loss: 1493.2134 - loglik: -1.4914e+03 - logprior: -1.7968e+00
Epoch 3/10
34/34 - 16s - loss: 1472.5662 - loglik: -1.4705e+03 - logprior: -2.0294e+00
Epoch 4/10
34/34 - 16s - loss: 1469.3206 - loglik: -1.4672e+03 - logprior: -2.0991e+00
Epoch 5/10
34/34 - 15s - loss: 1469.4961 - loglik: -1.4673e+03 - logprior: -2.2432e+00
Fitted a model with MAP estimate = -1468.0412
expansions: [(13, 2), (14, 1), (15, 3), (16, 1), (17, 4), (22, 1), (29, 1), (30, 2), (41, 1), (51, 2), (55, 2), (56, 2), (65, 1), (68, 1), (73, 2), (93, 1), (94, 1), (96, 2), (99, 1), (100, 3), (101, 1), (102, 1), (104, 1), (109, 1), (131, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (165, 1), (167, 1), (172, 2), (173, 1), (175, 1), (178, 1), (188, 1), (189, 1), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 25s - loss: 1455.3657 - loglik: -1.4516e+03 - logprior: -3.7561e+00
Epoch 2/2
34/34 - 20s - loss: 1437.4208 - loglik: -1.4359e+03 - logprior: -1.4895e+00
Fitted a model with MAP estimate = -1432.5318
expansions: [(187, 1)]
discards: [ 18  41 131]
Re-initialized the encoder parameters.
Fitting a model of length 293 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 25s - loss: 1440.4423 - loglik: -1.4382e+03 - logprior: -2.2419e+00
Epoch 2/2
34/34 - 21s - loss: 1434.7162 - loglik: -1.4343e+03 - logprior: -4.2242e-01
Fitted a model with MAP estimate = -1433.0574
expansions: [(218, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 294 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 24s - loss: 1437.0703 - loglik: -1.4351e+03 - logprior: -1.9777e+00
Epoch 2/10
34/34 - 21s - loss: 1435.1786 - loglik: -1.4351e+03 - logprior: -1.1174e-01
Epoch 3/10
34/34 - 21s - loss: 1432.1567 - loglik: -1.4322e+03 - logprior: 0.0024
Epoch 4/10
34/34 - 21s - loss: 1429.9843 - loglik: -1.4300e+03 - logprior: 0.0039
Epoch 5/10
34/34 - 21s - loss: 1430.8280 - loglik: -1.4308e+03 - logprior: 0.0068
Fitted a model with MAP estimate = -1428.2659
Time for alignment: 382.1242
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 20s - loss: 1617.6145 - loglik: -1.6147e+03 - logprior: -2.9338e+00
Epoch 2/10
34/34 - 16s - loss: 1488.7572 - loglik: -1.4869e+03 - logprior: -1.8372e+00
Epoch 3/10
34/34 - 16s - loss: 1472.2941 - loglik: -1.4702e+03 - logprior: -2.1079e+00
Epoch 4/10
34/34 - 16s - loss: 1469.5562 - loglik: -1.4674e+03 - logprior: -2.1901e+00
Epoch 5/10
34/34 - 16s - loss: 1466.6384 - loglik: -1.4643e+03 - logprior: -2.3142e+00
Epoch 6/10
34/34 - 16s - loss: 1466.1250 - loglik: -1.4637e+03 - logprior: -2.4076e+00
Epoch 7/10
34/34 - 16s - loss: 1465.7242 - loglik: -1.4632e+03 - logprior: -2.5165e+00
Epoch 8/10
34/34 - 16s - loss: 1465.6343 - loglik: -1.4630e+03 - logprior: -2.5993e+00
Epoch 9/10
34/34 - 15s - loss: 1462.2504 - loglik: -1.4596e+03 - logprior: -2.6469e+00
Epoch 10/10
34/34 - 16s - loss: 1467.3118 - loglik: -1.4646e+03 - logprior: -2.6641e+00
Fitted a model with MAP estimate = -1464.3971
expansions: [(9, 1), (10, 1), (12, 1), (14, 1), (16, 2), (18, 5), (23, 2), (29, 1), (30, 2), (39, 1), (45, 2), (55, 1), (56, 2), (65, 1), (68, 1), (73, 2), (93, 1), (94, 1), (96, 2), (99, 1), (101, 1), (102, 1), (103, 1), (105, 1), (110, 1), (130, 1), (131, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (165, 1), (168, 3), (172, 1), (173, 2), (175, 1), (178, 1), (188, 1), (190, 1), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 25s - loss: 1456.1069 - loglik: -1.4522e+03 - logprior: -3.9057e+00
Epoch 2/2
34/34 - 20s - loss: 1438.3245 - loglik: -1.4366e+03 - logprior: -1.6999e+00
Fitted a model with MAP estimate = -1433.1154
expansions: [(186, 1), (215, 2)]
discards: [ 10  20  23  24  33  42  61  62  75 222]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 24s - loss: 1442.8479 - loglik: -1.4407e+03 - logprior: -2.1707e+00
Epoch 2/2
34/34 - 20s - loss: 1437.4861 - loglik: -1.4371e+03 - logprior: -4.1623e-01
Fitted a model with MAP estimate = -1435.6915
expansions: []
discards: [206 207 208 209]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 24s - loss: 1445.0254 - loglik: -1.4430e+03 - logprior: -1.9991e+00
Epoch 2/10
34/34 - 20s - loss: 1438.0840 - loglik: -1.4380e+03 - logprior: -1.1458e-01
Epoch 3/10
34/34 - 20s - loss: 1436.9746 - loglik: -1.4370e+03 - logprior: 0.0088
Epoch 4/10
34/34 - 20s - loss: 1436.9908 - loglik: -1.4370e+03 - logprior: 0.0360
Fitted a model with MAP estimate = -1434.5921
Time for alignment: 436.2171
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 19s - loss: 1618.8705 - loglik: -1.6160e+03 - logprior: -2.9089e+00
Epoch 2/10
34/34 - 15s - loss: 1488.9119 - loglik: -1.4873e+03 - logprior: -1.6392e+00
Epoch 3/10
34/34 - 16s - loss: 1473.0227 - loglik: -1.4711e+03 - logprior: -1.9286e+00
Epoch 4/10
34/34 - 16s - loss: 1467.8309 - loglik: -1.4659e+03 - logprior: -1.9676e+00
Epoch 5/10
34/34 - 15s - loss: 1470.0029 - loglik: -1.4679e+03 - logprior: -2.0603e+00
Fitted a model with MAP estimate = -1466.4799
expansions: [(13, 1), (14, 2), (16, 1), (30, 1), (31, 2), (35, 1), (57, 6), (66, 1), (67, 1), (71, 1), (74, 3), (93, 1), (96, 1), (97, 2), (100, 1), (101, 2), (102, 1), (103, 1), (105, 1), (110, 1), (130, 1), (131, 1), (137, 1), (140, 1), (142, 1), (143, 3), (144, 1), (165, 1), (168, 3), (172, 2), (173, 2), (175, 1), (178, 1), (184, 3), (185, 1), (186, 3), (187, 2), (203, 2), (205, 1), (207, 1), (224, 1), (225, 1), (227, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 301 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 25s - loss: 1451.3910 - loglik: -1.4477e+03 - logprior: -3.7302e+00
Epoch 2/2
34/34 - 22s - loss: 1429.1980 - loglik: -1.4277e+03 - logprior: -1.5020e+00
Fitted a model with MAP estimate = -1422.9812
expansions: [(181, 1), (209, 1), (210, 1)]
discards: [ 34  64  65  66  67  68  69 218 261]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 26s - loss: 1433.8799 - loglik: -1.4316e+03 - logprior: -2.2559e+00
Epoch 2/2
34/34 - 21s - loss: 1428.9026 - loglik: -1.4284e+03 - logprior: -5.3748e-01
Fitted a model with MAP estimate = -1425.6649
expansions: [(63, 6)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 301 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 25s - loss: 1429.8594 - loglik: -1.4277e+03 - logprior: -2.1720e+00
Epoch 2/10
34/34 - 22s - loss: 1423.4821 - loglik: -1.4232e+03 - logprior: -2.9922e-01
Epoch 3/10
34/34 - 21s - loss: 1419.0116 - loglik: -1.4187e+03 - logprior: -3.2524e-01
Epoch 4/10
34/34 - 21s - loss: 1419.6810 - loglik: -1.4194e+03 - logprior: -2.3468e-01
Fitted a model with MAP estimate = -1417.9577
Time for alignment: 365.3301
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 19s - loss: 1616.9833 - loglik: -1.6141e+03 - logprior: -2.9330e+00
Epoch 2/10
34/34 - 15s - loss: 1491.1787 - loglik: -1.4893e+03 - logprior: -1.8358e+00
Epoch 3/10
34/34 - 16s - loss: 1473.0242 - loglik: -1.4709e+03 - logprior: -2.0874e+00
Epoch 4/10
34/34 - 15s - loss: 1473.4854 - loglik: -1.4713e+03 - logprior: -2.1364e+00
Fitted a model with MAP estimate = -1469.3144
expansions: [(13, 2), (14, 1), (15, 3), (16, 1), (17, 4), (22, 1), (29, 2), (42, 1), (51, 2), (55, 1), (65, 1), (68, 1), (70, 1), (73, 3), (92, 1), (95, 1), (96, 2), (99, 1), (100, 2), (101, 1), (102, 1), (104, 1), (109, 1), (129, 1), (134, 1), (140, 1), (142, 1), (143, 3), (144, 1), (147, 1), (164, 2), (167, 1), (171, 1), (172, 1), (178, 1), (184, 1), (185, 2), (186, 1), (187, 3), (188, 1), (190, 1), (204, 1), (205, 2), (208, 1), (225, 1), (226, 1), (229, 1), (230, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 27s - loss: 1452.3658 - loglik: -1.4487e+03 - logprior: -3.6833e+00
Epoch 2/2
34/34 - 21s - loss: 1431.1648 - loglik: -1.4298e+03 - logprior: -1.3524e+00
Fitted a model with MAP estimate = -1423.5014
expansions: [(183, 1)]
discards: [ 18 235 258 259 260 264]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 24s - loss: 1432.9434 - loglik: -1.4308e+03 - logprior: -2.1175e+00
Epoch 2/2
34/34 - 21s - loss: 1427.0226 - loglik: -1.4266e+03 - logprior: -3.9799e-01
Fitted a model with MAP estimate = -1423.1761
expansions: [(207, 2), (228, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 300 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 25s - loss: 1427.1122 - loglik: -1.4252e+03 - logprior: -1.9083e+00
Epoch 2/10
34/34 - 21s - loss: 1421.9843 - loglik: -1.4218e+03 - logprior: -1.5831e-01
Epoch 3/10
34/34 - 21s - loss: 1420.9541 - loglik: -1.4209e+03 - logprior: -4.5575e-02
Epoch 4/10
34/34 - 22s - loss: 1419.3577 - loglik: -1.4193e+03 - logprior: -3.7405e-02
Epoch 5/10
34/34 - 21s - loss: 1417.7380 - loglik: -1.4177e+03 - logprior: -3.2037e-02
Epoch 6/10
34/34 - 22s - loss: 1415.1295 - loglik: -1.4151e+03 - logprior: -6.4247e-03
Epoch 7/10
34/34 - 21s - loss: 1416.4561 - loglik: -1.4165e+03 - logprior: 0.0259
Fitted a model with MAP estimate = -1415.9709
Time for alignment: 414.0210
Computed alignments with likelihoods: ['-1422.9787', '-1428.2659', '-1433.1154', '-1417.9577', '-1415.9709']
Best model has likelihood: -1415.9709  (prior= -0.0324 )
time for generating output: 0.3990
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/lyase_1.projection.fasta
SP score = 0.7180246913580247
Training of 5 independent models on file toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5314fde490>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5300109760>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f53443ac2b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b082cf4f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530041ca90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530dee6eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530dee6100>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5315f83730>, <__main__.SimpleDirichletPrior object at 0x7f53151470a0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 472.5141 - loglik: -3.7769e+02 - logprior: -9.4827e+01
Epoch 2/10
10/10 - 1s - loss: 379.4360 - loglik: -3.5426e+02 - logprior: -2.5176e+01
Epoch 3/10
10/10 - 1s - loss: 347.6349 - loglik: -3.3593e+02 - logprior: -1.1701e+01
Epoch 4/10
10/10 - 1s - loss: 333.2018 - loglik: -3.2636e+02 - logprior: -6.8464e+00
Epoch 5/10
10/10 - 1s - loss: 326.5386 - loglik: -3.2217e+02 - logprior: -4.3736e+00
Epoch 6/10
10/10 - 1s - loss: 323.0156 - loglik: -3.2001e+02 - logprior: -3.0068e+00
Epoch 7/10
10/10 - 1s - loss: 321.1515 - loglik: -3.1905e+02 - logprior: -2.1036e+00
Epoch 8/10
10/10 - 1s - loss: 320.1197 - loglik: -3.1856e+02 - logprior: -1.5633e+00
Epoch 9/10
10/10 - 1s - loss: 319.6004 - loglik: -3.1840e+02 - logprior: -1.2022e+00
Epoch 10/10
10/10 - 1s - loss: 319.2893 - loglik: -3.1839e+02 - logprior: -9.0033e-01
Fitted a model with MAP estimate = -319.1539
expansions: [(7, 1), (8, 3), (9, 2), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 417.8121 - loglik: -3.1709e+02 - logprior: -1.0072e+02
Epoch 2/2
10/10 - 1s - loss: 351.7314 - loglik: -3.1116e+02 - logprior: -4.0567e+01
Fitted a model with MAP estimate = -340.3588
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 386.9316 - loglik: -3.0796e+02 - logprior: -7.8974e+01
Epoch 2/2
10/10 - 1s - loss: 326.3766 - loglik: -3.0570e+02 - logprior: -2.0674e+01
Fitted a model with MAP estimate = -317.4398
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 382.3708 - loglik: -3.0517e+02 - logprior: -7.7197e+01
Epoch 2/10
10/10 - 1s - loss: 324.8747 - loglik: -3.0485e+02 - logprior: -2.0020e+01
Epoch 3/10
10/10 - 1s - loss: 312.9505 - loglik: -3.0476e+02 - logprior: -8.1917e+00
Epoch 4/10
10/10 - 1s - loss: 308.2836 - loglik: -3.0511e+02 - logprior: -3.1763e+00
Epoch 5/10
10/10 - 1s - loss: 305.9064 - loglik: -3.0552e+02 - logprior: -3.8662e-01
Epoch 6/10
10/10 - 1s - loss: 304.5951 - loglik: -3.0586e+02 - logprior: 1.2677
Epoch 7/10
10/10 - 1s - loss: 303.8157 - loglik: -3.0612e+02 - logprior: 2.3049
Epoch 8/10
10/10 - 1s - loss: 303.2786 - loglik: -3.0626e+02 - logprior: 2.9831
Epoch 9/10
10/10 - 1s - loss: 302.8247 - loglik: -3.0630e+02 - logprior: 3.4766
Epoch 10/10
10/10 - 1s - loss: 302.3869 - loglik: -3.0620e+02 - logprior: 3.8157
Fitted a model with MAP estimate = -302.1242
Time for alignment: 35.2841
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 472.5141 - loglik: -3.7769e+02 - logprior: -9.4827e+01
Epoch 2/10
10/10 - 1s - loss: 379.4360 - loglik: -3.5426e+02 - logprior: -2.5176e+01
Epoch 3/10
10/10 - 1s - loss: 347.6349 - loglik: -3.3593e+02 - logprior: -1.1701e+01
Epoch 4/10
10/10 - 1s - loss: 333.2019 - loglik: -3.2636e+02 - logprior: -6.8464e+00
Epoch 5/10
10/10 - 1s - loss: 326.5386 - loglik: -3.2217e+02 - logprior: -4.3736e+00
Epoch 6/10
10/10 - 1s - loss: 323.0156 - loglik: -3.2001e+02 - logprior: -3.0068e+00
Epoch 7/10
10/10 - 1s - loss: 321.1515 - loglik: -3.1905e+02 - logprior: -2.1036e+00
Epoch 8/10
10/10 - 1s - loss: 320.1197 - loglik: -3.1856e+02 - logprior: -1.5633e+00
Epoch 9/10
10/10 - 1s - loss: 319.6005 - loglik: -3.1840e+02 - logprior: -1.2022e+00
Epoch 10/10
10/10 - 1s - loss: 319.2893 - loglik: -3.1839e+02 - logprior: -9.0033e-01
Fitted a model with MAP estimate = -319.1539
expansions: [(7, 1), (8, 3), (9, 2), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 417.8121 - loglik: -3.1709e+02 - logprior: -1.0072e+02
Epoch 2/2
10/10 - 1s - loss: 351.7314 - loglik: -3.1116e+02 - logprior: -4.0567e+01
Fitted a model with MAP estimate = -340.3588
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 386.9316 - loglik: -3.0796e+02 - logprior: -7.8974e+01
Epoch 2/2
10/10 - 1s - loss: 326.3766 - loglik: -3.0570e+02 - logprior: -2.0674e+01
Fitted a model with MAP estimate = -317.4398
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 382.3708 - loglik: -3.0517e+02 - logprior: -7.7197e+01
Epoch 2/10
10/10 - 1s - loss: 324.8747 - loglik: -3.0485e+02 - logprior: -2.0020e+01
Epoch 3/10
10/10 - 1s - loss: 312.9505 - loglik: -3.0476e+02 - logprior: -8.1917e+00
Epoch 4/10
10/10 - 1s - loss: 308.2836 - loglik: -3.0511e+02 - logprior: -3.1763e+00
Epoch 5/10
10/10 - 1s - loss: 305.9064 - loglik: -3.0552e+02 - logprior: -3.8662e-01
Epoch 6/10
10/10 - 1s - loss: 304.5952 - loglik: -3.0586e+02 - logprior: 1.2677
Epoch 7/10
10/10 - 1s - loss: 303.8157 - loglik: -3.0612e+02 - logprior: 2.3049
Epoch 8/10
10/10 - 1s - loss: 303.2786 - loglik: -3.0626e+02 - logprior: 2.9831
Epoch 9/10
10/10 - 1s - loss: 302.8247 - loglik: -3.0630e+02 - logprior: 3.4766
Epoch 10/10
10/10 - 1s - loss: 302.3869 - loglik: -3.0620e+02 - logprior: 3.8157
Fitted a model with MAP estimate = -302.1241
Time for alignment: 35.2538
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 472.5141 - loglik: -3.7769e+02 - logprior: -9.4827e+01
Epoch 2/10
10/10 - 1s - loss: 379.4360 - loglik: -3.5426e+02 - logprior: -2.5176e+01
Epoch 3/10
10/10 - 1s - loss: 347.6349 - loglik: -3.3593e+02 - logprior: -1.1701e+01
Epoch 4/10
10/10 - 1s - loss: 333.2019 - loglik: -3.2636e+02 - logprior: -6.8464e+00
Epoch 5/10
10/10 - 1s - loss: 326.5386 - loglik: -3.2217e+02 - logprior: -4.3736e+00
Epoch 6/10
10/10 - 1s - loss: 323.0156 - loglik: -3.2001e+02 - logprior: -3.0068e+00
Epoch 7/10
10/10 - 1s - loss: 321.1515 - loglik: -3.1905e+02 - logprior: -2.1036e+00
Epoch 8/10
10/10 - 1s - loss: 320.1197 - loglik: -3.1856e+02 - logprior: -1.5633e+00
Epoch 9/10
10/10 - 1s - loss: 319.6004 - loglik: -3.1840e+02 - logprior: -1.2022e+00
Epoch 10/10
10/10 - 1s - loss: 319.2893 - loglik: -3.1839e+02 - logprior: -9.0033e-01
Fitted a model with MAP estimate = -319.1539
expansions: [(7, 1), (8, 3), (9, 2), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 417.8121 - loglik: -3.1709e+02 - logprior: -1.0072e+02
Epoch 2/2
10/10 - 1s - loss: 351.7314 - loglik: -3.1116e+02 - logprior: -4.0567e+01
Fitted a model with MAP estimate = -340.3588
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 386.9316 - loglik: -3.0796e+02 - logprior: -7.8974e+01
Epoch 2/2
10/10 - 1s - loss: 326.3766 - loglik: -3.0570e+02 - logprior: -2.0674e+01
Fitted a model with MAP estimate = -317.4398
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 382.3708 - loglik: -3.0517e+02 - logprior: -7.7197e+01
Epoch 2/10
10/10 - 1s - loss: 324.8747 - loglik: -3.0485e+02 - logprior: -2.0020e+01
Epoch 3/10
10/10 - 1s - loss: 312.9505 - loglik: -3.0476e+02 - logprior: -8.1917e+00
Epoch 4/10
10/10 - 1s - loss: 308.2836 - loglik: -3.0511e+02 - logprior: -3.1763e+00
Epoch 5/10
10/10 - 1s - loss: 305.9064 - loglik: -3.0552e+02 - logprior: -3.8662e-01
Epoch 6/10
10/10 - 1s - loss: 304.5952 - loglik: -3.0586e+02 - logprior: 1.2678
Epoch 7/10
10/10 - 1s - loss: 303.8157 - loglik: -3.0612e+02 - logprior: 2.3049
Epoch 8/10
10/10 - 1s - loss: 303.2786 - loglik: -3.0626e+02 - logprior: 2.9831
Epoch 9/10
10/10 - 1s - loss: 302.8247 - loglik: -3.0630e+02 - logprior: 3.4766
Epoch 10/10
10/10 - 1s - loss: 302.3869 - loglik: -3.0620e+02 - logprior: 3.8157
Fitted a model with MAP estimate = -302.1241
Time for alignment: 34.4145
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 472.5141 - loglik: -3.7769e+02 - logprior: -9.4827e+01
Epoch 2/10
10/10 - 1s - loss: 379.4360 - loglik: -3.5426e+02 - logprior: -2.5176e+01
Epoch 3/10
10/10 - 1s - loss: 347.6349 - loglik: -3.3593e+02 - logprior: -1.1701e+01
Epoch 4/10
10/10 - 1s - loss: 333.2018 - loglik: -3.2636e+02 - logprior: -6.8464e+00
Epoch 5/10
10/10 - 1s - loss: 326.5386 - loglik: -3.2217e+02 - logprior: -4.3736e+00
Epoch 6/10
10/10 - 1s - loss: 323.0156 - loglik: -3.2001e+02 - logprior: -3.0068e+00
Epoch 7/10
10/10 - 1s - loss: 321.1515 - loglik: -3.1905e+02 - logprior: -2.1036e+00
Epoch 8/10
10/10 - 1s - loss: 320.1197 - loglik: -3.1856e+02 - logprior: -1.5633e+00
Epoch 9/10
10/10 - 1s - loss: 319.6004 - loglik: -3.1840e+02 - logprior: -1.2022e+00
Epoch 10/10
10/10 - 1s - loss: 319.2893 - loglik: -3.1839e+02 - logprior: -9.0033e-01
Fitted a model with MAP estimate = -319.1539
expansions: [(7, 1), (8, 3), (9, 2), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 417.8121 - loglik: -3.1709e+02 - logprior: -1.0072e+02
Epoch 2/2
10/10 - 1s - loss: 351.7314 - loglik: -3.1116e+02 - logprior: -4.0567e+01
Fitted a model with MAP estimate = -340.3588
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 386.9316 - loglik: -3.0796e+02 - logprior: -7.8974e+01
Epoch 2/2
10/10 - 1s - loss: 326.3766 - loglik: -3.0570e+02 - logprior: -2.0674e+01
Fitted a model with MAP estimate = -317.4398
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 382.3708 - loglik: -3.0517e+02 - logprior: -7.7197e+01
Epoch 2/10
10/10 - 1s - loss: 324.8747 - loglik: -3.0485e+02 - logprior: -2.0020e+01
Epoch 3/10
10/10 - 1s - loss: 312.9505 - loglik: -3.0476e+02 - logprior: -8.1917e+00
Epoch 4/10
10/10 - 1s - loss: 308.2836 - loglik: -3.0511e+02 - logprior: -3.1763e+00
Epoch 5/10
10/10 - 1s - loss: 305.9064 - loglik: -3.0552e+02 - logprior: -3.8662e-01
Epoch 6/10
10/10 - 1s - loss: 304.5952 - loglik: -3.0586e+02 - logprior: 1.2677
Epoch 7/10
10/10 - 1s - loss: 303.8157 - loglik: -3.0612e+02 - logprior: 2.3049
Epoch 8/10
10/10 - 1s - loss: 303.2786 - loglik: -3.0626e+02 - logprior: 2.9831
Epoch 9/10
10/10 - 1s - loss: 302.8247 - loglik: -3.0630e+02 - logprior: 3.4766
Epoch 10/10
10/10 - 1s - loss: 302.3869 - loglik: -3.0620e+02 - logprior: 3.8157
Fitted a model with MAP estimate = -302.1241
Time for alignment: 34.4899
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 472.5141 - loglik: -3.7769e+02 - logprior: -9.4827e+01
Epoch 2/10
10/10 - 1s - loss: 379.4360 - loglik: -3.5426e+02 - logprior: -2.5176e+01
Epoch 3/10
10/10 - 1s - loss: 347.6349 - loglik: -3.3593e+02 - logprior: -1.1701e+01
Epoch 4/10
10/10 - 1s - loss: 333.2018 - loglik: -3.2636e+02 - logprior: -6.8464e+00
Epoch 5/10
10/10 - 1s - loss: 326.5386 - loglik: -3.2217e+02 - logprior: -4.3736e+00
Epoch 6/10
10/10 - 1s - loss: 323.0156 - loglik: -3.2001e+02 - logprior: -3.0068e+00
Epoch 7/10
10/10 - 1s - loss: 321.1515 - loglik: -3.1905e+02 - logprior: -2.1036e+00
Epoch 8/10
10/10 - 1s - loss: 320.1197 - loglik: -3.1856e+02 - logprior: -1.5633e+00
Epoch 9/10
10/10 - 1s - loss: 319.6004 - loglik: -3.1840e+02 - logprior: -1.2022e+00
Epoch 10/10
10/10 - 1s - loss: 319.2893 - loglik: -3.1839e+02 - logprior: -9.0033e-01
Fitted a model with MAP estimate = -319.1539
expansions: [(7, 1), (8, 3), (9, 2), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 417.8121 - loglik: -3.1709e+02 - logprior: -1.0072e+02
Epoch 2/2
10/10 - 1s - loss: 351.7314 - loglik: -3.1116e+02 - logprior: -4.0567e+01
Fitted a model with MAP estimate = -340.3588
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 386.9316 - loglik: -3.0796e+02 - logprior: -7.8974e+01
Epoch 2/2
10/10 - 1s - loss: 326.3766 - loglik: -3.0570e+02 - logprior: -2.0674e+01
Fitted a model with MAP estimate = -317.4398
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 382.3708 - loglik: -3.0517e+02 - logprior: -7.7197e+01
Epoch 2/10
10/10 - 1s - loss: 324.8747 - loglik: -3.0485e+02 - logprior: -2.0020e+01
Epoch 3/10
10/10 - 1s - loss: 312.9505 - loglik: -3.0476e+02 - logprior: -8.1917e+00
Epoch 4/10
10/10 - 1s - loss: 308.2836 - loglik: -3.0511e+02 - logprior: -3.1763e+00
Epoch 5/10
10/10 - 1s - loss: 305.9064 - loglik: -3.0552e+02 - logprior: -3.8662e-01
Epoch 6/10
10/10 - 1s - loss: 304.5951 - loglik: -3.0586e+02 - logprior: 1.2677
Epoch 7/10
10/10 - 1s - loss: 303.8157 - loglik: -3.0612e+02 - logprior: 2.3049
Epoch 8/10
10/10 - 1s - loss: 303.2786 - loglik: -3.0626e+02 - logprior: 2.9831
Epoch 9/10
10/10 - 1s - loss: 302.8247 - loglik: -3.0630e+02 - logprior: 3.4766
Epoch 10/10
10/10 - 1s - loss: 302.3868 - loglik: -3.0620e+02 - logprior: 3.8157
Fitted a model with MAP estimate = -302.1241
Time for alignment: 34.2247
Computed alignments with likelihoods: ['-302.1242', '-302.1241', '-302.1241', '-302.1241', '-302.1241']
Best model has likelihood: -302.1241  (prior= 3.9521 )
time for generating output: 0.1190
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/toxin.projection.fasta
SP score = 0.8410951428051665
Training of 5 independent models on file msb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f531588b1f0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52f0e4bbb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52d826af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b0a0e49a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f10fa2b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f53158b3370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f1adf490>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52f120cfd0>, <__main__.SimpleDirichletPrior object at 0x7f596470a7f0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 637.1422 - loglik: -6.2843e+02 - logprior: -8.7097e+00
Epoch 2/10
13/13 - 2s - loss: 605.4473 - loglik: -6.0341e+02 - logprior: -2.0348e+00
Epoch 3/10
13/13 - 2s - loss: 580.3281 - loglik: -5.7863e+02 - logprior: -1.7009e+00
Epoch 4/10
13/13 - 2s - loss: 571.0200 - loglik: -5.6914e+02 - logprior: -1.8764e+00
Epoch 5/10
13/13 - 2s - loss: 568.8731 - loglik: -5.6707e+02 - logprior: -1.7989e+00
Epoch 6/10
13/13 - 2s - loss: 567.7631 - loglik: -5.6598e+02 - logprior: -1.7847e+00
Epoch 7/10
13/13 - 2s - loss: 568.2858 - loglik: -5.6641e+02 - logprior: -1.8744e+00
Fitted a model with MAP estimate = -567.7192
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 4), (29, 3), (30, 1), (39, 1), (40, 1), (44, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 577.3702 - loglik: -5.6789e+02 - logprior: -9.4767e+00
Epoch 2/2
13/13 - 2s - loss: 567.8541 - loglik: -5.6377e+02 - logprior: -4.0835e+00
Fitted a model with MAP estimate = -565.5613
expansions: [(0, 2)]
discards: [ 0 14 36 66 95]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 569.7817 - loglik: -5.6293e+02 - logprior: -6.8512e+00
Epoch 2/2
13/13 - 2s - loss: 562.8903 - loglik: -5.6108e+02 - logprior: -1.8140e+00
Fitted a model with MAP estimate = -562.4118
expansions: []
discards: [ 0 88]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 571.8434 - loglik: -5.6321e+02 - logprior: -8.6338e+00
Epoch 2/10
13/13 - 2s - loss: 564.5359 - loglik: -5.6195e+02 - logprior: -2.5841e+00
Epoch 3/10
13/13 - 2s - loss: 563.2729 - loglik: -5.6202e+02 - logprior: -1.2544e+00
Epoch 4/10
13/13 - 2s - loss: 562.0768 - loglik: -5.6112e+02 - logprior: -9.5704e-01
Epoch 5/10
13/13 - 2s - loss: 561.8064 - loglik: -5.6093e+02 - logprior: -8.7721e-01
Epoch 6/10
13/13 - 2s - loss: 562.0314 - loglik: -5.6113e+02 - logprior: -9.0567e-01
Fitted a model with MAP estimate = -561.1569
Time for alignment: 58.1203
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 637.0391 - loglik: -6.2833e+02 - logprior: -8.7081e+00
Epoch 2/10
13/13 - 2s - loss: 604.0709 - loglik: -6.0204e+02 - logprior: -2.0261e+00
Epoch 3/10
13/13 - 2s - loss: 581.0194 - loglik: -5.7934e+02 - logprior: -1.6801e+00
Epoch 4/10
13/13 - 2s - loss: 570.9788 - loglik: -5.6914e+02 - logprior: -1.8358e+00
Epoch 5/10
13/13 - 2s - loss: 568.8419 - loglik: -5.6707e+02 - logprior: -1.7691e+00
Epoch 6/10
13/13 - 2s - loss: 568.3076 - loglik: -5.6654e+02 - logprior: -1.7657e+00
Epoch 7/10
13/13 - 2s - loss: 568.5077 - loglik: -5.6666e+02 - logprior: -1.8452e+00
Fitted a model with MAP estimate = -567.7313
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 4), (29, 3), (30, 1), (39, 1), (40, 1), (44, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 578.1782 - loglik: -5.6870e+02 - logprior: -9.4766e+00
Epoch 2/2
13/13 - 2s - loss: 567.2741 - loglik: -5.6319e+02 - logprior: -4.0861e+00
Fitted a model with MAP estimate = -565.5796
expansions: [(0, 2)]
discards: [ 0 14 36 66 95]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 569.3987 - loglik: -5.6255e+02 - logprior: -6.8460e+00
Epoch 2/2
13/13 - 2s - loss: 563.2032 - loglik: -5.6138e+02 - logprior: -1.8281e+00
Fitted a model with MAP estimate = -562.4352
expansions: []
discards: [ 0 89]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 571.9597 - loglik: -5.6332e+02 - logprior: -8.6387e+00
Epoch 2/10
13/13 - 2s - loss: 564.5305 - loglik: -5.6193e+02 - logprior: -2.6046e+00
Epoch 3/10
13/13 - 2s - loss: 562.9899 - loglik: -5.6173e+02 - logprior: -1.2585e+00
Epoch 4/10
13/13 - 2s - loss: 562.6285 - loglik: -5.6167e+02 - logprior: -9.5406e-01
Epoch 5/10
13/13 - 2s - loss: 561.5252 - loglik: -5.6064e+02 - logprior: -8.8815e-01
Epoch 6/10
13/13 - 2s - loss: 561.2603 - loglik: -5.6036e+02 - logprior: -9.0024e-01
Epoch 7/10
13/13 - 2s - loss: 561.3291 - loglik: -5.6040e+02 - logprior: -9.2668e-01
Fitted a model with MAP estimate = -561.0487
Time for alignment: 57.6200
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 637.1945 - loglik: -6.2849e+02 - logprior: -8.7079e+00
Epoch 2/10
13/13 - 2s - loss: 604.7936 - loglik: -6.0277e+02 - logprior: -2.0248e+00
Epoch 3/10
13/13 - 2s - loss: 581.2264 - loglik: -5.7955e+02 - logprior: -1.6800e+00
Epoch 4/10
13/13 - 2s - loss: 573.1525 - loglik: -5.7133e+02 - logprior: -1.8270e+00
Epoch 5/10
13/13 - 2s - loss: 568.3970 - loglik: -5.6664e+02 - logprior: -1.7614e+00
Epoch 6/10
13/13 - 2s - loss: 568.4177 - loglik: -5.6665e+02 - logprior: -1.7654e+00
Fitted a model with MAP estimate = -567.8564
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 4), (29, 3), (30, 1), (31, 1), (39, 1), (40, 1), (42, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 577.9216 - loglik: -5.6845e+02 - logprior: -9.4711e+00
Epoch 2/2
13/13 - 2s - loss: 568.0720 - loglik: -5.6401e+02 - logprior: -4.0621e+00
Fitted a model with MAP estimate = -565.5241
expansions: [(0, 2)]
discards: [ 0 14 36 67 96]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 568.9772 - loglik: -5.6213e+02 - logprior: -6.8470e+00
Epoch 2/2
13/13 - 2s - loss: 563.8624 - loglik: -5.6204e+02 - logprior: -1.8265e+00
Fitted a model with MAP estimate = -562.3741
expansions: []
discards: [ 0 89]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 571.5309 - loglik: -5.6290e+02 - logprior: -8.6312e+00
Epoch 2/10
13/13 - 2s - loss: 565.4246 - loglik: -5.6284e+02 - logprior: -2.5882e+00
Epoch 3/10
13/13 - 2s - loss: 562.1739 - loglik: -5.6092e+02 - logprior: -1.2580e+00
Epoch 4/10
13/13 - 2s - loss: 562.1256 - loglik: -5.6116e+02 - logprior: -9.6175e-01
Epoch 5/10
13/13 - 2s - loss: 561.0379 - loglik: -5.6016e+02 - logprior: -8.7598e-01
Epoch 6/10
13/13 - 2s - loss: 562.2520 - loglik: -5.6136e+02 - logprior: -8.8835e-01
Fitted a model with MAP estimate = -561.1153
Time for alignment: 53.3111
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 637.3701 - loglik: -6.2866e+02 - logprior: -8.7117e+00
Epoch 2/10
13/13 - 2s - loss: 603.4078 - loglik: -6.0137e+02 - logprior: -2.0339e+00
Epoch 3/10
13/13 - 2s - loss: 579.9396 - loglik: -5.7824e+02 - logprior: -1.6956e+00
Epoch 4/10
13/13 - 2s - loss: 571.2883 - loglik: -5.6944e+02 - logprior: -1.8445e+00
Epoch 5/10
13/13 - 2s - loss: 569.0135 - loglik: -5.6723e+02 - logprior: -1.7876e+00
Epoch 6/10
13/13 - 2s - loss: 568.4612 - loglik: -5.6671e+02 - logprior: -1.7554e+00
Epoch 7/10
13/13 - 2s - loss: 567.4387 - loglik: -5.6559e+02 - logprior: -1.8482e+00
Epoch 8/10
13/13 - 2s - loss: 568.2104 - loglik: -5.6638e+02 - logprior: -1.8349e+00
Fitted a model with MAP estimate = -567.6106
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 4), (29, 3), (30, 1), (31, 1), (39, 1), (40, 1), (44, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 577.6196 - loglik: -5.6815e+02 - logprior: -9.4678e+00
Epoch 2/2
13/13 - 2s - loss: 567.5583 - loglik: -5.6348e+02 - logprior: -4.0789e+00
Fitted a model with MAP estimate = -565.5355
expansions: [(0, 2)]
discards: [ 0 14 36 67 96]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 569.2492 - loglik: -5.6240e+02 - logprior: -6.8454e+00
Epoch 2/2
13/13 - 2s - loss: 563.0820 - loglik: -5.6126e+02 - logprior: -1.8220e+00
Fitted a model with MAP estimate = -562.3659
expansions: []
discards: [ 0 91]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 572.4938 - loglik: -5.6387e+02 - logprior: -8.6231e+00
Epoch 2/10
13/13 - 2s - loss: 564.1873 - loglik: -5.6161e+02 - logprior: -2.5739e+00
Epoch 3/10
13/13 - 2s - loss: 562.4772 - loglik: -5.6123e+02 - logprior: -1.2476e+00
Epoch 4/10
13/13 - 2s - loss: 562.2852 - loglik: -5.6133e+02 - logprior: -9.5491e-01
Epoch 5/10
13/13 - 2s - loss: 561.7688 - loglik: -5.6088e+02 - logprior: -8.9266e-01
Epoch 6/10
13/13 - 2s - loss: 561.8608 - loglik: -5.6096e+02 - logprior: -8.9872e-01
Fitted a model with MAP estimate = -561.0955
Time for alignment: 56.5248
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 637.3875 - loglik: -6.2868e+02 - logprior: -8.7097e+00
Epoch 2/10
13/13 - 2s - loss: 604.0856 - loglik: -6.0205e+02 - logprior: -2.0359e+00
Epoch 3/10
13/13 - 2s - loss: 580.7982 - loglik: -5.7908e+02 - logprior: -1.7135e+00
Epoch 4/10
13/13 - 2s - loss: 571.7663 - loglik: -5.6986e+02 - logprior: -1.9084e+00
Epoch 5/10
13/13 - 2s - loss: 568.8214 - loglik: -5.6698e+02 - logprior: -1.8397e+00
Epoch 6/10
13/13 - 2s - loss: 568.4626 - loglik: -5.6663e+02 - logprior: -1.8341e+00
Epoch 7/10
13/13 - 2s - loss: 567.6663 - loglik: -5.6575e+02 - logprior: -1.9193e+00
Epoch 8/10
13/13 - 2s - loss: 566.4647 - loglik: -5.6455e+02 - logprior: -1.9109e+00
Epoch 9/10
13/13 - 2s - loss: 567.3212 - loglik: -5.6544e+02 - logprior: -1.8860e+00
Fitted a model with MAP estimate = -567.2109
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (21, 1), (27, 1), (28, 1), (29, 2), (30, 3), (31, 2), (39, 1), (40, 1), (42, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 578.2315 - loglik: -5.6872e+02 - logprior: -9.5155e+00
Epoch 2/2
13/13 - 2s - loss: 567.1049 - loglik: -5.6301e+02 - logprior: -4.0945e+00
Fitted a model with MAP estimate = -565.6321
expansions: [(0, 2)]
discards: [ 0 14 68 94 97]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 569.4666 - loglik: -5.6264e+02 - logprior: -6.8282e+00
Epoch 2/2
13/13 - 2s - loss: 562.7147 - loglik: -5.6091e+02 - logprior: -1.8032e+00
Fitted a model with MAP estimate = -562.3490
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 570.8281 - loglik: -5.6223e+02 - logprior: -8.6016e+00
Epoch 2/10
13/13 - 2s - loss: 564.7557 - loglik: -5.6221e+02 - logprior: -2.5441e+00
Epoch 3/10
13/13 - 2s - loss: 563.3949 - loglik: -5.6218e+02 - logprior: -1.2199e+00
Epoch 4/10
13/13 - 2s - loss: 562.4072 - loglik: -5.6147e+02 - logprior: -9.4006e-01
Epoch 5/10
13/13 - 2s - loss: 561.7948 - loglik: -5.6093e+02 - logprior: -8.6867e-01
Epoch 6/10
13/13 - 2s - loss: 560.8064 - loglik: -5.5993e+02 - logprior: -8.7909e-01
Epoch 7/10
13/13 - 2s - loss: 560.2026 - loglik: -5.5931e+02 - logprior: -8.9244e-01
Epoch 8/10
13/13 - 2s - loss: 561.4540 - loglik: -5.6056e+02 - logprior: -8.9812e-01
Fitted a model with MAP estimate = -560.7512
Time for alignment: 62.7135
Computed alignments with likelihoods: ['-561.1569', '-561.0487', '-561.1153', '-561.0955', '-560.7512']
Best model has likelihood: -560.7512  (prior= -0.8831 )
time for generating output: 0.1784
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/msb.projection.fasta
SP score = 0.92970737913486
Training of 5 independent models on file rnasemam.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52f15788e0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f531570f7f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f155cc40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f59400f1fa0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f023afd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f023a5e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b0a400670>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5cc0d6c310>, <__main__.SimpleDirichletPrior object at 0x7f52f10bb2b0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 812.5311 - loglik: -7.1352e+02 - logprior: -9.9011e+01
Epoch 2/10
10/10 - 1s - loss: 697.1877 - loglik: -6.7441e+02 - logprior: -2.2780e+01
Epoch 3/10
10/10 - 1s - loss: 643.5933 - loglik: -6.3465e+02 - logprior: -8.9473e+00
Epoch 4/10
10/10 - 1s - loss: 613.5065 - loglik: -6.0869e+02 - logprior: -4.8156e+00
Epoch 5/10
10/10 - 1s - loss: 600.2603 - loglik: -5.9787e+02 - logprior: -2.3902e+00
Epoch 6/10
10/10 - 1s - loss: 594.1700 - loglik: -5.9313e+02 - logprior: -1.0412e+00
Epoch 7/10
10/10 - 1s - loss: 591.2325 - loglik: -5.9104e+02 - logprior: -1.9200e-01
Epoch 8/10
10/10 - 1s - loss: 589.7738 - loglik: -5.9020e+02 - logprior: 0.4297
Epoch 9/10
10/10 - 1s - loss: 588.7308 - loglik: -5.8957e+02 - logprior: 0.8430
Epoch 10/10
10/10 - 1s - loss: 587.5701 - loglik: -5.8873e+02 - logprior: 1.1635
Fitted a model with MAP estimate = -587.1324
expansions: [(13, 3), (17, 1), (29, 2), (30, 1), (41, 3), (57, 3), (61, 1), (62, 1), (63, 1), (68, 1), (79, 2), (80, 2), (81, 2), (82, 2), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 686.3960 - loglik: -5.8629e+02 - logprior: -1.0010e+02
Epoch 2/2
10/10 - 2s - loss: 607.6986 - loglik: -5.6980e+02 - logprior: -3.7902e+01
Fitted a model with MAP estimate = -593.6213
expansions: [(0, 2), (16, 1)]
discards: [  0  32  48  96 101 104 114 115]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 640.4661 - loglik: -5.6483e+02 - logprior: -7.5634e+01
Epoch 2/2
10/10 - 2s - loss: 575.9081 - loglik: -5.5999e+02 - logprior: -1.5921e+01
Fitted a model with MAP estimate = -566.0316
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 652.7219 - loglik: -5.6268e+02 - logprior: -9.0037e+01
Epoch 2/10
10/10 - 2s - loss: 583.9166 - loglik: -5.6100e+02 - logprior: -2.2916e+01
Epoch 3/10
10/10 - 2s - loss: 564.4829 - loglik: -5.6006e+02 - logprior: -4.4268e+00
Epoch 4/10
10/10 - 2s - loss: 557.3913 - loglik: -5.5948e+02 - logprior: 2.0854
Epoch 5/10
10/10 - 2s - loss: 554.0651 - loglik: -5.5927e+02 - logprior: 5.2052
Epoch 6/10
10/10 - 2s - loss: 552.2570 - loglik: -5.5924e+02 - logprior: 6.9847
Epoch 7/10
10/10 - 2s - loss: 551.1365 - loglik: -5.5930e+02 - logprior: 8.1600
Epoch 8/10
10/10 - 2s - loss: 550.3380 - loglik: -5.5944e+02 - logprior: 9.1055
Epoch 9/10
10/10 - 1s - loss: 549.6924 - loglik: -5.5961e+02 - logprior: 9.9174
Epoch 10/10
10/10 - 2s - loss: 549.1259 - loglik: -5.5973e+02 - logprior: 10.6051
Fitted a model with MAP estimate = -548.8327
Time for alignment: 54.9438
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 812.5311 - loglik: -7.1352e+02 - logprior: -9.9011e+01
Epoch 2/10
10/10 - 1s - loss: 697.1877 - loglik: -6.7441e+02 - logprior: -2.2780e+01
Epoch 3/10
10/10 - 1s - loss: 643.5934 - loglik: -6.3465e+02 - logprior: -8.9473e+00
Epoch 4/10
10/10 - 1s - loss: 613.5065 - loglik: -6.0869e+02 - logprior: -4.8156e+00
Epoch 5/10
10/10 - 1s - loss: 600.2602 - loglik: -5.9787e+02 - logprior: -2.3902e+00
Epoch 6/10
10/10 - 1s - loss: 594.1700 - loglik: -5.9313e+02 - logprior: -1.0412e+00
Epoch 7/10
10/10 - 1s - loss: 591.2325 - loglik: -5.9104e+02 - logprior: -1.9199e-01
Epoch 8/10
10/10 - 1s - loss: 589.7739 - loglik: -5.9020e+02 - logprior: 0.4297
Epoch 9/10
10/10 - 1s - loss: 588.7308 - loglik: -5.8957e+02 - logprior: 0.8430
Epoch 10/10
10/10 - 1s - loss: 587.5702 - loglik: -5.8873e+02 - logprior: 1.1635
Fitted a model with MAP estimate = -587.1323
expansions: [(13, 3), (17, 1), (29, 2), (30, 1), (41, 3), (57, 3), (61, 1), (62, 1), (63, 1), (68, 1), (79, 2), (80, 2), (81, 2), (82, 2), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 686.3961 - loglik: -5.8629e+02 - logprior: -1.0010e+02
Epoch 2/2
10/10 - 2s - loss: 607.6986 - loglik: -5.6980e+02 - logprior: -3.7902e+01
Fitted a model with MAP estimate = -593.6213
expansions: [(0, 2), (16, 1)]
discards: [  0  32  48  96 101 104 114 115]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 640.4661 - loglik: -5.6483e+02 - logprior: -7.5634e+01
Epoch 2/2
10/10 - 2s - loss: 575.9081 - loglik: -5.5999e+02 - logprior: -1.5921e+01
Fitted a model with MAP estimate = -566.0316
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 652.7218 - loglik: -5.6268e+02 - logprior: -9.0037e+01
Epoch 2/10
10/10 - 2s - loss: 583.9166 - loglik: -5.6100e+02 - logprior: -2.2916e+01
Epoch 3/10
10/10 - 2s - loss: 564.4829 - loglik: -5.6006e+02 - logprior: -4.4268e+00
Epoch 4/10
10/10 - 2s - loss: 557.3913 - loglik: -5.5948e+02 - logprior: 2.0854
Epoch 5/10
10/10 - 2s - loss: 554.0651 - loglik: -5.5927e+02 - logprior: 5.2052
Epoch 6/10
10/10 - 2s - loss: 552.2570 - loglik: -5.5924e+02 - logprior: 6.9847
Epoch 7/10
10/10 - 1s - loss: 551.1364 - loglik: -5.5930e+02 - logprior: 8.1601
Epoch 8/10
10/10 - 2s - loss: 550.3379 - loglik: -5.5944e+02 - logprior: 9.1055
Epoch 9/10
10/10 - 2s - loss: 549.6923 - loglik: -5.5961e+02 - logprior: 9.9174
Epoch 10/10
10/10 - 2s - loss: 549.1259 - loglik: -5.5973e+02 - logprior: 10.6052
Fitted a model with MAP estimate = -548.8326
Time for alignment: 52.7390
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 812.5311 - loglik: -7.1352e+02 - logprior: -9.9011e+01
Epoch 2/10
10/10 - 1s - loss: 697.1877 - loglik: -6.7441e+02 - logprior: -2.2780e+01
Epoch 3/10
10/10 - 1s - loss: 643.5934 - loglik: -6.3465e+02 - logprior: -8.9473e+00
Epoch 4/10
10/10 - 1s - loss: 613.5065 - loglik: -6.0869e+02 - logprior: -4.8156e+00
Epoch 5/10
10/10 - 1s - loss: 600.2601 - loglik: -5.9787e+02 - logprior: -2.3902e+00
Epoch 6/10
10/10 - 1s - loss: 594.1700 - loglik: -5.9313e+02 - logprior: -1.0412e+00
Epoch 7/10
10/10 - 1s - loss: 591.2325 - loglik: -5.9104e+02 - logprior: -1.9200e-01
Epoch 8/10
10/10 - 1s - loss: 589.7738 - loglik: -5.9020e+02 - logprior: 0.4297
Epoch 9/10
10/10 - 1s - loss: 588.7307 - loglik: -5.8957e+02 - logprior: 0.8430
Epoch 10/10
10/10 - 1s - loss: 587.5701 - loglik: -5.8873e+02 - logprior: 1.1635
Fitted a model with MAP estimate = -587.1324
expansions: [(13, 3), (17, 1), (29, 2), (30, 1), (41, 3), (57, 3), (61, 1), (62, 1), (63, 1), (68, 1), (79, 2), (80, 2), (81, 2), (82, 2), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 686.3960 - loglik: -5.8629e+02 - logprior: -1.0010e+02
Epoch 2/2
10/10 - 2s - loss: 607.6986 - loglik: -5.6980e+02 - logprior: -3.7902e+01
Fitted a model with MAP estimate = -593.6213
expansions: [(0, 2), (16, 1)]
discards: [  0  32  48  96 101 104 114 115]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 640.4661 - loglik: -5.6483e+02 - logprior: -7.5634e+01
Epoch 2/2
10/10 - 2s - loss: 575.9081 - loglik: -5.5999e+02 - logprior: -1.5921e+01
Fitted a model with MAP estimate = -566.0316
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 652.7218 - loglik: -5.6268e+02 - logprior: -9.0037e+01
Epoch 2/10
10/10 - 2s - loss: 583.9166 - loglik: -5.6100e+02 - logprior: -2.2916e+01
Epoch 3/10
10/10 - 1s - loss: 564.4829 - loglik: -5.6006e+02 - logprior: -4.4268e+00
Epoch 4/10
10/10 - 1s - loss: 557.3912 - loglik: -5.5948e+02 - logprior: 2.0854
Epoch 5/10
10/10 - 1s - loss: 554.0651 - loglik: -5.5927e+02 - logprior: 5.2052
Epoch 6/10
10/10 - 2s - loss: 552.2570 - loglik: -5.5924e+02 - logprior: 6.9847
Epoch 7/10
10/10 - 2s - loss: 551.1365 - loglik: -5.5930e+02 - logprior: 8.1601
Epoch 8/10
10/10 - 2s - loss: 550.3380 - loglik: -5.5944e+02 - logprior: 9.1055
Epoch 9/10
10/10 - 2s - loss: 549.6923 - loglik: -5.5961e+02 - logprior: 9.9174
Epoch 10/10
10/10 - 1s - loss: 549.1259 - loglik: -5.5973e+02 - logprior: 10.6052
Fitted a model with MAP estimate = -548.8328
Time for alignment: 52.3378
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 812.5311 - loglik: -7.1352e+02 - logprior: -9.9011e+01
Epoch 2/10
10/10 - 1s - loss: 697.1877 - loglik: -6.7441e+02 - logprior: -2.2780e+01
Epoch 3/10
10/10 - 1s - loss: 643.5934 - loglik: -6.3465e+02 - logprior: -8.9473e+00
Epoch 4/10
10/10 - 1s - loss: 613.5065 - loglik: -6.0869e+02 - logprior: -4.8156e+00
Epoch 5/10
10/10 - 1s - loss: 600.2603 - loglik: -5.9787e+02 - logprior: -2.3902e+00
Epoch 6/10
10/10 - 1s - loss: 594.1700 - loglik: -5.9313e+02 - logprior: -1.0412e+00
Epoch 7/10
10/10 - 1s - loss: 591.2325 - loglik: -5.9104e+02 - logprior: -1.9200e-01
Epoch 8/10
10/10 - 1s - loss: 589.7738 - loglik: -5.9020e+02 - logprior: 0.4297
Epoch 9/10
10/10 - 1s - loss: 588.7308 - loglik: -5.8957e+02 - logprior: 0.8430
Epoch 10/10
10/10 - 1s - loss: 587.5702 - loglik: -5.8873e+02 - logprior: 1.1635
Fitted a model with MAP estimate = -587.1325
expansions: [(13, 3), (17, 1), (29, 2), (30, 1), (41, 3), (57, 3), (61, 1), (62, 1), (63, 1), (68, 1), (79, 2), (80, 2), (81, 2), (82, 2), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 686.3960 - loglik: -5.8629e+02 - logprior: -1.0010e+02
Epoch 2/2
10/10 - 2s - loss: 607.6986 - loglik: -5.6980e+02 - logprior: -3.7902e+01
Fitted a model with MAP estimate = -593.6213
expansions: [(0, 2), (16, 1)]
discards: [  0  32  48  96 101 104 114 115]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 640.4661 - loglik: -5.6483e+02 - logprior: -7.5634e+01
Epoch 2/2
10/10 - 2s - loss: 575.9081 - loglik: -5.5999e+02 - logprior: -1.5921e+01
Fitted a model with MAP estimate = -566.0316
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 652.7218 - loglik: -5.6268e+02 - logprior: -9.0037e+01
Epoch 2/10
10/10 - 2s - loss: 583.9166 - loglik: -5.6100e+02 - logprior: -2.2916e+01
Epoch 3/10
10/10 - 2s - loss: 564.4829 - loglik: -5.6006e+02 - logprior: -4.4268e+00
Epoch 4/10
10/10 - 2s - loss: 557.3914 - loglik: -5.5948e+02 - logprior: 2.0854
Epoch 5/10
10/10 - 1s - loss: 554.0651 - loglik: -5.5927e+02 - logprior: 5.2052
Epoch 6/10
10/10 - 2s - loss: 552.2570 - loglik: -5.5924e+02 - logprior: 6.9847
Epoch 7/10
10/10 - 1s - loss: 551.1365 - loglik: -5.5930e+02 - logprior: 8.1601
Epoch 8/10
10/10 - 1s - loss: 550.3380 - loglik: -5.5944e+02 - logprior: 9.1055
Epoch 9/10
10/10 - 2s - loss: 549.6923 - loglik: -5.5961e+02 - logprior: 9.9174
Epoch 10/10
10/10 - 2s - loss: 549.1259 - loglik: -5.5973e+02 - logprior: 10.6052
Fitted a model with MAP estimate = -548.8328
Time for alignment: 55.2398
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 812.5311 - loglik: -7.1352e+02 - logprior: -9.9011e+01
Epoch 2/10
10/10 - 1s - loss: 697.1877 - loglik: -6.7441e+02 - logprior: -2.2780e+01
Epoch 3/10
10/10 - 1s - loss: 643.5933 - loglik: -6.3465e+02 - logprior: -8.9473e+00
Epoch 4/10
10/10 - 1s - loss: 613.5065 - loglik: -6.0869e+02 - logprior: -4.8156e+00
Epoch 5/10
10/10 - 1s - loss: 600.2603 - loglik: -5.9787e+02 - logprior: -2.3902e+00
Epoch 6/10
10/10 - 1s - loss: 594.1700 - loglik: -5.9313e+02 - logprior: -1.0412e+00
Epoch 7/10
10/10 - 1s - loss: 591.2325 - loglik: -5.9104e+02 - logprior: -1.9200e-01
Epoch 8/10
10/10 - 1s - loss: 589.7739 - loglik: -5.9020e+02 - logprior: 0.4297
Epoch 9/10
10/10 - 1s - loss: 588.7307 - loglik: -5.8957e+02 - logprior: 0.8430
Epoch 10/10
10/10 - 1s - loss: 587.5702 - loglik: -5.8873e+02 - logprior: 1.1635
Fitted a model with MAP estimate = -587.1324
expansions: [(13, 3), (17, 1), (29, 2), (30, 1), (41, 3), (57, 3), (61, 1), (62, 1), (63, 1), (68, 1), (79, 2), (80, 2), (81, 2), (82, 2), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 686.3960 - loglik: -5.8629e+02 - logprior: -1.0010e+02
Epoch 2/2
10/10 - 2s - loss: 607.6986 - loglik: -5.6980e+02 - logprior: -3.7902e+01
Fitted a model with MAP estimate = -593.6213
expansions: [(0, 2), (16, 1)]
discards: [  0  32  48  96 101 104 114 115]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 640.4661 - loglik: -5.6483e+02 - logprior: -7.5634e+01
Epoch 2/2
10/10 - 2s - loss: 575.9081 - loglik: -5.5999e+02 - logprior: -1.5921e+01
Fitted a model with MAP estimate = -566.0317
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 652.7217 - loglik: -5.6268e+02 - logprior: -9.0037e+01
Epoch 2/10
10/10 - 2s - loss: 583.9165 - loglik: -5.6100e+02 - logprior: -2.2916e+01
Epoch 3/10
10/10 - 2s - loss: 564.4828 - loglik: -5.6006e+02 - logprior: -4.4268e+00
Epoch 4/10
10/10 - 1s - loss: 557.3912 - loglik: -5.5948e+02 - logprior: 2.0855
Epoch 5/10
10/10 - 2s - loss: 554.0650 - loglik: -5.5927e+02 - logprior: 5.2054
Epoch 6/10
10/10 - 2s - loss: 552.2567 - loglik: -5.5924e+02 - logprior: 6.9849
Epoch 7/10
10/10 - 1s - loss: 551.1363 - loglik: -5.5930e+02 - logprior: 8.1603
Epoch 8/10
10/10 - 2s - loss: 550.3376 - loglik: -5.5944e+02 - logprior: 9.1058
Epoch 9/10
10/10 - 2s - loss: 549.6920 - loglik: -5.5961e+02 - logprior: 9.9177
Epoch 10/10
10/10 - 2s - loss: 549.1255 - loglik: -5.5973e+02 - logprior: 10.6056
Fitted a model with MAP estimate = -548.8322
Time for alignment: 51.9790
Computed alignments with likelihoods: ['-548.8327', '-548.8326', '-548.8328', '-548.8328', '-548.8322']
Best model has likelihood: -548.8322  (prior= 10.9405 )
time for generating output: 0.1772
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rnasemam.projection.fasta
SP score = 0.8901492537313432
Training of 5 independent models on file rub.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52f9191be0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f590076e4f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f155cc40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530c214af0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5cc0d43370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f59002807c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5900280a60>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5aff637e80>, <__main__.SimpleDirichletPrior object at 0x7f530dd91220>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 311.2039 - loglik: -2.7776e+02 - logprior: -3.3440e+01
Epoch 2/10
10/10 - 1s - loss: 266.6342 - loglik: -2.5727e+02 - logprior: -9.3616e+00
Epoch 3/10
10/10 - 1s - loss: 241.1369 - loglik: -2.3608e+02 - logprior: -5.0530e+00
Epoch 4/10
10/10 - 1s - loss: 225.9704 - loglik: -2.2195e+02 - logprior: -4.0247e+00
Epoch 5/10
10/10 - 1s - loss: 221.0948 - loglik: -2.1742e+02 - logprior: -3.6790e+00
Epoch 6/10
10/10 - 1s - loss: 219.5231 - loglik: -2.1642e+02 - logprior: -3.1010e+00
Epoch 7/10
10/10 - 1s - loss: 218.5875 - loglik: -2.1595e+02 - logprior: -2.6340e+00
Epoch 8/10
10/10 - 1s - loss: 217.9304 - loglik: -2.1547e+02 - logprior: -2.4582e+00
Epoch 9/10
10/10 - 1s - loss: 217.5350 - loglik: -2.1517e+02 - logprior: -2.3625e+00
Epoch 10/10
10/10 - 1s - loss: 217.1944 - loglik: -2.1494e+02 - logprior: -2.2517e+00
Fitted a model with MAP estimate = -217.2322
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 1), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 48 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 256.1906 - loglik: -2.1344e+02 - logprior: -4.2749e+01
Epoch 2/2
10/10 - 1s - loss: 219.9616 - loglik: -2.0624e+02 - logprior: -1.3725e+01
Fitted a model with MAP estimate = -212.9153
expansions: []
discards: [32]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 232.9744 - loglik: -2.0314e+02 - logprior: -2.9833e+01
Epoch 2/2
10/10 - 1s - loss: 211.5644 - loglik: -2.0314e+02 - logprior: -8.4287e+00
Fitted a model with MAP estimate = -208.4359
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 230.9222 - loglik: -2.0271e+02 - logprior: -2.8213e+01
Epoch 2/10
10/10 - 1s - loss: 210.7908 - loglik: -2.0281e+02 - logprior: -7.9767e+00
Epoch 3/10
10/10 - 1s - loss: 207.1693 - loglik: -2.0319e+02 - logprior: -3.9800e+00
Epoch 4/10
10/10 - 1s - loss: 205.8523 - loglik: -2.0339e+02 - logprior: -2.4580e+00
Epoch 5/10
10/10 - 1s - loss: 205.2509 - loglik: -2.0352e+02 - logprior: -1.7319e+00
Epoch 6/10
10/10 - 1s - loss: 204.7843 - loglik: -2.0341e+02 - logprior: -1.3712e+00
Epoch 7/10
10/10 - 1s - loss: 204.7222 - loglik: -2.0359e+02 - logprior: -1.1353e+00
Epoch 8/10
10/10 - 1s - loss: 204.6751 - loglik: -2.0375e+02 - logprior: -9.2931e-01
Epoch 9/10
10/10 - 1s - loss: 204.4579 - loglik: -2.0369e+02 - logprior: -7.7235e-01
Epoch 10/10
10/10 - 1s - loss: 204.3648 - loglik: -2.0369e+02 - logprior: -6.7191e-01
Fitted a model with MAP estimate = -204.3553
Time for alignment: 31.5655
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 311.1764 - loglik: -2.7774e+02 - logprior: -3.3439e+01
Epoch 2/10
10/10 - 1s - loss: 266.4469 - loglik: -2.5709e+02 - logprior: -9.3585e+00
Epoch 3/10
10/10 - 1s - loss: 240.5517 - loglik: -2.3549e+02 - logprior: -5.0578e+00
Epoch 4/10
10/10 - 1s - loss: 225.2977 - loglik: -2.2127e+02 - logprior: -4.0240e+00
Epoch 5/10
10/10 - 1s - loss: 220.5557 - loglik: -2.1693e+02 - logprior: -3.6306e+00
Epoch 6/10
10/10 - 1s - loss: 218.8720 - loglik: -2.1583e+02 - logprior: -3.0389e+00
Epoch 7/10
10/10 - 1s - loss: 218.0999 - loglik: -2.1549e+02 - logprior: -2.6135e+00
Epoch 8/10
10/10 - 1s - loss: 217.7409 - loglik: -2.1528e+02 - logprior: -2.4564e+00
Epoch 9/10
10/10 - 1s - loss: 217.3818 - loglik: -2.1503e+02 - logprior: -2.3545e+00
Epoch 10/10
10/10 - 1s - loss: 217.2796 - loglik: -2.1504e+02 - logprior: -2.2433e+00
Fitted a model with MAP estimate = -217.1899
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 1), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 48 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 256.1966 - loglik: -2.1341e+02 - logprior: -4.2788e+01
Epoch 2/2
10/10 - 1s - loss: 219.8513 - loglik: -2.0611e+02 - logprior: -1.3739e+01
Fitted a model with MAP estimate = -212.9366
expansions: []
discards: [32]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 233.1196 - loglik: -2.0328e+02 - logprior: -2.9839e+01
Epoch 2/2
10/10 - 1s - loss: 211.3606 - loglik: -2.0293e+02 - logprior: -8.4303e+00
Fitted a model with MAP estimate = -208.4297
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 230.8500 - loglik: -2.0264e+02 - logprior: -2.8214e+01
Epoch 2/10
10/10 - 1s - loss: 210.8860 - loglik: -2.0291e+02 - logprior: -7.9777e+00
Epoch 3/10
10/10 - 1s - loss: 207.1691 - loglik: -2.0319e+02 - logprior: -3.9768e+00
Epoch 4/10
10/10 - 1s - loss: 205.8403 - loglik: -2.0338e+02 - logprior: -2.4588e+00
Epoch 5/10
10/10 - 1s - loss: 205.2767 - loglik: -2.0355e+02 - logprior: -1.7299e+00
Epoch 6/10
10/10 - 1s - loss: 204.7413 - loglik: -2.0337e+02 - logprior: -1.3732e+00
Epoch 7/10
10/10 - 1s - loss: 204.7608 - loglik: -2.0363e+02 - logprior: -1.1319e+00
Fitted a model with MAP estimate = -204.6103
Time for alignment: 28.7957
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 311.2961 - loglik: -2.7786e+02 - logprior: -3.3440e+01
Epoch 2/10
10/10 - 1s - loss: 266.2223 - loglik: -2.5686e+02 - logprior: -9.3642e+00
Epoch 3/10
10/10 - 1s - loss: 240.4033 - loglik: -2.3534e+02 - logprior: -5.0637e+00
Epoch 4/10
10/10 - 1s - loss: 225.5600 - loglik: -2.2154e+02 - logprior: -4.0211e+00
Epoch 5/10
10/10 - 1s - loss: 220.4879 - loglik: -2.1687e+02 - logprior: -3.6211e+00
Epoch 6/10
10/10 - 1s - loss: 218.7424 - loglik: -2.1571e+02 - logprior: -3.0340e+00
Epoch 7/10
10/10 - 1s - loss: 218.1435 - loglik: -2.1553e+02 - logprior: -2.6153e+00
Epoch 8/10
10/10 - 1s - loss: 217.7666 - loglik: -2.1530e+02 - logprior: -2.4655e+00
Epoch 9/10
10/10 - 1s - loss: 217.2921 - loglik: -2.1494e+02 - logprior: -2.3521e+00
Epoch 10/10
10/10 - 1s - loss: 217.1911 - loglik: -2.1495e+02 - logprior: -2.2422e+00
Fitted a model with MAP estimate = -217.1920
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 1), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 48 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 256.2106 - loglik: -2.1342e+02 - logprior: -4.2787e+01
Epoch 2/2
10/10 - 1s - loss: 219.9168 - loglik: -2.0618e+02 - logprior: -1.3741e+01
Fitted a model with MAP estimate = -212.9221
expansions: []
discards: [32]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 232.9954 - loglik: -2.0316e+02 - logprior: -2.9838e+01
Epoch 2/2
10/10 - 1s - loss: 211.4768 - loglik: -2.0305e+02 - logprior: -8.4258e+00
Fitted a model with MAP estimate = -208.4312
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 230.7962 - loglik: -2.0258e+02 - logprior: -2.8213e+01
Epoch 2/10
10/10 - 1s - loss: 210.8625 - loglik: -2.0289e+02 - logprior: -7.9766e+00
Epoch 3/10
10/10 - 1s - loss: 207.2285 - loglik: -2.0325e+02 - logprior: -3.9757e+00
Epoch 4/10
10/10 - 1s - loss: 205.8726 - loglik: -2.0341e+02 - logprior: -2.4616e+00
Epoch 5/10
10/10 - 1s - loss: 205.2175 - loglik: -2.0349e+02 - logprior: -1.7291e+00
Epoch 6/10
10/10 - 1s - loss: 204.8088 - loglik: -2.0343e+02 - logprior: -1.3757e+00
Epoch 7/10
10/10 - 1s - loss: 204.7062 - loglik: -2.0357e+02 - logprior: -1.1339e+00
Epoch 8/10
10/10 - 1s - loss: 204.6495 - loglik: -2.0372e+02 - logprior: -9.3341e-01
Epoch 9/10
10/10 - 1s - loss: 204.4324 - loglik: -2.0366e+02 - logprior: -7.6766e-01
Epoch 10/10
10/10 - 1s - loss: 204.3874 - loglik: -2.0371e+02 - logprior: -6.8116e-01
Fitted a model with MAP estimate = -204.3548
Time for alignment: 30.3698
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 311.3700 - loglik: -2.7793e+02 - logprior: -3.3439e+01
Epoch 2/10
10/10 - 1s - loss: 266.2226 - loglik: -2.5686e+02 - logprior: -9.3660e+00
Epoch 3/10
10/10 - 1s - loss: 240.5690 - loglik: -2.3549e+02 - logprior: -5.0758e+00
Epoch 4/10
10/10 - 1s - loss: 225.6311 - loglik: -2.2160e+02 - logprior: -4.0359e+00
Epoch 5/10
10/10 - 1s - loss: 220.5380 - loglik: -2.1691e+02 - logprior: -3.6307e+00
Epoch 6/10
10/10 - 1s - loss: 218.9479 - loglik: -2.1591e+02 - logprior: -3.0341e+00
Epoch 7/10
10/10 - 1s - loss: 218.0998 - loglik: -2.1549e+02 - logprior: -2.6144e+00
Epoch 8/10
10/10 - 1s - loss: 217.6641 - loglik: -2.1520e+02 - logprior: -2.4605e+00
Epoch 9/10
10/10 - 1s - loss: 217.4716 - loglik: -2.1512e+02 - logprior: -2.3538e+00
Epoch 10/10
10/10 - 1s - loss: 217.2168 - loglik: -2.1498e+02 - logprior: -2.2391e+00
Fitted a model with MAP estimate = -217.1952
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 1), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 48 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 256.1789 - loglik: -2.1340e+02 - logprior: -4.2778e+01
Epoch 2/2
10/10 - 1s - loss: 219.8980 - loglik: -2.0616e+02 - logprior: -1.3740e+01
Fitted a model with MAP estimate = -212.9242
expansions: []
discards: [32]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 233.1209 - loglik: -2.0329e+02 - logprior: -2.9835e+01
Epoch 2/2
10/10 - 1s - loss: 211.3087 - loglik: -2.0288e+02 - logprior: -8.4323e+00
Fitted a model with MAP estimate = -208.4311
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 230.9193 - loglik: -2.0271e+02 - logprior: -2.8208e+01
Epoch 2/10
10/10 - 1s - loss: 210.7835 - loglik: -2.0280e+02 - logprior: -7.9823e+00
Epoch 3/10
10/10 - 1s - loss: 207.1890 - loglik: -2.0321e+02 - logprior: -3.9759e+00
Epoch 4/10
10/10 - 1s - loss: 205.8479 - loglik: -2.0339e+02 - logprior: -2.4567e+00
Epoch 5/10
10/10 - 1s - loss: 205.2082 - loglik: -2.0348e+02 - logprior: -1.7322e+00
Epoch 6/10
10/10 - 1s - loss: 204.8615 - loglik: -2.0349e+02 - logprior: -1.3733e+00
Epoch 7/10
10/10 - 1s - loss: 204.7289 - loglik: -2.0360e+02 - logprior: -1.1331e+00
Epoch 8/10
10/10 - 1s - loss: 204.4540 - loglik: -2.0352e+02 - logprior: -9.2981e-01
Epoch 9/10
10/10 - 1s - loss: 204.6163 - loglik: -2.0385e+02 - logprior: -7.6904e-01
Fitted a model with MAP estimate = -204.4338
Time for alignment: 30.1616
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 311.1378 - loglik: -2.7770e+02 - logprior: -3.3440e+01
Epoch 2/10
10/10 - 1s - loss: 266.6348 - loglik: -2.5727e+02 - logprior: -9.3659e+00
Epoch 3/10
10/10 - 1s - loss: 240.9163 - loglik: -2.3585e+02 - logprior: -5.0663e+00
Epoch 4/10
10/10 - 1s - loss: 225.9809 - loglik: -2.2197e+02 - logprior: -4.0145e+00
Epoch 5/10
10/10 - 1s - loss: 220.9111 - loglik: -2.1726e+02 - logprior: -3.6492e+00
Epoch 6/10
10/10 - 1s - loss: 219.2349 - loglik: -2.1618e+02 - logprior: -3.0555e+00
Epoch 7/10
10/10 - 1s - loss: 218.1407 - loglik: -2.1552e+02 - logprior: -2.6196e+00
Epoch 8/10
10/10 - 1s - loss: 217.6875 - loglik: -2.1523e+02 - logprior: -2.4584e+00
Epoch 9/10
10/10 - 1s - loss: 217.5593 - loglik: -2.1520e+02 - logprior: -2.3555e+00
Epoch 10/10
10/10 - 1s - loss: 217.3136 - loglik: -2.1507e+02 - logprior: -2.2423e+00
Fitted a model with MAP estimate = -217.2067
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 1), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 48 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 256.1586 - loglik: -2.1339e+02 - logprior: -4.2772e+01
Epoch 2/2
10/10 - 1s - loss: 219.9347 - loglik: -2.0620e+02 - logprior: -1.3730e+01
Fitted a model with MAP estimate = -212.9302
expansions: []
discards: [32]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 233.0344 - loglik: -2.0320e+02 - logprior: -2.9830e+01
Epoch 2/2
10/10 - 1s - loss: 211.4309 - loglik: -2.0300e+02 - logprior: -8.4327e+00
Fitted a model with MAP estimate = -208.4335
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 230.6750 - loglik: -2.0247e+02 - logprior: -2.8209e+01
Epoch 2/10
10/10 - 1s - loss: 211.0082 - loglik: -2.0302e+02 - logprior: -7.9862e+00
Epoch 3/10
10/10 - 1s - loss: 207.3053 - loglik: -2.0334e+02 - logprior: -3.9697e+00
Epoch 4/10
10/10 - 1s - loss: 205.7688 - loglik: -2.0331e+02 - logprior: -2.4624e+00
Epoch 5/10
10/10 - 1s - loss: 205.1436 - loglik: -2.0342e+02 - logprior: -1.7270e+00
Epoch 6/10
10/10 - 1s - loss: 205.0309 - loglik: -2.0366e+02 - logprior: -1.3701e+00
Epoch 7/10
10/10 - 1s - loss: 204.5418 - loglik: -2.0341e+02 - logprior: -1.1351e+00
Epoch 8/10
10/10 - 1s - loss: 204.6347 - loglik: -2.0371e+02 - logprior: -9.2595e-01
Fitted a model with MAP estimate = -204.5126
Time for alignment: 30.7001
Computed alignments with likelihoods: ['-204.3553', '-204.6103', '-204.3548', '-204.4338', '-204.5126']
Best model has likelihood: -204.3548  (prior= -0.6346 )
time for generating output: 0.1177
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rub.projection.fasta
SP score = 0.9918367346938776
Training of 5 independent models on file cyclo.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52f1880f10>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52f18b3f40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52e8377550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52e83770a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530dd72eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530dc14fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530c214af0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52f9191be0>, <__main__.SimpleDirichletPrior object at 0x7f5b135346d0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 911.4254 - loglik: -9.0503e+02 - logprior: -6.3970e+00
Epoch 2/10
15/15 - 4s - loss: 832.2844 - loglik: -8.3085e+02 - logprior: -1.4394e+00
Epoch 3/10
15/15 - 4s - loss: 789.6126 - loglik: -7.8805e+02 - logprior: -1.5607e+00
Epoch 4/10
15/15 - 4s - loss: 776.7460 - loglik: -7.7514e+02 - logprior: -1.6048e+00
Epoch 5/10
15/15 - 4s - loss: 771.7510 - loglik: -7.7023e+02 - logprior: -1.5255e+00
Epoch 6/10
15/15 - 4s - loss: 766.8441 - loglik: -7.6527e+02 - logprior: -1.5760e+00
Epoch 7/10
15/15 - 4s - loss: 766.4566 - loglik: -7.6487e+02 - logprior: -1.5831e+00
Epoch 8/10
15/15 - 4s - loss: 764.6331 - loglik: -7.6306e+02 - logprior: -1.5762e+00
Epoch 9/10
15/15 - 4s - loss: 763.1364 - loglik: -7.6155e+02 - logprior: -1.5852e+00
Epoch 10/10
15/15 - 4s - loss: 763.6886 - loglik: -7.6211e+02 - logprior: -1.5771e+00
Fitted a model with MAP estimate = -763.2220
expansions: [(7, 3), (12, 1), (16, 1), (24, 2), (25, 1), (49, 2), (55, 1), (59, 1), (64, 2), (65, 2), (69, 1), (91, 1), (104, 1), (110, 1), (112, 1), (114, 3), (116, 3), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 767.0408 - loglik: -7.6019e+02 - logprior: -6.8532e+00
Epoch 2/2
15/15 - 5s - loss: 750.0350 - loglik: -7.4697e+02 - logprior: -3.0692e+00
Fitted a model with MAP estimate = -748.4987
expansions: [(0, 2)]
discards: [ 0  7 57 77 84]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 753.4772 - loglik: -7.4850e+02 - logprior: -4.9779e+00
Epoch 2/2
15/15 - 5s - loss: 746.7330 - loglik: -7.4529e+02 - logprior: -1.4381e+00
Fitted a model with MAP estimate = -745.7568
expansions: []
discards: [ 0 65 66]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 11s - loss: 757.3110 - loglik: -7.5089e+02 - logprior: -6.4193e+00
Epoch 2/10
15/15 - 5s - loss: 750.8348 - loglik: -7.4884e+02 - logprior: -1.9930e+00
Epoch 3/10
15/15 - 5s - loss: 750.1251 - loglik: -7.4918e+02 - logprior: -9.4164e-01
Epoch 4/10
15/15 - 5s - loss: 747.0594 - loglik: -7.4628e+02 - logprior: -7.8415e-01
Epoch 5/10
15/15 - 5s - loss: 746.6755 - loglik: -7.4596e+02 - logprior: -7.1599e-01
Epoch 6/10
15/15 - 5s - loss: 746.2006 - loglik: -7.4551e+02 - logprior: -6.8816e-01
Epoch 7/10
15/15 - 5s - loss: 744.9850 - loglik: -7.4435e+02 - logprior: -6.3966e-01
Epoch 8/10
15/15 - 5s - loss: 746.4987 - loglik: -7.4590e+02 - logprior: -5.9441e-01
Fitted a model with MAP estimate = -745.4188
Time for alignment: 137.2308
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 911.4481 - loglik: -9.0507e+02 - logprior: -6.3806e+00
Epoch 2/10
15/15 - 4s - loss: 831.5923 - loglik: -8.3017e+02 - logprior: -1.4238e+00
Epoch 3/10
15/15 - 4s - loss: 783.8193 - loglik: -7.8228e+02 - logprior: -1.5422e+00
Epoch 4/10
15/15 - 4s - loss: 770.9669 - loglik: -7.6933e+02 - logprior: -1.6355e+00
Epoch 5/10
15/15 - 4s - loss: 766.9821 - loglik: -7.6544e+02 - logprior: -1.5396e+00
Epoch 6/10
15/15 - 4s - loss: 764.0129 - loglik: -7.6245e+02 - logprior: -1.5635e+00
Epoch 7/10
15/15 - 4s - loss: 764.7103 - loglik: -7.6315e+02 - logprior: -1.5646e+00
Fitted a model with MAP estimate = -763.2892
expansions: [(8, 2), (11, 1), (15, 1), (16, 1), (24, 2), (27, 1), (50, 1), (53, 1), (55, 1), (59, 1), (66, 2), (69, 1), (91, 1), (92, 1), (112, 1), (114, 2), (115, 2), (116, 4), (117, 1), (118, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 155 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 767.2059 - loglik: -7.6039e+02 - logprior: -6.8112e+00
Epoch 2/2
15/15 - 5s - loss: 751.7543 - loglik: -7.4876e+02 - logprior: -2.9914e+00
Fitted a model with MAP estimate = -750.1652
expansions: [(0, 2)]
discards: [  0   7 137]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 754.2775 - loglik: -7.4931e+02 - logprior: -4.9677e+00
Epoch 2/2
15/15 - 5s - loss: 747.4138 - loglik: -7.4601e+02 - logprior: -1.4032e+00
Fitted a model with MAP estimate = -747.3211
expansions: []
discards: [  0  67  82 138]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 759.2426 - loglik: -7.5282e+02 - logprior: -6.4234e+00
Epoch 2/10
15/15 - 5s - loss: 751.2838 - loglik: -7.4926e+02 - logprior: -2.0233e+00
Epoch 3/10
15/15 - 5s - loss: 750.0864 - loglik: -7.4917e+02 - logprior: -9.2149e-01
Epoch 4/10
15/15 - 5s - loss: 748.2773 - loglik: -7.4750e+02 - logprior: -7.7900e-01
Epoch 5/10
15/15 - 5s - loss: 747.3718 - loglik: -7.4661e+02 - logprior: -7.6218e-01
Epoch 6/10
15/15 - 5s - loss: 745.7247 - loglik: -7.4500e+02 - logprior: -7.2963e-01
Epoch 7/10
15/15 - 5s - loss: 747.0531 - loglik: -7.4638e+02 - logprior: -6.7814e-01
Fitted a model with MAP estimate = -745.4738
Time for alignment: 119.9631
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 910.6794 - loglik: -9.0430e+02 - logprior: -6.3824e+00
Epoch 2/10
15/15 - 4s - loss: 832.4211 - loglik: -8.3100e+02 - logprior: -1.4219e+00
Epoch 3/10
15/15 - 4s - loss: 785.1864 - loglik: -7.8362e+02 - logprior: -1.5694e+00
Epoch 4/10
15/15 - 4s - loss: 772.1165 - loglik: -7.7037e+02 - logprior: -1.7477e+00
Epoch 5/10
15/15 - 4s - loss: 767.6677 - loglik: -7.6606e+02 - logprior: -1.6034e+00
Epoch 6/10
15/15 - 4s - loss: 765.5322 - loglik: -7.6396e+02 - logprior: -1.5756e+00
Epoch 7/10
15/15 - 4s - loss: 765.8136 - loglik: -7.6426e+02 - logprior: -1.5495e+00
Fitted a model with MAP estimate = -765.2190
expansions: [(7, 3), (10, 1), (16, 1), (24, 2), (25, 1), (49, 2), (60, 1), (66, 3), (69, 1), (91, 1), (92, 2), (108, 1), (112, 1), (114, 2), (115, 1), (116, 4), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 10s - loss: 770.9603 - loglik: -7.6411e+02 - logprior: -6.8528e+00
Epoch 2/2
15/15 - 5s - loss: 753.7439 - loglik: -7.5065e+02 - logprior: -3.0915e+00
Fitted a model with MAP estimate = -752.3018
expansions: [(0, 2)]
discards: [  0   7  57  76 107 134]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 756.4838 - loglik: -7.5148e+02 - logprior: -5.0012e+00
Epoch 2/2
15/15 - 5s - loss: 749.9050 - loglik: -7.4843e+02 - logprior: -1.4734e+00
Fitted a model with MAP estimate = -749.0724
expansions: []
discards: [ 0 80]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 758.7087 - loglik: -7.5224e+02 - logprior: -6.4726e+00
Epoch 2/10
15/15 - 5s - loss: 751.7294 - loglik: -7.4969e+02 - logprior: -2.0443e+00
Epoch 3/10
15/15 - 5s - loss: 749.6512 - loglik: -7.4866e+02 - logprior: -9.8640e-01
Epoch 4/10
15/15 - 5s - loss: 748.1824 - loglik: -7.4734e+02 - logprior: -8.4141e-01
Epoch 5/10
15/15 - 5s - loss: 748.8295 - loglik: -7.4803e+02 - logprior: -7.9867e-01
Fitted a model with MAP estimate = -747.2739
Time for alignment: 110.1721
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 911.0493 - loglik: -9.0466e+02 - logprior: -6.3869e+00
Epoch 2/10
15/15 - 4s - loss: 831.8594 - loglik: -8.3042e+02 - logprior: -1.4362e+00
Epoch 3/10
15/15 - 4s - loss: 783.8895 - loglik: -7.8231e+02 - logprior: -1.5842e+00
Epoch 4/10
15/15 - 4s - loss: 771.5629 - loglik: -7.6992e+02 - logprior: -1.6406e+00
Epoch 5/10
15/15 - 4s - loss: 766.4077 - loglik: -7.6490e+02 - logprior: -1.5047e+00
Epoch 6/10
15/15 - 4s - loss: 765.1852 - loglik: -7.6362e+02 - logprior: -1.5634e+00
Epoch 7/10
15/15 - 4s - loss: 761.9753 - loglik: -7.6041e+02 - logprior: -1.5605e+00
Epoch 8/10
15/15 - 4s - loss: 762.5053 - loglik: -7.6097e+02 - logprior: -1.5324e+00
Fitted a model with MAP estimate = -762.5431
expansions: [(7, 3), (10, 1), (16, 1), (24, 2), (27, 1), (53, 1), (55, 1), (59, 1), (66, 2), (67, 1), (69, 1), (91, 1), (92, 2), (112, 1), (114, 2), (115, 2), (116, 4), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 765.0387 - loglik: -7.5820e+02 - logprior: -6.8429e+00
Epoch 2/2
15/15 - 5s - loss: 752.2510 - loglik: -7.4919e+02 - logprior: -3.0620e+00
Fitted a model with MAP estimate = -749.2358
expansions: [(0, 2)]
discards: [  0   7  77  82 107 138 139]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 753.4776 - loglik: -7.4851e+02 - logprior: -4.9631e+00
Epoch 2/2
15/15 - 5s - loss: 748.6848 - loglik: -7.4728e+02 - logprior: -1.4079e+00
Fitted a model with MAP estimate = -747.3085
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 10s - loss: 754.8903 - loglik: -7.4847e+02 - logprior: -6.4213e+00
Epoch 2/10
15/15 - 5s - loss: 751.0087 - loglik: -7.4901e+02 - logprior: -1.9961e+00
Epoch 3/10
15/15 - 5s - loss: 746.8488 - loglik: -7.4593e+02 - logprior: -9.1382e-01
Epoch 4/10
15/15 - 5s - loss: 747.1318 - loglik: -7.4637e+02 - logprior: -7.6575e-01
Fitted a model with MAP estimate = -745.8605
Time for alignment: 111.0518
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 911.7540 - loglik: -9.0537e+02 - logprior: -6.3854e+00
Epoch 2/10
15/15 - 4s - loss: 831.6894 - loglik: -8.3027e+02 - logprior: -1.4152e+00
Epoch 3/10
15/15 - 4s - loss: 782.1214 - loglik: -7.8055e+02 - logprior: -1.5726e+00
Epoch 4/10
15/15 - 4s - loss: 770.4487 - loglik: -7.6873e+02 - logprior: -1.7218e+00
Epoch 5/10
15/15 - 4s - loss: 766.9065 - loglik: -7.6537e+02 - logprior: -1.5367e+00
Epoch 6/10
15/15 - 4s - loss: 765.8092 - loglik: -7.6430e+02 - logprior: -1.5081e+00
Epoch 7/10
15/15 - 4s - loss: 765.0465 - loglik: -7.6356e+02 - logprior: -1.4852e+00
Epoch 8/10
15/15 - 4s - loss: 763.9765 - loglik: -7.6252e+02 - logprior: -1.4536e+00
Epoch 9/10
15/15 - 4s - loss: 764.0381 - loglik: -7.6260e+02 - logprior: -1.4414e+00
Fitted a model with MAP estimate = -763.6649
expansions: [(5, 1), (7, 2), (10, 1), (16, 1), (24, 2), (25, 1), (26, 1), (60, 1), (65, 3), (69, 1), (91, 1), (92, 2), (112, 1), (114, 2), (115, 2), (116, 3), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 767.2776 - loglik: -7.6045e+02 - logprior: -6.8241e+00
Epoch 2/2
15/15 - 5s - loss: 753.8771 - loglik: -7.5078e+02 - logprior: -3.0946e+00
Fitted a model with MAP estimate = -751.0747
expansions: [(0, 2)]
discards: [  0   7  81 106]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 754.3338 - loglik: -7.4934e+02 - logprior: -4.9951e+00
Epoch 2/2
15/15 - 5s - loss: 750.3410 - loglik: -7.4891e+02 - logprior: -1.4282e+00
Fitted a model with MAP estimate = -748.3244
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 10s - loss: 757.7704 - loglik: -7.5131e+02 - logprior: -6.4578e+00
Epoch 2/10
15/15 - 5s - loss: 751.2636 - loglik: -7.4924e+02 - logprior: -2.0221e+00
Epoch 3/10
15/15 - 5s - loss: 747.6890 - loglik: -7.4675e+02 - logprior: -9.4256e-01
Epoch 4/10
15/15 - 5s - loss: 747.6892 - loglik: -7.4690e+02 - logprior: -7.9128e-01
Fitted a model with MAP estimate = -746.6756
Time for alignment: 112.8252
Computed alignments with likelihoods: ['-745.4188', '-745.4738', '-747.2739', '-745.8605', '-746.6756']
Best model has likelihood: -745.4188  (prior= -0.5484 )
time for generating output: 0.2243
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyclo.projection.fasta
SP score = 0.9180602006688964
Training of 5 independent models on file gpdh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5ca789e760>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5314481fa0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5301050eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5900fb7910>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5900fb79d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5900fb7670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5900fb7bb0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52e8377fa0>, <__main__.SimpleDirichletPrior object at 0x7f52e06a9eb0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 9s - loss: 671.7553 - loglik: -6.6846e+02 - logprior: -3.2932e+00
Epoch 2/10
34/34 - 6s - loss: 580.7962 - loglik: -5.7883e+02 - logprior: -1.9613e+00
Epoch 3/10
34/34 - 6s - loss: 574.6646 - loglik: -5.7277e+02 - logprior: -1.8988e+00
Epoch 4/10
34/34 - 5s - loss: 572.8967 - loglik: -5.7097e+02 - logprior: -1.9242e+00
Epoch 5/10
34/34 - 7s - loss: 570.5797 - loglik: -5.6866e+02 - logprior: -1.9238e+00
Epoch 6/10
34/34 - 6s - loss: 571.9595 - loglik: -5.7003e+02 - logprior: -1.9342e+00
Fitted a model with MAP estimate = -571.9841
expansions: [(0, 2), (15, 1), (16, 2), (17, 3), (26, 2), (27, 1), (28, 1), (42, 1), (45, 1), (46, 1), (48, 1), (54, 1), (55, 1), (56, 2), (57, 1), (64, 1), (65, 1), (66, 1), (78, 1), (81, 1), (84, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 560.1647 - loglik: -5.5689e+02 - logprior: -3.2714e+00
Epoch 2/2
34/34 - 7s - loss: 550.0562 - loglik: -5.4872e+02 - logprior: -1.3380e+00
Fitted a model with MAP estimate = -549.0275
expansions: []
discards: [ 20  34 140]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 9s - loss: 553.2853 - loglik: -5.5049e+02 - logprior: -2.7978e+00
Epoch 2/2
34/34 - 7s - loss: 551.0173 - loglik: -5.4982e+02 - logprior: -1.1953e+00
Fitted a model with MAP estimate = -549.2605
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 12s - loss: 552.0676 - loglik: -5.4940e+02 - logprior: -2.6643e+00
Epoch 2/10
34/34 - 7s - loss: 550.1826 - loglik: -5.4913e+02 - logprior: -1.0564e+00
Epoch 3/10
34/34 - 7s - loss: 549.1141 - loglik: -5.4818e+02 - logprior: -9.3545e-01
Epoch 4/10
34/34 - 7s - loss: 548.2943 - loglik: -5.4739e+02 - logprior: -9.0837e-01
Epoch 5/10
34/34 - 7s - loss: 550.2061 - loglik: -5.4933e+02 - logprior: -8.7588e-01
Fitted a model with MAP estimate = -547.9271
Time for alignment: 148.1845
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 673.1046 - loglik: -6.6980e+02 - logprior: -3.3030e+00
Epoch 2/10
34/34 - 6s - loss: 582.8412 - loglik: -5.8092e+02 - logprior: -1.9223e+00
Epoch 3/10
34/34 - 6s - loss: 572.6857 - loglik: -5.7076e+02 - logprior: -1.9299e+00
Epoch 4/10
34/34 - 6s - loss: 572.2319 - loglik: -5.7027e+02 - logprior: -1.9635e+00
Epoch 5/10
34/34 - 7s - loss: 574.9528 - loglik: -5.7300e+02 - logprior: -1.9555e+00
Fitted a model with MAP estimate = -572.5678
expansions: [(0, 2), (15, 1), (16, 2), (17, 3), (26, 1), (27, 1), (41, 1), (42, 1), (45, 1), (46, 1), (48, 1), (54, 1), (55, 1), (56, 2), (57, 1), (64, 1), (65, 1), (77, 1), (78, 1), (85, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 150 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 560.5101 - loglik: -5.5726e+02 - logprior: -3.2507e+00
Epoch 2/2
34/34 - 6s - loss: 550.2852 - loglik: -5.4899e+02 - logprior: -1.2964e+00
Fitted a model with MAP estimate = -549.5947
expansions: []
discards: [ 20 139]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 12s - loss: 553.5187 - loglik: -5.5071e+02 - logprior: -2.8042e+00
Epoch 2/2
34/34 - 7s - loss: 550.9880 - loglik: -5.4977e+02 - logprior: -1.2140e+00
Fitted a model with MAP estimate = -549.2398
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 553.5529 - loglik: -5.5087e+02 - logprior: -2.6847e+00
Epoch 2/10
34/34 - 7s - loss: 548.8028 - loglik: -5.4773e+02 - logprior: -1.0737e+00
Epoch 3/10
34/34 - 7s - loss: 549.8407 - loglik: -5.4888e+02 - logprior: -9.6551e-01
Fitted a model with MAP estimate = -548.4243
Time for alignment: 129.6016
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 672.5018 - loglik: -6.6922e+02 - logprior: -3.2837e+00
Epoch 2/10
34/34 - 6s - loss: 577.5972 - loglik: -5.7569e+02 - logprior: -1.9113e+00
Epoch 3/10
34/34 - 6s - loss: 571.9974 - loglik: -5.7011e+02 - logprior: -1.8887e+00
Epoch 4/10
34/34 - 6s - loss: 573.1131 - loglik: -5.7120e+02 - logprior: -1.9097e+00
Fitted a model with MAP estimate = -570.9718
expansions: [(0, 2), (15, 1), (16, 4), (26, 2), (27, 2), (40, 1), (41, 1), (44, 1), (45, 1), (47, 1), (53, 1), (54, 1), (55, 1), (56, 1), (59, 1), (63, 1), (64, 1), (78, 1), (81, 1), (84, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 150 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 11s - loss: 559.8215 - loglik: -5.5659e+02 - logprior: -3.2311e+00
Epoch 2/2
34/34 - 7s - loss: 550.2924 - loglik: -5.4891e+02 - logprior: -1.3776e+00
Fitted a model with MAP estimate = -548.9807
expansions: []
discards: [ 34 139]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 553.3394 - loglik: -5.5053e+02 - logprior: -2.8052e+00
Epoch 2/2
34/34 - 7s - loss: 547.8560 - loglik: -5.4664e+02 - logprior: -1.2149e+00
Fitted a model with MAP estimate = -549.2891
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 552.5909 - loglik: -5.4990e+02 - logprior: -2.6949e+00
Epoch 2/10
34/34 - 6s - loss: 548.2582 - loglik: -5.4719e+02 - logprior: -1.0702e+00
Epoch 3/10
34/34 - 7s - loss: 551.9351 - loglik: -5.5099e+02 - logprior: -9.4770e-01
Fitted a model with MAP estimate = -548.4977
Time for alignment: 122.7608
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 11s - loss: 671.1942 - loglik: -6.6789e+02 - logprior: -3.3051e+00
Epoch 2/10
34/34 - 6s - loss: 579.3335 - loglik: -5.7742e+02 - logprior: -1.9151e+00
Epoch 3/10
34/34 - 6s - loss: 575.8834 - loglik: -5.7404e+02 - logprior: -1.8481e+00
Epoch 4/10
34/34 - 6s - loss: 573.1978 - loglik: -5.7134e+02 - logprior: -1.8599e+00
Epoch 5/10
34/34 - 6s - loss: 570.7070 - loglik: -5.6883e+02 - logprior: -1.8770e+00
Epoch 6/10
34/34 - 6s - loss: 573.9281 - loglik: -5.7206e+02 - logprior: -1.8691e+00
Fitted a model with MAP estimate = -572.1831
expansions: [(0, 2), (15, 1), (16, 4), (26, 2), (27, 2), (40, 1), (41, 1), (44, 1), (48, 1), (49, 1), (55, 1), (56, 2), (57, 1), (60, 1), (63, 1), (64, 1), (78, 1), (81, 1), (84, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 150 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 560.0381 - loglik: -5.5678e+02 - logprior: -3.2565e+00
Epoch 2/2
34/34 - 7s - loss: 549.4250 - loglik: -5.4808e+02 - logprior: -1.3463e+00
Fitted a model with MAP estimate = -549.0850
expansions: []
discards: [ 34 139]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 553.3879 - loglik: -5.5059e+02 - logprior: -2.7945e+00
Epoch 2/2
34/34 - 6s - loss: 551.1409 - loglik: -5.4995e+02 - logprior: -1.1876e+00
Fitted a model with MAP estimate = -549.3237
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 552.7210 - loglik: -5.5005e+02 - logprior: -2.6686e+00
Epoch 2/10
34/34 - 7s - loss: 549.1554 - loglik: -5.4810e+02 - logprior: -1.0512e+00
Epoch 3/10
34/34 - 7s - loss: 548.8982 - loglik: -5.4796e+02 - logprior: -9.3866e-01
Epoch 4/10
34/34 - 7s - loss: 548.5062 - loglik: -5.4761e+02 - logprior: -9.0043e-01
Epoch 5/10
34/34 - 7s - loss: 549.4429 - loglik: -5.4858e+02 - logprior: -8.6074e-01
Fitted a model with MAP estimate = -548.0272
Time for alignment: 149.9791
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 9s - loss: 671.2678 - loglik: -6.6797e+02 - logprior: -3.2964e+00
Epoch 2/10
34/34 - 6s - loss: 583.3943 - loglik: -5.8150e+02 - logprior: -1.8955e+00
Epoch 3/10
34/34 - 6s - loss: 577.2544 - loglik: -5.7540e+02 - logprior: -1.8564e+00
Epoch 4/10
34/34 - 6s - loss: 571.8618 - loglik: -5.6996e+02 - logprior: -1.8988e+00
Epoch 5/10
34/34 - 6s - loss: 572.5208 - loglik: -5.7061e+02 - logprior: -1.9127e+00
Fitted a model with MAP estimate = -572.5807
expansions: [(0, 2), (15, 1), (16, 4), (17, 1), (25, 1), (27, 1), (42, 1), (43, 1), (44, 1), (45, 1), (47, 1), (53, 1), (54, 1), (55, 2), (56, 1), (63, 1), (64, 1), (78, 1), (81, 1), (84, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 9s - loss: 560.5966 - loglik: -5.5739e+02 - logprior: -3.2084e+00
Epoch 2/2
34/34 - 7s - loss: 553.5319 - loglik: -5.5218e+02 - logprior: -1.3540e+00
Fitted a model with MAP estimate = -550.7549
expansions: []
discards: [138]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 554.9398 - loglik: -5.5214e+02 - logprior: -2.7994e+00
Epoch 2/2
34/34 - 7s - loss: 549.1528 - loglik: -5.4795e+02 - logprior: -1.2074e+00
Fitted a model with MAP estimate = -550.5604
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 12s - loss: 554.7153 - loglik: -5.5203e+02 - logprior: -2.6815e+00
Epoch 2/10
34/34 - 6s - loss: 550.2744 - loglik: -5.4920e+02 - logprior: -1.0696e+00
Epoch 3/10
34/34 - 6s - loss: 549.7976 - loglik: -5.4882e+02 - logprior: -9.8106e-01
Epoch 4/10
34/34 - 7s - loss: 550.9929 - loglik: -5.5005e+02 - logprior: -9.4088e-01
Fitted a model with MAP estimate = -549.3135
Time for alignment: 133.8487
Computed alignments with likelihoods: ['-547.9271', '-548.4243', '-548.4977', '-548.0272', '-549.3135']
Best model has likelihood: -547.9271  (prior= -0.8395 )
time for generating output: 0.3589
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gpdh.projection.fasta
SP score = 0.628225893532016
Training of 5 independent models on file sodcu.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b08fcd490>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f530116f6d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52e8241fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52e0a25460>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f590089a910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5c840382b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530d5e5f70>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5900fb7910>, <__main__.SimpleDirichletPrior object at 0x7f52f08ea7c0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 835.3212 - loglik: -8.1073e+02 - logprior: -2.4596e+01
Epoch 2/10
10/10 - 2s - loss: 776.6730 - loglik: -7.7107e+02 - logprior: -5.6045e+00
Epoch 3/10
10/10 - 2s - loss: 732.7677 - loglik: -7.2990e+02 - logprior: -2.8668e+00
Epoch 4/10
10/10 - 2s - loss: 703.0439 - loglik: -7.0075e+02 - logprior: -2.2975e+00
Epoch 5/10
10/10 - 2s - loss: 692.8898 - loglik: -6.9080e+02 - logprior: -2.0924e+00
Epoch 6/10
10/10 - 2s - loss: 688.5213 - loglik: -6.8664e+02 - logprior: -1.8785e+00
Epoch 7/10
10/10 - 2s - loss: 686.6319 - loglik: -6.8491e+02 - logprior: -1.7237e+00
Epoch 8/10
10/10 - 2s - loss: 686.8464 - loglik: -6.8522e+02 - logprior: -1.6230e+00
Fitted a model with MAP estimate = -685.9488
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (23, 1), (24, 4), (26, 1), (53, 1), (55, 2), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (84, 1), (86, 1), (98, 1), (101, 2), (102, 1), (104, 1), (107, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 701.9147 - loglik: -6.8208e+02 - logprior: -1.9835e+01
Epoch 2/2
10/10 - 2s - loss: 676.1155 - loglik: -6.7135e+02 - logprior: -4.7638e+00
Fitted a model with MAP estimate = -671.5698
expansions: []
discards: [ 0 31 67]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 694.7490 - loglik: -6.7137e+02 - logprior: -2.3378e+01
Epoch 2/2
10/10 - 2s - loss: 679.3406 - loglik: -6.7021e+02 - logprior: -9.1336e+00
Fitted a model with MAP estimate = -675.7884
expansions: [(0, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 146 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 687.5393 - loglik: -6.6896e+02 - logprior: -1.8581e+01
Epoch 2/10
10/10 - 2s - loss: 672.2563 - loglik: -6.6819e+02 - logprior: -4.0671e+00
Epoch 3/10
10/10 - 2s - loss: 667.6542 - loglik: -6.6644e+02 - logprior: -1.2162e+00
Epoch 4/10
10/10 - 2s - loss: 666.4512 - loglik: -6.6625e+02 - logprior: -2.0515e-01
Epoch 5/10
10/10 - 2s - loss: 666.0688 - loglik: -6.6634e+02 - logprior: 0.2677
Epoch 6/10
10/10 - 2s - loss: 662.6036 - loglik: -6.6314e+02 - logprior: 0.5386
Epoch 7/10
10/10 - 2s - loss: 663.6563 - loglik: -6.6442e+02 - logprior: 0.7660
Fitted a model with MAP estimate = -663.4919
Time for alignment: 63.6478
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 836.4089 - loglik: -8.1182e+02 - logprior: -2.4593e+01
Epoch 2/10
10/10 - 2s - loss: 776.0711 - loglik: -7.7048e+02 - logprior: -5.5932e+00
Epoch 3/10
10/10 - 2s - loss: 731.7572 - loglik: -7.2890e+02 - logprior: -2.8559e+00
Epoch 4/10
10/10 - 2s - loss: 704.2240 - loglik: -7.0180e+02 - logprior: -2.4269e+00
Epoch 5/10
10/10 - 2s - loss: 692.6605 - loglik: -6.9028e+02 - logprior: -2.3831e+00
Epoch 6/10
10/10 - 2s - loss: 688.2872 - loglik: -6.8606e+02 - logprior: -2.2292e+00
Epoch 7/10
10/10 - 2s - loss: 686.3035 - loglik: -6.8419e+02 - logprior: -2.1145e+00
Epoch 8/10
10/10 - 2s - loss: 686.4437 - loglik: -6.8446e+02 - logprior: -1.9821e+00
Fitted a model with MAP estimate = -685.6681
expansions: [(8, 1), (9, 1), (10, 1), (13, 1), (17, 1), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (55, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (84, 1), (86, 1), (87, 1), (95, 2), (101, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 701.4164 - loglik: -6.8167e+02 - logprior: -1.9748e+01
Epoch 2/2
10/10 - 2s - loss: 675.6793 - loglik: -6.7104e+02 - logprior: -4.6396e+00
Fitted a model with MAP estimate = -670.7836
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 694.3860 - loglik: -6.7093e+02 - logprior: -2.3458e+01
Epoch 2/2
10/10 - 2s - loss: 677.0879 - loglik: -6.6782e+02 - logprior: -9.2692e+00
Fitted a model with MAP estimate = -674.3305
expansions: [(0, 5), (117, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 686.0775 - loglik: -6.6738e+02 - logprior: -1.8696e+01
Epoch 2/10
10/10 - 2s - loss: 668.5888 - loglik: -6.6444e+02 - logprior: -4.1504e+00
Epoch 3/10
10/10 - 2s - loss: 665.4484 - loglik: -6.6412e+02 - logprior: -1.3322e+00
Epoch 4/10
10/10 - 2s - loss: 661.4747 - loglik: -6.6117e+02 - logprior: -3.0200e-01
Epoch 5/10
10/10 - 2s - loss: 661.2261 - loglik: -6.6138e+02 - logprior: 0.1539
Epoch 6/10
10/10 - 2s - loss: 659.6415 - loglik: -6.6010e+02 - logprior: 0.4540
Epoch 7/10
10/10 - 2s - loss: 658.8720 - loglik: -6.5956e+02 - logprior: 0.6904
Epoch 8/10
10/10 - 2s - loss: 660.4765 - loglik: -6.6133e+02 - logprior: 0.8562
Fitted a model with MAP estimate = -659.3377
Time for alignment: 66.0634
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 835.5531 - loglik: -8.1096e+02 - logprior: -2.4596e+01
Epoch 2/10
10/10 - 2s - loss: 776.0610 - loglik: -7.7047e+02 - logprior: -5.5958e+00
Epoch 3/10
10/10 - 2s - loss: 734.6222 - loglik: -7.3183e+02 - logprior: -2.7952e+00
Epoch 4/10
10/10 - 2s - loss: 710.4940 - loglik: -7.0828e+02 - logprior: -2.2096e+00
Epoch 5/10
10/10 - 2s - loss: 696.9765 - loglik: -6.9493e+02 - logprior: -2.0462e+00
Epoch 6/10
10/10 - 2s - loss: 692.5112 - loglik: -6.9060e+02 - logprior: -1.9153e+00
Epoch 7/10
10/10 - 2s - loss: 690.5153 - loglik: -6.8870e+02 - logprior: -1.8193e+00
Epoch 8/10
10/10 - 2s - loss: 687.9145 - loglik: -6.8617e+02 - logprior: -1.7402e+00
Epoch 9/10
10/10 - 2s - loss: 687.0247 - loglik: -6.8531e+02 - logprior: -1.7149e+00
Epoch 10/10
10/10 - 2s - loss: 688.4499 - loglik: -6.8678e+02 - logprior: -1.6715e+00
Fitted a model with MAP estimate = -687.1258
expansions: [(8, 1), (9, 1), (14, 1), (17, 2), (23, 1), (24, 3), (29, 2), (53, 1), (60, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (87, 2), (89, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 142 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 703.2081 - loglik: -6.8340e+02 - logprior: -1.9811e+01
Epoch 2/2
10/10 - 2s - loss: 679.4969 - loglik: -6.7472e+02 - logprior: -4.7769e+00
Fitted a model with MAP estimate = -674.6589
expansions: []
discards: [  0  38 108]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 698.4476 - loglik: -6.7495e+02 - logprior: -2.3497e+01
Epoch 2/2
10/10 - 2s - loss: 682.7620 - loglik: -6.7354e+02 - logprior: -9.2238e+00
Fitted a model with MAP estimate = -679.3087
expansions: [(0, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 690.1688 - loglik: -6.7149e+02 - logprior: -1.8676e+01
Epoch 2/10
10/10 - 2s - loss: 676.7636 - loglik: -6.7260e+02 - logprior: -4.1686e+00
Epoch 3/10
10/10 - 2s - loss: 672.1232 - loglik: -6.7082e+02 - logprior: -1.3022e+00
Epoch 4/10
10/10 - 2s - loss: 668.6223 - loglik: -6.6831e+02 - logprior: -3.1317e-01
Epoch 5/10
10/10 - 2s - loss: 668.5317 - loglik: -6.6867e+02 - logprior: 0.1371
Epoch 6/10
10/10 - 2s - loss: 667.0455 - loglik: -6.6747e+02 - logprior: 0.4196
Epoch 7/10
10/10 - 2s - loss: 666.5759 - loglik: -6.6724e+02 - logprior: 0.6597
Epoch 8/10
10/10 - 2s - loss: 668.7460 - loglik: -6.6956e+02 - logprior: 0.8169
Fitted a model with MAP estimate = -666.8801
Time for alignment: 68.2216
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 835.1957 - loglik: -8.1060e+02 - logprior: -2.4597e+01
Epoch 2/10
10/10 - 2s - loss: 775.8488 - loglik: -7.7025e+02 - logprior: -5.6011e+00
Epoch 3/10
10/10 - 2s - loss: 730.2712 - loglik: -7.2743e+02 - logprior: -2.8461e+00
Epoch 4/10
10/10 - 2s - loss: 700.3043 - loglik: -6.9798e+02 - logprior: -2.3287e+00
Epoch 5/10
10/10 - 2s - loss: 690.9646 - loglik: -6.8871e+02 - logprior: -2.2517e+00
Epoch 6/10
10/10 - 2s - loss: 686.6174 - loglik: -6.8450e+02 - logprior: -2.1187e+00
Epoch 7/10
10/10 - 2s - loss: 684.4919 - loglik: -6.8249e+02 - logprior: -2.0038e+00
Epoch 8/10
10/10 - 2s - loss: 683.2841 - loglik: -6.8137e+02 - logprior: -1.9187e+00
Epoch 9/10
10/10 - 2s - loss: 683.7601 - loglik: -6.8191e+02 - logprior: -1.8472e+00
Fitted a model with MAP estimate = -683.0513
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (60, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (84, 1), (86, 1), (98, 1), (101, 2), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 700.0624 - loglik: -6.8029e+02 - logprior: -1.9768e+01
Epoch 2/2
10/10 - 2s - loss: 673.6776 - loglik: -6.6901e+02 - logprior: -4.6682e+00
Fitted a model with MAP estimate = -670.3531
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 694.1041 - loglik: -6.7072e+02 - logprior: -2.3388e+01
Epoch 2/2
10/10 - 2s - loss: 677.5030 - loglik: -6.6838e+02 - logprior: -9.1246e+00
Fitted a model with MAP estimate = -674.8240
expansions: [(0, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 146 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 687.9404 - loglik: -6.6937e+02 - logprior: -1.8568e+01
Epoch 2/10
10/10 - 2s - loss: 670.1783 - loglik: -6.6610e+02 - logprior: -4.0805e+00
Epoch 3/10
10/10 - 2s - loss: 667.3861 - loglik: -6.6618e+02 - logprior: -1.2097e+00
Epoch 4/10
10/10 - 2s - loss: 665.5886 - loglik: -6.6538e+02 - logprior: -2.1162e-01
Epoch 5/10
10/10 - 2s - loss: 663.7453 - loglik: -6.6400e+02 - logprior: 0.2515
Epoch 6/10
10/10 - 2s - loss: 663.4957 - loglik: -6.6402e+02 - logprior: 0.5235
Epoch 7/10
10/10 - 2s - loss: 662.7635 - loglik: -6.6354e+02 - logprior: 0.7718
Epoch 8/10
10/10 - 2s - loss: 662.8959 - loglik: -6.6383e+02 - logprior: 0.9334
Fitted a model with MAP estimate = -662.5775
Time for alignment: 66.3961
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 835.1417 - loglik: -8.1055e+02 - logprior: -2.4593e+01
Epoch 2/10
10/10 - 2s - loss: 776.9780 - loglik: -7.7138e+02 - logprior: -5.5974e+00
Epoch 3/10
10/10 - 2s - loss: 735.1911 - loglik: -7.3240e+02 - logprior: -2.7882e+00
Epoch 4/10
10/10 - 2s - loss: 705.4668 - loglik: -7.0322e+02 - logprior: -2.2496e+00
Epoch 5/10
10/10 - 2s - loss: 695.6934 - loglik: -6.9360e+02 - logprior: -2.0976e+00
Epoch 6/10
10/10 - 2s - loss: 691.4039 - loglik: -6.8949e+02 - logprior: -1.9140e+00
Epoch 7/10
10/10 - 2s - loss: 688.1273 - loglik: -6.8636e+02 - logprior: -1.7686e+00
Epoch 8/10
10/10 - 2s - loss: 688.1890 - loglik: -6.8653e+02 - logprior: -1.6629e+00
Fitted a model with MAP estimate = -687.1475
expansions: [(6, 3), (7, 1), (8, 1), (9, 1), (23, 1), (24, 4), (26, 1), (53, 1), (60, 1), (62, 1), (71, 1), (78, 1), (79, 2), (80, 1), (82, 1), (83, 1), (87, 1), (97, 4), (98, 1), (102, 1), (107, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 146 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 704.1953 - loglik: -6.8416e+02 - logprior: -2.0032e+01
Epoch 2/2
10/10 - 2s - loss: 677.7319 - loglik: -6.7268e+02 - logprior: -5.0563e+00
Fitted a model with MAP estimate = -672.0631
expansions: [(132, 1)]
discards: [  0   7   8  32  96 122 123 124 125]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 699.4384 - loglik: -6.7596e+02 - logprior: -2.3483e+01
Epoch 2/2
10/10 - 2s - loss: 682.0543 - loglik: -6.7286e+02 - logprior: -9.1908e+00
Fitted a model with MAP estimate = -678.5859
expansions: [(0, 5), (120, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 146 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 690.2304 - loglik: -6.7139e+02 - logprior: -1.8845e+01
Epoch 2/10
10/10 - 2s - loss: 670.4111 - loglik: -6.6619e+02 - logprior: -4.2234e+00
Epoch 3/10
10/10 - 2s - loss: 665.9595 - loglik: -6.6454e+02 - logprior: -1.4197e+00
Epoch 4/10
10/10 - 2s - loss: 663.2625 - loglik: -6.6278e+02 - logprior: -4.8000e-01
Epoch 5/10
10/10 - 2s - loss: 662.9146 - loglik: -6.6286e+02 - logprior: -5.6772e-02
Epoch 6/10
10/10 - 2s - loss: 660.4543 - loglik: -6.6071e+02 - logprior: 0.2524
Epoch 7/10
10/10 - 2s - loss: 661.7556 - loglik: -6.6225e+02 - logprior: 0.4912
Fitted a model with MAP estimate = -660.7228
Time for alignment: 62.4192
Computed alignments with likelihoods: ['-663.4919', '-659.3377', '-666.8801', '-662.5775', '-660.7228']
Best model has likelihood: -659.3377  (prior= 0.9226 )
time for generating output: 0.2073
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodcu.projection.fasta
SP score = 0.8504611330698287
Training of 5 independent models on file DEATH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52f9bd7b50>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52f9ebfc10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f1eacfa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f1eacfd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f9bcc400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5296b009d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f11a6880>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5296b0a5e0>, <__main__.SimpleDirichletPrior object at 0x7f52f006ba90>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 508.1620 - loglik: -4.6710e+02 - logprior: -4.1061e+01
Epoch 2/10
10/10 - 1s - loss: 465.8309 - loglik: -4.5547e+02 - logprior: -1.0361e+01
Epoch 3/10
10/10 - 1s - loss: 448.4856 - loglik: -4.4385e+02 - logprior: -4.6359e+00
Epoch 4/10
10/10 - 1s - loss: 436.4086 - loglik: -4.3354e+02 - logprior: -2.8699e+00
Epoch 5/10
10/10 - 1s - loss: 430.5417 - loglik: -4.2814e+02 - logprior: -2.4040e+00
Epoch 6/10
10/10 - 1s - loss: 427.6141 - loglik: -4.2593e+02 - logprior: -1.6838e+00
Epoch 7/10
10/10 - 1s - loss: 426.3346 - loglik: -4.2530e+02 - logprior: -1.0369e+00
Epoch 8/10
10/10 - 1s - loss: 425.9657 - loglik: -4.2512e+02 - logprior: -8.4299e-01
Epoch 9/10
10/10 - 1s - loss: 425.2695 - loglik: -4.2458e+02 - logprior: -6.9060e-01
Epoch 10/10
10/10 - 1s - loss: 425.0915 - loglik: -4.2458e+02 - logprior: -5.1073e-01
Fitted a model with MAP estimate = -424.9388
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (28, 1), (31, 2), (40, 2), (49, 1), (59, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 477.3659 - loglik: -4.2760e+02 - logprior: -4.9765e+01
Epoch 2/2
10/10 - 1s - loss: 436.1029 - loglik: -4.2169e+02 - logprior: -1.4414e+01
Fitted a model with MAP estimate = -428.2068
expansions: [(39, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 462.8439 - loglik: -4.2087e+02 - logprior: -4.1977e+01
Epoch 2/2
10/10 - 1s - loss: 435.6643 - loglik: -4.1970e+02 - logprior: -1.5963e+01
Fitted a model with MAP estimate = -431.2563
expansions: [(0, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 456.7771 - loglik: -4.1936e+02 - logprior: -3.7421e+01
Epoch 2/10
10/10 - 1s - loss: 428.6247 - loglik: -4.1928e+02 - logprior: -9.3456e+00
Epoch 3/10
10/10 - 1s - loss: 421.3031 - loglik: -4.1815e+02 - logprior: -3.1524e+00
Epoch 4/10
10/10 - 1s - loss: 419.3825 - loglik: -4.1845e+02 - logprior: -9.3241e-01
Epoch 5/10
10/10 - 1s - loss: 417.6315 - loglik: -4.1776e+02 - logprior: 0.1301
Epoch 6/10
10/10 - 1s - loss: 417.3424 - loglik: -4.1797e+02 - logprior: 0.6279
Epoch 7/10
10/10 - 1s - loss: 416.6129 - loglik: -4.1743e+02 - logprior: 0.8194
Epoch 8/10
10/10 - 1s - loss: 416.6901 - loglik: -4.1775e+02 - logprior: 1.0573
Fitted a model with MAP estimate = -416.2355
Time for alignment: 47.5395
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 507.8182 - loglik: -4.6676e+02 - logprior: -4.1060e+01
Epoch 2/10
10/10 - 1s - loss: 466.5284 - loglik: -4.5617e+02 - logprior: -1.0356e+01
Epoch 3/10
10/10 - 1s - loss: 448.8603 - loglik: -4.4423e+02 - logprior: -4.6347e+00
Epoch 4/10
10/10 - 1s - loss: 437.3818 - loglik: -4.3450e+02 - logprior: -2.8790e+00
Epoch 5/10
10/10 - 1s - loss: 431.1087 - loglik: -4.2868e+02 - logprior: -2.4318e+00
Epoch 6/10
10/10 - 1s - loss: 428.4186 - loglik: -4.2662e+02 - logprior: -1.8017e+00
Epoch 7/10
10/10 - 1s - loss: 427.2049 - loglik: -4.2603e+02 - logprior: -1.1760e+00
Epoch 8/10
10/10 - 1s - loss: 426.0697 - loglik: -4.2510e+02 - logprior: -9.7465e-01
Epoch 9/10
10/10 - 1s - loss: 425.5720 - loglik: -4.2476e+02 - logprior: -8.1281e-01
Epoch 10/10
10/10 - 1s - loss: 425.5466 - loglik: -4.2492e+02 - logprior: -6.2394e-01
Fitted a model with MAP estimate = -425.3279
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (31, 2), (43, 1), (45, 1), (49, 1), (57, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 478.8704 - loglik: -4.2902e+02 - logprior: -4.9853e+01
Epoch 2/2
10/10 - 1s - loss: 436.5828 - loglik: -4.2215e+02 - logprior: -1.4433e+01
Fitted a model with MAP estimate = -428.8975
expansions: [(38, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 463.2056 - loglik: -4.2115e+02 - logprior: -4.2060e+01
Epoch 2/2
10/10 - 1s - loss: 436.4829 - loglik: -4.2049e+02 - logprior: -1.5992e+01
Fitted a model with MAP estimate = -431.7083
expansions: [(0, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 457.4295 - loglik: -4.2000e+02 - logprior: -3.7429e+01
Epoch 2/10
10/10 - 1s - loss: 428.5846 - loglik: -4.1922e+02 - logprior: -9.3685e+00
Epoch 3/10
10/10 - 1s - loss: 422.1502 - loglik: -4.1896e+02 - logprior: -3.1908e+00
Epoch 4/10
10/10 - 1s - loss: 419.5625 - loglik: -4.1860e+02 - logprior: -9.6572e-01
Epoch 5/10
10/10 - 1s - loss: 418.3215 - loglik: -4.1841e+02 - logprior: 0.0922
Epoch 6/10
10/10 - 1s - loss: 417.8062 - loglik: -4.1840e+02 - logprior: 0.5904
Epoch 7/10
10/10 - 1s - loss: 417.1815 - loglik: -4.1797e+02 - logprior: 0.7883
Epoch 8/10
10/10 - 1s - loss: 416.8987 - loglik: -4.1792e+02 - logprior: 1.0241
Epoch 9/10
10/10 - 1s - loss: 416.5883 - loglik: -4.1792e+02 - logprior: 1.3304
Epoch 10/10
10/10 - 1s - loss: 416.2498 - loglik: -4.1778e+02 - logprior: 1.5327
Fitted a model with MAP estimate = -416.3030
Time for alignment: 46.3852
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 508.0548 - loglik: -4.6699e+02 - logprior: -4.1060e+01
Epoch 2/10
10/10 - 1s - loss: 465.8741 - loglik: -4.5552e+02 - logprior: -1.0353e+01
Epoch 3/10
10/10 - 1s - loss: 448.2647 - loglik: -4.4366e+02 - logprior: -4.6025e+00
Epoch 4/10
10/10 - 1s - loss: 436.2487 - loglik: -4.3345e+02 - logprior: -2.8000e+00
Epoch 5/10
10/10 - 1s - loss: 430.5021 - loglik: -4.2821e+02 - logprior: -2.2895e+00
Epoch 6/10
10/10 - 1s - loss: 427.8484 - loglik: -4.2626e+02 - logprior: -1.5877e+00
Epoch 7/10
10/10 - 1s - loss: 426.7458 - loglik: -4.2581e+02 - logprior: -9.4045e-01
Epoch 8/10
10/10 - 1s - loss: 426.0230 - loglik: -4.2524e+02 - logprior: -7.8109e-01
Epoch 9/10
10/10 - 1s - loss: 425.6788 - loglik: -4.2505e+02 - logprior: -6.2774e-01
Epoch 10/10
10/10 - 1s - loss: 425.5076 - loglik: -4.2506e+02 - logprior: -4.4672e-01
Fitted a model with MAP estimate = -425.2613
expansions: [(0, 3), (6, 1), (22, 1), (28, 2), (30, 1), (31, 2), (34, 1), (45, 2), (49, 1), (59, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 477.5204 - loglik: -4.2780e+02 - logprior: -4.9721e+01
Epoch 2/2
10/10 - 1s - loss: 435.8975 - loglik: -4.2157e+02 - logprior: -1.4328e+01
Fitted a model with MAP estimate = -428.5896
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 463.1079 - loglik: -4.2129e+02 - logprior: -4.1817e+01
Epoch 2/2
10/10 - 1s - loss: 436.6235 - loglik: -4.2073e+02 - logprior: -1.5891e+01
Fitted a model with MAP estimate = -432.0552
expansions: [(0, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 457.9742 - loglik: -4.2060e+02 - logprior: -3.7379e+01
Epoch 2/10
10/10 - 1s - loss: 428.5876 - loglik: -4.1929e+02 - logprior: -9.2960e+00
Epoch 3/10
10/10 - 1s - loss: 422.7798 - loglik: -4.1969e+02 - logprior: -3.0899e+00
Epoch 4/10
10/10 - 1s - loss: 419.9412 - loglik: -4.1908e+02 - logprior: -8.6092e-01
Epoch 5/10
10/10 - 1s - loss: 418.5063 - loglik: -4.1870e+02 - logprior: 0.1953
Epoch 6/10
10/10 - 1s - loss: 417.4132 - loglik: -4.1809e+02 - logprior: 0.6759
Epoch 7/10
10/10 - 1s - loss: 416.8183 - loglik: -4.1765e+02 - logprior: 0.8300
Epoch 8/10
10/10 - 1s - loss: 416.4754 - loglik: -4.1756e+02 - logprior: 1.0850
Epoch 9/10
10/10 - 1s - loss: 415.9014 - loglik: -4.1729e+02 - logprior: 1.3920
Epoch 10/10
10/10 - 1s - loss: 416.0882 - loglik: -4.1768e+02 - logprior: 1.5957
Fitted a model with MAP estimate = -415.8324
Time for alignment: 45.9342
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 508.0918 - loglik: -4.6703e+02 - logprior: -4.1058e+01
Epoch 2/10
10/10 - 1s - loss: 466.3364 - loglik: -4.5599e+02 - logprior: -1.0349e+01
Epoch 3/10
10/10 - 1s - loss: 447.8853 - loglik: -4.4331e+02 - logprior: -4.5709e+00
Epoch 4/10
10/10 - 1s - loss: 435.9016 - loglik: -4.3316e+02 - logprior: -2.7415e+00
Epoch 5/10
10/10 - 1s - loss: 430.1705 - loglik: -4.2791e+02 - logprior: -2.2643e+00
Epoch 6/10
10/10 - 1s - loss: 427.6013 - loglik: -4.2606e+02 - logprior: -1.5402e+00
Epoch 7/10
10/10 - 1s - loss: 426.2409 - loglik: -4.2535e+02 - logprior: -8.8730e-01
Epoch 8/10
10/10 - 1s - loss: 425.8182 - loglik: -4.2512e+02 - logprior: -6.9560e-01
Epoch 9/10
10/10 - 1s - loss: 425.2433 - loglik: -4.2469e+02 - logprior: -5.4886e-01
Epoch 10/10
10/10 - 1s - loss: 424.7085 - loglik: -4.2434e+02 - logprior: -3.6463e-01
Fitted a model with MAP estimate = -424.6854
expansions: [(0, 4), (6, 1), (22, 1), (25, 1), (27, 1), (31, 2), (40, 2), (43, 1), (59, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 477.3495 - loglik: -4.2756e+02 - logprior: -4.9793e+01
Epoch 2/2
10/10 - 1s - loss: 436.2252 - loglik: -4.2176e+02 - logprior: -1.4468e+01
Fitted a model with MAP estimate = -428.4785
expansions: [(39, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 463.3271 - loglik: -4.2123e+02 - logprior: -4.2100e+01
Epoch 2/2
10/10 - 1s - loss: 436.2292 - loglik: -4.2023e+02 - logprior: -1.5998e+01
Fitted a model with MAP estimate = -431.7693
expansions: [(0, 3), (36, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 457.3947 - loglik: -4.1996e+02 - logprior: -3.7435e+01
Epoch 2/10
10/10 - 1s - loss: 428.0506 - loglik: -4.1879e+02 - logprior: -9.2646e+00
Epoch 3/10
10/10 - 1s - loss: 421.3957 - loglik: -4.1831e+02 - logprior: -3.0865e+00
Epoch 4/10
10/10 - 1s - loss: 418.7569 - loglik: -4.1792e+02 - logprior: -8.3616e-01
Epoch 5/10
10/10 - 1s - loss: 417.3496 - loglik: -4.1759e+02 - logprior: 0.2372
Epoch 6/10
10/10 - 1s - loss: 416.6711 - loglik: -4.1739e+02 - logprior: 0.7194
Epoch 7/10
10/10 - 1s - loss: 416.2052 - loglik: -4.1713e+02 - logprior: 0.9261
Epoch 8/10
10/10 - 1s - loss: 415.6479 - loglik: -4.1682e+02 - logprior: 1.1758
Epoch 9/10
10/10 - 1s - loss: 415.5140 - loglik: -4.1697e+02 - logprior: 1.4590
Epoch 10/10
10/10 - 1s - loss: 415.5277 - loglik: -4.1718e+02 - logprior: 1.6506
Fitted a model with MAP estimate = -415.2694
Time for alignment: 46.2551
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 507.9978 - loglik: -4.6694e+02 - logprior: -4.1060e+01
Epoch 2/10
10/10 - 1s - loss: 466.3680 - loglik: -4.5601e+02 - logprior: -1.0361e+01
Epoch 3/10
10/10 - 1s - loss: 448.7447 - loglik: -4.4409e+02 - logprior: -4.6543e+00
Epoch 4/10
10/10 - 1s - loss: 437.7862 - loglik: -4.3492e+02 - logprior: -2.8643e+00
Epoch 5/10
10/10 - 1s - loss: 432.7511 - loglik: -4.3048e+02 - logprior: -2.2744e+00
Epoch 6/10
10/10 - 1s - loss: 429.7382 - loglik: -4.2816e+02 - logprior: -1.5803e+00
Epoch 7/10
10/10 - 1s - loss: 427.5991 - loglik: -4.2662e+02 - logprior: -9.8289e-01
Epoch 8/10
10/10 - 1s - loss: 426.6635 - loglik: -4.2586e+02 - logprior: -8.0318e-01
Epoch 9/10
10/10 - 1s - loss: 426.2826 - loglik: -4.2565e+02 - logprior: -6.3448e-01
Epoch 10/10
10/10 - 1s - loss: 425.7772 - loglik: -4.2535e+02 - logprior: -4.2445e-01
Fitted a model with MAP estimate = -425.7448
expansions: [(0, 3), (6, 1), (22, 1), (28, 2), (31, 3), (40, 2), (49, 1), (56, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 479.0262 - loglik: -4.2926e+02 - logprior: -4.9767e+01
Epoch 2/2
10/10 - 1s - loss: 436.6353 - loglik: -4.2229e+02 - logprior: -1.4346e+01
Fitted a model with MAP estimate = -428.8794
expansions: [(33, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 463.2724 - loglik: -4.2133e+02 - logprior: -4.1942e+01
Epoch 2/2
10/10 - 1s - loss: 436.1972 - loglik: -4.2027e+02 - logprior: -1.5925e+01
Fitted a model with MAP estimate = -431.7509
expansions: [(0, 2)]
discards: [ 0  1 38]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 457.9251 - loglik: -4.2058e+02 - logprior: -3.7346e+01
Epoch 2/10
10/10 - 1s - loss: 428.7612 - loglik: -4.1941e+02 - logprior: -9.3507e+00
Epoch 3/10
10/10 - 1s - loss: 422.2109 - loglik: -4.1898e+02 - logprior: -3.2340e+00
Epoch 4/10
10/10 - 1s - loss: 419.9596 - loglik: -4.1896e+02 - logprior: -9.9631e-01
Epoch 5/10
10/10 - 1s - loss: 418.1122 - loglik: -4.1820e+02 - logprior: 0.0860
Epoch 6/10
10/10 - 1s - loss: 417.5589 - loglik: -4.1813e+02 - logprior: 0.5729
Epoch 7/10
10/10 - 1s - loss: 417.0169 - loglik: -4.1778e+02 - logprior: 0.7627
Epoch 8/10
10/10 - 1s - loss: 416.7803 - loglik: -4.1779e+02 - logprior: 1.0147
Epoch 9/10
10/10 - 1s - loss: 416.7989 - loglik: -4.1812e+02 - logprior: 1.3233
Fitted a model with MAP estimate = -416.4773
Time for alignment: 43.8192
Computed alignments with likelihoods: ['-416.2355', '-416.3030', '-415.8324', '-415.2694', '-416.4773']
Best model has likelihood: -415.2694  (prior= 1.7184 )
time for generating output: 0.1721
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DEATH.projection.fasta
SP score = 0.7425587467362924
Training of 5 independent models on file aat.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52e82e8b80>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b0844d310>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f59406e8a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5900058df0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531d1042b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5aff38ea90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f0df37f0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5300996310>, <__main__.SimpleDirichletPrior object at 0x7f531561de20>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 32s - loss: 1999.7388 - loglik: -1.9980e+03 - logprior: -1.7172e+00
Epoch 2/10
43/43 - 29s - loss: 1885.9735 - loglik: -1.8844e+03 - logprior: -1.5553e+00
Epoch 3/10
43/43 - 29s - loss: 1873.6948 - loglik: -1.8721e+03 - logprior: -1.6224e+00
Epoch 4/10
43/43 - 29s - loss: 1867.6201 - loglik: -1.8660e+03 - logprior: -1.6327e+00
Epoch 5/10
43/43 - 29s - loss: 1865.9376 - loglik: -1.8642e+03 - logprior: -1.7805e+00
Epoch 6/10
43/43 - 30s - loss: 1862.7604 - loglik: -1.8608e+03 - logprior: -1.9701e+00
Epoch 7/10
43/43 - 29s - loss: 1862.2551 - loglik: -1.8600e+03 - logprior: -2.2609e+00
Epoch 8/10
43/43 - 29s - loss: 1861.4460 - loglik: -1.8590e+03 - logprior: -2.4636e+00
Epoch 9/10
43/43 - 30s - loss: 1861.7340 - loglik: -1.8591e+03 - logprior: -2.6381e+00
Fitted a model with MAP estimate = -1808.6767
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (23, 3), (24, 2), (30, 1), (40, 1), (42, 1), (43, 1), (46, 1), (47, 1), (57, 1), (60, 1), (61, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 1), (91, 1), (92, 2), (93, 1), (96, 1), (98, 2), (99, 1), (104, 2), (108, 1), (121, 1), (122, 1), (125, 1), (131, 1), (132, 2), (143, 1), (148, 1), (150, 1), (153, 2), (154, 1), (155, 3), (157, 2), (167, 1), (180, 1), (183, 1), (184, 2), (185, 1), (186, 1), (187, 1), (204, 1), (205, 3), (206, 2), (207, 1), (208, 2), (209, 1), (219, 2), (220, 2), (221, 1), (224, 2), (225, 2), (238, 4), (239, 1), (241, 1), (244, 1), (249, 1), (260, 2), (269, 3), (270, 2), (271, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 375 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 49s - loss: 1862.8474 - loglik: -1.8605e+03 - logprior: -2.3435e+00
Epoch 2/2
43/43 - 46s - loss: 1838.2413 - loglik: -1.8373e+03 - logprior: -9.5654e-01
Fitted a model with MAP estimate = -1769.2504
expansions: []
discards: [  0  30  32 120 128 136 171 207 240 266 267 268 275 293 300 302 319 320
 360 362 365]
Re-initialized the encoder parameters.
Fitting a model of length 354 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 48s - loss: 1845.8300 - loglik: -1.8440e+03 - logprior: -1.8116e+00
Epoch 2/2
43/43 - 42s - loss: 1840.4032 - loglik: -1.8400e+03 - logprior: -3.7168e-01
Fitted a model with MAP estimate = -1769.7260
expansions: [(0, 1)]
discards: [  0 192 193 282]
Re-initialized the encoder parameters.
Fitting a model of length 351 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 64s - loss: 1753.2360 - loglik: -1.7525e+03 - logprior: -6.8706e-01
Epoch 2/10
61/61 - 60s - loss: 1748.0325 - loglik: -1.7476e+03 - logprior: -4.2787e-01
Epoch 3/10
61/61 - 60s - loss: 1747.8528 - loglik: -1.7475e+03 - logprior: -3.9971e-01
Epoch 4/10
61/61 - 60s - loss: 1741.2601 - loglik: -1.7408e+03 - logprior: -4.7661e-01
Epoch 5/10
61/61 - 61s - loss: 1740.4930 - loglik: -1.7400e+03 - logprior: -4.9968e-01
Epoch 6/10
61/61 - 60s - loss: 1733.7679 - loglik: -1.7331e+03 - logprior: -6.7383e-01
Epoch 7/10
61/61 - 61s - loss: 1737.5398 - loglik: -1.7368e+03 - logprior: -7.7184e-01
Fitted a model with MAP estimate = -1732.7110
Time for alignment: 1183.6168
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 33s - loss: 2000.7783 - loglik: -1.9991e+03 - logprior: -1.7231e+00
Epoch 2/10
43/43 - 30s - loss: 1885.4324 - loglik: -1.8836e+03 - logprior: -1.7939e+00
Epoch 3/10
43/43 - 29s - loss: 1873.9543 - loglik: -1.8721e+03 - logprior: -1.9006e+00
Epoch 4/10
43/43 - 29s - loss: 1870.1008 - loglik: -1.8681e+03 - logprior: -1.9582e+00
Epoch 5/10
43/43 - 29s - loss: 1865.4498 - loglik: -1.8634e+03 - logprior: -2.0948e+00
Epoch 6/10
43/43 - 29s - loss: 1864.2965 - loglik: -1.8620e+03 - logprior: -2.3160e+00
Epoch 7/10
43/43 - 29s - loss: 1862.2203 - loglik: -1.8597e+03 - logprior: -2.5416e+00
Epoch 8/10
43/43 - 29s - loss: 1862.6251 - loglik: -1.8599e+03 - logprior: -2.7644e+00
Fitted a model with MAP estimate = -1805.2153
expansions: [(8, 1), (13, 1), (16, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 2), (29, 1), (39, 1), (40, 1), (41, 1), (42, 1), (45, 1), (51, 2), (57, 1), (60, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 1), (86, 2), (88, 1), (91, 1), (96, 1), (98, 1), (99, 1), (101, 1), (120, 1), (121, 1), (122, 1), (125, 1), (131, 1), (132, 2), (142, 1), (148, 2), (153, 2), (154, 1), (155, 3), (157, 2), (176, 2), (180, 1), (183, 1), (184, 1), (185, 1), (187, 1), (197, 2), (200, 1), (203, 1), (204, 1), (206, 2), (207, 1), (208, 1), (209, 1), (219, 1), (221, 1), (225, 1), (226, 2), (236, 1), (238, 3), (242, 1), (244, 1), (245, 1), (249, 1), (260, 2), (269, 2), (270, 2), (271, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 48s - loss: 1862.7469 - loglik: -1.8603e+03 - logprior: -2.4932e+00
Epoch 2/2
43/43 - 45s - loss: 1842.0470 - loglik: -1.8410e+03 - logprior: -1.0267e+00
Fitted a model with MAP estimate = -1768.6251
expansions: [(0, 2)]
discards: [  0  65 109 168 195 204 225 253 295 311 312 354 356]
Re-initialized the encoder parameters.
Fitting a model of length 355 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 46s - loss: 1844.6801 - loglik: -1.8435e+03 - logprior: -1.2039e+00
Epoch 2/2
43/43 - 42s - loss: 1839.0745 - loglik: -1.8387e+03 - logprior: -4.1368e-01
Fitted a model with MAP estimate = -1769.5557
expansions: [(304, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 355 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 63s - loss: 1754.5652 - loglik: -1.7537e+03 - logprior: -9.0339e-01
Epoch 2/10
61/61 - 59s - loss: 1748.7893 - loglik: -1.7484e+03 - logprior: -3.7575e-01
Epoch 3/10
61/61 - 59s - loss: 1744.6935 - loglik: -1.7444e+03 - logprior: -3.1369e-01
Epoch 4/10
61/61 - 59s - loss: 1739.6534 - loglik: -1.7393e+03 - logprior: -3.5300e-01
Epoch 5/10
61/61 - 59s - loss: 1738.7782 - loglik: -1.7384e+03 - logprior: -3.8861e-01
Epoch 6/10
61/61 - 59s - loss: 1736.1173 - loglik: -1.7356e+03 - logprior: -5.4595e-01
Epoch 7/10
61/61 - 59s - loss: 1733.8802 - loglik: -1.7332e+03 - logprior: -6.5638e-01
Epoch 8/10
61/61 - 59s - loss: 1732.6531 - loglik: -1.7319e+03 - logprior: -7.5911e-01
Epoch 9/10
61/61 - 59s - loss: 1732.4368 - loglik: -1.7315e+03 - logprior: -9.0646e-01
Epoch 10/10
61/61 - 59s - loss: 1729.0121 - loglik: -1.7281e+03 - logprior: -9.0384e-01
Fitted a model with MAP estimate = -1727.8512
Time for alignment: 1317.7897
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 32s - loss: 1999.4780 - loglik: -1.9978e+03 - logprior: -1.7092e+00
Epoch 2/10
43/43 - 29s - loss: 1887.7222 - loglik: -1.8862e+03 - logprior: -1.5564e+00
Epoch 3/10
43/43 - 30s - loss: 1873.0406 - loglik: -1.8714e+03 - logprior: -1.6476e+00
Epoch 4/10
43/43 - 29s - loss: 1871.1329 - loglik: -1.8695e+03 - logprior: -1.6828e+00
Epoch 5/10
43/43 - 29s - loss: 1865.3721 - loglik: -1.8635e+03 - logprior: -1.8256e+00
Epoch 6/10
43/43 - 29s - loss: 1864.9404 - loglik: -1.8629e+03 - logprior: -2.0880e+00
Epoch 7/10
43/43 - 30s - loss: 1863.2057 - loglik: -1.8610e+03 - logprior: -2.2514e+00
Epoch 8/10
43/43 - 29s - loss: 1862.9495 - loglik: -1.8605e+03 - logprior: -2.4680e+00
Epoch 9/10
43/43 - 29s - loss: 1862.4709 - loglik: -1.8598e+03 - logprior: -2.6860e+00
Epoch 10/10
43/43 - 29s - loss: 1861.2861 - loglik: -1.8585e+03 - logprior: -2.7617e+00
Fitted a model with MAP estimate = -1810.5354
expansions: [(0, 2), (16, 1), (20, 1), (21, 2), (22, 1), (23, 1), (24, 2), (29, 1), (39, 1), (40, 1), (41, 1), (42, 1), (45, 1), (48, 2), (57, 1), (59, 2), (62, 1), (65, 1), (80, 1), (81, 1), (82, 1), (86, 2), (90, 1), (91, 2), (92, 1), (95, 1), (97, 2), (98, 1), (103, 2), (121, 1), (122, 1), (125, 1), (128, 2), (130, 1), (142, 1), (148, 2), (152, 2), (154, 1), (155, 3), (157, 2), (167, 1), (180, 1), (182, 1), (184, 1), (185, 1), (187, 1), (197, 1), (200, 1), (203, 1), (204, 1), (205, 3), (207, 1), (208, 2), (209, 1), (219, 2), (220, 2), (221, 2), (224, 2), (236, 1), (238, 2), (242, 1), (245, 1), (248, 1), (249, 1), (260, 2), (269, 3), (270, 2), (271, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 375 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 49s - loss: 1862.6764 - loglik: -1.8603e+03 - logprior: -2.3853e+00
Epoch 2/2
43/43 - 47s - loss: 1840.6714 - loglik: -1.8396e+03 - logprior: -1.0245e+00
Fitted a model with MAP estimate = -1768.9199
expansions: []
discards: [  0  26  65  78 112 122 131 139 169 200 210 269 270 277 295 297 302 360
 362 365]
Re-initialized the encoder parameters.
Fitting a model of length 355 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 48s - loss: 1846.1786 - loglik: -1.8443e+03 - logprior: -1.9107e+00
Epoch 2/2
43/43 - 42s - loss: 1839.4218 - loglik: -1.8390e+03 - logprior: -3.9718e-01
Fitted a model with MAP estimate = -1770.0198
expansions: [(0, 1)]
discards: [  0 283]
Re-initialized the encoder parameters.
Fitting a model of length 354 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 62s - loss: 1750.9868 - loglik: -1.7503e+03 - logprior: -6.8224e-01
Epoch 2/10
61/61 - 59s - loss: 1749.3505 - loglik: -1.7489e+03 - logprior: -4.2199e-01
Epoch 3/10
61/61 - 59s - loss: 1746.4696 - loglik: -1.7461e+03 - logprior: -4.1451e-01
Epoch 4/10
61/61 - 58s - loss: 1741.8444 - loglik: -1.7414e+03 - logprior: -4.5898e-01
Epoch 5/10
61/61 - 59s - loss: 1738.5361 - loglik: -1.7380e+03 - logprior: -4.8807e-01
Epoch 6/10
61/61 - 58s - loss: 1736.6040 - loglik: -1.7359e+03 - logprior: -6.9176e-01
Epoch 7/10
61/61 - 58s - loss: 1732.0618 - loglik: -1.7313e+03 - logprior: -7.7695e-01
Epoch 8/10
61/61 - 59s - loss: 1731.8601 - loglik: -1.7310e+03 - logprior: -8.9463e-01
Epoch 9/10
61/61 - 58s - loss: 1731.1935 - loglik: -1.7302e+03 - logprior: -1.0176e+00
Epoch 10/10
61/61 - 59s - loss: 1729.7185 - loglik: -1.7286e+03 - logprior: -1.0866e+00
Fitted a model with MAP estimate = -1727.5881
Time for alignment: 1378.2380
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 33s - loss: 1999.7188 - loglik: -1.9980e+03 - logprior: -1.7066e+00
Epoch 2/10
43/43 - 29s - loss: 1882.8285 - loglik: -1.8810e+03 - logprior: -1.7825e+00
Epoch 3/10
43/43 - 29s - loss: 1874.4104 - loglik: -1.8725e+03 - logprior: -1.8985e+00
Epoch 4/10
43/43 - 29s - loss: 1867.8613 - loglik: -1.8659e+03 - logprior: -1.9432e+00
Epoch 5/10
43/43 - 29s - loss: 1866.1819 - loglik: -1.8641e+03 - logprior: -2.0818e+00
Epoch 6/10
43/43 - 29s - loss: 1864.0070 - loglik: -1.8617e+03 - logprior: -2.2899e+00
Epoch 7/10
43/43 - 29s - loss: 1863.4141 - loglik: -1.8609e+03 - logprior: -2.4774e+00
Epoch 8/10
43/43 - 29s - loss: 1861.5769 - loglik: -1.8589e+03 - logprior: -2.7051e+00
Epoch 9/10
43/43 - 29s - loss: 1862.1287 - loglik: -1.8592e+03 - logprior: -2.9049e+00
Fitted a model with MAP estimate = -1806.0139
expansions: [(8, 1), (13, 1), (16, 1), (20, 1), (21, 1), (22, 2), (23, 1), (24, 2), (30, 1), (40, 1), (42, 1), (43, 1), (46, 1), (47, 1), (57, 1), (60, 1), (61, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 1), (86, 2), (90, 1), (91, 1), (93, 1), (95, 1), (97, 1), (98, 1), (103, 2), (119, 1), (122, 1), (125, 1), (128, 2), (130, 1), (133, 2), (144, 1), (147, 1), (152, 2), (154, 1), (155, 3), (157, 2), (176, 2), (183, 2), (184, 3), (187, 2), (200, 1), (204, 1), (205, 3), (206, 2), (207, 1), (210, 1), (219, 1), (220, 1), (221, 1), (222, 1), (225, 1), (226, 2), (236, 1), (238, 3), (242, 1), (245, 1), (247, 1), (249, 1), (260, 2), (269, 2), (270, 2), (271, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 371 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 50s - loss: 1864.5769 - loglik: -1.8620e+03 - logprior: -2.5298e+00
Epoch 2/2
43/43 - 45s - loss: 1839.4066 - loglik: -1.8383e+03 - logprior: -1.0726e+00
Fitted a model with MAP estimate = -1768.3460
expansions: [(0, 2)]
discards: [  0  27 109 134 164 172 196 206 227 240 245 267 268 269 295 300 316 317
 359 361]
Re-initialized the encoder parameters.
Fitting a model of length 353 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 45s - loss: 1845.1593 - loglik: -1.8440e+03 - logprior: -1.1771e+00
Epoch 2/2
43/43 - 42s - loss: 1840.2220 - loglik: -1.8398e+03 - logprior: -3.9608e-01
Fitted a model with MAP estimate = -1769.0032
expansions: [(302, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 353 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 62s - loss: 1754.5704 - loglik: -1.7537e+03 - logprior: -8.9965e-01
Epoch 2/10
61/61 - 58s - loss: 1747.2998 - loglik: -1.7470e+03 - logprior: -3.4236e-01
Epoch 3/10
61/61 - 58s - loss: 1746.8229 - loglik: -1.7465e+03 - logprior: -3.2799e-01
Epoch 4/10
61/61 - 59s - loss: 1741.8472 - loglik: -1.7415e+03 - logprior: -3.0107e-01
Epoch 5/10
61/61 - 58s - loss: 1740.8715 - loglik: -1.7405e+03 - logprior: -3.4687e-01
Epoch 6/10
61/61 - 58s - loss: 1734.8113 - loglik: -1.7343e+03 - logprior: -4.9884e-01
Epoch 7/10
61/61 - 59s - loss: 1735.7391 - loglik: -1.7351e+03 - logprior: -6.5366e-01
Fitted a model with MAP estimate = -1733.1048
Time for alignment: 1165.3971
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 34s - loss: 1997.2623 - loglik: -1.9956e+03 - logprior: -1.6919e+00
Epoch 2/10
43/43 - 29s - loss: 1882.3765 - loglik: -1.8808e+03 - logprior: -1.6212e+00
Epoch 3/10
43/43 - 30s - loss: 1872.9070 - loglik: -1.8712e+03 - logprior: -1.7252e+00
Epoch 4/10
43/43 - 29s - loss: 1863.8914 - loglik: -1.8621e+03 - logprior: -1.8031e+00
Epoch 5/10
43/43 - 30s - loss: 1867.3198 - loglik: -1.8653e+03 - logprior: -2.0006e+00
Fitted a model with MAP estimate = -1797.8653
expansions: [(0, 2), (16, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 2), (30, 1), (39, 1), (40, 1), (41, 1), (42, 1), (45, 1), (49, 2), (57, 1), (60, 1), (62, 1), (65, 1), (80, 2), (81, 1), (82, 2), (86, 2), (90, 1), (91, 1), (96, 1), (98, 2), (99, 1), (102, 1), (120, 1), (121, 1), (122, 1), (125, 1), (131, 1), (133, 2), (143, 1), (148, 1), (150, 1), (153, 2), (156, 3), (157, 2), (181, 1), (183, 1), (185, 2), (186, 2), (187, 1), (188, 1), (197, 1), (204, 1), (205, 3), (206, 2), (207, 1), (210, 1), (219, 1), (220, 1), (221, 1), (222, 1), (225, 1), (226, 2), (236, 1), (238, 3), (242, 1), (244, 2), (245, 1), (249, 1), (260, 2), (261, 1), (269, 2), (270, 1), (271, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 371 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 48s - loss: 1857.9515 - loglik: -1.8558e+03 - logprior: -2.1748e+00
Epoch 2/2
43/43 - 45s - loss: 1840.4829 - loglik: -1.8395e+03 - logprior: -9.6080e-01
Fitted a model with MAP estimate = -1767.6726
expansions: []
discards: [  1  64 102 106 112 130 173 199 204 240 242 267 268 269 295 300 316 326
 358]
Re-initialized the encoder parameters.
Fitting a model of length 352 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 45s - loss: 1844.2604 - loglik: -1.8432e+03 - logprior: -1.1041e+00
Epoch 2/2
43/43 - 42s - loss: 1841.0227 - loglik: -1.8406e+03 - logprior: -4.5161e-01
Fitted a model with MAP estimate = -1769.7275
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 352 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 64s - loss: 1751.3564 - loglik: -1.7506e+03 - logprior: -7.3783e-01
Epoch 2/10
61/61 - 59s - loss: 1748.7135 - loglik: -1.7482e+03 - logprior: -4.7120e-01
Epoch 3/10
61/61 - 58s - loss: 1745.4358 - loglik: -1.7450e+03 - logprior: -4.8249e-01
Epoch 4/10
61/61 - 58s - loss: 1741.3689 - loglik: -1.7409e+03 - logprior: -5.1775e-01
Epoch 5/10
61/61 - 59s - loss: 1740.2742 - loglik: -1.7397e+03 - logprior: -5.3744e-01
Epoch 6/10
61/61 - 58s - loss: 1735.8887 - loglik: -1.7352e+03 - logprior: -7.0887e-01
Epoch 7/10
61/61 - 58s - loss: 1733.8362 - loglik: -1.7330e+03 - logprior: -8.5187e-01
Epoch 8/10
61/61 - 58s - loss: 1733.1183 - loglik: -1.7322e+03 - logprior: -9.5708e-01
Epoch 9/10
61/61 - 59s - loss: 1730.9791 - loglik: -1.7299e+03 - logprior: -1.0889e+00
Epoch 10/10
61/61 - 58s - loss: 1732.1821 - loglik: -1.7311e+03 - logprior: -1.1246e+00
Fitted a model with MAP estimate = -1728.2436
Time for alignment: 1224.0194
Computed alignments with likelihoods: ['-1732.7110', '-1727.8512', '-1727.5881', '-1733.1048', '-1728.2436']
Best model has likelihood: -1727.5881  (prior= -1.1626 )
time for generating output: 0.4580
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aat.projection.fasta
SP score = 0.8040770767242392
Training of 5 independent models on file ins.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5344230c70>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52f9024370>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f062a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52976b0df0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f0630040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52976ae7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5300ea6280>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52f9012790>, <__main__.SimpleDirichletPrior object at 0x7f52e1f1beb0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 449.4941 - loglik: -3.8877e+02 - logprior: -6.0721e+01
Epoch 2/10
10/10 - 2s - loss: 382.7744 - loglik: -3.6627e+02 - logprior: -1.6509e+01
Epoch 3/10
10/10 - 1s - loss: 357.1359 - loglik: -3.4922e+02 - logprior: -7.9202e+00
Epoch 4/10
10/10 - 1s - loss: 347.9575 - loglik: -3.4326e+02 - logprior: -4.6979e+00
Epoch 5/10
10/10 - 1s - loss: 346.0439 - loglik: -3.4295e+02 - logprior: -3.0971e+00
Epoch 6/10
10/10 - 1s - loss: 344.7986 - loglik: -3.4268e+02 - logprior: -2.1220e+00
Epoch 7/10
10/10 - 2s - loss: 344.6295 - loglik: -3.4305e+02 - logprior: -1.5834e+00
Epoch 8/10
10/10 - 1s - loss: 341.7285 - loglik: -3.4031e+02 - logprior: -1.4233e+00
Epoch 9/10
10/10 - 1s - loss: 342.8489 - loglik: -3.4155e+02 - logprior: -1.2964e+00
Fitted a model with MAP estimate = -342.6705
expansions: [(11, 1), (12, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 404.3925 - loglik: -3.4010e+02 - logprior: -6.4292e+01
Epoch 2/2
10/10 - 1s - loss: 367.5998 - loglik: -3.4051e+02 - logprior: -2.7090e+01
Fitted a model with MAP estimate = -360.8702
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 390.6706 - loglik: -3.3796e+02 - logprior: -5.2712e+01
Epoch 2/2
10/10 - 2s - loss: 352.7002 - loglik: -3.3815e+02 - logprior: -1.4553e+01
Fitted a model with MAP estimate = -347.2580
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 400.8418 - loglik: -3.3876e+02 - logprior: -6.2081e+01
Epoch 2/10
10/10 - 2s - loss: 359.9282 - loglik: -3.3913e+02 - logprior: -2.0802e+01
Epoch 3/10
10/10 - 1s - loss: 347.2099 - loglik: -3.3899e+02 - logprior: -8.2232e+00
Epoch 4/10
10/10 - 1s - loss: 342.3715 - loglik: -3.3844e+02 - logprior: -3.9316e+00
Epoch 5/10
10/10 - 2s - loss: 341.5717 - loglik: -3.3933e+02 - logprior: -2.2397e+00
Epoch 6/10
10/10 - 2s - loss: 341.8114 - loglik: -3.4041e+02 - logprior: -1.3998e+00
Fitted a model with MAP estimate = -340.3246
Time for alignment: 42.9482
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 449.4629 - loglik: -3.8875e+02 - logprior: -6.0718e+01
Epoch 2/10
10/10 - 2s - loss: 382.4338 - loglik: -3.6594e+02 - logprior: -1.6499e+01
Epoch 3/10
10/10 - 2s - loss: 357.1378 - loglik: -3.4921e+02 - logprior: -7.9281e+00
Epoch 4/10
10/10 - 2s - loss: 348.9131 - loglik: -3.4425e+02 - logprior: -4.6679e+00
Epoch 5/10
10/10 - 2s - loss: 346.5748 - loglik: -3.4348e+02 - logprior: -3.0920e+00
Epoch 6/10
10/10 - 1s - loss: 342.3728 - loglik: -3.4019e+02 - logprior: -2.1854e+00
Epoch 7/10
10/10 - 1s - loss: 344.9747 - loglik: -3.4336e+02 - logprior: -1.6188e+00
Fitted a model with MAP estimate = -343.6260
expansions: [(10, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 390.6743 - loglik: -3.3910e+02 - logprior: -5.1576e+01
Epoch 2/2
10/10 - 1s - loss: 354.0390 - loglik: -3.3941e+02 - logprior: -1.4631e+01
Fitted a model with MAP estimate = -347.4964
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 403.7106 - loglik: -3.3929e+02 - logprior: -6.4424e+01
Epoch 2/2
10/10 - 1s - loss: 366.7854 - loglik: -3.3946e+02 - logprior: -2.7328e+01
Fitted a model with MAP estimate = -361.0255
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 391.6465 - loglik: -3.3781e+02 - logprior: -5.3841e+01
Epoch 2/10
10/10 - 1s - loss: 352.7025 - loglik: -3.3800e+02 - logprior: -1.4701e+01
Epoch 3/10
10/10 - 1s - loss: 344.7724 - loglik: -3.3805e+02 - logprior: -6.7220e+00
Epoch 4/10
10/10 - 1s - loss: 342.8498 - loglik: -3.3929e+02 - logprior: -3.5633e+00
Epoch 5/10
10/10 - 1s - loss: 341.2469 - loglik: -3.3930e+02 - logprior: -1.9483e+00
Epoch 6/10
10/10 - 1s - loss: 340.2157 - loglik: -3.3913e+02 - logprior: -1.0821e+00
Epoch 7/10
10/10 - 1s - loss: 339.2126 - loglik: -3.3866e+02 - logprior: -5.5411e-01
Epoch 8/10
10/10 - 2s - loss: 339.0168 - loglik: -3.3883e+02 - logprior: -1.8733e-01
Epoch 9/10
10/10 - 2s - loss: 339.4534 - loglik: -3.3952e+02 - logprior: 0.0682
Fitted a model with MAP estimate = -339.2562
Time for alignment: 50.1656
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 449.3185 - loglik: -3.8860e+02 - logprior: -6.0716e+01
Epoch 2/10
10/10 - 2s - loss: 383.5836 - loglik: -3.6709e+02 - logprior: -1.6496e+01
Epoch 3/10
10/10 - 2s - loss: 357.0408 - loglik: -3.4912e+02 - logprior: -7.9169e+00
Epoch 4/10
10/10 - 2s - loss: 349.1794 - loglik: -3.4444e+02 - logprior: -4.7368e+00
Epoch 5/10
10/10 - 1s - loss: 345.7666 - loglik: -3.4247e+02 - logprior: -3.2953e+00
Epoch 6/10
10/10 - 1s - loss: 342.8790 - loglik: -3.4044e+02 - logprior: -2.4419e+00
Epoch 7/10
10/10 - 1s - loss: 343.1704 - loglik: -3.4130e+02 - logprior: -1.8693e+00
Fitted a model with MAP estimate = -342.5725
expansions: [(11, 1), (12, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 403.6808 - loglik: -3.3942e+02 - logprior: -6.4256e+01
Epoch 2/2
10/10 - 2s - loss: 365.0006 - loglik: -3.3810e+02 - logprior: -2.6904e+01
Fitted a model with MAP estimate = -358.6449
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 388.8395 - loglik: -3.3675e+02 - logprior: -5.2087e+01
Epoch 2/2
10/10 - 2s - loss: 349.9098 - loglik: -3.3550e+02 - logprior: -1.4410e+01
Fitted a model with MAP estimate = -345.0418
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 400.3293 - loglik: -3.3792e+02 - logprior: -6.2411e+01
Epoch 2/10
10/10 - 1s - loss: 356.8936 - loglik: -3.3514e+02 - logprior: -2.1750e+01
Epoch 3/10
10/10 - 1s - loss: 344.8569 - loglik: -3.3620e+02 - logprior: -8.6609e+00
Epoch 4/10
10/10 - 2s - loss: 340.7495 - loglik: -3.3680e+02 - logprior: -3.9508e+00
Epoch 5/10
10/10 - 1s - loss: 340.9949 - loglik: -3.3880e+02 - logprior: -2.1948e+00
Fitted a model with MAP estimate = -338.6564
Time for alignment: 42.0257
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 448.7308 - loglik: -3.8801e+02 - logprior: -6.0719e+01
Epoch 2/10
10/10 - 2s - loss: 383.8327 - loglik: -3.6733e+02 - logprior: -1.6504e+01
Epoch 3/10
10/10 - 2s - loss: 357.8118 - loglik: -3.4990e+02 - logprior: -7.9117e+00
Epoch 4/10
10/10 - 1s - loss: 348.6122 - loglik: -3.4392e+02 - logprior: -4.6882e+00
Epoch 5/10
10/10 - 2s - loss: 346.3180 - loglik: -3.4322e+02 - logprior: -3.0977e+00
Epoch 6/10
10/10 - 2s - loss: 343.4231 - loglik: -3.4131e+02 - logprior: -2.1133e+00
Epoch 7/10
10/10 - 2s - loss: 344.5455 - loglik: -3.4300e+02 - logprior: -1.5415e+00
Fitted a model with MAP estimate = -343.5502
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 407.1357 - loglik: -3.4277e+02 - logprior: -6.4364e+01
Epoch 2/2
10/10 - 1s - loss: 368.2389 - loglik: -3.4113e+02 - logprior: -2.7111e+01
Fitted a model with MAP estimate = -363.1988
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 392.7340 - loglik: -3.3988e+02 - logprior: -5.2855e+01
Epoch 2/2
10/10 - 1s - loss: 355.2469 - loglik: -3.4060e+02 - logprior: -1.4648e+01
Fitted a model with MAP estimate = -349.7388
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 404.1872 - loglik: -3.4203e+02 - logprior: -6.2158e+01
Epoch 2/10
10/10 - 2s - loss: 361.1900 - loglik: -3.4030e+02 - logprior: -2.0886e+01
Epoch 3/10
10/10 - 1s - loss: 348.7553 - loglik: -3.4045e+02 - logprior: -8.3097e+00
Epoch 4/10
10/10 - 2s - loss: 346.2841 - loglik: -3.4226e+02 - logprior: -4.0273e+00
Epoch 5/10
10/10 - 2s - loss: 344.1737 - loglik: -3.4183e+02 - logprior: -2.3391e+00
Epoch 6/10
10/10 - 1s - loss: 343.8093 - loglik: -3.4231e+02 - logprior: -1.5032e+00
Epoch 7/10
10/10 - 1s - loss: 341.7916 - loglik: -3.4090e+02 - logprior: -8.9554e-01
Epoch 8/10
10/10 - 1s - loss: 341.6323 - loglik: -3.4117e+02 - logprior: -4.6406e-01
Epoch 9/10
10/10 - 1s - loss: 343.5941 - loglik: -3.4338e+02 - logprior: -2.1608e-01
Fitted a model with MAP estimate = -341.9964
Time for alignment: 47.0062
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 449.5894 - loglik: -3.8887e+02 - logprior: -6.0717e+01
Epoch 2/10
10/10 - 1s - loss: 382.8423 - loglik: -3.6634e+02 - logprior: -1.6506e+01
Epoch 3/10
10/10 - 1s - loss: 358.5011 - loglik: -3.5059e+02 - logprior: -7.9155e+00
Epoch 4/10
10/10 - 1s - loss: 347.0400 - loglik: -3.4230e+02 - logprior: -4.7386e+00
Epoch 5/10
10/10 - 1s - loss: 344.7254 - loglik: -3.4141e+02 - logprior: -3.3182e+00
Epoch 6/10
10/10 - 1s - loss: 343.1092 - loglik: -3.4069e+02 - logprior: -2.4193e+00
Epoch 7/10
10/10 - 1s - loss: 343.9102 - loglik: -3.4210e+02 - logprior: -1.8118e+00
Fitted a model with MAP estimate = -342.5236
expansions: [(11, 1), (12, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 402.7445 - loglik: -3.3851e+02 - logprior: -6.4239e+01
Epoch 2/2
10/10 - 2s - loss: 366.2888 - loglik: -3.3941e+02 - logprior: -2.6884e+01
Fitted a model with MAP estimate = -358.6720
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 387.8671 - loglik: -3.3577e+02 - logprior: -5.2093e+01
Epoch 2/2
10/10 - 1s - loss: 350.9481 - loglik: -3.3653e+02 - logprior: -1.4417e+01
Fitted a model with MAP estimate = -345.0294
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 398.7924 - loglik: -3.3639e+02 - logprior: -6.2404e+01
Epoch 2/10
10/10 - 1s - loss: 358.6724 - loglik: -3.3694e+02 - logprior: -2.1729e+01
Epoch 3/10
10/10 - 1s - loss: 345.6152 - loglik: -3.3698e+02 - logprior: -8.6397e+00
Epoch 4/10
10/10 - 1s - loss: 341.1805 - loglik: -3.3724e+02 - logprior: -3.9382e+00
Epoch 5/10
10/10 - 2s - loss: 338.2559 - loglik: -3.3607e+02 - logprior: -2.1850e+00
Epoch 6/10
10/10 - 2s - loss: 338.5167 - loglik: -3.3719e+02 - logprior: -1.3232e+00
Fitted a model with MAP estimate = -338.0821
Time for alignment: 41.5384
Computed alignments with likelihoods: ['-340.3246', '-339.2562', '-338.6564', '-341.9964', '-338.0821']
Best model has likelihood: -338.0821  (prior= -0.9574 )
time for generating output: 0.1310
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ins.projection.fasta
SP score = 0.9334257975034674
Training of 5 independent models on file sti.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52f9c8cd90>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f530cc08f10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5297716eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5300b83e80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f9c882b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530d4854c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f59842457c0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52f88725b0>, <__main__.SimpleDirichletPrior object at 0x7f5334820c10>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1037.7761 - loglik: -9.5587e+02 - logprior: -8.1902e+01
Epoch 2/10
10/10 - 2s - loss: 926.6056 - loglik: -9.1001e+02 - logprior: -1.6591e+01
Epoch 3/10
10/10 - 2s - loss: 872.6237 - loglik: -8.6700e+02 - logprior: -5.6195e+00
Epoch 4/10
10/10 - 2s - loss: 840.5746 - loglik: -8.3804e+02 - logprior: -2.5317e+00
Epoch 5/10
10/10 - 2s - loss: 825.1146 - loglik: -8.2410e+02 - logprior: -1.0109e+00
Epoch 6/10
10/10 - 2s - loss: 818.4993 - loglik: -8.1855e+02 - logprior: 0.0532
Epoch 7/10
10/10 - 2s - loss: 814.0622 - loglik: -8.1486e+02 - logprior: 0.7965
Epoch 8/10
10/10 - 2s - loss: 812.4875 - loglik: -8.1372e+02 - logprior: 1.2315
Epoch 9/10
10/10 - 2s - loss: 811.5817 - loglik: -8.1313e+02 - logprior: 1.5434
Epoch 10/10
10/10 - 2s - loss: 810.8252 - loglik: -8.1267e+02 - logprior: 1.8474
Fitted a model with MAP estimate = -810.5118
expansions: [(11, 3), (12, 2), (19, 1), (26, 1), (27, 1), (28, 1), (37, 1), (38, 2), (39, 2), (55, 1), (66, 1), (68, 1), (77, 2), (78, 2), (80, 2), (89, 1), (90, 1), (103, 1), (110, 2), (114, 2), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 891.4191 - loglik: -8.1185e+02 - logprior: -7.9564e+01
Epoch 2/2
10/10 - 3s - loss: 824.0711 - loglik: -7.9567e+02 - logprior: -2.8398e+01
Fitted a model with MAP estimate = -812.5761
expansions: [(0, 2), (29, 1)]
discards: [  0  11  47  50 100 161]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 852.5638 - loglik: -7.9364e+02 - logprior: -5.8926e+01
Epoch 2/2
10/10 - 3s - loss: 797.1111 - loglik: -7.8656e+02 - logprior: -1.0549e+01
Fitted a model with MAP estimate = -789.6074
expansions: [(50, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 863.3724 - loglik: -7.9116e+02 - logprior: -7.2212e+01
Epoch 2/10
10/10 - 3s - loss: 807.1690 - loglik: -7.8815e+02 - logprior: -1.9016e+01
Epoch 3/10
10/10 - 3s - loss: 788.2864 - loglik: -7.8606e+02 - logprior: -2.2241e+00
Epoch 4/10
10/10 - 3s - loss: 781.1256 - loglik: -7.8500e+02 - logprior: 3.8776
Epoch 5/10
10/10 - 3s - loss: 778.2023 - loglik: -7.8460e+02 - logprior: 6.4007
Epoch 6/10
10/10 - 3s - loss: 776.7600 - loglik: -7.8466e+02 - logprior: 7.9013
Epoch 7/10
10/10 - 3s - loss: 776.0831 - loglik: -7.8515e+02 - logprior: 9.0712
Epoch 8/10
10/10 - 3s - loss: 774.1357 - loglik: -7.8414e+02 - logprior: 10.0004
Epoch 9/10
10/10 - 3s - loss: 774.0367 - loglik: -7.8476e+02 - logprior: 10.7252
Epoch 10/10
10/10 - 3s - loss: 773.2985 - loglik: -7.8460e+02 - logprior: 11.2970
Fitted a model with MAP estimate = -773.3833
Time for alignment: 84.3052
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 1037.9780 - loglik: -9.5607e+02 - logprior: -8.1906e+01
Epoch 2/10
10/10 - 2s - loss: 926.2590 - loglik: -9.0968e+02 - logprior: -1.6580e+01
Epoch 3/10
10/10 - 2s - loss: 872.4210 - loglik: -8.6713e+02 - logprior: -5.2958e+00
Epoch 4/10
10/10 - 2s - loss: 843.2336 - loglik: -8.4131e+02 - logprior: -1.9254e+00
Epoch 5/10
10/10 - 2s - loss: 829.7440 - loglik: -8.2958e+02 - logprior: -1.6645e-01
Epoch 6/10
10/10 - 2s - loss: 821.3146 - loglik: -8.2215e+02 - logprior: 0.8312
Epoch 7/10
10/10 - 2s - loss: 817.6798 - loglik: -8.1905e+02 - logprior: 1.3728
Epoch 8/10
10/10 - 2s - loss: 815.6237 - loglik: -8.1737e+02 - logprior: 1.7509
Epoch 9/10
10/10 - 2s - loss: 814.8659 - loglik: -8.1693e+02 - logprior: 2.0620
Epoch 10/10
10/10 - 2s - loss: 813.7698 - loglik: -8.1607e+02 - logprior: 2.3035
Fitted a model with MAP estimate = -813.4506
expansions: [(11, 3), (12, 2), (15, 2), (16, 2), (26, 1), (27, 1), (28, 1), (38, 2), (39, 2), (55, 1), (65, 2), (79, 6), (87, 2), (88, 1), (110, 2), (114, 3), (115, 1), (119, 1), (120, 1), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 891.2461 - loglik: -8.1185e+02 - logprior: -7.9397e+01
Epoch 2/2
10/10 - 3s - loss: 821.5021 - loglik: -7.9360e+02 - logprior: -2.7904e+01
Fitted a model with MAP estimate = -809.0092
expansions: [(0, 2), (130, 1)]
discards: [  0  11  22 164]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 846.6373 - loglik: -7.8824e+02 - logprior: -5.8395e+01
Epoch 2/2
10/10 - 3s - loss: 792.4213 - loglik: -7.8241e+02 - logprior: -1.0009e+01
Fitted a model with MAP estimate = -783.9827
expansions: [(128, 1)]
discards: [  0  19 143]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 859.2541 - loglik: -7.8766e+02 - logprior: -7.1590e+01
Epoch 2/10
10/10 - 3s - loss: 802.0117 - loglik: -7.8368e+02 - logprior: -1.8335e+01
Epoch 3/10
10/10 - 3s - loss: 784.4942 - loglik: -7.8282e+02 - logprior: -1.6773e+00
Epoch 4/10
10/10 - 3s - loss: 776.6544 - loglik: -7.8095e+02 - logprior: 4.2908
Epoch 5/10
10/10 - 3s - loss: 772.7679 - loglik: -7.7962e+02 - logprior: 6.8492
Epoch 6/10
10/10 - 3s - loss: 772.3201 - loglik: -7.8068e+02 - logprior: 8.3630
Epoch 7/10
10/10 - 3s - loss: 771.0317 - loglik: -7.8055e+02 - logprior: 9.5175
Epoch 8/10
10/10 - 3s - loss: 769.7617 - loglik: -7.8023e+02 - logprior: 10.4723
Epoch 9/10
10/10 - 3s - loss: 769.5640 - loglik: -7.8076e+02 - logprior: 11.1947
Epoch 10/10
10/10 - 3s - loss: 768.8578 - loglik: -7.8063e+02 - logprior: 11.7745
Fitted a model with MAP estimate = -768.5613
Time for alignment: 84.8761
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1038.2545 - loglik: -9.5635e+02 - logprior: -8.1905e+01
Epoch 2/10
10/10 - 2s - loss: 925.5547 - loglik: -9.0898e+02 - logprior: -1.6580e+01
Epoch 3/10
10/10 - 2s - loss: 871.4113 - loglik: -8.6600e+02 - logprior: -5.4094e+00
Epoch 4/10
10/10 - 2s - loss: 840.1429 - loglik: -8.3795e+02 - logprior: -2.1945e+00
Epoch 5/10
10/10 - 2s - loss: 826.0299 - loglik: -8.2527e+02 - logprior: -7.5646e-01
Epoch 6/10
10/10 - 2s - loss: 820.1027 - loglik: -8.2046e+02 - logprior: 0.3533
Epoch 7/10
10/10 - 2s - loss: 816.8686 - loglik: -8.1778e+02 - logprior: 0.9083
Epoch 8/10
10/10 - 2s - loss: 815.0395 - loglik: -8.1637e+02 - logprior: 1.3287
Epoch 9/10
10/10 - 2s - loss: 813.9840 - loglik: -8.1563e+02 - logprior: 1.6465
Epoch 10/10
10/10 - 2s - loss: 812.7660 - loglik: -8.1469e+02 - logprior: 1.9282
Fitted a model with MAP estimate = -812.7868
expansions: [(11, 3), (12, 2), (17, 2), (26, 1), (27, 1), (28, 1), (37, 1), (38, 2), (39, 2), (62, 1), (72, 1), (78, 2), (79, 3), (80, 2), (89, 1), (90, 1), (103, 1), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 893.6508 - loglik: -8.1391e+02 - logprior: -7.9742e+01
Epoch 2/2
10/10 - 3s - loss: 826.1754 - loglik: -7.9773e+02 - logprior: -2.8444e+01
Fitted a model with MAP estimate = -814.5788
expansions: [(0, 1), (21, 1)]
discards: [ 0 11 48 51 97]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 853.6262 - loglik: -7.9475e+02 - logprior: -5.8876e+01
Epoch 2/2
10/10 - 3s - loss: 799.9976 - loglik: -7.8938e+02 - logprior: -1.0618e+01
Fitted a model with MAP estimate = -791.9710
expansions: []
discards: [139 160]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 848.8995 - loglik: -7.9162e+02 - logprior: -5.7279e+01
Epoch 2/10
10/10 - 3s - loss: 797.7679 - loglik: -7.8808e+02 - logprior: -9.6914e+00
Epoch 3/10
10/10 - 3s - loss: 787.3481 - loglik: -7.8748e+02 - logprior: 0.1355
Epoch 4/10
10/10 - 3s - loss: 781.9276 - loglik: -7.8626e+02 - logprior: 4.3290
Epoch 5/10
10/10 - 3s - loss: 780.3248 - loglik: -7.8710e+02 - logprior: 6.7711
Epoch 6/10
10/10 - 3s - loss: 778.6327 - loglik: -7.8693e+02 - logprior: 8.2950
Epoch 7/10
10/10 - 3s - loss: 777.5843 - loglik: -7.8685e+02 - logprior: 9.2690
Epoch 8/10
10/10 - 3s - loss: 775.9892 - loglik: -7.8585e+02 - logprior: 9.8592
Epoch 9/10
10/10 - 3s - loss: 775.4122 - loglik: -7.8578e+02 - logprior: 10.3699
Epoch 10/10
10/10 - 3s - loss: 775.5154 - loglik: -7.8652e+02 - logprior: 11.0089
Fitted a model with MAP estimate = -774.7615
Time for alignment: 83.8560
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1037.8904 - loglik: -9.5599e+02 - logprior: -8.1904e+01
Epoch 2/10
10/10 - 2s - loss: 925.4485 - loglik: -9.0889e+02 - logprior: -1.6559e+01
Epoch 3/10
10/10 - 2s - loss: 870.1367 - loglik: -8.6483e+02 - logprior: -5.3020e+00
Epoch 4/10
10/10 - 2s - loss: 839.7715 - loglik: -8.3768e+02 - logprior: -2.0880e+00
Epoch 5/10
10/10 - 2s - loss: 828.1912 - loglik: -8.2768e+02 - logprior: -5.1405e-01
Epoch 6/10
10/10 - 2s - loss: 820.4689 - loglik: -8.2088e+02 - logprior: 0.4121
Epoch 7/10
10/10 - 2s - loss: 818.1268 - loglik: -8.1916e+02 - logprior: 1.0352
Epoch 8/10
10/10 - 2s - loss: 815.1802 - loglik: -8.1666e+02 - logprior: 1.4792
Epoch 9/10
10/10 - 2s - loss: 814.4742 - loglik: -8.1628e+02 - logprior: 1.8085
Epoch 10/10
10/10 - 2s - loss: 814.0420 - loglik: -8.1618e+02 - logprior: 2.1357
Fitted a model with MAP estimate = -813.6895
expansions: [(11, 3), (12, 2), (16, 3), (24, 1), (26, 1), (27, 1), (28, 1), (38, 2), (39, 2), (60, 1), (69, 1), (78, 2), (79, 3), (80, 2), (89, 1), (90, 1), (110, 2), (114, 2), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 892.6407 - loglik: -8.1325e+02 - logprior: -7.9388e+01
Epoch 2/2
10/10 - 3s - loss: 823.7188 - loglik: -7.9569e+02 - logprior: -2.8025e+01
Fitted a model with MAP estimate = -811.4393
expansions: [(0, 2), (82, 1), (129, 1)]
discards: [ 0 11]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 848.1819 - loglik: -7.8953e+02 - logprior: -5.8649e+01
Epoch 2/2
10/10 - 3s - loss: 794.0919 - loglik: -7.8405e+02 - logprior: -1.0042e+01
Fitted a model with MAP estimate = -784.7314
expansions: []
discards: [  0 164]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 857.7495 - loglik: -7.8681e+02 - logprior: -7.0935e+01
Epoch 2/10
10/10 - 3s - loss: 801.4550 - loglik: -7.8457e+02 - logprior: -1.6887e+01
Epoch 3/10
10/10 - 3s - loss: 782.4744 - loglik: -7.8135e+02 - logprior: -1.1267e+00
Epoch 4/10
10/10 - 3s - loss: 775.4272 - loglik: -7.7980e+02 - logprior: 4.3738
Epoch 5/10
10/10 - 3s - loss: 772.5386 - loglik: -7.7940e+02 - logprior: 6.8649
Epoch 6/10
10/10 - 3s - loss: 770.8575 - loglik: -7.7918e+02 - logprior: 8.3233
Epoch 7/10
10/10 - 3s - loss: 769.3132 - loglik: -7.7881e+02 - logprior: 9.4960
Epoch 8/10
10/10 - 3s - loss: 768.8817 - loglik: -7.7933e+02 - logprior: 10.4480
Epoch 9/10
10/10 - 3s - loss: 768.1915 - loglik: -7.7936e+02 - logprior: 11.1677
Epoch 10/10
10/10 - 3s - loss: 767.7876 - loglik: -7.7952e+02 - logprior: 11.7372
Fitted a model with MAP estimate = -767.2118
Time for alignment: 84.8798
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1038.1698 - loglik: -9.5627e+02 - logprior: -8.1904e+01
Epoch 2/10
10/10 - 2s - loss: 925.5582 - loglik: -9.0898e+02 - logprior: -1.6580e+01
Epoch 3/10
10/10 - 2s - loss: 870.4862 - loglik: -8.6509e+02 - logprior: -5.3930e+00
Epoch 4/10
10/10 - 2s - loss: 840.6592 - loglik: -8.3861e+02 - logprior: -2.0472e+00
Epoch 5/10
10/10 - 2s - loss: 828.1066 - loglik: -8.2780e+02 - logprior: -3.0368e-01
Epoch 6/10
10/10 - 2s - loss: 821.4966 - loglik: -8.2205e+02 - logprior: 0.5552
Epoch 7/10
10/10 - 2s - loss: 817.6410 - loglik: -8.1877e+02 - logprior: 1.1319
Epoch 8/10
10/10 - 2s - loss: 814.7982 - loglik: -8.1638e+02 - logprior: 1.5853
Epoch 9/10
10/10 - 2s - loss: 813.2549 - loglik: -8.1527e+02 - logprior: 2.0129
Epoch 10/10
10/10 - 2s - loss: 812.6823 - loglik: -8.1497e+02 - logprior: 2.2905
Fitted a model with MAP estimate = -812.0904
expansions: [(11, 3), (12, 2), (16, 3), (24, 1), (26, 2), (29, 2), (38, 1), (40, 2), (51, 1), (56, 1), (66, 1), (71, 1), (77, 2), (78, 2), (80, 2), (89, 1), (90, 2), (110, 2), (114, 3), (115, 1), (120, 1), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 892.7693 - loglik: -8.1311e+02 - logprior: -7.9658e+01
Epoch 2/2
10/10 - 3s - loss: 823.5240 - loglik: -7.9520e+02 - logprior: -2.8327e+01
Fitted a model with MAP estimate = -811.6771
expansions: [(0, 1), (85, 2), (131, 1), (153, 1)]
discards: [  0  11  35 117]
Re-initialized the encoder parameters.
Fitting a model of length 176 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 849.7346 - loglik: -7.9072e+02 - logprior: -5.9016e+01
Epoch 2/2
10/10 - 3s - loss: 795.0851 - loglik: -7.8472e+02 - logprior: -1.0361e+01
Fitted a model with MAP estimate = -785.5145
expansions: [(62, 1)]
discards: [ 84 144 165]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 842.8389 - loglik: -7.8582e+02 - logprior: -5.7016e+01
Epoch 2/10
10/10 - 3s - loss: 792.0309 - loglik: -7.8267e+02 - logprior: -9.3653e+00
Epoch 3/10
10/10 - 3s - loss: 780.2904 - loglik: -7.8079e+02 - logprior: 0.5003
Epoch 4/10
10/10 - 3s - loss: 775.9481 - loglik: -7.8071e+02 - logprior: 4.7622
Epoch 5/10
10/10 - 3s - loss: 773.4825 - loglik: -7.8070e+02 - logprior: 7.2183
Epoch 6/10
10/10 - 3s - loss: 771.8488 - loglik: -7.8061e+02 - logprior: 8.7659
Epoch 7/10
10/10 - 3s - loss: 770.4211 - loglik: -7.8019e+02 - logprior: 9.7684
Epoch 8/10
10/10 - 3s - loss: 769.7955 - loglik: -7.8017e+02 - logprior: 10.3712
Epoch 9/10
10/10 - 3s - loss: 769.4951 - loglik: -7.8037e+02 - logprior: 10.8790
Epoch 10/10
10/10 - 3s - loss: 767.9406 - loglik: -7.7946e+02 - logprior: 11.5204
Fitted a model with MAP estimate = -768.0665
Time for alignment: 84.7870
Computed alignments with likelihoods: ['-773.3833', '-768.5613', '-774.7615', '-767.2118', '-768.0665']
Best model has likelihood: -767.2118  (prior= 12.0321 )
time for generating output: 0.2204
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sti.projection.fasta
SP score = 0.8183508989460633
Training of 5 independent models on file glob.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b1c86b280>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b07cb1af0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f528a413ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f9c12580>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b07e88c10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f528a4059a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52d9ff4610>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f528a510910>, <__main__.SimpleDirichletPrior object at 0x7f5288501e50>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 584.0884 - loglik: -5.7295e+02 - logprior: -1.1141e+01
Epoch 2/10
12/12 - 2s - loss: 545.9289 - loglik: -5.4330e+02 - logprior: -2.6260e+00
Epoch 3/10
12/12 - 2s - loss: 517.6105 - loglik: -5.1583e+02 - logprior: -1.7792e+00
Epoch 4/10
12/12 - 2s - loss: 507.1938 - loglik: -5.0544e+02 - logprior: -1.7492e+00
Epoch 5/10
12/12 - 2s - loss: 501.4380 - loglik: -4.9972e+02 - logprior: -1.7195e+00
Epoch 6/10
12/12 - 2s - loss: 500.0967 - loglik: -4.9846e+02 - logprior: -1.6364e+00
Epoch 7/10
12/12 - 2s - loss: 499.6197 - loglik: -4.9801e+02 - logprior: -1.6067e+00
Epoch 8/10
12/12 - 2s - loss: 497.6098 - loglik: -4.9588e+02 - logprior: -1.7272e+00
Epoch 9/10
12/12 - 2s - loss: 496.5798 - loglik: -4.9483e+02 - logprior: -1.7504e+00
Epoch 10/10
12/12 - 2s - loss: 497.1614 - loglik: -4.9538e+02 - logprior: -1.7808e+00
Fitted a model with MAP estimate = -496.7705
expansions: [(6, 3), (10, 3), (11, 2), (20, 1), (29, 1), (36, 3), (49, 1), (50, 3), (52, 1), (58, 3), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 511.6953 - loglik: -4.9975e+02 - logprior: -1.1944e+01
Epoch 2/2
12/12 - 2s - loss: 491.0032 - loglik: -4.8621e+02 - logprior: -4.7948e+00
Fitted a model with MAP estimate = -488.2802
expansions: [(0, 6)]
discards: [ 0 16 47 79]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 495.0091 - loglik: -4.8565e+02 - logprior: -9.3578e+00
Epoch 2/2
12/12 - 2s - loss: 485.5233 - loglik: -4.8303e+02 - logprior: -2.4924e+00
Fitted a model with MAP estimate = -483.0856
expansions: []
discards: [1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 491.7070 - loglik: -4.8278e+02 - logprior: -8.9264e+00
Epoch 2/10
12/12 - 2s - loss: 484.6155 - loglik: -4.8210e+02 - logprior: -2.5115e+00
Epoch 3/10
12/12 - 2s - loss: 482.4408 - loglik: -4.8084e+02 - logprior: -1.6051e+00
Epoch 4/10
12/12 - 2s - loss: 480.0353 - loglik: -4.7878e+02 - logprior: -1.2596e+00
Epoch 5/10
12/12 - 2s - loss: 478.4303 - loglik: -4.7733e+02 - logprior: -1.1020e+00
Epoch 6/10
12/12 - 2s - loss: 477.1576 - loglik: -4.7610e+02 - logprior: -1.0582e+00
Epoch 7/10
12/12 - 2s - loss: 475.0862 - loglik: -4.7405e+02 - logprior: -1.0362e+00
Epoch 8/10
12/12 - 2s - loss: 474.9637 - loglik: -4.7390e+02 - logprior: -1.0601e+00
Epoch 9/10
12/12 - 2s - loss: 474.8416 - loglik: -4.7376e+02 - logprior: -1.0832e+00
Epoch 10/10
12/12 - 2s - loss: 473.8092 - loglik: -4.7275e+02 - logprior: -1.0575e+00
Fitted a model with MAP estimate = -474.0704
Time for alignment: 66.2417
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 584.2010 - loglik: -5.7306e+02 - logprior: -1.1138e+01
Epoch 2/10
12/12 - 2s - loss: 544.2785 - loglik: -5.4166e+02 - logprior: -2.6186e+00
Epoch 3/10
12/12 - 2s - loss: 518.5874 - loglik: -5.1683e+02 - logprior: -1.7536e+00
Epoch 4/10
12/12 - 2s - loss: 507.6501 - loglik: -5.0595e+02 - logprior: -1.7039e+00
Epoch 5/10
12/12 - 2s - loss: 503.0422 - loglik: -5.0134e+02 - logprior: -1.7043e+00
Epoch 6/10
12/12 - 2s - loss: 501.0841 - loglik: -4.9943e+02 - logprior: -1.6548e+00
Epoch 7/10
12/12 - 2s - loss: 498.8980 - loglik: -4.9723e+02 - logprior: -1.6705e+00
Epoch 8/10
12/12 - 2s - loss: 496.8420 - loglik: -4.9499e+02 - logprior: -1.8519e+00
Epoch 9/10
12/12 - 2s - loss: 496.9252 - loglik: -4.9496e+02 - logprior: -1.9659e+00
Fitted a model with MAP estimate = -496.4714
expansions: [(8, 1), (9, 1), (10, 5), (11, 2), (20, 1), (36, 4), (49, 1), (50, 3), (52, 1), (59, 5), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 510.8628 - loglik: -4.9892e+02 - logprior: -1.1941e+01
Epoch 2/2
12/12 - 2s - loss: 489.1405 - loglik: -4.8434e+02 - logprior: -4.8049e+00
Fitted a model with MAP estimate = -486.0176
expansions: [(0, 5), (49, 1)]
discards: [ 0 12 13 79]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 493.1968 - loglik: -4.8399e+02 - logprior: -9.2041e+00
Epoch 2/2
12/12 - 2s - loss: 482.6109 - loglik: -4.8030e+02 - logprior: -2.3117e+00
Fitted a model with MAP estimate = -480.6936
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 490.1502 - loglik: -4.8132e+02 - logprior: -8.8273e+00
Epoch 2/10
12/12 - 2s - loss: 481.9199 - loglik: -4.7960e+02 - logprior: -2.3245e+00
Epoch 3/10
12/12 - 2s - loss: 479.6016 - loglik: -4.7814e+02 - logprior: -1.4606e+00
Epoch 4/10
12/12 - 2s - loss: 478.3653 - loglik: -4.7732e+02 - logprior: -1.0403e+00
Epoch 5/10
12/12 - 2s - loss: 476.3314 - loglik: -4.7543e+02 - logprior: -9.0123e-01
Epoch 6/10
12/12 - 2s - loss: 475.8095 - loglik: -4.7497e+02 - logprior: -8.4004e-01
Epoch 7/10
12/12 - 2s - loss: 473.8024 - loglik: -4.7299e+02 - logprior: -8.1731e-01
Epoch 8/10
12/12 - 2s - loss: 473.0796 - loglik: -4.7226e+02 - logprior: -8.2072e-01
Epoch 9/10
12/12 - 2s - loss: 473.3077 - loglik: -4.7245e+02 - logprior: -8.5281e-01
Fitted a model with MAP estimate = -472.9383
Time for alignment: 64.0678
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 583.5163 - loglik: -5.7238e+02 - logprior: -1.1140e+01
Epoch 2/10
12/12 - 2s - loss: 545.0634 - loglik: -5.4244e+02 - logprior: -2.6228e+00
Epoch 3/10
12/12 - 2s - loss: 518.9410 - loglik: -5.1721e+02 - logprior: -1.7354e+00
Epoch 4/10
12/12 - 2s - loss: 508.3656 - loglik: -5.0665e+02 - logprior: -1.7131e+00
Epoch 5/10
12/12 - 2s - loss: 503.7818 - loglik: -5.0203e+02 - logprior: -1.7504e+00
Epoch 6/10
12/12 - 2s - loss: 501.3353 - loglik: -4.9966e+02 - logprior: -1.6766e+00
Epoch 7/10
12/12 - 2s - loss: 499.5023 - loglik: -4.9787e+02 - logprior: -1.6310e+00
Epoch 8/10
12/12 - 2s - loss: 499.5774 - loglik: -4.9782e+02 - logprior: -1.7578e+00
Fitted a model with MAP estimate = -498.6526
expansions: [(7, 2), (8, 1), (10, 4), (11, 2), (21, 1), (36, 3), (49, 1), (50, 3), (58, 3), (59, 5), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 512.7404 - loglik: -5.0081e+02 - logprior: -1.1928e+01
Epoch 2/2
12/12 - 2s - loss: 492.3667 - loglik: -4.8756e+02 - logprior: -4.8091e+00
Fitted a model with MAP estimate = -488.6191
expansions: [(0, 5)]
discards: [ 0 14 15 75 76 79]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 495.8123 - loglik: -4.8664e+02 - logprior: -9.1746e+00
Epoch 2/2
12/12 - 2s - loss: 486.5512 - loglik: -4.8426e+02 - logprior: -2.2923e+00
Fitted a model with MAP estimate = -484.2763
expansions: []
discards: [ 1  2  3  4 49]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 494.6458 - loglik: -4.8590e+02 - logprior: -8.7450e+00
Epoch 2/10
12/12 - 2s - loss: 486.2262 - loglik: -4.8397e+02 - logprior: -2.2542e+00
Epoch 3/10
12/12 - 2s - loss: 483.8031 - loglik: -4.8238e+02 - logprior: -1.4191e+00
Epoch 4/10
12/12 - 2s - loss: 482.8585 - loglik: -4.8183e+02 - logprior: -1.0235e+00
Epoch 5/10
12/12 - 2s - loss: 481.7484 - loglik: -4.8086e+02 - logprior: -8.8770e-01
Epoch 6/10
12/12 - 2s - loss: 479.5386 - loglik: -4.7872e+02 - logprior: -8.2290e-01
Epoch 7/10
12/12 - 2s - loss: 478.4632 - loglik: -4.7768e+02 - logprior: -7.8477e-01
Epoch 8/10
12/12 - 2s - loss: 477.9207 - loglik: -4.7713e+02 - logprior: -7.9052e-01
Epoch 9/10
12/12 - 2s - loss: 478.3589 - loglik: -4.7754e+02 - logprior: -8.1884e-01
Fitted a model with MAP estimate = -477.5579
Time for alignment: 60.3135
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 584.3687 - loglik: -5.7323e+02 - logprior: -1.1139e+01
Epoch 2/10
12/12 - 2s - loss: 544.0657 - loglik: -5.4145e+02 - logprior: -2.6194e+00
Epoch 3/10
12/12 - 2s - loss: 517.2780 - loglik: -5.1551e+02 - logprior: -1.7635e+00
Epoch 4/10
12/12 - 2s - loss: 509.2200 - loglik: -5.0746e+02 - logprior: -1.7640e+00
Epoch 5/10
12/12 - 2s - loss: 502.5349 - loglik: -5.0074e+02 - logprior: -1.7959e+00
Epoch 6/10
12/12 - 2s - loss: 500.2132 - loglik: -4.9851e+02 - logprior: -1.7051e+00
Epoch 7/10
12/12 - 2s - loss: 498.6152 - loglik: -4.9691e+02 - logprior: -1.7066e+00
Epoch 8/10
12/12 - 2s - loss: 497.0970 - loglik: -4.9522e+02 - logprior: -1.8723e+00
Epoch 9/10
12/12 - 2s - loss: 496.4453 - loglik: -4.9456e+02 - logprior: -1.8830e+00
Epoch 10/10
12/12 - 2s - loss: 496.5432 - loglik: -4.9468e+02 - logprior: -1.8660e+00
Fitted a model with MAP estimate = -496.5036
expansions: [(6, 3), (10, 4), (13, 1), (21, 1), (36, 3), (49, 1), (50, 3), (52, 1), (58, 3), (59, 4), (61, 1), (68, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 509.9183 - loglik: -4.9790e+02 - logprior: -1.2014e+01
Epoch 2/2
12/12 - 2s - loss: 490.0331 - loglik: -4.8518e+02 - logprior: -4.8527e+00
Fitted a model with MAP estimate = -486.7027
expansions: [(0, 4)]
discards: [ 0 78]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 492.3320 - loglik: -4.8302e+02 - logprior: -9.3135e+00
Epoch 2/2
12/12 - 2s - loss: 482.3617 - loglik: -4.7990e+02 - logprior: -2.4645e+00
Fitted a model with MAP estimate = -480.0991
expansions: []
discards: [ 1  2  3 49]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 491.0680 - loglik: -4.8211e+02 - logprior: -8.9589e+00
Epoch 2/10
12/12 - 2s - loss: 482.2504 - loglik: -4.7971e+02 - logprior: -2.5405e+00
Epoch 3/10
12/12 - 2s - loss: 480.2632 - loglik: -4.7867e+02 - logprior: -1.5904e+00
Epoch 4/10
12/12 - 2s - loss: 479.6688 - loglik: -4.7846e+02 - logprior: -1.2075e+00
Epoch 5/10
12/12 - 2s - loss: 477.4009 - loglik: -4.7634e+02 - logprior: -1.0586e+00
Epoch 6/10
12/12 - 2s - loss: 475.0086 - loglik: -4.7400e+02 - logprior: -1.0093e+00
Epoch 7/10
12/12 - 2s - loss: 474.7842 - loglik: -4.7381e+02 - logprior: -9.7595e-01
Epoch 8/10
12/12 - 2s - loss: 473.8207 - loglik: -4.7279e+02 - logprior: -1.0323e+00
Epoch 9/10
12/12 - 2s - loss: 472.7049 - loglik: -4.7162e+02 - logprior: -1.0865e+00
Epoch 10/10
12/12 - 2s - loss: 472.7585 - loglik: -4.7173e+02 - logprior: -1.0292e+00
Fitted a model with MAP estimate = -472.4708
Time for alignment: 64.6687
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 584.0123 - loglik: -5.7287e+02 - logprior: -1.1138e+01
Epoch 2/10
12/12 - 2s - loss: 545.2003 - loglik: -5.4257e+02 - logprior: -2.6284e+00
Epoch 3/10
12/12 - 2s - loss: 520.1685 - loglik: -5.1837e+02 - logprior: -1.8032e+00
Epoch 4/10
12/12 - 2s - loss: 509.1421 - loglik: -5.0733e+02 - logprior: -1.8139e+00
Epoch 5/10
12/12 - 2s - loss: 503.4178 - loglik: -5.0158e+02 - logprior: -1.8415e+00
Epoch 6/10
12/12 - 2s - loss: 499.4400 - loglik: -4.9768e+02 - logprior: -1.7556e+00
Epoch 7/10
12/12 - 2s - loss: 498.4442 - loglik: -4.9669e+02 - logprior: -1.7559e+00
Epoch 8/10
12/12 - 2s - loss: 496.7939 - loglik: -4.9488e+02 - logprior: -1.9122e+00
Epoch 9/10
12/12 - 2s - loss: 496.9021 - loglik: -4.9494e+02 - logprior: -1.9573e+00
Fitted a model with MAP estimate = -496.2957
expansions: [(6, 3), (10, 4), (13, 1), (21, 1), (36, 3), (49, 1), (50, 3), (52, 1), (59, 1), (60, 3), (61, 1), (65, 1), (67, 2), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 508.9359 - loglik: -4.9695e+02 - logprior: -1.1988e+01
Epoch 2/2
12/12 - 2s - loss: 490.3773 - loglik: -4.8550e+02 - logprior: -4.8778e+00
Fitted a model with MAP estimate = -486.5928
expansions: [(0, 4)]
discards: [ 0 46 78 89]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 492.9301 - loglik: -4.8378e+02 - logprior: -9.1531e+00
Epoch 2/2
12/12 - 2s - loss: 483.0036 - loglik: -4.8069e+02 - logprior: -2.3160e+00
Fitted a model with MAP estimate = -480.8630
expansions: []
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 493.4260 - loglik: -4.8232e+02 - logprior: -1.1101e+01
Epoch 2/10
12/12 - 2s - loss: 484.8274 - loglik: -4.8137e+02 - logprior: -3.4596e+00
Epoch 3/10
12/12 - 2s - loss: 481.4326 - loglik: -4.7980e+02 - logprior: -1.6333e+00
Epoch 4/10
12/12 - 2s - loss: 478.5880 - loglik: -4.7748e+02 - logprior: -1.1039e+00
Epoch 5/10
12/12 - 2s - loss: 476.7468 - loglik: -4.7575e+02 - logprior: -9.9870e-01
Epoch 6/10
12/12 - 2s - loss: 475.7271 - loglik: -4.7481e+02 - logprior: -9.1520e-01
Epoch 7/10
12/12 - 2s - loss: 474.8938 - loglik: -4.7396e+02 - logprior: -9.3331e-01
Epoch 8/10
12/12 - 2s - loss: 473.2680 - loglik: -4.7231e+02 - logprior: -9.5775e-01
Epoch 9/10
12/12 - 2s - loss: 473.0736 - loglik: -4.7203e+02 - logprior: -1.0416e+00
Epoch 10/10
12/12 - 2s - loss: 473.7385 - loglik: -4.7276e+02 - logprior: -9.7805e-01
Fitted a model with MAP estimate = -472.9797
Time for alignment: 62.5283
Computed alignments with likelihoods: ['-474.0704', '-472.9383', '-477.5579', '-472.4708', '-472.9797']
Best model has likelihood: -472.4708  (prior= -1.0438 )
time for generating output: 0.1936
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/glob.projection.fasta
SP score = 0.8653569341091696
Training of 5 independent models on file az.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f534447f160>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5300041be0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f528aed3f70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f53440f19a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f53440f1e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5964465250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f53004b8100>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f598c249760>, <__main__.SimpleDirichletPrior object at 0x7f52d85caac0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 628.6251 - loglik: -5.8361e+02 - logprior: -4.5019e+01
Epoch 2/10
10/10 - 1s - loss: 571.2293 - loglik: -5.6010e+02 - logprior: -1.1133e+01
Epoch 3/10
10/10 - 1s - loss: 545.4978 - loglik: -5.4092e+02 - logprior: -4.5760e+00
Epoch 4/10
10/10 - 1s - loss: 532.3597 - loglik: -5.3003e+02 - logprior: -2.3304e+00
Epoch 5/10
10/10 - 1s - loss: 526.3889 - loglik: -5.2506e+02 - logprior: -1.3318e+00
Epoch 6/10
10/10 - 1s - loss: 523.1329 - loglik: -5.2226e+02 - logprior: -8.7208e-01
Epoch 7/10
10/10 - 1s - loss: 522.1069 - loglik: -5.2148e+02 - logprior: -6.2226e-01
Epoch 8/10
10/10 - 1s - loss: 521.1008 - loglik: -5.2074e+02 - logprior: -3.5986e-01
Epoch 9/10
10/10 - 1s - loss: 520.6524 - loglik: -5.2049e+02 - logprior: -1.6649e-01
Epoch 10/10
10/10 - 1s - loss: 520.0558 - loglik: -5.2000e+02 - logprior: -6.0121e-02
Fitted a model with MAP estimate = -519.8920
expansions: [(0, 3), (5, 1), (8, 1), (36, 1), (37, 2), (44, 8), (51, 1), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 571.4056 - loglik: -5.1790e+02 - logprior: -5.3509e+01
Epoch 2/2
10/10 - 1s - loss: 528.4627 - loglik: -5.1270e+02 - logprior: -1.5759e+01
Fitted a model with MAP estimate = -519.9149
expansions: [(7, 1)]
discards: [ 0  1 71]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 557.6624 - loglik: -5.1181e+02 - logprior: -4.5852e+01
Epoch 2/2
10/10 - 1s - loss: 528.3011 - loglik: -5.1062e+02 - logprior: -1.7686e+01
Fitted a model with MAP estimate = -523.2519
expansions: [(0, 4), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 549.9121 - loglik: -5.0889e+02 - logprior: -4.1022e+01
Epoch 2/10
10/10 - 1s - loss: 518.3647 - loglik: -5.0822e+02 - logprior: -1.0142e+01
Epoch 3/10
10/10 - 1s - loss: 511.3685 - loglik: -5.0812e+02 - logprior: -3.2522e+00
Epoch 4/10
10/10 - 1s - loss: 509.0195 - loglik: -5.0828e+02 - logprior: -7.3915e-01
Epoch 5/10
10/10 - 1s - loss: 508.1461 - loglik: -5.0867e+02 - logprior: 0.5226
Epoch 6/10
10/10 - 1s - loss: 506.7156 - loglik: -5.0792e+02 - logprior: 1.2020
Epoch 7/10
10/10 - 1s - loss: 507.0641 - loglik: -5.0865e+02 - logprior: 1.5899
Fitted a model with MAP estimate = -506.6554
Time for alignment: 49.1051
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 628.9359 - loglik: -5.8392e+02 - logprior: -4.5020e+01
Epoch 2/10
10/10 - 1s - loss: 571.7843 - loglik: -5.6065e+02 - logprior: -1.1130e+01
Epoch 3/10
10/10 - 1s - loss: 543.9573 - loglik: -5.3943e+02 - logprior: -4.5284e+00
Epoch 4/10
10/10 - 1s - loss: 532.4515 - loglik: -5.3024e+02 - logprior: -2.2071e+00
Epoch 5/10
10/10 - 1s - loss: 526.0761 - loglik: -5.2488e+02 - logprior: -1.1960e+00
Epoch 6/10
10/10 - 1s - loss: 523.6901 - loglik: -5.2305e+02 - logprior: -6.4049e-01
Epoch 7/10
10/10 - 1s - loss: 522.0393 - loglik: -5.2169e+02 - logprior: -3.4731e-01
Epoch 8/10
10/10 - 1s - loss: 521.6234 - loglik: -5.2149e+02 - logprior: -1.3188e-01
Epoch 9/10
10/10 - 1s - loss: 520.8791 - loglik: -5.2092e+02 - logprior: 0.0406
Epoch 10/10
10/10 - 1s - loss: 520.0680 - loglik: -5.2022e+02 - logprior: 0.1474
Fitted a model with MAP estimate = -520.0757
expansions: [(0, 3), (8, 3), (36, 1), (43, 10), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 569.3906 - loglik: -5.1614e+02 - logprior: -5.3250e+01
Epoch 2/2
10/10 - 1s - loss: 526.2150 - loglik: -5.1080e+02 - logprior: -1.5420e+01
Fitted a model with MAP estimate = -518.7143
expansions: []
discards: [ 0  1  2 71]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 547.1598 - loglik: -5.1001e+02 - logprior: -3.7152e+01
Epoch 2/2
10/10 - 1s - loss: 518.9606 - loglik: -5.1008e+02 - logprior: -8.8829e+00
Fitted a model with MAP estimate = -514.1458
expansions: [(0, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 559.7856 - loglik: -5.0967e+02 - logprior: -5.0116e+01
Epoch 2/10
10/10 - 1s - loss: 522.6166 - loglik: -5.0872e+02 - logprior: -1.3895e+01
Epoch 3/10
10/10 - 1s - loss: 513.1761 - loglik: -5.0815e+02 - logprior: -5.0221e+00
Epoch 4/10
10/10 - 1s - loss: 510.7233 - loglik: -5.0912e+02 - logprior: -1.6041e+00
Epoch 5/10
10/10 - 1s - loss: 508.2482 - loglik: -5.0828e+02 - logprior: 0.0350
Epoch 6/10
10/10 - 1s - loss: 508.8747 - loglik: -5.0972e+02 - logprior: 0.8438
Fitted a model with MAP estimate = -507.7401
Time for alignment: 46.4756
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 628.4470 - loglik: -5.8343e+02 - logprior: -4.5019e+01
Epoch 2/10
10/10 - 1s - loss: 571.4606 - loglik: -5.6034e+02 - logprior: -1.1123e+01
Epoch 3/10
10/10 - 1s - loss: 544.1606 - loglik: -5.3964e+02 - logprior: -4.5172e+00
Epoch 4/10
10/10 - 1s - loss: 530.6718 - loglik: -5.2845e+02 - logprior: -2.2259e+00
Epoch 5/10
10/10 - 1s - loss: 525.1980 - loglik: -5.2399e+02 - logprior: -1.2069e+00
Epoch 6/10
10/10 - 1s - loss: 524.0436 - loglik: -5.2326e+02 - logprior: -7.8433e-01
Epoch 7/10
10/10 - 1s - loss: 522.3062 - loglik: -5.2179e+02 - logprior: -5.1534e-01
Epoch 8/10
10/10 - 1s - loss: 521.4267 - loglik: -5.2118e+02 - logprior: -2.4226e-01
Epoch 9/10
10/10 - 1s - loss: 520.1994 - loglik: -5.2013e+02 - logprior: -7.0235e-02
Epoch 10/10
10/10 - 1s - loss: 519.1236 - loglik: -5.1915e+02 - logprior: 0.0278
Fitted a model with MAP estimate = -519.3797
expansions: [(0, 3), (5, 1), (8, 1), (36, 4), (38, 1), (43, 11), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 571.1083 - loglik: -5.1750e+02 - logprior: -5.3606e+01
Epoch 2/2
10/10 - 1s - loss: 526.9819 - loglik: -5.1109e+02 - logprior: -1.5897e+01
Fitted a model with MAP estimate = -518.9458
expansions: [(7, 1)]
discards: [ 0  1 43 75]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 556.2748 - loglik: -5.1058e+02 - logprior: -4.5692e+01
Epoch 2/2
10/10 - 1s - loss: 526.4855 - loglik: -5.0884e+02 - logprior: -1.7645e+01
Fitted a model with MAP estimate = -522.0655
expansions: [(0, 3)]
discards: [ 0 45 54 55 56 57 58]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 552.3055 - loglik: -5.1164e+02 - logprior: -4.0663e+01
Epoch 2/10
10/10 - 1s - loss: 520.2798 - loglik: -5.1010e+02 - logprior: -1.0182e+01
Epoch 3/10
10/10 - 1s - loss: 513.0992 - loglik: -5.0962e+02 - logprior: -3.4747e+00
Epoch 4/10
10/10 - 1s - loss: 511.3229 - loglik: -5.1040e+02 - logprior: -9.2304e-01
Epoch 5/10
10/10 - 1s - loss: 509.1403 - loglik: -5.0950e+02 - logprior: 0.3616
Epoch 6/10
10/10 - 1s - loss: 509.2907 - loglik: -5.1032e+02 - logprior: 1.0308
Fitted a model with MAP estimate = -508.8510
Time for alignment: 46.8355
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 628.1676 - loglik: -5.8315e+02 - logprior: -4.5018e+01
Epoch 2/10
10/10 - 1s - loss: 572.2080 - loglik: -5.6109e+02 - logprior: -1.1121e+01
Epoch 3/10
10/10 - 1s - loss: 544.4631 - loglik: -5.3994e+02 - logprior: -4.5250e+00
Epoch 4/10
10/10 - 1s - loss: 532.3065 - loglik: -5.3011e+02 - logprior: -2.1944e+00
Epoch 5/10
10/10 - 1s - loss: 526.6342 - loglik: -5.2549e+02 - logprior: -1.1426e+00
Epoch 6/10
10/10 - 1s - loss: 523.3840 - loglik: -5.2266e+02 - logprior: -7.2273e-01
Epoch 7/10
10/10 - 1s - loss: 521.2057 - loglik: -5.2065e+02 - logprior: -5.5686e-01
Epoch 8/10
10/10 - 1s - loss: 520.4382 - loglik: -5.2015e+02 - logprior: -2.9317e-01
Epoch 9/10
10/10 - 1s - loss: 519.9666 - loglik: -5.1988e+02 - logprior: -8.9901e-02
Epoch 10/10
10/10 - 1s - loss: 518.8022 - loglik: -5.1877e+02 - logprior: -2.9028e-02
Fitted a model with MAP estimate = -518.8081
expansions: [(0, 3), (5, 1), (8, 1), (36, 1), (43, 12), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 97 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 569.8766 - loglik: -5.1641e+02 - logprior: -5.3469e+01
Epoch 2/2
10/10 - 1s - loss: 527.8174 - loglik: -5.1210e+02 - logprior: -1.5713e+01
Fitted a model with MAP estimate = -519.5619
expansions: []
discards: [ 0  1 54 55 72]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 558.0481 - loglik: -5.1227e+02 - logprior: -4.5778e+01
Epoch 2/2
10/10 - 1s - loss: 528.4387 - loglik: -5.1086e+02 - logprior: -1.7580e+01
Fitted a model with MAP estimate = -523.8635
expansions: [(0, 3), (41, 1)]
discards: [ 0 51]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 551.6514 - loglik: -5.1083e+02 - logprior: -4.0818e+01
Epoch 2/10
10/10 - 1s - loss: 518.9653 - loglik: -5.0890e+02 - logprior: -1.0069e+01
Epoch 3/10
10/10 - 1s - loss: 512.4745 - loglik: -5.0925e+02 - logprior: -3.2289e+00
Epoch 4/10
10/10 - 1s - loss: 509.6006 - loglik: -5.0886e+02 - logprior: -7.4531e-01
Epoch 5/10
10/10 - 1s - loss: 508.9394 - loglik: -5.0944e+02 - logprior: 0.4966
Epoch 6/10
10/10 - 1s - loss: 507.4115 - loglik: -5.0855e+02 - logprior: 1.1387
Epoch 7/10
10/10 - 1s - loss: 508.1822 - loglik: -5.0969e+02 - logprior: 1.5125
Fitted a model with MAP estimate = -507.5500
Time for alignment: 47.7329
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 628.0255 - loglik: -5.8301e+02 - logprior: -4.5019e+01
Epoch 2/10
10/10 - 1s - loss: 572.7244 - loglik: -5.6160e+02 - logprior: -1.1129e+01
Epoch 3/10
10/10 - 1s - loss: 545.8654 - loglik: -5.4130e+02 - logprior: -4.5607e+00
Epoch 4/10
10/10 - 1s - loss: 533.3258 - loglik: -5.3109e+02 - logprior: -2.2380e+00
Epoch 5/10
10/10 - 1s - loss: 527.1384 - loglik: -5.2598e+02 - logprior: -1.1556e+00
Epoch 6/10
10/10 - 1s - loss: 525.0123 - loglik: -5.2436e+02 - logprior: -6.5591e-01
Epoch 7/10
10/10 - 1s - loss: 524.0999 - loglik: -5.2370e+02 - logprior: -4.0397e-01
Epoch 8/10
10/10 - 1s - loss: 523.2510 - loglik: -5.2305e+02 - logprior: -1.9628e-01
Epoch 9/10
10/10 - 1s - loss: 522.1602 - loglik: -5.2212e+02 - logprior: -4.0050e-02
Epoch 10/10
10/10 - 1s - loss: 521.6498 - loglik: -5.2170e+02 - logprior: 0.0462
Fitted a model with MAP estimate = -521.3572
expansions: [(0, 3), (8, 3), (36, 1), (37, 2), (44, 11), (51, 1), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 571.7064 - loglik: -5.1830e+02 - logprior: -5.3407e+01
Epoch 2/2
10/10 - 1s - loss: 526.9152 - loglik: -5.1117e+02 - logprior: -1.5748e+01
Fitted a model with MAP estimate = -518.4829
expansions: []
discards: [ 0  1 57 58 75]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 557.0115 - loglik: -5.1144e+02 - logprior: -4.5572e+01
Epoch 2/2
10/10 - 1s - loss: 528.3241 - loglik: -5.1078e+02 - logprior: -1.7547e+01
Fitted a model with MAP estimate = -523.1837
expansions: [(0, 3)]
discards: [ 0 55]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 551.3158 - loglik: -5.1039e+02 - logprior: -4.0922e+01
Epoch 2/10
10/10 - 1s - loss: 519.4111 - loglik: -5.0921e+02 - logprior: -1.0203e+01
Epoch 3/10
10/10 - 1s - loss: 512.6243 - loglik: -5.0929e+02 - logprior: -3.3334e+00
Epoch 4/10
10/10 - 1s - loss: 510.1860 - loglik: -5.0937e+02 - logprior: -8.1192e-01
Epoch 5/10
10/10 - 1s - loss: 508.7221 - loglik: -5.0918e+02 - logprior: 0.4605
Epoch 6/10
10/10 - 1s - loss: 508.4129 - loglik: -5.0955e+02 - logprior: 1.1401
Epoch 7/10
10/10 - 1s - loss: 508.2161 - loglik: -5.0974e+02 - logprior: 1.5237
Epoch 8/10
10/10 - 1s - loss: 508.1140 - loglik: -5.0988e+02 - logprior: 1.7699
Epoch 9/10
10/10 - 1s - loss: 507.2884 - loglik: -5.0928e+02 - logprior: 1.9901
Epoch 10/10
10/10 - 1s - loss: 507.5578 - loglik: -5.0976e+02 - logprior: 2.2002
Fitted a model with MAP estimate = -507.2232
Time for alignment: 51.9762
Computed alignments with likelihoods: ['-506.6554', '-507.7401', '-508.8510', '-507.5500', '-507.2232']
Best model has likelihood: -506.6554  (prior= 1.7453 )
time for generating output: 0.1968
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/az.projection.fasta
SP score = 0.7242366660592967
Training of 5 independent models on file ghf11.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f531c6317c0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5301e1e8b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5301e28fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52d989e550>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5334fc5d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b1c597220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5335d928e0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f530decdbe0>, <__main__.SimpleDirichletPrior object at 0x7f5ca7fbe2b0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 1073.0166 - loglik: -9.7623e+02 - logprior: -9.6789e+01
Epoch 2/10
10/10 - 3s - loss: 926.5236 - loglik: -9.0713e+02 - logprior: -1.9389e+01
Epoch 3/10
10/10 - 3s - loss: 845.6447 - loglik: -8.3880e+02 - logprior: -6.8444e+00
Epoch 4/10
10/10 - 3s - loss: 799.1285 - loglik: -7.9484e+02 - logprior: -4.2910e+00
Epoch 5/10
10/10 - 3s - loss: 780.4572 - loglik: -7.7744e+02 - logprior: -3.0187e+00
Epoch 6/10
10/10 - 3s - loss: 774.3945 - loglik: -7.7293e+02 - logprior: -1.4626e+00
Epoch 7/10
10/10 - 3s - loss: 771.2774 - loglik: -7.7092e+02 - logprior: -3.6063e-01
Epoch 8/10
10/10 - 3s - loss: 768.8107 - loglik: -7.6900e+02 - logprior: 0.1931
Epoch 9/10
10/10 - 3s - loss: 768.9740 - loglik: -7.6951e+02 - logprior: 0.5402
Fitted a model with MAP estimate = -768.1590
expansions: [(18, 1), (20, 1), (22, 1), (26, 1), (39, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 2), (124, 1), (125, 1), (128, 1), (129, 3), (133, 1), (137, 1), (138, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 855.6071 - loglik: -7.6285e+02 - logprior: -9.2753e+01
Epoch 2/2
10/10 - 4s - loss: 777.1349 - loglik: -7.4434e+02 - logprior: -3.2799e+01
Fitted a model with MAP estimate = -765.6662
expansions: [(0, 3), (15, 4), (48, 2), (81, 1)]
discards: [  0 113]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 808.7552 - loglik: -7.3770e+02 - logprior: -7.1060e+01
Epoch 2/2
10/10 - 2s - loss: 742.6839 - loglik: -7.3042e+02 - logprior: -1.2263e+01
Fitted a model with MAP estimate = -731.1077
expansions: [(16, 1), (22, 1), (49, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 815.3382 - loglik: -7.2994e+02 - logprior: -8.5396e+01
Epoch 2/10
10/10 - 2s - loss: 751.4453 - loglik: -7.2536e+02 - logprior: -2.6090e+01
Epoch 3/10
10/10 - 2s - loss: 727.5121 - loglik: -7.2233e+02 - logprior: -5.1819e+00
Epoch 4/10
10/10 - 2s - loss: 720.2597 - loglik: -7.2612e+02 - logprior: 5.8596
Epoch 5/10
10/10 - 2s - loss: 711.4310 - loglik: -7.2118e+02 - logprior: 9.7467
Epoch 6/10
10/10 - 2s - loss: 710.4332 - loglik: -7.2210e+02 - logprior: 11.6675
Epoch 7/10
10/10 - 2s - loss: 710.2031 - loglik: -7.2312e+02 - logprior: 12.9121
Epoch 8/10
10/10 - 2s - loss: 709.6086 - loglik: -7.2350e+02 - logprior: 13.8964
Epoch 9/10
10/10 - 2s - loss: 707.2150 - loglik: -7.2197e+02 - logprior: 14.7517
Epoch 10/10
10/10 - 2s - loss: 708.1982 - loglik: -7.2368e+02 - logprior: 15.4775
Fitted a model with MAP estimate = -707.2304
Time for alignment: 80.4494
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 1072.9243 - loglik: -9.7613e+02 - logprior: -9.6790e+01
Epoch 2/10
10/10 - 3s - loss: 925.6831 - loglik: -9.0629e+02 - logprior: -1.9388e+01
Epoch 3/10
10/10 - 3s - loss: 846.9293 - loglik: -8.4008e+02 - logprior: -6.8461e+00
Epoch 4/10
10/10 - 3s - loss: 796.3647 - loglik: -7.9199e+02 - logprior: -4.3703e+00
Epoch 5/10
10/10 - 3s - loss: 780.4791 - loglik: -7.7731e+02 - logprior: -3.1715e+00
Epoch 6/10
10/10 - 3s - loss: 773.5869 - loglik: -7.7201e+02 - logprior: -1.5792e+00
Epoch 7/10
10/10 - 3s - loss: 770.0526 - loglik: -7.6941e+02 - logprior: -6.4082e-01
Epoch 8/10
10/10 - 3s - loss: 768.7975 - loglik: -7.6844e+02 - logprior: -3.5598e-01
Epoch 9/10
10/10 - 3s - loss: 768.7558 - loglik: -7.6876e+02 - logprior: 0.0021
Epoch 10/10
10/10 - 3s - loss: 765.8475 - loglik: -7.6633e+02 - logprior: 0.4828
Fitted a model with MAP estimate = -766.2066
expansions: [(18, 1), (20, 1), (22, 1), (26, 1), (39, 4), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (127, 2), (128, 1), (129, 1), (133, 1), (137, 1), (138, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 851.9183 - loglik: -7.5905e+02 - logprior: -9.2869e+01
Epoch 2/2
10/10 - 2s - loss: 772.7668 - loglik: -7.4017e+02 - logprior: -3.2599e+01
Fitted a model with MAP estimate = -760.1686
expansions: [(0, 3), (15, 3), (86, 1)]
discards: [  0  43  53 118]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 805.3051 - loglik: -7.3493e+02 - logprior: -7.0374e+01
Epoch 2/2
10/10 - 2s - loss: 740.4534 - loglik: -7.2839e+02 - logprior: -1.2065e+01
Fitted a model with MAP estimate = -729.8341
expansions: [(16, 1), (17, 1), (21, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 814.0844 - loglik: -7.2876e+02 - logprior: -8.5326e+01
Epoch 2/10
10/10 - 2s - loss: 751.1195 - loglik: -7.2516e+02 - logprior: -2.5956e+01
Epoch 3/10
10/10 - 2s - loss: 728.1921 - loglik: -7.2306e+02 - logprior: -5.1286e+00
Epoch 4/10
10/10 - 2s - loss: 718.4330 - loglik: -7.2421e+02 - logprior: 5.7810
Epoch 5/10
10/10 - 2s - loss: 712.4616 - loglik: -7.2201e+02 - logprior: 9.5438
Epoch 6/10
10/10 - 2s - loss: 710.7615 - loglik: -7.2223e+02 - logprior: 11.4689
Epoch 7/10
10/10 - 2s - loss: 709.5443 - loglik: -7.2226e+02 - logprior: 12.7178
Epoch 8/10
10/10 - 2s - loss: 710.2111 - loglik: -7.2391e+02 - logprior: 13.6978
Fitted a model with MAP estimate = -708.7213
Time for alignment: 75.6169
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 1073.5652 - loglik: -9.7677e+02 - logprior: -9.6790e+01
Epoch 2/10
10/10 - 3s - loss: 925.7931 - loglik: -9.0640e+02 - logprior: -1.9391e+01
Epoch 3/10
10/10 - 3s - loss: 846.1375 - loglik: -8.3931e+02 - logprior: -6.8293e+00
Epoch 4/10
10/10 - 3s - loss: 798.0836 - loglik: -7.9368e+02 - logprior: -4.4048e+00
Epoch 5/10
10/10 - 3s - loss: 779.4260 - loglik: -7.7628e+02 - logprior: -3.1488e+00
Epoch 6/10
10/10 - 3s - loss: 774.2371 - loglik: -7.7281e+02 - logprior: -1.4249e+00
Epoch 7/10
10/10 - 3s - loss: 770.7119 - loglik: -7.7042e+02 - logprior: -2.9570e-01
Epoch 8/10
10/10 - 3s - loss: 769.1971 - loglik: -7.6938e+02 - logprior: 0.1834
Epoch 9/10
10/10 - 3s - loss: 767.8746 - loglik: -7.6843e+02 - logprior: 0.5521
Epoch 10/10
10/10 - 3s - loss: 768.8952 - loglik: -7.6990e+02 - logprior: 1.0066
Fitted a model with MAP estimate = -767.6591
expansions: [(18, 1), (20, 1), (22, 1), (26, 1), (39, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 2), (124, 1), (125, 1), (128, 1), (129, 3), (133, 1), (137, 1), (138, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 854.3131 - loglik: -7.6171e+02 - logprior: -9.2605e+01
Epoch 2/2
10/10 - 4s - loss: 778.6583 - loglik: -7.4590e+02 - logprior: -3.2756e+01
Fitted a model with MAP estimate = -765.6641
expansions: [(0, 3), (15, 3), (16, 1), (48, 2), (81, 1)]
discards: [  0 113]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 810.0086 - loglik: -7.3901e+02 - logprior: -7.1003e+01
Epoch 2/2
10/10 - 2s - loss: 742.2399 - loglik: -7.3013e+02 - logprior: -1.2112e+01
Fitted a model with MAP estimate = -731.4505
expansions: [(16, 1), (17, 1), (49, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 815.2053 - loglik: -7.2983e+02 - logprior: -8.5375e+01
Epoch 2/10
10/10 - 2s - loss: 750.8517 - loglik: -7.2474e+02 - logprior: -2.6111e+01
Epoch 3/10
10/10 - 2s - loss: 730.4635 - loglik: -7.2532e+02 - logprior: -5.1402e+00
Epoch 4/10
10/10 - 2s - loss: 718.0807 - loglik: -7.2391e+02 - logprior: 5.8252
Epoch 5/10
10/10 - 2s - loss: 713.4390 - loglik: -7.2308e+02 - logprior: 9.6449
Epoch 6/10
10/10 - 2s - loss: 711.1801 - loglik: -7.2273e+02 - logprior: 11.5522
Epoch 7/10
10/10 - 2s - loss: 709.8698 - loglik: -7.2269e+02 - logprior: 12.8200
Epoch 8/10
10/10 - 2s - loss: 710.4365 - loglik: -7.2423e+02 - logprior: 13.7913
Fitted a model with MAP estimate = -708.9580
Time for alignment: 78.1443
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 1072.9539 - loglik: -9.7616e+02 - logprior: -9.6789e+01
Epoch 2/10
10/10 - 3s - loss: 926.4402 - loglik: -9.0705e+02 - logprior: -1.9386e+01
Epoch 3/10
10/10 - 3s - loss: 845.9208 - loglik: -8.3911e+02 - logprior: -6.8088e+00
Epoch 4/10
10/10 - 3s - loss: 797.6115 - loglik: -7.9336e+02 - logprior: -4.2535e+00
Epoch 5/10
10/10 - 3s - loss: 779.4373 - loglik: -7.7615e+02 - logprior: -3.2848e+00
Epoch 6/10
10/10 - 3s - loss: 772.4836 - loglik: -7.7055e+02 - logprior: -1.9354e+00
Epoch 7/10
10/10 - 3s - loss: 769.3746 - loglik: -7.6861e+02 - logprior: -7.6357e-01
Epoch 8/10
10/10 - 3s - loss: 768.6869 - loglik: -7.6852e+02 - logprior: -1.6436e-01
Epoch 9/10
10/10 - 3s - loss: 766.7154 - loglik: -7.6685e+02 - logprior: 0.1299
Epoch 10/10
10/10 - 3s - loss: 767.2843 - loglik: -7.6766e+02 - logprior: 0.3799
Fitted a model with MAP estimate = -766.3742
expansions: [(18, 1), (20, 1), (22, 1), (26, 1), (39, 2), (40, 1), (41, 3), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 2), (124, 1), (125, 1), (128, 1), (129, 3), (133, 1), (137, 1), (138, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 853.2092 - loglik: -7.6019e+02 - logprior: -9.3024e+01
Epoch 2/2
10/10 - 2s - loss: 772.7456 - loglik: -7.4002e+02 - logprior: -3.2728e+01
Fitted a model with MAP estimate = -760.4052
expansions: [(0, 3), (15, 4), (88, 1)]
discards: [  0  47  48  55 120]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 804.7877 - loglik: -7.3437e+02 - logprior: -7.0414e+01
Epoch 2/2
10/10 - 2s - loss: 739.1363 - loglik: -7.2719e+02 - logprior: -1.1947e+01
Fitted a model with MAP estimate = -728.0672
expansions: [(16, 1), (22, 1)]
discards: [ 0  1 49]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 812.6214 - loglik: -7.2749e+02 - logprior: -8.5135e+01
Epoch 2/10
10/10 - 2s - loss: 750.3121 - loglik: -7.2455e+02 - logprior: -2.5761e+01
Epoch 3/10
10/10 - 2s - loss: 729.3459 - loglik: -7.2451e+02 - logprior: -4.8339e+00
Epoch 4/10
10/10 - 2s - loss: 716.5381 - loglik: -7.2238e+02 - logprior: 5.8437
Epoch 5/10
10/10 - 2s - loss: 713.3694 - loglik: -7.2296e+02 - logprior: 9.5883
Epoch 6/10
10/10 - 2s - loss: 710.6827 - loglik: -7.2220e+02 - logprior: 11.5139
Epoch 7/10
10/10 - 2s - loss: 711.7228 - loglik: -7.2448e+02 - logprior: 12.7585
Fitted a model with MAP estimate = -709.5583
Time for alignment: 75.1456
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 1072.4978 - loglik: -9.7571e+02 - logprior: -9.6790e+01
Epoch 2/10
10/10 - 3s - loss: 926.0956 - loglik: -9.0671e+02 - logprior: -1.9389e+01
Epoch 3/10
10/10 - 3s - loss: 846.6979 - loglik: -8.3985e+02 - logprior: -6.8453e+00
Epoch 4/10
10/10 - 3s - loss: 797.4562 - loglik: -7.9311e+02 - logprior: -4.3481e+00
Epoch 5/10
10/10 - 3s - loss: 780.7254 - loglik: -7.7761e+02 - logprior: -3.1179e+00
Epoch 6/10
10/10 - 3s - loss: 773.7195 - loglik: -7.7198e+02 - logprior: -1.7416e+00
Epoch 7/10
10/10 - 3s - loss: 770.3794 - loglik: -7.6962e+02 - logprior: -7.5949e-01
Epoch 8/10
10/10 - 3s - loss: 768.4426 - loglik: -7.6828e+02 - logprior: -1.6384e-01
Epoch 9/10
10/10 - 3s - loss: 767.1376 - loglik: -7.6734e+02 - logprior: 0.2006
Epoch 10/10
10/10 - 3s - loss: 766.7028 - loglik: -7.6735e+02 - logprior: 0.6497
Fitted a model with MAP estimate = -766.3312
expansions: [(18, 1), (20, 1), (22, 1), (26, 1), (39, 4), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 2), (124, 1), (125, 1), (128, 1), (129, 3), (133, 1), (137, 1), (138, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 852.3919 - loglik: -7.5951e+02 - logprior: -9.2883e+01
Epoch 2/2
10/10 - 2s - loss: 773.3529 - loglik: -7.4081e+02 - logprior: -3.2546e+01
Fitted a model with MAP estimate = -760.4387
expansions: [(0, 3), (15, 3), (86, 1)]
discards: [  0  43  53 118]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 805.7042 - loglik: -7.3534e+02 - logprior: -7.0361e+01
Epoch 2/2
10/10 - 2s - loss: 740.5468 - loglik: -7.2847e+02 - logprior: -1.2079e+01
Fitted a model with MAP estimate = -729.9293
expansions: [(16, 1), (17, 1), (21, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 813.6307 - loglik: -7.2831e+02 - logprior: -8.5322e+01
Epoch 2/10
10/10 - 2s - loss: 752.0198 - loglik: -7.2603e+02 - logprior: -2.5986e+01
Epoch 3/10
10/10 - 2s - loss: 728.0669 - loglik: -7.2290e+02 - logprior: -5.1650e+00
Epoch 4/10
10/10 - 2s - loss: 717.7031 - loglik: -7.2346e+02 - logprior: 5.7608
Epoch 5/10
10/10 - 2s - loss: 712.8284 - loglik: -7.2239e+02 - logprior: 9.5631
Epoch 6/10
10/10 - 2s - loss: 711.3007 - loglik: -7.2277e+02 - logprior: 11.4733
Epoch 7/10
10/10 - 2s - loss: 710.8240 - loglik: -7.2355e+02 - logprior: 12.7251
Epoch 8/10
10/10 - 2s - loss: 708.6610 - loglik: -7.2237e+02 - logprior: 13.7109
Epoch 9/10
10/10 - 2s - loss: 707.5184 - loglik: -7.2206e+02 - logprior: 14.5414
Epoch 10/10
10/10 - 2s - loss: 709.5387 - loglik: -7.2481e+02 - logprior: 15.2688
Fitted a model with MAP estimate = -707.5401
Time for alignment: 79.8706
Computed alignments with likelihoods: ['-707.2304', '-708.7213', '-708.9580', '-709.5583', '-707.5401']
Best model has likelihood: -707.2304  (prior= 15.8511 )
time for generating output: 0.2336
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf11.projection.fasta
SP score = 0.9181614349775785
Training of 5 independent models on file subt.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b12e6a6d0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f531d0849a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52949356d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530c2b5730>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52d989e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531c6317c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5301e28fa0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5b129abb80>, <__main__.SimpleDirichletPrior object at 0x7f59405128b0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 26s - loss: 1687.8639 - loglik: -1.6850e+03 - logprior: -2.8286e+00
Epoch 2/10
33/33 - 23s - loss: 1594.2887 - loglik: -1.5937e+03 - logprior: -6.1515e-01
Epoch 3/10
33/33 - 23s - loss: 1585.0912 - loglik: -1.5845e+03 - logprior: -6.2880e-01
Epoch 4/10
33/33 - 23s - loss: 1579.8236 - loglik: -1.5791e+03 - logprior: -6.7580e-01
Epoch 5/10
33/33 - 23s - loss: 1583.1626 - loglik: -1.5824e+03 - logprior: -7.9977e-01
Fitted a model with MAP estimate = -1580.3740
expansions: [(0, 4), (10, 2), (11, 1), (35, 4), (62, 1), (63, 2), (72, 1), (73, 2), (78, 1), (95, 1), (113, 1), (116, 1), (118, 1), (134, 2), (155, 3), (163, 4), (183, 1), (204, 1), (220, 2), (221, 4), (230, 3)]
discards: [  3 225 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 31s - loss: 1583.1833 - loglik: -1.5801e+03 - logprior: -3.1241e+00
Epoch 2/2
33/33 - 28s - loss: 1572.3279 - loglik: -1.5719e+03 - logprior: -3.8474e-01
Fitted a model with MAP estimate = -1571.4072
expansions: [(0, 4), (267, 3)]
discards: [  3   4   5   6   7   8  42  72 179 190 191 192 253 262 263 264 265]
Re-initialized the encoder parameters.
Fitting a model of length 257 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 1580.0105 - loglik: -1.5770e+03 - logprior: -3.0453e+00
Epoch 2/2
33/33 - 27s - loss: 1576.1841 - loglik: -1.5761e+03 - logprior: -5.2019e-02
Fitted a model with MAP estimate = -1573.2178
expansions: [(0, 4), (257, 3)]
discards: [  4  14 252 253 254 255]
Re-initialized the encoder parameters.
Fitting a model of length 258 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 33s - loss: 1578.4985 - loglik: -1.5753e+03 - logprior: -3.2259e+00
Epoch 2/10
33/33 - 26s - loss: 1577.0433 - loglik: -1.5771e+03 - logprior: 0.0337
Epoch 3/10
33/33 - 27s - loss: 1573.1656 - loglik: -1.5734e+03 - logprior: 0.2001
Epoch 4/10
33/33 - 27s - loss: 1565.5319 - loglik: -1.5657e+03 - logprior: 0.2137
Epoch 5/10
33/33 - 27s - loss: 1576.3219 - loglik: -1.5765e+03 - logprior: 0.2104
Fitted a model with MAP estimate = -1569.2721
Time for alignment: 479.9218
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 27s - loss: 1687.8560 - loglik: -1.6850e+03 - logprior: -2.8290e+00
Epoch 2/10
33/33 - 23s - loss: 1596.0492 - loglik: -1.5954e+03 - logprior: -6.8786e-01
Epoch 3/10
33/33 - 23s - loss: 1581.5317 - loglik: -1.5809e+03 - logprior: -6.2311e-01
Epoch 4/10
33/33 - 23s - loss: 1579.3613 - loglik: -1.5787e+03 - logprior: -6.1316e-01
Epoch 5/10
33/33 - 23s - loss: 1573.7130 - loglik: -1.5730e+03 - logprior: -7.3514e-01
Epoch 6/10
33/33 - 23s - loss: 1583.3765 - loglik: -1.5825e+03 - logprior: -8.4771e-01
Fitted a model with MAP estimate = -1578.3390
expansions: [(0, 5), (9, 1), (10, 1), (35, 2), (63, 1), (72, 1), (73, 1), (79, 2), (111, 1), (114, 1), (119, 1), (133, 2), (156, 3), (164, 4), (184, 1), (204, 1), (217, 1), (220, 2), (221, 4), (230, 3)]
discards: [225 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 31s - loss: 1583.3475 - loglik: -1.5803e+03 - logprior: -3.0694e+00
Epoch 2/2
33/33 - 27s - loss: 1574.0438 - loglik: -1.5737e+03 - logprior: -3.5697e-01
Fitted a model with MAP estimate = -1572.4201
expansions: [(264, 4)]
discards: [  1 176 187 188 189 250 258 259 260 261 262 263]
Re-initialized the encoder parameters.
Fitting a model of length 256 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 31s - loss: 1580.4081 - loglik: -1.5784e+03 - logprior: -1.9965e+00
Epoch 2/2
33/33 - 26s - loss: 1578.9590 - loglik: -1.5789e+03 - logprior: -2.6952e-02
Fitted a model with MAP estimate = -1573.3090
expansions: [(0, 4), (256, 4)]
discards: [  4   5   6 252 253 254]
Re-initialized the encoder parameters.
Fitting a model of length 258 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 30s - loss: 1580.8960 - loglik: -1.5779e+03 - logprior: -2.9587e+00
Epoch 2/10
33/33 - 27s - loss: 1572.5127 - loglik: -1.5726e+03 - logprior: 0.0434
Epoch 3/10
33/33 - 27s - loss: 1572.4628 - loglik: -1.5726e+03 - logprior: 0.1823
Epoch 4/10
33/33 - 27s - loss: 1570.8918 - loglik: -1.5711e+03 - logprior: 0.2110
Epoch 5/10
33/33 - 27s - loss: 1572.9091 - loglik: -1.5731e+03 - logprior: 0.1781
Fitted a model with MAP estimate = -1569.6741
Time for alignment: 501.5553
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 26s - loss: 1685.2458 - loglik: -1.6824e+03 - logprior: -2.8097e+00
Epoch 2/10
33/33 - 23s - loss: 1602.2244 - loglik: -1.6016e+03 - logprior: -6.1270e-01
Epoch 3/10
33/33 - 23s - loss: 1574.9332 - loglik: -1.5743e+03 - logprior: -6.1523e-01
Epoch 4/10
33/33 - 23s - loss: 1590.2352 - loglik: -1.5896e+03 - logprior: -6.6452e-01
Fitted a model with MAP estimate = -1581.7308
expansions: [(0, 4), (10, 2), (11, 1), (32, 2), (33, 9), (63, 2), (65, 1), (73, 2), (78, 1), (95, 1), (113, 1), (116, 1), (118, 1), (134, 2), (155, 5), (183, 1), (210, 1), (220, 2), (221, 4), (230, 3)]
discards: [  3   4 225 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 270 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 33s - loss: 1584.0436 - loglik: -1.5809e+03 - logprior: -3.1221e+00
Epoch 2/2
33/33 - 28s - loss: 1581.4839 - loglik: -1.5812e+03 - logprior: -3.3030e-01
Fitted a model with MAP estimate = -1572.0196
expansions: [(0, 4), (270, 4)]
discards: [  4   5   6  38  41  42  43  44  45 185 186 256 262 263 264 265 266 267
 268]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 30s - loss: 1577.7971 - loglik: -1.5748e+03 - logprior: -3.0311e+00
Epoch 2/2
33/33 - 27s - loss: 1579.0356 - loglik: -1.5789e+03 - logprior: -1.0368e-01
Fitted a model with MAP estimate = -1572.3300
expansions: [(259, 4)]
discards: [  1   6   7   8  16 255 256 257 258]
Re-initialized the encoder parameters.
Fitting a model of length 254 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 29s - loss: 1579.4037 - loglik: -1.5775e+03 - logprior: -1.8666e+00
Epoch 2/10
33/33 - 26s - loss: 1574.3201 - loglik: -1.5745e+03 - logprior: 0.1844
Epoch 3/10
33/33 - 26s - loss: 1569.7043 - loglik: -1.5700e+03 - logprior: 0.3055
Epoch 4/10
33/33 - 26s - loss: 1571.7526 - loglik: -1.5721e+03 - logprior: 0.3334
Fitted a model with MAP estimate = -1569.8730
Time for alignment: 429.5066
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 26s - loss: 1682.6465 - loglik: -1.6798e+03 - logprior: -2.8023e+00
Epoch 2/10
33/33 - 23s - loss: 1603.8414 - loglik: -1.6033e+03 - logprior: -5.8140e-01
Epoch 3/10
33/33 - 23s - loss: 1583.4585 - loglik: -1.5829e+03 - logprior: -5.5681e-01
Epoch 4/10
33/33 - 23s - loss: 1582.9093 - loglik: -1.5823e+03 - logprior: -5.8330e-01
Epoch 5/10
33/33 - 23s - loss: 1581.2804 - loglik: -1.5806e+03 - logprior: -6.8763e-01
Epoch 6/10
33/33 - 23s - loss: 1577.4084 - loglik: -1.5766e+03 - logprior: -7.8972e-01
Epoch 7/10
33/33 - 23s - loss: 1585.5350 - loglik: -1.5846e+03 - logprior: -9.0879e-01
Fitted a model with MAP estimate = -1579.8866
expansions: [(0, 5), (9, 1), (10, 1), (33, 9), (60, 1), (63, 2), (65, 1), (73, 2), (78, 2), (90, 1), (111, 1), (112, 1), (115, 1), (133, 2), (154, 3), (162, 4), (203, 1), (220, 2), (221, 3), (230, 3)]
discards: [225 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 32s - loss: 1585.6370 - loglik: -1.5824e+03 - logprior: -3.2108e+00
Epoch 2/2
33/33 - 28s - loss: 1568.4370 - loglik: -1.5681e+03 - logprior: -3.7751e-01
Fitted a model with MAP estimate = -1570.8987
expansions: [(272, 4)]
discards: [  1   2   4  42  43  44  45  46  78 185 195 196 197 259 266 267 268 269
 270 271]
Re-initialized the encoder parameters.
Fitting a model of length 256 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 30s - loss: 1585.2010 - loglik: -1.5832e+03 - logprior: -2.0142e+00
Epoch 2/2
33/33 - 26s - loss: 1564.7671 - loglik: -1.5647e+03 - logprior: -2.4770e-02
Fitted a model with MAP estimate = -1572.3970
expansions: [(0, 4), (185, 5), (249, 1), (251, 1), (256, 3)]
discards: [  2   3   4 252 253 254 255]
Re-initialized the encoder parameters.
Fitting a model of length 263 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 31s - loss: 1576.6013 - loglik: -1.5736e+03 - logprior: -3.0364e+00
Epoch 2/10
33/33 - 27s - loss: 1577.6711 - loglik: -1.5778e+03 - logprior: 0.0958
Fitted a model with MAP estimate = -1571.0121
Time for alignment: 447.9659
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 28s - loss: 1688.0487 - loglik: -1.6852e+03 - logprior: -2.8184e+00
Epoch 2/10
33/33 - 23s - loss: 1595.3440 - loglik: -1.5948e+03 - logprior: -5.6796e-01
Epoch 3/10
33/33 - 23s - loss: 1583.0739 - loglik: -1.5825e+03 - logprior: -5.3750e-01
Epoch 4/10
33/33 - 23s - loss: 1584.9266 - loglik: -1.5843e+03 - logprior: -5.9259e-01
Fitted a model with MAP estimate = -1581.1943
expansions: [(0, 5), (9, 1), (10, 1), (35, 2), (72, 1), (73, 1), (74, 3), (78, 2), (113, 1), (116, 1), (133, 2), (156, 3), (164, 4), (181, 2), (204, 1), (223, 3), (224, 2), (230, 3)]
discards: [225 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 31s - loss: 1582.2203 - loglik: -1.5792e+03 - logprior: -2.9940e+00
Epoch 2/2
33/33 - 27s - loss: 1571.2794 - loglik: -1.5709e+03 - logprior: -3.6817e-01
Fitted a model with MAP estimate = -1572.1288
expansions: [(264, 5)]
discards: [  1   6  16 177 188 189 190 209 260 261 262 263]
Re-initialized the encoder parameters.
Fitting a model of length 257 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 30s - loss: 1578.9834 - loglik: -1.5769e+03 - logprior: -2.0412e+00
Epoch 2/2
33/33 - 27s - loss: 1577.3414 - loglik: -1.5773e+03 - logprior: -8.0338e-02
Fitted a model with MAP estimate = -1573.1294
expansions: [(0, 4), (257, 4)]
discards: [  6 252 253 254 255 256]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 30s - loss: 1581.6951 - loglik: -1.5786e+03 - logprior: -3.0610e+00
Epoch 2/10
33/33 - 27s - loss: 1573.4424 - loglik: -1.5734e+03 - logprior: -6.5493e-02
Epoch 3/10
33/33 - 27s - loss: 1573.6635 - loglik: -1.5738e+03 - logprior: 0.1002
Fitted a model with MAP estimate = -1570.9700
Time for alignment: 402.3470
Computed alignments with likelihoods: ['-1569.2721', '-1569.6741', '-1569.8730', '-1570.8987', '-1570.9700']
Best model has likelihood: -1569.2721  (prior= 0.1635 )
time for generating output: 0.3123
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/subt.projection.fasta
SP score = 0.7873031224095054
Training of 5 independent models on file profilin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5300f62580>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52f80e5820>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f8f2a400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52e87b0b80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52959fa880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5ca7fc1730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b13996460>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52d91c3fd0>, <__main__.SimpleDirichletPrior object at 0x7f52f98a9370>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 781.2198 - loglik: -7.0936e+02 - logprior: -7.1862e+01
Epoch 2/10
10/10 - 2s - loss: 683.8757 - loglik: -6.6740e+02 - logprior: -1.6480e+01
Epoch 3/10
10/10 - 2s - loss: 635.6095 - loglik: -6.2866e+02 - logprior: -6.9532e+00
Epoch 4/10
10/10 - 2s - loss: 610.8514 - loglik: -6.0672e+02 - logprior: -4.1329e+00
Epoch 5/10
10/10 - 2s - loss: 599.8788 - loglik: -5.9703e+02 - logprior: -2.8505e+00
Epoch 6/10
10/10 - 2s - loss: 595.9739 - loglik: -5.9406e+02 - logprior: -1.9096e+00
Epoch 7/10
10/10 - 2s - loss: 594.3569 - loglik: -5.9313e+02 - logprior: -1.2246e+00
Epoch 8/10
10/10 - 2s - loss: 593.1187 - loglik: -5.9225e+02 - logprior: -8.6772e-01
Epoch 9/10
10/10 - 2s - loss: 592.1884 - loglik: -5.9153e+02 - logprior: -6.5392e-01
Epoch 10/10
10/10 - 2s - loss: 591.8755 - loglik: -5.9143e+02 - logprior: -4.4286e-01
Fitted a model with MAP estimate = -591.4481
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 1), (69, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 4), (88, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 663.9337 - loglik: -5.9199e+02 - logprior: -7.1943e+01
Epoch 2/2
10/10 - 2s - loss: 601.3317 - loglik: -5.7466e+02 - logprior: -2.6672e+01
Fitted a model with MAP estimate = -589.9002
expansions: [(0, 5)]
discards: [  0 108]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 626.6562 - loglik: -5.7161e+02 - logprior: -5.5051e+01
Epoch 2/2
10/10 - 2s - loss: 578.2772 - loglik: -5.6645e+02 - logprior: -1.1830e+01
Fitted a model with MAP estimate = -570.4232
expansions: []
discards: [ 1  2  3  4 17]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 622.3580 - loglik: -5.6903e+02 - logprior: -5.3330e+01
Epoch 2/10
10/10 - 2s - loss: 578.2336 - loglik: -5.6717e+02 - logprior: -1.1062e+01
Epoch 3/10
10/10 - 2s - loss: 567.8921 - loglik: -5.6555e+02 - logprior: -2.3432e+00
Epoch 4/10
10/10 - 2s - loss: 562.5212 - loglik: -5.6381e+02 - logprior: 1.2919
Epoch 5/10
10/10 - 2s - loss: 559.9945 - loglik: -5.6322e+02 - logprior: 3.2283
Epoch 6/10
10/10 - 2s - loss: 557.6130 - loglik: -5.6186e+02 - logprior: 4.2463
Epoch 7/10
10/10 - 2s - loss: 557.8433 - loglik: -5.6282e+02 - logprior: 4.9792
Fitted a model with MAP estimate = -557.1463
Time for alignment: 50.5801
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 781.0095 - loglik: -7.0915e+02 - logprior: -7.1864e+01
Epoch 2/10
10/10 - 2s - loss: 683.9702 - loglik: -6.6749e+02 - logprior: -1.6476e+01
Epoch 3/10
10/10 - 2s - loss: 635.5707 - loglik: -6.2863e+02 - logprior: -6.9378e+00
Epoch 4/10
10/10 - 2s - loss: 611.6058 - loglik: -6.0752e+02 - logprior: -4.0823e+00
Epoch 5/10
10/10 - 2s - loss: 600.3187 - loglik: -5.9779e+02 - logprior: -2.5272e+00
Epoch 6/10
10/10 - 2s - loss: 596.2285 - loglik: -5.9473e+02 - logprior: -1.5015e+00
Epoch 7/10
10/10 - 2s - loss: 595.2770 - loglik: -5.9443e+02 - logprior: -8.4625e-01
Epoch 8/10
10/10 - 2s - loss: 593.9868 - loglik: -5.9346e+02 - logprior: -5.2985e-01
Epoch 9/10
10/10 - 2s - loss: 592.7457 - loglik: -5.9237e+02 - logprior: -3.8042e-01
Epoch 10/10
10/10 - 2s - loss: 591.2361 - loglik: -5.9095e+02 - logprior: -2.8833e-01
Fitted a model with MAP estimate = -591.9878
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 1), (71, 2), (78, 1), (79, 1), (83, 1), (87, 4), (88, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 664.2064 - loglik: -5.9226e+02 - logprior: -7.1947e+01
Epoch 2/2
10/10 - 2s - loss: 601.0724 - loglik: -5.7438e+02 - logprior: -2.6688e+01
Fitted a model with MAP estimate = -590.1181
expansions: [(0, 4)]
discards: [  0 108]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 626.9083 - loglik: -5.7204e+02 - logprior: -5.4872e+01
Epoch 2/2
10/10 - 2s - loss: 578.1227 - loglik: -5.6646e+02 - logprior: -1.1661e+01
Fitted a model with MAP estimate = -570.3457
expansions: []
discards: [ 1  2  3 16]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 622.2947 - loglik: -5.6900e+02 - logprior: -5.3296e+01
Epoch 2/10
10/10 - 2s - loss: 578.5873 - loglik: -5.6756e+02 - logprior: -1.1026e+01
Epoch 3/10
10/10 - 2s - loss: 567.5343 - loglik: -5.6524e+02 - logprior: -2.2961e+00
Epoch 4/10
10/10 - 2s - loss: 563.0524 - loglik: -5.6438e+02 - logprior: 1.3271
Epoch 5/10
10/10 - 2s - loss: 559.8612 - loglik: -5.6313e+02 - logprior: 3.2696
Epoch 6/10
10/10 - 2s - loss: 557.9371 - loglik: -5.6223e+02 - logprior: 4.2953
Epoch 7/10
10/10 - 2s - loss: 557.6978 - loglik: -5.6272e+02 - logprior: 5.0240
Epoch 8/10
10/10 - 2s - loss: 556.9476 - loglik: -5.6270e+02 - logprior: 5.7499
Epoch 9/10
10/10 - 2s - loss: 556.6901 - loglik: -5.6301e+02 - logprior: 6.3243
Epoch 10/10
10/10 - 2s - loss: 556.2535 - loglik: -5.6301e+02 - logprior: 6.7561
Fitted a model with MAP estimate = -555.8428
Time for alignment: 61.5454
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 781.3127 - loglik: -7.0944e+02 - logprior: -7.1869e+01
Epoch 2/10
10/10 - 2s - loss: 683.6247 - loglik: -6.6715e+02 - logprior: -1.6476e+01
Epoch 3/10
10/10 - 2s - loss: 637.2748 - loglik: -6.3038e+02 - logprior: -6.8900e+00
Epoch 4/10
10/10 - 2s - loss: 610.5724 - loglik: -6.0667e+02 - logprior: -3.9059e+00
Epoch 5/10
10/10 - 2s - loss: 601.5159 - loglik: -5.9897e+02 - logprior: -2.5478e+00
Epoch 6/10
10/10 - 2s - loss: 596.4225 - loglik: -5.9478e+02 - logprior: -1.6378e+00
Epoch 7/10
10/10 - 2s - loss: 595.4137 - loglik: -5.9448e+02 - logprior: -9.3040e-01
Epoch 8/10
10/10 - 2s - loss: 593.3906 - loglik: -5.9280e+02 - logprior: -5.9283e-01
Epoch 9/10
10/10 - 2s - loss: 592.9894 - loglik: -5.9253e+02 - logprior: -4.6059e-01
Epoch 10/10
10/10 - 2s - loss: 591.9803 - loglik: -5.9162e+02 - logprior: -3.5961e-01
Fitted a model with MAP estimate = -591.9900
expansions: [(11, 1), (12, 3), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (54, 1), (64, 1), (71, 3), (78, 1), (79, 1), (83, 1), (87, 4), (88, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 665.2181 - loglik: -5.9314e+02 - logprior: -7.2080e+01
Epoch 2/2
10/10 - 2s - loss: 599.5866 - loglik: -5.7277e+02 - logprior: -2.6813e+01
Fitted a model with MAP estimate = -589.0868
expansions: [(0, 4)]
discards: [  0  13  14  88 109]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 627.4039 - loglik: -5.7262e+02 - logprior: -5.4786e+01
Epoch 2/2
10/10 - 2s - loss: 579.3190 - loglik: -5.6770e+02 - logprior: -1.1621e+01
Fitted a model with MAP estimate = -571.5997
expansions: []
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 623.8329 - loglik: -5.7057e+02 - logprior: -5.3262e+01
Epoch 2/10
10/10 - 2s - loss: 577.1798 - loglik: -5.6623e+02 - logprior: -1.0948e+01
Epoch 3/10
10/10 - 2s - loss: 567.4808 - loglik: -5.6524e+02 - logprior: -2.2444e+00
Epoch 4/10
10/10 - 2s - loss: 563.9173 - loglik: -5.6534e+02 - logprior: 1.4192
Epoch 5/10
10/10 - 2s - loss: 559.3716 - loglik: -5.6277e+02 - logprior: 3.4001
Epoch 6/10
10/10 - 2s - loss: 558.7935 - loglik: -5.6325e+02 - logprior: 4.4538
Epoch 7/10
10/10 - 2s - loss: 557.2188 - loglik: -5.6230e+02 - logprior: 5.0790
Epoch 8/10
10/10 - 2s - loss: 556.5804 - loglik: -5.6229e+02 - logprior: 5.7137
Epoch 9/10
10/10 - 2s - loss: 556.9780 - loglik: -5.6331e+02 - logprior: 6.3272
Fitted a model with MAP estimate = -556.2794
Time for alignment: 56.9766
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 780.6780 - loglik: -7.0881e+02 - logprior: -7.1864e+01
Epoch 2/10
10/10 - 2s - loss: 684.4173 - loglik: -6.6793e+02 - logprior: -1.6485e+01
Epoch 3/10
10/10 - 2s - loss: 636.5802 - loglik: -6.2963e+02 - logprior: -6.9543e+00
Epoch 4/10
10/10 - 2s - loss: 610.6761 - loglik: -6.0667e+02 - logprior: -4.0081e+00
Epoch 5/10
10/10 - 2s - loss: 601.8733 - loglik: -5.9923e+02 - logprior: -2.6473e+00
Epoch 6/10
10/10 - 2s - loss: 597.0629 - loglik: -5.9523e+02 - logprior: -1.8340e+00
Epoch 7/10
10/10 - 2s - loss: 595.0560 - loglik: -5.9385e+02 - logprior: -1.2074e+00
Epoch 8/10
10/10 - 2s - loss: 592.7477 - loglik: -5.9182e+02 - logprior: -9.2782e-01
Epoch 9/10
10/10 - 2s - loss: 593.6521 - loglik: -5.9284e+02 - logprior: -8.1464e-01
Fitted a model with MAP estimate = -592.5587
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (54, 1), (63, 1), (69, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 4), (88, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 665.1927 - loglik: -5.9313e+02 - logprior: -7.2065e+01
Epoch 2/2
10/10 - 2s - loss: 602.2911 - loglik: -5.7549e+02 - logprior: -2.6804e+01
Fitted a model with MAP estimate = -590.8386
expansions: [(0, 4)]
discards: [  0 107]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 626.4722 - loglik: -5.7153e+02 - logprior: -5.4943e+01
Epoch 2/2
10/10 - 2s - loss: 578.5284 - loglik: -5.6674e+02 - logprior: -1.1791e+01
Fitted a model with MAP estimate = -570.5048
expansions: []
discards: [ 1  2  3 16]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 622.5002 - loglik: -5.6913e+02 - logprior: -5.3369e+01
Epoch 2/10
10/10 - 2s - loss: 578.7153 - loglik: -5.6763e+02 - logprior: -1.1090e+01
Epoch 3/10
10/10 - 2s - loss: 567.8256 - loglik: -5.6548e+02 - logprior: -2.3405e+00
Epoch 4/10
10/10 - 2s - loss: 562.2817 - loglik: -5.6359e+02 - logprior: 1.3035
Epoch 5/10
10/10 - 2s - loss: 560.2520 - loglik: -5.6348e+02 - logprior: 3.2250
Epoch 6/10
10/10 - 2s - loss: 558.5795 - loglik: -5.6282e+02 - logprior: 4.2396
Epoch 7/10
10/10 - 2s - loss: 556.8982 - loglik: -5.6187e+02 - logprior: 4.9732
Epoch 8/10
10/10 - 2s - loss: 557.7399 - loglik: -5.6342e+02 - logprior: 5.6817
Fitted a model with MAP estimate = -556.7509
Time for alignment: 53.3813
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 781.3757 - loglik: -7.0951e+02 - logprior: -7.1863e+01
Epoch 2/10
10/10 - 2s - loss: 683.8990 - loglik: -6.6742e+02 - logprior: -1.6479e+01
Epoch 3/10
10/10 - 2s - loss: 636.5878 - loglik: -6.2964e+02 - logprior: -6.9492e+00
Epoch 4/10
10/10 - 2s - loss: 611.5067 - loglik: -6.0748e+02 - logprior: -4.0276e+00
Epoch 5/10
10/10 - 2s - loss: 601.5537 - loglik: -5.9899e+02 - logprior: -2.5662e+00
Epoch 6/10
10/10 - 2s - loss: 597.4990 - loglik: -5.9598e+02 - logprior: -1.5189e+00
Epoch 7/10
10/10 - 2s - loss: 595.1389 - loglik: -5.9422e+02 - logprior: -9.2051e-01
Epoch 8/10
10/10 - 2s - loss: 594.1318 - loglik: -5.9355e+02 - logprior: -5.7973e-01
Epoch 9/10
10/10 - 2s - loss: 592.7682 - loglik: -5.9235e+02 - logprior: -4.2109e-01
Epoch 10/10
10/10 - 2s - loss: 592.8185 - loglik: -5.9252e+02 - logprior: -3.0253e-01
Fitted a model with MAP estimate = -592.0547
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 1), (71, 2), (78, 1), (79, 1), (83, 1), (87, 4), (88, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 664.2264 - loglik: -5.9227e+02 - logprior: -7.1960e+01
Epoch 2/2
10/10 - 2s - loss: 601.7383 - loglik: -5.7505e+02 - logprior: -2.6688e+01
Fitted a model with MAP estimate = -590.1459
expansions: [(0, 4)]
discards: [  0 107 108]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 628.8219 - loglik: -5.7387e+02 - logprior: -5.4948e+01
Epoch 2/2
10/10 - 2s - loss: 579.2809 - loglik: -5.6735e+02 - logprior: -1.1927e+01
Fitted a model with MAP estimate = -571.5579
expansions: [(110, 1)]
discards: [ 1  2  3 16]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 623.2969 - loglik: -5.6986e+02 - logprior: -5.3432e+01
Epoch 2/10
10/10 - 2s - loss: 578.7162 - loglik: -5.6766e+02 - logprior: -1.1059e+01
Epoch 3/10
10/10 - 2s - loss: 568.1211 - loglik: -5.6581e+02 - logprior: -2.3106e+00
Epoch 4/10
10/10 - 2s - loss: 562.1661 - loglik: -5.6348e+02 - logprior: 1.3188
Epoch 5/10
10/10 - 2s - loss: 560.3781 - loglik: -5.6362e+02 - logprior: 3.2453
Epoch 6/10
10/10 - 2s - loss: 557.1053 - loglik: -5.6138e+02 - logprior: 4.2722
Epoch 7/10
10/10 - 2s - loss: 557.7324 - loglik: -5.6274e+02 - logprior: 5.0028
Fitted a model with MAP estimate = -557.0485
Time for alignment: 53.1186
Computed alignments with likelihoods: ['-557.1463', '-555.8428', '-556.2794', '-556.7509', '-557.0485']
Best model has likelihood: -555.8428  (prior= 6.9568 )
time for generating output: 0.1866
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/profilin.projection.fasta
SP score = 0.9507269789983845
Training of 5 independent models on file rrm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52954a29d0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5300d87670>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5300d87bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52d83def40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f529530fd60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5295690190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f598434fca0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52f8f2aee0>, <__main__.SimpleDirichletPrior object at 0x7f5b13983130>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 402.3827 - loglik: -4.0004e+02 - logprior: -2.3430e+00
Epoch 2/10
22/22 - 2s - loss: 369.5120 - loglik: -3.6818e+02 - logprior: -1.3345e+00
Epoch 3/10
22/22 - 2s - loss: 361.7906 - loglik: -3.6037e+02 - logprior: -1.4159e+00
Epoch 4/10
22/22 - 2s - loss: 360.6589 - loglik: -3.5934e+02 - logprior: -1.3180e+00
Epoch 5/10
22/22 - 2s - loss: 360.1330 - loglik: -3.5881e+02 - logprior: -1.3203e+00
Epoch 6/10
22/22 - 2s - loss: 359.5242 - loglik: -3.5821e+02 - logprior: -1.3140e+00
Epoch 7/10
22/22 - 2s - loss: 359.4021 - loglik: -3.5808e+02 - logprior: -1.3216e+00
Epoch 8/10
22/22 - 2s - loss: 359.1035 - loglik: -3.5777e+02 - logprior: -1.3347e+00
Epoch 9/10
22/22 - 2s - loss: 359.0123 - loglik: -3.5767e+02 - logprior: -1.3422e+00
Epoch 10/10
22/22 - 2s - loss: 359.2028 - loglik: -3.5785e+02 - logprior: -1.3545e+00
Fitted a model with MAP estimate = -348.3441
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (20, 2), (22, 2), (23, 2), (28, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 363.7656 - loglik: -3.6082e+02 - logprior: -2.9501e+00
Epoch 2/2
22/22 - 2s - loss: 354.5508 - loglik: -3.5307e+02 - logprior: -1.4773e+00
Fitted a model with MAP estimate = -340.9275
expansions: [(0, 2)]
discards: [ 0  9 17 25 29 32 68]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 354.3328 - loglik: -3.5222e+02 - logprior: -2.1142e+00
Epoch 2/2
22/22 - 2s - loss: 351.6990 - loglik: -3.5069e+02 - logprior: -1.0069e+00
Fitted a model with MAP estimate = -339.7549
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 9s - loss: 340.2523 - loglik: -3.3912e+02 - logprior: -1.1328e+00
Epoch 2/10
32/32 - 3s - loss: 337.4374 - loglik: -3.3663e+02 - logprior: -8.0486e-01
Epoch 3/10
32/32 - 3s - loss: 337.7702 - loglik: -3.3699e+02 - logprior: -7.8344e-01
Fitted a model with MAP estimate = -337.0705
Time for alignment: 71.3628
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 402.1516 - loglik: -3.9981e+02 - logprior: -2.3429e+00
Epoch 2/10
22/22 - 2s - loss: 369.7008 - loglik: -3.6837e+02 - logprior: -1.3267e+00
Epoch 3/10
22/22 - 2s - loss: 362.4758 - loglik: -3.6106e+02 - logprior: -1.4139e+00
Epoch 4/10
22/22 - 2s - loss: 360.8449 - loglik: -3.5954e+02 - logprior: -1.3086e+00
Epoch 5/10
22/22 - 2s - loss: 360.3886 - loglik: -3.5906e+02 - logprior: -1.3238e+00
Epoch 6/10
22/22 - 2s - loss: 360.0896 - loglik: -3.5876e+02 - logprior: -1.3248e+00
Epoch 7/10
22/22 - 2s - loss: 359.8187 - loglik: -3.5848e+02 - logprior: -1.3343e+00
Epoch 8/10
22/22 - 2s - loss: 359.4928 - loglik: -3.5815e+02 - logprior: -1.3440e+00
Epoch 9/10
22/22 - 2s - loss: 359.5777 - loglik: -3.5822e+02 - logprior: -1.3549e+00
Fitted a model with MAP estimate = -348.4429
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (20, 2), (21, 1), (22, 2), (25, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 363.2799 - loglik: -3.6034e+02 - logprior: -2.9416e+00
Epoch 2/2
22/22 - 2s - loss: 354.5341 - loglik: -3.5309e+02 - logprior: -1.4483e+00
Fitted a model with MAP estimate = -340.7468
expansions: [(0, 2)]
discards: [ 0  9 17 25 30 67]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 354.3493 - loglik: -3.5224e+02 - logprior: -2.1136e+00
Epoch 2/2
22/22 - 2s - loss: 351.4707 - loglik: -3.5046e+02 - logprior: -1.0075e+00
Fitted a model with MAP estimate = -339.8517
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 8s - loss: 340.2022 - loglik: -3.3906e+02 - logprior: -1.1404e+00
Epoch 2/10
32/32 - 3s - loss: 337.6827 - loglik: -3.3688e+02 - logprior: -8.0081e-01
Epoch 3/10
32/32 - 3s - loss: 337.3958 - loglik: -3.3662e+02 - logprior: -7.7980e-01
Epoch 4/10
32/32 - 3s - loss: 337.6059 - loglik: -3.3682e+02 - logprior: -7.8472e-01
Fitted a model with MAP estimate = -336.7210
Time for alignment: 72.8666
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 401.9380 - loglik: -3.9959e+02 - logprior: -2.3463e+00
Epoch 2/10
22/22 - 2s - loss: 370.1377 - loglik: -3.6880e+02 - logprior: -1.3332e+00
Epoch 3/10
22/22 - 2s - loss: 362.6038 - loglik: -3.6119e+02 - logprior: -1.4157e+00
Epoch 4/10
22/22 - 2s - loss: 361.2594 - loglik: -3.5993e+02 - logprior: -1.3314e+00
Epoch 5/10
22/22 - 2s - loss: 360.3073 - loglik: -3.5897e+02 - logprior: -1.3390e+00
Epoch 6/10
22/22 - 2s - loss: 360.2332 - loglik: -3.5890e+02 - logprior: -1.3343e+00
Epoch 7/10
22/22 - 2s - loss: 359.9943 - loglik: -3.5865e+02 - logprior: -1.3409e+00
Epoch 8/10
22/22 - 2s - loss: 359.9084 - loglik: -3.5856e+02 - logprior: -1.3510e+00
Epoch 9/10
22/22 - 2s - loss: 359.7070 - loglik: -3.5835e+02 - logprior: -1.3575e+00
Epoch 10/10
22/22 - 2s - loss: 359.4341 - loglik: -3.5806e+02 - logprior: -1.3724e+00
Fitted a model with MAP estimate = -348.6465
expansions: [(8, 1), (9, 2), (11, 1), (14, 2), (17, 2), (21, 1), (22, 2), (25, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 363.2472 - loglik: -3.6031e+02 - logprior: -2.9378e+00
Epoch 2/2
22/22 - 2s - loss: 354.7473 - loglik: -3.5330e+02 - logprior: -1.4502e+00
Fitted a model with MAP estimate = -340.8757
expansions: [(0, 2)]
discards: [ 0  9 17 22 30 67]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 354.2107 - loglik: -3.5210e+02 - logprior: -2.1110e+00
Epoch 2/2
22/22 - 2s - loss: 351.2098 - loglik: -3.5020e+02 - logprior: -1.0092e+00
Fitted a model with MAP estimate = -339.9619
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 340.3629 - loglik: -3.3923e+02 - logprior: -1.1371e+00
Epoch 2/10
32/32 - 3s - loss: 337.6588 - loglik: -3.3686e+02 - logprior: -8.0253e-01
Epoch 3/10
32/32 - 3s - loss: 337.4007 - loglik: -3.3661e+02 - logprior: -7.8727e-01
Epoch 4/10
32/32 - 3s - loss: 337.0944 - loglik: -3.3632e+02 - logprior: -7.7886e-01
Epoch 5/10
32/32 - 3s - loss: 336.9583 - loglik: -3.3617e+02 - logprior: -7.8871e-01
Epoch 6/10
32/32 - 3s - loss: 336.2015 - loglik: -3.3540e+02 - logprior: -8.0187e-01
Epoch 7/10
32/32 - 3s - loss: 335.9491 - loglik: -3.3514e+02 - logprior: -8.0461e-01
Epoch 8/10
32/32 - 3s - loss: 335.9698 - loglik: -3.3514e+02 - logprior: -8.2611e-01
Fitted a model with MAP estimate = -335.7343
Time for alignment: 85.0723
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 402.2610 - loglik: -3.9992e+02 - logprior: -2.3428e+00
Epoch 2/10
22/22 - 2s - loss: 370.3872 - loglik: -3.6906e+02 - logprior: -1.3311e+00
Epoch 3/10
22/22 - 2s - loss: 362.6535 - loglik: -3.6125e+02 - logprior: -1.3998e+00
Epoch 4/10
22/22 - 2s - loss: 361.2543 - loglik: -3.5994e+02 - logprior: -1.3103e+00
Epoch 5/10
22/22 - 2s - loss: 360.5053 - loglik: -3.5918e+02 - logprior: -1.3206e+00
Epoch 6/10
22/22 - 2s - loss: 359.9508 - loglik: -3.5863e+02 - logprior: -1.3242e+00
Epoch 7/10
22/22 - 2s - loss: 359.8073 - loglik: -3.5848e+02 - logprior: -1.3317e+00
Epoch 8/10
22/22 - 2s - loss: 359.4463 - loglik: -3.5811e+02 - logprior: -1.3377e+00
Epoch 9/10
22/22 - 2s - loss: 359.2466 - loglik: -3.5790e+02 - logprior: -1.3466e+00
Epoch 10/10
22/22 - 2s - loss: 359.3686 - loglik: -3.5801e+02 - logprior: -1.3624e+00
Fitted a model with MAP estimate = -348.7485
expansions: [(8, 1), (9, 2), (12, 1), (13, 2), (17, 2), (21, 1), (22, 2), (28, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 363.4572 - loglik: -3.6051e+02 - logprior: -2.9507e+00
Epoch 2/2
22/22 - 2s - loss: 354.4100 - loglik: -3.5296e+02 - logprior: -1.4504e+00
Fitted a model with MAP estimate = -340.8998
expansions: [(0, 2)]
discards: [ 0  9 17 22 30 67]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 354.4251 - loglik: -3.5231e+02 - logprior: -2.1134e+00
Epoch 2/2
22/22 - 2s - loss: 351.4969 - loglik: -3.5049e+02 - logprior: -1.0088e+00
Fitted a model with MAP estimate = -339.7293
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 340.2407 - loglik: -3.3911e+02 - logprior: -1.1347e+00
Epoch 2/10
32/32 - 3s - loss: 337.8486 - loglik: -3.3705e+02 - logprior: -7.9937e-01
Epoch 3/10
32/32 - 3s - loss: 337.2407 - loglik: -3.3645e+02 - logprior: -7.8654e-01
Epoch 4/10
32/32 - 3s - loss: 337.0554 - loglik: -3.3627e+02 - logprior: -7.8166e-01
Epoch 5/10
32/32 - 3s - loss: 336.9659 - loglik: -3.3618e+02 - logprior: -7.8572e-01
Epoch 6/10
32/32 - 3s - loss: 335.9861 - loglik: -3.3519e+02 - logprior: -7.9938e-01
Epoch 7/10
32/32 - 3s - loss: 336.3674 - loglik: -3.3556e+02 - logprior: -8.0823e-01
Fitted a model with MAP estimate = -335.8643
Time for alignment: 80.6614
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 402.6670 - loglik: -4.0032e+02 - logprior: -2.3464e+00
Epoch 2/10
22/22 - 2s - loss: 371.1109 - loglik: -3.6978e+02 - logprior: -1.3357e+00
Epoch 3/10
22/22 - 2s - loss: 362.4382 - loglik: -3.6102e+02 - logprior: -1.4179e+00
Epoch 4/10
22/22 - 2s - loss: 361.1083 - loglik: -3.5980e+02 - logprior: -1.3078e+00
Epoch 5/10
22/22 - 2s - loss: 360.6462 - loglik: -3.5932e+02 - logprior: -1.3216e+00
Epoch 6/10
22/22 - 2s - loss: 360.1111 - loglik: -3.5879e+02 - logprior: -1.3174e+00
Epoch 7/10
22/22 - 2s - loss: 359.6653 - loglik: -3.5834e+02 - logprior: -1.3225e+00
Epoch 8/10
22/22 - 2s - loss: 359.6819 - loglik: -3.5835e+02 - logprior: -1.3287e+00
Fitted a model with MAP estimate = -348.3310
expansions: [(8, 1), (9, 2), (12, 1), (13, 2), (17, 2), (21, 1), (22, 1), (25, 1), (27, 1), (41, 1), (44, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 362.7288 - loglik: -3.5982e+02 - logprior: -2.9109e+00
Epoch 2/2
22/22 - 2s - loss: 354.2854 - loglik: -3.5288e+02 - logprior: -1.4042e+00
Fitted a model with MAP estimate = -340.7783
expansions: [(0, 2)]
discards: [ 0  9 17 22 66]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 354.3385 - loglik: -3.5223e+02 - logprior: -2.1098e+00
Epoch 2/2
22/22 - 2s - loss: 351.4600 - loglik: -3.5045e+02 - logprior: -1.0111e+00
Fitted a model with MAP estimate = -339.8031
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 7s - loss: 340.1611 - loglik: -3.3902e+02 - logprior: -1.1365e+00
Epoch 2/10
32/32 - 3s - loss: 337.9145 - loglik: -3.3711e+02 - logprior: -8.0421e-01
Epoch 3/10
32/32 - 3s - loss: 337.3580 - loglik: -3.3657e+02 - logprior: -7.8755e-01
Epoch 4/10
32/32 - 3s - loss: 336.9465 - loglik: -3.3617e+02 - logprior: -7.8145e-01
Epoch 5/10
32/32 - 3s - loss: 336.7968 - loglik: -3.3601e+02 - logprior: -7.8775e-01
Epoch 6/10
32/32 - 3s - loss: 336.4219 - loglik: -3.3562e+02 - logprior: -8.0685e-01
Epoch 7/10
32/32 - 3s - loss: 336.1522 - loglik: -3.3535e+02 - logprior: -8.0512e-01
Epoch 8/10
32/32 - 3s - loss: 336.1064 - loglik: -3.3528e+02 - logprior: -8.2840e-01
Epoch 9/10
32/32 - 3s - loss: 335.4870 - loglik: -3.3464e+02 - logprior: -8.4565e-01
Epoch 10/10
32/32 - 3s - loss: 335.6138 - loglik: -3.3475e+02 - logprior: -8.6028e-01
Fitted a model with MAP estimate = -335.4541
Time for alignment: 85.2230
Computed alignments with likelihoods: ['-337.0705', '-336.7210', '-335.7343', '-335.8643', '-335.4541']
Best model has likelihood: -335.4541  (prior= -0.8722 )
time for generating output: 0.1656
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rrm.projection.fasta
SP score = 0.8292341111344685
Training of 5 independent models on file KAS.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52e010db20>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52f0939c10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52d8546250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f0fe8fa0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f528a4fcd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f528b8d8af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5ca7a864f0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f53154b65e0>, <__main__.SimpleDirichletPrior object at 0x7f52891bc6a0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 972.3189 - loglik: -9.5569e+02 - logprior: -1.6632e+01
Epoch 2/10
17/17 - 5s - loss: 827.6664 - loglik: -8.2598e+02 - logprior: -1.6857e+00
Epoch 3/10
17/17 - 5s - loss: 780.8857 - loglik: -7.8007e+02 - logprior: -8.1468e-01
Epoch 4/10
17/17 - 5s - loss: 763.4027 - loglik: -7.6302e+02 - logprior: -3.8576e-01
Epoch 5/10
17/17 - 5s - loss: 763.1304 - loglik: -7.6288e+02 - logprior: -2.5299e-01
Epoch 6/10
17/17 - 5s - loss: 761.4521 - loglik: -7.6126e+02 - logprior: -1.9205e-01
Epoch 7/10
17/17 - 5s - loss: 760.1981 - loglik: -7.6002e+02 - logprior: -1.8089e-01
Epoch 8/10
17/17 - 5s - loss: 761.9625 - loglik: -7.6177e+02 - logprior: -1.9669e-01
Fitted a model with MAP estimate = -760.9579
expansions: [(0, 36), (24, 1), (26, 1), (40, 1), (50, 1), (61, 1), (82, 1), (91, 1), (108, 1), (138, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 207 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 773.8782 - loglik: -7.5662e+02 - logprior: -1.7254e+01
Epoch 2/2
17/17 - 6s - loss: 728.3757 - loglik: -7.2646e+02 - logprior: -1.9145e+00
Fitted a model with MAP estimate = -719.0694
expansions: [(0, 18)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31]
Re-initialized the encoder parameters.
Fitting a model of length 193 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 759.7200 - loglik: -7.4597e+02 - logprior: -1.3749e+01
Epoch 2/2
17/17 - 5s - loss: 735.8286 - loglik: -7.3542e+02 - logprior: -4.0471e-01
Fitted a model with MAP estimate = -728.5884
expansions: [(0, 29)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 12s - loss: 757.0370 - loglik: -7.4466e+02 - logprior: -1.2377e+01
Epoch 2/10
17/17 - 6s - loss: 724.0933 - loglik: -7.2406e+02 - logprior: -3.6675e-02
Epoch 3/10
17/17 - 6s - loss: 719.9678 - loglik: -7.2127e+02 - logprior: 1.3066
Epoch 4/10
17/17 - 6s - loss: 713.0775 - loglik: -7.1493e+02 - logprior: 1.8518
Epoch 5/10
17/17 - 6s - loss: 710.0421 - loglik: -7.1213e+02 - logprior: 2.0856
Epoch 6/10
17/17 - 6s - loss: 711.3043 - loglik: -7.1363e+02 - logprior: 2.3275
Fitted a model with MAP estimate = -710.8878
Time for alignment: 128.9650
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 967.9819 - loglik: -9.5127e+02 - logprior: -1.6709e+01
Epoch 2/10
17/17 - 5s - loss: 816.3290 - loglik: -8.1396e+02 - logprior: -2.3671e+00
Epoch 3/10
17/17 - 5s - loss: 760.4001 - loglik: -7.5861e+02 - logprior: -1.7877e+00
Epoch 4/10
17/17 - 5s - loss: 755.8044 - loglik: -7.5445e+02 - logprior: -1.3497e+00
Epoch 5/10
17/17 - 5s - loss: 752.0781 - loglik: -7.5074e+02 - logprior: -1.3370e+00
Epoch 6/10
17/17 - 5s - loss: 748.0722 - loglik: -7.4675e+02 - logprior: -1.3232e+00
Epoch 7/10
17/17 - 5s - loss: 745.8268 - loglik: -7.4446e+02 - logprior: -1.3623e+00
Epoch 8/10
17/17 - 5s - loss: 747.4988 - loglik: -7.4612e+02 - logprior: -1.3815e+00
Fitted a model with MAP estimate = -747.9255
expansions: [(25, 1), (50, 1), (59, 1), (80, 1), (89, 1), (100, 1), (136, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 774.9556 - loglik: -7.5729e+02 - logprior: -1.7670e+01
Epoch 2/2
17/17 - 5s - loss: 748.0212 - loglik: -7.4193e+02 - logprior: -6.0956e+00
Fitted a model with MAP estimate = -749.4315
expansions: [(0, 16)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 764.0032 - loglik: -7.5031e+02 - logprior: -1.3694e+01
Epoch 2/2
17/17 - 5s - loss: 741.6120 - loglik: -7.3958e+02 - logprior: -2.0313e+00
Fitted a model with MAP estimate = -739.5706
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 762.1985 - loglik: -7.4616e+02 - logprior: -1.6041e+01
Epoch 2/10
17/17 - 5s - loss: 746.8902 - loglik: -7.4504e+02 - logprior: -1.8463e+00
Epoch 3/10
17/17 - 5s - loss: 740.5091 - loglik: -7.4107e+02 - logprior: 0.5641
Epoch 4/10
17/17 - 5s - loss: 743.0567 - loglik: -7.4405e+02 - logprior: 0.9951
Fitted a model with MAP estimate = -739.3325
Time for alignment: 112.5936
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 969.4398 - loglik: -9.5281e+02 - logprior: -1.6628e+01
Epoch 2/10
17/17 - 6s - loss: 822.9318 - loglik: -8.2127e+02 - logprior: -1.6635e+00
Epoch 3/10
17/17 - 5s - loss: 774.3785 - loglik: -7.7360e+02 - logprior: -7.7547e-01
Epoch 4/10
17/17 - 5s - loss: 763.5309 - loglik: -7.6308e+02 - logprior: -4.5563e-01
Epoch 5/10
17/17 - 5s - loss: 763.0499 - loglik: -7.6270e+02 - logprior: -3.5023e-01
Epoch 6/10
17/17 - 5s - loss: 759.1962 - loglik: -7.5895e+02 - logprior: -2.4290e-01
Epoch 7/10
17/17 - 5s - loss: 755.7325 - loglik: -7.5547e+02 - logprior: -2.5800e-01
Epoch 8/10
17/17 - 5s - loss: 758.9747 - loglik: -7.5871e+02 - logprior: -2.6313e-01
Fitted a model with MAP estimate = -758.4463
expansions: [(0, 34), (1, 1), (25, 2), (26, 1), (27, 1), (50, 1), (60, 1), (80, 1), (89, 1), (117, 1), (137, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 207 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 768.7419 - loglik: -7.5146e+02 - logprior: -1.7280e+01
Epoch 2/2
17/17 - 6s - loss: 724.5751 - loglik: -7.2278e+02 - logprior: -1.7916e+00
Fitted a model with MAP estimate = -716.6758
expansions: [(0, 19)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31]
Re-initialized the encoder parameters.
Fitting a model of length 194 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 11s - loss: 756.7762 - loglik: -7.4304e+02 - logprior: -1.3737e+01
Epoch 2/2
17/17 - 5s - loss: 736.0908 - loglik: -7.3571e+02 - logprior: -3.7721e-01
Fitted a model with MAP estimate = -726.1412
expansions: [(0, 26)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 753.9825 - loglik: -7.4167e+02 - logprior: -1.2315e+01
Epoch 2/10
17/17 - 6s - loss: 724.9710 - loglik: -7.2497e+02 - logprior: -4.1917e-03
Epoch 3/10
17/17 - 5s - loss: 718.0200 - loglik: -7.1937e+02 - logprior: 1.3492
Epoch 4/10
17/17 - 5s - loss: 716.7329 - loglik: -7.1867e+02 - logprior: 1.9336
Epoch 5/10
17/17 - 5s - loss: 712.7191 - loglik: -7.1493e+02 - logprior: 2.2135
Epoch 6/10
17/17 - 5s - loss: 709.8766 - loglik: -7.1232e+02 - logprior: 2.4431
Epoch 7/10
17/17 - 6s - loss: 713.9584 - loglik: -7.1659e+02 - logprior: 2.6273
Fitted a model with MAP estimate = -711.6550
Time for alignment: 133.3059
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 970.7582 - loglik: -9.5412e+02 - logprior: -1.6642e+01
Epoch 2/10
17/17 - 5s - loss: 829.0032 - loglik: -8.2735e+02 - logprior: -1.6498e+00
Epoch 3/10
17/17 - 5s - loss: 771.4285 - loglik: -7.7053e+02 - logprior: -9.0072e-01
Epoch 4/10
17/17 - 5s - loss: 766.3395 - loglik: -7.6578e+02 - logprior: -5.6396e-01
Epoch 5/10
17/17 - 5s - loss: 764.6522 - loglik: -7.6424e+02 - logprior: -4.0782e-01
Epoch 6/10
17/17 - 5s - loss: 759.0104 - loglik: -7.5869e+02 - logprior: -3.2280e-01
Epoch 7/10
17/17 - 5s - loss: 756.9409 - loglik: -7.5662e+02 - logprior: -3.2025e-01
Epoch 8/10
17/17 - 5s - loss: 763.5031 - loglik: -7.6317e+02 - logprior: -3.2816e-01
Fitted a model with MAP estimate = -760.0737
expansions: [(0, 35), (7, 1), (24, 1), (26, 1), (33, 1), (50, 1), (81, 1), (83, 1), (93, 1), (126, 1), (137, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 207 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 771.6638 - loglik: -7.5435e+02 - logprior: -1.7314e+01
Epoch 2/2
17/17 - 6s - loss: 730.0959 - loglik: -7.2818e+02 - logprior: -1.9170e+00
Fitted a model with MAP estimate = -717.6552
expansions: [(0, 17), (36, 2)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34]
Re-initialized the encoder parameters.
Fitting a model of length 191 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 762.2372 - loglik: -7.4850e+02 - logprior: -1.3740e+01
Epoch 2/2
17/17 - 5s - loss: 733.5874 - loglik: -7.3325e+02 - logprior: -3.4060e-01
Fitted a model with MAP estimate = -729.2474
expansions: [(0, 30)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 755.7372 - loglik: -7.4337e+02 - logprior: -1.2370e+01
Epoch 2/10
17/17 - 6s - loss: 724.4019 - loglik: -7.2441e+02 - logprior: 0.0049
Epoch 3/10
17/17 - 6s - loss: 714.4893 - loglik: -7.1577e+02 - logprior: 1.2791
Epoch 4/10
17/17 - 6s - loss: 712.9862 - loglik: -7.1485e+02 - logprior: 1.8592
Epoch 5/10
17/17 - 5s - loss: 708.1958 - loglik: -7.1034e+02 - logprior: 2.1458
Epoch 6/10
17/17 - 6s - loss: 711.2051 - loglik: -7.1357e+02 - logprior: 2.3615
Fitted a model with MAP estimate = -709.0151
Time for alignment: 128.7486
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 968.2523 - loglik: -9.5158e+02 - logprior: -1.6668e+01
Epoch 2/10
17/17 - 5s - loss: 817.2582 - loglik: -8.1511e+02 - logprior: -2.1471e+00
Epoch 3/10
17/17 - 5s - loss: 763.3832 - loglik: -7.6170e+02 - logprior: -1.6835e+00
Epoch 4/10
17/17 - 5s - loss: 757.1918 - loglik: -7.5598e+02 - logprior: -1.2071e+00
Epoch 5/10
17/17 - 5s - loss: 751.9402 - loglik: -7.5073e+02 - logprior: -1.2140e+00
Epoch 6/10
17/17 - 5s - loss: 750.9256 - loglik: -7.4970e+02 - logprior: -1.2230e+00
Epoch 7/10
17/17 - 5s - loss: 752.9650 - loglik: -7.5172e+02 - logprior: -1.2469e+00
Fitted a model with MAP estimate = -749.9227
expansions: [(50, 1), (59, 1), (80, 1), (89, 1), (92, 1), (136, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 772.5905 - loglik: -7.5495e+02 - logprior: -1.7637e+01
Epoch 2/2
17/17 - 5s - loss: 756.0837 - loglik: -7.5008e+02 - logprior: -6.0064e+00
Fitted a model with MAP estimate = -752.8559
expansions: [(0, 16)]
discards: [ 0 46]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 764.8625 - loglik: -7.5118e+02 - logprior: -1.3683e+01
Epoch 2/2
17/17 - 5s - loss: 744.4966 - loglik: -7.4257e+02 - logprior: -1.9259e+00
Fitted a model with MAP estimate = -740.6344
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]
Re-initialized the encoder parameters.
Fitting a model of length 166 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 769.8486 - loglik: -7.5334e+02 - logprior: -1.6505e+01
Epoch 2/10
17/17 - 5s - loss: 750.1982 - loglik: -7.4771e+02 - logprior: -2.4924e+00
Epoch 3/10
17/17 - 5s - loss: 747.2290 - loglik: -7.4771e+02 - logprior: 0.4764
Epoch 4/10
17/17 - 5s - loss: 746.2863 - loglik: -7.4719e+02 - logprior: 0.9009
Epoch 5/10
17/17 - 5s - loss: 745.3267 - loglik: -7.4633e+02 - logprior: 1.0006
Epoch 6/10
17/17 - 5s - loss: 739.2853 - loglik: -7.4047e+02 - logprior: 1.1885
Epoch 7/10
17/17 - 5s - loss: 745.3251 - loglik: -7.4666e+02 - logprior: 1.3360
Fitted a model with MAP estimate = -742.7974
Time for alignment: 121.6518
Computed alignments with likelihoods: ['-710.8878', '-739.3325', '-711.6550', '-709.0151', '-740.6344']
Best model has likelihood: -709.0151  (prior= 2.4743 )
time for generating output: 0.4082
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/KAS.projection.fasta
SP score = 0.44141689373297005
Training of 5 independent models on file GEL.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f531d6ccd00>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52e17a5160>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b082b28b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52886b2b80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52e1228fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52d95c7be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b13594ac0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52e0d6a490>, <__main__.SimpleDirichletPrior object at 0x7f5c843016a0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 475.1023 - loglik: -4.5291e+02 - logprior: -2.2192e+01
Epoch 2/10
10/10 - 1s - loss: 443.7507 - loglik: -4.3790e+02 - logprior: -5.8501e+00
Epoch 3/10
10/10 - 1s - loss: 426.0657 - loglik: -4.2298e+02 - logprior: -3.0853e+00
Epoch 4/10
10/10 - 1s - loss: 417.4261 - loglik: -4.1502e+02 - logprior: -2.4110e+00
Epoch 5/10
10/10 - 1s - loss: 413.4217 - loglik: -4.1132e+02 - logprior: -2.0983e+00
Epoch 6/10
10/10 - 1s - loss: 411.7133 - loglik: -4.1002e+02 - logprior: -1.6928e+00
Epoch 7/10
10/10 - 1s - loss: 411.0789 - loglik: -4.0961e+02 - logprior: -1.4698e+00
Epoch 8/10
10/10 - 1s - loss: 410.6594 - loglik: -4.0924e+02 - logprior: -1.4173e+00
Epoch 9/10
10/10 - 1s - loss: 410.0923 - loglik: -4.0873e+02 - logprior: -1.3581e+00
Epoch 10/10
10/10 - 1s - loss: 409.8829 - loglik: -4.0858e+02 - logprior: -1.3057e+00
Fitted a model with MAP estimate = -410.0484
expansions: [(0, 2), (8, 1), (9, 1), (23, 1), (41, 3), (45, 2), (48, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 437.2874 - loglik: -4.1016e+02 - logprior: -2.7127e+01
Epoch 2/2
10/10 - 1s - loss: 414.4796 - loglik: -4.0626e+02 - logprior: -8.2192e+00
Fitted a model with MAP estimate = -410.3696
expansions: [(46, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 429.2681 - loglik: -4.0603e+02 - logprior: -2.3236e+01
Epoch 2/2
10/10 - 1s - loss: 414.0428 - loglik: -4.0497e+02 - logprior: -9.0696e+00
Fitted a model with MAP estimate = -411.2679
expansions: [(0, 2)]
discards: [ 0 53]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 424.3672 - loglik: -4.0418e+02 - logprior: -2.0192e+01
Epoch 2/10
10/10 - 1s - loss: 409.8944 - loglik: -4.0460e+02 - logprior: -5.2933e+00
Epoch 3/10
10/10 - 1s - loss: 405.7785 - loglik: -4.0353e+02 - logprior: -2.2469e+00
Epoch 4/10
10/10 - 1s - loss: 404.4525 - loglik: -4.0310e+02 - logprior: -1.3518e+00
Epoch 5/10
10/10 - 1s - loss: 403.7768 - loglik: -4.0277e+02 - logprior: -1.0105e+00
Epoch 6/10
10/10 - 1s - loss: 402.9977 - loglik: -4.0234e+02 - logprior: -6.5658e-01
Epoch 7/10
10/10 - 1s - loss: 402.9165 - loglik: -4.0252e+02 - logprior: -3.9886e-01
Epoch 8/10
10/10 - 1s - loss: 402.3961 - loglik: -4.0208e+02 - logprior: -3.1610e-01
Epoch 9/10
10/10 - 1s - loss: 402.9913 - loglik: -4.0274e+02 - logprior: -2.5272e-01
Fitted a model with MAP estimate = -402.5187
Time for alignment: 45.7679
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 475.3140 - loglik: -4.5312e+02 - logprior: -2.2192e+01
Epoch 2/10
10/10 - 1s - loss: 443.8695 - loglik: -4.3802e+02 - logprior: -5.8445e+00
Epoch 3/10
10/10 - 1s - loss: 425.7975 - loglik: -4.2274e+02 - logprior: -3.0588e+00
Epoch 4/10
10/10 - 1s - loss: 417.3176 - loglik: -4.1487e+02 - logprior: -2.4525e+00
Epoch 5/10
10/10 - 1s - loss: 413.6549 - loglik: -4.1151e+02 - logprior: -2.1456e+00
Epoch 6/10
10/10 - 1s - loss: 412.0055 - loglik: -4.1031e+02 - logprior: -1.6912e+00
Epoch 7/10
10/10 - 1s - loss: 410.8407 - loglik: -4.0938e+02 - logprior: -1.4559e+00
Epoch 8/10
10/10 - 1s - loss: 410.3897 - loglik: -4.0900e+02 - logprior: -1.3879e+00
Epoch 9/10
10/10 - 1s - loss: 410.9130 - loglik: -4.0959e+02 - logprior: -1.3218e+00
Fitted a model with MAP estimate = -410.2826
expansions: [(0, 2), (8, 1), (10, 1), (39, 1), (41, 3), (42, 2), (44, 2), (48, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 77 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 437.2192 - loglik: -4.1026e+02 - logprior: -2.6957e+01
Epoch 2/2
10/10 - 1s - loss: 413.7673 - loglik: -4.0562e+02 - logprior: -8.1474e+00
Fitted a model with MAP estimate = -409.6955
expansions: []
discards: [ 0 50]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 429.2545 - loglik: -4.0608e+02 - logprior: -2.3171e+01
Epoch 2/2
10/10 - 1s - loss: 414.4787 - loglik: -4.0541e+02 - logprior: -9.0718e+00
Fitted a model with MAP estimate = -411.3411
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 424.3346 - loglik: -4.0411e+02 - logprior: -2.0226e+01
Epoch 2/10
10/10 - 1s - loss: 409.5734 - loglik: -4.0428e+02 - logprior: -5.2972e+00
Epoch 3/10
10/10 - 1s - loss: 405.9270 - loglik: -4.0367e+02 - logprior: -2.2528e+00
Epoch 4/10
10/10 - 1s - loss: 404.3880 - loglik: -4.0303e+02 - logprior: -1.3546e+00
Epoch 5/10
10/10 - 1s - loss: 403.4901 - loglik: -4.0247e+02 - logprior: -1.0190e+00
Epoch 6/10
10/10 - 1s - loss: 402.9643 - loglik: -4.0231e+02 - logprior: -6.5905e-01
Epoch 7/10
10/10 - 1s - loss: 402.8261 - loglik: -4.0243e+02 - logprior: -4.0036e-01
Epoch 8/10
10/10 - 1s - loss: 402.4641 - loglik: -4.0214e+02 - logprior: -3.2330e-01
Epoch 9/10
10/10 - 1s - loss: 402.4585 - loglik: -4.0221e+02 - logprior: -2.5154e-01
Epoch 10/10
10/10 - 1s - loss: 402.4315 - loglik: -4.0223e+02 - logprior: -2.0175e-01
Fitted a model with MAP estimate = -402.2747
Time for alignment: 45.1717
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 475.6242 - loglik: -4.5343e+02 - logprior: -2.2190e+01
Epoch 2/10
10/10 - 1s - loss: 443.7139 - loglik: -4.3787e+02 - logprior: -5.8474e+00
Epoch 3/10
10/10 - 1s - loss: 426.7404 - loglik: -4.2367e+02 - logprior: -3.0705e+00
Epoch 4/10
10/10 - 1s - loss: 417.3497 - loglik: -4.1493e+02 - logprior: -2.4154e+00
Epoch 5/10
10/10 - 1s - loss: 413.9901 - loglik: -4.1192e+02 - logprior: -2.0701e+00
Epoch 6/10
10/10 - 1s - loss: 412.3205 - loglik: -4.1065e+02 - logprior: -1.6701e+00
Epoch 7/10
10/10 - 1s - loss: 411.3853 - loglik: -4.0990e+02 - logprior: -1.4836e+00
Epoch 8/10
10/10 - 1s - loss: 411.5649 - loglik: -4.1010e+02 - logprior: -1.4606e+00
Fitted a model with MAP estimate = -410.9581
expansions: [(0, 2), (8, 1), (9, 1), (22, 1), (41, 2), (42, 3), (44, 2), (46, 1), (48, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 78 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 437.1416 - loglik: -4.1030e+02 - logprior: -2.6839e+01
Epoch 2/2
10/10 - 1s - loss: 414.1051 - loglik: -4.0599e+02 - logprior: -8.1113e+00
Fitted a model with MAP estimate = -409.9751
expansions: []
discards: [ 0 49 54 61]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 429.8022 - loglik: -4.0661e+02 - logprior: -2.3191e+01
Epoch 2/2
10/10 - 1s - loss: 415.5732 - loglik: -4.0647e+02 - logprior: -9.0997e+00
Fitted a model with MAP estimate = -412.2497
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 425.3206 - loglik: -4.0512e+02 - logprior: -2.0201e+01
Epoch 2/10
10/10 - 1s - loss: 410.2257 - loglik: -4.0492e+02 - logprior: -5.3079e+00
Epoch 3/10
10/10 - 1s - loss: 407.0185 - loglik: -4.0475e+02 - logprior: -2.2727e+00
Epoch 4/10
10/10 - 1s - loss: 405.3548 - loglik: -4.0398e+02 - logprior: -1.3781e+00
Epoch 5/10
10/10 - 1s - loss: 404.6299 - loglik: -4.0358e+02 - logprior: -1.0520e+00
Epoch 6/10
10/10 - 1s - loss: 403.9920 - loglik: -4.0331e+02 - logprior: -6.8665e-01
Epoch 7/10
10/10 - 1s - loss: 404.0834 - loglik: -4.0365e+02 - logprior: -4.3651e-01
Fitted a model with MAP estimate = -403.6936
Time for alignment: 40.3545
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 474.9441 - loglik: -4.5275e+02 - logprior: -2.2192e+01
Epoch 2/10
10/10 - 1s - loss: 444.1102 - loglik: -4.3826e+02 - logprior: -5.8485e+00
Epoch 3/10
10/10 - 1s - loss: 425.8644 - loglik: -4.2280e+02 - logprior: -3.0672e+00
Epoch 4/10
10/10 - 1s - loss: 417.0592 - loglik: -4.1465e+02 - logprior: -2.4128e+00
Epoch 5/10
10/10 - 1s - loss: 413.7204 - loglik: -4.1165e+02 - logprior: -2.0746e+00
Epoch 6/10
10/10 - 1s - loss: 412.3446 - loglik: -4.1066e+02 - logprior: -1.6885e+00
Epoch 7/10
10/10 - 1s - loss: 411.7342 - loglik: -4.1026e+02 - logprior: -1.4790e+00
Epoch 8/10
10/10 - 1s - loss: 410.9203 - loglik: -4.0948e+02 - logprior: -1.4403e+00
Epoch 9/10
10/10 - 1s - loss: 410.6317 - loglik: -4.0925e+02 - logprior: -1.3862e+00
Epoch 10/10
10/10 - 1s - loss: 410.2507 - loglik: -4.0891e+02 - logprior: -1.3389e+00
Fitted a model with MAP estimate = -410.3638
expansions: [(0, 2), (7, 2), (8, 2), (23, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 2), (46, 1), (48, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 80 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 437.2281 - loglik: -4.1004e+02 - logprior: -2.7189e+01
Epoch 2/2
10/10 - 1s - loss: 414.9444 - loglik: -4.0659e+02 - logprior: -8.3593e+00
Fitted a model with MAP estimate = -410.2003
expansions: []
discards: [ 0 10 12 53 55 63]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 430.5065 - loglik: -4.0718e+02 - logprior: -2.3327e+01
Epoch 2/2
10/10 - 1s - loss: 415.4617 - loglik: -4.0627e+02 - logprior: -9.1895e+00
Fitted a model with MAP estimate = -412.4687
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 425.7663 - loglik: -4.0540e+02 - logprior: -2.0371e+01
Epoch 2/10
10/10 - 1s - loss: 410.3184 - loglik: -4.0489e+02 - logprior: -5.4252e+00
Epoch 3/10
10/10 - 1s - loss: 407.0413 - loglik: -4.0467e+02 - logprior: -2.3722e+00
Epoch 4/10
10/10 - 1s - loss: 405.4720 - loglik: -4.0402e+02 - logprior: -1.4518e+00
Epoch 5/10
10/10 - 1s - loss: 405.1044 - loglik: -4.0398e+02 - logprior: -1.1246e+00
Epoch 6/10
10/10 - 1s - loss: 403.8203 - loglik: -4.0305e+02 - logprior: -7.7432e-01
Epoch 7/10
10/10 - 1s - loss: 403.8345 - loglik: -4.0334e+02 - logprior: -4.9658e-01
Fitted a model with MAP estimate = -403.8100
Time for alignment: 42.2918
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 474.9041 - loglik: -4.5271e+02 - logprior: -2.2194e+01
Epoch 2/10
10/10 - 1s - loss: 443.8669 - loglik: -4.3802e+02 - logprior: -5.8485e+00
Epoch 3/10
10/10 - 1s - loss: 424.6316 - loglik: -4.2159e+02 - logprior: -3.0430e+00
Epoch 4/10
10/10 - 1s - loss: 417.0826 - loglik: -4.1467e+02 - logprior: -2.4137e+00
Epoch 5/10
10/10 - 1s - loss: 413.3044 - loglik: -4.1118e+02 - logprior: -2.1198e+00
Epoch 6/10
10/10 - 1s - loss: 412.4682 - loglik: -4.1077e+02 - logprior: -1.7023e+00
Epoch 7/10
10/10 - 1s - loss: 411.5789 - loglik: -4.1009e+02 - logprior: -1.4898e+00
Epoch 8/10
10/10 - 1s - loss: 410.7222 - loglik: -4.0923e+02 - logprior: -1.4963e+00
Epoch 9/10
10/10 - 1s - loss: 410.8463 - loglik: -4.0937e+02 - logprior: -1.4811e+00
Fitted a model with MAP estimate = -410.5282
expansions: [(0, 2), (8, 1), (9, 1), (39, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 2), (46, 1), (48, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 78 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 436.7912 - loglik: -4.0981e+02 - logprior: -2.6978e+01
Epoch 2/2
10/10 - 1s - loss: 414.1862 - loglik: -4.0598e+02 - logprior: -8.2037e+00
Fitted a model with MAP estimate = -409.9557
expansions: []
discards: [ 0 51 53 61]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 430.0426 - loglik: -4.0675e+02 - logprior: -2.3289e+01
Epoch 2/2
10/10 - 1s - loss: 415.4006 - loglik: -4.0623e+02 - logprior: -9.1667e+00
Fitted a model with MAP estimate = -412.2992
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 425.4169 - loglik: -4.0510e+02 - logprior: -2.0321e+01
Epoch 2/10
10/10 - 1s - loss: 410.6465 - loglik: -4.0525e+02 - logprior: -5.3932e+00
Epoch 3/10
10/10 - 1s - loss: 406.5571 - loglik: -4.0420e+02 - logprior: -2.3527e+00
Epoch 4/10
10/10 - 1s - loss: 405.7709 - loglik: -4.0433e+02 - logprior: -1.4407e+00
Epoch 5/10
10/10 - 1s - loss: 404.4054 - loglik: -4.0328e+02 - logprior: -1.1231e+00
Epoch 6/10
10/10 - 1s - loss: 404.0709 - loglik: -4.0330e+02 - logprior: -7.7313e-01
Epoch 7/10
10/10 - 1s - loss: 404.4225 - loglik: -4.0393e+02 - logprior: -4.9293e-01
Fitted a model with MAP estimate = -403.8130
Time for alignment: 41.1064
Computed alignments with likelihoods: ['-402.5187', '-402.2747', '-403.6936', '-403.8100', '-403.8130']
Best model has likelihood: -402.2747  (prior= -0.1806 )
time for generating output: 0.1696
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/GEL.projection.fasta
SP score = 0.6805970149253732
Training of 5 independent models on file hr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5900f56190>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52f8ba8370>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b135891c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f528bde0f10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f0ce53d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b1c3682e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5294989b20>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52f1446fd0>, <__main__.SimpleDirichletPrior object at 0x7f5afeb6d1f0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 403.3773 - loglik: -3.9091e+02 - logprior: -1.2469e+01
Epoch 2/10
11/11 - 1s - loss: 358.6005 - loglik: -3.5514e+02 - logprior: -3.4557e+00
Epoch 3/10
11/11 - 1s - loss: 326.3618 - loglik: -3.2387e+02 - logprior: -2.4904e+00
Epoch 4/10
11/11 - 1s - loss: 312.4716 - loglik: -3.1038e+02 - logprior: -2.0879e+00
Epoch 5/10
11/11 - 1s - loss: 309.1928 - loglik: -3.0734e+02 - logprior: -1.8555e+00
Epoch 6/10
11/11 - 1s - loss: 306.7261 - loglik: -3.0486e+02 - logprior: -1.8694e+00
Epoch 7/10
11/11 - 1s - loss: 306.6543 - loglik: -3.0485e+02 - logprior: -1.8062e+00
Epoch 8/10
11/11 - 1s - loss: 306.3532 - loglik: -3.0454e+02 - logprior: -1.8141e+00
Epoch 9/10
11/11 - 1s - loss: 305.5635 - loglik: -3.0378e+02 - logprior: -1.7880e+00
Epoch 10/10
11/11 - 1s - loss: 306.1565 - loglik: -3.0437e+02 - logprior: -1.7830e+00
Fitted a model with MAP estimate = -305.8297
expansions: [(0, 3), (15, 2), (28, 1), (29, 3), (30, 2), (31, 1), (32, 2), (33, 1), (36, 1), (37, 1), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 316.0105 - loglik: -3.0197e+02 - logprior: -1.4038e+01
Epoch 2/2
11/11 - 1s - loss: 298.6900 - loglik: -2.9439e+02 - logprior: -4.3049e+00
Fitted a model with MAP estimate = -295.8835
expansions: []
discards: [ 0 35 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 308.0581 - loglik: -2.9491e+02 - logprior: -1.3150e+01
Epoch 2/2
11/11 - 1s - loss: 298.9738 - loglik: -2.9346e+02 - logprior: -5.5167e+00
Fitted a model with MAP estimate = -296.7415
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 304.2635 - loglik: -2.9346e+02 - logprior: -1.0806e+01
Epoch 2/10
11/11 - 1s - loss: 295.8203 - loglik: -2.9286e+02 - logprior: -2.9595e+00
Epoch 3/10
11/11 - 1s - loss: 294.0188 - loglik: -2.9226e+02 - logprior: -1.7562e+00
Epoch 4/10
11/11 - 1s - loss: 293.6184 - loglik: -2.9207e+02 - logprior: -1.5434e+00
Epoch 5/10
11/11 - 1s - loss: 293.5127 - loglik: -2.9202e+02 - logprior: -1.4887e+00
Epoch 6/10
11/11 - 1s - loss: 292.7480 - loglik: -2.9140e+02 - logprior: -1.3459e+00
Epoch 7/10
11/11 - 1s - loss: 292.6360 - loglik: -2.9139e+02 - logprior: -1.2442e+00
Epoch 8/10
11/11 - 1s - loss: 293.0009 - loglik: -2.9177e+02 - logprior: -1.2321e+00
Fitted a model with MAP estimate = -292.5150
Time for alignment: 40.7628
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 402.0418 - loglik: -3.8958e+02 - logprior: -1.2467e+01
Epoch 2/10
11/11 - 1s - loss: 359.8881 - loglik: -3.5643e+02 - logprior: -3.4547e+00
Epoch 3/10
11/11 - 1s - loss: 325.5774 - loglik: -3.2308e+02 - logprior: -2.4930e+00
Epoch 4/10
11/11 - 1s - loss: 312.9422 - loglik: -3.1086e+02 - logprior: -2.0835e+00
Epoch 5/10
11/11 - 1s - loss: 308.9494 - loglik: -3.0709e+02 - logprior: -1.8612e+00
Epoch 6/10
11/11 - 1s - loss: 307.3589 - loglik: -3.0551e+02 - logprior: -1.8533e+00
Epoch 7/10
11/11 - 1s - loss: 306.1469 - loglik: -3.0435e+02 - logprior: -1.7999e+00
Epoch 8/10
11/11 - 1s - loss: 306.4181 - loglik: -3.0460e+02 - logprior: -1.8152e+00
Fitted a model with MAP estimate = -306.1449
expansions: [(0, 3), (15, 2), (28, 1), (29, 3), (30, 2), (31, 1), (32, 2), (33, 1), (36, 1), (37, 1), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 315.4904 - loglik: -3.0152e+02 - logprior: -1.3970e+01
Epoch 2/2
11/11 - 1s - loss: 299.4955 - loglik: -2.9522e+02 - logprior: -4.2764e+00
Fitted a model with MAP estimate = -295.8518
expansions: []
discards: [ 0 35 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 308.6534 - loglik: -2.9550e+02 - logprior: -1.3155e+01
Epoch 2/2
11/11 - 1s - loss: 298.3544 - loglik: -2.9281e+02 - logprior: -5.5459e+00
Fitted a model with MAP estimate = -296.7786
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 304.2501 - loglik: -2.9344e+02 - logprior: -1.0811e+01
Epoch 2/10
11/11 - 1s - loss: 296.0128 - loglik: -2.9304e+02 - logprior: -2.9702e+00
Epoch 3/10
11/11 - 1s - loss: 293.8645 - loglik: -2.9211e+02 - logprior: -1.7586e+00
Epoch 4/10
11/11 - 1s - loss: 293.9636 - loglik: -2.9241e+02 - logprior: -1.5507e+00
Fitted a model with MAP estimate = -293.2187
Time for alignment: 34.3677
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 402.4861 - loglik: -3.9002e+02 - logprior: -1.2469e+01
Epoch 2/10
11/11 - 1s - loss: 360.3793 - loglik: -3.5691e+02 - logprior: -3.4677e+00
Epoch 3/10
11/11 - 1s - loss: 328.1562 - loglik: -3.2563e+02 - logprior: -2.5302e+00
Epoch 4/10
11/11 - 1s - loss: 314.0624 - loglik: -3.1194e+02 - logprior: -2.1207e+00
Epoch 5/10
11/11 - 1s - loss: 308.1122 - loglik: -3.0619e+02 - logprior: -1.9214e+00
Epoch 6/10
11/11 - 1s - loss: 305.7596 - loglik: -3.0380e+02 - logprior: -1.9610e+00
Epoch 7/10
11/11 - 1s - loss: 305.1292 - loglik: -3.0321e+02 - logprior: -1.9174e+00
Epoch 8/10
11/11 - 1s - loss: 304.8391 - loglik: -3.0291e+02 - logprior: -1.9251e+00
Epoch 9/10
11/11 - 1s - loss: 303.9035 - loglik: -3.0200e+02 - logprior: -1.9007e+00
Epoch 10/10
11/11 - 1s - loss: 303.6049 - loglik: -3.0170e+02 - logprior: -1.9054e+00
Fitted a model with MAP estimate = -304.0453
expansions: [(0, 3), (15, 1), (26, 1), (27, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (33, 1), (38, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 315.7719 - loglik: -3.0171e+02 - logprior: -1.4058e+01
Epoch 2/2
11/11 - 1s - loss: 298.3993 - loglik: -2.9411e+02 - logprior: -4.2895e+00
Fitted a model with MAP estimate = -295.6421
expansions: []
discards: [ 0 36 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 308.1760 - loglik: -2.9505e+02 - logprior: -1.3131e+01
Epoch 2/2
11/11 - 1s - loss: 298.8497 - loglik: -2.9333e+02 - logprior: -5.5181e+00
Fitted a model with MAP estimate = -296.7156
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 304.0702 - loglik: -2.9327e+02 - logprior: -1.0803e+01
Epoch 2/10
11/11 - 1s - loss: 296.2475 - loglik: -2.9329e+02 - logprior: -2.9568e+00
Epoch 3/10
11/11 - 1s - loss: 294.3594 - loglik: -2.9261e+02 - logprior: -1.7473e+00
Epoch 4/10
11/11 - 1s - loss: 293.3215 - loglik: -2.9178e+02 - logprior: -1.5400e+00
Epoch 5/10
11/11 - 1s - loss: 292.8141 - loglik: -2.9134e+02 - logprior: -1.4767e+00
Epoch 6/10
11/11 - 1s - loss: 293.4570 - loglik: -2.9212e+02 - logprior: -1.3355e+00
Fitted a model with MAP estimate = -292.7032
Time for alignment: 38.0515
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 402.9956 - loglik: -3.9053e+02 - logprior: -1.2467e+01
Epoch 2/10
11/11 - 1s - loss: 359.6799 - loglik: -3.5624e+02 - logprior: -3.4446e+00
Epoch 3/10
11/11 - 1s - loss: 327.4174 - loglik: -3.2493e+02 - logprior: -2.4873e+00
Epoch 4/10
11/11 - 1s - loss: 313.0291 - loglik: -3.1091e+02 - logprior: -2.1231e+00
Epoch 5/10
11/11 - 1s - loss: 306.8144 - loglik: -3.0489e+02 - logprior: -1.9244e+00
Epoch 6/10
11/11 - 1s - loss: 305.8747 - loglik: -3.0392e+02 - logprior: -1.9578e+00
Epoch 7/10
11/11 - 1s - loss: 303.4072 - loglik: -3.0153e+02 - logprior: -1.8797e+00
Epoch 8/10
11/11 - 1s - loss: 304.8425 - loglik: -3.0296e+02 - logprior: -1.8859e+00
Fitted a model with MAP estimate = -304.0375
expansions: [(0, 3), (15, 1), (26, 1), (28, 1), (29, 3), (30, 2), (31, 1), (32, 2), (33, 1), (34, 1), (37, 1), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 314.8776 - loglik: -3.0090e+02 - logprior: -1.3977e+01
Epoch 2/2
11/11 - 1s - loss: 298.6170 - loglik: -2.9434e+02 - logprior: -4.2721e+00
Fitted a model with MAP estimate = -295.6773
expansions: []
discards: [ 0 36 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 308.2859 - loglik: -2.9513e+02 - logprior: -1.3152e+01
Epoch 2/2
11/11 - 1s - loss: 298.9489 - loglik: -2.9340e+02 - logprior: -5.5455e+00
Fitted a model with MAP estimate = -296.7786
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 304.5044 - loglik: -2.9369e+02 - logprior: -1.0813e+01
Epoch 2/10
11/11 - 1s - loss: 295.7356 - loglik: -2.9277e+02 - logprior: -2.9655e+00
Epoch 3/10
11/11 - 1s - loss: 294.5231 - loglik: -2.9277e+02 - logprior: -1.7561e+00
Epoch 4/10
11/11 - 1s - loss: 293.0807 - loglik: -2.9153e+02 - logprior: -1.5493e+00
Epoch 5/10
11/11 - 1s - loss: 293.2759 - loglik: -2.9179e+02 - logprior: -1.4845e+00
Fitted a model with MAP estimate = -292.9150
Time for alignment: 34.4610
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 402.3399 - loglik: -3.8987e+02 - logprior: -1.2471e+01
Epoch 2/10
11/11 - 1s - loss: 359.5816 - loglik: -3.5610e+02 - logprior: -3.4784e+00
Epoch 3/10
11/11 - 1s - loss: 326.1931 - loglik: -3.2368e+02 - logprior: -2.5116e+00
Epoch 4/10
11/11 - 1s - loss: 312.7927 - loglik: -3.1072e+02 - logprior: -2.0715e+00
Epoch 5/10
11/11 - 1s - loss: 308.3966 - loglik: -3.0653e+02 - logprior: -1.8706e+00
Epoch 6/10
11/11 - 1s - loss: 306.3310 - loglik: -3.0445e+02 - logprior: -1.8859e+00
Epoch 7/10
11/11 - 1s - loss: 305.9798 - loglik: -3.0413e+02 - logprior: -1.8481e+00
Epoch 8/10
11/11 - 1s - loss: 304.8214 - loglik: -3.0296e+02 - logprior: -1.8571e+00
Epoch 9/10
11/11 - 1s - loss: 305.4705 - loglik: -3.0363e+02 - logprior: -1.8380e+00
Fitted a model with MAP estimate = -304.9870
expansions: [(0, 3), (14, 1), (15, 1), (28, 1), (29, 3), (30, 2), (31, 1), (32, 1), (35, 1), (38, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 73 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 315.4472 - loglik: -3.0148e+02 - logprior: -1.3968e+01
Epoch 2/2
11/11 - 1s - loss: 298.4881 - loglik: -2.9427e+02 - logprior: -4.2164e+00
Fitted a model with MAP estimate = -295.6309
expansions: []
discards: [ 0 35 39]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 307.9230 - loglik: -2.9479e+02 - logprior: -1.3129e+01
Epoch 2/2
11/11 - 1s - loss: 298.9279 - loglik: -2.9340e+02 - logprior: -5.5250e+00
Fitted a model with MAP estimate = -296.7491
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 304.6258 - loglik: -2.9382e+02 - logprior: -1.0810e+01
Epoch 2/10
11/11 - 1s - loss: 295.6017 - loglik: -2.9265e+02 - logprior: -2.9538e+00
Epoch 3/10
11/11 - 1s - loss: 294.1854 - loglik: -2.9244e+02 - logprior: -1.7488e+00
Epoch 4/10
11/11 - 1s - loss: 293.5289 - loglik: -2.9199e+02 - logprior: -1.5404e+00
Epoch 5/10
11/11 - 1s - loss: 292.9822 - loglik: -2.9150e+02 - logprior: -1.4849e+00
Epoch 6/10
11/11 - 1s - loss: 293.1474 - loglik: -2.9180e+02 - logprior: -1.3466e+00
Fitted a model with MAP estimate = -292.7225
Time for alignment: 36.1468
Computed alignments with likelihoods: ['-292.5150', '-293.2187', '-292.7032', '-292.9150', '-292.7225']
Best model has likelihood: -292.5150  (prior= -1.2333 )
time for generating output: 0.1325
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hr.projection.fasta
SP score = 0.9957142857142857
Training of 5 independent models on file rvp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5334df03a0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f529465af10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5aff4d9730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5aff4d9820>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5315e9a7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5315e9a100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f528bf30970>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5ca7407b50>, <__main__.SimpleDirichletPrior object at 0x7f531df467f0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 445.3409 - loglik: -4.4418e+02 - logprior: -1.1615e+00
Epoch 2/10
42/42 - 4s - loss: 353.9971 - loglik: -3.5261e+02 - logprior: -1.3837e+00
Epoch 3/10
42/42 - 4s - loss: 352.1415 - loglik: -3.5082e+02 - logprior: -1.3191e+00
Epoch 4/10
42/42 - 4s - loss: 350.8368 - loglik: -3.4953e+02 - logprior: -1.3038e+00
Epoch 5/10
42/42 - 4s - loss: 350.8151 - loglik: -3.4955e+02 - logprior: -1.2686e+00
Epoch 6/10
42/42 - 5s - loss: 350.8398 - loglik: -3.4960e+02 - logprior: -1.2407e+00
Fitted a model with MAP estimate = -347.3229
expansions: [(1, 1), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (29, 1), (31, 1), (47, 1), (48, 1), (49, 1), (55, 1), (56, 1), (59, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 320.4275 - loglik: -3.1891e+02 - logprior: -1.5164e+00
Epoch 2/2
42/42 - 5s - loss: 307.9865 - loglik: -3.0668e+02 - logprior: -1.3026e+00
Fitted a model with MAP estimate = -304.5370
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 13s - loss: 304.3036 - loglik: -3.0337e+02 - logprior: -9.3137e-01
Epoch 2/10
59/59 - 6s - loss: 303.1124 - loglik: -3.0226e+02 - logprior: -8.5545e-01
Epoch 3/10
59/59 - 6s - loss: 303.2918 - loglik: -3.0245e+02 - logprior: -8.4270e-01
Fitted a model with MAP estimate = -302.8794
Time for alignment: 134.4509
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 444.8600 - loglik: -4.4369e+02 - logprior: -1.1685e+00
Epoch 2/10
42/42 - 4s - loss: 354.8670 - loglik: -3.5349e+02 - logprior: -1.3812e+00
Epoch 3/10
42/42 - 5s - loss: 351.3650 - loglik: -3.5004e+02 - logprior: -1.3240e+00
Epoch 4/10
42/42 - 4s - loss: 351.7697 - loglik: -3.5047e+02 - logprior: -1.2983e+00
Fitted a model with MAP estimate = -347.6279
expansions: [(1, 1), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (29, 1), (31, 1), (47, 1), (48, 1), (49, 1), (55, 1), (56, 1), (59, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 320.1617 - loglik: -3.1865e+02 - logprior: -1.5103e+00
Epoch 2/2
42/42 - 5s - loss: 308.4514 - loglik: -3.0715e+02 - logprior: -1.3034e+00
Fitted a model with MAP estimate = -304.4992
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 10s - loss: 303.8424 - loglik: -3.0289e+02 - logprior: -9.5269e-01
Epoch 2/10
59/59 - 6s - loss: 303.4500 - loglik: -3.0258e+02 - logprior: -8.7432e-01
Epoch 3/10
59/59 - 6s - loss: 303.4383 - loglik: -3.0257e+02 - logprior: -8.6862e-01
Epoch 4/10
59/59 - 6s - loss: 303.1470 - loglik: -3.0235e+02 - logprior: -7.9660e-01
Epoch 5/10
59/59 - 6s - loss: 302.6520 - loglik: -3.0190e+02 - logprior: -7.4799e-01
Epoch 6/10
59/59 - 6s - loss: 302.8251 - loglik: -3.0208e+02 - logprior: -7.4165e-01
Fitted a model with MAP estimate = -302.3435
Time for alignment: 143.5565
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 9s - loss: 445.7495 - loglik: -4.4458e+02 - logprior: -1.1675e+00
Epoch 2/10
42/42 - 5s - loss: 354.7608 - loglik: -3.5338e+02 - logprior: -1.3828e+00
Epoch 3/10
42/42 - 5s - loss: 352.0938 - loglik: -3.5078e+02 - logprior: -1.3125e+00
Epoch 4/10
42/42 - 5s - loss: 351.3066 - loglik: -3.5001e+02 - logprior: -1.3014e+00
Epoch 5/10
42/42 - 5s - loss: 351.2415 - loglik: -3.4998e+02 - logprior: -1.2659e+00
Epoch 6/10
42/42 - 5s - loss: 350.7835 - loglik: -3.4954e+02 - logprior: -1.2457e+00
Epoch 7/10
42/42 - 4s - loss: 350.7007 - loglik: -3.4948e+02 - logprior: -1.2169e+00
Epoch 8/10
42/42 - 4s - loss: 350.4064 - loglik: -3.4924e+02 - logprior: -1.1697e+00
Epoch 9/10
42/42 - 4s - loss: 350.1903 - loglik: -3.4903e+02 - logprior: -1.1629e+00
Epoch 10/10
42/42 - 5s - loss: 350.0783 - loglik: -3.4903e+02 - logprior: -1.0514e+00
Fitted a model with MAP estimate = -347.2736
expansions: [(1, 1), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (29, 1), (33, 1), (47, 1), (48, 1), (49, 1), (55, 1), (56, 1), (59, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 320.6299 - loglik: -3.1915e+02 - logprior: -1.4773e+00
Epoch 2/2
42/42 - 5s - loss: 307.7889 - loglik: -3.0649e+02 - logprior: -1.2962e+00
Fitted a model with MAP estimate = -304.3958
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 10s - loss: 304.1348 - loglik: -3.0321e+02 - logprior: -9.2657e-01
Epoch 2/10
59/59 - 6s - loss: 303.3025 - loglik: -3.0245e+02 - logprior: -8.4975e-01
Epoch 3/10
59/59 - 6s - loss: 303.2504 - loglik: -3.0240e+02 - logprior: -8.4547e-01
Epoch 4/10
59/59 - 6s - loss: 302.8930 - loglik: -3.0212e+02 - logprior: -7.6859e-01
Epoch 5/10
59/59 - 6s - loss: 302.7973 - loglik: -3.0205e+02 - logprior: -7.4241e-01
Epoch 6/10
59/59 - 6s - loss: 302.7016 - loglik: -3.0197e+02 - logprior: -7.3271e-01
Epoch 7/10
59/59 - 7s - loss: 302.3623 - loglik: -3.0165e+02 - logprior: -7.1161e-01
Epoch 8/10
59/59 - 6s - loss: 302.2661 - loglik: -3.0158e+02 - logprior: -6.8993e-01
Epoch 9/10
59/59 - 7s - loss: 302.1344 - loglik: -3.0145e+02 - logprior: -6.8679e-01
Epoch 10/10
59/59 - 6s - loss: 301.7777 - loglik: -3.0112e+02 - logprior: -6.5290e-01
Fitted a model with MAP estimate = -301.8689
Time for alignment: 197.7721
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 9s - loss: 445.3845 - loglik: -4.4422e+02 - logprior: -1.1680e+00
Epoch 2/10
42/42 - 5s - loss: 354.1952 - loglik: -3.5282e+02 - logprior: -1.3776e+00
Epoch 3/10
42/42 - 5s - loss: 351.8982 - loglik: -3.5058e+02 - logprior: -1.3209e+00
Epoch 4/10
42/42 - 5s - loss: 351.3033 - loglik: -3.5000e+02 - logprior: -1.3035e+00
Epoch 5/10
42/42 - 4s - loss: 351.2692 - loglik: -3.5000e+02 - logprior: -1.2706e+00
Epoch 6/10
42/42 - 4s - loss: 350.5085 - loglik: -3.4926e+02 - logprior: -1.2448e+00
Epoch 7/10
42/42 - 5s - loss: 350.4320 - loglik: -3.4922e+02 - logprior: -1.2125e+00
Epoch 8/10
42/42 - 4s - loss: 350.2410 - loglik: -3.4907e+02 - logprior: -1.1696e+00
Epoch 9/10
42/42 - 5s - loss: 350.3229 - loglik: -3.4918e+02 - logprior: -1.1478e+00
Fitted a model with MAP estimate = -347.1814
expansions: [(1, 1), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (29, 1), (31, 1), (47, 1), (48, 1), (49, 1), (55, 1), (56, 1), (59, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 320.2747 - loglik: -3.1879e+02 - logprior: -1.4834e+00
Epoch 2/2
42/42 - 5s - loss: 307.8607 - loglik: -3.0656e+02 - logprior: -1.2987e+00
Fitted a model with MAP estimate = -304.3683
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 10s - loss: 303.7887 - loglik: -3.0286e+02 - logprior: -9.2815e-01
Epoch 2/10
59/59 - 6s - loss: 303.3452 - loglik: -3.0250e+02 - logprior: -8.4744e-01
Epoch 3/10
59/59 - 6s - loss: 303.4067 - loglik: -3.0257e+02 - logprior: -8.4100e-01
Fitted a model with MAP estimate = -302.9184
Time for alignment: 148.4467
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 445.2782 - loglik: -4.4411e+02 - logprior: -1.1683e+00
Epoch 2/10
42/42 - 4s - loss: 354.2331 - loglik: -3.5284e+02 - logprior: -1.3889e+00
Epoch 3/10
42/42 - 5s - loss: 351.6674 - loglik: -3.5035e+02 - logprior: -1.3205e+00
Epoch 4/10
42/42 - 4s - loss: 351.3402 - loglik: -3.5004e+02 - logprior: -1.3014e+00
Epoch 5/10
42/42 - 4s - loss: 350.7011 - loglik: -3.4944e+02 - logprior: -1.2617e+00
Epoch 6/10
42/42 - 4s - loss: 350.7557 - loglik: -3.4952e+02 - logprior: -1.2334e+00
Fitted a model with MAP estimate = -347.1382
expansions: [(1, 1), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (29, 1), (31, 1), (44, 1), (47, 1), (48, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 9s - loss: 320.3062 - loglik: -3.1878e+02 - logprior: -1.5237e+00
Epoch 2/2
42/42 - 5s - loss: 308.0656 - loglik: -3.0676e+02 - logprior: -1.3040e+00
Fitted a model with MAP estimate = -304.5352
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 10s - loss: 304.0767 - loglik: -3.0314e+02 - logprior: -9.3484e-01
Epoch 2/10
59/59 - 7s - loss: 303.5288 - loglik: -3.0267e+02 - logprior: -8.6013e-01
Epoch 3/10
59/59 - 6s - loss: 303.3290 - loglik: -3.0247e+02 - logprior: -8.5474e-01
Epoch 4/10
59/59 - 7s - loss: 302.8574 - loglik: -3.0208e+02 - logprior: -7.7694e-01
Epoch 5/10
59/59 - 6s - loss: 302.5191 - loglik: -3.0177e+02 - logprior: -7.4495e-01
Epoch 6/10
59/59 - 7s - loss: 302.9184 - loglik: -3.0218e+02 - logprior: -7.3644e-01
Fitted a model with MAP estimate = -302.3027
Time for alignment: 153.9922
Computed alignments with likelihoods: ['-302.8794', '-302.3435', '-301.8689', '-302.9184', '-302.3027']
Best model has likelihood: -301.8689  (prior= -0.6313 )
time for generating output: 0.1655
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rvp.projection.fasta
SP score = 0.35994587280108253
Training of 5 independent models on file mmp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5301007880>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52d9ef6fa0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5301906910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f99c7430>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f59646f4490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5301cce730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531dcdb6a0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f59642a4190>, <__main__.SimpleDirichletPrior object at 0x7f52f1141580>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 945.1938 - loglik: -9.1711e+02 - logprior: -2.8081e+01
Epoch 2/10
14/14 - 4s - loss: 875.8058 - loglik: -8.7169e+02 - logprior: -4.1136e+00
Epoch 3/10
14/14 - 4s - loss: 841.1567 - loglik: -8.3944e+02 - logprior: -1.7211e+00
Epoch 4/10
14/14 - 4s - loss: 827.4249 - loglik: -8.2610e+02 - logprior: -1.3238e+00
Epoch 5/10
14/14 - 4s - loss: 822.3228 - loglik: -8.2113e+02 - logprior: -1.1954e+00
Epoch 6/10
14/14 - 4s - loss: 820.9042 - loglik: -8.2000e+02 - logprior: -9.0390e-01
Epoch 7/10
14/14 - 4s - loss: 822.7867 - loglik: -8.2195e+02 - logprior: -8.3514e-01
Fitted a model with MAP estimate = -819.7990
expansions: [(3, 1), (16, 5), (17, 1), (19, 1), (36, 1), (38, 2), (43, 1), (50, 1), (66, 1), (67, 1), (70, 1), (79, 1), (80, 3), (101, 1), (102, 3), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [ 0 45]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 847.3643 - loglik: -8.1818e+02 - logprior: -2.9184e+01
Epoch 2/2
14/14 - 4s - loss: 823.1427 - loglik: -8.1314e+02 - logprior: -1.0001e+01
Fitted a model with MAP estimate = -817.5581
expansions: [(0, 21), (4, 1), (15, 1)]
discards: [  0  16  46 122]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 835.3917 - loglik: -8.1350e+02 - logprior: -2.1896e+01
Epoch 2/2
14/14 - 4s - loss: 810.0751 - loglik: -8.0720e+02 - logprior: -2.8762e+00
Fitted a model with MAP estimate = -808.6980
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 832.0696 - loglik: -8.0725e+02 - logprior: -2.4819e+01
Epoch 2/10
14/14 - 4s - loss: 812.3220 - loglik: -8.0925e+02 - logprior: -3.0696e+00
Epoch 3/10
14/14 - 4s - loss: 807.0919 - loglik: -8.0774e+02 - logprior: 0.6443
Epoch 4/10
14/14 - 4s - loss: 805.1000 - loglik: -8.0686e+02 - logprior: 1.7635
Epoch 5/10
14/14 - 4s - loss: 803.8122 - loglik: -8.0617e+02 - logprior: 2.3555
Epoch 6/10
14/14 - 4s - loss: 804.8110 - loglik: -8.0751e+02 - logprior: 2.6999
Fitted a model with MAP estimate = -803.1361
Time for alignment: 97.4254
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 944.0180 - loglik: -9.1594e+02 - logprior: -2.8082e+01
Epoch 2/10
14/14 - 4s - loss: 875.6600 - loglik: -8.7145e+02 - logprior: -4.2057e+00
Epoch 3/10
14/14 - 4s - loss: 839.1510 - loglik: -8.3740e+02 - logprior: -1.7499e+00
Epoch 4/10
14/14 - 4s - loss: 824.9888 - loglik: -8.2367e+02 - logprior: -1.3170e+00
Epoch 5/10
14/14 - 4s - loss: 820.9335 - loglik: -8.1986e+02 - logprior: -1.0776e+00
Epoch 6/10
14/14 - 4s - loss: 820.5201 - loglik: -8.1969e+02 - logprior: -8.3427e-01
Epoch 7/10
14/14 - 4s - loss: 819.6475 - loglik: -8.1895e+02 - logprior: -6.9879e-01
Epoch 8/10
14/14 - 4s - loss: 817.6730 - loglik: -8.1688e+02 - logprior: -7.9386e-01
Epoch 9/10
14/14 - 4s - loss: 817.5283 - loglik: -8.1663e+02 - logprior: -8.9978e-01
Epoch 10/10
14/14 - 4s - loss: 818.8883 - loglik: -8.1802e+02 - logprior: -8.7180e-01
Fitted a model with MAP estimate = -817.8572
expansions: [(3, 1), (4, 1), (16, 4), (18, 1), (35, 1), (36, 3), (38, 2), (41, 1), (44, 2), (66, 2), (76, 1), (78, 1), (79, 3), (100, 1), (102, 1), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [ 0  1 45]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 849.1876 - loglik: -8.2003e+02 - logprior: -2.9160e+01
Epoch 2/2
14/14 - 4s - loss: 823.1689 - loglik: -8.1339e+02 - logprior: -9.7804e+00
Fitted a model with MAP estimate = -818.0311
expansions: [(0, 26), (4, 2), (15, 1), (81, 1)]
discards: [ 0  1 43 47 58]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 838.4911 - loglik: -8.1656e+02 - logprior: -2.1929e+01
Epoch 2/2
14/14 - 5s - loss: 809.6320 - loglik: -8.0722e+02 - logprior: -2.4087e+00
Fitted a model with MAP estimate = -809.2395
expansions: [(82, 2)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 834.9355 - loglik: -8.0936e+02 - logprior: -2.5577e+01
Epoch 2/10
14/14 - 4s - loss: 811.3114 - loglik: -8.0785e+02 - logprior: -3.4584e+00
Epoch 3/10
14/14 - 4s - loss: 805.5305 - loglik: -8.0628e+02 - logprior: 0.7447
Epoch 4/10
14/14 - 4s - loss: 803.3614 - loglik: -8.0526e+02 - logprior: 1.8995
Epoch 5/10
14/14 - 4s - loss: 804.3139 - loglik: -8.0676e+02 - logprior: 2.4457
Fitted a model with MAP estimate = -802.1014
Time for alignment: 106.2283
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 946.1615 - loglik: -9.1807e+02 - logprior: -2.8087e+01
Epoch 2/10
14/14 - 4s - loss: 874.2049 - loglik: -8.7006e+02 - logprior: -4.1413e+00
Epoch 3/10
14/14 - 4s - loss: 837.4414 - loglik: -8.3569e+02 - logprior: -1.7502e+00
Epoch 4/10
14/14 - 4s - loss: 826.0147 - loglik: -8.2461e+02 - logprior: -1.4067e+00
Epoch 5/10
14/14 - 4s - loss: 823.5621 - loglik: -8.2241e+02 - logprior: -1.1544e+00
Epoch 6/10
14/14 - 4s - loss: 820.2562 - loglik: -8.1930e+02 - logprior: -9.6047e-01
Epoch 7/10
14/14 - 4s - loss: 818.9688 - loglik: -8.1810e+02 - logprior: -8.6887e-01
Epoch 8/10
14/14 - 4s - loss: 818.5455 - loglik: -8.1759e+02 - logprior: -9.5721e-01
Epoch 9/10
14/14 - 4s - loss: 817.8004 - loglik: -8.1673e+02 - logprior: -1.0677e+00
Epoch 10/10
14/14 - 4s - loss: 821.2017 - loglik: -8.2016e+02 - logprior: -1.0437e+00
Fitted a model with MAP estimate = -818.1205
expansions: [(3, 1), (4, 1), (10, 1), (16, 4), (17, 1), (36, 3), (38, 2), (43, 1), (44, 2), (66, 1), (67, 1), (77, 1), (79, 2), (80, 3), (81, 1), (100, 1), (102, 1), (111, 3), (112, 1), (113, 2)]
discards: [ 0 45 46]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 845.6747 - loglik: -8.1645e+02 - logprior: -2.9220e+01
Epoch 2/2
14/14 - 4s - loss: 824.0618 - loglik: -8.1421e+02 - logprior: -9.8544e+00
Fitted a model with MAP estimate = -816.6063
expansions: [(0, 24)]
discards: [ 0 44 48 99]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 10s - loss: 832.5835 - loglik: -8.1078e+02 - logprior: -2.1806e+01
Epoch 2/2
14/14 - 5s - loss: 812.5800 - loglik: -8.1000e+02 - logprior: -2.5814e+00
Fitted a model with MAP estimate = -808.5760
expansions: [(43, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 34
 35]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 833.2972 - loglik: -8.0860e+02 - logprior: -2.4700e+01
Epoch 2/10
14/14 - 4s - loss: 812.6056 - loglik: -8.0968e+02 - logprior: -2.9239e+00
Epoch 3/10
14/14 - 4s - loss: 808.3174 - loglik: -8.0908e+02 - logprior: 0.7671
Epoch 4/10
14/14 - 4s - loss: 805.8563 - loglik: -8.0777e+02 - logprior: 1.9155
Epoch 5/10
14/14 - 4s - loss: 803.8334 - loglik: -8.0630e+02 - logprior: 2.4629
Epoch 6/10
14/14 - 4s - loss: 804.8400 - loglik: -8.0765e+02 - logprior: 2.8109
Fitted a model with MAP estimate = -803.5659
Time for alignment: 108.3288
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 945.0674 - loglik: -9.1699e+02 - logprior: -2.8078e+01
Epoch 2/10
14/14 - 4s - loss: 877.9849 - loglik: -8.7381e+02 - logprior: -4.1717e+00
Epoch 3/10
14/14 - 4s - loss: 840.0025 - loglik: -8.3821e+02 - logprior: -1.7920e+00
Epoch 4/10
14/14 - 4s - loss: 826.1078 - loglik: -8.2477e+02 - logprior: -1.3406e+00
Epoch 5/10
14/14 - 4s - loss: 822.8864 - loglik: -8.2170e+02 - logprior: -1.1901e+00
Epoch 6/10
14/14 - 4s - loss: 819.0790 - loglik: -8.1807e+02 - logprior: -1.0058e+00
Epoch 7/10
14/14 - 4s - loss: 823.3644 - loglik: -8.2243e+02 - logprior: -9.3352e-01
Fitted a model with MAP estimate = -819.4002
expansions: [(6, 1), (11, 1), (16, 5), (17, 1), (19, 1), (36, 1), (42, 1), (43, 1), (66, 1), (77, 1), (79, 5), (100, 1), (101, 1), (107, 1), (110, 3), (111, 1), (113, 1)]
discards: [ 0 67]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 849.6539 - loglik: -8.2056e+02 - logprior: -2.9094e+01
Epoch 2/2
14/14 - 4s - loss: 820.2490 - loglik: -8.1034e+02 - logprior: -9.9086e+00
Fitted a model with MAP estimate = -818.2493
expansions: [(0, 24), (16, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 833.6946 - loglik: -8.1179e+02 - logprior: -2.1903e+01
Epoch 2/2
14/14 - 4s - loss: 813.0912 - loglik: -8.1033e+02 - logprior: -2.7629e+00
Fitted a model with MAP estimate = -809.5010
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 835.3774 - loglik: -8.1067e+02 - logprior: -2.4703e+01
Epoch 2/10
14/14 - 4s - loss: 810.8417 - loglik: -8.0785e+02 - logprior: -2.9914e+00
Epoch 3/10
14/14 - 4s - loss: 804.8539 - loglik: -8.0556e+02 - logprior: 0.7109
Epoch 4/10
14/14 - 4s - loss: 807.3762 - loglik: -8.0929e+02 - logprior: 1.9121
Fitted a model with MAP estimate = -804.7341
Time for alignment: 88.3002
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 942.7857 - loglik: -9.1470e+02 - logprior: -2.8086e+01
Epoch 2/10
14/14 - 4s - loss: 876.4235 - loglik: -8.7233e+02 - logprior: -4.0942e+00
Epoch 3/10
14/14 - 4s - loss: 834.7578 - loglik: -8.3312e+02 - logprior: -1.6418e+00
Epoch 4/10
14/14 - 4s - loss: 824.5131 - loglik: -8.2339e+02 - logprior: -1.1194e+00
Epoch 5/10
14/14 - 4s - loss: 822.6378 - loglik: -8.2176e+02 - logprior: -8.7649e-01
Epoch 6/10
14/14 - 4s - loss: 819.9916 - loglik: -8.1943e+02 - logprior: -5.6032e-01
Epoch 7/10
14/14 - 4s - loss: 822.0721 - loglik: -8.2161e+02 - logprior: -4.6082e-01
Fitted a model with MAP estimate = -819.8251
expansions: [(12, 1), (16, 5), (17, 1), (19, 1), (36, 2), (37, 2), (66, 1), (67, 2), (77, 1), (79, 1), (80, 3), (101, 1), (102, 2), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [ 0 44]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 851.2791 - loglik: -8.2236e+02 - logprior: -2.8917e+01
Epoch 2/2
14/14 - 4s - loss: 819.8896 - loglik: -8.1014e+02 - logprior: -9.7492e+00
Fitted a model with MAP estimate = -818.1115
expansions: [(0, 22)]
discards: [ 0 12 46 78 80 81]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 836.4301 - loglik: -8.1462e+02 - logprior: -2.1813e+01
Epoch 2/2
14/14 - 5s - loss: 817.5631 - loglik: -8.1469e+02 - logprior: -2.8687e+00
Fitted a model with MAP estimate = -812.5663
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]
Re-initialized the encoder parameters.
Fitting a model of length 147 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 836.7123 - loglik: -8.1191e+02 - logprior: -2.4802e+01
Epoch 2/10
14/14 - 4s - loss: 815.3358 - loglik: -8.1227e+02 - logprior: -3.0689e+00
Epoch 3/10
14/14 - 4s - loss: 810.5021 - loglik: -8.1109e+02 - logprior: 0.5865
Epoch 4/10
14/14 - 4s - loss: 807.7394 - loglik: -8.0943e+02 - logprior: 1.6867
Epoch 5/10
14/14 - 4s - loss: 809.4935 - loglik: -8.1173e+02 - logprior: 2.2393
Fitted a model with MAP estimate = -806.9862
Time for alignment: 91.6096
Computed alignments with likelihoods: ['-803.1361', '-802.1014', '-803.5659', '-804.7341', '-806.9862']
Best model has likelihood: -802.1014  (prior= 2.6731 )
time for generating output: 0.2197
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mmp.projection.fasta
SP score = 0.9831862126944094
Training of 5 independent models on file icd.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f598459f340>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5287d48460>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531cce4640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530dbf7af0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52d815a040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f528825cdf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f86ca280>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52f99c7430>, <__main__.SimpleDirichletPrior object at 0x7f529741da00>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 23s - loss: 1802.3668 - loglik: -1.7982e+03 - logprior: -4.2055e+00
Epoch 2/10
29/29 - 20s - loss: 1626.9075 - loglik: -1.6252e+03 - logprior: -1.7436e+00
Epoch 3/10
29/29 - 20s - loss: 1595.5474 - loglik: -1.5936e+03 - logprior: -1.9850e+00
Epoch 4/10
29/29 - 20s - loss: 1588.6594 - loglik: -1.5866e+03 - logprior: -2.0825e+00
Epoch 5/10
29/29 - 20s - loss: 1584.9445 - loglik: -1.5828e+03 - logprior: -2.1555e+00
Epoch 6/10
29/29 - 20s - loss: 1585.7050 - loglik: -1.5835e+03 - logprior: -2.1883e+00
Fitted a model with MAP estimate = -1583.6942
expansions: [(16, 1), (22, 1), (24, 2), (30, 1), (31, 1), (36, 2), (41, 1), (48, 2), (49, 1), (50, 2), (77, 1), (88, 1), (89, 1), (90, 4), (97, 2), (120, 1), (121, 2), (124, 1), (125, 1), (128, 1), (142, 1), (152, 1), (153, 1), (154, 2), (162, 1), (173, 1), (184, 1), (185, 1), (186, 2), (192, 1), (205, 1), (216, 1), (217, 2), (218, 2), (219, 1), (234, 1), (248, 1), (249, 1), (251, 2), (258, 1), (260, 2), (261, 1), (262, 1), (263, 2), (264, 1), (267, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 32s - loss: 1567.8386 - loglik: -1.5624e+03 - logprior: -5.4588e+00
Epoch 2/2
29/29 - 29s - loss: 1541.3734 - loglik: -1.5391e+03 - logprior: -2.2246e+00
Fitted a model with MAP estimate = -1537.4328
expansions: [(0, 2)]
discards: [  0  25  61  62 108 109 117 327 328]
Re-initialized the encoder parameters.
Fitting a model of length 334 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 31s - loss: 1546.9954 - loglik: -1.5441e+03 - logprior: -2.9441e+00
Epoch 2/2
29/29 - 28s - loss: 1538.5455 - loglik: -1.5383e+03 - logprior: -2.8116e-01
Fitted a model with MAP estimate = -1537.3368
expansions: [(322, 2)]
discards: [  0 256 296]
Re-initialized the encoder parameters.
Fitting a model of length 333 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 35s - loss: 1546.8257 - loglik: -1.5426e+03 - logprior: -4.1853e+00
Epoch 2/10
29/29 - 28s - loss: 1540.9966 - loglik: -1.5404e+03 - logprior: -5.7709e-01
Epoch 3/10
29/29 - 28s - loss: 1538.6213 - loglik: -1.5391e+03 - logprior: 0.4988
Epoch 4/10
29/29 - 28s - loss: 1536.0480 - loglik: -1.5364e+03 - logprior: 0.3949
Epoch 5/10
29/29 - 28s - loss: 1533.3451 - loglik: -1.5341e+03 - logprior: 0.7810
Epoch 6/10
29/29 - 28s - loss: 1538.3605 - loglik: -1.5390e+03 - logprior: 0.6817
Fitted a model with MAP estimate = -1534.1752
Time for alignment: 521.7039
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 24s - loss: 1803.4120 - loglik: -1.7992e+03 - logprior: -4.2041e+00
Epoch 2/10
29/29 - 20s - loss: 1619.8563 - loglik: -1.6181e+03 - logprior: -1.7993e+00
Epoch 3/10
29/29 - 20s - loss: 1595.5426 - loglik: -1.5935e+03 - logprior: -2.0324e+00
Epoch 4/10
29/29 - 20s - loss: 1586.7206 - loglik: -1.5846e+03 - logprior: -2.1472e+00
Epoch 5/10
29/29 - 20s - loss: 1585.0717 - loglik: -1.5829e+03 - logprior: -2.2112e+00
Epoch 6/10
29/29 - 20s - loss: 1587.1340 - loglik: -1.5849e+03 - logprior: -2.2522e+00
Fitted a model with MAP estimate = -1585.5004
expansions: [(16, 1), (22, 1), (24, 2), (28, 1), (29, 1), (30, 1), (31, 1), (37, 1), (39, 1), (46, 1), (48, 2), (49, 1), (50, 2), (64, 1), (73, 1), (87, 2), (88, 2), (89, 1), (90, 2), (121, 2), (122, 1), (123, 1), (125, 2), (142, 2), (149, 1), (151, 1), (154, 1), (155, 1), (172, 1), (173, 1), (184, 1), (185, 1), (186, 2), (190, 1), (191, 1), (204, 1), (217, 2), (218, 2), (219, 1), (233, 1), (248, 1), (249, 1), (251, 2), (258, 1), (260, 2), (261, 1), (262, 1), (263, 2), (264, 1), (267, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 344 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 33s - loss: 1568.4969 - loglik: -1.5631e+03 - logprior: -5.3961e+00
Epoch 2/2
29/29 - 29s - loss: 1543.1721 - loglik: -1.5411e+03 - logprior: -2.0828e+00
Fitted a model with MAP estimate = -1538.0115
expansions: [(0, 2), (334, 2)]
discards: [  0  40  63  64 107 112 113 145 172 265 330 331]
Re-initialized the encoder parameters.
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 33s - loss: 1548.2477 - loglik: -1.5453e+03 - logprior: -2.9032e+00
Epoch 2/2
29/29 - 28s - loss: 1534.3094 - loglik: -1.5342e+03 - logprior: -1.0044e-01
Fitted a model with MAP estimate = -1539.1891
expansions: [(140, 1), (322, 2)]
discards: [  0 296 324]
Re-initialized the encoder parameters.
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 32s - loss: 1549.2202 - loglik: -1.5450e+03 - logprior: -4.2221e+00
Epoch 2/10
29/29 - 28s - loss: 1538.0029 - loglik: -1.5377e+03 - logprior: -3.5060e-01
Epoch 3/10
29/29 - 28s - loss: 1540.4403 - loglik: -1.5408e+03 - logprior: 0.3492
Fitted a model with MAP estimate = -1536.7302
Time for alignment: 441.0551
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 24s - loss: 1804.0750 - loglik: -1.7999e+03 - logprior: -4.2127e+00
Epoch 2/10
29/29 - 20s - loss: 1618.2980 - loglik: -1.6164e+03 - logprior: -1.8810e+00
Epoch 3/10
29/29 - 20s - loss: 1593.7003 - loglik: -1.5916e+03 - logprior: -2.1406e+00
Epoch 4/10
29/29 - 20s - loss: 1587.5432 - loglik: -1.5854e+03 - logprior: -2.1684e+00
Epoch 5/10
29/29 - 20s - loss: 1582.6394 - loglik: -1.5805e+03 - logprior: -2.1744e+00
Epoch 6/10
29/29 - 20s - loss: 1584.8824 - loglik: -1.5827e+03 - logprior: -2.2065e+00
Fitted a model with MAP estimate = -1583.3608
expansions: [(16, 1), (22, 1), (24, 2), (28, 1), (29, 1), (30, 1), (31, 1), (36, 1), (42, 1), (48, 2), (49, 1), (50, 1), (64, 1), (72, 1), (87, 1), (88, 1), (89, 1), (121, 3), (123, 1), (124, 1), (128, 1), (142, 1), (151, 1), (153, 1), (155, 1), (163, 1), (172, 1), (173, 1), (184, 1), (185, 1), (186, 1), (191, 1), (192, 1), (205, 1), (216, 2), (218, 2), (219, 1), (233, 1), (240, 1), (248, 1), (249, 1), (251, 2), (258, 1), (260, 2), (261, 1), (262, 1), (263, 2), (264, 1), (267, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 337 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 34s - loss: 1568.0587 - loglik: -1.5628e+03 - logprior: -5.2667e+00
Epoch 2/2
29/29 - 28s - loss: 1543.0504 - loglik: -1.5410e+03 - logprior: -2.0407e+00
Fitted a model with MAP estimate = -1540.2252
expansions: [(0, 2)]
discards: [  0  58 140 254 258 323 324]
Re-initialized the encoder parameters.
Fitting a model of length 332 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 32s - loss: 1545.3401 - loglik: -1.5428e+03 - logprior: -2.5627e+00
Epoch 2/2
29/29 - 28s - loss: 1540.5773 - loglik: -1.5407e+03 - logprior: 0.0776
Fitted a model with MAP estimate = -1538.5616
expansions: [(43, 1), (164, 1), (320, 2)]
discards: [  0 294]
Re-initialized the encoder parameters.
Fitting a model of length 334 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 31s - loss: 1546.9626 - loglik: -1.5426e+03 - logprior: -4.3978e+00
Epoch 2/10
29/29 - 28s - loss: 1541.3311 - loglik: -1.5409e+03 - logprior: -4.1232e-01
Epoch 3/10
29/29 - 28s - loss: 1535.9000 - loglik: -1.5363e+03 - logprior: 0.3685
Epoch 4/10
29/29 - 28s - loss: 1535.1401 - loglik: -1.5358e+03 - logprior: 0.6230
Epoch 5/10
29/29 - 28s - loss: 1529.7853 - loglik: -1.5305e+03 - logprior: 0.7420
Epoch 6/10
29/29 - 28s - loss: 1542.8201 - loglik: -1.5437e+03 - logprior: 0.8747
Fitted a model with MAP estimate = -1534.2512
Time for alignment: 521.5570
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 23s - loss: 1802.5308 - loglik: -1.7983e+03 - logprior: -4.2041e+00
Epoch 2/10
29/29 - 20s - loss: 1621.3898 - loglik: -1.6196e+03 - logprior: -1.7955e+00
Epoch 3/10
29/29 - 20s - loss: 1593.1199 - loglik: -1.5912e+03 - logprior: -1.9351e+00
Epoch 4/10
29/29 - 20s - loss: 1586.7305 - loglik: -1.5847e+03 - logprior: -2.0080e+00
Epoch 5/10
29/29 - 20s - loss: 1586.9025 - loglik: -1.5848e+03 - logprior: -2.0825e+00
Fitted a model with MAP estimate = -1585.1779
expansions: [(16, 1), (22, 1), (24, 2), (28, 1), (29, 1), (30, 1), (37, 1), (39, 1), (46, 1), (48, 2), (49, 2), (76, 1), (87, 1), (120, 2), (121, 1), (123, 1), (124, 1), (128, 1), (142, 1), (151, 1), (153, 1), (155, 1), (163, 1), (172, 1), (173, 1), (184, 1), (185, 1), (186, 2), (192, 1), (204, 1), (205, 1), (217, 2), (218, 2), (219, 1), (232, 1), (248, 1), (249, 1), (251, 2), (258, 1), (260, 2), (261, 1), (262, 1), (263, 2), (264, 1), (267, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 334 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 34s - loss: 1572.7286 - loglik: -1.5674e+03 - logprior: -5.2926e+00
Epoch 2/2
29/29 - 28s - loss: 1545.8983 - loglik: -1.5440e+03 - logprior: -1.9302e+00
Fitted a model with MAP estimate = -1544.3213
expansions: [(0, 2), (61, 1)]
discards: [  0 136 255 294 320 321]
Re-initialized the encoder parameters.
Fitting a model of length 331 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 31s - loss: 1550.3323 - loglik: -1.5476e+03 - logprior: -2.6966e+00
Epoch 2/2
29/29 - 28s - loss: 1543.2698 - loglik: -1.5432e+03 - logprior: -7.7129e-02
Fitted a model with MAP estimate = -1542.2882
expansions: [(319, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 332 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 31s - loss: 1550.7552 - loglik: -1.5463e+03 - logprior: -4.4823e+00
Epoch 2/10
29/29 - 28s - loss: 1543.1042 - loglik: -1.5426e+03 - logprior: -4.9597e-01
Epoch 3/10
29/29 - 28s - loss: 1544.1001 - loglik: -1.5443e+03 - logprior: 0.1584
Fitted a model with MAP estimate = -1540.4812
Time for alignment: 415.5669
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 23s - loss: 1800.9471 - loglik: -1.7967e+03 - logprior: -4.2002e+00
Epoch 2/10
29/29 - 20s - loss: 1625.5813 - loglik: -1.6237e+03 - logprior: -1.8407e+00
Epoch 3/10
29/29 - 20s - loss: 1596.3043 - loglik: -1.5943e+03 - logprior: -2.0072e+00
Epoch 4/10
29/29 - 20s - loss: 1592.3916 - loglik: -1.5904e+03 - logprior: -1.9857e+00
Epoch 5/10
29/29 - 20s - loss: 1592.5680 - loglik: -1.5905e+03 - logprior: -2.0549e+00
Fitted a model with MAP estimate = -1590.3792
expansions: [(16, 1), (22, 2), (24, 2), (27, 1), (28, 1), (29, 1), (30, 2), (31, 1), (39, 1), (49, 3), (64, 1), (65, 1), (76, 1), (87, 1), (90, 2), (121, 1), (123, 1), (124, 1), (125, 1), (128, 1), (142, 2), (147, 1), (152, 1), (153, 1), (154, 2), (163, 1), (172, 1), (173, 1), (184, 1), (185, 1), (186, 2), (192, 1), (204, 1), (205, 1), (217, 2), (218, 2), (219, 1), (232, 1), (248, 1), (249, 1), (251, 2), (258, 1), (260, 2), (261, 1), (262, 1), (263, 2), (264, 1), (267, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 34s - loss: 1572.4482 - loglik: -1.5671e+03 - logprior: -5.3930e+00
Epoch 2/2
29/29 - 29s - loss: 1552.3882 - loglik: -1.5504e+03 - logprior: -2.0218e+00
Fitted a model with MAP estimate = -1544.5527
expansions: [(0, 2), (63, 1)]
discards: [  0  23  26  37  60  61  78 167 184 262 301 327 328]
Re-initialized the encoder parameters.
Fitting a model of length 331 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 31s - loss: 1552.9033 - loglik: -1.5503e+03 - logprior: -2.6149e+00
Epoch 2/2
29/29 - 27s - loss: 1551.3450 - loglik: -1.5513e+03 - logprior: -5.0837e-02
Fitted a model with MAP estimate = -1545.8556
expansions: [(319, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 332 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 31s - loss: 1553.7504 - loglik: -1.5494e+03 - logprior: -4.3583e+00
Epoch 2/10
29/29 - 28s - loss: 1547.4998 - loglik: -1.5471e+03 - logprior: -3.9339e-01
Epoch 3/10
29/29 - 28s - loss: 1546.4100 - loglik: -1.5466e+03 - logprior: 0.1784
Epoch 4/10
29/29 - 28s - loss: 1541.7277 - loglik: -1.5423e+03 - logprior: 0.5558
Epoch 5/10
29/29 - 28s - loss: 1543.6654 - loglik: -1.5446e+03 - logprior: 0.8900
Fitted a model with MAP estimate = -1542.6473
Time for alignment: 471.9729
Computed alignments with likelihoods: ['-1534.1752', '-1536.7302', '-1534.2512', '-1540.4812', '-1542.6473']
Best model has likelihood: -1534.1752  (prior= 0.9845 )
time for generating output: 0.4146
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/icd.projection.fasta
SP score = 0.89232995658466
Training of 5 independent models on file seatoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5301f69970>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f528827d070>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5afe733160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5315cf6c10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f59840c8550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b07b8fe50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530cc08c10>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5b0830bdc0>, <__main__.SimpleDirichletPrior object at 0x7f52f98931f0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 779.9091 - loglik: -2.6618e+02 - logprior: -5.1373e+02
Epoch 2/10
10/10 - 0s - loss: 384.7927 - loglik: -2.4437e+02 - logprior: -1.4043e+02
Epoch 3/10
10/10 - 0s - loss: 289.7221 - loglik: -2.2554e+02 - logprior: -6.4183e+01
Epoch 4/10
10/10 - 0s - loss: 250.2419 - loglik: -2.1463e+02 - logprior: -3.5615e+01
Epoch 5/10
10/10 - 0s - loss: 231.2988 - loglik: -2.1099e+02 - logprior: -2.0306e+01
Epoch 6/10
10/10 - 0s - loss: 220.6916 - loglik: -2.1012e+02 - logprior: -1.0573e+01
Epoch 7/10
10/10 - 0s - loss: 214.1505 - loglik: -2.0961e+02 - logprior: -4.5419e+00
Epoch 8/10
10/10 - 1s - loss: 210.3128 - loglik: -2.0959e+02 - logprior: -7.2103e-01
Epoch 9/10
10/10 - 1s - loss: 207.8326 - loglik: -2.0982e+02 - logprior: 1.9833
Epoch 10/10
10/10 - 0s - loss: 206.0536 - loglik: -2.0992e+02 - logprior: 3.8666
Fitted a model with MAP estimate = -205.2572
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 861.7479 - loglik: -2.0210e+02 - logprior: -6.5965e+02
Epoch 2/2
10/10 - 0s - loss: 398.3863 - loglik: -1.9342e+02 - logprior: -2.0497e+02
Fitted a model with MAP estimate = -310.4798
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 643.7061 - loglik: -1.8797e+02 - logprior: -4.5573e+02
Epoch 2/2
10/10 - 0s - loss: 308.7632 - loglik: -1.8757e+02 - logprior: -1.2120e+02
Fitted a model with MAP estimate = -258.7951
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 614.6385 - loglik: -1.8689e+02 - logprior: -4.2775e+02
Epoch 2/10
10/10 - 0s - loss: 301.1808 - loglik: -1.8750e+02 - logprior: -1.1368e+02
Epoch 3/10
10/10 - 1s - loss: 235.2276 - loglik: -1.8820e+02 - logprior: -4.7028e+01
Epoch 4/10
10/10 - 0s - loss: 207.1993 - loglik: -1.8874e+02 - logprior: -1.8464e+01
Epoch 5/10
10/10 - 0s - loss: 191.7708 - loglik: -1.8924e+02 - logprior: -2.5351e+00
Epoch 6/10
10/10 - 1s - loss: 182.7329 - loglik: -1.8964e+02 - logprior: 6.9100
Epoch 7/10
10/10 - 0s - loss: 177.0821 - loglik: -1.8994e+02 - logprior: 12.8607
Epoch 8/10
10/10 - 0s - loss: 173.2228 - loglik: -1.9017e+02 - logprior: 16.9497
Epoch 9/10
10/10 - 1s - loss: 170.3410 - loglik: -1.9036e+02 - logprior: 20.0155
Epoch 10/10
10/10 - 0s - loss: 168.0220 - loglik: -1.9051e+02 - logprior: 22.4840
Fitted a model with MAP estimate = -166.8925
Time for alignment: 29.0322
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 779.9091 - loglik: -2.6618e+02 - logprior: -5.1373e+02
Epoch 2/10
10/10 - 0s - loss: 384.7927 - loglik: -2.4437e+02 - logprior: -1.4043e+02
Epoch 3/10
10/10 - 1s - loss: 289.7221 - loglik: -2.2554e+02 - logprior: -6.4183e+01
Epoch 4/10
10/10 - 0s - loss: 250.2419 - loglik: -2.1463e+02 - logprior: -3.5615e+01
Epoch 5/10
10/10 - 0s - loss: 231.2988 - loglik: -2.1099e+02 - logprior: -2.0306e+01
Epoch 6/10
10/10 - 0s - loss: 220.6915 - loglik: -2.1012e+02 - logprior: -1.0573e+01
Epoch 7/10
10/10 - 1s - loss: 214.1505 - loglik: -2.0961e+02 - logprior: -4.5419e+00
Epoch 8/10
10/10 - 0s - loss: 210.3128 - loglik: -2.0959e+02 - logprior: -7.2103e-01
Epoch 9/10
10/10 - 0s - loss: 207.8326 - loglik: -2.0982e+02 - logprior: 1.9833
Epoch 10/10
10/10 - 0s - loss: 206.0536 - loglik: -2.0992e+02 - logprior: 3.8666
Fitted a model with MAP estimate = -205.2572
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 861.7479 - loglik: -2.0210e+02 - logprior: -6.5965e+02
Epoch 2/2
10/10 - 0s - loss: 398.3863 - loglik: -1.9342e+02 - logprior: -2.0497e+02
Fitted a model with MAP estimate = -310.4798
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 643.7061 - loglik: -1.8797e+02 - logprior: -4.5573e+02
Epoch 2/2
10/10 - 0s - loss: 308.7633 - loglik: -1.8757e+02 - logprior: -1.2120e+02
Fitted a model with MAP estimate = -258.7953
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 614.6388 - loglik: -1.8689e+02 - logprior: -4.2775e+02
Epoch 2/10
10/10 - 0s - loss: 301.1815 - loglik: -1.8750e+02 - logprior: -1.1368e+02
Epoch 3/10
10/10 - 0s - loss: 235.2287 - loglik: -1.8820e+02 - logprior: -4.7029e+01
Epoch 4/10
10/10 - 1s - loss: 207.2007 - loglik: -1.8874e+02 - logprior: -1.8465e+01
Epoch 5/10
10/10 - 0s - loss: 191.7724 - loglik: -1.8924e+02 - logprior: -2.5366e+00
Epoch 6/10
10/10 - 1s - loss: 182.7347 - loglik: -1.8964e+02 - logprior: 6.9083
Epoch 7/10
10/10 - 0s - loss: 177.0841 - loglik: -1.8994e+02 - logprior: 12.8589
Epoch 8/10
10/10 - 1s - loss: 173.2249 - loglik: -1.9017e+02 - logprior: 16.9477
Epoch 9/10
10/10 - 1s - loss: 170.3433 - loglik: -1.9036e+02 - logprior: 20.0134
Epoch 10/10
10/10 - 1s - loss: 168.0244 - loglik: -1.9051e+02 - logprior: 22.4818
Fitted a model with MAP estimate = -166.8948
Time for alignment: 28.1734
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 779.9091 - loglik: -2.6618e+02 - logprior: -5.1373e+02
Epoch 2/10
10/10 - 0s - loss: 384.7927 - loglik: -2.4437e+02 - logprior: -1.4043e+02
Epoch 3/10
10/10 - 0s - loss: 289.7221 - loglik: -2.2554e+02 - logprior: -6.4183e+01
Epoch 4/10
10/10 - 0s - loss: 250.2419 - loglik: -2.1463e+02 - logprior: -3.5615e+01
Epoch 5/10
10/10 - 0s - loss: 231.2988 - loglik: -2.1099e+02 - logprior: -2.0306e+01
Epoch 6/10
10/10 - 0s - loss: 220.6915 - loglik: -2.1012e+02 - logprior: -1.0573e+01
Epoch 7/10
10/10 - 0s - loss: 214.1505 - loglik: -2.0961e+02 - logprior: -4.5419e+00
Epoch 8/10
10/10 - 0s - loss: 210.3129 - loglik: -2.0959e+02 - logprior: -7.2103e-01
Epoch 9/10
10/10 - 0s - loss: 207.8326 - loglik: -2.0982e+02 - logprior: 1.9833
Epoch 10/10
10/10 - 0s - loss: 206.0536 - loglik: -2.0992e+02 - logprior: 3.8666
Fitted a model with MAP estimate = -205.2572
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 861.7479 - loglik: -2.0210e+02 - logprior: -6.5965e+02
Epoch 2/2
10/10 - 0s - loss: 398.3863 - loglik: -1.9342e+02 - logprior: -2.0497e+02
Fitted a model with MAP estimate = -310.4798
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 643.7061 - loglik: -1.8797e+02 - logprior: -4.5573e+02
Epoch 2/2
10/10 - 1s - loss: 308.7632 - loglik: -1.8757e+02 - logprior: -1.2120e+02
Fitted a model with MAP estimate = -258.7952
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 614.6385 - loglik: -1.8689e+02 - logprior: -4.2775e+02
Epoch 2/10
10/10 - 0s - loss: 301.1810 - loglik: -1.8750e+02 - logprior: -1.1368e+02
Epoch 3/10
10/10 - 0s - loss: 235.2279 - loglik: -1.8820e+02 - logprior: -4.7029e+01
Epoch 4/10
10/10 - 0s - loss: 207.1997 - loglik: -1.8874e+02 - logprior: -1.8464e+01
Epoch 5/10
10/10 - 0s - loss: 191.7712 - loglik: -1.8924e+02 - logprior: -2.5355e+00
Epoch 6/10
10/10 - 1s - loss: 182.7334 - loglik: -1.8964e+02 - logprior: 6.9095
Epoch 7/10
10/10 - 0s - loss: 177.0827 - loglik: -1.8994e+02 - logprior: 12.8603
Epoch 8/10
10/10 - 0s - loss: 173.2235 - loglik: -1.9017e+02 - logprior: 16.9492
Epoch 9/10
10/10 - 1s - loss: 170.3417 - loglik: -1.9036e+02 - logprior: 20.0150
Epoch 10/10
10/10 - 0s - loss: 168.0227 - loglik: -1.9051e+02 - logprior: 22.4835
Fitted a model with MAP estimate = -166.8931
Time for alignment: 27.6856
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 779.9091 - loglik: -2.6618e+02 - logprior: -5.1373e+02
Epoch 2/10
10/10 - 0s - loss: 384.7927 - loglik: -2.4437e+02 - logprior: -1.4043e+02
Epoch 3/10
10/10 - 0s - loss: 289.7221 - loglik: -2.2554e+02 - logprior: -6.4183e+01
Epoch 4/10
10/10 - 0s - loss: 250.2419 - loglik: -2.1463e+02 - logprior: -3.5615e+01
Epoch 5/10
10/10 - 0s - loss: 231.2988 - loglik: -2.1099e+02 - logprior: -2.0306e+01
Epoch 6/10
10/10 - 0s - loss: 220.6915 - loglik: -2.1012e+02 - logprior: -1.0573e+01
Epoch 7/10
10/10 - 0s - loss: 214.1505 - loglik: -2.0961e+02 - logprior: -4.5419e+00
Epoch 8/10
10/10 - 0s - loss: 210.3128 - loglik: -2.0959e+02 - logprior: -7.2103e-01
Epoch 9/10
10/10 - 0s - loss: 207.8326 - loglik: -2.0982e+02 - logprior: 1.9833
Epoch 10/10
10/10 - 0s - loss: 206.0536 - loglik: -2.0992e+02 - logprior: 3.8666
Fitted a model with MAP estimate = -205.2573
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 861.7479 - loglik: -2.0210e+02 - logprior: -6.5965e+02
Epoch 2/2
10/10 - 0s - loss: 398.3863 - loglik: -1.9342e+02 - logprior: -2.0497e+02
Fitted a model with MAP estimate = -310.4798
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 643.7061 - loglik: -1.8797e+02 - logprior: -4.5573e+02
Epoch 2/2
10/10 - 0s - loss: 308.7632 - loglik: -1.8757e+02 - logprior: -1.2120e+02
Fitted a model with MAP estimate = -258.7952
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 614.6385 - loglik: -1.8689e+02 - logprior: -4.2775e+02
Epoch 2/10
10/10 - 1s - loss: 301.1811 - loglik: -1.8750e+02 - logprior: -1.1368e+02
Epoch 3/10
10/10 - 0s - loss: 235.2281 - loglik: -1.8820e+02 - logprior: -4.7029e+01
Epoch 4/10
10/10 - 0s - loss: 207.1999 - loglik: -1.8874e+02 - logprior: -1.8464e+01
Epoch 5/10
10/10 - 0s - loss: 191.7715 - loglik: -1.8924e+02 - logprior: -2.5357e+00
Epoch 6/10
10/10 - 0s - loss: 182.7337 - loglik: -1.8964e+02 - logprior: 6.9093
Epoch 7/10
10/10 - 0s - loss: 177.0830 - loglik: -1.8994e+02 - logprior: 12.8600
Epoch 8/10
10/10 - 0s - loss: 173.2237 - loglik: -1.9017e+02 - logprior: 16.9489
Epoch 9/10
10/10 - 0s - loss: 170.3420 - loglik: -1.9036e+02 - logprior: 20.0146
Epoch 10/10
10/10 - 1s - loss: 168.0230 - loglik: -1.9051e+02 - logprior: 22.4831
Fitted a model with MAP estimate = -166.8936
Time for alignment: 27.4523
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 779.9091 - loglik: -2.6618e+02 - logprior: -5.1373e+02
Epoch 2/10
10/10 - 0s - loss: 384.7927 - loglik: -2.4437e+02 - logprior: -1.4043e+02
Epoch 3/10
10/10 - 0s - loss: 289.7221 - loglik: -2.2554e+02 - logprior: -6.4183e+01
Epoch 4/10
10/10 - 0s - loss: 250.2419 - loglik: -2.1463e+02 - logprior: -3.5615e+01
Epoch 5/10
10/10 - 0s - loss: 231.2988 - loglik: -2.1099e+02 - logprior: -2.0306e+01
Epoch 6/10
10/10 - 0s - loss: 220.6915 - loglik: -2.1012e+02 - logprior: -1.0573e+01
Epoch 7/10
10/10 - 0s - loss: 214.1505 - loglik: -2.0961e+02 - logprior: -4.5419e+00
Epoch 8/10
10/10 - 0s - loss: 210.3128 - loglik: -2.0959e+02 - logprior: -7.2103e-01
Epoch 9/10
10/10 - 0s - loss: 207.8326 - loglik: -2.0982e+02 - logprior: 1.9833
Epoch 10/10
10/10 - 0s - loss: 206.0536 - loglik: -2.0992e+02 - logprior: 3.8666
Fitted a model with MAP estimate = -205.2573
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 861.7479 - loglik: -2.0210e+02 - logprior: -6.5965e+02
Epoch 2/2
10/10 - 1s - loss: 398.3863 - loglik: -1.9342e+02 - logprior: -2.0497e+02
Fitted a model with MAP estimate = -310.4798
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 643.7059 - loglik: -1.8797e+02 - logprior: -4.5573e+02
Epoch 2/2
10/10 - 0s - loss: 308.7631 - loglik: -1.8757e+02 - logprior: -1.2120e+02
Fitted a model with MAP estimate = -258.7950
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 614.6384 - loglik: -1.8689e+02 - logprior: -4.2775e+02
Epoch 2/10
10/10 - 0s - loss: 301.1808 - loglik: -1.8750e+02 - logprior: -1.1368e+02
Epoch 3/10
10/10 - 0s - loss: 235.2280 - loglik: -1.8820e+02 - logprior: -4.7029e+01
Epoch 4/10
10/10 - 1s - loss: 207.2000 - loglik: -1.8874e+02 - logprior: -1.8464e+01
Epoch 5/10
10/10 - 0s - loss: 191.7718 - loglik: -1.8924e+02 - logprior: -2.5362e+00
Epoch 6/10
10/10 - 0s - loss: 182.7342 - loglik: -1.8964e+02 - logprior: 6.9087
Epoch 7/10
10/10 - 0s - loss: 177.0836 - loglik: -1.8994e+02 - logprior: 12.8593
Epoch 8/10
10/10 - 0s - loss: 173.2245 - loglik: -1.9017e+02 - logprior: 16.9481
Epoch 9/10
10/10 - 0s - loss: 170.3429 - loglik: -1.9036e+02 - logprior: 20.0137
Epoch 10/10
10/10 - 0s - loss: 168.0240 - loglik: -1.9051e+02 - logprior: 22.4821
Fitted a model with MAP estimate = -166.8945
Time for alignment: 26.9747
Computed alignments with likelihoods: ['-166.8925', '-166.8948', '-166.8931', '-166.8936', '-166.8945']
Best model has likelihood: -166.8925  (prior= 23.6866 )
time for generating output: 0.1151
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/seatoxin.projection.fasta
SP score = 0.7268518518518519
Training of 5 independent models on file int.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5284fd82b0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5900edb310>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5300fb6a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5286c511f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5297972d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531dbe2130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5297a14f70>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5296e878b0>, <__main__.SimpleDirichletPrior object at 0x7f5284d19ee0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 957.9092 - loglik: -9.5283e+02 - logprior: -5.0773e+00
Epoch 2/10
16/16 - 5s - loss: 921.1600 - loglik: -9.2006e+02 - logprior: -1.0989e+00
Epoch 3/10
16/16 - 5s - loss: 896.5143 - loglik: -8.9524e+02 - logprior: -1.2727e+00
Epoch 4/10
16/16 - 5s - loss: 890.5885 - loglik: -8.8934e+02 - logprior: -1.2507e+00
Epoch 5/10
16/16 - 5s - loss: 885.6491 - loglik: -8.8432e+02 - logprior: -1.3337e+00
Epoch 6/10
16/16 - 5s - loss: 885.4457 - loglik: -8.8397e+02 - logprior: -1.4774e+00
Epoch 7/10
16/16 - 5s - loss: 879.4943 - loglik: -8.7792e+02 - logprior: -1.5728e+00
Epoch 8/10
16/16 - 5s - loss: 882.6574 - loglik: -8.8099e+02 - logprior: -1.6683e+00
Fitted a model with MAP estimate = -880.5685
expansions: [(14, 1), (15, 1), (17, 3), (28, 2), (30, 1), (41, 1), (45, 1), (48, 3), (50, 1), (57, 1), (58, 1), (72, 1), (73, 2), (74, 2), (75, 2), (94, 4), (95, 2), (99, 1), (102, 1), (116, 1), (120, 1), (123, 1), (126, 1), (128, 1), (129, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 890.1716 - loglik: -8.8616e+02 - logprior: -4.0160e+00
Epoch 2/2
33/33 - 8s - loss: 876.4760 - loglik: -8.7540e+02 - logprior: -1.0796e+00
Fitted a model with MAP estimate = -874.8657
expansions: [(178, 2)]
discards: [  0  19  33  34  58 117 121 175 176 177]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 11s - loss: 884.8713 - loglik: -8.8129e+02 - logprior: -3.5824e+00
Epoch 2/2
33/33 - 8s - loss: 879.0470 - loglik: -8.7826e+02 - logprior: -7.8455e-01
Fitted a model with MAP estimate = -876.6135
expansions: [(0, 1), (170, 2)]
discards: [168 169]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 14s - loss: 880.3691 - loglik: -8.7794e+02 - logprior: -2.4307e+00
Epoch 2/10
33/33 - 8s - loss: 876.2431 - loglik: -8.7576e+02 - logprior: -4.8548e-01
Epoch 3/10
33/33 - 8s - loss: 875.0306 - loglik: -8.7468e+02 - logprior: -3.5540e-01
Epoch 4/10
33/33 - 8s - loss: 871.2300 - loglik: -8.7086e+02 - logprior: -3.7416e-01
Epoch 5/10
33/33 - 8s - loss: 870.9892 - loglik: -8.7055e+02 - logprior: -4.3777e-01
Epoch 6/10
33/33 - 8s - loss: 867.2607 - loglik: -8.6676e+02 - logprior: -4.9902e-01
Epoch 7/10
33/33 - 8s - loss: 867.6129 - loglik: -8.6706e+02 - logprior: -5.5699e-01
Fitted a model with MAP estimate = -866.1824
Time for alignment: 177.5342
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 9s - loss: 958.6801 - loglik: -9.5358e+02 - logprior: -5.0995e+00
Epoch 2/10
16/16 - 5s - loss: 922.0402 - loglik: -9.2092e+02 - logprior: -1.1169e+00
Epoch 3/10
16/16 - 5s - loss: 898.2729 - loglik: -8.9698e+02 - logprior: -1.2960e+00
Epoch 4/10
16/16 - 5s - loss: 889.3303 - loglik: -8.8803e+02 - logprior: -1.2994e+00
Epoch 5/10
16/16 - 5s - loss: 884.6449 - loglik: -8.8333e+02 - logprior: -1.3198e+00
Epoch 6/10
16/16 - 5s - loss: 884.8127 - loglik: -8.8336e+02 - logprior: -1.4546e+00
Fitted a model with MAP estimate = -882.6013
expansions: [(13, 1), (14, 1), (23, 3), (28, 3), (42, 1), (44, 1), (48, 2), (50, 1), (56, 1), (58, 3), (69, 1), (71, 1), (72, 2), (73, 2), (94, 2), (95, 4), (99, 1), (102, 1), (116, 1), (117, 2), (122, 2), (125, 1), (129, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 887.7783 - loglik: -8.8379e+02 - logprior: -3.9925e+00
Epoch 2/2
33/33 - 8s - loss: 877.0226 - loglik: -8.7596e+02 - logprior: -1.0593e+00
Fitted a model with MAP estimate = -874.7883
expansions: [(36, 1), (94, 1), (179, 2)]
discards: [  0  25  33  34 117 118 148 176 177 178]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 11s - loss: 884.1041 - loglik: -8.8049e+02 - logprior: -3.6133e+00
Epoch 2/2
33/33 - 8s - loss: 879.0228 - loglik: -8.7825e+02 - logprior: -7.7148e-01
Fitted a model with MAP estimate = -876.4971
expansions: [(0, 1), (173, 2)]
discards: [171 172]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 13s - loss: 880.7394 - loglik: -8.7829e+02 - logprior: -2.4490e+00
Epoch 2/10
33/33 - 8s - loss: 874.9827 - loglik: -8.7449e+02 - logprior: -4.9248e-01
Epoch 3/10
33/33 - 8s - loss: 876.4931 - loglik: -8.7614e+02 - logprior: -3.5364e-01
Fitted a model with MAP estimate = -872.5405
Time for alignment: 139.0902
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 958.6261 - loglik: -9.5355e+02 - logprior: -5.0793e+00
Epoch 2/10
16/16 - 5s - loss: 922.1516 - loglik: -9.2106e+02 - logprior: -1.0944e+00
Epoch 3/10
16/16 - 5s - loss: 897.9471 - loglik: -8.9668e+02 - logprior: -1.2700e+00
Epoch 4/10
16/16 - 5s - loss: 888.5342 - loglik: -8.8718e+02 - logprior: -1.3532e+00
Epoch 5/10
16/16 - 5s - loss: 886.1030 - loglik: -8.8477e+02 - logprior: -1.3331e+00
Epoch 6/10
16/16 - 5s - loss: 884.3781 - loglik: -8.8289e+02 - logprior: -1.4928e+00
Epoch 7/10
16/16 - 5s - loss: 882.0953 - loglik: -8.8054e+02 - logprior: -1.5566e+00
Epoch 8/10
16/16 - 5s - loss: 880.1886 - loglik: -8.7855e+02 - logprior: -1.6424e+00
Epoch 9/10
16/16 - 5s - loss: 880.8080 - loglik: -8.7911e+02 - logprior: -1.6986e+00
Fitted a model with MAP estimate = -880.3617
expansions: [(13, 1), (14, 1), (24, 1), (27, 1), (28, 2), (30, 1), (41, 1), (45, 1), (49, 2), (50, 1), (53, 1), (56, 1), (58, 1), (69, 1), (72, 2), (74, 1), (75, 2), (80, 1), (94, 6), (102, 1), (116, 3), (119, 1), (122, 1), (125, 1), (129, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 890.2697 - loglik: -8.8625e+02 - logprior: -4.0225e+00
Epoch 2/2
33/33 - 8s - loss: 877.6108 - loglik: -8.7654e+02 - logprior: -1.0729e+00
Fitted a model with MAP estimate = -875.2908
expansions: [(177, 2)]
discards: [  0  31  32  95 116 117 146 174 175 176]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 11s - loss: 885.7970 - loglik: -8.8225e+02 - logprior: -3.5507e+00
Epoch 2/2
33/33 - 8s - loss: 879.6542 - loglik: -8.7892e+02 - logprior: -7.3833e-01
Fitted a model with MAP estimate = -877.1521
expansions: [(0, 1), (30, 2), (112, 2), (169, 2)]
discards: [167 168]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 13s - loss: 879.9322 - loglik: -8.7756e+02 - logprior: -2.3761e+00
Epoch 2/10
33/33 - 8s - loss: 875.4740 - loglik: -8.7505e+02 - logprior: -4.2165e-01
Epoch 3/10
33/33 - 8s - loss: 873.6816 - loglik: -8.7338e+02 - logprior: -2.9760e-01
Epoch 4/10
33/33 - 8s - loss: 872.8509 - loglik: -8.7253e+02 - logprior: -3.2272e-01
Epoch 5/10
33/33 - 8s - loss: 870.3924 - loglik: -8.7001e+02 - logprior: -3.8053e-01
Epoch 6/10
33/33 - 8s - loss: 865.6713 - loglik: -8.6522e+02 - logprior: -4.5600e-01
Epoch 7/10
33/33 - 8s - loss: 868.2950 - loglik: -8.6779e+02 - logprior: -5.0286e-01
Fitted a model with MAP estimate = -865.6302
Time for alignment: 181.6003
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 958.4186 - loglik: -9.5334e+02 - logprior: -5.0780e+00
Epoch 2/10
16/16 - 5s - loss: 921.0724 - loglik: -9.1998e+02 - logprior: -1.0930e+00
Epoch 3/10
16/16 - 5s - loss: 899.7473 - loglik: -8.9848e+02 - logprior: -1.2636e+00
Epoch 4/10
16/16 - 5s - loss: 887.5898 - loglik: -8.8627e+02 - logprior: -1.3189e+00
Epoch 5/10
16/16 - 5s - loss: 884.5716 - loglik: -8.8332e+02 - logprior: -1.2482e+00
Epoch 6/10
16/16 - 5s - loss: 884.2277 - loglik: -8.8289e+02 - logprior: -1.3383e+00
Epoch 7/10
16/16 - 5s - loss: 879.9786 - loglik: -8.7859e+02 - logprior: -1.3864e+00
Epoch 8/10
16/16 - 5s - loss: 880.5595 - loglik: -8.7907e+02 - logprior: -1.4881e+00
Fitted a model with MAP estimate = -879.5369
expansions: [(13, 1), (14, 1), (17, 1), (23, 1), (28, 3), (29, 1), (42, 1), (44, 1), (47, 3), (49, 2), (56, 1), (57, 1), (73, 2), (74, 3), (95, 4), (96, 1), (102, 1), (116, 3), (119, 1), (122, 1), (125, 1), (139, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 176 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 888.6572 - loglik: -8.8467e+02 - logprior: -3.9866e+00
Epoch 2/2
33/33 - 9s - loss: 878.1349 - loglik: -8.7712e+02 - logprior: -1.0166e+00
Fitted a model with MAP estimate = -874.9578
expansions: [(176, 2)]
discards: [  0  32  33  62 145 168 174 175]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 11s - loss: 884.8829 - loglik: -8.8131e+02 - logprior: -3.5727e+00
Epoch 2/2
33/33 - 8s - loss: 879.6250 - loglik: -8.7883e+02 - logprior: -7.9711e-01
Fitted a model with MAP estimate = -876.8799
expansions: [(0, 1), (89, 1), (170, 2)]
discards: [168 169]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 13s - loss: 881.2004 - loglik: -8.7878e+02 - logprior: -2.4185e+00
Epoch 2/10
33/33 - 8s - loss: 876.4185 - loglik: -8.7593e+02 - logprior: -4.8488e-01
Epoch 3/10
33/33 - 7s - loss: 873.9904 - loglik: -8.7365e+02 - logprior: -3.4084e-01
Epoch 4/10
33/33 - 8s - loss: 871.0214 - loglik: -8.7064e+02 - logprior: -3.7678e-01
Epoch 5/10
33/33 - 8s - loss: 870.1091 - loglik: -8.6969e+02 - logprior: -4.2159e-01
Epoch 6/10
33/33 - 7s - loss: 868.6276 - loglik: -8.6812e+02 - logprior: -5.0598e-01
Epoch 7/10
33/33 - 8s - loss: 867.3599 - loglik: -8.6681e+02 - logprior: -5.4622e-01
Epoch 8/10
33/33 - 8s - loss: 865.3099 - loglik: -8.6474e+02 - logprior: -5.7351e-01
Epoch 9/10
33/33 - 8s - loss: 865.7344 - loglik: -8.6511e+02 - logprior: -6.2908e-01
Fitted a model with MAP estimate = -865.0275
Time for alignment: 191.7661
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 958.0391 - loglik: -9.5296e+02 - logprior: -5.0786e+00
Epoch 2/10
16/16 - 5s - loss: 923.2250 - loglik: -9.2212e+02 - logprior: -1.1013e+00
Epoch 3/10
16/16 - 5s - loss: 896.9584 - loglik: -8.9566e+02 - logprior: -1.2979e+00
Epoch 4/10
16/16 - 5s - loss: 892.1549 - loglik: -8.9080e+02 - logprior: -1.3507e+00
Epoch 5/10
16/16 - 5s - loss: 884.5565 - loglik: -8.8315e+02 - logprior: -1.4078e+00
Epoch 6/10
16/16 - 5s - loss: 883.3195 - loglik: -8.8176e+02 - logprior: -1.5566e+00
Epoch 7/10
16/16 - 5s - loss: 883.0090 - loglik: -8.8138e+02 - logprior: -1.6244e+00
Epoch 8/10
16/16 - 5s - loss: 880.7540 - loglik: -8.7903e+02 - logprior: -1.7202e+00
Epoch 9/10
16/16 - 5s - loss: 880.9285 - loglik: -8.7916e+02 - logprior: -1.7657e+00
Fitted a model with MAP estimate = -880.3796
expansions: [(13, 1), (14, 1), (23, 1), (27, 1), (28, 2), (30, 1), (43, 1), (45, 1), (49, 2), (50, 1), (55, 1), (56, 1), (58, 1), (69, 1), (72, 3), (73, 2), (82, 1), (94, 4), (95, 1), (99, 1), (105, 1), (116, 1), (117, 2), (122, 2), (125, 1), (129, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 11s - loss: 890.1854 - loglik: -8.8613e+02 - logprior: -4.0587e+00
Epoch 2/2
33/33 - 8s - loss: 880.0207 - loglik: -8.7893e+02 - logprior: -1.0901e+00
Fitted a model with MAP estimate = -875.7798
expansions: [(177, 2)]
discards: [  0  31  32  57 117 146 174 175 176]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 11s - loss: 885.7228 - loglik: -8.8211e+02 - logprior: -3.6082e+00
Epoch 2/2
33/33 - 8s - loss: 880.2367 - loglik: -8.7942e+02 - logprior: -8.1688e-01
Fitted a model with MAP estimate = -877.2418
expansions: [(0, 1), (54, 1), (170, 2)]
discards: [168 169]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 11s - loss: 881.6653 - loglik: -8.7926e+02 - logprior: -2.4084e+00
Epoch 2/10
33/33 - 8s - loss: 875.5165 - loglik: -8.7506e+02 - logprior: -4.5666e-01
Epoch 3/10
33/33 - 8s - loss: 876.1748 - loglik: -8.7585e+02 - logprior: -3.2604e-01
Fitted a model with MAP estimate = -873.0348
Time for alignment: 148.0694
Computed alignments with likelihoods: ['-866.1824', '-872.5405', '-865.6302', '-865.0275', '-873.0348']
Best model has likelihood: -865.0275  (prior= -0.6168 )
time for generating output: 0.2259
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/int.projection.fasta
SP score = 0.8090200445434298
Training of 5 independent models on file phc.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5315d44520>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52866bba90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52866bb490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5aff368a30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5294d43400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f1fd3640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5984249f10>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5334e59a90>, <__main__.SimpleDirichletPrior object at 0x7f52863fa1f0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 573.1002 - loglik: -5.5660e+02 - logprior: -1.6502e+01
Epoch 2/10
10/10 - 2s - loss: 534.6388 - loglik: -5.3043e+02 - logprior: -4.2129e+00
Epoch 3/10
10/10 - 2s - loss: 505.5214 - loglik: -5.0331e+02 - logprior: -2.2111e+00
Epoch 4/10
10/10 - 2s - loss: 498.4224 - loglik: -4.9665e+02 - logprior: -1.7748e+00
Epoch 5/10
10/10 - 2s - loss: 491.4684 - loglik: -4.8984e+02 - logprior: -1.6311e+00
Epoch 6/10
10/10 - 2s - loss: 486.0213 - loglik: -4.8448e+02 - logprior: -1.5455e+00
Epoch 7/10
10/10 - 2s - loss: 485.8450 - loglik: -4.8433e+02 - logprior: -1.5101e+00
Epoch 8/10
10/10 - 2s - loss: 486.4662 - loglik: -4.8495e+02 - logprior: -1.5124e+00
Fitted a model with MAP estimate = -485.6612
expansions: [(10, 1), (19, 3), (39, 1), (44, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 509.2263 - loglik: -4.9174e+02 - logprior: -1.7488e+01
Epoch 2/2
10/10 - 2s - loss: 493.5229 - loglik: -4.8597e+02 - logprior: -7.5570e+00
Fitted a model with MAP estimate = -488.2650
expansions: [(0, 9), (22, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 496.8439 - loglik: -4.8224e+02 - logprior: -1.4601e+01
Epoch 2/2
10/10 - 2s - loss: 480.5711 - loglik: -4.7610e+02 - logprior: -4.4682e+00
Fitted a model with MAP estimate = -475.5448
expansions: []
discards: [0 1 2 3 4 5 6 7]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 494.7350 - loglik: -4.7887e+02 - logprior: -1.5865e+01
Epoch 2/10
10/10 - 1s - loss: 483.2090 - loglik: -4.7846e+02 - logprior: -4.7463e+00
Epoch 3/10
10/10 - 2s - loss: 479.4107 - loglik: -4.7701e+02 - logprior: -2.4002e+00
Epoch 4/10
10/10 - 2s - loss: 477.8046 - loglik: -4.7615e+02 - logprior: -1.6538e+00
Epoch 5/10
10/10 - 2s - loss: 476.9086 - loglik: -4.7566e+02 - logprior: -1.2447e+00
Epoch 6/10
10/10 - 2s - loss: 477.0173 - loglik: -4.7598e+02 - logprior: -1.0366e+00
Fitted a model with MAP estimate = -476.0262
Time for alignment: 52.3750
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 574.6176 - loglik: -5.5812e+02 - logprior: -1.6502e+01
Epoch 2/10
10/10 - 2s - loss: 531.5756 - loglik: -5.2733e+02 - logprior: -4.2488e+00
Epoch 3/10
10/10 - 2s - loss: 510.1357 - loglik: -5.0776e+02 - logprior: -2.3715e+00
Epoch 4/10
10/10 - 2s - loss: 495.3833 - loglik: -4.9343e+02 - logprior: -1.9562e+00
Epoch 5/10
10/10 - 2s - loss: 489.2517 - loglik: -4.8752e+02 - logprior: -1.7272e+00
Epoch 6/10
10/10 - 2s - loss: 487.0304 - loglik: -4.8539e+02 - logprior: -1.6362e+00
Epoch 7/10
10/10 - 2s - loss: 482.3564 - loglik: -4.8084e+02 - logprior: -1.5199e+00
Epoch 8/10
10/10 - 2s - loss: 479.9706 - loglik: -4.7850e+02 - logprior: -1.4717e+00
Epoch 9/10
10/10 - 2s - loss: 481.5468 - loglik: -4.8010e+02 - logprior: -1.4483e+00
Fitted a model with MAP estimate = -481.3778
expansions: [(5, 1), (8, 1), (9, 1), (10, 1), (21, 1), (33, 1), (36, 1), (44, 1), (48, 2), (56, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 505.6369 - loglik: -4.8807e+02 - logprior: -1.7567e+01
Epoch 2/2
10/10 - 2s - loss: 484.1852 - loglik: -4.7670e+02 - logprior: -7.4882e+00
Fitted a model with MAP estimate = -479.9288
expansions: [(0, 21)]
discards: [ 0 56 65 66]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 492.7455 - loglik: -4.7780e+02 - logprior: -1.4950e+01
Epoch 2/2
10/10 - 2s - loss: 473.0057 - loglik: -4.6847e+02 - logprior: -4.5365e+00
Fitted a model with MAP estimate = -467.4921
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 492.0937 - loglik: -4.7659e+02 - logprior: -1.5502e+01
Epoch 2/10
10/10 - 2s - loss: 478.4806 - loglik: -4.7404e+02 - logprior: -4.4383e+00
Epoch 3/10
10/10 - 2s - loss: 477.0780 - loglik: -4.7501e+02 - logprior: -2.0722e+00
Epoch 4/10
10/10 - 2s - loss: 473.4645 - loglik: -4.7222e+02 - logprior: -1.2494e+00
Epoch 5/10
10/10 - 2s - loss: 471.8462 - loglik: -4.7100e+02 - logprior: -8.5065e-01
Epoch 6/10
10/10 - 2s - loss: 473.1468 - loglik: -4.7250e+02 - logprior: -6.4934e-01
Fitted a model with MAP estimate = -472.2655
Time for alignment: 54.0358
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 574.7100 - loglik: -5.5821e+02 - logprior: -1.6501e+01
Epoch 2/10
10/10 - 2s - loss: 533.3268 - loglik: -5.2908e+02 - logprior: -4.2441e+00
Epoch 3/10
10/10 - 2s - loss: 508.9929 - loglik: -5.0658e+02 - logprior: -2.4137e+00
Epoch 4/10
10/10 - 2s - loss: 493.9045 - loglik: -4.9184e+02 - logprior: -2.0659e+00
Epoch 5/10
10/10 - 2s - loss: 487.9929 - loglik: -4.8619e+02 - logprior: -1.8012e+00
Epoch 6/10
10/10 - 2s - loss: 487.3095 - loglik: -4.8568e+02 - logprior: -1.6338e+00
Epoch 7/10
10/10 - 1s - loss: 486.7874 - loglik: -4.8529e+02 - logprior: -1.4964e+00
Epoch 8/10
10/10 - 2s - loss: 484.3080 - loglik: -4.8288e+02 - logprior: -1.4320e+00
Epoch 9/10
10/10 - 2s - loss: 483.2124 - loglik: -4.8179e+02 - logprior: -1.4183e+00
Epoch 10/10
10/10 - 2s - loss: 485.3951 - loglik: -4.8398e+02 - logprior: -1.4144e+00
Fitted a model with MAP estimate = -484.2830
expansions: [(10, 1), (11, 1), (19, 1), (21, 1), (37, 1), (39, 1), (44, 1), (54, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 506.4075 - loglik: -4.8884e+02 - logprior: -1.7567e+01
Epoch 2/2
10/10 - 2s - loss: 490.8412 - loglik: -4.8325e+02 - logprior: -7.5875e+00
Fitted a model with MAP estimate = -485.6850
expansions: [(0, 21)]
discards: [ 0 40 43]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 496.4852 - loglik: -4.8132e+02 - logprior: -1.5161e+01
Epoch 2/2
10/10 - 2s - loss: 474.7390 - loglik: -4.7014e+02 - logprior: -4.6002e+00
Fitted a model with MAP estimate = -471.5051
expansions: [(60, 1), (62, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 496.3007 - loglik: -4.8042e+02 - logprior: -1.5882e+01
Epoch 2/10
10/10 - 2s - loss: 480.6318 - loglik: -4.7584e+02 - logprior: -4.7886e+00
Epoch 3/10
10/10 - 2s - loss: 479.5229 - loglik: -4.7718e+02 - logprior: -2.3466e+00
Epoch 4/10
10/10 - 2s - loss: 476.9763 - loglik: -4.7553e+02 - logprior: -1.4503e+00
Epoch 5/10
10/10 - 2s - loss: 475.7772 - loglik: -4.7477e+02 - logprior: -1.0035e+00
Epoch 6/10
10/10 - 2s - loss: 475.4357 - loglik: -4.7469e+02 - logprior: -7.4218e-01
Epoch 7/10
10/10 - 2s - loss: 476.0203 - loglik: -4.7542e+02 - logprior: -5.9976e-01
Fitted a model with MAP estimate = -474.7914
Time for alignment: 53.8728
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 571.9329 - loglik: -5.5543e+02 - logprior: -1.6507e+01
Epoch 2/10
10/10 - 2s - loss: 534.6730 - loglik: -5.3043e+02 - logprior: -4.2420e+00
Epoch 3/10
10/10 - 2s - loss: 509.4823 - loglik: -5.0717e+02 - logprior: -2.3147e+00
Epoch 4/10
10/10 - 2s - loss: 491.4217 - loglik: -4.8949e+02 - logprior: -1.9351e+00
Epoch 5/10
10/10 - 2s - loss: 484.7757 - loglik: -4.8309e+02 - logprior: -1.6880e+00
Epoch 6/10
10/10 - 2s - loss: 486.3741 - loglik: -4.8480e+02 - logprior: -1.5789e+00
Fitted a model with MAP estimate = -483.8047
expansions: [(10, 1), (23, 2), (34, 1), (37, 1), (39, 2), (48, 1), (54, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 506.7450 - loglik: -4.8929e+02 - logprior: -1.7460e+01
Epoch 2/2
10/10 - 2s - loss: 489.2486 - loglik: -4.8181e+02 - logprior: -7.4422e+00
Fitted a model with MAP estimate = -483.9622
expansions: [(23, 1)]
discards: [ 0 43 64]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 498.6318 - loglik: -4.8159e+02 - logprior: -1.7046e+01
Epoch 2/2
10/10 - 2s - loss: 485.7387 - loglik: -4.7947e+02 - logprior: -6.2714e+00
Fitted a model with MAP estimate = -481.0973
expansions: [(0, 21)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 493.2588 - loglik: -4.7820e+02 - logprior: -1.5059e+01
Epoch 2/10
10/10 - 2s - loss: 473.5150 - loglik: -4.6901e+02 - logprior: -4.5071e+00
Epoch 3/10
10/10 - 2s - loss: 466.1743 - loglik: -4.6350e+02 - logprior: -2.6755e+00
Epoch 4/10
10/10 - 2s - loss: 465.0773 - loglik: -4.6317e+02 - logprior: -1.9064e+00
Epoch 5/10
10/10 - 2s - loss: 460.7987 - loglik: -4.5909e+02 - logprior: -1.7050e+00
Epoch 6/10
10/10 - 2s - loss: 465.8419 - loglik: -4.6429e+02 - logprior: -1.5538e+00
Fitted a model with MAP estimate = -462.5676
Time for alignment: 48.4037
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 574.5155 - loglik: -5.5801e+02 - logprior: -1.6504e+01
Epoch 2/10
10/10 - 2s - loss: 532.5797 - loglik: -5.2833e+02 - logprior: -4.2493e+00
Epoch 3/10
10/10 - 2s - loss: 510.6012 - loglik: -5.0820e+02 - logprior: -2.4059e+00
Epoch 4/10
10/10 - 2s - loss: 490.7762 - loglik: -4.8877e+02 - logprior: -2.0013e+00
Epoch 5/10
10/10 - 2s - loss: 487.6310 - loglik: -4.8579e+02 - logprior: -1.8377e+00
Epoch 6/10
10/10 - 2s - loss: 484.1762 - loglik: -4.8243e+02 - logprior: -1.7506e+00
Epoch 7/10
10/10 - 2s - loss: 482.9830 - loglik: -4.8127e+02 - logprior: -1.7148e+00
Epoch 8/10
10/10 - 2s - loss: 483.0772 - loglik: -4.8131e+02 - logprior: -1.7652e+00
Fitted a model with MAP estimate = -482.2988
expansions: [(10, 2), (25, 1), (34, 1), (37, 1), (39, 1), (44, 1), (45, 1), (48, 2), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 504.7755 - loglik: -4.8731e+02 - logprior: -1.7467e+01
Epoch 2/2
10/10 - 2s - loss: 486.5906 - loglik: -4.7917e+02 - logprior: -7.4202e+00
Fitted a model with MAP estimate = -482.5526
expansions: [(0, 9)]
discards: [ 0  9 36]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 493.3979 - loglik: -4.7893e+02 - logprior: -1.4467e+01
Epoch 2/2
10/10 - 2s - loss: 475.4810 - loglik: -4.7112e+02 - logprior: -4.3609e+00
Fitted a model with MAP estimate = -471.9291
expansions: []
discards: [0 1 2 3 4 5 6 7]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 493.0173 - loglik: -4.7755e+02 - logprior: -1.5472e+01
Epoch 2/10
10/10 - 2s - loss: 478.6646 - loglik: -4.7419e+02 - logprior: -4.4768e+00
Epoch 3/10
10/10 - 2s - loss: 474.1486 - loglik: -4.7188e+02 - logprior: -2.2646e+00
Epoch 4/10
10/10 - 2s - loss: 473.7102 - loglik: -4.7218e+02 - logprior: -1.5336e+00
Epoch 5/10
10/10 - 2s - loss: 475.0706 - loglik: -4.7397e+02 - logprior: -1.0987e+00
Fitted a model with MAP estimate = -473.0924
Time for alignment: 49.0937
Computed alignments with likelihoods: ['-475.5448', '-467.4921', '-471.5051', '-462.5676', '-471.9291']
Best model has likelihood: -462.5676  (prior= -1.4870 )
time for generating output: 0.2018
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phc.projection.fasta
SP score = 0.3888784165881244
Training of 5 independent models on file ricin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5284211160>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f531c884ca0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f528611fdf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f528612bd30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5ca79daaf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5ca79da850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5ca79da520>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52f09a3040>, <__main__.SimpleDirichletPrior object at 0x7f5ca7e418e0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 1203.0359 - loglik: -1.1350e+03 - logprior: -6.8077e+01
Epoch 2/10
10/10 - 3s - loss: 1092.2229 - loglik: -1.0801e+03 - logprior: -1.2144e+01
Epoch 3/10
10/10 - 3s - loss: 1028.9104 - loglik: -1.0261e+03 - logprior: -2.8354e+00
Epoch 4/10
10/10 - 3s - loss: 993.8954 - loglik: -9.9369e+02 - logprior: -2.0332e-01
Epoch 5/10
10/10 - 3s - loss: 982.4548 - loglik: -9.8360e+02 - logprior: 1.1450
Epoch 6/10
10/10 - 3s - loss: 975.4907 - loglik: -9.7736e+02 - logprior: 1.8736
Epoch 7/10
10/10 - 3s - loss: 975.6805 - loglik: -9.7786e+02 - logprior: 2.1756
Fitted a model with MAP estimate = -973.5111
expansions: [(0, 3), (14, 1), (22, 3), (23, 1), (29, 2), (43, 1), (44, 2), (46, 1), (52, 2), (76, 2), (81, 2), (82, 1), (94, 1), (106, 1), (114, 2), (116, 1), (117, 2), (145, 3), (146, 1), (150, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 200 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 1055.5801 - loglik: -9.8463e+02 - logprior: -7.0947e+01
Epoch 2/2
10/10 - 3s - loss: 981.8624 - loglik: -9.6569e+02 - logprior: -1.6169e+01
Fitted a model with MAP estimate = -968.0566
expansions: []
discards: [  0   1   2  27  28  66  99 137 173 174 175]
Re-initialized the encoder parameters.
Fitting a model of length 189 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 1017.6037 - loglik: -9.6861e+02 - logprior: -4.8995e+01
Epoch 2/2
10/10 - 3s - loss: 973.3256 - loglik: -9.6560e+02 - logprior: -7.7283e+00
Fitted a model with MAP estimate = -963.8069
expansions: [(0, 13), (33, 1)]
discards: [ 0 86]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 1018.2863 - loglik: -9.7034e+02 - logprior: -4.7950e+01
Epoch 2/10
10/10 - 3s - loss: 968.9684 - loglik: -9.6168e+02 - logprior: -7.2879e+00
Epoch 3/10
10/10 - 3s - loss: 954.7453 - loglik: -9.5585e+02 - logprior: 1.1041
Epoch 4/10
10/10 - 3s - loss: 951.4922 - loglik: -9.5628e+02 - logprior: 4.7840
Epoch 5/10
10/10 - 3s - loss: 945.6245 - loglik: -9.5229e+02 - logprior: 6.6687
Epoch 6/10
10/10 - 3s - loss: 944.3623 - loglik: -9.5219e+02 - logprior: 7.8259
Epoch 7/10
10/10 - 3s - loss: 941.0990 - loglik: -9.4967e+02 - logprior: 8.5721
Epoch 8/10
10/10 - 3s - loss: 940.9651 - loglik: -9.5012e+02 - logprior: 9.1538
Epoch 9/10
10/10 - 3s - loss: 941.9404 - loglik: -9.5160e+02 - logprior: 9.6602
Fitted a model with MAP estimate = -940.3343
Time for alignment: 81.1402
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 1204.3890 - loglik: -1.1363e+03 - logprior: -6.8084e+01
Epoch 2/10
10/10 - 3s - loss: 1091.4669 - loglik: -1.0793e+03 - logprior: -1.2139e+01
Epoch 3/10
10/10 - 3s - loss: 1031.2556 - loglik: -1.0284e+03 - logprior: -2.8912e+00
Epoch 4/10
10/10 - 3s - loss: 995.2294 - loglik: -9.9492e+02 - logprior: -3.1235e-01
Epoch 5/10
10/10 - 3s - loss: 981.5106 - loglik: -9.8232e+02 - logprior: 0.8139
Epoch 6/10
10/10 - 3s - loss: 976.2482 - loglik: -9.7770e+02 - logprior: 1.4477
Epoch 7/10
10/10 - 3s - loss: 974.7101 - loglik: -9.7658e+02 - logprior: 1.8745
Epoch 8/10
10/10 - 3s - loss: 972.5992 - loglik: -9.7473e+02 - logprior: 2.1321
Epoch 9/10
10/10 - 3s - loss: 972.6261 - loglik: -9.7501e+02 - logprior: 2.3864
Fitted a model with MAP estimate = -971.9461
expansions: [(7, 1), (8, 2), (19, 2), (22, 3), (23, 1), (25, 1), (29, 2), (43, 1), (44, 2), (46, 1), (52, 2), (71, 1), (75, 3), (81, 2), (97, 1), (104, 1), (105, 2), (106, 2), (113, 1), (116, 2), (145, 4), (149, 1), (150, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 206 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 1048.4768 - loglik: -9.8444e+02 - logprior: -6.4035e+01
Epoch 2/2
10/10 - 3s - loss: 983.6307 - loglik: -9.6176e+02 - logprior: -2.1870e+01
Fitted a model with MAP estimate = -972.3806
expansions: [(0, 15)]
discards: [  0  22  27  28  67  95 102 187 188]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 1013.8055 - loglik: -9.6547e+02 - logprior: -4.8335e+01
Epoch 2/2
10/10 - 3s - loss: 966.4564 - loglik: -9.5895e+02 - logprior: -7.5085e+00
Fitted a model with MAP estimate = -958.2316
expansions: [(50, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 44]
Re-initialized the encoder parameters.
Fitting a model of length 198 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 1014.2751 - loglik: -9.6067e+02 - logprior: -5.3609e+01
Epoch 2/10
10/10 - 3s - loss: 966.7010 - loglik: -9.5781e+02 - logprior: -8.8874e+00
Epoch 3/10
10/10 - 3s - loss: 953.1696 - loglik: -9.5449e+02 - logprior: 1.3210
Epoch 4/10
10/10 - 3s - loss: 945.8842 - loglik: -9.5117e+02 - logprior: 5.2844
Epoch 5/10
10/10 - 3s - loss: 941.5371 - loglik: -9.4884e+02 - logprior: 7.3044
Epoch 6/10
10/10 - 3s - loss: 939.0221 - loglik: -9.4729e+02 - logprior: 8.2648
Epoch 7/10
10/10 - 3s - loss: 937.4352 - loglik: -9.4634e+02 - logprior: 8.9068
Epoch 8/10
10/10 - 3s - loss: 934.9159 - loglik: -9.4450e+02 - logprior: 9.5890
Epoch 9/10
10/10 - 3s - loss: 936.4647 - loglik: -9.4671e+02 - logprior: 10.2423
Fitted a model with MAP estimate = -934.9621
Time for alignment: 85.6520
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 1204.5026 - loglik: -1.1364e+03 - logprior: -6.8080e+01
Epoch 2/10
10/10 - 3s - loss: 1091.9216 - loglik: -1.0798e+03 - logprior: -1.2090e+01
Epoch 3/10
10/10 - 3s - loss: 1029.5276 - loglik: -1.0268e+03 - logprior: -2.7065e+00
Epoch 4/10
10/10 - 3s - loss: 1000.9706 - loglik: -1.0010e+03 - logprior: 0.0751
Epoch 5/10
10/10 - 3s - loss: 986.2455 - loglik: -9.8772e+02 - logprior: 1.4709
Epoch 6/10
10/10 - 3s - loss: 980.9445 - loglik: -9.8306e+02 - logprior: 2.1156
Epoch 7/10
10/10 - 3s - loss: 980.0522 - loglik: -9.8249e+02 - logprior: 2.4337
Epoch 8/10
10/10 - 3s - loss: 976.1835 - loglik: -9.7879e+02 - logprior: 2.6098
Epoch 9/10
10/10 - 3s - loss: 975.9316 - loglik: -9.7867e+02 - logprior: 2.7392
Epoch 10/10
10/10 - 3s - loss: 975.3353 - loglik: -9.7828e+02 - logprior: 2.9445
Fitted a model with MAP estimate = -974.9877
expansions: [(0, 3), (6, 2), (9, 1), (22, 3), (23, 1), (25, 1), (29, 3), (43, 1), (44, 2), (46, 1), (52, 2), (76, 3), (81, 1), (82, 2), (103, 1), (116, 3), (145, 6), (150, 1), (152, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 205 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 1059.6931 - loglik: -9.8789e+02 - logprior: -7.1806e+01
Epoch 2/2
10/10 - 3s - loss: 985.7785 - loglik: -9.6898e+02 - logprior: -1.6797e+01
Fitted a model with MAP estimate = -970.8222
expansions: []
discards: [  0   1   2  29  70  96  97 106 175 186]
Re-initialized the encoder parameters.
Fitting a model of length 195 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 1020.2401 - loglik: -9.7080e+02 - logprior: -4.9441e+01
Epoch 2/2
10/10 - 3s - loss: 973.1509 - loglik: -9.6501e+02 - logprior: -8.1403e+00
Fitted a model with MAP estimate = -965.1053
expansions: [(0, 14), (7, 1)]
discards: [ 0 11]
Re-initialized the encoder parameters.
Fitting a model of length 208 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 1019.3319 - loglik: -9.7065e+02 - logprior: -4.8681e+01
Epoch 2/10
10/10 - 3s - loss: 971.1414 - loglik: -9.6330e+02 - logprior: -7.8381e+00
Epoch 3/10
10/10 - 3s - loss: 958.8849 - loglik: -9.5954e+02 - logprior: 0.6504
Epoch 4/10
10/10 - 3s - loss: 951.7361 - loglik: -9.5607e+02 - logprior: 4.3381
Epoch 5/10
10/10 - 3s - loss: 947.1662 - loglik: -9.5346e+02 - logprior: 6.2934
Epoch 6/10
10/10 - 3s - loss: 946.1720 - loglik: -9.5370e+02 - logprior: 7.5278
Epoch 7/10
10/10 - 3s - loss: 941.8579 - loglik: -9.5011e+02 - logprior: 8.2532
Epoch 8/10
10/10 - 3s - loss: 941.7990 - loglik: -9.5063e+02 - logprior: 8.8279
Epoch 9/10
10/10 - 3s - loss: 942.7487 - loglik: -9.5210e+02 - logprior: 9.3513
Fitted a model with MAP estimate = -941.3667
Time for alignment: 91.3223
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 1203.6835 - loglik: -1.1356e+03 - logprior: -6.8089e+01
Epoch 2/10
10/10 - 3s - loss: 1093.7380 - loglik: -1.0816e+03 - logprior: -1.2122e+01
Epoch 3/10
10/10 - 3s - loss: 1029.5775 - loglik: -1.0268e+03 - logprior: -2.8122e+00
Epoch 4/10
10/10 - 3s - loss: 997.4371 - loglik: -9.9709e+02 - logprior: -3.4305e-01
Epoch 5/10
10/10 - 3s - loss: 982.2863 - loglik: -9.8313e+02 - logprior: 0.8482
Epoch 6/10
10/10 - 3s - loss: 977.9742 - loglik: -9.7960e+02 - logprior: 1.6253
Epoch 7/10
10/10 - 3s - loss: 975.2209 - loglik: -9.7718e+02 - logprior: 1.9587
Epoch 8/10
10/10 - 3s - loss: 974.4844 - loglik: -9.7665e+02 - logprior: 2.1694
Epoch 9/10
10/10 - 3s - loss: 973.4432 - loglik: -9.7582e+02 - logprior: 2.3802
Epoch 10/10
10/10 - 3s - loss: 973.1406 - loglik: -9.7572e+02 - logprior: 2.5815
Fitted a model with MAP estimate = -972.7869
expansions: [(9, 2), (12, 1), (19, 2), (22, 1), (23, 3), (25, 1), (29, 2), (43, 1), (44, 1), (45, 1), (52, 2), (76, 2), (81, 1), (82, 2), (108, 2), (114, 1), (116, 3), (131, 1), (144, 4), (150, 1), (152, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 1050.1661 - loglik: -9.8643e+02 - logprior: -6.3738e+01
Epoch 2/2
10/10 - 3s - loss: 988.7290 - loglik: -9.6662e+02 - logprior: -2.2105e+01
Fitted a model with MAP estimate = -977.3084
expansions: [(0, 14), (117, 1)]
discards: [  0  22  29  30  34  38  39  66  92  93 101 129 141]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 1023.4523 - loglik: -9.7494e+02 - logprior: -4.8510e+01
Epoch 2/2
10/10 - 3s - loss: 973.9174 - loglik: -9.6574e+02 - logprior: -8.1747e+00
Fitted a model with MAP estimate = -965.7726
expansions: [(41, 2), (98, 2)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13 187]
Re-initialized the encoder parameters.
Fitting a model of length 193 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 1015.1421 - loglik: -9.6887e+02 - logprior: -4.6276e+01
Epoch 2/10
10/10 - 3s - loss: 970.2168 - loglik: -9.6314e+02 - logprior: -7.0779e+00
Epoch 3/10
10/10 - 3s - loss: 957.6481 - loglik: -9.5872e+02 - logprior: 1.0727
Epoch 4/10
10/10 - 3s - loss: 953.4593 - loglik: -9.5802e+02 - logprior: 4.5571
Epoch 5/10
10/10 - 3s - loss: 948.7645 - loglik: -9.5535e+02 - logprior: 6.5893
Epoch 6/10
10/10 - 3s - loss: 946.6507 - loglik: -9.5440e+02 - logprior: 7.7527
Epoch 7/10
10/10 - 3s - loss: 945.0041 - loglik: -9.5351e+02 - logprior: 8.5109
Epoch 8/10
10/10 - 3s - loss: 943.2983 - loglik: -9.5242e+02 - logprior: 9.1200
Epoch 9/10
10/10 - 3s - loss: 943.5685 - loglik: -9.5317e+02 - logprior: 9.6001
Fitted a model with MAP estimate = -942.9769
Time for alignment: 84.6793
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 1203.5364 - loglik: -1.1355e+03 - logprior: -6.8083e+01
Epoch 2/10
10/10 - 3s - loss: 1092.8306 - loglik: -1.0807e+03 - logprior: -1.2160e+01
Epoch 3/10
10/10 - 3s - loss: 1029.8440 - loglik: -1.0269e+03 - logprior: -2.8982e+00
Epoch 4/10
10/10 - 3s - loss: 1001.1852 - loglik: -1.0010e+03 - logprior: -1.4883e-01
Epoch 5/10
10/10 - 3s - loss: 983.4771 - loglik: -9.8467e+02 - logprior: 1.1976
Epoch 6/10
10/10 - 3s - loss: 980.5933 - loglik: -9.8250e+02 - logprior: 1.9063
Epoch 7/10
10/10 - 3s - loss: 975.8375 - loglik: -9.7812e+02 - logprior: 2.2869
Epoch 8/10
10/10 - 3s - loss: 975.0254 - loglik: -9.7759e+02 - logprior: 2.5639
Epoch 9/10
10/10 - 3s - loss: 973.1781 - loglik: -9.7594e+02 - logprior: 2.7591
Epoch 10/10
10/10 - 3s - loss: 973.9169 - loglik: -9.7683e+02 - logprior: 2.9162
Fitted a model with MAP estimate = -973.3391
expansions: [(0, 3), (19, 2), (22, 1), (23, 1), (25, 1), (30, 3), (44, 1), (45, 2), (46, 1), (75, 1), (81, 2), (94, 1), (102, 2), (104, 1), (107, 1), (117, 2), (118, 3), (145, 6), (150, 1), (152, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 203 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 1057.3724 - loglik: -9.8569e+02 - logprior: -7.1680e+01
Epoch 2/2
10/10 - 3s - loss: 984.2576 - loglik: -9.6766e+02 - logprior: -1.6594e+01
Fitted a model with MAP estimate = -969.6192
expansions: [(92, 2)]
discards: [  0   1   2  23  57  98 121 125 129 140 184]
Re-initialized the encoder parameters.
Fitting a model of length 194 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 1021.0468 - loglik: -9.7167e+02 - logprior: -4.9373e+01
Epoch 2/2
10/10 - 3s - loss: 974.1230 - loglik: -9.6606e+02 - logprior: -8.0586e+00
Fitted a model with MAP estimate = -965.4057
expansions: [(0, 13), (120, 1), (168, 3), (176, 1)]
discards: [  0  34 165]
Re-initialized the encoder parameters.
Fitting a model of length 209 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 1020.0242 - loglik: -9.7153e+02 - logprior: -4.8495e+01
Epoch 2/10
10/10 - 3s - loss: 969.6830 - loglik: -9.6217e+02 - logprior: -7.5168e+00
Epoch 3/10
10/10 - 3s - loss: 958.0783 - loglik: -9.5906e+02 - logprior: 0.9770
Epoch 4/10
10/10 - 3s - loss: 948.2591 - loglik: -9.5300e+02 - logprior: 4.7363
Epoch 5/10
10/10 - 3s - loss: 947.6130 - loglik: -9.5427e+02 - logprior: 6.6606
Epoch 6/10
10/10 - 3s - loss: 944.5233 - loglik: -9.5235e+02 - logprior: 7.8281
Epoch 7/10
10/10 - 3s - loss: 940.6136 - loglik: -9.4926e+02 - logprior: 8.6430
Epoch 8/10
10/10 - 3s - loss: 940.3978 - loglik: -9.4959e+02 - logprior: 9.1950
Epoch 9/10
10/10 - 3s - loss: 941.6434 - loglik: -9.5136e+02 - logprior: 9.7151
Fitted a model with MAP estimate = -939.6362
Time for alignment: 88.3771
Computed alignments with likelihoods: ['-940.3343', '-934.9621', '-941.3667', '-942.9769', '-939.6362']
Best model has likelihood: -934.9621  (prior= 10.5543 )
time for generating output: 0.2796
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ricin.projection.fasta
SP score = 0.7945660672400313
Training of 5 independent models on file PDZ.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5aff29c400>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52d93d9ac0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f86ca340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f86ca3a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b07aafee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f528aae9490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5294584820>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5335f56670>, <__main__.SimpleDirichletPrior object at 0x7f528aaf3be0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 480.7668 - loglik: -4.7739e+02 - logprior: -3.3812e+00
Epoch 2/10
19/19 - 2s - loss: 449.3212 - loglik: -4.4816e+02 - logprior: -1.1570e+00
Epoch 3/10
19/19 - 2s - loss: 440.8223 - loglik: -4.3962e+02 - logprior: -1.1990e+00
Epoch 4/10
19/19 - 2s - loss: 437.1644 - loglik: -4.3596e+02 - logprior: -1.2047e+00
Epoch 5/10
19/19 - 2s - loss: 437.0444 - loglik: -4.3587e+02 - logprior: -1.1718e+00
Epoch 6/10
19/19 - 2s - loss: 435.9786 - loglik: -4.3477e+02 - logprior: -1.2055e+00
Epoch 7/10
19/19 - 2s - loss: 436.1767 - loglik: -4.3495e+02 - logprior: -1.2233e+00
Fitted a model with MAP estimate = -415.8371
expansions: [(0, 2), (3, 1), (4, 1), (5, 2), (15, 2), (16, 4), (17, 2), (24, 1), (46, 2), (47, 1), (48, 2), (49, 1), (52, 2), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 438.9684 - loglik: -4.3471e+02 - logprior: -4.2565e+00
Epoch 2/2
19/19 - 2s - loss: 430.0762 - loglik: -4.2860e+02 - logprior: -1.4741e+00
Fitted a model with MAP estimate = -408.9448
expansions: [(13, 1)]
discards: [ 0  1  2 24 25 26 62 74]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 435.3155 - loglik: -4.3154e+02 - logprior: -3.7706e+00
Epoch 2/2
19/19 - 2s - loss: 430.5528 - loglik: -4.2905e+02 - logprior: -1.5034e+00
Fitted a model with MAP estimate = -409.2027
expansions: [(0, 4), (22, 3), (26, 2)]
discards: [ 0  7  8 19]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 410.6398 - loglik: -4.0875e+02 - logprior: -1.8943e+00
Epoch 2/10
23/23 - 3s - loss: 407.3239 - loglik: -4.0634e+02 - logprior: -9.8737e-01
Epoch 3/10
23/23 - 3s - loss: 407.9341 - loglik: -4.0697e+02 - logprior: -9.6502e-01
Fitted a model with MAP estimate = -406.7880
Time for alignment: 67.5943
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 481.5163 - loglik: -4.7813e+02 - logprior: -3.3822e+00
Epoch 2/10
19/19 - 2s - loss: 450.6601 - loglik: -4.4950e+02 - logprior: -1.1637e+00
Epoch 3/10
19/19 - 2s - loss: 439.9485 - loglik: -4.3874e+02 - logprior: -1.2050e+00
Epoch 4/10
19/19 - 2s - loss: 438.0757 - loglik: -4.3694e+02 - logprior: -1.1379e+00
Epoch 5/10
19/19 - 2s - loss: 437.5255 - loglik: -4.3639e+02 - logprior: -1.1320e+00
Epoch 6/10
19/19 - 2s - loss: 436.5925 - loglik: -4.3543e+02 - logprior: -1.1650e+00
Epoch 7/10
19/19 - 2s - loss: 436.6300 - loglik: -4.3543e+02 - logprior: -1.1985e+00
Fitted a model with MAP estimate = -416.5500
expansions: [(0, 2), (3, 1), (4, 1), (5, 1), (6, 1), (16, 5), (17, 1), (18, 2), (23, 1), (46, 3), (47, 1), (48, 2), (49, 2), (52, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 438.5764 - loglik: -4.3436e+02 - logprior: -4.2163e+00
Epoch 2/2
19/19 - 2s - loss: 429.9552 - loglik: -4.2852e+02 - logprior: -1.4400e+00
Fitted a model with MAP estimate = -408.9364
expansions: [(31, 1)]
discards: [ 1  2 24 25 26 62 70]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 432.7048 - loglik: -4.2996e+02 - logprior: -2.7407e+00
Epoch 2/2
19/19 - 2s - loss: 429.7037 - loglik: -4.2875e+02 - logprior: -9.4959e-01
Fitted a model with MAP estimate = -409.0755
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 8s - loss: 410.2833 - loglik: -4.0846e+02 - logprior: -1.8212e+00
Epoch 2/10
23/23 - 3s - loss: 408.5063 - loglik: -4.0756e+02 - logprior: -9.4143e-01
Epoch 3/10
23/23 - 3s - loss: 406.3274 - loglik: -4.0541e+02 - logprior: -9.1824e-01
Epoch 4/10
23/23 - 3s - loss: 407.5214 - loglik: -4.0664e+02 - logprior: -8.8214e-01
Fitted a model with MAP estimate = -406.6010
Time for alignment: 70.1601
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 480.9704 - loglik: -4.7759e+02 - logprior: -3.3805e+00
Epoch 2/10
19/19 - 2s - loss: 450.8299 - loglik: -4.4965e+02 - logprior: -1.1806e+00
Epoch 3/10
19/19 - 2s - loss: 439.9804 - loglik: -4.3873e+02 - logprior: -1.2483e+00
Epoch 4/10
19/19 - 2s - loss: 437.7307 - loglik: -4.3653e+02 - logprior: -1.2024e+00
Epoch 5/10
19/19 - 2s - loss: 437.0397 - loglik: -4.3586e+02 - logprior: -1.1779e+00
Epoch 6/10
19/19 - 2s - loss: 435.7094 - loglik: -4.3448e+02 - logprior: -1.2279e+00
Epoch 7/10
19/19 - 2s - loss: 435.9333 - loglik: -4.3469e+02 - logprior: -1.2392e+00
Fitted a model with MAP estimate = -415.7855
expansions: [(0, 2), (3, 1), (4, 1), (5, 1), (6, 1), (16, 5), (17, 1), (18, 2), (23, 1), (36, 1), (46, 1), (47, 1), (48, 2), (49, 1), (52, 2), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 438.1877 - loglik: -4.3398e+02 - logprior: -4.2057e+00
Epoch 2/2
19/19 - 2s - loss: 430.0492 - loglik: -4.2863e+02 - logprior: -1.4162e+00
Fitted a model with MAP estimate = -408.9693
expansions: [(31, 1)]
discards: [ 0  1  2 24 25 26 28 74]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 435.7129 - loglik: -4.3197e+02 - logprior: -3.7419e+00
Epoch 2/2
19/19 - 2s - loss: 431.2803 - loglik: -4.2971e+02 - logprior: -1.5662e+00
Fitted a model with MAP estimate = -409.9194
expansions: [(0, 4), (23, 1)]
discards: [ 0 19]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 410.9680 - loglik: -4.0907e+02 - logprior: -1.8973e+00
Epoch 2/10
23/23 - 3s - loss: 407.8957 - loglik: -4.0693e+02 - logprior: -9.6456e-01
Epoch 3/10
23/23 - 3s - loss: 407.8047 - loglik: -4.0687e+02 - logprior: -9.3561e-01
Epoch 4/10
23/23 - 3s - loss: 407.0652 - loglik: -4.0616e+02 - logprior: -9.0548e-01
Epoch 5/10
23/23 - 3s - loss: 407.2362 - loglik: -4.0631e+02 - logprior: -9.2355e-01
Fitted a model with MAP estimate = -406.3968
Time for alignment: 67.3036
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 481.5243 - loglik: -4.7814e+02 - logprior: -3.3818e+00
Epoch 2/10
19/19 - 2s - loss: 451.5074 - loglik: -4.5033e+02 - logprior: -1.1759e+00
Epoch 3/10
19/19 - 2s - loss: 439.9546 - loglik: -4.3870e+02 - logprior: -1.2539e+00
Epoch 4/10
19/19 - 2s - loss: 437.2675 - loglik: -4.3607e+02 - logprior: -1.1989e+00
Epoch 5/10
19/19 - 2s - loss: 436.6351 - loglik: -4.3545e+02 - logprior: -1.1836e+00
Epoch 6/10
19/19 - 2s - loss: 435.9561 - loglik: -4.3475e+02 - logprior: -1.2026e+00
Epoch 7/10
19/19 - 2s - loss: 436.1100 - loglik: -4.3487e+02 - logprior: -1.2408e+00
Fitted a model with MAP estimate = -415.7720
expansions: [(0, 2), (3, 1), (4, 1), (5, 1), (6, 1), (16, 5), (17, 1), (18, 2), (23, 1), (31, 1), (45, 1), (47, 1), (48, 2), (49, 2), (52, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 437.9958 - loglik: -4.3378e+02 - logprior: -4.2142e+00
Epoch 2/2
19/19 - 2s - loss: 430.1678 - loglik: -4.2875e+02 - logprior: -1.4139e+00
Fitted a model with MAP estimate = -408.9601
expansions: [(31, 1)]
discards: [ 0  1  2 24 25 26 67]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 435.5865 - loglik: -4.3185e+02 - logprior: -3.7348e+00
Epoch 2/2
19/19 - 2s - loss: 431.1414 - loglik: -4.2958e+02 - logprior: -1.5578e+00
Fitted a model with MAP estimate = -409.6843
expansions: [(0, 4)]
discards: [ 0 19]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 410.9243 - loglik: -4.0904e+02 - logprior: -1.8823e+00
Epoch 2/10
23/23 - 3s - loss: 407.9315 - loglik: -4.0697e+02 - logprior: -9.5820e-01
Epoch 3/10
23/23 - 3s - loss: 408.1337 - loglik: -4.0721e+02 - logprior: -9.2145e-01
Fitted a model with MAP estimate = -407.2907
Time for alignment: 65.7849
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 481.0811 - loglik: -4.7769e+02 - logprior: -3.3869e+00
Epoch 2/10
19/19 - 2s - loss: 450.8873 - loglik: -4.4972e+02 - logprior: -1.1642e+00
Epoch 3/10
19/19 - 2s - loss: 439.7599 - loglik: -4.3855e+02 - logprior: -1.2143e+00
Epoch 4/10
19/19 - 2s - loss: 437.2209 - loglik: -4.3602e+02 - logprior: -1.2031e+00
Epoch 5/10
19/19 - 2s - loss: 436.4932 - loglik: -4.3531e+02 - logprior: -1.1866e+00
Epoch 6/10
19/19 - 2s - loss: 435.4236 - loglik: -4.3422e+02 - logprior: -1.2074e+00
Epoch 7/10
19/19 - 2s - loss: 435.8313 - loglik: -4.3460e+02 - logprior: -1.2356e+00
Fitted a model with MAP estimate = -415.6717
expansions: [(0, 2), (3, 1), (4, 1), (5, 1), (6, 1), (16, 5), (17, 1), (18, 1), (22, 1), (31, 1), (46, 2), (47, 1), (48, 2), (49, 1), (53, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 89 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 438.3795 - loglik: -4.3416e+02 - logprior: -4.2205e+00
Epoch 2/2
19/19 - 2s - loss: 430.3261 - loglik: -4.2891e+02 - logprior: -1.4187e+00
Fitted a model with MAP estimate = -409.0983
expansions: [(30, 2)]
discards: [ 1  2 23 24 25 26 62]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 433.3523 - loglik: -4.3059e+02 - logprior: -2.7669e+00
Epoch 2/2
19/19 - 2s - loss: 430.1887 - loglik: -4.2921e+02 - logprior: -9.8189e-01
Fitted a model with MAP estimate = -409.4664
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 410.2126 - loglik: -4.0838e+02 - logprior: -1.8350e+00
Epoch 2/10
23/23 - 3s - loss: 408.5665 - loglik: -4.0760e+02 - logprior: -9.6200e-01
Epoch 3/10
23/23 - 3s - loss: 408.1229 - loglik: -4.0721e+02 - logprior: -9.1707e-01
Epoch 4/10
23/23 - 3s - loss: 406.9184 - loglik: -4.0603e+02 - logprior: -8.8421e-01
Epoch 5/10
23/23 - 3s - loss: 407.2870 - loglik: -4.0639e+02 - logprior: -9.0142e-01
Fitted a model with MAP estimate = -406.5260
Time for alignment: 70.6202
Computed alignments with likelihoods: ['-406.7880', '-406.6010', '-406.3968', '-407.2907', '-406.5260']
Best model has likelihood: -406.3968  (prior= -0.9278 )
time for generating output: 0.1493
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PDZ.projection.fasta
SP score = 0.858678955453149
Training of 5 independent models on file asp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52f9b1f910>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f530093ae80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531c49cb80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52886dfeb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f060fbb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5aff3bf370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f53004e21c0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52d8583f70>, <__main__.SimpleDirichletPrior object at 0x7f5286c61730>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 20s - loss: 1627.4790 - loglik: -1.6188e+03 - logprior: -8.6907e+00
Epoch 2/10
22/22 - 13s - loss: 1512.8441 - loglik: -1.5120e+03 - logprior: -8.0014e-01
Epoch 3/10
22/22 - 13s - loss: 1485.7710 - loglik: -1.4841e+03 - logprior: -1.6987e+00
Epoch 4/10
22/22 - 13s - loss: 1474.3643 - loglik: -1.4728e+03 - logprior: -1.5577e+00
Epoch 5/10
22/22 - 13s - loss: 1468.6509 - loglik: -1.4670e+03 - logprior: -1.6188e+00
Epoch 6/10
22/22 - 13s - loss: 1468.6335 - loglik: -1.4668e+03 - logprior: -1.8139e+00
Epoch 7/10
22/22 - 13s - loss: 1468.2417 - loglik: -1.4664e+03 - logprior: -1.8222e+00
Epoch 8/10
22/22 - 13s - loss: 1466.8673 - loglik: -1.4650e+03 - logprior: -1.8479e+00
Epoch 9/10
22/22 - 13s - loss: 1473.6875 - loglik: -1.4718e+03 - logprior: -1.9344e+00
Fitted a model with MAP estimate = -1467.7034
expansions: [(14, 1), (15, 1), (32, 2), (33, 1), (35, 3), (36, 1), (46, 2), (49, 1), (50, 1), (60, 1), (62, 1), (70, 1), (71, 1), (76, 1), (77, 2), (80, 1), (82, 1), (98, 1), (99, 1), (105, 1), (106, 1), (108, 1), (110, 1), (121, 2), (132, 1), (144, 1), (150, 1), (151, 1), (153, 1), (157, 2), (159, 3), (177, 1), (179, 3), (180, 1), (181, 1), (184, 3), (185, 1), (186, 1), (194, 2), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (214, 2), (224, 2), (226, 1), (236, 1), (237, 1), (238, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 317 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 22s - loss: 1469.5074 - loglik: -1.4630e+03 - logprior: -6.4814e+00
Epoch 2/2
22/22 - 18s - loss: 1448.6995 - loglik: -1.4491e+03 - logprior: 0.4138
Fitted a model with MAP estimate = -1446.4764
expansions: [(103, 1), (290, 1)]
discards: [ 34  40 196 197 220 230 231 284 285]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 1458.2280 - loglik: -1.4536e+03 - logprior: -4.6734e+00
Epoch 2/2
22/22 - 17s - loss: 1454.2709 - loglik: -1.4555e+03 - logprior: 1.2643
Fitted a model with MAP estimate = -1446.1220
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 21s - loss: 1455.2911 - loglik: -1.4510e+03 - logprior: -4.3032e+00
Epoch 2/10
22/22 - 17s - loss: 1448.1132 - loglik: -1.4498e+03 - logprior: 1.6398
Epoch 3/10
22/22 - 18s - loss: 1445.3462 - loglik: -1.4476e+03 - logprior: 2.2806
Epoch 4/10
22/22 - 17s - loss: 1441.3917 - loglik: -1.4439e+03 - logprior: 2.5112
Epoch 5/10
22/22 - 17s - loss: 1443.0370 - loglik: -1.4457e+03 - logprior: 2.7064
Fitted a model with MAP estimate = -1440.5939
Time for alignment: 348.3107
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 19s - loss: 1628.3615 - loglik: -1.6197e+03 - logprior: -8.6960e+00
Epoch 2/10
22/22 - 13s - loss: 1523.7281 - loglik: -1.5229e+03 - logprior: -8.3429e-01
Epoch 3/10
22/22 - 13s - loss: 1471.1600 - loglik: -1.4694e+03 - logprior: -1.7132e+00
Epoch 4/10
22/22 - 13s - loss: 1473.2446 - loglik: -1.4717e+03 - logprior: -1.5677e+00
Fitted a model with MAP estimate = -1469.1926
expansions: [(12, 1), (13, 1), (32, 1), (33, 2), (34, 3), (36, 1), (46, 2), (47, 1), (48, 1), (49, 1), (62, 1), (70, 1), (71, 2), (72, 1), (76, 2), (79, 1), (81, 1), (97, 1), (98, 1), (105, 1), (107, 1), (108, 1), (120, 2), (136, 1), (137, 1), (143, 1), (148, 1), (149, 1), (155, 2), (156, 1), (175, 1), (178, 2), (179, 1), (180, 1), (183, 2), (184, 2), (193, 2), (194, 1), (205, 1), (206, 1), (207, 1), (208, 1), (212, 2), (223, 2), (225, 1), (236, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 1471.6177 - loglik: -1.4625e+03 - logprior: -9.1072e+00
Epoch 2/2
22/22 - 18s - loss: 1451.6841 - loglik: -1.4494e+03 - logprior: -2.2865e+00
Fitted a model with MAP estimate = -1448.1696
expansions: [(0, 3), (103, 1), (130, 1)]
discards: [  0  39  58 279 280]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 1460.3407 - loglik: -1.4554e+03 - logprior: -4.9472e+00
Epoch 2/2
22/22 - 18s - loss: 1443.8209 - loglik: -1.4449e+03 - logprior: 1.0953
Fitted a model with MAP estimate = -1445.5966
expansions: []
discards: [  0   1   2 228]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 23s - loss: 1459.2626 - loglik: -1.4519e+03 - logprior: -7.3767e+00
Epoch 2/10
22/22 - 17s - loss: 1451.7000 - loglik: -1.4503e+03 - logprior: -1.4243e+00
Epoch 3/10
22/22 - 17s - loss: 1451.5767 - loglik: -1.4520e+03 - logprior: 0.4180
Epoch 4/10
22/22 - 17s - loss: 1442.3962 - loglik: -1.4449e+03 - logprior: 2.5145
Epoch 5/10
22/22 - 17s - loss: 1444.9091 - loglik: -1.4476e+03 - logprior: 2.6587
Fitted a model with MAP estimate = -1441.3696
Time for alignment: 281.2930
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 17s - loss: 1630.2262 - loglik: -1.6215e+03 - logprior: -8.7001e+00
Epoch 2/10
22/22 - 13s - loss: 1516.1809 - loglik: -1.5154e+03 - logprior: -8.1891e-01
Epoch 3/10
22/22 - 13s - loss: 1472.3187 - loglik: -1.4706e+03 - logprior: -1.7075e+00
Epoch 4/10
22/22 - 13s - loss: 1472.8313 - loglik: -1.4712e+03 - logprior: -1.5865e+00
Fitted a model with MAP estimate = -1469.8798
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 2), (35, 1), (46, 2), (49, 1), (58, 1), (71, 2), (72, 3), (73, 1), (77, 2), (80, 1), (82, 1), (98, 1), (99, 1), (104, 1), (105, 2), (107, 1), (108, 1), (120, 2), (136, 1), (144, 1), (149, 1), (150, 1), (152, 1), (159, 1), (177, 1), (179, 3), (180, 1), (181, 1), (184, 3), (185, 2), (193, 2), (206, 1), (207, 1), (208, 1), (209, 1), (213, 1), (214, 1), (224, 1), (225, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 1473.3649 - loglik: -1.4644e+03 - logprior: -8.9942e+00
Epoch 2/2
22/22 - 18s - loss: 1454.5952 - loglik: -1.4524e+03 - logprior: -2.1956e+00
Fitted a model with MAP estimate = -1448.3686
expansions: [(0, 3), (104, 1)]
discards: [  0  34  55  86 218 229]
Re-initialized the encoder parameters.
Fitting a model of length 311 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 1455.6903 - loglik: -1.4507e+03 - logprior: -5.0292e+00
Epoch 2/2
22/22 - 18s - loss: 1449.0503 - loglik: -1.4501e+03 - logprior: 1.0218
Fitted a model with MAP estimate = -1445.1280
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 309 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 23s - loss: 1455.0266 - loglik: -1.4476e+03 - logprior: -7.4143e+00
Epoch 2/10
22/22 - 17s - loss: 1453.9435 - loglik: -1.4534e+03 - logprior: -5.8095e-01
Epoch 3/10
22/22 - 17s - loss: 1440.9126 - loglik: -1.4432e+03 - logprior: 2.2531
Epoch 4/10
22/22 - 18s - loss: 1443.9753 - loglik: -1.4466e+03 - logprior: 2.6525
Fitted a model with MAP estimate = -1441.2138
Time for alignment: 261.1587
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 17s - loss: 1629.5309 - loglik: -1.6208e+03 - logprior: -8.6951e+00
Epoch 2/10
22/22 - 13s - loss: 1520.3374 - loglik: -1.5195e+03 - logprior: -8.7620e-01
Epoch 3/10
22/22 - 13s - loss: 1480.9196 - loglik: -1.4791e+03 - logprior: -1.7793e+00
Epoch 4/10
22/22 - 13s - loss: 1471.0433 - loglik: -1.4695e+03 - logprior: -1.4956e+00
Epoch 5/10
22/22 - 13s - loss: 1467.2372 - loglik: -1.4657e+03 - logprior: -1.5199e+00
Epoch 6/10
22/22 - 13s - loss: 1473.3215 - loglik: -1.4717e+03 - logprior: -1.6141e+00
Fitted a model with MAP estimate = -1469.2216
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 3), (35, 2), (45, 1), (48, 1), (49, 1), (61, 1), (65, 1), (69, 1), (70, 1), (75, 1), (76, 2), (80, 1), (81, 1), (97, 1), (99, 1), (105, 1), (106, 1), (107, 1), (108, 2), (120, 2), (131, 1), (143, 1), (144, 2), (148, 1), (152, 1), (155, 2), (156, 1), (158, 1), (178, 2), (179, 4), (183, 1), (184, 2), (193, 2), (194, 1), (205, 1), (206, 1), (207, 1), (208, 1), (212, 2), (225, 1), (236, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 314 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 1472.3088 - loglik: -1.4633e+03 - logprior: -9.0504e+00
Epoch 2/2
22/22 - 18s - loss: 1453.9233 - loglik: -1.4517e+03 - logprior: -2.2347e+00
Fitted a model with MAP estimate = -1449.3441
expansions: [(0, 3), (103, 1)]
discards: [  0  34  40 148 177 223]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 1457.6910 - loglik: -1.4529e+03 - logprior: -4.7848e+00
Epoch 2/2
22/22 - 18s - loss: 1450.0919 - loglik: -1.4513e+03 - logprior: 1.1850
Fitted a model with MAP estimate = -1445.1485
expansions: []
discards: [  0   1   2 228]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 23s - loss: 1459.8732 - loglik: -1.4526e+03 - logprior: -7.3081e+00
Epoch 2/10
22/22 - 17s - loss: 1451.0829 - loglik: -1.4498e+03 - logprior: -1.2674e+00
Epoch 3/10
22/22 - 17s - loss: 1450.8516 - loglik: -1.4512e+03 - logprior: 0.3845
Epoch 4/10
22/22 - 17s - loss: 1441.9377 - loglik: -1.4446e+03 - logprior: 2.6897
Epoch 5/10
22/22 - 17s - loss: 1444.5800 - loglik: -1.4474e+03 - logprior: 2.8676
Fitted a model with MAP estimate = -1441.2999
Time for alignment: 305.7582
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 17s - loss: 1628.3207 - loglik: -1.6196e+03 - logprior: -8.7015e+00
Epoch 2/10
22/22 - 13s - loss: 1520.6672 - loglik: -1.5198e+03 - logprior: -9.0894e-01
Epoch 3/10
22/22 - 13s - loss: 1478.9290 - loglik: -1.4770e+03 - logprior: -1.9198e+00
Epoch 4/10
22/22 - 13s - loss: 1467.8196 - loglik: -1.4661e+03 - logprior: -1.7495e+00
Epoch 5/10
22/22 - 13s - loss: 1471.1489 - loglik: -1.4693e+03 - logprior: -1.8002e+00
Fitted a model with MAP estimate = -1468.6251
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 2), (36, 1), (45, 2), (48, 1), (49, 1), (50, 1), (65, 1), (69, 1), (70, 1), (75, 1), (76, 2), (77, 1), (81, 1), (93, 1), (98, 1), (103, 1), (104, 2), (109, 1), (120, 2), (143, 2), (144, 1), (149, 1), (150, 1), (152, 1), (156, 1), (157, 3), (158, 1), (175, 1), (178, 3), (179, 1), (180, 1), (183, 1), (184, 1), (185, 2), (193, 2), (206, 1), (209, 1), (210, 1), (213, 1), (214, 2), (224, 1), (225, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 315 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 1471.6571 - loglik: -1.4626e+03 - logprior: -9.0756e+00
Epoch 2/2
22/22 - 18s - loss: 1446.5879 - loglik: -1.4444e+03 - logprior: -2.1844e+00
Fitted a model with MAP estimate = -1447.8311
expansions: [(0, 3)]
discards: [  0  34 173 193 194 195 220 231]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 1454.3486 - loglik: -1.4494e+03 - logprior: -4.9330e+00
Epoch 2/2
22/22 - 18s - loss: 1449.1138 - loglik: -1.4502e+03 - logprior: 1.1251
Fitted a model with MAP estimate = -1444.9882
expansions: []
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 23s - loss: 1459.3984 - loglik: -1.4520e+03 - logprior: -7.3626e+00
Epoch 2/10
22/22 - 17s - loss: 1451.6678 - loglik: -1.4503e+03 - logprior: -1.3377e+00
Epoch 3/10
22/22 - 17s - loss: 1446.0000 - loglik: -1.4464e+03 - logprior: 0.3545
Epoch 4/10
22/22 - 17s - loss: 1442.6984 - loglik: -1.4453e+03 - logprior: 2.6052
Epoch 5/10
22/22 - 17s - loss: 1442.4718 - loglik: -1.4452e+03 - logprior: 2.7534
Epoch 6/10
22/22 - 17s - loss: 1441.7012 - loglik: -1.4446e+03 - logprior: 2.8527
Epoch 7/10
22/22 - 17s - loss: 1442.1279 - loglik: -1.4451e+03 - logprior: 2.9651
Fitted a model with MAP estimate = -1439.8216
Time for alignment: 325.4681
Computed alignments with likelihoods: ['-1440.5939', '-1441.3696', '-1441.2138', '-1441.2999', '-1439.8216']
Best model has likelihood: -1439.8216  (prior= 2.9997 )
time for generating output: 0.3520
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/asp.projection.fasta
SP score = 0.9181693769119305
Training of 5 independent models on file bowman.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52864ccc70>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52962b6580>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5297beaa60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f528796d730>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52841765e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5284195b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531c49cb80>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52886dfeb0>, <__main__.SimpleDirichletPrior object at 0x7f531cd15340>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 259.8321 - loglik: -1.6470e+02 - logprior: -9.5131e+01
Epoch 2/10
10/10 - 1s - loss: 180.4184 - loglik: -1.5322e+02 - logprior: -2.7198e+01
Epoch 3/10
10/10 - 1s - loss: 158.2193 - loglik: -1.4471e+02 - logprior: -1.3514e+01
Epoch 4/10
10/10 - 1s - loss: 148.8482 - loglik: -1.4053e+02 - logprior: -8.3134e+00
Epoch 5/10
10/10 - 1s - loss: 143.9615 - loglik: -1.3830e+02 - logprior: -5.6618e+00
Epoch 6/10
10/10 - 1s - loss: 141.8263 - loglik: -1.3761e+02 - logprior: -4.2127e+00
Epoch 7/10
10/10 - 1s - loss: 140.6910 - loglik: -1.3730e+02 - logprior: -3.3878e+00
Epoch 8/10
10/10 - 1s - loss: 140.0438 - loglik: -1.3717e+02 - logprior: -2.8736e+00
Epoch 9/10
10/10 - 1s - loss: 139.7186 - loglik: -1.3720e+02 - logprior: -2.5143e+00
Epoch 10/10
10/10 - 1s - loss: 139.4933 - loglik: -1.3728e+02 - logprior: -2.2147e+00
Fitted a model with MAP estimate = -139.3980
expansions: [(0, 3), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 259.1207 - loglik: -1.3424e+02 - logprior: -1.2489e+02
Epoch 2/2
10/10 - 1s - loss: 172.4188 - loglik: -1.3206e+02 - logprior: -4.0354e+01
Fitted a model with MAP estimate = -155.5085
expansions: [(0, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 232.9153 - loglik: -1.2973e+02 - logprior: -1.0319e+02
Epoch 2/2
10/10 - 1s - loss: 165.7385 - loglik: -1.2919e+02 - logprior: -3.6549e+01
Fitted a model with MAP estimate = -151.4037
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 213.6413 - loglik: -1.2784e+02 - logprior: -8.5800e+01
Epoch 2/10
10/10 - 1s - loss: 153.2289 - loglik: -1.2854e+02 - logprior: -2.4685e+01
Epoch 3/10
10/10 - 1s - loss: 141.2109 - loglik: -1.2913e+02 - logprior: -1.2081e+01
Epoch 4/10
10/10 - 1s - loss: 136.4168 - loglik: -1.2946e+02 - logprior: -6.9573e+00
Epoch 5/10
10/10 - 1s - loss: 133.8503 - loglik: -1.2959e+02 - logprior: -4.2558e+00
Epoch 6/10
10/10 - 1s - loss: 132.4237 - loglik: -1.2962e+02 - logprior: -2.8004e+00
Epoch 7/10
10/10 - 1s - loss: 131.4619 - loglik: -1.2951e+02 - logprior: -1.9560e+00
Epoch 8/10
10/10 - 1s - loss: 130.3834 - loglik: -1.2901e+02 - logprior: -1.3777e+00
Epoch 9/10
10/10 - 1s - loss: 129.8853 - loglik: -1.2897e+02 - logprior: -9.1646e-01
Epoch 10/10
10/10 - 1s - loss: 129.5577 - loglik: -1.2904e+02 - logprior: -5.1873e-01
Fitted a model with MAP estimate = -129.4002
Time for alignment: 32.1252
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 259.8321 - loglik: -1.6470e+02 - logprior: -9.5131e+01
Epoch 2/10
10/10 - 1s - loss: 180.4184 - loglik: -1.5322e+02 - logprior: -2.7198e+01
Epoch 3/10
10/10 - 1s - loss: 158.2193 - loglik: -1.4471e+02 - logprior: -1.3514e+01
Epoch 4/10
10/10 - 1s - loss: 148.8482 - loglik: -1.4053e+02 - logprior: -8.3134e+00
Epoch 5/10
10/10 - 1s - loss: 143.9615 - loglik: -1.3830e+02 - logprior: -5.6618e+00
Epoch 6/10
10/10 - 1s - loss: 141.8263 - loglik: -1.3761e+02 - logprior: -4.2127e+00
Epoch 7/10
10/10 - 1s - loss: 140.6910 - loglik: -1.3730e+02 - logprior: -3.3878e+00
Epoch 8/10
10/10 - 1s - loss: 140.0438 - loglik: -1.3717e+02 - logprior: -2.8736e+00
Epoch 9/10
10/10 - 1s - loss: 139.7186 - loglik: -1.3720e+02 - logprior: -2.5143e+00
Epoch 10/10
10/10 - 1s - loss: 139.4934 - loglik: -1.3728e+02 - logprior: -2.2147e+00
Fitted a model with MAP estimate = -139.3980
expansions: [(0, 3), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 259.1207 - loglik: -1.3424e+02 - logprior: -1.2489e+02
Epoch 2/2
10/10 - 1s - loss: 172.4188 - loglik: -1.3206e+02 - logprior: -4.0354e+01
Fitted a model with MAP estimate = -155.5085
expansions: [(0, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 232.9153 - loglik: -1.2973e+02 - logprior: -1.0319e+02
Epoch 2/2
10/10 - 1s - loss: 165.7385 - loglik: -1.2919e+02 - logprior: -3.6549e+01
Fitted a model with MAP estimate = -151.4037
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 213.6413 - loglik: -1.2784e+02 - logprior: -8.5800e+01
Epoch 2/10
10/10 - 1s - loss: 153.2289 - loglik: -1.2854e+02 - logprior: -2.4685e+01
Epoch 3/10
10/10 - 1s - loss: 141.2110 - loglik: -1.2913e+02 - logprior: -1.2081e+01
Epoch 4/10
10/10 - 1s - loss: 136.4168 - loglik: -1.2946e+02 - logprior: -6.9573e+00
Epoch 5/10
10/10 - 1s - loss: 133.8503 - loglik: -1.2959e+02 - logprior: -4.2558e+00
Epoch 6/10
10/10 - 1s - loss: 132.4237 - loglik: -1.2962e+02 - logprior: -2.8004e+00
Epoch 7/10
10/10 - 1s - loss: 131.4619 - loglik: -1.2951e+02 - logprior: -1.9560e+00
Epoch 8/10
10/10 - 1s - loss: 130.3834 - loglik: -1.2901e+02 - logprior: -1.3777e+00
Epoch 9/10
10/10 - 1s - loss: 129.8852 - loglik: -1.2897e+02 - logprior: -9.1646e-01
Epoch 10/10
10/10 - 1s - loss: 129.5577 - loglik: -1.2904e+02 - logprior: -5.1873e-01
Fitted a model with MAP estimate = -129.4002
Time for alignment: 36.3326
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 259.8321 - loglik: -1.6470e+02 - logprior: -9.5131e+01
Epoch 2/10
10/10 - 1s - loss: 180.4184 - loglik: -1.5322e+02 - logprior: -2.7198e+01
Epoch 3/10
10/10 - 1s - loss: 158.2193 - loglik: -1.4471e+02 - logprior: -1.3514e+01
Epoch 4/10
10/10 - 1s - loss: 148.8482 - loglik: -1.4053e+02 - logprior: -8.3134e+00
Epoch 5/10
10/10 - 1s - loss: 143.9615 - loglik: -1.3830e+02 - logprior: -5.6618e+00
Epoch 6/10
10/10 - 1s - loss: 141.8263 - loglik: -1.3761e+02 - logprior: -4.2127e+00
Epoch 7/10
10/10 - 1s - loss: 140.6910 - loglik: -1.3730e+02 - logprior: -3.3878e+00
Epoch 8/10
10/10 - 1s - loss: 140.0438 - loglik: -1.3717e+02 - logprior: -2.8736e+00
Epoch 9/10
10/10 - 1s - loss: 139.7186 - loglik: -1.3720e+02 - logprior: -2.5143e+00
Epoch 10/10
10/10 - 1s - loss: 139.4933 - loglik: -1.3728e+02 - logprior: -2.2147e+00
Fitted a model with MAP estimate = -139.3979
expansions: [(0, 3), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 259.1207 - loglik: -1.3424e+02 - logprior: -1.2489e+02
Epoch 2/2
10/10 - 1s - loss: 172.4188 - loglik: -1.3206e+02 - logprior: -4.0354e+01
Fitted a model with MAP estimate = -155.5085
expansions: [(0, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 232.9153 - loglik: -1.2973e+02 - logprior: -1.0319e+02
Epoch 2/2
10/10 - 1s - loss: 165.7385 - loglik: -1.2919e+02 - logprior: -3.6549e+01
Fitted a model with MAP estimate = -151.4037
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 213.6413 - loglik: -1.2784e+02 - logprior: -8.5800e+01
Epoch 2/10
10/10 - 1s - loss: 153.2289 - loglik: -1.2854e+02 - logprior: -2.4685e+01
Epoch 3/10
10/10 - 1s - loss: 141.2109 - loglik: -1.2913e+02 - logprior: -1.2081e+01
Epoch 4/10
10/10 - 1s - loss: 136.4168 - loglik: -1.2946e+02 - logprior: -6.9573e+00
Epoch 5/10
10/10 - 1s - loss: 133.8503 - loglik: -1.2959e+02 - logprior: -4.2558e+00
Epoch 6/10
10/10 - 1s - loss: 132.4237 - loglik: -1.2962e+02 - logprior: -2.8004e+00
Epoch 7/10
10/10 - 1s - loss: 131.4618 - loglik: -1.2951e+02 - logprior: -1.9560e+00
Epoch 8/10
10/10 - 1s - loss: 130.3834 - loglik: -1.2901e+02 - logprior: -1.3777e+00
Epoch 9/10
10/10 - 1s - loss: 129.8852 - loglik: -1.2897e+02 - logprior: -9.1646e-01
Epoch 10/10
10/10 - 1s - loss: 129.5577 - loglik: -1.2904e+02 - logprior: -5.1873e-01
Fitted a model with MAP estimate = -129.4001
Time for alignment: 35.9011
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 259.8321 - loglik: -1.6470e+02 - logprior: -9.5131e+01
Epoch 2/10
10/10 - 1s - loss: 180.4184 - loglik: -1.5322e+02 - logprior: -2.7198e+01
Epoch 3/10
10/10 - 1s - loss: 158.2193 - loglik: -1.4471e+02 - logprior: -1.3514e+01
Epoch 4/10
10/10 - 1s - loss: 148.8482 - loglik: -1.4053e+02 - logprior: -8.3134e+00
Epoch 5/10
10/10 - 1s - loss: 143.9615 - loglik: -1.3830e+02 - logprior: -5.6618e+00
Epoch 6/10
10/10 - 1s - loss: 141.8264 - loglik: -1.3761e+02 - logprior: -4.2127e+00
Epoch 7/10
10/10 - 1s - loss: 140.6910 - loglik: -1.3730e+02 - logprior: -3.3878e+00
Epoch 8/10
10/10 - 1s - loss: 140.0438 - loglik: -1.3717e+02 - logprior: -2.8736e+00
Epoch 9/10
10/10 - 1s - loss: 139.7186 - loglik: -1.3720e+02 - logprior: -2.5143e+00
Epoch 10/10
10/10 - 1s - loss: 139.4934 - loglik: -1.3728e+02 - logprior: -2.2147e+00
Fitted a model with MAP estimate = -139.3979
expansions: [(0, 3), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 259.1207 - loglik: -1.3424e+02 - logprior: -1.2489e+02
Epoch 2/2
10/10 - 1s - loss: 172.4188 - loglik: -1.3206e+02 - logprior: -4.0354e+01
Fitted a model with MAP estimate = -155.5085
expansions: [(0, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 232.9153 - loglik: -1.2973e+02 - logprior: -1.0319e+02
Epoch 2/2
10/10 - 1s - loss: 165.7385 - loglik: -1.2919e+02 - logprior: -3.6549e+01
Fitted a model with MAP estimate = -151.4037
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 213.6413 - loglik: -1.2784e+02 - logprior: -8.5800e+01
Epoch 2/10
10/10 - 1s - loss: 153.2289 - loglik: -1.2854e+02 - logprior: -2.4685e+01
Epoch 3/10
10/10 - 1s - loss: 141.2109 - loglik: -1.2913e+02 - logprior: -1.2081e+01
Epoch 4/10
10/10 - 1s - loss: 136.4168 - loglik: -1.2946e+02 - logprior: -6.9573e+00
Epoch 5/10
10/10 - 1s - loss: 133.8503 - loglik: -1.2959e+02 - logprior: -4.2558e+00
Epoch 6/10
10/10 - 1s - loss: 132.4237 - loglik: -1.2962e+02 - logprior: -2.8004e+00
Epoch 7/10
10/10 - 1s - loss: 131.4618 - loglik: -1.2951e+02 - logprior: -1.9560e+00
Epoch 8/10
10/10 - 1s - loss: 130.3834 - loglik: -1.2901e+02 - logprior: -1.3777e+00
Epoch 9/10
10/10 - 1s - loss: 129.8852 - loglik: -1.2897e+02 - logprior: -9.1646e-01
Epoch 10/10
10/10 - 1s - loss: 129.5577 - loglik: -1.2904e+02 - logprior: -5.1873e-01
Fitted a model with MAP estimate = -129.4002
Time for alignment: 35.4305
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 259.8321 - loglik: -1.6470e+02 - logprior: -9.5131e+01
Epoch 2/10
10/10 - 1s - loss: 180.4184 - loglik: -1.5322e+02 - logprior: -2.7198e+01
Epoch 3/10
10/10 - 1s - loss: 158.2193 - loglik: -1.4471e+02 - logprior: -1.3514e+01
Epoch 4/10
10/10 - 1s - loss: 148.8482 - loglik: -1.4053e+02 - logprior: -8.3134e+00
Epoch 5/10
10/10 - 1s - loss: 143.9615 - loglik: -1.3830e+02 - logprior: -5.6618e+00
Epoch 6/10
10/10 - 1s - loss: 141.8263 - loglik: -1.3761e+02 - logprior: -4.2127e+00
Epoch 7/10
10/10 - 1s - loss: 140.6910 - loglik: -1.3730e+02 - logprior: -3.3878e+00
Epoch 8/10
10/10 - 1s - loss: 140.0438 - loglik: -1.3717e+02 - logprior: -2.8736e+00
Epoch 9/10
10/10 - 1s - loss: 139.7186 - loglik: -1.3720e+02 - logprior: -2.5143e+00
Epoch 10/10
10/10 - 1s - loss: 139.4934 - loglik: -1.3728e+02 - logprior: -2.2147e+00
Fitted a model with MAP estimate = -139.3980
expansions: [(0, 3), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 259.1207 - loglik: -1.3424e+02 - logprior: -1.2489e+02
Epoch 2/2
10/10 - 1s - loss: 172.4188 - loglik: -1.3206e+02 - logprior: -4.0354e+01
Fitted a model with MAP estimate = -155.5085
expansions: [(0, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 232.9153 - loglik: -1.2973e+02 - logprior: -1.0319e+02
Epoch 2/2
10/10 - 1s - loss: 165.7385 - loglik: -1.2919e+02 - logprior: -3.6549e+01
Fitted a model with MAP estimate = -151.4037
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 213.6413 - loglik: -1.2784e+02 - logprior: -8.5800e+01
Epoch 2/10
10/10 - 1s - loss: 153.2289 - loglik: -1.2854e+02 - logprior: -2.4685e+01
Epoch 3/10
10/10 - 1s - loss: 141.2109 - loglik: -1.2913e+02 - logprior: -1.2081e+01
Epoch 4/10
10/10 - 1s - loss: 136.4168 - loglik: -1.2946e+02 - logprior: -6.9573e+00
Epoch 5/10
10/10 - 1s - loss: 133.8503 - loglik: -1.2959e+02 - logprior: -4.2557e+00
Epoch 6/10
10/10 - 1s - loss: 132.4237 - loglik: -1.2962e+02 - logprior: -2.8004e+00
Epoch 7/10
10/10 - 1s - loss: 131.4619 - loglik: -1.2951e+02 - logprior: -1.9560e+00
Epoch 8/10
10/10 - 1s - loss: 130.3834 - loglik: -1.2901e+02 - logprior: -1.3777e+00
Epoch 9/10
10/10 - 1s - loss: 129.8852 - loglik: -1.2897e+02 - logprior: -9.1646e-01
Epoch 10/10
10/10 - 1s - loss: 129.5576 - loglik: -1.2904e+02 - logprior: -5.1872e-01
Fitted a model with MAP estimate = -129.4001
Time for alignment: 34.9144
Computed alignments with likelihoods: ['-129.4002', '-129.4002', '-129.4001', '-129.4002', '-129.4001']
Best model has likelihood: -129.4001  (prior= -0.3329 )
time for generating output: 0.1433
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/bowman.projection.fasta
SP score = 0.9418604651162791
Training of 5 independent models on file oxidored_q6.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52854ceeb0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f530dcb5a30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531c9033a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531c903cd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52877336d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52d8c80070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52841a5100>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52d91264c0>, <__main__.SimpleDirichletPrior object at 0x7f52881367c0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 727.6067 - loglik: -7.1358e+02 - logprior: -1.4025e+01
Epoch 2/10
11/11 - 3s - loss: 671.4882 - loglik: -6.6821e+02 - logprior: -3.2826e+00
Epoch 3/10
11/11 - 2s - loss: 632.7292 - loglik: -6.3069e+02 - logprior: -2.0441e+00
Epoch 4/10
11/11 - 2s - loss: 611.5121 - loglik: -6.0944e+02 - logprior: -2.0703e+00
Epoch 5/10
11/11 - 3s - loss: 607.7156 - loglik: -6.0562e+02 - logprior: -2.0972e+00
Epoch 6/10
11/11 - 3s - loss: 604.6168 - loglik: -6.0254e+02 - logprior: -2.0747e+00
Epoch 7/10
11/11 - 2s - loss: 602.0427 - loglik: -6.0002e+02 - logprior: -2.0261e+00
Epoch 8/10
11/11 - 2s - loss: 602.8135 - loglik: -6.0075e+02 - logprior: -2.0627e+00
Fitted a model with MAP estimate = -602.0462
expansions: [(8, 2), (9, 3), (10, 1), (12, 2), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 3), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 618.3915 - loglik: -6.0359e+02 - logprior: -1.4805e+01
Epoch 2/2
11/11 - 3s - loss: 589.7519 - loglik: -5.8362e+02 - logprior: -6.1270e+00
Fitted a model with MAP estimate = -587.4552
expansions: [(0, 19)]
discards: [  0   8  76  77  84  87 110]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 597.6096 - loglik: -5.8549e+02 - logprior: -1.2120e+01
Epoch 2/2
11/11 - 3s - loss: 580.5549 - loglik: -5.7714e+02 - logprior: -3.4125e+00
Fitted a model with MAP estimate = -578.7478
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 595.5683 - loglik: -5.8349e+02 - logprior: -1.2076e+01
Epoch 2/10
11/11 - 3s - loss: 579.7317 - loglik: -5.7689e+02 - logprior: -2.8368e+00
Epoch 3/10
11/11 - 3s - loss: 578.1572 - loglik: -5.7695e+02 - logprior: -1.2084e+00
Epoch 4/10
11/11 - 3s - loss: 576.8657 - loglik: -5.7614e+02 - logprior: -7.2832e-01
Epoch 5/10
11/11 - 3s - loss: 577.3099 - loglik: -5.7675e+02 - logprior: -5.6386e-01
Fitted a model with MAP estimate = -576.0328
Time for alignment: 69.3878
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 9s - loss: 728.0475 - loglik: -7.1402e+02 - logprior: -1.4024e+01
Epoch 2/10
11/11 - 2s - loss: 672.8597 - loglik: -6.6958e+02 - logprior: -3.2807e+00
Epoch 3/10
11/11 - 3s - loss: 630.0947 - loglik: -6.2806e+02 - logprior: -2.0384e+00
Epoch 4/10
11/11 - 2s - loss: 613.7612 - loglik: -6.1168e+02 - logprior: -2.0812e+00
Epoch 5/10
11/11 - 3s - loss: 606.6835 - loglik: -6.0455e+02 - logprior: -2.1312e+00
Epoch 6/10
11/11 - 2s - loss: 605.2849 - loglik: -6.0313e+02 - logprior: -2.1517e+00
Epoch 7/10
11/11 - 2s - loss: 602.9576 - loglik: -6.0083e+02 - logprior: -2.1291e+00
Epoch 8/10
11/11 - 2s - loss: 601.8640 - loglik: -5.9969e+02 - logprior: -2.1744e+00
Epoch 9/10
11/11 - 3s - loss: 601.0661 - loglik: -5.9880e+02 - logprior: -2.2649e+00
Epoch 10/10
11/11 - 3s - loss: 601.1658 - loglik: -5.9887e+02 - logprior: -2.2912e+00
Fitted a model with MAP estimate = -601.0380
expansions: [(8, 2), (9, 3), (10, 1), (12, 2), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (63, 2), (65, 1), (66, 1), (67, 1), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 617.2117 - loglik: -6.0238e+02 - logprior: -1.4833e+01
Epoch 2/2
11/11 - 3s - loss: 591.5623 - loglik: -5.8552e+02 - logprior: -6.0436e+00
Fitted a model with MAP estimate = -587.6125
expansions: [(0, 19)]
discards: [  0   7  76  78 108]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 595.2791 - loglik: -5.8319e+02 - logprior: -1.2093e+01
Epoch 2/2
11/11 - 3s - loss: 582.2068 - loglik: -5.7891e+02 - logprior: -3.2990e+00
Fitted a model with MAP estimate = -577.7725
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 594.1741 - loglik: -5.8166e+02 - logprior: -1.2518e+01
Epoch 2/10
11/11 - 3s - loss: 582.0497 - loglik: -5.7910e+02 - logprior: -2.9466e+00
Epoch 3/10
11/11 - 3s - loss: 581.3988 - loglik: -5.8020e+02 - logprior: -1.1955e+00
Epoch 4/10
11/11 - 3s - loss: 577.2654 - loglik: -5.7659e+02 - logprior: -6.7614e-01
Epoch 5/10
11/11 - 3s - loss: 578.8073 - loglik: -5.7835e+02 - logprior: -4.5949e-01
Fitted a model with MAP estimate = -577.5081
Time for alignment: 78.5908
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 8s - loss: 728.4862 - loglik: -7.1446e+02 - logprior: -1.4023e+01
Epoch 2/10
11/11 - 3s - loss: 671.4707 - loglik: -6.6819e+02 - logprior: -3.2834e+00
Epoch 3/10
11/11 - 3s - loss: 631.5470 - loglik: -6.2948e+02 - logprior: -2.0713e+00
Epoch 4/10
11/11 - 2s - loss: 612.9610 - loglik: -6.1081e+02 - logprior: -2.1553e+00
Epoch 5/10
11/11 - 2s - loss: 607.3243 - loglik: -6.0509e+02 - logprior: -2.2370e+00
Epoch 6/10
11/11 - 2s - loss: 603.8121 - loglik: -6.0151e+02 - logprior: -2.2993e+00
Epoch 7/10
11/11 - 2s - loss: 601.7578 - loglik: -5.9950e+02 - logprior: -2.2548e+00
Epoch 8/10
11/11 - 2s - loss: 602.6513 - loglik: -6.0035e+02 - logprior: -2.2971e+00
Fitted a model with MAP estimate = -600.8019
expansions: [(8, 2), (9, 3), (10, 2), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (58, 2), (61, 1), (62, 2), (63, 2), (65, 1), (66, 1), (67, 1), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 617.8697 - loglik: -6.0308e+02 - logprior: -1.4785e+01
Epoch 2/2
11/11 - 3s - loss: 590.9539 - loglik: -5.8492e+02 - logprior: -6.0375e+00
Fitted a model with MAP estimate = -587.6614
expansions: [(0, 20)]
discards: [ 0  8 14 78 80]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 3348 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 8s - loss: 588.7786 - loglik: -5.8098e+02 - logprior: -7.7984e+00
Epoch 2/2
22/22 - 5s - loss: 578.1684 - loglik: -5.7637e+02 - logprior: -1.8023e+00
Fitted a model with MAP estimate = -575.7054
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 87]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 592.6768 - loglik: -5.8080e+02 - logprior: -1.1881e+01
Epoch 2/10
11/11 - 3s - loss: 582.2573 - loglik: -5.7951e+02 - logprior: -2.7482e+00
Epoch 3/10
11/11 - 3s - loss: 580.4526 - loglik: -5.7929e+02 - logprior: -1.1649e+00
Epoch 4/10
11/11 - 3s - loss: 580.3256 - loglik: -5.7965e+02 - logprior: -6.7767e-01
Epoch 5/10
11/11 - 3s - loss: 576.8116 - loglik: -5.7633e+02 - logprior: -4.8468e-01
Epoch 6/10
11/11 - 3s - loss: 577.0573 - loglik: -5.7673e+02 - logprior: -3.3199e-01
Fitted a model with MAP estimate = -577.0079
Time for alignment: 79.2437
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 8s - loss: 728.2349 - loglik: -7.1421e+02 - logprior: -1.4021e+01
Epoch 2/10
11/11 - 3s - loss: 670.3572 - loglik: -6.6708e+02 - logprior: -3.2811e+00
Epoch 3/10
11/11 - 2s - loss: 631.2085 - loglik: -6.2915e+02 - logprior: -2.0552e+00
Epoch 4/10
11/11 - 3s - loss: 611.4809 - loglik: -6.0942e+02 - logprior: -2.0567e+00
Epoch 5/10
11/11 - 3s - loss: 606.8643 - loglik: -6.0479e+02 - logprior: -2.0748e+00
Epoch 6/10
11/11 - 2s - loss: 604.6367 - loglik: -6.0259e+02 - logprior: -2.0515e+00
Epoch 7/10
11/11 - 3s - loss: 601.7006 - loglik: -5.9970e+02 - logprior: -2.0007e+00
Epoch 8/10
11/11 - 2s - loss: 602.5543 - loglik: -6.0051e+02 - logprior: -2.0453e+00
Fitted a model with MAP estimate = -602.0247
expansions: [(8, 2), (9, 3), (10, 1), (12, 2), (33, 1), (34, 1), (38, 1), (39, 2), (61, 2), (62, 2), (63, 2), (65, 1), (66, 1), (67, 2), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 615.0424 - loglik: -6.0031e+02 - logprior: -1.4734e+01
Epoch 2/2
11/11 - 3s - loss: 590.6287 - loglik: -5.8456e+02 - logprior: -6.0646e+00
Fitted a model with MAP estimate = -586.4057
expansions: [(0, 19)]
discards: [ 0  7 73 77 79 87]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 596.4111 - loglik: -5.8432e+02 - logprior: -1.2093e+01
Epoch 2/2
11/11 - 3s - loss: 580.9814 - loglik: -5.7763e+02 - logprior: -3.3469e+00
Fitted a model with MAP estimate = -577.2211
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 593.6331 - loglik: -5.8119e+02 - logprior: -1.2447e+01
Epoch 2/10
11/11 - 3s - loss: 581.5605 - loglik: -5.7866e+02 - logprior: -2.9005e+00
Epoch 3/10
11/11 - 3s - loss: 580.3373 - loglik: -5.7919e+02 - logprior: -1.1487e+00
Epoch 4/10
11/11 - 3s - loss: 576.1122 - loglik: -5.7549e+02 - logprior: -6.2364e-01
Epoch 5/10
11/11 - 3s - loss: 577.3859 - loglik: -5.7698e+02 - logprior: -4.1067e-01
Fitted a model with MAP estimate = -576.5180
Time for alignment: 72.1915
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 726.2654 - loglik: -7.1224e+02 - logprior: -1.4022e+01
Epoch 2/10
11/11 - 3s - loss: 673.3497 - loglik: -6.7007e+02 - logprior: -3.2809e+00
Epoch 3/10
11/11 - 2s - loss: 631.4091 - loglik: -6.2935e+02 - logprior: -2.0633e+00
Epoch 4/10
11/11 - 2s - loss: 611.4628 - loglik: -6.0933e+02 - logprior: -2.1357e+00
Epoch 5/10
11/11 - 2s - loss: 606.9815 - loglik: -6.0483e+02 - logprior: -2.1475e+00
Epoch 6/10
11/11 - 2s - loss: 602.1089 - loglik: -5.9999e+02 - logprior: -2.1188e+00
Epoch 7/10
11/11 - 2s - loss: 602.2471 - loglik: -6.0015e+02 - logprior: -2.0935e+00
Fitted a model with MAP estimate = -601.4791
expansions: [(8, 2), (9, 3), (10, 1), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (63, 1), (64, 2), (65, 2), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 8s - loss: 614.3359 - loglik: -5.9963e+02 - logprior: -1.4701e+01
Epoch 2/2
11/11 - 3s - loss: 592.6431 - loglik: -5.8658e+02 - logprior: -6.0592e+00
Fitted a model with MAP estimate = -587.3584
expansions: [(0, 18), (4, 1)]
discards: [  0   7  76  83 108]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 595.5278 - loglik: -5.8347e+02 - logprior: -1.2060e+01
Epoch 2/2
11/11 - 3s - loss: 579.8496 - loglik: -5.7659e+02 - logprior: -3.2604e+00
Fitted a model with MAP estimate = -576.2350
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 591.3972 - loglik: -5.7959e+02 - logprior: -1.1806e+01
Epoch 2/10
11/11 - 3s - loss: 583.7382 - loglik: -5.8103e+02 - logprior: -2.7100e+00
Epoch 3/10
11/11 - 3s - loss: 576.9253 - loglik: -5.7577e+02 - logprior: -1.1582e+00
Epoch 4/10
11/11 - 3s - loss: 578.2983 - loglik: -5.7761e+02 - logprior: -6.8828e-01
Fitted a model with MAP estimate = -577.0071
Time for alignment: 67.0634
Computed alignments with likelihoods: ['-576.0328', '-577.5081', '-575.7054', '-576.5180', '-576.2350']
Best model has likelihood: -575.7054  (prior= -1.5116 )
time for generating output: 0.2882
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/oxidored_q6.projection.fasta
SP score = 0.5352713178294574
Training of 5 independent models on file aldosered.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5297fb0f40>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b08b77e50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b08b6d340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5286206dc0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52867001c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530c214a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f527c380b20>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52762665e0>, <__main__.SimpleDirichletPrior object at 0x7f53002d45e0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1593.5918 - loglik: -1.5914e+03 - logprior: -2.1479e+00
Epoch 2/10
39/39 - 18s - loss: 1477.6154 - loglik: -1.4761e+03 - logprior: -1.5368e+00
Epoch 3/10
39/39 - 18s - loss: 1465.8882 - loglik: -1.4641e+03 - logprior: -1.7971e+00
Epoch 4/10
39/39 - 18s - loss: 1463.3040 - loglik: -1.4614e+03 - logprior: -1.8565e+00
Epoch 5/10
39/39 - 18s - loss: 1462.2487 - loglik: -1.4602e+03 - logprior: -2.0026e+00
Epoch 6/10
39/39 - 18s - loss: 1461.1683 - loglik: -1.4590e+03 - logprior: -2.1916e+00
Epoch 7/10
39/39 - 18s - loss: 1461.1104 - loglik: -1.4587e+03 - logprior: -2.4123e+00
Epoch 8/10
39/39 - 18s - loss: 1460.4166 - loglik: -1.4578e+03 - logprior: -2.6106e+00
Epoch 9/10
39/39 - 18s - loss: 1460.5770 - loglik: -1.4578e+03 - logprior: -2.7364e+00
Fitted a model with MAP estimate = -1357.5253
expansions: [(12, 4), (13, 1), (15, 1), (16, 1), (19, 1), (35, 1), (36, 1), (39, 1), (45, 2), (46, 1), (58, 2), (59, 2), (61, 1), (62, 4), (63, 1), (66, 2), (67, 2), (68, 2), (120, 15), (129, 2), (131, 1), (133, 1), (136, 1), (137, 1), (141, 2), (155, 2), (156, 1), (162, 1), (163, 1), (167, 1), (168, 3), (169, 2), (170, 2), (181, 2), (182, 1), (191, 1), (193, 1), (194, 1), (206, 1), (209, 1), (211, 2), (215, 1), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 1452.3124 - loglik: -1.4491e+03 - logprior: -3.1876e+00
Epoch 2/2
39/39 - 26s - loss: 1433.9980 - loglik: -1.4328e+03 - logprior: -1.1552e+00
Fitted a model with MAP estimate = -1328.3921
expansions: [(0, 2), (157, 3)]
discards: [  0  11  12  74  78  90  95 174 191 210 211 212 246 285]
Re-initialized the encoder parameters.
Fitting a model of length 293 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 1436.3418 - loglik: -1.4347e+03 - logprior: -1.6609e+00
Epoch 2/2
39/39 - 25s - loss: 1431.1019 - loglik: -1.4305e+03 - logprior: -6.3662e-01
Fitted a model with MAP estimate = -1327.9290
expansions: [(206, 2), (209, 1)]
discards: [  0 155 201 202 203]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 32s - loss: 1329.2231 - loglik: -1.3276e+03 - logprior: -1.6624e+00
Epoch 2/10
45/45 - 28s - loss: 1324.5710 - loglik: -1.3239e+03 - logprior: -6.3256e-01
Epoch 3/10
45/45 - 29s - loss: 1324.1757 - loglik: -1.3236e+03 - logprior: -5.7015e-01
Epoch 4/10
45/45 - 28s - loss: 1322.7140 - loglik: -1.3220e+03 - logprior: -7.0057e-01
Epoch 5/10
45/45 - 29s - loss: 1320.7290 - loglik: -1.3201e+03 - logprior: -6.7904e-01
Epoch 6/10
45/45 - 29s - loss: 1315.8580 - loglik: -1.3150e+03 - logprior: -8.5099e-01
Epoch 7/10
45/45 - 28s - loss: 1319.6985 - loglik: -1.3188e+03 - logprior: -9.1729e-01
Fitted a model with MAP estimate = -1317.9401
Time for alignment: 620.2153
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1593.3959 - loglik: -1.5912e+03 - logprior: -2.2048e+00
Epoch 2/10
39/39 - 18s - loss: 1474.0863 - loglik: -1.4724e+03 - logprior: -1.6770e+00
Epoch 3/10
39/39 - 18s - loss: 1461.4443 - loglik: -1.4595e+03 - logprior: -1.9442e+00
Epoch 4/10
39/39 - 18s - loss: 1458.7034 - loglik: -1.4567e+03 - logprior: -2.0097e+00
Epoch 5/10
39/39 - 18s - loss: 1457.6772 - loglik: -1.4555e+03 - logprior: -2.1516e+00
Epoch 6/10
39/39 - 18s - loss: 1456.9456 - loglik: -1.4546e+03 - logprior: -2.3473e+00
Epoch 7/10
39/39 - 18s - loss: 1456.4302 - loglik: -1.4539e+03 - logprior: -2.5557e+00
Epoch 8/10
39/39 - 18s - loss: 1456.1138 - loglik: -1.4533e+03 - logprior: -2.7825e+00
Epoch 9/10
39/39 - 18s - loss: 1456.0020 - loglik: -1.4531e+03 - logprior: -2.8945e+00
Epoch 10/10
39/39 - 18s - loss: 1456.1727 - loglik: -1.4532e+03 - logprior: -2.9886e+00
Fitted a model with MAP estimate = -1353.1003
expansions: [(12, 2), (13, 1), (14, 2), (15, 1), (16, 1), (20, 1), (36, 1), (37, 1), (40, 1), (46, 2), (47, 2), (59, 2), (60, 1), (63, 2), (64, 1), (65, 1), (66, 1), (68, 1), (69, 1), (70, 1), (101, 2), (119, 1), (121, 1), (123, 3), (124, 2), (125, 2), (126, 2), (127, 2), (129, 1), (132, 1), (134, 3), (135, 2), (137, 1), (138, 1), (139, 1), (145, 2), (151, 1), (158, 2), (159, 2), (164, 3), (167, 3), (168, 4), (169, 2), (171, 1), (180, 2), (191, 1), (193, 1), (194, 1), (210, 1), (212, 2), (213, 2), (215, 1), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 1452.7491 - loglik: -1.4495e+03 - logprior: -3.2051e+00
Epoch 2/2
39/39 - 27s - loss: 1432.7076 - loglik: -1.4315e+03 - logprior: -1.1667e+00
Fitted a model with MAP estimate = -1327.7212
expansions: [(0, 2)]
discards: [  0  17  59 127 157 158 161 177 180 195 214 216 228 231 232 250 289 291]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 1435.7526 - loglik: -1.4341e+03 - logprior: -1.6241e+00
Epoch 2/2
39/39 - 25s - loss: 1430.8079 - loglik: -1.4302e+03 - logprior: -6.3251e-01
Fitted a model with MAP estimate = -1327.3793
expansions: [(59, 1)]
discards: [  0  12  13 155 201 202 203]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 31s - loss: 1330.5801 - loglik: -1.3289e+03 - logprior: -1.6769e+00
Epoch 2/10
45/45 - 28s - loss: 1327.3710 - loglik: -1.3268e+03 - logprior: -5.4620e-01
Epoch 3/10
45/45 - 27s - loss: 1321.1393 - loglik: -1.3205e+03 - logprior: -6.4798e-01
Epoch 4/10
45/45 - 27s - loss: 1322.1711 - loglik: -1.3216e+03 - logprior: -6.0497e-01
Fitted a model with MAP estimate = -1320.0854
Time for alignment: 550.4410
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 1592.9500 - loglik: -1.5907e+03 - logprior: -2.2073e+00
Epoch 2/10
39/39 - 18s - loss: 1474.1721 - loglik: -1.4725e+03 - logprior: -1.7193e+00
Epoch 3/10
39/39 - 18s - loss: 1463.8954 - loglik: -1.4621e+03 - logprior: -1.8391e+00
Epoch 4/10
39/39 - 18s - loss: 1460.2229 - loglik: -1.4583e+03 - logprior: -1.9009e+00
Epoch 5/10
39/39 - 18s - loss: 1458.4818 - loglik: -1.4564e+03 - logprior: -2.0361e+00
Epoch 6/10
39/39 - 18s - loss: 1457.3936 - loglik: -1.4551e+03 - logprior: -2.2443e+00
Epoch 7/10
39/39 - 18s - loss: 1457.2180 - loglik: -1.4548e+03 - logprior: -2.4503e+00
Epoch 8/10
39/39 - 18s - loss: 1457.1240 - loglik: -1.4545e+03 - logprior: -2.6516e+00
Epoch 9/10
39/39 - 18s - loss: 1456.7123 - loglik: -1.4539e+03 - logprior: -2.7652e+00
Epoch 10/10
39/39 - 18s - loss: 1456.7870 - loglik: -1.4539e+03 - logprior: -2.8412e+00
Fitted a model with MAP estimate = -1355.0997
expansions: [(12, 2), (13, 1), (14, 1), (16, 1), (17, 1), (19, 1), (37, 1), (39, 1), (45, 3), (46, 1), (59, 3), (60, 2), (61, 5), (67, 1), (68, 1), (69, 1), (96, 1), (99, 2), (117, 3), (118, 3), (119, 8), (120, 2), (121, 2), (128, 1), (132, 1), (136, 1), (139, 1), (143, 2), (157, 2), (158, 2), (163, 1), (164, 1), (167, 3), (168, 4), (169, 3), (180, 2), (191, 1), (193, 1), (203, 1), (206, 1), (209, 1), (211, 2), (212, 2), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 1453.6119 - loglik: -1.4504e+03 - logprior: -3.2045e+00
Epoch 2/2
39/39 - 27s - loss: 1434.0466 - loglik: -1.4329e+03 - logprior: -1.1873e+00
Fitted a model with MAP estimate = -1328.2106
expansions: [(0, 2), (81, 1)]
discards: [  0  12  75  76 125 148 157 162 193 212 213 214 216 226 229 230 248 287
 289]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 1437.0859 - loglik: -1.4355e+03 - logprior: -1.6111e+00
Epoch 2/2
39/39 - 25s - loss: 1432.0120 - loglik: -1.4314e+03 - logprior: -6.1398e-01
Fitted a model with MAP estimate = -1327.7180
expansions: []
discards: [  0 154 201 202]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 31s - loss: 1331.3405 - loglik: -1.3296e+03 - logprior: -1.6906e+00
Epoch 2/10
45/45 - 27s - loss: 1325.5233 - loglik: -1.3250e+03 - logprior: -5.4251e-01
Epoch 3/10
45/45 - 27s - loss: 1326.3145 - loglik: -1.3258e+03 - logprior: -5.1668e-01
Fitted a model with MAP estimate = -1322.1525
Time for alignment: 522.1005
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 1589.2771 - loglik: -1.5871e+03 - logprior: -2.1596e+00
Epoch 2/10
39/39 - 18s - loss: 1472.1772 - loglik: -1.4709e+03 - logprior: -1.3257e+00
Epoch 3/10
39/39 - 18s - loss: 1461.7711 - loglik: -1.4604e+03 - logprior: -1.3590e+00
Epoch 4/10
39/39 - 18s - loss: 1459.7994 - loglik: -1.4584e+03 - logprior: -1.4218e+00
Epoch 5/10
39/39 - 18s - loss: 1457.9500 - loglik: -1.4564e+03 - logprior: -1.5964e+00
Epoch 6/10
39/39 - 18s - loss: 1456.7275 - loglik: -1.4549e+03 - logprior: -1.8341e+00
Epoch 7/10
39/39 - 18s - loss: 1455.8676 - loglik: -1.4538e+03 - logprior: -2.0419e+00
Epoch 8/10
39/39 - 18s - loss: 1455.7926 - loglik: -1.4536e+03 - logprior: -2.2360e+00
Epoch 9/10
39/39 - 18s - loss: 1455.2600 - loglik: -1.4529e+03 - logprior: -2.3689e+00
Epoch 10/10
39/39 - 18s - loss: 1455.3956 - loglik: -1.4529e+03 - logprior: -2.4534e+00
Fitted a model with MAP estimate = -1353.3000
expansions: [(12, 2), (13, 1), (14, 2), (16, 1), (17, 1), (36, 1), (37, 1), (39, 1), (45, 3), (46, 1), (60, 2), (62, 4), (63, 1), (66, 1), (68, 1), (69, 1), (70, 1), (101, 2), (125, 2), (126, 5), (129, 1), (142, 1), (146, 1), (147, 1), (148, 1), (161, 1), (162, 1), (163, 5), (164, 1), (165, 1), (166, 3), (167, 1), (168, 3), (169, 4), (170, 3), (181, 2), (182, 1), (191, 1), (193, 1), (203, 1), (210, 2), (211, 3), (224, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 1451.4948 - loglik: -1.4483e+03 - logprior: -3.1489e+00
Epoch 2/2
39/39 - 26s - loss: 1433.3091 - loglik: -1.4321e+03 - logprior: -1.2233e+00
Fitted a model with MAP estimate = -1329.2048
expansions: [(0, 2), (152, 2)]
discards: [  0 125 155 156 157 203 204 220 223 224 242 278 280]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 1437.4752 - loglik: -1.4358e+03 - logprior: -1.6493e+00
Epoch 2/2
39/39 - 25s - loss: 1432.6033 - loglik: -1.4320e+03 - logprior: -6.1301e-01
Fitted a model with MAP estimate = -1328.8116
expansions: []
discards: [  0  12  13 153 202]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 31s - loss: 1331.5132 - loglik: -1.3298e+03 - logprior: -1.6869e+00
Epoch 2/10
45/45 - 27s - loss: 1325.7264 - loglik: -1.3252e+03 - logprior: -5.2822e-01
Epoch 3/10
45/45 - 27s - loss: 1322.9355 - loglik: -1.3223e+03 - logprior: -6.1938e-01
Epoch 4/10
45/45 - 27s - loss: 1321.6144 - loglik: -1.3210e+03 - logprior: -5.8372e-01
Epoch 5/10
45/45 - 28s - loss: 1319.1294 - loglik: -1.3185e+03 - logprior: -6.7266e-01
Epoch 6/10
45/45 - 27s - loss: 1322.8154 - loglik: -1.3220e+03 - logprior: -7.7878e-01
Fitted a model with MAP estimate = -1318.9159
Time for alignment: 601.8225
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 1594.4835 - loglik: -1.5923e+03 - logprior: -2.1817e+00
Epoch 2/10
39/39 - 18s - loss: 1476.0807 - loglik: -1.4744e+03 - logprior: -1.6967e+00
Epoch 3/10
39/39 - 18s - loss: 1464.5316 - loglik: -1.4625e+03 - logprior: -1.9901e+00
Epoch 4/10
39/39 - 18s - loss: 1462.1567 - loglik: -1.4601e+03 - logprior: -2.0659e+00
Epoch 5/10
39/39 - 18s - loss: 1460.8214 - loglik: -1.4586e+03 - logprior: -2.2155e+00
Epoch 6/10
39/39 - 18s - loss: 1460.2509 - loglik: -1.4579e+03 - logprior: -2.3911e+00
Epoch 7/10
39/39 - 18s - loss: 1459.9172 - loglik: -1.4573e+03 - logprior: -2.6120e+00
Epoch 8/10
39/39 - 18s - loss: 1459.6339 - loglik: -1.4568e+03 - logprior: -2.7990e+00
Epoch 9/10
39/39 - 18s - loss: 1459.5427 - loglik: -1.4566e+03 - logprior: -2.9212e+00
Epoch 10/10
39/39 - 18s - loss: 1459.3683 - loglik: -1.4564e+03 - logprior: -3.0073e+00
Fitted a model with MAP estimate = -1356.9803
expansions: [(12, 4), (14, 1), (16, 1), (17, 1), (19, 1), (35, 1), (36, 1), (45, 3), (46, 1), (59, 2), (60, 1), (62, 5), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (101, 2), (122, 1), (124, 3), (125, 5), (126, 2), (127, 2), (128, 3), (134, 2), (136, 1), (138, 1), (139, 1), (140, 1), (141, 1), (145, 1), (147, 2), (160, 1), (163, 1), (164, 1), (167, 2), (168, 4), (169, 2), (170, 2), (181, 2), (182, 1), (191, 1), (193, 1), (194, 1), (203, 1), (211, 2), (213, 2), (215, 1), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 1453.5728 - loglik: -1.4504e+03 - logprior: -3.2220e+00
Epoch 2/2
39/39 - 27s - loss: 1432.9556 - loglik: -1.4318e+03 - logprior: -1.1738e+00
Fitted a model with MAP estimate = -1328.6695
expansions: [(0, 2)]
discards: [  0  11  12  79  80 127 158 159 163 170 179 199 210 211 248 286 289]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 1437.5107 - loglik: -1.4359e+03 - logprior: -1.6111e+00
Epoch 2/2
39/39 - 25s - loss: 1432.6337 - loglik: -1.4321e+03 - logprior: -5.8357e-01
Fitted a model with MAP estimate = -1328.3729
expansions: [(201, 4)]
discards: [  0 223]
Re-initialized the encoder parameters.
Fitting a model of length 292 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 32s - loss: 1329.0999 - loglik: -1.3274e+03 - logprior: -1.6961e+00
Epoch 2/10
45/45 - 28s - loss: 1324.3601 - loglik: -1.3238e+03 - logprior: -5.3834e-01
Epoch 3/10
45/45 - 29s - loss: 1321.3850 - loglik: -1.3208e+03 - logprior: -5.8589e-01
Epoch 4/10
45/45 - 29s - loss: 1320.2056 - loglik: -1.3196e+03 - logprior: -6.2694e-01
Epoch 5/10
45/45 - 28s - loss: 1318.1532 - loglik: -1.3174e+03 - logprior: -7.4147e-01
Epoch 6/10
45/45 - 29s - loss: 1316.1836 - loglik: -1.3155e+03 - logprior: -7.2779e-01
Epoch 7/10
45/45 - 28s - loss: 1319.3743 - loglik: -1.3185e+03 - logprior: -9.0066e-01
Fitted a model with MAP estimate = -1316.3805
Time for alignment: 638.7305
Computed alignments with likelihoods: ['-1317.9401', '-1320.0854', '-1322.1525', '-1318.9159', '-1316.3805']
Best model has likelihood: -1316.3805  (prior= -0.9093 )
time for generating output: 0.3651
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aldosered.projection.fasta
SP score = 0.8942467575656801
Training of 5 independent models on file sdr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f53345bf730>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5301b36fd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f528aa00190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5286280ac0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52e0518310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530d0245b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f53140ede50>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5314586220>, <__main__.SimpleDirichletPrior object at 0x7f529730dd60>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 15s - loss: 936.8181 - loglik: -9.3563e+02 - logprior: -1.1915e+00
Epoch 2/10
30/30 - 7s - loss: 865.5543 - loglik: -8.6442e+02 - logprior: -1.1344e+00
Epoch 3/10
30/30 - 8s - loss: 853.5606 - loglik: -8.5242e+02 - logprior: -1.1403e+00
Epoch 4/10
30/30 - 8s - loss: 853.0847 - loglik: -8.5193e+02 - logprior: -1.1586e+00
Epoch 5/10
30/30 - 8s - loss: 850.5844 - loglik: -8.4943e+02 - logprior: -1.1544e+00
Epoch 6/10
30/30 - 8s - loss: 850.5745 - loglik: -8.4939e+02 - logprior: -1.1862e+00
Epoch 7/10
30/30 - 8s - loss: 849.9227 - loglik: -8.4871e+02 - logprior: -1.2171e+00
Epoch 8/10
30/30 - 8s - loss: 849.8103 - loglik: -8.4856e+02 - logprior: -1.2553e+00
Epoch 9/10
30/30 - 8s - loss: 848.9786 - loglik: -8.4766e+02 - logprior: -1.3235e+00
Epoch 10/10
30/30 - 8s - loss: 849.5001 - loglik: -8.4814e+02 - logprior: -1.3564e+00
Fitted a model with MAP estimate = -814.8903
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (26, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (54, 1), (56, 1), (72, 2), (74, 2), (75, 1), (76, 2), (82, 1), (92, 2), (93, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (126, 1), (127, 1), (128, 1), (131, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 18s - loss: 844.4881 - loglik: -8.4325e+02 - logprior: -1.2385e+00
Epoch 2/2
61/61 - 14s - loss: 836.5782 - loglik: -8.3571e+02 - logprior: -8.7293e-01
Fitted a model with MAP estimate = -799.1507
expansions: []
discards: [ 30  48  51  92  95 100 120 134 148 152 155]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 17s - loss: 838.6227 - loglik: -8.3760e+02 - logprior: -1.0215e+00
Epoch 2/2
61/61 - 13s - loss: 836.6245 - loglik: -8.3588e+02 - logprior: -7.4802e-01
Fitted a model with MAP estimate = -799.6099
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 22s - loss: 798.5396 - loglik: -7.9787e+02 - logprior: -6.6798e-01
Epoch 2/10
87/87 - 18s - loss: 796.9677 - loglik: -7.9641e+02 - logprior: -5.5491e-01
Epoch 3/10
87/87 - 18s - loss: 796.4953 - loglik: -7.9594e+02 - logprior: -5.5506e-01
Epoch 4/10
87/87 - 18s - loss: 796.0768 - loglik: -7.9553e+02 - logprior: -5.4571e-01
Epoch 5/10
87/87 - 18s - loss: 794.0179 - loglik: -7.9346e+02 - logprior: -5.5525e-01
Epoch 6/10
87/87 - 18s - loss: 793.4893 - loglik: -7.9290e+02 - logprior: -5.8909e-01
Epoch 7/10
87/87 - 18s - loss: 793.2762 - loglik: -7.9267e+02 - logprior: -6.0284e-01
Epoch 8/10
87/87 - 18s - loss: 793.3199 - loglik: -7.9266e+02 - logprior: -6.5952e-01
Fitted a model with MAP estimate = -792.5863
Time for alignment: 454.3412
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 12s - loss: 936.9437 - loglik: -9.3575e+02 - logprior: -1.1973e+00
Epoch 2/10
30/30 - 8s - loss: 867.5583 - loglik: -8.6641e+02 - logprior: -1.1515e+00
Epoch 3/10
30/30 - 8s - loss: 855.6371 - loglik: -8.5450e+02 - logprior: -1.1356e+00
Epoch 4/10
30/30 - 8s - loss: 852.4518 - loglik: -8.5129e+02 - logprior: -1.1572e+00
Epoch 5/10
30/30 - 8s - loss: 851.9424 - loglik: -8.5079e+02 - logprior: -1.1533e+00
Epoch 6/10
30/30 - 8s - loss: 851.1267 - loglik: -8.4994e+02 - logprior: -1.1867e+00
Epoch 7/10
30/30 - 8s - loss: 850.2303 - loglik: -8.4901e+02 - logprior: -1.2162e+00
Epoch 8/10
30/30 - 7s - loss: 849.6410 - loglik: -8.4838e+02 - logprior: -1.2592e+00
Epoch 9/10
30/30 - 8s - loss: 849.7170 - loglik: -8.4840e+02 - logprior: -1.3199e+00
Fitted a model with MAP estimate = -814.9916
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (26, 2), (29, 1), (36, 1), (38, 1), (39, 2), (41, 2), (42, 2), (43, 2), (52, 1), (54, 1), (55, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (88, 1), (93, 1), (94, 1), (97, 2), (99, 1), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 17s - loss: 844.2297 - loglik: -8.4302e+02 - logprior: -1.2120e+00
Epoch 2/2
61/61 - 14s - loss: 836.5582 - loglik: -8.3569e+02 - logprior: -8.6585e-01
Fitted a model with MAP estimate = -799.0852
expansions: []
discards: [ 30  48  51  57  93  95 127 147 150 155]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 17s - loss: 837.7009 - loglik: -8.3667e+02 - logprior: -1.0280e+00
Epoch 2/2
61/61 - 13s - loss: 837.1487 - loglik: -8.3640e+02 - logprior: -7.4519e-01
Fitted a model with MAP estimate = -799.5380
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 24s - loss: 797.7999 - loglik: -7.9713e+02 - logprior: -6.6978e-01
Epoch 2/10
87/87 - 18s - loss: 797.5196 - loglik: -7.9696e+02 - logprior: -5.5694e-01
Epoch 3/10
87/87 - 18s - loss: 796.2125 - loglik: -7.9566e+02 - logprior: -5.5626e-01
Epoch 4/10
87/87 - 18s - loss: 795.9454 - loglik: -7.9540e+02 - logprior: -5.4644e-01
Epoch 5/10
87/87 - 18s - loss: 794.8575 - loglik: -7.9430e+02 - logprior: -5.5850e-01
Epoch 6/10
87/87 - 19s - loss: 793.3663 - loglik: -7.9277e+02 - logprior: -5.9473e-01
Epoch 7/10
87/87 - 18s - loss: 792.8312 - loglik: -7.9222e+02 - logprior: -6.1118e-01
Epoch 8/10
87/87 - 18s - loss: 792.8438 - loglik: -7.9218e+02 - logprior: -6.6458e-01
Fitted a model with MAP estimate = -792.6251
Time for alignment: 446.4816
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 11s - loss: 936.9799 - loglik: -9.3578e+02 - logprior: -1.1993e+00
Epoch 2/10
30/30 - 8s - loss: 866.0594 - loglik: -8.6491e+02 - logprior: -1.1485e+00
Epoch 3/10
30/30 - 8s - loss: 853.5713 - loglik: -8.5243e+02 - logprior: -1.1413e+00
Epoch 4/10
30/30 - 8s - loss: 852.0727 - loglik: -8.5092e+02 - logprior: -1.1504e+00
Epoch 5/10
30/30 - 8s - loss: 851.4234 - loglik: -8.5028e+02 - logprior: -1.1459e+00
Epoch 6/10
30/30 - 8s - loss: 850.0585 - loglik: -8.4889e+02 - logprior: -1.1709e+00
Epoch 7/10
30/30 - 8s - loss: 849.5518 - loglik: -8.4835e+02 - logprior: -1.2036e+00
Epoch 8/10
30/30 - 8s - loss: 849.4095 - loglik: -8.4816e+02 - logprior: -1.2472e+00
Epoch 9/10
30/30 - 8s - loss: 847.5851 - loglik: -8.4628e+02 - logprior: -1.3048e+00
Epoch 10/10
30/30 - 8s - loss: 849.4191 - loglik: -8.4807e+02 - logprior: -1.3465e+00
Fitted a model with MAP estimate = -814.2857
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 2), (48, 1), (51, 1), (54, 1), (56, 1), (71, 2), (72, 2), (75, 1), (76, 2), (82, 1), (88, 1), (93, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (126, 1), (127, 1), (128, 1), (131, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 17s - loss: 843.9635 - loglik: -8.4274e+02 - logprior: -1.2250e+00
Epoch 2/2
61/61 - 14s - loss: 836.8615 - loglik: -8.3599e+02 - logprior: -8.6975e-01
Fitted a model with MAP estimate = -799.2295
expansions: []
discards: [ 25  48  51  57  93  95 101 134 149 152 155]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 17s - loss: 838.0677 - loglik: -8.3704e+02 - logprior: -1.0257e+00
Epoch 2/2
61/61 - 13s - loss: 836.4586 - loglik: -8.3572e+02 - logprior: -7.3675e-01
Fitted a model with MAP estimate = -799.7664
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 24s - loss: 798.5179 - loglik: -7.9785e+02 - logprior: -6.6654e-01
Epoch 2/10
87/87 - 18s - loss: 797.2376 - loglik: -7.9669e+02 - logprior: -5.5235e-01
Epoch 3/10
87/87 - 18s - loss: 796.7847 - loglik: -7.9623e+02 - logprior: -5.5044e-01
Epoch 4/10
87/87 - 18s - loss: 795.2615 - loglik: -7.9472e+02 - logprior: -5.4086e-01
Epoch 5/10
87/87 - 19s - loss: 794.5292 - loglik: -7.9397e+02 - logprior: -5.5742e-01
Epoch 6/10
87/87 - 18s - loss: 793.8247 - loglik: -7.9324e+02 - logprior: -5.8952e-01
Epoch 7/10
87/87 - 18s - loss: 792.5625 - loglik: -7.9196e+02 - logprior: -6.0030e-01
Epoch 8/10
87/87 - 18s - loss: 792.9255 - loglik: -7.9227e+02 - logprior: -6.5759e-01
Fitted a model with MAP estimate = -792.6434
Time for alignment: 450.4316
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 11s - loss: 937.0159 - loglik: -9.3582e+02 - logprior: -1.2009e+00
Epoch 2/10
30/30 - 8s - loss: 865.5716 - loglik: -8.6443e+02 - logprior: -1.1452e+00
Epoch 3/10
30/30 - 8s - loss: 854.0344 - loglik: -8.5291e+02 - logprior: -1.1279e+00
Epoch 4/10
30/30 - 8s - loss: 851.1697 - loglik: -8.5002e+02 - logprior: -1.1466e+00
Epoch 5/10
30/30 - 7s - loss: 850.7793 - loglik: -8.4963e+02 - logprior: -1.1481e+00
Epoch 6/10
30/30 - 8s - loss: 849.9697 - loglik: -8.4879e+02 - logprior: -1.1788e+00
Epoch 7/10
30/30 - 8s - loss: 849.3035 - loglik: -8.4811e+02 - logprior: -1.1912e+00
Epoch 8/10
30/30 - 8s - loss: 848.8672 - loglik: -8.4763e+02 - logprior: -1.2380e+00
Epoch 9/10
30/30 - 8s - loss: 848.9476 - loglik: -8.4765e+02 - logprior: -1.2933e+00
Fitted a model with MAP estimate = -813.9593
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (55, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (88, 1), (91, 1), (94, 1), (97, 1), (99, 1), (101, 2), (113, 2), (114, 2), (116, 2), (117, 1), (125, 1), (128, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 17s - loss: 843.6315 - loglik: -8.4244e+02 - logprior: -1.1921e+00
Epoch 2/2
61/61 - 14s - loss: 836.2755 - loglik: -8.3544e+02 - logprior: -8.3812e-01
Fitted a model with MAP estimate = -799.3362
expansions: []
discards: [ 25  48  51  92  94 132 147 150 154]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 17s - loss: 838.1451 - loglik: -8.3712e+02 - logprior: -1.0295e+00
Epoch 2/2
61/61 - 13s - loss: 836.7354 - loglik: -8.3599e+02 - logprior: -7.4743e-01
Fitted a model with MAP estimate = -799.6308
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 21s - loss: 798.8973 - loglik: -7.9823e+02 - logprior: -6.6798e-01
Epoch 2/10
87/87 - 18s - loss: 797.0451 - loglik: -7.9649e+02 - logprior: -5.5778e-01
Epoch 3/10
87/87 - 18s - loss: 796.0753 - loglik: -7.9552e+02 - logprior: -5.5505e-01
Epoch 4/10
87/87 - 19s - loss: 795.4583 - loglik: -7.9491e+02 - logprior: -5.4465e-01
Epoch 5/10
87/87 - 18s - loss: 794.9928 - loglik: -7.9443e+02 - logprior: -5.6404e-01
Epoch 6/10
87/87 - 18s - loss: 793.9072 - loglik: -7.9331e+02 - logprior: -5.9788e-01
Epoch 7/10
87/87 - 19s - loss: 792.5928 - loglik: -7.9199e+02 - logprior: -6.0422e-01
Epoch 8/10
87/87 - 19s - loss: 793.7386 - loglik: -7.9308e+02 - logprior: -6.6335e-01
Fitted a model with MAP estimate = -792.5873
Time for alignment: 438.0722
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 13s - loss: 936.8018 - loglik: -9.3561e+02 - logprior: -1.1922e+00
Epoch 2/10
30/30 - 8s - loss: 865.6515 - loglik: -8.6452e+02 - logprior: -1.1338e+00
Epoch 3/10
30/30 - 8s - loss: 854.7728 - loglik: -8.5365e+02 - logprior: -1.1259e+00
Epoch 4/10
30/30 - 8s - loss: 853.0304 - loglik: -8.5189e+02 - logprior: -1.1405e+00
Epoch 5/10
30/30 - 8s - loss: 852.1302 - loglik: -8.5099e+02 - logprior: -1.1380e+00
Epoch 6/10
30/30 - 8s - loss: 851.1074 - loglik: -8.4994e+02 - logprior: -1.1659e+00
Epoch 7/10
30/30 - 8s - loss: 850.6873 - loglik: -8.4950e+02 - logprior: -1.1913e+00
Epoch 8/10
30/30 - 7s - loss: 850.5006 - loglik: -8.4927e+02 - logprior: -1.2278e+00
Epoch 9/10
30/30 - 8s - loss: 850.3032 - loglik: -8.4901e+02 - logprior: -1.2910e+00
Epoch 10/10
30/30 - 8s - loss: 849.7863 - loglik: -8.4845e+02 - logprior: -1.3353e+00
Fitted a model with MAP estimate = -815.6947
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (55, 1), (56, 1), (72, 2), (73, 2), (75, 1), (78, 2), (82, 1), (88, 1), (93, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (126, 1), (127, 1), (128, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 17s - loss: 844.1299 - loglik: -8.4291e+02 - logprior: -1.2158e+00
Epoch 2/2
61/61 - 14s - loss: 836.9065 - loglik: -8.3604e+02 - logprior: -8.6791e-01
Fitted a model with MAP estimate = -799.3081
expansions: []
discards: [ 25  51  92  94 102 133 147 151 154]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 17s - loss: 838.2256 - loglik: -8.3719e+02 - logprior: -1.0332e+00
Epoch 2/2
61/61 - 13s - loss: 836.3785 - loglik: -8.3562e+02 - logprior: -7.5401e-01
Fitted a model with MAP estimate = -799.4093
expansions: []
discards: [47]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 21s - loss: 798.8872 - loglik: -7.9822e+02 - logprior: -6.6688e-01
Epoch 2/10
87/87 - 18s - loss: 796.9617 - loglik: -7.9641e+02 - logprior: -5.5545e-01
Epoch 3/10
87/87 - 18s - loss: 796.0480 - loglik: -7.9549e+02 - logprior: -5.5723e-01
Epoch 4/10
87/87 - 18s - loss: 795.4478 - loglik: -7.9490e+02 - logprior: -5.4448e-01
Epoch 5/10
87/87 - 18s - loss: 794.7643 - loglik: -7.9421e+02 - logprior: -5.5704e-01
Epoch 6/10
87/87 - 18s - loss: 793.6227 - loglik: -7.9303e+02 - logprior: -5.9469e-01
Epoch 7/10
87/87 - 18s - loss: 793.7856 - loglik: -7.9318e+02 - logprior: -6.0409e-01
Fitted a model with MAP estimate = -792.7567
Time for alignment: 430.6425
Computed alignments with likelihoods: ['-792.5863', '-792.6251', '-792.6434', '-792.5873', '-792.7567']
Best model has likelihood: -792.5863  (prior= -0.6553 )
time for generating output: 0.3049
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sdr.projection.fasta
SP score = 0.6658387150190429
Training of 5 independent models on file p450.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5277d6d700>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52e1a9ca60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5288159910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5314909ac0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52d8d76370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52978359a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52d8a1fd90>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5295b05a90>, <__main__.SimpleDirichletPrior object at 0x7f531c2497f0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 51s - loss: 2572.1440 - loglik: -2.5704e+03 - logprior: -1.7367e+00
Epoch 2/10
40/40 - 44s - loss: 2469.2065 - loglik: -2.4692e+03 - logprior: 0.0309
Epoch 3/10
40/40 - 45s - loss: 2461.2434 - loglik: -2.4613e+03 - logprior: 0.0496
Epoch 4/10
40/40 - 44s - loss: 2455.7205 - loglik: -2.4556e+03 - logprior: -8.6141e-02
Epoch 5/10
40/40 - 44s - loss: 2449.1541 - loglik: -2.4487e+03 - logprior: -4.7378e-01
Epoch 6/10
40/40 - 45s - loss: 2444.2661 - loglik: -2.4433e+03 - logprior: -1.0108e+00
Epoch 7/10
40/40 - 44s - loss: 2438.1753 - loglik: -2.4366e+03 - logprior: -1.5780e+00
Epoch 8/10
40/40 - 44s - loss: 2434.6389 - loglik: -2.4326e+03 - logprior: -2.0506e+00
Epoch 9/10
40/40 - 44s - loss: 2430.0103 - loglik: -2.4276e+03 - logprior: -2.4434e+00
Epoch 10/10
40/40 - 44s - loss: 2426.1255 - loglik: -2.4234e+03 - logprior: -2.7675e+00
Fitted a model with MAP estimate = -1810.3037
expansions: [(25, 1), (123, 1), (124, 1), (172, 1), (289, 2), (290, 3), (291, 8), (293, 1), (295, 1), (317, 1), (318, 2), (319, 5), (327, 2), (328, 10), (329, 9), (330, 78)]
discards: [  1 189]
Re-initialized the encoder parameters.
Fitting a model of length 454 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 78s - loss: 2425.1294 - loglik: -2.4234e+03 - logprior: -1.6928e+00
Epoch 2/2
80/80 - 74s - loss: 2363.9607 - loglik: -2.3640e+03 - logprior: 0.0406
Fitted a model with MAP estimate = -1736.5543
expansions: [(163, 1), (454, 2)]
discards: [294 295 296 334 339 353 369 389 450]
Re-initialized the encoder parameters.
Fitting a model of length 448 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 73s - loss: 2369.1375 - loglik: -2.3681e+03 - logprior: -1.0481e+00
Epoch 2/2
40/40 - 69s - loss: 2364.1392 - loglik: -2.3644e+03 - logprior: 0.2610
Fitted a model with MAP estimate = -1737.6008
expansions: [(434, 1)]
discards: [428 446 447]
Re-initialized the encoder parameters.
Fitting a model of length 446 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 101s - loss: 1733.9630 - loglik: -1.7336e+03 - logprior: -3.8804e-01
Epoch 2/10
56/56 - 96s - loss: 1729.6887 - loglik: -1.7297e+03 - logprior: 0.0221
Epoch 3/10
56/56 - 95s - loss: 1724.1266 - loglik: -1.7241e+03 - logprior: -1.4811e-02
Epoch 4/10
56/56 - 95s - loss: 1726.5027 - loglik: -1.7265e+03 - logprior: -1.6262e-02
Fitted a model with MAP estimate = -1719.5364
Time for alignment: 1565.7873
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 48s - loss: 2562.5806 - loglik: -2.5601e+03 - logprior: -2.5239e+00
Epoch 2/10
40/40 - 44s - loss: 2442.0903 - loglik: -2.4389e+03 - logprior: -3.2144e+00
Epoch 3/10
40/40 - 44s - loss: 2431.1545 - loglik: -2.4280e+03 - logprior: -3.1929e+00
Epoch 4/10
40/40 - 44s - loss: 2424.8562 - loglik: -2.4215e+03 - logprior: -3.3478e+00
Epoch 5/10
40/40 - 44s - loss: 2417.8311 - loglik: -2.4142e+03 - logprior: -3.6324e+00
Epoch 6/10
40/40 - 45s - loss: 2411.6116 - loglik: -2.4074e+03 - logprior: -4.2048e+00
Epoch 7/10
40/40 - 44s - loss: 2403.9888 - loglik: -2.3992e+03 - logprior: -4.7418e+00
Epoch 8/10
40/40 - 44s - loss: 2396.6335 - loglik: -2.3914e+03 - logprior: -5.2052e+00
Epoch 9/10
40/40 - 44s - loss: 2390.9297 - loglik: -2.3852e+03 - logprior: -5.7486e+00
Epoch 10/10
40/40 - 44s - loss: 2384.7095 - loglik: -2.3786e+03 - logprior: -6.0971e+00
Fitted a model with MAP estimate = -1801.8638
expansions: [(124, 1), (169, 2), (201, 1), (214, 2), (330, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 108 109 110 155 156
 157 158 159 160 161 162 163 182 183 184 185 186 232 235 236 237 238 239
 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257
 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275
 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293
 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311
 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 20s - loss: 2654.8767 - loglik: -2.6516e+03 - logprior: -3.3150e+00
Epoch 2/2
40/40 - 16s - loss: 2592.4832 - loglik: -2.5910e+03 - logprior: -1.4678e+00
Fitted a model with MAP estimate = -1884.3752
expansions: [(0, 143), (3, 11), (5, 20), (15, 36), (16, 2), (17, 8), (18, 4), (19, 59), (20, 7), (21, 8), (22, 6), (23, 1), (24, 6), (25, 21), (65, 15), (69, 1), (70, 5), (98, 2), (118, 1)]
discards: [ 50  57  58  59  60  99 100 101 102 103 104 105 106]
Re-initialized the encoder parameters.
Fitting a model of length 465 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 82s - loss: 2440.1714 - loglik: -2.4381e+03 - logprior: -2.0436e+00
Epoch 2/2
80/80 - 76s - loss: 2384.9792 - loglik: -2.3842e+03 - logprior: -8.0039e-01
Fitted a model with MAP estimate = -1748.2400
expansions: [(117, 1), (154, 1), (171, 2), (172, 1), (173, 2), (174, 1), (205, 1), (207, 3), (213, 1), (217, 3), (218, 1), (240, 2), (262, 1), (273, 2), (277, 1), (325, 5), (326, 4), (351, 4), (352, 6), (446, 1), (451, 3)]
discards: [  1  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34
  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52
  53  54  55  56  57  58  59  60  61  62  63  64  65 226 237 400 401 413
 464]
Re-initialized the encoder parameters.
Fitting a model of length 456 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 108s - loss: 1736.4385 - loglik: -1.7358e+03 - logprior: -6.4070e-01
Epoch 2/10
113/113 - 104s - loss: 1732.4534 - loglik: -1.7320e+03 - logprior: -4.6217e-01
Epoch 3/10
113/113 - 104s - loss: 1722.5007 - loglik: -1.7220e+03 - logprior: -5.1509e-01
Epoch 4/10
113/113 - 104s - loss: 1720.9841 - loglik: -1.7204e+03 - logprior: -5.8763e-01
Epoch 5/10
113/113 - 104s - loss: 1715.2367 - loglik: -1.7144e+03 - logprior: -8.7499e-01
Epoch 6/10
113/113 - 103s - loss: 1708.1830 - loglik: -1.7070e+03 - logprior: -1.1999e+00
Epoch 7/10
113/113 - 104s - loss: 1713.0270 - loglik: -1.7116e+03 - logprior: -1.4365e+00
Fitted a model with MAP estimate = -1706.3299
Time for alignment: 1716.9153
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 48s - loss: 2569.1333 - loglik: -2.5672e+03 - logprior: -1.9319e+00
Epoch 2/10
40/40 - 44s - loss: 2445.0410 - loglik: -2.4427e+03 - logprior: -2.3889e+00
Epoch 3/10
40/40 - 44s - loss: 2431.4634 - loglik: -2.4289e+03 - logprior: -2.5860e+00
Epoch 4/10
40/40 - 44s - loss: 2426.0791 - loglik: -2.4232e+03 - logprior: -2.8676e+00
Epoch 5/10
40/40 - 44s - loss: 2418.3823 - loglik: -2.4151e+03 - logprior: -3.3299e+00
Epoch 6/10
40/40 - 44s - loss: 2411.1226 - loglik: -2.4072e+03 - logprior: -3.9451e+00
Epoch 7/10
40/40 - 44s - loss: 2404.7104 - loglik: -2.4001e+03 - logprior: -4.6273e+00
Epoch 8/10
40/40 - 44s - loss: 2397.0266 - loglik: -2.3918e+03 - logprior: -5.1841e+00
Epoch 9/10
40/40 - 44s - loss: 2391.5801 - loglik: -2.3859e+03 - logprior: -5.7255e+00
Epoch 10/10
40/40 - 44s - loss: 2386.4211 - loglik: -2.3803e+03 - logprior: -6.1334e+00
Fitted a model with MAP estimate = -1800.5373
expansions: [(129, 2), (138, 1), (203, 1), (224, 1), (330, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 108 109 110 124
 157 160 161 184 185 186 187 188 235 236 237 238 239 240 241 242 243 244
 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262
 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280
 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298
 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316
 317 318 319 320 321 322 323 324 325 326 327 328 329]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 20s - loss: 2641.5166 - loglik: -2.6377e+03 - logprior: -3.8291e+00
Epoch 2/2
40/40 - 17s - loss: 2591.1455 - loglik: -2.5895e+03 - logprior: -1.6315e+00
Fitted a model with MAP estimate = -1881.7532
expansions: [(0, 120), (10, 35), (17, 48), (18, 4), (19, 1), (20, 25), (22, 56), (23, 9), (25, 19), (51, 2), (52, 2), (54, 1), (64, 2), (75, 1), (76, 5), (114, 1), (117, 1), (118, 1)]
discards: [  0  65 109 110 111 112]
Re-initialized the encoder parameters.
Fitting a model of length 453 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 77s - loss: 2432.9121 - loglik: -2.4313e+03 - logprior: -1.5919e+00
Epoch 2/2
80/80 - 73s - loss: 2374.7317 - loglik: -2.3740e+03 - logprior: -6.9086e-01
Fitted a model with MAP estimate = -1740.8113
expansions: [(148, 1), (183, 1), (185, 3), (191, 1), (192, 2), (193, 3), (256, 2), (259, 4), (260, 1), (262, 1), (263, 3), (264, 1), (278, 1), (289, 1), (298, 1), (299, 2), (300, 2), (302, 2), (314, 1), (440, 2), (452, 1)]
discards: [  0  21  22  23  24  69  70  71  73  74  75  95  96 151 152 153 154 155
 156 157 158 159 160 161 162 208 223 224 331 368 370 400 435 436]
Re-initialized the encoder parameters.
Fitting a model of length 455 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 107s - loss: 1735.9131 - loglik: -1.7352e+03 - logprior: -7.2629e-01
Epoch 2/10
113/113 - 103s - loss: 1720.3960 - loglik: -1.7201e+03 - logprior: -3.4488e-01
Epoch 3/10
113/113 - 103s - loss: 1734.3539 - loglik: -1.7339e+03 - logprior: -4.7261e-01
Fitted a model with MAP estimate = -1719.4582
Time for alignment: 1287.8320
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 47s - loss: 2557.6113 - loglik: -2.5551e+03 - logprior: -2.5397e+00
Epoch 2/10
40/40 - 44s - loss: 2439.9497 - loglik: -2.4368e+03 - logprior: -3.1480e+00
Epoch 3/10
40/40 - 44s - loss: 2429.9485 - loglik: -2.4268e+03 - logprior: -3.1753e+00
Epoch 4/10
40/40 - 44s - loss: 2423.5623 - loglik: -2.4202e+03 - logprior: -3.3615e+00
Epoch 5/10
40/40 - 44s - loss: 2417.7378 - loglik: -2.4140e+03 - logprior: -3.7122e+00
Epoch 6/10
40/40 - 44s - loss: 2410.3174 - loglik: -2.4060e+03 - logprior: -4.3141e+00
Epoch 7/10
40/40 - 44s - loss: 2403.3152 - loglik: -2.3984e+03 - logprior: -4.8678e+00
Epoch 8/10
40/40 - 44s - loss: 2395.7605 - loglik: -2.3904e+03 - logprior: -5.3766e+00
Epoch 9/10
40/40 - 44s - loss: 2389.1450 - loglik: -2.3833e+03 - logprior: -5.8505e+00
Epoch 10/10
40/40 - 45s - loss: 2384.0452 - loglik: -2.3777e+03 - logprior: -6.2959e+00
Fitted a model with MAP estimate = -1801.1903
expansions: [(117, 1), (121, 1), (169, 1), (212, 1), (330, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 160 161 181 182 183 184 185 186 233 234 235 236 237 238 239
 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257
 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275
 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293
 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311
 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 20s - loss: 2642.8208 - loglik: -2.6393e+03 - logprior: -3.5249e+00
Epoch 2/2
40/40 - 17s - loss: 2583.2534 - loglik: -2.5825e+03 - logprior: -7.8753e-01
Fitted a model with MAP estimate = -1876.5847
expansions: [(0, 317), (13, 2), (21, 8), (51, 3), (58, 4), (71, 6), (112, 1), (114, 1)]
discards: [ 11  59 119]
Re-initialized the encoder parameters.
Fitting a model of length 459 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 78s - loss: 2420.9797 - loglik: -2.4195e+03 - logprior: -1.5191e+00
Epoch 2/2
80/80 - 75s - loss: 2367.3975 - loglik: -2.3670e+03 - logprior: -3.6277e-01
Fitted a model with MAP estimate = -1736.8997
expansions: [(257, 1), (265, 1), (282, 1), (295, 1), (298, 1), (301, 1), (307, 1), (341, 2), (459, 2)]
discards: [  0  21  22  23  69  70  71  93 135 237 330 379 458]
Re-initialized the encoder parameters.
Fitting a model of length 457 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 107s - loss: 1732.4382 - loglik: -1.7318e+03 - logprior: -6.4647e-01
Epoch 2/10
113/113 - 104s - loss: 1724.0990 - loglik: -1.7237e+03 - logprior: -3.6771e-01
Epoch 3/10
113/113 - 104s - loss: 1727.7758 - loglik: -1.7273e+03 - logprior: -4.4429e-01
Fitted a model with MAP estimate = -1719.6740
Time for alignment: 1293.5169
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 50s - loss: 2559.1111 - loglik: -2.5566e+03 - logprior: -2.4627e+00
Epoch 2/10
40/40 - 44s - loss: 2437.8491 - loglik: -2.4350e+03 - logprior: -2.8259e+00
Epoch 3/10
40/40 - 44s - loss: 2428.3169 - loglik: -2.4255e+03 - logprior: -2.8035e+00
Epoch 4/10
40/40 - 44s - loss: 2420.9473 - loglik: -2.4180e+03 - logprior: -2.9557e+00
Epoch 5/10
40/40 - 44s - loss: 2414.7083 - loglik: -2.4113e+03 - logprior: -3.4073e+00
Epoch 6/10
40/40 - 44s - loss: 2408.1458 - loglik: -2.4041e+03 - logprior: -4.0738e+00
Epoch 7/10
40/40 - 44s - loss: 2402.3816 - loglik: -2.3977e+03 - logprior: -4.6927e+00
Epoch 8/10
40/40 - 44s - loss: 2395.1694 - loglik: -2.3900e+03 - logprior: -5.1999e+00
Epoch 9/10
40/40 - 44s - loss: 2388.6650 - loglik: -2.3830e+03 - logprior: -5.6623e+00
Epoch 10/10
40/40 - 44s - loss: 2383.7559 - loglik: -2.3777e+03 - logprior: -6.0069e+00
Fitted a model with MAP estimate = -1797.9098
expansions: [(117, 1), (121, 1), (209, 1), (212, 1), (213, 3), (214, 2), (215, 4), (330, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 109 110 111 122
 158 159 160 161 162 179 180 181 182 183 184 185 186 187 226 227 228 229
 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247
 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265
 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283
 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301
 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319
 320 321 322 323 324 325 326 327 328 329]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 20s - loss: 2647.9023 - loglik: -2.6443e+03 - logprior: -3.6198e+00
Epoch 2/2
40/40 - 17s - loss: 2596.6316 - loglik: -2.5951e+03 - logprior: -1.5726e+00
Fitted a model with MAP estimate = -1882.1748
expansions: [(0, 224), (2, 2), (5, 60), (15, 26), (17, 2), (18, 3), (19, 9), (48, 2), (49, 4), (57, 12), (62, 1), (68, 2), (94, 1), (95, 2), (96, 2), (98, 1), (99, 2)]
discards: [  0  58  59  64  65  66 101 102 103 104 105 118]
Re-initialized the encoder parameters.
Fitting a model of length 462 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 79s - loss: 2422.8132 - loglik: -2.4214e+03 - logprior: -1.4359e+00
Epoch 2/2
80/80 - 75s - loss: 2367.5818 - loglik: -2.3672e+03 - logprior: -3.5080e-01
Fitted a model with MAP estimate = -1736.2443
expansions: [(241, 2), (251, 2), (255, 1), (262, 1), (263, 1), (264, 1), (329, 2), (462, 2)]
discards: [  0  19  70 106 134 135 136 203 204 205 226 332 335 378 439 440 442 443
 447 461]
Re-initialized the encoder parameters.
Fitting a model of length 454 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 108s - loss: 1739.7178 - loglik: -1.7390e+03 - logprior: -7.0286e-01
Epoch 2/10
113/113 - 103s - loss: 1718.0090 - loglik: -1.7175e+03 - logprior: -4.6676e-01
Epoch 3/10
113/113 - 103s - loss: 1730.6094 - loglik: -1.7301e+03 - logprior: -4.7816e-01
Fitted a model with MAP estimate = -1719.5657
Time for alignment: 1296.1780
Computed alignments with likelihoods: ['-1719.5364', '-1706.3299', '-1719.4582', '-1719.6740', '-1719.5657']
Best model has likelihood: -1706.3299  (prior= -1.6306 )
time for generating output: 0.5727
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/p450.projection.fasta
SP score = 0.7205840016450751
Training of 5 independent models on file hla.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5334246b50>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b07e76e50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b0a42c070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5335387fd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52e85f1280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52963e5ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52881fb2b0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5288159910>, <__main__.SimpleDirichletPrior object at 0x7f527d4cf580>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 973.3006 - loglik: -9.6977e+02 - logprior: -3.5287e+00
Epoch 2/10
19/19 - 4s - loss: 808.8761 - loglik: -8.0714e+02 - logprior: -1.7406e+00
Epoch 3/10
19/19 - 4s - loss: 736.6179 - loglik: -7.3428e+02 - logprior: -2.3404e+00
Epoch 4/10
19/19 - 4s - loss: 725.8986 - loglik: -7.2344e+02 - logprior: -2.4574e+00
Epoch 5/10
19/19 - 4s - loss: 722.1763 - loglik: -7.1977e+02 - logprior: -2.4077e+00
Epoch 6/10
19/19 - 4s - loss: 720.9324 - loglik: -7.1854e+02 - logprior: -2.3935e+00
Epoch 7/10
19/19 - 4s - loss: 719.7225 - loglik: -7.1733e+02 - logprior: -2.3948e+00
Epoch 8/10
19/19 - 4s - loss: 718.0317 - loglik: -7.1559e+02 - logprior: -2.4384e+00
Epoch 9/10
19/19 - 4s - loss: 717.8270 - loglik: -7.1534e+02 - logprior: -2.4918e+00
Epoch 10/10
19/19 - 4s - loss: 715.7829 - loglik: -7.1322e+02 - logprior: -2.5649e+00
Fitted a model with MAP estimate = -640.6364
expansions: [(7, 1), (8, 2), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (33, 1), (34, 1), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 2), (102, 1), (111, 2), (114, 2), (121, 2), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 706.7153 - loglik: -7.0346e+02 - logprior: -3.2578e+00
Epoch 2/2
19/19 - 6s - loss: 667.8661 - loglik: -6.6661e+02 - logprior: -1.2584e+00
Fitted a model with MAP estimate = -595.5020
expansions: []
discards: [ 75 125 138 143 153 155 160]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 670.3443 - loglik: -6.6737e+02 - logprior: -2.9728e+00
Epoch 2/2
19/19 - 6s - loss: 664.7646 - loglik: -6.6380e+02 - logprior: -9.6532e-01
Fitted a model with MAP estimate = -594.7749
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 10s - loss: 597.1967 - loglik: -5.9511e+02 - logprior: -2.0830e+00
Epoch 2/10
22/22 - 7s - loss: 592.4983 - loglik: -5.9147e+02 - logprior: -1.0309e+00
Epoch 3/10
22/22 - 7s - loss: 589.2249 - loglik: -5.8814e+02 - logprior: -1.0803e+00
Epoch 4/10
22/22 - 7s - loss: 588.1404 - loglik: -5.8712e+02 - logprior: -1.0192e+00
Epoch 5/10
22/22 - 7s - loss: 587.9550 - loglik: -5.8695e+02 - logprior: -1.0044e+00
Epoch 6/10
22/22 - 7s - loss: 585.4599 - loglik: -5.8440e+02 - logprior: -1.0563e+00
Epoch 7/10
22/22 - 7s - loss: 583.3099 - loglik: -5.8221e+02 - logprior: -1.0968e+00
Epoch 8/10
22/22 - 7s - loss: 580.3943 - loglik: -5.7923e+02 - logprior: -1.1611e+00
Epoch 9/10
22/22 - 7s - loss: 581.5275 - loglik: -5.8031e+02 - logprior: -1.2177e+00
Fitted a model with MAP estimate = -580.7694
Time for alignment: 184.3184
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 973.1793 - loglik: -9.6965e+02 - logprior: -3.5281e+00
Epoch 2/10
19/19 - 4s - loss: 807.5355 - loglik: -8.0578e+02 - logprior: -1.7585e+00
Epoch 3/10
19/19 - 4s - loss: 736.2537 - loglik: -7.3389e+02 - logprior: -2.3671e+00
Epoch 4/10
19/19 - 4s - loss: 726.5969 - loglik: -7.2414e+02 - logprior: -2.4612e+00
Epoch 5/10
19/19 - 4s - loss: 722.6507 - loglik: -7.2024e+02 - logprior: -2.4124e+00
Epoch 6/10
19/19 - 4s - loss: 720.3264 - loglik: -7.1793e+02 - logprior: -2.3991e+00
Epoch 7/10
19/19 - 4s - loss: 718.1726 - loglik: -7.1577e+02 - logprior: -2.4043e+00
Epoch 8/10
19/19 - 4s - loss: 717.0850 - loglik: -7.1463e+02 - logprior: -2.4549e+00
Epoch 9/10
19/19 - 4s - loss: 715.5992 - loglik: -7.1309e+02 - logprior: -2.5127e+00
Epoch 10/10
19/19 - 4s - loss: 716.2209 - loglik: -7.1364e+02 - logprior: -2.5856e+00
Fitted a model with MAP estimate = -640.1660
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (71, 1), (75, 1), (76, 1), (83, 1), (91, 1), (92, 1), (99, 1), (102, 2), (111, 2), (114, 2), (121, 2), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 186 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 706.7482 - loglik: -7.0309e+02 - logprior: -3.6603e+00
Epoch 2/2
19/19 - 6s - loss: 668.5132 - loglik: -6.6685e+02 - logprior: -1.6658e+00
Fitted a model with MAP estimate = -595.2944
expansions: []
discards: [ 50  76 128 139 144 154 156 161]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 669.4584 - loglik: -6.6611e+02 - logprior: -3.3455e+00
Epoch 2/2
19/19 - 6s - loss: 664.8276 - loglik: -6.6349e+02 - logprior: -1.3393e+00
Fitted a model with MAP estimate = -594.5239
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 10s - loss: 596.7605 - loglik: -5.9439e+02 - logprior: -2.3679e+00
Epoch 2/10
22/22 - 7s - loss: 591.1475 - loglik: -5.8999e+02 - logprior: -1.1612e+00
Epoch 3/10
22/22 - 7s - loss: 591.5468 - loglik: -5.9049e+02 - logprior: -1.0554e+00
Fitted a model with MAP estimate = -588.5481
Time for alignment: 147.8186
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 973.0545 - loglik: -9.6953e+02 - logprior: -3.5246e+00
Epoch 2/10
19/19 - 4s - loss: 804.5662 - loglik: -8.0279e+02 - logprior: -1.7752e+00
Epoch 3/10
19/19 - 4s - loss: 735.7451 - loglik: -7.3338e+02 - logprior: -2.3686e+00
Epoch 4/10
19/19 - 4s - loss: 725.3406 - loglik: -7.2289e+02 - logprior: -2.4456e+00
Epoch 5/10
19/19 - 4s - loss: 721.4969 - loglik: -7.1912e+02 - logprior: -2.3784e+00
Epoch 6/10
19/19 - 4s - loss: 718.9807 - loglik: -7.1659e+02 - logprior: -2.3906e+00
Epoch 7/10
19/19 - 4s - loss: 718.6730 - loglik: -7.1630e+02 - logprior: -2.3746e+00
Epoch 8/10
19/19 - 4s - loss: 716.5940 - loglik: -7.1414e+02 - logprior: -2.4532e+00
Epoch 9/10
19/19 - 4s - loss: 715.5418 - loglik: -7.1304e+02 - logprior: -2.5047e+00
Epoch 10/10
19/19 - 4s - loss: 715.9156 - loglik: -7.1334e+02 - logprior: -2.5790e+00
Fitted a model with MAP estimate = -639.6412
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (71, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 2), (102, 1), (111, 2), (114, 1), (121, 2), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 706.1460 - loglik: -7.0285e+02 - logprior: -3.2987e+00
Epoch 2/2
19/19 - 6s - loss: 667.8572 - loglik: -6.6619e+02 - logprior: -1.6717e+00
Fitted a model with MAP estimate = -595.3086
expansions: []
discards: [ 50  76 126 139 153 155 160]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 669.5057 - loglik: -6.6616e+02 - logprior: -3.3479e+00
Epoch 2/2
19/19 - 6s - loss: 664.7133 - loglik: -6.6336e+02 - logprior: -1.3537e+00
Fitted a model with MAP estimate = -594.2549
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 10s - loss: 596.1472 - loglik: -5.9377e+02 - logprior: -2.3739e+00
Epoch 2/10
22/22 - 7s - loss: 593.4073 - loglik: -5.9221e+02 - logprior: -1.1951e+00
Epoch 3/10
22/22 - 7s - loss: 589.6985 - loglik: -5.8864e+02 - logprior: -1.0606e+00
Epoch 4/10
22/22 - 7s - loss: 588.6281 - loglik: -5.8761e+02 - logprior: -1.0137e+00
Epoch 5/10
22/22 - 7s - loss: 584.9604 - loglik: -5.8394e+02 - logprior: -1.0172e+00
Epoch 6/10
22/22 - 7s - loss: 586.3739 - loglik: -5.8533e+02 - logprior: -1.0481e+00
Fitted a model with MAP estimate = -583.5196
Time for alignment: 167.1717
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 972.8927 - loglik: -9.6936e+02 - logprior: -3.5310e+00
Epoch 2/10
19/19 - 4s - loss: 807.5615 - loglik: -8.0580e+02 - logprior: -1.7616e+00
Epoch 3/10
19/19 - 4s - loss: 736.2518 - loglik: -7.3390e+02 - logprior: -2.3498e+00
Epoch 4/10
19/19 - 4s - loss: 725.8528 - loglik: -7.2339e+02 - logprior: -2.4589e+00
Epoch 5/10
19/19 - 4s - loss: 722.1740 - loglik: -7.1975e+02 - logprior: -2.4256e+00
Epoch 6/10
19/19 - 4s - loss: 720.3657 - loglik: -7.1794e+02 - logprior: -2.4230e+00
Epoch 7/10
19/19 - 4s - loss: 718.4355 - loglik: -7.1600e+02 - logprior: -2.4323e+00
Epoch 8/10
19/19 - 4s - loss: 717.0219 - loglik: -7.1456e+02 - logprior: -2.4656e+00
Epoch 9/10
19/19 - 4s - loss: 716.7864 - loglik: -7.1427e+02 - logprior: -2.5137e+00
Epoch 10/10
19/19 - 4s - loss: 715.4357 - loglik: -7.1284e+02 - logprior: -2.5960e+00
Fitted a model with MAP estimate = -640.0765
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (33, 1), (34, 1), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 2), (102, 1), (111, 1), (114, 2), (121, 2), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 706.0743 - loglik: -7.0281e+02 - logprior: -3.2616e+00
Epoch 2/2
19/19 - 6s - loss: 668.1847 - loglik: -6.6695e+02 - logprior: -1.2353e+00
Fitted a model with MAP estimate = -595.5591
expansions: []
discards: [ 75 125 142 152 154 159]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 670.1404 - loglik: -6.6715e+02 - logprior: -2.9881e+00
Epoch 2/2
19/19 - 6s - loss: 665.4064 - loglik: -6.6445e+02 - logprior: -9.5602e-01
Fitted a model with MAP estimate = -594.5391
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 10s - loss: 598.8859 - loglik: -5.9679e+02 - logprior: -2.0916e+00
Epoch 2/10
22/22 - 7s - loss: 590.1565 - loglik: -5.8895e+02 - logprior: -1.2092e+00
Epoch 3/10
22/22 - 7s - loss: 589.3672 - loglik: -5.8829e+02 - logprior: -1.0808e+00
Epoch 4/10
22/22 - 7s - loss: 590.4625 - loglik: -5.8946e+02 - logprior: -1.0073e+00
Fitted a model with MAP estimate = -586.9802
Time for alignment: 153.5901
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 973.2147 - loglik: -9.6968e+02 - logprior: -3.5352e+00
Epoch 2/10
19/19 - 4s - loss: 805.8228 - loglik: -8.0403e+02 - logprior: -1.7940e+00
Epoch 3/10
19/19 - 4s - loss: 736.1389 - loglik: -7.3375e+02 - logprior: -2.3902e+00
Epoch 4/10
19/19 - 4s - loss: 724.5662 - loglik: -7.2208e+02 - logprior: -2.4827e+00
Epoch 5/10
19/19 - 4s - loss: 721.8144 - loglik: -7.1937e+02 - logprior: -2.4419e+00
Epoch 6/10
19/19 - 4s - loss: 719.5756 - loglik: -7.1713e+02 - logprior: -2.4423e+00
Epoch 7/10
19/19 - 4s - loss: 717.9848 - loglik: -7.1554e+02 - logprior: -2.4471e+00
Epoch 8/10
19/19 - 4s - loss: 716.2957 - loglik: -7.1383e+02 - logprior: -2.4661e+00
Epoch 9/10
19/19 - 4s - loss: 715.1700 - loglik: -7.1263e+02 - logprior: -2.5361e+00
Epoch 10/10
19/19 - 4s - loss: 715.6989 - loglik: -7.1309e+02 - logprior: -2.6077e+00
Fitted a model with MAP estimate = -639.1761
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (33, 1), (34, 1), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 1), (61, 1), (71, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 2), (102, 1), (104, 1), (114, 1), (121, 2), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 704.5438 - loglik: -7.0132e+02 - logprior: -3.2257e+00
Epoch 2/2
19/19 - 6s - loss: 668.4202 - loglik: -6.6725e+02 - logprior: -1.1743e+00
Fitted a model with MAP estimate = -595.4609
expansions: []
discards: [124 150 152 157]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 670.3224 - loglik: -6.6735e+02 - logprior: -2.9763e+00
Epoch 2/2
19/19 - 6s - loss: 664.5837 - loglik: -6.6360e+02 - logprior: -9.8635e-01
Fitted a model with MAP estimate = -594.6280
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 10s - loss: 596.2648 - loglik: -5.9409e+02 - logprior: -2.1731e+00
Epoch 2/10
22/22 - 7s - loss: 592.9654 - loglik: -5.9170e+02 - logprior: -1.2633e+00
Epoch 3/10
22/22 - 7s - loss: 592.8842 - loglik: -5.9179e+02 - logprior: -1.0930e+00
Epoch 4/10
22/22 - 7s - loss: 584.4346 - loglik: -5.8341e+02 - logprior: -1.0205e+00
Epoch 5/10
22/22 - 7s - loss: 588.5482 - loglik: -5.8752e+02 - logprior: -1.0304e+00
Fitted a model with MAP estimate = -585.1977
Time for alignment: 159.7793
Computed alignments with likelihoods: ['-580.7694', '-588.5481', '-583.5196', '-586.9802', '-585.1977']
Best model has likelihood: -580.7694  (prior= -1.3049 )
time for generating output: 0.2184
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hla.projection.fasta
SP score = 1.0
Training of 5 independent models on file ltn.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5295ee9640>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f529708aee0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f0d46e80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531cafc130>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5288390fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f53351b0070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f529738a880>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52d8385c40>, <__main__.SimpleDirichletPrior object at 0x7f5b133c9040>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 1209.4159 - loglik: -1.1668e+03 - logprior: -4.2598e+01
Epoch 2/10
12/12 - 3s - loss: 1117.0602 - loglik: -1.1118e+03 - logprior: -5.2119e+00
Epoch 3/10
12/12 - 3s - loss: 1054.9154 - loglik: -1.0536e+03 - logprior: -1.3292e+00
Epoch 4/10
12/12 - 3s - loss: 1025.6244 - loglik: -1.0251e+03 - logprior: -4.7528e-01
Epoch 5/10
12/12 - 3s - loss: 1021.7485 - loglik: -1.0219e+03 - logprior: 0.1710
Epoch 6/10
12/12 - 3s - loss: 1016.9684 - loglik: -1.0175e+03 - logprior: 0.5637
Epoch 7/10
12/12 - 3s - loss: 1012.5708 - loglik: -1.0134e+03 - logprior: 0.8068
Epoch 8/10
12/12 - 3s - loss: 1015.3436 - loglik: -1.0163e+03 - logprior: 0.9636
Fitted a model with MAP estimate = -1013.5763
expansions: [(11, 3), (19, 1), (30, 2), (31, 2), (32, 1), (46, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (75, 2), (76, 2), (77, 2), (86, 1), (90, 1), (91, 1), (101, 1), (102, 1), (119, 1), (120, 2), (127, 1), (129, 4), (146, 3), (149, 2), (153, 1), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 1052.2279 - loglik: -1.0123e+03 - logprior: -3.9901e+01
Epoch 2/2
12/12 - 4s - loss: 1011.4255 - loglik: -9.9964e+02 - logprior: -1.1786e+01
Fitted a model with MAP estimate = -1006.6373
expansions: [(0, 4), (109, 1), (154, 5)]
discards: [  0  35  90 128 147 159 160 185 214]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 1028.1074 - loglik: -1.0001e+03 - logprior: -2.7987e+01
Epoch 2/2
12/12 - 4s - loss: 1001.3696 - loglik: -9.9993e+02 - logprior: -1.4416e+00
Fitted a model with MAP estimate = -993.2538
expansions: [(164, 1)]
discards: [  1   2   3 154 155 156]
Re-initialized the encoder parameters.
Fitting a model of length 224 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 1022.9262 - loglik: -9.9643e+02 - logprior: -2.6500e+01
Epoch 2/10
12/12 - 4s - loss: 997.0826 - loglik: -9.9628e+02 - logprior: -8.0022e-01
Epoch 3/10
12/12 - 4s - loss: 986.8438 - loglik: -9.9087e+02 - logprior: 4.0218
Epoch 4/10
12/12 - 4s - loss: 989.3965 - loglik: -9.9547e+02 - logprior: 6.0732
Fitted a model with MAP estimate = -986.6382
Time for alignment: 86.3562
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 1210.3063 - loglik: -1.1677e+03 - logprior: -4.2603e+01
Epoch 2/10
12/12 - 3s - loss: 1113.9314 - loglik: -1.1087e+03 - logprior: -5.2065e+00
Epoch 3/10
12/12 - 3s - loss: 1055.5403 - loglik: -1.0542e+03 - logprior: -1.3831e+00
Epoch 4/10
12/12 - 3s - loss: 1024.1271 - loglik: -1.0236e+03 - logprior: -5.0613e-01
Epoch 5/10
12/12 - 3s - loss: 1021.3925 - loglik: -1.0216e+03 - logprior: 0.2328
Epoch 6/10
12/12 - 3s - loss: 1019.1349 - loglik: -1.0198e+03 - logprior: 0.6225
Epoch 7/10
12/12 - 3s - loss: 1014.6055 - loglik: -1.0154e+03 - logprior: 0.7811
Epoch 8/10
12/12 - 3s - loss: 1016.7341 - loglik: -1.0177e+03 - logprior: 1.0042
Fitted a model with MAP estimate = -1015.1960
expansions: [(11, 3), (21, 1), (30, 2), (31, 3), (32, 1), (41, 1), (45, 1), (60, 2), (61, 1), (62, 1), (63, 1), (75, 3), (76, 1), (77, 2), (86, 1), (87, 1), (90, 1), (101, 1), (102, 2), (120, 2), (126, 5), (127, 1), (129, 2), (130, 1), (138, 2), (139, 1), (149, 1), (154, 2), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 1052.9918 - loglik: -1.0130e+03 - logprior: -3.9971e+01
Epoch 2/2
12/12 - 4s - loss: 1010.6519 - loglik: -9.9906e+02 - logprior: -1.1593e+01
Fitted a model with MAP estimate = -1005.4565
expansions: [(0, 4)]
discards: [  0  35  36  72  97 150 156 157 158 177 220]
Re-initialized the encoder parameters.
Fitting a model of length 227 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 1029.5302 - loglik: -1.0020e+03 - logprior: -2.7553e+01
Epoch 2/2
12/12 - 4s - loss: 1000.5327 - loglik: -9.9926e+02 - logprior: -1.2760e+00
Fitted a model with MAP estimate = -993.9249
expansions: []
discards: [  1   2   3 128]
Re-initialized the encoder parameters.
Fitting a model of length 223 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 1026.8055 - loglik: -1.0004e+03 - logprior: -2.6441e+01
Epoch 2/10
12/12 - 4s - loss: 992.4907 - loglik: -9.9171e+02 - logprior: -7.7575e-01
Epoch 3/10
12/12 - 4s - loss: 987.5062 - loglik: -9.9147e+02 - logprior: 3.9672
Epoch 4/10
12/12 - 4s - loss: 988.8948 - loglik: -9.9494e+02 - logprior: 6.0437
Fitted a model with MAP estimate = -986.8501
Time for alignment: 85.3267
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 1209.3832 - loglik: -1.1668e+03 - logprior: -4.2592e+01
Epoch 2/10
12/12 - 3s - loss: 1118.6064 - loglik: -1.1134e+03 - logprior: -5.2128e+00
Epoch 3/10
12/12 - 3s - loss: 1054.8114 - loglik: -1.0535e+03 - logprior: -1.3293e+00
Epoch 4/10
12/12 - 3s - loss: 1030.5543 - loglik: -1.0301e+03 - logprior: -4.3504e-01
Epoch 5/10
12/12 - 3s - loss: 1021.6126 - loglik: -1.0219e+03 - logprior: 0.3345
Epoch 6/10
12/12 - 3s - loss: 1017.9016 - loglik: -1.0186e+03 - logprior: 0.7425
Epoch 7/10
12/12 - 3s - loss: 1017.8151 - loglik: -1.0187e+03 - logprior: 0.8633
Epoch 8/10
12/12 - 3s - loss: 1014.4913 - loglik: -1.0156e+03 - logprior: 1.1286
Epoch 9/10
12/12 - 3s - loss: 1016.0978 - loglik: -1.0174e+03 - logprior: 1.3398
Fitted a model with MAP estimate = -1015.0873
expansions: [(0, 3), (10, 1), (24, 1), (31, 2), (32, 1), (41, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (74, 2), (76, 2), (77, 2), (86, 1), (90, 1), (92, 1), (101, 1), (102, 1), (127, 2), (129, 3), (146, 1), (147, 2), (149, 2), (152, 2), (153, 2), (173, 2), (174, 1), (175, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 227 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 1054.2314 - loglik: -1.0116e+03 - logprior: -4.2613e+01
Epoch 2/2
12/12 - 4s - loss: 1009.2357 - loglik: -1.0030e+03 - logprior: -6.2159e+00
Fitted a model with MAP estimate = -1000.9168
expansions: [(109, 1)]
discards: [  0   1   2  89  93 182 187 213]
Re-initialized the encoder parameters.
Fitting a model of length 220 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 1031.2885 - loglik: -1.0016e+03 - logprior: -2.9735e+01
Epoch 2/2
12/12 - 4s - loss: 999.5873 - loglik: -9.9797e+02 - logprior: -1.6218e+00
Fitted a model with MAP estimate = -995.2799
expansions: [(0, 4)]
discards: [176]
Re-initialized the encoder parameters.
Fitting a model of length 223 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 1037.2867 - loglik: -9.9895e+02 - logprior: -3.8338e+01
Epoch 2/10
12/12 - 4s - loss: 1002.5293 - loglik: -9.9855e+02 - logprior: -3.9782e+00
Epoch 3/10
12/12 - 4s - loss: 988.5718 - loglik: -9.9175e+02 - logprior: 3.1765
Epoch 4/10
12/12 - 4s - loss: 990.9277 - loglik: -9.9679e+02 - logprior: 5.8606
Fitted a model with MAP estimate = -987.4338
Time for alignment: 88.5515
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 1210.2074 - loglik: -1.1676e+03 - logprior: -4.2608e+01
Epoch 2/10
12/12 - 3s - loss: 1117.2531 - loglik: -1.1121e+03 - logprior: -5.1727e+00
Epoch 3/10
12/12 - 3s - loss: 1053.9403 - loglik: -1.0527e+03 - logprior: -1.2478e+00
Epoch 4/10
12/12 - 3s - loss: 1027.1707 - loglik: -1.0268e+03 - logprior: -3.4626e-01
Epoch 5/10
12/12 - 3s - loss: 1024.6632 - loglik: -1.0248e+03 - logprior: 0.1752
Epoch 6/10
12/12 - 3s - loss: 1015.5344 - loglik: -1.0161e+03 - logprior: 0.6122
Epoch 7/10
12/12 - 3s - loss: 1017.7339 - loglik: -1.0185e+03 - logprior: 0.8109
Fitted a model with MAP estimate = -1016.2927
expansions: [(11, 3), (19, 1), (30, 2), (31, 3), (32, 1), (41, 1), (48, 1), (60, 1), (61, 1), (62, 1), (63, 1), (75, 1), (76, 2), (77, 2), (78, 2), (86, 1), (90, 2), (92, 1), (100, 1), (101, 1), (128, 1), (130, 2), (131, 3), (146, 3), (149, 2), (153, 1), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 1053.7161 - loglik: -1.0138e+03 - logprior: -3.9900e+01
Epoch 2/2
12/12 - 4s - loss: 1015.5532 - loglik: -1.0037e+03 - logprior: -1.1900e+01
Fitted a model with MAP estimate = -1008.2508
expansions: [(0, 4), (115, 1), (155, 4)]
discards: [  0  35  36  96 130 161 162 186 215]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 1031.4056 - loglik: -1.0034e+03 - logprior: -2.7979e+01
Epoch 2/2
12/12 - 4s - loss: 998.7291 - loglik: -9.9733e+02 - logprior: -1.3993e+00
Fitted a model with MAP estimate = -995.3483
expansions: [(168, 1)]
discards: [  1   2   3 155 156]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 1026.4186 - loglik: -9.9990e+02 - logprior: -2.6515e+01
Epoch 2/10
12/12 - 4s - loss: 997.8979 - loglik: -9.9710e+02 - logprior: -7.9622e-01
Epoch 3/10
12/12 - 4s - loss: 989.4391 - loglik: -9.9348e+02 - logprior: 4.0406
Epoch 4/10
12/12 - 4s - loss: 990.3380 - loglik: -9.9641e+02 - logprior: 6.0685
Fitted a model with MAP estimate = -987.3824
Time for alignment: 83.3525
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 1209.2094 - loglik: -1.1666e+03 - logprior: -4.2595e+01
Epoch 2/10
12/12 - 3s - loss: 1118.1290 - loglik: -1.1129e+03 - logprior: -5.2022e+00
Epoch 3/10
12/12 - 3s - loss: 1057.1575 - loglik: -1.0557e+03 - logprior: -1.4279e+00
Epoch 4/10
12/12 - 3s - loss: 1029.9381 - loglik: -1.0294e+03 - logprior: -5.7405e-01
Epoch 5/10
12/12 - 3s - loss: 1020.2045 - loglik: -1.0202e+03 - logprior: 0.0168
Epoch 6/10
12/12 - 3s - loss: 1016.3069 - loglik: -1.0166e+03 - logprior: 0.2557
Epoch 7/10
12/12 - 3s - loss: 1015.8262 - loglik: -1.0163e+03 - logprior: 0.4941
Epoch 8/10
12/12 - 3s - loss: 1012.8120 - loglik: -1.0135e+03 - logprior: 0.7167
Epoch 9/10
12/12 - 3s - loss: 1012.8055 - loglik: -1.0136e+03 - logprior: 0.7806
Epoch 10/10
12/12 - 3s - loss: 1013.0925 - loglik: -1.0140e+03 - logprior: 0.8850
Fitted a model with MAP estimate = -1012.2095
expansions: [(11, 3), (19, 1), (30, 2), (31, 2), (39, 1), (41, 1), (48, 1), (58, 1), (61, 2), (62, 2), (74, 1), (75, 2), (76, 1), (77, 1), (86, 1), (87, 1), (90, 2), (101, 1), (102, 1), (118, 1), (126, 5), (127, 1), (129, 2), (130, 1), (137, 1), (145, 1), (146, 2), (151, 1), (154, 1), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 1046.6245 - loglik: -1.0070e+03 - logprior: -3.9657e+01
Epoch 2/2
12/12 - 4s - loss: 1009.9836 - loglik: -9.9870e+02 - logprior: -1.1282e+01
Fitted a model with MAP estimate = -1002.3638
expansions: [(0, 4)]
discards: [ 35 155 217]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 1022.8932 - loglik: -9.9559e+02 - logprior: -2.7301e+01
Epoch 2/2
12/12 - 4s - loss: 997.1629 - loglik: -9.9618e+02 - logprior: -9.8191e-01
Fitted a model with MAP estimate = -990.8717
expansions: []
discards: [  1   2   3   4 156 157]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 9s - loss: 1021.3337 - loglik: -9.9518e+02 - logprior: -2.6153e+01
Epoch 2/10
12/12 - 4s - loss: 995.7704 - loglik: -9.9531e+02 - logprior: -4.5579e-01
Epoch 3/10
12/12 - 4s - loss: 985.5674 - loglik: -9.8988e+02 - logprior: 4.3147
Epoch 4/10
12/12 - 4s - loss: 986.4212 - loglik: -9.9276e+02 - logprior: 6.3417
Fitted a model with MAP estimate = -984.7455
Time for alignment: 91.4089
Computed alignments with likelihoods: ['-986.6382', '-986.8501', '-987.4338', '-987.3824', '-984.7455']
Best model has likelihood: -984.7455  (prior= 7.0528 )
time for generating output: 0.2906
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ltn.projection.fasta
SP score = 0.9418241130895746
Training of 5 independent models on file aadh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f526fd61bb0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5273400cd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f527e623310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526ee05e50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526ee05dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526ee05f40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52718fdbe0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f526f4ffd00>, <__main__.SimpleDirichletPrior object at 0x7f52852d5430>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 11s - loss: 1253.2400 - loglik: -1.2436e+03 - logprior: -9.6724e+00
Epoch 2/10
21/21 - 7s - loss: 1123.0463 - loglik: -1.1209e+03 - logprior: -2.1264e+00
Epoch 3/10
21/21 - 8s - loss: 1083.6687 - loglik: -1.0811e+03 - logprior: -2.5578e+00
Epoch 4/10
21/21 - 8s - loss: 1073.4060 - loglik: -1.0710e+03 - logprior: -2.3877e+00
Epoch 5/10
21/21 - 7s - loss: 1074.8580 - loglik: -1.0725e+03 - logprior: -2.3545e+00
Fitted a model with MAP estimate = -1071.9572
expansions: [(15, 1), (16, 3), (19, 1), (20, 2), (21, 3), (22, 1), (24, 1), (37, 1), (39, 2), (40, 1), (50, 1), (60, 1), (63, 3), (66, 1), (76, 7), (78, 1), (80, 1), (96, 1), (99, 1), (119, 2), (121, 1), (122, 1), (124, 1), (143, 1), (146, 1), (147, 2), (154, 1), (160, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (178, 1), (181, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 1070.6670 - loglik: -1.0601e+03 - logprior: -1.0558e+01
Epoch 2/2
21/21 - 9s - loss: 1044.0454 - loglik: -1.0411e+03 - logprior: -2.9923e+00
Fitted a model with MAP estimate = -1038.6339
expansions: []
discards: [ 17  28 151 217]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 1046.2169 - loglik: -1.0394e+03 - logprior: -6.7829e+00
Epoch 2/2
21/21 - 10s - loss: 1035.2240 - loglik: -1.0356e+03 - logprior: 0.3296
Fitted a model with MAP estimate = -1033.5470
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 1039.8530 - loglik: -1.0339e+03 - logprior: -5.9952e+00
Epoch 2/10
21/21 - 10s - loss: 1036.8282 - loglik: -1.0375e+03 - logprior: 0.6415
Epoch 3/10
21/21 - 9s - loss: 1033.5066 - loglik: -1.0350e+03 - logprior: 1.5055
Epoch 4/10
21/21 - 10s - loss: 1031.7653 - loglik: -1.0336e+03 - logprior: 1.8564
Epoch 5/10
21/21 - 9s - loss: 1028.8722 - loglik: -1.0309e+03 - logprior: 2.0148
Epoch 6/10
21/21 - 10s - loss: 1031.1439 - loglik: -1.0332e+03 - logprior: 2.1045
Fitted a model with MAP estimate = -1029.2416
Time for alignment: 179.5790
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 14s - loss: 1252.6942 - loglik: -1.2430e+03 - logprior: -9.6928e+00
Epoch 2/10
21/21 - 8s - loss: 1128.2217 - loglik: -1.1262e+03 - logprior: -2.0475e+00
Epoch 3/10
21/21 - 7s - loss: 1091.3552 - loglik: -1.0888e+03 - logprior: -2.5533e+00
Epoch 4/10
21/21 - 8s - loss: 1081.8695 - loglik: -1.0792e+03 - logprior: -2.6906e+00
Epoch 5/10
21/21 - 7s - loss: 1079.2078 - loglik: -1.0766e+03 - logprior: -2.5976e+00
Epoch 6/10
21/21 - 8s - loss: 1077.8625 - loglik: -1.0752e+03 - logprior: -2.6837e+00
Epoch 7/10
21/21 - 8s - loss: 1076.8916 - loglik: -1.0742e+03 - logprior: -2.7214e+00
Epoch 8/10
21/21 - 7s - loss: 1077.4342 - loglik: -1.0747e+03 - logprior: -2.7601e+00
Fitted a model with MAP estimate = -1076.4859
expansions: [(7, 1), (17, 1), (18, 1), (19, 1), (20, 3), (21, 2), (22, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (50, 1), (60, 1), (63, 3), (66, 1), (76, 7), (78, 1), (97, 1), (98, 1), (100, 1), (115, 1), (116, 1), (119, 1), (121, 1), (122, 1), (131, 1), (143, 1), (146, 1), (147, 1), (150, 1), (158, 1), (161, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (178, 1), (179, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 1073.5341 - loglik: -1.0630e+03 - logprior: -1.0530e+01
Epoch 2/2
21/21 - 10s - loss: 1050.2081 - loglik: -1.0470e+03 - logprior: -3.1720e+00
Fitted a model with MAP estimate = -1043.8437
expansions: [(0, 5), (97, 1)]
discards: [  0  24  49  80  85 217]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 1051.7795 - loglik: -1.0452e+03 - logprior: -6.5496e+00
Epoch 2/2
21/21 - 10s - loss: 1041.5701 - loglik: -1.0415e+03 - logprior: -2.0711e-02
Fitted a model with MAP estimate = -1038.7959
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 13s - loss: 1048.3788 - loglik: -1.0423e+03 - logprior: -6.0333e+00
Epoch 2/10
21/21 - 10s - loss: 1041.5828 - loglik: -1.0422e+03 - logprior: 0.6308
Epoch 3/10
21/21 - 10s - loss: 1039.2056 - loglik: -1.0407e+03 - logprior: 1.4755
Epoch 4/10
21/21 - 9s - loss: 1035.9865 - loglik: -1.0378e+03 - logprior: 1.8403
Epoch 5/10
21/21 - 9s - loss: 1037.3468 - loglik: -1.0394e+03 - logprior: 2.0151
Fitted a model with MAP estimate = -1035.6494
Time for alignment: 198.4283
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 11s - loss: 1251.6663 - loglik: -1.2420e+03 - logprior: -9.6748e+00
Epoch 2/10
21/21 - 8s - loss: 1129.8020 - loglik: -1.1278e+03 - logprior: -1.9801e+00
Epoch 3/10
21/21 - 8s - loss: 1090.9128 - loglik: -1.0884e+03 - logprior: -2.5231e+00
Epoch 4/10
21/21 - 7s - loss: 1078.8083 - loglik: -1.0765e+03 - logprior: -2.3037e+00
Epoch 5/10
21/21 - 8s - loss: 1076.6190 - loglik: -1.0743e+03 - logprior: -2.3183e+00
Epoch 6/10
21/21 - 7s - loss: 1076.7941 - loglik: -1.0744e+03 - logprior: -2.4373e+00
Fitted a model with MAP estimate = -1075.7121
expansions: [(7, 1), (17, 1), (18, 1), (19, 1), (20, 3), (21, 3), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (40, 1), (46, 1), (62, 4), (75, 7), (78, 1), (94, 1), (96, 1), (99, 1), (119, 2), (121, 1), (122, 1), (146, 2), (147, 1), (149, 1), (154, 1), (160, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (172, 1), (178, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 1073.2311 - loglik: -1.0627e+03 - logprior: -1.0580e+01
Epoch 2/2
21/21 - 10s - loss: 1043.5990 - loglik: -1.0405e+03 - logprior: -3.0733e+00
Fitted a model with MAP estimate = -1041.6292
expansions: [(0, 5)]
discards: [  0  24  28 150 215]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 1050.0472 - loglik: -1.0435e+03 - logprior: -6.5898e+00
Epoch 2/2
21/21 - 10s - loss: 1038.8123 - loglik: -1.0388e+03 - logprior: -4.3539e-03
Fitted a model with MAP estimate = -1036.3572
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 13s - loss: 1046.8884 - loglik: -1.0409e+03 - logprior: -6.0197e+00
Epoch 2/10
21/21 - 10s - loss: 1038.0938 - loglik: -1.0387e+03 - logprior: 0.6384
Epoch 3/10
21/21 - 10s - loss: 1036.2545 - loglik: -1.0377e+03 - logprior: 1.4881
Epoch 4/10
21/21 - 10s - loss: 1033.8271 - loglik: -1.0357e+03 - logprior: 1.8429
Epoch 5/10
21/21 - 9s - loss: 1035.6439 - loglik: -1.0376e+03 - logprior: 1.9855
Fitted a model with MAP estimate = -1033.1712
Time for alignment: 180.9610
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 11s - loss: 1253.6516 - loglik: -1.2440e+03 - logprior: -9.6626e+00
Epoch 2/10
21/21 - 7s - loss: 1126.0223 - loglik: -1.1240e+03 - logprior: -2.0193e+00
Epoch 3/10
21/21 - 8s - loss: 1090.5352 - loglik: -1.0879e+03 - logprior: -2.6053e+00
Epoch 4/10
21/21 - 7s - loss: 1081.9636 - loglik: -1.0797e+03 - logprior: -2.2973e+00
Epoch 5/10
21/21 - 8s - loss: 1079.1693 - loglik: -1.0769e+03 - logprior: -2.2791e+00
Epoch 6/10
21/21 - 7s - loss: 1077.2777 - loglik: -1.0749e+03 - logprior: -2.4149e+00
Epoch 7/10
21/21 - 7s - loss: 1080.9143 - loglik: -1.0785e+03 - logprior: -2.4307e+00
Fitted a model with MAP estimate = -1077.9119
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (50, 1), (60, 1), (63, 2), (65, 1), (75, 9), (77, 1), (96, 1), (99, 2), (114, 1), (119, 2), (121, 1), (122, 1), (131, 1), (143, 1), (146, 1), (147, 1), (150, 1), (158, 1), (160, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (177, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 14s - loss: 1076.2468 - loglik: -1.0656e+03 - logprior: -1.0638e+01
Epoch 2/2
21/21 - 9s - loss: 1053.3744 - loglik: -1.0501e+03 - logprior: -3.2959e+00
Fitted a model with MAP estimate = -1046.7247
expansions: [(0, 5)]
discards: [  0  13  18  28  50 132 147 153 219]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 1059.1976 - loglik: -1.0525e+03 - logprior: -6.6654e+00
Epoch 2/2
21/21 - 9s - loss: 1043.9124 - loglik: -1.0438e+03 - logprior: -1.1605e-01
Fitted a model with MAP estimate = -1043.6894
expansions: [(146, 1)]
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 13s - loss: 1053.7742 - loglik: -1.0477e+03 - logprior: -6.0935e+00
Epoch 2/10
21/21 - 10s - loss: 1044.3005 - loglik: -1.0449e+03 - logprior: 0.6176
Epoch 3/10
21/21 - 10s - loss: 1043.0154 - loglik: -1.0445e+03 - logprior: 1.4467
Epoch 4/10
21/21 - 9s - loss: 1041.2859 - loglik: -1.0431e+03 - logprior: 1.8073
Epoch 5/10
21/21 - 10s - loss: 1041.5587 - loglik: -1.0435e+03 - logprior: 1.9864
Fitted a model with MAP estimate = -1040.1293
Time for alignment: 188.3773
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 13s - loss: 1253.9658 - loglik: -1.2443e+03 - logprior: -9.6451e+00
Epoch 2/10
21/21 - 7s - loss: 1125.7809 - loglik: -1.1238e+03 - logprior: -1.9965e+00
Epoch 3/10
21/21 - 8s - loss: 1091.7498 - loglik: -1.0892e+03 - logprior: -2.5320e+00
Epoch 4/10
21/21 - 7s - loss: 1084.0217 - loglik: -1.0817e+03 - logprior: -2.3485e+00
Epoch 5/10
21/21 - 8s - loss: 1081.2386 - loglik: -1.0789e+03 - logprior: -2.3157e+00
Epoch 6/10
21/21 - 8s - loss: 1080.0360 - loglik: -1.0777e+03 - logprior: -2.3065e+00
Epoch 7/10
21/21 - 8s - loss: 1079.6346 - loglik: -1.0773e+03 - logprior: -2.3583e+00
Epoch 8/10
21/21 - 7s - loss: 1078.8131 - loglik: -1.0764e+03 - logprior: -2.4060e+00
Epoch 9/10
21/21 - 8s - loss: 1080.2504 - loglik: -1.0778e+03 - logprior: -2.4950e+00
Fitted a model with MAP estimate = -1079.1245
expansions: [(14, 1), (17, 2), (20, 1), (21, 2), (22, 3), (23, 1), (25, 1), (38, 1), (39, 1), (40, 1), (41, 1), (51, 1), (61, 1), (64, 4), (66, 1), (74, 2), (75, 7), (78, 1), (97, 1), (100, 2), (116, 1), (119, 1), (120, 1), (121, 1), (122, 1), (147, 1), (148, 2), (155, 1), (161, 2), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (177, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 1075.6017 - loglik: -1.0650e+03 - logprior: -1.0597e+01
Epoch 2/2
21/21 - 10s - loss: 1051.6359 - loglik: -1.0484e+03 - logprior: -3.2097e+00
Fitted a model with MAP estimate = -1046.2356
expansions: [(2, 1), (152, 1)]
discards: [  0  13  17  28  82  96 134 219]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 1061.5779 - loglik: -1.0522e+03 - logprior: -9.3919e+00
Epoch 2/2
21/21 - 10s - loss: 1047.4714 - loglik: -1.0460e+03 - logprior: -1.4380e+00
Fitted a model with MAP estimate = -1042.8052
expansions: []
discards: [ 82 145]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 13s - loss: 1053.1498 - loglik: -1.0470e+03 - logprior: -6.1264e+00
Epoch 2/10
21/21 - 10s - loss: 1045.6166 - loglik: -1.0462e+03 - logprior: 0.5552
Epoch 3/10
21/21 - 9s - loss: 1040.8240 - loglik: -1.0422e+03 - logprior: 1.4011
Epoch 4/10
21/21 - 10s - loss: 1041.9271 - loglik: -1.0437e+03 - logprior: 1.7736
Fitted a model with MAP estimate = -1040.1245
Time for alignment: 195.0094
Computed alignments with likelihoods: ['-1029.2416', '-1035.6494', '-1033.1712', '-1040.1293', '-1040.1245']
Best model has likelihood: -1029.2416  (prior= 2.1959 )
time for generating output: 0.4225
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aadh.projection.fasta
SP score = 0.5875541329316513
Training of 5 independent models on file gluts.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52d94359a0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5285e84430>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52748fbd30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52748cdca0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f527e840b80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5277b483d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f527f36b7c0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52944ec4c0>, <__main__.SimpleDirichletPrior object at 0x7f530d81cf40>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 571.1348 - loglik: -5.6780e+02 - logprior: -3.3329e+00
Epoch 2/10
19/19 - 3s - loss: 540.9814 - loglik: -5.4010e+02 - logprior: -8.8463e-01
Epoch 3/10
19/19 - 3s - loss: 530.7514 - loglik: -5.2989e+02 - logprior: -8.6095e-01
Epoch 4/10
19/19 - 3s - loss: 528.9634 - loglik: -5.2816e+02 - logprior: -8.0203e-01
Epoch 5/10
19/19 - 3s - loss: 527.1069 - loglik: -5.2630e+02 - logprior: -8.0925e-01
Epoch 6/10
19/19 - 3s - loss: 526.2320 - loglik: -5.2541e+02 - logprior: -8.2309e-01
Epoch 7/10
19/19 - 3s - loss: 526.7298 - loglik: -5.2588e+02 - logprior: -8.4638e-01
Fitted a model with MAP estimate = -522.9610
expansions: [(0, 10), (16, 2), (20, 1), (21, 2), (22, 1), (35, 1), (57, 2), (59, 3), (61, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 532.6968 - loglik: -5.2879e+02 - logprior: -3.9115e+00
Epoch 2/2
19/19 - 4s - loss: 525.2267 - loglik: -5.2411e+02 - logprior: -1.1123e+00
Fitted a model with MAP estimate = -521.6014
expansions: [(0, 8), (35, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 75]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 528.9590 - loglik: -5.2517e+02 - logprior: -3.7916e+00
Epoch 2/2
19/19 - 4s - loss: 524.7643 - loglik: -5.2355e+02 - logprior: -1.2119e+00
Fitted a model with MAP estimate = -521.4730
expansions: [(0, 7), (35, 1)]
discards: [ 0  1  2  3  4  5  8 77 78]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 525.7246 - loglik: -5.2253e+02 - logprior: -3.1962e+00
Epoch 2/10
19/19 - 4s - loss: 521.7736 - loglik: -5.2084e+02 - logprior: -9.3221e-01
Epoch 3/10
19/19 - 3s - loss: 520.9016 - loglik: -5.2010e+02 - logprior: -8.0506e-01
Epoch 4/10
19/19 - 4s - loss: 519.4341 - loglik: -5.1866e+02 - logprior: -7.7005e-01
Epoch 5/10
19/19 - 3s - loss: 518.4610 - loglik: -5.1773e+02 - logprior: -7.3499e-01
Epoch 6/10
19/19 - 3s - loss: 517.3253 - loglik: -5.1658e+02 - logprior: -7.4382e-01
Epoch 7/10
19/19 - 4s - loss: 516.8580 - loglik: -5.1610e+02 - logprior: -7.5656e-01
Epoch 8/10
19/19 - 4s - loss: 516.0545 - loglik: -5.1525e+02 - logprior: -8.0845e-01
Epoch 9/10
19/19 - 4s - loss: 515.3235 - loglik: -5.1448e+02 - logprior: -8.3856e-01
Epoch 10/10
19/19 - 3s - loss: 515.2617 - loglik: -5.1437e+02 - logprior: -8.9289e-01
Fitted a model with MAP estimate = -514.7866
Time for alignment: 109.5986
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 570.5338 - loglik: -5.6721e+02 - logprior: -3.3283e+00
Epoch 2/10
19/19 - 3s - loss: 538.7147 - loglik: -5.3783e+02 - logprior: -8.8201e-01
Epoch 3/10
19/19 - 3s - loss: 530.3469 - loglik: -5.2950e+02 - logprior: -8.4211e-01
Epoch 4/10
19/19 - 3s - loss: 527.2488 - loglik: -5.2649e+02 - logprior: -7.5915e-01
Epoch 5/10
19/19 - 3s - loss: 526.7039 - loglik: -5.2594e+02 - logprior: -7.6796e-01
Epoch 6/10
19/19 - 3s - loss: 525.4534 - loglik: -5.2467e+02 - logprior: -7.7958e-01
Epoch 7/10
19/19 - 3s - loss: 524.7895 - loglik: -5.2397e+02 - logprior: -8.1650e-01
Epoch 8/10
19/19 - 3s - loss: 524.0433 - loglik: -5.2315e+02 - logprior: -8.9022e-01
Epoch 9/10
19/19 - 3s - loss: 524.1721 - loglik: -5.2321e+02 - logprior: -9.5912e-01
Fitted a model with MAP estimate = -521.1629
expansions: [(0, 10), (11, 2), (12, 4), (13, 1), (14, 2), (54, 1), (57, 2), (59, 3), (61, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 533.7690 - loglik: -5.2952e+02 - logprior: -4.2492e+00
Epoch 2/2
19/19 - 4s - loss: 524.4940 - loglik: -5.2335e+02 - logprior: -1.1419e+00
Fitted a model with MAP estimate = -521.0914
expansions: [(0, 8)]
discards: [ 1  2  3  4  5  6  7  8  9 31 78 83]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 528.5218 - loglik: -5.2482e+02 - logprior: -3.6972e+00
Epoch 2/2
19/19 - 4s - loss: 524.7855 - loglik: -5.2363e+02 - logprior: -1.1564e+00
Fitted a model with MAP estimate = -521.2489
expansions: [(0, 7)]
discards: [ 0  1  2  3  4  8 78 79]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 525.3125 - loglik: -5.2217e+02 - logprior: -3.1447e+00
Epoch 2/10
19/19 - 3s - loss: 521.1675 - loglik: -5.2029e+02 - logprior: -8.8190e-01
Epoch 3/10
19/19 - 3s - loss: 520.6705 - loglik: -5.1992e+02 - logprior: -7.5006e-01
Epoch 4/10
19/19 - 3s - loss: 520.4442 - loglik: -5.1974e+02 - logprior: -7.0523e-01
Epoch 5/10
19/19 - 3s - loss: 517.3293 - loglik: -5.1664e+02 - logprior: -6.9109e-01
Epoch 6/10
19/19 - 3s - loss: 518.0118 - loglik: -5.1733e+02 - logprior: -6.7750e-01
Fitted a model with MAP estimate = -516.5970
Time for alignment: 100.7875
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 570.2590 - loglik: -5.6693e+02 - logprior: -3.3321e+00
Epoch 2/10
19/19 - 3s - loss: 538.6548 - loglik: -5.3777e+02 - logprior: -8.8560e-01
Epoch 3/10
19/19 - 3s - loss: 530.2219 - loglik: -5.2938e+02 - logprior: -8.4660e-01
Epoch 4/10
19/19 - 3s - loss: 528.4788 - loglik: -5.2769e+02 - logprior: -7.8726e-01
Epoch 5/10
19/19 - 3s - loss: 527.2811 - loglik: -5.2648e+02 - logprior: -8.0195e-01
Epoch 6/10
19/19 - 3s - loss: 526.6611 - loglik: -5.2583e+02 - logprior: -8.2613e-01
Epoch 7/10
19/19 - 3s - loss: 524.4218 - loglik: -5.2356e+02 - logprior: -8.6040e-01
Epoch 8/10
19/19 - 3s - loss: 525.1912 - loglik: -5.2426e+02 - logprior: -9.3342e-01
Fitted a model with MAP estimate = -522.2479
expansions: [(0, 9), (16, 3), (19, 2), (20, 1), (54, 1), (57, 2), (59, 2), (61, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 533.8253 - loglik: -5.2985e+02 - logprior: -3.9709e+00
Epoch 2/2
19/19 - 3s - loss: 525.5428 - loglik: -5.2439e+02 - logprior: -1.1540e+00
Fitted a model with MAP estimate = -522.0844
expansions: [(0, 8), (25, 1), (26, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 34 74 81 82]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 529.8730 - loglik: -5.2606e+02 - logprior: -3.8125e+00
Epoch 2/2
19/19 - 3s - loss: 525.1009 - loglik: -5.2397e+02 - logprior: -1.1328e+00
Fitted a model with MAP estimate = -521.9251
expansions: [(0, 8)]
discards: [0 1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 526.0835 - loglik: -5.2302e+02 - logprior: -3.0678e+00
Epoch 2/10
19/19 - 3s - loss: 522.3209 - loglik: -5.2149e+02 - logprior: -8.3199e-01
Epoch 3/10
19/19 - 4s - loss: 521.0900 - loglik: -5.2036e+02 - logprior: -7.3353e-01
Epoch 4/10
19/19 - 3s - loss: 520.2001 - loglik: -5.1947e+02 - logprior: -7.2621e-01
Epoch 5/10
19/19 - 3s - loss: 519.2507 - loglik: -5.1855e+02 - logprior: -7.0426e-01
Epoch 6/10
19/19 - 3s - loss: 518.4714 - loglik: -5.1777e+02 - logprior: -7.0487e-01
Epoch 7/10
19/19 - 3s - loss: 517.4996 - loglik: -5.1677e+02 - logprior: -7.2821e-01
Epoch 8/10
19/19 - 3s - loss: 517.8543 - loglik: -5.1710e+02 - logprior: -7.5376e-01
Fitted a model with MAP estimate = -516.5655
Time for alignment: 104.7875
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 570.3557 - loglik: -5.6702e+02 - logprior: -3.3319e+00
Epoch 2/10
19/19 - 3s - loss: 539.6447 - loglik: -5.3875e+02 - logprior: -8.9482e-01
Epoch 3/10
19/19 - 3s - loss: 531.3597 - loglik: -5.3049e+02 - logprior: -8.6697e-01
Epoch 4/10
19/19 - 3s - loss: 527.8579 - loglik: -5.2705e+02 - logprior: -8.0921e-01
Epoch 5/10
19/19 - 3s - loss: 526.8043 - loglik: -5.2599e+02 - logprior: -8.0989e-01
Epoch 6/10
19/19 - 3s - loss: 526.2290 - loglik: -5.2539e+02 - logprior: -8.3825e-01
Epoch 7/10
19/19 - 3s - loss: 525.9957 - loglik: -5.2513e+02 - logprior: -8.6479e-01
Epoch 8/10
19/19 - 3s - loss: 524.7068 - loglik: -5.2378e+02 - logprior: -9.2779e-01
Epoch 9/10
19/19 - 3s - loss: 524.0815 - loglik: -5.2309e+02 - logprior: -9.9284e-01
Epoch 10/10
19/19 - 3s - loss: 524.2274 - loglik: -5.2318e+02 - logprior: -1.0433e+00
Fitted a model with MAP estimate = -521.3669
expansions: [(0, 10), (12, 1), (16, 2), (19, 4), (54, 1), (56, 3), (57, 1), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 535.5120 - loglik: -5.3145e+02 - logprior: -4.0592e+00
Epoch 2/2
19/19 - 3s - loss: 525.4861 - loglik: -5.2432e+02 - logprior: -1.1624e+00
Fitted a model with MAP estimate = -521.4345
expansions: [(0, 9), (33, 1), (35, 1)]
discards: [ 1  2  3  4  5  6  7  8 27 28 76 77]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 529.7136 - loglik: -5.2596e+02 - logprior: -3.7544e+00
Epoch 2/2
19/19 - 3s - loss: 524.3990 - loglik: -5.2320e+02 - logprior: -1.1981e+00
Fitted a model with MAP estimate = -521.2381
expansions: [(0, 6), (28, 1)]
discards: [0 1 2 3 4 5 6 7 8 9]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 525.7606 - loglik: -5.2261e+02 - logprior: -3.1476e+00
Epoch 2/10
19/19 - 3s - loss: 521.5333 - loglik: -5.2070e+02 - logprior: -8.3269e-01
Epoch 3/10
19/19 - 4s - loss: 520.3444 - loglik: -5.1963e+02 - logprior: -7.1338e-01
Epoch 4/10
19/19 - 4s - loss: 519.5963 - loglik: -5.1892e+02 - logprior: -6.7276e-01
Epoch 5/10
19/19 - 4s - loss: 518.4447 - loglik: -5.1780e+02 - logprior: -6.4868e-01
Epoch 6/10
19/19 - 4s - loss: 518.4998 - loglik: -5.1786e+02 - logprior: -6.4437e-01
Fitted a model with MAP estimate = -517.0858
Time for alignment: 103.1204
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 570.1659 - loglik: -5.6683e+02 - logprior: -3.3323e+00
Epoch 2/10
19/19 - 3s - loss: 539.0160 - loglik: -5.3813e+02 - logprior: -8.8579e-01
Epoch 3/10
19/19 - 3s - loss: 530.9064 - loglik: -5.3005e+02 - logprior: -8.5273e-01
Epoch 4/10
19/19 - 3s - loss: 529.2698 - loglik: -5.2846e+02 - logprior: -8.0983e-01
Epoch 5/10
19/19 - 3s - loss: 526.3430 - loglik: -5.2551e+02 - logprior: -8.3123e-01
Epoch 6/10
19/19 - 3s - loss: 527.0903 - loglik: -5.2624e+02 - logprior: -8.4728e-01
Fitted a model with MAP estimate = -523.6987
expansions: [(0, 9), (17, 1), (18, 1), (19, 2), (21, 2), (22, 1), (54, 1), (57, 2), (59, 3), (61, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 531.9882 - loglik: -5.2819e+02 - logprior: -3.8016e+00
Epoch 2/2
19/19 - 3s - loss: 524.9111 - loglik: -5.2382e+02 - logprior: -1.0864e+00
Fitted a model with MAP estimate = -520.8795
expansions: [(0, 8), (34, 1)]
discards: [ 0  1  2  3  4  5  6  7 26 80]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 527.6907 - loglik: -5.2459e+02 - logprior: -3.0977e+00
Epoch 2/2
19/19 - 3s - loss: 523.6494 - loglik: -5.2270e+02 - logprior: -9.4959e-01
Fitted a model with MAP estimate = -520.5893
expansions: [(0, 7)]
discards: [ 0  1  2  3  4  5  6 75 78 79]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 525.1514 - loglik: -5.2232e+02 - logprior: -2.8348e+00
Epoch 2/10
19/19 - 3s - loss: 521.4139 - loglik: -5.2058e+02 - logprior: -8.2953e-01
Epoch 3/10
19/19 - 3s - loss: 520.3492 - loglik: -5.1960e+02 - logprior: -7.5175e-01
Epoch 4/10
19/19 - 3s - loss: 520.3354 - loglik: -5.1960e+02 - logprior: -7.3517e-01
Epoch 5/10
19/19 - 4s - loss: 518.9501 - loglik: -5.1824e+02 - logprior: -7.1453e-01
Epoch 6/10
19/19 - 3s - loss: 517.8475 - loglik: -5.1713e+02 - logprior: -7.1934e-01
Epoch 7/10
19/19 - 3s - loss: 516.9893 - loglik: -5.1625e+02 - logprior: -7.4146e-01
Epoch 8/10
19/19 - 3s - loss: 515.9810 - loglik: -5.1521e+02 - logprior: -7.7305e-01
Epoch 9/10
19/19 - 3s - loss: 515.2466 - loglik: -5.1443e+02 - logprior: -8.1717e-01
Epoch 10/10
19/19 - 3s - loss: 515.7505 - loglik: -5.1488e+02 - logprior: -8.7274e-01
Fitted a model with MAP estimate = -515.0905
Time for alignment: 101.4980
Computed alignments with likelihoods: ['-514.7866', '-516.5970', '-516.5655', '-517.0858', '-515.0905']
Best model has likelihood: -514.7866  (prior= -0.9371 )
time for generating output: 0.2699
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gluts.projection.fasta
SP score = 0.7974736842105263
Training of 5 independent models on file tms.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f527350dcd0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5274b6e880>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f527e6de430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5277f49940>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5286360bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52861a0a00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530cf1a340>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f528a005250>, <__main__.SimpleDirichletPrior object at 0x7f5271f50190>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 1515.5432 - loglik: -1.4988e+03 - logprior: -1.6779e+01
Epoch 2/10
17/17 - 7s - loss: 1332.3369 - loglik: -1.3300e+03 - logprior: -2.3738e+00
Epoch 3/10
17/17 - 7s - loss: 1232.3907 - loglik: -1.2288e+03 - logprior: -3.6259e+00
Epoch 4/10
17/17 - 7s - loss: 1213.0277 - loglik: -1.2091e+03 - logprior: -3.9419e+00
Epoch 5/10
17/17 - 7s - loss: 1212.6061 - loglik: -1.2091e+03 - logprior: -3.5210e+00
Epoch 6/10
17/17 - 7s - loss: 1207.0466 - loglik: -1.2038e+03 - logprior: -3.2768e+00
Epoch 7/10
17/17 - 7s - loss: 1207.4077 - loglik: -1.2041e+03 - logprior: -3.3013e+00
Fitted a model with MAP estimate = -1206.1984
expansions: [(8, 1), (9, 1), (11, 1), (13, 1), (15, 1), (24, 1), (26, 1), (32, 1), (44, 1), (46, 1), (54, 1), (56, 1), (58, 1), (60, 1), (61, 1), (63, 1), (86, 2), (95, 2), (96, 1), (97, 1), (114, 1), (115, 2), (116, 1), (123, 1), (129, 2), (131, 1), (140, 1), (141, 1), (155, 1), (160, 1), (162, 1), (163, 1), (176, 1), (178, 1), (180, 1), (184, 1), (191, 1), (192, 1), (195, 1), (196, 1), (197, 1), (200, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 1209.1555 - loglik: -1.1922e+03 - logprior: -1.6916e+01
Epoch 2/2
17/17 - 9s - loss: 1170.4570 - loglik: -1.1658e+03 - logprior: -4.6276e+00
Fitted a model with MAP estimate = -1166.4833
expansions: [(0, 9)]
discards: [  0 137 155]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 1177.2825 - loglik: -1.1663e+03 - logprior: -1.1021e+01
Epoch 2/2
17/17 - 10s - loss: 1161.2214 - loglik: -1.1615e+03 - logprior: 0.3250
Fitted a model with MAP estimate = -1158.5839
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 1173.6935 - loglik: -1.1637e+03 - logprior: -1.0016e+01
Epoch 2/10
17/17 - 9s - loss: 1161.6221 - loglik: -1.1628e+03 - logprior: 1.1545
Epoch 3/10
17/17 - 9s - loss: 1156.2096 - loglik: -1.1591e+03 - logprior: 2.8489
Epoch 4/10
17/17 - 10s - loss: 1153.0710 - loglik: -1.1567e+03 - logprior: 3.6461
Epoch 5/10
17/17 - 9s - loss: 1152.8718 - loglik: -1.1568e+03 - logprior: 3.9450
Epoch 6/10
17/17 - 9s - loss: 1155.2266 - loglik: -1.1594e+03 - logprior: 4.1729
Fitted a model with MAP estimate = -1151.5389
Time for alignment: 188.2209
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 14s - loss: 1516.1578 - loglik: -1.4994e+03 - logprior: -1.6791e+01
Epoch 2/10
17/17 - 8s - loss: 1335.2415 - loglik: -1.3329e+03 - logprior: -2.3396e+00
Epoch 3/10
17/17 - 7s - loss: 1234.2627 - loglik: -1.2308e+03 - logprior: -3.4568e+00
Epoch 4/10
17/17 - 7s - loss: 1214.0896 - loglik: -1.2104e+03 - logprior: -3.6961e+00
Epoch 5/10
17/17 - 7s - loss: 1211.9585 - loglik: -1.2087e+03 - logprior: -3.2584e+00
Epoch 6/10
17/17 - 7s - loss: 1211.2755 - loglik: -1.2082e+03 - logprior: -3.0531e+00
Epoch 7/10
17/17 - 7s - loss: 1206.0984 - loglik: -1.2030e+03 - logprior: -3.0709e+00
Epoch 8/10
17/17 - 8s - loss: 1207.0642 - loglik: -1.2039e+03 - logprior: -3.1614e+00
Fitted a model with MAP estimate = -1206.8482
expansions: [(8, 1), (9, 1), (12, 1), (14, 1), (15, 1), (24, 1), (26, 1), (32, 2), (45, 1), (54, 1), (56, 1), (58, 1), (60, 1), (61, 1), (63, 1), (70, 3), (95, 2), (97, 1), (100, 1), (114, 1), (115, 1), (116, 3), (129, 3), (130, 2), (139, 1), (140, 1), (159, 1), (160, 1), (162, 1), (176, 1), (178, 1), (180, 1), (184, 1), (191, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 268 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 14s - loss: 1209.5667 - loglik: -1.1925e+03 - logprior: -1.7029e+01
Epoch 2/2
17/17 - 10s - loss: 1170.7651 - loglik: -1.1660e+03 - logprior: -4.7398e+00
Fitted a model with MAP estimate = -1166.4151
expansions: [(0, 9)]
discards: [  0  86 141 157 160]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 1176.2158 - loglik: -1.1653e+03 - logprior: -1.0924e+01
Epoch 2/2
17/17 - 10s - loss: 1162.6343 - loglik: -1.1631e+03 - logprior: 0.4807
Fitted a model with MAP estimate = -1158.2142
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 1174.9810 - loglik: -1.1651e+03 - logprior: -9.8750e+00
Epoch 2/10
17/17 - 9s - loss: 1158.9728 - loglik: -1.1603e+03 - logprior: 1.2858
Epoch 3/10
17/17 - 10s - loss: 1156.0819 - loglik: -1.1591e+03 - logprior: 3.0072
Epoch 4/10
17/17 - 9s - loss: 1152.2109 - loglik: -1.1560e+03 - logprior: 3.7852
Epoch 5/10
17/17 - 9s - loss: 1154.1002 - loglik: -1.1582e+03 - logprior: 4.1018
Fitted a model with MAP estimate = -1151.7516
Time for alignment: 191.9811
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 1515.7839 - loglik: -1.4990e+03 - logprior: -1.6783e+01
Epoch 2/10
17/17 - 7s - loss: 1332.7847 - loglik: -1.3305e+03 - logprior: -2.3038e+00
Epoch 3/10
17/17 - 7s - loss: 1232.1495 - loglik: -1.2289e+03 - logprior: -3.2320e+00
Epoch 4/10
17/17 - 7s - loss: 1221.3895 - loglik: -1.2180e+03 - logprior: -3.3517e+00
Epoch 5/10
17/17 - 7s - loss: 1211.8800 - loglik: -1.2089e+03 - logprior: -3.0072e+00
Epoch 6/10
17/17 - 8s - loss: 1209.3561 - loglik: -1.2065e+03 - logprior: -2.8523e+00
Epoch 7/10
17/17 - 7s - loss: 1207.4062 - loglik: -1.2045e+03 - logprior: -2.8698e+00
Epoch 8/10
17/17 - 8s - loss: 1212.2467 - loglik: -1.2093e+03 - logprior: -2.9218e+00
Fitted a model with MAP estimate = -1208.0091
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (26, 1), (32, 2), (45, 1), (57, 1), (58, 2), (60, 1), (61, 1), (63, 1), (71, 1), (86, 1), (95, 2), (96, 1), (97, 1), (114, 2), (115, 3), (116, 1), (128, 2), (130, 1), (141, 1), (150, 1), (160, 1), (161, 1), (162, 1), (176, 1), (178, 1), (180, 1), (181, 1), (187, 1), (190, 1), (191, 1), (196, 1), (197, 1), (200, 1), (209, 1), (212, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 267 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 14s - loss: 1201.2543 - loglik: -1.1891e+03 - logprior: -1.2185e+01
Epoch 2/2
17/17 - 9s - loss: 1163.2550 - loglik: -1.1633e+03 - logprior: 5.3513e-04
Fitted a model with MAP estimate = -1159.6356
expansions: []
discards: [  0 136 156]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 1182.5923 - loglik: -1.1675e+03 - logprior: -1.5136e+01
Epoch 2/2
17/17 - 10s - loss: 1166.3213 - loglik: -1.1623e+03 - logprior: -4.0041e+00
Fitted a model with MAP estimate = -1164.5446
expansions: [(0, 9)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 1175.9189 - loglik: -1.1654e+03 - logprior: -1.0530e+01
Epoch 2/10
17/17 - 10s - loss: 1158.7911 - loglik: -1.1597e+03 - logprior: 0.9467
Epoch 3/10
17/17 - 10s - loss: 1156.0371 - loglik: -1.1585e+03 - logprior: 2.4732
Epoch 4/10
17/17 - 9s - loss: 1158.1282 - loglik: -1.1612e+03 - logprior: 3.0607
Fitted a model with MAP estimate = -1153.2146
Time for alignment: 180.1145
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 1516.1200 - loglik: -1.4993e+03 - logprior: -1.6788e+01
Epoch 2/10
17/17 - 7s - loss: 1331.6362 - loglik: -1.3293e+03 - logprior: -2.3651e+00
Epoch 3/10
17/17 - 8s - loss: 1235.5833 - loglik: -1.2319e+03 - logprior: -3.6761e+00
Epoch 4/10
17/17 - 7s - loss: 1216.3931 - loglik: -1.2125e+03 - logprior: -3.9095e+00
Epoch 5/10
17/17 - 7s - loss: 1210.0856 - loglik: -1.2065e+03 - logprior: -3.5790e+00
Epoch 6/10
17/17 - 8s - loss: 1207.0203 - loglik: -1.2037e+03 - logprior: -3.2833e+00
Epoch 7/10
17/17 - 7s - loss: 1206.0693 - loglik: -1.2028e+03 - logprior: -3.2741e+00
Epoch 8/10
17/17 - 8s - loss: 1207.4390 - loglik: -1.2041e+03 - logprior: -3.3675e+00
Fitted a model with MAP estimate = -1205.6117
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (26, 1), (32, 2), (45, 1), (46, 1), (53, 1), (55, 1), (57, 1), (59, 1), (60, 1), (62, 1), (86, 2), (94, 2), (97, 1), (99, 1), (113, 1), (114, 2), (115, 1), (116, 1), (128, 2), (130, 1), (141, 1), (150, 1), (159, 1), (160, 1), (162, 1), (176, 1), (178, 1), (180, 1), (181, 1), (183, 1), (187, 1), (191, 1), (196, 1), (197, 1), (200, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 15s - loss: 1208.0956 - loglik: -1.1912e+03 - logprior: -1.6905e+01
Epoch 2/2
17/17 - 9s - loss: 1171.6654 - loglik: -1.1671e+03 - logprior: -4.5831e+00
Fitted a model with MAP estimate = -1166.5279
expansions: [(0, 9)]
discards: [  0 137 155]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 1178.4388 - loglik: -1.1675e+03 - logprior: -1.0984e+01
Epoch 2/2
17/17 - 10s - loss: 1159.4332 - loglik: -1.1599e+03 - logprior: 0.4629
Fitted a model with MAP estimate = -1158.6136
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 1173.6006 - loglik: -1.1637e+03 - logprior: -9.8751e+00
Epoch 2/10
17/17 - 9s - loss: 1161.8062 - loglik: -1.1631e+03 - logprior: 1.2905
Epoch 3/10
17/17 - 9s - loss: 1157.0607 - loglik: -1.1600e+03 - logprior: 2.9474
Epoch 4/10
17/17 - 9s - loss: 1154.8618 - loglik: -1.1586e+03 - logprior: 3.7345
Epoch 5/10
17/17 - 9s - loss: 1151.9822 - loglik: -1.1560e+03 - logprior: 4.0611
Epoch 6/10
17/17 - 9s - loss: 1152.1798 - loglik: -1.1564e+03 - logprior: 4.2618
Fitted a model with MAP estimate = -1151.4075
Time for alignment: 198.3601
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 1516.1025 - loglik: -1.4993e+03 - logprior: -1.6788e+01
Epoch 2/10
17/17 - 7s - loss: 1327.4771 - loglik: -1.3253e+03 - logprior: -2.1823e+00
Epoch 3/10
17/17 - 7s - loss: 1232.3813 - loglik: -1.2292e+03 - logprior: -3.1397e+00
Epoch 4/10
17/17 - 8s - loss: 1216.4076 - loglik: -1.2130e+03 - logprior: -3.4141e+00
Epoch 5/10
17/17 - 7s - loss: 1211.3538 - loglik: -1.2083e+03 - logprior: -3.1030e+00
Epoch 6/10
17/17 - 8s - loss: 1208.2183 - loglik: -1.2053e+03 - logprior: -2.9450e+00
Epoch 7/10
17/17 - 7s - loss: 1209.4141 - loglik: -1.2064e+03 - logprior: -2.9687e+00
Fitted a model with MAP estimate = -1206.5924
expansions: [(8, 1), (9, 1), (10, 1), (11, 1), (13, 1), (24, 1), (26, 1), (32, 1), (43, 1), (55, 1), (56, 1), (58, 3), (59, 1), (60, 1), (62, 1), (86, 1), (95, 2), (97, 1), (100, 1), (114, 2), (115, 3), (129, 2), (131, 1), (140, 1), (141, 1), (160, 2), (163, 1), (177, 1), (179, 1), (181, 1), (182, 1), (184, 1), (191, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 1207.3655 - loglik: -1.1906e+03 - logprior: -1.6814e+01
Epoch 2/2
17/17 - 9s - loss: 1169.6162 - loglik: -1.1651e+03 - logprior: -4.5271e+00
Fitted a model with MAP estimate = -1166.0231
expansions: [(0, 9)]
discards: [  0 135 155]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 14s - loss: 1176.4489 - loglik: -1.1654e+03 - logprior: -1.1026e+01
Epoch 2/2
17/17 - 10s - loss: 1160.3834 - loglik: -1.1608e+03 - logprior: 0.3920
Fitted a model with MAP estimate = -1158.0925
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 1173.4324 - loglik: -1.1635e+03 - logprior: -9.9033e+00
Epoch 2/10
17/17 - 9s - loss: 1159.8478 - loglik: -1.1611e+03 - logprior: 1.2401
Epoch 3/10
17/17 - 9s - loss: 1157.1774 - loglik: -1.1601e+03 - logprior: 2.9393
Epoch 4/10
17/17 - 9s - loss: 1153.5421 - loglik: -1.1573e+03 - logprior: 3.7170
Epoch 5/10
17/17 - 9s - loss: 1154.2911 - loglik: -1.1583e+03 - logprior: 4.0226
Fitted a model with MAP estimate = -1152.0775
Time for alignment: 180.8317
Computed alignments with likelihoods: ['-1151.5389', '-1151.7516', '-1153.2146', '-1151.4075', '-1152.0775']
Best model has likelihood: -1151.4075  (prior= 4.4142 )
time for generating output: 0.3633
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tms.projection.fasta
SP score = 0.9436997319034852
Training of 5 independent models on file kunitz.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5335635640>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52f02b7f70>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52976482b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52976488e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f598436b250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5301ae3610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5294462a00>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f527c6511c0>, <__main__.SimpleDirichletPrior object at 0x7f5270dd03a0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 347.6717 - loglik: -3.2639e+02 - logprior: -2.1286e+01
Epoch 2/10
10/10 - 1s - loss: 315.1796 - loglik: -3.0917e+02 - logprior: -6.0072e+00
Epoch 3/10
10/10 - 1s - loss: 298.1663 - loglik: -2.9482e+02 - logprior: -3.3479e+00
Epoch 4/10
10/10 - 1s - loss: 287.9249 - loglik: -2.8523e+02 - logprior: -2.6936e+00
Epoch 5/10
10/10 - 1s - loss: 281.6956 - loglik: -2.7909e+02 - logprior: -2.6016e+00
Epoch 6/10
10/10 - 1s - loss: 279.5766 - loglik: -2.7701e+02 - logprior: -2.5697e+00
Epoch 7/10
10/10 - 1s - loss: 278.5506 - loglik: -2.7605e+02 - logprior: -2.5002e+00
Epoch 8/10
10/10 - 1s - loss: 277.8534 - loglik: -2.7546e+02 - logprior: -2.3932e+00
Epoch 9/10
10/10 - 1s - loss: 277.6092 - loglik: -2.7529e+02 - logprior: -2.3153e+00
Epoch 10/10
10/10 - 1s - loss: 277.5397 - loglik: -2.7526e+02 - logprior: -2.2825e+00
Fitted a model with MAP estimate = -277.5073
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (17, 1), (18, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 302.3787 - loglik: -2.7942e+02 - logprior: -2.2955e+01
Epoch 2/2
10/10 - 1s - loss: 282.8396 - loglik: -2.7303e+02 - logprior: -9.8108e+00
Fitted a model with MAP estimate = -279.4087
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 288.6115 - loglik: -2.7076e+02 - logprior: -1.7851e+01
Epoch 2/2
10/10 - 1s - loss: 274.8120 - loglik: -2.6993e+02 - logprior: -4.8807e+00
Fitted a model with MAP estimate = -273.0212
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 291.7538 - loglik: -2.7151e+02 - logprior: -2.0239e+01
Epoch 2/10
10/10 - 1s - loss: 276.6277 - loglik: -2.7073e+02 - logprior: -5.8999e+00
Epoch 3/10
10/10 - 1s - loss: 273.7807 - loglik: -2.7063e+02 - logprior: -3.1521e+00
Epoch 4/10
10/10 - 1s - loss: 271.9544 - loglik: -2.6992e+02 - logprior: -2.0371e+00
Epoch 5/10
10/10 - 1s - loss: 271.5587 - loglik: -2.7013e+02 - logprior: -1.4265e+00
Epoch 6/10
10/10 - 1s - loss: 271.6321 - loglik: -2.7044e+02 - logprior: -1.1932e+00
Fitted a model with MAP estimate = -271.2454
Time for alignment: 33.2511
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 347.6573 - loglik: -3.2637e+02 - logprior: -2.1288e+01
Epoch 2/10
10/10 - 1s - loss: 315.4686 - loglik: -3.0946e+02 - logprior: -6.0113e+00
Epoch 3/10
10/10 - 1s - loss: 298.0298 - loglik: -2.9463e+02 - logprior: -3.3967e+00
Epoch 4/10
10/10 - 1s - loss: 286.2461 - loglik: -2.8340e+02 - logprior: -2.8421e+00
Epoch 5/10
10/10 - 1s - loss: 280.8849 - loglik: -2.7814e+02 - logprior: -2.7489e+00
Epoch 6/10
10/10 - 1s - loss: 279.4730 - loglik: -2.7680e+02 - logprior: -2.6692e+00
Epoch 7/10
10/10 - 1s - loss: 278.4954 - loglik: -2.7596e+02 - logprior: -2.5336e+00
Epoch 8/10
10/10 - 1s - loss: 277.9383 - loglik: -2.7554e+02 - logprior: -2.3965e+00
Epoch 9/10
10/10 - 1s - loss: 277.5877 - loglik: -2.7527e+02 - logprior: -2.3143e+00
Epoch 10/10
10/10 - 1s - loss: 277.9994 - loglik: -2.7570e+02 - logprior: -2.2974e+00
Fitted a model with MAP estimate = -277.6049
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (19, 1), (20, 1), (22, 2), (23, 1), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 302.5557 - loglik: -2.7960e+02 - logprior: -2.2957e+01
Epoch 2/2
10/10 - 1s - loss: 283.0667 - loglik: -2.7323e+02 - logprior: -9.8317e+00
Fitted a model with MAP estimate = -279.4778
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 32]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 288.4224 - loglik: -2.7056e+02 - logprior: -1.7859e+01
Epoch 2/2
10/10 - 1s - loss: 275.0045 - loglik: -2.7012e+02 - logprior: -4.8856e+00
Fitted a model with MAP estimate = -273.0106
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 291.8396 - loglik: -2.7159e+02 - logprior: -2.0253e+01
Epoch 2/10
10/10 - 1s - loss: 276.5394 - loglik: -2.7063e+02 - logprior: -5.9095e+00
Epoch 3/10
10/10 - 1s - loss: 273.6492 - loglik: -2.7049e+02 - logprior: -3.1606e+00
Epoch 4/10
10/10 - 1s - loss: 272.1342 - loglik: -2.7009e+02 - logprior: -2.0446e+00
Epoch 5/10
10/10 - 1s - loss: 271.6558 - loglik: -2.7022e+02 - logprior: -1.4313e+00
Epoch 6/10
10/10 - 1s - loss: 271.1734 - loglik: -2.6997e+02 - logprior: -1.2005e+00
Epoch 7/10
10/10 - 1s - loss: 271.4871 - loglik: -2.7050e+02 - logprior: -9.8354e-01
Fitted a model with MAP estimate = -271.1372
Time for alignment: 33.9362
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 347.8434 - loglik: -3.2656e+02 - logprior: -2.1286e+01
Epoch 2/10
10/10 - 1s - loss: 315.2334 - loglik: -3.0922e+02 - logprior: -6.0091e+00
Epoch 3/10
10/10 - 1s - loss: 297.9355 - loglik: -2.9456e+02 - logprior: -3.3744e+00
Epoch 4/10
10/10 - 1s - loss: 286.4201 - loglik: -2.8362e+02 - logprior: -2.7997e+00
Epoch 5/10
10/10 - 1s - loss: 281.3413 - loglik: -2.7862e+02 - logprior: -2.7254e+00
Epoch 6/10
10/10 - 1s - loss: 278.8161 - loglik: -2.7616e+02 - logprior: -2.6568e+00
Epoch 7/10
10/10 - 1s - loss: 278.5276 - loglik: -2.7601e+02 - logprior: -2.5219e+00
Epoch 8/10
10/10 - 1s - loss: 278.1341 - loglik: -2.7576e+02 - logprior: -2.3761e+00
Epoch 9/10
10/10 - 1s - loss: 277.7950 - loglik: -2.7550e+02 - logprior: -2.2937e+00
Epoch 10/10
10/10 - 1s - loss: 277.8268 - loglik: -2.7555e+02 - logprior: -2.2788e+00
Fitted a model with MAP estimate = -277.6991
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (17, 1), (19, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 302.6268 - loglik: -2.7967e+02 - logprior: -2.2953e+01
Epoch 2/2
10/10 - 1s - loss: 282.5776 - loglik: -2.7277e+02 - logprior: -9.8104e+00
Fitted a model with MAP estimate = -279.3739
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 288.6606 - loglik: -2.7081e+02 - logprior: -1.7853e+01
Epoch 2/2
10/10 - 1s - loss: 274.7233 - loglik: -2.6984e+02 - logprior: -4.8822e+00
Fitted a model with MAP estimate = -273.0155
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 291.5326 - loglik: -2.7129e+02 - logprior: -2.0247e+01
Epoch 2/10
10/10 - 1s - loss: 277.1162 - loglik: -2.7121e+02 - logprior: -5.9028e+00
Epoch 3/10
10/10 - 1s - loss: 273.4164 - loglik: -2.7026e+02 - logprior: -3.1520e+00
Epoch 4/10
10/10 - 1s - loss: 272.0623 - loglik: -2.7003e+02 - logprior: -2.0367e+00
Epoch 5/10
10/10 - 1s - loss: 271.3441 - loglik: -2.6992e+02 - logprior: -1.4286e+00
Epoch 6/10
10/10 - 1s - loss: 271.7630 - loglik: -2.7057e+02 - logprior: -1.1944e+00
Fitted a model with MAP estimate = -271.2440
Time for alignment: 30.6393
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 347.7647 - loglik: -3.2648e+02 - logprior: -2.1287e+01
Epoch 2/10
10/10 - 1s - loss: 315.3715 - loglik: -3.0937e+02 - logprior: -6.0061e+00
Epoch 3/10
10/10 - 1s - loss: 297.3520 - loglik: -2.9398e+02 - logprior: -3.3699e+00
Epoch 4/10
10/10 - 1s - loss: 285.7505 - loglik: -2.8296e+02 - logprior: -2.7934e+00
Epoch 5/10
10/10 - 1s - loss: 280.6816 - loglik: -2.7795e+02 - logprior: -2.7319e+00
Epoch 6/10
10/10 - 1s - loss: 279.1393 - loglik: -2.7648e+02 - logprior: -2.6556e+00
Epoch 7/10
10/10 - 1s - loss: 278.3212 - loglik: -2.7580e+02 - logprior: -2.5183e+00
Epoch 8/10
10/10 - 1s - loss: 277.9772 - loglik: -2.7561e+02 - logprior: -2.3721e+00
Epoch 9/10
10/10 - 1s - loss: 277.9864 - loglik: -2.7570e+02 - logprior: -2.2908e+00
Fitted a model with MAP estimate = -277.7219
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (19, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 302.6367 - loglik: -2.7969e+02 - logprior: -2.2950e+01
Epoch 2/2
10/10 - 1s - loss: 282.6044 - loglik: -2.7280e+02 - logprior: -9.8024e+00
Fitted a model with MAP estimate = -279.3466
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 288.5154 - loglik: -2.7066e+02 - logprior: -1.7858e+01
Epoch 2/2
10/10 - 1s - loss: 274.8099 - loglik: -2.6993e+02 - logprior: -4.8837e+00
Fitted a model with MAP estimate = -273.0188
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 291.7419 - loglik: -2.7149e+02 - logprior: -2.0255e+01
Epoch 2/10
10/10 - 1s - loss: 276.9344 - loglik: -2.7102e+02 - logprior: -5.9125e+00
Epoch 3/10
10/10 - 1s - loss: 273.3441 - loglik: -2.7018e+02 - logprior: -3.1604e+00
Epoch 4/10
10/10 - 1s - loss: 272.1260 - loglik: -2.7008e+02 - logprior: -2.0474e+00
Epoch 5/10
10/10 - 1s - loss: 271.4791 - loglik: -2.7005e+02 - logprior: -1.4304e+00
Epoch 6/10
10/10 - 1s - loss: 271.4998 - loglik: -2.7030e+02 - logprior: -1.2016e+00
Fitted a model with MAP estimate = -271.2470
Time for alignment: 32.3801
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 347.7087 - loglik: -3.2642e+02 - logprior: -2.1287e+01
Epoch 2/10
10/10 - 1s - loss: 315.4987 - loglik: -3.0949e+02 - logprior: -6.0096e+00
Epoch 3/10
10/10 - 1s - loss: 297.4660 - loglik: -2.9410e+02 - logprior: -3.3633e+00
Epoch 4/10
10/10 - 1s - loss: 286.4439 - loglik: -2.8375e+02 - logprior: -2.6968e+00
Epoch 5/10
10/10 - 1s - loss: 282.4302 - loglik: -2.7994e+02 - logprior: -2.4880e+00
Epoch 6/10
10/10 - 1s - loss: 281.3349 - loglik: -2.7899e+02 - logprior: -2.3457e+00
Epoch 7/10
10/10 - 1s - loss: 280.1780 - loglik: -2.7796e+02 - logprior: -2.2138e+00
Epoch 8/10
10/10 - 1s - loss: 280.2565 - loglik: -2.7818e+02 - logprior: -2.0788e+00
Fitted a model with MAP estimate = -280.0005
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (19, 1), (22, 1), (35, 4), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 302.8471 - loglik: -2.7992e+02 - logprior: -2.2930e+01
Epoch 2/2
10/10 - 1s - loss: 283.6003 - loglik: -2.7378e+02 - logprior: -9.8184e+00
Fitted a model with MAP estimate = -279.6731
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 51]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 289.2354 - loglik: -2.7135e+02 - logprior: -1.7887e+01
Epoch 2/2
10/10 - 1s - loss: 275.1435 - loglik: -2.7025e+02 - logprior: -4.8929e+00
Fitted a model with MAP estimate = -273.1232
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 291.9214 - loglik: -2.7167e+02 - logprior: -2.0254e+01
Epoch 2/10
10/10 - 1s - loss: 276.9548 - loglik: -2.7104e+02 - logprior: -5.9141e+00
Epoch 3/10
10/10 - 1s - loss: 273.1116 - loglik: -2.6994e+02 - logprior: -3.1681e+00
Epoch 4/10
10/10 - 1s - loss: 272.2772 - loglik: -2.7022e+02 - logprior: -2.0617e+00
Epoch 5/10
10/10 - 1s - loss: 271.6050 - loglik: -2.7017e+02 - logprior: -1.4311e+00
Epoch 6/10
10/10 - 1s - loss: 271.3536 - loglik: -2.7015e+02 - logprior: -1.2044e+00
Epoch 7/10
10/10 - 1s - loss: 271.3629 - loglik: -2.7037e+02 - logprior: -9.9288e-01
Fitted a model with MAP estimate = -271.1454
Time for alignment: 32.2209
Computed alignments with likelihoods: ['-271.2454', '-271.1372', '-271.2440', '-271.2470', '-271.1454']
Best model has likelihood: -271.1372  (prior= -0.9236 )
time for generating output: 0.1217
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kunitz.projection.fasta
SP score = 0.9893069306930693
Training of 5 independent models on file rhv.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5aff25d6d0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f531c8cb370>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530dc66af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530c393580>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f590026e340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5272c9e400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f528809a370>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52f93761c0>, <__main__.SimpleDirichletPrior object at 0x7f5b13e731c0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 1008.2877 - loglik: -1.0062e+03 - logprior: -2.0480e+00
Epoch 2/10
39/39 - 11s - loss: 927.5469 - loglik: -9.2658e+02 - logprior: -9.6751e-01
Epoch 3/10
39/39 - 10s - loss: 920.0908 - loglik: -9.1908e+02 - logprior: -1.0066e+00
Epoch 4/10
39/39 - 10s - loss: 918.1472 - loglik: -9.1713e+02 - logprior: -1.0132e+00
Epoch 5/10
39/39 - 10s - loss: 917.0548 - loglik: -9.1598e+02 - logprior: -1.0767e+00
Epoch 6/10
39/39 - 11s - loss: 916.7690 - loglik: -9.1565e+02 - logprior: -1.1147e+00
Epoch 7/10
39/39 - 11s - loss: 916.1675 - loglik: -9.1498e+02 - logprior: -1.1906e+00
Epoch 8/10
39/39 - 10s - loss: 915.7388 - loglik: -9.1452e+02 - logprior: -1.2192e+00
Epoch 9/10
39/39 - 11s - loss: 915.9734 - loglik: -9.1473e+02 - logprior: -1.2454e+00
Fitted a model with MAP estimate = -735.8354
expansions: [(0, 13), (10, 1), (11, 1), (18, 1), (28, 2), (30, 1), (33, 3), (37, 1), (44, 1), (45, 1), (50, 1), (71, 1), (88, 1), (89, 1), (90, 1), (91, 1), (103, 2), (107, 1), (108, 1), (112, 1), (130, 3), (134, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 177 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 906.6658 - loglik: -9.0359e+02 - logprior: -3.0790e+00
Epoch 2/2
39/39 - 13s - loss: 892.6650 - loglik: -8.9125e+02 - logprior: -1.4102e+00
Fitted a model with MAP estimate = -716.1707
expansions: []
discards: [  1   2   3   4   5   6   7   8   9  10  53 134 166 167 173 174 175 176]
Re-initialized the encoder parameters.
Fitting a model of length 159 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 901.5652 - loglik: -8.9958e+02 - logprior: -1.9834e+00
Epoch 2/2
39/39 - 11s - loss: 897.2498 - loglik: -8.9659e+02 - logprior: -6.5965e-01
Fitted a model with MAP estimate = -721.3132
expansions: [(45, 2), (159, 8)]
discards: [35]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 22s - loss: 716.8506 - loglik: -7.1562e+02 - logprior: -1.2352e+00
Epoch 2/10
52/52 - 14s - loss: 711.7809 - loglik: -7.1097e+02 - logprior: -8.0779e-01
Epoch 3/10
52/52 - 14s - loss: 707.6931 - loglik: -7.0697e+02 - logprior: -7.1986e-01
Epoch 4/10
52/52 - 15s - loss: 706.9540 - loglik: -7.0631e+02 - logprior: -6.4508e-01
Epoch 5/10
52/52 - 14s - loss: 705.0063 - loglik: -7.0438e+02 - logprior: -6.2409e-01
Epoch 6/10
52/52 - 15s - loss: 708.6434 - loglik: -7.0806e+02 - logprior: -5.7994e-01
Fitted a model with MAP estimate = -705.3786
Time for alignment: 332.1472
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 1009.9293 - loglik: -1.0078e+03 - logprior: -2.0968e+00
Epoch 2/10
39/39 - 11s - loss: 930.4294 - loglik: -9.2938e+02 - logprior: -1.0485e+00
Epoch 3/10
39/39 - 10s - loss: 921.0978 - loglik: -9.2004e+02 - logprior: -1.0542e+00
Epoch 4/10
39/39 - 10s - loss: 918.7406 - loglik: -9.1765e+02 - logprior: -1.0868e+00
Epoch 5/10
39/39 - 11s - loss: 917.8201 - loglik: -9.1670e+02 - logprior: -1.1178e+00
Epoch 6/10
39/39 - 11s - loss: 916.9122 - loglik: -9.1575e+02 - logprior: -1.1660e+00
Epoch 7/10
39/39 - 11s - loss: 916.4058 - loglik: -9.1518e+02 - logprior: -1.2216e+00
Epoch 8/10
39/39 - 10s - loss: 916.1724 - loglik: -9.1492e+02 - logprior: -1.2570e+00
Epoch 9/10
39/39 - 11s - loss: 914.9633 - loglik: -9.1370e+02 - logprior: -1.2612e+00
Epoch 10/10
39/39 - 10s - loss: 914.6950 - loglik: -9.1339e+02 - logprior: -1.3078e+00
Fitted a model with MAP estimate = -734.4039
expansions: [(0, 10), (12, 1), (15, 1), (18, 1), (28, 1), (30, 3), (34, 1), (35, 1), (38, 1), (43, 2), (44, 3), (71, 1), (87, 1), (88, 1), (89, 2), (91, 1), (103, 1), (107, 1), (108, 1), (112, 1), (126, 9), (130, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 897.5297 - loglik: -8.9447e+02 - logprior: -3.0643e+00
Epoch 2/2
39/39 - 11s - loss: 881.8472 - loglik: -8.8036e+02 - logprior: -1.4855e+00
Fitted a model with MAP estimate = -708.8296
expansions: [(165, 4), (167, 1)]
discards: [  0   1   2   3   4   5   6   7   8  64  65 175]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 888.2928 - loglik: -8.8506e+02 - logprior: -3.2378e+00
Epoch 2/2
39/39 - 12s - loss: 882.4350 - loglik: -8.8134e+02 - logprior: -1.0910e+00
Fitted a model with MAP estimate = -710.2399
expansions: [(0, 15), (55, 1)]
discards: [ 44 156 157]
Re-initialized the encoder parameters.
Fitting a model of length 187 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 20s - loss: 704.9448 - loglik: -7.0336e+02 - logprior: -1.5865e+00
Epoch 2/10
52/52 - 13s - loss: 701.7325 - loglik: -7.0064e+02 - logprior: -1.0932e+00
Epoch 3/10
52/52 - 15s - loss: 701.9854 - loglik: -7.0100e+02 - logprior: -9.8615e-01
Fitted a model with MAP estimate = -698.7173
Time for alignment: 296.5073
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 1010.2769 - loglik: -1.0082e+03 - logprior: -2.1061e+00
Epoch 2/10
39/39 - 11s - loss: 930.4722 - loglik: -9.2951e+02 - logprior: -9.6030e-01
Epoch 3/10
39/39 - 11s - loss: 923.1432 - loglik: -9.2225e+02 - logprior: -8.9696e-01
Epoch 4/10
39/39 - 11s - loss: 921.5347 - loglik: -9.2063e+02 - logprior: -9.0826e-01
Epoch 5/10
39/39 - 11s - loss: 919.9810 - loglik: -9.1902e+02 - logprior: -9.6130e-01
Epoch 6/10
39/39 - 10s - loss: 919.0889 - loglik: -9.1806e+02 - logprior: -1.0290e+00
Epoch 7/10
39/39 - 11s - loss: 917.9788 - loglik: -9.1685e+02 - logprior: -1.1294e+00
Epoch 8/10
39/39 - 10s - loss: 917.6347 - loglik: -9.1640e+02 - logprior: -1.2305e+00
Epoch 9/10
39/39 - 11s - loss: 916.8470 - loglik: -9.1557e+02 - logprior: -1.2807e+00
Epoch 10/10
39/39 - 10s - loss: 916.6250 - loglik: -9.1526e+02 - logprior: -1.3648e+00
Fitted a model with MAP estimate = -736.7360
expansions: [(0, 7), (12, 1), (19, 1), (29, 2), (30, 3), (31, 1), (33, 2), (35, 2), (38, 1), (42, 3), (44, 2), (56, 1), (71, 1), (87, 1), (88, 3), (89, 1), (90, 1), (102, 1), (104, 1), (107, 1), (130, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 905.2639 - loglik: -9.0216e+02 - logprior: -3.1065e+00
Epoch 2/2
39/39 - 12s - loss: 889.8280 - loglik: -8.8855e+02 - logprior: -1.2790e+00
Fitted a model with MAP estimate = -715.1168
expansions: [(168, 1)]
discards: [ 0  2  3  4  5 39 42 43 49 52 53 63 67]
Re-initialized the encoder parameters.
Fitting a model of length 161 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 898.7429 - loglik: -8.9588e+02 - logprior: -2.8651e+00
Epoch 2/2
39/39 - 12s - loss: 893.5386 - loglik: -8.9282e+02 - logprior: -7.1715e-01
Fitted a model with MAP estimate = -718.6945
expansions: [(0, 15), (36, 2), (121, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 179 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 20s - loss: 715.1635 - loglik: -7.1352e+02 - logprior: -1.6403e+00
Epoch 2/10
52/52 - 15s - loss: 706.7993 - loglik: -7.0550e+02 - logprior: -1.3016e+00
Epoch 3/10
52/52 - 16s - loss: 710.1876 - loglik: -7.0891e+02 - logprior: -1.2785e+00
Fitted a model with MAP estimate = -706.3475
Time for alignment: 295.9169
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 1010.3290 - loglik: -1.0082e+03 - logprior: -2.0876e+00
Epoch 2/10
39/39 - 11s - loss: 931.0969 - loglik: -9.3003e+02 - logprior: -1.0671e+00
Epoch 3/10
39/39 - 11s - loss: 922.8181 - loglik: -9.2179e+02 - logprior: -1.0265e+00
Epoch 4/10
39/39 - 11s - loss: 920.9827 - loglik: -9.1990e+02 - logprior: -1.0876e+00
Epoch 5/10
39/39 - 10s - loss: 919.6953 - loglik: -9.1856e+02 - logprior: -1.1344e+00
Epoch 6/10
39/39 - 11s - loss: 919.6465 - loglik: -9.1848e+02 - logprior: -1.1640e+00
Epoch 7/10
39/39 - 10s - loss: 919.0994 - loglik: -9.1788e+02 - logprior: -1.2157e+00
Epoch 8/10
39/39 - 11s - loss: 918.5953 - loglik: -9.1734e+02 - logprior: -1.2549e+00
Epoch 9/10
39/39 - 10s - loss: 918.7778 - loglik: -9.1746e+02 - logprior: -1.3181e+00
Fitted a model with MAP estimate = -736.1402
expansions: [(0, 11), (10, 1), (11, 1), (18, 1), (28, 1), (29, 1), (34, 2), (36, 1), (37, 2), (44, 3), (71, 1), (88, 5), (90, 1), (110, 1), (123, 5), (125, 2), (126, 1), (130, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 177 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 907.4169 - loglik: -9.0436e+02 - logprior: -3.0603e+00
Epoch 2/2
39/39 - 13s - loss: 892.5588 - loglik: -8.9110e+02 - logprior: -1.4595e+00
Fitted a model with MAP estimate = -715.4064
expansions: []
discards: [  0   3   4   5   6   7   8   9  10  11  54  66 115 158 159 163]
Re-initialized the encoder parameters.
Fitting a model of length 161 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 905.5600 - loglik: -9.0270e+02 - logprior: -2.8638e+00
Epoch 2/2
39/39 - 12s - loss: 900.0707 - loglik: -8.9925e+02 - logprior: -8.2065e-01
Fitted a model with MAP estimate = -722.2743
expansions: [(0, 17)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 176 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 18s - loss: 718.8736 - loglik: -7.1732e+02 - logprior: -1.5518e+00
Epoch 2/10
52/52 - 15s - loss: 713.8402 - loglik: -7.1270e+02 - logprior: -1.1431e+00
Epoch 3/10
52/52 - 14s - loss: 708.7682 - loglik: -7.0773e+02 - logprior: -1.0366e+00
Epoch 4/10
52/52 - 15s - loss: 709.4619 - loglik: -7.0847e+02 - logprior: -9.8906e-01
Fitted a model with MAP estimate = -708.9046
Time for alignment: 304.4938
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 1010.3215 - loglik: -1.0082e+03 - logprior: -2.0809e+00
Epoch 2/10
39/39 - 11s - loss: 926.9898 - loglik: -9.2588e+02 - logprior: -1.1053e+00
Epoch 3/10
39/39 - 11s - loss: 918.1165 - loglik: -9.1700e+02 - logprior: -1.1141e+00
Epoch 4/10
39/39 - 10s - loss: 915.9781 - loglik: -9.1482e+02 - logprior: -1.1578e+00
Epoch 5/10
39/39 - 10s - loss: 913.1879 - loglik: -9.1193e+02 - logprior: -1.2540e+00
Epoch 6/10
39/39 - 11s - loss: 913.1627 - loglik: -9.1187e+02 - logprior: -1.2908e+00
Epoch 7/10
39/39 - 11s - loss: 912.9608 - loglik: -9.1161e+02 - logprior: -1.3459e+00
Epoch 8/10
39/39 - 11s - loss: 912.3932 - loglik: -9.1102e+02 - logprior: -1.3780e+00
Epoch 9/10
39/39 - 11s - loss: 912.4267 - loglik: -9.1101e+02 - logprior: -1.4129e+00
Fitted a model with MAP estimate = -732.2749
expansions: [(0, 11), (10, 1), (15, 1), (18, 1), (28, 1), (30, 1), (31, 5), (32, 1), (43, 2), (44, 1), (45, 1), (50, 1), (69, 3), (71, 1), (88, 1), (89, 1), (90, 1), (91, 1), (103, 1), (107, 2), (108, 1), (112, 1), (125, 8), (130, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 186 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 894.9551 - loglik: -8.9190e+02 - logprior: -3.0574e+00
Epoch 2/2
39/39 - 11s - loss: 878.1985 - loglik: -8.7680e+02 - logprior: -1.3988e+00
Fitted a model with MAP estimate = -706.5793
expansions: [(169, 5)]
discards: [  1   2   3   4   5   6   7   8   9  65 143 179]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 882.1883 - loglik: -8.7997e+02 - logprior: -2.2184e+00
Epoch 2/2
39/39 - 12s - loss: 876.7527 - loglik: -8.7581e+02 - logprior: -9.3970e-01
Fitted a model with MAP estimate = -706.4528
expansions: [(0, 13)]
discards: [ 42 160]
Re-initialized the encoder parameters.
Fitting a model of length 190 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 17s - loss: 707.2961 - loglik: -7.0550e+02 - logprior: -1.7986e+00
Epoch 2/10
52/52 - 17s - loss: 695.9404 - loglik: -6.9451e+02 - logprior: -1.4344e+00
Epoch 3/10
52/52 - 13s - loss: 697.7056 - loglik: -6.9641e+02 - logprior: -1.2922e+00
Fitted a model with MAP estimate = -696.4609
Time for alignment: 289.9541
Computed alignments with likelihoods: ['-705.3786', '-698.7173', '-706.3475', '-708.9046', '-696.4609']
Best model has likelihood: -696.4609  (prior= -1.2566 )
time for generating output: 0.5667
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rhv.projection.fasta
SP score = 0.20049528526526336
Training of 5 independent models on file blm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f53001778e0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52952cac40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5296a9e4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f9ebcac0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b0a8a7910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f528a154b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f1749250>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52f17491f0>, <__main__.SimpleDirichletPrior object at 0x7f527d5ef970>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 29s - loss: 1688.2145 - loglik: -1.6861e+03 - logprior: -2.1616e+00
Epoch 2/10
37/37 - 22s - loss: 1606.5829 - loglik: -1.6060e+03 - logprior: -5.4851e-01
Epoch 3/10
37/37 - 22s - loss: 1593.9714 - loglik: -1.5933e+03 - logprior: -6.5499e-01
Epoch 4/10
37/37 - 22s - loss: 1592.3160 - loglik: -1.5916e+03 - logprior: -6.8443e-01
Epoch 5/10
37/37 - 22s - loss: 1586.6576 - loglik: -1.5858e+03 - logprior: -9.0697e-01
Epoch 6/10
37/37 - 22s - loss: 1585.7443 - loglik: -1.5846e+03 - logprior: -1.1515e+00
Epoch 7/10
37/37 - 22s - loss: 1584.8882 - loglik: -1.5835e+03 - logprior: -1.3884e+00
Epoch 8/10
37/37 - 22s - loss: 1584.3333 - loglik: -1.5825e+03 - logprior: -1.7977e+00
Epoch 9/10
37/37 - 22s - loss: 1587.7921 - loglik: -1.5858e+03 - logprior: -1.9949e+00
Fitted a model with MAP estimate = -1581.6825
expansions: [(0, 4), (30, 1), (32, 2), (34, 3), (38, 1), (66, 1), (75, 2), (91, 3), (92, 7), (115, 1), (116, 1), (122, 1), (133, 1), (135, 3), (155, 4), (156, 16), (197, 12), (210, 3), (239, 1), (240, 1)]
discards: [ 98  99 100 101 144 145 146 147 148 149 162 163 164 165 166]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 32s - loss: 1606.4043 - loglik: -1.6035e+03 - logprior: -2.9324e+00
Epoch 2/2
37/37 - 28s - loss: 1582.1826 - loglik: -1.5815e+03 - logprior: -6.7565e-01
Fitted a model with MAP estimate = -1576.8780
expansions: [(0, 2), (39, 1), (40, 1), (159, 1), (162, 1), (240, 1), (242, 3)]
discards: [  0 108 112 113 114 115 178 179 185 186 187 188 189 190 191 192 199 200
 201 202]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 31s - loss: 1587.4862 - loglik: -1.5856e+03 - logprior: -1.8364e+00
Epoch 2/2
37/37 - 27s - loss: 1578.5601 - loglik: -1.5783e+03 - logprior: -2.8551e-01
Fitted a model with MAP estimate = -1578.0350
expansions: [(0, 2), (234, 1)]
discards: [  1  90 235]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 30s - loss: 1584.7698 - loglik: -1.5827e+03 - logprior: -2.0476e+00
Epoch 2/10
37/37 - 27s - loss: 1579.4742 - loglik: -1.5793e+03 - logprior: -1.5082e-01
Epoch 3/10
37/37 - 27s - loss: 1577.6593 - loglik: -1.5776e+03 - logprior: -7.6443e-02
Epoch 4/10
37/37 - 27s - loss: 1571.6106 - loglik: -1.5716e+03 - logprior: -2.2380e-02
Epoch 5/10
37/37 - 27s - loss: 1572.8118 - loglik: -1.5727e+03 - logprior: -7.4977e-02
Fitted a model with MAP estimate = -1568.6881
Time for alignment: 589.9585
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 28s - loss: 1685.8156 - loglik: -1.6837e+03 - logprior: -2.1546e+00
Epoch 2/10
37/37 - 22s - loss: 1606.5502 - loglik: -1.6061e+03 - logprior: -4.8183e-01
Epoch 3/10
37/37 - 22s - loss: 1595.3766 - loglik: -1.5949e+03 - logprior: -4.3850e-01
Epoch 4/10
37/37 - 22s - loss: 1592.1813 - loglik: -1.5917e+03 - logprior: -4.9552e-01
Epoch 5/10
37/37 - 22s - loss: 1586.9832 - loglik: -1.5863e+03 - logprior: -6.7695e-01
Epoch 6/10
37/37 - 22s - loss: 1582.9445 - loglik: -1.5819e+03 - logprior: -1.0940e+00
Epoch 7/10
37/37 - 22s - loss: 1583.8390 - loglik: -1.5826e+03 - logprior: -1.2865e+00
Fitted a model with MAP estimate = -1583.2300
expansions: [(0, 4), (30, 1), (33, 4), (66, 1), (70, 1), (115, 1), (116, 1), (126, 1), (133, 1), (135, 3), (197, 9), (239, 1)]
discards: [ 98  99 100 101 104 152 153 156]
Re-initialized the encoder parameters.
Fitting a model of length 274 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 28s - loss: 1605.3875 - loglik: -1.6028e+03 - logprior: -2.6294e+00
Epoch 2/2
37/37 - 24s - loss: 1587.8665 - loglik: -1.5874e+03 - logprior: -4.7786e-01
Fitted a model with MAP estimate = -1585.0222
expansions: [(0, 2), (37, 2), (38, 2), (145, 1), (148, 1), (165, 1), (166, 1), (213, 4)]
discards: [  0 216]
Re-initialized the encoder parameters.
Fitting a model of length 286 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 29s - loss: 1587.9797 - loglik: -1.5862e+03 - logprior: -1.8058e+00
Epoch 2/2
37/37 - 25s - loss: 1582.3486 - loglik: -1.5821e+03 - logprior: -2.7780e-01
Fitted a model with MAP estimate = -1579.8582
expansions: [(0, 2), (108, 6), (220, 1), (223, 2), (236, 2)]
discards: [  1 172 173 174 224]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 30s - loss: 1586.9105 - loglik: -1.5848e+03 - logprior: -2.0609e+00
Epoch 2/10
37/37 - 26s - loss: 1581.3086 - loglik: -1.5812e+03 - logprior: -1.5759e-01
Epoch 3/10
37/37 - 27s - loss: 1578.4617 - loglik: -1.5784e+03 - logprior: -3.0233e-02
Epoch 4/10
37/37 - 26s - loss: 1573.2219 - loglik: -1.5731e+03 - logprior: -1.0210e-01
Epoch 5/10
37/37 - 27s - loss: 1571.7395 - loglik: -1.5717e+03 - logprior: -7.8573e-02
Epoch 6/10
37/37 - 27s - loss: 1571.3158 - loglik: -1.5711e+03 - logprior: -2.5685e-01
Epoch 7/10
37/37 - 26s - loss: 1567.9698 - loglik: -1.5676e+03 - logprior: -4.0674e-01
Epoch 8/10
37/37 - 27s - loss: 1563.9829 - loglik: -1.5633e+03 - logprior: -6.8069e-01
Epoch 9/10
37/37 - 26s - loss: 1569.5996 - loglik: -1.5686e+03 - logprior: -9.6738e-01
Fitted a model with MAP estimate = -1564.4391
Time for alignment: 630.3563
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 28s - loss: 1691.8699 - loglik: -1.6897e+03 - logprior: -2.1682e+00
Epoch 2/10
37/37 - 22s - loss: 1608.5421 - loglik: -1.6081e+03 - logprior: -4.7602e-01
Epoch 3/10
37/37 - 22s - loss: 1597.1135 - loglik: -1.5966e+03 - logprior: -5.2663e-01
Epoch 4/10
37/37 - 22s - loss: 1595.6228 - loglik: -1.5950e+03 - logprior: -6.3459e-01
Epoch 5/10
37/37 - 22s - loss: 1589.0059 - loglik: -1.5882e+03 - logprior: -8.1525e-01
Epoch 6/10
37/37 - 22s - loss: 1583.1958 - loglik: -1.5822e+03 - logprior: -1.0452e+00
Epoch 7/10
37/37 - 22s - loss: 1587.8450 - loglik: -1.5865e+03 - logprior: -1.3008e+00
Fitted a model with MAP estimate = -1584.0097
expansions: [(0, 4), (30, 1), (32, 2), (35, 3), (39, 2), (60, 1), (65, 1), (92, 1), (93, 2), (94, 6), (116, 1), (121, 2), (122, 1), (133, 1), (136, 4), (197, 12), (222, 1), (239, 1), (244, 1)]
discards: [100 101 102 103 150 153 154]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 30s - loss: 1600.5664 - loglik: -1.5978e+03 - logprior: -2.7456e+00
Epoch 2/2
37/37 - 27s - loss: 1582.0114 - loglik: -1.5813e+03 - logprior: -6.6153e-01
Fitted a model with MAP estimate = -1578.9786
expansions: [(0, 2), (38, 1), (159, 1), (235, 3)]
discards: [  0 107 110 184 185 186 187 291]
Re-initialized the encoder parameters.
Fitting a model of length 293 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 30s - loss: 1585.1042 - loglik: -1.5832e+03 - logprior: -1.9370e+00
Epoch 2/2
37/37 - 26s - loss: 1579.8491 - loglik: -1.5795e+03 - logprior: -3.7746e-01
Fitted a model with MAP estimate = -1576.7696
expansions: [(0, 2), (178, 8), (185, 2)]
discards: [  1   2   3 291]
Re-initialized the encoder parameters.
Fitting a model of length 301 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 31s - loss: 1581.9512 - loglik: -1.5797e+03 - logprior: -2.2181e+00
Epoch 2/10
37/37 - 28s - loss: 1578.1625 - loglik: -1.5779e+03 - logprior: -2.2073e-01
Epoch 3/10
37/37 - 27s - loss: 1574.0153 - loglik: -1.5739e+03 - logprior: -9.3512e-02
Epoch 4/10
37/37 - 27s - loss: 1568.0139 - loglik: -1.5680e+03 - logprior: -4.7618e-02
Epoch 5/10
37/37 - 27s - loss: 1565.5453 - loglik: -1.5654e+03 - logprior: -1.0223e-01
Epoch 6/10
37/37 - 27s - loss: 1563.5629 - loglik: -1.5633e+03 - logprior: -2.9466e-01
Epoch 7/10
37/37 - 28s - loss: 1565.4287 - loglik: -1.5650e+03 - logprior: -4.4965e-01
Fitted a model with MAP estimate = -1561.8547
Time for alignment: 594.8123
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 28s - loss: 1687.8590 - loglik: -1.6857e+03 - logprior: -2.1859e+00
Epoch 2/10
37/37 - 22s - loss: 1602.1552 - loglik: -1.6017e+03 - logprior: -4.5052e-01
Epoch 3/10
37/37 - 22s - loss: 1592.4310 - loglik: -1.5920e+03 - logprior: -4.1803e-01
Epoch 4/10
37/37 - 22s - loss: 1583.6304 - loglik: -1.5832e+03 - logprior: -4.7510e-01
Epoch 5/10
37/37 - 22s - loss: 1585.0509 - loglik: -1.5844e+03 - logprior: -6.5414e-01
Fitted a model with MAP estimate = -1582.0791
expansions: [(0, 4), (34, 3), (35, 2), (39, 1), (61, 1), (66, 1), (99, 1), (197, 12), (239, 1), (242, 1)]
discards: [ 92 101 102 188 189]
Re-initialized the encoder parameters.
Fitting a model of length 276 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 28s - loss: 1593.8676 - loglik: -1.5914e+03 - logprior: -2.5102e+00
Epoch 2/2
37/37 - 24s - loss: 1580.0784 - loglik: -1.5796e+03 - logprior: -5.2783e-01
Fitted a model with MAP estimate = -1579.8426
expansions: [(0, 2), (39, 1), (44, 1), (196, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 280 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 28s - loss: 1584.3730 - loglik: -1.5826e+03 - logprior: -1.7605e+00
Epoch 2/2
37/37 - 25s - loss: 1582.3008 - loglik: -1.5821e+03 - logprior: -2.2768e-01
Fitted a model with MAP estimate = -1578.2905
expansions: [(0, 2), (219, 3), (221, 3), (229, 2)]
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 287 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 29s - loss: 1584.8289 - loglik: -1.5827e+03 - logprior: -2.0879e+00
Epoch 2/10
37/37 - 26s - loss: 1580.8661 - loglik: -1.5807e+03 - logprior: -1.9049e-01
Epoch 3/10
37/37 - 26s - loss: 1576.3525 - loglik: -1.5763e+03 - logprior: -1.1678e-02
Epoch 4/10
37/37 - 26s - loss: 1571.3082 - loglik: -1.5713e+03 - logprior: 0.0213
Epoch 5/10
37/37 - 26s - loss: 1571.8259 - loglik: -1.5718e+03 - logprior: -3.0703e-02
Fitted a model with MAP estimate = -1569.0431
Time for alignment: 475.1125
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 28s - loss: 1683.6324 - loglik: -1.6815e+03 - logprior: -2.1478e+00
Epoch 2/10
37/37 - 22s - loss: 1597.8219 - loglik: -1.5974e+03 - logprior: -3.7317e-01
Epoch 3/10
37/37 - 22s - loss: 1588.8450 - loglik: -1.5885e+03 - logprior: -3.3190e-01
Epoch 4/10
37/37 - 22s - loss: 1583.7987 - loglik: -1.5834e+03 - logprior: -3.9439e-01
Epoch 5/10
37/37 - 22s - loss: 1579.0502 - loglik: -1.5783e+03 - logprior: -7.1481e-01
Epoch 6/10
37/37 - 22s - loss: 1579.8344 - loglik: -1.5790e+03 - logprior: -8.4660e-01
Fitted a model with MAP estimate = -1578.3684
expansions: [(0, 3), (34, 3), (35, 2), (36, 1), (66, 1), (72, 3), (91, 4), (92, 15), (230, 1), (240, 1), (241, 1)]
discards: [151 153 154 155]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 29s - loss: 1594.3761 - loglik: -1.5919e+03 - logprior: -2.5248e+00
Epoch 2/2
37/37 - 25s - loss: 1581.1699 - loglik: -1.5808e+03 - logprior: -3.4671e-01
Fitted a model with MAP estimate = -1577.6737
expansions: [(0, 3), (38, 1), (41, 1), (42, 1), (84, 1), (184, 1), (185, 3), (239, 2), (285, 2)]
discards: [104 105]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 31s - loss: 1583.5487 - loglik: -1.5810e+03 - logprior: -2.5077e+00
Epoch 2/2
37/37 - 27s - loss: 1576.9449 - loglik: -1.5764e+03 - logprior: -4.9957e-01
Fitted a model with MAP estimate = -1574.4445
expansions: [(248, 2), (298, 2)]
discards: [  1   2 191 192 296 297]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 30s - loss: 1580.5635 - loglik: -1.5790e+03 - logprior: -1.5857e+00
Epoch 2/10
37/37 - 26s - loss: 1577.4065 - loglik: -1.5773e+03 - logprior: -1.5571e-01
Epoch 3/10
37/37 - 27s - loss: 1571.3997 - loglik: -1.5714e+03 - logprior: 0.0015
Epoch 4/10
37/37 - 27s - loss: 1573.3085 - loglik: -1.5734e+03 - logprior: 0.0590
Fitted a model with MAP estimate = -1567.3886
Time for alignment: 486.1888
Computed alignments with likelihoods: ['-1568.6881', '-1564.4391', '-1561.8547', '-1569.0431', '-1567.3886']
Best model has likelihood: -1561.8547  (prior= -0.5713 )
time for generating output: 0.2828
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blm.projection.fasta
SP score = 0.9423799582463466
Training of 5 independent models on file cyt3.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f526f92f4f0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f526b21df40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5277cd2ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52722dffa0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f527301d790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f527798ad00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526f8f6d90>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5315614940>, <__main__.SimpleDirichletPrior object at 0x7f5301d9e5e0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 680.1888 - loglik: -5.5355e+02 - logprior: -1.2663e+02
Epoch 2/10
10/10 - 1s - loss: 572.5021 - loglik: -5.4227e+02 - logprior: -3.0236e+01
Epoch 3/10
10/10 - 1s - loss: 541.4066 - loglik: -5.3058e+02 - logprior: -1.0826e+01
Epoch 4/10
10/10 - 1s - loss: 525.9639 - loglik: -5.2199e+02 - logprior: -3.9711e+00
Epoch 5/10
10/10 - 1s - loss: 517.7081 - loglik: -5.1697e+02 - logprior: -7.3858e-01
Epoch 6/10
10/10 - 1s - loss: 512.6407 - loglik: -5.1371e+02 - logprior: 1.0644
Epoch 7/10
10/10 - 1s - loss: 509.7571 - loglik: -5.1180e+02 - logprior: 2.0460
Epoch 8/10
10/10 - 1s - loss: 508.0824 - loglik: -5.1087e+02 - logprior: 2.7867
Epoch 9/10
10/10 - 1s - loss: 507.0251 - loglik: -5.1049e+02 - logprior: 3.4642
Epoch 10/10
10/10 - 1s - loss: 506.2487 - loglik: -5.1025e+02 - logprior: 4.0009
Fitted a model with MAP estimate = -505.9142
expansions: [(0, 6), (51, 3), (65, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 661.2739 - loglik: -5.0881e+02 - logprior: -1.5247e+02
Epoch 2/2
10/10 - 1s - loss: 550.3691 - loglik: -5.0717e+02 - logprior: -4.3195e+01
Fitted a model with MAP estimate = -529.8737
expansions: [(0, 4)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 626.5399 - loglik: -5.0682e+02 - logprior: -1.1972e+02
Epoch 2/2
10/10 - 1s - loss: 537.4691 - loglik: -5.0661e+02 - logprior: -3.0858e+01
Fitted a model with MAP estimate = -522.3022
expansions: [(0, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 89 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 633.4277 - loglik: -5.0587e+02 - logprior: -1.2756e+02
Epoch 2/10
10/10 - 1s - loss: 542.3098 - loglik: -5.0589e+02 - logprior: -3.6424e+01
Epoch 3/10
10/10 - 1s - loss: 518.1473 - loglik: -5.0647e+02 - logprior: -1.1677e+01
Epoch 4/10
10/10 - 1s - loss: 508.1995 - loglik: -5.0694e+02 - logprior: -1.2620e+00
Epoch 5/10
10/10 - 1s - loss: 503.6966 - loglik: -5.0717e+02 - logprior: 3.4763
Epoch 6/10
10/10 - 1s - loss: 501.2255 - loglik: -5.0717e+02 - logprior: 5.9448
Epoch 7/10
10/10 - 1s - loss: 499.6779 - loglik: -5.0708e+02 - logprior: 7.4008
Epoch 8/10
10/10 - 1s - loss: 498.6024 - loglik: -5.0698e+02 - logprior: 8.3732
Epoch 9/10
10/10 - 1s - loss: 497.7856 - loglik: -5.0694e+02 - logprior: 9.1566
Epoch 10/10
10/10 - 1s - loss: 497.1133 - loglik: -5.0698e+02 - logprior: 9.8713
Fitted a model with MAP estimate = -496.7781
Time for alignment: 47.5991
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 680.1888 - loglik: -5.5355e+02 - logprior: -1.2663e+02
Epoch 2/10
10/10 - 1s - loss: 572.5021 - loglik: -5.4227e+02 - logprior: -3.0236e+01
Epoch 3/10
10/10 - 1s - loss: 541.4066 - loglik: -5.3058e+02 - logprior: -1.0826e+01
Epoch 4/10
10/10 - 1s - loss: 525.9639 - loglik: -5.2199e+02 - logprior: -3.9711e+00
Epoch 5/10
10/10 - 1s - loss: 517.7081 - loglik: -5.1697e+02 - logprior: -7.3859e-01
Epoch 6/10
10/10 - 1s - loss: 512.6407 - loglik: -5.1371e+02 - logprior: 1.0644
Epoch 7/10
10/10 - 1s - loss: 509.7570 - loglik: -5.1180e+02 - logprior: 2.0460
Epoch 8/10
10/10 - 1s - loss: 508.0824 - loglik: -5.1087e+02 - logprior: 2.7867
Epoch 9/10
10/10 - 1s - loss: 507.0251 - loglik: -5.1049e+02 - logprior: 3.4641
Epoch 10/10
10/10 - 1s - loss: 506.2487 - loglik: -5.1025e+02 - logprior: 4.0009
Fitted a model with MAP estimate = -505.9142
expansions: [(0, 6), (51, 3), (65, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 661.2739 - loglik: -5.0881e+02 - logprior: -1.5247e+02
Epoch 2/2
10/10 - 1s - loss: 550.3691 - loglik: -5.0717e+02 - logprior: -4.3195e+01
Fitted a model with MAP estimate = -529.8736
expansions: [(0, 4)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 626.5399 - loglik: -5.0682e+02 - logprior: -1.1972e+02
Epoch 2/2
10/10 - 1s - loss: 537.4691 - loglik: -5.0661e+02 - logprior: -3.0858e+01
Fitted a model with MAP estimate = -522.3021
expansions: [(0, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 89 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 633.4277 - loglik: -5.0587e+02 - logprior: -1.2756e+02
Epoch 2/10
10/10 - 1s - loss: 542.3098 - loglik: -5.0589e+02 - logprior: -3.6424e+01
Epoch 3/10
10/10 - 1s - loss: 518.1473 - loglik: -5.0647e+02 - logprior: -1.1677e+01
Epoch 4/10
10/10 - 1s - loss: 508.1995 - loglik: -5.0694e+02 - logprior: -1.2620e+00
Epoch 5/10
10/10 - 1s - loss: 503.6966 - loglik: -5.0717e+02 - logprior: 3.4764
Epoch 6/10
10/10 - 1s - loss: 501.2254 - loglik: -5.0717e+02 - logprior: 5.9448
Epoch 7/10
10/10 - 1s - loss: 499.6779 - loglik: -5.0708e+02 - logprior: 7.4008
Epoch 8/10
10/10 - 1s - loss: 498.6024 - loglik: -5.0698e+02 - logprior: 8.3733
Epoch 9/10
10/10 - 1s - loss: 497.7857 - loglik: -5.0694e+02 - logprior: 9.1566
Epoch 10/10
10/10 - 1s - loss: 497.1133 - loglik: -5.0698e+02 - logprior: 9.8713
Fitted a model with MAP estimate = -496.7781
Time for alignment: 46.6094
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 680.1888 - loglik: -5.5355e+02 - logprior: -1.2663e+02
Epoch 2/10
10/10 - 1s - loss: 572.5021 - loglik: -5.4227e+02 - logprior: -3.0236e+01
Epoch 3/10
10/10 - 1s - loss: 541.4066 - loglik: -5.3058e+02 - logprior: -1.0826e+01
Epoch 4/10
10/10 - 1s - loss: 525.9639 - loglik: -5.2199e+02 - logprior: -3.9711e+00
Epoch 5/10
10/10 - 1s - loss: 517.7081 - loglik: -5.1697e+02 - logprior: -7.3858e-01
Epoch 6/10
10/10 - 1s - loss: 512.6407 - loglik: -5.1371e+02 - logprior: 1.0644
Epoch 7/10
10/10 - 1s - loss: 509.7570 - loglik: -5.1180e+02 - logprior: 2.0460
Epoch 8/10
10/10 - 1s - loss: 508.0823 - loglik: -5.1087e+02 - logprior: 2.7867
Epoch 9/10
10/10 - 1s - loss: 507.0252 - loglik: -5.1049e+02 - logprior: 3.4642
Epoch 10/10
10/10 - 1s - loss: 506.2487 - loglik: -5.1025e+02 - logprior: 4.0009
Fitted a model with MAP estimate = -505.9143
expansions: [(0, 6), (51, 3), (65, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 661.2740 - loglik: -5.0881e+02 - logprior: -1.5247e+02
Epoch 2/2
10/10 - 1s - loss: 550.3691 - loglik: -5.0717e+02 - logprior: -4.3195e+01
Fitted a model with MAP estimate = -529.8737
expansions: [(0, 4)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 626.5399 - loglik: -5.0682e+02 - logprior: -1.1972e+02
Epoch 2/2
10/10 - 1s - loss: 537.4691 - loglik: -5.0661e+02 - logprior: -3.0858e+01
Fitted a model with MAP estimate = -522.3022
expansions: [(0, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 89 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 633.4276 - loglik: -5.0587e+02 - logprior: -1.2756e+02
Epoch 2/10
10/10 - 1s - loss: 542.3097 - loglik: -5.0589e+02 - logprior: -3.6424e+01
Epoch 3/10
10/10 - 1s - loss: 518.1473 - loglik: -5.0647e+02 - logprior: -1.1677e+01
Epoch 4/10
10/10 - 1s - loss: 508.1995 - loglik: -5.0694e+02 - logprior: -1.2620e+00
Epoch 5/10
10/10 - 1s - loss: 503.6966 - loglik: -5.0717e+02 - logprior: 3.4764
Epoch 6/10
10/10 - 1s - loss: 501.2255 - loglik: -5.0717e+02 - logprior: 5.9448
Epoch 7/10
10/10 - 1s - loss: 499.6779 - loglik: -5.0708e+02 - logprior: 7.4009
Epoch 8/10
10/10 - 1s - loss: 498.6024 - loglik: -5.0698e+02 - logprior: 8.3733
Epoch 9/10
10/10 - 1s - loss: 497.7856 - loglik: -5.0694e+02 - logprior: 9.1566
Epoch 10/10
10/10 - 1s - loss: 497.1132 - loglik: -5.0698e+02 - logprior: 9.8713
Fitted a model with MAP estimate = -496.7781
Time for alignment: 44.9208
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 680.1888 - loglik: -5.5355e+02 - logprior: -1.2663e+02
Epoch 2/10
10/10 - 1s - loss: 572.5021 - loglik: -5.4227e+02 - logprior: -3.0236e+01
Epoch 3/10
10/10 - 1s - loss: 541.4066 - loglik: -5.3058e+02 - logprior: -1.0826e+01
Epoch 4/10
10/10 - 1s - loss: 525.9639 - loglik: -5.2199e+02 - logprior: -3.9711e+00
Epoch 5/10
10/10 - 1s - loss: 517.7081 - loglik: -5.1697e+02 - logprior: -7.3858e-01
Epoch 6/10
10/10 - 1s - loss: 512.6407 - loglik: -5.1371e+02 - logprior: 1.0644
Epoch 7/10
10/10 - 1s - loss: 509.7570 - loglik: -5.1180e+02 - logprior: 2.0460
Epoch 8/10
10/10 - 1s - loss: 508.0824 - loglik: -5.1087e+02 - logprior: 2.7867
Epoch 9/10
10/10 - 1s - loss: 507.0251 - loglik: -5.1049e+02 - logprior: 3.4642
Epoch 10/10
10/10 - 1s - loss: 506.2487 - loglik: -5.1025e+02 - logprior: 4.0009
Fitted a model with MAP estimate = -505.9142
expansions: [(0, 6), (51, 3), (65, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 661.2740 - loglik: -5.0881e+02 - logprior: -1.5247e+02
Epoch 2/2
10/10 - 1s - loss: 550.3691 - loglik: -5.0717e+02 - logprior: -4.3195e+01
Fitted a model with MAP estimate = -529.8736
expansions: [(0, 4)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 626.5399 - loglik: -5.0682e+02 - logprior: -1.1972e+02
Epoch 2/2
10/10 - 1s - loss: 537.4691 - loglik: -5.0661e+02 - logprior: -3.0858e+01
Fitted a model with MAP estimate = -522.3022
expansions: [(0, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 89 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 633.4276 - loglik: -5.0587e+02 - logprior: -1.2756e+02
Epoch 2/10
10/10 - 1s - loss: 542.3097 - loglik: -5.0589e+02 - logprior: -3.6424e+01
Epoch 3/10
10/10 - 1s - loss: 518.1473 - loglik: -5.0647e+02 - logprior: -1.1677e+01
Epoch 4/10
10/10 - 1s - loss: 508.1995 - loglik: -5.0694e+02 - logprior: -1.2620e+00
Epoch 5/10
10/10 - 1s - loss: 503.6966 - loglik: -5.0717e+02 - logprior: 3.4764
Epoch 6/10
10/10 - 1s - loss: 501.2254 - loglik: -5.0717e+02 - logprior: 5.9448
Epoch 7/10
10/10 - 1s - loss: 499.6779 - loglik: -5.0708e+02 - logprior: 7.4008
Epoch 8/10
10/10 - 1s - loss: 498.6024 - loglik: -5.0698e+02 - logprior: 8.3733
Epoch 9/10
10/10 - 1s - loss: 497.7856 - loglik: -5.0694e+02 - logprior: 9.1566
Epoch 10/10
10/10 - 1s - loss: 497.1132 - loglik: -5.0698e+02 - logprior: 9.8713
Fitted a model with MAP estimate = -496.7781
Time for alignment: 42.9613
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 680.1888 - loglik: -5.5355e+02 - logprior: -1.2663e+02
Epoch 2/10
10/10 - 1s - loss: 572.5021 - loglik: -5.4227e+02 - logprior: -3.0236e+01
Epoch 3/10
10/10 - 1s - loss: 541.4066 - loglik: -5.3058e+02 - logprior: -1.0826e+01
Epoch 4/10
10/10 - 1s - loss: 525.9639 - loglik: -5.2199e+02 - logprior: -3.9711e+00
Epoch 5/10
10/10 - 1s - loss: 517.7081 - loglik: -5.1697e+02 - logprior: -7.3859e-01
Epoch 6/10
10/10 - 1s - loss: 512.6407 - loglik: -5.1371e+02 - logprior: 1.0644
Epoch 7/10
10/10 - 1s - loss: 509.7570 - loglik: -5.1180e+02 - logprior: 2.0460
Epoch 8/10
10/10 - 1s - loss: 508.0824 - loglik: -5.1087e+02 - logprior: 2.7867
Epoch 9/10
10/10 - 1s - loss: 507.0251 - loglik: -5.1049e+02 - logprior: 3.4641
Epoch 10/10
10/10 - 1s - loss: 506.2487 - loglik: -5.1025e+02 - logprior: 4.0009
Fitted a model with MAP estimate = -505.9141
expansions: [(0, 6), (51, 3), (65, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 661.2739 - loglik: -5.0881e+02 - logprior: -1.5247e+02
Epoch 2/2
10/10 - 1s - loss: 550.3691 - loglik: -5.0717e+02 - logprior: -4.3195e+01
Fitted a model with MAP estimate = -529.8737
expansions: [(0, 4)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 626.5399 - loglik: -5.0682e+02 - logprior: -1.1972e+02
Epoch 2/2
10/10 - 1s - loss: 537.4691 - loglik: -5.0661e+02 - logprior: -3.0858e+01
Fitted a model with MAP estimate = -522.3022
expansions: [(0, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 89 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 633.4277 - loglik: -5.0587e+02 - logprior: -1.2756e+02
Epoch 2/10
10/10 - 1s - loss: 542.3098 - loglik: -5.0589e+02 - logprior: -3.6424e+01
Epoch 3/10
10/10 - 1s - loss: 518.1473 - loglik: -5.0647e+02 - logprior: -1.1677e+01
Epoch 4/10
10/10 - 1s - loss: 508.1995 - loglik: -5.0694e+02 - logprior: -1.2620e+00
Epoch 5/10
10/10 - 1s - loss: 503.6966 - loglik: -5.0717e+02 - logprior: 3.4764
Epoch 6/10
10/10 - 1s - loss: 501.2254 - loglik: -5.0717e+02 - logprior: 5.9448
Epoch 7/10
10/10 - 1s - loss: 499.6779 - loglik: -5.0708e+02 - logprior: 7.4008
Epoch 8/10
10/10 - 1s - loss: 498.6024 - loglik: -5.0698e+02 - logprior: 8.3733
Epoch 9/10
10/10 - 1s - loss: 497.7856 - loglik: -5.0694e+02 - logprior: 9.1566
Epoch 10/10
10/10 - 1s - loss: 497.1133 - loglik: -5.0698e+02 - logprior: 9.8713
Fitted a model with MAP estimate = -496.7780
Time for alignment: 45.3234
Computed alignments with likelihoods: ['-496.7781', '-496.7781', '-496.7781', '-496.7781', '-496.7780']
Best model has likelihood: -496.7780  (prior= 10.2502 )
time for generating output: 0.1701
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyt3.projection.fasta
SP score = 0.7019041365725541
Training of 5 independent models on file mofe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5273434d60>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5271c9f880>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52970d8f40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f85f3eb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5270cd8a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5afeba7550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5afeba7460>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f527ede81f0>, <__main__.SimpleDirichletPrior object at 0x7f5334ce7ac0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 22s - loss: 2030.3787 - loglik: -2.0175e+03 - logprior: -1.2898e+01
Epoch 2/10
19/19 - 19s - loss: 1890.8900 - loglik: -1.8915e+03 - logprior: 0.5790
Epoch 3/10
19/19 - 19s - loss: 1839.4105 - loglik: -1.8394e+03 - logprior: 0.0107
Epoch 4/10
19/19 - 19s - loss: 1821.8320 - loglik: -1.8218e+03 - logprior: -2.0647e-02
Epoch 5/10
19/19 - 19s - loss: 1813.2301 - loglik: -1.8132e+03 - logprior: 0.0078
Epoch 6/10
19/19 - 19s - loss: 1816.1963 - loglik: -1.8160e+03 - logprior: -1.6699e-01
Fitted a model with MAP estimate = -1812.4587
expansions: [(28, 1), (57, 1), (99, 1), (100, 3), (101, 1), (107, 1), (112, 1), (113, 1), (115, 1), (116, 1), (117, 1), (118, 1), (120, 6), (121, 3), (154, 1), (156, 2), (163, 1), (165, 3), (168, 1), (175, 2), (176, 1), (177, 1), (189, 4), (204, 1), (218, 1), (219, 1), (221, 1), (222, 2), (224, 1), (238, 1), (243, 1), (261, 1), (265, 5), (295, 4), (301, 1), (302, 3), (309, 1), (310, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 385 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 32s - loss: 1823.0890 - loglik: -1.8109e+03 - logprior: -1.2175e+01
Epoch 2/2
19/19 - 24s - loss: 1794.1252 - loglik: -1.7918e+03 - logprior: -2.2795e+00
Fitted a model with MAP estimate = -1784.1197
expansions: [(0, 2), (315, 2)]
discards: [  0 102 103 205 349 350 351]
Re-initialized the encoder parameters.
Fitting a model of length 382 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 28s - loss: 1796.3386 - loglik: -1.7898e+03 - logprior: -6.5419e+00
Epoch 2/2
19/19 - 24s - loss: 1779.4489 - loglik: -1.7820e+03 - logprior: 2.5297
Fitted a model with MAP estimate = -1775.6174
expansions: []
discards: [  0 142 355]
Re-initialized the encoder parameters.
Fitting a model of length 379 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 28s - loss: 1796.3435 - loglik: -1.7871e+03 - logprior: -9.2093e+00
Epoch 2/10
19/19 - 24s - loss: 1785.4628 - loglik: -1.7861e+03 - logprior: 0.6424
Epoch 3/10
19/19 - 24s - loss: 1771.6953 - loglik: -1.7758e+03 - logprior: 4.0940
Epoch 4/10
19/19 - 24s - loss: 1773.3356 - loglik: -1.7780e+03 - logprior: 4.7027
Fitted a model with MAP estimate = -1769.7869
Time for alignment: 392.3416
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 23s - loss: 2026.4541 - loglik: -2.0136e+03 - logprior: -1.2879e+01
Epoch 2/10
19/19 - 20s - loss: 1898.9303 - loglik: -1.8995e+03 - logprior: 0.6097
Epoch 3/10
19/19 - 19s - loss: 1830.3375 - loglik: -1.8303e+03 - logprior: 0.0059
Epoch 4/10
19/19 - 19s - loss: 1823.6836 - loglik: -1.8237e+03 - logprior: -2.5822e-02
Epoch 5/10
19/19 - 19s - loss: 1813.0498 - loglik: -1.8131e+03 - logprior: 0.0132
Epoch 6/10
19/19 - 19s - loss: 1811.8805 - loglik: -1.8115e+03 - logprior: -3.5657e-01
Epoch 7/10
19/19 - 19s - loss: 1811.9589 - loglik: -1.8116e+03 - logprior: -3.4834e-01
Fitted a model with MAP estimate = -1810.7635
expansions: [(30, 1), (57, 1), (95, 1), (96, 1), (97, 2), (98, 2), (106, 1), (112, 1), (113, 1), (116, 1), (117, 1), (118, 1), (121, 2), (122, 4), (124, 3), (142, 1), (143, 1), (157, 1), (164, 1), (166, 2), (168, 1), (169, 1), (176, 2), (177, 1), (178, 1), (190, 4), (205, 1), (212, 2), (221, 2), (223, 2), (225, 1), (238, 1), (242, 1), (243, 1), (267, 8), (301, 1), (302, 3), (311, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 384 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 31s - loss: 1823.6772 - loglik: -1.8114e+03 - logprior: -1.2251e+01
Epoch 2/2
19/19 - 24s - loss: 1796.7157 - loglik: -1.7943e+03 - logprior: -2.4400e+00
Fitted a model with MAP estimate = -1788.0316
expansions: [(0, 2), (353, 4), (378, 1)]
discards: [  0  29 104 135 145 146 206 251 263 319]
Re-initialized the encoder parameters.
Fitting a model of length 381 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 28s - loss: 1800.2341 - loglik: -1.7935e+03 - logprior: -6.7535e+00
Epoch 2/2
19/19 - 24s - loss: 1774.9044 - loglik: -1.7771e+03 - logprior: 2.1929
Fitted a model with MAP estimate = -1777.5751
expansions: [(30, 1), (254, 1), (260, 1)]
discards: [  0 246 345 346]
Re-initialized the encoder parameters.
Fitting a model of length 380 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 28s - loss: 1801.7008 - loglik: -1.7921e+03 - logprior: -9.5984e+00
Epoch 2/10
19/19 - 24s - loss: 1781.7549 - loglik: -1.7822e+03 - logprior: 0.4779
Epoch 3/10
19/19 - 24s - loss: 1777.1431 - loglik: -1.7812e+03 - logprior: 4.0111
Epoch 4/10
19/19 - 24s - loss: 1769.9550 - loglik: -1.7744e+03 - logprior: 4.3985
Epoch 5/10
19/19 - 23s - loss: 1770.8955 - loglik: -1.7755e+03 - logprior: 4.6305
Fitted a model with MAP estimate = -1769.2136
Time for alignment: 434.6096
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 23s - loss: 2030.2008 - loglik: -2.0173e+03 - logprior: -1.2883e+01
Epoch 2/10
19/19 - 19s - loss: 1898.0138 - loglik: -1.8985e+03 - logprior: 0.5352
Epoch 3/10
19/19 - 19s - loss: 1838.0841 - loglik: -1.8382e+03 - logprior: 0.0688
Epoch 4/10
19/19 - 19s - loss: 1820.3555 - loglik: -1.8203e+03 - logprior: -7.9421e-02
Epoch 5/10
19/19 - 20s - loss: 1814.0490 - loglik: -1.8138e+03 - logprior: -2.3096e-01
Epoch 6/10
19/19 - 19s - loss: 1810.5481 - loglik: -1.8100e+03 - logprior: -5.2395e-01
Epoch 7/10
19/19 - 20s - loss: 1813.9860 - loglik: -1.8135e+03 - logprior: -5.0307e-01
Fitted a model with MAP estimate = -1811.3542
expansions: [(32, 1), (57, 1), (97, 1), (98, 1), (107, 1), (113, 1), (114, 1), (118, 1), (119, 1), (120, 1), (123, 2), (124, 3), (126, 3), (145, 1), (146, 1), (148, 1), (159, 1), (165, 2), (166, 2), (167, 3), (168, 1), (170, 1), (177, 1), (178, 2), (179, 1), (190, 3), (196, 1), (205, 1), (210, 1), (218, 1), (220, 1), (221, 1), (222, 2), (224, 1), (238, 2), (259, 1), (260, 1), (263, 1), (264, 3), (265, 2), (301, 1), (302, 3), (311, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 383 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 31s - loss: 1827.4426 - loglik: -1.8152e+03 - logprior: -1.2281e+01
Epoch 2/2
19/19 - 25s - loss: 1788.4420 - loglik: -1.7859e+03 - logprior: -2.5712e+00
Fitted a model with MAP estimate = -1786.3598
expansions: [(0, 2), (377, 1)]
discards: [  0 143 144 190 191 209]
Re-initialized the encoder parameters.
Fitting a model of length 380 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 28s - loss: 1796.4303 - loglik: -1.7898e+03 - logprior: -6.5803e+00
Epoch 2/2
19/19 - 24s - loss: 1782.7671 - loglik: -1.7851e+03 - logprior: 2.3664
Fitted a model with MAP estimate = -1777.0715
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 379 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 27s - loss: 1801.0941 - loglik: -1.7916e+03 - logprior: -9.4747e+00
Epoch 2/10
19/19 - 24s - loss: 1783.5839 - loglik: -1.7841e+03 - logprior: 0.5374
Epoch 3/10
19/19 - 24s - loss: 1772.9484 - loglik: -1.7768e+03 - logprior: 3.8848
Epoch 4/10
19/19 - 24s - loss: 1771.3240 - loglik: -1.7758e+03 - logprior: 4.4885
Epoch 5/10
19/19 - 24s - loss: 1771.6079 - loglik: -1.7765e+03 - logprior: 4.8838
Fitted a model with MAP estimate = -1769.1620
Time for alignment: 435.2335
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 23s - loss: 2028.9663 - loglik: -2.0161e+03 - logprior: -1.2898e+01
Epoch 2/10
19/19 - 19s - loss: 1893.8506 - loglik: -1.8946e+03 - logprior: 0.7043
Epoch 3/10
19/19 - 19s - loss: 1838.7164 - loglik: -1.8387e+03 - logprior: -1.9766e-03
Epoch 4/10
19/19 - 19s - loss: 1828.1539 - loglik: -1.8280e+03 - logprior: -1.2131e-01
Epoch 5/10
19/19 - 20s - loss: 1813.8754 - loglik: -1.8138e+03 - logprior: -3.5268e-02
Epoch 6/10
19/19 - 19s - loss: 1819.6215 - loglik: -1.8195e+03 - logprior: -1.3445e-01
Fitted a model with MAP estimate = -1814.4227
expansions: [(30, 1), (57, 1), (109, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 2), (121, 5), (124, 2), (125, 3), (126, 1), (142, 1), (143, 1), (145, 1), (163, 1), (166, 1), (167, 1), (168, 1), (170, 1), (177, 1), (178, 2), (181, 1), (190, 4), (202, 1), (205, 1), (212, 2), (221, 2), (223, 1), (224, 1), (227, 1), (237, 1), (238, 1), (242, 2), (266, 5), (294, 4), (301, 1), (302, 3), (311, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 384 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 27s - loss: 1828.0541 - loglik: -1.8160e+03 - logprior: -1.2048e+01
Epoch 2/2
19/19 - 24s - loss: 1799.5364 - loglik: -1.7965e+03 - logprior: -3.0567e+00
Fitted a model with MAP estimate = -1786.9438
expansions: [(0, 2), (315, 1), (316, 1), (376, 1)]
discards: [  0  29 142 206 289 347 348 349]
Re-initialized the encoder parameters.
Fitting a model of length 381 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 31s - loss: 1798.5156 - loglik: -1.7924e+03 - logprior: -6.0980e+00
Epoch 2/2
19/19 - 24s - loss: 1777.2930 - loglik: -1.7798e+03 - logprior: 2.5414
Fitted a model with MAP estimate = -1777.9002
expansions: [(30, 1)]
discards: [  0 354]
Re-initialized the encoder parameters.
Fitting a model of length 380 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 28s - loss: 1797.2662 - loglik: -1.7881e+03 - logprior: -9.2162e+00
Epoch 2/10
19/19 - 24s - loss: 1786.1838 - loglik: -1.7867e+03 - logprior: 0.5393
Epoch 3/10
19/19 - 24s - loss: 1776.9210 - loglik: -1.7810e+03 - logprior: 4.0955
Epoch 4/10
19/19 - 24s - loss: 1764.8302 - loglik: -1.7695e+03 - logprior: 4.6976
Epoch 5/10
19/19 - 24s - loss: 1773.2151 - loglik: -1.7782e+03 - logprior: 4.9763
Fitted a model with MAP estimate = -1768.1298
Time for alignment: 415.5847
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 23s - loss: 2030.2438 - loglik: -2.0173e+03 - logprior: -1.2908e+01
Epoch 2/10
19/19 - 19s - loss: 1894.7944 - loglik: -1.8954e+03 - logprior: 0.5845
Epoch 3/10
19/19 - 19s - loss: 1844.8774 - loglik: -1.8450e+03 - logprior: 0.0760
Epoch 4/10
19/19 - 19s - loss: 1827.6812 - loglik: -1.8276e+03 - logprior: -6.5734e-02
Epoch 5/10
19/19 - 19s - loss: 1822.7393 - loglik: -1.8227e+03 - logprior: -6.7658e-03
Epoch 6/10
19/19 - 20s - loss: 1817.7307 - loglik: -1.8177e+03 - logprior: -4.3652e-02
Epoch 7/10
19/19 - 20s - loss: 1814.7292 - loglik: -1.8146e+03 - logprior: -1.3277e-01
Epoch 8/10
19/19 - 19s - loss: 1821.5321 - loglik: -1.8213e+03 - logprior: -2.5337e-01
Fitted a model with MAP estimate = -1816.1266
expansions: [(14, 1), (65, 1), (82, 1), (98, 1), (99, 3), (107, 1), (113, 1), (114, 1), (117, 1), (118, 1), (119, 1), (121, 4), (123, 2), (143, 1), (155, 1), (165, 1), (167, 3), (168, 1), (170, 1), (177, 1), (178, 1), (181, 1), (191, 3), (199, 1), (206, 1), (212, 2), (219, 3), (223, 2), (225, 1), (239, 1), (242, 1), (243, 1), (266, 7), (294, 1), (301, 2), (302, 4), (303, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 381 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 27s - loss: 1833.4976 - loglik: -1.8217e+03 - logprior: -1.1829e+01
Epoch 2/2
19/19 - 24s - loss: 1792.1102 - loglik: -1.7898e+03 - logprior: -2.3367e+00
Fitted a model with MAP estimate = -1791.3147
expansions: [(0, 2), (220, 1), (263, 1)]
discards: [  0 103 104 136 246 247 257]
Re-initialized the encoder parameters.
Fitting a model of length 378 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 30s - loss: 1805.0250 - loglik: -1.7984e+03 - logprior: -6.6469e+00
Epoch 2/2
19/19 - 24s - loss: 1782.2837 - loglik: -1.7847e+03 - logprior: 2.4321
Fitted a model with MAP estimate = -1779.7771
expansions: [(175, 1), (257, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 379 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 27s - loss: 1800.6149 - loglik: -1.7912e+03 - logprior: -9.3672e+00
Epoch 2/10
19/19 - 24s - loss: 1786.4642 - loglik: -1.7872e+03 - logprior: 0.7273
Epoch 3/10
19/19 - 24s - loss: 1772.4471 - loglik: -1.7765e+03 - logprior: 4.0947
Epoch 4/10
19/19 - 24s - loss: 1777.8621 - loglik: -1.7826e+03 - logprior: 4.6911
Fitted a model with MAP estimate = -1770.4555
Time for alignment: 428.0345
Computed alignments with likelihoods: ['-1769.7869', '-1769.2136', '-1769.1620', '-1768.1298', '-1770.4555']
Best model has likelihood: -1768.1298  (prior= 5.1539 )
time for generating output: 0.5580
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mofe.projection.fasta
SP score = 0.6826923076923077
Training of 5 independent models on file ghf22.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5ca76925e0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52712849d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52772f1d30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f527eadf3d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f527ddfe310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526f322970>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52d8f99490>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5295197a60>, <__main__.SimpleDirichletPrior object at 0x7f5b130ba160>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 703.2689 - loglik: -6.3838e+02 - logprior: -6.4889e+01
Epoch 2/10
10/10 - 1s - loss: 617.9055 - loglik: -6.0265e+02 - logprior: -1.5253e+01
Epoch 3/10
10/10 - 1s - loss: 569.6029 - loglik: -5.6310e+02 - logprior: -6.4998e+00
Epoch 4/10
10/10 - 1s - loss: 541.1818 - loglik: -5.3716e+02 - logprior: -4.0199e+00
Epoch 5/10
10/10 - 1s - loss: 529.4931 - loglik: -5.2656e+02 - logprior: -2.9341e+00
Epoch 6/10
10/10 - 1s - loss: 525.1046 - loglik: -5.2287e+02 - logprior: -2.2319e+00
Epoch 7/10
10/10 - 1s - loss: 522.1063 - loglik: -5.2035e+02 - logprior: -1.7529e+00
Epoch 8/10
10/10 - 1s - loss: 520.9481 - loglik: -5.1946e+02 - logprior: -1.4920e+00
Epoch 9/10
10/10 - 1s - loss: 520.4080 - loglik: -5.1912e+02 - logprior: -1.2927e+00
Epoch 10/10
10/10 - 1s - loss: 519.3751 - loglik: -5.1828e+02 - logprior: -1.0948e+00
Fitted a model with MAP estimate = -519.6895
expansions: [(13, 5), (17, 2), (25, 1), (32, 1), (33, 1), (34, 1), (53, 1), (55, 2), (56, 2), (57, 1), (60, 1), (62, 1), (78, 4), (81, 1), (86, 1), (87, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 578.1093 - loglik: -5.1212e+02 - logprior: -6.5990e+01
Epoch 2/2
10/10 - 1s - loss: 524.6831 - loglik: -4.9953e+02 - logprior: -2.5156e+01
Fitted a model with MAP estimate = -515.5480
expansions: [(0, 2), (69, 1)]
discards: [ 0 21 22 98]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 547.5604 - loglik: -4.9727e+02 - logprior: -5.0291e+01
Epoch 2/2
10/10 - 1s - loss: 505.0498 - loglik: -4.9395e+02 - logprior: -1.1098e+01
Fitted a model with MAP estimate = -499.1221
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 556.2527 - loglik: -4.9587e+02 - logprior: -6.0386e+01
Epoch 2/10
10/10 - 1s - loss: 512.0594 - loglik: -4.9511e+02 - logprior: -1.6952e+01
Epoch 3/10
10/10 - 1s - loss: 500.0174 - loglik: -4.9588e+02 - logprior: -4.1349e+00
Epoch 4/10
10/10 - 1s - loss: 494.0873 - loglik: -4.9435e+02 - logprior: 0.2637
Epoch 5/10
10/10 - 1s - loss: 492.2710 - loglik: -4.9442e+02 - logprior: 2.1451
Epoch 6/10
10/10 - 1s - loss: 491.5161 - loglik: -4.9477e+02 - logprior: 3.2540
Epoch 7/10
10/10 - 1s - loss: 490.1646 - loglik: -4.9429e+02 - logprior: 4.1256
Epoch 8/10
10/10 - 1s - loss: 489.4449 - loglik: -4.9421e+02 - logprior: 4.7640
Epoch 9/10
10/10 - 1s - loss: 490.2258 - loglik: -4.9543e+02 - logprior: 5.2063
Fitted a model with MAP estimate = -489.4790
Time for alignment: 46.9185
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 702.9781 - loglik: -6.3809e+02 - logprior: -6.4890e+01
Epoch 2/10
10/10 - 1s - loss: 619.6007 - loglik: -6.0435e+02 - logprior: -1.5249e+01
Epoch 3/10
10/10 - 1s - loss: 569.9491 - loglik: -5.6328e+02 - logprior: -6.6653e+00
Epoch 4/10
10/10 - 1s - loss: 541.1465 - loglik: -5.3684e+02 - logprior: -4.3063e+00
Epoch 5/10
10/10 - 1s - loss: 531.4086 - loglik: -5.2854e+02 - logprior: -2.8682e+00
Epoch 6/10
10/10 - 1s - loss: 528.1826 - loglik: -5.2603e+02 - logprior: -2.1567e+00
Epoch 7/10
10/10 - 1s - loss: 523.5709 - loglik: -5.2176e+02 - logprior: -1.8112e+00
Epoch 8/10
10/10 - 1s - loss: 521.8668 - loglik: -5.2029e+02 - logprior: -1.5746e+00
Epoch 9/10
10/10 - 1s - loss: 522.2277 - loglik: -5.2088e+02 - logprior: -1.3492e+00
Fitted a model with MAP estimate = -520.8372
expansions: [(13, 4), (17, 2), (25, 1), (32, 1), (33, 1), (34, 2), (35, 1), (52, 1), (54, 2), (56, 2), (57, 1), (62, 1), (65, 1), (76, 1), (77, 3), (78, 1), (86, 1), (87, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 580.7714 - loglik: -5.1464e+02 - logprior: -6.6132e+01
Epoch 2/2
10/10 - 1s - loss: 525.2923 - loglik: -5.0019e+02 - logprior: -2.5101e+01
Fitted a model with MAP estimate = -516.4429
expansions: [(0, 2)]
discards: [ 0 99]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 545.4238 - loglik: -4.9518e+02 - logprior: -5.0244e+01
Epoch 2/2
10/10 - 1s - loss: 504.2959 - loglik: -4.9310e+02 - logprior: -1.1195e+01
Fitted a model with MAP estimate = -497.8394
expansions: [(15, 1)]
discards: [ 0 21 22]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 557.8422 - loglik: -4.9678e+02 - logprior: -6.1062e+01
Epoch 2/10
10/10 - 1s - loss: 513.9621 - loglik: -4.9540e+02 - logprior: -1.8558e+01
Epoch 3/10
10/10 - 1s - loss: 499.4648 - loglik: -4.9467e+02 - logprior: -4.7947e+00
Epoch 4/10
10/10 - 1s - loss: 493.8480 - loglik: -4.9406e+02 - logprior: 0.2159
Epoch 5/10
10/10 - 1s - loss: 492.2007 - loglik: -4.9436e+02 - logprior: 2.1600
Epoch 6/10
10/10 - 1s - loss: 490.2805 - loglik: -4.9356e+02 - logprior: 3.2795
Epoch 7/10
10/10 - 1s - loss: 490.4719 - loglik: -4.9463e+02 - logprior: 4.1584
Fitted a model with MAP estimate = -489.8708
Time for alignment: 47.8335
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 703.5317 - loglik: -6.3864e+02 - logprior: -6.4887e+01
Epoch 2/10
10/10 - 1s - loss: 617.7556 - loglik: -6.0250e+02 - logprior: -1.5259e+01
Epoch 3/10
10/10 - 1s - loss: 567.1493 - loglik: -5.6054e+02 - logprior: -6.6096e+00
Epoch 4/10
10/10 - 1s - loss: 539.4850 - loglik: -5.3530e+02 - logprior: -4.1889e+00
Epoch 5/10
10/10 - 1s - loss: 529.4813 - loglik: -5.2651e+02 - logprior: -2.9664e+00
Epoch 6/10
10/10 - 1s - loss: 523.9546 - loglik: -5.2167e+02 - logprior: -2.2864e+00
Epoch 7/10
10/10 - 1s - loss: 521.5555 - loglik: -5.1975e+02 - logprior: -1.8039e+00
Epoch 8/10
10/10 - 1s - loss: 520.4976 - loglik: -5.1897e+02 - logprior: -1.5232e+00
Epoch 9/10
10/10 - 1s - loss: 519.2897 - loglik: -5.1801e+02 - logprior: -1.2794e+00
Epoch 10/10
10/10 - 1s - loss: 519.4283 - loglik: -5.1836e+02 - logprior: -1.0683e+00
Fitted a model with MAP estimate = -519.0218
expansions: [(13, 3), (17, 3), (19, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (60, 1), (62, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 578.0524 - loglik: -5.1200e+02 - logprior: -6.6056e+01
Epoch 2/2
10/10 - 1s - loss: 525.8793 - loglik: -5.0069e+02 - logprior: -2.5185e+01
Fitted a model with MAP estimate = -516.6146
expansions: [(0, 2), (69, 1)]
discards: [ 0 20 98]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 546.9833 - loglik: -4.9681e+02 - logprior: -5.0175e+01
Epoch 2/2
10/10 - 1s - loss: 504.3681 - loglik: -4.9345e+02 - logprior: -1.0920e+01
Fitted a model with MAP estimate = -498.6517
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 556.4523 - loglik: -4.9623e+02 - logprior: -6.0224e+01
Epoch 2/10
10/10 - 1s - loss: 511.3729 - loglik: -4.9450e+02 - logprior: -1.6871e+01
Epoch 3/10
10/10 - 1s - loss: 499.2302 - loglik: -4.9523e+02 - logprior: -3.9979e+00
Epoch 4/10
10/10 - 1s - loss: 493.5848 - loglik: -4.9401e+02 - logprior: 0.4207
Epoch 5/10
10/10 - 1s - loss: 491.9901 - loglik: -4.9428e+02 - logprior: 2.2943
Epoch 6/10
10/10 - 1s - loss: 490.8076 - loglik: -4.9421e+02 - logprior: 3.3991
Epoch 7/10
10/10 - 1s - loss: 489.8481 - loglik: -4.9412e+02 - logprior: 4.2714
Epoch 8/10
10/10 - 1s - loss: 489.1858 - loglik: -4.9410e+02 - logprior: 4.9189
Epoch 9/10
10/10 - 1s - loss: 489.3772 - loglik: -4.9474e+02 - logprior: 5.3666
Fitted a model with MAP estimate = -489.0123
Time for alignment: 50.1647
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 703.2915 - loglik: -6.3840e+02 - logprior: -6.4890e+01
Epoch 2/10
10/10 - 1s - loss: 618.5975 - loglik: -6.0335e+02 - logprior: -1.5251e+01
Epoch 3/10
10/10 - 1s - loss: 566.6547 - loglik: -5.6009e+02 - logprior: -6.5649e+00
Epoch 4/10
10/10 - 1s - loss: 539.9664 - loglik: -5.3586e+02 - logprior: -4.1057e+00
Epoch 5/10
10/10 - 1s - loss: 530.7867 - loglik: -5.2794e+02 - logprior: -2.8479e+00
Epoch 6/10
10/10 - 1s - loss: 525.8636 - loglik: -5.2376e+02 - logprior: -2.1014e+00
Epoch 7/10
10/10 - 1s - loss: 523.1854 - loglik: -5.2159e+02 - logprior: -1.5916e+00
Epoch 8/10
10/10 - 1s - loss: 521.7044 - loglik: -5.2040e+02 - logprior: -1.3067e+00
Epoch 9/10
10/10 - 1s - loss: 520.4601 - loglik: -5.1933e+02 - logprior: -1.1320e+00
Epoch 10/10
10/10 - 1s - loss: 521.5334 - loglik: -5.2053e+02 - logprior: -1.0078e+00
Fitted a model with MAP estimate = -520.1060
expansions: [(13, 4), (17, 2), (25, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (58, 1), (62, 1), (78, 4), (81, 1), (86, 1), (87, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 580.2606 - loglik: -5.1417e+02 - logprior: -6.6089e+01
Epoch 2/2
10/10 - 1s - loss: 525.2139 - loglik: -4.9994e+02 - logprior: -2.5278e+01
Fitted a model with MAP estimate = -516.9142
expansions: [(0, 2), (14, 1)]
discards: [ 0 20 21 98]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 548.0338 - loglik: -4.9768e+02 - logprior: -5.0358e+01
Epoch 2/2
10/10 - 1s - loss: 505.8443 - loglik: -4.9465e+02 - logprior: -1.1193e+01
Fitted a model with MAP estimate = -499.5514
expansions: [(69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 557.8994 - loglik: -4.9691e+02 - logprior: -6.0993e+01
Epoch 2/10
10/10 - 1s - loss: 514.2041 - loglik: -4.9579e+02 - logprior: -1.8411e+01
Epoch 3/10
10/10 - 1s - loss: 499.1781 - loglik: -4.9447e+02 - logprior: -4.7118e+00
Epoch 4/10
10/10 - 1s - loss: 494.3149 - loglik: -4.9459e+02 - logprior: 0.2784
Epoch 5/10
10/10 - 1s - loss: 491.9810 - loglik: -4.9420e+02 - logprior: 2.2173
Epoch 6/10
10/10 - 1s - loss: 490.4520 - loglik: -4.9377e+02 - logprior: 3.3177
Epoch 7/10
10/10 - 1s - loss: 490.5400 - loglik: -4.9475e+02 - logprior: 4.2144
Fitted a model with MAP estimate = -489.8929
Time for alignment: 47.5596
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 703.3276 - loglik: -6.3844e+02 - logprior: -6.4887e+01
Epoch 2/10
10/10 - 1s - loss: 618.8893 - loglik: -6.0362e+02 - logprior: -1.5264e+01
Epoch 3/10
10/10 - 1s - loss: 571.3892 - loglik: -5.6471e+02 - logprior: -6.6840e+00
Epoch 4/10
10/10 - 1s - loss: 543.9938 - loglik: -5.3970e+02 - logprior: -4.2890e+00
Epoch 5/10
10/10 - 1s - loss: 531.0023 - loglik: -5.2794e+02 - logprior: -3.0650e+00
Epoch 6/10
10/10 - 1s - loss: 526.6417 - loglik: -5.2411e+02 - logprior: -2.5341e+00
Epoch 7/10
10/10 - 1s - loss: 523.3019 - loglik: -5.2115e+02 - logprior: -2.1496e+00
Epoch 8/10
10/10 - 1s - loss: 520.9162 - loglik: -5.1899e+02 - logprior: -1.9257e+00
Epoch 9/10
10/10 - 1s - loss: 521.0373 - loglik: -5.1926e+02 - logprior: -1.7761e+00
Fitted a model with MAP estimate = -520.2303
expansions: [(11, 1), (13, 2), (16, 1), (17, 1), (18, 3), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (62, 1), (65, 1), (72, 1), (77, 3), (78, 1), (81, 1), (86, 1), (87, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 580.1765 - loglik: -5.1388e+02 - logprior: -6.6301e+01
Epoch 2/2
10/10 - 1s - loss: 524.9739 - loglik: -4.9961e+02 - logprior: -2.5360e+01
Fitted a model with MAP estimate = -515.8720
expansions: [(0, 2), (23, 1)]
discards: [ 0 71 97 98]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 545.6614 - loglik: -4.9538e+02 - logprior: -5.0283e+01
Epoch 2/2
10/10 - 1s - loss: 504.3921 - loglik: -4.9328e+02 - logprior: -1.1114e+01
Fitted a model with MAP estimate = -497.9756
expansions: []
discards: [ 0 48]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 555.9574 - loglik: -4.9534e+02 - logprior: -6.0622e+01
Epoch 2/10
10/10 - 1s - loss: 512.5239 - loglik: -4.9515e+02 - logprior: -1.7379e+01
Epoch 3/10
10/10 - 1s - loss: 497.8350 - loglik: -4.9342e+02 - logprior: -4.4189e+00
Epoch 4/10
10/10 - 1s - loss: 493.7539 - loglik: -4.9384e+02 - logprior: 0.0889
Epoch 5/10
10/10 - 1s - loss: 491.5347 - loglik: -4.9353e+02 - logprior: 1.9966
Epoch 6/10
10/10 - 1s - loss: 490.6175 - loglik: -4.9372e+02 - logprior: 3.1061
Epoch 7/10
10/10 - 1s - loss: 490.1323 - loglik: -4.9412e+02 - logprior: 3.9906
Epoch 8/10
10/10 - 1s - loss: 489.5044 - loglik: -4.9414e+02 - logprior: 4.6391
Epoch 9/10
10/10 - 1s - loss: 488.6273 - loglik: -4.9371e+02 - logprior: 5.0829
Epoch 10/10
10/10 - 1s - loss: 489.5025 - loglik: -4.9493e+02 - logprior: 5.4260
Fitted a model with MAP estimate = -488.6694
Time for alignment: 50.1304
Computed alignments with likelihoods: ['-489.4790', '-489.8708', '-489.0123', '-489.8929', '-488.6694']
Best model has likelihood: -488.6694  (prior= 5.6120 )
time for generating output: 0.2035
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf22.projection.fasta
SP score = 0.9178967915090886
Training of 5 independent models on file flav.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b1403edf0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f528a516e20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526f81c2e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5300b7e4c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f530dd69e80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526f815220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5271386c10>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5940424bb0>, <__main__.SimpleDirichletPrior object at 0x7f52736ce280>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 797.1236 - loglik: -7.8771e+02 - logprior: -9.4131e+00
Epoch 2/10
13/13 - 3s - loss: 753.4974 - loglik: -7.5148e+02 - logprior: -2.0191e+00
Epoch 3/10
13/13 - 3s - loss: 728.6101 - loglik: -7.2704e+02 - logprior: -1.5726e+00
Epoch 4/10
13/13 - 3s - loss: 713.5859 - loglik: -7.1183e+02 - logprior: -1.7555e+00
Epoch 5/10
13/13 - 2s - loss: 708.8118 - loglik: -7.0713e+02 - logprior: -1.6774e+00
Epoch 6/10
13/13 - 2s - loss: 706.7462 - loglik: -7.0511e+02 - logprior: -1.6325e+00
Epoch 7/10
13/13 - 3s - loss: 705.2669 - loglik: -7.0359e+02 - logprior: -1.6790e+00
Epoch 8/10
13/13 - 3s - loss: 704.4728 - loglik: -7.0271e+02 - logprior: -1.7605e+00
Epoch 9/10
13/13 - 3s - loss: 704.6429 - loglik: -7.0284e+02 - logprior: -1.7986e+00
Fitted a model with MAP estimate = -704.5232
expansions: [(12, 1), (15, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (29, 1), (48, 1), (52, 1), (55, 1), (58, 2), (64, 2), (82, 4), (83, 1), (93, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 717.1281 - loglik: -7.0725e+02 - logprior: -9.8829e+00
Epoch 2/2
13/13 - 3s - loss: 701.6833 - loglik: -6.9749e+02 - logprior: -4.1899e+00
Fitted a model with MAP estimate = -697.7339
expansions: [(0, 2), (118, 1)]
discards: [  0  35  80 102]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 700.9347 - loglik: -6.9385e+02 - logprior: -7.0816e+00
Epoch 2/2
13/13 - 3s - loss: 693.6112 - loglik: -6.9190e+02 - logprior: -1.7145e+00
Fitted a model with MAP estimate = -691.6774
expansions: []
discards: [ 0 73]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 703.6266 - loglik: -6.9481e+02 - logprior: -8.8211e+00
Epoch 2/10
13/13 - 3s - loss: 694.3871 - loglik: -6.9201e+02 - logprior: -2.3724e+00
Epoch 3/10
13/13 - 3s - loss: 691.5822 - loglik: -6.9057e+02 - logprior: -1.0170e+00
Epoch 4/10
13/13 - 3s - loss: 690.0659 - loglik: -6.8942e+02 - logprior: -6.4586e-01
Epoch 5/10
13/13 - 3s - loss: 689.2665 - loglik: -6.8875e+02 - logprior: -5.1236e-01
Epoch 6/10
13/13 - 3s - loss: 688.1678 - loglik: -6.8768e+02 - logprior: -4.8492e-01
Epoch 7/10
13/13 - 3s - loss: 688.1691 - loglik: -6.8768e+02 - logprior: -4.9247e-01
Fitted a model with MAP estimate = -687.6969
Time for alignment: 81.4738
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 9s - loss: 795.9244 - loglik: -7.8651e+02 - logprior: -9.4188e+00
Epoch 2/10
13/13 - 3s - loss: 754.2006 - loglik: -7.5218e+02 - logprior: -2.0211e+00
Epoch 3/10
13/13 - 3s - loss: 725.7413 - loglik: -7.2417e+02 - logprior: -1.5751e+00
Epoch 4/10
13/13 - 2s - loss: 710.1352 - loglik: -7.0833e+02 - logprior: -1.8022e+00
Epoch 5/10
13/13 - 3s - loss: 707.3083 - loglik: -7.0558e+02 - logprior: -1.7234e+00
Epoch 6/10
13/13 - 2s - loss: 703.9417 - loglik: -7.0227e+02 - logprior: -1.6677e+00
Epoch 7/10
13/13 - 3s - loss: 703.3651 - loglik: -7.0165e+02 - logprior: -1.7197e+00
Epoch 8/10
13/13 - 3s - loss: 702.1105 - loglik: -7.0031e+02 - logprior: -1.7997e+00
Epoch 9/10
13/13 - 3s - loss: 702.8799 - loglik: -7.0101e+02 - logprior: -1.8677e+00
Fitted a model with MAP estimate = -701.8674
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (33, 1), (51, 1), (52, 1), (55, 1), (58, 3), (64, 2), (80, 1), (81, 1), (82, 2), (83, 1), (93, 1), (100, 2), (101, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 713.9843 - loglik: -7.0405e+02 - logprior: -9.9297e+00
Epoch 2/2
13/13 - 3s - loss: 698.4982 - loglik: -6.9435e+02 - logprior: -4.1507e+00
Fitted a model with MAP estimate = -694.6922
expansions: [(0, 2)]
discards: [  0  36  73  74  81 101 126]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 700.4398 - loglik: -6.9340e+02 - logprior: -7.0436e+00
Epoch 2/2
13/13 - 3s - loss: 692.2898 - loglik: -6.9059e+02 - logprior: -1.6966e+00
Fitted a model with MAP estimate = -690.6941
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 702.5419 - loglik: -6.9374e+02 - logprior: -8.8022e+00
Epoch 2/10
13/13 - 3s - loss: 693.4146 - loglik: -6.9110e+02 - logprior: -2.3171e+00
Epoch 3/10
13/13 - 3s - loss: 691.1208 - loglik: -6.9022e+02 - logprior: -9.0399e-01
Epoch 4/10
13/13 - 3s - loss: 688.0069 - loglik: -6.8753e+02 - logprior: -4.7901e-01
Epoch 5/10
13/13 - 3s - loss: 688.7618 - loglik: -6.8843e+02 - logprior: -3.3565e-01
Fitted a model with MAP estimate = -687.6425
Time for alignment: 79.9184
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 797.6048 - loglik: -7.8820e+02 - logprior: -9.4085e+00
Epoch 2/10
13/13 - 2s - loss: 753.0264 - loglik: -7.5102e+02 - logprior: -2.0048e+00
Epoch 3/10
13/13 - 3s - loss: 724.6277 - loglik: -7.2308e+02 - logprior: -1.5434e+00
Epoch 4/10
13/13 - 2s - loss: 712.1065 - loglik: -7.1035e+02 - logprior: -1.7577e+00
Epoch 5/10
13/13 - 2s - loss: 706.7025 - loglik: -7.0501e+02 - logprior: -1.6935e+00
Epoch 6/10
13/13 - 3s - loss: 704.5795 - loglik: -7.0293e+02 - logprior: -1.6489e+00
Epoch 7/10
13/13 - 3s - loss: 702.8368 - loglik: -7.0115e+02 - logprior: -1.6856e+00
Epoch 8/10
13/13 - 3s - loss: 702.1624 - loglik: -7.0041e+02 - logprior: -1.7505e+00
Epoch 9/10
13/13 - 3s - loss: 702.6986 - loglik: -7.0091e+02 - logprior: -1.7908e+00
Fitted a model with MAP estimate = -702.1719
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (33, 1), (51, 1), (52, 1), (55, 1), (58, 3), (64, 2), (82, 3), (85, 1), (99, 1), (100, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 714.0131 - loglik: -7.0414e+02 - logprior: -9.8713e+00
Epoch 2/2
13/13 - 3s - loss: 699.0173 - loglik: -6.9494e+02 - logprior: -4.0790e+00
Fitted a model with MAP estimate = -695.3899
expansions: [(0, 2)]
discards: [  0  36  73  74  81 102]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 700.6757 - loglik: -6.9365e+02 - logprior: -7.0304e+00
Epoch 2/2
13/13 - 3s - loss: 691.9538 - loglik: -6.9030e+02 - logprior: -1.6581e+00
Fitted a model with MAP estimate = -690.7425
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 701.9899 - loglik: -6.9324e+02 - logprior: -8.7543e+00
Epoch 2/10
13/13 - 3s - loss: 694.0253 - loglik: -6.9176e+02 - logprior: -2.2641e+00
Epoch 3/10
13/13 - 3s - loss: 691.1334 - loglik: -6.9026e+02 - logprior: -8.7298e-01
Epoch 4/10
13/13 - 3s - loss: 689.3747 - loglik: -6.8892e+02 - logprior: -4.5686e-01
Epoch 5/10
13/13 - 3s - loss: 688.5465 - loglik: -6.8824e+02 - logprior: -3.1024e-01
Epoch 6/10
13/13 - 3s - loss: 687.9567 - loglik: -6.8769e+02 - logprior: -2.6777e-01
Epoch 7/10
13/13 - 3s - loss: 687.2070 - loglik: -6.8695e+02 - logprior: -2.5654e-01
Epoch 8/10
13/13 - 3s - loss: 687.5029 - loglik: -6.8723e+02 - logprior: -2.7652e-01
Fitted a model with MAP estimate = -687.0649
Time for alignment: 87.2330
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 797.6842 - loglik: -7.8827e+02 - logprior: -9.4100e+00
Epoch 2/10
13/13 - 3s - loss: 752.7989 - loglik: -7.5078e+02 - logprior: -2.0141e+00
Epoch 3/10
13/13 - 3s - loss: 727.7830 - loglik: -7.2624e+02 - logprior: -1.5459e+00
Epoch 4/10
13/13 - 3s - loss: 710.9643 - loglik: -7.0918e+02 - logprior: -1.7827e+00
Epoch 5/10
13/13 - 3s - loss: 706.6946 - loglik: -7.0497e+02 - logprior: -1.7234e+00
Epoch 6/10
13/13 - 2s - loss: 703.9091 - loglik: -7.0221e+02 - logprior: -1.6981e+00
Epoch 7/10
13/13 - 3s - loss: 703.1478 - loglik: -7.0142e+02 - logprior: -1.7326e+00
Epoch 8/10
13/13 - 2s - loss: 702.4544 - loglik: -7.0067e+02 - logprior: -1.7855e+00
Epoch 9/10
13/13 - 3s - loss: 702.3363 - loglik: -7.0058e+02 - logprior: -1.7541e+00
Epoch 10/10
13/13 - 3s - loss: 702.0489 - loglik: -7.0024e+02 - logprior: -1.8078e+00
Fitted a model with MAP estimate = -702.0083
expansions: [(12, 1), (15, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 2), (33, 1), (51, 1), (52, 1), (55, 1), (58, 3), (80, 1), (81, 1), (82, 1), (92, 8)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 714.6917 - loglik: -7.0481e+02 - logprior: -9.8795e+00
Epoch 2/2
13/13 - 3s - loss: 699.5085 - loglik: -6.9536e+02 - logprior: -4.1505e+00
Fitted a model with MAP estimate = -695.6222
expansions: [(0, 2), (127, 1)]
discards: [  0  35  73  74 117 118]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 9s - loss: 700.2054 - loglik: -6.9313e+02 - logprior: -7.0709e+00
Epoch 2/2
13/13 - 3s - loss: 691.2471 - loglik: -6.8953e+02 - logprior: -1.7134e+00
Fitted a model with MAP estimate = -689.8227
expansions: [(124, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 701.8012 - loglik: -6.9282e+02 - logprior: -8.9828e+00
Epoch 2/10
13/13 - 3s - loss: 692.4414 - loglik: -6.8993e+02 - logprior: -2.5133e+00
Epoch 3/10
13/13 - 3s - loss: 690.4485 - loglik: -6.8948e+02 - logprior: -9.6952e-01
Epoch 4/10
13/13 - 3s - loss: 687.8417 - loglik: -6.8727e+02 - logprior: -5.7095e-01
Epoch 5/10
13/13 - 3s - loss: 686.0919 - loglik: -6.8566e+02 - logprior: -4.2810e-01
Epoch 6/10
13/13 - 3s - loss: 686.4553 - loglik: -6.8607e+02 - logprior: -3.8856e-01
Fitted a model with MAP estimate = -685.9236
Time for alignment: 84.8196
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 796.7228 - loglik: -7.8731e+02 - logprior: -9.4098e+00
Epoch 2/10
13/13 - 2s - loss: 752.4957 - loglik: -7.5048e+02 - logprior: -2.0173e+00
Epoch 3/10
13/13 - 3s - loss: 724.0313 - loglik: -7.2249e+02 - logprior: -1.5427e+00
Epoch 4/10
13/13 - 3s - loss: 711.7044 - loglik: -7.1001e+02 - logprior: -1.6897e+00
Epoch 5/10
13/13 - 3s - loss: 707.2126 - loglik: -7.0561e+02 - logprior: -1.6041e+00
Epoch 6/10
13/13 - 2s - loss: 704.3026 - loglik: -7.0273e+02 - logprior: -1.5722e+00
Epoch 7/10
13/13 - 3s - loss: 704.4088 - loglik: -7.0278e+02 - logprior: -1.6287e+00
Fitted a model with MAP estimate = -703.5167
expansions: [(6, 1), (19, 1), (20, 1), (21, 2), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (33, 1), (51, 1), (52, 1), (55, 1), (59, 3), (64, 2), (82, 4), (83, 2), (100, 2), (101, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 715.5689 - loglik: -7.0571e+02 - logprior: -9.8630e+00
Epoch 2/2
13/13 - 3s - loss: 698.0658 - loglik: -6.9390e+02 - logprior: -4.1621e+00
Fitted a model with MAP estimate = -695.0784
expansions: [(0, 2)]
discards: [  0  23  37  75  82 104]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 698.9273 - loglik: -6.9187e+02 - logprior: -7.0614e+00
Epoch 2/2
13/13 - 3s - loss: 691.0918 - loglik: -6.8939e+02 - logprior: -1.7005e+00
Fitted a model with MAP estimate = -689.6304
expansions: []
discards: [  0 125]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 702.2708 - loglik: -6.9345e+02 - logprior: -8.8185e+00
Epoch 2/10
13/13 - 3s - loss: 692.4741 - loglik: -6.9011e+02 - logprior: -2.3608e+00
Epoch 3/10
13/13 - 3s - loss: 689.5781 - loglik: -6.8866e+02 - logprior: -9.1841e-01
Epoch 4/10
13/13 - 3s - loss: 688.7313 - loglik: -6.8822e+02 - logprior: -5.0695e-01
Epoch 5/10
13/13 - 3s - loss: 686.7921 - loglik: -6.8643e+02 - logprior: -3.6516e-01
Epoch 6/10
13/13 - 3s - loss: 686.8098 - loglik: -6.8648e+02 - logprior: -3.3089e-01
Fitted a model with MAP estimate = -686.3807
Time for alignment: 76.7268
Computed alignments with likelihoods: ['-687.6969', '-687.6425', '-687.0649', '-685.9236', '-686.3807']
Best model has likelihood: -685.9236  (prior= -0.3946 )
time for generating output: 0.2279
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/flav.projection.fasta
SP score = 0.8255451713395638
Training of 5 independent models on file sodfe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f526f29cf70>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52736d7c70>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b13da3df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5274a4dbb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f94757c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f1bb1850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531492a9a0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f598c0c0160>, <__main__.SimpleDirichletPrior object at 0x7f528981d910>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 451.7656 - loglik: -4.4233e+02 - logprior: -9.4321e+00
Epoch 2/10
13/13 - 2s - loss: 402.3005 - loglik: -3.9993e+02 - logprior: -2.3741e+00
Epoch 3/10
13/13 - 2s - loss: 373.1999 - loglik: -3.7140e+02 - logprior: -1.7963e+00
Epoch 4/10
13/13 - 2s - loss: 365.1185 - loglik: -3.6349e+02 - logprior: -1.6259e+00
Epoch 5/10
13/13 - 2s - loss: 362.5212 - loglik: -3.6094e+02 - logprior: -1.5857e+00
Epoch 6/10
13/13 - 2s - loss: 360.8728 - loglik: -3.5934e+02 - logprior: -1.5322e+00
Epoch 7/10
13/13 - 2s - loss: 361.1209 - loglik: -3.5960e+02 - logprior: -1.5184e+00
Fitted a model with MAP estimate = -360.7065
expansions: [(0, 4), (13, 1), (16, 1), (34, 1), (35, 2), (36, 2), (37, 1), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 366.9194 - loglik: -3.5654e+02 - logprior: -1.0379e+01
Epoch 2/2
13/13 - 2s - loss: 349.8603 - loglik: -3.4675e+02 - logprior: -3.1112e+00
Fitted a model with MAP estimate = -347.0339
expansions: [(0, 2)]
discards: [46 50]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 355.3053 - loglik: -3.4517e+02 - logprior: -1.0136e+01
Epoch 2/2
13/13 - 2s - loss: 346.0645 - loglik: -3.4281e+02 - logprior: -3.2536e+00
Fitted a model with MAP estimate = -344.2602
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 350.4049 - loglik: -3.4247e+02 - logprior: -7.9362e+00
Epoch 2/10
13/13 - 2s - loss: 344.2186 - loglik: -3.4197e+02 - logprior: -2.2438e+00
Epoch 3/10
13/13 - 2s - loss: 343.4594 - loglik: -3.4177e+02 - logprior: -1.6892e+00
Epoch 4/10
13/13 - 2s - loss: 342.3329 - loglik: -3.4098e+02 - logprior: -1.3481e+00
Epoch 5/10
13/13 - 2s - loss: 342.2950 - loglik: -3.4102e+02 - logprior: -1.2744e+00
Epoch 6/10
13/13 - 2s - loss: 342.1491 - loglik: -3.4092e+02 - logprior: -1.2290e+00
Epoch 7/10
13/13 - 2s - loss: 342.2682 - loglik: -3.4104e+02 - logprior: -1.2272e+00
Fitted a model with MAP estimate = -341.9664
Time for alignment: 55.8981
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 451.5864 - loglik: -4.4215e+02 - logprior: -9.4381e+00
Epoch 2/10
13/13 - 2s - loss: 400.0335 - loglik: -3.9765e+02 - logprior: -2.3842e+00
Epoch 3/10
13/13 - 2s - loss: 371.2220 - loglik: -3.6943e+02 - logprior: -1.7928e+00
Epoch 4/10
13/13 - 2s - loss: 365.7254 - loglik: -3.6416e+02 - logprior: -1.5608e+00
Epoch 5/10
13/13 - 2s - loss: 362.8686 - loglik: -3.6140e+02 - logprior: -1.4712e+00
Epoch 6/10
13/13 - 2s - loss: 361.2794 - loglik: -3.5987e+02 - logprior: -1.4143e+00
Epoch 7/10
13/13 - 2s - loss: 360.6284 - loglik: -3.5921e+02 - logprior: -1.4148e+00
Epoch 8/10
13/13 - 2s - loss: 361.6168 - loglik: -3.6021e+02 - logprior: -1.4089e+00
Fitted a model with MAP estimate = -360.7336
expansions: [(0, 4), (13, 1), (35, 1), (36, 3), (37, 2), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 9s - loss: 366.1592 - loglik: -3.5579e+02 - logprior: -1.0374e+01
Epoch 2/2
13/13 - 2s - loss: 350.3149 - loglik: -3.4721e+02 - logprior: -3.1023e+00
Fitted a model with MAP estimate = -347.2922
expansions: [(0, 2)]
discards: [47]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 355.7949 - loglik: -3.4568e+02 - logprior: -1.0116e+01
Epoch 2/2
13/13 - 2s - loss: 346.5154 - loglik: -3.4324e+02 - logprior: -3.2733e+00
Fitted a model with MAP estimate = -344.9179
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 351.2711 - loglik: -3.4332e+02 - logprior: -7.9537e+00
Epoch 2/10
13/13 - 2s - loss: 344.7152 - loglik: -3.4246e+02 - logprior: -2.2508e+00
Epoch 3/10
13/13 - 2s - loss: 343.6081 - loglik: -3.4190e+02 - logprior: -1.7056e+00
Epoch 4/10
13/13 - 2s - loss: 343.4498 - loglik: -3.4208e+02 - logprior: -1.3732e+00
Epoch 5/10
13/13 - 2s - loss: 342.8122 - loglik: -3.4153e+02 - logprior: -1.2799e+00
Epoch 6/10
13/13 - 2s - loss: 342.9942 - loglik: -3.4174e+02 - logprior: -1.2582e+00
Fitted a model with MAP estimate = -342.6757
Time for alignment: 59.7812
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 451.4938 - loglik: -4.4206e+02 - logprior: -9.4305e+00
Epoch 2/10
13/13 - 2s - loss: 402.9180 - loglik: -4.0053e+02 - logprior: -2.3861e+00
Epoch 3/10
13/13 - 2s - loss: 374.2070 - loglik: -3.7237e+02 - logprior: -1.8350e+00
Epoch 4/10
13/13 - 2s - loss: 365.0162 - loglik: -3.6335e+02 - logprior: -1.6618e+00
Epoch 5/10
13/13 - 2s - loss: 362.8753 - loglik: -3.6130e+02 - logprior: -1.5716e+00
Epoch 6/10
13/13 - 2s - loss: 360.7176 - loglik: -3.5920e+02 - logprior: -1.5137e+00
Epoch 7/10
13/13 - 2s - loss: 360.9398 - loglik: -3.5944e+02 - logprior: -1.5042e+00
Fitted a model with MAP estimate = -360.6005
expansions: [(0, 4), (13, 1), (32, 1), (34, 1), (35, 2), (36, 1), (37, 1), (38, 2), (43, 3), (44, 2), (45, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 366.0903 - loglik: -3.5566e+02 - logprior: -1.0427e+01
Epoch 2/2
13/13 - 2s - loss: 347.9186 - loglik: -3.4472e+02 - logprior: -3.1965e+00
Fitted a model with MAP estimate = -344.6874
expansions: [(0, 2)]
discards: [49 61 62 63 64]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 356.8777 - loglik: -3.4675e+02 - logprior: -1.0131e+01
Epoch 2/2
13/13 - 2s - loss: 346.2051 - loglik: -3.4293e+02 - logprior: -3.2788e+00
Fitted a model with MAP estimate = -345.0151
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 351.0248 - loglik: -3.4307e+02 - logprior: -7.9505e+00
Epoch 2/10
13/13 - 2s - loss: 345.0348 - loglik: -3.4278e+02 - logprior: -2.2570e+00
Epoch 3/10
13/13 - 2s - loss: 343.4088 - loglik: -3.4170e+02 - logprior: -1.7093e+00
Epoch 4/10
13/13 - 2s - loss: 343.5591 - loglik: -3.4220e+02 - logprior: -1.3593e+00
Fitted a model with MAP estimate = -343.0509
Time for alignment: 52.6439
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 452.4660 - loglik: -4.4303e+02 - logprior: -9.4324e+00
Epoch 2/10
13/13 - 2s - loss: 399.1650 - loglik: -3.9679e+02 - logprior: -2.3764e+00
Epoch 3/10
13/13 - 2s - loss: 371.8844 - loglik: -3.7011e+02 - logprior: -1.7776e+00
Epoch 4/10
13/13 - 2s - loss: 365.5266 - loglik: -3.6395e+02 - logprior: -1.5785e+00
Epoch 5/10
13/13 - 2s - loss: 362.3960 - loglik: -3.6095e+02 - logprior: -1.4471e+00
Epoch 6/10
13/13 - 2s - loss: 362.8377 - loglik: -3.6144e+02 - logprior: -1.4025e+00
Fitted a model with MAP estimate = -361.4186
expansions: [(0, 4), (13, 1), (35, 1), (36, 3), (37, 2), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 366.1081 - loglik: -3.5586e+02 - logprior: -1.0251e+01
Epoch 2/2
13/13 - 2s - loss: 350.8250 - loglik: -3.4779e+02 - logprior: -3.0352e+00
Fitted a model with MAP estimate = -347.5735
expansions: [(0, 2)]
discards: [49]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 355.6421 - loglik: -3.4552e+02 - logprior: -1.0125e+01
Epoch 2/2
13/13 - 2s - loss: 346.3883 - loglik: -3.4315e+02 - logprior: -3.2338e+00
Fitted a model with MAP estimate = -344.5788
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 350.7865 - loglik: -3.4285e+02 - logprior: -7.9414e+00
Epoch 2/10
13/13 - 2s - loss: 344.3939 - loglik: -3.4214e+02 - logprior: -2.2517e+00
Epoch 3/10
13/13 - 2s - loss: 343.1854 - loglik: -3.4148e+02 - logprior: -1.7018e+00
Epoch 4/10
13/13 - 2s - loss: 342.9487 - loglik: -3.4160e+02 - logprior: -1.3491e+00
Epoch 5/10
13/13 - 2s - loss: 342.9156 - loglik: -3.4163e+02 - logprior: -1.2825e+00
Epoch 6/10
13/13 - 2s - loss: 341.9589 - loglik: -3.4072e+02 - logprior: -1.2432e+00
Epoch 7/10
13/13 - 2s - loss: 341.9612 - loglik: -3.4073e+02 - logprior: -1.2307e+00
Fitted a model with MAP estimate = -342.1290
Time for alignment: 53.2724
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 451.9145 - loglik: -4.4248e+02 - logprior: -9.4345e+00
Epoch 2/10
13/13 - 2s - loss: 400.5454 - loglik: -3.9816e+02 - logprior: -2.3860e+00
Epoch 3/10
13/13 - 2s - loss: 371.4123 - loglik: -3.6959e+02 - logprior: -1.8189e+00
Epoch 4/10
13/13 - 2s - loss: 364.8181 - loglik: -3.6321e+02 - logprior: -1.6077e+00
Epoch 5/10
13/13 - 2s - loss: 362.6299 - loglik: -3.6110e+02 - logprior: -1.5303e+00
Epoch 6/10
13/13 - 2s - loss: 361.7339 - loglik: -3.6028e+02 - logprior: -1.4495e+00
Epoch 7/10
13/13 - 2s - loss: 360.8661 - loglik: -3.5942e+02 - logprior: -1.4415e+00
Epoch 8/10
13/13 - 2s - loss: 361.3310 - loglik: -3.5989e+02 - logprior: -1.4394e+00
Fitted a model with MAP estimate = -360.9638
expansions: [(0, 4), (13, 1), (34, 2), (35, 3), (36, 1), (37, 1), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 366.9088 - loglik: -3.5649e+02 - logprior: -1.0417e+01
Epoch 2/2
13/13 - 2s - loss: 350.1170 - loglik: -3.4695e+02 - logprior: -3.1706e+00
Fitted a model with MAP estimate = -346.9837
expansions: [(0, 2)]
discards: [40 50]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 355.0365 - loglik: -3.4491e+02 - logprior: -1.0123e+01
Epoch 2/2
13/13 - 2s - loss: 346.3391 - loglik: -3.4305e+02 - logprior: -3.2874e+00
Fitted a model with MAP estimate = -344.4606
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 351.0082 - loglik: -3.4306e+02 - logprior: -7.9497e+00
Epoch 2/10
13/13 - 2s - loss: 344.0961 - loglik: -3.4186e+02 - logprior: -2.2343e+00
Epoch 3/10
13/13 - 2s - loss: 342.8065 - loglik: -3.4111e+02 - logprior: -1.6971e+00
Epoch 4/10
13/13 - 2s - loss: 343.1193 - loglik: -3.4177e+02 - logprior: -1.3517e+00
Fitted a model with MAP estimate = -342.5427
Time for alignment: 53.5016
Computed alignments with likelihoods: ['-341.9664', '-342.6757', '-343.0509', '-342.1290', '-342.5427']
Best model has likelihood: -341.9664  (prior= -1.2089 )
time for generating output: 0.2633
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodfe.projection.fasta
SP score = 0.5086563793752352
Training of 5 independent models on file zf-CCHH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5900bd5f40>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5afec6a7f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531cafcc40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5272dab4c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52d8cd3eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52973a2580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f528a1bffa0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5344298bb0>, <__main__.SimpleDirichletPrior object at 0x7f5301d82670>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 133.1095 - loglik: -1.3227e+02 - logprior: -8.4351e-01
Epoch 2/10
41/41 - 2s - loss: 119.7597 - loglik: -1.1891e+02 - logprior: -8.5418e-01
Epoch 3/10
41/41 - 2s - loss: 119.0362 - loglik: -1.1820e+02 - logprior: -8.3641e-01
Epoch 4/10
41/41 - 2s - loss: 118.9956 - loglik: -1.1816e+02 - logprior: -8.3147e-01
Epoch 5/10
41/41 - 2s - loss: 118.6533 - loglik: -1.1782e+02 - logprior: -8.3025e-01
Epoch 6/10
41/41 - 2s - loss: 118.6326 - loglik: -1.1780e+02 - logprior: -8.3475e-01
Epoch 7/10
41/41 - 2s - loss: 118.3408 - loglik: -1.1750e+02 - logprior: -8.3755e-01
Epoch 8/10
41/41 - 2s - loss: 118.3442 - loglik: -1.1750e+02 - logprior: -8.4480e-01
Fitted a model with MAP estimate = -117.1591
expansions: [(4, 1), (9, 2), (10, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 8s - loss: 118.6054 - loglik: -1.1756e+02 - logprior: -1.0407e+00
Epoch 2/2
41/41 - 2s - loss: 117.0321 - loglik: -1.1624e+02 - logprior: -7.9533e-01
Fitted a model with MAP estimate = -115.1491
expansions: []
discards: [11]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 117.6012 - loglik: -1.1658e+02 - logprior: -1.0171e+00
Epoch 2/2
41/41 - 2s - loss: 117.0049 - loglik: -1.1622e+02 - logprior: -7.8011e-01
Fitted a model with MAP estimate = -115.1771
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 6s - loss: 115.0018 - loglik: -1.1437e+02 - logprior: -6.3363e-01
Epoch 2/10
58/58 - 2s - loss: 114.5756 - loglik: -1.1404e+02 - logprior: -5.3130e-01
Epoch 3/10
58/58 - 2s - loss: 114.5835 - loglik: -1.1406e+02 - logprior: -5.2764e-01
Fitted a model with MAP estimate = -114.3203
Time for alignment: 75.0124
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 5s - loss: 132.7998 - loglik: -1.3195e+02 - logprior: -8.5243e-01
Epoch 2/10
41/41 - 2s - loss: 119.6746 - loglik: -1.1882e+02 - logprior: -8.5341e-01
Epoch 3/10
41/41 - 2s - loss: 119.0319 - loglik: -1.1820e+02 - logprior: -8.3601e-01
Epoch 4/10
41/41 - 2s - loss: 118.8553 - loglik: -1.1803e+02 - logprior: -8.3016e-01
Epoch 5/10
41/41 - 2s - loss: 118.8052 - loglik: -1.1797e+02 - logprior: -8.3285e-01
Epoch 6/10
41/41 - 2s - loss: 118.5503 - loglik: -1.1772e+02 - logprior: -8.3264e-01
Epoch 7/10
41/41 - 2s - loss: 118.3945 - loglik: -1.1756e+02 - logprior: -8.3797e-01
Epoch 8/10
41/41 - 2s - loss: 118.2693 - loglik: -1.1742e+02 - logprior: -8.4561e-01
Epoch 9/10
41/41 - 2s - loss: 118.2803 - loglik: -1.1743e+02 - logprior: -8.4786e-01
Fitted a model with MAP estimate = -117.2793
expansions: [(4, 1), (9, 1), (10, 2), (11, 2), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 118.9329 - loglik: -1.1788e+02 - logprior: -1.0507e+00
Epoch 2/2
41/41 - 2s - loss: 116.9936 - loglik: -1.1619e+02 - logprior: -8.0563e-01
Fitted a model with MAP estimate = -115.1860
expansions: []
discards: [13 15]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 7s - loss: 117.7717 - loglik: -1.1676e+02 - logprior: -1.0144e+00
Epoch 2/2
41/41 - 2s - loss: 116.8707 - loglik: -1.1609e+02 - logprior: -7.8238e-01
Fitted a model with MAP estimate = -115.1710
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 6s - loss: 115.0696 - loglik: -1.1443e+02 - logprior: -6.3520e-01
Epoch 2/10
58/58 - 2s - loss: 114.5441 - loglik: -1.1401e+02 - logprior: -5.3123e-01
Epoch 3/10
58/58 - 2s - loss: 114.5446 - loglik: -1.1402e+02 - logprior: -5.2624e-01
Fitted a model with MAP estimate = -114.3110
Time for alignment: 76.3370
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 5s - loss: 133.0790 - loglik: -1.3223e+02 - logprior: -8.4820e-01
Epoch 2/10
41/41 - 2s - loss: 119.6568 - loglik: -1.1880e+02 - logprior: -8.5299e-01
Epoch 3/10
41/41 - 2s - loss: 119.0886 - loglik: -1.1825e+02 - logprior: -8.3800e-01
Epoch 4/10
41/41 - 2s - loss: 118.8733 - loglik: -1.1804e+02 - logprior: -8.3058e-01
Epoch 5/10
41/41 - 2s - loss: 118.7204 - loglik: -1.1789e+02 - logprior: -8.3337e-01
Epoch 6/10
41/41 - 2s - loss: 118.5433 - loglik: -1.1771e+02 - logprior: -8.3174e-01
Epoch 7/10
41/41 - 2s - loss: 118.4286 - loglik: -1.1759e+02 - logprior: -8.3968e-01
Epoch 8/10
41/41 - 2s - loss: 118.4161 - loglik: -1.1757e+02 - logprior: -8.4116e-01
Epoch 9/10
41/41 - 2s - loss: 118.1388 - loglik: -1.1729e+02 - logprior: -8.5061e-01
Epoch 10/10
41/41 - 2s - loss: 118.1189 - loglik: -1.1727e+02 - logprior: -8.5193e-01
Fitted a model with MAP estimate = -117.3085
expansions: [(4, 1), (9, 2), (10, 2), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 118.9925 - loglik: -1.1794e+02 - logprior: -1.0495e+00
Epoch 2/2
41/41 - 2s - loss: 116.8831 - loglik: -1.1608e+02 - logprior: -8.0695e-01
Fitted a model with MAP estimate = -115.1730
expansions: []
discards: [11 13]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 117.5773 - loglik: -1.1656e+02 - logprior: -1.0147e+00
Epoch 2/2
41/41 - 2s - loss: 117.0180 - loglik: -1.1624e+02 - logprior: -7.8170e-01
Fitted a model with MAP estimate = -115.1923
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 8s - loss: 114.9744 - loglik: -1.1434e+02 - logprior: -6.3587e-01
Epoch 2/10
58/58 - 2s - loss: 114.6864 - loglik: -1.1416e+02 - logprior: -5.3119e-01
Epoch 3/10
58/58 - 2s - loss: 114.5000 - loglik: -1.1397e+02 - logprior: -5.2618e-01
Epoch 4/10
58/58 - 2s - loss: 114.2695 - loglik: -1.1374e+02 - logprior: -5.2613e-01
Epoch 5/10
58/58 - 2s - loss: 114.2608 - loglik: -1.1374e+02 - logprior: -5.2505e-01
Epoch 6/10
58/58 - 2s - loss: 114.2269 - loglik: -1.1370e+02 - logprior: -5.2221e-01
Epoch 7/10
58/58 - 2s - loss: 113.8028 - loglik: -1.1327e+02 - logprior: -5.3343e-01
Epoch 8/10
58/58 - 2s - loss: 113.8069 - loglik: -1.1328e+02 - logprior: -5.3001e-01
Fitted a model with MAP estimate = -113.7361
Time for alignment: 87.6172
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 5s - loss: 132.7827 - loglik: -1.3192e+02 - logprior: -8.5989e-01
Epoch 2/10
41/41 - 2s - loss: 119.7694 - loglik: -1.1892e+02 - logprior: -8.4978e-01
Epoch 3/10
41/41 - 2s - loss: 119.0741 - loglik: -1.1824e+02 - logprior: -8.3615e-01
Epoch 4/10
41/41 - 2s - loss: 118.8258 - loglik: -1.1800e+02 - logprior: -8.2947e-01
Epoch 5/10
41/41 - 2s - loss: 118.8080 - loglik: -1.1797e+02 - logprior: -8.3334e-01
Epoch 6/10
41/41 - 2s - loss: 118.6112 - loglik: -1.1778e+02 - logprior: -8.3277e-01
Epoch 7/10
41/41 - 2s - loss: 118.3221 - loglik: -1.1748e+02 - logprior: -8.3835e-01
Epoch 8/10
41/41 - 2s - loss: 118.3278 - loglik: -1.1748e+02 - logprior: -8.4369e-01
Fitted a model with MAP estimate = -117.1673
expansions: [(4, 1), (9, 2), (10, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 118.5847 - loglik: -1.1754e+02 - logprior: -1.0402e+00
Epoch 2/2
41/41 - 2s - loss: 117.0723 - loglik: -1.1628e+02 - logprior: -7.9710e-01
Fitted a model with MAP estimate = -115.1179
expansions: []
discards: [11]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 117.5372 - loglik: -1.1652e+02 - logprior: -1.0160e+00
Epoch 2/2
41/41 - 2s - loss: 117.0039 - loglik: -1.1622e+02 - logprior: -7.8155e-01
Fitted a model with MAP estimate = -115.1674
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 115.0783 - loglik: -1.1444e+02 - logprior: -6.3438e-01
Epoch 2/10
58/58 - 2s - loss: 114.5983 - loglik: -1.1407e+02 - logprior: -5.3219e-01
Epoch 3/10
58/58 - 2s - loss: 114.4803 - loglik: -1.1395e+02 - logprior: -5.2683e-01
Epoch 4/10
58/58 - 2s - loss: 114.2780 - loglik: -1.1375e+02 - logprior: -5.2800e-01
Epoch 5/10
58/58 - 2s - loss: 114.1918 - loglik: -1.1367e+02 - logprior: -5.2331e-01
Epoch 6/10
58/58 - 2s - loss: 114.3366 - loglik: -1.1381e+02 - logprior: -5.2384e-01
Fitted a model with MAP estimate = -113.8998
Time for alignment: 77.4812
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 132.9252 - loglik: -1.3207e+02 - logprior: -8.5130e-01
Epoch 2/10
41/41 - 2s - loss: 119.5941 - loglik: -1.1874e+02 - logprior: -8.5290e-01
Epoch 3/10
41/41 - 2s - loss: 119.0963 - loglik: -1.1826e+02 - logprior: -8.3650e-01
Epoch 4/10
41/41 - 2s - loss: 118.9074 - loglik: -1.1808e+02 - logprior: -8.3117e-01
Epoch 5/10
41/41 - 2s - loss: 118.7767 - loglik: -1.1794e+02 - logprior: -8.3231e-01
Epoch 6/10
41/41 - 2s - loss: 118.5483 - loglik: -1.1771e+02 - logprior: -8.3350e-01
Epoch 7/10
41/41 - 2s - loss: 118.4196 - loglik: -1.1758e+02 - logprior: -8.3853e-01
Epoch 8/10
41/41 - 2s - loss: 118.3696 - loglik: -1.1753e+02 - logprior: -8.4186e-01
Epoch 9/10
41/41 - 2s - loss: 118.0163 - loglik: -1.1717e+02 - logprior: -8.4895e-01
Epoch 10/10
41/41 - 2s - loss: 118.1916 - loglik: -1.1734e+02 - logprior: -8.5205e-01
Fitted a model with MAP estimate = -117.3181
expansions: [(4, 1), (9, 2), (10, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 118.6547 - loglik: -1.1761e+02 - logprior: -1.0436e+00
Epoch 2/2
41/41 - 2s - loss: 117.0214 - loglik: -1.1623e+02 - logprior: -7.9590e-01
Fitted a model with MAP estimate = -115.1502
expansions: []
discards: [11]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 117.5717 - loglik: -1.1656e+02 - logprior: -1.0149e+00
Epoch 2/2
41/41 - 2s - loss: 116.9965 - loglik: -1.1622e+02 - logprior: -7.8107e-01
Fitted a model with MAP estimate = -115.1663
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 114.9959 - loglik: -1.1436e+02 - logprior: -6.3548e-01
Epoch 2/10
58/58 - 2s - loss: 114.5943 - loglik: -1.1406e+02 - logprior: -5.3080e-01
Epoch 3/10
58/58 - 2s - loss: 114.5781 - loglik: -1.1405e+02 - logprior: -5.2722e-01
Epoch 4/10
58/58 - 2s - loss: 114.3322 - loglik: -1.1380e+02 - logprior: -5.2740e-01
Epoch 5/10
58/58 - 2s - loss: 114.2139 - loglik: -1.1369e+02 - logprior: -5.2576e-01
Epoch 6/10
58/58 - 2s - loss: 114.2242 - loglik: -1.1370e+02 - logprior: -5.2272e-01
Fitted a model with MAP estimate = -113.9160
Time for alignment: 82.8491
Computed alignments with likelihoods: ['-114.3203', '-114.3110', '-113.7361', '-113.8998', '-113.9160']
Best model has likelihood: -113.7361  (prior= -0.5233 )
time for generating output: 0.1007
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/zf-CCHH.projection.fasta
SP score = 0.966464502318944
Training of 5 independent models on file scorptoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f527f18e6d0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5289c79190>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5aff5c8d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52d9d32c40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52747e3dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52721a8430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f527f1a4a00>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5267f90fa0>, <__main__.SimpleDirichletPrior object at 0x7f527e94c4f0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 456.1231 - loglik: -3.2385e+02 - logprior: -1.3227e+02
Epoch 2/10
10/10 - 1s - loss: 337.3698 - loglik: -3.0181e+02 - logprior: -3.5561e+01
Epoch 3/10
10/10 - 1s - loss: 301.3564 - loglik: -2.8470e+02 - logprior: -1.6656e+01
Epoch 4/10
10/10 - 1s - loss: 285.0734 - loglik: -2.7511e+02 - logprior: -9.9673e+00
Epoch 5/10
10/10 - 1s - loss: 277.0597 - loglik: -2.7070e+02 - logprior: -6.3594e+00
Epoch 6/10
10/10 - 1s - loss: 272.5774 - loglik: -2.6820e+02 - logprior: -4.3743e+00
Epoch 7/10
10/10 - 1s - loss: 270.3705 - loglik: -2.6727e+02 - logprior: -3.1006e+00
Epoch 8/10
10/10 - 1s - loss: 269.2925 - loglik: -2.6707e+02 - logprior: -2.2226e+00
Epoch 9/10
10/10 - 1s - loss: 268.5431 - loglik: -2.6697e+02 - logprior: -1.5773e+00
Epoch 10/10
10/10 - 1s - loss: 268.0219 - loglik: -2.6683e+02 - logprior: -1.1890e+00
Fitted a model with MAP estimate = -267.8122
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (19, 1), (22, 2), (31, 1), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 407.9927 - loglik: -2.6647e+02 - logprior: -1.4152e+02
Epoch 2/2
10/10 - 1s - loss: 318.8209 - loglik: -2.6086e+02 - logprior: -5.7963e+01
Fitted a model with MAP estimate = -304.2072
expansions: [(0, 2)]
discards: [ 0  8 20 29]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 369.8444 - loglik: -2.5779e+02 - logprior: -1.1206e+02
Epoch 2/2
10/10 - 1s - loss: 286.8795 - loglik: -2.5682e+02 - logprior: -3.0059e+01
Fitted a model with MAP estimate = -274.7359
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 387.4270 - loglik: -2.5859e+02 - logprior: -1.2884e+02
Epoch 2/10
10/10 - 1s - loss: 294.8573 - loglik: -2.5834e+02 - logprior: -3.6520e+01
Epoch 3/10
10/10 - 1s - loss: 272.5933 - loglik: -2.5848e+02 - logprior: -1.4113e+01
Epoch 4/10
10/10 - 1s - loss: 264.6722 - loglik: -2.5869e+02 - logprior: -5.9835e+00
Epoch 5/10
10/10 - 1s - loss: 260.7653 - loglik: -2.5887e+02 - logprior: -1.8951e+00
Epoch 6/10
10/10 - 1s - loss: 258.5828 - loglik: -2.5898e+02 - logprior: 0.4015
Epoch 7/10
10/10 - 1s - loss: 257.2986 - loglik: -2.5907e+02 - logprior: 1.7747
Epoch 8/10
10/10 - 1s - loss: 256.4762 - loglik: -2.5919e+02 - logprior: 2.7149
Epoch 9/10
10/10 - 1s - loss: 255.8823 - loglik: -2.5934e+02 - logprior: 3.4565
Epoch 10/10
10/10 - 1s - loss: 255.4104 - loglik: -2.5949e+02 - logprior: 4.0782
Fitted a model with MAP estimate = -255.1812
Time for alignment: 34.6363
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 456.1231 - loglik: -3.2385e+02 - logprior: -1.3227e+02
Epoch 2/10
10/10 - 1s - loss: 337.3698 - loglik: -3.0181e+02 - logprior: -3.5561e+01
Epoch 3/10
10/10 - 1s - loss: 301.3564 - loglik: -2.8470e+02 - logprior: -1.6656e+01
Epoch 4/10
10/10 - 1s - loss: 285.0734 - loglik: -2.7511e+02 - logprior: -9.9673e+00
Epoch 5/10
10/10 - 1s - loss: 277.0596 - loglik: -2.7070e+02 - logprior: -6.3594e+00
Epoch 6/10
10/10 - 1s - loss: 272.5774 - loglik: -2.6820e+02 - logprior: -4.3743e+00
Epoch 7/10
10/10 - 1s - loss: 270.3705 - loglik: -2.6727e+02 - logprior: -3.1006e+00
Epoch 8/10
10/10 - 1s - loss: 269.2925 - loglik: -2.6707e+02 - logprior: -2.2226e+00
Epoch 9/10
10/10 - 1s - loss: 268.5431 - loglik: -2.6697e+02 - logprior: -1.5773e+00
Epoch 10/10
10/10 - 1s - loss: 268.0219 - loglik: -2.6683e+02 - logprior: -1.1890e+00
Fitted a model with MAP estimate = -267.8121
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (19, 1), (22, 2), (31, 1), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 407.9927 - loglik: -2.6647e+02 - logprior: -1.4152e+02
Epoch 2/2
10/10 - 1s - loss: 318.8209 - loglik: -2.6086e+02 - logprior: -5.7963e+01
Fitted a model with MAP estimate = -304.2072
expansions: [(0, 2)]
discards: [ 0  8 20 29]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 369.8450 - loglik: -2.5779e+02 - logprior: -1.1206e+02
Epoch 2/2
10/10 - 1s - loss: 286.8796 - loglik: -2.5682e+02 - logprior: -3.0059e+01
Fitted a model with MAP estimate = -274.7361
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 387.4271 - loglik: -2.5859e+02 - logprior: -1.2884e+02
Epoch 2/10
10/10 - 1s - loss: 294.8575 - loglik: -2.5834e+02 - logprior: -3.6520e+01
Epoch 3/10
10/10 - 1s - loss: 272.5933 - loglik: -2.5848e+02 - logprior: -1.4114e+01
Epoch 4/10
10/10 - 1s - loss: 264.6721 - loglik: -2.5869e+02 - logprior: -5.9834e+00
Epoch 5/10
10/10 - 1s - loss: 260.7651 - loglik: -2.5887e+02 - logprior: -1.8951e+00
Epoch 6/10
10/10 - 1s - loss: 258.5826 - loglik: -2.5898e+02 - logprior: 0.4017
Epoch 7/10
10/10 - 1s - loss: 257.2983 - loglik: -2.5907e+02 - logprior: 1.7749
Epoch 8/10
10/10 - 1s - loss: 256.4759 - loglik: -2.5919e+02 - logprior: 2.7151
Epoch 9/10
10/10 - 1s - loss: 255.8820 - loglik: -2.5934e+02 - logprior: 3.4568
Epoch 10/10
10/10 - 1s - loss: 255.4099 - loglik: -2.5949e+02 - logprior: 4.0786
Fitted a model with MAP estimate = -255.1806
Time for alignment: 34.5061
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 456.1231 - loglik: -3.2385e+02 - logprior: -1.3227e+02
Epoch 2/10
10/10 - 1s - loss: 337.3698 - loglik: -3.0181e+02 - logprior: -3.5561e+01
Epoch 3/10
10/10 - 1s - loss: 301.3564 - loglik: -2.8470e+02 - logprior: -1.6656e+01
Epoch 4/10
10/10 - 1s - loss: 285.0734 - loglik: -2.7511e+02 - logprior: -9.9673e+00
Epoch 5/10
10/10 - 1s - loss: 277.0597 - loglik: -2.7070e+02 - logprior: -6.3594e+00
Epoch 6/10
10/10 - 1s - loss: 272.5774 - loglik: -2.6820e+02 - logprior: -4.3743e+00
Epoch 7/10
10/10 - 1s - loss: 270.3705 - loglik: -2.6727e+02 - logprior: -3.1006e+00
Epoch 8/10
10/10 - 1s - loss: 269.2925 - loglik: -2.6707e+02 - logprior: -2.2226e+00
Epoch 9/10
10/10 - 1s - loss: 268.5431 - loglik: -2.6697e+02 - logprior: -1.5773e+00
Epoch 10/10
10/10 - 1s - loss: 268.0219 - loglik: -2.6683e+02 - logprior: -1.1890e+00
Fitted a model with MAP estimate = -267.8122
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (19, 1), (22, 2), (31, 1), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 407.9927 - loglik: -2.6647e+02 - logprior: -1.4152e+02
Epoch 2/2
10/10 - 1s - loss: 318.8209 - loglik: -2.6086e+02 - logprior: -5.7963e+01
Fitted a model with MAP estimate = -304.2072
expansions: [(0, 2)]
discards: [ 0  8 20 29]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 369.8446 - loglik: -2.5779e+02 - logprior: -1.1206e+02
Epoch 2/2
10/10 - 1s - loss: 286.8795 - loglik: -2.5682e+02 - logprior: -3.0059e+01
Fitted a model with MAP estimate = -274.7360
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 387.4272 - loglik: -2.5859e+02 - logprior: -1.2884e+02
Epoch 2/10
10/10 - 1s - loss: 294.8574 - loglik: -2.5834e+02 - logprior: -3.6520e+01
Epoch 3/10
10/10 - 1s - loss: 272.5934 - loglik: -2.5848e+02 - logprior: -1.4113e+01
Epoch 4/10
10/10 - 1s - loss: 264.6723 - loglik: -2.5869e+02 - logprior: -5.9835e+00
Epoch 5/10
10/10 - 1s - loss: 260.7653 - loglik: -2.5887e+02 - logprior: -1.8952e+00
Epoch 6/10
10/10 - 1s - loss: 258.5828 - loglik: -2.5898e+02 - logprior: 0.4015
Epoch 7/10
10/10 - 1s - loss: 257.2985 - loglik: -2.5907e+02 - logprior: 1.7747
Epoch 8/10
10/10 - 1s - loss: 256.4763 - loglik: -2.5919e+02 - logprior: 2.7148
Epoch 9/10
10/10 - 1s - loss: 255.8824 - loglik: -2.5934e+02 - logprior: 3.4565
Epoch 10/10
10/10 - 1s - loss: 255.4104 - loglik: -2.5949e+02 - logprior: 4.0782
Fitted a model with MAP estimate = -255.1812
Time for alignment: 31.6710
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 456.1231 - loglik: -3.2385e+02 - logprior: -1.3227e+02
Epoch 2/10
10/10 - 1s - loss: 337.3698 - loglik: -3.0181e+02 - logprior: -3.5561e+01
Epoch 3/10
10/10 - 1s - loss: 301.3564 - loglik: -2.8470e+02 - logprior: -1.6656e+01
Epoch 4/10
10/10 - 1s - loss: 285.0734 - loglik: -2.7511e+02 - logprior: -9.9673e+00
Epoch 5/10
10/10 - 1s - loss: 277.0596 - loglik: -2.7070e+02 - logprior: -6.3594e+00
Epoch 6/10
10/10 - 1s - loss: 272.5775 - loglik: -2.6820e+02 - logprior: -4.3743e+00
Epoch 7/10
10/10 - 1s - loss: 270.3705 - loglik: -2.6727e+02 - logprior: -3.1006e+00
Epoch 8/10
10/10 - 1s - loss: 269.2925 - loglik: -2.6707e+02 - logprior: -2.2226e+00
Epoch 9/10
10/10 - 1s - loss: 268.5431 - loglik: -2.6697e+02 - logprior: -1.5773e+00
Epoch 10/10
10/10 - 1s - loss: 268.0219 - loglik: -2.6683e+02 - logprior: -1.1890e+00
Fitted a model with MAP estimate = -267.8122
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (19, 1), (22, 2), (31, 1), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 407.9927 - loglik: -2.6647e+02 - logprior: -1.4152e+02
Epoch 2/2
10/10 - 1s - loss: 318.8209 - loglik: -2.6086e+02 - logprior: -5.7963e+01
Fitted a model with MAP estimate = -304.2072
expansions: [(0, 2)]
discards: [ 0  8 20 29]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 369.8456 - loglik: -2.5779e+02 - logprior: -1.1206e+02
Epoch 2/2
10/10 - 1s - loss: 286.8796 - loglik: -2.5682e+02 - logprior: -3.0059e+01
Fitted a model with MAP estimate = -274.7362
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 387.4272 - loglik: -2.5859e+02 - logprior: -1.2884e+02
Epoch 2/10
10/10 - 1s - loss: 294.8576 - loglik: -2.5834e+02 - logprior: -3.6520e+01
Epoch 3/10
10/10 - 1s - loss: 272.5931 - loglik: -2.5848e+02 - logprior: -1.4114e+01
Epoch 4/10
10/10 - 1s - loss: 264.6718 - loglik: -2.5869e+02 - logprior: -5.9832e+00
Epoch 5/10
10/10 - 1s - loss: 260.7647 - loglik: -2.5887e+02 - logprior: -1.8949e+00
Epoch 6/10
10/10 - 1s - loss: 258.5821 - loglik: -2.5898e+02 - logprior: 0.4022
Epoch 7/10
10/10 - 1s - loss: 257.2977 - loglik: -2.5907e+02 - logprior: 1.7754
Epoch 8/10
10/10 - 1s - loss: 256.4753 - loglik: -2.5919e+02 - logprior: 2.7157
Epoch 9/10
10/10 - 1s - loss: 255.8813 - loglik: -2.5934e+02 - logprior: 3.4575
Epoch 10/10
10/10 - 1s - loss: 255.4091 - loglik: -2.5949e+02 - logprior: 4.0794
Fitted a model with MAP estimate = -255.1797
Time for alignment: 33.5828
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 456.1231 - loglik: -3.2385e+02 - logprior: -1.3227e+02
Epoch 2/10
10/10 - 1s - loss: 337.3698 - loglik: -3.0181e+02 - logprior: -3.5561e+01
Epoch 3/10
10/10 - 1s - loss: 301.3564 - loglik: -2.8470e+02 - logprior: -1.6656e+01
Epoch 4/10
10/10 - 1s - loss: 285.0734 - loglik: -2.7511e+02 - logprior: -9.9673e+00
Epoch 5/10
10/10 - 1s - loss: 277.0596 - loglik: -2.7070e+02 - logprior: -6.3594e+00
Epoch 6/10
10/10 - 1s - loss: 272.5774 - loglik: -2.6820e+02 - logprior: -4.3743e+00
Epoch 7/10
10/10 - 1s - loss: 270.3705 - loglik: -2.6727e+02 - logprior: -3.1006e+00
Epoch 8/10
10/10 - 1s - loss: 269.2925 - loglik: -2.6707e+02 - logprior: -2.2226e+00
Epoch 9/10
10/10 - 1s - loss: 268.5431 - loglik: -2.6697e+02 - logprior: -1.5773e+00
Epoch 10/10
10/10 - 1s - loss: 268.0219 - loglik: -2.6683e+02 - logprior: -1.1890e+00
Fitted a model with MAP estimate = -267.8122
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (19, 1), (22, 2), (31, 1), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 407.9927 - loglik: -2.6647e+02 - logprior: -1.4152e+02
Epoch 2/2
10/10 - 1s - loss: 318.8209 - loglik: -2.6086e+02 - logprior: -5.7963e+01
Fitted a model with MAP estimate = -304.2072
expansions: [(0, 2)]
discards: [ 0  8 20 29]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 369.8448 - loglik: -2.5779e+02 - logprior: -1.1206e+02
Epoch 2/2
10/10 - 1s - loss: 286.8796 - loglik: -2.5682e+02 - logprior: -3.0059e+01
Fitted a model with MAP estimate = -274.7360
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 387.4270 - loglik: -2.5859e+02 - logprior: -1.2884e+02
Epoch 2/10
10/10 - 1s - loss: 294.8574 - loglik: -2.5834e+02 - logprior: -3.6520e+01
Epoch 3/10
10/10 - 1s - loss: 272.5933 - loglik: -2.5848e+02 - logprior: -1.4114e+01
Epoch 4/10
10/10 - 1s - loss: 264.6722 - loglik: -2.5869e+02 - logprior: -5.9835e+00
Epoch 5/10
10/10 - 1s - loss: 260.7652 - loglik: -2.5887e+02 - logprior: -1.8951e+00
Epoch 6/10
10/10 - 1s - loss: 258.5827 - loglik: -2.5898e+02 - logprior: 0.4016
Epoch 7/10
10/10 - 1s - loss: 257.2984 - loglik: -2.5907e+02 - logprior: 1.7748
Epoch 8/10
10/10 - 1s - loss: 256.4760 - loglik: -2.5919e+02 - logprior: 2.7150
Epoch 9/10
10/10 - 1s - loss: 255.8821 - loglik: -2.5934e+02 - logprior: 3.4567
Epoch 10/10
10/10 - 1s - loss: 255.4101 - loglik: -2.5949e+02 - logprior: 4.0784
Fitted a model with MAP estimate = -255.1809
Time for alignment: 33.3454
Computed alignments with likelihoods: ['-255.1812', '-255.1806', '-255.1812', '-255.1797', '-255.1809']
Best model has likelihood: -255.1797  (prior= 4.3831 )
time for generating output: 0.1317
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/scorptoxin.projection.fasta
SP score = 0.8723277909738717
Training of 5 independent models on file biotin_lipoyl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52844018e0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52e0ad16a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52966c7820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f528987a4c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52e13c3bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52d8b0f670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526c464a60>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52f935c4f0>, <__main__.SimpleDirichletPrior object at 0x7f526b1004c0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 401.9320 - loglik: -3.9856e+02 - logprior: -3.3761e+00
Epoch 2/10
19/19 - 2s - loss: 362.7909 - loglik: -3.6158e+02 - logprior: -1.2069e+00
Epoch 3/10
19/19 - 1s - loss: 348.3419 - loglik: -3.4705e+02 - logprior: -1.2890e+00
Epoch 4/10
19/19 - 1s - loss: 346.0896 - loglik: -3.4483e+02 - logprior: -1.2612e+00
Epoch 5/10
19/19 - 2s - loss: 345.4872 - loglik: -3.4428e+02 - logprior: -1.2121e+00
Epoch 6/10
19/19 - 2s - loss: 345.3127 - loglik: -3.4413e+02 - logprior: -1.1839e+00
Epoch 7/10
19/19 - 2s - loss: 344.8714 - loglik: -3.4371e+02 - logprior: -1.1649e+00
Epoch 8/10
19/19 - 2s - loss: 345.0009 - loglik: -3.4385e+02 - logprior: -1.1478e+00
Fitted a model with MAP estimate = -342.3873
expansions: [(0, 3), (12, 1), (13, 1), (14, 1), (15, 1), (20, 1), (33, 1), (44, 1), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 347.1359 - loglik: -3.4279e+02 - logprior: -4.3454e+00
Epoch 2/2
19/19 - 2s - loss: 337.7151 - loglik: -3.3630e+02 - logprior: -1.4152e+00
Fitted a model with MAP estimate = -334.8716
expansions: [(0, 2)]
discards: [58 63]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 339.9472 - loglik: -3.3594e+02 - logprior: -4.0110e+00
Epoch 2/2
19/19 - 2s - loss: 335.4135 - loglik: -3.3416e+02 - logprior: -1.2552e+00
Fitted a model with MAP estimate = -333.0895
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 336.9463 - loglik: -3.3357e+02 - logprior: -3.3801e+00
Epoch 2/10
21/21 - 2s - loss: 333.4111 - loglik: -3.3195e+02 - logprior: -1.4621e+00
Epoch 3/10
21/21 - 2s - loss: 332.0061 - loglik: -3.3091e+02 - logprior: -1.0938e+00
Epoch 4/10
21/21 - 2s - loss: 331.7172 - loglik: -3.3067e+02 - logprior: -1.0520e+00
Epoch 5/10
21/21 - 2s - loss: 330.9861 - loglik: -3.2994e+02 - logprior: -1.0503e+00
Epoch 6/10
21/21 - 2s - loss: 331.2139 - loglik: -3.3017e+02 - logprior: -1.0439e+00
Fitted a model with MAP estimate = -330.8216
Time for alignment: 54.4765
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 401.7420 - loglik: -3.9837e+02 - logprior: -3.3758e+00
Epoch 2/10
19/19 - 2s - loss: 363.0536 - loglik: -3.6186e+02 - logprior: -1.1904e+00
Epoch 3/10
19/19 - 2s - loss: 348.2058 - loglik: -3.4693e+02 - logprior: -1.2763e+00
Epoch 4/10
19/19 - 1s - loss: 345.8939 - loglik: -3.4464e+02 - logprior: -1.2511e+00
Epoch 5/10
19/19 - 1s - loss: 344.9484 - loglik: -3.4375e+02 - logprior: -1.1986e+00
Epoch 6/10
19/19 - 1s - loss: 344.7826 - loglik: -3.4360e+02 - logprior: -1.1780e+00
Epoch 7/10
19/19 - 2s - loss: 344.6729 - loglik: -3.4352e+02 - logprior: -1.1516e+00
Epoch 8/10
19/19 - 2s - loss: 344.4102 - loglik: -3.4327e+02 - logprior: -1.1362e+00
Epoch 9/10
19/19 - 2s - loss: 344.6601 - loglik: -3.4353e+02 - logprior: -1.1308e+00
Fitted a model with MAP estimate = -341.9972
expansions: [(0, 3), (13, 2), (14, 2), (19, 1), (20, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 348.7207 - loglik: -3.4429e+02 - logprior: -4.4267e+00
Epoch 2/2
19/19 - 2s - loss: 338.4041 - loglik: -3.3690e+02 - logprior: -1.5056e+00
Fitted a model with MAP estimate = -335.1897
expansions: [(0, 2)]
discards: [19 28 50 61 66]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 340.4701 - loglik: -3.3644e+02 - logprior: -4.0337e+00
Epoch 2/2
19/19 - 2s - loss: 335.2960 - loglik: -3.3400e+02 - logprior: -1.2974e+00
Fitted a model with MAP estimate = -332.2276
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 336.1430 - loglik: -3.3273e+02 - logprior: -3.4151e+00
Epoch 2/10
21/21 - 2s - loss: 332.7634 - loglik: -3.3122e+02 - logprior: -1.5437e+00
Epoch 3/10
21/21 - 2s - loss: 331.5809 - loglik: -3.3048e+02 - logprior: -1.0981e+00
Epoch 4/10
21/21 - 2s - loss: 331.3342 - loglik: -3.3029e+02 - logprior: -1.0483e+00
Epoch 5/10
21/21 - 2s - loss: 330.8766 - loglik: -3.2982e+02 - logprior: -1.0560e+00
Epoch 6/10
21/21 - 2s - loss: 330.8800 - loglik: -3.2983e+02 - logprior: -1.0491e+00
Fitted a model with MAP estimate = -330.5444
Time for alignment: 61.8856
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 402.0658 - loglik: -3.9869e+02 - logprior: -3.3752e+00
Epoch 2/10
19/19 - 2s - loss: 364.1801 - loglik: -3.6299e+02 - logprior: -1.1933e+00
Epoch 3/10
19/19 - 2s - loss: 350.7256 - loglik: -3.4949e+02 - logprior: -1.2345e+00
Epoch 4/10
19/19 - 2s - loss: 347.8648 - loglik: -3.4666e+02 - logprior: -1.2064e+00
Epoch 5/10
19/19 - 2s - loss: 347.2969 - loglik: -3.4613e+02 - logprior: -1.1628e+00
Epoch 6/10
19/19 - 2s - loss: 346.9431 - loglik: -3.4580e+02 - logprior: -1.1476e+00
Epoch 7/10
19/19 - 2s - loss: 346.7011 - loglik: -3.4557e+02 - logprior: -1.1326e+00
Epoch 8/10
19/19 - 2s - loss: 346.6668 - loglik: -3.4554e+02 - logprior: -1.1292e+00
Epoch 9/10
19/19 - 2s - loss: 346.4894 - loglik: -3.4536e+02 - logprior: -1.1304e+00
Epoch 10/10
19/19 - 1s - loss: 346.6815 - loglik: -3.4555e+02 - logprior: -1.1350e+00
Fitted a model with MAP estimate = -344.0468
expansions: [(0, 3), (12, 1), (13, 1), (14, 1), (19, 2), (33, 1), (34, 1), (39, 2), (46, 1), (47, 1), (52, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 347.8387 - loglik: -3.4340e+02 - logprior: -4.4404e+00
Epoch 2/2
19/19 - 2s - loss: 337.7759 - loglik: -3.3636e+02 - logprior: -1.4155e+00
Fitted a model with MAP estimate = -335.3267
expansions: [(0, 2)]
discards: [25 49]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 340.0813 - loglik: -3.3612e+02 - logprior: -3.9590e+00
Epoch 2/2
19/19 - 2s - loss: 335.6157 - loglik: -3.3426e+02 - logprior: -1.3561e+00
Fitted a model with MAP estimate = -333.3017
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 336.9984 - loglik: -3.3360e+02 - logprior: -3.4032e+00
Epoch 2/10
21/21 - 2s - loss: 333.7344 - loglik: -3.3207e+02 - logprior: -1.6684e+00
Epoch 3/10
21/21 - 2s - loss: 332.5981 - loglik: -3.3149e+02 - logprior: -1.1050e+00
Epoch 4/10
21/21 - 2s - loss: 331.3061 - loglik: -3.3027e+02 - logprior: -1.0364e+00
Epoch 5/10
21/21 - 2s - loss: 331.4133 - loglik: -3.3036e+02 - logprior: -1.0492e+00
Fitted a model with MAP estimate = -331.0325
Time for alignment: 60.0194
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 402.0916 - loglik: -3.9872e+02 - logprior: -3.3729e+00
Epoch 2/10
19/19 - 2s - loss: 362.9343 - loglik: -3.6174e+02 - logprior: -1.1910e+00
Epoch 3/10
19/19 - 2s - loss: 348.0100 - loglik: -3.4673e+02 - logprior: -1.2788e+00
Epoch 4/10
19/19 - 2s - loss: 345.6271 - loglik: -3.4437e+02 - logprior: -1.2527e+00
Epoch 5/10
19/19 - 2s - loss: 345.2010 - loglik: -3.4400e+02 - logprior: -1.1963e+00
Epoch 6/10
19/19 - 2s - loss: 344.5731 - loglik: -3.4341e+02 - logprior: -1.1672e+00
Epoch 7/10
19/19 - 2s - loss: 344.6385 - loglik: -3.4349e+02 - logprior: -1.1461e+00
Fitted a model with MAP estimate = -341.9719
expansions: [(0, 3), (13, 2), (14, 2), (19, 1), (24, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 348.5855 - loglik: -3.4426e+02 - logprior: -4.3281e+00
Epoch 2/2
19/19 - 2s - loss: 338.1483 - loglik: -3.3665e+02 - logprior: -1.4984e+00
Fitted a model with MAP estimate = -335.4687
expansions: [(0, 2)]
discards: [19 32 50 61 66]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 340.3756 - loglik: -3.3635e+02 - logprior: -4.0236e+00
Epoch 2/2
19/19 - 2s - loss: 335.0992 - loglik: -3.3380e+02 - logprior: -1.2975e+00
Fitted a model with MAP estimate = -332.3350
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 8s - loss: 336.0202 - loglik: -3.3261e+02 - logprior: -3.4128e+00
Epoch 2/10
21/21 - 2s - loss: 332.8988 - loglik: -3.3137e+02 - logprior: -1.5261e+00
Epoch 3/10
21/21 - 2s - loss: 331.6854 - loglik: -3.3059e+02 - logprior: -1.0969e+00
Epoch 4/10
21/21 - 2s - loss: 331.1722 - loglik: -3.3012e+02 - logprior: -1.0566e+00
Epoch 5/10
21/21 - 2s - loss: 330.7838 - loglik: -3.2972e+02 - logprior: -1.0595e+00
Epoch 6/10
21/21 - 2s - loss: 330.7622 - loglik: -3.2971e+02 - logprior: -1.0483e+00
Epoch 7/10
21/21 - 2s - loss: 330.5062 - loglik: -3.2946e+02 - logprior: -1.0416e+00
Epoch 8/10
21/21 - 2s - loss: 330.6833 - loglik: -3.2965e+02 - logprior: -1.0362e+00
Fitted a model with MAP estimate = -330.4235
Time for alignment: 59.5729
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 401.9408 - loglik: -3.9857e+02 - logprior: -3.3742e+00
Epoch 2/10
19/19 - 2s - loss: 361.6573 - loglik: -3.6047e+02 - logprior: -1.1917e+00
Epoch 3/10
19/19 - 2s - loss: 348.0356 - loglik: -3.4675e+02 - logprior: -1.2814e+00
Epoch 4/10
19/19 - 1s - loss: 345.9377 - loglik: -3.4468e+02 - logprior: -1.2554e+00
Epoch 5/10
19/19 - 2s - loss: 345.5392 - loglik: -3.4434e+02 - logprior: -1.2029e+00
Epoch 6/10
19/19 - 1s - loss: 345.0357 - loglik: -3.4385e+02 - logprior: -1.1848e+00
Epoch 7/10
19/19 - 2s - loss: 344.9135 - loglik: -3.4375e+02 - logprior: -1.1593e+00
Epoch 8/10
19/19 - 2s - loss: 344.8234 - loglik: -3.4367e+02 - logprior: -1.1559e+00
Epoch 9/10
19/19 - 2s - loss: 345.0172 - loglik: -3.4387e+02 - logprior: -1.1433e+00
Fitted a model with MAP estimate = -342.2988
expansions: [(0, 3), (12, 1), (13, 1), (14, 1), (15, 1), (20, 1), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 76 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 347.7773 - loglik: -3.4338e+02 - logprior: -4.3946e+00
Epoch 2/2
19/19 - 2s - loss: 338.0581 - loglik: -3.3662e+02 - logprior: -1.4414e+00
Fitted a model with MAP estimate = -335.2710
expansions: [(0, 2)]
discards: [48 59 64]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 340.3842 - loglik: -3.3641e+02 - logprior: -3.9764e+00
Epoch 2/2
19/19 - 2s - loss: 335.6601 - loglik: -3.3434e+02 - logprior: -1.3244e+00
Fitted a model with MAP estimate = -333.0898
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 336.9072 - loglik: -3.3351e+02 - logprior: -3.3992e+00
Epoch 2/10
21/21 - 2s - loss: 333.8386 - loglik: -3.3221e+02 - logprior: -1.6290e+00
Epoch 3/10
21/21 - 2s - loss: 332.5385 - loglik: -3.3144e+02 - logprior: -1.0982e+00
Epoch 4/10
21/21 - 2s - loss: 331.6642 - loglik: -3.3063e+02 - logprior: -1.0371e+00
Epoch 5/10
21/21 - 2s - loss: 331.5267 - loglik: -3.3047e+02 - logprior: -1.0517e+00
Epoch 6/10
21/21 - 2s - loss: 331.2864 - loglik: -3.3024e+02 - logprior: -1.0423e+00
Epoch 7/10
21/21 - 2s - loss: 331.4446 - loglik: -3.3040e+02 - logprior: -1.0444e+00
Fitted a model with MAP estimate = -331.1369
Time for alignment: 57.8808
Computed alignments with likelihoods: ['-330.8216', '-330.5444', '-331.0325', '-330.4235', '-331.1369']
Best model has likelihood: -330.4235  (prior= -1.0189 )
time for generating output: 0.1582
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/biotin_lipoyl.projection.fasta
SP score = 0.941983780411728
Training of 5 independent models on file uce.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52975ab430>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52973e7370>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52973e7670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5ca7892c70>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5900465520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52751d1e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52751d19a0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f528858b6a0>, <__main__.SimpleDirichletPrior object at 0x7f5285208d90>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 9s - loss: 789.6656 - loglik: -7.8016e+02 - logprior: -9.5029e+00
Epoch 2/10
13/13 - 2s - loss: 733.3028 - loglik: -7.3121e+02 - logprior: -2.0953e+00
Epoch 3/10
13/13 - 2s - loss: 691.9641 - loglik: -6.9015e+02 - logprior: -1.8177e+00
Epoch 4/10
13/13 - 2s - loss: 680.2695 - loglik: -6.7827e+02 - logprior: -2.0025e+00
Epoch 5/10
13/13 - 2s - loss: 675.8741 - loglik: -6.7385e+02 - logprior: -2.0239e+00
Epoch 6/10
13/13 - 2s - loss: 674.3788 - loglik: -6.7244e+02 - logprior: -1.9399e+00
Epoch 7/10
13/13 - 2s - loss: 671.7141 - loglik: -6.6977e+02 - logprior: -1.9392e+00
Epoch 8/10
13/13 - 2s - loss: 672.5402 - loglik: -6.7052e+02 - logprior: -2.0216e+00
Fitted a model with MAP estimate = -672.3837
expansions: [(7, 2), (8, 2), (11, 1), (15, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (61, 2), (70, 2), (75, 2), (76, 4), (97, 1), (98, 2), (99, 3), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 681.0117 - loglik: -6.7103e+02 - logprior: -9.9837e+00
Epoch 2/2
13/13 - 3s - loss: 661.7568 - loglik: -6.5772e+02 - logprior: -4.0365e+00
Fitted a model with MAP estimate = -658.7948
expansions: [(0, 3)]
discards: [  0  77  94 128]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 662.8638 - loglik: -6.5550e+02 - logprior: -7.3590e+00
Epoch 2/2
13/13 - 3s - loss: 654.7944 - loglik: -6.5307e+02 - logprior: -1.7225e+00
Fitted a model with MAP estimate = -652.6942
expansions: []
discards: [ 0  2 89]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 665.4376 - loglik: -6.5627e+02 - logprior: -9.1676e+00
Epoch 2/10
13/13 - 3s - loss: 656.7827 - loglik: -6.5403e+02 - logprior: -2.7494e+00
Epoch 3/10
13/13 - 3s - loss: 652.8796 - loglik: -6.5195e+02 - logprior: -9.2829e-01
Epoch 4/10
13/13 - 3s - loss: 652.6503 - loglik: -6.5225e+02 - logprior: -4.0428e-01
Epoch 5/10
13/13 - 3s - loss: 650.8617 - loglik: -6.5056e+02 - logprior: -2.9808e-01
Epoch 6/10
13/13 - 3s - loss: 650.5195 - loglik: -6.5025e+02 - logprior: -2.7012e-01
Epoch 7/10
13/13 - 3s - loss: 650.4318 - loglik: -6.5015e+02 - logprior: -2.8597e-01
Epoch 8/10
13/13 - 3s - loss: 649.6256 - loglik: -6.4930e+02 - logprior: -3.2981e-01
Epoch 9/10
13/13 - 3s - loss: 649.8531 - loglik: -6.4953e+02 - logprior: -3.2355e-01
Fitted a model with MAP estimate = -649.3785
Time for alignment: 86.9599
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 789.1132 - loglik: -7.7961e+02 - logprior: -9.5005e+00
Epoch 2/10
13/13 - 2s - loss: 731.8852 - loglik: -7.2979e+02 - logprior: -2.0939e+00
Epoch 3/10
13/13 - 2s - loss: 693.3856 - loglik: -6.9158e+02 - logprior: -1.8099e+00
Epoch 4/10
13/13 - 2s - loss: 680.2689 - loglik: -6.7824e+02 - logprior: -2.0286e+00
Epoch 5/10
13/13 - 2s - loss: 675.5364 - loglik: -6.7350e+02 - logprior: -2.0398e+00
Epoch 6/10
13/13 - 2s - loss: 674.5623 - loglik: -6.7260e+02 - logprior: -1.9595e+00
Epoch 7/10
13/13 - 2s - loss: 673.6459 - loglik: -6.7165e+02 - logprior: -1.9911e+00
Epoch 8/10
13/13 - 2s - loss: 672.4672 - loglik: -6.7039e+02 - logprior: -2.0768e+00
Epoch 9/10
13/13 - 2s - loss: 672.6345 - loglik: -6.7055e+02 - logprior: -2.0808e+00
Fitted a model with MAP estimate = -672.7396
expansions: [(7, 2), (8, 3), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (49, 1), (50, 1), (51, 1), (70, 2), (75, 2), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 680.6277 - loglik: -6.7063e+02 - logprior: -9.9945e+00
Epoch 2/2
13/13 - 3s - loss: 661.5470 - loglik: -6.5749e+02 - logprior: -4.0528e+00
Fitted a model with MAP estimate = -658.6728
expansions: [(0, 2)]
discards: [ 0  9 93]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 664.2757 - loglik: -6.5696e+02 - logprior: -7.3184e+00
Epoch 2/2
13/13 - 3s - loss: 652.7870 - loglik: -6.5106e+02 - logprior: -1.7254e+00
Fitted a model with MAP estimate = -652.7643
expansions: []
discards: [ 0 87]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 666.5947 - loglik: -6.5737e+02 - logprior: -9.2231e+00
Epoch 2/10
13/13 - 3s - loss: 656.9840 - loglik: -6.5406e+02 - logprior: -2.9231e+00
Epoch 3/10
13/13 - 3s - loss: 653.3590 - loglik: -6.5235e+02 - logprior: -1.0106e+00
Epoch 4/10
13/13 - 3s - loss: 651.8578 - loglik: -6.5145e+02 - logprior: -4.0944e-01
Epoch 5/10
13/13 - 3s - loss: 651.1663 - loglik: -6.5085e+02 - logprior: -3.1538e-01
Epoch 6/10
13/13 - 3s - loss: 651.1991 - loglik: -6.5090e+02 - logprior: -2.9470e-01
Fitted a model with MAP estimate = -650.2601
Time for alignment: 78.8855
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 789.5687 - loglik: -7.8008e+02 - logprior: -9.4908e+00
Epoch 2/10
13/13 - 2s - loss: 731.9851 - loglik: -7.2990e+02 - logprior: -2.0858e+00
Epoch 3/10
13/13 - 2s - loss: 691.2057 - loglik: -6.8939e+02 - logprior: -1.8118e+00
Epoch 4/10
13/13 - 2s - loss: 679.1288 - loglik: -6.7715e+02 - logprior: -1.9760e+00
Epoch 5/10
13/13 - 2s - loss: 674.2865 - loglik: -6.7230e+02 - logprior: -1.9860e+00
Epoch 6/10
13/13 - 2s - loss: 674.0618 - loglik: -6.7213e+02 - logprior: -1.9307e+00
Epoch 7/10
13/13 - 2s - loss: 673.5893 - loglik: -6.7164e+02 - logprior: -1.9445e+00
Epoch 8/10
13/13 - 2s - loss: 672.4923 - loglik: -6.7044e+02 - logprior: -2.0481e+00
Epoch 9/10
13/13 - 2s - loss: 673.0052 - loglik: -6.7096e+02 - logprior: -2.0458e+00
Fitted a model with MAP estimate = -672.2576
expansions: [(7, 2), (8, 3), (11, 1), (13, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (61, 2), (70, 2), (75, 2), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 681.0912 - loglik: -6.7109e+02 - logprior: -1.0001e+01
Epoch 2/2
13/13 - 3s - loss: 661.0358 - loglik: -6.5698e+02 - logprior: -4.0568e+00
Fitted a model with MAP estimate = -658.5794
expansions: [(0, 3)]
discards: [ 0  9 78 95]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 9s - loss: 663.2902 - loglik: -6.5592e+02 - logprior: -7.3670e+00
Epoch 2/2
13/13 - 3s - loss: 654.7379 - loglik: -6.5302e+02 - logprior: -1.7225e+00
Fitted a model with MAP estimate = -652.6913
expansions: []
discards: [ 0  2 24 89]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 665.7825 - loglik: -6.5663e+02 - logprior: -9.1574e+00
Epoch 2/10
13/13 - 3s - loss: 657.3145 - loglik: -6.5452e+02 - logprior: -2.7920e+00
Epoch 3/10
13/13 - 3s - loss: 652.4812 - loglik: -6.5156e+02 - logprior: -9.2157e-01
Epoch 4/10
13/13 - 3s - loss: 653.1456 - loglik: -6.5278e+02 - logprior: -3.6274e-01
Fitted a model with MAP estimate = -651.6035
Time for alignment: 73.8106
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 788.0191 - loglik: -7.7852e+02 - logprior: -9.5026e+00
Epoch 2/10
13/13 - 2s - loss: 735.1411 - loglik: -7.3304e+02 - logprior: -2.1022e+00
Epoch 3/10
13/13 - 2s - loss: 696.5548 - loglik: -6.9471e+02 - logprior: -1.8400e+00
Epoch 4/10
13/13 - 2s - loss: 680.7483 - loglik: -6.7870e+02 - logprior: -2.0502e+00
Epoch 5/10
13/13 - 2s - loss: 676.1588 - loglik: -6.7412e+02 - logprior: -2.0380e+00
Epoch 6/10
13/13 - 2s - loss: 674.9263 - loglik: -6.7297e+02 - logprior: -1.9537e+00
Epoch 7/10
13/13 - 2s - loss: 673.7509 - loglik: -6.7181e+02 - logprior: -1.9452e+00
Epoch 8/10
13/13 - 2s - loss: 674.3606 - loglik: -6.7237e+02 - logprior: -1.9879e+00
Fitted a model with MAP estimate = -673.4349
expansions: [(7, 2), (8, 3), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (24, 1), (36, 1), (37, 1), (49, 1), (52, 1), (53, 2), (70, 3), (75, 2), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 681.1386 - loglik: -6.7114e+02 - logprior: -9.9982e+00
Epoch 2/2
13/13 - 3s - loss: 661.5751 - loglik: -6.5746e+02 - logprior: -4.1138e+00
Fitted a model with MAP estimate = -658.2905
expansions: [(0, 3)]
discards: [ 0  9 68 88 89 95]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 663.7829 - loglik: -6.5642e+02 - logprior: -7.3672e+00
Epoch 2/2
13/13 - 3s - loss: 655.0682 - loglik: -6.5334e+02 - logprior: -1.7321e+00
Fitted a model with MAP estimate = -653.1485
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 665.7866 - loglik: -6.5659e+02 - logprior: -9.1962e+00
Epoch 2/10
13/13 - 3s - loss: 656.3558 - loglik: -6.5361e+02 - logprior: -2.7455e+00
Epoch 3/10
13/13 - 3s - loss: 653.6272 - loglik: -6.5270e+02 - logprior: -9.3210e-01
Epoch 4/10
13/13 - 3s - loss: 651.1518 - loglik: -6.5076e+02 - logprior: -3.8825e-01
Epoch 5/10
13/13 - 3s - loss: 652.0743 - loglik: -6.5176e+02 - logprior: -3.1027e-01
Fitted a model with MAP estimate = -650.7408
Time for alignment: 73.6117
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 788.3593 - loglik: -7.7885e+02 - logprior: -9.5074e+00
Epoch 2/10
13/13 - 2s - loss: 733.9595 - loglik: -7.3187e+02 - logprior: -2.0929e+00
Epoch 3/10
13/13 - 2s - loss: 692.7558 - loglik: -6.9097e+02 - logprior: -1.7879e+00
Epoch 4/10
13/13 - 2s - loss: 680.6760 - loglik: -6.7867e+02 - logprior: -2.0024e+00
Epoch 5/10
13/13 - 2s - loss: 675.2426 - loglik: -6.7318e+02 - logprior: -2.0670e+00
Epoch 6/10
13/13 - 2s - loss: 673.6302 - loglik: -6.7164e+02 - logprior: -1.9942e+00
Epoch 7/10
13/13 - 2s - loss: 673.2931 - loglik: -6.7128e+02 - logprior: -2.0176e+00
Epoch 8/10
13/13 - 2s - loss: 673.0097 - loglik: -6.7093e+02 - logprior: -2.0808e+00
Epoch 9/10
13/13 - 2s - loss: 673.4974 - loglik: -6.7140e+02 - logprior: -2.0987e+00
Fitted a model with MAP estimate = -672.4668
expansions: [(7, 2), (8, 3), (11, 1), (15, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (49, 1), (50, 1), (51, 1), (70, 2), (75, 2), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 680.0159 - loglik: -6.6999e+02 - logprior: -1.0021e+01
Epoch 2/2
13/13 - 3s - loss: 662.0240 - loglik: -6.5795e+02 - logprior: -4.0727e+00
Fitted a model with MAP estimate = -658.4908
expansions: [(0, 3)]
discards: [ 0  9 88 94]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 663.6644 - loglik: -6.5629e+02 - logprior: -7.3784e+00
Epoch 2/2
13/13 - 3s - loss: 654.4130 - loglik: -6.5266e+02 - logprior: -1.7543e+00
Fitted a model with MAP estimate = -653.0815
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 665.0809 - loglik: -6.5585e+02 - logprior: -9.2274e+00
Epoch 2/10
13/13 - 3s - loss: 657.0604 - loglik: -6.5426e+02 - logprior: -2.7983e+00
Epoch 3/10
13/13 - 3s - loss: 652.9336 - loglik: -6.5197e+02 - logprior: -9.6358e-01
Epoch 4/10
13/13 - 3s - loss: 651.8604 - loglik: -6.5143e+02 - logprior: -4.2586e-01
Epoch 5/10
13/13 - 3s - loss: 652.1381 - loglik: -6.5181e+02 - logprior: -3.2959e-01
Fitted a model with MAP estimate = -650.6122
Time for alignment: 73.5295
Computed alignments with likelihoods: ['-649.3785', '-650.2601', '-651.6035', '-650.7408', '-650.6122']
Best model has likelihood: -649.3785  (prior= -0.3021 )
time for generating output: 0.2040
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/uce.projection.fasta
SP score = 0.95188246097337
Training of 5 independent models on file ghf1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b0a880af0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5271dc8c40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5276f856d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526f411b50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52717bfd30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526f401520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5273cc5430>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f529569cee0>, <__main__.SimpleDirichletPrior object at 0x7f52728cd580>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 38s - loss: 2273.3713 - loglik: -2.2671e+03 - logprior: -6.3034e+00
Epoch 2/10
25/25 - 30s - loss: 2010.5208 - loglik: -2.0092e+03 - logprior: -1.3019e+00
Epoch 3/10
25/25 - 30s - loss: 1956.2450 - loglik: -1.9540e+03 - logprior: -2.2766e+00
Epoch 4/10
25/25 - 30s - loss: 1943.5822 - loglik: -1.9412e+03 - logprior: -2.4220e+00
Epoch 5/10
25/25 - 30s - loss: 1945.6469 - loglik: -1.9432e+03 - logprior: -2.4702e+00
Fitted a model with MAP estimate = -1942.9898
expansions: [(0, 2), (50, 1), (125, 1), (131, 1), (133, 1), (135, 1), (145, 1), (161, 1), (162, 2), (163, 1), (168, 1), (172, 4), (173, 2), (174, 2), (175, 1), (188, 1), (189, 1), (190, 4), (191, 2), (192, 1), (196, 1), (197, 1), (198, 1), (200, 1), (201, 1), (203, 1), (205, 1), (207, 1), (208, 1), (209, 1), (210, 1), (211, 1), (216, 1), (222, 1), (223, 1), (224, 2), (225, 1), (226, 1), (227, 2), (228, 1), (229, 1), (234, 1), (236, 1), (237, 1), (248, 1), (252, 1), (253, 2), (254, 3), (255, 2), (257, 1), (258, 1), (266, 1), (280, 1), (282, 1), (283, 1), (299, 1), (300, 1), (301, 2), (304, 1), (315, 1), (317, 1), (318, 1), (319, 1), (326, 2), (328, 2), (337, 1), (356, 1), (361, 1), (364, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 460 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 48s - loss: 1919.0529 - loglik: -1.9132e+03 - logprior: -5.8931e+00
Epoch 2/2
25/25 - 44s - loss: 1900.6992 - loglik: -1.9008e+03 - logprior: 0.0631
Fitted a model with MAP estimate = -1891.0734
expansions: [(314, 1)]
discards: [  2 172 188 189 216 316 317 318 374]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 47s - loss: 1901.4083 - loglik: -1.8988e+03 - logprior: -2.6057e+00
Epoch 2/2
25/25 - 43s - loss: 1893.1257 - loglik: -1.8945e+03 - logprior: 1.3417
Fitted a model with MAP estimate = -1890.6282
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 46s - loss: 1898.4653 - loglik: -1.8961e+03 - logprior: -2.4062e+00
Epoch 2/10
25/25 - 43s - loss: 1893.7053 - loglik: -1.8953e+03 - logprior: 1.6400
Epoch 3/10
25/25 - 43s - loss: 1897.8850 - loglik: -1.9000e+03 - logprior: 2.1257
Fitted a model with MAP estimate = -1888.5678
Time for alignment: 621.7095
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 37s - loss: 2274.5142 - loglik: -2.2681e+03 - logprior: -6.4284e+00
Epoch 2/10
25/25 - 30s - loss: 2014.9360 - loglik: -2.0138e+03 - logprior: -1.0876e+00
Epoch 3/10
25/25 - 30s - loss: 1968.4398 - loglik: -1.9667e+03 - logprior: -1.7385e+00
Epoch 4/10
25/25 - 30s - loss: 1964.8263 - loglik: -1.9631e+03 - logprior: -1.7460e+00
Epoch 5/10
25/25 - 30s - loss: 1963.1743 - loglik: -1.9614e+03 - logprior: -1.8046e+00
Epoch 6/10
25/25 - 30s - loss: 1954.4727 - loglik: -1.9526e+03 - logprior: -1.8813e+00
Epoch 7/10
25/25 - 30s - loss: 1956.4296 - loglik: -1.9544e+03 - logprior: -2.0308e+00
Fitted a model with MAP estimate = -1956.8697
expansions: [(0, 2), (126, 1), (145, 2), (147, 1), (164, 2), (170, 1), (174, 5), (175, 2), (176, 2), (177, 1), (190, 1), (191, 1), (192, 4), (193, 1), (194, 1), (197, 1), (198, 3), (199, 1), (201, 1), (202, 1), (204, 1), (206, 2), (207, 1), (208, 1), (210, 1), (211, 1), (212, 1), (217, 1), (218, 1), (222, 1), (223, 1), (224, 2), (225, 1), (226, 1), (227, 2), (228, 1), (229, 1), (237, 2), (238, 1), (249, 1), (252, 1), (254, 3), (255, 1), (256, 1), (259, 1), (266, 3), (282, 1), (300, 1), (301, 1), (302, 2), (304, 1), (305, 1), (314, 1), (316, 1), (319, 1), (325, 2), (327, 2), (354, 2), (358, 1), (360, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 457 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 48s - loss: 1940.0403 - loglik: -1.9338e+03 - logprior: -6.2097e+00
Epoch 2/2
25/25 - 43s - loss: 1898.8042 - loglik: -1.8987e+03 - logprior: -1.2322e-01
Fitted a model with MAP estimate = -1897.3729
expansions: [(171, 1), (405, 1), (435, 1)]
discards: [  2 185 186 187 215 242 310 436]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 47s - loss: 1905.5145 - loglik: -1.9024e+03 - logprior: -3.1614e+00
Epoch 2/2
25/25 - 43s - loss: 1902.5704 - loglik: -1.9033e+03 - logprior: 0.6977
Fitted a model with MAP estimate = -1895.2772
expansions: [(185, 3)]
discards: [190 309 430]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 46s - loss: 1907.6624 - loglik: -1.9052e+03 - logprior: -2.4821e+00
Epoch 2/10
25/25 - 43s - loss: 1895.3159 - loglik: -1.8971e+03 - logprior: 1.7818
Epoch 3/10
25/25 - 43s - loss: 1886.0327 - loglik: -1.8883e+03 - logprior: 2.3038
Epoch 4/10
25/25 - 43s - loss: 1900.4178 - loglik: -1.9029e+03 - logprior: 2.4954
Fitted a model with MAP estimate = -1891.5610
Time for alignment: 722.5782
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 37s - loss: 2277.1726 - loglik: -2.2708e+03 - logprior: -6.3239e+00
Epoch 2/10
25/25 - 31s - loss: 2025.4805 - loglik: -2.0244e+03 - logprior: -1.0362e+00
Epoch 3/10
25/25 - 30s - loss: 1966.2205 - loglik: -1.9643e+03 - logprior: -1.8757e+00
Epoch 4/10
25/25 - 30s - loss: 1950.9120 - loglik: -1.9488e+03 - logprior: -2.1017e+00
Epoch 5/10
25/25 - 31s - loss: 1944.6724 - loglik: -1.9425e+03 - logprior: -2.2037e+00
Epoch 6/10
25/25 - 30s - loss: 1950.1483 - loglik: -1.9478e+03 - logprior: -2.3138e+00
Fitted a model with MAP estimate = -1947.1936
expansions: [(0, 2), (52, 1), (132, 1), (144, 1), (148, 1), (164, 1), (165, 1), (171, 1), (176, 2), (177, 2), (178, 2), (192, 1), (193, 2), (194, 4), (195, 1), (196, 1), (199, 1), (200, 3), (201, 1), (203, 1), (204, 2), (206, 1), (207, 1), (209, 1), (210, 1), (212, 1), (213, 1), (214, 1), (216, 1), (218, 1), (219, 1), (223, 1), (224, 1), (225, 2), (226, 1), (227, 1), (228, 2), (229, 1), (230, 1), (234, 1), (236, 2), (237, 1), (248, 1), (250, 1), (254, 3), (255, 3), (257, 1), (258, 1), (266, 1), (280, 1), (281, 2), (282, 1), (298, 1), (300, 2), (301, 1), (315, 1), (317, 5), (318, 2), (323, 1), (326, 1), (351, 2), (358, 1), (360, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 460 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 49s - loss: 1930.3789 - loglik: -1.9239e+03 - logprior: -6.4761e+00
Epoch 2/2
25/25 - 44s - loss: 1895.4126 - loglik: -1.8949e+03 - logprior: -4.7952e-01
Fitted a model with MAP estimate = -1890.9702
expansions: [(434, 1)]
discards: [  2 186 213 234 313 314 372 436 437]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 47s - loss: 1897.0934 - loglik: -1.8941e+03 - logprior: -3.0136e+00
Epoch 2/2
25/25 - 43s - loss: 1896.5939 - loglik: -1.8974e+03 - logprior: 0.8299
Fitted a model with MAP estimate = -1890.2075
expansions: [(310, 1)]
discards: [178 179 180]
Re-initialized the encoder parameters.
Fitting a model of length 450 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 46s - loss: 1900.1719 - loglik: -1.8973e+03 - logprior: -2.8589e+00
Epoch 2/10
25/25 - 42s - loss: 1897.8015 - loglik: -1.8994e+03 - logprior: 1.5782
Epoch 3/10
25/25 - 42s - loss: 1891.2455 - loglik: -1.8934e+03 - logprior: 2.1673
Epoch 4/10
25/25 - 42s - loss: 1892.9176 - loglik: -1.8953e+03 - logprior: 2.3575
Fitted a model with MAP estimate = -1888.5303
Time for alignment: 692.8934
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 34s - loss: 2274.7041 - loglik: -2.2683e+03 - logprior: -6.4098e+00
Epoch 2/10
25/25 - 31s - loss: 2013.9404 - loglik: -2.0129e+03 - logprior: -1.0687e+00
Epoch 3/10
25/25 - 31s - loss: 1970.6857 - loglik: -1.9689e+03 - logprior: -1.8027e+00
Epoch 4/10
25/25 - 30s - loss: 1955.5741 - loglik: -1.9536e+03 - logprior: -1.9243e+00
Epoch 5/10
25/25 - 31s - loss: 1956.8799 - loglik: -1.9550e+03 - logprior: -1.8713e+00
Fitted a model with MAP estimate = -1953.8331
expansions: [(0, 2), (133, 1), (145, 1), (165, 1), (166, 2), (170, 1), (176, 4), (177, 1), (179, 1), (180, 1), (193, 1), (195, 3), (196, 5), (197, 2), (199, 1), (200, 4), (201, 1), (203, 1), (204, 2), (206, 1), (207, 2), (209, 1), (210, 2), (212, 1), (216, 1), (218, 1), (224, 1), (225, 1), (226, 2), (227, 1), (228, 1), (229, 2), (230, 1), (231, 1), (236, 1), (237, 1), (238, 1), (239, 1), (250, 1), (251, 1), (252, 1), (255, 3), (256, 2), (258, 1), (259, 1), (267, 1), (281, 1), (282, 1), (283, 1), (297, 1), (299, 1), (300, 1), (301, 2), (304, 1), (314, 2), (316, 1), (318, 1), (321, 2), (326, 1), (327, 2), (339, 2), (352, 2), (356, 1), (358, 1), (360, 1), (363, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 465 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 50s - loss: 1944.0441 - loglik: -1.9373e+03 - logprior: -6.7717e+00
Epoch 2/2
25/25 - 45s - loss: 1895.6724 - loglik: -1.8943e+03 - logprior: -1.3286e+00
Fitted a model with MAP estimate = -1894.9398
expansions: [(414, 1)]
discards: [  2 188 189 190 191 218 237 312 376 403 426 441 442]
Re-initialized the encoder parameters.
Fitting a model of length 453 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 47s - loss: 1908.8221 - loglik: -1.9057e+03 - logprior: -3.0765e+00
Epoch 2/2
25/25 - 43s - loss: 1899.7950 - loglik: -1.9009e+03 - logprior: 1.1007
Fitted a model with MAP estimate = -1895.6968
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 453 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 46s - loss: 1904.7993 - loglik: -1.9020e+03 - logprior: -2.8068e+00
Epoch 2/10
25/25 - 43s - loss: 1896.4250 - loglik: -1.8978e+03 - logprior: 1.3625
Epoch 3/10
25/25 - 43s - loss: 1885.7489 - loglik: -1.8879e+03 - logprior: 2.1406
Epoch 4/10
25/25 - 43s - loss: 1887.2690 - loglik: -1.8897e+03 - logprior: 2.3850
Fitted a model with MAP estimate = -1886.7070
Time for alignment: 667.5267
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 34s - loss: 2273.9880 - loglik: -2.2677e+03 - logprior: -6.3095e+00
Epoch 2/10
25/25 - 30s - loss: 2019.2266 - loglik: -2.0179e+03 - logprior: -1.3013e+00
Epoch 3/10
25/25 - 30s - loss: 1957.2427 - loglik: -1.9551e+03 - logprior: -2.1334e+00
Epoch 4/10
25/25 - 30s - loss: 1949.4174 - loglik: -1.9470e+03 - logprior: -2.4032e+00
Epoch 5/10
25/25 - 30s - loss: 1944.7468 - loglik: -1.9423e+03 - logprior: -2.4470e+00
Epoch 6/10
25/25 - 30s - loss: 1946.4167 - loglik: -1.9439e+03 - logprior: -2.5039e+00
Fitted a model with MAP estimate = -1943.2250
expansions: [(0, 2), (43, 1), (88, 2), (124, 1), (130, 1), (132, 1), (133, 1), (140, 1), (160, 1), (161, 2), (166, 1), (167, 1), (171, 5), (172, 2), (173, 2), (176, 1), (187, 1), (188, 1), (190, 3), (191, 1), (192, 1), (195, 1), (196, 1), (197, 1), (198, 1), (200, 1), (201, 1), (203, 1), (205, 1), (207, 1), (208, 1), (209, 1), (212, 1), (214, 1), (216, 1), (222, 1), (223, 1), (224, 2), (225, 1), (226, 1), (228, 1), (229, 1), (230, 1), (234, 1), (236, 2), (237, 1), (248, 1), (250, 1), (254, 3), (255, 2), (257, 1), (258, 1), (266, 1), (280, 1), (282, 1), (299, 1), (301, 1), (302, 2), (316, 1), (318, 2), (319, 2), (321, 2), (325, 2), (327, 2), (350, 1), (353, 1), (358, 1), (360, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 461 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 51s - loss: 1923.0347 - loglik: -1.9166e+03 - logprior: -6.4624e+00
Epoch 2/2
25/25 - 44s - loss: 1899.5518 - loglik: -1.8994e+03 - logprior: -1.3400e-01
Fitted a model with MAP estimate = -1890.4576
expansions: [(439, 1)]
discards: [  2 172 188 189 190 191 218 401 441]
Re-initialized the encoder parameters.
Fitting a model of length 453 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 47s - loss: 1899.5621 - loglik: -1.8964e+03 - logprior: -3.1760e+00
Epoch 2/2
25/25 - 43s - loss: 1895.7485 - loglik: -1.8969e+03 - logprior: 1.1490
Fitted a model with MAP estimate = -1890.5004
expansions: [(186, 4)]
discards: [ 90 431]
Re-initialized the encoder parameters.
Fitting a model of length 455 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 47s - loss: 1903.6090 - loglik: -1.9011e+03 - logprior: -2.4815e+00
Epoch 2/10
25/25 - 43s - loss: 1887.4337 - loglik: -1.8891e+03 - logprior: 1.6675
Epoch 3/10
25/25 - 43s - loss: 1889.5277 - loglik: -1.8916e+03 - logprior: 2.0867
Fitted a model with MAP estimate = -1887.9259
Time for alignment: 652.1235
Computed alignments with likelihoods: ['-1888.5678', '-1891.5610', '-1888.5303', '-1886.7070', '-1887.9259']
Best model has likelihood: -1886.7070  (prior= 2.3669 )
time for generating output: 0.5128
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf1.projection.fasta
SP score = 0.9097284661032189
Training of 5 independent models on file ldh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b0a6c39a0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52730aee50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f9cab310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52d8cd1310>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5c8416b100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5271af5850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526e406df0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52f18bdd90>, <__main__.SimpleDirichletPrior object at 0x7f527335e820>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 672.3688 - loglik: -6.6714e+02 - logprior: -5.2292e+00
Epoch 2/10
16/16 - 4s - loss: 599.8118 - loglik: -5.9824e+02 - logprior: -1.5735e+00
Epoch 3/10
16/16 - 4s - loss: 571.3412 - loglik: -5.6961e+02 - logprior: -1.7343e+00
Epoch 4/10
16/16 - 4s - loss: 561.0391 - loglik: -5.5940e+02 - logprior: -1.6403e+00
Epoch 5/10
16/16 - 4s - loss: 555.6439 - loglik: -5.5401e+02 - logprior: -1.6388e+00
Epoch 6/10
16/16 - 4s - loss: 552.6860 - loglik: -5.5099e+02 - logprior: -1.6965e+00
Epoch 7/10
16/16 - 4s - loss: 552.3712 - loglik: -5.5060e+02 - logprior: -1.7731e+00
Epoch 8/10
16/16 - 4s - loss: 549.3098 - loglik: -5.4744e+02 - logprior: -1.8714e+00
Epoch 9/10
16/16 - 4s - loss: 548.4857 - loglik: -5.4653e+02 - logprior: -1.9596e+00
Epoch 10/10
16/16 - 4s - loss: 547.9136 - loglik: -5.4580e+02 - logprior: -2.1096e+00
Fitted a model with MAP estimate = -546.8580
expansions: [(0, 1), (11, 1), (12, 1), (14, 1), (15, 2), (16, 1), (25, 3), (57, 1), (82, 1), (97, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 125 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 550.7119 - loglik: -5.4682e+02 - logprior: -3.8958e+00
Epoch 2/2
33/33 - 7s - loss: 543.1918 - loglik: -5.4149e+02 - logprior: -1.7013e+00
Fitted a model with MAP estimate = -540.2828
expansions: []
discards: [33 34]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 546.2437 - loglik: -5.4300e+02 - logprior: -3.2462e+00
Epoch 2/2
33/33 - 7s - loss: 540.2509 - loglik: -5.3874e+02 - logprior: -1.5145e+00
Fitted a model with MAP estimate = -540.5622
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 123 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 13s - loss: 545.1019 - loglik: -5.4193e+02 - logprior: -3.1709e+00
Epoch 2/10
33/33 - 7s - loss: 542.2879 - loglik: -5.4092e+02 - logprior: -1.3719e+00
Epoch 3/10
33/33 - 7s - loss: 537.3281 - loglik: -5.3607e+02 - logprior: -1.2554e+00
Epoch 4/10
33/33 - 6s - loss: 540.2110 - loglik: -5.3904e+02 - logprior: -1.1728e+00
Fitted a model with MAP estimate = -537.9567
Time for alignment: 139.5635
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 672.3738 - loglik: -6.6712e+02 - logprior: -5.2533e+00
Epoch 2/10
16/16 - 4s - loss: 601.0804 - loglik: -5.9948e+02 - logprior: -1.6001e+00
Epoch 3/10
16/16 - 4s - loss: 572.9363 - loglik: -5.7120e+02 - logprior: -1.7408e+00
Epoch 4/10
16/16 - 4s - loss: 561.5439 - loglik: -5.5993e+02 - logprior: -1.6100e+00
Epoch 5/10
16/16 - 4s - loss: 556.0037 - loglik: -5.5434e+02 - logprior: -1.6600e+00
Epoch 6/10
16/16 - 4s - loss: 551.2031 - loglik: -5.4929e+02 - logprior: -1.9127e+00
Epoch 7/10
16/16 - 4s - loss: 549.5279 - loglik: -5.4763e+02 - logprior: -1.9012e+00
Epoch 8/10
16/16 - 4s - loss: 547.5403 - loglik: -5.4547e+02 - logprior: -2.0693e+00
Epoch 9/10
16/16 - 4s - loss: 547.0515 - loglik: -5.4494e+02 - logprior: -2.1107e+00
Epoch 10/10
16/16 - 4s - loss: 547.0869 - loglik: -5.4491e+02 - logprior: -2.1794e+00
Fitted a model with MAP estimate = -546.3501
expansions: [(10, 1), (12, 2), (15, 1), (16, 2), (17, 1), (18, 1), (69, 1), (71, 1), (94, 1), (97, 1)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 552.6692 - loglik: -5.4836e+02 - logprior: -4.3135e+00
Epoch 2/2
33/33 - 6s - loss: 542.0441 - loglik: -5.3977e+02 - logprior: -2.2747e+00
Fitted a model with MAP estimate = -539.0469
expansions: [(0, 1), (19, 1)]
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 543.8973 - loglik: -5.4071e+02 - logprior: -3.1857e+00
Epoch 2/2
33/33 - 6s - loss: 537.6626 - loglik: -5.3615e+02 - logprior: -1.5127e+00
Fitted a model with MAP estimate = -537.7189
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 123 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 541.6029 - loglik: -5.3858e+02 - logprior: -3.0202e+00
Epoch 2/10
33/33 - 7s - loss: 538.6285 - loglik: -5.3732e+02 - logprior: -1.3069e+00
Epoch 3/10
33/33 - 6s - loss: 537.2111 - loglik: -5.3600e+02 - logprior: -1.2160e+00
Epoch 4/10
33/33 - 6s - loss: 536.6066 - loglik: -5.3547e+02 - logprior: -1.1374e+00
Epoch 5/10
33/33 - 6s - loss: 534.8461 - loglik: -5.3378e+02 - logprior: -1.0635e+00
Epoch 6/10
33/33 - 6s - loss: 534.3362 - loglik: -5.3327e+02 - logprior: -1.0665e+00
Epoch 7/10
33/33 - 7s - loss: 534.7010 - loglik: -5.3364e+02 - logprior: -1.0595e+00
Fitted a model with MAP estimate = -534.2498
Time for alignment: 154.7047
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 10s - loss: 673.2083 - loglik: -6.6796e+02 - logprior: -5.2516e+00
Epoch 2/10
16/16 - 4s - loss: 601.7000 - loglik: -6.0011e+02 - logprior: -1.5894e+00
Epoch 3/10
16/16 - 4s - loss: 567.6833 - loglik: -5.6595e+02 - logprior: -1.7329e+00
Epoch 4/10
16/16 - 5s - loss: 561.4177 - loglik: -5.5985e+02 - logprior: -1.5638e+00
Epoch 5/10
16/16 - 4s - loss: 553.0416 - loglik: -5.5149e+02 - logprior: -1.5521e+00
Epoch 6/10
16/16 - 4s - loss: 551.2745 - loglik: -5.4967e+02 - logprior: -1.6078e+00
Epoch 7/10
16/16 - 4s - loss: 549.5446 - loglik: -5.4782e+02 - logprior: -1.7280e+00
Epoch 8/10
16/16 - 4s - loss: 545.7491 - loglik: -5.4396e+02 - logprior: -1.7940e+00
Epoch 9/10
16/16 - 4s - loss: 545.7550 - loglik: -5.4390e+02 - logprior: -1.8516e+00
Fitted a model with MAP estimate = -546.2571
expansions: [(0, 1), (12, 1), (15, 1), (16, 2), (17, 1), (18, 1), (73, 1), (97, 1), (98, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 550.4690 - loglik: -5.4642e+02 - logprior: -4.0525e+00
Epoch 2/2
33/33 - 6s - loss: 543.9941 - loglik: -5.4240e+02 - logprior: -1.5913e+00
Fitted a model with MAP estimate = -541.0549
expansions: [(20, 1)]
discards: [24]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 545.4990 - loglik: -5.4224e+02 - logprior: -3.2575e+00
Epoch 2/2
33/33 - 6s - loss: 542.0295 - loglik: -5.4057e+02 - logprior: -1.4564e+00
Fitted a model with MAP estimate = -540.7225
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 544.7540 - loglik: -5.4162e+02 - logprior: -3.1389e+00
Epoch 2/10
33/33 - 7s - loss: 541.6636 - loglik: -5.4034e+02 - logprior: -1.3244e+00
Epoch 3/10
33/33 - 6s - loss: 540.2454 - loglik: -5.3903e+02 - logprior: -1.2129e+00
Epoch 4/10
33/33 - 7s - loss: 540.0767 - loglik: -5.3893e+02 - logprior: -1.1509e+00
Epoch 5/10
33/33 - 6s - loss: 536.0933 - loglik: -5.3504e+02 - logprior: -1.0542e+00
Epoch 6/10
33/33 - 7s - loss: 536.7358 - loglik: -5.3567e+02 - logprior: -1.0677e+00
Fitted a model with MAP estimate = -535.6268
Time for alignment: 148.1049
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 673.3709 - loglik: -6.6812e+02 - logprior: -5.2537e+00
Epoch 2/10
16/16 - 4s - loss: 602.8247 - loglik: -6.0123e+02 - logprior: -1.5935e+00
Epoch 3/10
16/16 - 5s - loss: 567.7405 - loglik: -5.6596e+02 - logprior: -1.7816e+00
Epoch 4/10
16/16 - 4s - loss: 561.1492 - loglik: -5.5937e+02 - logprior: -1.7822e+00
Epoch 5/10
16/16 - 4s - loss: 555.8735 - loglik: -5.5419e+02 - logprior: -1.6878e+00
Epoch 6/10
16/16 - 4s - loss: 552.0947 - loglik: -5.5033e+02 - logprior: -1.7635e+00
Epoch 7/10
16/16 - 4s - loss: 546.6613 - loglik: -5.4478e+02 - logprior: -1.8817e+00
Epoch 8/10
16/16 - 4s - loss: 546.5473 - loglik: -5.4454e+02 - logprior: -2.0088e+00
Epoch 9/10
16/16 - 4s - loss: 545.7902 - loglik: -5.4371e+02 - logprior: -2.0795e+00
Epoch 10/10
16/16 - 4s - loss: 544.2733 - loglik: -5.4213e+02 - logprior: -2.1451e+00
Fitted a model with MAP estimate = -544.2818
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 2), (17, 1), (18, 1), (76, 1), (97, 1), (98, 1)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 551.4213 - loglik: -5.4710e+02 - logprior: -4.3209e+00
Epoch 2/2
33/33 - 6s - loss: 542.0436 - loglik: -5.3979e+02 - logprior: -2.2489e+00
Fitted a model with MAP estimate = -539.3155
expansions: [(0, 1), (21, 1)]
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 543.3041 - loglik: -5.4016e+02 - logprior: -3.1443e+00
Epoch 2/2
33/33 - 6s - loss: 538.2362 - loglik: -5.3678e+02 - logprior: -1.4562e+00
Fitted a model with MAP estimate = -537.9750
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 541.5399 - loglik: -5.3856e+02 - logprior: -2.9761e+00
Epoch 2/10
33/33 - 6s - loss: 540.6082 - loglik: -5.3934e+02 - logprior: -1.2701e+00
Epoch 3/10
33/33 - 6s - loss: 536.0139 - loglik: -5.3483e+02 - logprior: -1.1880e+00
Epoch 4/10
33/33 - 7s - loss: 537.0209 - loglik: -5.3590e+02 - logprior: -1.1209e+00
Fitted a model with MAP estimate = -535.5240
Time for alignment: 138.3755
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 673.7234 - loglik: -6.6847e+02 - logprior: -5.2512e+00
Epoch 2/10
16/16 - 4s - loss: 598.9150 - loglik: -5.9734e+02 - logprior: -1.5790e+00
Epoch 3/10
16/16 - 4s - loss: 571.5011 - loglik: -5.6974e+02 - logprior: -1.7629e+00
Epoch 4/10
16/16 - 4s - loss: 558.8392 - loglik: -5.5719e+02 - logprior: -1.6464e+00
Epoch 5/10
16/16 - 4s - loss: 553.5243 - loglik: -5.5190e+02 - logprior: -1.6265e+00
Epoch 6/10
16/16 - 4s - loss: 550.6005 - loglik: -5.4892e+02 - logprior: -1.6768e+00
Epoch 7/10
16/16 - 4s - loss: 552.4646 - loglik: -5.5071e+02 - logprior: -1.7531e+00
Fitted a model with MAP estimate = -548.2514
expansions: [(0, 1), (12, 1), (15, 1), (16, 2), (17, 2), (18, 1), (25, 3), (57, 1), (82, 1), (97, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 126 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 550.7452 - loglik: -5.4669e+02 - logprior: -4.0534e+00
Epoch 2/2
33/33 - 6s - loss: 542.8737 - loglik: -5.4118e+02 - logprior: -1.6986e+00
Fitted a model with MAP estimate = -538.9951
expansions: []
discards: [25 34 35]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 543.5071 - loglik: -5.4018e+02 - logprior: -3.3315e+00
Epoch 2/2
33/33 - 7s - loss: 539.3511 - loglik: -5.3783e+02 - logprior: -1.5257e+00
Fitted a model with MAP estimate = -538.1790
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 123 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 541.9296 - loglik: -5.3872e+02 - logprior: -3.2110e+00
Epoch 2/10
33/33 - 6s - loss: 538.6601 - loglik: -5.3727e+02 - logprior: -1.3857e+00
Epoch 3/10
33/33 - 7s - loss: 539.5523 - loglik: -5.3828e+02 - logprior: -1.2678e+00
Fitted a model with MAP estimate = -536.6850
Time for alignment: 120.0396
Computed alignments with likelihoods: ['-537.9567', '-534.2498', '-535.6268', '-535.5240', '-536.6850']
Best model has likelihood: -534.2498  (prior= -1.0810 )
time for generating output: 0.3555
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ldh.projection.fasta
SP score = 0.5375557668397571
Training of 5 independent models on file Rhodanese.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f527e8383d0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5271df3070>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52876ab820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5272ecf760>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52689e1b80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5900e94e20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f9cabac0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f528422d610>, <__main__.SimpleDirichletPrior object at 0x7f5c844aea00>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 622.1769 - loglik: -6.1872e+02 - logprior: -3.4556e+00
Epoch 2/10
19/19 - 3s - loss: 590.9586 - loglik: -5.8979e+02 - logprior: -1.1668e+00
Epoch 3/10
19/19 - 3s - loss: 575.3104 - loglik: -5.7383e+02 - logprior: -1.4762e+00
Epoch 4/10
19/19 - 3s - loss: 572.1232 - loglik: -5.7071e+02 - logprior: -1.4132e+00
Epoch 5/10
19/19 - 3s - loss: 570.1360 - loglik: -5.6871e+02 - logprior: -1.4285e+00
Epoch 6/10
19/19 - 3s - loss: 570.3587 - loglik: -5.6892e+02 - logprior: -1.4387e+00
Fitted a model with MAP estimate = -538.7526
expansions: [(6, 3), (7, 2), (8, 1), (10, 1), (34, 5), (39, 3), (40, 1), (42, 3), (55, 1), (58, 1), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 573.8307 - loglik: -5.6978e+02 - logprior: -4.0557e+00
Epoch 2/2
19/19 - 3s - loss: 565.0660 - loglik: -5.6311e+02 - logprior: -1.9595e+00
Fitted a model with MAP estimate = -532.9860
expansions: [(0, 2), (49, 2), (56, 1)]
discards: [ 0  9 11 85]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 564.9902 - loglik: -5.6208e+02 - logprior: -2.9068e+00
Epoch 2/2
19/19 - 3s - loss: 561.2668 - loglik: -5.6017e+02 - logprior: -1.0950e+00
Fitted a model with MAP estimate = -530.6019
expansions: []
discards: [ 0 48 49 60]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 533.3122 - loglik: -5.3084e+02 - logprior: -2.4746e+00
Epoch 2/10
23/23 - 4s - loss: 530.1951 - loglik: -5.2918e+02 - logprior: -1.0174e+00
Epoch 3/10
23/23 - 3s - loss: 528.3680 - loglik: -5.2738e+02 - logprior: -9.8791e-01
Epoch 4/10
23/23 - 4s - loss: 529.1285 - loglik: -5.2816e+02 - logprior: -9.7048e-01
Fitted a model with MAP estimate = -527.6764
Time for alignment: 77.1437
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 622.3707 - loglik: -6.1892e+02 - logprior: -3.4552e+00
Epoch 2/10
19/19 - 3s - loss: 590.4577 - loglik: -5.8931e+02 - logprior: -1.1523e+00
Epoch 3/10
19/19 - 3s - loss: 576.2885 - loglik: -5.7483e+02 - logprior: -1.4543e+00
Epoch 4/10
19/19 - 3s - loss: 571.6581 - loglik: -5.7023e+02 - logprior: -1.4268e+00
Epoch 5/10
19/19 - 3s - loss: 570.4012 - loglik: -5.6896e+02 - logprior: -1.4362e+00
Epoch 6/10
19/19 - 3s - loss: 568.9662 - loglik: -5.6751e+02 - logprior: -1.4564e+00
Epoch 7/10
19/19 - 3s - loss: 568.9949 - loglik: -5.6751e+02 - logprior: -1.4873e+00
Fitted a model with MAP estimate = -538.3004
expansions: [(6, 3), (7, 2), (10, 2), (34, 9), (39, 2), (40, 1), (43, 2), (55, 1), (58, 2), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 574.2372 - loglik: -5.7012e+02 - logprior: -4.1163e+00
Epoch 2/2
19/19 - 3s - loss: 564.6687 - loglik: -5.6261e+02 - logprior: -2.0563e+00
Fitted a model with MAP estimate = -532.5599
expansions: [(0, 2), (46, 2)]
discards: [ 0  9 14 54 80 87]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 564.8074 - loglik: -5.6188e+02 - logprior: -2.9309e+00
Epoch 2/2
19/19 - 3s - loss: 560.9670 - loglik: -5.5984e+02 - logprior: -1.1301e+00
Fitted a model with MAP estimate = -530.4714
expansions: []
discards: [ 0 46 47 48]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 533.3352 - loglik: -5.3081e+02 - logprior: -2.5230e+00
Epoch 2/10
23/23 - 3s - loss: 529.9828 - loglik: -5.2895e+02 - logprior: -1.0320e+00
Epoch 3/10
23/23 - 4s - loss: 527.5106 - loglik: -5.2652e+02 - logprior: -9.9525e-01
Epoch 4/10
23/23 - 3s - loss: 528.7656 - loglik: -5.2780e+02 - logprior: -9.6988e-01
Fitted a model with MAP estimate = -527.3932
Time for alignment: 86.8841
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 622.3536 - loglik: -6.1890e+02 - logprior: -3.4518e+00
Epoch 2/10
19/19 - 3s - loss: 591.0156 - loglik: -5.8987e+02 - logprior: -1.1480e+00
Epoch 3/10
19/19 - 3s - loss: 576.2657 - loglik: -5.7483e+02 - logprior: -1.4402e+00
Epoch 4/10
19/19 - 3s - loss: 572.8221 - loglik: -5.7139e+02 - logprior: -1.4284e+00
Epoch 5/10
19/19 - 3s - loss: 570.8007 - loglik: -5.6935e+02 - logprior: -1.4470e+00
Epoch 6/10
19/19 - 3s - loss: 570.7557 - loglik: -5.6926e+02 - logprior: -1.4960e+00
Epoch 7/10
19/19 - 3s - loss: 570.4097 - loglik: -5.6889e+02 - logprior: -1.5206e+00
Epoch 8/10
19/19 - 3s - loss: 569.4641 - loglik: -5.6791e+02 - logprior: -1.5575e+00
Epoch 9/10
19/19 - 3s - loss: 568.8947 - loglik: -5.6728e+02 - logprior: -1.6160e+00
Epoch 10/10
19/19 - 3s - loss: 568.2607 - loglik: -5.6657e+02 - logprior: -1.6867e+00
Fitted a model with MAP estimate = -540.1416
expansions: [(6, 3), (7, 2), (10, 2), (21, 2), (33, 8), (39, 2), (40, 1), (43, 2), (58, 1), (59, 1), (60, 1), (63, 2), (66, 3), (67, 1), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 575.9025 - loglik: -5.7166e+02 - logprior: -4.2395e+00
Epoch 2/2
19/19 - 3s - loss: 564.7554 - loglik: -5.6262e+02 - logprior: -2.1322e+00
Fitted a model with MAP estimate = -532.7934
expansions: [(0, 2)]
discards: [ 0  8 14 28 45 46 47 48 49 56 88 93]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 566.6375 - loglik: -5.6374e+02 - logprior: -2.8958e+00
Epoch 2/2
19/19 - 3s - loss: 563.1418 - loglik: -5.6202e+02 - logprior: -1.1171e+00
Fitted a model with MAP estimate = -531.8564
expansions: [(43, 7)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 534.6668 - loglik: -5.3206e+02 - logprior: -2.6100e+00
Epoch 2/10
23/23 - 3s - loss: 528.6258 - loglik: -5.2755e+02 - logprior: -1.0739e+00
Epoch 3/10
23/23 - 3s - loss: 528.7433 - loglik: -5.2776e+02 - logprior: -9.8325e-01
Fitted a model with MAP estimate = -527.6847
Time for alignment: 89.4562
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 622.3517 - loglik: -6.1890e+02 - logprior: -3.4551e+00
Epoch 2/10
19/19 - 3s - loss: 590.0168 - loglik: -5.8887e+02 - logprior: -1.1462e+00
Epoch 3/10
19/19 - 3s - loss: 574.6663 - loglik: -5.7321e+02 - logprior: -1.4521e+00
Epoch 4/10
19/19 - 3s - loss: 571.7116 - loglik: -5.7028e+02 - logprior: -1.4305e+00
Epoch 5/10
19/19 - 3s - loss: 569.5014 - loglik: -5.6807e+02 - logprior: -1.4326e+00
Epoch 6/10
19/19 - 3s - loss: 569.9633 - loglik: -5.6851e+02 - logprior: -1.4569e+00
Fitted a model with MAP estimate = -538.4931
expansions: [(6, 3), (7, 2), (8, 1), (10, 1), (21, 2), (33, 9), (38, 2), (39, 1), (42, 2), (58, 1), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 573.7336 - loglik: -5.6964e+02 - logprior: -4.0896e+00
Epoch 2/2
19/19 - 3s - loss: 564.8212 - loglik: -5.6279e+02 - logprior: -2.0336e+00
Fitted a model with MAP estimate = -532.4371
expansions: [(0, 2), (47, 2)]
discards: [ 0  9 11 28 55 88]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 564.5908 - loglik: -5.6166e+02 - logprior: -2.9346e+00
Epoch 2/2
19/19 - 3s - loss: 561.4059 - loglik: -5.6027e+02 - logprior: -1.1325e+00
Fitted a model with MAP estimate = -530.2972
expansions: []
discards: [ 0 46 47 48 49]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 533.2851 - loglik: -5.3076e+02 - logprior: -2.5267e+00
Epoch 2/10
23/23 - 3s - loss: 530.1860 - loglik: -5.2916e+02 - logprior: -1.0272e+00
Epoch 3/10
23/23 - 3s - loss: 528.8742 - loglik: -5.2788e+02 - logprior: -9.9719e-01
Epoch 4/10
23/23 - 3s - loss: 528.6824 - loglik: -5.2771e+02 - logprior: -9.6846e-01
Epoch 5/10
23/23 - 4s - loss: 526.8628 - loglik: -5.2588e+02 - logprior: -9.8386e-01
Epoch 6/10
23/23 - 3s - loss: 527.1968 - loglik: -5.2617e+02 - logprior: -1.0252e+00
Fitted a model with MAP estimate = -526.4238
Time for alignment: 89.4663
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 622.4292 - loglik: -6.1898e+02 - logprior: -3.4528e+00
Epoch 2/10
19/19 - 3s - loss: 590.7570 - loglik: -5.8960e+02 - logprior: -1.1590e+00
Epoch 3/10
19/19 - 3s - loss: 575.6600 - loglik: -5.7418e+02 - logprior: -1.4836e+00
Epoch 4/10
19/19 - 3s - loss: 572.1388 - loglik: -5.7069e+02 - logprior: -1.4477e+00
Epoch 5/10
19/19 - 3s - loss: 570.5277 - loglik: -5.6907e+02 - logprior: -1.4536e+00
Epoch 6/10
19/19 - 3s - loss: 570.3358 - loglik: -5.6886e+02 - logprior: -1.4770e+00
Epoch 7/10
19/19 - 3s - loss: 569.2507 - loglik: -5.6774e+02 - logprior: -1.5119e+00
Epoch 8/10
19/19 - 3s - loss: 569.3022 - loglik: -5.6776e+02 - logprior: -1.5414e+00
Fitted a model with MAP estimate = -538.9226
expansions: [(6, 3), (7, 2), (10, 1), (21, 2), (36, 6), (39, 2), (40, 1), (43, 2), (55, 1), (58, 2), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 574.4419 - loglik: -5.7031e+02 - logprior: -4.1361e+00
Epoch 2/2
19/19 - 3s - loss: 565.6460 - loglik: -5.6358e+02 - logprior: -2.0628e+00
Fitted a model with MAP estimate = -533.2928
expansions: [(0, 2)]
discards: [ 0  9 27 45 46 47 48 52 78 85]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 566.7355 - loglik: -5.6385e+02 - logprior: -2.8838e+00
Epoch 2/2
19/19 - 3s - loss: 563.6368 - loglik: -5.6253e+02 - logprior: -1.1084e+00
Fitted a model with MAP estimate = -532.0556
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 534.1736 - loglik: -5.3169e+02 - logprior: -2.4843e+00
Epoch 2/10
23/23 - 3s - loss: 530.2252 - loglik: -5.2924e+02 - logprior: -9.8198e-01
Epoch 3/10
23/23 - 3s - loss: 531.1793 - loglik: -5.3024e+02 - logprior: -9.3704e-01
Fitted a model with MAP estimate = -529.2922
Time for alignment: 79.5882
Computed alignments with likelihoods: ['-527.6764', '-527.3932', '-527.6847', '-526.4238', '-529.2922']
Best model has likelihood: -526.4238  (prior= -1.0394 )
time for generating output: 0.2059
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Rhodanese.projection.fasta
SP score = 0.7781995661605207
Training of 5 independent models on file slectin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f527e9cc7c0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5271a48d60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52852289d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b082de760>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52854156a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5cf3a26a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f531c2b3eb0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52e865a3a0>, <__main__.SimpleDirichletPrior object at 0x7f5264b36880>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 831.9282 - loglik: -7.7875e+02 - logprior: -5.3181e+01
Epoch 2/10
10/10 - 2s - loss: 763.6291 - loglik: -7.5164e+02 - logprior: -1.1989e+01
Epoch 3/10
10/10 - 2s - loss: 729.6308 - loglik: -7.2492e+02 - logprior: -4.7121e+00
Epoch 4/10
10/10 - 2s - loss: 704.9962 - loglik: -7.0262e+02 - logprior: -2.3756e+00
Epoch 5/10
10/10 - 2s - loss: 695.3589 - loglik: -6.9401e+02 - logprior: -1.3486e+00
Epoch 6/10
10/10 - 2s - loss: 691.5128 - loglik: -6.9068e+02 - logprior: -8.3615e-01
Epoch 7/10
10/10 - 2s - loss: 689.6632 - loglik: -6.8922e+02 - logprior: -4.3919e-01
Epoch 8/10
10/10 - 2s - loss: 688.8177 - loglik: -6.8863e+02 - logprior: -1.9265e-01
Epoch 9/10
10/10 - 2s - loss: 689.0020 - loglik: -6.8897e+02 - logprior: -3.5586e-02
Fitted a model with MAP estimate = -687.8355
expansions: [(10, 4), (17, 1), (18, 1), (27, 1), (28, 3), (40, 2), (48, 1), (49, 1), (59, 1), (62, 2), (64, 1), (72, 1), (73, 1), (84, 1), (91, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 740.8135 - loglik: -6.8819e+02 - logprior: -5.2622e+01
Epoch 2/2
10/10 - 3s - loss: 697.4570 - loglik: -6.7797e+02 - logprior: -1.9487e+01
Fitted a model with MAP estimate = -689.9968
expansions: []
discards: [ 0 34 50 76]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 727.2904 - loglik: -6.7755e+02 - logprior: -4.9744e+01
Epoch 2/2
10/10 - 2s - loss: 690.6261 - loglik: -6.7418e+02 - logprior: -1.6451e+01
Fitted a model with MAP estimate = -682.3867
expansions: [(2, 2), (9, 1), (12, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 723.7247 - loglik: -6.7352e+02 - logprior: -5.0202e+01
Epoch 2/10
10/10 - 2s - loss: 689.1965 - loglik: -6.7096e+02 - logprior: -1.8238e+01
Epoch 3/10
10/10 - 3s - loss: 681.4125 - loglik: -6.6996e+02 - logprior: -1.1452e+01
Epoch 4/10
10/10 - 2s - loss: 675.6700 - loglik: -6.6928e+02 - logprior: -6.3853e+00
Epoch 5/10
10/10 - 2s - loss: 667.2216 - loglik: -6.6791e+02 - logprior: 0.6906
Epoch 6/10
10/10 - 2s - loss: 666.3634 - loglik: -6.6974e+02 - logprior: 3.3726
Epoch 7/10
10/10 - 3s - loss: 664.5313 - loglik: -6.6872e+02 - logprior: 4.1882
Epoch 8/10
10/10 - 2s - loss: 664.1134 - loglik: -6.6880e+02 - logprior: 4.6876
Epoch 9/10
10/10 - 2s - loss: 663.5258 - loglik: -6.6854e+02 - logprior: 5.0131
Epoch 10/10
10/10 - 2s - loss: 664.0516 - loglik: -6.6929e+02 - logprior: 5.2397
Fitted a model with MAP estimate = -663.5063
Time for alignment: 75.7892
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 831.5848 - loglik: -7.7841e+02 - logprior: -5.3179e+01
Epoch 2/10
10/10 - 2s - loss: 763.9908 - loglik: -7.5200e+02 - logprior: -1.1986e+01
Epoch 3/10
10/10 - 2s - loss: 725.7552 - loglik: -7.2101e+02 - logprior: -4.7470e+00
Epoch 4/10
10/10 - 2s - loss: 703.7845 - loglik: -7.0135e+02 - logprior: -2.4363e+00
Epoch 5/10
10/10 - 2s - loss: 693.9874 - loglik: -6.9267e+02 - logprior: -1.3174e+00
Epoch 6/10
10/10 - 2s - loss: 690.5484 - loglik: -6.8981e+02 - logprior: -7.3721e-01
Epoch 7/10
10/10 - 2s - loss: 688.1491 - loglik: -6.8779e+02 - logprior: -3.6025e-01
Epoch 8/10
10/10 - 2s - loss: 688.0361 - loglik: -6.8792e+02 - logprior: -1.1707e-01
Epoch 9/10
10/10 - 2s - loss: 687.1194 - loglik: -6.8717e+02 - logprior: 0.0502
Epoch 10/10
10/10 - 2s - loss: 686.9364 - loglik: -6.8710e+02 - logprior: 0.1654
Fitted a model with MAP estimate = -686.4451
expansions: [(7, 2), (10, 1), (17, 1), (18, 1), (29, 3), (41, 1), (49, 2), (58, 1), (59, 2), (62, 1), (65, 1), (73, 1), (82, 1), (90, 2), (91, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 739.4882 - loglik: -6.8689e+02 - logprior: -5.2598e+01
Epoch 2/2
10/10 - 2s - loss: 697.7646 - loglik: -6.7836e+02 - logprior: -1.9409e+01
Fitted a model with MAP estimate = -690.3619
expansions: [(9, 3)]
discards: [  0  57  70 108]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 725.5516 - loglik: -6.7576e+02 - logprior: -4.9788e+01
Epoch 2/2
10/10 - 2s - loss: 691.1974 - loglik: -6.7434e+02 - logprior: -1.6862e+01
Fitted a model with MAP estimate = -682.9762
expansions: [(0, 3)]
discards: [ 0 36]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 711.5577 - loglik: -6.7262e+02 - logprior: -3.8937e+01
Epoch 2/10
10/10 - 2s - loss: 677.5581 - loglik: -6.7017e+02 - logprior: -7.3862e+00
Epoch 3/10
10/10 - 3s - loss: 670.4283 - loglik: -6.6949e+02 - logprior: -9.3715e-01
Epoch 4/10
10/10 - 2s - loss: 667.5584 - loglik: -6.6926e+02 - logprior: 1.7017
Epoch 5/10
10/10 - 2s - loss: 666.4264 - loglik: -6.6958e+02 - logprior: 3.1552
Epoch 6/10
10/10 - 2s - loss: 664.5568 - loglik: -6.6855e+02 - logprior: 3.9889
Epoch 7/10
10/10 - 3s - loss: 664.8833 - loglik: -6.6938e+02 - logprior: 4.4980
Fitted a model with MAP estimate = -664.5335
Time for alignment: 69.2361
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 831.2736 - loglik: -7.7809e+02 - logprior: -5.3183e+01
Epoch 2/10
10/10 - 2s - loss: 763.3364 - loglik: -7.5136e+02 - logprior: -1.1977e+01
Epoch 3/10
10/10 - 2s - loss: 722.8035 - loglik: -7.1811e+02 - logprior: -4.6928e+00
Epoch 4/10
10/10 - 2s - loss: 704.1727 - loglik: -7.0173e+02 - logprior: -2.4470e+00
Epoch 5/10
10/10 - 2s - loss: 693.5879 - loglik: -6.9227e+02 - logprior: -1.3156e+00
Epoch 6/10
10/10 - 2s - loss: 691.2756 - loglik: -6.9056e+02 - logprior: -7.1890e-01
Epoch 7/10
10/10 - 2s - loss: 689.1597 - loglik: -6.8880e+02 - logprior: -3.6142e-01
Epoch 8/10
10/10 - 2s - loss: 688.1504 - loglik: -6.8807e+02 - logprior: -8.0058e-02
Epoch 9/10
10/10 - 2s - loss: 687.9812 - loglik: -6.8806e+02 - logprior: 0.0751
Epoch 10/10
10/10 - 2s - loss: 687.1428 - loglik: -6.8731e+02 - logprior: 0.1667
Fitted a model with MAP estimate = -687.0798
expansions: [(7, 2), (10, 2), (17, 1), (18, 1), (26, 2), (28, 2), (48, 2), (59, 3), (62, 1), (71, 1), (72, 1), (85, 1), (92, 6), (93, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 740.4276 - loglik: -6.8781e+02 - logprior: -5.2618e+01
Epoch 2/2
10/10 - 2s - loss: 697.2930 - loglik: -6.7778e+02 - logprior: -1.9512e+01
Fitted a model with MAP estimate = -690.3643
expansions: [(9, 2)]
discards: [ 0 11 57 71]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 726.6579 - loglik: -6.7678e+02 - logprior: -4.9878e+01
Epoch 2/2
10/10 - 2s - loss: 691.5364 - loglik: -6.7451e+02 - logprior: -1.7024e+01
Fitted a model with MAP estimate = -683.1089
expansions: [(0, 3), (9, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 711.1580 - loglik: -6.7208e+02 - logprior: -3.9079e+01
Epoch 2/10
10/10 - 3s - loss: 676.7062 - loglik: -6.6930e+02 - logprior: -7.4085e+00
Epoch 3/10
10/10 - 2s - loss: 670.2094 - loglik: -6.6927e+02 - logprior: -9.3742e-01
Epoch 4/10
10/10 - 2s - loss: 666.0074 - loglik: -6.6771e+02 - logprior: 1.6983
Epoch 5/10
10/10 - 3s - loss: 665.7006 - loglik: -6.6884e+02 - logprior: 3.1373
Epoch 6/10
10/10 - 2s - loss: 664.2106 - loglik: -6.6818e+02 - logprior: 3.9730
Epoch 7/10
10/10 - 2s - loss: 664.0514 - loglik: -6.6853e+02 - logprior: 4.4806
Epoch 8/10
10/10 - 2s - loss: 663.5503 - loglik: -6.6838e+02 - logprior: 4.8257
Epoch 9/10
10/10 - 3s - loss: 663.7964 - loglik: -6.6891e+02 - logprior: 5.1148
Fitted a model with MAP estimate = -663.3256
Time for alignment: 73.8586
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 832.6849 - loglik: -7.7951e+02 - logprior: -5.3180e+01
Epoch 2/10
10/10 - 2s - loss: 762.6005 - loglik: -7.5062e+02 - logprior: -1.1983e+01
Epoch 3/10
10/10 - 2s - loss: 728.2695 - loglik: -7.2362e+02 - logprior: -4.6540e+00
Epoch 4/10
10/10 - 2s - loss: 703.9932 - loglik: -7.0170e+02 - logprior: -2.2971e+00
Epoch 5/10
10/10 - 2s - loss: 694.0356 - loglik: -6.9263e+02 - logprior: -1.4039e+00
Epoch 6/10
10/10 - 2s - loss: 690.1678 - loglik: -6.8921e+02 - logprior: -9.5320e-01
Epoch 7/10
10/10 - 2s - loss: 689.5400 - loglik: -6.8906e+02 - logprior: -4.7867e-01
Epoch 8/10
10/10 - 2s - loss: 687.0003 - loglik: -6.8684e+02 - logprior: -1.6464e-01
Epoch 9/10
10/10 - 2s - loss: 687.4002 - loglik: -6.8733e+02 - logprior: -7.3511e-02
Fitted a model with MAP estimate = -686.8386
expansions: [(7, 2), (10, 2), (17, 1), (18, 1), (26, 1), (28, 4), (42, 1), (58, 2), (59, 1), (62, 1), (64, 1), (68, 1), (72, 1), (90, 2), (91, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 740.4291 - loglik: -6.8771e+02 - logprior: -5.2719e+01
Epoch 2/2
10/10 - 2s - loss: 697.4825 - loglik: -6.7802e+02 - logprior: -1.9466e+01
Fitted a model with MAP estimate = -690.6020
expansions: [(9, 2)]
discards: [  0  11  70 109]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 728.2596 - loglik: -6.7847e+02 - logprior: -4.9792e+01
Epoch 2/2
10/10 - 2s - loss: 689.6320 - loglik: -6.7275e+02 - logprior: -1.6885e+01
Fitted a model with MAP estimate = -682.7865
expansions: [(0, 3), (9, 1)]
discards: [ 0 34]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 711.6749 - loglik: -6.7262e+02 - logprior: -3.9052e+01
Epoch 2/10
10/10 - 3s - loss: 676.9193 - loglik: -6.6951e+02 - logprior: -7.4091e+00
Epoch 3/10
10/10 - 2s - loss: 669.8929 - loglik: -6.6894e+02 - logprior: -9.5202e-01
Epoch 4/10
10/10 - 2s - loss: 666.4257 - loglik: -6.6811e+02 - logprior: 1.6808
Epoch 5/10
10/10 - 2s - loss: 665.8632 - loglik: -6.6900e+02 - logprior: 3.1341
Epoch 6/10
10/10 - 2s - loss: 664.7561 - loglik: -6.6872e+02 - logprior: 3.9668
Epoch 7/10
10/10 - 2s - loss: 663.8751 - loglik: -6.6835e+02 - logprior: 4.4765
Epoch 8/10
10/10 - 2s - loss: 664.0825 - loglik: -6.6891e+02 - logprior: 4.8271
Fitted a model with MAP estimate = -663.6093
Time for alignment: 66.4132
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 832.5562 - loglik: -7.7937e+02 - logprior: -5.3184e+01
Epoch 2/10
10/10 - 2s - loss: 762.7916 - loglik: -7.5080e+02 - logprior: -1.1990e+01
Epoch 3/10
10/10 - 2s - loss: 726.8749 - loglik: -7.2221e+02 - logprior: -4.6684e+00
Epoch 4/10
10/10 - 2s - loss: 703.0106 - loglik: -7.0053e+02 - logprior: -2.4772e+00
Epoch 5/10
10/10 - 2s - loss: 692.4880 - loglik: -6.9094e+02 - logprior: -1.5475e+00
Epoch 6/10
10/10 - 2s - loss: 690.0633 - loglik: -6.8905e+02 - logprior: -1.0133e+00
Epoch 7/10
10/10 - 2s - loss: 688.3267 - loglik: -6.8782e+02 - logprior: -5.0882e-01
Epoch 8/10
10/10 - 2s - loss: 687.1095 - loglik: -6.8695e+02 - logprior: -1.6164e-01
Epoch 9/10
10/10 - 2s - loss: 686.5801 - loglik: -6.8658e+02 - logprior: -3.7788e-03
Epoch 10/10
10/10 - 2s - loss: 686.2628 - loglik: -6.8638e+02 - logprior: 0.1157
Fitted a model with MAP estimate = -686.2986
expansions: [(12, 1), (13, 1), (16, 1), (19, 1), (26, 1), (27, 4), (48, 2), (58, 3), (61, 1), (64, 1), (72, 1), (84, 1), (91, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 738.6462 - loglik: -6.8618e+02 - logprior: -5.2471e+01
Epoch 2/2
10/10 - 2s - loss: 698.6811 - loglik: -6.7931e+02 - logprior: -1.9373e+01
Fitted a model with MAP estimate = -691.2750
expansions: []
discards: [ 0 56 69]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 728.7912 - loglik: -6.7918e+02 - logprior: -4.9614e+01
Epoch 2/2
10/10 - 2s - loss: 692.3383 - loglik: -6.7621e+02 - logprior: -1.6125e+01
Fitted a model with MAP estimate = -684.7602
expansions: [(0, 6), (5, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 714.7464 - loglik: -6.7523e+02 - logprior: -3.9516e+01
Epoch 2/10
10/10 - 2s - loss: 678.6559 - loglik: -6.7103e+02 - logprior: -7.6251e+00
Epoch 3/10
10/10 - 2s - loss: 671.3415 - loglik: -6.7020e+02 - logprior: -1.1366e+00
Epoch 4/10
10/10 - 2s - loss: 667.4067 - loglik: -6.6888e+02 - logprior: 1.4750
Epoch 5/10
10/10 - 2s - loss: 666.9063 - loglik: -6.6980e+02 - logprior: 2.8981
Epoch 6/10
10/10 - 3s - loss: 665.0699 - loglik: -6.6883e+02 - logprior: 3.7581
Epoch 7/10
10/10 - 2s - loss: 664.5716 - loglik: -6.6885e+02 - logprior: 4.2786
Epoch 8/10
10/10 - 2s - loss: 664.5762 - loglik: -6.6921e+02 - logprior: 4.6293
Fitted a model with MAP estimate = -664.3154
Time for alignment: 70.7056
Computed alignments with likelihoods: ['-663.5063', '-664.5335', '-663.3256', '-663.6093', '-664.3154']
Best model has likelihood: -663.3256  (prior= 5.2574 )
time for generating output: 0.1703
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/slectin.projection.fasta
SP score = 0.880123743232792
Training of 5 independent models on file hip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5b082a57c0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5276902460>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5276902fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f527693cca0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5268ef2f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f8103cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52d97f4bb0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5301dd71c0>, <__main__.SimpleDirichletPrior object at 0x7f5287d66310>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 672.4418 - loglik: -3.8386e+02 - logprior: -2.8858e+02
Epoch 2/10
10/10 - 1s - loss: 432.7123 - loglik: -3.5806e+02 - logprior: -7.4655e+01
Epoch 3/10
10/10 - 1s - loss: 369.8235 - loglik: -3.3747e+02 - logprior: -3.2358e+01
Epoch 4/10
10/10 - 1s - loss: 341.6210 - loglik: -3.2424e+02 - logprior: -1.7377e+01
Epoch 5/10
10/10 - 1s - loss: 328.2401 - loglik: -3.1908e+02 - logprior: -9.1564e+00
Epoch 6/10
10/10 - 1s - loss: 321.2939 - loglik: -3.1760e+02 - logprior: -3.6975e+00
Epoch 7/10
10/10 - 1s - loss: 316.9343 - loglik: -3.1664e+02 - logprior: -2.9693e-01
Epoch 8/10
10/10 - 1s - loss: 313.7591 - loglik: -3.1558e+02 - logprior: 1.8211
Epoch 9/10
10/10 - 1s - loss: 311.5096 - loglik: -3.1501e+02 - logprior: 3.4996
Epoch 10/10
10/10 - 1s - loss: 310.1681 - loglik: -3.1486e+02 - logprior: 4.6936
Fitted a model with MAP estimate = -309.6057
expansions: [(0, 3), (10, 1), (17, 2), (23, 3), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 671.4974 - loglik: -3.0884e+02 - logprior: -3.6265e+02
Epoch 2/2
10/10 - 1s - loss: 405.5029 - loglik: -2.9688e+02 - logprior: -1.0863e+02
Fitted a model with MAP estimate = -355.9782
expansions: [(21, 1)]
discards: [ 0 29 36 47 52]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 597.6955 - loglik: -2.9589e+02 - logprior: -3.0180e+02
Epoch 2/2
10/10 - 1s - loss: 408.0714 - loglik: -2.9366e+02 - logprior: -1.1441e+02
Fitted a model with MAP estimate = -377.9208
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 567.2296 - loglik: -2.9273e+02 - logprior: -2.7450e+02
Epoch 2/10
10/10 - 1s - loss: 364.6461 - loglik: -2.9191e+02 - logprior: -7.2741e+01
Epoch 3/10
10/10 - 1s - loss: 314.2361 - loglik: -2.9211e+02 - logprior: -2.2129e+01
Epoch 4/10
10/10 - 1s - loss: 296.1560 - loglik: -2.9252e+02 - logprior: -3.6340e+00
Epoch 5/10
10/10 - 1s - loss: 286.9949 - loglik: -2.9289e+02 - logprior: 5.8954
Epoch 6/10
10/10 - 1s - loss: 281.6850 - loglik: -2.9319e+02 - logprior: 11.5010
Epoch 7/10
10/10 - 1s - loss: 278.3086 - loglik: -2.9337e+02 - logprior: 15.0660
Epoch 8/10
10/10 - 1s - loss: 275.9406 - loglik: -2.9349e+02 - logprior: 17.5489
Epoch 9/10
10/10 - 1s - loss: 274.1291 - loglik: -2.9359e+02 - logprior: 19.4658
Epoch 10/10
10/10 - 1s - loss: 272.6327 - loglik: -2.9369e+02 - logprior: 21.0621
Fitted a model with MAP estimate = -271.8895
Time for alignment: 37.5811
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 672.4418 - loglik: -3.8386e+02 - logprior: -2.8858e+02
Epoch 2/10
10/10 - 1s - loss: 432.7123 - loglik: -3.5806e+02 - logprior: -7.4655e+01
Epoch 3/10
10/10 - 1s - loss: 369.8235 - loglik: -3.3747e+02 - logprior: -3.2358e+01
Epoch 4/10
10/10 - 1s - loss: 341.6210 - loglik: -3.2424e+02 - logprior: -1.7377e+01
Epoch 5/10
10/10 - 1s - loss: 328.2402 - loglik: -3.1908e+02 - logprior: -9.1564e+00
Epoch 6/10
10/10 - 1s - loss: 321.2939 - loglik: -3.1760e+02 - logprior: -3.6975e+00
Epoch 7/10
10/10 - 1s - loss: 316.9342 - loglik: -3.1664e+02 - logprior: -2.9694e-01
Epoch 8/10
10/10 - 1s - loss: 313.7591 - loglik: -3.1558e+02 - logprior: 1.8211
Epoch 9/10
10/10 - 1s - loss: 311.5096 - loglik: -3.1501e+02 - logprior: 3.4996
Epoch 10/10
10/10 - 1s - loss: 310.1681 - loglik: -3.1486e+02 - logprior: 4.6936
Fitted a model with MAP estimate = -309.6055
expansions: [(0, 3), (10, 1), (17, 2), (23, 3), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 671.4974 - loglik: -3.0884e+02 - logprior: -3.6265e+02
Epoch 2/2
10/10 - 1s - loss: 405.5029 - loglik: -2.9688e+02 - logprior: -1.0863e+02
Fitted a model with MAP estimate = -355.9781
expansions: [(21, 1)]
discards: [ 0 29 36 47 52]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 597.6955 - loglik: -2.9589e+02 - logprior: -3.0180e+02
Epoch 2/2
10/10 - 1s - loss: 408.0714 - loglik: -2.9366e+02 - logprior: -1.1441e+02
Fitted a model with MAP estimate = -377.9207
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 567.2296 - loglik: -2.9273e+02 - logprior: -2.7450e+02
Epoch 2/10
10/10 - 1s - loss: 364.6462 - loglik: -2.9191e+02 - logprior: -7.2741e+01
Epoch 3/10
10/10 - 1s - loss: 314.2361 - loglik: -2.9211e+02 - logprior: -2.2130e+01
Epoch 4/10
10/10 - 1s - loss: 296.1561 - loglik: -2.9252e+02 - logprior: -3.6340e+00
Epoch 5/10
10/10 - 1s - loss: 286.9949 - loglik: -2.9289e+02 - logprior: 5.8954
Epoch 6/10
10/10 - 1s - loss: 281.6851 - loglik: -2.9319e+02 - logprior: 11.5010
Epoch 7/10
10/10 - 1s - loss: 278.3086 - loglik: -2.9337e+02 - logprior: 15.0659
Epoch 8/10
10/10 - 1s - loss: 275.9406 - loglik: -2.9349e+02 - logprior: 17.5489
Epoch 9/10
10/10 - 1s - loss: 274.1292 - loglik: -2.9360e+02 - logprior: 19.4658
Epoch 10/10
10/10 - 1s - loss: 272.6328 - loglik: -2.9369e+02 - logprior: 21.0621
Fitted a model with MAP estimate = -271.8896
Time for alignment: 36.8709
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 672.4418 - loglik: -3.8386e+02 - logprior: -2.8858e+02
Epoch 2/10
10/10 - 1s - loss: 432.7123 - loglik: -3.5806e+02 - logprior: -7.4655e+01
Epoch 3/10
10/10 - 1s - loss: 369.8235 - loglik: -3.3747e+02 - logprior: -3.2358e+01
Epoch 4/10
10/10 - 1s - loss: 341.6210 - loglik: -3.2424e+02 - logprior: -1.7377e+01
Epoch 5/10
10/10 - 1s - loss: 328.2401 - loglik: -3.1908e+02 - logprior: -9.1564e+00
Epoch 6/10
10/10 - 1s - loss: 321.2939 - loglik: -3.1760e+02 - logprior: -3.6975e+00
Epoch 7/10
10/10 - 1s - loss: 316.9343 - loglik: -3.1664e+02 - logprior: -2.9693e-01
Epoch 8/10
10/10 - 1s - loss: 313.7592 - loglik: -3.1558e+02 - logprior: 1.8211
Epoch 9/10
10/10 - 1s - loss: 311.5096 - loglik: -3.1501e+02 - logprior: 3.4996
Epoch 10/10
10/10 - 1s - loss: 310.1681 - loglik: -3.1486e+02 - logprior: 4.6936
Fitted a model with MAP estimate = -309.6056
expansions: [(0, 3), (10, 1), (17, 2), (23, 3), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 671.4974 - loglik: -3.0884e+02 - logprior: -3.6265e+02
Epoch 2/2
10/10 - 1s - loss: 405.5029 - loglik: -2.9688e+02 - logprior: -1.0863e+02
Fitted a model with MAP estimate = -355.9781
expansions: [(21, 1)]
discards: [ 0 29 36 47 52]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 597.6955 - loglik: -2.9589e+02 - logprior: -3.0180e+02
Epoch 2/2
10/10 - 1s - loss: 408.0714 - loglik: -2.9366e+02 - logprior: -1.1441e+02
Fitted a model with MAP estimate = -377.9208
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 567.2296 - loglik: -2.9273e+02 - logprior: -2.7450e+02
Epoch 2/10
10/10 - 1s - loss: 364.6461 - loglik: -2.9191e+02 - logprior: -7.2741e+01
Epoch 3/10
10/10 - 1s - loss: 314.2361 - loglik: -2.9211e+02 - logprior: -2.2130e+01
Epoch 4/10
10/10 - 1s - loss: 296.1560 - loglik: -2.9252e+02 - logprior: -3.6340e+00
Epoch 5/10
10/10 - 1s - loss: 286.9949 - loglik: -2.9289e+02 - logprior: 5.8955
Epoch 6/10
10/10 - 1s - loss: 281.6850 - loglik: -2.9319e+02 - logprior: 11.5010
Epoch 7/10
10/10 - 1s - loss: 278.3085 - loglik: -2.9337e+02 - logprior: 15.0660
Epoch 8/10
10/10 - 1s - loss: 275.9405 - loglik: -2.9349e+02 - logprior: 17.5490
Epoch 9/10
10/10 - 1s - loss: 274.1290 - loglik: -2.9359e+02 - logprior: 19.4659
Epoch 10/10
10/10 - 1s - loss: 272.6326 - loglik: -2.9369e+02 - logprior: 21.0622
Fitted a model with MAP estimate = -271.8896
Time for alignment: 32.9554
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 672.4418 - loglik: -3.8386e+02 - logprior: -2.8858e+02
Epoch 2/10
10/10 - 1s - loss: 432.7123 - loglik: -3.5806e+02 - logprior: -7.4655e+01
Epoch 3/10
10/10 - 1s - loss: 369.8235 - loglik: -3.3747e+02 - logprior: -3.2358e+01
Epoch 4/10
10/10 - 1s - loss: 341.6210 - loglik: -3.2424e+02 - logprior: -1.7377e+01
Epoch 5/10
10/10 - 1s - loss: 328.2401 - loglik: -3.1908e+02 - logprior: -9.1564e+00
Epoch 6/10
10/10 - 1s - loss: 321.2939 - loglik: -3.1760e+02 - logprior: -3.6975e+00
Epoch 7/10
10/10 - 1s - loss: 316.9343 - loglik: -3.1664e+02 - logprior: -2.9693e-01
Epoch 8/10
10/10 - 1s - loss: 313.7591 - loglik: -3.1558e+02 - logprior: 1.8211
Epoch 9/10
10/10 - 1s - loss: 311.5096 - loglik: -3.1501e+02 - logprior: 3.4996
Epoch 10/10
10/10 - 1s - loss: 310.1681 - loglik: -3.1486e+02 - logprior: 4.6936
Fitted a model with MAP estimate = -309.6057
expansions: [(0, 3), (10, 1), (17, 2), (23, 3), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 671.4974 - loglik: -3.0884e+02 - logprior: -3.6265e+02
Epoch 2/2
10/10 - 1s - loss: 405.5029 - loglik: -2.9688e+02 - logprior: -1.0863e+02
Fitted a model with MAP estimate = -355.9782
expansions: [(21, 1)]
discards: [ 0 29 36 47 52]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 597.6954 - loglik: -2.9589e+02 - logprior: -3.0180e+02
Epoch 2/2
10/10 - 1s - loss: 408.0714 - loglik: -2.9366e+02 - logprior: -1.1441e+02
Fitted a model with MAP estimate = -377.9207
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 567.2296 - loglik: -2.9273e+02 - logprior: -2.7450e+02
Epoch 2/10
10/10 - 1s - loss: 364.6461 - loglik: -2.9191e+02 - logprior: -7.2741e+01
Epoch 3/10
10/10 - 1s - loss: 314.2360 - loglik: -2.9211e+02 - logprior: -2.2129e+01
Epoch 4/10
10/10 - 1s - loss: 296.1559 - loglik: -2.9252e+02 - logprior: -3.6340e+00
Epoch 5/10
10/10 - 1s - loss: 286.9948 - loglik: -2.9289e+02 - logprior: 5.8955
Epoch 6/10
10/10 - 1s - loss: 281.6850 - loglik: -2.9319e+02 - logprior: 11.5011
Epoch 7/10
10/10 - 1s - loss: 278.3085 - loglik: -2.9337e+02 - logprior: 15.0661
Epoch 8/10
10/10 - 1s - loss: 275.9405 - loglik: -2.9349e+02 - logprior: 17.5490
Epoch 9/10
10/10 - 1s - loss: 274.1290 - loglik: -2.9359e+02 - logprior: 19.4659
Epoch 10/10
10/10 - 1s - loss: 272.6327 - loglik: -2.9369e+02 - logprior: 21.0622
Fitted a model with MAP estimate = -271.8894
Time for alignment: 35.8665
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 672.4418 - loglik: -3.8386e+02 - logprior: -2.8858e+02
Epoch 2/10
10/10 - 1s - loss: 432.7123 - loglik: -3.5806e+02 - logprior: -7.4655e+01
Epoch 3/10
10/10 - 1s - loss: 369.8235 - loglik: -3.3747e+02 - logprior: -3.2358e+01
Epoch 4/10
10/10 - 1s - loss: 341.6210 - loglik: -3.2424e+02 - logprior: -1.7377e+01
Epoch 5/10
10/10 - 1s - loss: 328.2401 - loglik: -3.1908e+02 - logprior: -9.1564e+00
Epoch 6/10
10/10 - 1s - loss: 321.2939 - loglik: -3.1760e+02 - logprior: -3.6975e+00
Epoch 7/10
10/10 - 1s - loss: 316.9344 - loglik: -3.1664e+02 - logprior: -2.9693e-01
Epoch 8/10
10/10 - 1s - loss: 313.7591 - loglik: -3.1558e+02 - logprior: 1.8211
Epoch 9/10
10/10 - 1s - loss: 311.5096 - loglik: -3.1501e+02 - logprior: 3.4996
Epoch 10/10
10/10 - 1s - loss: 310.1680 - loglik: -3.1486e+02 - logprior: 4.6937
Fitted a model with MAP estimate = -309.6057
expansions: [(0, 3), (10, 1), (17, 2), (23, 3), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 671.4974 - loglik: -3.0884e+02 - logprior: -3.6265e+02
Epoch 2/2
10/10 - 1s - loss: 405.5029 - loglik: -2.9688e+02 - logprior: -1.0863e+02
Fitted a model with MAP estimate = -355.9782
expansions: [(21, 1)]
discards: [ 0 29 36 47 52]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 597.6955 - loglik: -2.9589e+02 - logprior: -3.0180e+02
Epoch 2/2
10/10 - 1s - loss: 408.0713 - loglik: -2.9366e+02 - logprior: -1.1441e+02
Fitted a model with MAP estimate = -377.9207
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 567.2296 - loglik: -2.9273e+02 - logprior: -2.7450e+02
Epoch 2/10
10/10 - 1s - loss: 364.6461 - loglik: -2.9191e+02 - logprior: -7.2741e+01
Epoch 3/10
10/10 - 1s - loss: 314.2360 - loglik: -2.9211e+02 - logprior: -2.2129e+01
Epoch 4/10
10/10 - 1s - loss: 296.1560 - loglik: -2.9252e+02 - logprior: -3.6340e+00
Epoch 5/10
10/10 - 1s - loss: 286.9949 - loglik: -2.9289e+02 - logprior: 5.8955
Epoch 6/10
10/10 - 1s - loss: 281.6851 - loglik: -2.9319e+02 - logprior: 11.5010
Epoch 7/10
10/10 - 1s - loss: 278.3086 - loglik: -2.9337e+02 - logprior: 15.0660
Epoch 8/10
10/10 - 1s - loss: 275.9406 - loglik: -2.9349e+02 - logprior: 17.5489
Epoch 9/10
10/10 - 1s - loss: 274.1290 - loglik: -2.9359e+02 - logprior: 19.4659
Epoch 10/10
10/10 - 1s - loss: 272.6327 - loglik: -2.9369e+02 - logprior: 21.0621
Fitted a model with MAP estimate = -271.8895
Time for alignment: 35.7722
Computed alignments with likelihoods: ['-271.8895', '-271.8896', '-271.8896', '-271.8894', '-271.8895']
Best model has likelihood: -271.8894  (prior= 21.8582 )
time for generating output: 0.1335
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hip.projection.fasta
SP score = 0.8293051359516617
Training of 5 independent models on file peroxidase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52714fcbe0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5284a9b100>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526ef50550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526ef50dc0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52768dd310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526d105fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52709bf100>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f52709bf610>, <__main__.SimpleDirichletPrior object at 0x7f52694ead90>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 13s - loss: 1434.3503 - loglik: -1.4288e+03 - logprior: -5.5099e+00
Epoch 2/10
26/26 - 10s - loss: 1335.9988 - loglik: -1.3349e+03 - logprior: -1.1396e+00
Epoch 3/10
26/26 - 10s - loss: 1318.8007 - loglik: -1.3177e+03 - logprior: -1.0964e+00
Epoch 4/10
26/26 - 10s - loss: 1320.0449 - loglik: -1.3190e+03 - logprior: -1.0688e+00
Fitted a model with MAP estimate = -1316.8258
expansions: [(9, 1), (57, 2), (163, 1), (179, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 205 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 13s - loss: 1322.3081 - loglik: -1.3179e+03 - logprior: -4.3806e+00
Epoch 2/2
26/26 - 10s - loss: 1315.9073 - loglik: -1.3150e+03 - logprior: -9.1394e-01
Fitted a model with MAP estimate = -1314.8237
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 13s - loss: 1323.0229 - loglik: -1.3167e+03 - logprior: -6.3299e+00
Epoch 2/2
26/26 - 10s - loss: 1319.8666 - loglik: -1.3172e+03 - logprior: -2.6177e+00
Fitted a model with MAP estimate = -1317.0815
expansions: [(0, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 207 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 13s - loss: 1319.2338 - loglik: -1.3152e+03 - logprior: -4.0096e+00
Epoch 2/10
26/26 - 10s - loss: 1316.0623 - loglik: -1.3156e+03 - logprior: -4.9667e-01
Epoch 3/10
26/26 - 10s - loss: 1312.7003 - loglik: -1.3125e+03 - logprior: -2.3624e-01
Epoch 4/10
26/26 - 10s - loss: 1314.3333 - loglik: -1.3142e+03 - logprior: -1.3294e-01
Fitted a model with MAP estimate = -1312.1168
Time for alignment: 176.1280
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 18s - loss: 1432.9451 - loglik: -1.4274e+03 - logprior: -5.5241e+00
Epoch 2/10
26/26 - 10s - loss: 1332.0046 - loglik: -1.3306e+03 - logprior: -1.3703e+00
Epoch 3/10
26/26 - 10s - loss: 1319.4364 - loglik: -1.3181e+03 - logprior: -1.3100e+00
Epoch 4/10
26/26 - 10s - loss: 1313.3209 - loglik: -1.3118e+03 - logprior: -1.4804e+00
Epoch 5/10
26/26 - 10s - loss: 1312.8359 - loglik: -1.3112e+03 - logprior: -1.6000e+00
Epoch 6/10
26/26 - 10s - loss: 1310.1721 - loglik: -1.3085e+03 - logprior: -1.6656e+00
Epoch 7/10
26/26 - 10s - loss: 1310.9786 - loglik: -1.3092e+03 - logprior: -1.7750e+00
Fitted a model with MAP estimate = -1311.2605
expansions: [(11, 1), (53, 1), (108, 5), (173, 1), (174, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 208 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 14s - loss: 1322.3357 - loglik: -1.3154e+03 - logprior: -6.9214e+00
Epoch 2/2
26/26 - 10s - loss: 1308.9718 - loglik: -1.3059e+03 - logprior: -3.1152e+00
Fitted a model with MAP estimate = -1308.5844
expansions: [(0, 5), (111, 1), (113, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 14s - loss: 1310.8927 - loglik: -1.3063e+03 - logprior: -4.6411e+00
Epoch 2/2
26/26 - 11s - loss: 1307.4260 - loglik: -1.3061e+03 - logprior: -1.2773e+00
Fitted a model with MAP estimate = -1304.0480
expansions: []
discards: [0 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 210 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 14s - loss: 1313.7435 - loglik: -1.3073e+03 - logprior: -6.4188e+00
Epoch 2/10
26/26 - 10s - loss: 1309.0432 - loglik: -1.3068e+03 - logprior: -2.1971e+00
Epoch 3/10
26/26 - 10s - loss: 1308.1088 - loglik: -1.3076e+03 - logprior: -4.7450e-01
Epoch 4/10
26/26 - 11s - loss: 1303.3490 - loglik: -1.3029e+03 - logprior: -4.3429e-01
Epoch 5/10
26/26 - 11s - loss: 1300.2072 - loglik: -1.2998e+03 - logprior: -3.7910e-01
Epoch 6/10
26/26 - 11s - loss: 1301.8464 - loglik: -1.3015e+03 - logprior: -3.5309e-01
Fitted a model with MAP estimate = -1301.9940
Time for alignment: 237.3907
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 13s - loss: 1433.4030 - loglik: -1.4279e+03 - logprior: -5.4982e+00
Epoch 2/10
26/26 - 10s - loss: 1331.4545 - loglik: -1.3300e+03 - logprior: -1.4486e+00
Epoch 3/10
26/26 - 10s - loss: 1317.0151 - loglik: -1.3156e+03 - logprior: -1.3779e+00
Epoch 4/10
26/26 - 10s - loss: 1317.2728 - loglik: -1.3158e+03 - logprior: -1.4585e+00
Fitted a model with MAP estimate = -1314.7352
expansions: [(173, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 201 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 16s - loss: 1320.5012 - loglik: -1.3157e+03 - logprior: -4.7705e+00
Epoch 2/2
26/26 - 10s - loss: 1318.1766 - loglik: -1.3168e+03 - logprior: -1.3802e+00
Fitted a model with MAP estimate = -1315.3603
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 201 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 14s - loss: 1319.0282 - loglik: -1.3144e+03 - logprior: -4.5834e+00
Epoch 2/10
26/26 - 10s - loss: 1317.5626 - loglik: -1.3164e+03 - logprior: -1.1738e+00
Epoch 3/10
26/26 - 10s - loss: 1317.1437 - loglik: -1.3162e+03 - logprior: -9.5084e-01
Epoch 4/10
26/26 - 10s - loss: 1310.3666 - loglik: -1.3094e+03 - logprior: -9.4151e-01
Epoch 5/10
26/26 - 10s - loss: 1316.6941 - loglik: -1.3158e+03 - logprior: -8.8379e-01
Fitted a model with MAP estimate = -1312.5547
Time for alignment: 153.7282
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 13s - loss: 1435.5603 - loglik: -1.4300e+03 - logprior: -5.5463e+00
Epoch 2/10
26/26 - 10s - loss: 1326.8854 - loglik: -1.3253e+03 - logprior: -1.5701e+00
Epoch 3/10
26/26 - 10s - loss: 1314.2623 - loglik: -1.3126e+03 - logprior: -1.6343e+00
Epoch 4/10
26/26 - 10s - loss: 1311.1750 - loglik: -1.3095e+03 - logprior: -1.7168e+00
Epoch 5/10
26/26 - 10s - loss: 1310.7000 - loglik: -1.3089e+03 - logprior: -1.8253e+00
Epoch 6/10
26/26 - 10s - loss: 1307.4875 - loglik: -1.3056e+03 - logprior: -1.9271e+00
Epoch 7/10
26/26 - 10s - loss: 1310.5768 - loglik: -1.3085e+03 - logprior: -2.0522e+00
Fitted a model with MAP estimate = -1308.6609
expansions: [(131, 2), (160, 3), (173, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 205 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 13s - loss: 1315.9338 - loglik: -1.3109e+03 - logprior: -5.0503e+00
Epoch 2/2
26/26 - 10s - loss: 1308.6831 - loglik: -1.3072e+03 - logprior: -1.4802e+00
Fitted a model with MAP estimate = -1307.5329
expansions: []
discards: [162 163]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 13s - loss: 1313.5793 - loglik: -1.3088e+03 - logprior: -4.7431e+00
Epoch 2/2
26/26 - 10s - loss: 1309.9640 - loglik: -1.3088e+03 - logprior: -1.1811e+00
Fitted a model with MAP estimate = -1307.9749
expansions: [(161, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 204 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 16s - loss: 1312.2908 - loglik: -1.3078e+03 - logprior: -4.4894e+00
Epoch 2/10
26/26 - 10s - loss: 1307.8771 - loglik: -1.3069e+03 - logprior: -9.3735e-01
Epoch 3/10
26/26 - 10s - loss: 1308.3885 - loglik: -1.3076e+03 - logprior: -7.4751e-01
Fitted a model with MAP estimate = -1305.8827
Time for alignment: 199.3592
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 14s - loss: 1434.0110 - loglik: -1.4285e+03 - logprior: -5.5152e+00
Epoch 2/10
26/26 - 10s - loss: 1329.5430 - loglik: -1.3281e+03 - logprior: -1.4493e+00
Epoch 3/10
26/26 - 10s - loss: 1320.2595 - loglik: -1.3189e+03 - logprior: -1.3794e+00
Epoch 4/10
26/26 - 10s - loss: 1312.2336 - loglik: -1.3108e+03 - logprior: -1.4646e+00
Epoch 5/10
26/26 - 10s - loss: 1313.7993 - loglik: -1.3123e+03 - logprior: -1.4990e+00
Fitted a model with MAP estimate = -1312.7733
expansions: [(0, 3), (75, 1), (173, 1), (179, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 206 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 14s - loss: 1322.3710 - loglik: -1.3158e+03 - logprior: -6.5226e+00
Epoch 2/2
26/26 - 10s - loss: 1313.6624 - loglik: -1.3120e+03 - logprior: -1.6340e+00
Fitted a model with MAP estimate = -1311.8544
expansions: []
discards: [  1   2   3 180]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 13s - loss: 1321.7943 - loglik: -1.3172e+03 - logprior: -4.5559e+00
Epoch 2/2
26/26 - 10s - loss: 1311.0574 - loglik: -1.3100e+03 - logprior: -1.0472e+00
Fitted a model with MAP estimate = -1313.1636
expansions: [(0, 3), (177, 3)]
discards: [75]
Re-initialized the encoder parameters.
Fitting a model of length 207 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 13s - loss: 1320.8784 - loglik: -1.3144e+03 - logprior: -6.4963e+00
Epoch 2/10
26/26 - 10s - loss: 1312.8699 - loglik: -1.3114e+03 - logprior: -1.5047e+00
Epoch 3/10
26/26 - 10s - loss: 1310.0109 - loglik: -1.3089e+03 - logprior: -1.1206e+00
Epoch 4/10
26/26 - 10s - loss: 1308.6133 - loglik: -1.3077e+03 - logprior: -8.7335e-01
Epoch 5/10
26/26 - 10s - loss: 1310.7644 - loglik: -1.3100e+03 - logprior: -7.5234e-01
Fitted a model with MAP estimate = -1308.1530
Time for alignment: 198.6923
Computed alignments with likelihoods: ['-1312.1168', '-1301.9940', '-1312.5547', '-1305.8827', '-1308.1530']
Best model has likelihood: -1301.9940  (prior= -0.2431 )
time for generating output: 0.3657
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/peroxidase.projection.fasta
SP score = 0.5190322580645161
Training of 5 independent models on file TNF.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5286f8b040>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52760c6ca0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52e14aed00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52e14ae4c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52954cc4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f528b574760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526c6f66a0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5294aa7ac0>, <__main__.SimpleDirichletPrior object at 0x7f530de682b0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 784.2469 - loglik: -6.9559e+02 - logprior: -8.8655e+01
Epoch 2/10
10/10 - 2s - loss: 688.9924 - loglik: -6.6880e+02 - logprior: -2.0194e+01
Epoch 3/10
10/10 - 2s - loss: 648.6122 - loglik: -6.4130e+02 - logprior: -7.3073e+00
Epoch 4/10
10/10 - 2s - loss: 629.5403 - loglik: -6.2673e+02 - logprior: -2.8145e+00
Epoch 5/10
10/10 - 2s - loss: 621.5351 - loglik: -6.2098e+02 - logprior: -5.5316e-01
Epoch 6/10
10/10 - 2s - loss: 618.8111 - loglik: -6.1942e+02 - logprior: 0.6053
Epoch 7/10
10/10 - 2s - loss: 616.4103 - loglik: -6.1775e+02 - logprior: 1.3398
Epoch 8/10
10/10 - 2s - loss: 615.0464 - loglik: -6.1687e+02 - logprior: 1.8234
Epoch 9/10
10/10 - 2s - loss: 613.5163 - loglik: -6.1567e+02 - logprior: 2.1561
Epoch 10/10
10/10 - 2s - loss: 613.1674 - loglik: -6.1558e+02 - logprior: 2.4089
Fitted a model with MAP estimate = -612.6400
expansions: [(11, 4), (12, 1), (19, 2), (36, 4), (45, 3), (55, 2), (63, 4), (83, 4), (86, 1), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 704.1164 - loglik: -6.1498e+02 - logprior: -8.9132e+01
Epoch 2/2
10/10 - 2s - loss: 636.6492 - loglik: -6.0299e+02 - logprior: -3.3656e+01
Fitted a model with MAP estimate = -625.4544
expansions: [(0, 3), (9, 1), (111, 3)]
discards: [  0  42  43 104 105]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 669.0199 - loglik: -6.0040e+02 - logprior: -6.8621e+01
Epoch 2/2
10/10 - 2s - loss: 608.8445 - loglik: -5.9432e+02 - logprior: -1.4520e+01
Fitted a model with MAP estimate = -599.4934
expansions: [(114, 3)]
discards: [ 0  1 79 80]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 679.3441 - loglik: -5.9612e+02 - logprior: -8.3225e+01
Epoch 2/10
10/10 - 2s - loss: 618.4017 - loglik: -5.9228e+02 - logprior: -2.6126e+01
Epoch 3/10
10/10 - 2s - loss: 598.0963 - loglik: -5.9151e+02 - logprior: -6.5842e+00
Epoch 4/10
10/10 - 2s - loss: 588.6461 - loglik: -5.9027e+02 - logprior: 1.6220
Epoch 5/10
10/10 - 2s - loss: 584.6620 - loglik: -5.8937e+02 - logprior: 4.7104
Epoch 6/10
10/10 - 2s - loss: 583.4837 - loglik: -5.8978e+02 - logprior: 6.2967
Epoch 7/10
10/10 - 2s - loss: 582.0482 - loglik: -5.8934e+02 - logprior: 7.2884
Epoch 8/10
10/10 - 2s - loss: 581.3912 - loglik: -5.8947e+02 - logprior: 8.0744
Epoch 9/10
10/10 - 2s - loss: 580.9243 - loglik: -5.8967e+02 - logprior: 8.7477
Epoch 10/10
10/10 - 2s - loss: 580.3722 - loglik: -5.8968e+02 - logprior: 9.3101
Fitted a model with MAP estimate = -580.2450
Time for alignment: 60.6268
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 784.1564 - loglik: -6.9550e+02 - logprior: -8.8653e+01
Epoch 2/10
10/10 - 2s - loss: 690.0074 - loglik: -6.6980e+02 - logprior: -2.0202e+01
Epoch 3/10
10/10 - 2s - loss: 648.9249 - loglik: -6.4157e+02 - logprior: -7.3553e+00
Epoch 4/10
10/10 - 2s - loss: 629.3402 - loglik: -6.2637e+02 - logprior: -2.9751e+00
Epoch 5/10
10/10 - 2s - loss: 621.2760 - loglik: -6.2053e+02 - logprior: -7.4553e-01
Epoch 6/10
10/10 - 2s - loss: 617.9550 - loglik: -6.1827e+02 - logprior: 0.3196
Epoch 7/10
10/10 - 2s - loss: 615.9945 - loglik: -6.1708e+02 - logprior: 1.0813
Epoch 8/10
10/10 - 2s - loss: 614.9337 - loglik: -6.1663e+02 - logprior: 1.6934
Epoch 9/10
10/10 - 2s - loss: 614.2412 - loglik: -6.1633e+02 - logprior: 2.0871
Epoch 10/10
10/10 - 1s - loss: 614.0681 - loglik: -6.1648e+02 - logprior: 2.4075
Fitted a model with MAP estimate = -613.5045
expansions: [(10, 1), (11, 4), (12, 1), (19, 2), (36, 4), (45, 3), (63, 3), (79, 2), (84, 3), (86, 1), (88, 3), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 706.1890 - loglik: -6.1710e+02 - logprior: -8.9092e+01
Epoch 2/2
10/10 - 2s - loss: 637.1405 - loglik: -6.0381e+02 - logprior: -3.3328e+01
Fitted a model with MAP estimate = -625.1793
expansions: [(0, 3), (110, 3)]
discards: [  0  43  44  74  96 104]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 667.1359 - loglik: -5.9890e+02 - logprior: -6.8237e+01
Epoch 2/2
10/10 - 2s - loss: 606.6561 - loglik: -5.9236e+02 - logprior: -1.4298e+01
Fitted a model with MAP estimate = -597.1446
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 674.6736 - loglik: -5.9362e+02 - logprior: -8.1049e+01
Epoch 2/10
10/10 - 2s - loss: 612.9517 - loglik: -5.9163e+02 - logprior: -2.1327e+01
Epoch 3/10
10/10 - 2s - loss: 594.4530 - loglik: -5.9033e+02 - logprior: -4.1217e+00
Epoch 4/10
10/10 - 2s - loss: 588.0865 - loglik: -5.8997e+02 - logprior: 1.8872
Epoch 5/10
10/10 - 2s - loss: 585.1986 - loglik: -5.8984e+02 - logprior: 4.6449
Epoch 6/10
10/10 - 2s - loss: 583.8767 - loglik: -5.9004e+02 - logprior: 6.1626
Epoch 7/10
10/10 - 2s - loss: 582.2764 - loglik: -5.8943e+02 - logprior: 7.1580
Epoch 8/10
10/10 - 2s - loss: 581.4865 - loglik: -5.8942e+02 - logprior: 7.9329
Epoch 9/10
10/10 - 2s - loss: 581.4308 - loglik: -5.9004e+02 - logprior: 8.6089
Epoch 10/10
10/10 - 2s - loss: 580.7351 - loglik: -5.8991e+02 - logprior: 9.1799
Fitted a model with MAP estimate = -580.5605
Time for alignment: 59.2592
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 783.9872 - loglik: -6.9533e+02 - logprior: -8.8656e+01
Epoch 2/10
10/10 - 2s - loss: 689.3489 - loglik: -6.6915e+02 - logprior: -2.0195e+01
Epoch 3/10
10/10 - 2s - loss: 647.9085 - loglik: -6.4060e+02 - logprior: -7.3083e+00
Epoch 4/10
10/10 - 2s - loss: 628.5507 - loglik: -6.2564e+02 - logprior: -2.9105e+00
Epoch 5/10
10/10 - 2s - loss: 620.6307 - loglik: -6.1994e+02 - logprior: -6.9546e-01
Epoch 6/10
10/10 - 2s - loss: 617.5736 - loglik: -6.1811e+02 - logprior: 0.5378
Epoch 7/10
10/10 - 2s - loss: 615.6031 - loglik: -6.1695e+02 - logprior: 1.3497
Epoch 8/10
10/10 - 2s - loss: 614.5954 - loglik: -6.1647e+02 - logprior: 1.8754
Epoch 9/10
10/10 - 2s - loss: 613.6947 - loglik: -6.1593e+02 - logprior: 2.2324
Epoch 10/10
10/10 - 2s - loss: 611.5985 - loglik: -6.1411e+02 - logprior: 2.5134
Fitted a model with MAP estimate = -611.8460
expansions: [(9, 2), (10, 2), (12, 3), (19, 2), (36, 4), (45, 3), (62, 4), (79, 2), (84, 3), (86, 2), (89, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 705.3096 - loglik: -6.1584e+02 - logprior: -8.9465e+01
Epoch 2/2
10/10 - 2s - loss: 635.9548 - loglik: -6.0248e+02 - logprior: -3.3476e+01
Fitted a model with MAP estimate = -623.6307
expansions: [(0, 3), (16, 1)]
discards: [  0  44  45  98 106]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 666.6654 - loglik: -5.9877e+02 - logprior: -6.7894e+01
Epoch 2/2
10/10 - 2s - loss: 608.6312 - loglik: -5.9450e+02 - logprior: -1.4128e+01
Fitted a model with MAP estimate = -599.4716
expansions: [(114, 2), (115, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 678.7347 - loglik: -5.9584e+02 - logprior: -8.2891e+01
Epoch 2/10
10/10 - 2s - loss: 616.8672 - loglik: -5.9076e+02 - logprior: -2.6102e+01
Epoch 3/10
10/10 - 2s - loss: 594.4907 - loglik: -5.8794e+02 - logprior: -6.5516e+00
Epoch 4/10
10/10 - 2s - loss: 584.3510 - loglik: -5.8626e+02 - logprior: 1.9068
Epoch 5/10
10/10 - 2s - loss: 581.3331 - loglik: -5.8635e+02 - logprior: 5.0150
Epoch 6/10
10/10 - 2s - loss: 578.9020 - loglik: -5.8549e+02 - logprior: 6.5873
Epoch 7/10
10/10 - 2s - loss: 578.4730 - loglik: -5.8605e+02 - logprior: 7.5808
Epoch 8/10
10/10 - 2s - loss: 577.0839 - loglik: -5.8544e+02 - logprior: 8.3562
Epoch 9/10
10/10 - 2s - loss: 577.1371 - loglik: -5.8617e+02 - logprior: 9.0347
Fitted a model with MAP estimate = -576.6058
Time for alignment: 57.5996
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 784.1849 - loglik: -6.9553e+02 - logprior: -8.8653e+01
Epoch 2/10
10/10 - 2s - loss: 689.1154 - loglik: -6.6892e+02 - logprior: -2.0198e+01
Epoch 3/10
10/10 - 2s - loss: 648.9585 - loglik: -6.4163e+02 - logprior: -7.3299e+00
Epoch 4/10
10/10 - 1s - loss: 629.6422 - loglik: -6.2671e+02 - logprior: -2.9278e+00
Epoch 5/10
10/10 - 2s - loss: 621.7372 - loglik: -6.2095e+02 - logprior: -7.8945e-01
Epoch 6/10
10/10 - 2s - loss: 617.8050 - loglik: -6.1801e+02 - logprior: 0.2001
Epoch 7/10
10/10 - 2s - loss: 616.3345 - loglik: -6.1732e+02 - logprior: 0.9833
Epoch 8/10
10/10 - 2s - loss: 615.4510 - loglik: -6.1699e+02 - logprior: 1.5409
Epoch 9/10
10/10 - 2s - loss: 614.5056 - loglik: -6.1641e+02 - logprior: 1.9003
Epoch 10/10
10/10 - 2s - loss: 614.0959 - loglik: -6.1631e+02 - logprior: 2.2177
Fitted a model with MAP estimate = -613.8117
expansions: [(5, 1), (6, 1), (10, 2), (12, 3), (18, 2), (36, 4), (45, 3), (48, 1), (63, 3), (78, 2), (84, 3), (86, 1), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 705.0406 - loglik: -6.1596e+02 - logprior: -8.9077e+01
Epoch 2/2
10/10 - 2s - loss: 636.9903 - loglik: -6.0357e+02 - logprior: -3.3424e+01
Fitted a model with MAP estimate = -625.3973
expansions: [(0, 3), (16, 1), (112, 4)]
discards: [  0  44  45  97 106]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 668.6151 - loglik: -6.0018e+02 - logprior: -6.8434e+01
Epoch 2/2
10/10 - 2s - loss: 607.9193 - loglik: -5.9370e+02 - logprior: -1.4221e+01
Fitted a model with MAP estimate = -597.8338
expansions: [(111, 1), (116, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 676.4637 - loglik: -5.9392e+02 - logprior: -8.2542e+01
Epoch 2/10
10/10 - 2s - loss: 615.1075 - loglik: -5.9017e+02 - logprior: -2.4942e+01
Epoch 3/10
10/10 - 2s - loss: 594.6176 - loglik: -5.8907e+02 - logprior: -5.5477e+00
Epoch 4/10
10/10 - 2s - loss: 585.5381 - loglik: -5.8769e+02 - logprior: 2.1492
Epoch 5/10
10/10 - 2s - loss: 582.6476 - loglik: -5.8779e+02 - logprior: 5.1432
Epoch 6/10
10/10 - 2s - loss: 581.0290 - loglik: -5.8772e+02 - logprior: 6.6890
Epoch 7/10
10/10 - 2s - loss: 579.5945 - loglik: -5.8727e+02 - logprior: 7.6732
Epoch 8/10
10/10 - 2s - loss: 579.1460 - loglik: -5.8761e+02 - logprior: 8.4599
Epoch 9/10
10/10 - 2s - loss: 578.5197 - loglik: -5.8766e+02 - logprior: 9.1409
Epoch 10/10
10/10 - 2s - loss: 578.4506 - loglik: -5.8816e+02 - logprior: 9.7051
Fitted a model with MAP estimate = -577.9334
Time for alignment: 56.6987
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 784.5009 - loglik: -6.9585e+02 - logprior: -8.8655e+01
Epoch 2/10
10/10 - 2s - loss: 688.8177 - loglik: -6.6862e+02 - logprior: -2.0194e+01
Epoch 3/10
10/10 - 1s - loss: 646.5146 - loglik: -6.3926e+02 - logprior: -7.2514e+00
Epoch 4/10
10/10 - 2s - loss: 628.0186 - loglik: -6.2522e+02 - logprior: -2.8015e+00
Epoch 5/10
10/10 - 2s - loss: 620.8630 - loglik: -6.2030e+02 - logprior: -5.6523e-01
Epoch 6/10
10/10 - 1s - loss: 616.9759 - loglik: -6.1756e+02 - logprior: 0.5792
Epoch 7/10
10/10 - 2s - loss: 614.6414 - loglik: -6.1596e+02 - logprior: 1.3232
Epoch 8/10
10/10 - 2s - loss: 613.3260 - loglik: -6.1520e+02 - logprior: 1.8690
Epoch 9/10
10/10 - 2s - loss: 612.1801 - loglik: -6.1442e+02 - logprior: 2.2409
Epoch 10/10
10/10 - 2s - loss: 612.0113 - loglik: -6.1457e+02 - logprior: 2.5610
Fitted a model with MAP estimate = -611.3891
expansions: [(9, 2), (10, 2), (12, 4), (18, 2), (36, 4), (45, 3), (63, 3), (84, 4), (86, 1), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 701.6771 - loglik: -6.1282e+02 - logprior: -8.8852e+01
Epoch 2/2
10/10 - 2s - loss: 634.9962 - loglik: -6.0179e+02 - logprior: -3.3208e+01
Fitted a model with MAP estimate = -623.2836
expansions: [(0, 3)]
discards: [ 0 45 46]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 667.0729 - loglik: -5.9908e+02 - logprior: -6.7995e+01
Epoch 2/2
10/10 - 2s - loss: 609.9186 - loglik: -5.9533e+02 - logprior: -1.4588e+01
Fitted a model with MAP estimate = -601.1183
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 678.1164 - loglik: -5.9722e+02 - logprior: -8.0892e+01
Epoch 2/10
10/10 - 2s - loss: 616.9648 - loglik: -5.9613e+02 - logprior: -2.0832e+01
Epoch 3/10
10/10 - 2s - loss: 599.0996 - loglik: -5.9483e+02 - logprior: -4.2712e+00
Epoch 4/10
10/10 - 2s - loss: 592.6673 - loglik: -5.9420e+02 - logprior: 1.5280
Epoch 5/10
10/10 - 2s - loss: 590.0117 - loglik: -5.9430e+02 - logprior: 4.2917
Epoch 6/10
10/10 - 2s - loss: 588.4698 - loglik: -5.9429e+02 - logprior: 5.8201
Epoch 7/10
10/10 - 2s - loss: 587.4406 - loglik: -5.9424e+02 - logprior: 6.7952
Epoch 8/10
10/10 - 2s - loss: 586.5107 - loglik: -5.9409e+02 - logprior: 7.5789
Epoch 9/10
10/10 - 2s - loss: 586.0439 - loglik: -5.9429e+02 - logprior: 8.2456
Epoch 10/10
10/10 - 2s - loss: 585.4709 - loglik: -5.9427e+02 - logprior: 8.8022
Fitted a model with MAP estimate = -585.2975
Time for alignment: 57.6255
Computed alignments with likelihoods: ['-580.2450', '-580.5605', '-576.6058', '-577.9334', '-585.2975']
Best model has likelihood: -576.6058  (prior= 9.3661 )
time for generating output: 0.1877
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/TNF.projection.fasta
SP score = 0.8035971223021583
Training of 5 independent models on file egf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f530cc8e100>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52e1b1dfd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f9ab7970>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52f9ab78b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f598c028580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5288409940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526f41edf0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5b130afc10>, <__main__.SimpleDirichletPrior object at 0x7f52683699a0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 199.0030 - loglik: -1.9442e+02 - logprior: -4.5832e+00
Epoch 2/10
17/17 - 1s - loss: 176.7128 - loglik: -1.7512e+02 - logprior: -1.5965e+00
Epoch 3/10
17/17 - 1s - loss: 166.5516 - loglik: -1.6492e+02 - logprior: -1.6266e+00
Epoch 4/10
17/17 - 1s - loss: 164.7699 - loglik: -1.6310e+02 - logprior: -1.6691e+00
Epoch 5/10
17/17 - 1s - loss: 164.0470 - loglik: -1.6246e+02 - logprior: -1.5835e+00
Epoch 6/10
17/17 - 1s - loss: 163.9694 - loglik: -1.6233e+02 - logprior: -1.6354e+00
Epoch 7/10
17/17 - 1s - loss: 163.6795 - loglik: -1.6205e+02 - logprior: -1.6334e+00
Epoch 8/10
17/17 - 1s - loss: 163.8357 - loglik: -1.6221e+02 - logprior: -1.6259e+00
Fitted a model with MAP estimate = -163.6882
expansions: [(2, 1), (10, 1), (11, 2), (12, 3), (13, 1), (14, 1), (15, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 171.0006 - loglik: -1.6531e+02 - logprior: -5.6866e+00
Epoch 2/2
17/17 - 1s - loss: 162.8609 - loglik: -1.6012e+02 - logprior: -2.7432e+00
Fitted a model with MAP estimate = -161.0709
expansions: [(2, 1)]
discards: [ 0 13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 165.6342 - loglik: -1.6062e+02 - logprior: -5.0107e+00
Epoch 2/2
17/17 - 1s - loss: 160.1471 - loglik: -1.5841e+02 - logprior: -1.7392e+00
Fitted a model with MAP estimate = -159.7938
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 163.1288 - loglik: -1.5879e+02 - logprior: -4.3435e+00
Epoch 2/10
17/17 - 1s - loss: 160.0816 - loglik: -1.5844e+02 - logprior: -1.6369e+00
Epoch 3/10
17/17 - 1s - loss: 159.5512 - loglik: -1.5815e+02 - logprior: -1.4058e+00
Epoch 4/10
17/17 - 1s - loss: 159.4769 - loglik: -1.5813e+02 - logprior: -1.3469e+00
Epoch 5/10
17/17 - 1s - loss: 159.3221 - loglik: -1.5798e+02 - logprior: -1.3466e+00
Epoch 6/10
17/17 - 1s - loss: 159.4285 - loglik: -1.5809e+02 - logprior: -1.3391e+00
Fitted a model with MAP estimate = -159.2567
Time for alignment: 39.2224
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 198.9464 - loglik: -1.9436e+02 - logprior: -4.5821e+00
Epoch 2/10
17/17 - 1s - loss: 177.2129 - loglik: -1.7563e+02 - logprior: -1.5826e+00
Epoch 3/10
17/17 - 1s - loss: 167.4843 - loglik: -1.6585e+02 - logprior: -1.6333e+00
Epoch 4/10
17/17 - 1s - loss: 165.3024 - loglik: -1.6365e+02 - logprior: -1.6491e+00
Epoch 5/10
17/17 - 1s - loss: 164.5602 - loglik: -1.6298e+02 - logprior: -1.5765e+00
Epoch 6/10
17/17 - 1s - loss: 164.4260 - loglik: -1.6279e+02 - logprior: -1.6381e+00
Epoch 7/10
17/17 - 1s - loss: 164.2517 - loglik: -1.6263e+02 - logprior: -1.6233e+00
Epoch 8/10
17/17 - 1s - loss: 164.1999 - loglik: -1.6258e+02 - logprior: -1.6224e+00
Epoch 9/10
17/17 - 1s - loss: 164.1329 - loglik: -1.6252e+02 - logprior: -1.6079e+00
Epoch 10/10
17/17 - 1s - loss: 164.2594 - loglik: -1.6266e+02 - logprior: -1.6030e+00
Fitted a model with MAP estimate = -164.1151
expansions: [(2, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 171.4360 - loglik: -1.6574e+02 - logprior: -5.6937e+00
Epoch 2/2
17/17 - 1s - loss: 163.0250 - loglik: -1.6023e+02 - logprior: -2.7996e+00
Fitted a model with MAP estimate = -161.1629
expansions: [(2, 1)]
discards: [ 0 13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 165.6759 - loglik: -1.6067e+02 - logprior: -5.0017e+00
Epoch 2/2
17/17 - 1s - loss: 160.1889 - loglik: -1.5847e+02 - logprior: -1.7236e+00
Fitted a model with MAP estimate = -159.8047
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 163.2289 - loglik: -1.5888e+02 - logprior: -4.3445e+00
Epoch 2/10
17/17 - 1s - loss: 159.8881 - loglik: -1.5826e+02 - logprior: -1.6295e+00
Epoch 3/10
17/17 - 1s - loss: 159.7098 - loglik: -1.5831e+02 - logprior: -1.4043e+00
Epoch 4/10
17/17 - 1s - loss: 159.4459 - loglik: -1.5809e+02 - logprior: -1.3541e+00
Epoch 5/10
17/17 - 1s - loss: 159.2862 - loglik: -1.5795e+02 - logprior: -1.3411e+00
Epoch 6/10
17/17 - 1s - loss: 159.5046 - loglik: -1.5818e+02 - logprior: -1.3283e+00
Fitted a model with MAP estimate = -159.2547
Time for alignment: 38.8567
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 198.9871 - loglik: -1.9441e+02 - logprior: -4.5815e+00
Epoch 2/10
17/17 - 1s - loss: 176.6398 - loglik: -1.7508e+02 - logprior: -1.5643e+00
Epoch 3/10
17/17 - 1s - loss: 167.0859 - loglik: -1.6547e+02 - logprior: -1.6177e+00
Epoch 4/10
17/17 - 1s - loss: 165.4373 - loglik: -1.6379e+02 - logprior: -1.6428e+00
Epoch 5/10
17/17 - 1s - loss: 164.8339 - loglik: -1.6327e+02 - logprior: -1.5684e+00
Epoch 6/10
17/17 - 1s - loss: 164.7052 - loglik: -1.6307e+02 - logprior: -1.6358e+00
Epoch 7/10
17/17 - 1s - loss: 164.6123 - loglik: -1.6299e+02 - logprior: -1.6235e+00
Epoch 8/10
17/17 - 1s - loss: 164.6463 - loglik: -1.6303e+02 - logprior: -1.6171e+00
Fitted a model with MAP estimate = -164.5167
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 171.9330 - loglik: -1.6630e+02 - logprior: -5.6298e+00
Epoch 2/2
17/17 - 1s - loss: 163.0871 - loglik: -1.6056e+02 - logprior: -2.5321e+00
Fitted a model with MAP estimate = -160.9443
expansions: []
discards: [13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 163.8190 - loglik: -1.5938e+02 - logprior: -4.4434e+00
Epoch 2/2
17/17 - 1s - loss: 160.1667 - loglik: -1.5851e+02 - logprior: -1.6564e+00
Fitted a model with MAP estimate = -159.7430
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 163.1944 - loglik: -1.5886e+02 - logprior: -4.3351e+00
Epoch 2/10
17/17 - 1s - loss: 159.8239 - loglik: -1.5819e+02 - logprior: -1.6362e+00
Epoch 3/10
17/17 - 1s - loss: 159.7546 - loglik: -1.5835e+02 - logprior: -1.4001e+00
Epoch 4/10
17/17 - 1s - loss: 159.5819 - loglik: -1.5822e+02 - logprior: -1.3643e+00
Epoch 5/10
17/17 - 1s - loss: 159.2936 - loglik: -1.5796e+02 - logprior: -1.3346e+00
Epoch 6/10
17/17 - 1s - loss: 159.3844 - loglik: -1.5805e+02 - logprior: -1.3393e+00
Fitted a model with MAP estimate = -159.2539
Time for alignment: 38.9093
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 199.0507 - loglik: -1.9447e+02 - logprior: -4.5837e+00
Epoch 2/10
17/17 - 1s - loss: 176.6671 - loglik: -1.7507e+02 - logprior: -1.5933e+00
Epoch 3/10
17/17 - 1s - loss: 166.6409 - loglik: -1.6502e+02 - logprior: -1.6170e+00
Epoch 4/10
17/17 - 1s - loss: 165.0988 - loglik: -1.6344e+02 - logprior: -1.6593e+00
Epoch 5/10
17/17 - 1s - loss: 164.6909 - loglik: -1.6311e+02 - logprior: -1.5774e+00
Epoch 6/10
17/17 - 1s - loss: 164.4474 - loglik: -1.6282e+02 - logprior: -1.6261e+00
Epoch 7/10
17/17 - 1s - loss: 164.1457 - loglik: -1.6252e+02 - logprior: -1.6266e+00
Epoch 8/10
17/17 - 1s - loss: 164.3415 - loglik: -1.6273e+02 - logprior: -1.6081e+00
Fitted a model with MAP estimate = -164.1688
expansions: [(2, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 171.5407 - loglik: -1.6585e+02 - logprior: -5.6917e+00
Epoch 2/2
17/17 - 1s - loss: 162.9491 - loglik: -1.6021e+02 - logprior: -2.7412e+00
Fitted a model with MAP estimate = -161.0793
expansions: [(2, 1)]
discards: [ 0 13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 6s - loss: 165.6697 - loglik: -1.6065e+02 - logprior: -5.0217e+00
Epoch 2/2
17/17 - 1s - loss: 160.3088 - loglik: -1.5858e+02 - logprior: -1.7260e+00
Fitted a model with MAP estimate = -159.8118
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 163.1032 - loglik: -1.5876e+02 - logprior: -4.3440e+00
Epoch 2/10
17/17 - 1s - loss: 160.1336 - loglik: -1.5850e+02 - logprior: -1.6313e+00
Epoch 3/10
17/17 - 1s - loss: 159.5735 - loglik: -1.5817e+02 - logprior: -1.4019e+00
Epoch 4/10
17/17 - 1s - loss: 159.3638 - loglik: -1.5801e+02 - logprior: -1.3495e+00
Epoch 5/10
17/17 - 1s - loss: 159.4587 - loglik: -1.5812e+02 - logprior: -1.3434e+00
Fitted a model with MAP estimate = -159.3110
Time for alignment: 37.2994
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 199.0121 - loglik: -1.9443e+02 - logprior: -4.5838e+00
Epoch 2/10
17/17 - 1s - loss: 178.0806 - loglik: -1.7648e+02 - logprior: -1.5962e+00
Epoch 3/10
17/17 - 1s - loss: 167.7821 - loglik: -1.6621e+02 - logprior: -1.5716e+00
Epoch 4/10
17/17 - 1s - loss: 165.5042 - loglik: -1.6387e+02 - logprior: -1.6303e+00
Epoch 5/10
17/17 - 1s - loss: 164.1687 - loglik: -1.6261e+02 - logprior: -1.5613e+00
Epoch 6/10
17/17 - 1s - loss: 164.1896 - loglik: -1.6262e+02 - logprior: -1.5715e+00
Fitted a model with MAP estimate = -163.7468
expansions: [(1, 1), (2, 2), (10, 2), (11, 2), (13, 1), (16, 1), (18, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 34 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 166.9956 - loglik: -1.6242e+02 - logprior: -4.5762e+00
Epoch 2/2
17/17 - 1s - loss: 160.4985 - loglik: -1.5873e+02 - logprior: -1.7721e+00
Fitted a model with MAP estimate = -160.0382
expansions: []
discards: [ 3 14 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 163.4923 - loglik: -1.5915e+02 - logprior: -4.3381e+00
Epoch 2/2
17/17 - 1s - loss: 160.0540 - loglik: -1.5841e+02 - logprior: -1.6475e+00
Fitted a model with MAP estimate = -159.7341
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 163.1731 - loglik: -1.5884e+02 - logprior: -4.3312e+00
Epoch 2/10
17/17 - 1s - loss: 160.0037 - loglik: -1.5837e+02 - logprior: -1.6299e+00
Epoch 3/10
17/17 - 1s - loss: 159.4748 - loglik: -1.5808e+02 - logprior: -1.3959e+00
Epoch 4/10
17/17 - 1s - loss: 159.6666 - loglik: -1.5831e+02 - logprior: -1.3605e+00
Fitted a model with MAP estimate = -159.3837
Time for alignment: 31.9786
Computed alignments with likelihoods: ['-159.2567', '-159.2547', '-159.2539', '-159.3110', '-159.3837']
Best model has likelihood: -159.2539  (prior= -1.3436 )
time for generating output: 0.1107
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/egf.projection.fasta
SP score = 0.760324199150907
Training of 5 independent models on file HMG_box.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5274cf0ac0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5274cd32b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5271c47640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52898a1ac0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52846f6bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f527ec9e310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f527ec9e460>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f527ec9e340>, <__main__.SimpleDirichletPrior object at 0x7f526320be20>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 388.2690 - loglik: -3.7950e+02 - logprior: -8.7653e+00
Epoch 2/10
13/13 - 1s - loss: 358.2646 - loglik: -3.5590e+02 - logprior: -2.3632e+00
Epoch 3/10
13/13 - 1s - loss: 340.7007 - loglik: -3.3873e+02 - logprior: -1.9676e+00
Epoch 4/10
13/13 - 1s - loss: 333.1475 - loglik: -3.3105e+02 - logprior: -2.0954e+00
Epoch 5/10
13/13 - 1s - loss: 329.2227 - loglik: -3.2718e+02 - logprior: -2.0421e+00
Epoch 6/10
13/13 - 1s - loss: 327.8673 - loglik: -3.2592e+02 - logprior: -1.9436e+00
Epoch 7/10
13/13 - 1s - loss: 327.5154 - loglik: -3.2556e+02 - logprior: -1.9587e+00
Epoch 8/10
13/13 - 1s - loss: 326.9424 - loglik: -3.2500e+02 - logprior: -1.9451e+00
Epoch 9/10
13/13 - 1s - loss: 327.0722 - loglik: -3.2515e+02 - logprior: -1.9177e+00
Fitted a model with MAP estimate = -326.9771
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (32, 1), (41, 1), (44, 1), (45, 1), (46, 1), (48, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 337.2525 - loglik: -3.2748e+02 - logprior: -9.7773e+00
Epoch 2/2
13/13 - 1s - loss: 322.7078 - loglik: -3.1825e+02 - logprior: -4.4529e+00
Fitted a model with MAP estimate = -321.4548
expansions: [(0, 2)]
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 324.7973 - loglik: -3.1746e+02 - logprior: -7.3341e+00
Epoch 2/2
13/13 - 1s - loss: 318.4573 - loglik: -3.1627e+02 - logprior: -2.1831e+00
Fitted a model with MAP estimate = -317.3397
expansions: [(17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 328.1141 - loglik: -3.1875e+02 - logprior: -9.3689e+00
Epoch 2/10
13/13 - 1s - loss: 319.2973 - loglik: -3.1582e+02 - logprior: -3.4763e+00
Epoch 3/10
13/13 - 1s - loss: 316.6452 - loglik: -3.1497e+02 - logprior: -1.6796e+00
Epoch 4/10
13/13 - 1s - loss: 315.0563 - loglik: -3.1382e+02 - logprior: -1.2319e+00
Epoch 5/10
13/13 - 1s - loss: 314.7062 - loglik: -3.1356e+02 - logprior: -1.1424e+00
Epoch 6/10
13/13 - 1s - loss: 313.6826 - loglik: -3.1252e+02 - logprior: -1.1585e+00
Epoch 7/10
13/13 - 1s - loss: 314.3677 - loglik: -3.1321e+02 - logprior: -1.1612e+00
Fitted a model with MAP estimate = -313.5520
Time for alignment: 44.4270
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 388.0161 - loglik: -3.7925e+02 - logprior: -8.7658e+00
Epoch 2/10
13/13 - 1s - loss: 359.3566 - loglik: -3.5698e+02 - logprior: -2.3763e+00
Epoch 3/10
13/13 - 1s - loss: 340.6536 - loglik: -3.3867e+02 - logprior: -1.9807e+00
Epoch 4/10
13/13 - 1s - loss: 332.9517 - loglik: -3.3089e+02 - logprior: -2.0659e+00
Epoch 5/10
13/13 - 1s - loss: 330.9710 - loglik: -3.2900e+02 - logprior: -1.9660e+00
Epoch 6/10
13/13 - 1s - loss: 329.2310 - loglik: -3.2738e+02 - logprior: -1.8550e+00
Epoch 7/10
13/13 - 1s - loss: 328.3149 - loglik: -3.2644e+02 - logprior: -1.8710e+00
Epoch 8/10
13/13 - 1s - loss: 328.0619 - loglik: -3.2620e+02 - logprior: -1.8654e+00
Epoch 9/10
13/13 - 1s - loss: 328.0538 - loglik: -3.2622e+02 - logprior: -1.8349e+00
Epoch 10/10
13/13 - 1s - loss: 327.9664 - loglik: -3.2612e+02 - logprior: -1.8473e+00
Fitted a model with MAP estimate = -327.8758
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (32, 1), (38, 1), (41, 1), (45, 1), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 337.7666 - loglik: -3.2796e+02 - logprior: -9.8043e+00
Epoch 2/2
13/13 - 1s - loss: 322.8735 - loglik: -3.1841e+02 - logprior: -4.4636e+00
Fitted a model with MAP estimate = -321.5363
expansions: [(0, 2)]
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 324.6251 - loglik: -3.1730e+02 - logprior: -7.3255e+00
Epoch 2/2
13/13 - 1s - loss: 318.7078 - loglik: -3.1653e+02 - logprior: -2.1751e+00
Fitted a model with MAP estimate = -317.3527
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 328.3545 - loglik: -3.1899e+02 - logprior: -9.3642e+00
Epoch 2/10
13/13 - 1s - loss: 319.9204 - loglik: -3.1651e+02 - logprior: -3.4057e+00
Epoch 3/10
13/13 - 1s - loss: 317.9266 - loglik: -3.1623e+02 - logprior: -1.6962e+00
Epoch 4/10
13/13 - 1s - loss: 316.0431 - loglik: -3.1477e+02 - logprior: -1.2756e+00
Epoch 5/10
13/13 - 1s - loss: 314.7559 - loglik: -3.1355e+02 - logprior: -1.2024e+00
Epoch 6/10
13/13 - 1s - loss: 314.6422 - loglik: -3.1342e+02 - logprior: -1.2214e+00
Epoch 7/10
13/13 - 1s - loss: 314.4411 - loglik: -3.1323e+02 - logprior: -1.2142e+00
Epoch 8/10
13/13 - 1s - loss: 314.4057 - loglik: -3.1321e+02 - logprior: -1.1949e+00
Epoch 9/10
13/13 - 1s - loss: 314.0819 - loglik: -3.1290e+02 - logprior: -1.1807e+00
Epoch 10/10
13/13 - 1s - loss: 313.9863 - loglik: -3.1283e+02 - logprior: -1.1594e+00
Fitted a model with MAP estimate = -313.9216
Time for alignment: 47.8298
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 388.2614 - loglik: -3.7950e+02 - logprior: -8.7660e+00
Epoch 2/10
13/13 - 1s - loss: 357.6215 - loglik: -3.5527e+02 - logprior: -2.3553e+00
Epoch 3/10
13/13 - 1s - loss: 340.5306 - loglik: -3.3858e+02 - logprior: -1.9503e+00
Epoch 4/10
13/13 - 1s - loss: 333.7428 - loglik: -3.3170e+02 - logprior: -2.0459e+00
Epoch 5/10
13/13 - 1s - loss: 330.4445 - loglik: -3.2852e+02 - logprior: -1.9238e+00
Epoch 6/10
13/13 - 1s - loss: 329.3768 - loglik: -3.2755e+02 - logprior: -1.8224e+00
Epoch 7/10
13/13 - 1s - loss: 328.8933 - loglik: -3.2705e+02 - logprior: -1.8441e+00
Epoch 8/10
13/13 - 1s - loss: 328.6805 - loglik: -3.2686e+02 - logprior: -1.8195e+00
Epoch 9/10
13/13 - 1s - loss: 328.2242 - loglik: -3.2642e+02 - logprior: -1.8068e+00
Epoch 10/10
13/13 - 1s - loss: 328.7339 - loglik: -3.2691e+02 - logprior: -1.8223e+00
Fitted a model with MAP estimate = -328.2959
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (34, 1), (41, 1), (44, 1), (45, 1), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 337.0189 - loglik: -3.2723e+02 - logprior: -9.7920e+00
Epoch 2/2
13/13 - 1s - loss: 323.0600 - loglik: -3.1860e+02 - logprior: -4.4582e+00
Fitted a model with MAP estimate = -321.5287
expansions: [(0, 2)]
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 324.8957 - loglik: -3.1756e+02 - logprior: -7.3348e+00
Epoch 2/2
13/13 - 1s - loss: 318.1021 - loglik: -3.1592e+02 - logprior: -2.1805e+00
Fitted a model with MAP estimate = -317.3313
expansions: [(17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 327.5957 - loglik: -3.1823e+02 - logprior: -9.3616e+00
Epoch 2/10
13/13 - 1s - loss: 319.6976 - loglik: -3.1625e+02 - logprior: -3.4433e+00
Epoch 3/10
13/13 - 1s - loss: 316.7386 - loglik: -3.1507e+02 - logprior: -1.6721e+00
Epoch 4/10
13/13 - 1s - loss: 315.6640 - loglik: -3.1443e+02 - logprior: -1.2326e+00
Epoch 5/10
13/13 - 1s - loss: 314.1829 - loglik: -3.1304e+02 - logprior: -1.1436e+00
Epoch 6/10
13/13 - 1s - loss: 314.2116 - loglik: -3.1306e+02 - logprior: -1.1540e+00
Fitted a model with MAP estimate = -313.7845
Time for alignment: 43.0657
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 388.2121 - loglik: -3.7945e+02 - logprior: -8.7664e+00
Epoch 2/10
13/13 - 1s - loss: 358.6789 - loglik: -3.5631e+02 - logprior: -2.3660e+00
Epoch 3/10
13/13 - 1s - loss: 339.8444 - loglik: -3.3789e+02 - logprior: -1.9509e+00
Epoch 4/10
13/13 - 1s - loss: 333.9702 - loglik: -3.3193e+02 - logprior: -2.0396e+00
Epoch 5/10
13/13 - 1s - loss: 330.8784 - loglik: -3.2897e+02 - logprior: -1.9072e+00
Epoch 6/10
13/13 - 1s - loss: 329.8494 - loglik: -3.2804e+02 - logprior: -1.8048e+00
Epoch 7/10
13/13 - 1s - loss: 329.1758 - loglik: -3.2734e+02 - logprior: -1.8396e+00
Epoch 8/10
13/13 - 1s - loss: 328.5382 - loglik: -3.2671e+02 - logprior: -1.8292e+00
Epoch 9/10
13/13 - 1s - loss: 328.9154 - loglik: -3.2710e+02 - logprior: -1.8107e+00
Fitted a model with MAP estimate = -328.4445
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (34, 1), (41, 1), (44, 1), (45, 1), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 337.1854 - loglik: -3.2742e+02 - logprior: -9.7695e+00
Epoch 2/2
13/13 - 1s - loss: 323.1985 - loglik: -3.1875e+02 - logprior: -4.4493e+00
Fitted a model with MAP estimate = -321.4721
expansions: [(0, 2)]
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 325.1323 - loglik: -3.1780e+02 - logprior: -7.3291e+00
Epoch 2/2
13/13 - 1s - loss: 318.4102 - loglik: -3.1623e+02 - logprior: -2.1817e+00
Fitted a model with MAP estimate = -317.3347
expansions: [(17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 328.3456 - loglik: -3.1898e+02 - logprior: -9.3628e+00
Epoch 2/10
13/13 - 1s - loss: 319.4479 - loglik: -3.1601e+02 - logprior: -3.4401e+00
Epoch 3/10
13/13 - 1s - loss: 316.1270 - loglik: -3.1445e+02 - logprior: -1.6760e+00
Epoch 4/10
13/13 - 1s - loss: 314.7034 - loglik: -3.1347e+02 - logprior: -1.2330e+00
Epoch 5/10
13/13 - 1s - loss: 315.2168 - loglik: -3.1407e+02 - logprior: -1.1443e+00
Fitted a model with MAP estimate = -314.1817
Time for alignment: 37.4399
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 388.4814 - loglik: -3.7972e+02 - logprior: -8.7661e+00
Epoch 2/10
13/13 - 1s - loss: 358.5511 - loglik: -3.5619e+02 - logprior: -2.3655e+00
Epoch 3/10
13/13 - 1s - loss: 340.3828 - loglik: -3.3841e+02 - logprior: -1.9759e+00
Epoch 4/10
13/13 - 1s - loss: 333.4586 - loglik: -3.3140e+02 - logprior: -2.0562e+00
Epoch 5/10
13/13 - 1s - loss: 331.0044 - loglik: -3.2908e+02 - logprior: -1.9248e+00
Epoch 6/10
13/13 - 1s - loss: 329.4744 - loglik: -3.2766e+02 - logprior: -1.8190e+00
Epoch 7/10
13/13 - 1s - loss: 329.2092 - loglik: -3.2737e+02 - logprior: -1.8432e+00
Epoch 8/10
13/13 - 1s - loss: 329.0598 - loglik: -3.2723e+02 - logprior: -1.8307e+00
Epoch 9/10
13/13 - 1s - loss: 328.1548 - loglik: -3.2635e+02 - logprior: -1.8092e+00
Epoch 10/10
13/13 - 1s - loss: 328.8495 - loglik: -3.2703e+02 - logprior: -1.8216e+00
Fitted a model with MAP estimate = -328.5385
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (34, 1), (41, 1), (42, 1), (45, 1), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 337.3530 - loglik: -3.2755e+02 - logprior: -9.8017e+00
Epoch 2/2
13/13 - 1s - loss: 323.2969 - loglik: -3.1884e+02 - logprior: -4.4527e+00
Fitted a model with MAP estimate = -321.5602
expansions: [(0, 2)]
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 324.9409 - loglik: -3.1761e+02 - logprior: -7.3271e+00
Epoch 2/2
13/13 - 1s - loss: 318.5097 - loglik: -3.1633e+02 - logprior: -2.1824e+00
Fitted a model with MAP estimate = -317.3405
expansions: [(17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 327.9606 - loglik: -3.1860e+02 - logprior: -9.3574e+00
Epoch 2/10
13/13 - 1s - loss: 319.3147 - loglik: -3.1589e+02 - logprior: -3.4283e+00
Epoch 3/10
13/13 - 1s - loss: 316.5097 - loglik: -3.1484e+02 - logprior: -1.6692e+00
Epoch 4/10
13/13 - 1s - loss: 315.5700 - loglik: -3.1434e+02 - logprior: -1.2303e+00
Epoch 5/10
13/13 - 1s - loss: 314.3924 - loglik: -3.1325e+02 - logprior: -1.1435e+00
Epoch 6/10
13/13 - 1s - loss: 314.0120 - loglik: -3.1286e+02 - logprior: -1.1544e+00
Epoch 7/10
13/13 - 1s - loss: 313.7116 - loglik: -3.1255e+02 - logprior: -1.1624e+00
Epoch 8/10
13/13 - 1s - loss: 313.6911 - loglik: -3.1255e+02 - logprior: -1.1391e+00
Epoch 9/10
13/13 - 1s - loss: 314.1180 - loglik: -3.1300e+02 - logprior: -1.1226e+00
Fitted a model with MAP estimate = -313.3438
Time for alignment: 45.7551
Computed alignments with likelihoods: ['-313.5520', '-313.9216', '-313.7845', '-314.1817', '-313.3438']
Best model has likelihood: -313.3438  (prior= -1.0943 )
time for generating output: 0.1363
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HMG_box.projection.fasta
SP score = 0.8824362606232294
Training of 5 independent models on file hpr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f526aa10b50>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f528b6e40d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526a6e0f40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5300670dc0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f59204b97f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526a2be1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5289311910>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f528a9e81f0>, <__main__.SimpleDirichletPrior object at 0x7f594054d3d0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 477.6318 - loglik: -4.6380e+02 - logprior: -1.3833e+01
Epoch 2/10
11/11 - 1s - loss: 443.5191 - loglik: -4.3999e+02 - logprior: -3.5300e+00
Epoch 3/10
11/11 - 1s - loss: 416.4350 - loglik: -4.1430e+02 - logprior: -2.1341e+00
Epoch 4/10
11/11 - 1s - loss: 400.9590 - loglik: -3.9934e+02 - logprior: -1.6140e+00
Epoch 5/10
11/11 - 1s - loss: 395.7191 - loglik: -3.9432e+02 - logprior: -1.3972e+00
Epoch 6/10
11/11 - 1s - loss: 393.8069 - loglik: -3.9259e+02 - logprior: -1.2194e+00
Epoch 7/10
11/11 - 1s - loss: 392.9969 - loglik: -3.9198e+02 - logprior: -1.0197e+00
Epoch 8/10
11/11 - 1s - loss: 392.4020 - loglik: -3.9152e+02 - logprior: -8.7959e-01
Epoch 9/10
11/11 - 1s - loss: 392.0452 - loglik: -3.9121e+02 - logprior: -8.3220e-01
Epoch 10/10
11/11 - 1s - loss: 391.6050 - loglik: -3.9082e+02 - logprior: -7.8775e-01
Fitted a model with MAP estimate = -391.6255
expansions: [(0, 6), (24, 2), (27, 1), (29, 2), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 407.4014 - loglik: -3.9131e+02 - logprior: -1.6095e+01
Epoch 2/2
11/11 - 1s - loss: 386.6615 - loglik: -3.8213e+02 - logprior: -4.5266e+00
Fitted a model with MAP estimate = -383.0118
expansions: []
discards: [ 0 30 38]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 8s - loss: 397.8539 - loglik: -3.8373e+02 - logprior: -1.4127e+01
Epoch 2/2
11/11 - 1s - loss: 386.8745 - loglik: -3.8131e+02 - logprior: -5.5645e+00
Fitted a model with MAP estimate = -384.4735
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 393.4112 - loglik: -3.8112e+02 - logprior: -1.2288e+01
Epoch 2/10
11/11 - 1s - loss: 383.4187 - loglik: -3.8017e+02 - logprior: -3.2505e+00
Epoch 3/10
11/11 - 1s - loss: 381.1466 - loglik: -3.7944e+02 - logprior: -1.7023e+00
Epoch 4/10
11/11 - 1s - loss: 380.3124 - loglik: -3.7930e+02 - logprior: -1.0159e+00
Epoch 5/10
11/11 - 1s - loss: 378.9272 - loglik: -3.7811e+02 - logprior: -8.1599e-01
Epoch 6/10
11/11 - 1s - loss: 379.0827 - loglik: -3.7843e+02 - logprior: -6.5260e-01
Fitted a model with MAP estimate = -378.6988
Time for alignment: 43.2171
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 477.7669 - loglik: -4.6393e+02 - logprior: -1.3833e+01
Epoch 2/10
11/11 - 1s - loss: 443.6169 - loglik: -4.4009e+02 - logprior: -3.5296e+00
Epoch 3/10
11/11 - 1s - loss: 419.1359 - loglik: -4.1700e+02 - logprior: -2.1387e+00
Epoch 4/10
11/11 - 1s - loss: 403.4161 - loglik: -4.0182e+02 - logprior: -1.5988e+00
Epoch 5/10
11/11 - 1s - loss: 397.6582 - loglik: -3.9632e+02 - logprior: -1.3335e+00
Epoch 6/10
11/11 - 1s - loss: 395.5796 - loglik: -3.9446e+02 - logprior: -1.1178e+00
Epoch 7/10
11/11 - 1s - loss: 394.4497 - loglik: -3.9352e+02 - logprior: -9.2674e-01
Epoch 8/10
11/11 - 1s - loss: 393.9999 - loglik: -3.9317e+02 - logprior: -8.2845e-01
Epoch 9/10
11/11 - 1s - loss: 394.0376 - loglik: -3.9323e+02 - logprior: -8.0421e-01
Fitted a model with MAP estimate = -393.5353
expansions: [(0, 6), (22, 1), (27, 1), (29, 2), (31, 2), (32, 2), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 408.2887 - loglik: -3.9224e+02 - logprior: -1.6050e+01
Epoch 2/2
11/11 - 1s - loss: 386.5569 - loglik: -3.8204e+02 - logprior: -4.5192e+00
Fitted a model with MAP estimate = -383.1599
expansions: []
discards: [ 0 37 41 42]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 397.8085 - loglik: -3.8368e+02 - logprior: -1.4124e+01
Epoch 2/2
11/11 - 1s - loss: 387.4805 - loglik: -3.8192e+02 - logprior: -5.5559e+00
Fitted a model with MAP estimate = -384.5648
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 393.6692 - loglik: -3.8139e+02 - logprior: -1.2276e+01
Epoch 2/10
11/11 - 1s - loss: 383.5283 - loglik: -3.8026e+02 - logprior: -3.2668e+00
Epoch 3/10
11/11 - 1s - loss: 381.2453 - loglik: -3.7954e+02 - logprior: -1.7023e+00
Epoch 4/10
11/11 - 1s - loss: 379.6367 - loglik: -3.7861e+02 - logprior: -1.0289e+00
Epoch 5/10
11/11 - 1s - loss: 379.2775 - loglik: -3.7846e+02 - logprior: -8.2035e-01
Epoch 6/10
11/11 - 1s - loss: 378.9915 - loglik: -3.7833e+02 - logprior: -6.6491e-01
Epoch 7/10
11/11 - 1s - loss: 378.5377 - loglik: -3.7796e+02 - logprior: -5.7621e-01
Epoch 8/10
11/11 - 1s - loss: 378.7758 - loglik: -3.7825e+02 - logprior: -5.3013e-01
Fitted a model with MAP estimate = -378.4168
Time for alignment: 43.6095
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 477.3828 - loglik: -4.6355e+02 - logprior: -1.3832e+01
Epoch 2/10
11/11 - 1s - loss: 442.8671 - loglik: -4.3933e+02 - logprior: -3.5348e+00
Epoch 3/10
11/11 - 1s - loss: 416.6281 - loglik: -4.1448e+02 - logprior: -2.1444e+00
Epoch 4/10
11/11 - 1s - loss: 401.0930 - loglik: -3.9951e+02 - logprior: -1.5783e+00
Epoch 5/10
11/11 - 1s - loss: 395.4263 - loglik: -3.9407e+02 - logprior: -1.3535e+00
Epoch 6/10
11/11 - 1s - loss: 393.6613 - loglik: -3.9248e+02 - logprior: -1.1818e+00
Epoch 7/10
11/11 - 1s - loss: 392.8344 - loglik: -3.9181e+02 - logprior: -1.0201e+00
Epoch 8/10
11/11 - 1s - loss: 392.4100 - loglik: -3.9151e+02 - logprior: -8.9584e-01
Epoch 9/10
11/11 - 1s - loss: 391.7842 - loglik: -3.9093e+02 - logprior: -8.5149e-01
Epoch 10/10
11/11 - 1s - loss: 391.7879 - loglik: -3.9098e+02 - logprior: -8.0518e-01
Fitted a model with MAP estimate = -391.5877
expansions: [(0, 6), (21, 1), (27, 1), (28, 1), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 405.9921 - loglik: -3.8993e+02 - logprior: -1.6058e+01
Epoch 2/2
11/11 - 1s - loss: 386.3265 - loglik: -3.8190e+02 - logprior: -4.4256e+00
Fitted a model with MAP estimate = -382.8655
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 397.2618 - loglik: -3.8315e+02 - logprior: -1.4110e+01
Epoch 2/2
11/11 - 1s - loss: 386.6957 - loglik: -3.8113e+02 - logprior: -5.5664e+00
Fitted a model with MAP estimate = -384.1701
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 393.3898 - loglik: -3.8110e+02 - logprior: -1.2285e+01
Epoch 2/10
11/11 - 1s - loss: 383.3653 - loglik: -3.8011e+02 - logprior: -3.2568e+00
Epoch 3/10
11/11 - 1s - loss: 380.8876 - loglik: -3.7920e+02 - logprior: -1.6834e+00
Epoch 4/10
11/11 - 1s - loss: 379.8127 - loglik: -3.7879e+02 - logprior: -1.0256e+00
Epoch 5/10
11/11 - 1s - loss: 379.6099 - loglik: -3.7879e+02 - logprior: -8.1616e-01
Epoch 6/10
11/11 - 1s - loss: 378.7938 - loglik: -3.7813e+02 - logprior: -6.5926e-01
Epoch 7/10
11/11 - 1s - loss: 378.7904 - loglik: -3.7822e+02 - logprior: -5.6615e-01
Epoch 8/10
11/11 - 1s - loss: 378.3962 - loglik: -3.7787e+02 - logprior: -5.2587e-01
Epoch 9/10
11/11 - 1s - loss: 378.5484 - loglik: -3.7806e+02 - logprior: -4.9181e-01
Fitted a model with MAP estimate = -378.3261
Time for alignment: 42.1222
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 477.6885 - loglik: -4.6385e+02 - logprior: -1.3834e+01
Epoch 2/10
11/11 - 1s - loss: 443.4356 - loglik: -4.3989e+02 - logprior: -3.5407e+00
Epoch 3/10
11/11 - 1s - loss: 418.3135 - loglik: -4.1613e+02 - logprior: -2.1873e+00
Epoch 4/10
11/11 - 1s - loss: 401.6935 - loglik: -4.0002e+02 - logprior: -1.6750e+00
Epoch 5/10
11/11 - 1s - loss: 396.8719 - loglik: -3.9544e+02 - logprior: -1.4288e+00
Epoch 6/10
11/11 - 1s - loss: 393.9716 - loglik: -3.9273e+02 - logprior: -1.2397e+00
Epoch 7/10
11/11 - 1s - loss: 392.8203 - loglik: -3.9180e+02 - logprior: -1.0225e+00
Epoch 8/10
11/11 - 1s - loss: 392.6823 - loglik: -3.9180e+02 - logprior: -8.8398e-01
Epoch 9/10
11/11 - 1s - loss: 392.3404 - loglik: -3.9150e+02 - logprior: -8.3897e-01
Epoch 10/10
11/11 - 1s - loss: 392.1104 - loglik: -3.9131e+02 - logprior: -7.9934e-01
Fitted a model with MAP estimate = -391.8531
expansions: [(0, 6), (21, 1), (23, 2), (29, 2), (32, 2), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 408.4465 - loglik: -3.9233e+02 - logprior: -1.6119e+01
Epoch 2/2
11/11 - 1s - loss: 386.4500 - loglik: -3.8187e+02 - logprior: -4.5844e+00
Fitted a model with MAP estimate = -383.2937
expansions: []
discards: [ 0 30 39 43]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 398.0278 - loglik: -3.8389e+02 - logprior: -1.4140e+01
Epoch 2/2
11/11 - 1s - loss: 386.6296 - loglik: -3.8106e+02 - logprior: -5.5679e+00
Fitted a model with MAP estimate = -384.2267
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 393.5761 - loglik: -3.8129e+02 - logprior: -1.2283e+01
Epoch 2/10
11/11 - 1s - loss: 383.1095 - loglik: -3.7985e+02 - logprior: -3.2554e+00
Epoch 3/10
11/11 - 1s - loss: 381.0639 - loglik: -3.7937e+02 - logprior: -1.6936e+00
Epoch 4/10
11/11 - 1s - loss: 379.8629 - loglik: -3.7884e+02 - logprior: -1.0207e+00
Epoch 5/10
11/11 - 1s - loss: 379.3663 - loglik: -3.7854e+02 - logprior: -8.2435e-01
Epoch 6/10
11/11 - 1s - loss: 379.1742 - loglik: -3.7853e+02 - logprior: -6.4320e-01
Epoch 7/10
11/11 - 1s - loss: 378.5847 - loglik: -3.7800e+02 - logprior: -5.8082e-01
Epoch 8/10
11/11 - 1s - loss: 378.6778 - loglik: -3.7816e+02 - logprior: -5.1952e-01
Fitted a model with MAP estimate = -378.4087
Time for alignment: 44.4224
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 477.6372 - loglik: -4.6380e+02 - logprior: -1.3832e+01
Epoch 2/10
11/11 - 1s - loss: 443.6958 - loglik: -4.4017e+02 - logprior: -3.5272e+00
Epoch 3/10
11/11 - 1s - loss: 417.7153 - loglik: -4.1558e+02 - logprior: -2.1349e+00
Epoch 4/10
11/11 - 1s - loss: 401.3334 - loglik: -3.9974e+02 - logprior: -1.5898e+00
Epoch 5/10
11/11 - 1s - loss: 396.6550 - loglik: -3.9532e+02 - logprior: -1.3375e+00
Epoch 6/10
11/11 - 1s - loss: 393.5749 - loglik: -3.9243e+02 - logprior: -1.1477e+00
Epoch 7/10
11/11 - 1s - loss: 392.9623 - loglik: -3.9201e+02 - logprior: -9.5265e-01
Epoch 8/10
11/11 - 1s - loss: 392.8711 - loglik: -3.9205e+02 - logprior: -8.2276e-01
Epoch 9/10
11/11 - 1s - loss: 392.2139 - loglik: -3.9142e+02 - logprior: -7.9724e-01
Epoch 10/10
11/11 - 1s - loss: 391.8681 - loglik: -3.9111e+02 - logprior: -7.5954e-01
Fitted a model with MAP estimate = -391.8184
expansions: [(0, 6), (22, 1), (23, 2), (28, 1), (29, 2), (31, 2), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 407.9875 - loglik: -3.9188e+02 - logprior: -1.6111e+01
Epoch 2/2
11/11 - 1s - loss: 386.6222 - loglik: -3.8204e+02 - logprior: -4.5865e+00
Fitted a model with MAP estimate = -383.2383
expansions: []
discards: [ 0 30 39 43]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 397.8431 - loglik: -3.8371e+02 - logprior: -1.4134e+01
Epoch 2/2
11/11 - 1s - loss: 386.5364 - loglik: -3.8097e+02 - logprior: -5.5618e+00
Fitted a model with MAP estimate = -384.2545
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 393.2981 - loglik: -3.8101e+02 - logprior: -1.2283e+01
Epoch 2/10
11/11 - 1s - loss: 383.3312 - loglik: -3.8008e+02 - logprior: -3.2484e+00
Epoch 3/10
11/11 - 1s - loss: 381.0385 - loglik: -3.7934e+02 - logprior: -1.7012e+00
Epoch 4/10
11/11 - 1s - loss: 379.9042 - loglik: -3.7888e+02 - logprior: -1.0233e+00
Epoch 5/10
11/11 - 1s - loss: 379.5496 - loglik: -3.7873e+02 - logprior: -8.1689e-01
Epoch 6/10
11/11 - 1s - loss: 378.6733 - loglik: -3.7802e+02 - logprior: -6.5760e-01
Epoch 7/10
11/11 - 1s - loss: 378.7516 - loglik: -3.7818e+02 - logprior: -5.7290e-01
Fitted a model with MAP estimate = -378.5217
Time for alignment: 42.2899
Computed alignments with likelihoods: ['-378.6988', '-378.4168', '-378.3261', '-378.4087', '-378.5217']
Best model has likelihood: -378.3261  (prior= -0.4611 )
time for generating output: 0.1592
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hpr.projection.fasta
SP score = 0.993006993006993
Training of 5 independent models on file tim.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f530db80640>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52642d0d00>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f527070f5b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f527f4a3ac0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5285d4ab80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52776b23a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b0854e040>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5cf3d0f5e0>, <__main__.SimpleDirichletPrior object at 0x7f5b12c6a820>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 10s - loss: 1116.2684 - loglik: -1.1092e+03 - logprior: -7.0776e+00
Epoch 2/10
24/24 - 7s - loss: 971.1292 - loglik: -9.6880e+02 - logprior: -2.3334e+00
Epoch 3/10
24/24 - 7s - loss: 935.1883 - loglik: -9.3266e+02 - logprior: -2.5265e+00
Epoch 4/10
24/24 - 7s - loss: 935.3397 - loglik: -9.3298e+02 - logprior: -2.3622e+00
Fitted a model with MAP estimate = -932.7387
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (16, 1), (18, 1), (19, 1), (35, 2), (36, 1), (37, 1), (38, 1), (40, 1), (49, 2), (50, 2), (64, 1), (66, 1), (76, 1), (83, 1), (87, 1), (88, 1), (91, 1), (111, 1), (114, 1), (117, 1), (120, 2), (121, 2), (123, 1), (150, 1), (153, 2), (154, 4), (155, 1), (156, 1), (158, 1), (172, 1), (173, 2), (174, 2), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 12s - loss: 924.0300 - loglik: -9.1630e+02 - logprior: -7.7275e+00
Epoch 2/2
24/24 - 9s - loss: 902.9649 - loglik: -9.0041e+02 - logprior: -2.5516e+00
Fitted a model with MAP estimate = -900.7906
expansions: [(0, 2)]
discards: [ 0 12 63]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 12s - loss: 903.6899 - loglik: -8.9919e+02 - logprior: -4.4962e+00
Epoch 2/2
24/24 - 9s - loss: 898.6417 - loglik: -8.9860e+02 - logprior: -3.7377e-02
Fitted a model with MAP estimate = -896.5922
expansions: [(150, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 12s - loss: 904.0994 - loglik: -8.9994e+02 - logprior: -4.1614e+00
Epoch 2/10
24/24 - 9s - loss: 896.2891 - loglik: -8.9660e+02 - logprior: 0.3132
Epoch 3/10
24/24 - 9s - loss: 898.9860 - loglik: -8.9979e+02 - logprior: 0.8042
Fitted a model with MAP estimate = -894.4267
Time for alignment: 132.9940
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 10s - loss: 1116.2130 - loglik: -1.1091e+03 - logprior: -7.0804e+00
Epoch 2/10
24/24 - 7s - loss: 967.0893 - loglik: -9.6474e+02 - logprior: -2.3532e+00
Epoch 3/10
24/24 - 7s - loss: 941.1241 - loglik: -9.3857e+02 - logprior: -2.5548e+00
Epoch 4/10
24/24 - 7s - loss: 934.5276 - loglik: -9.3220e+02 - logprior: -2.3322e+00
Epoch 5/10
24/24 - 7s - loss: 934.6972 - loglik: -9.3232e+02 - logprior: -2.3767e+00
Fitted a model with MAP estimate = -932.5470
expansions: [(11, 1), (12, 3), (15, 1), (17, 1), (18, 2), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (62, 1), (66, 1), (76, 1), (78, 1), (85, 1), (88, 1), (91, 1), (109, 1), (110, 1), (117, 1), (118, 1), (119, 2), (120, 1), (121, 1), (122, 2), (123, 1), (150, 1), (153, 1), (154, 4), (155, 1), (158, 1), (173, 2), (174, 2), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 16s - loss: 925.2963 - loglik: -9.1754e+02 - logprior: -7.7586e+00
Epoch 2/2
24/24 - 8s - loss: 905.4975 - loglik: -9.0292e+02 - logprior: -2.5781e+00
Fitted a model with MAP estimate = -902.2563
expansions: [(0, 2), (12, 1), (13, 1), (190, 1), (192, 1)]
discards: [  0  22 151]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 13s - loss: 906.1201 - loglik: -9.0163e+02 - logprior: -4.4879e+00
Epoch 2/2
24/24 - 9s - loss: 897.1336 - loglik: -8.9715e+02 - logprior: 0.0204
Fitted a model with MAP estimate = -896.3040
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 12s - loss: 905.6816 - loglik: -9.0155e+02 - logprior: -4.1343e+00
Epoch 2/10
24/24 - 9s - loss: 892.7684 - loglik: -8.9310e+02 - logprior: 0.3302
Epoch 3/10
24/24 - 9s - loss: 896.2646 - loglik: -8.9706e+02 - logprior: 0.7911
Fitted a model with MAP estimate = -894.4958
Time for alignment: 145.5177
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 10s - loss: 1115.0392 - loglik: -1.1079e+03 - logprior: -7.1080e+00
Epoch 2/10
24/24 - 7s - loss: 966.8730 - loglik: -9.6446e+02 - logprior: -2.4096e+00
Epoch 3/10
24/24 - 7s - loss: 937.6682 - loglik: -9.3497e+02 - logprior: -2.6966e+00
Epoch 4/10
24/24 - 7s - loss: 931.4800 - loglik: -9.2898e+02 - logprior: -2.5011e+00
Epoch 5/10
24/24 - 7s - loss: 934.5898 - loglik: -9.3210e+02 - logprior: -2.4862e+00
Fitted a model with MAP estimate = -931.5082
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (15, 1), (16, 1), (17, 2), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (62, 1), (63, 1), (65, 1), (83, 1), (87, 1), (88, 1), (90, 1), (93, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (155, 1), (158, 1), (173, 2), (174, 2), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 12s - loss: 925.3246 - loglik: -9.1758e+02 - logprior: -7.7466e+00
Epoch 2/2
24/24 - 9s - loss: 904.3577 - loglik: -9.0186e+02 - logprior: -2.4944e+00
Fitted a model with MAP estimate = -901.8852
expansions: [(0, 2), (191, 1), (193, 1)]
discards: [ 0 12]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 12s - loss: 906.1120 - loglik: -9.0161e+02 - logprior: -4.5044e+00
Epoch 2/2
24/24 - 9s - loss: 897.5918 - loglik: -8.9756e+02 - logprior: -3.4264e-02
Fitted a model with MAP estimate = -896.1890
expansions: [(151, 1)]
discards: [25]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 13s - loss: 900.9935 - loglik: -8.9683e+02 - logprior: -4.1597e+00
Epoch 2/10
24/24 - 9s - loss: 901.8343 - loglik: -9.0214e+02 - logprior: 0.3054
Fitted a model with MAP estimate = -895.6970
Time for alignment: 135.3111
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 10s - loss: 1116.0942 - loglik: -1.1090e+03 - logprior: -7.0689e+00
Epoch 2/10
24/24 - 7s - loss: 967.4849 - loglik: -9.6521e+02 - logprior: -2.2717e+00
Epoch 3/10
24/24 - 7s - loss: 938.3755 - loglik: -9.3582e+02 - logprior: -2.5547e+00
Epoch 4/10
24/24 - 7s - loss: 934.8826 - loglik: -9.3252e+02 - logprior: -2.3591e+00
Epoch 5/10
24/24 - 7s - loss: 931.3283 - loglik: -9.2894e+02 - logprior: -2.3876e+00
Epoch 6/10
24/24 - 7s - loss: 931.0670 - loglik: -9.2862e+02 - logprior: -2.4464e+00
Epoch 7/10
24/24 - 7s - loss: 930.9138 - loglik: -9.2839e+02 - logprior: -2.5207e+00
Epoch 8/10
24/24 - 7s - loss: 930.9659 - loglik: -9.2837e+02 - logprior: -2.5913e+00
Fitted a model with MAP estimate = -930.6570
expansions: [(11, 1), (12, 3), (16, 1), (17, 1), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (62, 1), (63, 1), (65, 1), (76, 1), (85, 1), (87, 1), (90, 1), (93, 1), (110, 1), (113, 1), (118, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (155, 2), (158, 1), (171, 1), (172, 1), (173, 2), (174, 1), (186, 1), (187, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 12s - loss: 923.5622 - loglik: -9.1579e+02 - logprior: -7.7746e+00
Epoch 2/2
24/24 - 8s - loss: 908.1709 - loglik: -9.0562e+02 - logprior: -2.5558e+00
Fitted a model with MAP estimate = -903.4403
expansions: [(0, 3), (12, 1), (13, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 12s - loss: 905.6218 - loglik: -9.0110e+02 - logprior: -4.5236e+00
Epoch 2/2
24/24 - 8s - loss: 898.2007 - loglik: -8.9819e+02 - logprior: -1.5371e-02
Fitted a model with MAP estimate = -896.4889
expansions: [(151, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 12s - loss: 910.0545 - loglik: -9.0367e+02 - logprior: -6.3835e+00
Epoch 2/10
24/24 - 8s - loss: 899.3519 - loglik: -8.9857e+02 - logprior: -7.8136e-01
Epoch 3/10
24/24 - 8s - loss: 899.4837 - loglik: -9.0071e+02 - logprior: 1.2282
Fitted a model with MAP estimate = -896.2379
Time for alignment: 159.8530
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 12s - loss: 1115.9777 - loglik: -1.1089e+03 - logprior: -7.0835e+00
Epoch 2/10
24/24 - 7s - loss: 967.1815 - loglik: -9.6483e+02 - logprior: -2.3533e+00
Epoch 3/10
24/24 - 7s - loss: 936.9017 - loglik: -9.3422e+02 - logprior: -2.6786e+00
Epoch 4/10
24/24 - 7s - loss: 933.5150 - loglik: -9.3103e+02 - logprior: -2.4853e+00
Epoch 5/10
24/24 - 7s - loss: 929.7990 - loglik: -9.2731e+02 - logprior: -2.4842e+00
Epoch 6/10
24/24 - 7s - loss: 930.7122 - loglik: -9.2810e+02 - logprior: -2.6089e+00
Fitted a model with MAP estimate = -930.1869
expansions: [(12, 2), (13, 1), (14, 2), (16, 2), (18, 1), (19, 1), (21, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (62, 1), (63, 1), (65, 1), (83, 1), (85, 1), (87, 1), (90, 1), (93, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 3), (122, 1), (150, 1), (153, 1), (154, 4), (158, 1), (172, 1), (173, 3), (174, 2), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 12s - loss: 925.5563 - loglik: -9.1780e+02 - logprior: -7.7522e+00
Epoch 2/2
24/24 - 9s - loss: 901.4534 - loglik: -8.9901e+02 - logprior: -2.4408e+00
Fitted a model with MAP estimate = -901.3843
expansions: [(0, 2), (191, 1), (193, 1)]
discards: [  0 218]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 12s - loss: 904.9656 - loglik: -9.0054e+02 - logprior: -4.4301e+00
Epoch 2/2
24/24 - 9s - loss: 897.1272 - loglik: -8.9718e+02 - logprior: 0.0553
Fitted a model with MAP estimate = -895.9140
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 243 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 12s - loss: 904.9107 - loglik: -9.0082e+02 - logprior: -4.0920e+00
Epoch 2/10
24/24 - 9s - loss: 895.3718 - loglik: -8.9575e+02 - logprior: 0.3745
Epoch 3/10
24/24 - 9s - loss: 894.3565 - loglik: -8.9522e+02 - logprior: 0.8655
Epoch 4/10
24/24 - 9s - loss: 895.4441 - loglik: -8.9648e+02 - logprior: 1.0361
Fitted a model with MAP estimate = -893.2473
Time for alignment: 158.5460
Computed alignments with likelihoods: ['-894.4267', '-894.4958', '-895.6970', '-896.2379', '-893.2473']
Best model has likelihood: -893.2473  (prior= 1.1035 )
time for generating output: 0.2974
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tim.projection.fasta
SP score = 0.9799167722091551
Training of 5 independent models on file tgfb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5264a1f400>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52697386d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526c1125e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526e9379d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5263f4ff70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5263f4fc40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5272e26280>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f526e94b730>, <__main__.SimpleDirichletPrior object at 0x7f5314afb220>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 597.7668 - loglik: -5.6716e+02 - logprior: -3.0606e+01
Epoch 2/10
10/10 - 1s - loss: 538.3757 - loglik: -5.3069e+02 - logprior: -7.6863e+00
Epoch 3/10
10/10 - 1s - loss: 500.0045 - loglik: -4.9608e+02 - logprior: -3.9221e+00
Epoch 4/10
10/10 - 1s - loss: 481.8771 - loglik: -4.7910e+02 - logprior: -2.7762e+00
Epoch 5/10
10/10 - 1s - loss: 472.8603 - loglik: -4.7061e+02 - logprior: -2.2529e+00
Epoch 6/10
10/10 - 1s - loss: 470.0514 - loglik: -4.6821e+02 - logprior: -1.8415e+00
Epoch 7/10
10/10 - 1s - loss: 468.2376 - loglik: -4.6665e+02 - logprior: -1.5846e+00
Epoch 8/10
10/10 - 1s - loss: 467.6100 - loglik: -4.6607e+02 - logprior: -1.5369e+00
Epoch 9/10
10/10 - 1s - loss: 466.8669 - loglik: -4.6537e+02 - logprior: -1.4941e+00
Epoch 10/10
10/10 - 1s - loss: 465.4863 - loglik: -4.6406e+02 - logprior: -1.4252e+00
Fitted a model with MAP estimate = -465.7853
expansions: [(0, 3), (5, 1), (10, 1), (34, 1), (36, 5), (48, 3), (49, 2), (60, 1), (69, 1), (70, 1), (72, 2), (73, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 106 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 496.2585 - loglik: -4.6093e+02 - logprior: -3.5327e+01
Epoch 2/2
10/10 - 1s - loss: 460.7031 - loglik: -4.5038e+02 - logprior: -1.0327e+01
Fitted a model with MAP estimate = -453.7269
expansions: []
discards: [ 0 92]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 481.0607 - loglik: -4.5013e+02 - logprior: -3.0931e+01
Epoch 2/2
10/10 - 1s - loss: 459.8233 - loglik: -4.4757e+02 - logprior: -1.2250e+01
Fitted a model with MAP estimate = -455.9196
expansions: [(92, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 476.3236 - loglik: -4.4733e+02 - logprior: -2.8996e+01
Epoch 2/10
10/10 - 1s - loss: 454.3017 - loglik: -4.4613e+02 - logprior: -8.1702e+00
Epoch 3/10
10/10 - 1s - loss: 447.2062 - loglik: -4.4424e+02 - logprior: -2.9631e+00
Epoch 4/10
10/10 - 1s - loss: 445.1785 - loglik: -4.4369e+02 - logprior: -1.4851e+00
Epoch 5/10
10/10 - 1s - loss: 443.7351 - loglik: -4.4297e+02 - logprior: -7.6463e-01
Epoch 6/10
10/10 - 1s - loss: 443.0147 - loglik: -4.4287e+02 - logprior: -1.4547e-01
Epoch 7/10
10/10 - 1s - loss: 442.5898 - loglik: -4.4277e+02 - logprior: 0.1763
Epoch 8/10
10/10 - 1s - loss: 442.6048 - loglik: -4.4293e+02 - logprior: 0.3299
Fitted a model with MAP estimate = -442.3679
Time for alignment: 46.7284
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 598.3099 - loglik: -5.6770e+02 - logprior: -3.0608e+01
Epoch 2/10
10/10 - 1s - loss: 537.5757 - loglik: -5.2989e+02 - logprior: -7.6891e+00
Epoch 3/10
10/10 - 1s - loss: 499.3167 - loglik: -4.9539e+02 - logprior: -3.9292e+00
Epoch 4/10
10/10 - 1s - loss: 481.9478 - loglik: -4.7912e+02 - logprior: -2.8241e+00
Epoch 5/10
10/10 - 1s - loss: 474.1163 - loglik: -4.7184e+02 - logprior: -2.2795e+00
Epoch 6/10
10/10 - 1s - loss: 469.9629 - loglik: -4.6804e+02 - logprior: -1.9184e+00
Epoch 7/10
10/10 - 1s - loss: 467.2241 - loglik: -4.6549e+02 - logprior: -1.7300e+00
Epoch 8/10
10/10 - 1s - loss: 466.8026 - loglik: -4.6516e+02 - logprior: -1.6396e+00
Epoch 9/10
10/10 - 1s - loss: 465.9837 - loglik: -4.6444e+02 - logprior: -1.5483e+00
Epoch 10/10
10/10 - 1s - loss: 466.5351 - loglik: -4.6504e+02 - logprior: -1.4965e+00
Fitted a model with MAP estimate = -465.7853
expansions: [(0, 2), (11, 1), (12, 1), (34, 1), (35, 3), (36, 2), (37, 1), (48, 3), (49, 2), (69, 2), (70, 1), (72, 2), (73, 1), (74, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 107 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 496.1950 - loglik: -4.6088e+02 - logprior: -3.5311e+01
Epoch 2/2
10/10 - 1s - loss: 461.4479 - loglik: -4.5107e+02 - logprior: -1.0381e+01
Fitted a model with MAP estimate = -454.6367
expansions: []
discards: [44 91]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 473.2554 - loglik: -4.4795e+02 - logprior: -2.5305e+01
Epoch 2/2
10/10 - 1s - loss: 452.7477 - loglik: -4.4640e+02 - logprior: -6.3487e+00
Fitted a model with MAP estimate = -448.9659
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 469.2085 - loglik: -4.4516e+02 - logprior: -2.4051e+01
Epoch 2/10
10/10 - 1s - loss: 450.3256 - loglik: -4.4433e+02 - logprior: -5.9921e+00
Epoch 3/10
10/10 - 1s - loss: 447.2961 - loglik: -4.4486e+02 - logprior: -2.4400e+00
Epoch 4/10
10/10 - 1s - loss: 444.9516 - loglik: -4.4386e+02 - logprior: -1.0907e+00
Epoch 5/10
10/10 - 1s - loss: 443.9967 - loglik: -4.4355e+02 - logprior: -4.4349e-01
Epoch 6/10
10/10 - 1s - loss: 443.7615 - loglik: -4.4365e+02 - logprior: -1.1510e-01
Epoch 7/10
10/10 - 1s - loss: 443.0441 - loglik: -4.4320e+02 - logprior: 0.1596
Epoch 8/10
10/10 - 1s - loss: 443.2825 - loglik: -4.4366e+02 - logprior: 0.3753
Fitted a model with MAP estimate = -442.7949
Time for alignment: 45.8463
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 598.0731 - loglik: -5.6747e+02 - logprior: -3.0607e+01
Epoch 2/10
10/10 - 1s - loss: 536.6810 - loglik: -5.2900e+02 - logprior: -7.6824e+00
Epoch 3/10
10/10 - 1s - loss: 498.1261 - loglik: -4.9425e+02 - logprior: -3.8790e+00
Epoch 4/10
10/10 - 1s - loss: 478.7946 - loglik: -4.7600e+02 - logprior: -2.7961e+00
Epoch 5/10
10/10 - 1s - loss: 472.2488 - loglik: -4.6990e+02 - logprior: -2.3492e+00
Epoch 6/10
10/10 - 1s - loss: 469.3318 - loglik: -4.6736e+02 - logprior: -1.9725e+00
Epoch 7/10
10/10 - 1s - loss: 466.9900 - loglik: -4.6520e+02 - logprior: -1.7901e+00
Epoch 8/10
10/10 - 1s - loss: 466.7379 - loglik: -4.6500e+02 - logprior: -1.7350e+00
Epoch 9/10
10/10 - 1s - loss: 466.4995 - loglik: -4.6483e+02 - logprior: -1.6716e+00
Epoch 10/10
10/10 - 1s - loss: 465.2717 - loglik: -4.6366e+02 - logprior: -1.6133e+00
Fitted a model with MAP estimate = -465.3540
expansions: [(0, 2), (11, 1), (12, 1), (34, 1), (36, 4), (48, 3), (49, 2), (58, 1), (69, 1), (70, 1), (72, 2), (73, 1), (74, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 496.7152 - loglik: -4.6124e+02 - logprior: -3.5480e+01
Epoch 2/2
10/10 - 1s - loss: 461.9738 - loglik: -4.5157e+02 - logprior: -1.0406e+01
Fitted a model with MAP estimate = -455.6283
expansions: []
discards: [90]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 474.1068 - loglik: -4.4869e+02 - logprior: -2.5418e+01
Epoch 2/2
10/10 - 1s - loss: 453.9269 - loglik: -4.4744e+02 - logprior: -6.4822e+00
Fitted a model with MAP estimate = -450.1799
expansions: [(46, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 470.6066 - loglik: -4.4650e+02 - logprior: -2.4110e+01
Epoch 2/10
10/10 - 1s - loss: 450.4947 - loglik: -4.4449e+02 - logprior: -6.0015e+00
Epoch 3/10
10/10 - 1s - loss: 447.0470 - loglik: -4.4462e+02 - logprior: -2.4275e+00
Epoch 4/10
10/10 - 1s - loss: 444.9113 - loglik: -4.4383e+02 - logprior: -1.0789e+00
Epoch 5/10
10/10 - 1s - loss: 444.1444 - loglik: -4.4371e+02 - logprior: -4.3015e-01
Epoch 6/10
10/10 - 1s - loss: 442.7936 - loglik: -4.4269e+02 - logprior: -1.0407e-01
Epoch 7/10
10/10 - 1s - loss: 443.7563 - loglik: -4.4393e+02 - logprior: 0.1726
Fitted a model with MAP estimate = -442.8720
Time for alignment: 41.7905
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 598.0895 - loglik: -5.6748e+02 - logprior: -3.0605e+01
Epoch 2/10
10/10 - 1s - loss: 538.0869 - loglik: -5.3039e+02 - logprior: -7.6940e+00
Epoch 3/10
10/10 - 1s - loss: 500.2755 - loglik: -4.9629e+02 - logprior: -3.9847e+00
Epoch 4/10
10/10 - 1s - loss: 480.3852 - loglik: -4.7738e+02 - logprior: -3.0091e+00
Epoch 5/10
10/10 - 1s - loss: 470.7202 - loglik: -4.6808e+02 - logprior: -2.6445e+00
Epoch 6/10
10/10 - 1s - loss: 467.7349 - loglik: -4.6551e+02 - logprior: -2.2259e+00
Epoch 7/10
10/10 - 1s - loss: 466.2286 - loglik: -4.6429e+02 - logprior: -1.9407e+00
Epoch 8/10
10/10 - 1s - loss: 465.7215 - loglik: -4.6386e+02 - logprior: -1.8584e+00
Epoch 9/10
10/10 - 1s - loss: 465.4665 - loglik: -4.6366e+02 - logprior: -1.8063e+00
Epoch 10/10
10/10 - 1s - loss: 465.1183 - loglik: -4.6334e+02 - logprior: -1.7783e+00
Fitted a model with MAP estimate = -464.8062
expansions: [(0, 2), (11, 1), (12, 1), (34, 1), (35, 3), (36, 2), (37, 1), (48, 3), (49, 2), (60, 1), (69, 2), (70, 1), (71, 2), (72, 1), (73, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 107 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 496.0605 - loglik: -4.6062e+02 - logprior: -3.5443e+01
Epoch 2/2
10/10 - 1s - loss: 460.3904 - loglik: -4.4995e+02 - logprior: -1.0439e+01
Fitted a model with MAP estimate = -454.2044
expansions: []
discards: [44 91]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 473.2969 - loglik: -4.4795e+02 - logprior: -2.5350e+01
Epoch 2/2
10/10 - 1s - loss: 450.6190 - loglik: -4.4427e+02 - logprior: -6.3466e+00
Fitted a model with MAP estimate = -448.0434
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 468.8569 - loglik: -4.4482e+02 - logprior: -2.4038e+01
Epoch 2/10
10/10 - 1s - loss: 449.5349 - loglik: -4.4355e+02 - logprior: -5.9817e+00
Epoch 3/10
10/10 - 1s - loss: 446.7537 - loglik: -4.4431e+02 - logprior: -2.4408e+00
Epoch 4/10
10/10 - 1s - loss: 444.1303 - loglik: -4.4303e+02 - logprior: -1.0967e+00
Epoch 5/10
10/10 - 1s - loss: 443.3588 - loglik: -4.4290e+02 - logprior: -4.5411e-01
Epoch 6/10
10/10 - 1s - loss: 443.4376 - loglik: -4.4331e+02 - logprior: -1.2334e-01
Fitted a model with MAP estimate = -442.7797
Time for alignment: 42.5397
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 598.2017 - loglik: -5.6760e+02 - logprior: -3.0605e+01
Epoch 2/10
10/10 - 1s - loss: 537.7059 - loglik: -5.3003e+02 - logprior: -7.6809e+00
Epoch 3/10
10/10 - 1s - loss: 499.9843 - loglik: -4.9608e+02 - logprior: -3.9012e+00
Epoch 4/10
10/10 - 1s - loss: 482.2839 - loglik: -4.7955e+02 - logprior: -2.7297e+00
Epoch 5/10
10/10 - 1s - loss: 473.1063 - loglik: -4.7093e+02 - logprior: -2.1739e+00
Epoch 6/10
10/10 - 1s - loss: 469.1143 - loglik: -4.6742e+02 - logprior: -1.6923e+00
Epoch 7/10
10/10 - 1s - loss: 468.3055 - loglik: -4.6690e+02 - logprior: -1.4094e+00
Epoch 8/10
10/10 - 1s - loss: 466.0278 - loglik: -4.6470e+02 - logprior: -1.3306e+00
Epoch 9/10
10/10 - 1s - loss: 466.2215 - loglik: -4.6498e+02 - logprior: -1.2397e+00
Fitted a model with MAP estimate = -465.9724
expansions: [(0, 4), (12, 1), (33, 1), (36, 4), (50, 3), (69, 2), (70, 1), (72, 2), (73, 1), (74, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 496.2334 - loglik: -4.6138e+02 - logprior: -3.4853e+01
Epoch 2/2
10/10 - 1s - loss: 462.9515 - loglik: -4.5275e+02 - logprior: -1.0200e+01
Fitted a model with MAP estimate = -456.1323
expansions: []
discards: [ 0 89]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 483.4707 - loglik: -4.5257e+02 - logprior: -3.0898e+01
Epoch 2/2
10/10 - 1s - loss: 463.1349 - loglik: -4.5090e+02 - logprior: -1.2231e+01
Fitted a model with MAP estimate = -459.1878
expansions: [(0, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 477.5525 - loglik: -4.5010e+02 - logprior: -2.7455e+01
Epoch 2/10
10/10 - 1s - loss: 454.9767 - loglik: -4.4800e+02 - logprior: -6.9751e+00
Epoch 3/10
10/10 - 1s - loss: 450.0775 - loglik: -4.4740e+02 - logprior: -2.6766e+00
Epoch 4/10
10/10 - 1s - loss: 447.3753 - loglik: -4.4620e+02 - logprior: -1.1752e+00
Epoch 5/10
10/10 - 1s - loss: 446.7082 - loglik: -4.4622e+02 - logprior: -4.8963e-01
Epoch 6/10
10/10 - 1s - loss: 445.9308 - loglik: -4.4579e+02 - logprior: -1.4401e-01
Epoch 7/10
10/10 - 1s - loss: 446.0259 - loglik: -4.4611e+02 - logprior: 0.0792
Fitted a model with MAP estimate = -445.3472
Time for alignment: 41.6537
Computed alignments with likelihoods: ['-442.3679', '-442.7949', '-442.8720', '-442.7797', '-445.3472']
Best model has likelihood: -442.3679  (prior= 0.4217 )
time for generating output: 0.1694
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tgfb.projection.fasta
SP score = 0.8244725738396624
Training of 5 independent models on file HLH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5268391310>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52692a9fa0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f53445f6100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52e19e4220>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526e9379d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5264a1f400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52697386d0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5aff38b910>, <__main__.SimpleDirichletPrior object at 0x7f527ea525b0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 303.9420 - loglik: -2.9854e+02 - logprior: -5.4041e+00
Epoch 2/10
16/16 - 2s - loss: 277.9839 - loglik: -2.7633e+02 - logprior: -1.6571e+00
Epoch 3/10
16/16 - 2s - loss: 267.7491 - loglik: -2.6610e+02 - logprior: -1.6515e+00
Epoch 4/10
16/16 - 2s - loss: 263.4638 - loglik: -2.6185e+02 - logprior: -1.6181e+00
Epoch 5/10
16/16 - 2s - loss: 262.3939 - loglik: -2.6078e+02 - logprior: -1.6120e+00
Epoch 6/10
16/16 - 2s - loss: 261.6787 - loglik: -2.6000e+02 - logprior: -1.6829e+00
Epoch 7/10
16/16 - 2s - loss: 261.3725 - loglik: -2.5971e+02 - logprior: -1.6620e+00
Epoch 8/10
16/16 - 2s - loss: 262.0760 - loglik: -2.6040e+02 - logprior: -1.6772e+00
Fitted a model with MAP estimate = -261.4929
expansions: [(3, 1), (6, 1), (13, 1), (15, 2), (16, 1), (23, 6), (25, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 266.1033 - loglik: -2.5961e+02 - logprior: -6.4884e+00
Epoch 2/2
16/16 - 2s - loss: 258.0632 - loglik: -2.5504e+02 - logprior: -3.0275e+00
Fitted a model with MAP estimate = -256.6954
expansions: [(0, 1)]
discards: [ 0 31 36]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 259.2703 - loglik: -2.5465e+02 - logprior: -4.6186e+00
Epoch 2/2
16/16 - 2s - loss: 255.1252 - loglik: -2.5349e+02 - logprior: -1.6346e+00
Fitted a model with MAP estimate = -254.9450
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 261.7061 - loglik: -2.5543e+02 - logprior: -6.2722e+00
Epoch 2/10
16/16 - 2s - loss: 256.9903 - loglik: -2.5426e+02 - logprior: -2.7259e+00
Epoch 3/10
16/16 - 2s - loss: 254.9895 - loglik: -2.5348e+02 - logprior: -1.5111e+00
Epoch 4/10
16/16 - 2s - loss: 254.1166 - loglik: -2.5285e+02 - logprior: -1.2664e+00
Epoch 5/10
16/16 - 2s - loss: 254.5512 - loglik: -2.5330e+02 - logprior: -1.2510e+00
Fitted a model with MAP estimate = -253.7996
Time for alignment: 49.9500
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 303.8480 - loglik: -2.9845e+02 - logprior: -5.4021e+00
Epoch 2/10
16/16 - 2s - loss: 277.1407 - loglik: -2.7549e+02 - logprior: -1.6517e+00
Epoch 3/10
16/16 - 2s - loss: 265.9861 - loglik: -2.6428e+02 - logprior: -1.7073e+00
Epoch 4/10
16/16 - 2s - loss: 263.0841 - loglik: -2.6140e+02 - logprior: -1.6847e+00
Epoch 5/10
16/16 - 2s - loss: 261.1614 - loglik: -2.5950e+02 - logprior: -1.6649e+00
Epoch 6/10
16/16 - 2s - loss: 261.0790 - loglik: -2.5935e+02 - logprior: -1.7261e+00
Epoch 7/10
16/16 - 2s - loss: 261.2981 - loglik: -2.5960e+02 - logprior: -1.7000e+00
Fitted a model with MAP estimate = -261.0077
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (17, 1), (23, 5), (24, 2), (25, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 9s - loss: 266.5648 - loglik: -2.6008e+02 - logprior: -6.4839e+00
Epoch 2/2
16/16 - 2s - loss: 258.3144 - loglik: -2.5527e+02 - logprior: -3.0399e+00
Fitted a model with MAP estimate = -256.7852
expansions: [(0, 1)]
discards: [ 0 31 32 37]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 259.4838 - loglik: -2.5486e+02 - logprior: -4.6250e+00
Epoch 2/2
16/16 - 2s - loss: 255.6741 - loglik: -2.5403e+02 - logprior: -1.6409e+00
Fitted a model with MAP estimate = -254.9568
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 261.6669 - loglik: -2.5540e+02 - logprior: -6.2702e+00
Epoch 2/10
16/16 - 2s - loss: 256.7358 - loglik: -2.5404e+02 - logprior: -2.6970e+00
Epoch 3/10
16/16 - 2s - loss: 255.2072 - loglik: -2.5371e+02 - logprior: -1.4954e+00
Epoch 4/10
16/16 - 2s - loss: 254.4842 - loglik: -2.5321e+02 - logprior: -1.2717e+00
Epoch 5/10
16/16 - 2s - loss: 254.2016 - loglik: -2.5295e+02 - logprior: -1.2552e+00
Epoch 6/10
16/16 - 2s - loss: 254.0038 - loglik: -2.5269e+02 - logprior: -1.3156e+00
Epoch 7/10
16/16 - 2s - loss: 253.3569 - loglik: -2.5203e+02 - logprior: -1.3298e+00
Epoch 8/10
16/16 - 2s - loss: 252.8494 - loglik: -2.5151e+02 - logprior: -1.3439e+00
Epoch 9/10
16/16 - 2s - loss: 253.4329 - loglik: -2.5209e+02 - logprior: -1.3434e+00
Fitted a model with MAP estimate = -252.9624
Time for alignment: 59.6648
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 303.4362 - loglik: -2.9804e+02 - logprior: -5.4004e+00
Epoch 2/10
16/16 - 2s - loss: 280.1865 - loglik: -2.7855e+02 - logprior: -1.6356e+00
Epoch 3/10
16/16 - 2s - loss: 266.9289 - loglik: -2.6529e+02 - logprior: -1.6386e+00
Epoch 4/10
16/16 - 2s - loss: 263.1440 - loglik: -2.6155e+02 - logprior: -1.5951e+00
Epoch 5/10
16/16 - 2s - loss: 262.4775 - loglik: -2.6088e+02 - logprior: -1.5955e+00
Epoch 6/10
16/16 - 2s - loss: 261.8862 - loglik: -2.6026e+02 - logprior: -1.6262e+00
Epoch 7/10
16/16 - 2s - loss: 261.4503 - loglik: -2.5979e+02 - logprior: -1.6574e+00
Epoch 8/10
16/16 - 2s - loss: 261.5905 - loglik: -2.5992e+02 - logprior: -1.6704e+00
Fitted a model with MAP estimate = -261.3756
expansions: [(3, 1), (6, 1), (12, 2), (15, 1), (17, 1), (23, 5), (24, 2), (25, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 266.5050 - loglik: -2.5998e+02 - logprior: -6.5299e+00
Epoch 2/2
16/16 - 2s - loss: 258.2656 - loglik: -2.5520e+02 - logprior: -3.0668e+00
Fitted a model with MAP estimate = -256.7815
expansions: [(0, 1)]
discards: [ 0 31 32 37]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 259.3642 - loglik: -2.5473e+02 - logprior: -4.6292e+00
Epoch 2/2
16/16 - 2s - loss: 255.2632 - loglik: -2.5363e+02 - logprior: -1.6377e+00
Fitted a model with MAP estimate = -254.9655
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 261.2741 - loglik: -2.5500e+02 - logprior: -6.2770e+00
Epoch 2/10
16/16 - 2s - loss: 257.2898 - loglik: -2.5462e+02 - logprior: -2.6730e+00
Epoch 3/10
16/16 - 2s - loss: 254.5502 - loglik: -2.5306e+02 - logprior: -1.4942e+00
Epoch 4/10
16/16 - 2s - loss: 255.2158 - loglik: -2.5395e+02 - logprior: -1.2660e+00
Fitted a model with MAP estimate = -254.2692
Time for alignment: 52.6502
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 303.8874 - loglik: -2.9849e+02 - logprior: -5.4011e+00
Epoch 2/10
16/16 - 2s - loss: 279.2945 - loglik: -2.7766e+02 - logprior: -1.6338e+00
Epoch 3/10
16/16 - 2s - loss: 269.1053 - loglik: -2.6748e+02 - logprior: -1.6207e+00
Epoch 4/10
16/16 - 2s - loss: 264.4926 - loglik: -2.6293e+02 - logprior: -1.5647e+00
Epoch 5/10
16/16 - 2s - loss: 263.0396 - loglik: -2.6146e+02 - logprior: -1.5794e+00
Epoch 6/10
16/16 - 2s - loss: 261.5529 - loglik: -2.5989e+02 - logprior: -1.6616e+00
Epoch 7/10
16/16 - 2s - loss: 261.8202 - loglik: -2.6015e+02 - logprior: -1.6701e+00
Fitted a model with MAP estimate = -261.2249
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (24, 8), (25, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 266.8928 - loglik: -2.6039e+02 - logprior: -6.5052e+00
Epoch 2/2
16/16 - 2s - loss: 258.0879 - loglik: -2.5503e+02 - logprior: -3.0538e+00
Fitted a model with MAP estimate = -256.7475
expansions: [(0, 1)]
discards: [ 0 31 32 37]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 259.2910 - loglik: -2.5466e+02 - logprior: -4.6317e+00
Epoch 2/2
16/16 - 2s - loss: 255.6123 - loglik: -2.5396e+02 - logprior: -1.6530e+00
Fitted a model with MAP estimate = -254.9592
expansions: [(3, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 258.2575 - loglik: -2.5363e+02 - logprior: -4.6281e+00
Epoch 2/10
16/16 - 2s - loss: 254.9848 - loglik: -2.5338e+02 - logprior: -1.6018e+00
Epoch 3/10
16/16 - 2s - loss: 254.3877 - loglik: -2.5305e+02 - logprior: -1.3359e+00
Epoch 4/10
16/16 - 2s - loss: 254.0377 - loglik: -2.5275e+02 - logprior: -1.2858e+00
Epoch 5/10
16/16 - 2s - loss: 253.5726 - loglik: -2.5230e+02 - logprior: -1.2677e+00
Epoch 6/10
16/16 - 2s - loss: 252.9391 - loglik: -2.5163e+02 - logprior: -1.3048e+00
Epoch 7/10
16/16 - 2s - loss: 253.4486 - loglik: -2.5212e+02 - logprior: -1.3282e+00
Fitted a model with MAP estimate = -252.9831
Time for alignment: 52.1799
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 304.0395 - loglik: -2.9864e+02 - logprior: -5.3979e+00
Epoch 2/10
16/16 - 2s - loss: 277.1551 - loglik: -2.7553e+02 - logprior: -1.6291e+00
Epoch 3/10
16/16 - 2s - loss: 266.5388 - loglik: -2.6487e+02 - logprior: -1.6648e+00
Epoch 4/10
16/16 - 2s - loss: 261.8500 - loglik: -2.6018e+02 - logprior: -1.6697e+00
Epoch 5/10
16/16 - 2s - loss: 261.9983 - loglik: -2.6034e+02 - logprior: -1.6580e+00
Fitted a model with MAP estimate = -261.2711
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (14, 1), (17, 1), (23, 5), (24, 2), (25, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 266.8567 - loglik: -2.6041e+02 - logprior: -6.4484e+00
Epoch 2/2
16/16 - 2s - loss: 258.3453 - loglik: -2.5535e+02 - logprior: -2.9927e+00
Fitted a model with MAP estimate = -256.5991
expansions: [(0, 1)]
discards: [ 0 31 32 37]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 258.8478 - loglik: -2.5422e+02 - logprior: -4.6233e+00
Epoch 2/2
16/16 - 2s - loss: 255.4884 - loglik: -2.5385e+02 - logprior: -1.6428e+00
Fitted a model with MAP estimate = -254.8958
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 261.8314 - loglik: -2.5554e+02 - logprior: -6.2920e+00
Epoch 2/10
16/16 - 2s - loss: 255.8207 - loglik: -2.5310e+02 - logprior: -2.7171e+00
Epoch 3/10
16/16 - 2s - loss: 255.8454 - loglik: -2.5434e+02 - logprior: -1.5101e+00
Fitted a model with MAP estimate = -254.6745
Time for alignment: 45.0247
Computed alignments with likelihoods: ['-253.7996', '-252.9624', '-254.2692', '-252.9831', '-254.6745']
Best model has likelihood: -252.9624  (prior= -1.3389 )
time for generating output: 0.1300
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HLH.projection.fasta
SP score = 0.9143780290791599
Training of 5 independent models on file blmb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f534413ba60>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f527e2e75b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5263b35c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526a3bd850>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5286284370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5286284250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5b08351c40>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5263bc3640>, <__main__.SimpleDirichletPrior object at 0x7f5269734f40>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 1203.3701 - loglik: -1.2014e+03 - logprior: -2.0126e+00
Epoch 2/10
39/39 - 12s - loss: 1154.7325 - loglik: -1.1537e+03 - logprior: -9.9796e-01
Epoch 3/10
39/39 - 12s - loss: 1148.2869 - loglik: -1.1471e+03 - logprior: -1.1490e+00
Epoch 4/10
39/39 - 12s - loss: 1145.7108 - loglik: -1.1445e+03 - logprior: -1.2351e+00
Epoch 5/10
39/39 - 12s - loss: 1143.4390 - loglik: -1.1421e+03 - logprior: -1.3383e+00
Epoch 6/10
39/39 - 12s - loss: 1142.1759 - loglik: -1.1407e+03 - logprior: -1.4832e+00
Epoch 7/10
39/39 - 12s - loss: 1140.6973 - loglik: -1.1391e+03 - logprior: -1.6348e+00
Epoch 8/10
39/39 - 12s - loss: 1139.8428 - loglik: -1.1380e+03 - logprior: -1.8274e+00
Epoch 9/10
39/39 - 12s - loss: 1138.6322 - loglik: -1.1366e+03 - logprior: -1.9916e+00
Epoch 10/10
39/39 - 12s - loss: 1137.5599 - loglik: -1.1354e+03 - logprior: -2.1535e+00
Fitted a model with MAP estimate = -1060.3484
expansions: [(6, 1), (7, 1), (11, 1), (13, 1), (21, 2), (24, 5), (25, 1), (26, 2), (55, 2), (67, 1), (70, 1), (72, 1), (77, 9), (86, 1), (111, 1), (123, 2), (126, 4), (128, 1), (141, 6)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 198 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 1155.7233 - loglik: -1.1534e+03 - logprior: -2.3313e+00
Epoch 2/2
39/39 - 14s - loss: 1138.5765 - loglik: -1.1376e+03 - logprior: -9.3327e-01
Fitted a model with MAP estimate = -1039.4834
expansions: [(142, 1), (159, 1), (181, 1)]
discards: [  0  25  71 173 174 175]
Re-initialized the encoder parameters.
Fitting a model of length 195 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 1143.6592 - loglik: -1.1410e+03 - logprior: -2.7050e+00
Epoch 2/2
39/39 - 14s - loss: 1138.1444 - loglik: -1.1371e+03 - logprior: -1.0129e+00
Fitted a model with MAP estimate = -1038.9886
expansions: [(0, 2), (31, 1), (141, 1), (177, 4)]
discards: [  0 139]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 22s - loss: 1030.0022 - loglik: -1.0288e+03 - logprior: -1.1932e+00
Epoch 2/10
51/51 - 18s - loss: 1025.4800 - loglik: -1.0247e+03 - logprior: -7.6342e-01
Epoch 3/10
51/51 - 18s - loss: 1027.8130 - loglik: -1.0271e+03 - logprior: -7.6039e-01
Fitted a model with MAP estimate = -1022.8067
Time for alignment: 337.7953
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 1203.1503 - loglik: -1.2011e+03 - logprior: -2.0054e+00
Epoch 2/10
39/39 - 12s - loss: 1154.2603 - loglik: -1.1533e+03 - logprior: -9.7300e-01
Epoch 3/10
39/39 - 12s - loss: 1147.9719 - loglik: -1.1469e+03 - logprior: -1.0886e+00
Epoch 4/10
39/39 - 12s - loss: 1144.6302 - loglik: -1.1434e+03 - logprior: -1.1803e+00
Epoch 5/10
39/39 - 12s - loss: 1142.4218 - loglik: -1.1411e+03 - logprior: -1.2896e+00
Epoch 6/10
39/39 - 12s - loss: 1140.9077 - loglik: -1.1395e+03 - logprior: -1.4230e+00
Epoch 7/10
39/39 - 12s - loss: 1138.9457 - loglik: -1.1373e+03 - logprior: -1.6081e+00
Epoch 8/10
39/39 - 12s - loss: 1137.5929 - loglik: -1.1357e+03 - logprior: -1.8516e+00
Epoch 9/10
39/39 - 12s - loss: 1135.5353 - loglik: -1.1335e+03 - logprior: -2.0553e+00
Epoch 10/10
39/39 - 12s - loss: 1134.4720 - loglik: -1.1323e+03 - logprior: -2.1551e+00
Fitted a model with MAP estimate = -1059.7052
expansions: [(6, 1), (7, 1), (10, 2), (11, 1), (13, 2), (20, 2), (24, 1), (30, 2), (55, 2), (56, 1), (70, 2), (77, 2), (92, 3), (112, 1), (122, 5), (125, 1), (126, 1), (127, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 1158.7307 - loglik: -1.1564e+03 - logprior: -2.3727e+00
Epoch 2/2
39/39 - 13s - loss: 1139.1990 - loglik: -1.1383e+03 - logprior: -9.4454e-01
Fitted a model with MAP estimate = -1039.2104
expansions: [(32, 6), (95, 1), (149, 2), (150, 2), (151, 3), (174, 9), (178, 3)]
discards: [  0  14  18  27  40  69  87  88 113 157 159 160 161 162 163 164 165 166
 167 168 169 170 171]
Re-initialized the encoder parameters.
Fitting a model of length 190 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 1144.4637 - loglik: -1.1419e+03 - logprior: -2.5731e+00
Epoch 2/2
39/39 - 13s - loss: 1136.9611 - loglik: -1.1360e+03 - logprior: -9.6817e-01
Fitted a model with MAP estimate = -1036.6874
expansions: [(0, 2), (86, 1), (87, 1), (153, 2)]
discards: [  0  25  91 159 160 161 162 167]
Re-initialized the encoder parameters.
Fitting a model of length 188 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 22s - loss: 1031.6604 - loglik: -1.0305e+03 - logprior: -1.1293e+00
Epoch 2/10
51/51 - 17s - loss: 1030.5629 - loglik: -1.0299e+03 - logprior: -6.9892e-01
Epoch 3/10
51/51 - 17s - loss: 1026.5287 - loglik: -1.0258e+03 - logprior: -6.9739e-01
Epoch 4/10
51/51 - 16s - loss: 1023.7851 - loglik: -1.0231e+03 - logprior: -6.9894e-01
Epoch 5/10
51/51 - 16s - loss: 1021.7419 - loglik: -1.0210e+03 - logprior: -7.6446e-01
Epoch 6/10
51/51 - 16s - loss: 1021.3693 - loglik: -1.0205e+03 - logprior: -8.4229e-01
Epoch 7/10
51/51 - 16s - loss: 1018.8910 - loglik: -1.0180e+03 - logprior: -9.2449e-01
Epoch 8/10
51/51 - 17s - loss: 1017.5650 - loglik: -1.0165e+03 - logprior: -1.0223e+00
Epoch 9/10
51/51 - 16s - loss: 1016.5313 - loglik: -1.0154e+03 - logprior: -1.1454e+00
Epoch 10/10
51/51 - 17s - loss: 1015.0084 - loglik: -1.0138e+03 - logprior: -1.2223e+00
Fitted a model with MAP estimate = -1013.2766
Time for alignment: 446.1107
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 1203.2754 - loglik: -1.2013e+03 - logprior: -1.9919e+00
Epoch 2/10
39/39 - 12s - loss: 1157.2035 - loglik: -1.1563e+03 - logprior: -8.7917e-01
Epoch 3/10
39/39 - 12s - loss: 1149.8857 - loglik: -1.1488e+03 - logprior: -1.0732e+00
Epoch 4/10
39/39 - 12s - loss: 1146.2664 - loglik: -1.1451e+03 - logprior: -1.1781e+00
Epoch 5/10
39/39 - 12s - loss: 1143.8877 - loglik: -1.1426e+03 - logprior: -1.3168e+00
Epoch 6/10
39/39 - 12s - loss: 1141.9816 - loglik: -1.1405e+03 - logprior: -1.4810e+00
Epoch 7/10
39/39 - 12s - loss: 1139.8522 - loglik: -1.1382e+03 - logprior: -1.6973e+00
Epoch 8/10
39/39 - 12s - loss: 1138.1794 - loglik: -1.1363e+03 - logprior: -1.8854e+00
Epoch 9/10
39/39 - 12s - loss: 1137.3142 - loglik: -1.1353e+03 - logprior: -2.0331e+00
Epoch 10/10
39/39 - 12s - loss: 1136.4197 - loglik: -1.1343e+03 - logprior: -2.1386e+00
Fitted a model with MAP estimate = -1063.2691
expansions: [(6, 1), (7, 1), (10, 2), (11, 1), (21, 2), (25, 1), (55, 2), (71, 2), (78, 10), (80, 2), (81, 2), (84, 1), (89, 1), (108, 2), (109, 1), (114, 1), (124, 1), (126, 3), (127, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 194 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 1161.7529 - loglik: -1.1594e+03 - logprior: -2.3400e+00
Epoch 2/2
39/39 - 13s - loss: 1141.9977 - loglik: -1.1411e+03 - logprior: -9.2084e-01
Fitted a model with MAP estimate = -1042.0148
expansions: [(31, 5)]
discards: [  0  14  26  65  82  83  84  85  86  87  95  96  97  98 105]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 1146.2162 - loglik: -1.1436e+03 - logprior: -2.6337e+00
Epoch 2/2
39/39 - 13s - loss: 1141.3158 - loglik: -1.1404e+03 - logprior: -9.5107e-01
Fitted a model with MAP estimate = -1040.8390
expansions: [(0, 2), (37, 1), (89, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 189 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 19s - loss: 1034.9076 - loglik: -1.0337e+03 - logprior: -1.1778e+00
Epoch 2/10
51/51 - 16s - loss: 1027.7144 - loglik: -1.0270e+03 - logprior: -7.2725e-01
Epoch 3/10
51/51 - 16s - loss: 1028.0698 - loglik: -1.0274e+03 - logprior: -7.0918e-01
Fitted a model with MAP estimate = -1025.2088
Time for alignment: 328.1510
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 1202.5811 - loglik: -1.2006e+03 - logprior: -1.9914e+00
Epoch 2/10
39/39 - 12s - loss: 1154.9199 - loglik: -1.1540e+03 - logprior: -9.2426e-01
Epoch 3/10
39/39 - 12s - loss: 1148.7244 - loglik: -1.1476e+03 - logprior: -1.0749e+00
Epoch 4/10
39/39 - 12s - loss: 1146.2690 - loglik: -1.1451e+03 - logprior: -1.1482e+00
Epoch 5/10
39/39 - 12s - loss: 1144.0815 - loglik: -1.1428e+03 - logprior: -1.2676e+00
Epoch 6/10
39/39 - 12s - loss: 1142.3724 - loglik: -1.1409e+03 - logprior: -1.4516e+00
Epoch 7/10
39/39 - 12s - loss: 1141.1409 - loglik: -1.1395e+03 - logprior: -1.6733e+00
Epoch 8/10
39/39 - 12s - loss: 1139.8541 - loglik: -1.1380e+03 - logprior: -1.8998e+00
Epoch 9/10
39/39 - 12s - loss: 1138.5787 - loglik: -1.1365e+03 - logprior: -2.0719e+00
Epoch 10/10
39/39 - 12s - loss: 1137.2648 - loglik: -1.1351e+03 - logprior: -2.2076e+00
Fitted a model with MAP estimate = -1062.5896
expansions: [(6, 1), (7, 1), (10, 2), (11, 1), (20, 2), (23, 6), (27, 1), (54, 1), (55, 1), (69, 3), (71, 1), (77, 11), (78, 1), (83, 1), (98, 2), (123, 2), (124, 3), (125, 1), (142, 10)]
discards: [135 136 137 138 139]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1158.1260 - loglik: -1.1557e+03 - logprior: -2.4281e+00
Epoch 2/2
39/39 - 14s - loss: 1138.0883 - loglik: -1.1370e+03 - logprior: -1.0943e+00
Fitted a model with MAP estimate = -1038.4471
expansions: [(41, 1), (162, 2), (181, 2), (182, 2), (183, 6)]
discards: [  0  14  27  87  88  89  90  91  92  93 165 166 167 168 169 170 171 172
 173 174 175 176 177 178]
Re-initialized the encoder parameters.
Fitting a model of length 190 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 1144.7654 - loglik: -1.1422e+03 - logprior: -2.6042e+00
Epoch 2/2
39/39 - 13s - loss: 1138.6426 - loglik: -1.1378e+03 - logprior: -8.9164e-01
Fitted a model with MAP estimate = -1038.6269
expansions: [(0, 2)]
discards: [  0 157 158]
Re-initialized the encoder parameters.
Fitting a model of length 189 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 20s - loss: 1031.8950 - loglik: -1.0308e+03 - logprior: -1.1147e+00
Epoch 2/10
51/51 - 16s - loss: 1028.5981 - loglik: -1.0279e+03 - logprior: -7.0408e-01
Epoch 3/10
51/51 - 17s - loss: 1027.7668 - loglik: -1.0271e+03 - logprior: -6.8842e-01
Epoch 4/10
51/51 - 17s - loss: 1026.5243 - loglik: -1.0258e+03 - logprior: -7.0079e-01
Epoch 5/10
51/51 - 17s - loss: 1023.6570 - loglik: -1.0229e+03 - logprior: -7.6716e-01
Epoch 6/10
51/51 - 17s - loss: 1018.9832 - loglik: -1.0181e+03 - logprior: -8.5943e-01
Epoch 7/10
51/51 - 16s - loss: 1019.9962 - loglik: -1.0190e+03 - logprior: -9.4704e-01
Fitted a model with MAP estimate = -1018.1927
Time for alignment: 398.0737
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 1202.8253 - loglik: -1.2008e+03 - logprior: -2.0112e+00
Epoch 2/10
39/39 - 12s - loss: 1153.4194 - loglik: -1.1525e+03 - logprior: -9.5463e-01
Epoch 3/10
39/39 - 12s - loss: 1146.9584 - loglik: -1.1459e+03 - logprior: -1.0682e+00
Epoch 4/10
39/39 - 12s - loss: 1144.2012 - loglik: -1.1430e+03 - logprior: -1.1578e+00
Epoch 5/10
39/39 - 12s - loss: 1142.3049 - loglik: -1.1410e+03 - logprior: -1.2808e+00
Epoch 6/10
39/39 - 12s - loss: 1140.4365 - loglik: -1.1390e+03 - logprior: -1.4783e+00
Epoch 7/10
39/39 - 12s - loss: 1139.4347 - loglik: -1.1377e+03 - logprior: -1.7076e+00
Epoch 8/10
39/39 - 12s - loss: 1138.5419 - loglik: -1.1366e+03 - logprior: -1.9237e+00
Epoch 9/10
39/39 - 12s - loss: 1137.0620 - loglik: -1.1350e+03 - logprior: -2.0510e+00
Epoch 10/10
39/39 - 12s - loss: 1135.8361 - loglik: -1.1337e+03 - logprior: -2.1702e+00
Fitted a model with MAP estimate = -1058.8489
expansions: [(6, 1), (7, 1), (11, 1), (13, 1), (21, 2), (24, 5), (25, 1), (26, 2), (27, 1), (56, 1), (57, 1), (71, 3), (77, 6), (79, 1), (111, 1), (121, 1), (122, 1), (124, 2), (125, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 190 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 1155.3037 - loglik: -1.1530e+03 - logprior: -2.3430e+00
Epoch 2/2
39/39 - 13s - loss: 1138.4976 - loglik: -1.1376e+03 - logprior: -9.3024e-01
Fitted a model with MAP estimate = -1039.2960
expansions: [(99, 1), (137, 1), (163, 1), (177, 8), (181, 3)]
discards: [  0  25  30  31  32  40  72  89 118 119 120 167 168 169 170 171 172 173]
Re-initialized the encoder parameters.
Fitting a model of length 186 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 1145.2427 - loglik: -1.1426e+03 - logprior: -2.6029e+00
Epoch 2/2
39/39 - 13s - loss: 1137.5760 - loglik: -1.1366e+03 - logprior: -9.5325e-01
Fitted a model with MAP estimate = -1037.0060
expansions: [(0, 2), (28, 5), (148, 1), (166, 4)]
discards: [  0  89 158 159 160 161 162 163]
Re-initialized the encoder parameters.
Fitting a model of length 190 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 20s - loss: 1032.9214 - loglik: -1.0318e+03 - logprior: -1.1298e+00
Epoch 2/10
51/51 - 17s - loss: 1026.2279 - loglik: -1.0255e+03 - logprior: -7.0699e-01
Epoch 3/10
51/51 - 17s - loss: 1026.2123 - loglik: -1.0255e+03 - logprior: -7.1168e-01
Epoch 4/10
51/51 - 16s - loss: 1023.7242 - loglik: -1.0230e+03 - logprior: -7.1525e-01
Epoch 5/10
51/51 - 17s - loss: 1020.5444 - loglik: -1.0198e+03 - logprior: -7.7979e-01
Epoch 6/10
51/51 - 17s - loss: 1022.6061 - loglik: -1.0217e+03 - logprior: -8.7594e-01
Fitted a model with MAP estimate = -1018.9208
Time for alignment: 378.7499
Computed alignments with likelihoods: ['-1022.8067', '-1013.2766', '-1025.2088', '-1018.1927', '-1018.9208']
Best model has likelihood: -1013.2766  (prior= -1.2456 )
time for generating output: 0.3054
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blmb.projection.fasta
SP score = 0.6942633637548892
Training of 5 independent models on file proteasome.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f5a9c079b50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f5a9c079d90>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c0795e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0796a0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c079460>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5a9c079820>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0793a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0798b0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c0799d0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f430>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14ffa0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f7c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14fd90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5a9c14f940>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5a9c14f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5271b117c0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52e0de3fa0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f527ed5ab50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526aa7b040>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5a3c6a8310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52952edcd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526c71bf10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5289adf130>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7f5d10a43310>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f5bb09fc1f0>, <function make_default_emission_matrix at 0x7f5bb09fc1f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f5269325b50>, <__main__.SimpleDirichletPrior object at 0x7f52868d3220>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 994.0712 - loglik: -9.8675e+02 - logprior: -7.3234e+00
Epoch 2/10
14/14 - 4s - loss: 938.1247 - loglik: -9.3669e+02 - logprior: -1.4330e+00
Epoch 3/10
14/14 - 4s - loss: 903.2354 - loglik: -9.0197e+02 - logprior: -1.2613e+00
Epoch 4/10
14/14 - 4s - loss: 891.8143 - loglik: -8.9068e+02 - logprior: -1.1384e+00
Epoch 5/10
14/14 - 4s - loss: 887.0321 - loglik: -8.8593e+02 - logprior: -1.1042e+00
Epoch 6/10
14/14 - 4s - loss: 886.4853 - loglik: -8.8541e+02 - logprior: -1.0782e+00
Epoch 7/10
14/14 - 4s - loss: 883.8395 - loglik: -8.8273e+02 - logprior: -1.1067e+00
Epoch 8/10
14/14 - 4s - loss: 883.7933 - loglik: -8.8260e+02 - logprior: -1.1894e+00
Epoch 9/10
14/14 - 4s - loss: 882.4655 - loglik: -8.8126e+02 - logprior: -1.2059e+00
Epoch 10/10
14/14 - 4s - loss: 885.3693 - loglik: -8.8411e+02 - logprior: -1.2573e+00
Fitted a model with MAP estimate = -883.2045
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (61, 1), (69, 1), (70, 3), (71, 1), (102, 4), (117, 1), (124, 1), (128, 1), (129, 2), (130, 2), (131, 1), (133, 2), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 9s - loss: 884.4628 - loglik: -8.7983e+02 - logprior: -4.6280e+00
Epoch 2/2
29/29 - 7s - loss: 867.5624 - loglik: -8.6654e+02 - logprior: -1.0200e+00
Fitted a model with MAP estimate = -866.7068
expansions: [(126, 1), (127, 1), (129, 1)]
discards: [ 41  91  92 159 167]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 9s - loss: 872.2677 - loglik: -8.6914e+02 - logprior: -3.1303e+00
Epoch 2/2
29/29 - 7s - loss: 867.3451 - loglik: -8.6670e+02 - logprior: -6.4170e-01
Fitted a model with MAP estimate = -865.8824
expansions: [(101, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 9s - loss: 870.6569 - loglik: -8.6768e+02 - logprior: -2.9764e+00
Epoch 2/10
29/29 - 6s - loss: 865.3365 - loglik: -8.6490e+02 - logprior: -4.3915e-01
Epoch 3/10
29/29 - 6s - loss: 866.3448 - loglik: -8.6608e+02 - logprior: -2.6299e-01
Fitted a model with MAP estimate = -863.7403
Time for alignment: 128.0575
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 11s - loss: 992.5797 - loglik: -9.8526e+02 - logprior: -7.3163e+00
Epoch 2/10
14/14 - 4s - loss: 940.4570 - loglik: -9.3902e+02 - logprior: -1.4398e+00
Epoch 3/10
14/14 - 4s - loss: 903.3228 - loglik: -9.0208e+02 - logprior: -1.2384e+00
Epoch 4/10
14/14 - 4s - loss: 890.0918 - loglik: -8.8900e+02 - logprior: -1.0948e+00
Epoch 5/10
14/14 - 4s - loss: 884.1549 - loglik: -8.8306e+02 - logprior: -1.0975e+00
Epoch 6/10
14/14 - 4s - loss: 885.3248 - loglik: -8.8423e+02 - logprior: -1.0962e+00
Fitted a model with MAP estimate = -882.8558
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (52, 1), (53, 1), (54, 1), (57, 1), (58, 1), (66, 1), (69, 1), (70, 3), (71, 2), (89, 1), (102, 4), (103, 2), (117, 1), (120, 2), (128, 1), (131, 1), (133, 2), (134, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 10s - loss: 881.4243 - loglik: -8.7699e+02 - logprior: -4.4385e+00
Epoch 2/2
29/29 - 7s - loss: 867.2463 - loglik: -8.6620e+02 - logprior: -1.0454e+00
Fitted a model with MAP estimate = -864.6913
expansions: [(128, 1), (129, 1)]
discards: [ 41  91  92  93 132 151 168]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 10s - loss: 872.6158 - loglik: -8.6949e+02 - logprior: -3.1242e+00
Epoch 2/2
29/29 - 6s - loss: 866.0643 - loglik: -8.6544e+02 - logprior: -6.2509e-01
Fitted a model with MAP estimate = -865.7692
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 10s - loss: 871.0344 - loglik: -8.6804e+02 - logprior: -2.9896e+00
Epoch 2/10
29/29 - 6s - loss: 866.3755 - loglik: -8.6593e+02 - logprior: -4.4331e-01
Epoch 3/10
29/29 - 6s - loss: 865.7296 - loglik: -8.6545e+02 - logprior: -2.7558e-01
Epoch 4/10
29/29 - 7s - loss: 863.5495 - loglik: -8.6335e+02 - logprior: -1.9887e-01
Epoch 5/10
29/29 - 7s - loss: 862.3489 - loglik: -8.6220e+02 - logprior: -1.4788e-01
Epoch 6/10
29/29 - 6s - loss: 861.1785 - loglik: -8.6106e+02 - logprior: -1.1920e-01
Epoch 7/10
29/29 - 7s - loss: 860.8748 - loglik: -8.6077e+02 - logprior: -1.0083e-01
Epoch 8/10
29/29 - 6s - loss: 861.3333 - loglik: -8.6125e+02 - logprior: -8.0081e-02
Fitted a model with MAP estimate = -860.4239
Time for alignment: 150.2796
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 993.6869 - loglik: -9.8636e+02 - logprior: -7.3311e+00
Epoch 2/10
14/14 - 4s - loss: 940.2399 - loglik: -9.3879e+02 - logprior: -1.4483e+00
Epoch 3/10
14/14 - 4s - loss: 904.4386 - loglik: -9.0312e+02 - logprior: -1.3198e+00
Epoch 4/10
14/14 - 4s - loss: 893.3454 - loglik: -8.9219e+02 - logprior: -1.1573e+00
Epoch 5/10
14/14 - 4s - loss: 888.6899 - loglik: -8.8756e+02 - logprior: -1.1264e+00
Epoch 6/10
14/14 - 4s - loss: 887.3302 - loglik: -8.8621e+02 - logprior: -1.1233e+00
Epoch 7/10
14/14 - 4s - loss: 885.6157 - loglik: -8.8445e+02 - logprior: -1.1681e+00
Epoch 8/10
14/14 - 4s - loss: 883.4987 - loglik: -8.8228e+02 - logprior: -1.2159e+00
Epoch 9/10
14/14 - 4s - loss: 884.6064 - loglik: -8.8339e+02 - logprior: -1.2210e+00
Fitted a model with MAP estimate = -884.0648
expansions: [(0, 2), (21, 1), (22, 1), (23, 2), (24, 1), (25, 2), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (58, 1), (66, 1), (69, 1), (70, 4), (102, 4), (117, 1), (120, 2), (129, 1), (131, 2), (133, 2), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 9s - loss: 885.5729 - loglik: -8.8101e+02 - logprior: -4.5658e+00
Epoch 2/2
29/29 - 6s - loss: 869.8223 - loglik: -8.6891e+02 - logprior: -9.1231e-01
Fitted a model with MAP estimate = -868.9455
expansions: [(127, 1), (128, 1), (130, 1)]
discards: [ 32  42 148 166]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 10s - loss: 873.5780 - loglik: -8.7043e+02 - logprior: -3.1486e+00
Epoch 2/2
29/29 - 6s - loss: 869.9442 - loglik: -8.6928e+02 - logprior: -6.6295e-01
Fitted a model with MAP estimate = -867.5693
expansions: [(157, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 10s - loss: 871.5197 - loglik: -8.6844e+02 - logprior: -3.0756e+00
Epoch 2/10
29/29 - 6s - loss: 864.1284 - loglik: -8.6353e+02 - logprior: -5.9429e-01
Epoch 3/10
29/29 - 6s - loss: 865.8169 - loglik: -8.6540e+02 - logprior: -4.1229e-01
Fitted a model with MAP estimate = -862.8776
Time for alignment: 128.4687
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 993.8926 - loglik: -9.8656e+02 - logprior: -7.3319e+00
Epoch 2/10
14/14 - 4s - loss: 939.9410 - loglik: -9.3851e+02 - logprior: -1.4261e+00
Epoch 3/10
14/14 - 4s - loss: 905.2606 - loglik: -9.0394e+02 - logprior: -1.3255e+00
Epoch 4/10
14/14 - 4s - loss: 892.6284 - loglik: -8.9140e+02 - logprior: -1.2294e+00
Epoch 5/10
14/14 - 4s - loss: 888.0425 - loglik: -8.8686e+02 - logprior: -1.1826e+00
Epoch 6/10
14/14 - 4s - loss: 887.0460 - loglik: -8.8588e+02 - logprior: -1.1698e+00
Epoch 7/10
14/14 - 4s - loss: 884.9002 - loglik: -8.8369e+02 - logprior: -1.2075e+00
Epoch 8/10
14/14 - 4s - loss: 885.8746 - loglik: -8.8459e+02 - logprior: -1.2824e+00
Fitted a model with MAP estimate = -884.5890
expansions: [(0, 2), (10, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (57, 1), (64, 1), (71, 1), (74, 1), (76, 1), (102, 4), (117, 1), (120, 2), (128, 1), (129, 2), (130, 2), (131, 1), (133, 2), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 10s - loss: 884.1974 - loglik: -8.7966e+02 - logprior: -4.5414e+00
Epoch 2/2
29/29 - 7s - loss: 870.0695 - loglik: -8.6917e+02 - logprior: -9.0442e-01
Fitted a model with MAP estimate = -867.8565
expansions: [(125, 1), (126, 1), (128, 1)]
discards: [ 41 146 159 167]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 9s - loss: 872.6152 - loglik: -8.6958e+02 - logprior: -3.0315e+00
Epoch 2/2
29/29 - 7s - loss: 869.3316 - loglik: -8.6884e+02 - logprior: -4.9102e-01
Fitted a model with MAP estimate = -866.4857
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 13s - loss: 871.3020 - loglik: -8.6837e+02 - logprior: -2.9272e+00
Epoch 2/10
29/29 - 6s - loss: 868.7532 - loglik: -8.6845e+02 - logprior: -3.0373e-01
Epoch 3/10
29/29 - 7s - loss: 864.5162 - loglik: -8.6440e+02 - logprior: -1.1302e-01
Epoch 4/10
29/29 - 6s - loss: 866.0626 - loglik: -8.6601e+02 - logprior: -5.7510e-02
Fitted a model with MAP estimate = -863.1791
Time for alignment: 130.8244
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 995.0902 - loglik: -9.8777e+02 - logprior: -7.3194e+00
Epoch 2/10
14/14 - 4s - loss: 938.4917 - loglik: -9.3705e+02 - logprior: -1.4416e+00
Epoch 3/10
14/14 - 4s - loss: 904.0871 - loglik: -9.0276e+02 - logprior: -1.3252e+00
Epoch 4/10
14/14 - 4s - loss: 893.3472 - loglik: -8.9210e+02 - logprior: -1.2521e+00
Epoch 5/10
14/14 - 4s - loss: 887.3466 - loglik: -8.8609e+02 - logprior: -1.2532e+00
Epoch 6/10
14/14 - 4s - loss: 887.1092 - loglik: -8.8588e+02 - logprior: -1.2290e+00
Epoch 7/10
14/14 - 4s - loss: 884.3953 - loglik: -8.8315e+02 - logprior: -1.2458e+00
Epoch 8/10
14/14 - 4s - loss: 885.2919 - loglik: -8.8400e+02 - logprior: -1.2926e+00
Fitted a model with MAP estimate = -884.2463
expansions: [(0, 2), (21, 1), (22, 1), (23, 2), (24, 1), (25, 2), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (66, 1), (69, 1), (70, 3), (89, 2), (103, 4), (117, 1), (120, 2), (128, 1), (131, 1), (133, 2), (134, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 10s - loss: 885.0141 - loglik: -8.8047e+02 - logprior: -4.5460e+00
Epoch 2/2
29/29 - 6s - loss: 872.9333 - loglik: -8.7203e+02 - logprior: -9.0743e-01
Fitted a model with MAP estimate = -869.6785
expansions: [(127, 1), (129, 1)]
discards: [ 32  42 111 149 166]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 9s - loss: 874.0286 - loglik: -8.7088e+02 - logprior: -3.1535e+00
Epoch 2/2
29/29 - 6s - loss: 870.4257 - loglik: -8.6977e+02 - logprior: -6.5584e-01
Fitted a model with MAP estimate = -868.8550
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 9s - loss: 873.8829 - loglik: -8.7086e+02 - logprior: -3.0216e+00
Epoch 2/10
29/29 - 6s - loss: 869.9373 - loglik: -8.6945e+02 - logprior: -4.8445e-01
Epoch 3/10
29/29 - 6s - loss: 868.1406 - loglik: -8.6782e+02 - logprior: -3.1771e-01
Epoch 4/10
29/29 - 6s - loss: 865.2461 - loglik: -8.6499e+02 - logprior: -2.5349e-01
Epoch 5/10
29/29 - 6s - loss: 865.4336 - loglik: -8.6523e+02 - logprior: -2.0092e-01
Fitted a model with MAP estimate = -864.1050
Time for alignment: 133.8237
Computed alignments with likelihoods: ['-863.7403', '-860.4239', '-862.8776', '-863.1791', '-864.1050']
Best model has likelihood: -860.4239  (prior= -0.1497 )
time for generating output: 0.2922
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/proteasome.projection.fasta
SP score = 0.7757076999276108
