Training of 3 independent models on file PF00079.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff6dadbec40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6fceae6a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6fce9bf40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6dadd87f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6dadd8fd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff6dade09a0>, <__main__.SimpleDirichletPrior object at 0x7ff6a7b7eaf0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff71d622040>

Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 771.3799 - loglik: -7.6920e+02 - logprior: -1.7438e+00
Epoch 2/10
39/39 - 35s - loss: 641.6880 - loglik: -6.3782e+02 - logprior: -2.5129e+00
Epoch 3/10
39/39 - 37s - loss: 630.5671 - loglik: -6.2658e+02 - logprior: -2.4925e+00
Epoch 4/10
39/39 - 38s - loss: 628.2195 - loglik: -6.2449e+02 - logprior: -2.4090e+00
Epoch 5/10
39/39 - 38s - loss: 626.0314 - loglik: -6.2237e+02 - logprior: -2.4252e+00
Epoch 6/10
39/39 - 38s - loss: 625.6558 - loglik: -6.2203e+02 - logprior: -2.4616e+00
Epoch 7/10
39/39 - 40s - loss: 625.0505 - loglik: -6.2147e+02 - logprior: -2.4834e+00
Epoch 8/10
39/39 - 40s - loss: 624.4023 - loglik: -6.2088e+02 - logprior: -2.5001e+00
Epoch 9/10
39/39 - 41s - loss: 623.9820 - loglik: -6.2053e+02 - logprior: -2.5034e+00
Epoch 10/10
39/39 - 41s - loss: 624.1318 - loglik: -6.2071e+02 - logprior: -2.5257e+00
Fitted a model with MAP estimate = -622.3282
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (46, 1), (53, 3), (56, 1), (61, 1), (62, 1), (64, 1), (65, 1), (66, 1), (67, 1), (78, 1), (79, 1), (80, 1), (83, 1), (84, 1), (85, 1), (91, 1), (92, 1), (95, 1), (98, 1), (100, 1), (103, 1), (113, 1), (118, 1), (120, 1), (122, 1), (135, 1), (140, 1), (142, 1), (143, 1), (147, 1), (157, 1), (158, 1), (161, 1), (163, 1), (164, 1), (166, 1), (167, 1), (173, 1), (186, 2), (189, 1), (195, 1), (196, 1), (197, 2), (198, 1), (199, 1), (200, 1), (214, 1), (215, 1), (218, 1), (220, 1), (224, 1), (229, 1), (234, 1), (236, 2), (238, 1), (242, 1), (262, 1), (263, 1), (264, 1), (266, 2), (267, 3), (268, 1), (277, 1), (278, 2), (279, 1), (280, 1)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 65s - loss: 617.4541 - loglik: -6.1393e+02 - logprior: -2.0367e+00
Epoch 2/2
39/39 - 61s - loss: 600.2244 - loglik: -5.9716e+02 - logprior: -1.0016e+00
Fitted a model with MAP estimate = -594.2875
expansions: []
discards: [336 337]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 64s - loss: 604.4860 - loglik: -6.0001e+02 - logprior: -1.7668e+00
Epoch 2/2
39/39 - 61s - loss: 598.5136 - loglik: -5.9510e+02 - logprior: -6.7130e-01
Fitted a model with MAP estimate = -593.2269
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 63s - loss: 603.3914 - loglik: -5.9865e+02 - logprior: -1.5461e+00
Epoch 2/10
39/39 - 59s - loss: 598.2088 - loglik: -5.9494e+02 - logprior: -4.0300e-01
Epoch 3/10
39/39 - 59s - loss: 595.4393 - loglik: -5.9263e+02 - logprior: -2.4854e-01
Epoch 4/10
39/39 - 61s - loss: 594.0823 - loglik: -5.9188e+02 - logprior: -7.0941e-02
Epoch 5/10
39/39 - 64s - loss: 592.7424 - loglik: -5.9097e+02 - logprior: 0.1054
Epoch 6/10
39/39 - 64s - loss: 592.6271 - loglik: -5.9119e+02 - logprior: 0.2290
Epoch 7/10
39/39 - 65s - loss: 590.4054 - loglik: -5.8936e+02 - logprior: 0.4246
Epoch 8/10
39/39 - 68s - loss: 591.6389 - loglik: -5.9067e+02 - logprior: 0.4020
Fitted a model with MAP estimate = -588.6923
Time for alignment: 1359.2589
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 770.5353 - loglik: -7.6837e+02 - logprior: -1.7237e+00
Epoch 2/10
39/39 - 46s - loss: 641.8972 - loglik: -6.3841e+02 - logprior: -2.3835e+00
Epoch 3/10
39/39 - 46s - loss: 630.5577 - loglik: -6.2669e+02 - logprior: -2.4502e+00
Epoch 4/10
39/39 - 46s - loss: 627.2994 - loglik: -6.2354e+02 - logprior: -2.4003e+00
Epoch 5/10
39/39 - 48s - loss: 625.7997 - loglik: -6.2211e+02 - logprior: -2.4120e+00
Epoch 6/10
39/39 - 48s - loss: 624.9436 - loglik: -6.2132e+02 - logprior: -2.4196e+00
Epoch 7/10
39/39 - 49s - loss: 625.2242 - loglik: -6.2166e+02 - logprior: -2.4519e+00
Fitted a model with MAP estimate = -622.4272
expansions: [(9, 1), (12, 1), (14, 1), (15, 1), (16, 1), (45, 1), (52, 3), (55, 1), (61, 2), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (77, 1), (78, 1), (79, 1), (83, 1), (84, 1), (85, 1), (87, 1), (91, 1), (98, 1), (100, 1), (103, 1), (106, 1), (108, 1), (117, 1), (119, 1), (121, 1), (134, 1), (139, 1), (141, 1), (142, 1), (158, 1), (159, 1), (160, 1), (162, 1), (163, 1), (165, 1), (166, 1), (174, 1), (185, 1), (188, 1), (192, 1), (194, 1), (195, 1), (196, 1), (198, 1), (199, 2), (200, 1), (201, 1), (214, 1), (217, 1), (218, 1), (220, 1), (229, 1), (234, 1), (236, 2), (247, 1), (263, 1), (264, 1), (265, 1), (266, 3), (267, 2), (268, 1), (279, 2), (280, 3)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 71s - loss: 615.4775 - loglik: -6.1164e+02 - logprior: -2.0595e+00
Epoch 2/2
39/39 - 68s - loss: 600.1037 - loglik: -5.9690e+02 - logprior: -9.7201e-01
Fitted a model with MAP estimate = -594.4041
expansions: []
discards: [ 73 250]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 62s - loss: 604.7281 - loglik: -6.0009e+02 - logprior: -1.7870e+00
Epoch 2/2
39/39 - 59s - loss: 599.0454 - loglik: -5.9555e+02 - logprior: -7.4590e-01
Fitted a model with MAP estimate = -593.6949
expansions: [(183, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 366 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 68s - loss: 603.7922 - loglik: -5.9901e+02 - logprior: -1.5732e+00
Epoch 2/10
39/39 - 66s - loss: 597.9067 - loglik: -5.9460e+02 - logprior: -4.2542e-01
Epoch 3/10
39/39 - 66s - loss: 595.4597 - loglik: -5.9259e+02 - logprior: -2.9797e-01
Epoch 4/10
39/39 - 70s - loss: 593.4327 - loglik: -5.9118e+02 - logprior: -9.8191e-02
Epoch 5/10
39/39 - 69s - loss: 592.9506 - loglik: -5.9114e+02 - logprior: 0.0781
Epoch 6/10
39/39 - 70s - loss: 591.8345 - loglik: -5.9040e+02 - logprior: 0.2228
Epoch 7/10
39/39 - 69s - loss: 591.2481 - loglik: -5.9012e+02 - logprior: 0.3754
Epoch 8/10
39/39 - 63s - loss: 590.5989 - loglik: -5.8975e+02 - logprior: 0.5299
Epoch 9/10
39/39 - 59s - loss: 590.0021 - loglik: -5.8938e+02 - logprior: 0.6436
Epoch 10/10
39/39 - 59s - loss: 590.0688 - loglik: -5.8972e+02 - logprior: 0.8231
Fitted a model with MAP estimate = -587.4879
Time for alignment: 1490.9180
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 44s - loss: 770.0169 - loglik: -7.6788e+02 - logprior: -1.7023e+00
Epoch 2/10
39/39 - 41s - loss: 640.8737 - loglik: -6.3716e+02 - logprior: -2.4071e+00
Epoch 3/10
39/39 - 41s - loss: 630.3383 - loglik: -6.2637e+02 - logprior: -2.3858e+00
Epoch 4/10
39/39 - 41s - loss: 628.1667 - loglik: -6.2445e+02 - logprior: -2.3026e+00
Epoch 5/10
39/39 - 41s - loss: 626.5208 - loglik: -6.2292e+02 - logprior: -2.2995e+00
Epoch 6/10
39/39 - 41s - loss: 625.5090 - loglik: -6.2199e+02 - logprior: -2.3220e+00
Epoch 7/10
39/39 - 40s - loss: 625.2656 - loglik: -6.2179e+02 - logprior: -2.3402e+00
Epoch 8/10
39/39 - 41s - loss: 624.7409 - loglik: -6.2131e+02 - logprior: -2.3630e+00
Epoch 9/10
39/39 - 41s - loss: 624.5610 - loglik: -6.2122e+02 - logprior: -2.3694e+00
Epoch 10/10
39/39 - 41s - loss: 624.4014 - loglik: -6.2112e+02 - logprior: -2.3757e+00
Fitted a model with MAP estimate = -622.2949
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (50, 1), (51, 4), (55, 1), (61, 2), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (77, 1), (78, 1), (81, 1), (83, 1), (84, 1), (85, 1), (87, 1), (90, 1), (98, 1), (99, 1), (100, 1), (106, 1), (112, 1), (117, 1), (119, 1), (121, 1), (135, 1), (140, 1), (143, 1), (146, 1), (148, 1), (158, 1), (159, 1), (160, 1), (162, 2), (163, 3), (164, 1), (172, 1), (183, 1), (187, 1), (193, 1), (194, 1), (195, 1), (197, 1), (198, 1), (199, 1), (213, 1), (217, 1), (218, 1), (220, 1), (229, 1), (233, 1), (236, 2), (238, 1), (263, 1), (264, 1), (265, 1), (266, 3), (267, 2), (268, 1), (279, 2), (280, 3)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 61s - loss: 618.1985 - loglik: -6.1470e+02 - logprior: -2.0340e+00
Epoch 2/2
39/39 - 60s - loss: 600.3218 - loglik: -5.9731e+02 - logprior: -9.5543e-01
Fitted a model with MAP estimate = -594.2413
expansions: [(16, 1)]
discards: [55 56 73]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 65s - loss: 604.5091 - loglik: -6.0002e+02 - logprior: -1.7535e+00
Epoch 2/2
39/39 - 62s - loss: 598.3807 - loglik: -5.9503e+02 - logprior: -6.3807e-01
Fitted a model with MAP estimate = -593.3629
expansions: [(56, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 67s - loss: 603.3217 - loglik: -5.9862e+02 - logprior: -1.5403e+00
Epoch 2/10
39/39 - 63s - loss: 597.6969 - loglik: -5.9445e+02 - logprior: -3.6304e-01
Epoch 3/10
39/39 - 63s - loss: 595.3272 - loglik: -5.9251e+02 - logprior: -2.2096e-01
Epoch 4/10
39/39 - 62s - loss: 593.4446 - loglik: -5.9125e+02 - logprior: -1.2894e-02
Epoch 5/10
39/39 - 61s - loss: 592.7781 - loglik: -5.9100e+02 - logprior: 0.1127
Epoch 6/10
39/39 - 60s - loss: 591.9614 - loglik: -5.9048e+02 - logprior: 0.2100
Epoch 7/10
39/39 - 60s - loss: 590.7662 - loglik: -5.8963e+02 - logprior: 0.3741
Epoch 8/10
39/39 - 60s - loss: 590.7932 - loglik: -5.8997e+02 - logprior: 0.5599
Fitted a model with MAP estimate = -588.0589
Time for alignment: 1381.9777
Computed alignments with likelihoods: ['-588.6923', '-587.4879', '-588.0589']
Best model has likelihood: -587.4879
time for generating output: 0.3596
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00079.projection.fasta
SP score = 0.9400871459694989
Training of 3 independent models on file PF01381.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff6daf48790>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6fce9bf40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6e3a133d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6dadd8ac0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6dadd87f0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff6fceae6a0>, <__main__.SimpleDirichletPrior object at 0x7ff3986e8b50>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff71d622040>

Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.1011 - loglik: -1.4685e+02 - logprior: -3.2262e+00
Epoch 2/10
19/19 - 1s - loss: 129.8281 - loglik: -1.2805e+02 - logprior: -1.3937e+00
Epoch 3/10
19/19 - 1s - loss: 119.6292 - loglik: -1.1744e+02 - logprior: -1.6414e+00
Epoch 4/10
19/19 - 1s - loss: 117.2697 - loglik: -1.1542e+02 - logprior: -1.5445e+00
Epoch 5/10
19/19 - 1s - loss: 116.6639 - loglik: -1.1488e+02 - logprior: -1.5309e+00
Epoch 6/10
19/19 - 1s - loss: 116.2301 - loglik: -1.1450e+02 - logprior: -1.4973e+00
Epoch 7/10
19/19 - 1s - loss: 116.2397 - loglik: -1.1453e+02 - logprior: -1.4864e+00
Fitted a model with MAP estimate = -115.6660
expansions: [(6, 1), (7, 1), (8, 1), (10, 1), (19, 2), (26, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 121.3289 - loglik: -1.1679e+02 - logprior: -4.1881e+00
Epoch 2/2
19/19 - 1s - loss: 113.0470 - loglik: -1.1049e+02 - logprior: -2.1009e+00
Fitted a model with MAP estimate = -110.9501
expansions: [(0, 2)]
discards: [ 0 22 34 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 112.9691 - loglik: -1.0937e+02 - logprior: -3.0787e+00
Epoch 2/2
19/19 - 1s - loss: 109.7284 - loglik: -1.0796e+02 - logprior: -1.2784e+00
Fitted a model with MAP estimate = -108.4624
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 114.4922 - loglik: -1.1012e+02 - logprior: -3.8040e+00
Epoch 2/10
19/19 - 1s - loss: 110.2523 - loglik: -1.0828e+02 - logprior: -1.4730e+00
Epoch 3/10
19/19 - 1s - loss: 109.3892 - loglik: -1.0757e+02 - logprior: -1.3094e+00
Epoch 4/10
19/19 - 1s - loss: 108.9684 - loglik: -1.0720e+02 - logprior: -1.2777e+00
Epoch 5/10
19/19 - 1s - loss: 108.6044 - loglik: -1.0692e+02 - logprior: -1.2557e+00
Epoch 6/10
19/19 - 1s - loss: 108.4038 - loglik: -1.0676e+02 - logprior: -1.2477e+00
Epoch 7/10
19/19 - 1s - loss: 108.1265 - loglik: -1.0654e+02 - logprior: -1.2309e+00
Epoch 8/10
19/19 - 1s - loss: 107.9396 - loglik: -1.0639e+02 - logprior: -1.2130e+00
Epoch 9/10
19/19 - 1s - loss: 107.9982 - loglik: -1.0647e+02 - logprior: -1.2005e+00
Fitted a model with MAP estimate = -107.4033
Time for alignment: 48.2031
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 149.9652 - loglik: -1.4672e+02 - logprior: -3.2221e+00
Epoch 2/10
19/19 - 1s - loss: 128.8144 - loglik: -1.2712e+02 - logprior: -1.3729e+00
Epoch 3/10
19/19 - 1s - loss: 120.0207 - loglik: -1.1800e+02 - logprior: -1.5706e+00
Epoch 4/10
19/19 - 1s - loss: 118.3590 - loglik: -1.1659e+02 - logprior: -1.4829e+00
Epoch 5/10
19/19 - 1s - loss: 117.9891 - loglik: -1.1629e+02 - logprior: -1.4579e+00
Epoch 6/10
19/19 - 1s - loss: 117.7476 - loglik: -1.1611e+02 - logprior: -1.4335e+00
Epoch 7/10
19/19 - 1s - loss: 117.5981 - loglik: -1.1596e+02 - logprior: -1.4166e+00
Epoch 8/10
19/19 - 1s - loss: 117.3415 - loglik: -1.1573e+02 - logprior: -1.4123e+00
Epoch 9/10
19/19 - 1s - loss: 117.3751 - loglik: -1.1578e+02 - logprior: -1.4021e+00
Fitted a model with MAP estimate = -116.9260
expansions: [(6, 3), (7, 1), (19, 2), (20, 1), (27, 2), (28, 2), (30, 1), (31, 2), (32, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 121.8126 - loglik: -1.1731e+02 - logprior: -4.1920e+00
Epoch 2/2
19/19 - 1s - loss: 113.1177 - loglik: -1.1049e+02 - logprior: -2.1649e+00
Fitted a model with MAP estimate = -111.0757
expansions: [(0, 2)]
discards: [ 0 23 34 36 42]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.0165 - loglik: -1.0940e+02 - logprior: -3.0756e+00
Epoch 2/2
19/19 - 1s - loss: 109.6712 - loglik: -1.0790e+02 - logprior: -1.2802e+00
Fitted a model with MAP estimate = -108.4781
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 114.5310 - loglik: -1.1014e+02 - logprior: -3.8143e+00
Epoch 2/10
19/19 - 1s - loss: 110.1897 - loglik: -1.0824e+02 - logprior: -1.4687e+00
Epoch 3/10
19/19 - 1s - loss: 109.4912 - loglik: -1.0766e+02 - logprior: -1.3134e+00
Epoch 4/10
19/19 - 1s - loss: 109.0086 - loglik: -1.0725e+02 - logprior: -1.2740e+00
Epoch 5/10
19/19 - 1s - loss: 108.6530 - loglik: -1.0696e+02 - logprior: -1.2555e+00
Epoch 6/10
19/19 - 1s - loss: 108.2901 - loglik: -1.0666e+02 - logprior: -1.2449e+00
Epoch 7/10
19/19 - 1s - loss: 108.0865 - loglik: -1.0650e+02 - logprior: -1.2286e+00
Epoch 8/10
19/19 - 1s - loss: 107.8817 - loglik: -1.0633e+02 - logprior: -1.2136e+00
Epoch 9/10
19/19 - 1s - loss: 108.0210 - loglik: -1.0650e+02 - logprior: -1.1984e+00
Fitted a model with MAP estimate = -107.3976
Time for alignment: 49.5451
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.0305 - loglik: -1.4678e+02 - logprior: -3.2242e+00
Epoch 2/10
19/19 - 1s - loss: 127.8571 - loglik: -1.2616e+02 - logprior: -1.4160e+00
Epoch 3/10
19/19 - 1s - loss: 118.6090 - loglik: -1.1647e+02 - logprior: -1.6812e+00
Epoch 4/10
19/19 - 1s - loss: 116.9893 - loglik: -1.1511e+02 - logprior: -1.5654e+00
Epoch 5/10
19/19 - 1s - loss: 116.3915 - loglik: -1.1459e+02 - logprior: -1.5368e+00
Epoch 6/10
19/19 - 1s - loss: 116.1629 - loglik: -1.1442e+02 - logprior: -1.5101e+00
Epoch 7/10
19/19 - 1s - loss: 116.0018 - loglik: -1.1429e+02 - logprior: -1.4904e+00
Epoch 8/10
19/19 - 1s - loss: 116.0465 - loglik: -1.1435e+02 - logprior: -1.4814e+00
Fitted a model with MAP estimate = -115.4808
expansions: [(6, 1), (7, 1), (8, 1), (10, 1), (19, 2), (23, 2), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 122.1559 - loglik: -1.1763e+02 - logprior: -4.1977e+00
Epoch 2/2
19/19 - 1s - loss: 113.2991 - loglik: -1.1068e+02 - logprior: -2.1562e+00
Fitted a model with MAP estimate = -111.0916
expansions: [(0, 2)]
discards: [ 0 23 28 35 42]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.1990 - loglik: -1.0960e+02 - logprior: -3.0816e+00
Epoch 2/2
19/19 - 1s - loss: 109.7146 - loglik: -1.0795e+02 - logprior: -1.2783e+00
Fitted a model with MAP estimate = -108.4952
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 114.5871 - loglik: -1.1021e+02 - logprior: -3.8125e+00
Epoch 2/10
19/19 - 1s - loss: 110.3051 - loglik: -1.0834e+02 - logprior: -1.4724e+00
Epoch 3/10
19/19 - 1s - loss: 109.4183 - loglik: -1.0759e+02 - logprior: -1.3095e+00
Epoch 4/10
19/19 - 1s - loss: 108.9795 - loglik: -1.0722e+02 - logprior: -1.2720e+00
Epoch 5/10
19/19 - 1s - loss: 108.6078 - loglik: -1.0691e+02 - logprior: -1.2593e+00
Epoch 6/10
19/19 - 1s - loss: 108.3939 - loglik: -1.0676e+02 - logprior: -1.2430e+00
Epoch 7/10
19/19 - 1s - loss: 107.9728 - loglik: -1.0638e+02 - logprior: -1.2320e+00
Epoch 8/10
19/19 - 1s - loss: 108.2014 - loglik: -1.0665e+02 - logprior: -1.2110e+00
Fitted a model with MAP estimate = -107.5195
Time for alignment: 48.1647
Computed alignments with likelihoods: ['-107.4033', '-107.3976', '-107.5195']
Best model has likelihood: -107.3976
time for generating output: 0.1178
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01381.projection.fasta
SP score = 0.676250322802789
Training of 3 independent models on file PF02878.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff33c45d9a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff52d024520>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff52d023640>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6a7d9feb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff3963c4280>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff33c430df0>, <__main__.SimpleDirichletPrior object at 0x7ff52d2433a0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff396483ee0>

Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 360.0577 - loglik: -3.5692e+02 - logprior: -3.0783e+00
Epoch 2/10
19/19 - 3s - loss: 305.9368 - loglik: -3.0442e+02 - logprior: -1.2839e+00
Epoch 3/10
19/19 - 3s - loss: 277.9758 - loglik: -2.7555e+02 - logprior: -1.5477e+00
Epoch 4/10
19/19 - 4s - loss: 270.2852 - loglik: -2.6784e+02 - logprior: -1.5223e+00
Epoch 5/10
19/19 - 4s - loss: 268.5681 - loglik: -2.6641e+02 - logprior: -1.4620e+00
Epoch 6/10
19/19 - 4s - loss: 267.2268 - loglik: -2.6519e+02 - logprior: -1.4484e+00
Epoch 7/10
19/19 - 4s - loss: 266.3901 - loglik: -2.6442e+02 - logprior: -1.4408e+00
Epoch 8/10
19/19 - 4s - loss: 265.9001 - loglik: -2.6396e+02 - logprior: -1.4401e+00
Epoch 9/10
19/19 - 4s - loss: 265.8494 - loglik: -2.6391e+02 - logprior: -1.4326e+00
Epoch 10/10
19/19 - 4s - loss: 265.6317 - loglik: -2.6371e+02 - logprior: -1.4310e+00
Fitted a model with MAP estimate = -264.8924
expansions: [(12, 1), (14, 4), (16, 1), (23, 1), (26, 1), (28, 1), (29, 2), (30, 3), (43, 1), (45, 1), (48, 1), (55, 1), (56, 1), (69, 2), (70, 1), (71, 1), (93, 1), (97, 1), (101, 2), (103, 1), (108, 1), (109, 1), (110, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 270.2780 - loglik: -2.6558e+02 - logprior: -4.1005e+00
Epoch 2/2
19/19 - 5s - loss: 257.7955 - loglik: -2.5471e+02 - logprior: -2.1873e+00
Fitted a model with MAP estimate = -254.9331
expansions: [(0, 2)]
discards: [  0  14  15  38  88 125 139]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 137 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 259.8307 - loglik: -2.5589e+02 - logprior: -2.9511e+00
Epoch 2/2
19/19 - 4s - loss: 255.5354 - loglik: -2.5351e+02 - logprior: -1.2092e+00
Fitted a model with MAP estimate = -253.7666
expansions: [(38, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 137 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 260.5632 - loglik: -2.5573e+02 - logprior: -3.9618e+00
Epoch 2/10
19/19 - 4s - loss: 256.5679 - loglik: -2.5409e+02 - logprior: -1.6636e+00
Epoch 3/10
19/19 - 4s - loss: 254.6149 - loglik: -2.5288e+02 - logprior: -9.4205e-01
Epoch 4/10
19/19 - 5s - loss: 254.0211 - loglik: -2.5238e+02 - logprior: -8.9342e-01
Epoch 5/10
19/19 - 4s - loss: 252.7707 - loglik: -2.5122e+02 - logprior: -8.6713e-01
Epoch 6/10
19/19 - 4s - loss: 252.9164 - loglik: -2.5149e+02 - logprior: -8.0150e-01
Fitted a model with MAP estimate = -251.8415
Time for alignment: 121.1312
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 360.6029 - loglik: -3.5746e+02 - logprior: -3.0787e+00
Epoch 2/10
19/19 - 4s - loss: 304.6054 - loglik: -3.0311e+02 - logprior: -1.2731e+00
Epoch 3/10
19/19 - 4s - loss: 277.4808 - loglik: -2.7521e+02 - logprior: -1.4701e+00
Epoch 4/10
19/19 - 4s - loss: 271.0832 - loglik: -2.6876e+02 - logprior: -1.4894e+00
Epoch 5/10
19/19 - 4s - loss: 267.7978 - loglik: -2.6559e+02 - logprior: -1.5056e+00
Epoch 6/10
19/19 - 4s - loss: 267.2243 - loglik: -2.6514e+02 - logprior: -1.4730e+00
Epoch 7/10
19/19 - 4s - loss: 266.1424 - loglik: -2.6413e+02 - logprior: -1.4511e+00
Epoch 8/10
19/19 - 4s - loss: 266.4187 - loglik: -2.6445e+02 - logprior: -1.4368e+00
Fitted a model with MAP estimate = -265.2463
expansions: [(12, 2), (13, 4), (16, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (56, 1), (69, 2), (70, 1), (71, 1), (93, 1), (96, 1), (101, 1), (107, 1), (108, 1), (109, 1), (110, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 269.8782 - loglik: -2.6513e+02 - logprior: -4.0972e+00
Epoch 2/2
19/19 - 5s - loss: 258.5432 - loglik: -2.5546e+02 - logprior: -2.1305e+00
Fitted a model with MAP estimate = -255.4160
expansions: [(0, 2)]
discards: [  0  13  14  38  88 138]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 137 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 259.9821 - loglik: -2.5606e+02 - logprior: -2.9283e+00
Epoch 2/2
19/19 - 4s - loss: 256.0970 - loglik: -2.5407e+02 - logprior: -1.1930e+00
Fitted a model with MAP estimate = -254.1907
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 261.1020 - loglik: -2.5624e+02 - logprior: -3.9527e+00
Epoch 2/10
19/19 - 4s - loss: 257.3004 - loglik: -2.5486e+02 - logprior: -1.6213e+00
Epoch 3/10
19/19 - 4s - loss: 254.9165 - loglik: -2.5317e+02 - logprior: -9.3495e-01
Epoch 4/10
19/19 - 5s - loss: 254.5082 - loglik: -2.5289e+02 - logprior: -8.6983e-01
Epoch 5/10
19/19 - 5s - loss: 253.3900 - loglik: -2.5185e+02 - logprior: -8.4124e-01
Epoch 6/10
19/19 - 5s - loss: 253.6998 - loglik: -2.5231e+02 - logprior: -7.6158e-01
Fitted a model with MAP estimate = -252.4312
Time for alignment: 113.7721
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 360.6120 - loglik: -3.5747e+02 - logprior: -3.0812e+00
Epoch 2/10
19/19 - 4s - loss: 304.2365 - loglik: -3.0270e+02 - logprior: -1.3124e+00
Epoch 3/10
19/19 - 4s - loss: 275.8586 - loglik: -2.7382e+02 - logprior: -1.5854e+00
Epoch 4/10
19/19 - 4s - loss: 270.3040 - loglik: -2.6796e+02 - logprior: -1.6082e+00
Epoch 5/10
19/19 - 4s - loss: 268.6444 - loglik: -2.6650e+02 - logprior: -1.5233e+00
Epoch 6/10
19/19 - 4s - loss: 267.0931 - loglik: -2.6503e+02 - logprior: -1.4837e+00
Epoch 7/10
19/19 - 4s - loss: 266.7068 - loglik: -2.6472e+02 - logprior: -1.4631e+00
Epoch 8/10
19/19 - 4s - loss: 266.1883 - loglik: -2.6424e+02 - logprior: -1.4431e+00
Epoch 9/10
19/19 - 4s - loss: 266.4594 - loglik: -2.6452e+02 - logprior: -1.4374e+00
Fitted a model with MAP estimate = -265.4097
expansions: [(12, 1), (14, 3), (16, 1), (19, 1), (26, 1), (28, 1), (29, 3), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (93, 1), (97, 1), (101, 1), (107, 1), (108, 1), (109, 1), (110, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 269.6633 - loglik: -2.6495e+02 - logprior: -4.0981e+00
Epoch 2/2
19/19 - 5s - loss: 258.5950 - loglik: -2.5561e+02 - logprior: -2.1007e+00
Fitted a model with MAP estimate = -255.6370
expansions: [(0, 2)]
discards: [  0  37  38  76 138]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 259.5834 - loglik: -2.5566e+02 - logprior: -2.9456e+00
Epoch 2/2
19/19 - 5s - loss: 256.0605 - loglik: -2.5401e+02 - logprior: -1.1914e+00
Fitted a model with MAP estimate = -254.2234
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 137 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 260.8813 - loglik: -2.5599e+02 - logprior: -3.9408e+00
Epoch 2/10
19/19 - 5s - loss: 256.7252 - loglik: -2.5431e+02 - logprior: -1.5387e+00
Epoch 3/10
19/19 - 5s - loss: 255.0078 - loglik: -2.5324e+02 - logprior: -9.2932e-01
Epoch 4/10
19/19 - 5s - loss: 254.1616 - loglik: -2.5247e+02 - logprior: -9.0909e-01
Epoch 5/10
19/19 - 5s - loss: 253.9182 - loglik: -2.5232e+02 - logprior: -8.6723e-01
Epoch 6/10
19/19 - 5s - loss: 252.8338 - loglik: -2.5135e+02 - logprior: -8.0650e-01
Epoch 7/10
19/19 - 5s - loss: 252.7248 - loglik: -2.5133e+02 - logprior: -7.5657e-01
Epoch 8/10
19/19 - 5s - loss: 252.7416 - loglik: -2.5142e+02 - logprior: -7.1848e-01
Fitted a model with MAP estimate = -251.6941
Time for alignment: 128.8351
Computed alignments with likelihoods: ['-251.8415', '-252.4312', '-251.6941']
Best model has likelihood: -251.6941
time for generating output: 0.1860
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02878.projection.fasta
SP score = 0.7629139072847683
Training of 3 independent models on file PF00970.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff52d3caf70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff36050ddc0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff51a2ab040>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6c16a8370>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff31c7cefd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff660440940>, <__main__.SimpleDirichletPrior object at 0x7ff51a06c040>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff52365e670>

Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 274.6859 - loglik: -2.7150e+02 - logprior: -3.1208e+00
Epoch 2/10
19/19 - 3s - loss: 237.2283 - loglik: -2.3555e+02 - logprior: -1.3159e+00
Epoch 3/10
19/19 - 3s - loss: 221.7577 - loglik: -2.1956e+02 - logprior: -1.5076e+00
Epoch 4/10
19/19 - 3s - loss: 218.1938 - loglik: -2.1617e+02 - logprior: -1.5183e+00
Epoch 5/10
19/19 - 3s - loss: 216.4981 - loglik: -2.1460e+02 - logprior: -1.4842e+00
Epoch 6/10
19/19 - 3s - loss: 215.8036 - loglik: -2.1400e+02 - logprior: -1.4585e+00
Epoch 7/10
19/19 - 3s - loss: 216.0041 - loglik: -2.1422e+02 - logprior: -1.4410e+00
Fitted a model with MAP estimate = -214.8563
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (13, 1), (18, 1), (19, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 218.3858 - loglik: -2.1395e+02 - logprior: -3.9720e+00
Epoch 2/2
19/19 - 3s - loss: 207.0759 - loglik: -2.0497e+02 - logprior: -1.4871e+00
Fitted a model with MAP estimate = -204.6685
expansions: [(25, 1)]
discards: [ 0 45 74 81 83 84 95]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 211.9698 - loglik: -2.0712e+02 - logprior: -4.1571e+00
Epoch 2/2
19/19 - 3s - loss: 207.3103 - loglik: -2.0505e+02 - logprior: -1.6655e+00
Fitted a model with MAP estimate = -205.1830
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 100 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 208.6904 - loglik: -2.0501e+02 - logprior: -3.0207e+00
Epoch 2/10
19/19 - 3s - loss: 205.8290 - loglik: -2.0399e+02 - logprior: -1.2325e+00
Epoch 3/10
19/19 - 3s - loss: 204.9978 - loglik: -2.0319e+02 - logprior: -1.1907e+00
Epoch 4/10
19/19 - 3s - loss: 204.6398 - loglik: -2.0292e+02 - logprior: -1.1409e+00
Epoch 5/10
19/19 - 3s - loss: 204.0136 - loglik: -2.0239e+02 - logprior: -1.0986e+00
Epoch 6/10
19/19 - 3s - loss: 203.6890 - loglik: -2.0214e+02 - logprior: -1.0840e+00
Epoch 7/10
19/19 - 3s - loss: 203.7699 - loglik: -2.0229e+02 - logprior: -1.0568e+00
Fitted a model with MAP estimate = -203.0730
Time for alignment: 88.6233
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 274.6438 - loglik: -2.7146e+02 - logprior: -3.1201e+00
Epoch 2/10
19/19 - 3s - loss: 236.0408 - loglik: -2.3436e+02 - logprior: -1.3060e+00
Epoch 3/10
19/19 - 3s - loss: 220.7928 - loglik: -2.1864e+02 - logprior: -1.5365e+00
Epoch 4/10
19/19 - 3s - loss: 217.1946 - loglik: -2.1516e+02 - logprior: -1.5409e+00
Epoch 5/10
19/19 - 3s - loss: 216.2723 - loglik: -2.1441e+02 - logprior: -1.4657e+00
Epoch 6/10
19/19 - 3s - loss: 215.4029 - loglik: -2.1357e+02 - logprior: -1.4694e+00
Epoch 7/10
19/19 - 3s - loss: 215.2801 - loglik: -2.1351e+02 - logprior: -1.4446e+00
Epoch 8/10
19/19 - 3s - loss: 215.1234 - loglik: -2.1336e+02 - logprior: -1.4342e+00
Epoch 9/10
19/19 - 3s - loss: 214.6031 - loglik: -2.1285e+02 - logprior: -1.4328e+00
Epoch 10/10
19/19 - 3s - loss: 215.0165 - loglik: -2.1325e+02 - logprior: -1.4364e+00
Fitted a model with MAP estimate = -213.9381
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (18, 1), (19, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 218.6160 - loglik: -2.1414e+02 - logprior: -4.0307e+00
Epoch 2/2
19/19 - 3s - loss: 207.5705 - loglik: -2.0546e+02 - logprior: -1.4621e+00
Fitted a model with MAP estimate = -204.9794
expansions: []
discards: [ 0 44 73 80 82 83 94]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 97 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 212.2325 - loglik: -2.0743e+02 - logprior: -4.1141e+00
Epoch 2/2
19/19 - 3s - loss: 207.4816 - loglik: -2.0534e+02 - logprior: -1.5635e+00
Fitted a model with MAP estimate = -205.4704
expansions: [(24, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 98 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 209.4246 - loglik: -2.0570e+02 - logprior: -3.0783e+00
Epoch 2/10
19/19 - 3s - loss: 206.3515 - loglik: -2.0452e+02 - logprior: -1.2334e+00
Epoch 3/10
19/19 - 3s - loss: 205.3117 - loglik: -2.0356e+02 - logprior: -1.1463e+00
Epoch 4/10
19/19 - 3s - loss: 204.9783 - loglik: -2.0330e+02 - logprior: -1.0979e+00
Epoch 5/10
19/19 - 3s - loss: 204.6605 - loglik: -2.0309e+02 - logprior: -1.0591e+00
Epoch 6/10
19/19 - 3s - loss: 204.3124 - loglik: -2.0281e+02 - logprior: -1.0346e+00
Epoch 7/10
19/19 - 3s - loss: 204.0105 - loglik: -2.0256e+02 - logprior: -1.0183e+00
Epoch 8/10
19/19 - 3s - loss: 203.9662 - loglik: -2.0256e+02 - logprior: -1.0061e+00
Epoch 9/10
19/19 - 3s - loss: 203.9384 - loglik: -2.0260e+02 - logprior: -9.6565e-01
Epoch 10/10
19/19 - 3s - loss: 203.7706 - loglik: -2.0245e+02 - logprior: -9.4413e-01
Fitted a model with MAP estimate = -203.1965
Time for alignment: 104.8407
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 274.5809 - loglik: -2.7140e+02 - logprior: -3.1241e+00
Epoch 2/10
19/19 - 3s - loss: 236.1640 - loglik: -2.3446e+02 - logprior: -1.3300e+00
Epoch 3/10
19/19 - 3s - loss: 220.2999 - loglik: -2.1804e+02 - logprior: -1.5621e+00
Epoch 4/10
19/19 - 3s - loss: 216.0177 - loglik: -2.1392e+02 - logprior: -1.5595e+00
Epoch 5/10
19/19 - 3s - loss: 215.1749 - loglik: -2.1325e+02 - logprior: -1.5112e+00
Epoch 6/10
19/19 - 3s - loss: 214.6216 - loglik: -2.1276e+02 - logprior: -1.4710e+00
Epoch 7/10
19/19 - 3s - loss: 214.4285 - loglik: -2.1262e+02 - logprior: -1.4380e+00
Epoch 8/10
19/19 - 3s - loss: 214.1837 - loglik: -2.1240e+02 - logprior: -1.4280e+00
Epoch 9/10
19/19 - 3s - loss: 213.9241 - loglik: -2.1216e+02 - logprior: -1.4244e+00
Epoch 10/10
19/19 - 3s - loss: 214.0027 - loglik: -2.1225e+02 - logprior: -1.4217e+00
Fitted a model with MAP estimate = -213.0744
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (17, 1), (18, 2), (29, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (63, 3), (64, 1), (71, 2), (77, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 217.3323 - loglik: -2.1286e+02 - logprior: -4.0220e+00
Epoch 2/2
19/19 - 3s - loss: 206.2750 - loglik: -2.0418e+02 - logprior: -1.4294e+00
Fitted a model with MAP estimate = -203.8652
expansions: [(25, 1)]
discards: [ 0 45 74 78 94]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 210.3672 - loglik: -2.0557e+02 - logprior: -4.1009e+00
Epoch 2/2
19/19 - 3s - loss: 206.0090 - loglik: -2.0387e+02 - logprior: -1.5555e+00
Fitted a model with MAP estimate = -203.8839
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 208.7345 - loglik: -2.0488e+02 - logprior: -3.2079e+00
Epoch 2/10
19/19 - 3s - loss: 205.2657 - loglik: -2.0355e+02 - logprior: -1.1267e+00
Epoch 3/10
19/19 - 3s - loss: 204.7608 - loglik: -2.0314e+02 - logprior: -1.0207e+00
Epoch 4/10
19/19 - 3s - loss: 203.8409 - loglik: -2.0227e+02 - logprior: -1.0085e+00
Epoch 5/10
19/19 - 3s - loss: 203.6761 - loglik: -2.0222e+02 - logprior: -9.4765e-01
Epoch 6/10
19/19 - 3s - loss: 203.3201 - loglik: -2.0194e+02 - logprior: -9.2856e-01
Epoch 7/10
19/19 - 3s - loss: 203.0929 - loglik: -2.0177e+02 - logprior: -9.0469e-01
Epoch 8/10
19/19 - 3s - loss: 203.1172 - loglik: -2.0183e+02 - logprior: -8.8556e-01
Fitted a model with MAP estimate = -202.4857
Time for alignment: 100.3957
Computed alignments with likelihoods: ['-203.0730', '-203.1965', '-202.4857']
Best model has likelihood: -202.4857
time for generating output: 0.2043
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00970.projection.fasta
SP score = 0.6542272275886973
Training of 3 independent models on file PF00313.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff6dae924f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff523849460>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff523849ac0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6606afc40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6606aff70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff52383ef70>, <__main__.SimpleDirichletPrior object at 0x7ff2f4786970>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff2fc12c8b0>

Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 176.4039 - loglik: -1.7314e+02 - logprior: -3.2485e+00
Epoch 2/10
19/19 - 2s - loss: 133.4044 - loglik: -1.3189e+02 - logprior: -1.4934e+00
Epoch 3/10
19/19 - 2s - loss: 117.4387 - loglik: -1.1572e+02 - logprior: -1.5133e+00
Epoch 4/10
19/19 - 2s - loss: 115.0614 - loglik: -1.1327e+02 - logprior: -1.5231e+00
Epoch 5/10
19/19 - 1s - loss: 113.9747 - loglik: -1.1230e+02 - logprior: -1.4819e+00
Epoch 6/10
19/19 - 2s - loss: 113.6175 - loglik: -1.1199e+02 - logprior: -1.4530e+00
Epoch 7/10
19/19 - 1s - loss: 113.2643 - loglik: -1.1163e+02 - logprior: -1.4439e+00
Epoch 8/10
19/19 - 2s - loss: 113.2102 - loglik: -1.1159e+02 - logprior: -1.4327e+00
Epoch 9/10
19/19 - 2s - loss: 112.9433 - loglik: -1.1131e+02 - logprior: -1.4290e+00
Epoch 10/10
19/19 - 2s - loss: 113.1419 - loglik: -1.1150e+02 - logprior: -1.4283e+00
Fitted a model with MAP estimate = -112.7114
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (27, 1), (28, 2), (29, 2), (30, 1), (31, 1), (34, 1), (40, 1), (41, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 114.6543 - loglik: -1.1023e+02 - logprior: -4.2377e+00
Epoch 2/2
19/19 - 1s - loss: 104.9803 - loglik: -1.0348e+02 - logprior: -1.3460e+00
Fitted a model with MAP estimate = -103.3201
expansions: []
discards: [ 0 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.4340 - loglik: -1.0517e+02 - logprior: -4.1221e+00
Epoch 2/2
19/19 - 2s - loss: 104.9865 - loglik: -1.0323e+02 - logprior: -1.5804e+00
Fitted a model with MAP estimate = -103.7237
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 107.5037 - loglik: -1.0406e+02 - logprior: -3.2603e+00
Epoch 2/10
19/19 - 2s - loss: 104.3603 - loglik: -1.0272e+02 - logprior: -1.4543e+00
Epoch 3/10
19/19 - 2s - loss: 103.3623 - loglik: -1.0174e+02 - logprior: -1.3616e+00
Epoch 4/10
19/19 - 2s - loss: 102.9325 - loglik: -1.0134e+02 - logprior: -1.3152e+00
Epoch 5/10
19/19 - 1s - loss: 102.2940 - loglik: -1.0073e+02 - logprior: -1.2828e+00
Epoch 6/10
19/19 - 2s - loss: 101.9927 - loglik: -1.0046e+02 - logprior: -1.2629e+00
Epoch 7/10
19/19 - 2s - loss: 101.8252 - loglik: -1.0033e+02 - logprior: -1.2436e+00
Epoch 8/10
19/19 - 2s - loss: 101.4169 - loglik: -9.9922e+01 - logprior: -1.2291e+00
Epoch 9/10
19/19 - 2s - loss: 101.5008 - loglik: -1.0003e+02 - logprior: -1.2053e+00
Fitted a model with MAP estimate = -101.0626
Time for alignment: 60.7582
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.4271 - loglik: -1.7316e+02 - logprior: -3.2529e+00
Epoch 2/10
19/19 - 1s - loss: 132.9431 - loglik: -1.3143e+02 - logprior: -1.4899e+00
Epoch 3/10
19/19 - 1s - loss: 117.0685 - loglik: -1.1534e+02 - logprior: -1.5086e+00
Epoch 4/10
19/19 - 2s - loss: 114.4413 - loglik: -1.1266e+02 - logprior: -1.5203e+00
Epoch 5/10
19/19 - 2s - loss: 113.6951 - loglik: -1.1202e+02 - logprior: -1.4631e+00
Epoch 6/10
19/19 - 1s - loss: 113.1080 - loglik: -1.1148e+02 - logprior: -1.4369e+00
Epoch 7/10
19/19 - 2s - loss: 112.9740 - loglik: -1.1136e+02 - logprior: -1.4228e+00
Epoch 8/10
19/19 - 1s - loss: 112.8568 - loglik: -1.1125e+02 - logprior: -1.4113e+00
Epoch 9/10
19/19 - 2s - loss: 112.6864 - loglik: -1.1107e+02 - logprior: -1.4093e+00
Epoch 10/10
19/19 - 2s - loss: 112.5834 - loglik: -1.1097e+02 - logprior: -1.4063e+00
Fitted a model with MAP estimate = -112.3255
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.3803 - loglik: -1.0994e+02 - logprior: -4.2514e+00
Epoch 2/2
19/19 - 2s - loss: 104.7819 - loglik: -1.0329e+02 - logprior: -1.3498e+00
Fitted a model with MAP estimate = -103.3094
expansions: []
discards: [ 0 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 109.3165 - loglik: -1.0508e+02 - logprior: -4.1017e+00
Epoch 2/2
19/19 - 2s - loss: 104.8977 - loglik: -1.0316e+02 - logprior: -1.5725e+00
Fitted a model with MAP estimate = -103.7128
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.4665 - loglik: -1.0402e+02 - logprior: -3.2644e+00
Epoch 2/10
19/19 - 1s - loss: 104.3050 - loglik: -1.0266e+02 - logprior: -1.4522e+00
Epoch 3/10
19/19 - 2s - loss: 103.5797 - loglik: -1.0196e+02 - logprior: -1.3626e+00
Epoch 4/10
19/19 - 2s - loss: 102.7574 - loglik: -1.0116e+02 - logprior: -1.3161e+00
Epoch 5/10
19/19 - 2s - loss: 102.0901 - loglik: -1.0053e+02 - logprior: -1.2833e+00
Epoch 6/10
19/19 - 2s - loss: 102.0871 - loglik: -1.0055e+02 - logprior: -1.2673e+00
Epoch 7/10
19/19 - 2s - loss: 101.8856 - loglik: -1.0038e+02 - logprior: -1.2453e+00
Epoch 8/10
19/19 - 2s - loss: 101.5886 - loglik: -1.0010e+02 - logprior: -1.2296e+00
Epoch 9/10
19/19 - 2s - loss: 101.4708 - loglik: -9.9985e+01 - logprior: -1.2068e+00
Epoch 10/10
19/19 - 2s - loss: 101.3968 - loglik: -9.9921e+01 - logprior: -1.1966e+00
Fitted a model with MAP estimate = -100.9547
Time for alignment: 61.0984
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 176.4812 - loglik: -1.7322e+02 - logprior: -3.2514e+00
Epoch 2/10
19/19 - 2s - loss: 133.7545 - loglik: -1.3224e+02 - logprior: -1.4931e+00
Epoch 3/10
19/19 - 1s - loss: 116.8962 - loglik: -1.1521e+02 - logprior: -1.5089e+00
Epoch 4/10
19/19 - 1s - loss: 113.8770 - loglik: -1.1211e+02 - logprior: -1.5348e+00
Epoch 5/10
19/19 - 1s - loss: 113.0628 - loglik: -1.1141e+02 - logprior: -1.4742e+00
Epoch 6/10
19/19 - 1s - loss: 112.7697 - loglik: -1.1114e+02 - logprior: -1.4450e+00
Epoch 7/10
19/19 - 2s - loss: 112.4920 - loglik: -1.1088e+02 - logprior: -1.4300e+00
Epoch 8/10
19/19 - 1s - loss: 112.2547 - loglik: -1.1065e+02 - logprior: -1.4194e+00
Epoch 9/10
19/19 - 2s - loss: 112.4833 - loglik: -1.1088e+02 - logprior: -1.4138e+00
Fitted a model with MAP estimate = -111.9334
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (34, 1), (40, 1), (41, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 114.0255 - loglik: -1.0963e+02 - logprior: -4.2193e+00
Epoch 2/2
19/19 - 2s - loss: 104.8084 - loglik: -1.0332e+02 - logprior: -1.3482e+00
Fitted a model with MAP estimate = -103.2745
expansions: []
discards: [ 0 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 109.3302 - loglik: -1.0507e+02 - logprior: -4.1284e+00
Epoch 2/2
19/19 - 2s - loss: 104.8793 - loglik: -1.0313e+02 - logprior: -1.5899e+00
Fitted a model with MAP estimate = -103.7475
expansions: [(0, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.3232 - loglik: -1.0420e+02 - logprior: -2.9644e+00
Epoch 2/10
19/19 - 1s - loss: 104.1059 - loglik: -1.0284e+02 - logprior: -1.0977e+00
Epoch 3/10
19/19 - 2s - loss: 103.1222 - loglik: -1.0156e+02 - logprior: -1.3149e+00
Epoch 4/10
19/19 - 2s - loss: 102.5066 - loglik: -1.0106e+02 - logprior: -1.1849e+00
Epoch 5/10
19/19 - 2s - loss: 101.6905 - loglik: -1.0028e+02 - logprior: -1.1489e+00
Epoch 6/10
19/19 - 2s - loss: 101.9781 - loglik: -1.0059e+02 - logprior: -1.1405e+00
Fitted a model with MAP estimate = -101.2287
Time for alignment: 53.4986
Computed alignments with likelihoods: ['-101.0626', '-100.9547', '-101.2287']
Best model has likelihood: -100.9547
time for generating output: 0.1182
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00313.projection.fasta
SP score = 0.8095975232198143
Training of 3 independent models on file PF00009.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff6a7e21430>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff3847731c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff5176dc490>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6e337e7f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff396482640>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff384235970>, <__main__.SimpleDirichletPrior object at 0x7ff6a795cd30>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff6ca796b80>

Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 568.3280 - loglik: -5.6624e+02 - logprior: -1.9606e+00
Epoch 2/10
39/39 - 18s - loss: 482.3826 - loglik: -4.8009e+02 - logprior: -1.4306e+00
Epoch 3/10
39/39 - 18s - loss: 473.2605 - loglik: -4.7101e+02 - logprior: -1.3509e+00
Epoch 4/10
39/39 - 18s - loss: 471.3159 - loglik: -4.6915e+02 - logprior: -1.3512e+00
Epoch 5/10
39/39 - 18s - loss: 470.4151 - loglik: -4.6831e+02 - logprior: -1.3422e+00
Epoch 6/10
39/39 - 18s - loss: 469.4769 - loglik: -4.6742e+02 - logprior: -1.3453e+00
Epoch 7/10
39/39 - 18s - loss: 469.7527 - loglik: -4.6772e+02 - logprior: -1.3421e+00
Fitted a model with MAP estimate = -466.9178
expansions: [(20, 1), (57, 1), (76, 3), (78, 1), (93, 1), (100, 1), (103, 1), (105, 1), (106, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 472.3474 - loglik: -4.6918e+02 - logprior: -2.2078e+00
Epoch 2/2
39/39 - 19s - loss: 465.1736 - loglik: -4.6287e+02 - logprior: -1.1793e+00
Fitted a model with MAP estimate = -460.9115
expansions: []
discards: [ 0 79]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 472.0648 - loglik: -4.6785e+02 - logprior: -3.0552e+00
Epoch 2/2
39/39 - 19s - loss: 467.1976 - loglik: -4.6460e+02 - logprior: -1.4571e+00
Fitted a model with MAP estimate = -462.2068
expansions: [(0, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 180 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 468.2841 - loglik: -4.6526e+02 - logprior: -1.8371e+00
Epoch 2/10
39/39 - 19s - loss: 464.4013 - loglik: -4.6233e+02 - logprior: -9.2691e-01
Epoch 3/10
39/39 - 19s - loss: 462.4476 - loglik: -4.6054e+02 - logprior: -8.0421e-01
Epoch 4/10
39/39 - 19s - loss: 461.2491 - loglik: -4.5955e+02 - logprior: -7.0706e-01
Epoch 5/10
39/39 - 20s - loss: 460.9736 - loglik: -4.5950e+02 - logprior: -6.1191e-01
Epoch 6/10
39/39 - 19s - loss: 460.2650 - loglik: -4.5894e+02 - logprior: -5.2383e-01
Epoch 7/10
39/39 - 19s - loss: 460.6047 - loglik: -4.5944e+02 - logprior: -4.3147e-01
Fitted a model with MAP estimate = -458.9260
Time for alignment: 434.2768
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 570.1714 - loglik: -5.6806e+02 - logprior: -1.9400e+00
Epoch 2/10
39/39 - 18s - loss: 481.2004 - loglik: -4.7894e+02 - logprior: -1.3999e+00
Epoch 3/10
39/39 - 19s - loss: 471.4710 - loglik: -4.6923e+02 - logprior: -1.3375e+00
Epoch 4/10
39/39 - 19s - loss: 469.3865 - loglik: -4.6725e+02 - logprior: -1.3387e+00
Epoch 5/10
39/39 - 18s - loss: 468.2614 - loglik: -4.6618e+02 - logprior: -1.3337e+00
Epoch 6/10
39/39 - 19s - loss: 467.7119 - loglik: -4.6567e+02 - logprior: -1.3333e+00
Epoch 7/10
39/39 - 19s - loss: 467.6505 - loglik: -4.6564e+02 - logprior: -1.3370e+00
Epoch 8/10
39/39 - 19s - loss: 467.1185 - loglik: -4.6513e+02 - logprior: -1.3366e+00
Epoch 9/10
39/39 - 19s - loss: 466.7065 - loglik: -4.6473e+02 - logprior: -1.3351e+00
Epoch 10/10
39/39 - 18s - loss: 466.5731 - loglik: -4.6459e+02 - logprior: -1.3427e+00
Fitted a model with MAP estimate = -464.2980
expansions: [(26, 1), (37, 3), (58, 1), (78, 2), (80, 1), (81, 1), (82, 2), (83, 1), (101, 1), (103, 1), (137, 3), (138, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 190 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 469.6142 - loglik: -4.6651e+02 - logprior: -2.1850e+00
Epoch 2/2
39/39 - 21s - loss: 461.4963 - loglik: -4.5916e+02 - logprior: -1.2004e+00
Fitted a model with MAP estimate = -456.8295
expansions: []
discards: [ 83  91 152]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 465.7022 - loglik: -4.6248e+02 - logprior: -2.0313e+00
Epoch 2/2
39/39 - 20s - loss: 461.4727 - loglik: -4.5931e+02 - logprior: -9.7601e-01
Fitted a model with MAP estimate = -457.0608
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 186 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 466.2016 - loglik: -4.6220e+02 - logprior: -2.8209e+00
Epoch 2/10
39/39 - 20s - loss: 460.8690 - loglik: -4.5820e+02 - logprior: -1.5100e+00
Epoch 3/10
39/39 - 21s - loss: 458.2545 - loglik: -4.5641e+02 - logprior: -7.2833e-01
Epoch 4/10
39/39 - 21s - loss: 456.8197 - loglik: -4.5516e+02 - logprior: -6.8243e-01
Epoch 5/10
39/39 - 21s - loss: 455.3634 - loglik: -4.5393e+02 - logprior: -5.7233e-01
Epoch 6/10
39/39 - 20s - loss: 456.4544 - loglik: -4.5519e+02 - logprior: -4.9208e-01
Fitted a model with MAP estimate = -454.4803
Time for alignment: 489.5614
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 564.0859 - loglik: -5.6191e+02 - logprior: -1.9751e+00
Epoch 2/10
39/39 - 18s - loss: 473.9352 - loglik: -4.7156e+02 - logprior: -1.3636e+00
Epoch 3/10
39/39 - 19s - loss: 466.8633 - loglik: -4.6455e+02 - logprior: -1.3040e+00
Epoch 4/10
39/39 - 18s - loss: 464.8904 - loglik: -4.6275e+02 - logprior: -1.2821e+00
Epoch 5/10
39/39 - 19s - loss: 463.6549 - loglik: -4.6160e+02 - logprior: -1.2782e+00
Epoch 6/10
39/39 - 18s - loss: 463.3631 - loglik: -4.6136e+02 - logprior: -1.2890e+00
Epoch 7/10
39/39 - 19s - loss: 463.1870 - loglik: -4.6123e+02 - logprior: -1.2898e+00
Epoch 8/10
39/39 - 18s - loss: 463.1653 - loglik: -4.6122e+02 - logprior: -1.2971e+00
Epoch 9/10
39/39 - 18s - loss: 462.6723 - loglik: -4.6074e+02 - logprior: -1.2976e+00
Epoch 10/10
39/39 - 19s - loss: 462.5704 - loglik: -4.6066e+02 - logprior: -1.3007e+00
Fitted a model with MAP estimate = -460.1953
expansions: [(25, 1), (32, 1), (35, 3), (56, 1), (57, 1), (75, 1), (102, 1), (104, 1), (107, 1), (138, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 186 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 467.8231 - loglik: -4.6487e+02 - logprior: -2.0832e+00
Epoch 2/2
39/39 - 21s - loss: 459.9604 - loglik: -4.5776e+02 - logprior: -1.0820e+00
Fitted a model with MAP estimate = -455.6447
expansions: []
discards: [  0  37  38 152 153]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 467.9537 - loglik: -4.6389e+02 - logprior: -2.9001e+00
Epoch 2/2
39/39 - 20s - loss: 462.7703 - loglik: -4.6019e+02 - logprior: -1.4228e+00
Fitted a model with MAP estimate = -457.5474
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 463.4510 - loglik: -4.6041e+02 - logprior: -1.8363e+00
Epoch 2/10
39/39 - 20s - loss: 458.5034 - loglik: -4.5658e+02 - logprior: -7.6852e-01
Epoch 3/10
39/39 - 20s - loss: 457.2354 - loglik: -4.5543e+02 - logprior: -6.7073e-01
Epoch 4/10
39/39 - 20s - loss: 456.2083 - loglik: -4.5464e+02 - logprior: -5.8369e-01
Epoch 5/10
39/39 - 20s - loss: 455.2147 - loglik: -4.5385e+02 - logprior: -4.9405e-01
Epoch 6/10
39/39 - 20s - loss: 454.6505 - loglik: -4.5347e+02 - logprior: -4.0781e-01
Epoch 7/10
39/39 - 20s - loss: 454.1721 - loglik: -4.5312e+02 - logprior: -3.2750e-01
Epoch 8/10
39/39 - 20s - loss: 454.2546 - loglik: -4.5333e+02 - logprior: -2.3333e-01
Fitted a model with MAP estimate = -453.2170
Time for alignment: 523.8746
Computed alignments with likelihoods: ['-458.9260', '-454.4803', '-453.2170']
Best model has likelihood: -453.2170
time for generating output: 0.2369
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00009.projection.fasta
SP score = 0.7831440101636649
Training of 3 independent models on file PF14604.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff52cd49c40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff3605f8b80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6e31e0a00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff2fc7daf10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff31c376f40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff6601a3d30>, <__main__.SimpleDirichletPrior object at 0x7ff6e3a18760>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff2f4661d30>

Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 142.7811 - loglik: -1.3953e+02 - logprior: -3.2429e+00
Epoch 2/10
19/19 - 1s - loss: 113.9032 - loglik: -1.1239e+02 - logprior: -1.4556e+00
Epoch 3/10
19/19 - 1s - loss: 104.7058 - loglik: -1.0283e+02 - logprior: -1.6260e+00
Epoch 4/10
19/19 - 1s - loss: 102.3070 - loglik: -1.0059e+02 - logprior: -1.4869e+00
Epoch 5/10
19/19 - 1s - loss: 101.6044 - loglik: -9.9941e+01 - logprior: -1.4786e+00
Epoch 6/10
19/19 - 1s - loss: 101.3815 - loglik: -9.9743e+01 - logprior: -1.4598e+00
Epoch 7/10
19/19 - 1s - loss: 101.1930 - loglik: -9.9559e+01 - logprior: -1.4428e+00
Epoch 8/10
19/19 - 1s - loss: 100.9720 - loglik: -9.9336e+01 - logprior: -1.4365e+00
Epoch 9/10
19/19 - 1s - loss: 100.7898 - loglik: -9.9118e+01 - logprior: -1.4361e+00
Epoch 10/10
19/19 - 1s - loss: 100.5895 - loglik: -9.8903e+01 - logprior: -1.4350e+00
Fitted a model with MAP estimate = -100.2233
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (20, 2), (21, 3), (28, 2), (29, 2), (31, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 108.2629 - loglik: -1.0372e+02 - logprior: -4.2411e+00
Epoch 2/2
19/19 - 1s - loss: 99.9533 - loglik: -9.7324e+01 - logprior: -2.2695e+00
Fitted a model with MAP estimate = -97.8753
expansions: [(0, 1)]
discards: [ 0  8 17 26 29 39 41 45]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.3309 - loglik: -9.7748e+01 - logprior: -3.2582e+00
Epoch 2/2
19/19 - 1s - loss: 97.1414 - loglik: -9.5346e+01 - logprior: -1.5140e+00
Fitted a model with MAP estimate = -96.3120
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 99.5725 - loglik: -9.5973e+01 - logprior: -3.3072e+00
Epoch 2/10
19/19 - 1s - loss: 96.8468 - loglik: -9.5089e+01 - logprior: -1.4880e+00
Epoch 3/10
19/19 - 1s - loss: 96.4684 - loglik: -9.4775e+01 - logprior: -1.4081e+00
Epoch 4/10
19/19 - 1s - loss: 96.1422 - loglik: -9.4510e+01 - logprior: -1.3595e+00
Epoch 5/10
19/19 - 1s - loss: 95.8007 - loglik: -9.4220e+01 - logprior: -1.3251e+00
Epoch 6/10
19/19 - 1s - loss: 95.7573 - loglik: -9.4201e+01 - logprior: -1.3101e+00
Epoch 7/10
19/19 - 1s - loss: 95.6267 - loglik: -9.4093e+01 - logprior: -1.2914e+00
Epoch 8/10
19/19 - 1s - loss: 95.4481 - loglik: -9.3928e+01 - logprior: -1.2774e+00
Epoch 9/10
19/19 - 1s - loss: 95.4613 - loglik: -9.3945e+01 - logprior: -1.2678e+00
Fitted a model with MAP estimate = -95.1165
Time for alignment: 54.6688
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.7055 - loglik: -1.3946e+02 - logprior: -3.2386e+00
Epoch 2/10
19/19 - 1s - loss: 114.1125 - loglik: -1.1260e+02 - logprior: -1.4530e+00
Epoch 3/10
19/19 - 1s - loss: 103.8913 - loglik: -1.0198e+02 - logprior: -1.6406e+00
Epoch 4/10
19/19 - 1s - loss: 101.9317 - loglik: -1.0016e+02 - logprior: -1.5127e+00
Epoch 5/10
19/19 - 1s - loss: 100.9986 - loglik: -9.9270e+01 - logprior: -1.5089e+00
Epoch 6/10
19/19 - 1s - loss: 100.7611 - loglik: -9.9048e+01 - logprior: -1.4904e+00
Epoch 7/10
19/19 - 1s - loss: 100.5414 - loglik: -9.8821e+01 - logprior: -1.4779e+00
Epoch 8/10
19/19 - 1s - loss: 100.3304 - loglik: -9.8607e+01 - logprior: -1.4706e+00
Epoch 9/10
19/19 - 1s - loss: 100.4112 - loglik: -9.8679e+01 - logprior: -1.4660e+00
Fitted a model with MAP estimate = -99.9307
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (27, 2), (28, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.9149 - loglik: -1.0336e+02 - logprior: -4.2330e+00
Epoch 2/2
19/19 - 1s - loss: 99.7945 - loglik: -9.7232e+01 - logprior: -2.2167e+00
Fitted a model with MAP estimate = -97.7072
expansions: [(0, 1)]
discards: [ 0  8 17 26 27 39 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.2305 - loglik: -9.7684e+01 - logprior: -3.2527e+00
Epoch 2/2
19/19 - 1s - loss: 97.1006 - loglik: -9.5317e+01 - logprior: -1.5149e+00
Fitted a model with MAP estimate = -96.2589
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.4895 - loglik: -9.5914e+01 - logprior: -3.3036e+00
Epoch 2/10
19/19 - 1s - loss: 96.8794 - loglik: -9.5138e+01 - logprior: -1.4992e+00
Epoch 3/10
19/19 - 1s - loss: 96.3140 - loglik: -9.4631e+01 - logprior: -1.4110e+00
Epoch 4/10
19/19 - 1s - loss: 95.9429 - loglik: -9.4334e+01 - logprior: -1.3576e+00
Epoch 5/10
19/19 - 1s - loss: 95.8941 - loglik: -9.4314e+01 - logprior: -1.3277e+00
Epoch 6/10
19/19 - 1s - loss: 95.7245 - loglik: -9.4181e+01 - logprior: -1.3096e+00
Epoch 7/10
19/19 - 1s - loss: 95.5392 - loglik: -9.4008e+01 - logprior: -1.2939e+00
Epoch 8/10
19/19 - 1s - loss: 95.3469 - loglik: -9.3834e+01 - logprior: -1.2791e+00
Epoch 9/10
19/19 - 1s - loss: 95.5796 - loglik: -9.4075e+01 - logprior: -1.2649e+00
Fitted a model with MAP estimate = -95.0917
Time for alignment: 51.4247
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 142.7170 - loglik: -1.3947e+02 - logprior: -3.2403e+00
Epoch 2/10
19/19 - 1s - loss: 114.0584 - loglik: -1.1254e+02 - logprior: -1.4517e+00
Epoch 3/10
19/19 - 1s - loss: 103.9718 - loglik: -1.0206e+02 - logprior: -1.6449e+00
Epoch 4/10
19/19 - 1s - loss: 101.7340 - loglik: -9.9969e+01 - logprior: -1.5162e+00
Epoch 5/10
19/19 - 1s - loss: 101.0299 - loglik: -9.9310e+01 - logprior: -1.5085e+00
Epoch 6/10
19/19 - 1s - loss: 100.8369 - loglik: -9.9123e+01 - logprior: -1.4919e+00
Epoch 7/10
19/19 - 1s - loss: 100.3956 - loglik: -9.8679e+01 - logprior: -1.4762e+00
Epoch 8/10
19/19 - 1s - loss: 100.3139 - loglik: -9.8588e+01 - logprior: -1.4693e+00
Epoch 9/10
19/19 - 1s - loss: 100.3231 - loglik: -9.8590e+01 - logprior: -1.4663e+00
Fitted a model with MAP estimate = -99.9230
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (27, 2), (28, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.9047 - loglik: -1.0335e+02 - logprior: -4.2267e+00
Epoch 2/2
19/19 - 1s - loss: 99.7624 - loglik: -9.7207e+01 - logprior: -2.2115e+00
Fitted a model with MAP estimate = -97.6781
expansions: [(0, 1)]
discards: [ 0  8 17 26 27 39 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 101.2019 - loglik: -9.7657e+01 - logprior: -3.2524e+00
Epoch 2/2
19/19 - 1s - loss: 97.1245 - loglik: -9.5350e+01 - logprior: -1.5105e+00
Fitted a model with MAP estimate = -96.2696
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.5181 - loglik: -9.5948e+01 - logprior: -3.3061e+00
Epoch 2/10
19/19 - 1s - loss: 96.7947 - loglik: -9.5054e+01 - logprior: -1.4952e+00
Epoch 3/10
19/19 - 1s - loss: 96.3847 - loglik: -9.4705e+01 - logprior: -1.4093e+00
Epoch 4/10
19/19 - 1s - loss: 96.0204 - loglik: -9.4403e+01 - logprior: -1.3610e+00
Epoch 5/10
19/19 - 1s - loss: 95.8504 - loglik: -9.4278e+01 - logprior: -1.3237e+00
Epoch 6/10
19/19 - 1s - loss: 95.7018 - loglik: -9.4154e+01 - logprior: -1.3122e+00
Epoch 7/10
19/19 - 1s - loss: 95.6780 - loglik: -9.4154e+01 - logprior: -1.2895e+00
Epoch 8/10
19/19 - 1s - loss: 95.3129 - loglik: -9.3798e+01 - logprior: -1.2800e+00
Epoch 9/10
19/19 - 1s - loss: 95.4430 - loglik: -9.3943e+01 - logprior: -1.2621e+00
Fitted a model with MAP estimate = -95.0836
Time for alignment: 52.0657
Computed alignments with likelihoods: ['-95.1165', '-95.0917', '-95.0836']
Best model has likelihood: -95.0836
time for generating output: 0.1128
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF14604.projection.fasta
SP score = 0.7560617193240264
Training of 3 independent models on file PF00625.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff51a663070>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff3962f6790>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff696cad610>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6b0c79670>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6b0c79f70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff51aa4b130>, <__main__.SimpleDirichletPrior object at 0x7ff6603eae80>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff2f4661d30>

Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 457.3928 - loglik: -4.5442e+02 - logprior: -2.9443e+00
Epoch 2/10
19/19 - 6s - loss: 380.7353 - loglik: -3.7912e+02 - logprior: -1.4975e+00
Epoch 3/10
19/19 - 7s - loss: 348.5147 - loglik: -3.4599e+02 - logprior: -1.9509e+00
Epoch 4/10
19/19 - 7s - loss: 341.3372 - loglik: -3.3878e+02 - logprior: -2.0596e+00
Epoch 5/10
19/19 - 7s - loss: 340.3995 - loglik: -3.3795e+02 - logprior: -1.9605e+00
Epoch 6/10
19/19 - 7s - loss: 338.6348 - loglik: -3.3623e+02 - logprior: -1.9416e+00
Epoch 7/10
19/19 - 7s - loss: 337.6880 - loglik: -3.3527e+02 - logprior: -1.9534e+00
Epoch 8/10
19/19 - 7s - loss: 337.7219 - loglik: -3.3532e+02 - logprior: -1.9454e+00
Fitted a model with MAP estimate = -333.5386
expansions: [(0, 2), (14, 1), (15, 3), (16, 1), (17, 1), (18, 1), (22, 1), (36, 1), (43, 2), (44, 1), (48, 1), (50, 1), (51, 1), (57, 1), (66, 4), (68, 1), (71, 1), (80, 1), (87, 2), (88, 1), (89, 1), (99, 1), (100, 1), (111, 1), (113, 1), (114, 1), (115, 1), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 324.8551 - loglik: -3.2161e+02 - logprior: -2.5677e+00
Epoch 2/2
39/39 - 11s - loss: 312.0844 - loglik: -3.1025e+02 - logprior: -1.1720e+00
Fitted a model with MAP estimate = -306.4060
expansions: []
discards: [ 0 55 85]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 317.8954 - loglik: -3.1419e+02 - logprior: -2.9866e+00
Epoch 2/2
39/39 - 11s - loss: 311.8456 - loglik: -3.1006e+02 - logprior: -1.1180e+00
Fitted a model with MAP estimate = -306.4329
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 312.5033 - loglik: -3.0985e+02 - logprior: -1.9057e+00
Epoch 2/10
39/39 - 11s - loss: 308.3464 - loglik: -3.0687e+02 - logprior: -8.0560e-01
Epoch 3/10
39/39 - 11s - loss: 306.0826 - loglik: -3.0461e+02 - logprior: -7.2231e-01
Epoch 4/10
39/39 - 11s - loss: 305.2829 - loglik: -3.0394e+02 - logprior: -6.5444e-01
Epoch 5/10
39/39 - 11s - loss: 303.9041 - loglik: -3.0270e+02 - logprior: -5.8417e-01
Epoch 6/10
39/39 - 11s - loss: 304.4846 - loglik: -3.0341e+02 - logprior: -5.1732e-01
Fitted a model with MAP estimate = -302.8214
Time for alignment: 234.3415
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 457.8417 - loglik: -4.5485e+02 - logprior: -2.9610e+00
Epoch 2/10
19/19 - 7s - loss: 379.9006 - loglik: -3.7819e+02 - logprior: -1.5107e+00
Epoch 3/10
19/19 - 7s - loss: 347.7340 - loglik: -3.4516e+02 - logprior: -1.9059e+00
Epoch 4/10
19/19 - 7s - loss: 341.2705 - loglik: -3.3871e+02 - logprior: -2.0015e+00
Epoch 5/10
19/19 - 7s - loss: 339.3571 - loglik: -3.3686e+02 - logprior: -1.9590e+00
Epoch 6/10
19/19 - 7s - loss: 337.2994 - loglik: -3.3487e+02 - logprior: -1.9338e+00
Epoch 7/10
19/19 - 7s - loss: 337.1047 - loglik: -3.3470e+02 - logprior: -1.9272e+00
Epoch 8/10
19/19 - 7s - loss: 335.6439 - loglik: -3.3325e+02 - logprior: -1.9255e+00
Epoch 9/10
19/19 - 7s - loss: 335.9319 - loglik: -3.3355e+02 - logprior: -1.9162e+00
Fitted a model with MAP estimate = -332.1969
expansions: [(0, 2), (14, 1), (15, 2), (16, 1), (17, 1), (18, 1), (22, 1), (30, 1), (38, 1), (44, 1), (48, 1), (49, 1), (50, 1), (51, 1), (58, 1), (65, 1), (66, 1), (68, 1), (71, 1), (80, 1), (87, 2), (88, 1), (89, 1), (99, 1), (100, 1), (111, 1), (113, 1), (114, 2), (115, 1), (116, 1), (122, 1), (123, 1), (135, 1), (136, 1), (137, 1), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 325.6329 - loglik: -3.2241e+02 - logprior: -2.5077e+00
Epoch 2/2
39/39 - 11s - loss: 312.6342 - loglik: -3.1083e+02 - logprior: -1.1012e+00
Fitted a model with MAP estimate = -306.8882
expansions: [(20, 1)]
discards: [  0 111]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 317.9217 - loglik: -3.1428e+02 - logprior: -2.9187e+00
Epoch 2/2
39/39 - 11s - loss: 311.8953 - loglik: -3.1016e+02 - logprior: -1.0469e+00
Fitted a model with MAP estimate = -306.3502
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 312.3748 - loglik: -3.0978e+02 - logprior: -1.8700e+00
Epoch 2/10
39/39 - 11s - loss: 308.4491 - loglik: -3.0703e+02 - logprior: -7.6834e-01
Epoch 3/10
39/39 - 11s - loss: 306.5098 - loglik: -3.0510e+02 - logprior: -6.9918e-01
Epoch 4/10
39/39 - 11s - loss: 305.0052 - loglik: -3.0374e+02 - logprior: -6.1139e-01
Epoch 5/10
39/39 - 11s - loss: 304.7717 - loglik: -3.0362e+02 - logprior: -5.5098e-01
Epoch 6/10
39/39 - 11s - loss: 304.1518 - loglik: -3.0314e+02 - logprior: -4.7056e-01
Epoch 7/10
39/39 - 11s - loss: 304.3227 - loglik: -3.0343e+02 - logprior: -3.9008e-01
Fitted a model with MAP estimate = -302.8260
Time for alignment: 249.7531
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 458.0402 - loglik: -4.5506e+02 - logprior: -2.9469e+00
Epoch 2/10
19/19 - 6s - loss: 383.3640 - loglik: -3.8176e+02 - logprior: -1.4743e+00
Epoch 3/10
19/19 - 7s - loss: 354.1871 - loglik: -3.5169e+02 - logprior: -1.8869e+00
Epoch 4/10
19/19 - 7s - loss: 343.9766 - loglik: -3.4129e+02 - logprior: -2.0441e+00
Epoch 5/10
19/19 - 7s - loss: 340.6105 - loglik: -3.3793e+02 - logprior: -2.0478e+00
Epoch 6/10
19/19 - 7s - loss: 339.3514 - loglik: -3.3679e+02 - logprior: -2.0021e+00
Epoch 7/10
19/19 - 7s - loss: 338.6072 - loglik: -3.3610e+02 - logprior: -1.9839e+00
Epoch 8/10
19/19 - 7s - loss: 337.0232 - loglik: -3.3454e+02 - logprior: -1.9830e+00
Epoch 9/10
19/19 - 7s - loss: 337.9400 - loglik: -3.3548e+02 - logprior: -1.9870e+00
Fitted a model with MAP estimate = -333.7542
expansions: [(0, 2), (11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (20, 1), (22, 1), (23, 1), (43, 2), (44, 1), (48, 1), (49, 1), (50, 1), (51, 1), (58, 1), (65, 1), (66, 1), (68, 1), (80, 1), (87, 2), (88, 1), (89, 1), (91, 1), (111, 1), (113, 1), (114, 1), (115, 2), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 328.2545 - loglik: -3.2494e+02 - logprior: -2.5891e+00
Epoch 2/2
39/39 - 11s - loss: 315.7775 - loglik: -3.1391e+02 - logprior: -1.1594e+00
Fitted a model with MAP estimate = -310.2800
expansions: []
discards: [  0 112 146]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 322.1923 - loglik: -3.1847e+02 - logprior: -3.0078e+00
Epoch 2/2
39/39 - 11s - loss: 316.5042 - loglik: -3.1473e+02 - logprior: -1.1143e+00
Fitted a model with MAP estimate = -310.6803
expansions: [(92, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 315.9181 - loglik: -3.1327e+02 - logprior: -1.9217e+00
Epoch 2/10
39/39 - 11s - loss: 311.9112 - loglik: -3.1043e+02 - logprior: -8.2199e-01
Epoch 3/10
39/39 - 11s - loss: 310.0471 - loglik: -3.0860e+02 - logprior: -7.3550e-01
Epoch 4/10
39/39 - 11s - loss: 308.7202 - loglik: -3.0741e+02 - logprior: -6.6035e-01
Epoch 5/10
39/39 - 11s - loss: 307.9638 - loglik: -3.0679e+02 - logprior: -5.8011e-01
Epoch 6/10
39/39 - 11s - loss: 308.0781 - loglik: -3.0706e+02 - logprior: -4.8819e-01
Fitted a model with MAP estimate = -306.4423
Time for alignment: 235.5992
Computed alignments with likelihoods: ['-302.8214', '-302.8260', '-306.4423']
Best model has likelihood: -302.8214
time for generating output: 0.3777
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00625.projection.fasta
SP score = 0.3425896359541068
Training of 3 independent models on file PF00476.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff6a7df2fd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff51aa39610>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff51ab855e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff517fffa90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff39627eb50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff6603c78e0>, <__main__.SimpleDirichletPrior object at 0x7ff2fdbdefd0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff31c15c160>

Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 54s - loss: 904.5523 - loglik: -9.0275e+02 - logprior: -1.6286e+00
Epoch 2/10
39/39 - 52s - loss: 716.9670 - loglik: -7.1331e+02 - logprior: -2.2637e+00
Epoch 3/10
39/39 - 54s - loss: 699.5902 - loglik: -6.9562e+02 - logprior: -2.3803e+00
Epoch 4/10
39/39 - 55s - loss: 694.6400 - loglik: -6.9080e+02 - logprior: -2.3941e+00
Epoch 5/10
39/39 - 55s - loss: 692.8160 - loglik: -6.8915e+02 - logprior: -2.3915e+00
Epoch 6/10
39/39 - 51s - loss: 691.4810 - loglik: -6.8787e+02 - logprior: -2.4517e+00
Epoch 7/10
39/39 - 52s - loss: 690.3735 - loglik: -6.8687e+02 - logprior: -2.4655e+00
Epoch 8/10
39/39 - 52s - loss: 690.2529 - loglik: -6.8679e+02 - logprior: -2.5025e+00
Epoch 9/10
39/39 - 50s - loss: 689.2267 - loglik: -6.8588e+02 - logprior: -2.4680e+00
Epoch 10/10
39/39 - 52s - loss: 689.4300 - loglik: -6.8601e+02 - logprior: -2.5714e+00
Fitted a model with MAP estimate = -687.7235
expansions: [(0, 4), (36, 1), (46, 1), (55, 1), (60, 1), (62, 2), (66, 1), (72, 1), (73, 1), (81, 1), (83, 2), (84, 3), (90, 1), (91, 1), (92, 1), (112, 1), (113, 1), (118, 1), (122, 2), (125, 2), (127, 1), (142, 1), (144, 1), (146, 1), (149, 1), (155, 1), (158, 1), (161, 1), (162, 1), (163, 1), (165, 1), (166, 1), (168, 1), (186, 1), (188, 2), (189, 1), (194, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (231, 8), (233, 1), (256, 1), (257, 4), (258, 1), (259, 1), (260, 1), (261, 1), (279, 2), (280, 1), (281, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 388 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 86s - loss: 678.0247 - loglik: -6.7394e+02 - logprior: -2.9244e+00
Epoch 2/2
39/39 - 90s - loss: 651.5356 - loglik: -6.4850e+02 - logprior: -1.4144e+00
Fitted a model with MAP estimate = -644.5472
expansions: []
discards: [  0 100 147 152 231 287 288 289 323 351 367 368]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 81s - loss: 661.8612 - loglik: -6.5723e+02 - logprior: -2.8435e+00
Epoch 2/2
39/39 - 75s - loss: 652.3038 - loglik: -6.4918e+02 - logprior: -1.0178e+00
Fitted a model with MAP estimate = -645.8537
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 78s - loss: 658.4711 - loglik: -6.5455e+02 - logprior: -1.7912e+00
Epoch 2/10
39/39 - 73s - loss: 651.0397 - loglik: -6.4821e+02 - logprior: -6.1653e-01
Epoch 3/10
39/39 - 77s - loss: 646.7845 - loglik: -6.4433e+02 - logprior: -4.2141e-01
Epoch 4/10
39/39 - 83s - loss: 643.9509 - loglik: -6.4180e+02 - logprior: -3.0132e-01
Epoch 5/10
39/39 - 68s - loss: 641.5677 - loglik: -6.3990e+02 - logprior: -7.6683e-02
Epoch 6/10
39/39 - 63s - loss: 640.7534 - loglik: -6.3948e+02 - logprior: 0.1311
Epoch 7/10
39/39 - 61s - loss: 640.0942 - loglik: -6.3920e+02 - logprior: 0.3521
Epoch 8/10
39/39 - 60s - loss: 639.3607 - loglik: -6.3872e+02 - logprior: 0.4984
Epoch 9/10
39/39 - 60s - loss: 638.8280 - loglik: -6.3846e+02 - logprior: 0.6729
Epoch 10/10
39/39 - 61s - loss: 637.5771 - loglik: -6.3755e+02 - logprior: 0.9540
Fitted a model with MAP estimate = -636.5477
Time for alignment: 1900.7996
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 903.7705 - loglik: -9.0198e+02 - logprior: -1.6275e+00
Epoch 2/10
39/39 - 43s - loss: 715.4937 - loglik: -7.1221e+02 - logprior: -2.3554e+00
Epoch 3/10
39/39 - 43s - loss: 699.4315 - loglik: -6.9572e+02 - logprior: -2.4557e+00
Epoch 4/10
39/39 - 43s - loss: 694.9318 - loglik: -6.9131e+02 - logprior: -2.3715e+00
Epoch 5/10
39/39 - 43s - loss: 693.3857 - loglik: -6.8969e+02 - logprior: -2.4753e+00
Epoch 6/10
39/39 - 44s - loss: 691.8444 - loglik: -6.8829e+02 - logprior: -2.4187e+00
Epoch 7/10
39/39 - 46s - loss: 691.2614 - loglik: -6.8774e+02 - logprior: -2.4883e+00
Epoch 8/10
39/39 - 48s - loss: 691.0202 - loglik: -6.8761e+02 - logprior: -2.4604e+00
Epoch 9/10
39/39 - 51s - loss: 690.4052 - loglik: -6.8708e+02 - logprior: -2.4453e+00
Epoch 10/10
39/39 - 53s - loss: 690.2753 - loglik: -6.8706e+02 - logprior: -2.3753e+00
Fitted a model with MAP estimate = -688.3744
expansions: [(0, 4), (36, 1), (46, 1), (61, 1), (63, 2), (67, 1), (73, 1), (74, 1), (80, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (93, 1), (103, 1), (112, 1), (113, 1), (118, 1), (122, 2), (125, 2), (127, 1), (142, 1), (144, 1), (149, 1), (151, 1), (155, 1), (158, 1), (160, 1), (162, 1), (163, 1), (165, 1), (166, 1), (180, 1), (186, 1), (188, 2), (189, 1), (190, 1), (193, 1), (205, 2), (207, 2), (208, 1), (209, 1), (227, 1), (228, 1), (229, 1), (230, 3), (232, 1), (255, 1), (256, 1), (257, 1), (258, 2), (259, 2), (260, 3), (278, 2), (279, 1), (281, 1), (284, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 386 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 83s - loss: 681.0436 - loglik: -6.7709e+02 - logprior: -2.8325e+00
Epoch 2/2
39/39 - 79s - loss: 652.6060 - loglik: -6.4954e+02 - logprior: -1.5109e+00
Fitted a model with MAP estimate = -645.4669
expansions: []
discards: [  0 147 152 231 323 325 350 365 366]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 79s - loss: 660.7676 - loglik: -6.5657e+02 - logprior: -2.4973e+00
Epoch 2/2
39/39 - 71s - loss: 652.1719 - loglik: -6.4925e+02 - logprior: -9.1913e-01
Fitted a model with MAP estimate = -645.7382
expansions: []
discards: [100]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 89s - loss: 658.8414 - loglik: -6.5510e+02 - logprior: -1.7441e+00
Epoch 2/10
39/39 - 86s - loss: 651.1827 - loglik: -6.4864e+02 - logprior: -4.6800e-01
Epoch 3/10
39/39 - 86s - loss: 647.7132 - loglik: -6.4526e+02 - logprior: -4.9970e-01
Epoch 4/10
39/39 - 85s - loss: 644.0966 - loglik: -6.4205e+02 - logprior: -2.8335e-01
Epoch 5/10
39/39 - 86s - loss: 642.7559 - loglik: -6.4115e+02 - logprior: -4.7963e-02
Epoch 6/10
39/39 - 79s - loss: 641.4648 - loglik: -6.4019e+02 - logprior: 0.1000
Epoch 7/10
39/39 - 80s - loss: 640.2974 - loglik: -6.3935e+02 - logprior: 0.3005
Epoch 8/10
39/39 - 82s - loss: 639.4220 - loglik: -6.3873e+02 - logprior: 0.4253
Epoch 9/10
39/39 - 82s - loss: 639.3152 - loglik: -6.3895e+02 - logprior: 0.6852
Epoch 10/10
39/39 - 79s - loss: 639.2043 - loglik: -6.3907e+02 - logprior: 0.8432
Fitted a model with MAP estimate = -637.1468
Time for alignment: 1965.2269
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 54s - loss: 904.3117 - loglik: -9.0245e+02 - logprior: -1.6235e+00
Epoch 2/10
39/39 - 57s - loss: 717.7643 - loglik: -7.1382e+02 - logprior: -2.2630e+00
Epoch 3/10
39/39 - 52s - loss: 700.1182 - loglik: -6.9608e+02 - logprior: -2.3807e+00
Epoch 4/10
39/39 - 49s - loss: 695.4955 - loglik: -6.9157e+02 - logprior: -2.4216e+00
Epoch 5/10
39/39 - 53s - loss: 692.8394 - loglik: -6.8899e+02 - logprior: -2.5473e+00
Epoch 6/10
39/39 - 57s - loss: 692.0568 - loglik: -6.8840e+02 - logprior: -2.4961e+00
Epoch 7/10
39/39 - 51s - loss: 691.0823 - loglik: -6.8763e+02 - logprior: -2.4229e+00
Epoch 8/10
39/39 - 49s - loss: 691.1936 - loglik: -6.8783e+02 - logprior: -2.4025e+00
Fitted a model with MAP estimate = -688.8856
expansions: [(0, 4), (36, 1), (42, 1), (45, 1), (59, 1), (62, 2), (66, 1), (72, 1), (73, 1), (79, 1), (80, 1), (82, 2), (83, 1), (84, 1), (90, 1), (91, 1), (102, 1), (112, 1), (113, 1), (118, 1), (122, 2), (125, 2), (127, 1), (142, 1), (144, 1), (146, 1), (149, 1), (155, 1), (158, 1), (160, 1), (162, 1), (163, 1), (165, 1), (166, 1), (171, 1), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (227, 1), (228, 1), (231, 3), (257, 4), (259, 2), (260, 2), (261, 3), (279, 2), (280, 1), (281, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 386 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 93s - loss: 679.6862 - loglik: -6.7550e+02 - logprior: -2.9004e+00
Epoch 2/2
39/39 - 92s - loss: 652.5809 - loglik: -6.4931e+02 - logprior: -1.5454e+00
Fitted a model with MAP estimate = -645.3807
expansions: []
discards: [  0 147 152 231 322 323 349 365 366]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 83s - loss: 661.2816 - loglik: -6.5671e+02 - logprior: -2.7502e+00
Epoch 2/2
39/39 - 81s - loss: 651.7497 - loglik: -6.4880e+02 - logprior: -8.6223e-01
Fitted a model with MAP estimate = -645.6114
expansions: []
discards: [100]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 86s - loss: 658.9114 - loglik: -6.5494e+02 - logprior: -1.8752e+00
Epoch 2/10
39/39 - 82s - loss: 651.6405 - loglik: -6.4894e+02 - logprior: -5.4344e-01
Epoch 3/10
39/39 - 76s - loss: 647.1823 - loglik: -6.4484e+02 - logprior: -3.5066e-01
Epoch 4/10
39/39 - 79s - loss: 643.9479 - loglik: -6.4195e+02 - logprior: -1.6047e-01
Epoch 5/10
39/39 - 78s - loss: 642.7518 - loglik: -6.4120e+02 - logprior: 0.0337
Epoch 6/10
39/39 - 72s - loss: 641.5275 - loglik: -6.4037e+02 - logprior: 0.2288
Epoch 7/10
39/39 - 81s - loss: 640.8387 - loglik: -6.3971e+02 - logprior: 0.1156
Epoch 8/10
39/39 - 73s - loss: 639.3403 - loglik: -6.3855e+02 - logprior: 0.3331
Epoch 9/10
39/39 - 76s - loss: 639.9760 - loglik: -6.3949e+02 - logprior: 0.5478
Fitted a model with MAP estimate = -637.6959
Time for alignment: 1869.9559
Computed alignments with likelihoods: ['-636.5477', '-637.1468', '-637.6959']
Best model has likelihood: -636.5477
time for generating output: 0.4100
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00476.projection.fasta
SP score = 0.9438522107813446
Training of 3 independent models on file PF02836.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff3963e2eb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff2f5efc910>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2f5efc340>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff2f47f3220>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2f47f3c10>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff51a052fd0>, <__main__.SimpleDirichletPrior object at 0x7ff2fc315460>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff31c15c160>

Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 645.2574 - loglik: -6.4304e+02 - logprior: -1.9792e+00
Epoch 2/10
39/39 - 22s - loss: 551.8571 - loglik: -5.4921e+02 - logprior: -1.7188e+00
Epoch 3/10
39/39 - 23s - loss: 541.3278 - loglik: -5.3811e+02 - logprior: -1.8604e+00
Epoch 4/10
39/39 - 22s - loss: 538.4080 - loglik: -5.3515e+02 - logprior: -1.9208e+00
Epoch 5/10
39/39 - 23s - loss: 536.3369 - loglik: -5.3310e+02 - logprior: -1.9482e+00
Epoch 6/10
39/39 - 22s - loss: 535.3046 - loglik: -5.3212e+02 - logprior: -1.9749e+00
Epoch 7/10
39/39 - 23s - loss: 534.4777 - loglik: -5.3129e+02 - logprior: -2.0054e+00
Epoch 8/10
39/39 - 23s - loss: 534.2336 - loglik: -5.3106e+02 - logprior: -2.0095e+00
Epoch 9/10
39/39 - 22s - loss: 533.6066 - loglik: -5.3044e+02 - logprior: -2.0229e+00
Epoch 10/10
39/39 - 23s - loss: 532.8339 - loglik: -5.2967e+02 - logprior: -2.0411e+00
Fitted a model with MAP estimate = -531.1906
expansions: [(4, 1), (6, 1), (33, 1), (83, 7), (121, 1), (122, 5), (123, 10), (130, 1), (131, 1), (133, 8), (136, 1), (137, 1), (139, 1), (142, 2), (143, 9), (144, 1), (146, 1), (147, 1), (152, 1), (153, 1), (157, 1), (160, 5), (161, 1)]
discards: [  0 162 163 164 165 166 167 168 169 170 174 175 176 177 178 179 180 181
 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199
 200 201 202 203 204 205 206 207 208 209 210 211]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 226 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 564.4739 - loglik: -5.5989e+02 - logprior: -2.9740e+00
Epoch 2/2
39/39 - 25s - loss: 546.9304 - loglik: -5.4330e+02 - logprior: -1.4081e+00
Fitted a model with MAP estimate = -540.7501
expansions: [(215, 1), (223, 1), (226, 34)]
discards: [  0  84 133 134 135 136 138 162 163 192]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 252 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 32s - loss: 541.3152 - loglik: -5.3580e+02 - logprior: -2.9168e+00
Epoch 2/2
39/39 - 29s - loss: 526.7758 - loglik: -5.2316e+02 - logprior: -1.3742e+00
Fitted a model with MAP estimate = -520.7813
expansions: [(0, 2), (230, 2), (252, 3)]
discards: [  0  80  81  82  83 154 178 182 225 226 227 228 234 235 236 237 238 239
 240 241 242 243 244 245 246 247 248 249 250 251]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 229 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 541.5506 - loglik: -5.3687e+02 - logprior: -1.9341e+00
Epoch 2/10
39/39 - 25s - loss: 534.9162 - loglik: -5.3186e+02 - logprior: -6.9758e-01
Epoch 3/10
39/39 - 26s - loss: 532.0779 - loglik: -5.2930e+02 - logprior: -5.6052e-01
Epoch 4/10
39/39 - 25s - loss: 530.2733 - loglik: -5.2776e+02 - logprior: -4.9606e-01
Epoch 5/10
39/39 - 25s - loss: 527.9384 - loglik: -5.2564e+02 - logprior: -4.3664e-01
Epoch 6/10
39/39 - 23s - loss: 527.3316 - loglik: -5.2523e+02 - logprior: -3.6770e-01
Epoch 7/10
39/39 - 22s - loss: 526.2194 - loglik: -5.2425e+02 - logprior: -2.8755e-01
Epoch 8/10
39/39 - 22s - loss: 525.5090 - loglik: -5.2365e+02 - logprior: -2.1611e-01
Epoch 9/10
39/39 - 21s - loss: 524.1795 - loglik: -5.2245e+02 - logprior: -1.5004e-01
Epoch 10/10
39/39 - 21s - loss: 523.7650 - loglik: -5.2218e+02 - logprior: -7.2236e-02
Fitted a model with MAP estimate = -521.5812
Time for alignment: 701.7330
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 647.5891 - loglik: -6.4534e+02 - logprior: -1.9980e+00
Epoch 2/10
39/39 - 19s - loss: 553.0206 - loglik: -5.4987e+02 - logprior: -1.9028e+00
Epoch 3/10
39/39 - 19s - loss: 542.3900 - loglik: -5.3879e+02 - logprior: -2.0953e+00
Epoch 4/10
39/39 - 19s - loss: 539.1293 - loglik: -5.3546e+02 - logprior: -2.1440e+00
Epoch 5/10
39/39 - 19s - loss: 537.8351 - loglik: -5.3423e+02 - logprior: -2.1735e+00
Epoch 6/10
39/39 - 19s - loss: 536.5950 - loglik: -5.3306e+02 - logprior: -2.1894e+00
Epoch 7/10
39/39 - 19s - loss: 535.8676 - loglik: -5.3240e+02 - logprior: -2.2099e+00
Epoch 8/10
39/39 - 19s - loss: 535.3878 - loglik: -5.3195e+02 - logprior: -2.2288e+00
Epoch 9/10
39/39 - 19s - loss: 534.6528 - loglik: -5.3125e+02 - logprior: -2.2542e+00
Epoch 10/10
39/39 - 19s - loss: 534.3634 - loglik: -5.3097e+02 - logprior: -2.2748e+00
Fitted a model with MAP estimate = -532.3995
expansions: [(4, 1), (6, 1), (33, 1), (40, 1), (44, 1), (81, 6), (83, 1), (88, 1), (89, 1), (90, 1), (114, 1), (115, 6), (116, 1), (117, 2), (118, 1), (119, 1), (127, 1), (130, 8), (135, 1), (138, 1), (140, 2), (141, 9), (142, 1), (145, 1), (146, 1), (151, 2), (154, 1), (158, 3), (160, 1), (172, 5), (174, 1), (183, 1)]
discards: [  0 148 161 162 163 164 165 166 167 168 169 170 175 176 177 178 179 180
 181 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203
 204 205 206 207 208 209 210 211]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 234 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 565.2675 - loglik: -5.6062e+02 - logprior: -3.0307e+00
Epoch 2/2
39/39 - 21s - loss: 543.8783 - loglik: -5.4010e+02 - logprior: -1.5592e+00
Fitted a model with MAP estimate = -536.7434
expansions: [(95, 1), (214, 1), (222, 1), (234, 10)]
discards: [  0 131 132 133 159 160 184 189 200 201 202 223 224]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 234 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 549.6808 - loglik: -5.4392e+02 - logprior: -3.0443e+00
Epoch 2/2
39/39 - 21s - loss: 541.0144 - loglik: -5.3716e+02 - logprior: -1.3732e+00
Fitted a model with MAP estimate = -535.3396
expansions: [(0, 2), (193, 3), (218, 1), (234, 3)]
discards: [  0 101 156 179 215 226 227 228 229 230 231 232]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 231 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 544.7628 - loglik: -5.3971e+02 - logprior: -2.0795e+00
Epoch 2/10
39/39 - 21s - loss: 537.9962 - loglik: -5.3467e+02 - logprior: -7.2118e-01
Epoch 3/10
39/39 - 21s - loss: 534.9565 - loglik: -5.3189e+02 - logprior: -5.6955e-01
Epoch 4/10
39/39 - 21s - loss: 532.4850 - loglik: -5.2973e+02 - logprior: -5.0234e-01
Epoch 5/10
39/39 - 21s - loss: 530.6733 - loglik: -5.2824e+02 - logprior: -4.3439e-01
Epoch 6/10
39/39 - 21s - loss: 529.7477 - loglik: -5.2753e+02 - logprior: -3.6309e-01
Epoch 7/10
39/39 - 21s - loss: 528.6603 - loglik: -5.2667e+02 - logprior: -2.8068e-01
Epoch 8/10
39/39 - 21s - loss: 527.6668 - loglik: -5.2583e+02 - logprior: -1.9654e-01
Epoch 9/10
39/39 - 21s - loss: 527.0405 - loglik: -5.2535e+02 - logprior: -1.2840e-01
Epoch 10/10
39/39 - 21s - loss: 526.1071 - loglik: -5.2458e+02 - logprior: -4.0324e-02
Fitted a model with MAP estimate = -524.0686
Time for alignment: 596.6060
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 647.7213 - loglik: -6.4550e+02 - logprior: -1.9862e+00
Epoch 2/10
39/39 - 19s - loss: 552.5027 - loglik: -5.4978e+02 - logprior: -1.8476e+00
Epoch 3/10
39/39 - 19s - loss: 542.7741 - loglik: -5.3938e+02 - logprior: -1.9863e+00
Epoch 4/10
39/39 - 19s - loss: 539.8896 - loglik: -5.3641e+02 - logprior: -2.0245e+00
Epoch 5/10
39/39 - 19s - loss: 538.0908 - loglik: -5.3456e+02 - logprior: -2.0703e+00
Epoch 6/10
39/39 - 19s - loss: 537.1302 - loglik: -5.3368e+02 - logprior: -2.0953e+00
Epoch 7/10
39/39 - 19s - loss: 536.2057 - loglik: -5.3282e+02 - logprior: -2.1029e+00
Epoch 8/10
39/39 - 19s - loss: 535.9558 - loglik: -5.3263e+02 - logprior: -2.1222e+00
Epoch 9/10
39/39 - 19s - loss: 534.9258 - loglik: -5.3165e+02 - logprior: -2.1242e+00
Epoch 10/10
39/39 - 19s - loss: 534.9097 - loglik: -5.3161e+02 - logprior: -2.1375e+00
Fitted a model with MAP estimate = -532.9424
expansions: [(4, 1), (6, 1), (33, 1), (52, 1), (82, 9), (89, 2), (90, 1), (114, 1), (115, 1), (116, 8), (117, 3), (118, 1), (119, 2), (128, 1), (129, 8), (130, 2), (133, 1), (137, 1), (138, 1), (139, 1), (141, 9), (142, 1), (145, 1), (146, 1), (150, 1), (154, 1), (155, 1), (160, 1), (170, 1), (171, 4), (174, 1), (183, 1)]
discards: [  0 157 158 161 162 163 164 165 166 167 168 175 176 177 178 179 180 181
 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204
 205 206 207 208 209 210 211]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 239 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 563.2867 - loglik: -5.5860e+02 - logprior: -3.0442e+00
Epoch 2/2
39/39 - 21s - loss: 543.4368 - loglik: -5.3962e+02 - logprior: -1.5854e+00
Fitted a model with MAP estimate = -536.6951
expansions: [(218, 7), (239, 15)]
discards: [  0  94 143 164 165 166 184 185 186 221 235]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 250 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 541.2588 - loglik: -5.3552e+02 - logprior: -3.0429e+00
Epoch 2/2
39/39 - 23s - loss: 532.1437 - loglik: -5.2823e+02 - logprior: -1.4624e+00
Fitted a model with MAP estimate = -526.5866
expansions: [(0, 2), (226, 14), (230, 1)]
discards: [  0 161 162 197 198 199 200 201 202 235 236 237 238 239 240 241 242 243
 244 245 246 247 248 249]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 243 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 541.6599 - loglik: -5.3684e+02 - logprior: -2.0086e+00
Epoch 2/10
39/39 - 22s - loss: 533.1071 - loglik: -5.2997e+02 - logprior: -7.8001e-01
Epoch 3/10
39/39 - 22s - loss: 530.5829 - loglik: -5.2760e+02 - logprior: -6.7156e-01
Epoch 4/10
39/39 - 22s - loss: 528.8008 - loglik: -5.2610e+02 - logprior: -5.8961e-01
Epoch 5/10
39/39 - 22s - loss: 526.8010 - loglik: -5.2439e+02 - logprior: -5.0578e-01
Epoch 6/10
39/39 - 22s - loss: 525.8940 - loglik: -5.2369e+02 - logprior: -4.1897e-01
Epoch 7/10
39/39 - 22s - loss: 524.7183 - loglik: -5.2266e+02 - logprior: -3.3101e-01
Epoch 8/10
39/39 - 22s - loss: 523.3245 - loglik: -5.2134e+02 - logprior: -2.4911e-01
Epoch 9/10
39/39 - 22s - loss: 522.7707 - loglik: -5.2092e+02 - logprior: -1.8597e-01
Epoch 10/10
39/39 - 22s - loss: 521.9103 - loglik: -5.2023e+02 - logprior: -1.1010e-01
Fitted a model with MAP estimate = -519.2548
Time for alignment: 611.3771
Computed alignments with likelihoods: ['-520.7813', '-524.0686', '-519.2548']
Best model has likelihood: -519.2548
time for generating output: 0.3240
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02836.projection.fasta
SP score = 0.8945642577878673
Training of 3 independent models on file PF02777.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff6a7f6b9a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff38474b8e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fed2814e790>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff2fcd98460>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fed281448e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff6b08e0d00>, <__main__.SimpleDirichletPrior object at 0x7ff360039f10>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff6a7a0c550>

Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 269.8081 - loglik: -2.6662e+02 - logprior: -3.1797e+00
Epoch 2/10
19/19 - 3s - loss: 200.0781 - loglik: -1.9864e+02 - logprior: -1.3970e+00
Epoch 3/10
19/19 - 3s - loss: 175.4930 - loglik: -1.7357e+02 - logprior: -1.8230e+00
Epoch 4/10
19/19 - 3s - loss: 171.8183 - loglik: -1.6977e+02 - logprior: -1.7990e+00
Epoch 5/10
19/19 - 3s - loss: 170.1274 - loglik: -1.6820e+02 - logprior: -1.7135e+00
Epoch 6/10
19/19 - 3s - loss: 170.0492 - loglik: -1.6815e+02 - logprior: -1.6985e+00
Epoch 7/10
19/19 - 3s - loss: 169.3320 - loglik: -1.6747e+02 - logprior: -1.6679e+00
Epoch 8/10
19/19 - 3s - loss: 169.6435 - loglik: -1.6778e+02 - logprior: -1.6628e+00
Fitted a model with MAP estimate = -168.9956
expansions: [(4, 1), (6, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 5), (47, 1), (69, 1), (70, 3), (71, 1), (72, 1), (77, 1)]
discards: [2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 166.0588 - loglik: -1.6283e+02 - logprior: -3.0626e+00
Epoch 2/2
19/19 - 3s - loss: 154.3967 - loglik: -1.5287e+02 - logprior: -1.3354e+00
Fitted a model with MAP estimate = -152.4871
expansions: []
discards: [42 59 60 90]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 156.6180 - loglik: -1.5335e+02 - logprior: -3.0421e+00
Epoch 2/2
19/19 - 3s - loss: 152.7004 - loglik: -1.5120e+02 - logprior: -1.2488e+00
Fitted a model with MAP estimate = -151.6911
expansions: [(2, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 159.4785 - loglik: -1.5521e+02 - logprior: -4.0362e+00
Epoch 2/10
19/19 - 3s - loss: 154.5360 - loglik: -1.5194e+02 - logprior: -2.3426e+00
Epoch 3/10
19/19 - 3s - loss: 153.2968 - loglik: -1.5083e+02 - logprior: -2.1817e+00
Epoch 4/10
19/19 - 3s - loss: 152.0316 - loglik: -1.4998e+02 - logprior: -1.7519e+00
Epoch 5/10
19/19 - 3s - loss: 151.6789 - loglik: -1.5022e+02 - logprior: -1.1747e+00
Epoch 6/10
19/19 - 3s - loss: 150.9194 - loglik: -1.4947e+02 - logprior: -1.1709e+00
Epoch 7/10
19/19 - 3s - loss: 150.7872 - loglik: -1.4939e+02 - logprior: -1.1418e+00
Epoch 8/10
19/19 - 3s - loss: 150.5734 - loglik: -1.4920e+02 - logprior: -1.1211e+00
Epoch 9/10
19/19 - 3s - loss: 150.4526 - loglik: -1.4912e+02 - logprior: -1.0839e+00
Epoch 10/10
19/19 - 3s - loss: 150.3910 - loglik: -1.4908e+02 - logprior: -1.0663e+00
Fitted a model with MAP estimate = -150.0340
Time for alignment: 103.1017
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 269.9141 - loglik: -2.6672e+02 - logprior: -3.1817e+00
Epoch 2/10
19/19 - 3s - loss: 200.3791 - loglik: -1.9895e+02 - logprior: -1.3933e+00
Epoch 3/10
19/19 - 3s - loss: 176.6789 - loglik: -1.7479e+02 - logprior: -1.7839e+00
Epoch 4/10
19/19 - 3s - loss: 172.4608 - loglik: -1.7048e+02 - logprior: -1.7438e+00
Epoch 5/10
19/19 - 3s - loss: 171.9105 - loglik: -1.7002e+02 - logprior: -1.6806e+00
Epoch 6/10
19/19 - 3s - loss: 170.9119 - loglik: -1.6903e+02 - logprior: -1.6764e+00
Epoch 7/10
19/19 - 3s - loss: 170.4361 - loglik: -1.6855e+02 - logprior: -1.6781e+00
Epoch 8/10
19/19 - 3s - loss: 169.7191 - loglik: -1.6784e+02 - logprior: -1.6720e+00
Epoch 9/10
19/19 - 3s - loss: 170.1379 - loglik: -1.6827e+02 - logprior: -1.6687e+00
Fitted a model with MAP estimate = -169.4326
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 168.7099 - loglik: -1.6446e+02 - logprior: -4.0593e+00
Epoch 2/2
19/19 - 3s - loss: 156.5443 - loglik: -1.5418e+02 - logprior: -2.1598e+00
Fitted a model with MAP estimate = -154.2743
expansions: [(0, 2)]
discards: [ 0 42 61 62]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 156.5026 - loglik: -1.5327e+02 - logprior: -3.0239e+00
Epoch 2/2
19/19 - 3s - loss: 152.2767 - loglik: -1.5082e+02 - logprior: -1.2261e+00
Fitted a model with MAP estimate = -151.0400
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 158.6444 - loglik: -1.5443e+02 - logprior: -3.9652e+00
Epoch 2/10
19/19 - 3s - loss: 153.7566 - loglik: -1.5192e+02 - logprior: -1.5848e+00
Epoch 3/10
19/19 - 3s - loss: 151.7358 - loglik: -1.5030e+02 - logprior: -1.1514e+00
Epoch 4/10
19/19 - 3s - loss: 151.4155 - loglik: -1.4997e+02 - logprior: -1.1422e+00
Epoch 5/10
19/19 - 3s - loss: 150.4967 - loglik: -1.4909e+02 - logprior: -1.1226e+00
Epoch 6/10
19/19 - 3s - loss: 150.3944 - loglik: -1.4902e+02 - logprior: -1.0954e+00
Epoch 7/10
19/19 - 3s - loss: 150.1426 - loglik: -1.4879e+02 - logprior: -1.0804e+00
Epoch 8/10
19/19 - 3s - loss: 149.5343 - loglik: -1.4823e+02 - logprior: -1.0546e+00
Epoch 9/10
19/19 - 3s - loss: 150.2050 - loglik: -1.4892e+02 - logprior: -1.0346e+00
Fitted a model with MAP estimate = -149.3606
Time for alignment: 101.0388
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 270.0425 - loglik: -2.6685e+02 - logprior: -3.1838e+00
Epoch 2/10
19/19 - 3s - loss: 201.0200 - loglik: -1.9959e+02 - logprior: -1.3919e+00
Epoch 3/10
19/19 - 3s - loss: 175.9066 - loglik: -1.7400e+02 - logprior: -1.7940e+00
Epoch 4/10
19/19 - 3s - loss: 172.0247 - loglik: -1.6998e+02 - logprior: -1.7840e+00
Epoch 5/10
19/19 - 3s - loss: 171.3305 - loglik: -1.6942e+02 - logprior: -1.7023e+00
Epoch 6/10
19/19 - 3s - loss: 170.2187 - loglik: -1.6834e+02 - logprior: -1.6926e+00
Epoch 7/10
19/19 - 3s - loss: 169.8914 - loglik: -1.6804e+02 - logprior: -1.6656e+00
Epoch 8/10
19/19 - 3s - loss: 169.6469 - loglik: -1.6779e+02 - logprior: -1.6645e+00
Epoch 9/10
19/19 - 3s - loss: 169.3181 - loglik: -1.6747e+02 - logprior: -1.6547e+00
Epoch 10/10
19/19 - 3s - loss: 169.2240 - loglik: -1.6737e+02 - logprior: -1.6602e+00
Fitted a model with MAP estimate = -168.9234
expansions: [(4, 1), (6, 1), (8, 1), (9, 1), (14, 1), (16, 1), (19, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 165.0087 - loglik: -1.6178e+02 - logprior: -3.0626e+00
Epoch 2/2
19/19 - 3s - loss: 153.4192 - loglik: -1.5193e+02 - logprior: -1.2985e+00
Fitted a model with MAP estimate = -151.9936
expansions: []
discards: [42 61 62]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 155.7143 - loglik: -1.5249e+02 - logprior: -3.0275e+00
Epoch 2/2
19/19 - 3s - loss: 152.6912 - loglik: -1.5123e+02 - logprior: -1.2339e+00
Fitted a model with MAP estimate = -151.6099
expansions: [(2, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 159.4372 - loglik: -1.5521e+02 - logprior: -3.9926e+00
Epoch 2/10
19/19 - 3s - loss: 154.1415 - loglik: -1.5157e+02 - logprior: -2.3276e+00
Epoch 3/10
19/19 - 3s - loss: 153.6322 - loglik: -1.5116e+02 - logprior: -2.1926e+00
Epoch 4/10
19/19 - 3s - loss: 151.8232 - loglik: -1.4976e+02 - logprior: -1.7644e+00
Epoch 5/10
19/19 - 3s - loss: 151.4333 - loglik: -1.4996e+02 - logprior: -1.1806e+00
Epoch 6/10
19/19 - 3s - loss: 151.4841 - loglik: -1.5003e+02 - logprior: -1.1792e+00
Fitted a model with MAP estimate = -150.4967
Time for alignment: 94.0155
Computed alignments with likelihoods: ['-150.0340', '-149.3606', '-150.4967']
Best model has likelihood: -149.3606
time for generating output: 0.1480
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02777.projection.fasta
SP score = 0.9146099290780142
Training of 3 independent models on file PF07654.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2fdecd820>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff51a6450a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2f47d4d60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff51afcda30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff51afcd040>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff38452dcd0>, <__main__.SimpleDirichletPrior object at 0x7fed11634e50>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff6a7a0c550>

Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 232.1071 - loglik: -2.2892e+02 - logprior: -3.1694e+00
Epoch 2/10
19/19 - 2s - loss: 197.0656 - loglik: -1.9560e+02 - logprior: -1.3531e+00
Epoch 3/10
19/19 - 2s - loss: 184.0542 - loglik: -1.8226e+02 - logprior: -1.4674e+00
Epoch 4/10
19/19 - 2s - loss: 181.1533 - loglik: -1.7939e+02 - logprior: -1.4474e+00
Epoch 5/10
19/19 - 2s - loss: 179.7203 - loglik: -1.7801e+02 - logprior: -1.4301e+00
Epoch 6/10
19/19 - 2s - loss: 179.4896 - loglik: -1.7785e+02 - logprior: -1.4013e+00
Epoch 7/10
19/19 - 2s - loss: 178.8415 - loglik: -1.7722e+02 - logprior: -1.3862e+00
Epoch 8/10
19/19 - 2s - loss: 179.0111 - loglik: -1.7739e+02 - logprior: -1.3775e+00
Fitted a model with MAP estimate = -178.4697
expansions: [(7, 2), (8, 2), (9, 3), (21, 1), (22, 1), (31, 2), (32, 1), (33, 1), (34, 2), (46, 1), (48, 1), (53, 1), (55, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 183.5730 - loglik: -1.7926e+02 - logprior: -4.0087e+00
Epoch 2/2
19/19 - 2s - loss: 175.7998 - loglik: -1.7326e+02 - logprior: -2.2185e+00
Fitted a model with MAP estimate = -173.8183
expansions: [(0, 2)]
discards: [ 0 40 44 73]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 177.0655 - loglik: -1.7377e+02 - logprior: -3.0010e+00
Epoch 2/2
19/19 - 2s - loss: 173.4419 - loglik: -1.7187e+02 - logprior: -1.2319e+00
Fitted a model with MAP estimate = -172.0973
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 177.9587 - loglik: -1.7392e+02 - logprior: -3.7303e+00
Epoch 2/10
19/19 - 2s - loss: 173.9015 - loglik: -1.7224e+02 - logprior: -1.3430e+00
Epoch 3/10
19/19 - 2s - loss: 172.4477 - loglik: -1.7085e+02 - logprior: -1.1746e+00
Epoch 4/10
19/19 - 2s - loss: 171.1680 - loglik: -1.6960e+02 - logprior: -1.1498e+00
Epoch 5/10
19/19 - 2s - loss: 171.2043 - loglik: -1.6970e+02 - logprior: -1.1133e+00
Fitted a model with MAP estimate = -170.3930
Time for alignment: 60.4999
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 231.7800 - loglik: -2.2859e+02 - logprior: -3.1730e+00
Epoch 2/10
19/19 - 2s - loss: 196.0868 - loglik: -1.9462e+02 - logprior: -1.3502e+00
Epoch 3/10
19/19 - 2s - loss: 183.6907 - loglik: -1.8184e+02 - logprior: -1.4328e+00
Epoch 4/10
19/19 - 2s - loss: 181.0135 - loglik: -1.7925e+02 - logprior: -1.4093e+00
Epoch 5/10
19/19 - 2s - loss: 179.5083 - loglik: -1.7780e+02 - logprior: -1.3969e+00
Epoch 6/10
19/19 - 2s - loss: 179.2357 - loglik: -1.7760e+02 - logprior: -1.3793e+00
Epoch 7/10
19/19 - 2s - loss: 178.6163 - loglik: -1.7700e+02 - logprior: -1.3690e+00
Epoch 8/10
19/19 - 2s - loss: 178.7238 - loglik: -1.7712e+02 - logprior: -1.3572e+00
Fitted a model with MAP estimate = -178.2578
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (32, 1), (33, 1), (35, 2), (37, 1), (49, 1), (50, 2), (53, 2), (55, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 183.2979 - loglik: -1.7896e+02 - logprior: -4.0019e+00
Epoch 2/2
19/19 - 2s - loss: 175.5652 - loglik: -1.7302e+02 - logprior: -2.2022e+00
Fitted a model with MAP estimate = -173.5028
expansions: [(0, 2), (10, 1)]
discards: [ 0 64 69 73]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 177.2258 - loglik: -1.7393e+02 - logprior: -2.9886e+00
Epoch 2/2
19/19 - 2s - loss: 173.2394 - loglik: -1.7166e+02 - logprior: -1.2151e+00
Fitted a model with MAP estimate = -171.7394
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 177.6621 - loglik: -1.7362e+02 - logprior: -3.7178e+00
Epoch 2/10
19/19 - 2s - loss: 173.3949 - loglik: -1.7173e+02 - logprior: -1.3295e+00
Epoch 3/10
19/19 - 2s - loss: 172.0777 - loglik: -1.7046e+02 - logprior: -1.1680e+00
Epoch 4/10
19/19 - 2s - loss: 170.9917 - loglik: -1.6943e+02 - logprior: -1.1269e+00
Epoch 5/10
19/19 - 2s - loss: 170.4661 - loglik: -1.6897e+02 - logprior: -1.0853e+00
Epoch 6/10
19/19 - 2s - loss: 170.0097 - loglik: -1.6855e+02 - logprior: -1.0783e+00
Epoch 7/10
19/19 - 2s - loss: 170.1740 - loglik: -1.6875e+02 - logprior: -1.0536e+00
Fitted a model with MAP estimate = -169.4175
Time for alignment: 62.9629
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 231.9345 - loglik: -2.2874e+02 - logprior: -3.1720e+00
Epoch 2/10
19/19 - 2s - loss: 196.1211 - loglik: -1.9468e+02 - logprior: -1.3588e+00
Epoch 3/10
19/19 - 2s - loss: 183.5482 - loglik: -1.8178e+02 - logprior: -1.4699e+00
Epoch 4/10
19/19 - 2s - loss: 180.8136 - loglik: -1.7906e+02 - logprior: -1.4323e+00
Epoch 5/10
19/19 - 2s - loss: 179.6418 - loglik: -1.7795e+02 - logprior: -1.4059e+00
Epoch 6/10
19/19 - 2s - loss: 179.1440 - loglik: -1.7751e+02 - logprior: -1.3760e+00
Epoch 7/10
19/19 - 2s - loss: 178.6824 - loglik: -1.7707e+02 - logprior: -1.3583e+00
Epoch 8/10
19/19 - 2s - loss: 179.0461 - loglik: -1.7745e+02 - logprior: -1.3516e+00
Fitted a model with MAP estimate = -178.3613
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (32, 1), (33, 1), (35, 2), (37, 1), (49, 1), (50, 2), (53, 2), (55, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 182.9530 - loglik: -1.7864e+02 - logprior: -4.0016e+00
Epoch 2/2
19/19 - 2s - loss: 175.2889 - loglik: -1.7276e+02 - logprior: -2.1984e+00
Fitted a model with MAP estimate = -173.3665
expansions: [(0, 2), (10, 1)]
discards: [ 0 64 68 73]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 177.1059 - loglik: -1.7382e+02 - logprior: -2.9908e+00
Epoch 2/2
19/19 - 2s - loss: 173.0624 - loglik: -1.7151e+02 - logprior: -1.2171e+00
Fitted a model with MAP estimate = -171.6396
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 177.5054 - loglik: -1.7347e+02 - logprior: -3.7178e+00
Epoch 2/10
19/19 - 2s - loss: 173.5086 - loglik: -1.7185e+02 - logprior: -1.3349e+00
Epoch 3/10
19/19 - 2s - loss: 172.0352 - loglik: -1.7044e+02 - logprior: -1.1612e+00
Epoch 4/10
19/19 - 2s - loss: 170.9563 - loglik: -1.6939e+02 - logprior: -1.1325e+00
Epoch 5/10
19/19 - 2s - loss: 170.5420 - loglik: -1.6904e+02 - logprior: -1.0840e+00
Epoch 6/10
19/19 - 2s - loss: 170.2090 - loglik: -1.6875e+02 - logprior: -1.0713e+00
Epoch 7/10
19/19 - 2s - loss: 169.6635 - loglik: -1.6824e+02 - logprior: -1.0485e+00
Epoch 8/10
19/19 - 2s - loss: 169.6478 - loglik: -1.6827e+02 - logprior: -1.0303e+00
Epoch 9/10
19/19 - 2s - loss: 170.1692 - loglik: -1.6881e+02 - logprior: -1.0112e+00
Fitted a model with MAP estimate = -169.1466
Time for alignment: 66.1041
Computed alignments with likelihoods: ['-170.3930', '-169.4175', '-169.1466']
Best model has likelihood: -169.1466
time for generating output: 0.1502
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07654.projection.fasta
SP score = 0.8489795918367347
Training of 3 independent models on file PF04082.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff51aaff5e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff31c235310>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff31c2350a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff31c3c8940>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2f468af70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff517cae370>, <__main__.SimpleDirichletPrior object at 0x7ff2fd045af0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff517b36b80>

Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 642.0418 - loglik: -6.3963e+02 - logprior: -1.8788e+00
Epoch 2/10
39/39 - 19s - loss: 587.5864 - loglik: -5.8371e+02 - logprior: -1.3210e+00
Epoch 3/10
39/39 - 20s - loss: 576.3065 - loglik: -5.7146e+02 - logprior: -1.4743e+00
Epoch 4/10
39/39 - 20s - loss: 570.4742 - loglik: -5.6565e+02 - logprior: -1.5961e+00
Epoch 5/10
39/39 - 20s - loss: 566.2958 - loglik: -5.6179e+02 - logprior: -1.6757e+00
Epoch 6/10
39/39 - 20s - loss: 563.8947 - loglik: -5.5993e+02 - logprior: -1.7155e+00
Epoch 7/10
39/39 - 20s - loss: 563.1767 - loglik: -5.5958e+02 - logprior: -1.7355e+00
Epoch 8/10
39/39 - 20s - loss: 561.5107 - loglik: -5.5814e+02 - logprior: -1.7306e+00
Epoch 9/10
39/39 - 20s - loss: 561.1073 - loglik: -5.5780e+02 - logprior: -1.7378e+00
Epoch 10/10
39/39 - 20s - loss: 559.3720 - loglik: -5.5591e+02 - logprior: -1.7590e+00
Fitted a model with MAP estimate = -556.8399
expansions: [(17, 1), (22, 1), (23, 6), (44, 2), (48, 5), (53, 1), (62, 2), (63, 1), (64, 1), (65, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 2), (85, 1), (102, 1), (103, 1), (104, 2), (107, 1), (108, 1), (122, 1), (123, 1), (125, 1), (132, 1), (141, 2), (153, 9), (161, 1), (163, 1), (168, 1), (169, 1), (179, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 245 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 605.1903 - loglik: -5.9879e+02 - logprior: -3.3130e+00
Epoch 2/2
39/39 - 26s - loss: 566.7211 - loglik: -5.6078e+02 - logprior: -1.3892e+00
Fitted a model with MAP estimate = -554.9653
expansions: [(227, 2), (229, 1)]
discards: [ 26  80 104 180 181]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 572.5646 - loglik: -5.6462e+02 - logprior: -2.0029e+00
Epoch 2/2
39/39 - 27s - loss: 562.3436 - loglik: -5.5542e+02 - logprior: -7.7704e-01
Fitted a model with MAP estimate = -552.2518
expansions: []
discards: [ 55 130 225 242]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 239 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 30s - loss: 571.6770 - loglik: -5.6254e+02 - logprior: -1.9483e+00
Epoch 2/10
39/39 - 26s - loss: 562.2392 - loglik: -5.5487e+02 - logprior: -5.9349e-01
Epoch 3/10
39/39 - 26s - loss: 556.9685 - loglik: -5.5069e+02 - logprior: -4.9864e-01
Epoch 4/10
39/39 - 25s - loss: 552.1400 - loglik: -5.4681e+02 - logprior: -3.9782e-01
Epoch 5/10
39/39 - 25s - loss: 549.2354 - loglik: -5.4501e+02 - logprior: -3.4496e-01
Epoch 6/10
39/39 - 26s - loss: 546.7709 - loglik: -5.4342e+02 - logprior: -2.4834e-01
Epoch 7/10
39/39 - 26s - loss: 545.4940 - loglik: -5.4276e+02 - logprior: -1.5249e-01
Epoch 8/10
39/39 - 26s - loss: 544.5982 - loglik: -5.4221e+02 - logprior: -6.0384e-02
Epoch 9/10
39/39 - 25s - loss: 542.3857 - loglik: -5.4015e+02 - logprior: 0.0116
Epoch 10/10
39/39 - 24s - loss: 541.3620 - loglik: -5.3872e+02 - logprior: 0.0725
Fitted a model with MAP estimate = -535.7696
Time for alignment: 678.2289
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 642.0836 - loglik: -6.3969e+02 - logprior: -1.8600e+00
Epoch 2/10
39/39 - 19s - loss: 585.0192 - loglik: -5.8134e+02 - logprior: -1.4161e+00
Epoch 3/10
39/39 - 19s - loss: 573.1792 - loglik: -5.6870e+02 - logprior: -1.5868e+00
Epoch 4/10
39/39 - 19s - loss: 568.6661 - loglik: -5.6401e+02 - logprior: -1.6142e+00
Epoch 5/10
39/39 - 19s - loss: 565.7189 - loglik: -5.6133e+02 - logprior: -1.6534e+00
Epoch 6/10
39/39 - 19s - loss: 563.7422 - loglik: -5.5980e+02 - logprior: -1.6686e+00
Epoch 7/10
39/39 - 19s - loss: 562.1578 - loglik: -5.5853e+02 - logprior: -1.6791e+00
Epoch 8/10
39/39 - 19s - loss: 561.1825 - loglik: -5.5777e+02 - logprior: -1.6937e+00
Epoch 9/10
39/39 - 19s - loss: 559.9837 - loglik: -5.5666e+02 - logprior: -1.7031e+00
Epoch 10/10
39/39 - 19s - loss: 559.4160 - loglik: -5.5607e+02 - logprior: -1.7163e+00
Fitted a model with MAP estimate = -556.6055
expansions: [(21, 1), (22, 1), (24, 4), (27, 1), (44, 2), (48, 5), (62, 2), (63, 1), (65, 1), (82, 1), (83, 1), (84, 1), (85, 1), (105, 2), (106, 3), (109, 1), (110, 1), (124, 1), (125, 1), (127, 1), (134, 1), (150, 6), (161, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 1), (174, 1), (179, 1)]
discards: [  1 189]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 237 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 606.0410 - loglik: -6.0074e+02 - logprior: -2.4927e+00
Epoch 2/2
39/39 - 23s - loss: 568.2327 - loglik: -5.6289e+02 - logprior: -1.1743e+00
Fitted a model with MAP estimate = -557.9809
expansions: [(29, 1), (104, 1), (176, 1)]
discards: [ 78 105]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 238 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 573.4912 - loglik: -5.6591e+02 - logprior: -2.0543e+00
Epoch 2/2
39/39 - 24s - loss: 563.0953 - loglik: -5.5645e+02 - logprior: -7.7996e-01
Fitted a model with MAP estimate = -553.0382
expansions: [(238, 4)]
discards: [ 26  56 128]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 239 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 571.9463 - loglik: -5.6305e+02 - logprior: -1.9226e+00
Epoch 2/10
39/39 - 24s - loss: 562.0663 - loglik: -5.5466e+02 - logprior: -8.0841e-01
Epoch 3/10
39/39 - 24s - loss: 557.0405 - loglik: -5.5075e+02 - logprior: -6.9806e-01
Epoch 4/10
39/39 - 24s - loss: 552.7119 - loglik: -5.4733e+02 - logprior: -6.4119e-01
Epoch 5/10
39/39 - 24s - loss: 549.5010 - loglik: -5.4519e+02 - logprior: -5.6145e-01
Epoch 6/10
39/39 - 24s - loss: 547.5364 - loglik: -5.4406e+02 - logprior: -4.7341e-01
Epoch 7/10
39/39 - 25s - loss: 546.3307 - loglik: -5.4345e+02 - logprior: -3.7929e-01
Epoch 8/10
39/39 - 25s - loss: 544.9209 - loglik: -5.4244e+02 - logprior: -2.8963e-01
Epoch 9/10
39/39 - 25s - loss: 543.5529 - loglik: -5.4129e+02 - logprior: -2.0266e-01
Epoch 10/10
39/39 - 25s - loss: 542.4916 - loglik: -5.4022e+02 - logprior: -1.0412e-01
Fitted a model with MAP estimate = -539.1282
Time for alignment: 637.7033
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 641.5473 - loglik: -6.3915e+02 - logprior: -1.8635e+00
Epoch 2/10
39/39 - 21s - loss: 586.7969 - loglik: -5.8303e+02 - logprior: -1.2760e+00
Epoch 3/10
39/39 - 20s - loss: 574.4586 - loglik: -5.6959e+02 - logprior: -1.5111e+00
Epoch 4/10
39/39 - 20s - loss: 568.4056 - loglik: -5.6357e+02 - logprior: -1.5732e+00
Epoch 5/10
39/39 - 21s - loss: 565.1035 - loglik: -5.6073e+02 - logprior: -1.6201e+00
Epoch 6/10
39/39 - 21s - loss: 563.4953 - loglik: -5.5962e+02 - logprior: -1.6648e+00
Epoch 7/10
39/39 - 20s - loss: 561.7972 - loglik: -5.5830e+02 - logprior: -1.6801e+00
Epoch 8/10
39/39 - 20s - loss: 560.9254 - loglik: -5.5762e+02 - logprior: -1.7080e+00
Epoch 9/10
39/39 - 20s - loss: 560.0236 - loglik: -5.5681e+02 - logprior: -1.7369e+00
Epoch 10/10
39/39 - 21s - loss: 559.2051 - loglik: -5.5590e+02 - logprior: -1.7600e+00
Fitted a model with MAP estimate = -556.7055
expansions: [(22, 1), (24, 6), (27, 1), (44, 2), (48, 5), (62, 2), (63, 1), (65, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 2), (97, 1), (103, 1), (104, 3), (105, 1), (108, 1), (109, 1), (124, 1), (127, 1), (134, 1), (135, 1), (156, 3), (161, 1), (163, 1), (165, 1), (168, 1), (170, 1), (172, 1), (176, 3), (179, 1)]
discards: [  0 189]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 240 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 604.9993 - loglik: -5.9898e+02 - logprior: -3.2676e+00
Epoch 2/2
39/39 - 26s - loss: 568.0617 - loglik: -5.6252e+02 - logprior: -1.3908e+00
Fitted a model with MAP estimate = -556.6034
expansions: [(31, 1), (195, 1), (240, 3)]
discards: [ 26  27  79 105 133]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 240 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 573.0569 - loglik: -5.6540e+02 - logprior: -2.1323e+00
Epoch 2/2
39/39 - 29s - loss: 562.8659 - loglik: -5.5608e+02 - logprior: -9.1633e-01
Fitted a model with MAP estimate = -552.8811
expansions: [(26, 3)]
discards: [238 239]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 241 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 31s - loss: 570.8195 - loglik: -5.6187e+02 - logprior: -1.9754e+00
Epoch 2/10
39/39 - 28s - loss: 561.4789 - loglik: -5.5416e+02 - logprior: -7.2986e-01
Epoch 3/10
39/39 - 28s - loss: 556.4528 - loglik: -5.5020e+02 - logprior: -6.1959e-01
Epoch 4/10
39/39 - 27s - loss: 551.8825 - loglik: -5.4650e+02 - logprior: -5.5350e-01
Epoch 5/10
39/39 - 27s - loss: 548.7360 - loglik: -5.4445e+02 - logprior: -4.6945e-01
Epoch 6/10
39/39 - 27s - loss: 546.6480 - loglik: -5.4324e+02 - logprior: -3.7332e-01
Epoch 7/10
39/39 - 27s - loss: 545.3912 - loglik: -5.4262e+02 - logprior: -2.7282e-01
Epoch 8/10
39/39 - 27s - loss: 543.9058 - loglik: -5.4152e+02 - logprior: -1.9007e-01
Epoch 9/10
39/39 - 27s - loss: 542.7921 - loglik: -5.4066e+02 - logprior: -8.7504e-02
Epoch 10/10
39/39 - 27s - loss: 541.7339 - loglik: -5.3948e+02 - logprior: 0.0019
Fitted a model with MAP estimate = -537.6380
Time for alignment: 708.0517
Computed alignments with likelihoods: ['-535.7696', '-539.1282', '-537.6380']
Best model has likelihood: -535.7696
time for generating output: 0.2861
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF04082.projection.fasta
SP score = 0.6632841328413284
Training of 3 independent models on file PF00046.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2fd6833a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff2fd555eb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2fd555f70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff39878bac0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fed2908b400>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff3961fdca0>, <__main__.SimpleDirichletPrior object at 0x7fed29126460>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff2f46bb160>

Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 154.1438 - loglik: -1.5088e+02 - logprior: -3.2560e+00
Epoch 2/10
19/19 - 1s - loss: 124.1964 - loglik: -1.2262e+02 - logprior: -1.5389e+00
Epoch 3/10
19/19 - 1s - loss: 110.8094 - loglik: -1.0884e+02 - logprior: -1.6225e+00
Epoch 4/10
19/19 - 1s - loss: 107.3155 - loglik: -1.0532e+02 - logprior: -1.6945e+00
Epoch 5/10
19/19 - 1s - loss: 106.5880 - loglik: -1.0475e+02 - logprior: -1.6237e+00
Epoch 6/10
19/19 - 1s - loss: 106.0492 - loglik: -1.0425e+02 - logprior: -1.6194e+00
Epoch 7/10
19/19 - 1s - loss: 105.5583 - loglik: -1.0380e+02 - logprior: -1.5863e+00
Epoch 8/10
19/19 - 1s - loss: 105.7198 - loglik: -1.0397e+02 - logprior: -1.5745e+00
Fitted a model with MAP estimate = -105.2946
expansions: [(3, 1), (5, 1), (7, 1), (10, 1), (16, 1), (17, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.2459 - loglik: -1.0390e+02 - logprior: -3.1452e+00
Epoch 2/2
19/19 - 1s - loss: 99.5910 - loglik: -9.7995e+01 - logprior: -1.4044e+00
Fitted a model with MAP estimate = -98.4110
expansions: []
discards: [22 37 40]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 101.5626 - loglik: -9.8345e+01 - logprior: -3.0400e+00
Epoch 2/2
19/19 - 1s - loss: 98.8520 - loglik: -9.7389e+01 - logprior: -1.2651e+00
Fitted a model with MAP estimate = -98.0125
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 105.0710 - loglik: -1.0080e+02 - logprior: -4.0990e+00
Epoch 2/10
19/19 - 1s - loss: 101.6934 - loglik: -9.9241e+01 - logprior: -2.2613e+00
Epoch 3/10
19/19 - 1s - loss: 100.5799 - loglik: -9.8208e+01 - logprior: -2.1373e+00
Epoch 4/10
19/19 - 1s - loss: 98.7878 - loglik: -9.7005e+01 - logprior: -1.5355e+00
Epoch 5/10
19/19 - 1s - loss: 98.2352 - loglik: -9.6742e+01 - logprior: -1.2504e+00
Epoch 6/10
19/19 - 2s - loss: 97.5881 - loglik: -9.6118e+01 - logprior: -1.2445e+00
Epoch 7/10
19/19 - 1s - loss: 97.7970 - loglik: -9.6363e+01 - logprior: -1.2243e+00
Fitted a model with MAP estimate = -97.3213
Time for alignment: 51.8211
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 154.1639 - loglik: -1.5090e+02 - logprior: -3.2550e+00
Epoch 2/10
19/19 - 1s - loss: 123.5065 - loglik: -1.2195e+02 - logprior: -1.5227e+00
Epoch 3/10
19/19 - 1s - loss: 111.0227 - loglik: -1.0919e+02 - logprior: -1.5970e+00
Epoch 4/10
19/19 - 1s - loss: 107.8109 - loglik: -1.0586e+02 - logprior: -1.6590e+00
Epoch 5/10
19/19 - 1s - loss: 107.1178 - loglik: -1.0533e+02 - logprior: -1.5687e+00
Epoch 6/10
19/19 - 1s - loss: 106.5069 - loglik: -1.0473e+02 - logprior: -1.5791e+00
Epoch 7/10
19/19 - 1s - loss: 106.4265 - loglik: -1.0470e+02 - logprior: -1.5432e+00
Epoch 8/10
19/19 - 1s - loss: 106.1425 - loglik: -1.0442e+02 - logprior: -1.5373e+00
Epoch 9/10
19/19 - 1s - loss: 106.2297 - loglik: -1.0451e+02 - logprior: -1.5331e+00
Fitted a model with MAP estimate = -105.7859
expansions: [(3, 1), (5, 1), (7, 2), (9, 1), (16, 1), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 106.6039 - loglik: -1.0327e+02 - logprior: -3.1220e+00
Epoch 2/2
19/19 - 1s - loss: 99.3926 - loglik: -9.7829e+01 - logprior: -1.3720e+00
Fitted a model with MAP estimate = -98.3983
expansions: []
discards: [ 0 36 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 105.3551 - loglik: -1.0106e+02 - logprior: -4.1250e+00
Epoch 2/2
19/19 - 1s - loss: 101.4434 - loglik: -9.8958e+01 - logprior: -2.3115e+00
Fitted a model with MAP estimate = -99.9583
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 104.1337 - loglik: -9.9983e+01 - logprior: -3.9831e+00
Epoch 2/10
19/19 - 1s - loss: 99.8256 - loglik: -9.8059e+01 - logprior: -1.5691e+00
Epoch 3/10
19/19 - 1s - loss: 99.0300 - loglik: -9.7471e+01 - logprior: -1.3221e+00
Epoch 4/10
19/19 - 1s - loss: 98.4539 - loglik: -9.6920e+01 - logprior: -1.2818e+00
Epoch 5/10
19/19 - 1s - loss: 97.8957 - loglik: -9.6404e+01 - logprior: -1.2547e+00
Epoch 6/10
19/19 - 1s - loss: 97.8692 - loglik: -9.6407e+01 - logprior: -1.2384e+00
Epoch 7/10
19/19 - 1s - loss: 97.6262 - loglik: -9.6209e+01 - logprior: -1.2125e+00
Epoch 8/10
19/19 - 1s - loss: 97.5301 - loglik: -9.6136e+01 - logprior: -1.1962e+00
Epoch 9/10
19/19 - 1s - loss: 97.5401 - loglik: -9.6174e+01 - logprior: -1.1793e+00
Fitted a model with MAP estimate = -97.1581
Time for alignment: 55.3563
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.1743 - loglik: -1.5091e+02 - logprior: -3.2557e+00
Epoch 2/10
19/19 - 1s - loss: 124.3177 - loglik: -1.2276e+02 - logprior: -1.5195e+00
Epoch 3/10
19/19 - 1s - loss: 110.1859 - loglik: -1.0829e+02 - logprior: -1.6393e+00
Epoch 4/10
19/19 - 1s - loss: 106.9431 - loglik: -1.0496e+02 - logprior: -1.6937e+00
Epoch 5/10
19/19 - 1s - loss: 105.7543 - loglik: -1.0393e+02 - logprior: -1.6267e+00
Epoch 6/10
19/19 - 1s - loss: 105.4479 - loglik: -1.0364e+02 - logprior: -1.6246e+00
Epoch 7/10
19/19 - 1s - loss: 105.1301 - loglik: -1.0338e+02 - logprior: -1.5838e+00
Epoch 8/10
19/19 - 1s - loss: 104.9772 - loglik: -1.0322e+02 - logprior: -1.5807e+00
Epoch 9/10
19/19 - 1s - loss: 105.0544 - loglik: -1.0332e+02 - logprior: -1.5711e+00
Fitted a model with MAP estimate = -104.6726
expansions: [(3, 1), (5, 1), (7, 1), (10, 1), (14, 1), (16, 1), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 106.6021 - loglik: -1.0327e+02 - logprior: -3.1276e+00
Epoch 2/2
19/19 - 1s - loss: 99.4577 - loglik: -9.7898e+01 - logprior: -1.3707e+00
Fitted a model with MAP estimate = -98.3925
expansions: []
discards: [36 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.4740 - loglik: -9.8257e+01 - logprior: -3.0368e+00
Epoch 2/2
19/19 - 1s - loss: 98.9295 - loglik: -9.7475e+01 - logprior: -1.2630e+00
Fitted a model with MAP estimate = -98.0258
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 101.1493 - loglik: -9.7962e+01 - logprior: -2.9970e+00
Epoch 2/10
19/19 - 1s - loss: 98.6723 - loglik: -9.7229e+01 - logprior: -1.2463e+00
Epoch 3/10
19/19 - 1s - loss: 98.1790 - loglik: -9.6714e+01 - logprior: -1.2131e+00
Epoch 4/10
19/19 - 1s - loss: 97.6374 - loglik: -9.6198e+01 - logprior: -1.1760e+00
Epoch 5/10
19/19 - 1s - loss: 97.1825 - loglik: -9.5805e+01 - logprior: -1.1341e+00
Epoch 6/10
19/19 - 1s - loss: 97.0548 - loglik: -9.5704e+01 - logprior: -1.1274e+00
Epoch 7/10
19/19 - 1s - loss: 96.7196 - loglik: -9.5397e+01 - logprior: -1.1078e+00
Epoch 8/10
19/19 - 1s - loss: 96.9427 - loglik: -9.5640e+01 - logprior: -1.0993e+00
Fitted a model with MAP estimate = -96.4641
Time for alignment: 53.8161
Computed alignments with likelihoods: ['-97.3213', '-97.1581', '-96.4641']
Best model has likelihood: -96.4641
time for generating output: 0.1170
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00046.projection.fasta
SP score = 0.9629629629629629
Training of 3 independent models on file PF01814.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff51aadc5b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff31c342c10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff52cd3eb50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff31c0fb130>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff36029b3a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff2fd068b20>, <__main__.SimpleDirichletPrior object at 0x7ff6e359f5e0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff2f46bb0d0>

Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 365.4537 - loglik: -3.6228e+02 - logprior: -3.0590e+00
Epoch 2/10
19/19 - 4s - loss: 335.3721 - loglik: -3.3342e+02 - logprior: -1.0967e+00
Epoch 3/10
19/19 - 4s - loss: 322.1428 - loglik: -3.1930e+02 - logprior: -1.4000e+00
Epoch 4/10
19/19 - 4s - loss: 314.6760 - loglik: -3.1160e+02 - logprior: -1.4005e+00
Epoch 5/10
19/19 - 4s - loss: 311.4491 - loglik: -3.0830e+02 - logprior: -1.4448e+00
Epoch 6/10
19/19 - 4s - loss: 309.5793 - loglik: -3.0673e+02 - logprior: -1.4526e+00
Epoch 7/10
19/19 - 4s - loss: 308.9437 - loglik: -3.0630e+02 - logprior: -1.4541e+00
Epoch 8/10
19/19 - 4s - loss: 308.2136 - loglik: -3.0575e+02 - logprior: -1.4500e+00
Epoch 9/10
19/19 - 4s - loss: 308.1152 - loglik: -3.0575e+02 - logprior: -1.4512e+00
Epoch 10/10
19/19 - 4s - loss: 307.1407 - loglik: -3.0489e+02 - logprior: -1.4502e+00
Fitted a model with MAP estimate = -306.3911
expansions: [(15, 1), (17, 1), (19, 1), (21, 3), (23, 4), (25, 1), (28, 1), (31, 1), (52, 1), (54, 3), (74, 1), (75, 4), (82, 1), (83, 1), (84, 3), (86, 1)]
discards: [2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 332.9805 - loglik: -3.2908e+02 - logprior: -2.9955e+00
Epoch 2/2
19/19 - 5s - loss: 311.3218 - loglik: -3.0864e+02 - logprior: -1.3047e+00
Fitted a model with MAP estimate = -305.7001
expansions: [(22, 1), (27, 2)]
discards: [107]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 312.2682 - loglik: -3.0729e+02 - logprior: -3.0301e+00
Epoch 2/2
19/19 - 5s - loss: 306.4170 - loglik: -3.0260e+02 - logprior: -1.2468e+00
Fitted a model with MAP estimate = -301.1619
expansions: [(32, 2)]
discards: [29]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 133 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 310.3506 - loglik: -3.0418e+02 - logprior: -3.0184e+00
Epoch 2/10
19/19 - 5s - loss: 305.3136 - loglik: -3.0070e+02 - logprior: -1.2254e+00
Epoch 3/10
19/19 - 5s - loss: 302.1926 - loglik: -2.9789e+02 - logprior: -1.1750e+00
Epoch 4/10
19/19 - 5s - loss: 300.2764 - loglik: -2.9636e+02 - logprior: -1.1387e+00
Epoch 5/10
19/19 - 5s - loss: 298.4244 - loglik: -2.9504e+02 - logprior: -1.0977e+00
Epoch 6/10
19/19 - 5s - loss: 297.7613 - loglik: -2.9480e+02 - logprior: -1.0726e+00
Epoch 7/10
19/19 - 5s - loss: 296.8636 - loglik: -2.9423e+02 - logprior: -1.0441e+00
Epoch 8/10
19/19 - 5s - loss: 296.0086 - loglik: -2.9360e+02 - logprior: -1.0120e+00
Epoch 9/10
19/19 - 5s - loss: 295.5920 - loglik: -2.9334e+02 - logprior: -9.7884e-01
Epoch 10/10
19/19 - 5s - loss: 295.5261 - loglik: -2.9337e+02 - logprior: -9.5549e-01
Fitted a model with MAP estimate = -293.8748
Time for alignment: 152.0399
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 365.3867 - loglik: -3.6221e+02 - logprior: -3.0599e+00
Epoch 2/10
19/19 - 4s - loss: 336.1814 - loglik: -3.3420e+02 - logprior: -1.1078e+00
Epoch 3/10
19/19 - 4s - loss: 321.7106 - loglik: -3.1855e+02 - logprior: -1.4006e+00
Epoch 4/10
19/19 - 4s - loss: 315.2357 - loglik: -3.1167e+02 - logprior: -1.3710e+00
Epoch 5/10
19/19 - 4s - loss: 312.0867 - loglik: -3.0880e+02 - logprior: -1.3975e+00
Epoch 6/10
19/19 - 4s - loss: 309.8374 - loglik: -3.0699e+02 - logprior: -1.4257e+00
Epoch 7/10
19/19 - 4s - loss: 308.4553 - loglik: -3.0588e+02 - logprior: -1.4414e+00
Epoch 8/10
19/19 - 4s - loss: 307.5937 - loglik: -3.0515e+02 - logprior: -1.4433e+00
Epoch 9/10
19/19 - 4s - loss: 307.1268 - loglik: -3.0477e+02 - logprior: -1.4433e+00
Epoch 10/10
19/19 - 4s - loss: 307.2866 - loglik: -3.0498e+02 - logprior: -1.4479e+00
Fitted a model with MAP estimate = -305.7976
expansions: [(15, 1), (17, 1), (19, 1), (24, 5), (27, 1), (29, 1), (32, 1), (51, 1), (53, 4), (73, 1), (74, 1), (75, 3), (82, 1), (84, 3), (86, 1)]
discards: [2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 334.9359 - loglik: -3.3084e+02 - logprior: -2.9952e+00
Epoch 2/2
19/19 - 5s - loss: 312.0517 - loglik: -3.0903e+02 - logprior: -1.2950e+00
Fitted a model with MAP estimate = -306.2629
expansions: [(24, 2)]
discards: [ 65  95 106]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 127 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 313.6193 - loglik: -3.0840e+02 - logprior: -2.9955e+00
Epoch 2/2
19/19 - 5s - loss: 307.9771 - loglik: -3.0409e+02 - logprior: -1.2116e+00
Fitted a model with MAP estimate = -302.7112
expansions: [(22, 1), (23, 1), (25, 2), (99, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 134 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 311.6998 - loglik: -3.0554e+02 - logprior: -2.9706e+00
Epoch 2/10
19/19 - 5s - loss: 305.8677 - loglik: -3.0126e+02 - logprior: -1.2031e+00
Epoch 3/10
19/19 - 5s - loss: 302.9431 - loglik: -2.9870e+02 - logprior: -1.1372e+00
Epoch 4/10
19/19 - 5s - loss: 300.8727 - loglik: -2.9699e+02 - logprior: -1.1011e+00
Epoch 5/10
19/19 - 5s - loss: 299.0590 - loglik: -2.9574e+02 - logprior: -1.0616e+00
Epoch 6/10
19/19 - 5s - loss: 298.4802 - loglik: -2.9556e+02 - logprior: -1.0338e+00
Epoch 7/10
19/19 - 5s - loss: 297.2354 - loglik: -2.9465e+02 - logprior: -1.0082e+00
Epoch 8/10
19/19 - 5s - loss: 296.5238 - loglik: -2.9417e+02 - logprior: -9.7475e-01
Epoch 9/10
19/19 - 5s - loss: 296.2726 - loglik: -2.9409e+02 - logprior: -9.4205e-01
Epoch 10/10
19/19 - 5s - loss: 295.7602 - loglik: -2.9369e+02 - logprior: -9.1069e-01
Fitted a model with MAP estimate = -294.4096
Time for alignment: 150.4610
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 365.2426 - loglik: -3.6206e+02 - logprior: -3.0624e+00
Epoch 2/10
19/19 - 4s - loss: 334.4520 - loglik: -3.3247e+02 - logprior: -1.1173e+00
Epoch 3/10
19/19 - 4s - loss: 322.0084 - loglik: -3.1915e+02 - logprior: -1.3574e+00
Epoch 4/10
19/19 - 4s - loss: 317.9651 - loglik: -3.1546e+02 - logprior: -1.2567e+00
Epoch 5/10
19/19 - 4s - loss: 313.1777 - loglik: -3.1070e+02 - logprior: -1.3575e+00
Epoch 6/10
19/19 - 4s - loss: 310.1676 - loglik: -3.0750e+02 - logprior: -1.3966e+00
Epoch 7/10
19/19 - 4s - loss: 309.1273 - loglik: -3.0658e+02 - logprior: -1.3985e+00
Epoch 8/10
19/19 - 4s - loss: 308.9919 - loglik: -3.0662e+02 - logprior: -1.4004e+00
Epoch 9/10
19/19 - 4s - loss: 308.2341 - loglik: -3.0595e+02 - logprior: -1.4004e+00
Epoch 10/10
19/19 - 4s - loss: 307.6184 - loglik: -3.0543e+02 - logprior: -1.3994e+00
Fitted a model with MAP estimate = -306.7364
expansions: [(18, 1), (20, 2), (22, 5), (24, 4), (27, 1), (29, 1), (32, 1), (55, 4), (75, 1), (76, 4), (82, 1), (83, 1), (84, 3), (86, 1)]
discards: [2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 330.7166 - loglik: -3.2681e+02 - logprior: -2.9584e+00
Epoch 2/2
19/19 - 5s - loss: 310.2798 - loglik: -3.0757e+02 - logprior: -1.2720e+00
Fitted a model with MAP estimate = -304.7783
expansions: []
discards: [109]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 311.8966 - loglik: -3.0688e+02 - logprior: -3.0297e+00
Epoch 2/2
19/19 - 5s - loss: 306.4978 - loglik: -3.0267e+02 - logprior: -1.2356e+00
Fitted a model with MAP estimate = -301.5254
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 310.6229 - loglik: -3.0444e+02 - logprior: -3.0120e+00
Epoch 2/10
19/19 - 5s - loss: 305.5540 - loglik: -3.0091e+02 - logprior: -1.2172e+00
Epoch 3/10
19/19 - 5s - loss: 302.8204 - loglik: -2.9854e+02 - logprior: -1.1640e+00
Epoch 4/10
19/19 - 5s - loss: 300.8811 - loglik: -2.9699e+02 - logprior: -1.1239e+00
Epoch 5/10
19/19 - 5s - loss: 299.1068 - loglik: -2.9579e+02 - logprior: -1.0761e+00
Epoch 6/10
19/19 - 5s - loss: 298.1931 - loglik: -2.9530e+02 - logprior: -1.0508e+00
Epoch 7/10
19/19 - 5s - loss: 297.2139 - loglik: -2.9465e+02 - logprior: -1.0140e+00
Epoch 8/10
19/19 - 5s - loss: 296.8983 - loglik: -2.9455e+02 - logprior: -9.8494e-01
Epoch 9/10
19/19 - 5s - loss: 296.3318 - loglik: -2.9414e+02 - logprior: -9.4899e-01
Epoch 10/10
19/19 - 5s - loss: 295.5917 - loglik: -2.9351e+02 - logprior: -9.1812e-01
Fitted a model with MAP estimate = -294.5139
Time for alignment: 152.3210
Computed alignments with likelihoods: ['-293.8748', '-294.4096', '-294.5139']
Best model has likelihood: -293.8748
time for generating output: 1.0392
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01814.projection.fasta
SP score = 0.8188050930460333
Training of 3 independent models on file PF00155.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff5179d6400>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff51b295b80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff51b295d30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff33c6d87f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff517ed1700>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff6e3660070>, <__main__.SimpleDirichletPrior object at 0x7ff33c1a0c70>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff517617040>

Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 917.6872 - loglik: -9.1551e+02 - logprior: -1.6353e+00
Epoch 2/10
39/39 - 34s - loss: 815.7908 - loglik: -8.1163e+02 - logprior: -1.8538e+00
Epoch 3/10
39/39 - 34s - loss: 803.4490 - loglik: -7.9865e+02 - logprior: -1.9309e+00
Epoch 4/10
39/39 - 34s - loss: 798.8595 - loglik: -7.9445e+02 - logprior: -1.9717e+00
Epoch 5/10
39/39 - 34s - loss: 796.9774 - loglik: -7.9282e+02 - logprior: -2.0232e+00
Epoch 6/10
39/39 - 34s - loss: 795.9995 - loglik: -7.9202e+02 - logprior: -2.0803e+00
Epoch 7/10
39/39 - 34s - loss: 794.9286 - loglik: -7.9114e+02 - logprior: -2.0786e+00
Epoch 8/10
39/39 - 35s - loss: 794.0457 - loglik: -7.9039e+02 - logprior: -2.1369e+00
Epoch 9/10
39/39 - 34s - loss: 792.9628 - loglik: -7.8944e+02 - logprior: -2.1414e+00
Epoch 10/10
39/39 - 34s - loss: 792.5928 - loglik: -7.8923e+02 - logprior: -2.1376e+00
Fitted a model with MAP estimate = -783.2948
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (24, 1), (25, 1), (26, 1), (30, 2), (32, 1), (40, 1), (42, 1), (43, 2), (45, 2), (56, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 1), (86, 1), (88, 1), (95, 3), (103, 1), (105, 2), (121, 1), (130, 2), (131, 1), (132, 2), (145, 1), (148, 1), (149, 1), (153, 2), (154, 1), (155, 2), (169, 1), (181, 1), (184, 1), (186, 1), (187, 1), (188, 1), (189, 1), (197, 1), (206, 3), (207, 1), (208, 1), (209, 1), (210, 1), (219, 1), (220, 1), (226, 2), (227, 2), (239, 2), (240, 1), (242, 1), (245, 1), (261, 2), (270, 2), (271, 1), (272, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 358 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 53s - loss: 805.7654 - loglik: -8.0106e+02 - logprior: -2.6523e+00
Epoch 2/2
39/39 - 49s - loss: 773.5993 - loglik: -7.6931e+02 - logprior: -1.2489e+00
Fitted a model with MAP estimate = -758.3621
expansions: [(120, 1), (336, 1)]
discards: [  0  39 121 135 168 195 291 307 345]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 351 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 51s - loss: 780.7184 - loglik: -7.7373e+02 - logprior: -2.9253e+00
Epoch 2/2
39/39 - 49s - loss: 772.1198 - loglik: -7.6684e+02 - logprior: -9.5397e-01
Fitted a model with MAP estimate = -757.1959
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 352 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 55s - loss: 769.8871 - loglik: -7.6295e+02 - logprior: -1.8267e+00
Epoch 2/10
39/39 - 51s - loss: 763.2032 - loglik: -7.5769e+02 - logprior: -6.9262e-01
Epoch 3/10
39/39 - 51s - loss: 761.0978 - loglik: -7.5659e+02 - logprior: -4.0499e-01
Epoch 4/10
39/39 - 50s - loss: 757.7524 - loglik: -7.5403e+02 - logprior: -2.3581e-01
Epoch 5/10
39/39 - 49s - loss: 755.5882 - loglik: -7.5248e+02 - logprior: -1.0379e-01
Epoch 6/10
39/39 - 48s - loss: 754.6923 - loglik: -7.5207e+02 - logprior: 0.0289
Epoch 7/10
39/39 - 48s - loss: 754.4923 - loglik: -7.5231e+02 - logprior: 0.1947
Epoch 8/10
39/39 - 48s - loss: 752.0451 - loglik: -7.5023e+02 - logprior: 0.3314
Epoch 9/10
39/39 - 49s - loss: 752.1070 - loglik: -7.5063e+02 - logprior: 0.4521
Fitted a model with MAP estimate = -748.4471
Time for alignment: 1217.3241
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 915.8680 - loglik: -9.1368e+02 - logprior: -1.6431e+00
Epoch 2/10
39/39 - 34s - loss: 818.6361 - loglik: -8.1476e+02 - logprior: -1.8124e+00
Epoch 3/10
39/39 - 34s - loss: 803.3022 - loglik: -7.9915e+02 - logprior: -2.1222e+00
Epoch 4/10
39/39 - 34s - loss: 798.9154 - loglik: -7.9463e+02 - logprior: -2.1125e+00
Epoch 5/10
39/39 - 35s - loss: 796.2368 - loglik: -7.9208e+02 - logprior: -2.1299e+00
Epoch 6/10
39/39 - 35s - loss: 795.3348 - loglik: -7.9134e+02 - logprior: -2.1791e+00
Epoch 7/10
39/39 - 35s - loss: 794.0671 - loglik: -7.9014e+02 - logprior: -2.2249e+00
Epoch 8/10
39/39 - 35s - loss: 793.3913 - loglik: -7.8962e+02 - logprior: -2.2244e+00
Epoch 9/10
39/39 - 35s - loss: 792.6913 - loglik: -7.8900e+02 - logprior: -2.2588e+00
Epoch 10/10
39/39 - 35s - loss: 792.1216 - loglik: -7.8852e+02 - logprior: -2.2791e+00
Fitted a model with MAP estimate = -782.6849
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (23, 3), (24, 1), (25, 1), (30, 1), (36, 1), (40, 1), (41, 1), (42, 2), (44, 1), (45, 1), (55, 1), (58, 1), (61, 1), (78, 1), (80, 1), (82, 1), (86, 1), (88, 1), (91, 1), (93, 1), (95, 1), (97, 1), (101, 1), (103, 1), (121, 1), (129, 2), (131, 1), (132, 2), (142, 1), (148, 2), (149, 1), (154, 1), (155, 3), (169, 1), (181, 1), (184, 1), (185, 1), (187, 1), (188, 1), (189, 1), (206, 4), (207, 1), (208, 1), (209, 1), (210, 1), (219, 1), (220, 1), (228, 2), (240, 2), (241, 1), (243, 1), (245, 1), (258, 1), (261, 1), (262, 1), (263, 1), (269, 2), (270, 2), (271, 1), (272, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 360 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 54s - loss: 800.8447 - loglik: -7.9619e+02 - logprior: -2.5124e+00
Epoch 2/2
39/39 - 49s - loss: 772.5672 - loglik: -7.6844e+02 - logprior: -1.0657e+00
Fitted a model with MAP estimate = -757.5871
expansions: []
discards: [  1  30 117 168 198 306 344 347]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 352 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 57s - loss: 778.6071 - loglik: -7.7271e+02 - logprior: -1.8516e+00
Epoch 2/2
39/39 - 57s - loss: 771.3238 - loglik: -7.6607e+02 - logprior: -8.9277e-01
Fitted a model with MAP estimate = -757.1237
expansions: [(340, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 353 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 63s - loss: 769.4751 - loglik: -7.6254e+02 - logprior: -1.8102e+00
Epoch 2/10
39/39 - 55s - loss: 763.2618 - loglik: -7.5773e+02 - logprior: -7.0930e-01
Epoch 3/10
39/39 - 57s - loss: 760.7662 - loglik: -7.5622e+02 - logprior: -4.1865e-01
Epoch 4/10
39/39 - 59s - loss: 757.3438 - loglik: -7.5362e+02 - logprior: -2.6217e-01
Epoch 5/10
39/39 - 54s - loss: 755.9346 - loglik: -7.5281e+02 - logprior: -1.1266e-01
Epoch 6/10
39/39 - 53s - loss: 754.5044 - loglik: -7.5188e+02 - logprior: 0.0171
Epoch 7/10
39/39 - 59s - loss: 753.4510 - loglik: -7.5124e+02 - logprior: 0.1759
Epoch 8/10
39/39 - 57s - loss: 752.2361 - loglik: -7.5041e+02 - logprior: 0.3193
Epoch 9/10
39/39 - 54s - loss: 751.6291 - loglik: -7.5017e+02 - logprior: 0.4707
Epoch 10/10
39/39 - 56s - loss: 749.9398 - loglik: -7.4879e+02 - logprior: 0.5901
Fitted a model with MAP estimate = -747.8003
Time for alignment: 1384.1030
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 44s - loss: 915.7557 - loglik: -9.1357e+02 - logprior: -1.6439e+00
Epoch 2/10
39/39 - 43s - loss: 817.0132 - loglik: -8.1310e+02 - logprior: -1.7590e+00
Epoch 3/10
39/39 - 44s - loss: 803.1080 - loglik: -7.9847e+02 - logprior: -1.8377e+00
Epoch 4/10
39/39 - 45s - loss: 798.0492 - loglik: -7.9358e+02 - logprior: -1.8930e+00
Epoch 5/10
39/39 - 44s - loss: 795.5494 - loglik: -7.9137e+02 - logprior: -1.9541e+00
Epoch 6/10
39/39 - 45s - loss: 793.9203 - loglik: -7.9000e+02 - logprior: -1.9823e+00
Epoch 7/10
39/39 - 45s - loss: 793.3620 - loglik: -7.8958e+02 - logprior: -2.0730e+00
Epoch 8/10
39/39 - 44s - loss: 792.4069 - loglik: -7.8882e+02 - logprior: -2.0804e+00
Epoch 9/10
39/39 - 44s - loss: 791.8643 - loglik: -7.8844e+02 - logprior: -2.0728e+00
Epoch 10/10
39/39 - 43s - loss: 791.5766 - loglik: -7.8826e+02 - logprior: -2.1003e+00
Fitted a model with MAP estimate = -782.1574
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2), (26, 1), (30, 1), (40, 1), (42, 1), (43, 2), (45, 2), (56, 1), (59, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 1), (87, 1), (88, 1), (95, 2), (104, 1), (120, 1), (125, 2), (131, 1), (134, 1), (145, 1), (150, 1), (155, 1), (156, 2), (162, 1), (182, 1), (185, 1), (186, 1), (188, 1), (189, 1), (190, 1), (205, 2), (206, 4), (207, 1), (208, 1), (209, 1), (211, 1), (220, 1), (227, 2), (228, 2), (240, 2), (241, 1), (243, 1), (246, 1), (258, 1), (263, 1), (269, 2), (270, 2), (271, 1), (272, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 354 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 66s - loss: 803.9633 - loglik: -7.9943e+02 - logprior: -2.5350e+00
Epoch 2/2
39/39 - 65s - loss: 773.5262 - loglik: -7.6940e+02 - logprior: -1.1179e+00
Fitted a model with MAP estimate = -758.4462
expansions: [(157, 1)]
discards: [  0  30 255 286 302 338 341]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 62s - loss: 780.9846 - loglik: -7.7399e+02 - logprior: -2.9885e+00
Epoch 2/2
39/39 - 57s - loss: 772.3009 - loglik: -7.6703e+02 - logprior: -9.8963e-01
Fitted a model with MAP estimate = -757.5950
expansions: [(0, 2), (325, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 350 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 62s - loss: 769.9323 - loglik: -7.6314e+02 - logprior: -1.7173e+00
Epoch 2/10
39/39 - 56s - loss: 763.9796 - loglik: -7.5857e+02 - logprior: -5.9642e-01
Epoch 3/10
39/39 - 53s - loss: 760.6988 - loglik: -7.5621e+02 - logprior: -4.0347e-01
Epoch 4/10
39/39 - 58s - loss: 758.3478 - loglik: -7.5462e+02 - logprior: -2.9141e-01
Epoch 5/10
39/39 - 62s - loss: 756.5598 - loglik: -7.5342e+02 - logprior: -1.4493e-01
Epoch 6/10
39/39 - 65s - loss: 755.3771 - loglik: -7.5273e+02 - logprior: -1.4406e-02
Epoch 7/10
39/39 - 66s - loss: 753.7714 - loglik: -7.5150e+02 - logprior: 0.1067
Epoch 8/10
39/39 - 66s - loss: 752.8337 - loglik: -7.5096e+02 - logprior: 0.2728
Epoch 9/10
39/39 - 66s - loss: 751.1556 - loglik: -7.4958e+02 - logprior: 0.3874
Epoch 10/10
39/39 - 66s - loss: 751.3896 - loglik: -7.5014e+02 - logprior: 0.5208
Fitted a model with MAP estimate = -747.9821
Time for alignment: 1580.4955
Computed alignments with likelihoods: ['-748.4471', '-747.8003', '-747.9821']
Best model has likelihood: -747.8003
time for generating output: 1.4235
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00155.projection.fasta
SP score = 0.461162273869464
Training of 3 independent models on file PF00450.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff6e32c1d90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff51a089b80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff51ade13d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fed093da3a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fed093dab50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff2fd1764f0>, <__main__.SimpleDirichletPrior object at 0x7fed09439fa0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff517617040>

Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 66s - loss: 892.4644 - loglik: -8.9054e+02 - logprior: -1.6731e+00
Epoch 2/10
39/39 - 64s - loss: 752.8931 - loglik: -7.5012e+02 - logprior: -1.9593e+00
Epoch 3/10
39/39 - 66s - loss: 739.9170 - loglik: -7.3658e+02 - logprior: -2.1421e+00
Epoch 4/10
39/39 - 66s - loss: 737.0995 - loglik: -7.3380e+02 - logprior: -2.1307e+00
Epoch 5/10
39/39 - 61s - loss: 733.6794 - loglik: -7.3037e+02 - logprior: -2.2065e+00
Epoch 6/10
39/39 - 59s - loss: 733.2186 - loglik: -7.2982e+02 - logprior: -2.2926e+00
Epoch 7/10
39/39 - 64s - loss: 732.6382 - loglik: -7.2925e+02 - logprior: -2.2754e+00
Epoch 8/10
39/39 - 58s - loss: 731.5743 - loglik: -7.2819e+02 - logprior: -2.3252e+00
Epoch 9/10
39/39 - 57s - loss: 732.0352 - loglik: -7.2867e+02 - logprior: -2.3209e+00
Fitted a model with MAP estimate = -729.2179
expansions: [(10, 1), (19, 1), (21, 1), (31, 1), (67, 1), (102, 2), (105, 1), (107, 1), (108, 1), (109, 1), (122, 1), (141, 1), (143, 1), (144, 1), (146, 5), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (168, 1), (175, 1), (177, 1), (178, 5), (181, 2), (182, 2), (184, 1), (185, 2), (188, 1), (189, 1), (190, 1), (191, 2), (192, 1), (200, 1), (203, 2), (210, 2), (211, 2), (216, 1), (217, 1), (218, 1), (219, 1), (227, 1), (232, 3), (235, 1), (238, 1), (240, 1), (242, 2), (244, 1), (250, 1), (273, 1), (277, 1), (278, 3), (282, 3), (283, 2), (296, 1), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1 193 194 195 196 197]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 405 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 86s - loss: 728.8124 - loglik: -7.2503e+02 - logprior: -2.2842e+00
Epoch 2/2
39/39 - 88s - loss: 711.0112 - loglik: -7.0809e+02 - logprior: -1.0584e+00
Fitted a model with MAP estimate = -705.5278
expansions: [(214, 1), (353, 1)]
discards: [207 208 222 244 256 301 349 350 351]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 398 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 98s - loss: 717.3134 - loglik: -7.1328e+02 - logprior: -1.8313e+00
Epoch 2/2
39/39 - 97s - loss: 710.9722 - loglik: -7.0836e+02 - logprior: -5.8643e-01
Fitted a model with MAP estimate = -706.3997
expansions: [(5, 1), (232, 3)]
discards: [297]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 401 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 75s - loss: 714.2655 - loglik: -7.1037e+02 - logprior: -1.6099e+00
Epoch 2/10
39/39 - 71s - loss: 709.1603 - loglik: -7.0671e+02 - logprior: -3.6401e-01
Epoch 3/10
39/39 - 73s - loss: 706.3243 - loglik: -7.0409e+02 - logprior: -2.9522e-01
Epoch 4/10
39/39 - 76s - loss: 705.4398 - loglik: -7.0363e+02 - logprior: -8.3138e-02
Epoch 5/10
39/39 - 76s - loss: 703.2032 - loglik: -7.0171e+02 - logprior: 0.0794
Epoch 6/10
39/39 - 74s - loss: 703.3640 - loglik: -7.0211e+02 - logprior: 0.2383
Fitted a model with MAP estimate = -700.6881
Time for alignment: 1772.3972
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 55s - loss: 895.1286 - loglik: -8.9319e+02 - logprior: -1.6924e+00
Epoch 2/10
39/39 - 51s - loss: 756.2242 - loglik: -7.5384e+02 - logprior: -1.8742e+00
Epoch 3/10
39/39 - 51s - loss: 740.7511 - loglik: -7.3762e+02 - logprior: -2.0997e+00
Epoch 4/10
39/39 - 51s - loss: 737.0137 - loglik: -7.3380e+02 - logprior: -2.1327e+00
Epoch 5/10
39/39 - 51s - loss: 734.6362 - loglik: -7.3148e+02 - logprior: -2.1310e+00
Epoch 6/10
39/39 - 52s - loss: 733.5775 - loglik: -7.3038e+02 - logprior: -2.1513e+00
Epoch 7/10
39/39 - 53s - loss: 732.6755 - loglik: -7.2942e+02 - logprior: -2.2090e+00
Epoch 8/10
39/39 - 53s - loss: 732.6008 - loglik: -7.2933e+02 - logprior: -2.2527e+00
Epoch 9/10
39/39 - 53s - loss: 731.2828 - loglik: -7.2805e+02 - logprior: -2.2422e+00
Epoch 10/10
39/39 - 53s - loss: 731.2126 - loglik: -7.2794e+02 - logprior: -2.2816e+00
Fitted a model with MAP estimate = -729.0759
expansions: [(9, 1), (19, 1), (22, 1), (31, 1), (63, 1), (66, 1), (97, 1), (100, 2), (103, 1), (105, 1), (111, 1), (114, 1), (119, 1), (120, 1), (138, 1), (140, 1), (141, 1), (142, 3), (143, 2), (144, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (166, 1), (167, 1), (175, 1), (176, 5), (179, 1), (180, 5), (181, 1), (182, 1), (185, 1), (186, 2), (187, 2), (188, 1), (201, 2), (209, 1), (210, 5), (217, 2), (218, 1), (219, 1), (233, 3), (240, 1), (242, 2), (243, 2), (245, 1), (249, 1), (250, 1), (269, 1), (273, 1), (276, 1), (278, 1), (282, 5), (283, 1), (288, 1), (296, 1), (311, 1), (313, 1), (317, 1), (319, 1), (321, 1)]
discards: [  1 190 191 192 193 194 205 206 207]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 408 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 87s - loss: 730.1674 - loglik: -7.2649e+02 - logprior: -2.2343e+00
Epoch 2/2
39/39 - 83s - loss: 710.7117 - loglik: -7.0782e+02 - logprior: -1.0451e+00
Fitted a model with MAP estimate = -704.9253
expansions: [(233, 1), (234, 6)]
discards: [161 209 210 219 239 240 241 242 243 244 245 246 247 248 249 257 258 259
 262 267 302 303 304 352 353]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 390 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 84s - loss: 718.6393 - loglik: -7.1455e+02 - logprior: -1.8871e+00
Epoch 2/2
39/39 - 82s - loss: 711.4870 - loglik: -7.0886e+02 - logprior: -5.8457e-01
Fitted a model with MAP estimate = -706.1472
expansions: [(5, 1), (248, 2), (289, 2)]
discards: [235]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 394 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 85s - loss: 713.9175 - loglik: -7.1003e+02 - logprior: -1.6210e+00
Epoch 2/10
39/39 - 80s - loss: 708.6376 - loglik: -7.0621e+02 - logprior: -3.5366e-01
Epoch 3/10
39/39 - 80s - loss: 705.9127 - loglik: -7.0380e+02 - logprior: -1.7510e-01
Epoch 4/10
39/39 - 86s - loss: 705.0462 - loglik: -7.0329e+02 - logprior: -2.8113e-02
Epoch 5/10
39/39 - 84s - loss: 703.1542 - loglik: -7.0164e+02 - logprior: 0.0498
Epoch 6/10
39/39 - 86s - loss: 702.7133 - loglik: -7.0145e+02 - logprior: 0.1806
Epoch 7/10
39/39 - 83s - loss: 702.1436 - loglik: -7.0116e+02 - logprior: 0.3654
Epoch 8/10
39/39 - 81s - loss: 701.2435 - loglik: -7.0047e+02 - logprior: 0.5520
Epoch 9/10
39/39 - 81s - loss: 701.0146 - loglik: -7.0035e+02 - logprior: 0.6355
Epoch 10/10
39/39 - 83s - loss: 699.6822 - loglik: -6.9910e+02 - logprior: 0.7343
Fitted a model with MAP estimate = -697.7175
Time for alignment: 2043.5323
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 61s - loss: 896.6005 - loglik: -8.9464e+02 - logprior: -1.6865e+00
Epoch 2/10
39/39 - 54s - loss: 754.2041 - loglik: -7.5127e+02 - logprior: -2.1170e+00
Epoch 3/10
39/39 - 53s - loss: 740.3530 - loglik: -7.3692e+02 - logprior: -2.2834e+00
Epoch 4/10
39/39 - 53s - loss: 737.9732 - loglik: -7.3461e+02 - logprior: -2.2496e+00
Epoch 5/10
39/39 - 52s - loss: 736.1134 - loglik: -7.3280e+02 - logprior: -2.2867e+00
Epoch 6/10
39/39 - 51s - loss: 735.3167 - loglik: -7.3199e+02 - logprior: -2.3396e+00
Epoch 7/10
39/39 - 50s - loss: 735.1265 - loglik: -7.3185e+02 - logprior: -2.3058e+00
Epoch 8/10
39/39 - 50s - loss: 733.4186 - loglik: -7.3010e+02 - logprior: -2.3657e+00
Epoch 9/10
39/39 - 50s - loss: 734.2967 - loglik: -7.3102e+02 - logprior: -2.3513e+00
Fitted a model with MAP estimate = -731.5485
expansions: [(9, 1), (19, 2), (20, 1), (63, 2), (77, 1), (100, 2), (104, 1), (105, 1), (108, 1), (114, 1), (119, 1), (120, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 3), (142, 2), (143, 1), (158, 1), (160, 1), (161, 1), (162, 1), (166, 1), (175, 2), (176, 4), (179, 1), (180, 4), (182, 1), (184, 1), (186, 1), (187, 1), (188, 1), (189, 2), (202, 2), (211, 2), (219, 1), (220, 2), (221, 1), (226, 1), (234, 1), (235, 3), (237, 1), (240, 1), (242, 1), (246, 1), (250, 1), (254, 1), (257, 1), (269, 1), (277, 1), (278, 2), (279, 1), (281, 1), (282, 1), (283, 2), (289, 1), (291, 1), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  0   1 190 191 192 193 194 206 207 208]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 401 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 75s - loss: 732.4457 - loglik: -7.2801e+02 - logprior: -3.0348e+00
Epoch 2/2
39/39 - 73s - loss: 714.7292 - loglik: -7.1105e+02 - logprior: -1.8953e+00
Fitted a model with MAP estimate = -708.8851
expansions: [(4, 2), (70, 1), (233, 5), (246, 5), (295, 1), (339, 1)]
discards: [  0   1  66 160 162 208 217 234 235 240 241 242 252 263 283 343 344 345]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 398 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 76s - loss: 721.8837 - loglik: -7.1713e+02 - logprior: -2.6252e+00
Epoch 2/2
39/39 - 70s - loss: 712.7206 - loglik: -7.0965e+02 - logprior: -1.0363e+00
Fitted a model with MAP estimate = -706.3713
expansions: [(4, 1)]
discards: [233 234 235 236 237 238 239 240 243]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 390 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 69s - loss: 714.9358 - loglik: -7.1103e+02 - logprior: -1.6023e+00
Epoch 2/10
39/39 - 66s - loss: 710.1651 - loglik: -7.0781e+02 - logprior: -3.3377e-01
Epoch 3/10
39/39 - 67s - loss: 707.2398 - loglik: -7.0526e+02 - logprior: -1.2839e-01
Epoch 4/10
39/39 - 68s - loss: 705.9290 - loglik: -7.0419e+02 - logprior: -8.4197e-02
Epoch 5/10
39/39 - 68s - loss: 704.8407 - loglik: -7.0349e+02 - logprior: 0.1373
Epoch 6/10
39/39 - 67s - loss: 704.5956 - loglik: -7.0346e+02 - logprior: 0.2654
Epoch 7/10
39/39 - 68s - loss: 703.3487 - loglik: -7.0250e+02 - logprior: 0.4436
Epoch 8/10
39/39 - 68s - loss: 703.1524 - loglik: -7.0246e+02 - logprior: 0.5881
Epoch 9/10
39/39 - 68s - loss: 702.4542 - loglik: -7.0193e+02 - logprior: 0.7278
Epoch 10/10
39/39 - 69s - loss: 702.1369 - loglik: -7.0170e+02 - logprior: 0.8352
Fitted a model with MAP estimate = -699.9328
Time for alignment: 1730.7196
Computed alignments with likelihoods: ['-700.6881', '-697.7175', '-699.9328']
Best model has likelihood: -697.7175
time for generating output: 0.4661
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00450.projection.fasta
SP score = 0.8082908754052802
Training of 3 independent models on file PF07679.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fed282bf730>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fed291eb6d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff3604423a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff2fdbf1fa0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff5239e26d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fed28728760>, <__main__.SimpleDirichletPrior object at 0x7ff6602270d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff523515430>

Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 252.7677 - loglik: -2.4957e+02 - logprior: -3.1148e+00
Epoch 2/10
19/19 - 2s - loss: 221.0682 - loglik: -2.1943e+02 - logprior: -1.3214e+00
Epoch 3/10
19/19 - 2s - loss: 207.8438 - loglik: -2.0568e+02 - logprior: -1.5400e+00
Epoch 4/10
19/19 - 2s - loss: 205.4780 - loglik: -2.0356e+02 - logprior: -1.4237e+00
Epoch 5/10
19/19 - 2s - loss: 204.6044 - loglik: -2.0276e+02 - logprior: -1.4355e+00
Epoch 6/10
19/19 - 2s - loss: 204.1357 - loglik: -2.0234e+02 - logprior: -1.4147e+00
Epoch 7/10
19/19 - 2s - loss: 203.6534 - loglik: -2.0186e+02 - logprior: -1.4040e+00
Epoch 8/10
19/19 - 2s - loss: 203.4313 - loglik: -2.0164e+02 - logprior: -1.3973e+00
Epoch 9/10
19/19 - 2s - loss: 203.5676 - loglik: -2.0177e+02 - logprior: -1.4006e+00
Fitted a model with MAP estimate = -202.6592
expansions: [(8, 1), (9, 3), (10, 1), (12, 1), (14, 1), (19, 1), (24, 1), (34, 1), (39, 1), (41, 2), (43, 2), (48, 1), (49, 2), (60, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 208.4357 - loglik: -2.0386e+02 - logprior: -3.9961e+00
Epoch 2/2
19/19 - 2s - loss: 199.0695 - loglik: -1.9601e+02 - logprior: -2.2121e+00
Fitted a model with MAP estimate = -196.6618
expansions: [(0, 2)]
discards: [ 0 10 65]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 199.4283 - loglik: -1.9549e+02 - logprior: -2.9950e+00
Epoch 2/2
19/19 - 2s - loss: 196.0227 - loglik: -1.9406e+02 - logprior: -1.2015e+00
Fitted a model with MAP estimate = -194.3856
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 200.5574 - loglik: -1.9591e+02 - logprior: -3.7907e+00
Epoch 2/10
19/19 - 2s - loss: 196.2885 - loglik: -1.9420e+02 - logprior: -1.3397e+00
Epoch 3/10
19/19 - 2s - loss: 195.2184 - loglik: -1.9336e+02 - logprior: -1.1415e+00
Epoch 4/10
19/19 - 2s - loss: 194.5414 - loglik: -1.9276e+02 - logprior: -1.0953e+00
Epoch 5/10
19/19 - 2s - loss: 194.2679 - loglik: -1.9257e+02 - logprior: -1.0573e+00
Epoch 6/10
19/19 - 2s - loss: 193.8462 - loglik: -1.9221e+02 - logprior: -1.0379e+00
Epoch 7/10
19/19 - 2s - loss: 193.2222 - loglik: -1.9161e+02 - logprior: -1.0236e+00
Epoch 8/10
19/19 - 2s - loss: 193.3803 - loglik: -1.9180e+02 - logprior: -1.0034e+00
Fitted a model with MAP estimate = -192.2446
Time for alignment: 77.2082
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 252.6943 - loglik: -2.4949e+02 - logprior: -3.1158e+00
Epoch 2/10
19/19 - 2s - loss: 222.2398 - loglik: -2.2063e+02 - logprior: -1.3021e+00
Epoch 3/10
19/19 - 2s - loss: 208.3024 - loglik: -2.0637e+02 - logprior: -1.5186e+00
Epoch 4/10
19/19 - 2s - loss: 205.1294 - loglik: -2.0327e+02 - logprior: -1.4415e+00
Epoch 5/10
19/19 - 2s - loss: 204.3125 - loglik: -2.0249e+02 - logprior: -1.4528e+00
Epoch 6/10
19/19 - 2s - loss: 203.8593 - loglik: -2.0205e+02 - logprior: -1.4356e+00
Epoch 7/10
19/19 - 2s - loss: 203.7038 - loglik: -2.0189e+02 - logprior: -1.4297e+00
Epoch 8/10
19/19 - 2s - loss: 203.3421 - loglik: -2.0154e+02 - logprior: -1.4278e+00
Epoch 9/10
19/19 - 2s - loss: 203.1357 - loglik: -2.0133e+02 - logprior: -1.4279e+00
Epoch 10/10
19/19 - 2s - loss: 202.8260 - loglik: -2.0101e+02 - logprior: -1.4216e+00
Fitted a model with MAP estimate = -202.3193
expansions: [(8, 1), (9, 3), (20, 1), (21, 1), (22, 1), (23, 1), (26, 1), (34, 1), (39, 1), (40, 2), (43, 2), (47, 1), (48, 2), (49, 2), (60, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 207.9705 - loglik: -2.0342e+02 - logprior: -4.0102e+00
Epoch 2/2
19/19 - 2s - loss: 199.2783 - loglik: -1.9623e+02 - logprior: -2.2273e+00
Fitted a model with MAP estimate = -196.7996
expansions: [(0, 2)]
discards: [ 0 10 51 66]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 199.5209 - loglik: -1.9560e+02 - logprior: -2.9923e+00
Epoch 2/2
19/19 - 2s - loss: 195.8424 - loglik: -1.9389e+02 - logprior: -1.1933e+00
Fitted a model with MAP estimate = -194.4405
expansions: [(54, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 200.7619 - loglik: -1.9606e+02 - logprior: -3.8345e+00
Epoch 2/10
19/19 - 2s - loss: 196.1910 - loglik: -1.9405e+02 - logprior: -1.3831e+00
Epoch 3/10
19/19 - 2s - loss: 194.9787 - loglik: -1.9314e+02 - logprior: -1.1243e+00
Epoch 4/10
19/19 - 2s - loss: 194.4332 - loglik: -1.9266e+02 - logprior: -1.0863e+00
Epoch 5/10
19/19 - 2s - loss: 194.0495 - loglik: -1.9236e+02 - logprior: -1.0532e+00
Epoch 6/10
19/19 - 2s - loss: 193.5579 - loglik: -1.9192e+02 - logprior: -1.0371e+00
Epoch 7/10
19/19 - 2s - loss: 193.0271 - loglik: -1.9143e+02 - logprior: -1.0146e+00
Epoch 8/10
19/19 - 2s - loss: 193.0935 - loglik: -1.9152e+02 - logprior: -1.0002e+00
Fitted a model with MAP estimate = -192.0533
Time for alignment: 79.3573
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 253.0423 - loglik: -2.4984e+02 - logprior: -3.1119e+00
Epoch 2/10
19/19 - 2s - loss: 221.9970 - loglik: -2.2041e+02 - logprior: -1.3042e+00
Epoch 3/10
19/19 - 2s - loss: 207.3596 - loglik: -2.0519e+02 - logprior: -1.5722e+00
Epoch 4/10
19/19 - 2s - loss: 204.1658 - loglik: -2.0219e+02 - logprior: -1.4880e+00
Epoch 5/10
19/19 - 2s - loss: 203.4458 - loglik: -2.0154e+02 - logprior: -1.4963e+00
Epoch 6/10
19/19 - 2s - loss: 202.8444 - loglik: -2.0094e+02 - logprior: -1.4849e+00
Epoch 7/10
19/19 - 2s - loss: 202.4771 - loglik: -2.0058e+02 - logprior: -1.4735e+00
Epoch 8/10
19/19 - 2s - loss: 202.3306 - loglik: -2.0043e+02 - logprior: -1.4715e+00
Epoch 9/10
19/19 - 2s - loss: 202.1638 - loglik: -2.0026e+02 - logprior: -1.4694e+00
Epoch 10/10
19/19 - 2s - loss: 201.9657 - loglik: -2.0004e+02 - logprior: -1.4697e+00
Fitted a model with MAP estimate = -201.1656
expansions: [(9, 1), (10, 1), (11, 1), (13, 2), (14, 1), (25, 1), (35, 1), (37, 2), (39, 2), (40, 2), (42, 2), (47, 1), (48, 2), (49, 1), (60, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 208.7935 - loglik: -2.0417e+02 - logprior: -4.0134e+00
Epoch 2/2
19/19 - 2s - loss: 199.3848 - loglik: -1.9625e+02 - logprior: -2.2266e+00
Fitted a model with MAP estimate = -196.8065
expansions: [(0, 2)]
discards: [ 0 45 49 65]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 199.4633 - loglik: -1.9549e+02 - logprior: -2.9938e+00
Epoch 2/2
19/19 - 2s - loss: 195.8852 - loglik: -1.9393e+02 - logprior: -1.1979e+00
Fitted a model with MAP estimate = -194.3674
expansions: [(54, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 200.7405 - loglik: -1.9604e+02 - logprior: -3.8167e+00
Epoch 2/10
19/19 - 2s - loss: 196.1075 - loglik: -1.9398e+02 - logprior: -1.3632e+00
Epoch 3/10
19/19 - 2s - loss: 194.9774 - loglik: -1.9313e+02 - logprior: -1.1264e+00
Epoch 4/10
19/19 - 2s - loss: 194.3795 - loglik: -1.9262e+02 - logprior: -1.0843e+00
Epoch 5/10
19/19 - 2s - loss: 194.1186 - loglik: -1.9243e+02 - logprior: -1.0526e+00
Epoch 6/10
19/19 - 2s - loss: 193.3348 - loglik: -1.9170e+02 - logprior: -1.0359e+00
Epoch 7/10
19/19 - 2s - loss: 193.2535 - loglik: -1.9167e+02 - logprior: -1.0119e+00
Epoch 8/10
19/19 - 2s - loss: 192.8168 - loglik: -1.9126e+02 - logprior: -9.9454e-01
Epoch 9/10
19/19 - 2s - loss: 192.6362 - loglik: -1.9108e+02 - logprior: -9.8197e-01
Epoch 10/10
19/19 - 2s - loss: 192.1670 - loglik: -1.9058e+02 - logprior: -9.6966e-01
Fitted a model with MAP estimate = -191.2838
Time for alignment: 82.6700
Computed alignments with likelihoods: ['-192.2446', '-192.0533', '-191.2838']
Best model has likelihood: -191.2838
time for generating output: 0.2180
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07679.projection.fasta
SP score = 0.7688261351052049
Training of 3 independent models on file PF07686.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff696c014c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff2fd605d60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff51746fcd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff2fc392bb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff51acfee80>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff6605dcdf0>, <__main__.SimpleDirichletPrior object at 0x7ff6b0c4ef70>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7fed203c9160>

Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 301.4411 - loglik: -2.9826e+02 - logprior: -3.0907e+00
Epoch 2/10
19/19 - 2s - loss: 270.2661 - loglik: -2.6853e+02 - logprior: -1.3158e+00
Epoch 3/10
19/19 - 2s - loss: 257.7749 - loglik: -2.5568e+02 - logprior: -1.3746e+00
Epoch 4/10
19/19 - 2s - loss: 254.6021 - loglik: -2.5253e+02 - logprior: -1.2647e+00
Epoch 5/10
19/19 - 2s - loss: 253.1191 - loglik: -2.5108e+02 - logprior: -1.2505e+00
Epoch 6/10
19/19 - 2s - loss: 252.0894 - loglik: -2.5003e+02 - logprior: -1.2431e+00
Epoch 7/10
19/19 - 3s - loss: 251.5919 - loglik: -2.4957e+02 - logprior: -1.2466e+00
Epoch 8/10
19/19 - 3s - loss: 250.7759 - loglik: -2.4881e+02 - logprior: -1.2403e+00
Epoch 9/10
19/19 - 3s - loss: 250.9439 - loglik: -2.4900e+02 - logprior: -1.2448e+00
Fitted a model with MAP estimate = -249.0415
expansions: [(0, 2), (17, 1), (20, 3), (21, 2), (22, 1), (23, 1), (38, 2), (41, 3), (42, 1), (44, 2), (53, 1), (54, 1), (74, 1), (75, 2), (76, 2), (77, 4), (78, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 115 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 262.1995 - loglik: -2.5709e+02 - logprior: -4.2607e+00
Epoch 2/2
19/19 - 3s - loss: 252.0337 - loglik: -2.4940e+02 - logprior: -1.4401e+00
Fitted a model with MAP estimate = -248.2048
expansions: []
discards: [  1  28  60  97 100]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 254.1816 - loglik: -2.4991e+02 - logprior: -2.9980e+00
Epoch 2/2
19/19 - 3s - loss: 250.5891 - loglik: -2.4834e+02 - logprior: -1.2490e+00
Fitted a model with MAP estimate = -247.7250
expansions: []
discards: [47]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 253.0123 - loglik: -2.4896e+02 - logprior: -2.9205e+00
Epoch 2/10
19/19 - 3s - loss: 249.8073 - loglik: -2.4762e+02 - logprior: -1.2279e+00
Epoch 3/10
19/19 - 3s - loss: 248.0879 - loglik: -2.4593e+02 - logprior: -1.0956e+00
Epoch 4/10
19/19 - 3s - loss: 247.3688 - loglik: -2.4521e+02 - logprior: -1.0427e+00
Epoch 5/10
19/19 - 3s - loss: 246.3474 - loglik: -2.4420e+02 - logprior: -1.0199e+00
Epoch 6/10
19/19 - 3s - loss: 244.8840 - loglik: -2.4275e+02 - logprior: -9.9093e-01
Epoch 7/10
19/19 - 3s - loss: 244.5737 - loglik: -2.4246e+02 - logprior: -9.7209e-01
Epoch 8/10
19/19 - 3s - loss: 243.7331 - loglik: -2.4167e+02 - logprior: -9.4898e-01
Epoch 9/10
19/19 - 3s - loss: 243.0090 - loglik: -2.4102e+02 - logprior: -9.3733e-01
Epoch 10/10
19/19 - 3s - loss: 242.5845 - loglik: -2.4065e+02 - logprior: -9.1469e-01
Fitted a model with MAP estimate = -241.1542
Time for alignment: 100.0201
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 301.6177 - loglik: -2.9845e+02 - logprior: -3.0834e+00
Epoch 2/10
19/19 - 3s - loss: 271.4940 - loglik: -2.6974e+02 - logprior: -1.3043e+00
Epoch 3/10
19/19 - 3s - loss: 257.4726 - loglik: -2.5549e+02 - logprior: -1.3581e+00
Epoch 4/10
19/19 - 3s - loss: 254.5545 - loglik: -2.5252e+02 - logprior: -1.2495e+00
Epoch 5/10
19/19 - 3s - loss: 253.1856 - loglik: -2.5117e+02 - logprior: -1.2279e+00
Epoch 6/10
19/19 - 3s - loss: 252.0356 - loglik: -2.5002e+02 - logprior: -1.2193e+00
Epoch 7/10
19/19 - 3s - loss: 251.2627 - loglik: -2.4926e+02 - logprior: -1.2137e+00
Epoch 8/10
19/19 - 3s - loss: 251.0156 - loglik: -2.4903e+02 - logprior: -1.2278e+00
Epoch 9/10
19/19 - 3s - loss: 250.5548 - loglik: -2.4863e+02 - logprior: -1.2149e+00
Epoch 10/10
19/19 - 3s - loss: 249.7002 - loglik: -2.4782e+02 - logprior: -1.2161e+00
Fitted a model with MAP estimate = -248.6906
expansions: [(0, 2), (7, 1), (20, 3), (21, 2), (22, 1), (23, 1), (38, 2), (41, 3), (42, 1), (44, 2), (53, 3), (75, 2), (76, 1), (77, 4), (78, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 113 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 263.9789 - loglik: -2.5888e+02 - logprior: -4.3099e+00
Epoch 2/2
19/19 - 3s - loss: 252.5238 - loglik: -2.4993e+02 - logprior: -1.4674e+00
Fitted a model with MAP estimate = -248.6176
expansions: []
discards: [ 1 28 60]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 254.4258 - loglik: -2.5014e+02 - logprior: -3.0418e+00
Epoch 2/2
19/19 - 3s - loss: 250.4897 - loglik: -2.4819e+02 - logprior: -1.2943e+00
Fitted a model with MAP estimate = -247.6440
expansions: [(52, 1), (103, 1)]
discards: [47]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 253.1531 - loglik: -2.4899e+02 - logprior: -3.0238e+00
Epoch 2/10
19/19 - 3s - loss: 249.2067 - loglik: -2.4701e+02 - logprior: -1.2278e+00
Epoch 3/10
19/19 - 3s - loss: 248.3299 - loglik: -2.4621e+02 - logprior: -1.0441e+00
Epoch 4/10
19/19 - 3s - loss: 246.3864 - loglik: -2.4426e+02 - logprior: -9.9170e-01
Epoch 5/10
19/19 - 3s - loss: 246.0743 - loglik: -2.4397e+02 - logprior: -9.5839e-01
Epoch 6/10
19/19 - 3s - loss: 244.9174 - loglik: -2.4281e+02 - logprior: -9.3973e-01
Epoch 7/10
19/19 - 3s - loss: 243.6146 - loglik: -2.4154e+02 - logprior: -9.1771e-01
Epoch 8/10
19/19 - 3s - loss: 243.2305 - loglik: -2.4120e+02 - logprior: -8.9857e-01
Epoch 9/10
19/19 - 3s - loss: 243.0321 - loglik: -2.4106e+02 - logprior: -8.8287e-01
Epoch 10/10
19/19 - 3s - loss: 241.9800 - loglik: -2.4004e+02 - logprior: -8.8169e-01
Fitted a model with MAP estimate = -240.6304
Time for alignment: 106.1819
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 301.7920 - loglik: -2.9861e+02 - logprior: -3.0918e+00
Epoch 2/10
19/19 - 3s - loss: 275.2659 - loglik: -2.7349e+02 - logprior: -1.2835e+00
Epoch 3/10
19/19 - 3s - loss: 259.6219 - loglik: -2.5740e+02 - logprior: -1.3416e+00
Epoch 4/10
19/19 - 3s - loss: 255.2417 - loglik: -2.5319e+02 - logprior: -1.2243e+00
Epoch 5/10
19/19 - 3s - loss: 253.2986 - loglik: -2.5133e+02 - logprior: -1.2234e+00
Epoch 6/10
19/19 - 3s - loss: 252.2725 - loglik: -2.5025e+02 - logprior: -1.2428e+00
Epoch 7/10
19/19 - 3s - loss: 251.7176 - loglik: -2.4970e+02 - logprior: -1.2402e+00
Epoch 8/10
19/19 - 3s - loss: 250.7120 - loglik: -2.4873e+02 - logprior: -1.2417e+00
Epoch 9/10
19/19 - 3s - loss: 250.5477 - loglik: -2.4858e+02 - logprior: -1.2474e+00
Epoch 10/10
19/19 - 3s - loss: 250.2714 - loglik: -2.4836e+02 - logprior: -1.2530e+00
Fitted a model with MAP estimate = -248.8926
expansions: [(0, 2), (17, 1), (20, 3), (21, 2), (22, 1), (39, 2), (41, 3), (42, 1), (44, 1), (51, 1), (52, 1), (74, 1), (75, 4), (76, 2), (77, 2), (78, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 263.6054 - loglik: -2.5849e+02 - logprior: -4.3186e+00
Epoch 2/2
19/19 - 3s - loss: 252.6838 - loglik: -2.5007e+02 - logprior: -1.4584e+00
Fitted a model with MAP estimate = -248.7531
expansions: []
discards: [  1  28 102 104]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 254.6857 - loglik: -2.5041e+02 - logprior: -3.0303e+00
Epoch 2/2
19/19 - 3s - loss: 250.6610 - loglik: -2.4840e+02 - logprior: -1.2717e+00
Fitted a model with MAP estimate = -247.7558
expansions: []
discards: [47]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 253.2547 - loglik: -2.4909e+02 - logprior: -3.0287e+00
Epoch 2/10
19/19 - 3s - loss: 249.6204 - loglik: -2.4744e+02 - logprior: -1.2120e+00
Epoch 3/10
19/19 - 3s - loss: 248.4736 - loglik: -2.4638e+02 - logprior: -1.0314e+00
Epoch 4/10
19/19 - 3s - loss: 247.1751 - loglik: -2.4509e+02 - logprior: -9.7956e-01
Epoch 5/10
19/19 - 3s - loss: 246.1780 - loglik: -2.4411e+02 - logprior: -9.5025e-01
Epoch 6/10
19/19 - 3s - loss: 245.1442 - loglik: -2.4310e+02 - logprior: -9.3115e-01
Epoch 7/10
19/19 - 3s - loss: 245.1006 - loglik: -2.4308e+02 - logprior: -9.0775e-01
Epoch 8/10
19/19 - 3s - loss: 243.5720 - loglik: -2.4162e+02 - logprior: -8.9169e-01
Epoch 9/10
19/19 - 3s - loss: 243.0164 - loglik: -2.4112e+02 - logprior: -8.8293e-01
Epoch 10/10
19/19 - 3s - loss: 242.9841 - loglik: -2.4116e+02 - logprior: -8.6856e-01
Fitted a model with MAP estimate = -241.5515
Time for alignment: 102.8928
Computed alignments with likelihoods: ['-241.1542', '-240.6304', '-241.5515']
Best model has likelihood: -240.6304
time for generating output: 0.1682
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07686.projection.fasta
SP score = 0.8680020603878854
Training of 3 independent models on file PF00505.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2f547e460>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff2f5119100>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6b0c70670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6b0c70a30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fed481326d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fed28f05a00>, <__main__.SimpleDirichletPrior object at 0x7fed10069760>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff31c1dbca0>

Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 186.4881 - loglik: -1.8324e+02 - logprior: -3.2372e+00
Epoch 2/10
19/19 - 2s - loss: 153.6038 - loglik: -1.5182e+02 - logprior: -1.7123e+00
Epoch 3/10
19/19 - 2s - loss: 141.2878 - loglik: -1.3931e+02 - logprior: -1.6685e+00
Epoch 4/10
19/19 - 2s - loss: 138.4224 - loglik: -1.3642e+02 - logprior: -1.7129e+00
Epoch 5/10
19/19 - 2s - loss: 137.2484 - loglik: -1.3535e+02 - logprior: -1.6684e+00
Epoch 6/10
19/19 - 2s - loss: 136.7451 - loglik: -1.3488e+02 - logprior: -1.6518e+00
Epoch 7/10
19/19 - 2s - loss: 136.4653 - loglik: -1.3464e+02 - logprior: -1.6397e+00
Epoch 8/10
19/19 - 2s - loss: 136.1423 - loglik: -1.3433e+02 - logprior: -1.6301e+00
Epoch 9/10
19/19 - 2s - loss: 136.0726 - loglik: -1.3426e+02 - logprior: -1.6408e+00
Epoch 10/10
19/19 - 1s - loss: 135.0661 - loglik: -1.3324e+02 - logprior: -1.6635e+00
Fitted a model with MAP estimate = -134.8536
expansions: [(14, 1), (16, 1), (17, 1), (18, 1), (21, 2), (22, 2), (23, 1), (24, 1), (34, 1), (38, 1), (40, 1), (43, 1), (44, 1), (46, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 136.7694 - loglik: -1.3309e+02 - logprior: -3.4247e+00
Epoch 2/2
19/19 - 2s - loss: 127.9606 - loglik: -1.2608e+02 - logprior: -1.5065e+00
Fitted a model with MAP estimate = -126.4185
expansions: []
discards: [26 28]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 130.5931 - loglik: -1.2696e+02 - logprior: -3.2911e+00
Epoch 2/2
19/19 - 2s - loss: 127.1585 - loglik: -1.2542e+02 - logprior: -1.4186e+00
Fitted a model with MAP estimate = -125.9289
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 130.3494 - loglik: -1.2675e+02 - logprior: -3.2564e+00
Epoch 2/10
19/19 - 2s - loss: 126.9854 - loglik: -1.2526e+02 - logprior: -1.3925e+00
Epoch 3/10
19/19 - 2s - loss: 126.2287 - loglik: -1.2455e+02 - logprior: -1.2619e+00
Epoch 4/10
19/19 - 2s - loss: 125.1179 - loglik: -1.2351e+02 - logprior: -1.2287e+00
Epoch 5/10
19/19 - 2s - loss: 124.9415 - loglik: -1.2341e+02 - logprior: -1.1983e+00
Epoch 6/10
19/19 - 2s - loss: 124.3662 - loglik: -1.2288e+02 - logprior: -1.1845e+00
Epoch 7/10
19/19 - 2s - loss: 124.2592 - loglik: -1.2284e+02 - logprior: -1.1666e+00
Epoch 8/10
19/19 - 2s - loss: 124.1600 - loglik: -1.2278e+02 - logprior: -1.1407e+00
Epoch 9/10
19/19 - 2s - loss: 124.3519 - loglik: -1.2300e+02 - logprior: -1.1262e+00
Fitted a model with MAP estimate = -123.7351
Time for alignment: 63.3518
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 186.5304 - loglik: -1.8328e+02 - logprior: -3.2382e+00
Epoch 2/10
19/19 - 2s - loss: 155.4401 - loglik: -1.5369e+02 - logprior: -1.6756e+00
Epoch 3/10
19/19 - 2s - loss: 142.4634 - loglik: -1.4044e+02 - logprior: -1.6698e+00
Epoch 4/10
19/19 - 2s - loss: 138.2121 - loglik: -1.3611e+02 - logprior: -1.7518e+00
Epoch 5/10
19/19 - 2s - loss: 137.1892 - loglik: -1.3522e+02 - logprior: -1.7124e+00
Epoch 6/10
19/19 - 1s - loss: 136.0615 - loglik: -1.3412e+02 - logprior: -1.7053e+00
Epoch 7/10
19/19 - 1s - loss: 135.8332 - loglik: -1.3393e+02 - logprior: -1.6941e+00
Epoch 8/10
19/19 - 2s - loss: 135.3258 - loglik: -1.3346e+02 - logprior: -1.6848e+00
Epoch 9/10
19/19 - 1s - loss: 135.4866 - loglik: -1.3362e+02 - logprior: -1.6843e+00
Fitted a model with MAP estimate = -134.9265
expansions: [(14, 1), (16, 1), (17, 1), (21, 2), (22, 1), (23, 1), (24, 1), (26, 1), (34, 1), (41, 1), (42, 1), (43, 1), (44, 1), (48, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 136.3774 - loglik: -1.3271e+02 - logprior: -3.3930e+00
Epoch 2/2
19/19 - 2s - loss: 127.9372 - loglik: -1.2607e+02 - logprior: -1.4792e+00
Fitted a model with MAP estimate = -126.3605
expansions: []
discards: [24]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 130.7410 - loglik: -1.2711e+02 - logprior: -3.2872e+00
Epoch 2/2
19/19 - 2s - loss: 127.2448 - loglik: -1.2549e+02 - logprior: -1.4201e+00
Fitted a model with MAP estimate = -125.8699
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 130.3944 - loglik: -1.2678e+02 - logprior: -3.2612e+00
Epoch 2/10
19/19 - 2s - loss: 126.9864 - loglik: -1.2525e+02 - logprior: -1.3925e+00
Epoch 3/10
19/19 - 2s - loss: 126.0926 - loglik: -1.2442e+02 - logprior: -1.2677e+00
Epoch 4/10
19/19 - 2s - loss: 125.1166 - loglik: -1.2349e+02 - logprior: -1.2322e+00
Epoch 5/10
19/19 - 2s - loss: 125.0322 - loglik: -1.2348e+02 - logprior: -1.2100e+00
Epoch 6/10
19/19 - 2s - loss: 124.4169 - loglik: -1.2292e+02 - logprior: -1.1868e+00
Epoch 7/10
19/19 - 2s - loss: 124.3709 - loglik: -1.2294e+02 - logprior: -1.1667e+00
Epoch 8/10
19/19 - 2s - loss: 124.0674 - loglik: -1.2268e+02 - logprior: -1.1473e+00
Epoch 9/10
19/19 - 2s - loss: 124.2074 - loglik: -1.2285e+02 - logprior: -1.1312e+00
Fitted a model with MAP estimate = -123.7429
Time for alignment: 60.8187
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 186.4093 - loglik: -1.8316e+02 - logprior: -3.2400e+00
Epoch 2/10
19/19 - 2s - loss: 152.5812 - loglik: -1.5082e+02 - logprior: -1.7050e+00
Epoch 3/10
19/19 - 1s - loss: 140.1496 - loglik: -1.3819e+02 - logprior: -1.6791e+00
Epoch 4/10
19/19 - 2s - loss: 137.8742 - loglik: -1.3586e+02 - logprior: -1.7253e+00
Epoch 5/10
19/19 - 1s - loss: 137.0158 - loglik: -1.3511e+02 - logprior: -1.6841e+00
Epoch 6/10
19/19 - 2s - loss: 136.4794 - loglik: -1.3462e+02 - logprior: -1.6666e+00
Epoch 7/10
19/19 - 2s - loss: 136.1740 - loglik: -1.3434e+02 - logprior: -1.6569e+00
Epoch 8/10
19/19 - 2s - loss: 136.3493 - loglik: -1.3454e+02 - logprior: -1.6440e+00
Fitted a model with MAP estimate = -135.8284
expansions: [(14, 1), (16, 1), (17, 1), (18, 1), (21, 2), (22, 2), (23, 1), (34, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 136.0807 - loglik: -1.3245e+02 - logprior: -3.3924e+00
Epoch 2/2
19/19 - 2s - loss: 128.0139 - loglik: -1.2622e+02 - logprior: -1.4722e+00
Fitted a model with MAP estimate = -126.4571
expansions: []
discards: [28]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 130.6906 - loglik: -1.2709e+02 - logprior: -3.2858e+00
Epoch 2/2
19/19 - 2s - loss: 127.1872 - loglik: -1.2546e+02 - logprior: -1.4147e+00
Fitted a model with MAP estimate = -125.9216
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 130.2785 - loglik: -1.2668e+02 - logprior: -3.2641e+00
Epoch 2/10
19/19 - 2s - loss: 127.1907 - loglik: -1.2548e+02 - logprior: -1.3869e+00
Epoch 3/10
19/19 - 2s - loss: 126.0164 - loglik: -1.2435e+02 - logprior: -1.2623e+00
Epoch 4/10
19/19 - 2s - loss: 125.2538 - loglik: -1.2363e+02 - logprior: -1.2362e+00
Epoch 5/10
19/19 - 2s - loss: 124.8245 - loglik: -1.2327e+02 - logprior: -1.2100e+00
Epoch 6/10
19/19 - 2s - loss: 124.5332 - loglik: -1.2306e+02 - logprior: -1.1858e+00
Epoch 7/10
19/19 - 2s - loss: 124.2954 - loglik: -1.2286e+02 - logprior: -1.1675e+00
Epoch 8/10
19/19 - 2s - loss: 124.0881 - loglik: -1.2270e+02 - logprior: -1.1489e+00
Epoch 9/10
19/19 - 2s - loss: 124.3242 - loglik: -1.2296e+02 - logprior: -1.1366e+00
Fitted a model with MAP estimate = -123.7143
Time for alignment: 58.4030
Computed alignments with likelihoods: ['-123.7351', '-123.7429', '-123.7143']
Best model has likelihood: -123.7143
time for generating output: 0.1521
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00505.projection.fasta
SP score = 0.9338945283486011
Training of 3 independent models on file PF13393.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff51a5cda60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff31c625340>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff517d307f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff51a3c4790>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff52cd599d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fed20b88520>, <__main__.SimpleDirichletPrior object at 0x7fed11091190>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff2f473a430>

Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 31s - loss: 729.1924 - loglik: -7.2694e+02 - logprior: -1.8191e+00
Epoch 2/10
39/39 - 28s - loss: 612.8209 - loglik: -6.0965e+02 - logprior: -1.9701e+00
Epoch 3/10
39/39 - 29s - loss: 598.7281 - loglik: -5.9539e+02 - logprior: -2.1130e+00
Epoch 4/10
39/39 - 29s - loss: 596.1675 - loglik: -5.9302e+02 - logprior: -2.0901e+00
Epoch 5/10
39/39 - 29s - loss: 594.1439 - loglik: -5.9112e+02 - logprior: -2.0773e+00
Epoch 6/10
39/39 - 29s - loss: 593.3623 - loglik: -5.9040e+02 - logprior: -2.0805e+00
Epoch 7/10
39/39 - 30s - loss: 592.9117 - loglik: -5.9000e+02 - logprior: -2.0968e+00
Epoch 8/10
39/39 - 29s - loss: 591.9392 - loglik: -5.8908e+02 - logprior: -2.1010e+00
Epoch 9/10
39/39 - 28s - loss: 592.1567 - loglik: -5.8934e+02 - logprior: -2.1028e+00
Fitted a model with MAP estimate = -590.0390
expansions: [(10, 1), (11, 1), (12, 2), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (33, 1), (42, 1), (46, 1), (47, 1), (52, 1), (53, 1), (67, 1), (68, 1), (69, 1), (71, 1), (77, 1), (78, 1), (102, 1), (115, 1), (116, 1), (117, 1), (121, 5), (133, 1), (135, 1), (137, 3), (138, 1), (150, 1), (152, 1), (163, 1), (164, 2), (175, 1), (177, 1), (180, 1), (183, 1), (185, 1), (192, 2), (193, 1), (194, 1), (197, 1), (211, 1), (212, 4), (213, 3), (227, 1), (230, 1), (231, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 304 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 43s - loss: 588.7623 - loglik: -5.8466e+02 - logprior: -2.9258e+00
Epoch 2/2
39/39 - 39s - loss: 570.7596 - loglik: -5.6732e+02 - logprior: -1.8026e+00
Fitted a model with MAP estimate = -564.6346
expansions: [(0, 3), (148, 1), (214, 1)]
discards: [  0 168 238 268]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 305 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 44s - loss: 573.0697 - loglik: -5.6925e+02 - logprior: -1.8650e+00
Epoch 2/2
39/39 - 42s - loss: 565.6453 - loglik: -5.6285e+02 - logprior: -8.5411e-01
Fitted a model with MAP estimate = -560.3472
expansions: [(46, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 304 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 45s - loss: 573.5899 - loglik: -5.6897e+02 - logprior: -2.5022e+00
Epoch 2/10
39/39 - 43s - loss: 565.5164 - loglik: -5.6276e+02 - logprior: -7.6169e-01
Epoch 3/10
39/39 - 43s - loss: 561.7736 - loglik: -5.5962e+02 - logprior: -3.8061e-01
Epoch 4/10
39/39 - 44s - loss: 559.6183 - loglik: -5.5785e+02 - logprior: -2.1855e-01
Epoch 5/10
39/39 - 44s - loss: 558.4510 - loglik: -5.5703e+02 - logprior: -9.5706e-02
Epoch 6/10
39/39 - 40s - loss: 558.1526 - loglik: -5.5698e+02 - logprior: 0.0085
Epoch 7/10
39/39 - 41s - loss: 557.1782 - loglik: -5.5629e+02 - logprior: 0.1857
Epoch 8/10
39/39 - 40s - loss: 556.8193 - loglik: -5.5614e+02 - logprior: 0.3314
Epoch 9/10
39/39 - 39s - loss: 556.6227 - loglik: -5.5619e+02 - logprior: 0.5056
Epoch 10/10
39/39 - 41s - loss: 556.0446 - loglik: -5.5578e+02 - logprior: 0.6347
Fitted a model with MAP estimate = -554.5162
Time for alignment: 1034.2308
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 31s - loss: 730.8058 - loglik: -7.2853e+02 - logprior: -1.8171e+00
Epoch 2/10
39/39 - 28s - loss: 614.2807 - loglik: -6.1135e+02 - logprior: -1.9360e+00
Epoch 3/10
39/39 - 29s - loss: 598.6374 - loglik: -5.9537e+02 - logprior: -2.1052e+00
Epoch 4/10
39/39 - 30s - loss: 596.2656 - loglik: -5.9315e+02 - logprior: -2.0733e+00
Epoch 5/10
39/39 - 30s - loss: 594.4022 - loglik: -5.9134e+02 - logprior: -2.0935e+00
Epoch 6/10
39/39 - 28s - loss: 593.2902 - loglik: -5.9024e+02 - logprior: -2.1136e+00
Epoch 7/10
39/39 - 26s - loss: 592.8466 - loglik: -5.8986e+02 - logprior: -2.1180e+00
Epoch 8/10
39/39 - 25s - loss: 592.6890 - loglik: -5.8974e+02 - logprior: -2.1298e+00
Epoch 9/10
39/39 - 25s - loss: 591.6803 - loglik: -5.8877e+02 - logprior: -2.1276e+00
Epoch 10/10
39/39 - 25s - loss: 591.9297 - loglik: -5.8907e+02 - logprior: -2.1308e+00
Fitted a model with MAP estimate = -590.0079
expansions: [(10, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (35, 1), (37, 1), (42, 2), (46, 1), (47, 1), (52, 1), (53, 1), (67, 1), (68, 1), (69, 1), (71, 1), (77, 1), (78, 1), (102, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (128, 1), (132, 1), (134, 1), (136, 2), (147, 1), (154, 2), (155, 6), (163, 2), (164, 3), (174, 1), (175, 1), (177, 1), (180, 1), (183, 1), (185, 1), (192, 3), (193, 1), (196, 1), (211, 1), (212, 3), (213, 3), (227, 1), (230, 1), (231, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 312 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 39s - loss: 588.3112 - loglik: -5.8415e+02 - logprior: -2.9693e+00
Epoch 2/2
39/39 - 36s - loss: 566.8256 - loglik: -5.6313e+02 - logprior: -1.9699e+00
Fitted a model with MAP estimate = -560.3293
expansions: [(0, 3), (148, 1), (195, 1), (207, 1)]
discards: [  0  50 276]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 315 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 567.9757 - loglik: -5.6394e+02 - logprior: -1.9221e+00
Epoch 2/2
39/39 - 38s - loss: 560.8037 - loglik: -5.5782e+02 - logprior: -8.9415e-01
Fitted a model with MAP estimate = -554.9444
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 313 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 39s - loss: 569.1260 - loglik: -5.6429e+02 - logprior: -2.5248e+00
Epoch 2/10
39/39 - 36s - loss: 560.6349 - loglik: -5.5767e+02 - logprior: -8.1630e-01
Epoch 3/10
39/39 - 37s - loss: 557.0279 - loglik: -5.5476e+02 - logprior: -4.1357e-01
Epoch 4/10
39/39 - 37s - loss: 554.8889 - loglik: -5.5301e+02 - logprior: -2.6815e-01
Epoch 5/10
39/39 - 37s - loss: 554.1373 - loglik: -5.5266e+02 - logprior: -1.0244e-01
Epoch 6/10
39/39 - 36s - loss: 553.2001 - loglik: -5.5201e+02 - logprior: 0.0092
Epoch 7/10
39/39 - 37s - loss: 552.3328 - loglik: -5.5143e+02 - logprior: 0.1883
Epoch 8/10
39/39 - 36s - loss: 551.7885 - loglik: -5.5112e+02 - logprior: 0.3408
Epoch 9/10
39/39 - 35s - loss: 551.8159 - loglik: -5.5138e+02 - logprior: 0.4994
Fitted a model with MAP estimate = -550.1615
Time for alignment: 921.8539
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 730.7532 - loglik: -7.2849e+02 - logprior: -1.8153e+00
Epoch 2/10
39/39 - 24s - loss: 616.3506 - loglik: -6.1341e+02 - logprior: -1.8934e+00
Epoch 3/10
39/39 - 24s - loss: 600.9083 - loglik: -5.9758e+02 - logprior: -2.1530e+00
Epoch 4/10
39/39 - 24s - loss: 598.2353 - loglik: -5.9507e+02 - logprior: -2.1179e+00
Epoch 5/10
39/39 - 24s - loss: 596.0042 - loglik: -5.9297e+02 - logprior: -2.1215e+00
Epoch 6/10
39/39 - 25s - loss: 595.5710 - loglik: -5.9261e+02 - logprior: -2.1151e+00
Epoch 7/10
39/39 - 25s - loss: 595.2858 - loglik: -5.9234e+02 - logprior: -2.1240e+00
Epoch 8/10
39/39 - 25s - loss: 594.6550 - loglik: -5.9174e+02 - logprior: -2.1231e+00
Epoch 9/10
39/39 - 25s - loss: 594.4329 - loglik: -5.9155e+02 - logprior: -2.1307e+00
Epoch 10/10
39/39 - 25s - loss: 593.6959 - loglik: -5.9085e+02 - logprior: -2.1278e+00
Fitted a model with MAP estimate = -592.2684
expansions: [(8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (29, 1), (41, 1), (45, 1), (46, 1), (51, 1), (64, 1), (68, 1), (69, 2), (70, 2), (76, 1), (77, 1), (112, 1), (114, 1), (115, 1), (120, 4), (121, 1), (123, 1), (132, 1), (135, 1), (138, 4), (147, 1), (149, 1), (154, 1), (163, 1), (164, 3), (174, 2), (175, 1), (179, 1), (184, 1), (185, 2), (192, 2), (193, 1), (194, 1), (197, 1), (211, 1), (212, 4), (213, 3), (227, 1), (230, 1), (231, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 310 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 39s - loss: 590.4520 - loglik: -5.8628e+02 - logprior: -2.9594e+00
Epoch 2/2
39/39 - 35s - loss: 568.8790 - loglik: -5.6526e+02 - logprior: -1.9558e+00
Fitted a model with MAP estimate = -562.2017
expansions: [(0, 3), (149, 1)]
discards: [  0  86 232 274]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 310 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 39s - loss: 570.2211 - loglik: -5.6640e+02 - logprior: -1.8977e+00
Epoch 2/2
39/39 - 34s - loss: 563.1255 - loglik: -5.6031e+02 - logprior: -8.8477e-01
Fitted a model with MAP estimate = -557.9253
expansions: []
discards: [  0   1 175 176 245]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 305 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 572.9764 - loglik: -5.6834e+02 - logprior: -2.5127e+00
Epoch 2/10
39/39 - 33s - loss: 564.4644 - loglik: -5.6169e+02 - logprior: -7.8560e-01
Epoch 3/10
39/39 - 33s - loss: 560.6516 - loglik: -5.5850e+02 - logprior: -4.1648e-01
Epoch 4/10
39/39 - 34s - loss: 558.9767 - loglik: -5.5721e+02 - logprior: -2.7122e-01
Epoch 5/10
39/39 - 35s - loss: 558.1714 - loglik: -5.5678e+02 - logprior: -1.1704e-01
Epoch 6/10
39/39 - 36s - loss: 556.3323 - loglik: -5.5521e+02 - logprior: 0.0228
Epoch 7/10
39/39 - 37s - loss: 556.8177 - loglik: -5.5598e+02 - logprior: 0.2174
Fitted a model with MAP estimate = -554.6896
Time for alignment: 792.8915
Computed alignments with likelihoods: ['-554.5162', '-550.1615', '-554.6896']
Best model has likelihood: -550.1615
time for generating output: 0.4114
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13393.projection.fasta
SP score = 0.3466839792249301
Training of 3 independent models on file PF00202.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2fcce6ee0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fed1119c6d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fed082b8430>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff360104a90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff51aaadbb0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff6605b4c40>, <__main__.SimpleDirichletPrior object at 0x7fed29673af0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7fed281b2430>

Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 53s - loss: 970.9244 - loglik: -9.6893e+02 - logprior: -1.5583e+00
Epoch 2/10
39/39 - 51s - loss: 808.4873 - loglik: -8.0593e+02 - logprior: -1.6064e+00
Epoch 3/10
39/39 - 54s - loss: 792.7621 - loglik: -7.8938e+02 - logprior: -1.8434e+00
Epoch 4/10
39/39 - 57s - loss: 788.5471 - loglik: -7.8504e+02 - logprior: -1.8265e+00
Epoch 5/10
39/39 - 58s - loss: 787.2312 - loglik: -7.8373e+02 - logprior: -1.8673e+00
Epoch 6/10
39/39 - 55s - loss: 785.9410 - loglik: -7.8254e+02 - logprior: -1.9008e+00
Epoch 7/10
39/39 - 54s - loss: 785.0944 - loglik: -7.8181e+02 - logprior: -1.9088e+00
Epoch 8/10
39/39 - 54s - loss: 784.8647 - loglik: -7.8168e+02 - logprior: -1.9349e+00
Epoch 9/10
39/39 - 54s - loss: 784.1645 - loglik: -7.8107e+02 - logprior: -1.9374e+00
Epoch 10/10
39/39 - 54s - loss: 783.3844 - loglik: -7.8032e+02 - logprior: -1.9718e+00
Fitted a model with MAP estimate = -773.9105
expansions: [(0, 3), (15, 1), (51, 1), (52, 1), (53, 1), (57, 2), (62, 1), (70, 1), (74, 2), (100, 1), (121, 2), (122, 2), (123, 1), (132, 1), (147, 3), (148, 2), (167, 1), (174, 1), (176, 1), (177, 1), (178, 1), (193, 2), (195, 1), (196, 1), (218, 1), (219, 1), (220, 1), (222, 1), (223, 1), (224, 3), (245, 1), (247, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (258, 1), (259, 1), (263, 1), (264, 1), (266, 2), (285, 7), (286, 1), (288, 1), (289, 1), (297, 4), (299, 1), (309, 1), (313, 2), (314, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 396 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 85s - loss: 774.9156 - loglik: -7.7045e+02 - logprior: -2.7972e+00
Epoch 2/2
39/39 - 80s - loss: 750.8002 - loglik: -7.4726e+02 - logprior: -1.2844e+00
Fitted a model with MAP estimate = -737.2048
expansions: [(347, 2), (348, 2)]
discards: [  2  65  85 136 170 320 390]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 393 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 69s - loss: 753.6321 - loglik: -7.4888e+02 - logprior: -1.8682e+00
Epoch 2/2
39/39 - 62s - loss: 748.3545 - loglik: -7.4482e+02 - logprior: -6.9580e-01
Fitted a model with MAP estimate = -735.6848
expansions: []
discards: [334 335 336]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 390 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 66s - loss: 745.6279 - loglik: -7.4078e+02 - logprior: -1.6079e+00
Epoch 2/10
39/39 - 63s - loss: 740.5148 - loglik: -7.3731e+02 - logprior: -3.6530e-01
Epoch 3/10
39/39 - 66s - loss: 739.1912 - loglik: -7.3651e+02 - logprior: -1.5320e-01
Epoch 4/10
39/39 - 71s - loss: 737.6148 - loglik: -7.3537e+02 - logprior: -4.7497e-02
Epoch 5/10
39/39 - 74s - loss: 736.0930 - loglik: -7.3435e+02 - logprior: 0.1491
Epoch 6/10
39/39 - 73s - loss: 734.9051 - loglik: -7.3350e+02 - logprior: 0.3214
Epoch 7/10
39/39 - 73s - loss: 735.3520 - loglik: -7.3427e+02 - logprior: 0.4884
Fitted a model with MAP estimate = -732.0117
Time for alignment: 1689.4413
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 55s - loss: 971.9752 - loglik: -9.6999e+02 - logprior: -1.5746e+00
Epoch 2/10
39/39 - 52s - loss: 803.7789 - loglik: -8.0082e+02 - logprior: -1.6851e+00
Epoch 3/10
39/39 - 56s - loss: 789.9432 - loglik: -7.8639e+02 - logprior: -1.7187e+00
Epoch 4/10
39/39 - 58s - loss: 786.4479 - loglik: -7.8295e+02 - logprior: -1.6806e+00
Epoch 5/10
39/39 - 57s - loss: 785.3423 - loglik: -7.8193e+02 - logprior: -1.7305e+00
Epoch 6/10
39/39 - 56s - loss: 783.6756 - loglik: -7.8039e+02 - logprior: -1.7380e+00
Epoch 7/10
39/39 - 56s - loss: 783.2217 - loglik: -7.8003e+02 - logprior: -1.7683e+00
Epoch 8/10
39/39 - 56s - loss: 782.2404 - loglik: -7.7917e+02 - logprior: -1.7646e+00
Epoch 9/10
39/39 - 54s - loss: 782.4379 - loglik: -7.7943e+02 - logprior: -1.7960e+00
Fitted a model with MAP estimate = -772.4846
expansions: [(0, 3), (42, 1), (51, 2), (52, 1), (56, 1), (63, 1), (69, 1), (70, 1), (74, 2), (98, 1), (119, 2), (120, 1), (122, 1), (131, 1), (146, 3), (147, 2), (170, 1), (173, 1), (175, 1), (176, 1), (188, 1), (192, 2), (195, 1), (196, 1), (199, 1), (219, 1), (220, 1), (221, 1), (222, 2), (223, 1), (225, 3), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (257, 1), (258, 1), (259, 1), (263, 1), (264, 1), (285, 7), (286, 3), (287, 1), (288, 1), (296, 4), (309, 1), (313, 2), (314, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 394 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 78s - loss: 774.5468 - loglik: -7.6985e+02 - logprior: -2.8951e+00
Epoch 2/2
39/39 - 75s - loss: 752.2726 - loglik: -7.4858e+02 - logprior: -1.2873e+00
Fitted a model with MAP estimate = -738.0035
expansions: [(348, 1)]
discards: [  1  85 134 166 221 265 387]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 388 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 82s - loss: 754.2067 - loglik: -7.4943e+02 - logprior: -1.7415e+00
Epoch 2/2
39/39 - 78s - loss: 749.2819 - loglik: -7.4567e+02 - logprior: -6.1776e-01
Fitted a model with MAP estimate = -736.4885
expansions: [(0, 2), (339, 1)]
discards: [332]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 390 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 76s - loss: 746.7270 - loglik: -7.4077e+02 - logprior: -2.5434e+00
Epoch 2/10
39/39 - 75s - loss: 741.3866 - loglik: -7.3800e+02 - logprior: -3.8704e-01
Epoch 3/10
39/39 - 76s - loss: 738.3461 - loglik: -7.3550e+02 - logprior: -1.7306e-01
Epoch 4/10
39/39 - 81s - loss: 737.9335 - loglik: -7.3562e+02 - logprior: -1.5649e-02
Epoch 5/10
39/39 - 84s - loss: 735.2195 - loglik: -7.3335e+02 - logprior: 0.1211
Epoch 6/10
39/39 - 77s - loss: 735.3922 - loglik: -7.3392e+02 - logprior: 0.3052
Fitted a model with MAP estimate = -732.4660
Time for alignment: 1677.4617
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 53s - loss: 971.7600 - loglik: -9.6976e+02 - logprior: -1.5712e+00
Epoch 2/10
39/39 - 49s - loss: 806.9697 - loglik: -8.0399e+02 - logprior: -1.6673e+00
Epoch 3/10
39/39 - 48s - loss: 793.2764 - loglik: -7.8979e+02 - logprior: -1.7259e+00
Epoch 4/10
39/39 - 47s - loss: 789.8299 - loglik: -7.8646e+02 - logprior: -1.6996e+00
Epoch 5/10
39/39 - 47s - loss: 788.4649 - loglik: -7.8524e+02 - logprior: -1.7145e+00
Epoch 6/10
39/39 - 47s - loss: 787.4545 - loglik: -7.8433e+02 - logprior: -1.7413e+00
Epoch 7/10
39/39 - 48s - loss: 786.5114 - loglik: -7.8343e+02 - logprior: -1.7902e+00
Epoch 8/10
39/39 - 48s - loss: 785.8071 - loglik: -7.8275e+02 - logprior: -1.8570e+00
Epoch 9/10
39/39 - 49s - loss: 785.4384 - loglik: -7.8249e+02 - logprior: -1.8235e+00
Epoch 10/10
39/39 - 48s - loss: 785.3943 - loglik: -7.8240e+02 - logprior: -1.9589e+00
Fitted a model with MAP estimate = -775.6389
expansions: [(0, 3), (23, 1), (50, 1), (51, 1), (52, 1), (56, 2), (61, 1), (69, 1), (70, 1), (72, 2), (74, 2), (103, 1), (118, 2), (119, 1), (121, 1), (145, 4), (146, 1), (147, 2), (165, 1), (172, 1), (174, 1), (175, 1), (176, 1), (191, 2), (194, 1), (195, 1), (218, 1), (219, 1), (220, 1), (222, 2), (223, 3), (247, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (257, 1), (258, 3), (263, 1), (266, 2), (285, 7), (286, 3), (287, 1), (288, 1), (296, 1), (297, 1), (298, 2), (299, 1), (309, 1), (313, 2), (314, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 400 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 71s - loss: 773.3400 - loglik: -7.6886e+02 - logprior: -2.8425e+00
Epoch 2/2
39/39 - 66s - loss: 749.6103 - loglik: -7.4613e+02 - logprior: -1.2598e+00
Fitted a model with MAP estimate = -736.3647
expansions: [(351, 2)]
discards: [  1  64  88 136 168 171 224 265 322 370 393]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 391 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 67s - loss: 753.2769 - loglik: -7.4867e+02 - logprior: -1.7677e+00
Epoch 2/2
39/39 - 67s - loss: 748.2331 - loglik: -7.4481e+02 - logprior: -6.2622e-01
Fitted a model with MAP estimate = -735.9371
expansions: [(0, 2)]
discards: [333 335 343]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 390 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 67s - loss: 746.3179 - loglik: -7.4074e+02 - logprior: -2.3791e+00
Epoch 2/10
39/39 - 63s - loss: 740.6812 - loglik: -7.3744e+02 - logprior: -4.0109e-01
Epoch 3/10
39/39 - 65s - loss: 738.6597 - loglik: -7.3592e+02 - logprior: -2.1498e-01
Epoch 4/10
39/39 - 66s - loss: 737.0927 - loglik: -7.3486e+02 - logprior: -5.6016e-02
Epoch 5/10
39/39 - 68s - loss: 736.1190 - loglik: -7.3433e+02 - logprior: 0.1205
Epoch 6/10
39/39 - 77s - loss: 734.9823 - loglik: -7.3357e+02 - logprior: 0.2957
Epoch 7/10
39/39 - 87s - loss: 735.1566 - loglik: -7.3408e+02 - logprior: 0.4675
Fitted a model with MAP estimate = -732.2052
Time for alignment: 1575.4336
Computed alignments with likelihoods: ['-732.0117', '-732.4660', '-732.2052']
Best model has likelihood: -732.0117
time for generating output: 1.8196
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00202.projection.fasta
SP score = 0.3325478132205643
Training of 3 independent models on file PF00018.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fed21a142b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6c17725b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fed28636940>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff5179a17f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff517a07700>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fed285ed250>, <__main__.SimpleDirichletPrior object at 0x7fed28b19ac0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff36073e0d0>

Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.7125 - loglik: -1.3046e+02 - logprior: -3.2440e+00
Epoch 2/10
19/19 - 1s - loss: 109.0296 - loglik: -1.0753e+02 - logprior: -1.4201e+00
Epoch 3/10
19/19 - 1s - loss: 101.0045 - loglik: -9.9224e+01 - logprior: -1.5988e+00
Epoch 4/10
19/19 - 1s - loss: 98.7569 - loglik: -9.7094e+01 - logprior: -1.4906e+00
Epoch 5/10
19/19 - 1s - loss: 98.2848 - loglik: -9.6690e+01 - logprior: -1.4765e+00
Epoch 6/10
19/19 - 1s - loss: 98.0589 - loglik: -9.6484e+01 - logprior: -1.4603e+00
Epoch 7/10
19/19 - 1s - loss: 97.8051 - loglik: -9.6248e+01 - logprior: -1.4430e+00
Epoch 8/10
19/19 - 1s - loss: 97.8821 - loglik: -9.6332e+01 - logprior: -1.4332e+00
Fitted a model with MAP estimate = -97.6131
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.2064 - loglik: -9.7820e+01 - logprior: -4.2335e+00
Epoch 2/2
19/19 - 1s - loss: 95.1640 - loglik: -9.2833e+01 - logprior: -2.1516e+00
Fitted a model with MAP estimate = -93.4696
expansions: [(0, 2)]
discards: [ 0  9 12 29 39 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 95.4344 - loglik: -9.2127e+01 - logprior: -3.1290e+00
Epoch 2/2
19/19 - 1s - loss: 92.5044 - loglik: -9.0998e+01 - logprior: -1.3252e+00
Fitted a model with MAP estimate = -91.8245
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 96.7094 - loglik: -9.2926e+01 - logprior: -3.6002e+00
Epoch 2/10
19/19 - 1s - loss: 93.2304 - loglik: -9.1555e+01 - logprior: -1.5001e+00
Epoch 3/10
19/19 - 1s - loss: 92.5996 - loglik: -9.0987e+01 - logprior: -1.4194e+00
Epoch 4/10
19/19 - 1s - loss: 92.4237 - loglik: -9.0863e+01 - logprior: -1.3708e+00
Epoch 5/10
19/19 - 1s - loss: 92.1275 - loglik: -9.0615e+01 - logprior: -1.3406e+00
Epoch 6/10
19/19 - 1s - loss: 92.1519 - loglik: -9.0668e+01 - logprior: -1.3237e+00
Fitted a model with MAP estimate = -91.8662
Time for alignment: 43.5622
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 133.7034 - loglik: -1.3045e+02 - logprior: -3.2469e+00
Epoch 2/10
19/19 - 1s - loss: 108.7851 - loglik: -1.0728e+02 - logprior: -1.4295e+00
Epoch 3/10
19/19 - 1s - loss: 100.5491 - loglik: -9.8727e+01 - logprior: -1.6135e+00
Epoch 4/10
19/19 - 1s - loss: 98.6009 - loglik: -9.6941e+01 - logprior: -1.5064e+00
Epoch 5/10
19/19 - 1s - loss: 98.2822 - loglik: -9.6686e+01 - logprior: -1.4743e+00
Epoch 6/10
19/19 - 1s - loss: 98.0336 - loglik: -9.6455e+01 - logprior: -1.4605e+00
Epoch 7/10
19/19 - 1s - loss: 97.9589 - loglik: -9.6404e+01 - logprior: -1.4409e+00
Epoch 8/10
19/19 - 1s - loss: 97.8552 - loglik: -9.6313e+01 - logprior: -1.4298e+00
Epoch 9/10
19/19 - 1s - loss: 97.7668 - loglik: -9.6225e+01 - logprior: -1.4253e+00
Epoch 10/10
19/19 - 1s - loss: 97.8448 - loglik: -9.6311e+01 - logprior: -1.4182e+00
Fitted a model with MAP estimate = -97.5340
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.3391 - loglik: -9.7938e+01 - logprior: -4.2319e+00
Epoch 2/2
19/19 - 1s - loss: 95.0972 - loglik: -9.2709e+01 - logprior: -2.1900e+00
Fitted a model with MAP estimate = -93.5731
expansions: [(0, 1)]
discards: [ 0  9 12 29 39 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 96.4588 - loglik: -9.3120e+01 - logprior: -3.1571e+00
Epoch 2/2
19/19 - 1s - loss: 93.4554 - loglik: -9.1766e+01 - logprior: -1.5150e+00
Fitted a model with MAP estimate = -92.6053
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 95.6050 - loglik: -9.2146e+01 - logprior: -3.2705e+00
Epoch 2/10
19/19 - 1s - loss: 93.1498 - loglik: -9.1471e+01 - logprior: -1.5035e+00
Epoch 3/10
19/19 - 1s - loss: 92.5717 - loglik: -9.0959e+01 - logprior: -1.4203e+00
Epoch 4/10
19/19 - 1s - loss: 92.2769 - loglik: -9.0719e+01 - logprior: -1.3694e+00
Epoch 5/10
19/19 - 1s - loss: 92.1686 - loglik: -9.0659e+01 - logprior: -1.3361e+00
Epoch 6/10
19/19 - 1s - loss: 92.1449 - loglik: -9.0659e+01 - logprior: -1.3225e+00
Epoch 7/10
19/19 - 1s - loss: 92.0267 - loglik: -9.0567e+01 - logprior: -1.3033e+00
Epoch 8/10
19/19 - 1s - loss: 91.9402 - loglik: -9.0495e+01 - logprior: -1.2877e+00
Epoch 9/10
19/19 - 1s - loss: 91.8983 - loglik: -9.0462e+01 - logprior: -1.2783e+00
Epoch 10/10
19/19 - 1s - loss: 91.9378 - loglik: -9.0514e+01 - logprior: -1.2654e+00
Fitted a model with MAP estimate = -91.6278
Time for alignment: 50.7358
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.8378 - loglik: -1.3058e+02 - logprior: -3.2481e+00
Epoch 2/10
19/19 - 1s - loss: 109.2385 - loglik: -1.0774e+02 - logprior: -1.4280e+00
Epoch 3/10
19/19 - 1s - loss: 101.1515 - loglik: -9.9286e+01 - logprior: -1.6072e+00
Epoch 4/10
19/19 - 1s - loss: 98.9784 - loglik: -9.7319e+01 - logprior: -1.4961e+00
Epoch 5/10
19/19 - 1s - loss: 98.2391 - loglik: -9.6628e+01 - logprior: -1.4736e+00
Epoch 6/10
19/19 - 1s - loss: 97.9738 - loglik: -9.6396e+01 - logprior: -1.4639e+00
Epoch 7/10
19/19 - 1s - loss: 97.9640 - loglik: -9.6410e+01 - logprior: -1.4451e+00
Epoch 8/10
19/19 - 1s - loss: 97.8374 - loglik: -9.6285e+01 - logprior: -1.4355e+00
Epoch 9/10
19/19 - 1s - loss: 97.8526 - loglik: -9.6306e+01 - logprior: -1.4262e+00
Fitted a model with MAP estimate = -97.5673
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 102.2788 - loglik: -9.7880e+01 - logprior: -4.2297e+00
Epoch 2/2
19/19 - 1s - loss: 95.2192 - loglik: -9.2846e+01 - logprior: -2.1747e+00
Fitted a model with MAP estimate = -93.5394
expansions: [(0, 1)]
discards: [ 0  9 12 29 39 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 96.4366 - loglik: -9.3092e+01 - logprior: -3.1631e+00
Epoch 2/2
19/19 - 1s - loss: 93.4754 - loglik: -9.1775e+01 - logprior: -1.5181e+00
Fitted a model with MAP estimate = -92.6173
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 95.6266 - loglik: -9.2170e+01 - logprior: -3.2688e+00
Epoch 2/10
19/19 - 1s - loss: 93.1383 - loglik: -9.1460e+01 - logprior: -1.5063e+00
Epoch 3/10
19/19 - 1s - loss: 92.5613 - loglik: -9.0944e+01 - logprior: -1.4198e+00
Epoch 4/10
19/19 - 1s - loss: 92.2728 - loglik: -9.0718e+01 - logprior: -1.3699e+00
Epoch 5/10
19/19 - 1s - loss: 92.1819 - loglik: -9.0672e+01 - logprior: -1.3366e+00
Epoch 6/10
19/19 - 1s - loss: 92.1934 - loglik: -9.0710e+01 - logprior: -1.3221e+00
Fitted a model with MAP estimate = -91.8232
Time for alignment: 43.5774
Computed alignments with likelihoods: ['-91.8245', '-91.6278', '-91.8232']
Best model has likelihood: -91.6278
time for generating output: 0.1025
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00018.projection.fasta
SP score = 0.8313542762663461
Training of 3 independent models on file PF00687.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff31c1ca220>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff2fdfc9580>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff3606bcd00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff51a84e9a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff5237978b0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff523475880>, <__main__.SimpleDirichletPrior object at 0x7ff38415eca0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff6b0a379d0>

Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 472.6711 - loglik: -4.7071e+02 - logprior: -1.9380e+00
Epoch 2/10
39/39 - 12s - loss: 374.0930 - loglik: -3.7187e+02 - logprior: -1.9665e+00
Epoch 3/10
39/39 - 12s - loss: 365.0230 - loglik: -3.6265e+02 - logprior: -1.9912e+00
Epoch 4/10
39/39 - 12s - loss: 361.5714 - loglik: -3.5925e+02 - logprior: -1.9330e+00
Epoch 5/10
39/39 - 12s - loss: 360.3868 - loglik: -3.5812e+02 - logprior: -1.9035e+00
Epoch 6/10
39/39 - 12s - loss: 359.5991 - loglik: -3.5738e+02 - logprior: -1.8898e+00
Epoch 7/10
39/39 - 12s - loss: 359.2227 - loglik: -3.5704e+02 - logprior: -1.8853e+00
Epoch 8/10
39/39 - 12s - loss: 358.9055 - loglik: -3.5674e+02 - logprior: -1.8816e+00
Epoch 9/10
39/39 - 12s - loss: 359.0126 - loglik: -3.5686e+02 - logprior: -1.8764e+00
Fitted a model with MAP estimate = -357.8862
expansions: [(4, 1), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (22, 1), (27, 1), (34, 1), (39, 1), (40, 1), (41, 1), (42, 1), (51, 1), (58, 1), (60, 1), (63, 1), (66, 1), (74, 1), (98, 2), (99, 1), (100, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 1), (120, 1), (127, 1), (129, 2), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 194 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 343.4326 - loglik: -3.4123e+02 - logprior: -2.0057e+00
Epoch 2/2
39/39 - 15s - loss: 328.6420 - loglik: -3.2737e+02 - logprior: -9.7935e-01
Fitted a model with MAP estimate = -325.7343
expansions: []
discards: [118 170]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 332.4792 - loglik: -3.3034e+02 - logprior: -1.8120e+00
Epoch 2/2
39/39 - 15s - loss: 328.3191 - loglik: -3.2718e+02 - logprior: -7.6144e-01
Fitted a model with MAP estimate = -325.7400
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 332.1766 - loglik: -3.3015e+02 - logprior: -1.6650e+00
Epoch 2/10
39/39 - 14s - loss: 327.6768 - loglik: -3.2667e+02 - logprior: -6.1356e-01
Epoch 3/10
39/39 - 15s - loss: 325.0558 - loglik: -3.2407e+02 - logprior: -5.1062e-01
Epoch 4/10
39/39 - 15s - loss: 323.0668 - loglik: -3.2220e+02 - logprior: -3.8920e-01
Epoch 5/10
39/39 - 15s - loss: 321.4172 - loglik: -3.2074e+02 - logprior: -2.6120e-01
Epoch 6/10
39/39 - 15s - loss: 320.7455 - loglik: -3.2022e+02 - logprior: -1.4710e-01
Epoch 7/10
39/39 - 15s - loss: 318.8361 - loglik: -3.1833e+02 - logprior: -3.0342e-02
Epoch 8/10
39/39 - 15s - loss: 317.1553 - loglik: -3.1672e+02 - logprior: 0.0758
Epoch 9/10
39/39 - 15s - loss: 317.5743 - loglik: -3.1728e+02 - logprior: 0.1721
Fitted a model with MAP estimate = -316.1812
Time for alignment: 382.8933
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 470.9745 - loglik: -4.6901e+02 - logprior: -1.9304e+00
Epoch 2/10
39/39 - 12s - loss: 371.7314 - loglik: -3.6946e+02 - logprior: -1.9926e+00
Epoch 3/10
39/39 - 11s - loss: 362.5527 - loglik: -3.6019e+02 - logprior: -2.0147e+00
Epoch 4/10
39/39 - 12s - loss: 359.9829 - loglik: -3.5769e+02 - logprior: -1.9478e+00
Epoch 5/10
39/39 - 12s - loss: 358.4712 - loglik: -3.5621e+02 - logprior: -1.9074e+00
Epoch 6/10
39/39 - 12s - loss: 357.6289 - loglik: -3.5539e+02 - logprior: -1.9085e+00
Epoch 7/10
39/39 - 12s - loss: 357.6771 - loglik: -3.5548e+02 - logprior: -1.8996e+00
Fitted a model with MAP estimate = -356.5767
expansions: [(4, 1), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (39, 1), (40, 1), (41, 1), (42, 1), (58, 1), (59, 1), (61, 1), (63, 1), (66, 1), (75, 1), (98, 2), (99, 1), (100, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 2), (123, 1), (127, 1), (129, 1), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 194 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 342.0015 - loglik: -3.3980e+02 - logprior: -1.9895e+00
Epoch 2/2
39/39 - 15s - loss: 327.8176 - loglik: -3.2653e+02 - logprior: -9.8896e-01
Fitted a model with MAP estimate = -325.0263
expansions: []
discards: [118 170]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 331.9259 - loglik: -3.2976e+02 - logprior: -1.8447e+00
Epoch 2/2
39/39 - 15s - loss: 327.6205 - loglik: -3.2645e+02 - logprior: -7.9134e-01
Fitted a model with MAP estimate = -325.0234
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 331.2668 - loglik: -3.2922e+02 - logprior: -1.6798e+00
Epoch 2/10
39/39 - 15s - loss: 327.0433 - loglik: -3.2603e+02 - logprior: -6.1265e-01
Epoch 3/10
39/39 - 15s - loss: 324.9003 - loglik: -3.2391e+02 - logprior: -5.0399e-01
Epoch 4/10
39/39 - 15s - loss: 322.3945 - loglik: -3.2152e+02 - logprior: -4.0078e-01
Epoch 5/10
39/39 - 15s - loss: 321.2769 - loglik: -3.2058e+02 - logprior: -2.7695e-01
Epoch 6/10
39/39 - 16s - loss: 320.4089 - loglik: -3.1984e+02 - logprior: -1.7240e-01
Epoch 7/10
39/39 - 16s - loss: 319.0002 - loglik: -3.1854e+02 - logprior: -5.0063e-02
Epoch 8/10
39/39 - 16s - loss: 317.1903 - loglik: -3.1677e+02 - logprior: 0.0481
Epoch 9/10
39/39 - 16s - loss: 316.6479 - loglik: -3.1633e+02 - logprior: 0.1452
Epoch 10/10
39/39 - 16s - loss: 316.7374 - loglik: -3.1653e+02 - logprior: 0.2353
Fitted a model with MAP estimate = -315.9374
Time for alignment: 378.8013
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 472.0954 - loglik: -4.7013e+02 - logprior: -1.9348e+00
Epoch 2/10
39/39 - 12s - loss: 372.7566 - loglik: -3.7047e+02 - logprior: -1.9313e+00
Epoch 3/10
39/39 - 12s - loss: 363.4466 - loglik: -3.6112e+02 - logprior: -1.9510e+00
Epoch 4/10
39/39 - 12s - loss: 360.2256 - loglik: -3.5794e+02 - logprior: -1.9124e+00
Epoch 5/10
39/39 - 12s - loss: 357.9662 - loglik: -3.5573e+02 - logprior: -1.9153e+00
Epoch 6/10
39/39 - 12s - loss: 357.5737 - loglik: -3.5535e+02 - logprior: -1.9124e+00
Epoch 7/10
39/39 - 12s - loss: 356.8943 - loglik: -3.5470e+02 - logprior: -1.8875e+00
Epoch 8/10
39/39 - 12s - loss: 355.7936 - loglik: -3.5352e+02 - logprior: -1.8575e+00
Epoch 9/10
39/39 - 12s - loss: 354.2568 - loglik: -3.5198e+02 - logprior: -1.8624e+00
Epoch 10/10
39/39 - 12s - loss: 354.7422 - loglik: -3.5247e+02 - logprior: -1.8751e+00
Fitted a model with MAP estimate = -353.7242
expansions: [(4, 1), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (34, 1), (39, 1), (40, 2), (41, 1), (42, 2), (58, 1), (59, 1), (60, 1), (63, 2), (66, 1), (72, 1), (74, 1), (97, 2), (99, 1), (102, 1), (103, 1), (108, 1), (110, 1), (112, 1), (116, 1), (120, 1), (123, 1), (129, 2), (130, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 196 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 342.3412 - loglik: -3.4006e+02 - logprior: -2.0931e+00
Epoch 2/2
39/39 - 17s - loss: 327.5305 - loglik: -3.2621e+02 - logprior: -1.0647e+00
Fitted a model with MAP estimate = -324.7509
expansions: []
discards: [ 51  56  82 121]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 331.9930 - loglik: -3.2988e+02 - logprior: -1.8218e+00
Epoch 2/2
39/39 - 17s - loss: 327.8029 - loglik: -3.2666e+02 - logprior: -7.7792e-01
Fitted a model with MAP estimate = -325.1008
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 331.0620 - loglik: -3.2903e+02 - logprior: -1.6746e+00
Epoch 2/10
39/39 - 16s - loss: 327.3482 - loglik: -3.2636e+02 - logprior: -5.9468e-01
Epoch 3/10
39/39 - 16s - loss: 324.6153 - loglik: -3.2367e+02 - logprior: -4.7844e-01
Epoch 4/10
39/39 - 16s - loss: 322.3884 - loglik: -3.2157e+02 - logprior: -3.5266e-01
Epoch 5/10
39/39 - 16s - loss: 321.1828 - loglik: -3.2054e+02 - logprior: -2.3201e-01
Epoch 6/10
39/39 - 16s - loss: 319.8662 - loglik: -3.1936e+02 - logprior: -1.2081e-01
Epoch 7/10
39/39 - 15s - loss: 318.3197 - loglik: -3.1785e+02 - logprior: 6.5692e-04
Epoch 8/10
39/39 - 15s - loss: 316.9049 - loglik: -3.1652e+02 - logprior: 0.1056
Epoch 9/10
39/39 - 15s - loss: 316.7102 - loglik: -3.1645e+02 - logprior: 0.1973
Epoch 10/10
39/39 - 16s - loss: 316.3192 - loglik: -3.1617e+02 - logprior: 0.2935
Fitted a model with MAP estimate = -315.6265
Time for alignment: 434.5859
Computed alignments with likelihoods: ['-316.1812', '-315.9374', '-315.6265']
Best model has likelihood: -315.6265
time for generating output: 0.2094
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00687.projection.fasta
SP score = 0.7524457677584007
Training of 3 independent models on file PF03129.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2fd068cd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fed2866a6d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fed2866a1c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff396371b20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff3963711f0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff3844b3cd0>, <__main__.SimpleDirichletPrior object at 0x7ff2fda05160>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff71d622040>

Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 258.2118 - loglik: -2.5497e+02 - logprior: -3.1612e+00
Epoch 2/10
19/19 - 2s - loss: 225.1522 - loglik: -2.2333e+02 - logprior: -1.3495e+00
Epoch 3/10
19/19 - 2s - loss: 211.4966 - loglik: -2.0940e+02 - logprior: -1.4424e+00
Epoch 4/10
19/19 - 2s - loss: 208.1641 - loglik: -2.0643e+02 - logprior: -1.3441e+00
Epoch 5/10
19/19 - 2s - loss: 207.0238 - loglik: -2.0538e+02 - logprior: -1.3378e+00
Epoch 6/10
19/19 - 2s - loss: 205.7844 - loglik: -2.0420e+02 - logprior: -1.3220e+00
Epoch 7/10
19/19 - 2s - loss: 204.5372 - loglik: -2.0302e+02 - logprior: -1.2995e+00
Epoch 8/10
19/19 - 2s - loss: 204.0365 - loglik: -2.0256e+02 - logprior: -1.2829e+00
Epoch 9/10
19/19 - 2s - loss: 203.9212 - loglik: -2.0247e+02 - logprior: -1.2695e+00
Epoch 10/10
19/19 - 2s - loss: 203.6019 - loglik: -2.0216e+02 - logprior: -1.2652e+00
Fitted a model with MAP estimate = -202.9510
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (27, 1), (29, 1), (37, 2), (38, 2), (50, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 206.2968 - loglik: -2.0191e+02 - logprior: -4.0776e+00
Epoch 2/2
19/19 - 3s - loss: 196.3060 - loglik: -1.9371e+02 - logprior: -2.0442e+00
Fitted a model with MAP estimate = -193.5893
expansions: [(0, 2)]
discards: [ 0  7 46]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 196.4044 - loglik: -1.9280e+02 - logprior: -2.9515e+00
Epoch 2/2
19/19 - 3s - loss: 192.7754 - loglik: -1.9105e+02 - logprior: -1.1401e+00
Fitted a model with MAP estimate = -191.0424
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 197.3855 - loglik: -1.9320e+02 - logprior: -3.4953e+00
Epoch 2/10
19/19 - 3s - loss: 193.1744 - loglik: -1.9126e+02 - logprior: -1.2967e+00
Epoch 3/10
19/19 - 3s - loss: 191.9768 - loglik: -1.9022e+02 - logprior: -1.1738e+00
Epoch 4/10
19/19 - 3s - loss: 191.3737 - loglik: -1.8973e+02 - logprior: -1.1245e+00
Epoch 5/10
19/19 - 3s - loss: 191.3475 - loglik: -1.8981e+02 - logprior: -1.0892e+00
Epoch 6/10
19/19 - 3s - loss: 190.7799 - loglik: -1.8932e+02 - logprior: -1.0655e+00
Epoch 7/10
19/19 - 3s - loss: 191.0040 - loglik: -1.8961e+02 - logprior: -1.0380e+00
Fitted a model with MAP estimate = -190.2336
Time for alignment: 83.4993
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 258.1489 - loglik: -2.5491e+02 - logprior: -3.1605e+00
Epoch 2/10
19/19 - 2s - loss: 226.3469 - loglik: -2.2455e+02 - logprior: -1.3526e+00
Epoch 3/10
19/19 - 2s - loss: 210.9209 - loglik: -2.0885e+02 - logprior: -1.4963e+00
Epoch 4/10
19/19 - 2s - loss: 206.4432 - loglik: -2.0459e+02 - logprior: -1.4471e+00
Epoch 5/10
19/19 - 2s - loss: 204.2299 - loglik: -2.0247e+02 - logprior: -1.4409e+00
Epoch 6/10
19/19 - 2s - loss: 203.5793 - loglik: -2.0197e+02 - logprior: -1.3640e+00
Epoch 7/10
19/19 - 2s - loss: 203.2244 - loglik: -2.0169e+02 - logprior: -1.3302e+00
Epoch 8/10
19/19 - 2s - loss: 202.5917 - loglik: -2.0107e+02 - logprior: -1.3328e+00
Epoch 9/10
19/19 - 2s - loss: 202.6961 - loglik: -2.0119e+02 - logprior: -1.3226e+00
Fitted a model with MAP estimate = -201.9735
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (24, 1), (29, 1), (36, 1), (37, 2), (38, 1), (50, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 206.1542 - loglik: -2.0176e+02 - logprior: -4.0786e+00
Epoch 2/2
19/19 - 3s - loss: 196.1675 - loglik: -1.9358e+02 - logprior: -2.0488e+00
Fitted a model with MAP estimate = -193.5301
expansions: [(0, 2)]
discards: [ 0  7 46]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 196.3802 - loglik: -1.9277e+02 - logprior: -2.9603e+00
Epoch 2/2
19/19 - 3s - loss: 192.4732 - loglik: -1.9073e+02 - logprior: -1.1431e+00
Fitted a model with MAP estimate = -191.0627
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 197.4156 - loglik: -1.9324e+02 - logprior: -3.4888e+00
Epoch 2/10
19/19 - 2s - loss: 193.1534 - loglik: -1.9123e+02 - logprior: -1.2980e+00
Epoch 3/10
19/19 - 3s - loss: 192.0410 - loglik: -1.9028e+02 - logprior: -1.1802e+00
Epoch 4/10
19/19 - 3s - loss: 191.3643 - loglik: -1.8973e+02 - logprior: -1.1229e+00
Epoch 5/10
19/19 - 3s - loss: 191.3581 - loglik: -1.8982e+02 - logprior: -1.0924e+00
Epoch 6/10
19/19 - 3s - loss: 190.9084 - loglik: -1.8945e+02 - logprior: -1.0635e+00
Epoch 7/10
19/19 - 3s - loss: 190.5787 - loglik: -1.8919e+02 - logprior: -1.0339e+00
Epoch 8/10
19/19 - 3s - loss: 190.6735 - loglik: -1.8933e+02 - logprior: -1.0152e+00
Fitted a model with MAP estimate = -190.1117
Time for alignment: 81.6696
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 258.0229 - loglik: -2.5478e+02 - logprior: -3.1549e+00
Epoch 2/10
19/19 - 2s - loss: 226.1884 - loglik: -2.2434e+02 - logprior: -1.3549e+00
Epoch 3/10
19/19 - 2s - loss: 211.9041 - loglik: -2.0970e+02 - logprior: -1.4763e+00
Epoch 4/10
19/19 - 2s - loss: 208.4777 - loglik: -2.0664e+02 - logprior: -1.4173e+00
Epoch 5/10
19/19 - 2s - loss: 206.8976 - loglik: -2.0517e+02 - logprior: -1.4100e+00
Epoch 6/10
19/19 - 2s - loss: 206.2564 - loglik: -2.0459e+02 - logprior: -1.3669e+00
Epoch 7/10
19/19 - 2s - loss: 205.7733 - loglik: -2.0417e+02 - logprior: -1.3280e+00
Epoch 8/10
19/19 - 2s - loss: 205.3405 - loglik: -2.0379e+02 - logprior: -1.2891e+00
Epoch 9/10
19/19 - 2s - loss: 204.5311 - loglik: -2.0299e+02 - logprior: -1.2979e+00
Epoch 10/10
19/19 - 2s - loss: 203.9156 - loglik: -2.0241e+02 - logprior: -1.2920e+00
Fitted a model with MAP estimate = -203.3625
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (24, 1), (31, 1), (37, 2), (38, 2), (54, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 206.6790 - loglik: -2.0228e+02 - logprior: -4.0806e+00
Epoch 2/2
19/19 - 2s - loss: 196.6474 - loglik: -1.9404e+02 - logprior: -2.0444e+00
Fitted a model with MAP estimate = -193.6774
expansions: [(0, 2)]
discards: [ 0  7 45]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 196.3752 - loglik: -1.9274e+02 - logprior: -2.9553e+00
Epoch 2/2
19/19 - 2s - loss: 192.7018 - loglik: -1.9096e+02 - logprior: -1.1396e+00
Fitted a model with MAP estimate = -190.9913
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 197.4436 - loglik: -1.9324e+02 - logprior: -3.5037e+00
Epoch 2/10
19/19 - 2s - loss: 192.9708 - loglik: -1.9106e+02 - logprior: -1.2999e+00
Epoch 3/10
19/19 - 2s - loss: 192.0879 - loglik: -1.9033e+02 - logprior: -1.1697e+00
Epoch 4/10
19/19 - 2s - loss: 191.4820 - loglik: -1.8984e+02 - logprior: -1.1248e+00
Epoch 5/10
19/19 - 2s - loss: 191.1802 - loglik: -1.8965e+02 - logprior: -1.0850e+00
Epoch 6/10
19/19 - 2s - loss: 190.7496 - loglik: -1.8929e+02 - logprior: -1.0666e+00
Epoch 7/10
19/19 - 2s - loss: 191.1190 - loglik: -1.8973e+02 - logprior: -1.0325e+00
Fitted a model with MAP estimate = -190.2671
Time for alignment: 81.3640
Computed alignments with likelihoods: ['-190.2336', '-190.1117', '-190.2671']
Best model has likelihood: -190.1117
time for generating output: 0.1578
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF03129.projection.fasta
SP score = 0.9733136907365206
Training of 3 independent models on file PF13378.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2fc517a60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fed2185d8b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2fda65ca0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff398793430>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff51b039f40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff2fcce9610>, <__main__.SimpleDirichletPrior object at 0x7fed208761f0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff71d622040>

Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 566.0648 - loglik: -5.6371e+02 - logprior: -1.8548e+00
Epoch 2/10
39/39 - 13s - loss: 486.8069 - loglik: -4.8412e+02 - logprior: -1.5015e+00
Epoch 3/10
39/39 - 13s - loss: 476.4536 - loglik: -4.7384e+02 - logprior: -1.5716e+00
Epoch 4/10
39/39 - 13s - loss: 474.3202 - loglik: -4.7178e+02 - logprior: -1.5474e+00
Epoch 5/10
39/39 - 13s - loss: 473.6529 - loglik: -4.7120e+02 - logprior: -1.5468e+00
Epoch 6/10
39/39 - 13s - loss: 472.8477 - loglik: -4.7045e+02 - logprior: -1.5562e+00
Epoch 7/10
39/39 - 13s - loss: 472.6175 - loglik: -4.7028e+02 - logprior: -1.5546e+00
Epoch 8/10
39/39 - 13s - loss: 472.1921 - loglik: -4.6986e+02 - logprior: -1.5597e+00
Epoch 9/10
39/39 - 13s - loss: 472.0183 - loglik: -4.6969e+02 - logprior: -1.5674e+00
Epoch 10/10
39/39 - 13s - loss: 471.7014 - loglik: -4.6937e+02 - logprior: -1.5721e+00
Fitted a model with MAP estimate = -468.2058
expansions: [(0, 3), (11, 1), (18, 2), (19, 1), (20, 1), (30, 1), (34, 2), (44, 2), (45, 1), (49, 1), (65, 1), (68, 1), (72, 1), (75, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (103, 1), (108, 1), (109, 1), (113, 1), (116, 1), (125, 3), (136, 1), (142, 5), (147, 1), (149, 2), (150, 2), (151, 2), (153, 1), (159, 1), (161, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 222 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 468.1355 - loglik: -4.6394e+02 - logprior: -2.8983e+00
Epoch 2/2
39/39 - 18s - loss: 453.2020 - loglik: -4.5026e+02 - logprior: -1.1993e+00
Fitted a model with MAP estimate = -446.8683
expansions: []
discards: [ 44  56 112 178 179 192]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 216 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 456.3874 - loglik: -4.5217e+02 - logprior: -2.1247e+00
Epoch 2/2
39/39 - 17s - loss: 451.9713 - loglik: -4.4915e+02 - logprior: -9.6436e-01
Fitted a model with MAP estimate = -446.5210
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 216 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 453.3861 - loglik: -4.4928e+02 - logprior: -1.9589e+00
Epoch 2/10
39/39 - 17s - loss: 449.6481 - loglik: -4.4701e+02 - logprior: -7.9907e-01
Epoch 3/10
39/39 - 17s - loss: 448.0129 - loglik: -4.4574e+02 - logprior: -6.7285e-01
Epoch 4/10
39/39 - 17s - loss: 446.9143 - loglik: -4.4502e+02 - logprior: -5.7946e-01
Epoch 5/10
39/39 - 17s - loss: 446.7477 - loglik: -4.4515e+02 - logprior: -4.7153e-01
Epoch 6/10
39/39 - 17s - loss: 445.6935 - loglik: -4.4430e+02 - logprior: -3.8397e-01
Epoch 7/10
39/39 - 17s - loss: 445.6892 - loglik: -4.4444e+02 - logprior: -2.9390e-01
Epoch 8/10
39/39 - 17s - loss: 445.4097 - loglik: -4.4430e+02 - logprior: -2.0088e-01
Epoch 9/10
39/39 - 17s - loss: 444.3582 - loglik: -4.4336e+02 - logprior: -1.1145e-01
Epoch 10/10
39/39 - 17s - loss: 444.5601 - loglik: -4.4365e+02 - logprior: -2.5084e-02
Fitted a model with MAP estimate = -442.8836
Time for alignment: 464.1745
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 566.8380 - loglik: -5.6448e+02 - logprior: -1.8548e+00
Epoch 2/10
39/39 - 13s - loss: 489.8479 - loglik: -4.8706e+02 - logprior: -1.3996e+00
Epoch 3/10
39/39 - 13s - loss: 479.7297 - loglik: -4.7685e+02 - logprior: -1.3999e+00
Epoch 4/10
39/39 - 13s - loss: 477.2473 - loglik: -4.7466e+02 - logprior: -1.4178e+00
Epoch 5/10
39/39 - 13s - loss: 475.7632 - loglik: -4.7334e+02 - logprior: -1.4509e+00
Epoch 6/10
39/39 - 14s - loss: 475.2751 - loglik: -4.7291e+02 - logprior: -1.4609e+00
Epoch 7/10
39/39 - 14s - loss: 474.7898 - loglik: -4.7245e+02 - logprior: -1.4691e+00
Epoch 8/10
39/39 - 13s - loss: 474.5792 - loglik: -4.7227e+02 - logprior: -1.4775e+00
Epoch 9/10
39/39 - 14s - loss: 474.0672 - loglik: -4.7175e+02 - logprior: -1.4802e+00
Epoch 10/10
39/39 - 14s - loss: 473.9234 - loglik: -4.7158e+02 - logprior: -1.4928e+00
Fitted a model with MAP estimate = -470.2380
expansions: [(0, 3), (19, 2), (20, 1), (21, 1), (34, 1), (36, 1), (45, 2), (46, 1), (49, 1), (51, 1), (65, 1), (68, 1), (76, 1), (85, 1), (86, 1), (87, 3), (88, 2), (89, 2), (92, 1), (108, 1), (109, 1), (110, 1), (116, 1), (124, 4), (131, 2), (140, 2), (149, 2), (150, 2), (151, 2), (153, 1), (159, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 221 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 469.5390 - loglik: -4.6519e+02 - logprior: -2.9427e+00
Epoch 2/2
39/39 - 18s - loss: 453.5473 - loglik: -4.5041e+02 - logprior: -1.2973e+00
Fitted a model with MAP estimate = -447.2790
expansions: [(186, 1)]
discards: [ 55 107 110 113 179 191]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 216 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 456.5444 - loglik: -4.5226e+02 - logprior: -2.1255e+00
Epoch 2/2
39/39 - 18s - loss: 452.1211 - loglik: -4.4932e+02 - logprior: -9.4529e-01
Fitted a model with MAP estimate = -446.6746
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 216 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 453.3983 - loglik: -4.4930e+02 - logprior: -1.9424e+00
Epoch 2/10
39/39 - 19s - loss: 449.6272 - loglik: -4.4701e+02 - logprior: -7.7987e-01
Epoch 3/10
39/39 - 18s - loss: 447.8352 - loglik: -4.4559e+02 - logprior: -6.5513e-01
Epoch 4/10
39/39 - 18s - loss: 447.3225 - loglik: -4.4545e+02 - logprior: -5.6532e-01
Epoch 5/10
39/39 - 19s - loss: 446.5550 - loglik: -4.4495e+02 - logprior: -4.7958e-01
Epoch 6/10
39/39 - 18s - loss: 445.9887 - loglik: -4.4459e+02 - logprior: -3.8303e-01
Epoch 7/10
39/39 - 18s - loss: 445.8067 - loglik: -4.4458e+02 - logprior: -2.8262e-01
Epoch 8/10
39/39 - 19s - loss: 445.3082 - loglik: -4.4421e+02 - logprior: -1.9676e-01
Epoch 9/10
39/39 - 19s - loss: 445.1661 - loglik: -4.4415e+02 - logprior: -1.0762e-01
Epoch 10/10
39/39 - 19s - loss: 443.9987 - loglik: -4.4307e+02 - logprior: -2.9139e-02
Fitted a model with MAP estimate = -442.9827
Time for alignment: 493.2399
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 566.4016 - loglik: -5.6402e+02 - logprior: -1.8744e+00
Epoch 2/10
39/39 - 14s - loss: 486.6497 - loglik: -4.8336e+02 - logprior: -1.6204e+00
Epoch 3/10
39/39 - 14s - loss: 477.1181 - loglik: -4.7398e+02 - logprior: -1.6485e+00
Epoch 4/10
39/39 - 14s - loss: 474.5612 - loglik: -4.7183e+02 - logprior: -1.6482e+00
Epoch 5/10
39/39 - 14s - loss: 473.5214 - loglik: -4.7095e+02 - logprior: -1.6467e+00
Epoch 6/10
39/39 - 14s - loss: 472.8338 - loglik: -4.7031e+02 - logprior: -1.6558e+00
Epoch 7/10
39/39 - 14s - loss: 472.6354 - loglik: -4.7014e+02 - logprior: -1.6630e+00
Epoch 8/10
39/39 - 14s - loss: 472.1811 - loglik: -4.6970e+02 - logprior: -1.6683e+00
Epoch 9/10
39/39 - 14s - loss: 471.9174 - loglik: -4.6943e+02 - logprior: -1.6741e+00
Epoch 10/10
39/39 - 14s - loss: 471.6906 - loglik: -4.6922e+02 - logprior: -1.6793e+00
Fitted a model with MAP estimate = -468.1753
expansions: [(0, 3), (12, 1), (20, 2), (21, 1), (26, 1), (35, 2), (45, 2), (46, 1), (51, 1), (52, 1), (65, 1), (68, 1), (75, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (103, 1), (108, 1), (109, 1), (113, 1), (117, 1), (126, 2), (127, 1), (132, 1), (138, 1), (141, 2), (149, 2), (150, 2), (151, 2), (153, 1), (157, 1), (158, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 220 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 470.0228 - loglik: -4.6571e+02 - logprior: -2.9434e+00
Epoch 2/2
39/39 - 18s - loss: 454.4169 - loglik: -4.5138e+02 - logprior: -1.2240e+00
Fitted a model with MAP estimate = -448.0617
expansions: []
discards: [ 44  56  94 113 178]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 215 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 457.1310 - loglik: -4.5283e+02 - logprior: -2.1314e+00
Epoch 2/2
39/39 - 18s - loss: 452.9255 - loglik: -4.5007e+02 - logprior: -9.7460e-01
Fitted a model with MAP estimate = -447.5015
expansions: []
discards: [25]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 214 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 454.5338 - loglik: -4.5039e+02 - logprior: -1.9655e+00
Epoch 2/10
39/39 - 17s - loss: 450.6487 - loglik: -4.4798e+02 - logprior: -8.1736e-01
Epoch 3/10
39/39 - 18s - loss: 448.7603 - loglik: -4.4648e+02 - logprior: -6.8520e-01
Epoch 4/10
39/39 - 18s - loss: 447.6640 - loglik: -4.4577e+02 - logprior: -5.9472e-01
Epoch 5/10
39/39 - 17s - loss: 447.2966 - loglik: -4.4567e+02 - logprior: -4.9681e-01
Epoch 6/10
39/39 - 17s - loss: 446.9002 - loglik: -4.4548e+02 - logprior: -4.1231e-01
Epoch 7/10
39/39 - 17s - loss: 446.4650 - loglik: -4.4521e+02 - logprior: -3.0501e-01
Epoch 8/10
39/39 - 16s - loss: 446.1098 - loglik: -4.4497e+02 - logprior: -2.2922e-01
Epoch 9/10
39/39 - 16s - loss: 445.3193 - loglik: -4.4425e+02 - logprior: -1.4676e-01
Epoch 10/10
39/39 - 16s - loss: 444.8187 - loglik: -4.4384e+02 - logprior: -5.4954e-02
Fitted a model with MAP estimate = -443.5503
Time for alignment: 478.2698
Computed alignments with likelihoods: ['-442.8836', '-442.9827', '-443.5503']
Best model has likelihood: -442.8836
time for generating output: 0.3303
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13378.projection.fasta
SP score = 0.7153406183120871
Training of 3 independent models on file PF00084.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff31c1f4bb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff2fd798a00>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2fd7981f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff2fda74760>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2fda743a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff2f41dae80>, <__main__.SimpleDirichletPrior object at 0x7fed11f29af0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7fed2988f790>

Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 164.8852 - loglik: -1.6163e+02 - logprior: -3.2201e+00
Epoch 2/10
19/19 - 1s - loss: 136.9096 - loglik: -1.3547e+02 - logprior: -1.4280e+00
Epoch 3/10
19/19 - 1s - loss: 128.6824 - loglik: -1.2693e+02 - logprior: -1.5320e+00
Epoch 4/10
19/19 - 1s - loss: 127.2855 - loglik: -1.2562e+02 - logprior: -1.3868e+00
Epoch 5/10
19/19 - 1s - loss: 126.3874 - loglik: -1.2474e+02 - logprior: -1.3939e+00
Epoch 6/10
19/19 - 1s - loss: 126.2183 - loglik: -1.2458e+02 - logprior: -1.3750e+00
Epoch 7/10
19/19 - 1s - loss: 125.9621 - loglik: -1.2432e+02 - logprior: -1.3647e+00
Epoch 8/10
19/19 - 1s - loss: 125.9449 - loglik: -1.2429e+02 - logprior: -1.3562e+00
Epoch 9/10
19/19 - 1s - loss: 125.8835 - loglik: -1.2423e+02 - logprior: -1.3505e+00
Epoch 10/10
19/19 - 1s - loss: 125.9177 - loglik: -1.2426e+02 - logprior: -1.3479e+00
Fitted a model with MAP estimate = -125.3916
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (18, 1), (30, 2), (34, 3), (36, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 133.7235 - loglik: -1.2923e+02 - logprior: -4.1541e+00
Epoch 2/2
19/19 - 1s - loss: 126.7687 - loglik: -1.2409e+02 - logprior: -2.2957e+00
Fitted a model with MAP estimate = -124.0665
expansions: []
discards: [13 14 16 17 41 47 51]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 126.8717 - loglik: -1.2311e+02 - logprior: -3.4106e+00
Epoch 2/2
19/19 - 1s - loss: 123.1120 - loglik: -1.2134e+02 - logprior: -1.4777e+00
Fitted a model with MAP estimate = -122.1940
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.5661 - loglik: -1.2200e+02 - logprior: -3.2555e+00
Epoch 2/10
19/19 - 1s - loss: 123.0178 - loglik: -1.2125e+02 - logprior: -1.4597e+00
Epoch 3/10
19/19 - 1s - loss: 122.2832 - loglik: -1.2057e+02 - logprior: -1.3646e+00
Epoch 4/10
19/19 - 1s - loss: 122.0524 - loglik: -1.2038e+02 - logprior: -1.3046e+00
Epoch 5/10
19/19 - 1s - loss: 121.8168 - loglik: -1.2018e+02 - logprior: -1.2758e+00
Epoch 6/10
19/19 - 1s - loss: 121.6169 - loglik: -1.1999e+02 - logprior: -1.2582e+00
Epoch 7/10
19/19 - 1s - loss: 121.3228 - loglik: -1.1972e+02 - logprior: -1.2419e+00
Epoch 8/10
19/19 - 1s - loss: 121.3777 - loglik: -1.1979e+02 - logprior: -1.2275e+00
Fitted a model with MAP estimate = -120.7871
Time for alignment: 57.1604
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 164.9944 - loglik: -1.6174e+02 - logprior: -3.2175e+00
Epoch 2/10
19/19 - 1s - loss: 137.5743 - loglik: -1.3613e+02 - logprior: -1.4231e+00
Epoch 3/10
19/19 - 1s - loss: 128.7527 - loglik: -1.2707e+02 - logprior: -1.5348e+00
Epoch 4/10
19/19 - 1s - loss: 127.0499 - loglik: -1.2534e+02 - logprior: -1.3988e+00
Epoch 5/10
19/19 - 1s - loss: 126.3802 - loglik: -1.2474e+02 - logprior: -1.3930e+00
Epoch 6/10
19/19 - 1s - loss: 126.0148 - loglik: -1.2438e+02 - logprior: -1.3635e+00
Epoch 7/10
19/19 - 1s - loss: 126.2245 - loglik: -1.2458e+02 - logprior: -1.3547e+00
Fitted a model with MAP estimate = -125.5470
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (26, 1), (30, 2), (34, 3), (36, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 133.3694 - loglik: -1.2888e+02 - logprior: -4.1631e+00
Epoch 2/2
19/19 - 1s - loss: 126.3311 - loglik: -1.2373e+02 - logprior: -2.2626e+00
Fitted a model with MAP estimate = -123.4858
expansions: []
discards: [13 14 16 17 41 47 51]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.5629 - loglik: -1.2287e+02 - logprior: -3.3759e+00
Epoch 2/2
19/19 - 1s - loss: 122.9821 - loglik: -1.2121e+02 - logprior: -1.4761e+00
Fitted a model with MAP estimate = -122.2055
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.6464 - loglik: -1.2206e+02 - logprior: -3.2640e+00
Epoch 2/10
19/19 - 1s - loss: 122.9486 - loglik: -1.2119e+02 - logprior: -1.4587e+00
Epoch 3/10
19/19 - 1s - loss: 122.2791 - loglik: -1.2056e+02 - logprior: -1.3658e+00
Epoch 4/10
19/19 - 1s - loss: 122.0413 - loglik: -1.2037e+02 - logprior: -1.3139e+00
Epoch 5/10
19/19 - 1s - loss: 121.8522 - loglik: -1.2021e+02 - logprior: -1.2747e+00
Epoch 6/10
19/19 - 1s - loss: 121.5927 - loglik: -1.1997e+02 - logprior: -1.2618e+00
Epoch 7/10
19/19 - 1s - loss: 121.4831 - loglik: -1.1987e+02 - logprior: -1.2449e+00
Epoch 8/10
19/19 - 1s - loss: 121.1004 - loglik: -1.1950e+02 - logprior: -1.2324e+00
Epoch 9/10
19/19 - 1s - loss: 121.0778 - loglik: -1.1950e+02 - logprior: -1.2160e+00
Epoch 10/10
19/19 - 1s - loss: 121.3233 - loglik: -1.1975e+02 - logprior: -1.2035e+00
Fitted a model with MAP estimate = -120.5681
Time for alignment: 53.8524
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 164.9425 - loglik: -1.6169e+02 - logprior: -3.2185e+00
Epoch 2/10
19/19 - 1s - loss: 136.3784 - loglik: -1.3494e+02 - logprior: -1.4225e+00
Epoch 3/10
19/19 - 1s - loss: 128.3146 - loglik: -1.2659e+02 - logprior: -1.5451e+00
Epoch 4/10
19/19 - 1s - loss: 126.5246 - loglik: -1.2480e+02 - logprior: -1.4108e+00
Epoch 5/10
19/19 - 1s - loss: 126.1193 - loglik: -1.2445e+02 - logprior: -1.4068e+00
Epoch 6/10
19/19 - 1s - loss: 125.7079 - loglik: -1.2404e+02 - logprior: -1.3828e+00
Epoch 7/10
19/19 - 1s - loss: 125.6090 - loglik: -1.2394e+02 - logprior: -1.3730e+00
Epoch 8/10
19/19 - 1s - loss: 125.4711 - loglik: -1.2381e+02 - logprior: -1.3644e+00
Epoch 9/10
19/19 - 1s - loss: 125.2747 - loglik: -1.2361e+02 - logprior: -1.3626e+00
Epoch 10/10
19/19 - 1s - loss: 125.3144 - loglik: -1.2364e+02 - logprior: -1.3569e+00
Fitted a model with MAP estimate = -124.8849
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (18, 1), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.7839 - loglik: -1.2930e+02 - logprior: -4.1453e+00
Epoch 2/2
19/19 - 1s - loss: 126.7146 - loglik: -1.2403e+02 - logprior: -2.3014e+00
Fitted a model with MAP estimate = -124.1201
expansions: []
discards: [13 14 41 42 51]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 126.4857 - loglik: -1.2268e+02 - logprior: -3.4491e+00
Epoch 2/2
19/19 - 2s - loss: 122.8273 - loglik: -1.2102e+02 - logprior: -1.4852e+00
Fitted a model with MAP estimate = -121.9453
expansions: []
discards: [14]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.6196 - loglik: -1.2203e+02 - logprior: -3.2683e+00
Epoch 2/10
19/19 - 1s - loss: 122.8117 - loglik: -1.2103e+02 - logprior: -1.4687e+00
Epoch 3/10
19/19 - 1s - loss: 122.4254 - loglik: -1.2070e+02 - logprior: -1.3739e+00
Epoch 4/10
19/19 - 1s - loss: 121.8079 - loglik: -1.2012e+02 - logprior: -1.3239e+00
Epoch 5/10
19/19 - 1s - loss: 121.6644 - loglik: -1.2001e+02 - logprior: -1.2851e+00
Epoch 6/10
19/19 - 1s - loss: 121.5713 - loglik: -1.1993e+02 - logprior: -1.2694e+00
Epoch 7/10
19/19 - 1s - loss: 121.2130 - loglik: -1.1960e+02 - logprior: -1.2504e+00
Epoch 8/10
19/19 - 1s - loss: 121.1943 - loglik: -1.1959e+02 - logprior: -1.2365e+00
Epoch 9/10
19/19 - 1s - loss: 121.0253 - loglik: -1.1944e+02 - logprior: -1.2168e+00
Epoch 10/10
19/19 - 1s - loss: 120.9556 - loglik: -1.1937e+02 - logprior: -1.2058e+00
Fitted a model with MAP estimate = -120.4439
Time for alignment: 59.2404
Computed alignments with likelihoods: ['-120.7871', '-120.5681', '-120.4439']
Best model has likelihood: -120.4439
time for generating output: 0.1129
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00084.projection.fasta
SP score = 0.8885542168674698
Training of 3 independent models on file PF00232.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fed21b32a00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fed08714d90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fed08707bb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fed09a621f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fed09a70730>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fed089f9d60>, <__main__.SimpleDirichletPrior object at 0x7fed21f92b20>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff2fc4ff040>

Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 74s - loss: 903.0383 - loglik: -9.0123e+02 - logprior: -1.7049e+00
Epoch 2/10
39/39 - 78s - loss: 718.8887 - loglik: -7.1640e+02 - logprior: -1.8305e+00
Epoch 3/10
39/39 - 81s - loss: 703.1780 - loglik: -7.0030e+02 - logprior: -1.8744e+00
Epoch 4/10
39/39 - 68s - loss: 700.5866 - loglik: -6.9775e+02 - logprior: -1.7993e+00
Epoch 5/10
39/39 - 62s - loss: 698.1528 - loglik: -6.9535e+02 - logprior: -1.8033e+00
Epoch 6/10
39/39 - 59s - loss: 697.8838 - loglik: -6.9509e+02 - logprior: -1.8313e+00
Epoch 7/10
39/39 - 60s - loss: 696.7377 - loglik: -6.9393e+02 - logprior: -1.8713e+00
Epoch 8/10
39/39 - 61s - loss: 694.8981 - loglik: -6.9211e+02 - logprior: -1.9020e+00
Epoch 9/10
39/39 - 64s - loss: 695.1653 - loglik: -6.9239e+02 - logprior: -1.9365e+00
Fitted a model with MAP estimate = -693.4225
expansions: [(0, 3), (37, 1), (38, 1), (62, 1), (131, 1), (133, 1), (134, 1), (135, 1), (145, 1), (161, 1), (162, 1), (168, 1), (172, 6), (173, 2), (174, 1), (175, 1), (187, 1), (188, 1), (189, 6), (190, 1), (193, 1), (194, 1), (195, 1), (196, 1), (198, 1), (199, 2), (200, 2), (201, 1), (204, 1), (209, 1), (213, 1), (215, 1), (216, 1), (220, 2), (221, 5), (222, 2), (225, 1), (226, 2), (227, 3), (228, 2), (229, 1), (240, 1), (242, 1), (243, 2), (244, 2), (245, 3), (247, 2), (248, 5), (249, 2), (250, 1), (252, 1), (268, 1), (270, 1), (284, 1), (285, 1), (287, 1), (288, 2), (290, 1), (301, 1), (303, 2), (304, 1), (307, 1), (316, 1), (326, 1), (339, 1), (349, 1), (355, 6)]
discards: [  2   3 127]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 461 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 117s - loss: 675.1918 - loglik: -6.7117e+02 - logprior: -2.8168e+00
Epoch 2/2
39/39 - 111s - loss: 654.8901 - loglik: -6.5188e+02 - logprior: -1.4355e+00
Fitted a model with MAP estimate = -649.1780
expansions: []
discards: [  2   3 185 186 187 188 237 272 274 308 320 321 322 459]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 447 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 100s - loss: 660.5784 - loglik: -6.5666e+02 - logprior: -1.8728e+00
Epoch 2/2
39/39 - 97s - loss: 655.3865 - loglik: -6.5277e+02 - logprior: -6.8937e-01
Fitted a model with MAP estimate = -651.4478
expansions: [(183, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 452 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 98s - loss: 657.2574 - loglik: -6.5334e+02 - logprior: -1.7167e+00
Epoch 2/10
39/39 - 92s - loss: 652.9358 - loglik: -6.5049e+02 - logprior: -4.8926e-01
Epoch 3/10
39/39 - 92s - loss: 650.4858 - loglik: -6.4848e+02 - logprior: -2.2558e-01
Epoch 4/10
39/39 - 92s - loss: 649.0631 - loglik: -6.4744e+02 - logprior: -5.4027e-02
Epoch 5/10
39/39 - 92s - loss: 648.4653 - loglik: -6.4703e+02 - logprior: -2.4934e-02
Epoch 6/10
39/39 - 95s - loss: 647.3021 - loglik: -6.4625e+02 - logprior: 0.2592
Epoch 7/10
39/39 - 94s - loss: 646.7778 - loglik: -6.4605e+02 - logprior: 0.4890
Epoch 8/10
39/39 - 95s - loss: 646.3132 - loglik: -6.4570e+02 - logprior: 0.5760
Epoch 9/10
39/39 - 95s - loss: 645.5475 - loglik: -6.4518e+02 - logprior: 0.7796
Epoch 10/10
39/39 - 94s - loss: 645.5096 - loglik: -6.4524e+02 - logprior: 0.8737
Fitted a model with MAP estimate = -643.1398
Time for alignment: 2433.3758
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 66s - loss: 906.9459 - loglik: -9.0516e+02 - logprior: -1.6592e+00
Epoch 2/10
39/39 - 60s - loss: 719.2998 - loglik: -7.1662e+02 - logprior: -1.7181e+00
Epoch 3/10
39/39 - 58s - loss: 703.3614 - loglik: -7.0036e+02 - logprior: -1.7770e+00
Epoch 4/10
39/39 - 58s - loss: 698.5931 - loglik: -6.9570e+02 - logprior: -1.7342e+00
Epoch 5/10
39/39 - 57s - loss: 697.0692 - loglik: -6.9424e+02 - logprior: -1.7142e+00
Epoch 6/10
39/39 - 58s - loss: 696.3799 - loglik: -6.9361e+02 - logprior: -1.7275e+00
Epoch 7/10
39/39 - 58s - loss: 695.2742 - loglik: -6.9252e+02 - logprior: -1.7609e+00
Epoch 8/10
39/39 - 59s - loss: 695.4686 - loglik: -6.9275e+02 - logprior: -1.7741e+00
Fitted a model with MAP estimate = -693.2580
expansions: [(0, 3), (43, 1), (45, 1), (106, 1), (133, 1), (142, 1), (162, 1), (164, 1), (168, 1), (174, 6), (175, 2), (176, 1), (177, 1), (178, 1), (189, 2), (190, 5), (191, 1), (192, 1), (195, 1), (196, 1), (197, 1), (198, 2), (199, 1), (200, 2), (201, 2), (203, 1), (207, 1), (210, 1), (214, 1), (217, 1), (222, 3), (223, 6), (226, 1), (227, 2), (228, 3), (230, 1), (242, 1), (244, 1), (245, 1), (246, 2), (247, 2), (249, 2), (250, 5), (253, 1), (255, 1), (256, 1), (270, 1), (272, 1), (288, 2), (289, 3), (291, 1), (305, 1), (306, 1), (307, 1), (309, 1), (328, 2), (331, 1), (340, 1), (350, 1), (352, 1), (355, 5)]
discards: [  1 126]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 455 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 102s - loss: 673.3094 - loglik: -6.6929e+02 - logprior: -2.6530e+00
Epoch 2/2
39/39 - 98s - loss: 653.9714 - loglik: -6.5105e+02 - logprior: -1.2540e+00
Fitted a model with MAP estimate = -648.9479
expansions: [(282, 1), (312, 1)]
discards: [  2   3 185 186 187 188 213 238 267 324 325]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 446 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 97s - loss: 660.4625 - loglik: -6.5636e+02 - logprior: -2.0376e+00
Epoch 2/2
39/39 - 84s - loss: 654.6382 - loglik: -6.5200e+02 - logprior: -6.4823e-01
Fitted a model with MAP estimate = -650.7563
expansions: [(183, 5), (312, 3), (313, 1), (363, 1)]
discards: [306 307]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 454 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 90s - loss: 657.5228 - loglik: -6.5352e+02 - logprior: -1.7624e+00
Epoch 2/10
39/39 - 88s - loss: 651.6826 - loglik: -6.4917e+02 - logprior: -4.6314e-01
Epoch 3/10
39/39 - 92s - loss: 649.2393 - loglik: -6.4712e+02 - logprior: -2.5819e-01
Epoch 4/10
39/39 - 88s - loss: 647.6646 - loglik: -6.4580e+02 - logprior: -2.3304e-01
Epoch 5/10
39/39 - 87s - loss: 647.4870 - loglik: -6.4591e+02 - logprior: -1.0975e-01
Epoch 6/10
39/39 - 87s - loss: 645.7280 - loglik: -6.4464e+02 - logprior: 0.2872
Epoch 7/10
39/39 - 88s - loss: 646.2838 - loglik: -6.4537e+02 - logprior: 0.3747
Fitted a model with MAP estimate = -642.8557
Time for alignment: 1890.1708
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 62s - loss: 902.5461 - loglik: -9.0066e+02 - logprior: -1.7796e+00
Epoch 2/10
39/39 - 58s - loss: 719.8041 - loglik: -7.1735e+02 - logprior: -1.8656e+00
Epoch 3/10
39/39 - 59s - loss: 704.2111 - loglik: -7.0112e+02 - logprior: -2.0460e+00
Epoch 4/10
39/39 - 59s - loss: 701.8269 - loglik: -6.9845e+02 - logprior: -2.3369e+00
Epoch 5/10
39/39 - 61s - loss: 700.1761 - loglik: -6.9696e+02 - logprior: -2.2279e+00
Epoch 6/10
39/39 - 65s - loss: 698.6293 - loglik: -6.9554e+02 - logprior: -2.1349e+00
Epoch 7/10
39/39 - 69s - loss: 698.1229 - loglik: -6.9502e+02 - logprior: -2.1598e+00
Epoch 8/10
39/39 - 67s - loss: 698.2396 - loglik: -6.9494e+02 - logprior: -2.3953e+00
Fitted a model with MAP estimate = -696.0504
expansions: [(0, 3), (37, 1), (43, 1), (65, 1), (134, 1), (143, 1), (163, 1), (164, 1), (175, 1), (176, 6), (177, 1), (178, 1), (182, 1), (190, 2), (191, 5), (192, 1), (193, 1), (196, 1), (197, 1), (198, 1), (199, 2), (200, 1), (201, 2), (202, 2), (203, 1), (208, 1), (211, 1), (215, 2), (217, 2), (222, 7), (223, 2), (226, 1), (227, 2), (228, 3), (229, 2), (242, 1), (243, 1), (245, 2), (246, 2), (247, 4), (248, 2), (249, 5), (250, 1), (251, 1), (253, 1), (269, 1), (286, 1), (287, 2), (288, 4), (290, 1), (291, 1), (300, 1), (302, 1), (303, 1), (304, 1), (306, 1), (326, 1), (329, 1), (338, 1), (345, 1), (351, 1), (352, 2), (355, 3)]
discards: [2 3]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 461 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 110s - loss: 674.5096 - loglik: -6.7058e+02 - logprior: -2.6284e+00
Epoch 2/2
39/39 - 114s - loss: 653.3857 - loglik: -6.5034e+02 - logprior: -1.4255e+00
Fitted a model with MAP estimate = -648.7837
expansions: [(215, 1), (282, 1), (311, 2), (320, 1), (321, 1)]
discards: [  2   3 185 186 212 213 237 259 274 275 276 277 288 309 327 455 456 457
 458 459 460]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 446 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 109s - loss: 664.1055 - loglik: -6.6028e+02 - logprior: -1.8055e+00
Epoch 2/2
39/39 - 102s - loss: 657.3386 - loglik: -6.5511e+02 - logprior: -3.1682e-01
Fitted a model with MAP estimate = -652.8660
expansions: [(0, 3), (267, 4), (446, 5)]
discards: [  2 304 305 306 307 308 309 316 317 318]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 448 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 116s - loss: 660.4338 - loglik: -6.5571e+02 - logprior: -2.4646e+00
Epoch 2/10
39/39 - 116s - loss: 653.3188 - loglik: -6.5080e+02 - logprior: -5.2426e-01
Epoch 3/10
39/39 - 109s - loss: 651.6532 - loglik: -6.4956e+02 - logprior: -3.1365e-01
Epoch 4/10
39/39 - 103s - loss: 650.1561 - loglik: -6.4790e+02 - logprior: -6.9233e-01
Epoch 5/10
39/39 - 101s - loss: 650.4653 - loglik: -6.4832e+02 - logprior: -7.1123e-01
Fitted a model with MAP estimate = -646.7784
Time for alignment: 1979.0948
Computed alignments with likelihoods: ['-643.1398', '-642.8557', '-646.7784']
Best model has likelihood: -642.8557
time for generating output: 0.4820
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00232.projection.fasta
SP score = 0.8574449143111507
Training of 3 independent models on file PF13522.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fed48302dc0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fed29b01400>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff3962b6100>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fed100fb6d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fed20bed8e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff2f53c28b0>, <__main__.SimpleDirichletPrior object at 0x7fed101ad790>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff2fc4ff040>

Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 408.6773 - loglik: -4.0566e+02 - logprior: -3.0008e+00
Epoch 2/10
19/19 - 5s - loss: 325.3679 - loglik: -3.2419e+02 - logprior: -1.0812e+00
Epoch 3/10
19/19 - 5s - loss: 297.4475 - loglik: -2.9550e+02 - logprior: -1.2754e+00
Epoch 4/10
19/19 - 5s - loss: 291.3907 - loglik: -2.8949e+02 - logprior: -1.2150e+00
Epoch 5/10
19/19 - 5s - loss: 290.3274 - loglik: -2.8849e+02 - logprior: -1.2031e+00
Epoch 6/10
19/19 - 5s - loss: 289.0014 - loglik: -2.8720e+02 - logprior: -1.1851e+00
Epoch 7/10
19/19 - 5s - loss: 288.5262 - loglik: -2.8678e+02 - logprior: -1.1667e+00
Epoch 8/10
19/19 - 5s - loss: 288.2717 - loglik: -2.8654e+02 - logprior: -1.1675e+00
Epoch 9/10
19/19 - 5s - loss: 287.7968 - loglik: -2.8609e+02 - logprior: -1.1626e+00
Epoch 10/10
19/19 - 5s - loss: 287.8154 - loglik: -2.8612e+02 - logprior: -1.1575e+00
Fitted a model with MAP estimate = -286.8123
expansions: [(0, 6), (7, 3), (8, 2), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (53, 1), (55, 1), (58, 1), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (96, 2), (97, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 156 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 289.7168 - loglik: -2.8494e+02 - logprior: -4.1439e+00
Epoch 2/2
19/19 - 7s - loss: 276.9204 - loglik: -2.7483e+02 - logprior: -1.2776e+00
Fitted a model with MAP estimate = -273.7376
expansions: [(0, 4)]
discards: [  1   2   3   4   5  21  98  99 145]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 281.9157 - loglik: -2.7701e+02 - logprior: -4.0275e+00
Epoch 2/2
19/19 - 6s - loss: 276.4323 - loglik: -2.7428e+02 - logprior: -1.2631e+00
Fitted a model with MAP estimate = -273.8288
expansions: [(0, 4)]
discards: [1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 280.7893 - loglik: -2.7599e+02 - logprior: -3.8546e+00
Epoch 2/10
19/19 - 7s - loss: 276.0501 - loglik: -2.7371e+02 - logprior: -1.4082e+00
Epoch 3/10
19/19 - 7s - loss: 274.1508 - loglik: -2.7213e+02 - logprior: -1.0867e+00
Epoch 4/10
19/19 - 7s - loss: 273.0423 - loglik: -2.7130e+02 - logprior: -8.5625e-01
Epoch 5/10
19/19 - 7s - loss: 271.9287 - loglik: -2.7032e+02 - logprior: -7.3512e-01
Epoch 6/10
19/19 - 7s - loss: 271.9222 - loglik: -2.7044e+02 - logprior: -6.7674e-01
Epoch 7/10
19/19 - 7s - loss: 270.3430 - loglik: -2.6895e+02 - logprior: -6.3386e-01
Epoch 8/10
19/19 - 7s - loss: 271.1910 - loglik: -2.6987e+02 - logprior: -6.0047e-01
Fitted a model with MAP estimate = -269.9120
Time for alignment: 182.8073
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 408.8064 - loglik: -4.0580e+02 - logprior: -2.9937e+00
Epoch 2/10
19/19 - 5s - loss: 327.2257 - loglik: -3.2604e+02 - logprior: -1.0802e+00
Epoch 3/10
19/19 - 5s - loss: 297.0667 - loglik: -2.9511e+02 - logprior: -1.3067e+00
Epoch 4/10
19/19 - 5s - loss: 291.4389 - loglik: -2.8955e+02 - logprior: -1.2656e+00
Epoch 5/10
19/19 - 5s - loss: 289.7782 - loglik: -2.8798e+02 - logprior: -1.2261e+00
Epoch 6/10
19/19 - 5s - loss: 288.3962 - loglik: -2.8665e+02 - logprior: -1.2116e+00
Epoch 7/10
19/19 - 5s - loss: 288.2076 - loglik: -2.8645e+02 - logprior: -1.2056e+00
Epoch 8/10
19/19 - 5s - loss: 287.6507 - loglik: -2.8591e+02 - logprior: -1.1951e+00
Epoch 9/10
19/19 - 5s - loss: 287.3484 - loglik: -2.8563e+02 - logprior: -1.1902e+00
Epoch 10/10
19/19 - 5s - loss: 287.6284 - loglik: -2.8592e+02 - logprior: -1.1889e+00
Fitted a model with MAP estimate = -286.3038
expansions: [(0, 6), (7, 3), (8, 2), (10, 2), (28, 1), (36, 1), (37, 1), (49, 1), (52, 1), (55, 1), (58, 1), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (97, 1), (98, 1), (100, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 156 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 290.0463 - loglik: -2.8530e+02 - logprior: -4.1368e+00
Epoch 2/2
19/19 - 6s - loss: 277.1724 - loglik: -2.7513e+02 - logprior: -1.2751e+00
Fitted a model with MAP estimate = -274.1217
expansions: [(0, 4)]
discards: [  1   2   3   4   5  21  98  99 145]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 282.5847 - loglik: -2.7768e+02 - logprior: -4.0440e+00
Epoch 2/2
19/19 - 6s - loss: 276.9576 - loglik: -2.7478e+02 - logprior: -1.2813e+00
Fitted a model with MAP estimate = -274.1394
expansions: [(0, 4)]
discards: [1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 281.0022 - loglik: -2.7616e+02 - logprior: -3.8686e+00
Epoch 2/10
19/19 - 6s - loss: 276.5339 - loglik: -2.7420e+02 - logprior: -1.4249e+00
Epoch 3/10
19/19 - 6s - loss: 274.1543 - loglik: -2.7205e+02 - logprior: -1.1686e+00
Epoch 4/10
19/19 - 6s - loss: 274.0532 - loglik: -2.7203e+02 - logprior: -1.0961e+00
Epoch 5/10
19/19 - 6s - loss: 272.4343 - loglik: -2.7055e+02 - logprior: -1.0220e+00
Epoch 6/10
19/19 - 6s - loss: 272.1579 - loglik: -2.7037e+02 - logprior: -9.6990e-01
Epoch 7/10
19/19 - 6s - loss: 271.9700 - loglik: -2.7033e+02 - logprior: -8.8247e-01
Epoch 8/10
19/19 - 6s - loss: 271.6977 - loglik: -2.7021e+02 - logprior: -7.5285e-01
Epoch 9/10
19/19 - 6s - loss: 271.5151 - loglik: -2.7013e+02 - logprior: -6.8679e-01
Epoch 10/10
19/19 - 6s - loss: 271.2269 - loglik: -2.6990e+02 - logprior: -6.4981e-01
Fitted a model with MAP estimate = -270.2723
Time for alignment: 184.4951
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 408.9273 - loglik: -4.0592e+02 - logprior: -2.9929e+00
Epoch 2/10
19/19 - 5s - loss: 325.3940 - loglik: -3.2425e+02 - logprior: -1.0608e+00
Epoch 3/10
19/19 - 5s - loss: 296.5531 - loglik: -2.9467e+02 - logprior: -1.2642e+00
Epoch 4/10
19/19 - 5s - loss: 291.8350 - loglik: -2.8987e+02 - logprior: -1.2099e+00
Epoch 5/10
19/19 - 5s - loss: 289.4366 - loglik: -2.8766e+02 - logprior: -1.1924e+00
Epoch 6/10
19/19 - 5s - loss: 289.1391 - loglik: -2.8737e+02 - logprior: -1.1819e+00
Epoch 7/10
19/19 - 5s - loss: 288.2936 - loglik: -2.8658e+02 - logprior: -1.1639e+00
Epoch 8/10
19/19 - 5s - loss: 288.8282 - loglik: -2.8712e+02 - logprior: -1.1538e+00
Fitted a model with MAP estimate = -287.2615
expansions: [(0, 6), (7, 3), (8, 2), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (53, 1), (55, 1), (58, 1), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (96, 2), (97, 2), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 157 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 289.9906 - loglik: -2.8526e+02 - logprior: -4.0929e+00
Epoch 2/2
19/19 - 6s - loss: 277.0377 - loglik: -2.7497e+02 - logprior: -1.3015e+00
Fitted a model with MAP estimate = -273.7619
expansions: [(0, 4)]
discards: [  1   2   3   4   5  21  98  99 127 146]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 282.2339 - loglik: -2.7733e+02 - logprior: -4.0405e+00
Epoch 2/2
19/19 - 6s - loss: 276.5533 - loglik: -2.7438e+02 - logprior: -1.2715e+00
Fitted a model with MAP estimate = -273.8064
expansions: [(0, 4)]
discards: [1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 280.7071 - loglik: -2.7589e+02 - logprior: -3.8631e+00
Epoch 2/10
19/19 - 6s - loss: 276.0114 - loglik: -2.7364e+02 - logprior: -1.4271e+00
Epoch 3/10
19/19 - 6s - loss: 274.4663 - loglik: -2.7242e+02 - logprior: -1.1076e+00
Epoch 4/10
19/19 - 6s - loss: 273.1341 - loglik: -2.7137e+02 - logprior: -8.7802e-01
Epoch 5/10
19/19 - 7s - loss: 271.6991 - loglik: -2.7010e+02 - logprior: -7.3435e-01
Epoch 6/10
19/19 - 6s - loss: 271.5695 - loglik: -2.7009e+02 - logprior: -6.7613e-01
Epoch 7/10
19/19 - 7s - loss: 270.9535 - loglik: -2.6956e+02 - logprior: -6.4012e-01
Epoch 8/10
19/19 - 7s - loss: 270.8282 - loglik: -2.6949e+02 - logprior: -6.0923e-01
Epoch 9/10
19/19 - 7s - loss: 270.5985 - loglik: -2.6932e+02 - logprior: -5.8490e-01
Epoch 10/10
19/19 - 6s - loss: 270.5403 - loglik: -2.6932e+02 - logprior: -5.4763e-01
Fitted a model with MAP estimate = -269.4987
Time for alignment: 173.1945
Computed alignments with likelihoods: ['-269.9120', '-270.2723', '-269.4987']
Best model has likelihood: -269.4987
time for generating output: 0.2096
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13522.projection.fasta
SP score = 0.6558845941613894
Training of 3 independent models on file PF00037.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fed09a03f70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff523492c70>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fece84a7490>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fed1074a6a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fed29ecf370>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff6dabe2af0>, <__main__.SimpleDirichletPrior object at 0x7ff5238356a0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7fed114ae040>

Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 68.9688 - loglik: -6.5592e+01 - logprior: -3.3631e+00
Epoch 2/10
19/19 - 1s - loss: 52.1420 - loglik: -5.0669e+01 - logprior: -1.4631e+00
Epoch 3/10
19/19 - 1s - loss: 46.3270 - loglik: -4.4730e+01 - logprior: -1.5256e+00
Epoch 4/10
19/19 - 1s - loss: 44.6630 - loglik: -4.2944e+01 - logprior: -1.5522e+00
Epoch 5/10
19/19 - 1s - loss: 44.1970 - loglik: -4.2562e+01 - logprior: -1.5220e+00
Epoch 6/10
19/19 - 1s - loss: 44.0301 - loglik: -4.2365e+01 - logprior: -1.5208e+00
Epoch 7/10
19/19 - 1s - loss: 43.9758 - loglik: -4.2302e+01 - logprior: -1.5157e+00
Epoch 8/10
19/19 - 1s - loss: 43.8143 - loglik: -4.2141e+01 - logprior: -1.5082e+00
Epoch 9/10
19/19 - 1s - loss: 43.8232 - loglik: -4.2145e+01 - logprior: -1.4991e+00
Fitted a model with MAP estimate = -43.5726
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 47.2100 - loglik: -4.2381e+01 - logprior: -4.6872e+00
Epoch 2/2
19/19 - 1s - loss: 42.0634 - loglik: -4.0629e+01 - logprior: -1.3685e+00
Fitted a model with MAP estimate = -41.1286
expansions: [(2, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 44.2042 - loglik: -4.0802e+01 - logprior: -3.3344e+00
Epoch 2/2
19/19 - 1s - loss: 41.6242 - loglik: -3.9970e+01 - logprior: -1.5505e+00
Fitted a model with MAP estimate = -41.1206
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 43.8656 - loglik: -4.0474e+01 - logprior: -3.3001e+00
Epoch 2/10
19/19 - 1s - loss: 41.5169 - loglik: -3.9876e+01 - logprior: -1.5263e+00
Epoch 3/10
19/19 - 1s - loss: 41.1386 - loglik: -3.9528e+01 - logprior: -1.4397e+00
Epoch 4/10
19/19 - 1s - loss: 40.8209 - loglik: -3.9232e+01 - logprior: -1.3935e+00
Epoch 5/10
19/19 - 1s - loss: 40.5293 - loglik: -3.8948e+01 - logprior: -1.3711e+00
Epoch 6/10
19/19 - 1s - loss: 40.4697 - loglik: -3.8900e+01 - logprior: -1.3531e+00
Epoch 7/10
19/19 - 1s - loss: 40.4197 - loglik: -3.8854e+01 - logprior: -1.3471e+00
Epoch 8/10
19/19 - 1s - loss: 40.3356 - loglik: -3.8766e+01 - logprior: -1.3307e+00
Epoch 9/10
19/19 - 1s - loss: 40.2558 - loglik: -3.8697e+01 - logprior: -1.3272e+00
Epoch 10/10
19/19 - 1s - loss: 40.2329 - loglik: -3.8665e+01 - logprior: -1.3150e+00
Fitted a model with MAP estimate = -39.9196
Time for alignment: 37.6013
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 68.8774 - loglik: -6.5498e+01 - logprior: -3.3661e+00
Epoch 2/10
19/19 - 1s - loss: 51.8788 - loglik: -5.0411e+01 - logprior: -1.4598e+00
Epoch 3/10
19/19 - 1s - loss: 46.2826 - loglik: -4.4710e+01 - logprior: -1.5033e+00
Epoch 4/10
19/19 - 1s - loss: 44.6102 - loglik: -4.2923e+01 - logprior: -1.5438e+00
Epoch 5/10
19/19 - 1s - loss: 44.2615 - loglik: -4.2624e+01 - logprior: -1.5251e+00
Epoch 6/10
19/19 - 1s - loss: 44.0243 - loglik: -4.2373e+01 - logprior: -1.5167e+00
Epoch 7/10
19/19 - 1s - loss: 43.9762 - loglik: -4.2327e+01 - logprior: -1.5064e+00
Epoch 8/10
19/19 - 1s - loss: 43.8818 - loglik: -4.2225e+01 - logprior: -1.5013e+00
Epoch 9/10
19/19 - 1s - loss: 43.9106 - loglik: -4.2252e+01 - logprior: -1.4938e+00
Fitted a model with MAP estimate = -43.6282
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 47.1997 - loglik: -4.2393e+01 - logprior: -4.6876e+00
Epoch 2/2
19/19 - 1s - loss: 42.0591 - loglik: -4.0650e+01 - logprior: -1.3630e+00
Fitted a model with MAP estimate = -41.1927
expansions: [(2, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 44.2327 - loglik: -4.0844e+01 - logprior: -3.3368e+00
Epoch 2/2
19/19 - 1s - loss: 41.6744 - loglik: -4.0050e+01 - logprior: -1.5435e+00
Fitted a model with MAP estimate = -41.2509
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 43.8912 - loglik: -4.0545e+01 - logprior: -3.2841e+00
Epoch 2/10
19/19 - 1s - loss: 41.6183 - loglik: -4.0019e+01 - logprior: -1.5196e+00
Epoch 3/10
19/19 - 1s - loss: 41.1894 - loglik: -3.9625e+01 - logprior: -1.4330e+00
Epoch 4/10
19/19 - 1s - loss: 40.9220 - loglik: -3.9377e+01 - logprior: -1.3860e+00
Epoch 5/10
19/19 - 1s - loss: 40.6915 - loglik: -3.9137e+01 - logprior: -1.3641e+00
Epoch 6/10
19/19 - 1s - loss: 40.4808 - loglik: -3.8930e+01 - logprior: -1.3553e+00
Epoch 7/10
19/19 - 1s - loss: 40.4462 - loglik: -3.8896e+01 - logprior: -1.3403e+00
Epoch 8/10
19/19 - 1s - loss: 40.2986 - loglik: -3.8746e+01 - logprior: -1.3336e+00
Epoch 9/10
19/19 - 1s - loss: 40.2740 - loglik: -3.8717e+01 - logprior: -1.3263e+00
Epoch 10/10
19/19 - 1s - loss: 40.2092 - loglik: -3.8649e+01 - logprior: -1.3165e+00
Fitted a model with MAP estimate = -39.9288
Time for alignment: 37.2119
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 68.9554 - loglik: -6.5573e+01 - logprior: -3.3689e+00
Epoch 2/10
19/19 - 1s - loss: 52.4055 - loglik: -5.0925e+01 - logprior: -1.4700e+00
Epoch 3/10
19/19 - 1s - loss: 46.3471 - loglik: -4.4763e+01 - logprior: -1.5454e+00
Epoch 4/10
19/19 - 1s - loss: 44.1875 - loglik: -4.2463e+01 - logprior: -1.5585e+00
Epoch 5/10
19/19 - 1s - loss: 43.7412 - loglik: -4.2079e+01 - logprior: -1.5364e+00
Epoch 6/10
19/19 - 1s - loss: 43.5521 - loglik: -4.1882e+01 - logprior: -1.5262e+00
Epoch 7/10
19/19 - 1s - loss: 43.4416 - loglik: -4.1774e+01 - logprior: -1.5198e+00
Epoch 8/10
19/19 - 1s - loss: 43.3776 - loglik: -4.1719e+01 - logprior: -1.5052e+00
Epoch 9/10
19/19 - 1s - loss: 43.3942 - loglik: -4.1736e+01 - logprior: -1.4989e+00
Fitted a model with MAP estimate = -43.1310
expansions: [(0, 1), (3, 1), (4, 1), (9, 1), (11, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 47.0141 - loglik: -4.2188e+01 - logprior: -4.6834e+00
Epoch 2/2
19/19 - 1s - loss: 42.0357 - loglik: -4.0577e+01 - logprior: -1.3676e+00
Fitted a model with MAP estimate = -41.0479
expansions: [(2, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 44.1955 - loglik: -4.0774e+01 - logprior: -3.3413e+00
Epoch 2/2
19/19 - 1s - loss: 41.6300 - loglik: -3.9960e+01 - logprior: -1.5533e+00
Fitted a model with MAP estimate = -41.1142
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 43.8688 - loglik: -4.0477e+01 - logprior: -3.2925e+00
Epoch 2/10
19/19 - 1s - loss: 41.5452 - loglik: -3.9902e+01 - logprior: -1.5263e+00
Epoch 3/10
19/19 - 1s - loss: 41.1005 - loglik: -3.9496e+01 - logprior: -1.4377e+00
Epoch 4/10
19/19 - 1s - loss: 40.8381 - loglik: -3.9243e+01 - logprior: -1.3962e+00
Epoch 5/10
19/19 - 1s - loss: 40.5909 - loglik: -3.9009e+01 - logprior: -1.3675e+00
Epoch 6/10
19/19 - 1s - loss: 40.4815 - loglik: -3.8913e+01 - logprior: -1.3550e+00
Epoch 7/10
19/19 - 1s - loss: 40.3127 - loglik: -3.8742e+01 - logprior: -1.3454e+00
Epoch 8/10
19/19 - 1s - loss: 40.3022 - loglik: -3.8742e+01 - logprior: -1.3301e+00
Epoch 9/10
19/19 - 1s - loss: 40.3590 - loglik: -3.8794e+01 - logprior: -1.3247e+00
Fitted a model with MAP estimate = -39.9624
Time for alignment: 35.5790
Computed alignments with likelihoods: ['-39.9196', '-39.9288', '-39.9624']
Best model has likelihood: -39.9196
time for generating output: 0.0974
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00037.projection.fasta
SP score = 0.8964686998394864
Training of 3 independent models on file PF00224.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff6dae7e250>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fed100ca760>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff31c077490>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff2f5200070>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fed29ecf370>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fece84a7ca0>, <__main__.SimpleDirichletPrior object at 0x7fed21d929d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7fed114ae040>

Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 32s - loss: 736.6207 - loglik: -7.3489e+02 - logprior: -1.7148e+00
Epoch 2/10
39/39 - 30s - loss: 529.1538 - loglik: -5.2681e+02 - logprior: -1.9158e+00
Epoch 3/10
39/39 - 32s - loss: 516.6896 - loglik: -5.1427e+02 - logprior: -1.9021e+00
Epoch 4/10
39/39 - 33s - loss: 513.9377 - loglik: -5.1158e+02 - logprior: -1.8416e+00
Epoch 5/10
39/39 - 35s - loss: 512.9725 - loglik: -5.1063e+02 - logprior: -1.8363e+00
Epoch 6/10
39/39 - 36s - loss: 512.5099 - loglik: -5.1018e+02 - logprior: -1.8462e+00
Epoch 7/10
39/39 - 35s - loss: 512.0265 - loglik: -5.0970e+02 - logprior: -1.8565e+00
Epoch 8/10
39/39 - 36s - loss: 511.7788 - loglik: -5.0946e+02 - logprior: -1.8615e+00
Epoch 9/10
39/39 - 36s - loss: 511.7599 - loglik: -5.0943e+02 - logprior: -1.8760e+00
Epoch 10/10
39/39 - 38s - loss: 511.1922 - loglik: -5.0884e+02 - logprior: -1.8954e+00
Fitted a model with MAP estimate = -509.8718
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (18, 2), (37, 1), (39, 1), (44, 1), (45, 1), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (96, 1), (109, 1), (110, 1), (111, 1), (113, 1), (120, 1), (132, 1), (133, 1), (145, 2), (147, 1), (148, 1), (149, 1), (163, 1), (166, 1), (168, 1), (169, 1), (171, 2), (172, 1), (179, 1), (182, 1), (183, 1), (187, 1), (206, 1), (208, 1), (214, 2), (216, 1), (217, 1), (227, 1), (229, 2), (231, 1), (243, 1), (244, 1), (258, 1), (267, 1), (269, 1), (270, 1), (272, 3), (273, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 55s - loss: 482.6322 - loglik: -4.7971e+02 - logprior: -2.3043e+00
Epoch 2/2
39/39 - 51s - loss: 463.5607 - loglik: -4.6174e+02 - logprior: -1.0212e+00
Fitted a model with MAP estimate = -459.7069
expansions: []
discards: [  0   2  97 178 213]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 471.2662 - loglik: -4.6782e+02 - logprior: -2.4805e+00
Epoch 2/2
39/39 - 53s - loss: 464.1652 - loglik: -4.6270e+02 - logprior: -4.6620e-01
Fitted a model with MAP estimate = -460.4210
expansions: [(0, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 58s - loss: 466.5503 - loglik: -4.6378e+02 - logprior: -1.6645e+00
Epoch 2/10
39/39 - 54s - loss: 461.8438 - loglik: -4.6052e+02 - logprior: -2.8913e-01
Epoch 3/10
39/39 - 54s - loss: 460.0477 - loglik: -4.5871e+02 - logprior: -3.6064e-01
Epoch 4/10
39/39 - 51s - loss: 458.3687 - loglik: -4.5752e+02 - logprior: 0.0083
Epoch 5/10
39/39 - 50s - loss: 457.9411 - loglik: -4.5715e+02 - logprior: -2.9688e-02
Epoch 6/10
39/39 - 49s - loss: 456.7077 - loglik: -4.5626e+02 - logprior: 0.2444
Epoch 7/10
39/39 - 49s - loss: 456.9763 - loglik: -4.5661e+02 - logprior: 0.2845
Fitted a model with MAP estimate = -455.5401
Time for alignment: 1185.8164
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 734.9586 - loglik: -7.3322e+02 - logprior: -1.7194e+00
Epoch 2/10
39/39 - 36s - loss: 529.1685 - loglik: -5.2691e+02 - logprior: -1.8156e+00
Epoch 3/10
39/39 - 35s - loss: 517.6639 - loglik: -5.1537e+02 - logprior: -1.7889e+00
Epoch 4/10
39/39 - 32s - loss: 514.9376 - loglik: -5.1271e+02 - logprior: -1.7537e+00
Epoch 5/10
39/39 - 31s - loss: 513.8954 - loglik: -5.1165e+02 - logprior: -1.7681e+00
Epoch 6/10
39/39 - 30s - loss: 513.3660 - loglik: -5.1111e+02 - logprior: -1.7928e+00
Epoch 7/10
39/39 - 30s - loss: 513.3789 - loglik: -5.1114e+02 - logprior: -1.8036e+00
Fitted a model with MAP estimate = -511.6745
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (105, 1), (109, 1), (110, 1), (111, 1), (113, 1), (132, 3), (133, 1), (145, 2), (147, 1), (148, 1), (159, 1), (163, 1), (167, 1), (170, 1), (171, 3), (172, 1), (182, 2), (183, 1), (188, 1), (205, 1), (209, 1), (212, 1), (214, 2), (216, 1), (217, 1), (228, 1), (229, 3), (230, 1), (243, 1), (265, 1), (267, 1), (269, 3), (271, 1), (272, 2), (273, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 46s - loss: 483.0874 - loglik: -4.8008e+02 - logprior: -2.4742e+00
Epoch 2/2
39/39 - 43s - loss: 463.8381 - loglik: -4.6170e+02 - logprior: -1.3715e+00
Fitted a model with MAP estimate = -460.3367
expansions: []
discards: [  2  97 163 179 214 229 347]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 342 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 46s - loss: 469.0342 - loglik: -4.6640e+02 - logprior: -1.6908e+00
Epoch 2/2
39/39 - 42s - loss: 463.2849 - loglik: -4.6164e+02 - logprior: -6.4412e-01
Fitted a model with MAP estimate = -459.5425
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 342 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 44s - loss: 466.1797 - loglik: -4.6359e+02 - logprior: -1.5028e+00
Epoch 2/10
39/39 - 40s - loss: 462.3452 - loglik: -4.6088e+02 - logprior: -4.2507e-01
Epoch 3/10
39/39 - 40s - loss: 460.4991 - loglik: -4.5905e+02 - logprior: -4.6790e-01
Epoch 4/10
39/39 - 40s - loss: 459.1228 - loglik: -4.5817e+02 - logprior: -1.0781e-01
Epoch 5/10
39/39 - 39s - loss: 458.0851 - loglik: -4.5717e+02 - logprior: -1.5925e-01
Epoch 6/10
39/39 - 39s - loss: 457.2996 - loglik: -4.5684e+02 - logprior: 0.2260
Epoch 7/10
39/39 - 38s - loss: 457.4966 - loglik: -4.5695e+02 - logprior: 0.0862
Fitted a model with MAP estimate = -456.0235
Time for alignment: 887.5557
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 32s - loss: 733.7697 - loglik: -7.3205e+02 - logprior: -1.7068e+00
Epoch 2/10
39/39 - 27s - loss: 526.8718 - loglik: -5.2469e+02 - logprior: -1.8271e+00
Epoch 3/10
39/39 - 27s - loss: 515.2247 - loglik: -5.1294e+02 - logprior: -1.7845e+00
Epoch 4/10
39/39 - 27s - loss: 512.8479 - loglik: -5.1064e+02 - logprior: -1.6969e+00
Epoch 5/10
39/39 - 27s - loss: 511.6955 - loglik: -5.0953e+02 - logprior: -1.6758e+00
Epoch 6/10
39/39 - 27s - loss: 511.3290 - loglik: -5.0920e+02 - logprior: -1.6822e+00
Epoch 7/10
39/39 - 27s - loss: 510.8456 - loglik: -5.0872e+02 - logprior: -1.6937e+00
Epoch 8/10
39/39 - 28s - loss: 510.8801 - loglik: -5.0876e+02 - logprior: -1.6955e+00
Fitted a model with MAP estimate = -509.3096
expansions: [(0, 2), (1, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (105, 1), (109, 1), (110, 1), (111, 1), (113, 1), (120, 1), (132, 1), (133, 1), (145, 2), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (183, 1), (184, 1), (188, 1), (207, 1), (209, 1), (215, 2), (217, 2), (228, 1), (230, 3), (231, 1), (244, 1), (259, 1), (265, 1), (267, 2), (269, 1), (272, 3), (273, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 43s - loss: 481.5979 - loglik: -4.7872e+02 - logprior: -2.3306e+00
Epoch 2/2
39/39 - 40s - loss: 463.5452 - loglik: -4.6187e+02 - logprior: -9.1997e-01
Fitted a model with MAP estimate = -459.7576
expansions: []
discards: [  0  97 178 213]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 342 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 44s - loss: 469.5836 - loglik: -4.6600e+02 - logprior: -2.6553e+00
Epoch 2/2
39/39 - 41s - loss: 464.1010 - loglik: -4.6220e+02 - logprior: -9.1579e-01
Fitted a model with MAP estimate = -460.0425
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 45s - loss: 465.9240 - loglik: -4.6328e+02 - logprior: -1.5683e+00
Epoch 2/10
39/39 - 42s - loss: 461.7695 - loglik: -4.6037e+02 - logprior: -3.7853e-01
Epoch 3/10
39/39 - 44s - loss: 459.8745 - loglik: -4.5881e+02 - logprior: -9.4982e-02
Epoch 4/10
39/39 - 47s - loss: 458.4833 - loglik: -4.5748e+02 - logprior: -1.5450e-01
Epoch 5/10
39/39 - 50s - loss: 457.5505 - loglik: -4.5700e+02 - logprior: 0.1908
Epoch 6/10
39/39 - 53s - loss: 457.3934 - loglik: -4.5672e+02 - logprior: 0.0029
Epoch 7/10
39/39 - 56s - loss: 456.3882 - loglik: -4.5619e+02 - logprior: 0.4460
Epoch 8/10
39/39 - 52s - loss: 456.6591 - loglik: -4.5651e+02 - logprior: 0.4843
Fitted a model with MAP estimate = -455.4197
Time for alignment: 970.2081
Computed alignments with likelihoods: ['-455.5401', '-456.0235', '-455.4197']
Best model has likelihood: -455.4197
time for generating output: 0.3004
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00224.projection.fasta
SP score = 0.9229183293429103
Training of 3 independent models on file PF13365.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff66021baf0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fece804a940>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feca3961a90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7feca37a2af0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feca37a26a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7feca37b49a0>, <__main__.SimpleDirichletPrior object at 0x7fed109f5fd0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7feca3872ca0>

Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 398.0092 - loglik: -3.9490e+02 - logprior: -3.0950e+00
Epoch 2/10
19/19 - 6s - loss: 327.7881 - loglik: -3.2637e+02 - logprior: -1.3214e+00
Epoch 3/10
19/19 - 6s - loss: 302.0122 - loglik: -2.9980e+02 - logprior: -1.7809e+00
Epoch 4/10
19/19 - 6s - loss: 297.2875 - loglik: -2.9482e+02 - logprior: -1.6519e+00
Epoch 5/10
19/19 - 6s - loss: 294.6891 - loglik: -2.9221e+02 - logprior: -1.5942e+00
Epoch 6/10
19/19 - 6s - loss: 293.8302 - loglik: -2.9144e+02 - logprior: -1.5459e+00
Epoch 7/10
19/19 - 6s - loss: 292.9895 - loglik: -2.9070e+02 - logprior: -1.5237e+00
Epoch 8/10
19/19 - 6s - loss: 292.0390 - loglik: -2.8981e+02 - logprior: -1.5142e+00
Epoch 9/10
19/19 - 6s - loss: 292.6633 - loglik: -2.9049e+02 - logprior: -1.5084e+00
Fitted a model with MAP estimate = -290.5458
expansions: [(7, 2), (22, 1), (23, 1), (24, 1), (25, 2), (29, 2), (35, 1), (36, 1), (37, 2), (46, 3), (49, 2), (50, 2), (55, 1), (71, 1), (84, 1), (85, 3), (86, 6), (106, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 146 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 301.7581 - loglik: -2.9768e+02 - logprior: -3.3410e+00
Epoch 2/2
39/39 - 10s - loss: 289.6047 - loglik: -2.8691e+02 - logprior: -1.7113e+00
Fitted a model with MAP estimate = -284.1637
expansions: []
discards: [ 29  35  47  59  65  67 138]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 292.7147 - loglik: -2.8933e+02 - logprior: -2.4507e+00
Epoch 2/2
39/39 - 9s - loss: 288.3917 - loglik: -2.8595e+02 - logprior: -1.3765e+00
Fitted a model with MAP estimate = -283.9572
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 291.3167 - loglik: -2.8802e+02 - logprior: -2.3253e+00
Epoch 2/10
39/39 - 9s - loss: 286.9994 - loglik: -2.8467e+02 - logprior: -1.2681e+00
Epoch 3/10
39/39 - 9s - loss: 284.6089 - loglik: -2.8231e+02 - logprior: -1.1195e+00
Epoch 4/10
39/39 - 9s - loss: 282.5833 - loglik: -2.8048e+02 - logprior: -9.8741e-01
Epoch 5/10
39/39 - 9s - loss: 280.8175 - loglik: -2.7903e+02 - logprior: -8.6948e-01
Epoch 6/10
39/39 - 9s - loss: 280.3764 - loglik: -2.7877e+02 - logprior: -7.9641e-01
Epoch 7/10
39/39 - 9s - loss: 279.8807 - loglik: -2.7840e+02 - logprior: -7.2949e-01
Epoch 8/10
39/39 - 10s - loss: 279.5515 - loglik: -2.7816e+02 - logprior: -6.6713e-01
Epoch 9/10
39/39 - 10s - loss: 280.0685 - loglik: -2.7876e+02 - logprior: -6.0129e-01
Fitted a model with MAP estimate = -278.3165
Time for alignment: 236.7192
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 397.9437 - loglik: -3.9484e+02 - logprior: -3.0918e+00
Epoch 2/10
19/19 - 6s - loss: 327.2071 - loglik: -3.2575e+02 - logprior: -1.3366e+00
Epoch 3/10
19/19 - 6s - loss: 302.7090 - loglik: -3.0019e+02 - logprior: -1.7362e+00
Epoch 4/10
19/19 - 6s - loss: 298.7213 - loglik: -2.9607e+02 - logprior: -1.5791e+00
Epoch 5/10
19/19 - 6s - loss: 295.6954 - loglik: -2.9316e+02 - logprior: -1.5205e+00
Epoch 6/10
19/19 - 6s - loss: 296.2046 - loglik: -2.9380e+02 - logprior: -1.4932e+00
Fitted a model with MAP estimate = -292.4965
expansions: [(7, 2), (21, 3), (22, 1), (25, 1), (29, 2), (35, 1), (36, 1), (37, 2), (46, 3), (49, 1), (50, 2), (57, 1), (71, 1), (84, 1), (85, 3), (86, 5), (106, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 301.2050 - loglik: -2.9713e+02 - logprior: -3.2993e+00
Epoch 2/2
39/39 - 9s - loss: 289.4779 - loglik: -2.8678e+02 - logprior: -1.6700e+00
Fitted a model with MAP estimate = -284.3098
expansions: []
discards: [ 23  35  47  59  66 136]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 292.9700 - loglik: -2.8957e+02 - logprior: -2.4521e+00
Epoch 2/2
39/39 - 9s - loss: 288.2594 - loglik: -2.8578e+02 - logprior: -1.3894e+00
Fitted a model with MAP estimate = -284.0872
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 291.6122 - loglik: -2.8827e+02 - logprior: -2.3399e+00
Epoch 2/10
39/39 - 9s - loss: 287.1480 - loglik: -2.8478e+02 - logprior: -1.2798e+00
Epoch 3/10
39/39 - 10s - loss: 284.6774 - loglik: -2.8232e+02 - logprior: -1.1406e+00
Epoch 4/10
39/39 - 10s - loss: 282.5647 - loglik: -2.8045e+02 - logprior: -9.9307e-01
Epoch 5/10
39/39 - 10s - loss: 281.1251 - loglik: -2.7930e+02 - logprior: -8.8753e-01
Epoch 6/10
39/39 - 10s - loss: 280.7475 - loglik: -2.7913e+02 - logprior: -8.1243e-01
Epoch 7/10
39/39 - 10s - loss: 279.6127 - loglik: -2.7810e+02 - logprior: -7.5490e-01
Epoch 8/10
39/39 - 10s - loss: 280.0732 - loglik: -2.7866e+02 - logprior: -6.8712e-01
Fitted a model with MAP estimate = -278.7116
Time for alignment: 208.0809
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 397.9985 - loglik: -3.9489e+02 - logprior: -3.0955e+00
Epoch 2/10
19/19 - 6s - loss: 327.7417 - loglik: -3.2630e+02 - logprior: -1.3241e+00
Epoch 3/10
19/19 - 6s - loss: 301.5768 - loglik: -2.9908e+02 - logprior: -1.7292e+00
Epoch 4/10
19/19 - 6s - loss: 297.3250 - loglik: -2.9460e+02 - logprior: -1.5893e+00
Epoch 5/10
19/19 - 6s - loss: 295.4839 - loglik: -2.9291e+02 - logprior: -1.5306e+00
Epoch 6/10
19/19 - 6s - loss: 294.4980 - loglik: -2.9207e+02 - logprior: -1.5017e+00
Epoch 7/10
19/19 - 7s - loss: 294.1536 - loglik: -2.9180e+02 - logprior: -1.5209e+00
Epoch 8/10
19/19 - 7s - loss: 292.8758 - loglik: -2.9061e+02 - logprior: -1.5110e+00
Epoch 9/10
19/19 - 6s - loss: 291.5317 - loglik: -2.8934e+02 - logprior: -1.5058e+00
Epoch 10/10
19/19 - 6s - loss: 291.7157 - loglik: -2.8956e+02 - logprior: -1.5028e+00
Fitted a model with MAP estimate = -290.2508
expansions: [(7, 2), (22, 1), (23, 1), (24, 1), (25, 2), (29, 2), (35, 1), (36, 1), (37, 2), (46, 3), (49, 2), (50, 2), (55, 1), (71, 1), (84, 2), (85, 3), (86, 6), (106, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 147 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 301.9509 - loglik: -2.9785e+02 - logprior: -3.3587e+00
Epoch 2/2
39/39 - 10s - loss: 289.6616 - loglik: -2.8690e+02 - logprior: -1.8054e+00
Fitted a model with MAP estimate = -284.2282
expansions: []
discards: [ 29  35  47  59  65  67 106 139]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 292.7518 - loglik: -2.8935e+02 - logprior: -2.4786e+00
Epoch 2/2
39/39 - 10s - loss: 288.3184 - loglik: -2.8583e+02 - logprior: -1.3954e+00
Fitted a model with MAP estimate = -283.9155
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 291.5334 - loglik: -2.8818e+02 - logprior: -2.3543e+00
Epoch 2/10
39/39 - 10s - loss: 286.9385 - loglik: -2.8457e+02 - logprior: -1.2816e+00
Epoch 3/10
39/39 - 10s - loss: 284.4807 - loglik: -2.8216e+02 - logprior: -1.1271e+00
Epoch 4/10
39/39 - 10s - loss: 282.2707 - loglik: -2.8018e+02 - logprior: -9.7225e-01
Epoch 5/10
39/39 - 10s - loss: 281.4052 - loglik: -2.7961e+02 - logprior: -8.6429e-01
Epoch 6/10
39/39 - 10s - loss: 280.1476 - loglik: -2.7855e+02 - logprior: -7.8826e-01
Epoch 7/10
39/39 - 10s - loss: 280.1126 - loglik: -2.7863e+02 - logprior: -7.2499e-01
Epoch 8/10
39/39 - 10s - loss: 279.0622 - loglik: -2.7768e+02 - logprior: -6.6433e-01
Epoch 9/10
39/39 - 10s - loss: 279.2362 - loglik: -2.7793e+02 - logprior: -5.9399e-01
Fitted a model with MAP estimate = -278.2220
Time for alignment: 247.9857
Computed alignments with likelihoods: ['-278.3165', '-278.7116', '-278.2220']
Best model has likelihood: -278.2220
time for generating output: 0.2644
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13365.projection.fasta
SP score = 0.4443803581259995
Training of 3 independent models on file PF00078.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fed21d6c970>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fed20d68970>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fed100ca940>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff2f42476d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fece83e6250>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff6da99fbe0>, <__main__.SimpleDirichletPrior object at 0x7fece9bfc280>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7fed1188f160>

Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 469.3306 - loglik: -4.6686e+02 - logprior: -2.0196e+00
Epoch 2/10
39/39 - 11s - loss: 425.6631 - loglik: -4.2279e+02 - logprior: -1.5567e+00
Epoch 3/10
39/39 - 11s - loss: 418.2751 - loglik: -4.1494e+02 - logprior: -1.6435e+00
Epoch 4/10
39/39 - 11s - loss: 415.5099 - loglik: -4.1242e+02 - logprior: -1.6444e+00
Epoch 5/10
39/39 - 11s - loss: 414.2286 - loglik: -4.1139e+02 - logprior: -1.6467e+00
Epoch 6/10
39/39 - 11s - loss: 413.2901 - loglik: -4.1060e+02 - logprior: -1.6498e+00
Epoch 7/10
39/39 - 11s - loss: 412.7981 - loglik: -4.1021e+02 - logprior: -1.6459e+00
Epoch 8/10
39/39 - 11s - loss: 412.6701 - loglik: -4.1018e+02 - logprior: -1.6466e+00
Epoch 9/10
39/39 - 11s - loss: 412.2905 - loglik: -4.0986e+02 - logprior: -1.6560e+00
Epoch 10/10
39/39 - 11s - loss: 411.5873 - loglik: -4.0922e+02 - logprior: -1.6534e+00
Fitted a model with MAP estimate = -410.8887
expansions: [(19, 2), (20, 1), (31, 4), (32, 2), (46, 1), (47, 1), (48, 1), (49, 2), (76, 1), (81, 8), (95, 1), (98, 1), (99, 1), (105, 1), (108, 1), (117, 1), (118, 1), (119, 1)]
discards: [ 1 58]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 157 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 424.5836 - loglik: -4.2108e+02 - logprior: -2.2663e+00
Epoch 2/2
39/39 - 12s - loss: 411.3661 - loglik: -4.0824e+02 - logprior: -1.2184e+00
Fitted a model with MAP estimate = -406.2444
expansions: []
discards: [ 35  57  97  98  99 100]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 416.3054 - loglik: -4.1167e+02 - logprior: -2.1329e+00
Epoch 2/2
39/39 - 12s - loss: 411.1783 - loglik: -4.0770e+02 - logprior: -1.0768e+00
Fitted a model with MAP estimate = -406.4137
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 415.6322 - loglik: -4.1084e+02 - logprior: -1.9824e+00
Epoch 2/10
39/39 - 12s - loss: 410.6640 - loglik: -4.0742e+02 - logprior: -8.3775e-01
Epoch 3/10
39/39 - 12s - loss: 408.1349 - loglik: -4.0508e+02 - logprior: -7.5917e-01
Epoch 4/10
39/39 - 12s - loss: 405.6161 - loglik: -4.0299e+02 - logprior: -7.0056e-01
Epoch 5/10
39/39 - 12s - loss: 404.8957 - loglik: -4.0265e+02 - logprior: -6.4501e-01
Epoch 6/10
39/39 - 12s - loss: 403.7841 - loglik: -4.0183e+02 - logprior: -5.8123e-01
Epoch 7/10
39/39 - 12s - loss: 403.5719 - loglik: -4.0184e+02 - logprior: -5.2291e-01
Epoch 8/10
39/39 - 12s - loss: 402.5665 - loglik: -4.0104e+02 - logprior: -4.7165e-01
Epoch 9/10
39/39 - 12s - loss: 402.2572 - loglik: -4.0087e+02 - logprior: -4.1026e-01
Epoch 10/10
39/39 - 13s - loss: 401.9816 - loglik: -4.0073e+02 - logprior: -3.5134e-01
Fitted a model with MAP estimate = -400.7479
Time for alignment: 352.9182
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 469.6288 - loglik: -4.6717e+02 - logprior: -2.0090e+00
Epoch 2/10
39/39 - 11s - loss: 424.6956 - loglik: -4.2168e+02 - logprior: -1.5249e+00
Epoch 3/10
39/39 - 11s - loss: 416.7259 - loglik: -4.1333e+02 - logprior: -1.6089e+00
Epoch 4/10
39/39 - 11s - loss: 415.0847 - loglik: -4.1204e+02 - logprior: -1.5952e+00
Epoch 5/10
39/39 - 11s - loss: 413.7024 - loglik: -4.1088e+02 - logprior: -1.6048e+00
Epoch 6/10
39/39 - 11s - loss: 412.9498 - loglik: -4.1025e+02 - logprior: -1.6196e+00
Epoch 7/10
39/39 - 11s - loss: 411.5811 - loglik: -4.0899e+02 - logprior: -1.6568e+00
Epoch 8/10
39/39 - 11s - loss: 411.4466 - loglik: -4.0895e+02 - logprior: -1.6571e+00
Epoch 9/10
39/39 - 11s - loss: 410.9042 - loglik: -4.0846e+02 - logprior: -1.6627e+00
Epoch 10/10
39/39 - 11s - loss: 410.5278 - loglik: -4.0814e+02 - logprior: -1.6722e+00
Fitted a model with MAP estimate = -409.6786
expansions: [(19, 2), (20, 1), (21, 1), (31, 5), (32, 1), (33, 1), (48, 1), (49, 1), (50, 2), (54, 1), (74, 1), (75, 2), (77, 1), (78, 1), (79, 8), (106, 1), (107, 1), (108, 1), (117, 1), (118, 9)]
discards: [ 1 58 59]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 167 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 422.2234 - loglik: -4.1867e+02 - logprior: -2.3183e+00
Epoch 2/2
39/39 - 14s - loss: 407.6941 - loglik: -4.0448e+02 - logprior: -1.3191e+00
Fitted a model with MAP estimate = -402.1361
expansions: [(24, 1)]
discards: [ 21  22  37  59  99 100 101 102 103 149 150 151 152 153 154 155]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 414.1453 - loglik: -4.0951e+02 - logprior: -2.1333e+00
Epoch 2/2
39/39 - 12s - loss: 408.6084 - loglik: -4.0504e+02 - logprior: -1.0756e+00
Fitted a model with MAP estimate = -403.6605
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 412.8775 - loglik: -4.0796e+02 - logprior: -2.0192e+00
Epoch 2/10
39/39 - 13s - loss: 407.9695 - loglik: -4.0449e+02 - logprior: -9.5211e-01
Epoch 3/10
39/39 - 13s - loss: 405.6462 - loglik: -4.0236e+02 - logprior: -9.1958e-01
Epoch 4/10
39/39 - 13s - loss: 403.5930 - loglik: -4.0079e+02 - logprior: -8.5003e-01
Epoch 5/10
39/39 - 13s - loss: 402.5173 - loglik: -4.0008e+02 - logprior: -7.9176e-01
Epoch 6/10
39/39 - 13s - loss: 401.4849 - loglik: -3.9935e+02 - logprior: -7.3499e-01
Epoch 7/10
39/39 - 12s - loss: 400.9438 - loglik: -3.9904e+02 - logprior: -6.8023e-01
Epoch 8/10
39/39 - 13s - loss: 400.1212 - loglik: -3.9840e+02 - logprior: -6.2612e-01
Epoch 9/10
39/39 - 13s - loss: 400.3012 - loglik: -3.9874e+02 - logprior: -5.6270e-01
Fitted a model with MAP estimate = -398.5653
Time for alignment: 348.4302
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 468.7875 - loglik: -4.6632e+02 - logprior: -2.0180e+00
Epoch 2/10
39/39 - 11s - loss: 426.4070 - loglik: -4.2382e+02 - logprior: -1.4839e+00
Epoch 3/10
39/39 - 11s - loss: 418.7005 - loglik: -4.1552e+02 - logprior: -1.5645e+00
Epoch 4/10
39/39 - 11s - loss: 416.5450 - loglik: -4.1353e+02 - logprior: -1.5439e+00
Epoch 5/10
39/39 - 11s - loss: 414.8796 - loglik: -4.1207e+02 - logprior: -1.5543e+00
Epoch 6/10
39/39 - 11s - loss: 413.9867 - loglik: -4.1134e+02 - logprior: -1.5656e+00
Epoch 7/10
39/39 - 11s - loss: 413.2032 - loglik: -4.1065e+02 - logprior: -1.5766e+00
Epoch 8/10
39/39 - 11s - loss: 412.8741 - loglik: -4.1039e+02 - logprior: -1.5773e+00
Epoch 9/10
39/39 - 11s - loss: 412.7984 - loglik: -4.1036e+02 - logprior: -1.5885e+00
Epoch 10/10
39/39 - 11s - loss: 412.1696 - loglik: -4.0978e+02 - logprior: -1.5831e+00
Fitted a model with MAP estimate = -410.9914
expansions: [(20, 1), (21, 1), (32, 1), (33, 2), (34, 3), (49, 3), (50, 2), (59, 1), (60, 8), (75, 1), (77, 1), (79, 2), (80, 4), (93, 1), (94, 1), (108, 3), (110, 1), (112, 2)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 165 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 420.4910 - loglik: -4.1692e+02 - logprior: -2.2516e+00
Epoch 2/2
39/39 - 13s - loss: 405.8914 - loglik: -4.0270e+02 - logprior: -1.2106e+00
Fitted a model with MAP estimate = -400.4148
expansions: []
discards: [ 36  57  75  76  77  78  79 108]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 157 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 410.9178 - loglik: -4.0626e+02 - logprior: -2.1057e+00
Epoch 2/2
39/39 - 13s - loss: 405.5772 - loglik: -4.0205e+02 - logprior: -1.0568e+00
Fitted a model with MAP estimate = -400.7831
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 157 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 410.1085 - loglik: -4.0523e+02 - logprior: -1.9743e+00
Epoch 2/10
39/39 - 13s - loss: 405.4243 - loglik: -4.0197e+02 - logprior: -9.2651e-01
Epoch 3/10
39/39 - 12s - loss: 402.7338 - loglik: -3.9950e+02 - logprior: -8.7150e-01
Epoch 4/10
39/39 - 12s - loss: 400.4634 - loglik: -3.9768e+02 - logprior: -8.1774e-01
Epoch 5/10
39/39 - 12s - loss: 399.6797 - loglik: -3.9730e+02 - logprior: -7.5380e-01
Epoch 6/10
39/39 - 12s - loss: 398.5583 - loglik: -3.9647e+02 - logprior: -7.0694e-01
Epoch 7/10
39/39 - 12s - loss: 397.9384 - loglik: -3.9610e+02 - logprior: -6.4382e-01
Epoch 8/10
39/39 - 13s - loss: 398.0982 - loglik: -3.9646e+02 - logprior: -5.7378e-01
Fitted a model with MAP estimate = -396.0402
Time for alignment: 336.3521
Computed alignments with likelihoods: ['-400.7479', '-398.5653', '-396.0402']
Best model has likelihood: -396.0402
time for generating output: 0.2106
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00078.projection.fasta
SP score = 0.8226385013949781
Training of 3 independent models on file PF00150.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2f528efa0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fed09f5d970>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fece9b06a30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fece9e10400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff31c30b220>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7feca3d19fd0>, <__main__.SimpleDirichletPrior object at 0x7fecf95b0730>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff6602cc820>

Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 31s - loss: 790.4877 - loglik: -7.8844e+02 - logprior: -1.5721e+00
Epoch 2/10
39/39 - 28s - loss: 717.4642 - loglik: -7.1475e+02 - logprior: -1.1223e+00
Epoch 3/10
39/39 - 27s - loss: 705.7006 - loglik: -7.0175e+02 - logprior: -1.3423e+00
Epoch 4/10
39/39 - 26s - loss: 700.4388 - loglik: -6.9624e+02 - logprior: -1.3852e+00
Epoch 5/10
39/39 - 27s - loss: 696.9412 - loglik: -6.9274e+02 - logprior: -1.4558e+00
Epoch 6/10
39/39 - 28s - loss: 694.8575 - loglik: -6.9091e+02 - logprior: -1.5094e+00
Epoch 7/10
39/39 - 30s - loss: 691.7532 - loglik: -6.8805e+02 - logprior: -1.5546e+00
Epoch 8/10
39/39 - 30s - loss: 691.3001 - loglik: -6.8782e+02 - logprior: -1.5742e+00
Epoch 9/10
39/39 - 31s - loss: 690.1358 - loglik: -6.8677e+02 - logprior: -1.5813e+00
Epoch 10/10
39/39 - 31s - loss: 688.3989 - loglik: -6.8494e+02 - logprior: -1.5876e+00
Fitted a model with MAP estimate = -685.2051
expansions: [(0, 3), (9, 1), (10, 1), (24, 1), (40, 1), (43, 1), (51, 1), (52, 1), (61, 1), (85, 4), (86, 2), (87, 4), (88, 1), (116, 4), (117, 2), (118, 2), (121, 1), (124, 1), (154, 1), (155, 1), (175, 5), (180, 1), (181, 1), (205, 1), (207, 7), (208, 1), (214, 1)]
discards: [157]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 279 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 46s - loss: 729.4572 - loglik: -7.2364e+02 - logprior: -2.8908e+00
Epoch 2/2
39/39 - 43s - loss: 695.4814 - loglik: -6.9030e+02 - logprior: -1.1511e+00
Fitted a model with MAP estimate = -685.7079
expansions: [(0, 3), (50, 1), (97, 1), (98, 1), (178, 5), (213, 1)]
discards: [  1   2   3 144 145 146 147 187 246 247]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 281 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 45s - loss: 701.2118 - loglik: -6.9319e+02 - logprior: -3.0137e+00
Epoch 2/2
39/39 - 37s - loss: 691.7643 - loglik: -6.8599e+02 - logprior: -1.0366e+00
Fitted a model with MAP estimate = -682.9659
expansions: [(185, 1)]
discards: [  1   2   3  52 178 179 180 181]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 274 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 698.1167 - loglik: -6.9081e+02 - logprior: -1.8438e+00
Epoch 2/10
39/39 - 32s - loss: 691.1795 - loglik: -6.8567e+02 - logprior: -7.0460e-01
Epoch 3/10
39/39 - 33s - loss: 686.9169 - loglik: -6.8194e+02 - logprior: -5.7341e-01
Epoch 4/10
39/39 - 32s - loss: 682.3046 - loglik: -6.7780e+02 - logprior: -5.0139e-01
Epoch 5/10
39/39 - 32s - loss: 679.9364 - loglik: -6.7601e+02 - logprior: -4.1162e-01
Epoch 6/10
39/39 - 32s - loss: 677.3359 - loglik: -6.7402e+02 - logprior: -3.4269e-01
Epoch 7/10
39/39 - 32s - loss: 675.2264 - loglik: -6.7227e+02 - logprior: -3.3449e-01
Epoch 8/10
39/39 - 32s - loss: 673.6547 - loglik: -6.7113e+02 - logprior: -1.4558e-01
Epoch 9/10
39/39 - 32s - loss: 672.1757 - loglik: -6.6973e+02 - logprior: -2.5214e-02
Epoch 10/10
39/39 - 32s - loss: 669.7450 - loglik: -6.6710e+02 - logprior: 0.0727
Fitted a model with MAP estimate = -665.3856
Time for alignment: 952.9798
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 794.2959 - loglik: -7.9224e+02 - logprior: -1.5684e+00
Epoch 2/10
39/39 - 25s - loss: 721.6205 - loglik: -7.1895e+02 - logprior: -9.3990e-01
Epoch 3/10
39/39 - 25s - loss: 708.8363 - loglik: -7.0494e+02 - logprior: -1.2119e+00
Epoch 4/10
39/39 - 25s - loss: 701.8983 - loglik: -6.9784e+02 - logprior: -1.3149e+00
Epoch 5/10
39/39 - 25s - loss: 697.4241 - loglik: -6.9314e+02 - logprior: -1.3914e+00
Epoch 6/10
39/39 - 25s - loss: 693.9424 - loglik: -6.8994e+02 - logprior: -1.4518e+00
Epoch 7/10
39/39 - 25s - loss: 692.2892 - loglik: -6.8866e+02 - logprior: -1.4873e+00
Epoch 8/10
39/39 - 25s - loss: 690.6618 - loglik: -6.8725e+02 - logprior: -1.5230e+00
Epoch 9/10
39/39 - 25s - loss: 689.7934 - loglik: -6.8651e+02 - logprior: -1.5362e+00
Epoch 10/10
39/39 - 25s - loss: 688.3220 - loglik: -6.8502e+02 - logprior: -1.5384e+00
Fitted a model with MAP estimate = -684.9822
expansions: [(0, 3), (9, 1), (10, 1), (18, 1), (23, 1), (43, 2), (49, 1), (50, 1), (51, 1), (80, 1), (84, 3), (85, 6), (95, 1), (104, 1), (115, 4), (116, 2), (121, 1), (124, 1), (174, 4), (175, 1), (178, 1), (202, 1), (206, 7), (214, 1)]
discards: [183]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 275 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 732.0652 - loglik: -7.2632e+02 - logprior: -2.9303e+00
Epoch 2/2
39/39 - 33s - loss: 696.4465 - loglik: -6.9137e+02 - logprior: -1.1341e+00
Fitted a model with MAP estimate = -686.5742
expansions: [(0, 3), (98, 2), (99, 1), (100, 1), (189, 2), (213, 1), (214, 1), (239, 1)]
discards: [  1   2   3  52  92  93  94 140 141 211 240 247 248]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 274 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 703.2109 - loglik: -6.9532e+02 - logprior: -2.9756e+00
Epoch 2/2
39/39 - 33s - loss: 693.5897 - loglik: -6.8790e+02 - logprior: -9.8298e-01
Fitted a model with MAP estimate = -684.6797
expansions: [(188, 2)]
discards: [  1   2   3 141]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 272 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 698.9127 - loglik: -6.9162e+02 - logprior: -1.7674e+00
Epoch 2/10
39/39 - 31s - loss: 692.1943 - loglik: -6.8665e+02 - logprior: -6.6990e-01
Epoch 3/10
39/39 - 31s - loss: 687.7355 - loglik: -6.8265e+02 - logprior: -5.9504e-01
Epoch 4/10
39/39 - 30s - loss: 683.5839 - loglik: -6.7894e+02 - logprior: -6.2891e-01
Epoch 5/10
39/39 - 30s - loss: 680.6891 - loglik: -6.7684e+02 - logprior: -4.0139e-01
Epoch 6/10
39/39 - 31s - loss: 678.0804 - loglik: -6.7494e+02 - logprior: -3.1230e-01
Epoch 7/10
39/39 - 32s - loss: 676.4596 - loglik: -6.7383e+02 - logprior: -2.1026e-01
Epoch 8/10
39/39 - 32s - loss: 675.9241 - loglik: -6.7368e+02 - logprior: -1.0836e-01
Epoch 9/10
39/39 - 32s - loss: 674.0679 - loglik: -6.7203e+02 - logprior: -1.7466e-03
Epoch 10/10
39/39 - 32s - loss: 673.0809 - loglik: -6.7102e+02 - logprior: 0.0953
Fitted a model with MAP estimate = -669.1384
Time for alignment: 845.6780
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 792.1112 - loglik: -7.9005e+02 - logprior: -1.5654e+00
Epoch 2/10
39/39 - 26s - loss: 720.5007 - loglik: -7.1770e+02 - logprior: -9.8662e-01
Epoch 3/10
39/39 - 26s - loss: 707.3718 - loglik: -7.0347e+02 - logprior: -1.2560e+00
Epoch 4/10
39/39 - 26s - loss: 701.5373 - loglik: -6.9739e+02 - logprior: -1.3006e+00
Epoch 5/10
39/39 - 26s - loss: 697.3561 - loglik: -6.9322e+02 - logprior: -1.3621e+00
Epoch 6/10
39/39 - 26s - loss: 694.8904 - loglik: -6.9099e+02 - logprior: -1.4331e+00
Epoch 7/10
39/39 - 26s - loss: 692.8635 - loglik: -6.8930e+02 - logprior: -1.4540e+00
Epoch 8/10
39/39 - 26s - loss: 690.8780 - loglik: -6.8748e+02 - logprior: -1.4870e+00
Epoch 9/10
39/39 - 26s - loss: 689.7648 - loglik: -6.8637e+02 - logprior: -1.5042e+00
Epoch 10/10
39/39 - 26s - loss: 688.2634 - loglik: -6.8477e+02 - logprior: -1.5082e+00
Fitted a model with MAP estimate = -684.7090
expansions: [(0, 3), (9, 1), (10, 1), (41, 1), (43, 1), (51, 1), (53, 1), (62, 1), (81, 1), (85, 5), (86, 3), (87, 4), (105, 1), (116, 4), (117, 2), (118, 1), (122, 1), (174, 6), (179, 1), (183, 1), (189, 1), (203, 1), (206, 8), (214, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 732.5812 - loglik: -7.2655e+02 - logprior: -2.9652e+00
Epoch 2/2
39/39 - 35s - loss: 695.2409 - loglik: -6.8980e+02 - logprior: -1.2107e+00
Fitted a model with MAP estimate = -685.2313
expansions: [(100, 2), (187, 2), (213, 1), (215, 1), (221, 1)]
discards: [  1   2   3  91  92  93 218 230 251 252 253]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 276 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 700.9838 - loglik: -6.9381e+02 - logprior: -1.9172e+00
Epoch 2/2
39/39 - 33s - loss: 692.9452 - loglik: -6.8722e+02 - logprior: -8.3809e-01
Fitted a model with MAP estimate = -683.9358
expansions: [(0, 3)]
discards: [ 12  98 142]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 276 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 700.6772 - loglik: -6.9236e+02 - logprior: -2.6724e+00
Epoch 2/10
39/39 - 33s - loss: 692.7636 - loglik: -6.8701e+02 - logprior: -7.4553e-01
Epoch 3/10
39/39 - 33s - loss: 688.4769 - loglik: -6.8329e+02 - logprior: -6.0138e-01
Epoch 4/10
39/39 - 33s - loss: 683.7686 - loglik: -6.7911e+02 - logprior: -5.0979e-01
Epoch 5/10
39/39 - 33s - loss: 680.9662 - loglik: -6.7693e+02 - logprior: -4.2859e-01
Epoch 6/10
39/39 - 32s - loss: 678.1042 - loglik: -6.7474e+02 - logprior: -3.3158e-01
Epoch 7/10
39/39 - 32s - loss: 676.8417 - loglik: -6.7385e+02 - logprior: -3.6483e-01
Epoch 8/10
39/39 - 32s - loss: 675.3151 - loglik: -6.7251e+02 - logprior: -3.3730e-01
Epoch 9/10
39/39 - 31s - loss: 672.5575 - loglik: -6.6990e+02 - logprior: 0.0063
Epoch 10/10
39/39 - 31s - loss: 669.5311 - loglik: -6.6642e+02 - logprior: 0.2035
Fitted a model with MAP estimate = -663.5933
Time for alignment: 878.9106
Computed alignments with likelihoods: ['-665.3856', '-669.1384', '-663.5933']
Best model has likelihood: -663.5933
time for generating output: 0.3975
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00150.projection.fasta
SP score = 0.572513966480447
Training of 3 independent models on file PF13561.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff6daf84d00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fed1075df70>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fed106b4100>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff51b019f70>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff51b019580>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff2fc100fa0>, <__main__.SimpleDirichletPrior object at 0x7feca5364eb0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7ff6602cc820>

Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 573.2168 - loglik: -5.7082e+02 - logprior: -1.9588e+00
Epoch 2/10
39/39 - 13s - loss: 478.2625 - loglik: -4.7535e+02 - logprior: -1.8808e+00
Epoch 3/10
39/39 - 13s - loss: 470.5732 - loglik: -4.6735e+02 - logprior: -1.9586e+00
Epoch 4/10
39/39 - 13s - loss: 468.4559 - loglik: -4.6549e+02 - logprior: -1.9174e+00
Epoch 5/10
39/39 - 13s - loss: 467.3671 - loglik: -4.6448e+02 - logprior: -1.9067e+00
Epoch 6/10
39/39 - 13s - loss: 466.5910 - loglik: -4.6376e+02 - logprior: -1.9179e+00
Epoch 7/10
39/39 - 13s - loss: 466.2060 - loglik: -4.6338e+02 - logprior: -1.9419e+00
Epoch 8/10
39/39 - 13s - loss: 465.8394 - loglik: -4.6304e+02 - logprior: -1.9566e+00
Epoch 9/10
39/39 - 13s - loss: 465.4575 - loglik: -4.6271e+02 - logprior: -1.9636e+00
Epoch 10/10
39/39 - 13s - loss: 465.4106 - loglik: -4.6271e+02 - logprior: -1.9704e+00
Fitted a model with MAP estimate = -461.5284
expansions: [(12, 1), (19, 1), (22, 1), (23, 2), (24, 1), (31, 1), (33, 3), (35, 1), (36, 1), (38, 1), (43, 1), (46, 1), (48, 1), (49, 1), (66, 2), (67, 1), (68, 1), (71, 1), (77, 1), (87, 1), (88, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (128, 1), (149, 2), (150, 1), (151, 4), (153, 1), (154, 1), (155, 1), (156, 1), (157, 2), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 462.6872 - loglik: -4.5851e+02 - logprior: -2.9663e+00
Epoch 2/2
39/39 - 17s - loss: 448.0457 - loglik: -4.4466e+02 - logprior: -1.6567e+00
Fitted a model with MAP estimate = -440.8456
expansions: [(3, 1)]
discards: [  0  26  83 139 143 185 191 197]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 452.8727 - loglik: -4.4781e+02 - logprior: -2.7805e+00
Epoch 2/2
39/39 - 17s - loss: 446.9392 - loglik: -4.4375e+02 - logprior: -1.1030e+00
Fitted a model with MAP estimate = -440.6726
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 449.6731 - loglik: -4.4454e+02 - logprior: -2.6851e+00
Epoch 2/10
39/39 - 17s - loss: 444.9780 - loglik: -4.4143e+02 - logprior: -1.4506e+00
Epoch 3/10
39/39 - 17s - loss: 442.1384 - loglik: -4.3970e+02 - logprior: -5.5976e-01
Epoch 4/10
39/39 - 17s - loss: 441.4169 - loglik: -4.3931e+02 - logprior: -4.5955e-01
Epoch 5/10
39/39 - 18s - loss: 439.9157 - loglik: -4.3806e+02 - logprior: -3.5556e-01
Epoch 6/10
39/39 - 18s - loss: 439.5044 - loglik: -4.3785e+02 - logprior: -2.6847e-01
Epoch 7/10
39/39 - 18s - loss: 438.9770 - loglik: -4.3755e+02 - logprior: -1.6153e-01
Epoch 8/10
39/39 - 18s - loss: 437.9128 - loglik: -4.3664e+02 - logprior: -8.3722e-02
Epoch 9/10
39/39 - 18s - loss: 437.7065 - loglik: -4.3659e+02 - logprior: 0.0187
Epoch 10/10
39/39 - 18s - loss: 437.1816 - loglik: -4.3619e+02 - logprior: 0.1096
Fitted a model with MAP estimate = -435.1090
Time for alignment: 477.3987
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 575.3069 - loglik: -5.7290e+02 - logprior: -1.9490e+00
Epoch 2/10
39/39 - 14s - loss: 479.4519 - loglik: -4.7621e+02 - logprior: -1.9701e+00
Epoch 3/10
39/39 - 14s - loss: 471.8328 - loglik: -4.6862e+02 - logprior: -1.9803e+00
Epoch 4/10
39/39 - 14s - loss: 469.4615 - loglik: -4.6638e+02 - logprior: -1.9279e+00
Epoch 5/10
39/39 - 14s - loss: 468.4418 - loglik: -4.6546e+02 - logprior: -1.9347e+00
Epoch 6/10
39/39 - 14s - loss: 467.7238 - loglik: -4.6481e+02 - logprior: -1.9524e+00
Epoch 7/10
39/39 - 14s - loss: 467.2536 - loglik: -4.6439e+02 - logprior: -1.9473e+00
Epoch 8/10
39/39 - 14s - loss: 466.9037 - loglik: -4.6410e+02 - logprior: -1.9623e+00
Epoch 9/10
39/39 - 14s - loss: 466.6242 - loglik: -4.6386e+02 - logprior: -1.9651e+00
Epoch 10/10
39/39 - 14s - loss: 466.3701 - loglik: -4.6367e+02 - logprior: -1.9696e+00
Fitted a model with MAP estimate = -462.5925
expansions: [(8, 1), (11, 1), (18, 1), (23, 2), (24, 1), (31, 1), (33, 3), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (49, 1), (66, 1), (67, 2), (68, 1), (71, 1), (77, 1), (87, 1), (88, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (133, 1), (149, 2), (150, 1), (151, 4), (153, 1), (154, 1), (155, 1), (156, 1), (157, 2), (163, 1), (169, 1), (180, 1), (182, 2), (183, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 463.4471 - loglik: -4.5925e+02 - logprior: -2.9596e+00
Epoch 2/2
39/39 - 19s - loss: 448.1152 - loglik: -4.4472e+02 - logprior: -1.6562e+00
Fitted a model with MAP estimate = -440.8357
expansions: [(3, 1)]
discards: [  0  26  83 139 143 185 191 196]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 452.7166 - loglik: -4.4766e+02 - logprior: -2.7761e+00
Epoch 2/2
39/39 - 18s - loss: 447.0239 - loglik: -4.4382e+02 - logprior: -1.0968e+00
Fitted a model with MAP estimate = -440.6748
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 449.5394 - loglik: -4.4439e+02 - logprior: -2.6813e+00
Epoch 2/10
39/39 - 18s - loss: 444.9126 - loglik: -4.4136e+02 - logprior: -1.4565e+00
Epoch 3/10
39/39 - 18s - loss: 442.5290 - loglik: -4.4008e+02 - logprior: -5.5314e-01
Epoch 4/10
39/39 - 18s - loss: 440.9985 - loglik: -4.3891e+02 - logprior: -4.5051e-01
Epoch 5/10
39/39 - 18s - loss: 440.0352 - loglik: -4.3819e+02 - logprior: -3.5060e-01
Epoch 6/10
39/39 - 18s - loss: 439.6248 - loglik: -4.3797e+02 - logprior: -2.5791e-01
Epoch 7/10
39/39 - 18s - loss: 438.9807 - loglik: -4.3754e+02 - logprior: -1.5934e-01
Epoch 8/10
39/39 - 18s - loss: 437.8998 - loglik: -4.3664e+02 - logprior: -5.9891e-02
Epoch 9/10
39/39 - 18s - loss: 437.5446 - loglik: -4.3643e+02 - logprior: 0.0205
Epoch 10/10
39/39 - 17s - loss: 436.7020 - loglik: -4.3572e+02 - logprior: 0.1209
Fitted a model with MAP estimate = -435.1272
Time for alignment: 501.9968
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 573.2776 - loglik: -5.7088e+02 - logprior: -1.9720e+00
Epoch 2/10
39/39 - 14s - loss: 477.5657 - loglik: -4.7446e+02 - logprior: -1.9923e+00
Epoch 3/10
39/39 - 14s - loss: 470.6958 - loglik: -4.6760e+02 - logprior: -1.9180e+00
Epoch 4/10
39/39 - 14s - loss: 468.5726 - loglik: -4.6566e+02 - logprior: -1.8616e+00
Epoch 5/10
39/39 - 14s - loss: 467.4708 - loglik: -4.6462e+02 - logprior: -1.8774e+00
Epoch 6/10
39/39 - 14s - loss: 466.8986 - loglik: -4.6410e+02 - logprior: -1.8831e+00
Epoch 7/10
39/39 - 14s - loss: 466.4118 - loglik: -4.6364e+02 - logprior: -1.8880e+00
Epoch 8/10
39/39 - 13s - loss: 465.9875 - loglik: -4.6326e+02 - logprior: -1.8952e+00
Epoch 9/10
39/39 - 14s - loss: 465.7141 - loglik: -4.6302e+02 - logprior: -1.9028e+00
Epoch 10/10
39/39 - 13s - loss: 465.4831 - loglik: -4.6284e+02 - logprior: -1.9027e+00
Fitted a model with MAP estimate = -461.6982
expansions: [(8, 1), (15, 1), (22, 1), (23, 2), (24, 1), (29, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 2), (37, 1), (42, 1), (45, 1), (46, 1), (48, 1), (65, 2), (66, 1), (69, 1), (70, 1), (83, 1), (86, 1), (87, 1), (92, 1), (93, 1), (96, 1), (108, 1), (109, 1), (111, 2), (122, 1), (128, 1), (149, 2), (150, 1), (151, 4), (153, 1), (154, 1), (155, 2), (156, 4), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 463.2131 - loglik: -4.5902e+02 - logprior: -2.9466e+00
Epoch 2/2
39/39 - 17s - loss: 448.0425 - loglik: -4.4462e+02 - logprior: -1.6847e+00
Fitted a model with MAP estimate = -440.6758
expansions: [(3, 1)]
discards: [  0  26  83 142 184 190 196 200 202]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 452.6208 - loglik: -4.4752e+02 - logprior: -2.7863e+00
Epoch 2/2
39/39 - 17s - loss: 447.0181 - loglik: -4.4374e+02 - logprior: -1.0978e+00
Fitted a model with MAP estimate = -440.5054
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 449.6997 - loglik: -4.4450e+02 - logprior: -2.6816e+00
Epoch 2/10
39/39 - 17s - loss: 444.9247 - loglik: -4.4131e+02 - logprior: -1.4494e+00
Epoch 3/10
39/39 - 17s - loss: 442.0222 - loglik: -4.3952e+02 - logprior: -5.4520e-01
Epoch 4/10
39/39 - 17s - loss: 440.8521 - loglik: -4.3870e+02 - logprior: -4.5623e-01
Epoch 5/10
39/39 - 17s - loss: 439.9738 - loglik: -4.3805e+02 - logprior: -3.5506e-01
Epoch 6/10
39/39 - 17s - loss: 439.2738 - loglik: -4.3758e+02 - logprior: -2.6274e-01
Epoch 7/10
39/39 - 17s - loss: 438.4289 - loglik: -4.3694e+02 - logprior: -1.6539e-01
Epoch 8/10
39/39 - 18s - loss: 437.7127 - loglik: -4.3640e+02 - logprior: -7.3542e-02
Epoch 9/10
39/39 - 17s - loss: 436.6476 - loglik: -4.3552e+02 - logprior: 0.0137
Epoch 10/10
39/39 - 17s - loss: 436.5518 - loglik: -4.3556e+02 - logprior: 0.1159
Fitted a model with MAP estimate = -434.7141
Time for alignment: 477.1215
Computed alignments with likelihoods: ['-435.1090', '-435.1272', '-434.7141']
Best model has likelihood: -434.7141
time for generating output: 0.3997
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13561.projection.fasta
SP score = 0.736330852379318
Training of 3 independent models on file PF05746.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4a03b01f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4a03b0490>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044ee50>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e9d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044ed00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e580>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4a044e7f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e6d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eeb0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044eac0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a044e310>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431700>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431130>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431b50>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431c10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431760>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4a0431100> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fecf985dd30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff517976a00>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fecf9b9bdf0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fecf9b91a90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4a03a5e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4a0431610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fecf9b910a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff6e3a031f0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff4a037a160>, <function make_default_emission_matrix at 0x7ff4a037a160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7feca4111340>, <__main__.SimpleDirichletPrior object at 0x7fed08363af0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7feca43f33a0>

Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 320.7647 - loglik: -3.1753e+02 - logprior: -3.1470e+00
Epoch 2/10
19/19 - 3s - loss: 276.2285 - loglik: -2.7413e+02 - logprior: -1.4523e+00
Epoch 3/10
19/19 - 3s - loss: 251.9861 - loglik: -2.4894e+02 - logprior: -1.8666e+00
Epoch 4/10
19/19 - 3s - loss: 245.4648 - loglik: -2.4285e+02 - logprior: -1.7877e+00
Epoch 5/10
19/19 - 3s - loss: 242.9511 - loglik: -2.4063e+02 - logprior: -1.8103e+00
Epoch 6/10
19/19 - 3s - loss: 241.8809 - loglik: -2.3972e+02 - logprior: -1.7765e+00
Epoch 7/10
19/19 - 3s - loss: 241.5328 - loglik: -2.3943e+02 - logprior: -1.7486e+00
Epoch 8/10
19/19 - 3s - loss: 241.3242 - loglik: -2.3926e+02 - logprior: -1.7263e+00
Epoch 9/10
19/19 - 3s - loss: 241.4416 - loglik: -2.3941e+02 - logprior: -1.7136e+00
Fitted a model with MAP estimate = -240.6302
expansions: [(17, 2), (18, 4), (19, 2), (21, 2), (24, 1), (28, 2), (32, 2), (34, 1), (40, 1), (48, 1), (50, 1), (56, 1), (63, 1), (64, 1), (65, 2), (70, 1), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 248.3627 - loglik: -2.4380e+02 - logprior: -4.0897e+00
Epoch 2/2
19/19 - 4s - loss: 236.1532 - loglik: -2.3318e+02 - logprior: -2.2087e+00
Fitted a model with MAP estimate = -233.1742
expansions: [(0, 2)]
discards: [ 0 16 17 18 28 38 44 87]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 118 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 237.1406 - loglik: -2.3317e+02 - logprior: -3.0376e+00
Epoch 2/2
19/19 - 4s - loss: 232.8881 - loglik: -2.3084e+02 - logprior: -1.1940e+00
Fitted a model with MAP estimate = -230.8315
expansions: []
discards: [ 0 22]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 116 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 239.4873 - loglik: -2.3466e+02 - logprior: -3.8608e+00
Epoch 2/10
19/19 - 4s - loss: 234.1499 - loglik: -2.3189e+02 - logprior: -1.4038e+00
Epoch 3/10
19/19 - 4s - loss: 231.8250 - loglik: -2.2977e+02 - logprior: -1.2187e+00
Epoch 4/10
19/19 - 4s - loss: 231.1729 - loglik: -2.2923e+02 - logprior: -1.1701e+00
Epoch 5/10
19/19 - 4s - loss: 230.4443 - loglik: -2.2864e+02 - logprior: -1.1270e+00
Epoch 6/10
19/19 - 4s - loss: 230.3258 - loglik: -2.2863e+02 - logprior: -1.0886e+00
Epoch 7/10
19/19 - 4s - loss: 230.1157 - loglik: -2.2851e+02 - logprior: -1.0650e+00
Epoch 8/10
19/19 - 4s - loss: 229.7112 - loglik: -2.2818e+02 - logprior: -1.0360e+00
Epoch 9/10
19/19 - 4s - loss: 229.4491 - loglik: -2.2797e+02 - logprior: -1.0112e+00
Epoch 10/10
19/19 - 4s - loss: 229.8337 - loglik: -2.2842e+02 - logprior: -9.7979e-01
Fitted a model with MAP estimate = -228.8370
Time for alignment: 122.5280
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 321.1212 - loglik: -3.1788e+02 - logprior: -3.1491e+00
Epoch 2/10
19/19 - 3s - loss: 277.1557 - loglik: -2.7513e+02 - logprior: -1.4409e+00
Epoch 3/10
19/19 - 3s - loss: 253.3781 - loglik: -2.5075e+02 - logprior: -1.8549e+00
Epoch 4/10
19/19 - 3s - loss: 245.9621 - loglik: -2.4343e+02 - logprior: -1.8225e+00
Epoch 5/10
19/19 - 3s - loss: 244.6663 - loglik: -2.4237e+02 - logprior: -1.7905e+00
Epoch 6/10
19/19 - 3s - loss: 243.4149 - loglik: -2.4125e+02 - logprior: -1.7346e+00
Epoch 7/10
19/19 - 3s - loss: 242.7038 - loglik: -2.4059e+02 - logprior: -1.7217e+00
Epoch 8/10
19/19 - 3s - loss: 242.4095 - loglik: -2.4033e+02 - logprior: -1.7209e+00
Epoch 9/10
19/19 - 3s - loss: 242.1856 - loglik: -2.4014e+02 - logprior: -1.7124e+00
Epoch 10/10
19/19 - 3s - loss: 241.7854 - loglik: -2.3977e+02 - logprior: -1.7052e+00
Fitted a model with MAP estimate = -241.4239
expansions: [(17, 2), (20, 4), (23, 2), (27, 1), (28, 2), (32, 2), (37, 1), (40, 1), (43, 1), (44, 1), (56, 1), (63, 1), (64, 1), (66, 2), (68, 1), (69, 1), (71, 1), (76, 1), (77, 1), (79, 1), (81, 1), (84, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 248.6076 - loglik: -2.4403e+02 - logprior: -4.1194e+00
Epoch 2/2
19/19 - 4s - loss: 236.4735 - loglik: -2.3349e+02 - logprior: -2.2342e+00
Fitted a model with MAP estimate = -233.4062
expansions: [(0, 2)]
discards: [ 0 36 42 85 90]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 236.0451 - loglik: -2.3210e+02 - logprior: -3.0333e+00
Epoch 2/2
19/19 - 4s - loss: 231.6597 - loglik: -2.2959e+02 - logprior: -1.2014e+00
Fitted a model with MAP estimate = -229.8200
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 119 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 238.2381 - loglik: -2.3339e+02 - logprior: -3.8692e+00
Epoch 2/10
19/19 - 4s - loss: 233.2304 - loglik: -2.3094e+02 - logprior: -1.4089e+00
Epoch 3/10
19/19 - 4s - loss: 231.0832 - loglik: -2.2902e+02 - logprior: -1.2066e+00
Epoch 4/10
19/19 - 4s - loss: 230.3324 - loglik: -2.2838e+02 - logprior: -1.1558e+00
Epoch 5/10
19/19 - 4s - loss: 229.6058 - loglik: -2.2779e+02 - logprior: -1.1208e+00
Epoch 6/10
19/19 - 4s - loss: 229.5737 - loglik: -2.2787e+02 - logprior: -1.0820e+00
Epoch 7/10
19/19 - 4s - loss: 229.2377 - loglik: -2.2761e+02 - logprior: -1.0566e+00
Epoch 8/10
19/19 - 4s - loss: 228.7036 - loglik: -2.2716e+02 - logprior: -1.0242e+00
Epoch 9/10
19/19 - 4s - loss: 228.9773 - loglik: -2.2750e+02 - logprior: -9.9365e-01
Fitted a model with MAP estimate = -228.0859
Time for alignment: 123.2365
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 320.8202 - loglik: -3.1758e+02 - logprior: -3.1459e+00
Epoch 2/10
19/19 - 3s - loss: 275.3453 - loglik: -2.7328e+02 - logprior: -1.4484e+00
Epoch 3/10
19/19 - 3s - loss: 251.4765 - loglik: -2.4853e+02 - logprior: -1.8492e+00
Epoch 4/10
19/19 - 3s - loss: 245.2609 - loglik: -2.4269e+02 - logprior: -1.8002e+00
Epoch 5/10
19/19 - 3s - loss: 242.8904 - loglik: -2.4057e+02 - logprior: -1.8081e+00
Epoch 6/10
19/19 - 3s - loss: 242.2551 - loglik: -2.4007e+02 - logprior: -1.7616e+00
Epoch 7/10
19/19 - 3s - loss: 242.1691 - loglik: -2.4004e+02 - logprior: -1.7401e+00
Epoch 8/10
19/19 - 3s - loss: 241.2886 - loglik: -2.3919e+02 - logprior: -1.7320e+00
Epoch 9/10
19/19 - 3s - loss: 241.4397 - loglik: -2.3938e+02 - logprior: -1.7250e+00
Fitted a model with MAP estimate = -240.7689
expansions: [(17, 2), (20, 4), (27, 1), (28, 2), (32, 2), (34, 1), (40, 1), (44, 1), (48, 1), (49, 1), (55, 1), (62, 1), (63, 1), (65, 2), (71, 1), (74, 1), (76, 1), (79, 1), (85, 1), (89, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 248.5602 - loglik: -2.4396e+02 - logprior: -4.1063e+00
Epoch 2/2
19/19 - 4s - loss: 236.7081 - loglik: -2.3374e+02 - logprior: -2.1970e+00
Fitted a model with MAP estimate = -233.6408
expansions: [(0, 2), (21, 2)]
discards: [ 0 34 40 83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 236.0797 - loglik: -2.3209e+02 - logprior: -3.0499e+00
Epoch 2/2
19/19 - 4s - loss: 231.5781 - loglik: -2.2951e+02 - logprior: -1.2034e+00
Fitted a model with MAP estimate = -229.6950
expansions: []
discards: [ 0 17]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 118 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 238.5539 - loglik: -2.3374e+02 - logprior: -3.8335e+00
Epoch 2/10
19/19 - 4s - loss: 233.2697 - loglik: -2.3100e+02 - logprior: -1.3875e+00
Epoch 3/10
19/19 - 4s - loss: 231.1230 - loglik: -2.2903e+02 - logprior: -1.2129e+00
Epoch 4/10
19/19 - 4s - loss: 230.2509 - loglik: -2.2829e+02 - logprior: -1.1567e+00
Epoch 5/10
19/19 - 4s - loss: 229.9152 - loglik: -2.2809e+02 - logprior: -1.1172e+00
Epoch 6/10
19/19 - 4s - loss: 229.7624 - loglik: -2.2804e+02 - logprior: -1.0868e+00
Epoch 7/10
19/19 - 4s - loss: 229.4251 - loglik: -2.2779e+02 - logprior: -1.0610e+00
Epoch 8/10
19/19 - 4s - loss: 228.8329 - loglik: -2.2729e+02 - logprior: -1.0334e+00
Epoch 9/10
19/19 - 4s - loss: 228.9306 - loglik: -2.2745e+02 - logprior: -9.9869e-01
Fitted a model with MAP estimate = -228.2233
Time for alignment: 120.0632
Computed alignments with likelihoods: ['-228.8370', '-228.0859', '-228.2233']
Best model has likelihood: -228.0859
time for generating output: 0.1632
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF05746.projection.fasta
SP score = 0.8620961386918834
