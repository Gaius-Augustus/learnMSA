Training of 3 independent models on file cys.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1f1c4a91f0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1f1c4a9280>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c4a92b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c4a9370>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 172 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 577.3536 - loglik: -5.6650e+02 - logprior: -1.0849e+01
Epoch 2/10
12/12 - 7s - loss: 504.1205 - loglik: -5.0211e+02 - logprior: -2.0124e+00
Epoch 3/10
12/12 - 7s - loss: 442.0945 - loglik: -4.4042e+02 - logprior: -1.6740e+00
Epoch 4/10
12/12 - 7s - loss: 416.7576 - loglik: -4.1465e+02 - logprior: -2.1028e+00
Epoch 5/10
12/12 - 7s - loss: 410.0271 - loglik: -4.0786e+02 - logprior: -2.1675e+00
Epoch 6/10
12/12 - 7s - loss: 406.2875 - loglik: -4.0419e+02 - logprior: -2.0989e+00
Epoch 7/10
12/12 - 7s - loss: 403.9865 - loglik: -4.0180e+02 - logprior: -2.1866e+00
Epoch 8/10
12/12 - 7s - loss: 404.2827 - loglik: -4.0210e+02 - logprior: -2.1833e+00
Fitted a model with MAP estimate = -403.3981
expansions: [(9, 3), (10, 1), (11, 2), (31, 1), (32, 2), (33, 2), (35, 2), (36, 1), (47, 3), (58, 1), (60, 1), (62, 2), (74, 1), (75, 1), (80, 1), (82, 1), (83, 2), (87, 1), (90, 1), (92, 1), (95, 1), (98, 1), (101, 1), (112, 2), (115, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (150, 1), (159, 1), (163, 1), (167, 2)]
discards: [0]
Fitting a model of length 220 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 13s - loss: 411.7215 - loglik: -4.0106e+02 - logprior: -1.0658e+01
Epoch 2/2
12/12 - 10s - loss: 393.6513 - loglik: -3.8953e+02 - logprior: -4.1261e+00
Fitted a model with MAP estimate = -389.6834
expansions: [(0, 3)]
discards: [  0   9  41  45  82 144 178]
Fitting a model of length 216 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 13s - loss: 396.6086 - loglik: -3.8893e+02 - logprior: -7.6812e+00
Epoch 2/2
12/12 - 10s - loss: 386.2892 - loglik: -3.8491e+02 - logprior: -1.3794e+00
Fitted a model with MAP estimate = -385.3287
expansions: []
discards: [  0   2  61  62 106]
Fitting a model of length 211 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 13s - loss: 400.2786 - loglik: -3.9069e+02 - logprior: -9.5862e+00
Epoch 2/10
12/12 - 10s - loss: 389.4751 - loglik: -3.8612e+02 - logprior: -3.3527e+00
Epoch 3/10
12/12 - 10s - loss: 388.3545 - loglik: -3.8651e+02 - logprior: -1.8419e+00
Epoch 4/10
12/12 - 11s - loss: 385.6564 - loglik: -3.8541e+02 - logprior: -2.4232e-01
Epoch 5/10
12/12 - 11s - loss: 383.7878 - loglik: -3.8391e+02 - logprior: 0.1177
Epoch 6/10
12/12 - 11s - loss: 382.4272 - loglik: -3.8259e+02 - logprior: 0.1642
Epoch 7/10
12/12 - 11s - loss: 382.0509 - loglik: -3.8216e+02 - logprior: 0.1089
Epoch 8/10
12/12 - 11s - loss: 381.4157 - loglik: -3.8142e+02 - logprior: 9.9870e-04
Epoch 9/10
12/12 - 11s - loss: 380.9688 - loglik: -3.8098e+02 - logprior: 0.0096
Epoch 10/10
12/12 - 12s - loss: 380.9086 - loglik: -3.8096e+02 - logprior: 0.0498
Fitted a model with MAP estimate = -380.9131
Time for alignment: 249.8630
Fitting a model of length 172 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 11s - loss: 577.6940 - loglik: -5.6680e+02 - logprior: -1.0892e+01
Epoch 2/10
12/12 - 8s - loss: 507.5967 - loglik: -5.0557e+02 - logprior: -2.0275e+00
Epoch 3/10
12/12 - 8s - loss: 443.6213 - loglik: -4.4209e+02 - logprior: -1.5307e+00
Epoch 4/10
12/12 - 8s - loss: 418.1007 - loglik: -4.1625e+02 - logprior: -1.8495e+00
Epoch 5/10
12/12 - 8s - loss: 410.0187 - loglik: -4.0817e+02 - logprior: -1.8537e+00
Epoch 6/10
12/12 - 8s - loss: 408.8363 - loglik: -4.0708e+02 - logprior: -1.7538e+00
Epoch 7/10
12/12 - 8s - loss: 405.3328 - loglik: -4.0346e+02 - logprior: -1.8777e+00
Epoch 8/10
12/12 - 8s - loss: 405.2536 - loglik: -4.0333e+02 - logprior: -1.9252e+00
Epoch 9/10
12/12 - 8s - loss: 405.3498 - loglik: -4.0343e+02 - logprior: -1.9186e+00
Fitted a model with MAP estimate = -404.8031
expansions: [(9, 4), (11, 1), (16, 1), (31, 3), (32, 2), (34, 2), (47, 3), (58, 1), (60, 1), (62, 2), (76, 1), (81, 2), (82, 2), (83, 2), (90, 2), (92, 1), (96, 1), (98, 1), (112, 2), (115, 1), (123, 2), (126, 2), (137, 1), (140, 2), (160, 1), (162, 2), (168, 2)]
discards: [0]
Fitting a model of length 218 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 13s - loss: 412.6085 - loglik: -4.0191e+02 - logprior: -1.0697e+01
Epoch 2/2
12/12 - 11s - loss: 393.3030 - loglik: -3.8914e+02 - logprior: -4.1644e+00
Fitted a model with MAP estimate = -389.9750
expansions: [(0, 3), (213, 1)]
discards: [  0  40  44  61  81 107]
Fitting a model of length 216 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 14s - loss: 396.6609 - loglik: -3.8895e+02 - logprior: -7.7129e+00
Epoch 2/2
12/12 - 11s - loss: 387.4726 - loglik: -3.8601e+02 - logprior: -1.4655e+00
Fitted a model with MAP estimate = -385.5198
expansions: []
discards: [  0   2 102 202]
Fitting a model of length 212 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 14s - loss: 397.7191 - loglik: -3.8807e+02 - logprior: -9.6472e+00
Epoch 2/10
12/12 - 11s - loss: 391.6702 - loglik: -3.8828e+02 - logprior: -3.3949e+00
Epoch 3/10
12/12 - 11s - loss: 386.5913 - loglik: -3.8475e+02 - logprior: -1.8372e+00
Epoch 4/10
12/12 - 11s - loss: 384.4765 - loglik: -3.8423e+02 - logprior: -2.4432e-01
Epoch 5/10
12/12 - 10s - loss: 382.8966 - loglik: -3.8300e+02 - logprior: 0.1015
Epoch 6/10
12/12 - 10s - loss: 383.6430 - loglik: -3.8381e+02 - logprior: 0.1700
Fitted a model with MAP estimate = -381.6469
Time for alignment: 229.7636
Fitting a model of length 172 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 578.4691 - loglik: -5.6761e+02 - logprior: -1.0856e+01
Epoch 2/10
12/12 - 7s - loss: 506.1472 - loglik: -5.0413e+02 - logprior: -2.0159e+00
Epoch 3/10
12/12 - 7s - loss: 441.5162 - loglik: -4.3984e+02 - logprior: -1.6745e+00
Epoch 4/10
12/12 - 8s - loss: 418.0520 - loglik: -4.1599e+02 - logprior: -2.0644e+00
Epoch 5/10
12/12 - 8s - loss: 410.9875 - loglik: -4.0892e+02 - logprior: -2.0643e+00
Epoch 6/10
12/12 - 8s - loss: 408.8408 - loglik: -4.0695e+02 - logprior: -1.8862e+00
Epoch 7/10
12/12 - 8s - loss: 405.3690 - loglik: -4.0341e+02 - logprior: -1.9561e+00
Epoch 8/10
12/12 - 8s - loss: 406.0424 - loglik: -4.0409e+02 - logprior: -1.9535e+00
Fitted a model with MAP estimate = -405.4525
expansions: [(9, 3), (10, 1), (11, 2), (31, 3), (32, 2), (34, 2), (47, 3), (57, 2), (59, 1), (62, 1), (75, 1), (80, 1), (82, 1), (83, 2), (90, 2), (92, 1), (95, 1), (98, 1), (101, 1), (112, 2), (114, 1), (123, 2), (126, 2), (137, 2), (139, 1), (150, 1), (159, 1), (163, 1), (167, 2)]
discards: [0]
Fitting a model of length 217 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 14s - loss: 411.7226 - loglik: -4.0108e+02 - logprior: -1.0647e+01
Epoch 2/2
12/12 - 11s - loss: 394.5918 - loglik: -3.9053e+02 - logprior: -4.0612e+00
Fitted a model with MAP estimate = -390.6974
expansions: [(0, 3)]
discards: [  0   9  44  61 142]
Fitting a model of length 215 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 14s - loss: 396.2395 - loglik: -3.8856e+02 - logprior: -7.6819e+00
Epoch 2/2
12/12 - 11s - loss: 387.1276 - loglik: -3.8568e+02 - logprior: -1.4466e+00
Fitted a model with MAP estimate = -385.6906
expansions: []
discards: [  0   2 105]
Fitting a model of length 212 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 13s - loss: 398.3150 - loglik: -3.8866e+02 - logprior: -9.6512e+00
Epoch 2/10
12/12 - 10s - loss: 389.9640 - loglik: -3.8656e+02 - logprior: -3.4011e+00
Epoch 3/10
12/12 - 10s - loss: 386.0270 - loglik: -3.8424e+02 - logprior: -1.7829e+00
Epoch 4/10
12/12 - 10s - loss: 385.1165 - loglik: -3.8488e+02 - logprior: -2.3201e-01
Epoch 5/10
12/12 - 10s - loss: 384.1643 - loglik: -3.8424e+02 - logprior: 0.0761
Epoch 6/10
12/12 - 10s - loss: 382.1748 - loglik: -3.8232e+02 - logprior: 0.1445
Epoch 7/10
12/12 - 10s - loss: 381.4311 - loglik: -3.8153e+02 - logprior: 0.0965
Epoch 8/10
12/12 - 11s - loss: 380.5247 - loglik: -3.8051e+02 - logprior: -1.5516e-02
Epoch 9/10
12/12 - 11s - loss: 381.8242 - loglik: -3.8182e+02 - logprior: -6.1981e-03
Fitted a model with MAP estimate = -380.5779
Time for alignment: 249.8194
Computed alignments with likelihoods: ['-380.9131', '-381.6469', '-380.5779']
Best model has likelihood: -380.5779
SP score = 0.9242
Training of 3 independent models on file DMRL_synthase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f214af6a3a0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f21782321c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f214ac93f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f214aca9430>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 409.8458 - loglik: -3.8600e+02 - logprior: -2.3845e+01
Epoch 2/10
10/10 - 2s - loss: 342.0629 - loglik: -3.3664e+02 - logprior: -5.4212e+00
Epoch 3/10
10/10 - 2s - loss: 290.7408 - loglik: -2.8804e+02 - logprior: -2.7045e+00
Epoch 4/10
10/10 - 2s - loss: 258.1119 - loglik: -2.5582e+02 - logprior: -2.2930e+00
Epoch 5/10
10/10 - 2s - loss: 243.8152 - loglik: -2.4168e+02 - logprior: -2.1394e+00
Epoch 6/10
10/10 - 2s - loss: 237.8676 - loglik: -2.3599e+02 - logprior: -1.8743e+00
Epoch 7/10
10/10 - 2s - loss: 235.9512 - loglik: -2.3431e+02 - logprior: -1.6395e+00
Epoch 8/10
10/10 - 2s - loss: 234.6665 - loglik: -2.3319e+02 - logprior: -1.4799e+00
Epoch 9/10
10/10 - 2s - loss: 234.4403 - loglik: -2.3308e+02 - logprior: -1.3652e+00
Epoch 10/10
10/10 - 2s - loss: 233.8890 - loglik: -2.3257e+02 - logprior: -1.3221e+00
Fitted a model with MAP estimate = -233.5849
expansions: [(0, 4), (13, 1), (14, 1), (19, 1), (20, 1), (26, 1), (27, 1), (30, 1), (31, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 1), (78, 1), (81, 1), (98, 2), (114, 3), (115, 4)]
discards: []
Fitting a model of length 148 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 257.4292 - loglik: -2.2989e+02 - logprior: -2.7541e+01
Epoch 2/2
10/10 - 3s - loss: 220.0363 - loglik: -2.1238e+02 - logprior: -7.6554e+00
Fitted a model with MAP estimate = -213.0425
expansions: []
discards: [ 59  62 122 142]
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 227.1706 - loglik: -2.0865e+02 - logprior: -1.8519e+01
Epoch 2/2
10/10 - 3s - loss: 210.2197 - loglik: -2.0629e+02 - logprior: -3.9250e+00
Fitted a model with MAP estimate = -207.5984
expansions: []
discards: []
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 224.2657 - loglik: -2.0692e+02 - logprior: -1.7348e+01
Epoch 2/10
10/10 - 3s - loss: 209.1758 - loglik: -2.0565e+02 - logprior: -3.5223e+00
Epoch 3/10
10/10 - 3s - loss: 205.9869 - loglik: -2.0510e+02 - logprior: -8.8946e-01
Epoch 4/10
10/10 - 3s - loss: 204.8869 - loglik: -2.0490e+02 - logprior: 0.0139
Epoch 5/10
10/10 - 3s - loss: 203.6192 - loglik: -2.0407e+02 - logprior: 0.4517
Epoch 6/10
10/10 - 3s - loss: 203.0265 - loglik: -2.0380e+02 - logprior: 0.7735
Epoch 7/10
10/10 - 3s - loss: 202.9033 - loglik: -2.0395e+02 - logprior: 1.0474
Epoch 8/10
10/10 - 3s - loss: 202.8041 - loglik: -2.0403e+02 - logprior: 1.2245
Epoch 9/10
10/10 - 3s - loss: 201.8738 - loglik: -2.0320e+02 - logprior: 1.3223
Epoch 10/10
10/10 - 3s - loss: 202.9834 - loglik: -2.0439e+02 - logprior: 1.4042
Fitted a model with MAP estimate = -202.0449
Time for alignment: 85.3625
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 409.6427 - loglik: -3.8579e+02 - logprior: -2.3849e+01
Epoch 2/10
10/10 - 2s - loss: 342.7203 - loglik: -3.3729e+02 - logprior: -5.4312e+00
Epoch 3/10
10/10 - 2s - loss: 289.3519 - loglik: -2.8655e+02 - logprior: -2.8012e+00
Epoch 4/10
10/10 - 2s - loss: 257.7210 - loglik: -2.5517e+02 - logprior: -2.5474e+00
Epoch 5/10
10/10 - 2s - loss: 244.5151 - loglik: -2.4201e+02 - logprior: -2.5020e+00
Epoch 6/10
10/10 - 2s - loss: 239.7753 - loglik: -2.3752e+02 - logprior: -2.2589e+00
Epoch 7/10
10/10 - 2s - loss: 238.1171 - loglik: -2.3611e+02 - logprior: -2.0039e+00
Epoch 8/10
10/10 - 2s - loss: 236.4418 - loglik: -2.3453e+02 - logprior: -1.9137e+00
Epoch 9/10
10/10 - 2s - loss: 236.3174 - loglik: -2.3445e+02 - logprior: -1.8699e+00
Epoch 10/10
10/10 - 2s - loss: 235.5474 - loglik: -2.3371e+02 - logprior: -1.8361e+00
Fitted a model with MAP estimate = -235.5925
expansions: [(0, 3), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 2), (69, 1), (72, 1), (75, 1), (76, 1), (77, 2), (78, 1), (98, 2), (104, 3), (105, 2), (106, 1)]
discards: []
Fitting a model of length 147 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 256.9806 - loglik: -2.2945e+02 - logprior: -2.7527e+01
Epoch 2/2
10/10 - 3s - loss: 219.4139 - loglik: -2.1184e+02 - logprior: -7.5712e+00
Fitted a model with MAP estimate = -212.3606
expansions: []
discards: [ 59  62 122]
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 226.5808 - loglik: -2.0808e+02 - logprior: -1.8504e+01
Epoch 2/2
10/10 - 3s - loss: 210.2422 - loglik: -2.0633e+02 - logprior: -3.9099e+00
Fitted a model with MAP estimate = -207.5560
expansions: []
discards: []
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 223.6836 - loglik: -2.0633e+02 - logprior: -1.7356e+01
Epoch 2/10
10/10 - 3s - loss: 209.8619 - loglik: -2.0632e+02 - logprior: -3.5386e+00
Epoch 3/10
10/10 - 3s - loss: 206.1011 - loglik: -2.0521e+02 - logprior: -8.9273e-01
Epoch 4/10
10/10 - 3s - loss: 204.2572 - loglik: -2.0426e+02 - logprior: -1.9894e-03
Epoch 5/10
10/10 - 3s - loss: 203.6995 - loglik: -2.0414e+02 - logprior: 0.4378
Epoch 6/10
10/10 - 3s - loss: 203.5122 - loglik: -2.0427e+02 - logprior: 0.7592
Epoch 7/10
10/10 - 3s - loss: 203.0766 - loglik: -2.0411e+02 - logprior: 1.0340
Epoch 8/10
10/10 - 3s - loss: 201.8072 - loglik: -2.0302e+02 - logprior: 1.2095
Epoch 9/10
10/10 - 3s - loss: 202.9782 - loglik: -2.0429e+02 - logprior: 1.3103
Fitted a model with MAP estimate = -202.2321
Time for alignment: 84.3015
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 409.8189 - loglik: -3.8597e+02 - logprior: -2.3846e+01
Epoch 2/10
10/10 - 2s - loss: 341.7814 - loglik: -3.3636e+02 - logprior: -5.4224e+00
Epoch 3/10
10/10 - 2s - loss: 287.2238 - loglik: -2.8447e+02 - logprior: -2.7514e+00
Epoch 4/10
10/10 - 2s - loss: 254.6037 - loglik: -2.5222e+02 - logprior: -2.3794e+00
Epoch 5/10
10/10 - 2s - loss: 242.7007 - loglik: -2.4051e+02 - logprior: -2.1942e+00
Epoch 6/10
10/10 - 2s - loss: 237.6856 - loglik: -2.3577e+02 - logprior: -1.9108e+00
Epoch 7/10
10/10 - 2s - loss: 235.0200 - loglik: -2.3334e+02 - logprior: -1.6798e+00
Epoch 8/10
10/10 - 2s - loss: 234.1294 - loglik: -2.3263e+02 - logprior: -1.4971e+00
Epoch 9/10
10/10 - 2s - loss: 233.4591 - loglik: -2.3214e+02 - logprior: -1.3228e+00
Epoch 10/10
10/10 - 2s - loss: 232.9019 - loglik: -2.3165e+02 - logprior: -1.2509e+00
Fitted a model with MAP estimate = -232.9335
expansions: [(0, 4), (13, 1), (14, 1), (19, 1), (20, 1), (26, 1), (27, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 1), (78, 1), (81, 1), (98, 2), (114, 3), (115, 4)]
discards: []
Fitting a model of length 148 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 256.5484 - loglik: -2.2900e+02 - logprior: -2.7551e+01
Epoch 2/2
10/10 - 3s - loss: 219.7880 - loglik: -2.1213e+02 - logprior: -7.6542e+00
Fitted a model with MAP estimate = -212.8865
expansions: []
discards: [ 59  62 122 142]
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 227.0660 - loglik: -2.0854e+02 - logprior: -1.8526e+01
Epoch 2/2
10/10 - 3s - loss: 210.2159 - loglik: -2.0629e+02 - logprior: -3.9275e+00
Fitted a model with MAP estimate = -207.5809
expansions: []
discards: []
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 223.8056 - loglik: -2.0645e+02 - logprior: -1.7356e+01
Epoch 2/10
10/10 - 3s - loss: 209.2570 - loglik: -2.0573e+02 - logprior: -3.5306e+00
Epoch 3/10
10/10 - 3s - loss: 205.9930 - loglik: -2.0510e+02 - logprior: -8.9795e-01
Epoch 4/10
10/10 - 3s - loss: 205.2212 - loglik: -2.0523e+02 - logprior: 0.0046
Epoch 5/10
10/10 - 3s - loss: 203.2222 - loglik: -2.0367e+02 - logprior: 0.4516
Epoch 6/10
10/10 - 3s - loss: 203.7267 - loglik: -2.0450e+02 - logprior: 0.7689
Fitted a model with MAP estimate = -202.8869
Time for alignment: 74.8662
Computed alignments with likelihoods: ['-202.0449', '-202.2321', '-202.8869']
Best model has likelihood: -202.0449
SP score = 0.9119
Training of 3 independent models on file Stap_Strp_toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2153cdb9d0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f210f8eeeb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f210f921ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f21075b0ac0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 324.5321 - loglik: -2.4848e+02 - logprior: -7.6050e+01
Epoch 2/10
10/10 - 2s - loss: 238.1241 - loglik: -2.1925e+02 - logprior: -1.8871e+01
Epoch 3/10
10/10 - 2s - loss: 204.0026 - loglik: -1.9609e+02 - logprior: -7.9168e+00
Epoch 4/10
10/10 - 2s - loss: 190.2135 - loglik: -1.8620e+02 - logprior: -4.0145e+00
Epoch 5/10
10/10 - 2s - loss: 184.6230 - loglik: -1.8271e+02 - logprior: -1.9106e+00
Epoch 6/10
10/10 - 2s - loss: 181.3020 - loglik: -1.8050e+02 - logprior: -8.0128e-01
Epoch 7/10
10/10 - 2s - loss: 178.6804 - loglik: -1.7844e+02 - logprior: -2.3775e-01
Epoch 8/10
10/10 - 2s - loss: 177.0317 - loglik: -1.7713e+02 - logprior: 0.1009
Epoch 9/10
10/10 - 2s - loss: 175.9427 - loglik: -1.7628e+02 - logprior: 0.3408
Epoch 10/10
10/10 - 2s - loss: 175.1049 - loglik: -1.7562e+02 - logprior: 0.5142
Fitted a model with MAP estimate = -174.8192
expansions: [(9, 3), (14, 2), (27, 3), (39, 1), (52, 2), (58, 6)]
discards: [0]
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 253.5585 - loglik: -1.7489e+02 - logprior: -7.8670e+01
Epoch 2/2
10/10 - 2s - loss: 198.1552 - loglik: -1.6676e+02 - logprior: -3.1391e+01
Fitted a model with MAP estimate = -188.9001
expansions: [(32, 1)]
discards: [60]
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 238.0527 - loglik: -1.6450e+02 - logprior: -7.3550e+01
Epoch 2/2
10/10 - 2s - loss: 183.0729 - loglik: -1.6174e+02 - logprior: -2.1338e+01
Fitted a model with MAP estimate = -171.8933
expansions: []
discards: []
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 222.1419 - loglik: -1.6078e+02 - logprior: -6.1362e+01
Epoch 2/10
10/10 - 3s - loss: 175.1360 - loglik: -1.6028e+02 - logprior: -1.4856e+01
Epoch 3/10
10/10 - 2s - loss: 165.6927 - loglik: -1.6052e+02 - logprior: -5.1740e+00
Epoch 4/10
10/10 - 2s - loss: 161.8967 - loglik: -1.6074e+02 - logprior: -1.1525e+00
Epoch 5/10
10/10 - 2s - loss: 159.8914 - loglik: -1.6095e+02 - logprior: 1.0582
Epoch 6/10
10/10 - 2s - loss: 158.7630 - loglik: -1.6110e+02 - logprior: 2.3350
Epoch 7/10
10/10 - 2s - loss: 158.0708 - loglik: -1.6119e+02 - logprior: 3.1208
Epoch 8/10
10/10 - 2s - loss: 157.6121 - loglik: -1.6128e+02 - logprior: 3.6662
Epoch 9/10
10/10 - 2s - loss: 157.2585 - loglik: -1.6137e+02 - logprior: 4.1135
Epoch 10/10
10/10 - 2s - loss: 156.9566 - loglik: -1.6146e+02 - logprior: 4.5023
Fitted a model with MAP estimate = -156.7810
Time for alignment: 72.5860
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 324.4885 - loglik: -2.4844e+02 - logprior: -7.6051e+01
Epoch 2/10
10/10 - 2s - loss: 238.0334 - loglik: -2.1916e+02 - logprior: -1.8870e+01
Epoch 3/10
10/10 - 2s - loss: 203.5551 - loglik: -1.9560e+02 - logprior: -7.9597e+00
Epoch 4/10
10/10 - 2s - loss: 188.8711 - loglik: -1.8467e+02 - logprior: -4.1989e+00
Epoch 5/10
10/10 - 2s - loss: 182.4784 - loglik: -1.8042e+02 - logprior: -2.0594e+00
Epoch 6/10
10/10 - 2s - loss: 178.8318 - loglik: -1.7785e+02 - logprior: -9.8099e-01
Epoch 7/10
10/10 - 2s - loss: 176.4617 - loglik: -1.7609e+02 - logprior: -3.6784e-01
Epoch 8/10
10/10 - 2s - loss: 174.8612 - loglik: -1.7483e+02 - logprior: -3.4468e-02
Epoch 9/10
10/10 - 2s - loss: 174.1629 - loglik: -1.7437e+02 - logprior: 0.2077
Epoch 10/10
10/10 - 2s - loss: 173.6879 - loglik: -1.7410e+02 - logprior: 0.4170
Fitted a model with MAP estimate = -173.5122
expansions: [(9, 3), (14, 2), (25, 1), (26, 2), (27, 2), (37, 1), (52, 1), (58, 6)]
discards: [0]
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 251.9018 - loglik: -1.7327e+02 - logprior: -7.8632e+01
Epoch 2/2
10/10 - 2s - loss: 196.4992 - loglik: -1.6548e+02 - logprior: -3.1020e+01
Fitted a model with MAP estimate = -187.1592
expansions: []
discards: []
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 236.0814 - loglik: -1.6313e+02 - logprior: -7.2951e+01
Epoch 2/10
10/10 - 2s - loss: 181.9528 - loglik: -1.6126e+02 - logprior: -2.0694e+01
Epoch 3/10
10/10 - 2s - loss: 167.1665 - loglik: -1.6059e+02 - logprior: -6.5726e+00
Epoch 4/10
10/10 - 2s - loss: 162.3086 - loglik: -1.6063e+02 - logprior: -1.6760e+00
Epoch 5/10
10/10 - 2s - loss: 160.1256 - loglik: -1.6082e+02 - logprior: 0.6900
Epoch 6/10
10/10 - 2s - loss: 158.8078 - loglik: -1.6080e+02 - logprior: 1.9953
Epoch 7/10
10/10 - 2s - loss: 158.0002 - loglik: -1.6077e+02 - logprior: 2.7706
Epoch 8/10
10/10 - 2s - loss: 157.4830 - loglik: -1.6079e+02 - logprior: 3.3046
Epoch 9/10
10/10 - 2s - loss: 157.0915 - loglik: -1.6084e+02 - logprior: 3.7526
Epoch 10/10
10/10 - 2s - loss: 156.7842 - loglik: -1.6093e+02 - logprior: 4.1442
Fitted a model with MAP estimate = -156.6085
Time for alignment: 62.1799
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 324.5040 - loglik: -2.4845e+02 - logprior: -7.6050e+01
Epoch 2/10
10/10 - 2s - loss: 237.9507 - loglik: -2.1908e+02 - logprior: -1.8873e+01
Epoch 3/10
10/10 - 2s - loss: 203.8309 - loglik: -1.9590e+02 - logprior: -7.9335e+00
Epoch 4/10
10/10 - 2s - loss: 189.7669 - loglik: -1.8572e+02 - logprior: -4.0439e+00
Epoch 5/10
10/10 - 2s - loss: 183.1551 - loglik: -1.8116e+02 - logprior: -1.9927e+00
Epoch 6/10
10/10 - 2s - loss: 179.2957 - loglik: -1.7832e+02 - logprior: -9.7518e-01
Epoch 7/10
10/10 - 2s - loss: 177.1732 - loglik: -1.7679e+02 - logprior: -3.8349e-01
Epoch 8/10
10/10 - 2s - loss: 175.6525 - loglik: -1.7564e+02 - logprior: -1.1672e-02
Epoch 9/10
10/10 - 2s - loss: 174.7856 - loglik: -1.7501e+02 - logprior: 0.2244
Epoch 10/10
10/10 - 2s - loss: 174.3745 - loglik: -1.7477e+02 - logprior: 0.3935
Fitted a model with MAP estimate = -174.1637
expansions: [(14, 2), (15, 1), (24, 1), (26, 1), (27, 3), (37, 1), (39, 1), (58, 6)]
discards: [0]
Fitting a model of length 86 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 252.8739 - loglik: -1.7439e+02 - logprior: -7.8484e+01
Epoch 2/2
10/10 - 2s - loss: 197.6966 - loglik: -1.6651e+02 - logprior: -3.1187e+01
Fitted a model with MAP estimate = -188.4582
expansions: []
discards: [ 0 13]
Fitting a model of length 84 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 241.3506 - loglik: -1.6599e+02 - logprior: -7.5360e+01
Epoch 2/2
10/10 - 2s - loss: 189.9775 - loglik: -1.6427e+02 - logprior: -2.5709e+01
Fitted a model with MAP estimate = -178.6727
expansions: [(7, 2)]
discards: [0]
Fitting a model of length 85 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 243.3438 - loglik: -1.6569e+02 - logprior: -7.7656e+01
Epoch 2/10
10/10 - 2s - loss: 195.4872 - loglik: -1.6473e+02 - logprior: -3.0761e+01
Epoch 3/10
10/10 - 2s - loss: 185.8119 - loglik: -1.6482e+02 - logprior: -2.0993e+01
Epoch 4/10
10/10 - 2s - loss: 181.5977 - loglik: -1.6498e+02 - logprior: -1.6622e+01
Epoch 5/10
10/10 - 2s - loss: 177.2842 - loglik: -1.6509e+02 - logprior: -1.2192e+01
Epoch 6/10
10/10 - 2s - loss: 167.7679 - loglik: -1.6511e+02 - logprior: -2.6570e+00
Epoch 7/10
10/10 - 2s - loss: 162.3068 - loglik: -1.6464e+02 - logprior: 2.3350
Epoch 8/10
10/10 - 2s - loss: 161.2234 - loglik: -1.6450e+02 - logprior: 3.2810
Epoch 9/10
10/10 - 2s - loss: 160.6950 - loglik: -1.6453e+02 - logprior: 3.8376
Epoch 10/10
10/10 - 2s - loss: 160.3366 - loglik: -1.6465e+02 - logprior: 4.3126
Fitted a model with MAP estimate = -160.1533
Time for alignment: 70.5795
Computed alignments with likelihoods: ['-156.7810', '-156.6085', '-160.1533']
Best model has likelihood: -156.6085
SP score = 0.3543
Training of 3 independent models on file ghf5.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f214ac11430>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f214ab9b100>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2139a53670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20dc8defd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 225 on 2717 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 16s - loss: 783.4215 - loglik: -7.6391e+02 - logprior: -1.9516e+01
Epoch 2/10
10/10 - 14s - loss: 736.0643 - loglik: -7.3292e+02 - logprior: -3.1423e+00
Epoch 3/10
10/10 - 14s - loss: 700.4713 - loglik: -7.0017e+02 - logprior: -2.9777e-01
Epoch 4/10
10/10 - 15s - loss: 672.2924 - loglik: -6.7247e+02 - logprior: 0.1820
Epoch 5/10
10/10 - 15s - loss: 659.1151 - loglik: -6.5919e+02 - logprior: 0.0716
Epoch 6/10
10/10 - 15s - loss: 649.3114 - loglik: -6.4920e+02 - logprior: -1.0824e-01
Epoch 7/10
10/10 - 15s - loss: 649.6667 - loglik: -6.4959e+02 - logprior: -8.0804e-02
Fitted a model with MAP estimate = -646.7828
expansions: [(0, 4), (18, 2), (26, 1), (46, 2), (52, 2), (82, 3), (86, 9), (92, 1), (94, 1), (112, 2), (117, 1), (151, 1), (153, 2), (171, 9), (174, 1), (198, 4), (207, 1), (208, 1)]
discards: [217 218]
Fitting a model of length 270 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 25s - loss: 665.1140 - loglik: -6.5353e+02 - logprior: -1.1588e+01
Epoch 2/2
20/20 - 20s - loss: 641.8467 - loglik: -6.4156e+02 - logprior: -2.8675e-01
Fitted a model with MAP estimate = -638.7779
expansions: [(62, 1)]
discards: [  1   2   3   4   5  94  95 202 203 204 205 206 207 239 240]
Fitting a model of length 256 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 22s - loss: 650.9976 - loglik: -6.4385e+02 - logprior: -7.1495e+00
Epoch 2/2
20/20 - 19s - loss: 640.0742 - loglik: -6.4053e+02 - logprior: 0.4540
Fitted a model with MAP estimate = -638.0838
expansions: [(0, 4), (99, 1), (196, 8), (221, 2), (251, 1)]
discards: [1]
Fitting a model of length 271 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 24s - loss: 653.1505 - loglik: -6.4099e+02 - logprior: -1.2161e+01
Epoch 2/10
20/20 - 21s - loss: 637.9440 - loglik: -6.3781e+02 - logprior: -1.3259e-01
Epoch 3/10
20/20 - 21s - loss: 634.6663 - loglik: -6.3613e+02 - logprior: 1.4605
Epoch 4/10
20/20 - 20s - loss: 632.3091 - loglik: -6.3431e+02 - logprior: 2.0013
Epoch 5/10
20/20 - 20s - loss: 630.4368 - loglik: -6.3266e+02 - logprior: 2.2202
Epoch 6/10
20/20 - 19s - loss: 629.2506 - loglik: -6.3132e+02 - logprior: 2.0710
Epoch 7/10
20/20 - 19s - loss: 629.1963 - loglik: -6.3126e+02 - logprior: 2.0681
Epoch 8/10
20/20 - 19s - loss: 626.2103 - loglik: -6.2839e+02 - logprior: 2.1757
Epoch 9/10
20/20 - 20s - loss: 627.9572 - loglik: -6.3020e+02 - logprior: 2.2474
Fitted a model with MAP estimate = -626.7508
Time for alignment: 422.1714
Fitting a model of length 225 on 2717 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 18s - loss: 784.3802 - loglik: -7.6487e+02 - logprior: -1.9508e+01
Epoch 2/10
10/10 - 15s - loss: 735.7636 - loglik: -7.3263e+02 - logprior: -3.1342e+00
Epoch 3/10
10/10 - 15s - loss: 699.4146 - loglik: -6.9911e+02 - logprior: -3.0415e-01
Epoch 4/10
10/10 - 14s - loss: 672.2394 - loglik: -6.7244e+02 - logprior: 0.2001
Epoch 5/10
10/10 - 14s - loss: 660.4662 - loglik: -6.6064e+02 - logprior: 0.1737
Epoch 6/10
10/10 - 13s - loss: 655.0765 - loglik: -6.5516e+02 - logprior: 0.0827
Epoch 7/10
10/10 - 13s - loss: 654.3640 - loglik: -6.5447e+02 - logprior: 0.1059
Epoch 8/10
10/10 - 13s - loss: 649.7277 - loglik: -6.4965e+02 - logprior: -8.0159e-02
Epoch 9/10
10/10 - 14s - loss: 651.5432 - loglik: -6.5126e+02 - logprior: -2.7951e-01
Fitted a model with MAP estimate = -649.4815
expansions: [(0, 4), (18, 1), (25, 3), (45, 1), (54, 3), (82, 1), (86, 14), (87, 1), (93, 3), (94, 1), (111, 5), (116, 2), (117, 1), (151, 1), (154, 2), (155, 1), (199, 4), (208, 1)]
discards: [217 218]
Fitting a model of length 272 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 21s - loss: 670.1642 - loglik: -6.5806e+02 - logprior: -1.2101e+01
Epoch 2/2
20/20 - 19s - loss: 647.0151 - loglik: -6.4650e+02 - logprior: -5.1994e-01
Fitted a model with MAP estimate = -641.4341
expansions: [(29, 2), (246, 1), (256, 1)]
discards: [  1   2   3   4   5  30  94 104 122 143 146 147 153]
Fitting a model of length 263 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 22s - loss: 653.0657 - loglik: -6.4583e+02 - logprior: -7.2352e+00
Epoch 2/2
20/20 - 18s - loss: 642.0486 - loglik: -6.4247e+02 - logprior: 0.4231
Fitted a model with MAP estimate = -638.9798
expansions: [(0, 4), (136, 1), (258, 1)]
discards: [ 2 24]
Fitting a model of length 267 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 20s - loss: 653.8854 - loglik: -6.4168e+02 - logprior: -1.2204e+01
Epoch 2/10
20/20 - 17s - loss: 640.5333 - loglik: -6.4015e+02 - logprior: -3.8530e-01
Epoch 3/10
20/20 - 18s - loss: 633.6559 - loglik: -6.3499e+02 - logprior: 1.3367
Epoch 4/10
20/20 - 18s - loss: 634.4913 - loglik: -6.3644e+02 - logprior: 1.9496
Fitted a model with MAP estimate = -630.7798
Time for alignment: 329.8901
Fitting a model of length 225 on 2717 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 17s - loss: 783.1986 - loglik: -7.6368e+02 - logprior: -1.9515e+01
Epoch 2/10
10/10 - 15s - loss: 738.5753 - loglik: -7.3544e+02 - logprior: -3.1325e+00
Epoch 3/10
10/10 - 15s - loss: 699.3450 - loglik: -6.9907e+02 - logprior: -2.7537e-01
Epoch 4/10
10/10 - 16s - loss: 667.6996 - loglik: -6.6789e+02 - logprior: 0.1897
Epoch 5/10
10/10 - 16s - loss: 658.3560 - loglik: -6.5838e+02 - logprior: 0.0206
Epoch 6/10
10/10 - 16s - loss: 655.3011 - loglik: -6.5524e+02 - logprior: -6.2013e-02
Epoch 7/10
10/10 - 16s - loss: 646.1807 - loglik: -6.4617e+02 - logprior: -8.4205e-03
Epoch 8/10
10/10 - 16s - loss: 646.6752 - loglik: -6.4649e+02 - logprior: -1.8637e-01
Fitted a model with MAP estimate = -645.3370
expansions: [(0, 4), (24, 1), (51, 3), (81, 3), (85, 11), (94, 1), (100, 1), (112, 2), (118, 1), (121, 1), (150, 1), (169, 1), (170, 1), (198, 1), (200, 3), (206, 2)]
discards: [217 218]
Fitting a model of length 260 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 22s - loss: 666.8549 - loglik: -6.5484e+02 - logprior: -1.2015e+01
Epoch 2/2
20/20 - 20s - loss: 644.9135 - loglik: -6.4453e+02 - logprior: -3.8166e-01
Fitted a model with MAP estimate = -640.6160
expansions: [(49, 1)]
discards: [  2   3  90  91  92 242]
Fitting a model of length 255 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 23s - loss: 650.4457 - loglik: -6.4332e+02 - logprior: -7.1272e+00
Epoch 2/2
20/20 - 19s - loss: 642.1162 - loglik: -6.4265e+02 - logprior: 0.5313
Fitted a model with MAP estimate = -638.1577
expansions: [(0, 4), (89, 1), (92, 1), (250, 1)]
discards: [1 2 3]
Fitting a model of length 259 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 24s - loss: 655.4047 - loglik: -6.4220e+02 - logprior: -1.3209e+01
Epoch 2/10
20/20 - 20s - loss: 640.3120 - loglik: -6.3947e+02 - logprior: -8.3757e-01
Epoch 3/10
20/20 - 20s - loss: 637.8381 - loglik: -6.3916e+02 - logprior: 1.3216
Epoch 4/10
20/20 - 20s - loss: 632.7170 - loglik: -6.3470e+02 - logprior: 1.9831
Epoch 5/10
20/20 - 20s - loss: 630.7892 - loglik: -6.3299e+02 - logprior: 2.2002
Epoch 6/10
20/20 - 20s - loss: 633.1465 - loglik: -6.3531e+02 - logprior: 2.1682
Fitted a model with MAP estimate = -630.5964
Time for alignment: 384.7358
Computed alignments with likelihoods: ['-626.7508', '-630.7798', '-630.5964']
Best model has likelihood: -626.7508
SP score = 0.6382
Training of 3 independent models on file myb_DNA-binding.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2153efddf0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f214acdd9a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f214acdd790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f214acc1d90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 134.9073 - loglik: -1.3150e+02 - logprior: -3.4023e+00
Epoch 2/10
19/19 - 2s - loss: 114.0359 - loglik: -1.1264e+02 - logprior: -1.3984e+00
Epoch 3/10
19/19 - 2s - loss: 107.2072 - loglik: -1.0576e+02 - logprior: -1.4441e+00
Epoch 4/10
19/19 - 2s - loss: 104.7267 - loglik: -1.0334e+02 - logprior: -1.3884e+00
Epoch 5/10
19/19 - 2s - loss: 103.9225 - loglik: -1.0254e+02 - logprior: -1.3824e+00
Epoch 6/10
19/19 - 2s - loss: 103.4739 - loglik: -1.0207e+02 - logprior: -1.4060e+00
Epoch 7/10
19/19 - 2s - loss: 103.3464 - loglik: -1.0193e+02 - logprior: -1.4203e+00
Epoch 8/10
19/19 - 2s - loss: 103.1122 - loglik: -1.0166e+02 - logprior: -1.4478e+00
Epoch 9/10
19/19 - 2s - loss: 103.3052 - loglik: -1.0185e+02 - logprior: -1.4595e+00
Fitted a model with MAP estimate = -101.5968
expansions: [(6, 1), (10, 1), (12, 1), (13, 1), (14, 1), (20, 1), (23, 2), (30, 2)]
discards: [0]
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 107.3061 - loglik: -1.0301e+02 - logprior: -4.2971e+00
Epoch 2/2
19/19 - 2s - loss: 100.1976 - loglik: -9.8260e+01 - logprior: -1.9379e+00
Fitted a model with MAP estimate = -97.4951
expansions: [(0, 1), (35, 2)]
discards: [0]
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 98.8764 - loglik: -9.5749e+01 - logprior: -3.1275e+00
Epoch 2/2
19/19 - 2s - loss: 94.4427 - loglik: -9.2979e+01 - logprior: -1.4638e+00
Fitted a model with MAP estimate = -92.4306
expansions: []
discards: [29]
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 95.3668 - loglik: -9.2248e+01 - logprior: -3.1192e+00
Epoch 2/10
19/19 - 2s - loss: 92.8608 - loglik: -9.1436e+01 - logprior: -1.4247e+00
Epoch 3/10
19/19 - 2s - loss: 91.9095 - loglik: -9.0595e+01 - logprior: -1.3147e+00
Epoch 4/10
19/19 - 2s - loss: 91.3399 - loglik: -9.0046e+01 - logprior: -1.2942e+00
Epoch 5/10
19/19 - 2s - loss: 91.0255 - loglik: -8.9738e+01 - logprior: -1.2873e+00
Epoch 6/10
19/19 - 2s - loss: 90.6397 - loglik: -8.9350e+01 - logprior: -1.2902e+00
Epoch 7/10
19/19 - 2s - loss: 90.6460 - loglik: -8.9346e+01 - logprior: -1.3004e+00
Fitted a model with MAP estimate = -90.4219
Time for alignment: 55.9982
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 134.9183 - loglik: -1.3152e+02 - logprior: -3.3993e+00
Epoch 2/10
19/19 - 2s - loss: 113.6302 - loglik: -1.1224e+02 - logprior: -1.3897e+00
Epoch 3/10
19/19 - 2s - loss: 106.5262 - loglik: -1.0498e+02 - logprior: -1.5428e+00
Epoch 4/10
19/19 - 2s - loss: 104.5885 - loglik: -1.0314e+02 - logprior: -1.4514e+00
Epoch 5/10
19/19 - 2s - loss: 103.3174 - loglik: -1.0188e+02 - logprior: -1.4363e+00
Epoch 6/10
19/19 - 2s - loss: 102.4100 - loglik: -1.0093e+02 - logprior: -1.4765e+00
Epoch 7/10
19/19 - 2s - loss: 102.1422 - loglik: -1.0065e+02 - logprior: -1.4969e+00
Epoch 8/10
19/19 - 2s - loss: 102.3280 - loglik: -1.0079e+02 - logprior: -1.5338e+00
Fitted a model with MAP estimate = -100.6440
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (23, 2), (28, 1), (29, 1), (30, 2)]
discards: [0]
Fitting a model of length 48 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 104.0767 - loglik: -9.9782e+01 - logprior: -4.2944e+00
Epoch 2/2
19/19 - 2s - loss: 95.7953 - loglik: -9.3721e+01 - logprior: -2.0739e+00
Fitted a model with MAP estimate = -93.1926
expansions: [(0, 1)]
discards: [ 0  9 30]
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 96.7869 - loglik: -9.3722e+01 - logprior: -3.0646e+00
Epoch 2/2
19/19 - 2s - loss: 93.8253 - loglik: -9.2388e+01 - logprior: -1.4373e+00
Fitted a model with MAP estimate = -92.1001
expansions: []
discards: []
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 94.4873 - loglik: -9.1369e+01 - logprior: -3.1184e+00
Epoch 2/10
19/19 - 2s - loss: 92.2023 - loglik: -9.0760e+01 - logprior: -1.4419e+00
Epoch 3/10
19/19 - 2s - loss: 91.6275 - loglik: -9.0302e+01 - logprior: -1.3253e+00
Epoch 4/10
19/19 - 1s - loss: 91.2533 - loglik: -8.9959e+01 - logprior: -1.2943e+00
Epoch 5/10
19/19 - 2s - loss: 91.0155 - loglik: -8.9727e+01 - logprior: -1.2884e+00
Epoch 6/10
19/19 - 2s - loss: 90.8254 - loglik: -8.9536e+01 - logprior: -1.2891e+00
Epoch 7/10
19/19 - 2s - loss: 90.6366 - loglik: -8.9336e+01 - logprior: -1.3002e+00
Epoch 8/10
19/19 - 2s - loss: 90.7185 - loglik: -8.9391e+01 - logprior: -1.3272e+00
Fitted a model with MAP estimate = -90.4358
Time for alignment: 55.9987
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 134.9353 - loglik: -1.3154e+02 - logprior: -3.3969e+00
Epoch 2/10
19/19 - 2s - loss: 114.0942 - loglik: -1.1271e+02 - logprior: -1.3889e+00
Epoch 3/10
19/19 - 2s - loss: 107.0391 - loglik: -1.0551e+02 - logprior: -1.5244e+00
Epoch 4/10
19/19 - 2s - loss: 105.5281 - loglik: -1.0412e+02 - logprior: -1.4095e+00
Epoch 5/10
19/19 - 2s - loss: 104.8252 - loglik: -1.0345e+02 - logprior: -1.3779e+00
Epoch 6/10
19/19 - 2s - loss: 104.3920 - loglik: -1.0299e+02 - logprior: -1.4040e+00
Epoch 7/10
19/19 - 2s - loss: 104.1058 - loglik: -1.0268e+02 - logprior: -1.4209e+00
Epoch 8/10
19/19 - 2s - loss: 103.8881 - loglik: -1.0243e+02 - logprior: -1.4587e+00
Epoch 9/10
19/19 - 2s - loss: 103.6867 - loglik: -1.0219e+02 - logprior: -1.4921e+00
Epoch 10/10
19/19 - 2s - loss: 103.5166 - loglik: -1.0201e+02 - logprior: -1.5109e+00
Fitted a model with MAP estimate = -102.0928
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (22, 2), (28, 3), (30, 2)]
discards: [0]
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 106.3216 - loglik: -1.0199e+02 - logprior: -4.3342e+00
Epoch 2/2
19/19 - 2s - loss: 97.0456 - loglik: -9.4870e+01 - logprior: -2.1759e+00
Fitted a model with MAP estimate = -94.0688
expansions: [(0, 1)]
discards: [ 0 13 29 38]
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 97.6044 - loglik: -9.4562e+01 - logprior: -3.0429e+00
Epoch 2/2
19/19 - 2s - loss: 94.6641 - loglik: -9.3235e+01 - logprior: -1.4293e+00
Fitted a model with MAP estimate = -92.5232
expansions: []
discards: []
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 94.7060 - loglik: -9.1607e+01 - logprior: -3.0986e+00
Epoch 2/10
19/19 - 2s - loss: 92.4048 - loglik: -9.0980e+01 - logprior: -1.4249e+00
Epoch 3/10
19/19 - 2s - loss: 91.7179 - loglik: -9.0393e+01 - logprior: -1.3252e+00
Epoch 4/10
19/19 - 1s - loss: 91.8464 - loglik: -9.0554e+01 - logprior: -1.2928e+00
Fitted a model with MAP estimate = -91.2425
Time for alignment: 52.7674
Computed alignments with likelihoods: ['-90.4219', '-90.4358', '-91.2425']
Best model has likelihood: -90.4219
SP score = 0.9439
Training of 3 independent models on file ghf10.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2153fb0d60>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f212905d490>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20dc8c0f70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f21186dc3a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 225 on 1502 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 15s - loss: 681.4539 - loglik: -6.4624e+02 - logprior: -3.5217e+01
Epoch 2/10
10/10 - 12s - loss: 606.9860 - loglik: -6.0118e+02 - logprior: -5.8017e+00
Epoch 3/10
10/10 - 13s - loss: 554.9128 - loglik: -5.5418e+02 - logprior: -7.3452e-01
Epoch 4/10
10/10 - 13s - loss: 510.5487 - loglik: -5.1020e+02 - logprior: -3.4913e-01
Epoch 5/10
10/10 - 14s - loss: 484.9359 - loglik: -4.8446e+02 - logprior: -4.7642e-01
Epoch 6/10
10/10 - 14s - loss: 474.4863 - loglik: -4.7389e+02 - logprior: -5.9930e-01
Epoch 7/10
10/10 - 14s - loss: 472.5154 - loglik: -4.7169e+02 - logprior: -8.2543e-01
Epoch 8/10
10/10 - 14s - loss: 467.5983 - loglik: -4.6692e+02 - logprior: -6.8251e-01
Epoch 9/10
10/10 - 15s - loss: 468.2126 - loglik: -4.6756e+02 - logprior: -6.5007e-01
Fitted a model with MAP estimate = -466.8553
expansions: [(22, 1), (23, 2), (24, 1), (25, 1), (37, 3), (39, 1), (41, 1), (48, 2), (49, 1), (51, 2), (53, 2), (66, 1), (68, 2), (69, 3), (72, 1), (73, 1), (74, 1), (77, 1), (86, 1), (95, 1), (101, 1), (102, 2), (103, 3), (110, 1), (132, 2), (133, 2), (134, 4), (135, 2), (137, 1), (157, 1), (158, 5), (159, 2), (174, 8)]
discards: [  1   2 223]
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 19s - loss: 472.3026 - loglik: -4.5353e+02 - logprior: -1.8772e+01
Epoch 2/2
15/15 - 16s - loss: 439.2258 - loglik: -4.3933e+02 - logprior: 0.1054
Fitted a model with MAP estimate = -432.6718
expansions: [(91, 1)]
discards: [ 20  84 130 169 233 234 235 236]
Fitting a model of length 278 on 1502 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 23s - loss: 460.7344 - loglik: -4.3918e+02 - logprior: -2.1556e+01
Epoch 2/2
10/10 - 20s - loss: 433.4783 - loglik: -4.3135e+02 - logprior: -2.1284e+00
Fitted a model with MAP estimate = -432.6165
expansions: [(0, 3), (202, 1), (236, 3)]
discards: [164]
Fitting a model of length 284 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 17s - loss: 458.4591 - loglik: -4.3412e+02 - logprior: -2.4337e+01
Epoch 2/10
15/15 - 14s - loss: 432.9871 - loglik: -4.3147e+02 - logprior: -1.5139e+00
Epoch 3/10
15/15 - 14s - loss: 426.4759 - loglik: -4.3021e+02 - logprior: 3.7356
Epoch 4/10
15/15 - 15s - loss: 425.8607 - loglik: -4.3124e+02 - logprior: 5.3830
Epoch 5/10
15/15 - 15s - loss: 422.0127 - loglik: -4.2798e+02 - logprior: 5.9697
Epoch 6/10
15/15 - 16s - loss: 425.2270 - loglik: -4.3161e+02 - logprior: 6.3864
Fitted a model with MAP estimate = -422.6296
Time for alignment: 322.7254
Fitting a model of length 225 on 1502 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 17s - loss: 678.4500 - loglik: -6.4323e+02 - logprior: -3.5225e+01
Epoch 2/10
10/10 - 13s - loss: 611.3630 - loglik: -6.0555e+02 - logprior: -5.8180e+00
Epoch 3/10
10/10 - 13s - loss: 554.7880 - loglik: -5.5417e+02 - logprior: -6.2076e-01
Epoch 4/10
10/10 - 13s - loss: 518.0473 - loglik: -5.1795e+02 - logprior: -9.7278e-02
Epoch 5/10
10/10 - 13s - loss: 500.6523 - loglik: -5.0025e+02 - logprior: -4.0069e-01
Epoch 6/10
10/10 - 13s - loss: 482.8158 - loglik: -4.8190e+02 - logprior: -9.1146e-01
Epoch 7/10
10/10 - 13s - loss: 477.0846 - loglik: -4.7569e+02 - logprior: -1.3987e+00
Epoch 8/10
10/10 - 13s - loss: 473.6204 - loglik: -4.7237e+02 - logprior: -1.2461e+00
Epoch 9/10
10/10 - 13s - loss: 475.9346 - loglik: -4.7470e+02 - logprior: -1.2349e+00
Fitted a model with MAP estimate = -471.1159
expansions: [(19, 1), (23, 1), (24, 3), (39, 2), (41, 1), (43, 1), (46, 2), (49, 1), (50, 1), (51, 1), (52, 1), (53, 3), (55, 1), (66, 1), (68, 1), (69, 2), (71, 1), (72, 1), (73, 1), (76, 1), (79, 1), (86, 1), (94, 1), (100, 1), (101, 1), (103, 2), (105, 2), (132, 2), (133, 2), (134, 4), (135, 2), (137, 1), (139, 1), (156, 1), (157, 4), (159, 1), (168, 2), (173, 8), (182, 2), (183, 1), (198, 1)]
discards: [  1   2   6   7 200 223]
Fitting a model of length 287 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 17s - loss: 480.3377 - loglik: -4.6131e+02 - logprior: -1.9025e+01
Epoch 2/2
15/15 - 15s - loss: 435.3833 - loglik: -4.3528e+02 - logprior: -1.0501e-01
Fitted a model with MAP estimate = -431.9734
expansions: [(14, 1), (61, 1), (65, 1)]
discards: [ 21  84 135 167 231 232 243]
Fitting a model of length 283 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 17s - loss: 451.7582 - loglik: -4.3617e+02 - logprior: -1.5589e+01
Epoch 2/2
15/15 - 15s - loss: 433.8481 - loglik: -4.3504e+02 - logprior: 1.1966
Fitted a model with MAP estimate = -429.1377
expansions: [(0, 3), (206, 1)]
discards: [230]
Fitting a model of length 286 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 17s - loss: 455.9289 - loglik: -4.3204e+02 - logprior: -2.3891e+01
Epoch 2/10
15/15 - 14s - loss: 432.2576 - loglik: -4.3114e+02 - logprior: -1.1170e+00
Epoch 3/10
15/15 - 14s - loss: 428.6705 - loglik: -4.3256e+02 - logprior: 3.8886
Epoch 4/10
15/15 - 14s - loss: 426.3155 - loglik: -4.3181e+02 - logprior: 5.4986
Epoch 5/10
15/15 - 14s - loss: 421.9312 - loglik: -4.2800e+02 - logprior: 6.0658
Epoch 6/10
15/15 - 14s - loss: 420.1094 - loglik: -4.2660e+02 - logprior: 6.4884
Epoch 7/10
15/15 - 15s - loss: 424.1585 - loglik: -4.3099e+02 - logprior: 6.8338
Fitted a model with MAP estimate = -422.0973
Time for alignment: 311.0997
Fitting a model of length 225 on 1502 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 17s - loss: 679.6829 - loglik: -6.4447e+02 - logprior: -3.5211e+01
Epoch 2/10
10/10 - 14s - loss: 608.8561 - loglik: -6.0303e+02 - logprior: -5.8295e+00
Epoch 3/10
10/10 - 14s - loss: 553.9302 - loglik: -5.5356e+02 - logprior: -3.7215e-01
Epoch 4/10
10/10 - 14s - loss: 514.0587 - loglik: -5.1431e+02 - logprior: 0.2506
Epoch 5/10
10/10 - 15s - loss: 491.6510 - loglik: -4.9177e+02 - logprior: 0.1151
Epoch 6/10
10/10 - 14s - loss: 476.7829 - loglik: -4.7662e+02 - logprior: -1.6175e-01
Epoch 7/10
10/10 - 15s - loss: 474.0000 - loglik: -4.7344e+02 - logprior: -5.5866e-01
Epoch 8/10
10/10 - 15s - loss: 470.1652 - loglik: -4.6978e+02 - logprior: -3.8746e-01
Epoch 9/10
10/10 - 15s - loss: 469.5652 - loglik: -4.6927e+02 - logprior: -2.9858e-01
Epoch 10/10
10/10 - 15s - loss: 468.4468 - loglik: -4.6835e+02 - logprior: -9.9747e-02
Fitted a model with MAP estimate = -467.9404
expansions: [(25, 4), (54, 2), (55, 1), (58, 1), (66, 1), (70, 3), (71, 1), (72, 3), (73, 5), (74, 3), (77, 1), (86, 1), (94, 1), (101, 1), (102, 1), (103, 1), (104, 1), (132, 1), (133, 2), (134, 1), (135, 7), (137, 1), (157, 1), (158, 5), (159, 2), (174, 8), (198, 1)]
discards: [  1   2   6 200 223]
Fitting a model of length 280 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 18s - loss: 473.4891 - loglik: -4.5491e+02 - logprior: -1.8583e+01
Epoch 2/2
15/15 - 16s - loss: 442.6518 - loglik: -4.4285e+02 - logprior: 0.1985
Fitted a model with MAP estimate = -435.5238
expansions: [(21, 1), (84, 1)]
discards: [228 229 230 231]
Fitting a model of length 278 on 1502 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 25s - loss: 459.1227 - loglik: -4.3753e+02 - logprior: -2.1593e+01
Epoch 2/2
10/10 - 21s - loss: 439.1778 - loglik: -4.3708e+02 - logprior: -2.0942e+00
Fitted a model with MAP estimate = -433.8843
expansions: [(89, 1), (202, 1), (236, 3)]
discards: []
Fitting a model of length 283 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 19s - loss: 451.4227 - loglik: -4.3626e+02 - logprior: -1.5159e+01
Epoch 2/10
15/15 - 16s - loss: 429.3277 - loglik: -4.3114e+02 - logprior: 1.8131
Epoch 3/10
15/15 - 16s - loss: 428.9332 - loglik: -4.3347e+02 - logprior: 4.5398
Epoch 4/10
15/15 - 16s - loss: 425.0722 - loglik: -4.3063e+02 - logprior: 5.5533
Epoch 5/10
15/15 - 16s - loss: 424.4897 - loglik: -4.3053e+02 - logprior: 6.0366
Epoch 6/10
15/15 - 16s - loss: 423.6277 - loglik: -4.3013e+02 - logprior: 6.5059
Epoch 7/10
15/15 - 16s - loss: 423.8791 - loglik: -4.3079e+02 - logprior: 6.9072
Fitted a model with MAP estimate = -423.2397
Time for alignment: 375.8296
Computed alignments with likelihoods: ['-422.6296', '-422.0973', '-423.2397']
Best model has likelihood: -422.0973
SP score = 0.9238
Training of 3 independent models on file il8.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20902cfdf0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20a127dd60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2139a15220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20902afb80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 230.2503 - loglik: -1.8528e+02 - logprior: -4.4973e+01
Epoch 2/10
10/10 - 1s - loss: 181.0443 - loglik: -1.6914e+02 - logprior: -1.1902e+01
Epoch 3/10
10/10 - 1s - loss: 161.3553 - loglik: -1.5560e+02 - logprior: -5.7586e+00
Epoch 4/10
10/10 - 1s - loss: 152.2266 - loglik: -1.4861e+02 - logprior: -3.6185e+00
Epoch 5/10
10/10 - 1s - loss: 148.0090 - loglik: -1.4549e+02 - logprior: -2.5203e+00
Epoch 6/10
10/10 - 1s - loss: 146.2442 - loglik: -1.4425e+02 - logprior: -1.9967e+00
Epoch 7/10
10/10 - 1s - loss: 145.3600 - loglik: -1.4358e+02 - logprior: -1.7754e+00
Epoch 8/10
10/10 - 1s - loss: 144.7414 - loglik: -1.4314e+02 - logprior: -1.5999e+00
Epoch 9/10
10/10 - 1s - loss: 144.5677 - loglik: -1.4318e+02 - logprior: -1.3915e+00
Epoch 10/10
10/10 - 1s - loss: 144.1295 - loglik: -1.4291e+02 - logprior: -1.2178e+00
Fitted a model with MAP estimate = -144.0930
expansions: [(0, 2), (16, 1), (17, 1), (19, 1), (20, 2), (24, 1), (28, 1), (38, 1), (43, 3)]
discards: []
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 194.9304 - loglik: -1.3931e+02 - logprior: -5.5624e+01
Epoch 2/2
10/10 - 1s - loss: 151.2117 - loglik: -1.3446e+02 - logprior: -1.6749e+01
Fitted a model with MAP estimate = -143.4157
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 181.5477 - loglik: -1.3421e+02 - logprior: -4.7336e+01
Epoch 2/2
10/10 - 1s - loss: 152.1998 - loglik: -1.3387e+02 - logprior: -1.8331e+01
Fitted a model with MAP estimate = -147.3104
expansions: []
discards: [0]
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 175.8923 - loglik: -1.3340e+02 - logprior: -4.2489e+01
Epoch 2/10
10/10 - 1s - loss: 144.1486 - loglik: -1.3266e+02 - logprior: -1.1493e+01
Epoch 3/10
10/10 - 1s - loss: 137.3818 - loglik: -1.3295e+02 - logprior: -4.4290e+00
Epoch 4/10
10/10 - 1s - loss: 134.8018 - loglik: -1.3288e+02 - logprior: -1.9205e+00
Epoch 5/10
10/10 - 1s - loss: 133.6618 - loglik: -1.3294e+02 - logprior: -7.2345e-01
Epoch 6/10
10/10 - 1s - loss: 132.9305 - loglik: -1.3276e+02 - logprior: -1.7094e-01
Epoch 7/10
10/10 - 1s - loss: 132.4242 - loglik: -1.3253e+02 - logprior: 0.1023
Epoch 8/10
10/10 - 1s - loss: 132.4244 - loglik: -1.3279e+02 - logprior: 0.3650
Fitted a model with MAP estimate = -132.1797
Time for alignment: 33.6402
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 230.2935 - loglik: -1.8532e+02 - logprior: -4.4974e+01
Epoch 2/10
10/10 - 1s - loss: 180.9844 - loglik: -1.6908e+02 - logprior: -1.1907e+01
Epoch 3/10
10/10 - 1s - loss: 161.7060 - loglik: -1.5595e+02 - logprior: -5.7607e+00
Epoch 4/10
10/10 - 1s - loss: 152.5502 - loglik: -1.4893e+02 - logprior: -3.6183e+00
Epoch 5/10
10/10 - 1s - loss: 148.6783 - loglik: -1.4614e+02 - logprior: -2.5432e+00
Epoch 6/10
10/10 - 1s - loss: 146.6804 - loglik: -1.4468e+02 - logprior: -2.0015e+00
Epoch 7/10
10/10 - 1s - loss: 145.3227 - loglik: -1.4357e+02 - logprior: -1.7559e+00
Epoch 8/10
10/10 - 1s - loss: 144.6418 - loglik: -1.4310e+02 - logprior: -1.5466e+00
Epoch 9/10
10/10 - 1s - loss: 144.3586 - loglik: -1.4302e+02 - logprior: -1.3369e+00
Epoch 10/10
10/10 - 1s - loss: 144.0226 - loglik: -1.4284e+02 - logprior: -1.1779e+00
Fitted a model with MAP estimate = -143.9342
expansions: [(0, 2), (17, 1), (18, 3), (20, 1), (21, 1), (28, 1), (38, 1), (43, 3)]
discards: []
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 195.0122 - loglik: -1.3925e+02 - logprior: -5.5757e+01
Epoch 2/2
10/10 - 1s - loss: 151.3960 - loglik: -1.3449e+02 - logprior: -1.6909e+01
Fitted a model with MAP estimate = -143.7253
expansions: [(3, 1), (18, 1)]
discards: [ 0 22]
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 181.9906 - loglik: -1.3447e+02 - logprior: -4.7519e+01
Epoch 2/2
10/10 - 1s - loss: 152.5656 - loglik: -1.3422e+02 - logprior: -1.8342e+01
Fitted a model with MAP estimate = -147.5292
expansions: []
discards: [0]
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 176.3103 - loglik: -1.3384e+02 - logprior: -4.2473e+01
Epoch 2/10
10/10 - 1s - loss: 144.6569 - loglik: -1.3319e+02 - logprior: -1.1468e+01
Epoch 3/10
10/10 - 1s - loss: 137.5237 - loglik: -1.3311e+02 - logprior: -4.4129e+00
Epoch 4/10
10/10 - 1s - loss: 134.9815 - loglik: -1.3309e+02 - logprior: -1.8929e+00
Epoch 5/10
10/10 - 1s - loss: 133.5279 - loglik: -1.3282e+02 - logprior: -7.0648e-01
Epoch 6/10
10/10 - 1s - loss: 132.7737 - loglik: -1.3261e+02 - logprior: -1.5887e-01
Epoch 7/10
10/10 - 1s - loss: 132.7413 - loglik: -1.3285e+02 - logprior: 0.1078
Epoch 8/10
10/10 - 1s - loss: 132.2583 - loglik: -1.3265e+02 - logprior: 0.3952
Epoch 9/10
10/10 - 1s - loss: 132.0563 - loglik: -1.3275e+02 - logprior: 0.6940
Epoch 10/10
10/10 - 1s - loss: 132.0387 - loglik: -1.3294e+02 - logprior: 0.8995
Fitted a model with MAP estimate = -131.8911
Time for alignment: 35.1888
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 230.1333 - loglik: -1.8516e+02 - logprior: -4.4975e+01
Epoch 2/10
10/10 - 1s - loss: 180.4567 - loglik: -1.6854e+02 - logprior: -1.1912e+01
Epoch 3/10
10/10 - 1s - loss: 160.8903 - loglik: -1.5516e+02 - logprior: -5.7311e+00
Epoch 4/10
10/10 - 1s - loss: 151.3199 - loglik: -1.4774e+02 - logprior: -3.5840e+00
Epoch 5/10
10/10 - 1s - loss: 147.3787 - loglik: -1.4481e+02 - logprior: -2.5681e+00
Epoch 6/10
10/10 - 1s - loss: 145.6222 - loglik: -1.4359e+02 - logprior: -2.0337e+00
Epoch 7/10
10/10 - 1s - loss: 144.9305 - loglik: -1.4317e+02 - logprior: -1.7631e+00
Epoch 8/10
10/10 - 1s - loss: 144.5890 - loglik: -1.4303e+02 - logprior: -1.5611e+00
Epoch 9/10
10/10 - 1s - loss: 144.3252 - loglik: -1.4298e+02 - logprior: -1.3414e+00
Epoch 10/10
10/10 - 1s - loss: 144.0204 - loglik: -1.4284e+02 - logprior: -1.1762e+00
Fitted a model with MAP estimate = -144.0542
expansions: [(0, 2), (17, 1), (18, 3), (22, 1), (24, 1), (28, 1), (38, 1), (43, 3)]
discards: []
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 195.2441 - loglik: -1.3948e+02 - logprior: -5.5766e+01
Epoch 2/2
10/10 - 1s - loss: 151.6830 - loglik: -1.3476e+02 - logprior: -1.6926e+01
Fitted a model with MAP estimate = -143.8048
expansions: [(3, 1), (18, 1)]
discards: [0]
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 181.7508 - loglik: -1.3426e+02 - logprior: -4.7488e+01
Epoch 2/2
10/10 - 1s - loss: 152.6525 - loglik: -1.3421e+02 - logprior: -1.8445e+01
Fitted a model with MAP estimate = -147.6033
expansions: []
discards: [ 0 22]
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 176.2345 - loglik: -1.3390e+02 - logprior: -4.2330e+01
Epoch 2/10
10/10 - 1s - loss: 144.6837 - loglik: -1.3332e+02 - logprior: -1.1363e+01
Epoch 3/10
10/10 - 1s - loss: 137.7942 - loglik: -1.3342e+02 - logprior: -4.3750e+00
Epoch 4/10
10/10 - 1s - loss: 135.0326 - loglik: -1.3316e+02 - logprior: -1.8732e+00
Epoch 5/10
10/10 - 1s - loss: 133.8252 - loglik: -1.3313e+02 - logprior: -6.9348e-01
Epoch 6/10
10/10 - 1s - loss: 132.9851 - loglik: -1.3283e+02 - logprior: -1.5422e-01
Epoch 7/10
10/10 - 1s - loss: 132.8302 - loglik: -1.3296e+02 - logprior: 0.1275
Epoch 8/10
10/10 - 1s - loss: 132.3557 - loglik: -1.3278e+02 - logprior: 0.4222
Epoch 9/10
10/10 - 1s - loss: 132.3873 - loglik: -1.3311e+02 - logprior: 0.7250
Fitted a model with MAP estimate = -132.1931
Time for alignment: 34.4802
Computed alignments with likelihoods: ['-132.1797', '-131.8911', '-132.1931']
Best model has likelihood: -131.8911
SP score = 0.9121
Training of 3 independent models on file cytb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f214afc7880>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2129375220>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20dc8f1dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f21182d8eb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 252.1592 - loglik: -2.3776e+02 - logprior: -1.4399e+01
Epoch 2/10
11/11 - 2s - loss: 221.5114 - loglik: -2.1785e+02 - logprior: -3.6624e+00
Epoch 3/10
11/11 - 2s - loss: 200.1300 - loglik: -1.9789e+02 - logprior: -2.2351e+00
Epoch 4/10
11/11 - 2s - loss: 189.3985 - loglik: -1.8728e+02 - logprior: -2.1151e+00
Epoch 5/10
11/11 - 2s - loss: 186.5546 - loglik: -1.8448e+02 - logprior: -2.0715e+00
Epoch 6/10
11/11 - 2s - loss: 186.5593 - loglik: -1.8464e+02 - logprior: -1.9189e+00
Fitted a model with MAP estimate = -185.2116
expansions: [(8, 3), (9, 1), (10, 1), (12, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 199.8806 - loglik: -1.8456e+02 - logprior: -1.5317e+01
Epoch 2/2
11/11 - 2s - loss: 184.7915 - loglik: -1.7848e+02 - logprior: -6.3091e+00
Fitted a model with MAP estimate = -182.1076
expansions: [(0, 2)]
discards: [ 0  8 43]
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 188.2015 - loglik: -1.7637e+02 - logprior: -1.1835e+01
Epoch 2/2
11/11 - 2s - loss: 178.7595 - loglik: -1.7566e+02 - logprior: -3.0959e+00
Fitted a model with MAP estimate = -177.3468
expansions: []
discards: [0]
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 191.2676 - loglik: -1.7703e+02 - logprior: -1.4235e+01
Epoch 2/10
11/11 - 2s - loss: 181.3880 - loglik: -1.7718e+02 - logprior: -4.2090e+00
Epoch 3/10
11/11 - 2s - loss: 177.1318 - loglik: -1.7500e+02 - logprior: -2.1326e+00
Epoch 4/10
11/11 - 2s - loss: 176.2075 - loglik: -1.7479e+02 - logprior: -1.4143e+00
Epoch 5/10
11/11 - 2s - loss: 175.6122 - loglik: -1.7460e+02 - logprior: -1.0083e+00
Epoch 6/10
11/11 - 2s - loss: 175.5944 - loglik: -1.7467e+02 - logprior: -9.2150e-01
Epoch 7/10
11/11 - 2s - loss: 175.4352 - loglik: -1.7463e+02 - logprior: -8.0082e-01
Epoch 8/10
11/11 - 2s - loss: 174.5686 - loglik: -1.7380e+02 - logprior: -7.7281e-01
Epoch 9/10
11/11 - 2s - loss: 174.6379 - loglik: -1.7389e+02 - logprior: -7.5018e-01
Fitted a model with MAP estimate = -174.6409
Time for alignment: 54.3432
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 251.5462 - loglik: -2.3714e+02 - logprior: -1.4403e+01
Epoch 2/10
11/11 - 2s - loss: 221.9781 - loglik: -2.1831e+02 - logprior: -3.6682e+00
Epoch 3/10
11/11 - 2s - loss: 199.8087 - loglik: -1.9756e+02 - logprior: -2.2511e+00
Epoch 4/10
11/11 - 2s - loss: 189.9404 - loglik: -1.8780e+02 - logprior: -2.1444e+00
Epoch 5/10
11/11 - 2s - loss: 186.7333 - loglik: -1.8466e+02 - logprior: -2.0699e+00
Epoch 6/10
11/11 - 2s - loss: 186.9878 - loglik: -1.8510e+02 - logprior: -1.8897e+00
Fitted a model with MAP estimate = -185.3239
expansions: [(8, 2), (9, 1), (10, 2), (12, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 199.1157 - loglik: -1.8380e+02 - logprior: -1.5319e+01
Epoch 2/2
11/11 - 2s - loss: 184.8595 - loglik: -1.7853e+02 - logprior: -6.3245e+00
Fitted a model with MAP estimate = -181.9010
expansions: [(0, 2)]
discards: [ 0 10 43]
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 188.0022 - loglik: -1.7617e+02 - logprior: -1.1829e+01
Epoch 2/2
11/11 - 2s - loss: 177.7079 - loglik: -1.7464e+02 - logprior: -3.0728e+00
Fitted a model with MAP estimate = -176.8132
expansions: []
discards: [0]
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 190.5847 - loglik: -1.7631e+02 - logprior: -1.4270e+01
Epoch 2/10
11/11 - 2s - loss: 180.8267 - loglik: -1.7656e+02 - logprior: -4.2618e+00
Epoch 3/10
11/11 - 2s - loss: 177.6857 - loglik: -1.7555e+02 - logprior: -2.1348e+00
Epoch 4/10
11/11 - 2s - loss: 176.1926 - loglik: -1.7477e+02 - logprior: -1.4182e+00
Epoch 5/10
11/11 - 2s - loss: 176.1350 - loglik: -1.7513e+02 - logprior: -1.0065e+00
Epoch 6/10
11/11 - 2s - loss: 174.4589 - loglik: -1.7354e+02 - logprior: -9.1874e-01
Epoch 7/10
11/11 - 2s - loss: 175.5378 - loglik: -1.7473e+02 - logprior: -8.0473e-01
Fitted a model with MAP estimate = -174.8268
Time for alignment: 50.5025
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 252.1967 - loglik: -2.3780e+02 - logprior: -1.4399e+01
Epoch 2/10
11/11 - 2s - loss: 221.1190 - loglik: -2.1746e+02 - logprior: -3.6565e+00
Epoch 3/10
11/11 - 2s - loss: 200.8098 - loglik: -1.9858e+02 - logprior: -2.2254e+00
Epoch 4/10
11/11 - 2s - loss: 190.8789 - loglik: -1.8879e+02 - logprior: -2.0897e+00
Epoch 5/10
11/11 - 2s - loss: 187.9453 - loglik: -1.8589e+02 - logprior: -2.0600e+00
Epoch 6/10
11/11 - 2s - loss: 184.9845 - loglik: -1.8309e+02 - logprior: -1.8950e+00
Epoch 7/10
11/11 - 2s - loss: 186.3178 - loglik: -1.8457e+02 - logprior: -1.7474e+00
Fitted a model with MAP estimate = -185.1910
expansions: [(8, 2), (10, 2), (17, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 199.4830 - loglik: -1.8418e+02 - logprior: -1.5307e+01
Epoch 2/2
11/11 - 2s - loss: 184.9380 - loglik: -1.7864e+02 - logprior: -6.2937e+00
Fitted a model with MAP estimate = -182.0331
expansions: [(0, 2)]
discards: [ 0 42]
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 188.2576 - loglik: -1.7642e+02 - logprior: -1.1840e+01
Epoch 2/2
11/11 - 2s - loss: 177.3311 - loglik: -1.7424e+02 - logprior: -3.0890e+00
Fitted a model with MAP estimate = -176.8848
expansions: []
discards: [0]
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 191.3242 - loglik: -1.7716e+02 - logprior: -1.4168e+01
Epoch 2/10
11/11 - 2s - loss: 180.4193 - loglik: -1.7628e+02 - logprior: -4.1414e+00
Epoch 3/10
11/11 - 2s - loss: 177.5322 - loglik: -1.7540e+02 - logprior: -2.1340e+00
Epoch 4/10
11/11 - 2s - loss: 176.2023 - loglik: -1.7478e+02 - logprior: -1.4237e+00
Epoch 5/10
11/11 - 2s - loss: 175.9413 - loglik: -1.7494e+02 - logprior: -1.0058e+00
Epoch 6/10
11/11 - 2s - loss: 175.7111 - loglik: -1.7478e+02 - logprior: -9.2676e-01
Epoch 7/10
11/11 - 2s - loss: 173.9682 - loglik: -1.7317e+02 - logprior: -8.0308e-01
Epoch 8/10
11/11 - 2s - loss: 175.5670 - loglik: -1.7478e+02 - logprior: -7.8302e-01
Fitted a model with MAP estimate = -174.7399
Time for alignment: 54.9005
Computed alignments with likelihoods: ['-174.6409', '-174.8268', '-174.7399']
Best model has likelihood: -174.6409
SP score = 0.8118
Training of 3 independent models on file kringle.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f21792ccb50>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20e5691be0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20dca51430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2120e43fd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 268.4390 - loglik: -2.2391e+02 - logprior: -4.4530e+01
Epoch 2/10
10/10 - 1s - loss: 208.9499 - loglik: -1.9733e+02 - logprior: -1.1618e+01
Epoch 3/10
10/10 - 1s - loss: 176.7238 - loglik: -1.7099e+02 - logprior: -5.7356e+00
Epoch 4/10
10/10 - 1s - loss: 154.8229 - loglik: -1.5089e+02 - logprior: -3.9292e+00
Epoch 5/10
10/10 - 1s - loss: 147.6178 - loglik: -1.4447e+02 - logprior: -3.1469e+00
Epoch 6/10
10/10 - 1s - loss: 145.2390 - loglik: -1.4264e+02 - logprior: -2.6001e+00
Epoch 7/10
10/10 - 1s - loss: 144.1115 - loglik: -1.4184e+02 - logprior: -2.2673e+00
Epoch 8/10
10/10 - 1s - loss: 143.3328 - loglik: -1.4123e+02 - logprior: -2.0980e+00
Epoch 9/10
10/10 - 1s - loss: 142.6758 - loglik: -1.4070e+02 - logprior: -1.9808e+00
Epoch 10/10
10/10 - 1s - loss: 142.6330 - loglik: -1.4073e+02 - logprior: -1.9046e+00
Fitted a model with MAP estimate = -142.4628
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 171.5824 - loglik: -1.3354e+02 - logprior: -3.8044e+01
Epoch 2/2
10/10 - 1s - loss: 137.0038 - loglik: -1.2716e+02 - logprior: -9.8415e+00
Fitted a model with MAP estimate = -132.3730
expansions: []
discards: [ 0 46 59 66]
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 176.9148 - loglik: -1.3163e+02 - logprior: -4.5282e+01
Epoch 2/2
10/10 - 1s - loss: 149.2566 - loglik: -1.3080e+02 - logprior: -1.8459e+01
Fitted a model with MAP estimate = -145.2044
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 164.9236 - loglik: -1.2843e+02 - logprior: -3.6498e+01
Epoch 2/10
10/10 - 1s - loss: 135.9452 - loglik: -1.2689e+02 - logprior: -9.0552e+00
Epoch 3/10
10/10 - 1s - loss: 130.0421 - loglik: -1.2655e+02 - logprior: -3.4918e+00
Epoch 4/10
10/10 - 1s - loss: 127.8666 - loglik: -1.2658e+02 - logprior: -1.2825e+00
Epoch 5/10
10/10 - 1s - loss: 126.7568 - loglik: -1.2662e+02 - logprior: -1.3202e-01
Epoch 6/10
10/10 - 1s - loss: 126.6235 - loglik: -1.2710e+02 - logprior: 0.4788
Epoch 7/10
10/10 - 1s - loss: 126.0476 - loglik: -1.2688e+02 - logprior: 0.8309
Epoch 8/10
10/10 - 1s - loss: 125.8772 - loglik: -1.2695e+02 - logprior: 1.0689
Epoch 9/10
10/10 - 1s - loss: 125.6311 - loglik: -1.2691e+02 - logprior: 1.2746
Epoch 10/10
10/10 - 1s - loss: 125.7078 - loglik: -1.2715e+02 - logprior: 1.4436
Fitted a model with MAP estimate = -125.5638
Time for alignment: 39.6641
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 268.3154 - loglik: -2.2379e+02 - logprior: -4.4529e+01
Epoch 2/10
10/10 - 1s - loss: 209.2851 - loglik: -1.9767e+02 - logprior: -1.1612e+01
Epoch 3/10
10/10 - 1s - loss: 177.8176 - loglik: -1.7211e+02 - logprior: -5.7040e+00
Epoch 4/10
10/10 - 1s - loss: 157.1466 - loglik: -1.5330e+02 - logprior: -3.8419e+00
Epoch 5/10
10/10 - 1s - loss: 148.4961 - loglik: -1.4545e+02 - logprior: -3.0466e+00
Epoch 6/10
10/10 - 1s - loss: 145.7130 - loglik: -1.4309e+02 - logprior: -2.6218e+00
Epoch 7/10
10/10 - 1s - loss: 144.3836 - loglik: -1.4212e+02 - logprior: -2.2623e+00
Epoch 8/10
10/10 - 1s - loss: 144.0058 - loglik: -1.4196e+02 - logprior: -2.0443e+00
Epoch 9/10
10/10 - 1s - loss: 143.4772 - loglik: -1.4157e+02 - logprior: -1.9119e+00
Epoch 10/10
10/10 - 1s - loss: 143.2491 - loglik: -1.4143e+02 - logprior: -1.8182e+00
Fitted a model with MAP estimate = -142.9861
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 172.0218 - loglik: -1.3398e+02 - logprior: -3.8039e+01
Epoch 2/2
10/10 - 1s - loss: 137.2264 - loglik: -1.2740e+02 - logprior: -9.8264e+00
Fitted a model with MAP estimate = -132.3829
expansions: []
discards: [ 0 46 59 66]
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 176.7487 - loglik: -1.3151e+02 - logprior: -4.5236e+01
Epoch 2/2
10/10 - 1s - loss: 149.3255 - loglik: -1.3087e+02 - logprior: -1.8456e+01
Fitted a model with MAP estimate = -145.1708
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 164.5497 - loglik: -1.2817e+02 - logprior: -3.6380e+01
Epoch 2/10
10/10 - 1s - loss: 136.1730 - loglik: -1.2715e+02 - logprior: -9.0226e+00
Epoch 3/10
10/10 - 1s - loss: 129.9363 - loglik: -1.2645e+02 - logprior: -3.4903e+00
Epoch 4/10
10/10 - 1s - loss: 128.0518 - loglik: -1.2677e+02 - logprior: -1.2783e+00
Epoch 5/10
10/10 - 1s - loss: 126.6888 - loglik: -1.2656e+02 - logprior: -1.3305e-01
Epoch 6/10
10/10 - 1s - loss: 126.5966 - loglik: -1.2707e+02 - logprior: 0.4782
Epoch 7/10
10/10 - 1s - loss: 125.9777 - loglik: -1.2680e+02 - logprior: 0.8254
Epoch 8/10
10/10 - 1s - loss: 125.8232 - loglik: -1.2689e+02 - logprior: 1.0684
Epoch 9/10
10/10 - 1s - loss: 125.9129 - loglik: -1.2719e+02 - logprior: 1.2734
Fitted a model with MAP estimate = -125.6772
Time for alignment: 38.3137
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 268.2172 - loglik: -2.2369e+02 - logprior: -4.4530e+01
Epoch 2/10
10/10 - 1s - loss: 209.1660 - loglik: -1.9754e+02 - logprior: -1.1628e+01
Epoch 3/10
10/10 - 1s - loss: 177.8473 - loglik: -1.7209e+02 - logprior: -5.7587e+00
Epoch 4/10
10/10 - 1s - loss: 156.1739 - loglik: -1.5229e+02 - logprior: -3.8841e+00
Epoch 5/10
10/10 - 1s - loss: 147.7005 - loglik: -1.4461e+02 - logprior: -3.0951e+00
Epoch 6/10
10/10 - 1s - loss: 145.5503 - loglik: -1.4294e+02 - logprior: -2.6124e+00
Epoch 7/10
10/10 - 1s - loss: 144.1917 - loglik: -1.4192e+02 - logprior: -2.2751e+00
Epoch 8/10
10/10 - 1s - loss: 143.5394 - loglik: -1.4144e+02 - logprior: -2.1040e+00
Epoch 9/10
10/10 - 1s - loss: 142.7928 - loglik: -1.4081e+02 - logprior: -1.9808e+00
Epoch 10/10
10/10 - 1s - loss: 142.5565 - loglik: -1.4066e+02 - logprior: -1.8984e+00
Fitted a model with MAP estimate = -142.4899
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 171.6704 - loglik: -1.3363e+02 - logprior: -3.8040e+01
Epoch 2/2
10/10 - 1s - loss: 137.0085 - loglik: -1.2717e+02 - logprior: -9.8362e+00
Fitted a model with MAP estimate = -132.3714
expansions: []
discards: [ 0 46 59 66]
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 176.9382 - loglik: -1.3167e+02 - logprior: -4.5264e+01
Epoch 2/2
10/10 - 1s - loss: 149.2360 - loglik: -1.3078e+02 - logprior: -1.8460e+01
Fitted a model with MAP estimate = -145.1867
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 164.8310 - loglik: -1.2839e+02 - logprior: -3.6442e+01
Epoch 2/10
10/10 - 1s - loss: 135.9924 - loglik: -1.2695e+02 - logprior: -9.0465e+00
Epoch 3/10
10/10 - 1s - loss: 129.9949 - loglik: -1.2651e+02 - logprior: -3.4896e+00
Epoch 4/10
10/10 - 1s - loss: 127.8506 - loglik: -1.2657e+02 - logprior: -1.2758e+00
Epoch 5/10
10/10 - 1s - loss: 126.8523 - loglik: -1.2672e+02 - logprior: -1.3052e-01
Epoch 6/10
10/10 - 1s - loss: 126.5781 - loglik: -1.2706e+02 - logprior: 0.4820
Epoch 7/10
10/10 - 1s - loss: 126.0500 - loglik: -1.2688e+02 - logprior: 0.8305
Epoch 8/10
10/10 - 1s - loss: 125.8831 - loglik: -1.2696e+02 - logprior: 1.0733
Epoch 9/10
10/10 - 1s - loss: 125.8573 - loglik: -1.2713e+02 - logprior: 1.2756
Epoch 10/10
10/10 - 1s - loss: 125.5820 - loglik: -1.2703e+02 - logprior: 1.4472
Fitted a model with MAP estimate = -125.5569
Time for alignment: 39.7342
Computed alignments with likelihoods: ['-125.5638', '-125.6772', '-125.5569']
Best model has likelihood: -125.5569
SP score = 0.8583
Training of 3 independent models on file LIM.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2087fb2760>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2087f00a30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20fe9cfb80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20fe88fcd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.7316 - loglik: -1.6674e+02 - logprior: -5.9884e+00
Epoch 2/10
15/15 - 1s - loss: 144.4330 - loglik: -1.4263e+02 - logprior: -1.8076e+00
Epoch 3/10
15/15 - 1s - loss: 130.8660 - loglik: -1.2906e+02 - logprior: -1.8026e+00
Epoch 4/10
15/15 - 1s - loss: 127.6202 - loglik: -1.2589e+02 - logprior: -1.7308e+00
Epoch 5/10
15/15 - 1s - loss: 126.5643 - loglik: -1.2478e+02 - logprior: -1.7796e+00
Epoch 6/10
15/15 - 1s - loss: 126.1729 - loglik: -1.2426e+02 - logprior: -1.9085e+00
Epoch 7/10
15/15 - 1s - loss: 125.8682 - loglik: -1.2394e+02 - logprior: -1.9261e+00
Epoch 8/10
15/15 - 1s - loss: 125.7513 - loglik: -1.2380e+02 - logprior: -1.9529e+00
Epoch 9/10
15/15 - 1s - loss: 125.4586 - loglik: -1.2347e+02 - logprior: -1.9859e+00
Epoch 10/10
15/15 - 1s - loss: 125.2861 - loglik: -1.2328e+02 - logprior: -2.0080e+00
Fitted a model with MAP estimate = -125.2338
expansions: [(9, 2), (10, 3), (11, 2), (15, 1), (27, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 2)]
discards: [0]
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 136.3713 - loglik: -1.2906e+02 - logprior: -7.3112e+00
Epoch 2/2
15/15 - 1s - loss: 127.5303 - loglik: -1.2399e+02 - logprior: -3.5448e+00
Fitted a model with MAP estimate = -125.5560
expansions: [(0, 1)]
discards: [ 0 11 15 45 49]
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 128.9258 - loglik: -1.2358e+02 - logprior: -5.3416e+00
Epoch 2/2
15/15 - 1s - loss: 122.5388 - loglik: -1.2046e+02 - logprior: -2.0771e+00
Fitted a model with MAP estimate = -121.4257
expansions: []
discards: []
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 5s - loss: 125.7398 - loglik: -1.2029e+02 - logprior: -5.4533e+00
Epoch 2/10
15/15 - 1s - loss: 121.6283 - loglik: -1.1959e+02 - logprior: -2.0416e+00
Epoch 3/10
15/15 - 1s - loss: 121.0553 - loglik: -1.1943e+02 - logprior: -1.6301e+00
Epoch 4/10
15/15 - 1s - loss: 120.6319 - loglik: -1.1911e+02 - logprior: -1.5202e+00
Epoch 5/10
15/15 - 1s - loss: 120.4094 - loglik: -1.1887e+02 - logprior: -1.5432e+00
Epoch 6/10
15/15 - 1s - loss: 120.1507 - loglik: -1.1858e+02 - logprior: -1.5755e+00
Epoch 7/10
15/15 - 1s - loss: 120.1452 - loglik: -1.1853e+02 - logprior: -1.6128e+00
Epoch 8/10
15/15 - 1s - loss: 119.9356 - loglik: -1.1830e+02 - logprior: -1.6343e+00
Epoch 9/10
15/15 - 1s - loss: 119.8462 - loglik: -1.1820e+02 - logprior: -1.6423e+00
Epoch 10/10
15/15 - 1s - loss: 119.7602 - loglik: -1.1809e+02 - logprior: -1.6688e+00
Fitted a model with MAP estimate = -119.6116
Time for alignment: 47.7921
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.7283 - loglik: -1.6674e+02 - logprior: -5.9895e+00
Epoch 2/10
15/15 - 1s - loss: 145.3238 - loglik: -1.4350e+02 - logprior: -1.8248e+00
Epoch 3/10
15/15 - 1s - loss: 131.5368 - loglik: -1.2970e+02 - logprior: -1.8354e+00
Epoch 4/10
15/15 - 1s - loss: 127.2619 - loglik: -1.2547e+02 - logprior: -1.7936e+00
Epoch 5/10
15/15 - 1s - loss: 126.1224 - loglik: -1.2427e+02 - logprior: -1.8559e+00
Epoch 6/10
15/15 - 1s - loss: 125.5844 - loglik: -1.2360e+02 - logprior: -1.9842e+00
Epoch 7/10
15/15 - 1s - loss: 125.2849 - loglik: -1.2328e+02 - logprior: -2.0023e+00
Epoch 8/10
15/15 - 1s - loss: 125.0556 - loglik: -1.2304e+02 - logprior: -2.0204e+00
Epoch 9/10
15/15 - 1s - loss: 125.0627 - loglik: -1.2302e+02 - logprior: -2.0418e+00
Fitted a model with MAP estimate = -125.0001
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (15, 1), (27, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Fitting a model of length 61 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 5s - loss: 135.8197 - loglik: -1.2855e+02 - logprior: -7.2651e+00
Epoch 2/2
15/15 - 1s - loss: 127.2012 - loglik: -1.2372e+02 - logprior: -3.4811e+00
Fitted a model with MAP estimate = -125.0863
expansions: []
discards: [12 15 45]
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 128.1525 - loglik: -1.2205e+02 - logprior: -6.1009e+00
Epoch 2/2
15/15 - 1s - loss: 122.1776 - loglik: -1.2008e+02 - logprior: -2.1014e+00
Fitted a model with MAP estimate = -121.3501
expansions: []
discards: []
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 125.6783 - loglik: -1.2024e+02 - logprior: -5.4399e+00
Epoch 2/10
15/15 - 1s - loss: 121.8557 - loglik: -1.1986e+02 - logprior: -1.9934e+00
Epoch 3/10
15/15 - 1s - loss: 120.8379 - loglik: -1.1924e+02 - logprior: -1.5984e+00
Epoch 4/10
15/15 - 1s - loss: 120.8216 - loglik: -1.1932e+02 - logprior: -1.4998e+00
Epoch 5/10
15/15 - 1s - loss: 120.5083 - loglik: -1.1899e+02 - logprior: -1.5142e+00
Epoch 6/10
15/15 - 1s - loss: 120.1391 - loglik: -1.1860e+02 - logprior: -1.5412e+00
Epoch 7/10
15/15 - 1s - loss: 120.1768 - loglik: -1.1859e+02 - logprior: -1.5842e+00
Fitted a model with MAP estimate = -119.9619
Time for alignment: 43.1363
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.6890 - loglik: -1.6670e+02 - logprior: -5.9888e+00
Epoch 2/10
15/15 - 1s - loss: 144.5579 - loglik: -1.4274e+02 - logprior: -1.8208e+00
Epoch 3/10
15/15 - 1s - loss: 130.0607 - loglik: -1.2823e+02 - logprior: -1.8355e+00
Epoch 4/10
15/15 - 1s - loss: 127.1927 - loglik: -1.2542e+02 - logprior: -1.7772e+00
Epoch 5/10
15/15 - 1s - loss: 126.5259 - loglik: -1.2470e+02 - logprior: -1.8275e+00
Epoch 6/10
15/15 - 1s - loss: 125.9447 - loglik: -1.2401e+02 - logprior: -1.9329e+00
Epoch 7/10
15/15 - 1s - loss: 125.9892 - loglik: -1.2404e+02 - logprior: -1.9533e+00
Fitted a model with MAP estimate = -125.6929
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (15, 1), (27, 1), (29, 2), (30, 2), (32, 3), (34, 2)]
discards: [0]
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 136.2007 - loglik: -1.2894e+02 - logprior: -7.2564e+00
Epoch 2/2
15/15 - 1s - loss: 127.0143 - loglik: -1.2353e+02 - logprior: -3.4839e+00
Fitted a model with MAP estimate = -124.8985
expansions: []
discards: [12 15 45 49]
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 128.0180 - loglik: -1.2200e+02 - logprior: -6.0187e+00
Epoch 2/2
15/15 - 1s - loss: 122.0283 - loglik: -1.1993e+02 - logprior: -2.0944e+00
Fitted a model with MAP estimate = -121.3699
expansions: []
discards: [36]
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 126.0624 - loglik: -1.2063e+02 - logprior: -5.4283e+00
Epoch 2/10
15/15 - 1s - loss: 121.8731 - loglik: -1.1987e+02 - logprior: -2.0018e+00
Epoch 3/10
15/15 - 1s - loss: 121.2086 - loglik: -1.1963e+02 - logprior: -1.5824e+00
Epoch 4/10
15/15 - 1s - loss: 120.9853 - loglik: -1.1949e+02 - logprior: -1.4945e+00
Epoch 5/10
15/15 - 1s - loss: 120.5367 - loglik: -1.1904e+02 - logprior: -1.4995e+00
Epoch 6/10
15/15 - 1s - loss: 120.4411 - loglik: -1.1891e+02 - logprior: -1.5356e+00
Epoch 7/10
15/15 - 1s - loss: 120.3478 - loglik: -1.1878e+02 - logprior: -1.5708e+00
Epoch 8/10
15/15 - 1s - loss: 120.0777 - loglik: -1.1848e+02 - logprior: -1.5964e+00
Epoch 9/10
15/15 - 1s - loss: 119.9617 - loglik: -1.1836e+02 - logprior: -1.5993e+00
Epoch 10/10
15/15 - 1s - loss: 120.0011 - loglik: -1.1838e+02 - logprior: -1.6195e+00
Fitted a model with MAP estimate = -119.8020
Time for alignment: 43.4449
Computed alignments with likelihoods: ['-119.6116', '-119.9619', '-119.8020']
Best model has likelihood: -119.6116
SP score = 0.9790
Training of 3 independent models on file annexin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2118473d90>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2139f18a90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2153748b80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2153773bb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 198.9547 - loglik: -1.8349e+02 - logprior: -1.5462e+01
Epoch 2/10
10/10 - 2s - loss: 170.9080 - loglik: -1.6659e+02 - logprior: -4.3140e+00
Epoch 3/10
10/10 - 2s - loss: 153.5018 - loglik: -1.5100e+02 - logprior: -2.5004e+00
Epoch 4/10
10/10 - 2s - loss: 142.1279 - loglik: -1.3991e+02 - logprior: -2.2227e+00
Epoch 5/10
10/10 - 2s - loss: 137.0229 - loglik: -1.3472e+02 - logprior: -2.2988e+00
Epoch 6/10
10/10 - 2s - loss: 133.7142 - loglik: -1.3131e+02 - logprior: -2.4052e+00
Epoch 7/10
10/10 - 2s - loss: 132.8250 - loglik: -1.3045e+02 - logprior: -2.3720e+00
Epoch 8/10
10/10 - 2s - loss: 132.4320 - loglik: -1.3018e+02 - logprior: -2.2540e+00
Epoch 9/10
10/10 - 2s - loss: 131.9189 - loglik: -1.2975e+02 - logprior: -2.1687e+00
Epoch 10/10
10/10 - 3s - loss: 132.2240 - loglik: -1.3008e+02 - logprior: -2.1450e+00
Fitted a model with MAP estimate = -131.7774
expansions: [(4, 2), (5, 2), (7, 2), (8, 2), (24, 1), (30, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: [0]
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 147.2382 - loglik: -1.3085e+02 - logprior: -1.6386e+01
Epoch 2/2
10/10 - 2s - loss: 129.0192 - loglik: -1.2212e+02 - logprior: -6.8973e+00
Fitted a model with MAP estimate = -125.8348
expansions: [(0, 2)]
discards: [ 0 10 13]
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 132.0034 - loglik: -1.1929e+02 - logprior: -1.2716e+01
Epoch 2/2
10/10 - 2s - loss: 121.5995 - loglik: -1.1813e+02 - logprior: -3.4715e+00
Fitted a model with MAP estimate = -120.3438
expansions: []
discards: [0]
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 135.4153 - loglik: -1.2036e+02 - logprior: -1.5057e+01
Epoch 2/10
10/10 - 2s - loss: 124.3111 - loglik: -1.1960e+02 - logprior: -4.7103e+00
Epoch 3/10
10/10 - 2s - loss: 121.0247 - loglik: -1.1854e+02 - logprior: -2.4853e+00
Epoch 4/10
10/10 - 2s - loss: 120.1660 - loglik: -1.1844e+02 - logprior: -1.7281e+00
Epoch 5/10
10/10 - 2s - loss: 118.9664 - loglik: -1.1773e+02 - logprior: -1.2320e+00
Epoch 6/10
10/10 - 2s - loss: 119.1585 - loglik: -1.1803e+02 - logprior: -1.1243e+00
Fitted a model with MAP estimate = -118.6166
Time for alignment: 64.9861
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 198.7751 - loglik: -1.8331e+02 - logprior: -1.5460e+01
Epoch 2/10
10/10 - 2s - loss: 170.9958 - loglik: -1.6669e+02 - logprior: -4.3091e+00
Epoch 3/10
10/10 - 2s - loss: 152.9762 - loglik: -1.5053e+02 - logprior: -2.4478e+00
Epoch 4/10
10/10 - 2s - loss: 143.3910 - loglik: -1.4139e+02 - logprior: -2.0052e+00
Epoch 5/10
10/10 - 2s - loss: 138.1680 - loglik: -1.3623e+02 - logprior: -1.9346e+00
Epoch 6/10
10/10 - 2s - loss: 133.8274 - loglik: -1.3182e+02 - logprior: -2.0060e+00
Epoch 7/10
10/10 - 2s - loss: 132.6896 - loglik: -1.3079e+02 - logprior: -1.8963e+00
Epoch 8/10
10/10 - 2s - loss: 131.5814 - loglik: -1.2983e+02 - logprior: -1.7546e+00
Epoch 9/10
10/10 - 2s - loss: 130.9251 - loglik: -1.2918e+02 - logprior: -1.7487e+00
Epoch 10/10
10/10 - 2s - loss: 131.0862 - loglik: -1.2938e+02 - logprior: -1.7044e+00
Fitted a model with MAP estimate = -130.8626
expansions: [(0, 5), (24, 1), (30, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: []
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 147.5106 - loglik: -1.2929e+02 - logprior: -1.8217e+01
Epoch 2/2
10/10 - 2s - loss: 127.9780 - loglik: -1.2246e+02 - logprior: -5.5191e+00
Fitted a model with MAP estimate = -123.7886
expansions: []
discards: []
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 132.4972 - loglik: -1.1935e+02 - logprior: -1.3143e+01
Epoch 2/10
10/10 - 2s - loss: 122.4373 - loglik: -1.1884e+02 - logprior: -3.5993e+00
Epoch 3/10
10/10 - 2s - loss: 120.8136 - loglik: -1.1886e+02 - logprior: -1.9557e+00
Epoch 4/10
10/10 - 2s - loss: 119.2427 - loglik: -1.1771e+02 - logprior: -1.5343e+00
Epoch 5/10
10/10 - 2s - loss: 119.5963 - loglik: -1.1827e+02 - logprior: -1.3303e+00
Fitted a model with MAP estimate = -118.8811
Time for alignment: 51.4267
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 199.0095 - loglik: -1.8355e+02 - logprior: -1.5461e+01
Epoch 2/10
10/10 - 2s - loss: 171.1083 - loglik: -1.6678e+02 - logprior: -4.3240e+00
Epoch 3/10
10/10 - 2s - loss: 154.9658 - loglik: -1.5245e+02 - logprior: -2.5192e+00
Epoch 4/10
10/10 - 2s - loss: 144.0065 - loglik: -1.4182e+02 - logprior: -2.1878e+00
Epoch 5/10
10/10 - 2s - loss: 138.6790 - loglik: -1.3652e+02 - logprior: -2.1586e+00
Epoch 6/10
10/10 - 2s - loss: 136.3056 - loglik: -1.3408e+02 - logprior: -2.2225e+00
Epoch 7/10
10/10 - 2s - loss: 134.9067 - loglik: -1.3264e+02 - logprior: -2.2668e+00
Epoch 8/10
10/10 - 2s - loss: 134.2166 - loglik: -1.3200e+02 - logprior: -2.2135e+00
Epoch 9/10
10/10 - 2s - loss: 133.8662 - loglik: -1.3173e+02 - logprior: -2.1349e+00
Epoch 10/10
10/10 - 2s - loss: 133.9560 - loglik: -1.3186e+02 - logprior: -2.0956e+00
Fitted a model with MAP estimate = -133.8544
expansions: [(4, 2), (5, 3), (6, 2), (24, 1), (30, 1), (32, 1), (33, 1), (35, 1), (38, 1), (39, 1), (40, 1), (41, 1)]
discards: [0]
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 148.1039 - loglik: -1.3173e+02 - logprior: -1.6373e+01
Epoch 2/2
10/10 - 2s - loss: 129.8972 - loglik: -1.2302e+02 - logprior: -6.8777e+00
Fitted a model with MAP estimate = -126.3759
expansions: [(0, 2)]
discards: [0 7]
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 132.3349 - loglik: -1.1962e+02 - logprior: -1.2711e+01
Epoch 2/2
10/10 - 2s - loss: 121.7072 - loglik: -1.1824e+02 - logprior: -3.4648e+00
Fitted a model with MAP estimate = -120.3814
expansions: []
discards: [0]
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 135.5454 - loglik: -1.2053e+02 - logprior: -1.5014e+01
Epoch 2/10
10/10 - 2s - loss: 124.1940 - loglik: -1.1952e+02 - logprior: -4.6731e+00
Epoch 3/10
10/10 - 2s - loss: 120.9387 - loglik: -1.1846e+02 - logprior: -2.4770e+00
Epoch 4/10
10/10 - 2s - loss: 119.9491 - loglik: -1.1822e+02 - logprior: -1.7311e+00
Epoch 5/10
10/10 - 2s - loss: 119.2349 - loglik: -1.1800e+02 - logprior: -1.2364e+00
Epoch 6/10
10/10 - 2s - loss: 118.4063 - loglik: -1.1729e+02 - logprior: -1.1211e+00
Epoch 7/10
10/10 - 2s - loss: 119.1312 - loglik: -1.1812e+02 - logprior: -1.0104e+00
Fitted a model with MAP estimate = -118.4427
Time for alignment: 65.2826
Computed alignments with likelihoods: ['-118.6166', '-118.8811', '-118.4427']
Best model has likelihood: -118.4427
SP score = 0.9956
Training of 3 independent models on file hormone_rec.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20fe9abfa0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20febdca30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20cb619fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20cb619070>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 159 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 8s - loss: 554.8743 - loglik: -5.4094e+02 - logprior: -1.3934e+01
Epoch 2/10
11/11 - 5s - loss: 507.8893 - loglik: -5.0544e+02 - logprior: -2.4470e+00
Epoch 3/10
11/11 - 5s - loss: 466.9209 - loglik: -4.6589e+02 - logprior: -1.0281e+00
Epoch 4/10
11/11 - 5s - loss: 446.6705 - loglik: -4.4575e+02 - logprior: -9.2487e-01
Epoch 5/10
11/11 - 5s - loss: 437.5647 - loglik: -4.3669e+02 - logprior: -8.7612e-01
Epoch 6/10
11/11 - 5s - loss: 431.2927 - loglik: -4.3055e+02 - logprior: -7.3979e-01
Epoch 7/10
11/11 - 5s - loss: 428.7143 - loglik: -4.2810e+02 - logprior: -6.1806e-01
Epoch 8/10
11/11 - 5s - loss: 427.9087 - loglik: -4.2734e+02 - logprior: -5.6484e-01
Epoch 9/10
11/11 - 5s - loss: 426.7583 - loglik: -4.2620e+02 - logprior: -5.5875e-01
Epoch 10/10
11/11 - 5s - loss: 425.7103 - loglik: -4.2503e+02 - logprior: -6.7732e-01
Fitted a model with MAP estimate = -425.5544
expansions: [(0, 8), (9, 2), (15, 1), (31, 1), (42, 1), (56, 2), (57, 1), (59, 1), (66, 1), (71, 1), (72, 3), (79, 2), (80, 1), (83, 1), (107, 1), (112, 2), (113, 1), (115, 1), (121, 1), (124, 1), (127, 1), (130, 1), (149, 1), (150, 2), (159, 5)]
discards: []
Fitting a model of length 202 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 11s - loss: 451.8669 - loglik: -4.3747e+02 - logprior: -1.4393e+01
Epoch 2/2
11/11 - 7s - loss: 428.5624 - loglik: -4.2514e+02 - logprior: -3.4225e+00
Fitted a model with MAP estimate = -424.2402
expansions: [(0, 20)]
discards: [  0   1   2   3   4   5   6   7  70  92 104 140 197 198 199 200 201]
Fitting a model of length 205 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 10s - loss: 438.5274 - loglik: -4.2703e+02 - logprior: -1.1493e+01
Epoch 2/2
11/11 - 7s - loss: 425.3372 - loglik: -4.2313e+02 - logprior: -2.2077e+00
Fitted a model with MAP estimate = -422.6864
expansions: [(0, 18), (205, 7)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]
Fitting a model of length 209 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 10s - loss: 440.6130 - loglik: -4.2727e+02 - logprior: -1.3343e+01
Epoch 2/10
11/11 - 7s - loss: 424.7978 - loglik: -4.2190e+02 - logprior: -2.8988e+00
Epoch 3/10
11/11 - 7s - loss: 420.4317 - loglik: -4.1966e+02 - logprior: -7.7340e-01
Epoch 4/10
11/11 - 7s - loss: 416.3931 - loglik: -4.1636e+02 - logprior: -3.0552e-02
Epoch 5/10
11/11 - 7s - loss: 416.1790 - loglik: -4.1645e+02 - logprior: 0.2668
Epoch 6/10
11/11 - 7s - loss: 412.7287 - loglik: -4.1320e+02 - logprior: 0.4671
Epoch 7/10
11/11 - 7s - loss: 410.2579 - loglik: -4.1086e+02 - logprior: 0.6025
Epoch 8/10
11/11 - 7s - loss: 408.6646 - loglik: -4.0932e+02 - logprior: 0.6579
Epoch 9/10
11/11 - 7s - loss: 407.1902 - loglik: -4.0789e+02 - logprior: 0.6986
Epoch 10/10
11/11 - 7s - loss: 407.9725 - loglik: -4.0872e+02 - logprior: 0.7465
Fitted a model with MAP estimate = -407.1769
Time for alignment: 186.0064
Fitting a model of length 159 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 9s - loss: 555.3243 - loglik: -5.4138e+02 - logprior: -1.3941e+01
Epoch 2/10
11/11 - 5s - loss: 507.7605 - loglik: -5.0532e+02 - logprior: -2.4433e+00
Epoch 3/10
11/11 - 5s - loss: 470.6794 - loglik: -4.6969e+02 - logprior: -9.9374e-01
Epoch 4/10
11/11 - 5s - loss: 445.0919 - loglik: -4.4415e+02 - logprior: -9.3789e-01
Epoch 5/10
11/11 - 5s - loss: 436.0899 - loglik: -4.3522e+02 - logprior: -8.6876e-01
Epoch 6/10
11/11 - 5s - loss: 430.5146 - loglik: -4.2979e+02 - logprior: -7.2713e-01
Epoch 7/10
11/11 - 5s - loss: 428.9019 - loglik: -4.2824e+02 - logprior: -6.6401e-01
Epoch 8/10
11/11 - 5s - loss: 428.0288 - loglik: -4.2740e+02 - logprior: -6.2805e-01
Epoch 9/10
11/11 - 5s - loss: 424.9212 - loglik: -4.2431e+02 - logprior: -6.1508e-01
Epoch 10/10
11/11 - 5s - loss: 426.5082 - loglik: -4.2581e+02 - logprior: -7.0286e-01
Fitted a model with MAP estimate = -425.5978
expansions: [(0, 8), (9, 4), (31, 1), (43, 1), (51, 1), (57, 1), (58, 2), (59, 2), (66, 1), (70, 1), (71, 1), (74, 2), (75, 1), (76, 1), (83, 1), (86, 1), (110, 1), (111, 2), (112, 1), (118, 1), (124, 1), (127, 1), (149, 2), (150, 1), (159, 5)]
discards: []
Fitting a model of length 203 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 10s - loss: 452.5132 - loglik: -4.3811e+02 - logprior: -1.4401e+01
Epoch 2/2
11/11 - 7s - loss: 429.3630 - loglik: -4.2586e+02 - logprior: -3.5047e+00
Fitted a model with MAP estimate = -424.5876
expansions: [(0, 20), (187, 1)]
discards: [  0   1   2   3   4   5   6   7  75  77 142 198 199 200 201 202]
Fitting a model of length 208 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 10s - loss: 437.2378 - loglik: -4.2577e+02 - logprior: -1.1465e+01
Epoch 2/2
11/11 - 7s - loss: 426.2838 - loglik: -4.2414e+02 - logprior: -2.1437e+00
Fitted a model with MAP estimate = -421.9136
expansions: [(0, 17), (208, 7)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]
Fitting a model of length 210 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 11s - loss: 438.0260 - loglik: -4.2482e+02 - logprior: -1.3206e+01
Epoch 2/10
11/11 - 7s - loss: 424.9639 - loglik: -4.2209e+02 - logprior: -2.8696e+00
Epoch 3/10
11/11 - 7s - loss: 419.2488 - loglik: -4.1846e+02 - logprior: -7.8554e-01
Epoch 4/10
11/11 - 7s - loss: 416.3717 - loglik: -4.1636e+02 - logprior: -1.1708e-02
Epoch 5/10
11/11 - 7s - loss: 414.1601 - loglik: -4.1445e+02 - logprior: 0.2900
Epoch 6/10
11/11 - 7s - loss: 410.1185 - loglik: -4.1056e+02 - logprior: 0.4384
Epoch 7/10
11/11 - 8s - loss: 410.8653 - loglik: -4.1144e+02 - logprior: 0.5754
Fitted a model with MAP estimate = -408.1347
Time for alignment: 166.8351
Fitting a model of length 159 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 8s - loss: 554.9614 - loglik: -5.4104e+02 - logprior: -1.3926e+01
Epoch 2/10
11/11 - 5s - loss: 507.2977 - loglik: -5.0485e+02 - logprior: -2.4499e+00
Epoch 3/10
11/11 - 5s - loss: 468.1597 - loglik: -4.6712e+02 - logprior: -1.0394e+00
Epoch 4/10
11/11 - 5s - loss: 444.2832 - loglik: -4.4335e+02 - logprior: -9.3602e-01
Epoch 5/10
11/11 - 6s - loss: 437.2441 - loglik: -4.3641e+02 - logprior: -8.3022e-01
Epoch 6/10
11/11 - 5s - loss: 432.4814 - loglik: -4.3184e+02 - logprior: -6.3922e-01
Epoch 7/10
11/11 - 5s - loss: 428.1722 - loglik: -4.2762e+02 - logprior: -5.4918e-01
Epoch 8/10
11/11 - 5s - loss: 427.2751 - loglik: -4.2675e+02 - logprior: -5.2179e-01
Epoch 9/10
11/11 - 5s - loss: 427.6397 - loglik: -4.2711e+02 - logprior: -5.2979e-01
Fitted a model with MAP estimate = -426.5533
expansions: [(0, 8), (9, 4), (31, 1), (43, 1), (51, 1), (52, 1), (57, 2), (59, 1), (66, 1), (70, 2), (71, 1), (78, 3), (83, 1), (86, 1), (110, 1), (111, 2), (112, 1), (114, 1), (120, 1), (127, 1), (149, 2), (150, 1), (159, 6)]
discards: []
Fitting a model of length 203 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 10s - loss: 451.8257 - loglik: -4.3758e+02 - logprior: -1.4242e+01
Epoch 2/2
11/11 - 7s - loss: 428.8545 - loglik: -4.2544e+02 - logprior: -3.4171e+00
Fitted a model with MAP estimate = -423.7024
expansions: [(0, 20), (186, 1)]
discards: [  0   1   2   3   4   5   6   7 102 141 197 198 199 200 201 202]
Fitting a model of length 208 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 11s - loss: 436.7451 - loglik: -4.2532e+02 - logprior: -1.1422e+01
Epoch 2/2
11/11 - 8s - loss: 424.7102 - loglik: -4.2259e+02 - logprior: -2.1167e+00
Fitted a model with MAP estimate = -421.0844
expansions: [(0, 19), (208, 7)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
 25]
Fitting a model of length 209 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 11s - loss: 438.3602 - loglik: -4.2511e+02 - logprior: -1.3252e+01
Epoch 2/10
11/11 - 8s - loss: 423.3891 - loglik: -4.2054e+02 - logprior: -2.8464e+00
Epoch 3/10
11/11 - 8s - loss: 419.1538 - loglik: -4.1846e+02 - logprior: -6.9505e-01
Epoch 4/10
11/11 - 8s - loss: 417.0213 - loglik: -4.1709e+02 - logprior: 0.0680
Epoch 5/10
11/11 - 8s - loss: 413.3936 - loglik: -4.1375e+02 - logprior: 0.3525
Epoch 6/10
11/11 - 8s - loss: 412.4070 - loglik: -4.1284e+02 - logprior: 0.4311
Epoch 7/10
11/11 - 8s - loss: 408.5706 - loglik: -4.0913e+02 - logprior: 0.5591
Epoch 8/10
11/11 - 8s - loss: 407.9716 - loglik: -4.0861e+02 - logprior: 0.6418
Epoch 9/10
11/11 - 8s - loss: 406.1967 - loglik: -4.0690e+02 - logprior: 0.7049
Epoch 10/10
11/11 - 8s - loss: 406.9669 - loglik: -4.0771e+02 - logprior: 0.7463
Fitted a model with MAP estimate = -406.1672
Time for alignment: 195.6503
Computed alignments with likelihoods: ['-407.1769', '-408.1347', '-406.1672']
Best model has likelihood: -406.1672
SP score = 0.7085
Training of 3 independent models on file Acetyltransf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2153a2e040>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2178a97dc0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20f5c44cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20dce09430>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 8s - loss: 250.0958 - loglik: -2.4885e+02 - logprior: -1.2445e+00
Epoch 2/10
29/29 - 4s - loss: 224.0965 - loglik: -2.2326e+02 - logprior: -8.3454e-01
Epoch 3/10
29/29 - 4s - loss: 220.7256 - loglik: -2.1992e+02 - logprior: -8.0356e-01
Epoch 4/10
29/29 - 4s - loss: 219.5939 - loglik: -2.1879e+02 - logprior: -8.0285e-01
Epoch 5/10
29/29 - 4s - loss: 218.2768 - loglik: -2.1746e+02 - logprior: -8.1548e-01
Epoch 6/10
29/29 - 4s - loss: 217.9447 - loglik: -2.1712e+02 - logprior: -8.2893e-01
Epoch 7/10
29/29 - 4s - loss: 217.1259 - loglik: -2.1627e+02 - logprior: -8.5099e-01
Epoch 8/10
29/29 - 4s - loss: 216.9009 - loglik: -2.1603e+02 - logprior: -8.7415e-01
Epoch 9/10
29/29 - 4s - loss: 216.2270 - loglik: -2.1532e+02 - logprior: -9.0778e-01
Epoch 10/10
29/29 - 4s - loss: 215.8703 - loglik: -2.1494e+02 - logprior: -9.3383e-01
Fitted a model with MAP estimate = -203.6696
expansions: [(1, 1), (14, 3), (16, 2), (17, 2), (21, 1), (38, 1), (41, 1), (43, 1), (44, 2), (46, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Fitting a model of length 85 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 220.6903 - loglik: -2.1949e+02 - logprior: -1.1975e+00
Epoch 2/2
29/29 - 4s - loss: 215.8815 - loglik: -2.1513e+02 - logprior: -7.4790e-01
Fitted a model with MAP estimate = -197.4036
expansions: [(3, 1)]
discards: [57 65]
Fitting a model of length 84 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 216.6915 - loglik: -2.1560e+02 - logprior: -1.0868e+00
Epoch 2/2
29/29 - 4s - loss: 215.7975 - loglik: -2.1510e+02 - logprior: -7.0079e-01
Fitted a model with MAP estimate = -197.2651
expansions: []
discards: [1]
Fitting a model of length 83 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 9s - loss: 197.2805 - loglik: -1.9666e+02 - logprior: -6.1975e-01
Epoch 2/10
42/42 - 6s - loss: 196.5126 - loglik: -1.9601e+02 - logprior: -5.0295e-01
Epoch 3/10
42/42 - 6s - loss: 195.6238 - loglik: -1.9514e+02 - logprior: -4.8428e-01
Epoch 4/10
42/42 - 6s - loss: 195.9628 - loglik: -1.9548e+02 - logprior: -4.8206e-01
Fitted a model with MAP estimate = -195.0831
Time for alignment: 147.7936
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 249.8810 - loglik: -2.4864e+02 - logprior: -1.2437e+00
Epoch 2/10
29/29 - 4s - loss: 225.1227 - loglik: -2.2429e+02 - logprior: -8.3262e-01
Epoch 3/10
29/29 - 4s - loss: 220.7333 - loglik: -2.1994e+02 - logprior: -7.9388e-01
Epoch 4/10
29/29 - 4s - loss: 219.8510 - loglik: -2.1906e+02 - logprior: -7.9325e-01
Epoch 5/10
29/29 - 4s - loss: 218.9981 - loglik: -2.1821e+02 - logprior: -7.9302e-01
Epoch 6/10
29/29 - 4s - loss: 218.7338 - loglik: -2.1793e+02 - logprior: -8.0747e-01
Epoch 7/10
29/29 - 4s - loss: 217.5792 - loglik: -2.1674e+02 - logprior: -8.3948e-01
Epoch 8/10
29/29 - 4s - loss: 217.2727 - loglik: -2.1641e+02 - logprior: -8.6262e-01
Epoch 9/10
29/29 - 4s - loss: 216.8142 - loglik: -2.1591e+02 - logprior: -8.9948e-01
Epoch 10/10
29/29 - 4s - loss: 216.1071 - loglik: -2.1518e+02 - logprior: -9.2648e-01
Fitted a model with MAP estimate = -204.0900
expansions: [(1, 1), (2, 1), (13, 3), (14, 3), (21, 1), (38, 1), (41, 1), (43, 2), (44, 2), (45, 1), (46, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Fitting a model of length 86 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 220.8656 - loglik: -2.1968e+02 - logprior: -1.1894e+00
Epoch 2/2
29/29 - 4s - loss: 215.7537 - loglik: -2.1501e+02 - logprior: -7.4041e-01
Fitted a model with MAP estimate = -197.3197
expansions: []
discards: [ 1 55 58 66]
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 217.5687 - loglik: -2.1655e+02 - logprior: -1.0143e+00
Epoch 2/2
29/29 - 4s - loss: 216.0695 - loglik: -2.1535e+02 - logprior: -7.1685e-01
Fitted a model with MAP estimate = -197.6853
expansions: [(18, 1)]
discards: []
Fitting a model of length 83 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 9s - loss: 197.0463 - loglik: -1.9638e+02 - logprior: -6.6816e-01
Epoch 2/10
42/42 - 6s - loss: 196.5169 - loglik: -1.9603e+02 - logprior: -4.9085e-01
Epoch 3/10
42/42 - 6s - loss: 195.8572 - loglik: -1.9537e+02 - logprior: -4.8709e-01
Epoch 4/10
42/42 - 6s - loss: 195.5156 - loglik: -1.9503e+02 - logprior: -4.8163e-01
Epoch 5/10
42/42 - 6s - loss: 194.9607 - loglik: -1.9447e+02 - logprior: -4.9046e-01
Epoch 6/10
42/42 - 6s - loss: 194.2856 - loglik: -1.9379e+02 - logprior: -4.9688e-01
Epoch 7/10
42/42 - 6s - loss: 193.9173 - loglik: -1.9341e+02 - logprior: -5.0531e-01
Epoch 8/10
42/42 - 6s - loss: 193.0416 - loglik: -1.9252e+02 - logprior: -5.2088e-01
Epoch 9/10
42/42 - 6s - loss: 192.1373 - loglik: -1.9161e+02 - logprior: -5.2911e-01
Epoch 10/10
42/42 - 6s - loss: 192.2067 - loglik: -1.9166e+02 - logprior: -5.4707e-01
Fitted a model with MAP estimate = -191.5710
Time for alignment: 179.9436
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 250.1029 - loglik: -2.4886e+02 - logprior: -1.2441e+00
Epoch 2/10
29/29 - 4s - loss: 225.4045 - loglik: -2.2456e+02 - logprior: -8.4811e-01
Epoch 3/10
29/29 - 4s - loss: 220.6268 - loglik: -2.1981e+02 - logprior: -8.1664e-01
Epoch 4/10
29/29 - 4s - loss: 219.7547 - loglik: -2.1893e+02 - logprior: -8.2445e-01
Epoch 5/10
29/29 - 4s - loss: 218.7576 - loglik: -2.1793e+02 - logprior: -8.2279e-01
Epoch 6/10
29/29 - 4s - loss: 218.3764 - loglik: -2.1754e+02 - logprior: -8.3413e-01
Epoch 7/10
29/29 - 4s - loss: 217.5462 - loglik: -2.1669e+02 - logprior: -8.5668e-01
Epoch 8/10
29/29 - 4s - loss: 217.4886 - loglik: -2.1661e+02 - logprior: -8.7538e-01
Epoch 9/10
29/29 - 4s - loss: 216.9707 - loglik: -2.1606e+02 - logprior: -9.1467e-01
Epoch 10/10
29/29 - 4s - loss: 216.2665 - loglik: -2.1533e+02 - logprior: -9.3523e-01
Fitted a model with MAP estimate = -204.1030
expansions: [(5, 1), (13, 3), (14, 3), (27, 2), (38, 1), (41, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Fitting a model of length 85 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 221.3783 - loglik: -2.2017e+02 - logprior: -1.2117e+00
Epoch 2/2
29/29 - 4s - loss: 215.7706 - loglik: -2.1501e+02 - logprior: -7.5940e-01
Fitted a model with MAP estimate = -197.4046
expansions: [(3, 1)]
discards: [35 57 65]
Fitting a model of length 83 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 216.7285 - loglik: -2.1564e+02 - logprior: -1.0894e+00
Epoch 2/2
29/29 - 4s - loss: 215.6262 - loglik: -2.1493e+02 - logprior: -6.9525e-01
Fitted a model with MAP estimate = -197.2745
expansions: []
discards: []
Fitting a model of length 83 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 9s - loss: 196.7584 - loglik: -1.9609e+02 - logprior: -6.6816e-01
Epoch 2/10
42/42 - 6s - loss: 196.0453 - loglik: -1.9556e+02 - logprior: -4.8429e-01
Epoch 3/10
42/42 - 6s - loss: 195.8729 - loglik: -1.9539e+02 - logprior: -4.7894e-01
Epoch 4/10
42/42 - 6s - loss: 195.1512 - loglik: -1.9468e+02 - logprior: -4.7407e-01
Epoch 5/10
42/42 - 6s - loss: 194.4590 - loglik: -1.9398e+02 - logprior: -4.8341e-01
Epoch 6/10
42/42 - 6s - loss: 194.0012 - loglik: -1.9351e+02 - logprior: -4.8931e-01
Epoch 7/10
42/42 - 6s - loss: 193.1347 - loglik: -1.9264e+02 - logprior: -4.9879e-01
Epoch 8/10
42/42 - 6s - loss: 192.5117 - loglik: -1.9200e+02 - logprior: -5.1215e-01
Epoch 9/10
42/42 - 6s - loss: 192.2720 - loglik: -1.9175e+02 - logprior: -5.2400e-01
Epoch 10/10
42/42 - 6s - loss: 191.5228 - loglik: -1.9098e+02 - logprior: -5.4002e-01
Fitted a model with MAP estimate = -191.1286
Time for alignment: 180.2447
Computed alignments with likelihoods: ['-195.0831', '-191.5710', '-191.1286']
Best model has likelihood: -191.1286
SP score = 0.6134
Training of 3 independent models on file phoslip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20cb4414c0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2178a118b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f210fed8df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20cb42ddf0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 382.4331 - loglik: -3.3024e+02 - logprior: -5.2194e+01
Epoch 2/10
10/10 - 1s - loss: 299.5464 - loglik: -2.8719e+02 - logprior: -1.2355e+01
Epoch 3/10
10/10 - 2s - loss: 250.1507 - loglik: -2.4461e+02 - logprior: -5.5379e+00
Epoch 4/10
10/10 - 1s - loss: 220.4052 - loglik: -2.1673e+02 - logprior: -3.6727e+00
Epoch 5/10
10/10 - 1s - loss: 209.3538 - loglik: -2.0653e+02 - logprior: -2.8283e+00
Epoch 6/10
10/10 - 2s - loss: 206.2468 - loglik: -2.0402e+02 - logprior: -2.2261e+00
Epoch 7/10
10/10 - 1s - loss: 204.4331 - loglik: -2.0262e+02 - logprior: -1.8179e+00
Epoch 8/10
10/10 - 2s - loss: 203.7209 - loglik: -2.0218e+02 - logprior: -1.5423e+00
Epoch 9/10
10/10 - 2s - loss: 203.2025 - loglik: -2.0183e+02 - logprior: -1.3676e+00
Epoch 10/10
10/10 - 2s - loss: 202.9731 - loglik: -2.0174e+02 - logprior: -1.2370e+00
Fitted a model with MAP estimate = -202.7438
expansions: [(10, 2), (11, 2), (12, 2), (13, 2), (16, 1), (30, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (68, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 253.0464 - loglik: -2.0006e+02 - logprior: -5.2989e+01
Epoch 2/2
10/10 - 2s - loss: 208.0520 - loglik: -1.8749e+02 - logprior: -2.0566e+01
Fitted a model with MAP estimate = -200.2764
expansions: [(0, 3)]
discards: [  0   9  18 102 103]
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 226.8194 - loglik: -1.8612e+02 - logprior: -4.0695e+01
Epoch 2/2
10/10 - 2s - loss: 191.9618 - loglik: -1.8257e+02 - logprior: -9.3892e+00
Fitted a model with MAP estimate = -186.7835
expansions: []
discards: [ 0  2 54 55]
Fitting a model of length 113 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 235.7659 - loglik: -1.8633e+02 - logprior: -4.9432e+01
Epoch 2/10
10/10 - 2s - loss: 200.3473 - loglik: -1.8490e+02 - logprior: -1.5450e+01
Epoch 3/10
10/10 - 2s - loss: 188.2520 - loglik: -1.8379e+02 - logprior: -4.4644e+00
Epoch 4/10
10/10 - 2s - loss: 183.2719 - loglik: -1.8283e+02 - logprior: -4.3755e-01
Epoch 5/10
10/10 - 2s - loss: 181.7742 - loglik: -1.8280e+02 - logprior: 1.0208
Epoch 6/10
10/10 - 2s - loss: 180.4608 - loglik: -1.8224e+02 - logprior: 1.7824
Epoch 7/10
10/10 - 2s - loss: 179.9178 - loglik: -1.8234e+02 - logprior: 2.4237
Epoch 8/10
10/10 - 2s - loss: 179.5066 - loglik: -1.8245e+02 - logprior: 2.9437
Epoch 9/10
10/10 - 2s - loss: 179.0165 - loglik: -1.8231e+02 - logprior: 3.2892
Epoch 10/10
10/10 - 2s - loss: 178.5930 - loglik: -1.8215e+02 - logprior: 3.5559
Fitted a model with MAP estimate = -178.2097
Time for alignment: 56.8047
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 382.6643 - loglik: -3.3047e+02 - logprior: -5.2196e+01
Epoch 2/10
10/10 - 2s - loss: 299.1132 - loglik: -2.8677e+02 - logprior: -1.2348e+01
Epoch 3/10
10/10 - 2s - loss: 250.7200 - loglik: -2.4535e+02 - logprior: -5.3726e+00
Epoch 4/10
10/10 - 2s - loss: 224.5724 - loglik: -2.2141e+02 - logprior: -3.1578e+00
Epoch 5/10
10/10 - 2s - loss: 215.9065 - loglik: -2.1378e+02 - logprior: -2.1247e+00
Epoch 6/10
10/10 - 2s - loss: 212.1158 - loglik: -2.1054e+02 - logprior: -1.5725e+00
Epoch 7/10
10/10 - 2s - loss: 206.4571 - loglik: -2.0511e+02 - logprior: -1.3444e+00
Epoch 8/10
10/10 - 2s - loss: 203.3529 - loglik: -2.0209e+02 - logprior: -1.2628e+00
Epoch 9/10
10/10 - 2s - loss: 201.7553 - loglik: -2.0050e+02 - logprior: -1.2587e+00
Epoch 10/10
10/10 - 2s - loss: 200.9384 - loglik: -1.9974e+02 - logprior: -1.1980e+00
Fitted a model with MAP estimate = -200.6835
expansions: [(10, 2), (11, 2), (12, 3), (13, 3), (14, 1), (44, 2), (45, 2), (50, 1), (51, 1), (52, 1), (55, 1), (79, 1), (81, 2), (82, 2)]
discards: []
Fitting a model of length 120 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 237.2593 - loglik: -1.9455e+02 - logprior: -4.2708e+01
Epoch 2/2
10/10 - 2s - loss: 193.7921 - loglik: -1.8380e+02 - logprior: -9.9950e+00
Fitted a model with MAP estimate = -187.0366
expansions: []
discards: [  0  10  16  55  56 103 104]
Fitting a model of length 113 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 239.3159 - loglik: -1.8861e+02 - logprior: -5.0711e+01
Epoch 2/2
10/10 - 2s - loss: 206.7425 - loglik: -1.8694e+02 - logprior: -1.9802e+01
Fitted a model with MAP estimate = -200.8676
expansions: [(0, 3)]
discards: [0]
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 225.2249 - loglik: -1.8492e+02 - logprior: -4.0300e+01
Epoch 2/10
10/10 - 2s - loss: 191.8074 - loglik: -1.8273e+02 - logprior: -9.0729e+00
Epoch 3/10
10/10 - 2s - loss: 185.3107 - loglik: -1.8261e+02 - logprior: -2.6984e+00
Epoch 4/10
10/10 - 2s - loss: 181.7342 - loglik: -1.8167e+02 - logprior: -6.3086e-02
Epoch 5/10
10/10 - 2s - loss: 181.0665 - loglik: -1.8243e+02 - logprior: 1.3681
Epoch 6/10
10/10 - 2s - loss: 179.9656 - loglik: -1.8214e+02 - logprior: 2.1782
Epoch 7/10
10/10 - 2s - loss: 179.2115 - loglik: -1.8187e+02 - logprior: 2.6574
Epoch 8/10
10/10 - 2s - loss: 178.8715 - loglik: -1.8185e+02 - logprior: 2.9821
Epoch 9/10
10/10 - 2s - loss: 178.6897 - loglik: -1.8194e+02 - logprior: 3.2458
Epoch 10/10
10/10 - 2s - loss: 178.4453 - loglik: -1.8193e+02 - logprior: 3.4836
Fitted a model with MAP estimate = -178.2442
Time for alignment: 58.2233
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 382.5785 - loglik: -3.3038e+02 - logprior: -5.2195e+01
Epoch 2/10
10/10 - 1s - loss: 299.1321 - loglik: -2.8679e+02 - logprior: -1.2342e+01
Epoch 3/10
10/10 - 1s - loss: 250.8696 - loglik: -2.4543e+02 - logprior: -5.4403e+00
Epoch 4/10
10/10 - 1s - loss: 226.2443 - loglik: -2.2284e+02 - logprior: -3.4060e+00
Epoch 5/10
10/10 - 1s - loss: 215.7194 - loglik: -2.1334e+02 - logprior: -2.3770e+00
Epoch 6/10
10/10 - 1s - loss: 207.4771 - loglik: -2.0554e+02 - logprior: -1.9327e+00
Epoch 7/10
10/10 - 1s - loss: 203.8062 - loglik: -2.0202e+02 - logprior: -1.7886e+00
Epoch 8/10
10/10 - 1s - loss: 201.9574 - loglik: -2.0025e+02 - logprior: -1.7110e+00
Epoch 9/10
10/10 - 2s - loss: 201.2813 - loglik: -1.9971e+02 - logprior: -1.5711e+00
Epoch 10/10
10/10 - 2s - loss: 200.8127 - loglik: -1.9938e+02 - logprior: -1.4344e+00
Fitted a model with MAP estimate = -200.5949
expansions: [(10, 2), (11, 2), (12, 2), (13, 2), (14, 2), (16, 1), (27, 2), (44, 2), (45, 2), (50, 1), (51, 1), (56, 1), (58, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Fitting a model of length 121 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 252.7161 - loglik: -1.9955e+02 - logprior: -5.3169e+01
Epoch 2/2
10/10 - 2s - loss: 207.7665 - loglik: -1.8698e+02 - logprior: -2.0785e+01
Fitted a model with MAP estimate = -200.3517
expansions: [(0, 3)]
discards: [  0   9  18  21  37 104 105]
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 227.0654 - loglik: -1.8626e+02 - logprior: -4.0806e+01
Epoch 2/2
10/10 - 2s - loss: 191.8853 - loglik: -1.8248e+02 - logprior: -9.4095e+00
Fitted a model with MAP estimate = -186.9112
expansions: []
discards: [0 2]
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 233.9460 - loglik: -1.8472e+02 - logprior: -4.9228e+01
Epoch 2/10
10/10 - 2s - loss: 198.1565 - loglik: -1.8344e+02 - logprior: -1.4716e+01
Epoch 3/10
10/10 - 2s - loss: 186.5320 - loglik: -1.8248e+02 - logprior: -4.0496e+00
Epoch 4/10
10/10 - 2s - loss: 182.2985 - loglik: -1.8192e+02 - logprior: -3.7466e-01
Epoch 5/10
10/10 - 2s - loss: 180.3124 - loglik: -1.8135e+02 - logprior: 1.0333
Epoch 6/10
10/10 - 2s - loss: 179.3764 - loglik: -1.8118e+02 - logprior: 1.8049
Epoch 7/10
10/10 - 2s - loss: 178.2451 - loglik: -1.8071e+02 - logprior: 2.4604
Epoch 8/10
10/10 - 2s - loss: 177.7922 - loglik: -1.8078e+02 - logprior: 2.9889
Epoch 9/10
10/10 - 2s - loss: 177.3564 - loglik: -1.8069e+02 - logprior: 3.3381
Epoch 10/10
10/10 - 2s - loss: 177.4410 - loglik: -1.8106e+02 - logprior: 3.6157
Fitted a model with MAP estimate = -177.0888
Time for alignment: 56.0159
Computed alignments with likelihoods: ['-178.2097', '-178.2442', '-177.0888']
Best model has likelihood: -177.0888
SP score = 0.9332
Training of 3 independent models on file cah.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2179415f10>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2118182ee0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20cb609b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20dc9e6c40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 184 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 653.5985 - loglik: -6.1605e+02 - logprior: -3.7550e+01
Epoch 2/10
10/10 - 6s - loss: 575.7047 - loglik: -5.6890e+02 - logprior: -6.8018e+00
Epoch 3/10
10/10 - 6s - loss: 513.2906 - loglik: -5.1155e+02 - logprior: -1.7382e+00
Epoch 4/10
10/10 - 6s - loss: 476.8244 - loglik: -4.7599e+02 - logprior: -8.3099e-01
Epoch 5/10
10/10 - 6s - loss: 464.1494 - loglik: -4.6376e+02 - logprior: -3.9024e-01
Epoch 6/10
10/10 - 6s - loss: 460.0883 - loglik: -4.5991e+02 - logprior: -1.7487e-01
Epoch 7/10
10/10 - 6s - loss: 457.8179 - loglik: -4.5783e+02 - logprior: 0.0159
Epoch 8/10
10/10 - 6s - loss: 456.8685 - loglik: -4.5699e+02 - logprior: 0.1196
Epoch 9/10
10/10 - 6s - loss: 456.3827 - loglik: -4.5655e+02 - logprior: 0.1678
Epoch 10/10
10/10 - 6s - loss: 454.7145 - loglik: -4.5496e+02 - logprior: 0.2411
Fitted a model with MAP estimate = -455.1236
expansions: [(14, 1), (15, 1), (28, 1), (29, 2), (30, 2), (31, 2), (41, 2), (42, 1), (44, 1), (50, 1), (51, 1), (54, 4), (90, 1), (113, 1), (114, 2), (115, 2), (116, 4), (118, 2), (119, 2), (120, 3), (130, 2), (160, 1), (163, 1), (165, 2), (166, 6), (167, 1)]
discards: [0]
Fitting a model of length 232 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 12s - loss: 488.3905 - loglik: -4.5404e+02 - logprior: -3.4347e+01
Epoch 2/2
10/10 - 8s - loss: 451.8210 - loglik: -4.4057e+02 - logprior: -1.1249e+01
Fitted a model with MAP estimate = -447.6888
expansions: [(69, 2)]
discards: [  0  72  73 135 149 152]
Fitting a model of length 228 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 474.8992 - loglik: -4.4358e+02 - logprior: -3.1318e+01
Epoch 2/2
10/10 - 7s - loss: 449.5697 - loglik: -4.3988e+02 - logprior: -9.6884e+00
Fitted a model with MAP estimate = -444.8299
expansions: [(0, 4)]
discards: [  0  49  73 162]
Fitting a model of length 228 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 465.7440 - loglik: -4.4171e+02 - logprior: -2.4036e+01
Epoch 2/10
10/10 - 7s - loss: 440.1931 - loglik: -4.3743e+02 - logprior: -2.7608e+00
Epoch 3/10
10/10 - 8s - loss: 436.0265 - loglik: -4.3760e+02 - logprior: 1.5771
Epoch 4/10
10/10 - 8s - loss: 432.6981 - loglik: -4.3602e+02 - logprior: 3.3267
Epoch 5/10
10/10 - 8s - loss: 433.2966 - loglik: -4.3757e+02 - logprior: 4.2770
Fitted a model with MAP estimate = -431.4201
Time for alignment: 152.3095
Fitting a model of length 184 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 654.4452 - loglik: -6.1687e+02 - logprior: -3.7575e+01
Epoch 2/10
10/10 - 6s - loss: 577.2503 - loglik: -5.7044e+02 - logprior: -6.8087e+00
Epoch 3/10
10/10 - 6s - loss: 517.7369 - loglik: -5.1594e+02 - logprior: -1.7984e+00
Epoch 4/10
10/10 - 6s - loss: 478.9530 - loglik: -4.7782e+02 - logprior: -1.1359e+00
Epoch 5/10
10/10 - 6s - loss: 464.9246 - loglik: -4.6415e+02 - logprior: -7.7254e-01
Epoch 6/10
10/10 - 6s - loss: 461.4957 - loglik: -4.6096e+02 - logprior: -5.3408e-01
Epoch 7/10
10/10 - 6s - loss: 459.0377 - loglik: -4.5878e+02 - logprior: -2.5544e-01
Epoch 8/10
10/10 - 6s - loss: 458.1921 - loglik: -4.5814e+02 - logprior: -5.3316e-02
Epoch 9/10
10/10 - 6s - loss: 457.6912 - loglik: -4.5772e+02 - logprior: 0.0287
Epoch 10/10
10/10 - 6s - loss: 456.6388 - loglik: -4.5674e+02 - logprior: 0.0971
Fitted a model with MAP estimate = -456.7789
expansions: [(14, 1), (15, 1), (28, 1), (30, 1), (31, 2), (32, 2), (40, 2), (41, 2), (42, 1), (44, 1), (52, 1), (55, 4), (63, 1), (81, 1), (91, 7), (92, 2), (113, 1), (114, 2), (115, 2), (116, 4), (118, 2), (119, 2), (120, 3), (154, 1), (157, 1), (163, 1), (166, 2), (167, 8)]
discards: [0]
Fitting a model of length 242 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 12s - loss: 489.8951 - loglik: -4.5523e+02 - logprior: -3.4663e+01
Epoch 2/2
10/10 - 9s - loss: 451.8834 - loglik: -4.4060e+02 - logprior: -1.1281e+01
Fitted a model with MAP estimate = -445.8609
expansions: []
discards: [  0  48  50 145 159 162 215]
Fitting a model of length 235 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 12s - loss: 470.1710 - loglik: -4.3886e+02 - logprior: -3.1310e+01
Epoch 2/2
10/10 - 9s - loss: 444.7751 - loglik: -4.3477e+02 - logprior: -1.0008e+01
Fitted a model with MAP estimate = -440.3057
expansions: [(0, 4), (67, 2)]
discards: [  0  70  71 110 111 112 113]
Fitting a model of length 234 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 12s - loss: 465.2542 - loglik: -4.4112e+02 - logprior: -2.4134e+01
Epoch 2/10
10/10 - 9s - loss: 440.1105 - loglik: -4.3729e+02 - logprior: -2.8198e+00
Epoch 3/10
10/10 - 9s - loss: 434.5035 - loglik: -4.3601e+02 - logprior: 1.5074
Epoch 4/10
10/10 - 9s - loss: 432.7112 - loglik: -4.3595e+02 - logprior: 3.2419
Epoch 5/10
10/10 - 9s - loss: 431.2587 - loglik: -4.3548e+02 - logprior: 4.2186
Epoch 6/10
10/10 - 9s - loss: 430.2508 - loglik: -4.3503e+02 - logprior: 4.7782
Epoch 7/10
10/10 - 9s - loss: 430.0136 - loglik: -4.3515e+02 - logprior: 5.1371
Epoch 8/10
10/10 - 9s - loss: 429.5140 - loglik: -4.3492e+02 - logprior: 5.4015
Epoch 9/10
10/10 - 9s - loss: 429.8133 - loglik: -4.3544e+02 - logprior: 5.6291
Fitted a model with MAP estimate = -428.9266
Time for alignment: 201.7277
Fitting a model of length 184 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 654.7052 - loglik: -6.1714e+02 - logprior: -3.7567e+01
Epoch 2/10
10/10 - 6s - loss: 575.8314 - loglik: -5.6903e+02 - logprior: -6.8025e+00
Epoch 3/10
10/10 - 6s - loss: 516.3427 - loglik: -5.1457e+02 - logprior: -1.7768e+00
Epoch 4/10
10/10 - 6s - loss: 478.7717 - loglik: -4.7779e+02 - logprior: -9.7989e-01
Epoch 5/10
10/10 - 6s - loss: 463.8113 - loglik: -4.6326e+02 - logprior: -5.5619e-01
Epoch 6/10
10/10 - 6s - loss: 460.3788 - loglik: -4.6002e+02 - logprior: -3.5942e-01
Epoch 7/10
10/10 - 6s - loss: 457.8685 - loglik: -4.5775e+02 - logprior: -1.1687e-01
Epoch 8/10
10/10 - 6s - loss: 456.2682 - loglik: -4.5632e+02 - logprior: 0.0484
Epoch 9/10
10/10 - 6s - loss: 455.8187 - loglik: -4.5594e+02 - logprior: 0.1223
Epoch 10/10
10/10 - 6s - loss: 455.0087 - loglik: -4.5521e+02 - logprior: 0.1971
Fitted a model with MAP estimate = -454.9973
expansions: [(14, 1), (15, 1), (28, 1), (29, 1), (31, 4), (41, 2), (42, 1), (44, 1), (50, 1), (51, 1), (52, 1), (53, 3), (90, 1), (110, 1), (112, 1), (113, 2), (114, 2), (115, 2), (118, 2), (119, 2), (120, 1), (130, 2), (157, 1), (163, 1), (166, 8), (167, 1)]
discards: [0]
Fitting a model of length 228 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 12s - loss: 487.3608 - loglik: -4.5318e+02 - logprior: -3.4184e+01
Epoch 2/2
10/10 - 8s - loss: 454.0451 - loglik: -4.4294e+02 - logprior: -1.1104e+01
Fitted a model with MAP estimate = -448.1577
expansions: [(35, 1), (135, 1)]
discards: [  0  49 208]
Fitting a model of length 227 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 473.5440 - loglik: -4.4238e+02 - logprior: -3.1167e+01
Epoch 2/2
10/10 - 8s - loss: 448.0279 - loglik: -4.3848e+02 - logprior: -9.5464e+00
Fitted a model with MAP estimate = -444.4097
expansions: [(0, 4)]
discards: [  0 148 162]
Fitting a model of length 228 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 12s - loss: 463.6642 - loglik: -4.3968e+02 - logprior: -2.3987e+01
Epoch 2/10
10/10 - 8s - loss: 440.7267 - loglik: -4.3801e+02 - logprior: -2.7203e+00
Epoch 3/10
10/10 - 9s - loss: 433.9130 - loglik: -4.3558e+02 - logprior: 1.6673
Epoch 4/10
10/10 - 9s - loss: 432.2496 - loglik: -4.3569e+02 - logprior: 3.4385
Epoch 5/10
10/10 - 8s - loss: 431.4181 - loglik: -4.3582e+02 - logprior: 4.3976
Epoch 6/10
10/10 - 9s - loss: 431.2247 - loglik: -4.3618e+02 - logprior: 4.9511
Epoch 7/10
10/10 - 9s - loss: 430.6876 - loglik: -4.3599e+02 - logprior: 5.3016
Epoch 8/10
10/10 - 8s - loss: 430.0598 - loglik: -4.3562e+02 - logprior: 5.5647
Epoch 9/10
10/10 - 9s - loss: 430.1550 - loglik: -4.3597e+02 - logprior: 5.8186
Fitted a model with MAP estimate = -429.7930
Time for alignment: 200.5169
Computed alignments with likelihoods: ['-431.4201', '-428.9266', '-429.7930']
Best model has likelihood: -428.9266
SP score = 0.8484
Training of 3 independent models on file trfl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f214a9bb130>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f214a9bbd90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2098b07f40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20a124ba60>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 756.1168 - loglik: -6.9677e+02 - logprior: -5.9348e+01
Epoch 2/10
11/11 - 12s - loss: 632.5076 - loglik: -6.2605e+02 - logprior: -6.4602e+00
Epoch 3/10
11/11 - 11s - loss: 541.9593 - loglik: -5.4268e+02 - logprior: 0.7245
Epoch 4/10
11/11 - 12s - loss: 495.3832 - loglik: -4.9739e+02 - logprior: 2.0028
Epoch 5/10
11/11 - 11s - loss: 476.9822 - loglik: -4.7938e+02 - logprior: 2.3931
Epoch 6/10
11/11 - 12s - loss: 474.7651 - loglik: -4.7750e+02 - logprior: 2.7389
Epoch 7/10
11/11 - 11s - loss: 466.5514 - loglik: -4.6948e+02 - logprior: 2.9250
Epoch 8/10
11/11 - 12s - loss: 468.8733 - loglik: -4.7208e+02 - logprior: 3.2087
Fitted a model with MAP estimate = -466.6852
expansions: [(22, 4), (23, 1), (27, 1), (49, 2), (50, 1), (52, 2), (55, 1), (60, 1), (62, 1), (64, 1), (77, 3), (78, 3), (88, 1), (101, 3), (102, 1), (103, 1), (129, 1), (134, 1), (162, 1), (164, 6), (181, 1), (182, 1), (183, 3), (199, 6), (200, 9), (219, 1), (222, 1), (224, 1), (225, 1)]
discards: [  0 208]
Fitting a model of length 302 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 21s - loss: 516.4072 - loglik: -4.6452e+02 - logprior: -5.1888e+01
Epoch 2/2
11/11 - 17s - loss: 458.0875 - loglik: -4.4444e+02 - logprior: -1.3643e+01
Fitted a model with MAP estimate = -447.2400
expansions: [(0, 2), (220, 1)]
discards: [  0  95  96 123 124 298 299]
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 21s - loss: 479.6401 - loglik: -4.4425e+02 - logprior: -3.5389e+01
Epoch 2/2
11/11 - 17s - loss: 435.4343 - loglik: -4.3522e+02 - logprior: -2.1406e-01
Fitted a model with MAP estimate = -429.1049
expansions: [(96, 1), (238, 1), (296, 1)]
discards: [ 0 22 62]
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 21s - loss: 480.8271 - loglik: -4.3575e+02 - logprior: -4.5082e+01
Epoch 2/10
11/11 - 21s - loss: 444.3329 - loglik: -4.3458e+02 - logprior: -9.7521e+00
Epoch 3/10
11/11 - 19s - loss: 428.5455 - loglik: -4.3113e+02 - logprior: 2.5848
Epoch 4/10
11/11 - 20s - loss: 417.9813 - loglik: -4.2792e+02 - logprior: 9.9395
Epoch 5/10
11/11 - 20s - loss: 416.9090 - loglik: -4.2918e+02 - logprior: 12.2719
Epoch 6/10
11/11 - 19s - loss: 413.7939 - loglik: -4.2705e+02 - logprior: 13.2578
Epoch 7/10
11/11 - 20s - loss: 414.0949 - loglik: -4.2825e+02 - logprior: 14.1557
Fitted a model with MAP estimate = -413.1281
Time for alignment: 341.9825
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 16s - loss: 756.1102 - loglik: -6.9674e+02 - logprior: -5.9370e+01
Epoch 2/10
11/11 - 14s - loss: 628.6096 - loglik: -6.2214e+02 - logprior: -6.4694e+00
Epoch 3/10
11/11 - 13s - loss: 540.5526 - loglik: -5.4104e+02 - logprior: 0.4860
Epoch 4/10
11/11 - 16s - loss: 489.4753 - loglik: -4.9103e+02 - logprior: 1.5579
Epoch 5/10
11/11 - 14s - loss: 475.3480 - loglik: -4.7720e+02 - logprior: 1.8538
Epoch 6/10
11/11 - 13s - loss: 471.0296 - loglik: -4.7340e+02 - logprior: 2.3746
Epoch 7/10
11/11 - 14s - loss: 467.0225 - loglik: -4.6988e+02 - logprior: 2.8564
Epoch 8/10
11/11 - 13s - loss: 465.3150 - loglik: -4.6852e+02 - logprior: 3.2058
Epoch 9/10
11/11 - 12s - loss: 464.1742 - loglik: -4.6770e+02 - logprior: 3.5234
Epoch 10/10
11/11 - 13s - loss: 463.8409 - loglik: -4.6762e+02 - logprior: 3.7772
Fitted a model with MAP estimate = -464.1319
expansions: [(22, 4), (29, 1), (35, 1), (36, 1), (41, 1), (51, 2), (60, 1), (62, 1), (64, 1), (77, 3), (78, 1), (89, 1), (102, 4), (103, 1), (104, 1), (106, 1), (107, 1), (162, 1), (164, 6), (183, 1), (184, 3), (196, 1), (197, 1), (198, 3), (199, 3), (200, 2), (201, 5), (222, 1), (224, 1), (225, 1)]
discards: [1]
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 20s - loss: 502.7821 - loglik: -4.6279e+02 - logprior: -3.9997e+01
Epoch 2/2
11/11 - 16s - loss: 446.3711 - loglik: -4.4440e+02 - logprior: -1.9712e+00
Fitted a model with MAP estimate = -435.3887
expansions: [(217, 1), (239, 1), (244, 1)]
discards: [ 59 120 121 122]
Fitting a model of length 297 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 21s - loss: 475.6400 - loglik: -4.4129e+02 - logprior: -3.4352e+01
Epoch 2/2
11/11 - 18s - loss: 431.3888 - loglik: -4.3143e+02 - logprior: 0.0445
Fitted a model with MAP estimate = -428.1530
expansions: [(21, 3), (272, 1)]
discards: []
Fitting a model of length 301 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 22s - loss: 469.9025 - loglik: -4.3665e+02 - logprior: -3.3252e+01
Epoch 2/10
11/11 - 17s - loss: 429.6331 - loglik: -4.3101e+02 - logprior: 1.3732
Epoch 3/10
11/11 - 17s - loss: 420.8027 - loglik: -4.2912e+02 - logprior: 8.3178
Epoch 4/10
11/11 - 17s - loss: 418.6624 - loglik: -4.3000e+02 - logprior: 11.3338
Epoch 5/10
11/11 - 16s - loss: 414.9466 - loglik: -4.2797e+02 - logprior: 13.0280
Epoch 6/10
11/11 - 15s - loss: 415.2504 - loglik: -4.2926e+02 - logprior: 14.0098
Fitted a model with MAP estimate = -413.3289
Time for alignment: 343.5730
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 16s - loss: 757.1470 - loglik: -6.9779e+02 - logprior: -5.9354e+01
Epoch 2/10
11/11 - 10s - loss: 632.4659 - loglik: -6.2596e+02 - logprior: -6.5099e+00
Epoch 3/10
11/11 - 14s - loss: 538.2371 - loglik: -5.3858e+02 - logprior: 0.3412
Epoch 4/10
11/11 - 12s - loss: 495.2856 - loglik: -4.9691e+02 - logprior: 1.6283
Epoch 5/10
11/11 - 13s - loss: 477.0741 - loglik: -4.7917e+02 - logprior: 2.0991
Epoch 6/10
11/11 - 13s - loss: 473.7835 - loglik: -4.7640e+02 - logprior: 2.6194
Epoch 7/10
11/11 - 14s - loss: 472.5986 - loglik: -4.7562e+02 - logprior: 3.0235
Epoch 8/10
11/11 - 13s - loss: 466.4601 - loglik: -4.6972e+02 - logprior: 3.2605
Epoch 9/10
11/11 - 13s - loss: 465.6851 - loglik: -4.6926e+02 - logprior: 3.5704
Epoch 10/10
11/11 - 13s - loss: 466.0024 - loglik: -4.6985e+02 - logprior: 3.8434
Fitted a model with MAP estimate = -465.7334
expansions: [(22, 4), (29, 1), (36, 1), (47, 1), (52, 2), (61, 1), (63, 1), (65, 1), (78, 1), (79, 1), (80, 1), (91, 1), (104, 5), (105, 1), (107, 1), (131, 1), (138, 1), (163, 1), (165, 5), (182, 1), (183, 5), (199, 4), (202, 1), (203, 7), (221, 1), (223, 2), (224, 1), (225, 1)]
discards: [  1 123 211]
Fitting a model of length 295 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 18s - loss: 508.3340 - loglik: -4.6838e+02 - logprior: -3.9953e+01
Epoch 2/2
11/11 - 15s - loss: 448.8545 - loglik: -4.4677e+02 - logprior: -2.0844e+00
Fitted a model with MAP estimate = -439.4601
expansions: [(215, 1), (239, 2), (241, 1), (271, 1)]
discards: [ 59 121 122 193]
Fitting a model of length 296 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 21s - loss: 478.9961 - loglik: -4.4455e+02 - logprior: -3.4451e+01
Epoch 2/2
11/11 - 15s - loss: 437.9975 - loglik: -4.3812e+02 - logprior: 0.1205
Fitted a model with MAP estimate = -432.1892
expansions: [(21, 3)]
discards: [273 292]
Fitting a model of length 297 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 20s - loss: 473.8814 - loglik: -4.4065e+02 - logprior: -3.3235e+01
Epoch 2/10
11/11 - 18s - loss: 437.1709 - loglik: -4.3815e+02 - logprior: 0.9823
Epoch 3/10
11/11 - 16s - loss: 427.7891 - loglik: -4.3557e+02 - logprior: 7.7846
Epoch 4/10
11/11 - 19s - loss: 423.2305 - loglik: -4.3405e+02 - logprior: 10.8185
Epoch 5/10
11/11 - 19s - loss: 422.3001 - loglik: -4.3486e+02 - logprior: 12.5607
Epoch 6/10
11/11 - 17s - loss: 419.0966 - loglik: -4.3267e+02 - logprior: 13.5693
Epoch 7/10
11/11 - 20s - loss: 417.0371 - loglik: -4.3123e+02 - logprior: 14.1978
Epoch 8/10
11/11 - 18s - loss: 420.5927 - loglik: -4.3543e+02 - logprior: 14.8357
Fitted a model with MAP estimate = -417.1547
Time for alignment: 372.0749
Computed alignments with likelihoods: ['-413.1281', '-413.3289', '-417.1547']
Best model has likelihood: -413.1281
SP score = 0.9252
Training of 3 independent models on file serpin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2107518f40>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f21781d3370>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f21781d3940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f212924f640>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 31s - loss: 819.1623 - loglik: -8.0964e+02 - logprior: -9.5180e+00
Epoch 2/10
21/21 - 27s - loss: 696.6268 - loglik: -6.9562e+02 - logprior: -1.0096e+00
Epoch 3/10
21/21 - 27s - loss: 647.2305 - loglik: -6.4483e+02 - logprior: -2.3980e+00
Epoch 4/10
21/21 - 28s - loss: 636.2341 - loglik: -6.3405e+02 - logprior: -2.1826e+00
Epoch 5/10
21/21 - 28s - loss: 634.6581 - loglik: -6.3245e+02 - logprior: -2.2091e+00
Epoch 6/10
21/21 - 28s - loss: 634.8391 - loglik: -6.3253e+02 - logprior: -2.3099e+00
Fitted a model with MAP estimate = -633.6299
expansions: [(13, 1), (14, 2), (15, 1), (49, 1), (51, 2), (53, 3), (55, 1), (60, 2), (61, 2), (63, 2), (73, 1), (74, 1), (75, 1), (78, 1), (79, 1), (80, 2), (81, 1), (83, 1), (84, 2), (85, 1), (90, 1), (91, 1), (92, 1), (93, 1), (102, 1), (109, 1), (114, 1), (116, 1), (129, 1), (135, 1), (138, 1), (141, 2), (153, 1), (154, 1), (155, 1), (157, 1), (159, 1), (160, 3), (169, 1), (179, 1), (182, 1), (189, 1), (190, 1), (191, 2), (192, 1), (193, 1), (194, 1), (195, 1), (207, 1), (208, 1), (211, 1), (213, 1), (218, 1), (222, 1), (228, 2), (229, 2), (231, 1), (256, 1), (257, 3), (258, 2), (260, 2), (261, 1), (272, 2), (273, 1)]
discards: [0]
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 48s - loss: 634.1121 - loglik: -6.2445e+02 - logprior: -9.6634e+00
Epoch 2/2
21/21 - 40s - loss: 609.9976 - loglik: -6.0772e+02 - logprior: -2.2816e+00
Fitted a model with MAP estimate = -606.4590
expansions: [(0, 3), (19, 1), (76, 1)]
discards: [  0 181 295 334]
Fitting a model of length 367 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 46s - loss: 613.0519 - loglik: -6.0832e+02 - logprior: -4.7299e+00
Epoch 2/2
21/21 - 44s - loss: 602.9567 - loglik: -6.0478e+02 - logprior: 1.8256
Fitted a model with MAP estimate = -601.5471
expansions: []
discards: [ 1  2 58 59]
Fitting a model of length 363 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 46s - loss: 611.9583 - loglik: -6.0789e+02 - logprior: -4.0686e+00
Epoch 2/10
21/21 - 43s - loss: 602.2455 - loglik: -6.0466e+02 - logprior: 2.4193
Epoch 3/10
21/21 - 43s - loss: 602.8602 - loglik: -6.0586e+02 - logprior: 3.0044
Fitted a model with MAP estimate = -598.9158
Time for alignment: 585.7513
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 30s - loss: 818.9388 - loglik: -8.0942e+02 - logprior: -9.5196e+00
Epoch 2/10
21/21 - 26s - loss: 696.8019 - loglik: -6.9585e+02 - logprior: -9.5099e-01
Epoch 3/10
21/21 - 24s - loss: 645.6686 - loglik: -6.4313e+02 - logprior: -2.5389e+00
Epoch 4/10
21/21 - 24s - loss: 639.5170 - loglik: -6.3707e+02 - logprior: -2.4509e+00
Epoch 5/10
21/21 - 24s - loss: 635.5784 - loglik: -6.3317e+02 - logprior: -2.4046e+00
Epoch 6/10
21/21 - 26s - loss: 635.7319 - loglik: -6.3321e+02 - logprior: -2.5173e+00
Fitted a model with MAP estimate = -634.2779
expansions: [(13, 1), (14, 2), (15, 1), (46, 1), (49, 1), (51, 2), (53, 3), (54, 1), (57, 1), (59, 2), (60, 1), (61, 1), (63, 2), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 2), (81, 1), (84, 1), (85, 1), (86, 1), (93, 1), (94, 1), (95, 1), (102, 1), (111, 1), (113, 1), (115, 1), (117, 1), (136, 1), (138, 1), (139, 1), (155, 1), (156, 1), (157, 1), (159, 1), (161, 1), (162, 3), (171, 1), (181, 1), (184, 1), (191, 1), (192, 1), (193, 2), (194, 1), (195, 1), (196, 1), (200, 1), (210, 1), (213, 1), (214, 1), (216, 1), (220, 1), (230, 2), (231, 3), (232, 1), (236, 1), (256, 1), (257, 1), (258, 1), (259, 2), (261, 2), (262, 1), (271, 1), (272, 2), (273, 1)]
discards: [1]
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 42s - loss: 630.3239 - loglik: -6.2370e+02 - logprior: -6.6197e+00
Epoch 2/2
21/21 - 37s - loss: 606.4323 - loglik: -6.0775e+02 - logprior: 1.3151
Fitted a model with MAP estimate = -602.9143
expansions: []
discards: [334]
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 41s - loss: 613.5176 - loglik: -6.0883e+02 - logprior: -4.6888e+00
Epoch 2/2
21/21 - 39s - loss: 602.2181 - loglik: -6.0401e+02 - logprior: 1.7957
Fitted a model with MAP estimate = -601.6111
expansions: []
discards: [56]
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 40s - loss: 611.1964 - loglik: -6.0690e+02 - logprior: -4.2920e+00
Epoch 2/10
21/21 - 34s - loss: 603.2214 - loglik: -6.0552e+02 - logprior: 2.3009
Epoch 3/10
21/21 - 36s - loss: 602.1591 - loglik: -6.0521e+02 - logprior: 3.0480
Epoch 4/10
21/21 - 40s - loss: 598.4446 - loglik: -6.0187e+02 - logprior: 3.4221
Epoch 5/10
21/21 - 41s - loss: 595.1790 - loglik: -5.9884e+02 - logprior: 3.6577
Epoch 6/10
21/21 - 44s - loss: 596.5223 - loglik: -6.0035e+02 - logprior: 3.8299
Fitted a model with MAP estimate = -595.3522
Time for alignment: 643.7748
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 29s - loss: 819.0817 - loglik: -8.0957e+02 - logprior: -9.5074e+00
Epoch 2/10
21/21 - 29s - loss: 694.1433 - loglik: -6.9315e+02 - logprior: -9.9405e-01
Epoch 3/10
21/21 - 28s - loss: 647.9253 - loglik: -6.4542e+02 - logprior: -2.5080e+00
Epoch 4/10
21/21 - 29s - loss: 636.1159 - loglik: -6.3378e+02 - logprior: -2.3405e+00
Epoch 5/10
21/21 - 28s - loss: 632.4015 - loglik: -6.3000e+02 - logprior: -2.4025e+00
Epoch 6/10
21/21 - 27s - loss: 632.7729 - loglik: -6.3038e+02 - logprior: -2.3900e+00
Fitted a model with MAP estimate = -632.2215
expansions: [(13, 1), (14, 2), (15, 1), (46, 1), (52, 2), (54, 3), (56, 1), (61, 4), (62, 2), (64, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 2), (81, 1), (85, 1), (87, 1), (90, 1), (93, 1), (94, 1), (95, 1), (100, 1), (101, 1), (102, 1), (109, 1), (130, 1), (136, 1), (139, 1), (142, 1), (154, 1), (156, 1), (159, 1), (160, 1), (163, 1), (164, 1), (173, 1), (183, 2), (185, 1), (189, 1), (191, 1), (192, 1), (193, 2), (194, 3), (195, 1), (196, 1), (209, 1), (212, 1), (213, 1), (215, 1), (219, 1), (223, 1), (229, 2), (230, 2), (232, 1), (234, 1), (256, 1), (257, 3), (258, 2), (260, 2), (261, 1), (272, 2), (273, 1)]
discards: [1]
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 47s - loss: 628.5103 - loglik: -6.2175e+02 - logprior: -6.7638e+00
Epoch 2/2
21/21 - 44s - loss: 608.7796 - loglik: -6.0951e+02 - logprior: 0.7284
Fitted a model with MAP estimate = -603.2530
expansions: []
discards: [295 334]
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 45s - loss: 613.1133 - loglik: -6.0839e+02 - logprior: -4.7265e+00
Epoch 2/2
21/21 - 43s - loss: 604.1789 - loglik: -6.0607e+02 - logprior: 1.8947
Fitted a model with MAP estimate = -601.5148
expansions: [(146, 1)]
discards: [56 57]
Fitting a model of length 363 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 48s - loss: 612.6924 - loglik: -6.0855e+02 - logprior: -4.1388e+00
Epoch 2/10
21/21 - 45s - loss: 601.3064 - loglik: -6.0362e+02 - logprior: 2.3156
Epoch 3/10
21/21 - 42s - loss: 600.6306 - loglik: -6.0368e+02 - logprior: 3.0526
Epoch 4/10
21/21 - 43s - loss: 599.1592 - loglik: -6.0255e+02 - logprior: 3.3937
Epoch 5/10
21/21 - 40s - loss: 597.4256 - loglik: -6.0107e+02 - logprior: 3.6478
Epoch 6/10
21/21 - 37s - loss: 594.4349 - loglik: -5.9826e+02 - logprior: 3.8247
Epoch 7/10
21/21 - 37s - loss: 595.1766 - loglik: -5.9911e+02 - logprior: 3.9332
Fitted a model with MAP estimate = -594.7135
Time for alignment: 750.3331
Computed alignments with likelihoods: ['-598.9158', '-595.3522', '-594.7135']
Best model has likelihood: -594.7135
SP score = 0.9664
Training of 3 independent models on file ghf13.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f210745b730>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20d3dd8160>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20d3dd83d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20d3dd8f40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 839.8676 - loglik: -8.3759e+02 - logprior: -2.2743e+00
Epoch 2/10
39/39 - 33s - loss: 741.4895 - loglik: -7.4000e+02 - logprior: -1.4889e+00
Epoch 3/10
39/39 - 31s - loss: 728.9794 - loglik: -7.2729e+02 - logprior: -1.6911e+00
Epoch 4/10
39/39 - 30s - loss: 724.8483 - loglik: -7.2305e+02 - logprior: -1.7938e+00
Epoch 5/10
39/39 - 31s - loss: 723.0110 - loglik: -7.2104e+02 - logprior: -1.9695e+00
Epoch 6/10
39/39 - 32s - loss: 721.6275 - loglik: -7.1942e+02 - logprior: -2.2081e+00
Epoch 7/10
39/39 - 34s - loss: 721.2426 - loglik: -7.1880e+02 - logprior: -2.4425e+00
Epoch 8/10
39/39 - 32s - loss: 720.7298 - loglik: -7.1807e+02 - logprior: -2.6564e+00
Epoch 9/10
39/39 - 31s - loss: 720.7061 - loglik: -7.1784e+02 - logprior: -2.8673e+00
Epoch 10/10
39/39 - 30s - loss: 719.4860 - loglik: -7.1643e+02 - logprior: -3.0540e+00
Fitted a model with MAP estimate = -622.5191
expansions: [(14, 1), (19, 1), (41, 1), (56, 1), (81, 3), (82, 2), (83, 2), (93, 3), (95, 3), (101, 1), (105, 2), (106, 1), (108, 1), (109, 1), (110, 3), (118, 1), (120, 2), (121, 4), (141, 1), (153, 2), (160, 5), (162, 2), (169, 5), (170, 3), (171, 4), (173, 1), (174, 1), (179, 1), (180, 1), (185, 7), (186, 1), (188, 1), (189, 2), (191, 2), (192, 1), (207, 2), (208, 1), (211, 1), (215, 1), (224, 1), (244, 3)]
discards: [  0 148 149]
Fitting a model of length 323 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 57s - loss: 715.9924 - loglik: -7.1253e+02 - logprior: -3.4588e+00
Epoch 2/2
39/39 - 57s - loss: 697.2289 - loglik: -6.9529e+02 - logprior: -1.9380e+00
Fitted a model with MAP estimate = -599.4905
expansions: [(0, 2), (123, 1), (197, 1), (217, 1), (221, 1)]
discards: [  0 109 124 125 126 127 128 132 133 150 174 181 182 183 200 210 254 259
 278 320 321 322]
Fitting a model of length 307 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 57s - loss: 704.2934 - loglik: -7.0232e+02 - logprior: -1.9714e+00
Epoch 2/2
39/39 - 54s - loss: 698.2021 - loglik: -6.9744e+02 - logprior: -7.6447e-01
Fitted a model with MAP estimate = -600.6236
expansions: [(122, 2), (172, 1), (307, 3)]
discards: [  0 123 124 125]
Fitting a model of length 309 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 58s - loss: 605.4425 - loglik: -6.0326e+02 - logprior: -2.1808e+00
Epoch 2/10
43/43 - 55s - loss: 598.8271 - loglik: -5.9809e+02 - logprior: -7.4153e-01
Epoch 3/10
43/43 - 53s - loss: 598.9054 - loglik: -5.9831e+02 - logprior: -5.9500e-01
Fitted a model with MAP estimate = -595.6676
Time for alignment: 962.8553
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 35s - loss: 838.6844 - loglik: -8.3642e+02 - logprior: -2.2603e+00
Epoch 2/10
39/39 - 32s - loss: 740.2959 - loglik: -7.3891e+02 - logprior: -1.3849e+00
Epoch 3/10
39/39 - 31s - loss: 729.2216 - loglik: -7.2767e+02 - logprior: -1.5530e+00
Epoch 4/10
39/39 - 33s - loss: 725.7398 - loglik: -7.2406e+02 - logprior: -1.6798e+00
Epoch 5/10
39/39 - 34s - loss: 723.9507 - loglik: -7.2211e+02 - logprior: -1.8443e+00
Epoch 6/10
39/39 - 35s - loss: 723.0098 - loglik: -7.2092e+02 - logprior: -2.0874e+00
Epoch 7/10
39/39 - 34s - loss: 722.6246 - loglik: -7.2029e+02 - logprior: -2.3333e+00
Epoch 8/10
39/39 - 35s - loss: 722.3399 - loglik: -7.1980e+02 - logprior: -2.5354e+00
Epoch 9/10
39/39 - 36s - loss: 721.3256 - loglik: -7.1861e+02 - logprior: -2.7167e+00
Epoch 10/10
39/39 - 34s - loss: 721.4481 - loglik: -7.1854e+02 - logprior: -2.9100e+00
Fitted a model with MAP estimate = -623.0067
expansions: [(14, 1), (30, 1), (41, 1), (77, 1), (81, 4), (82, 2), (83, 2), (95, 1), (100, 2), (103, 2), (105, 2), (106, 3), (110, 1), (119, 1), (120, 4), (121, 1), (122, 1), (123, 1), (141, 1), (150, 5), (151, 1), (153, 1), (160, 4), (168, 2), (169, 2), (171, 4), (173, 2), (174, 1), (175, 1), (178, 5), (179, 2), (186, 1), (187, 2), (188, 2), (191, 2), (208, 1), (209, 1), (213, 4), (215, 1), (225, 1), (244, 3)]
discards: [0]
Fitting a model of length 323 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 54s - loss: 718.8186 - loglik: -7.1538e+02 - logprior: -3.4430e+00
Epoch 2/2
39/39 - 52s - loss: 700.4042 - loglik: -6.9841e+02 - logprior: -1.9973e+00
Fitted a model with MAP estimate = -602.3752
expansions: [(0, 2), (201, 3), (212, 1), (216, 3), (261, 1)]
discards: [  0  92 112 119 120 121 122 123 124 125 126 127 128 129 130 131 145 179
 180 181 182 183 184 213 223 234 235 236 237 238 249 250 257 284 285 320
 321 322]
Fitting a model of length 295 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 51s - loss: 710.6785 - loglik: -7.0871e+02 - logprior: -1.9679e+00
Epoch 2/2
39/39 - 49s - loss: 704.1082 - loglik: -7.0340e+02 - logprior: -7.1058e-01
Fitted a model with MAP estimate = -605.4172
expansions: [(117, 1), (119, 7), (200, 1), (220, 4), (224, 3), (295, 3)]
discards: [  0 164 165 166 261]
Fitting a model of length 309 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 55s - loss: 608.0358 - loglik: -6.0581e+02 - logprior: -2.2292e+00
Epoch 2/10
43/43 - 51s - loss: 599.5972 - loglik: -5.9880e+02 - logprior: -7.9872e-01
Epoch 3/10
43/43 - 51s - loss: 598.0546 - loglik: -5.9732e+02 - logprior: -7.3933e-01
Epoch 4/10
43/43 - 54s - loss: 595.3060 - loglik: -5.9452e+02 - logprior: -7.8235e-01
Epoch 5/10
43/43 - 55s - loss: 594.8154 - loglik: -5.9414e+02 - logprior: -6.7502e-01
Epoch 6/10
43/43 - 46s - loss: 594.2039 - loglik: -5.9346e+02 - logprior: -7.4147e-01
Epoch 7/10
43/43 - 44s - loss: 592.5443 - loglik: -5.9141e+02 - logprior: -1.1390e+00
Epoch 8/10
43/43 - 42s - loss: 593.2618 - loglik: -5.9220e+02 - logprior: -1.0649e+00
Fitted a model with MAP estimate = -592.0210
Time for alignment: 1191.2937
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 31s - loss: 838.4868 - loglik: -8.3621e+02 - logprior: -2.2749e+00
Epoch 2/10
39/39 - 27s - loss: 742.1193 - loglik: -7.4071e+02 - logprior: -1.4065e+00
Epoch 3/10
39/39 - 27s - loss: 730.0344 - loglik: -7.2839e+02 - logprior: -1.6429e+00
Epoch 4/10
39/39 - 27s - loss: 726.2498 - loglik: -7.2448e+02 - logprior: -1.7652e+00
Epoch 5/10
39/39 - 27s - loss: 724.8080 - loglik: -7.2287e+02 - logprior: -1.9351e+00
Epoch 6/10
39/39 - 27s - loss: 723.4627 - loglik: -7.2128e+02 - logprior: -2.1847e+00
Epoch 7/10
39/39 - 27s - loss: 722.2972 - loglik: -7.1985e+02 - logprior: -2.4453e+00
Epoch 8/10
39/39 - 27s - loss: 721.9500 - loglik: -7.1927e+02 - logprior: -2.6850e+00
Epoch 9/10
39/39 - 27s - loss: 721.5957 - loglik: -7.1869e+02 - logprior: -2.9050e+00
Epoch 10/10
39/39 - 27s - loss: 720.8488 - loglik: -7.1778e+02 - logprior: -3.0650e+00
Fitted a model with MAP estimate = -623.5167
expansions: [(14, 1), (27, 5), (41, 1), (56, 1), (81, 4), (82, 2), (84, 1), (92, 1), (94, 2), (95, 3), (100, 5), (111, 3), (119, 1), (120, 1), (121, 1), (122, 4), (123, 2), (125, 2), (160, 4), (162, 1), (169, 4), (170, 4), (171, 5), (173, 1), (174, 1), (179, 6), (185, 1), (186, 3), (187, 2), (191, 1), (193, 2), (207, 1), (208, 1), (212, 4), (224, 1), (244, 3)]
discards: [  0 104 105 106 107 108 109 148 149 150 151 152 153]
Fitting a model of length 316 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 723.8989 - loglik: -7.2051e+02 - logprior: -3.3908e+00
Epoch 2/2
39/39 - 42s - loss: 704.7842 - loglik: -7.0296e+02 - logprior: -1.8234e+00
Fitted a model with MAP estimate = -605.8399
expansions: [(0, 2), (231, 1)]
discards: [  0  28  29  30  31  92 109 113 121 122 149 152 157 232 277 278 279 313
 314 315]
Fitting a model of length 299 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 45s - loss: 708.0339 - loglik: -7.0616e+02 - logprior: -1.8758e+00
Epoch 2/2
39/39 - 43s - loss: 702.7549 - loglik: -7.0209e+02 - logprior: -6.6117e-01
Fitted a model with MAP estimate = -604.9278
expansions: [(178, 2), (299, 3)]
discards: [  0 108 222 223]
Fitting a model of length 300 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 52s - loss: 608.2775 - loglik: -6.0613e+02 - logprior: -2.1431e+00
Epoch 2/10
43/43 - 51s - loss: 601.2902 - loglik: -6.0060e+02 - logprior: -6.9114e-01
Epoch 3/10
43/43 - 53s - loss: 602.5739 - loglik: -6.0210e+02 - logprior: -4.7541e-01
Fitted a model with MAP estimate = -599.0612
Time for alignment: 805.5393
Computed alignments with likelihoods: ['-595.6676', '-592.0210', '-599.0612']
Best model has likelihood: -592.0210
SP score = 0.5394
Training of 3 independent models on file cryst.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20f5b04670>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2178f733d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f210f8c4bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2098e31400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 278.5093 - loglik: -2.3654e+02 - logprior: -4.1966e+01
Epoch 2/10
10/10 - 2s - loss: 224.3161 - loglik: -2.1341e+02 - logprior: -1.0904e+01
Epoch 3/10
10/10 - 2s - loss: 196.8421 - loglik: -1.9146e+02 - logprior: -5.3789e+00
Epoch 4/10
10/10 - 2s - loss: 180.5578 - loglik: -1.7704e+02 - logprior: -3.5210e+00
Epoch 5/10
10/10 - 2s - loss: 174.6977 - loglik: -1.7210e+02 - logprior: -2.5993e+00
Epoch 6/10
10/10 - 2s - loss: 172.7024 - loglik: -1.7055e+02 - logprior: -2.1572e+00
Epoch 7/10
10/10 - 2s - loss: 171.1074 - loglik: -1.6911e+02 - logprior: -1.9963e+00
Epoch 8/10
10/10 - 2s - loss: 169.6392 - loglik: -1.6773e+02 - logprior: -1.9121e+00
Epoch 9/10
10/10 - 2s - loss: 168.6123 - loglik: -1.6676e+02 - logprior: -1.8497e+00
Epoch 10/10
10/10 - 2s - loss: 167.8338 - loglik: -1.6601e+02 - logprior: -1.8196e+00
Fitted a model with MAP estimate = -167.7251
expansions: [(0, 2), (12, 1), (14, 2), (15, 1), (20, 1), (21, 1), (23, 2), (30, 1), (32, 1), (43, 1), (55, 2), (58, 4)]
discards: []
Fitting a model of length 84 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 213.3648 - loglik: -1.6214e+02 - logprior: -5.1230e+01
Epoch 2/2
10/10 - 2s - loss: 169.0551 - loglik: -1.5390e+02 - logprior: -1.5160e+01
Fitted a model with MAP estimate = -160.9897
expansions: []
discards: [ 0 17]
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 198.0694 - loglik: -1.5476e+02 - logprior: -4.3307e+01
Epoch 2/2
10/10 - 2s - loss: 170.8875 - loglik: -1.5405e+02 - logprior: -1.6842e+01
Fitted a model with MAP estimate = -166.0884
expansions: []
discards: []
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 192.5882 - loglik: -1.5300e+02 - logprior: -3.9584e+01
Epoch 2/10
10/10 - 2s - loss: 163.2402 - loglik: -1.5247e+02 - logprior: -1.0767e+01
Epoch 3/10
10/10 - 2s - loss: 156.0756 - loglik: -1.5213e+02 - logprior: -3.9458e+00
Epoch 4/10
10/10 - 2s - loss: 153.1568 - loglik: -1.5144e+02 - logprior: -1.7206e+00
Epoch 5/10
10/10 - 2s - loss: 151.6867 - loglik: -1.5088e+02 - logprior: -8.0198e-01
Epoch 6/10
10/10 - 2s - loss: 151.3115 - loglik: -1.5109e+02 - logprior: -2.1909e-01
Epoch 7/10
10/10 - 2s - loss: 150.8126 - loglik: -1.5113e+02 - logprior: 0.3148
Epoch 8/10
10/10 - 2s - loss: 150.6995 - loglik: -1.5136e+02 - logprior: 0.6609
Epoch 9/10
10/10 - 2s - loss: 150.3500 - loglik: -1.5120e+02 - logprior: 0.8541
Epoch 10/10
10/10 - 2s - loss: 150.2803 - loglik: -1.5127e+02 - logprior: 0.9867
Fitted a model with MAP estimate = -150.2562
Time for alignment: 59.2154
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 278.5638 - loglik: -2.3660e+02 - logprior: -4.1966e+01
Epoch 2/10
10/10 - 2s - loss: 223.7498 - loglik: -2.1284e+02 - logprior: -1.0909e+01
Epoch 3/10
10/10 - 2s - loss: 195.8917 - loglik: -1.9050e+02 - logprior: -5.3901e+00
Epoch 4/10
10/10 - 2s - loss: 183.4305 - loglik: -1.7994e+02 - logprior: -3.4899e+00
Epoch 5/10
10/10 - 2s - loss: 176.7085 - loglik: -1.7427e+02 - logprior: -2.4433e+00
Epoch 6/10
10/10 - 2s - loss: 171.7378 - loglik: -1.6956e+02 - logprior: -2.1729e+00
Epoch 7/10
10/10 - 2s - loss: 169.8089 - loglik: -1.6765e+02 - logprior: -2.1613e+00
Epoch 8/10
10/10 - 2s - loss: 169.0053 - loglik: -1.6705e+02 - logprior: -1.9601e+00
Epoch 9/10
10/10 - 2s - loss: 168.4259 - loglik: -1.6668e+02 - logprior: -1.7419e+00
Epoch 10/10
10/10 - 2s - loss: 167.8053 - loglik: -1.6614e+02 - logprior: -1.6664e+00
Fitted a model with MAP estimate = -167.6470
expansions: [(0, 2), (12, 1), (14, 2), (15, 1), (20, 2), (23, 3), (30, 1), (32, 1), (43, 1), (55, 2), (58, 4)]
discards: []
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 213.6169 - loglik: -1.6233e+02 - logprior: -5.1288e+01
Epoch 2/2
10/10 - 2s - loss: 169.2638 - loglik: -1.5400e+02 - logprior: -1.5259e+01
Fitted a model with MAP estimate = -161.0694
expansions: []
discards: [ 0 17 26]
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 198.0378 - loglik: -1.5473e+02 - logprior: -4.3309e+01
Epoch 2/2
10/10 - 2s - loss: 170.5804 - loglik: -1.5376e+02 - logprior: -1.6825e+01
Fitted a model with MAP estimate = -165.8047
expansions: []
discards: []
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 192.1086 - loglik: -1.5254e+02 - logprior: -3.9567e+01
Epoch 2/10
10/10 - 2s - loss: 163.0146 - loglik: -1.5228e+02 - logprior: -1.0732e+01
Epoch 3/10
10/10 - 2s - loss: 155.6986 - loglik: -1.5180e+02 - logprior: -3.9019e+00
Epoch 4/10
10/10 - 2s - loss: 152.9372 - loglik: -1.5127e+02 - logprior: -1.6698e+00
Epoch 5/10
10/10 - 2s - loss: 151.3756 - loglik: -1.5062e+02 - logprior: -7.5292e-01
Epoch 6/10
10/10 - 2s - loss: 150.8535 - loglik: -1.5070e+02 - logprior: -1.5786e-01
Epoch 7/10
10/10 - 2s - loss: 150.5682 - loglik: -1.5094e+02 - logprior: 0.3714
Epoch 8/10
10/10 - 2s - loss: 150.1825 - loglik: -1.5090e+02 - logprior: 0.7205
Epoch 9/10
10/10 - 2s - loss: 150.1716 - loglik: -1.5109e+02 - logprior: 0.9138
Epoch 10/10
10/10 - 2s - loss: 150.1246 - loglik: -1.5117e+02 - logprior: 1.0465
Fitted a model with MAP estimate = -149.9076
Time for alignment: 59.5447
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 278.8577 - loglik: -2.3689e+02 - logprior: -4.1965e+01
Epoch 2/10
10/10 - 2s - loss: 224.2144 - loglik: -2.1331e+02 - logprior: -1.0901e+01
Epoch 3/10
10/10 - 2s - loss: 196.2780 - loglik: -1.9089e+02 - logprior: -5.3925e+00
Epoch 4/10
10/10 - 2s - loss: 180.7513 - loglik: -1.7719e+02 - logprior: -3.5593e+00
Epoch 5/10
10/10 - 2s - loss: 173.4662 - loglik: -1.7080e+02 - logprior: -2.6621e+00
Epoch 6/10
10/10 - 2s - loss: 171.1577 - loglik: -1.6883e+02 - logprior: -2.3280e+00
Epoch 7/10
10/10 - 2s - loss: 169.8531 - loglik: -1.6771e+02 - logprior: -2.1394e+00
Epoch 8/10
10/10 - 2s - loss: 169.1771 - loglik: -1.6735e+02 - logprior: -1.8227e+00
Epoch 9/10
10/10 - 1s - loss: 168.8424 - loglik: -1.6722e+02 - logprior: -1.6200e+00
Epoch 10/10
10/10 - 2s - loss: 168.1823 - loglik: -1.6653e+02 - logprior: -1.6491e+00
Fitted a model with MAP estimate = -168.0303
expansions: [(0, 2), (12, 1), (14, 2), (15, 1), (20, 2), (21, 1), (23, 2), (30, 1), (35, 2), (46, 1), (55, 2), (58, 4)]
discards: []
Fitting a model of length 86 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 214.7196 - loglik: -1.6342e+02 - logprior: -5.1302e+01
Epoch 2/2
10/10 - 2s - loss: 169.4039 - loglik: -1.5395e+02 - logprior: -1.5458e+01
Fitted a model with MAP estimate = -161.2766
expansions: []
discards: [ 0 17 27 29 47]
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 198.4886 - loglik: -1.5502e+02 - logprior: -4.3472e+01
Epoch 2/2
10/10 - 2s - loss: 171.3654 - loglik: -1.5445e+02 - logprior: -1.6916e+01
Fitted a model with MAP estimate = -166.4469
expansions: []
discards: []
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 192.8162 - loglik: -1.5318e+02 - logprior: -3.9637e+01
Epoch 2/10
10/10 - 2s - loss: 163.7366 - loglik: -1.5293e+02 - logprior: -1.0806e+01
Epoch 3/10
10/10 - 2s - loss: 156.2392 - loglik: -1.5227e+02 - logprior: -3.9681e+00
Epoch 4/10
10/10 - 2s - loss: 153.5032 - loglik: -1.5175e+02 - logprior: -1.7561e+00
Epoch 5/10
10/10 - 2s - loss: 151.9891 - loglik: -1.5116e+02 - logprior: -8.3191e-01
Epoch 6/10
10/10 - 2s - loss: 151.4079 - loglik: -1.5117e+02 - logprior: -2.3826e-01
Epoch 7/10
10/10 - 2s - loss: 151.1389 - loglik: -1.5143e+02 - logprior: 0.2948
Epoch 8/10
10/10 - 2s - loss: 151.1093 - loglik: -1.5175e+02 - logprior: 0.6411
Epoch 9/10
10/10 - 2s - loss: 150.9046 - loglik: -1.5174e+02 - logprior: 0.8333
Epoch 10/10
10/10 - 2s - loss: 150.4927 - loglik: -1.5146e+02 - logprior: 0.9631
Fitted a model with MAP estimate = -150.5634
Time for alignment: 56.7708
Computed alignments with likelihoods: ['-150.2562', '-149.9076', '-150.5634']
Best model has likelihood: -149.9076
SP score = 0.9243
Training of 3 independent models on file Ald_Xan_dh_2.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20dcd37b50>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20f671ec70>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f21209ba7c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20d3bae790>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 87 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 325.3633 - loglik: -3.0623e+02 - logprior: -1.9131e+01
Epoch 2/10
10/10 - 5s - loss: 277.7148 - loglik: -2.7293e+02 - logprior: -4.7867e+00
Epoch 3/10
10/10 - 7s - loss: 246.5646 - loglik: -2.4415e+02 - logprior: -2.4156e+00
Epoch 4/10
10/10 - 6s - loss: 228.4178 - loglik: -2.2648e+02 - logprior: -1.9372e+00
Epoch 5/10
10/10 - 5s - loss: 222.2874 - loglik: -2.2051e+02 - logprior: -1.7729e+00
Epoch 6/10
10/10 - 6s - loss: 219.5555 - loglik: -2.1792e+02 - logprior: -1.6322e+00
Epoch 7/10
10/10 - 7s - loss: 218.5941 - loglik: -2.1710e+02 - logprior: -1.4989e+00
Epoch 8/10
10/10 - 6s - loss: 217.6694 - loglik: -2.1628e+02 - logprior: -1.3896e+00
Epoch 9/10
10/10 - 7s - loss: 217.3676 - loglik: -2.1602e+02 - logprior: -1.3461e+00
Epoch 10/10
10/10 - 6s - loss: 217.6511 - loglik: -2.1628e+02 - logprior: -1.3692e+00
Fitted a model with MAP estimate = -216.9984
expansions: [(10, 3), (11, 2), (12, 1), (15, 1), (26, 2), (43, 1), (47, 4), (48, 4), (49, 3), (72, 1), (73, 3), (74, 2)]
discards: [0]
Fitting a model of length 113 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 232.6947 - loglik: -2.1326e+02 - logprior: -1.9430e+01
Epoch 2/2
10/10 - 8s - loss: 211.2026 - loglik: -2.0354e+02 - logprior: -7.6652e+00
Fitted a model with MAP estimate = -207.3806
expansions: [(0, 2)]
discards: [ 0 13 14 58 59]
Fitting a model of length 110 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 215.7428 - loglik: -2.0092e+02 - logprior: -1.4819e+01
Epoch 2/2
10/10 - 7s - loss: 201.8214 - loglik: -1.9825e+02 - logprior: -3.5675e+00
Fitted a model with MAP estimate = -200.0073
expansions: []
discards: [0]
Fitting a model of length 109 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 218.4297 - loglik: -2.0097e+02 - logprior: -1.7464e+01
Epoch 2/10
10/10 - 7s - loss: 204.6002 - loglik: -1.9982e+02 - logprior: -4.7825e+00
Epoch 3/10
10/10 - 8s - loss: 200.5860 - loglik: -1.9858e+02 - logprior: -2.0109e+00
Epoch 4/10
10/10 - 8s - loss: 198.7448 - loglik: -1.9750e+02 - logprior: -1.2420e+00
Epoch 5/10
10/10 - 8s - loss: 198.6852 - loglik: -1.9810e+02 - logprior: -5.8037e-01
Epoch 6/10
10/10 - 8s - loss: 198.4417 - loglik: -1.9814e+02 - logprior: -2.9780e-01
Epoch 7/10
10/10 - 6s - loss: 198.2518 - loglik: -1.9807e+02 - logprior: -1.8362e-01
Epoch 8/10
10/10 - 7s - loss: 197.8416 - loglik: -1.9779e+02 - logprior: -4.6745e-02
Epoch 9/10
10/10 - 8s - loss: 197.1983 - loglik: -1.9723e+02 - logprior: 0.0270
Epoch 10/10
10/10 - 7s - loss: 198.7188 - loglik: -1.9880e+02 - logprior: 0.0776
Fitted a model with MAP estimate = -197.6887
Time for alignment: 199.1768
Fitting a model of length 87 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 325.3544 - loglik: -3.0623e+02 - logprior: -1.9128e+01
Epoch 2/10
10/10 - 6s - loss: 279.1804 - loglik: -2.7439e+02 - logprior: -4.7914e+00
Epoch 3/10
10/10 - 6s - loss: 244.8241 - loglik: -2.4236e+02 - logprior: -2.4659e+00
Epoch 4/10
10/10 - 5s - loss: 225.3916 - loglik: -2.2332e+02 - logprior: -2.0674e+00
Epoch 5/10
10/10 - 6s - loss: 221.2658 - loglik: -2.1934e+02 - logprior: -1.9230e+00
Epoch 6/10
10/10 - 6s - loss: 217.6911 - loglik: -2.1589e+02 - logprior: -1.8018e+00
Epoch 7/10
10/10 - 6s - loss: 217.6804 - loglik: -2.1603e+02 - logprior: -1.6461e+00
Epoch 8/10
10/10 - 6s - loss: 216.3540 - loglik: -2.1485e+02 - logprior: -1.5022e+00
Epoch 9/10
10/10 - 5s - loss: 216.0249 - loglik: -2.1460e+02 - logprior: -1.4293e+00
Epoch 10/10
10/10 - 7s - loss: 215.4282 - loglik: -2.1403e+02 - logprior: -1.4031e+00
Fitted a model with MAP estimate = -215.8478
expansions: [(7, 1), (9, 3), (10, 2), (11, 1), (26, 1), (29, 1), (48, 6), (49, 1), (50, 1), (57, 1), (73, 4), (74, 2)]
discards: [0]
Fitting a model of length 110 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 232.2655 - loglik: -2.1289e+02 - logprior: -1.9376e+01
Epoch 2/2
10/10 - 9s - loss: 211.5768 - loglik: -2.0391e+02 - logprior: -7.6706e+00
Fitted a model with MAP estimate = -207.6572
expansions: [(0, 2)]
discards: [ 0 13 14]
Fitting a model of length 109 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 215.8498 - loglik: -2.0103e+02 - logprior: -1.4820e+01
Epoch 2/2
10/10 - 7s - loss: 202.5570 - loglik: -1.9901e+02 - logprior: -3.5514e+00
Fitted a model with MAP estimate = -200.4574
expansions: []
discards: [ 0 59]
Fitting a model of length 107 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 12s - loss: 219.2314 - loglik: -2.0184e+02 - logprior: -1.7390e+01
Epoch 2/10
10/10 - 7s - loss: 205.0113 - loglik: -2.0030e+02 - logprior: -4.7105e+00
Epoch 3/10
10/10 - 8s - loss: 201.2754 - loglik: -1.9927e+02 - logprior: -2.0031e+00
Epoch 4/10
10/10 - 8s - loss: 200.0076 - loglik: -1.9877e+02 - logprior: -1.2344e+00
Epoch 5/10
10/10 - 6s - loss: 198.8196 - loglik: -1.9823e+02 - logprior: -5.8663e-01
Epoch 6/10
10/10 - 8s - loss: 199.5163 - loglik: -1.9922e+02 - logprior: -2.9659e-01
Fitted a model with MAP estimate = -198.7290
Time for alignment: 169.0894
Fitting a model of length 87 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 325.3735 - loglik: -3.0624e+02 - logprior: -1.9130e+01
Epoch 2/10
10/10 - 5s - loss: 278.8208 - loglik: -2.7403e+02 - logprior: -4.7907e+00
Epoch 3/10
10/10 - 6s - loss: 245.8262 - loglik: -2.4336e+02 - logprior: -2.4627e+00
Epoch 4/10
10/10 - 5s - loss: 228.0726 - loglik: -2.2598e+02 - logprior: -2.0927e+00
Epoch 5/10
10/10 - 5s - loss: 220.8167 - loglik: -2.1881e+02 - logprior: -2.0042e+00
Epoch 6/10
10/10 - 6s - loss: 219.7513 - loglik: -2.1787e+02 - logprior: -1.8841e+00
Epoch 7/10
10/10 - 5s - loss: 217.2627 - loglik: -2.1554e+02 - logprior: -1.7231e+00
Epoch 8/10
10/10 - 6s - loss: 216.7280 - loglik: -2.1517e+02 - logprior: -1.5587e+00
Epoch 9/10
10/10 - 7s - loss: 216.3192 - loglik: -2.1483e+02 - logprior: -1.4937e+00
Epoch 10/10
10/10 - 6s - loss: 215.3132 - loglik: -2.1385e+02 - logprior: -1.4639e+00
Fitted a model with MAP estimate = -215.9295
expansions: [(10, 3), (11, 2), (12, 1), (15, 1), (27, 1), (29, 1), (44, 1), (47, 4), (48, 4), (49, 1), (57, 1), (73, 4), (74, 2)]
discards: [0]
Fitting a model of length 112 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 233.2254 - loglik: -2.1376e+02 - logprior: -1.9462e+01
Epoch 2/2
10/10 - 6s - loss: 211.3684 - loglik: -2.0363e+02 - logprior: -7.7353e+00
Fitted a model with MAP estimate = -207.8597
expansions: [(0, 2)]
discards: [ 0 14 58 59 61]
Fitting a model of length 109 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 215.7457 - loglik: -2.0090e+02 - logprior: -1.4842e+01
Epoch 2/2
10/10 - 7s - loss: 202.3691 - loglik: -1.9880e+02 - logprior: -3.5674e+00
Fitted a model with MAP estimate = -200.3432
expansions: []
discards: [0]
Fitting a model of length 108 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 218.8827 - loglik: -2.0147e+02 - logprior: -1.7409e+01
Epoch 2/10
10/10 - 7s - loss: 204.4268 - loglik: -1.9979e+02 - logprior: -4.6378e+00
Epoch 3/10
10/10 - 8s - loss: 201.0604 - loglik: -1.9922e+02 - logprior: -1.8357e+00
Epoch 4/10
10/10 - 7s - loss: 199.0616 - loglik: -1.9799e+02 - logprior: -1.0757e+00
Epoch 5/10
10/10 - 6s - loss: 200.0126 - loglik: -1.9958e+02 - logprior: -4.3639e-01
Fitted a model with MAP estimate = -198.7506
Time for alignment: 150.0739
Computed alignments with likelihoods: ['-197.6887', '-198.7290', '-198.7506']
Best model has likelihood: -197.6887
SP score = 0.1542
Training of 3 independent models on file ace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2098c2e370>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f217855b7f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20f5bb0ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20f5bb0e80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 404 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 66s - loss: 1195.4518 - loglik: -1.1884e+03 - logprior: -7.0642e+00
Epoch 2/10
24/24 - 71s - loss: 1004.6235 - loglik: -1.0050e+03 - logprior: 0.3629
Epoch 3/10
24/24 - 74s - loss: 959.3925 - loglik: -9.5922e+02 - logprior: -1.7692e-01
Epoch 4/10
24/24 - 59s - loss: 951.9824 - loglik: -9.5190e+02 - logprior: -7.9595e-02
Epoch 5/10
24/24 - 53s - loss: 950.9395 - loglik: -9.5053e+02 - logprior: -4.1369e-01
Epoch 6/10
24/24 - 51s - loss: 945.6003 - loglik: -9.4498e+02 - logprior: -6.1594e-01
Epoch 7/10
24/24 - 53s - loss: 950.6353 - loglik: -9.4986e+02 - logprior: -7.7484e-01
Fitted a model with MAP estimate = -947.9064
expansions: [(0, 5), (140, 1), (192, 1), (213, 1), (214, 1), (228, 1), (230, 3), (231, 2), (232, 3), (233, 1), (236, 2), (243, 1), (244, 1), (249, 1), (250, 1), (251, 1), (254, 1), (255, 6), (256, 3), (257, 1), (273, 2), (274, 3), (275, 1), (276, 2), (293, 2), (294, 2), (295, 2), (296, 4), (299, 4), (301, 5), (302, 2), (310, 2), (313, 4), (314, 2), (315, 2), (323, 1), (333, 2), (344, 1), (345, 1), (346, 4), (347, 1), (348, 1), (366, 2), (394, 4), (395, 1), (399, 11), (400, 2)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 34 35 36]
Fitting a model of length 494 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 86s - loss: 951.1938 - loglik: -9.4421e+02 - logprior: -6.9816e+00
Epoch 2/2
24/24 - 81s - loss: 928.2988 - loglik: -9.2893e+02 - logprior: 0.6270
Fitted a model with MAP estimate = -923.0384
expansions: [(0, 6), (245, 1), (481, 1), (486, 5)]
discards: [  1   2   3   4   5   6   7   8 226 270 359 409 465 489 490 491 492 493]
Fitting a model of length 489 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 97s - loss: 938.6730 - loglik: -9.3293e+02 - logprior: -5.7474e+00
Epoch 2/2
24/24 - 106s - loss: 926.5583 - loglik: -9.2811e+02 - logprior: 1.5552
Fitted a model with MAP estimate = -922.4246
expansions: [(0, 6), (7, 2), (75, 1), (337, 1), (484, 1), (485, 1), (488, 3)]
discards: [0 1 2 3 4 5]
Fitting a model of length 498 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 109s - loss: 933.6155 - loglik: -9.2972e+02 - logprior: -3.8913e+00
Epoch 2/10
24/24 - 97s - loss: 920.0511 - loglik: -9.2224e+02 - logprior: 2.1913
Epoch 3/10
24/24 - 100s - loss: 923.7048 - loglik: -9.2647e+02 - logprior: 2.7682
Fitted a model with MAP estimate = -917.3898
Time for alignment: 1387.3763
Fitting a model of length 404 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 73s - loss: 1197.4269 - loglik: -1.1905e+03 - logprior: -6.9728e+00
Epoch 2/10
24/24 - 77s - loss: 1004.5864 - loglik: -1.0050e+03 - logprior: 0.4505
Epoch 3/10
24/24 - 79s - loss: 954.4754 - loglik: -9.5441e+02 - logprior: -6.3991e-02
Epoch 4/10
24/24 - 80s - loss: 951.6202 - loglik: -9.5147e+02 - logprior: -1.5052e-01
Epoch 5/10
24/24 - 79s - loss: 953.9587 - loglik: -9.5365e+02 - logprior: -3.0613e-01
Fitted a model with MAP estimate = -947.8944
expansions: [(0, 5), (139, 1), (181, 1), (209, 1), (211, 1), (212, 1), (226, 1), (229, 4), (230, 3), (231, 1), (234, 2), (238, 2), (240, 1), (241, 1), (246, 1), (249, 1), (251, 2), (252, 5), (253, 3), (254, 2), (270, 1), (271, 1), (272, 2), (273, 1), (274, 1), (287, 1), (292, 2), (293, 1), (294, 5), (295, 2), (296, 4), (298, 6), (308, 1), (309, 1), (311, 3), (313, 1), (314, 1), (325, 2), (326, 2), (327, 1), (345, 5), (346, 1), (347, 1), (366, 2), (392, 6), (404, 6)]
discards: [ 2  3  4  5  6  7  8  9 10 11 12 13 14 33 34]
Fitting a model of length 488 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 116s - loss: 952.9822 - loglik: -9.4649e+02 - logprior: -6.4880e+00
Epoch 2/2
24/24 - 110s - loss: 930.0314 - loglik: -9.3071e+02 - logprior: 0.6751
Fitted a model with MAP estimate = -925.3333
expansions: [(0, 6), (325, 2), (347, 1), (478, 2)]
discards: [  1   2   3   4   5   6   7   8 271 384 486 487]
Fitting a model of length 487 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 98s - loss: 941.3412 - loglik: -9.3567e+02 - logprior: -5.6716e+00
Epoch 2/2
24/24 - 97s - loss: 920.5690 - loglik: -9.2205e+02 - logprior: 1.4794
Fitted a model with MAP estimate = -923.2594
expansions: [(0, 6), (7, 2), (334, 1), (475, 1), (487, 6)]
discards: [  0   1   2   3   4   5 323]
Fitting a model of length 496 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 118s - loss: 932.1253 - loglik: -9.2842e+02 - logprior: -3.7030e+00
Epoch 2/10
24/24 - 119s - loss: 922.7051 - loglik: -9.2461e+02 - logprior: 1.9073
Epoch 3/10
24/24 - 114s - loss: 918.7106 - loglik: -9.2142e+02 - logprior: 2.7129
Epoch 4/10
24/24 - 116s - loss: 919.3972 - loglik: -9.2229e+02 - logprior: 2.8965
Fitted a model with MAP estimate = -916.0682
Time for alignment: 1612.6860
Fitting a model of length 404 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 81s - loss: 1195.2946 - loglik: -1.1882e+03 - logprior: -7.1161e+00
Epoch 2/10
24/24 - 72s - loss: 1008.5639 - loglik: -1.0090e+03 - logprior: 0.4403
Epoch 3/10
24/24 - 73s - loss: 960.5369 - loglik: -9.6047e+02 - logprior: -6.6241e-02
Epoch 4/10
24/24 - 67s - loss: 949.0633 - loglik: -9.4882e+02 - logprior: -2.4436e-01
Epoch 5/10
24/24 - 70s - loss: 949.7256 - loglik: -9.4936e+02 - logprior: -3.6280e-01
Fitted a model with MAP estimate = -949.0556
expansions: [(0, 5), (107, 1), (140, 1), (185, 1), (213, 3), (227, 1), (230, 4), (231, 3), (232, 1), (235, 1), (240, 2), (242, 1), (243, 1), (248, 1), (251, 1), (253, 2), (254, 5), (255, 3), (256, 2), (272, 1), (273, 1), (274, 2), (275, 1), (276, 1), (277, 1), (293, 2), (294, 2), (295, 4), (296, 2), (297, 3), (298, 3), (300, 6), (301, 2), (303, 1), (308, 1), (309, 1), (311, 4), (312, 1), (313, 1), (322, 1), (323, 2), (324, 2), (330, 1), (344, 2), (346, 1), (347, 1), (399, 12), (400, 2)]
discards: [ 2  3  4  5  6  7  8  9 10 11 12 13 14 34 35 36]
Fitting a model of length 491 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 106s - loss: 952.2521 - loglik: -9.4583e+02 - logprior: -6.4230e+00
Epoch 2/2
24/24 - 115s - loss: 926.0898 - loglik: -9.2698e+02 - logprior: 0.8903
Fitted a model with MAP estimate = -923.4632
expansions: [(0, 6), (479, 1), (481, 1), (483, 3)]
discards: [  1   2   3   4   5   6   7   8 271 272 334 389 390 486 487 488 489 490]
Fitting a model of length 484 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 107s - loss: 938.0065 - loglik: -9.3254e+02 - logprior: -5.4682e+00
Epoch 2/2
24/24 - 115s - loss: 926.1443 - loglik: -9.2761e+02 - logprior: 1.4670
Fitted a model with MAP estimate = -923.2531
expansions: [(0, 6), (384, 1), (408, 1), (480, 1), (481, 2), (482, 1), (483, 3)]
discards: [0 1 2 3 4 5 6 7]
Fitting a model of length 491 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 105s - loss: 933.4912 - loglik: -9.3015e+02 - logprior: -3.3412e+00
Epoch 2/10
24/24 - 102s - loss: 923.7698 - loglik: -9.2582e+02 - logprior: 2.0499
Epoch 3/10
24/24 - 95s - loss: 917.6071 - loglik: -9.2026e+02 - logprior: 2.6544
Epoch 4/10
24/24 - 103s - loss: 918.6060 - loglik: -9.2159e+02 - logprior: 2.9810
Fitted a model with MAP estimate = -917.3990
Time for alignment: 1577.0760
Computed alignments with likelihoods: ['-917.3898', '-916.0682', '-917.3990']
Best model has likelihood: -916.0682
SP score = 0.8253
Training of 3 independent models on file tRNA-synt_2b.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f210fd8e790>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f210f98c250>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f21788d4dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f21788d44c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 136 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 514.3837 - loglik: -5.1094e+02 - logprior: -3.4408e+00
Epoch 2/10
19/19 - 10s - loss: 445.1075 - loglik: -4.4395e+02 - logprior: -1.1541e+00
Epoch 3/10
19/19 - 10s - loss: 416.2847 - loglik: -4.1467e+02 - logprior: -1.6130e+00
Epoch 4/10
19/19 - 10s - loss: 408.3946 - loglik: -4.0673e+02 - logprior: -1.6691e+00
Epoch 5/10
19/19 - 10s - loss: 406.6137 - loglik: -4.0486e+02 - logprior: -1.7550e+00
Epoch 6/10
19/19 - 10s - loss: 403.3457 - loglik: -4.0153e+02 - logprior: -1.8151e+00
Epoch 7/10
19/19 - 10s - loss: 400.1596 - loglik: -3.9829e+02 - logprior: -1.8690e+00
Epoch 8/10
19/19 - 10s - loss: 400.0132 - loglik: -3.9805e+02 - logprior: -1.9617e+00
Epoch 9/10
19/19 - 10s - loss: 399.4706 - loglik: -3.9746e+02 - logprior: -2.0062e+00
Epoch 10/10
19/19 - 11s - loss: 399.5786 - loglik: -3.9753e+02 - logprior: -2.0505e+00
Fitted a model with MAP estimate = -400.0203
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (26, 2), (40, 3), (44, 1), (45, 1), (55, 2), (57, 2), (58, 1), (59, 2), (69, 1), (71, 2), (80, 1), (89, 2), (92, 1), (96, 1), (100, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (109, 1), (118, 1), (119, 1), (122, 1), (126, 2), (133, 2)]
discards: [0]
Fitting a model of length 179 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 19s - loss: 404.7786 - loglik: -4.0048e+02 - logprior: -4.2983e+00
Epoch 2/2
19/19 - 16s - loss: 386.6587 - loglik: -3.8456e+02 - logprior: -2.0982e+00
Fitted a model with MAP estimate = -384.4165
expansions: [(0, 1)]
discards: [  0  13  34  70  73  78  93 114 166 174]
Fitting a model of length 170 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 18s - loss: 387.9618 - loglik: -3.8504e+02 - logprior: -2.9182e+00
Epoch 2/2
19/19 - 16s - loss: 383.3077 - loglik: -3.8215e+02 - logprior: -1.1621e+00
Fitted a model with MAP estimate = -382.4693
expansions: []
discards: []
Fitting a model of length 170 on 11293 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 19s - loss: 384.1557 - loglik: -3.8148e+02 - logprior: -2.6801e+00
Epoch 2/10
20/20 - 16s - loss: 381.9441 - loglik: -3.8094e+02 - logprior: -1.0056e+00
Epoch 3/10
20/20 - 16s - loss: 379.6394 - loglik: -3.7877e+02 - logprior: -8.6474e-01
Epoch 4/10
20/20 - 16s - loss: 378.0572 - loglik: -3.7722e+02 - logprior: -8.3346e-01
Epoch 5/10
20/20 - 16s - loss: 377.4165 - loglik: -3.7658e+02 - logprior: -8.3195e-01
Epoch 6/10
20/20 - 16s - loss: 379.1104 - loglik: -3.7825e+02 - logprior: -8.6117e-01
Fitted a model with MAP estimate = -376.9951
Time for alignment: 339.3433
Fitting a model of length 136 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 514.0197 - loglik: -5.1059e+02 - logprior: -3.4304e+00
Epoch 2/10
19/19 - 11s - loss: 444.3680 - loglik: -4.4325e+02 - logprior: -1.1228e+00
Epoch 3/10
19/19 - 11s - loss: 411.6010 - loglik: -4.0996e+02 - logprior: -1.6390e+00
Epoch 4/10
19/19 - 11s - loss: 405.2417 - loglik: -4.0348e+02 - logprior: -1.7636e+00
Epoch 5/10
19/19 - 11s - loss: 400.9911 - loglik: -3.9914e+02 - logprior: -1.8527e+00
Epoch 6/10
19/19 - 11s - loss: 399.5500 - loglik: -3.9764e+02 - logprior: -1.9123e+00
Epoch 7/10
19/19 - 11s - loss: 399.7748 - loglik: -3.9781e+02 - logprior: -1.9667e+00
Fitted a model with MAP estimate = -399.0867
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (14, 2), (23, 2), (26, 1), (32, 1), (39, 2), (40, 1), (45, 1), (55, 2), (57, 2), (58, 1), (59, 2), (61, 1), (71, 2), (80, 1), (89, 2), (92, 1), (93, 1), (95, 1), (99, 1), (103, 1), (104, 1), (105, 1), (106, 1), (109, 1), (118, 1), (122, 1), (126, 2), (133, 2)]
discards: [0]
Fitting a model of length 179 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 19s - loss: 405.0822 - loglik: -4.0085e+02 - logprior: -4.2297e+00
Epoch 2/2
19/19 - 16s - loss: 386.8899 - loglik: -3.8482e+02 - logprior: -2.0737e+00
Fitted a model with MAP estimate = -383.3968
expansions: [(0, 1)]
discards: [  0  13  19  31  71  74  79  94 115 166 174]
Fitting a model of length 169 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 18s - loss: 386.8547 - loglik: -3.8398e+02 - logprior: -2.8773e+00
Epoch 2/2
19/19 - 14s - loss: 382.0109 - loglik: -3.8087e+02 - logprior: -1.1390e+00
Fitted a model with MAP estimate = -381.4756
expansions: []
discards: []
Fitting a model of length 169 on 11293 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 18s - loss: 382.6298 - loglik: -3.7998e+02 - logprior: -2.6527e+00
Epoch 2/10
20/20 - 15s - loss: 381.4873 - loglik: -3.8050e+02 - logprior: -9.8681e-01
Epoch 3/10
20/20 - 16s - loss: 378.2350 - loglik: -3.7740e+02 - logprior: -8.3183e-01
Epoch 4/10
20/20 - 16s - loss: 378.2185 - loglik: -3.7742e+02 - logprior: -7.9728e-01
Epoch 5/10
20/20 - 16s - loss: 376.5591 - loglik: -3.7575e+02 - logprior: -8.0959e-01
Epoch 6/10
20/20 - 16s - loss: 377.4390 - loglik: -3.7661e+02 - logprior: -8.3056e-01
Fitted a model with MAP estimate = -375.7087
Time for alignment: 305.3521
Fitting a model of length 136 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 514.0116 - loglik: -5.1058e+02 - logprior: -3.4327e+00
Epoch 2/10
19/19 - 10s - loss: 442.5721 - loglik: -4.4142e+02 - logprior: -1.1543e+00
Epoch 3/10
19/19 - 11s - loss: 413.5110 - loglik: -4.1188e+02 - logprior: -1.6300e+00
Epoch 4/10
19/19 - 11s - loss: 406.8024 - loglik: -4.0507e+02 - logprior: -1.7302e+00
Epoch 5/10
19/19 - 11s - loss: 403.4012 - loglik: -4.0162e+02 - logprior: -1.7803e+00
Epoch 6/10
19/19 - 11s - loss: 402.4267 - loglik: -4.0065e+02 - logprior: -1.7753e+00
Epoch 7/10
19/19 - 11s - loss: 402.1072 - loglik: -4.0029e+02 - logprior: -1.8210e+00
Epoch 8/10
19/19 - 11s - loss: 402.0710 - loglik: -4.0018e+02 - logprior: -1.8945e+00
Epoch 9/10
19/19 - 11s - loss: 400.7202 - loglik: -3.9875e+02 - logprior: -1.9672e+00
Epoch 10/10
19/19 - 10s - loss: 401.4636 - loglik: -3.9945e+02 - logprior: -2.0148e+00
Fitted a model with MAP estimate = -401.3746
expansions: [(9, 2), (10, 2), (11, 2), (12, 2), (22, 1), (23, 1), (27, 1), (40, 3), (41, 1), (44, 1), (45, 1), (55, 2), (57, 2), (58, 1), (59, 2), (69, 1), (71, 2), (80, 1), (89, 2), (92, 1), (93, 2), (98, 1), (99, 2), (103, 2), (104, 1), (105, 1), (106, 1), (109, 1), (118, 1), (119, 1), (122, 1), (126, 1), (128, 1), (133, 2)]
discards: [0]
Fitting a model of length 184 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 18s - loss: 407.3575 - loglik: -4.0304e+02 - logprior: -4.3189e+00
Epoch 2/2
19/19 - 15s - loss: 387.7138 - loglik: -3.8562e+02 - logprior: -2.0893e+00
Fitted a model with MAP estimate = -385.3566
expansions: [(0, 1)]
discards: [  0  12  15  52  72  75  80  95 116 123 132 161 179]
Fitting a model of length 172 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 18s - loss: 388.5887 - loglik: -3.8568e+02 - logprior: -2.9130e+00
Epoch 2/2
19/19 - 14s - loss: 383.6261 - loglik: -3.8243e+02 - logprior: -1.1943e+00
Fitted a model with MAP estimate = -382.6020
expansions: []
discards: []
Fitting a model of length 172 on 11293 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 19s - loss: 384.8145 - loglik: -3.8209e+02 - logprior: -2.7213e+00
Epoch 2/10
20/20 - 16s - loss: 380.8355 - loglik: -3.7981e+02 - logprior: -1.0210e+00
Epoch 3/10
20/20 - 15s - loss: 380.5957 - loglik: -3.7977e+02 - logprior: -8.2473e-01
Epoch 4/10
20/20 - 14s - loss: 378.0617 - loglik: -3.7727e+02 - logprior: -7.8805e-01
Epoch 5/10
20/20 - 14s - loss: 376.8889 - loglik: -3.7610e+02 - logprior: -7.9368e-01
Epoch 6/10
20/20 - 14s - loss: 377.0365 - loglik: -3.7623e+02 - logprior: -8.0593e-01
Fitted a model with MAP estimate = -376.4269
Time for alignment: 331.1586
Computed alignments with likelihoods: ['-376.9951', '-375.7087', '-376.4269']
Best model has likelihood: -375.7087
SP score = 0.2969
Training of 3 independent models on file ChtBD.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f21209ba070>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f21794a9a60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20a9bb0af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20a988fd30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 173.3906 - loglik: -1.1166e+02 - logprior: -6.1726e+01
Epoch 2/10
10/10 - 1s - loss: 105.9049 - loglik: -8.8705e+01 - logprior: -1.7200e+01
Epoch 3/10
10/10 - 1s - loss: 80.8564 - loglik: -7.2381e+01 - logprior: -8.4751e+00
Epoch 4/10
10/10 - 1s - loss: 69.9738 - loglik: -6.4659e+01 - logprior: -5.3146e+00
Epoch 5/10
10/10 - 1s - loss: 65.7765 - loglik: -6.2058e+01 - logprior: -3.7187e+00
Epoch 6/10
10/10 - 1s - loss: 64.0723 - loglik: -6.1176e+01 - logprior: -2.8958e+00
Epoch 7/10
10/10 - 1s - loss: 63.4590 - loglik: -6.1032e+01 - logprior: -2.4269e+00
Epoch 8/10
10/10 - 1s - loss: 63.1634 - loglik: -6.1049e+01 - logprior: -2.1141e+00
Epoch 9/10
10/10 - 1s - loss: 62.9167 - loglik: -6.1014e+01 - logprior: -1.9029e+00
Epoch 10/10
10/10 - 1s - loss: 62.5730 - loglik: -6.0792e+01 - logprior: -1.7810e+00
Fitted a model with MAP estimate = -62.5011
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 138.6636 - loglik: -5.8840e+01 - logprior: -7.9823e+01
Epoch 2/2
10/10 - 1s - loss: 79.8060 - loglik: -5.4052e+01 - logprior: -2.5754e+01
Fitted a model with MAP estimate = -68.7358
expansions: [(0, 2)]
discards: [14 32]
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 117.5458 - loglik: -5.1648e+01 - logprior: -6.5898e+01
Epoch 2/2
10/10 - 1s - loss: 74.0855 - loglik: -5.0985e+01 - logprior: -2.3101e+01
Fitted a model with MAP estimate = -65.1066
expansions: []
discards: [0]
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 117.4156 - loglik: -5.1684e+01 - logprior: -6.5732e+01
Epoch 2/10
10/10 - 1s - loss: 79.2204 - loglik: -5.1951e+01 - logprior: -2.7270e+01
Epoch 3/10
10/10 - 1s - loss: 68.2502 - loglik: -5.2280e+01 - logprior: -1.5970e+01
Epoch 4/10
10/10 - 1s - loss: 58.9496 - loglik: -5.2557e+01 - logprior: -6.3930e+00
Epoch 5/10
10/10 - 1s - loss: 55.2995 - loglik: -5.2583e+01 - logprior: -2.7162e+00
Epoch 6/10
10/10 - 1s - loss: 54.2494 - loglik: -5.2722e+01 - logprior: -1.5274e+00
Epoch 7/10
10/10 - 1s - loss: 53.6376 - loglik: -5.2696e+01 - logprior: -9.4131e-01
Epoch 8/10
10/10 - 1s - loss: 53.4028 - loglik: -5.2801e+01 - logprior: -6.0179e-01
Epoch 9/10
10/10 - 1s - loss: 53.3445 - loglik: -5.2996e+01 - logprior: -3.4803e-01
Epoch 10/10
10/10 - 1s - loss: 53.0942 - loglik: -5.2963e+01 - logprior: -1.3150e-01
Fitted a model with MAP estimate = -53.0381
Time for alignment: 31.1326
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 173.3737 - loglik: -1.1165e+02 - logprior: -6.1726e+01
Epoch 2/10
10/10 - 1s - loss: 106.0298 - loglik: -8.8830e+01 - logprior: -1.7200e+01
Epoch 3/10
10/10 - 1s - loss: 80.8258 - loglik: -7.2352e+01 - logprior: -8.4735e+00
Epoch 4/10
10/10 - 1s - loss: 69.9666 - loglik: -6.4656e+01 - logprior: -5.3102e+00
Epoch 5/10
10/10 - 1s - loss: 65.8648 - loglik: -6.2148e+01 - logprior: -3.7168e+00
Epoch 6/10
10/10 - 1s - loss: 64.0825 - loglik: -6.1188e+01 - logprior: -2.8949e+00
Epoch 7/10
10/10 - 1s - loss: 63.5624 - loglik: -6.1135e+01 - logprior: -2.4274e+00
Epoch 8/10
10/10 - 1s - loss: 63.0606 - loglik: -6.0944e+01 - logprior: -2.1162e+00
Epoch 9/10
10/10 - 1s - loss: 62.8108 - loglik: -6.0908e+01 - logprior: -1.9026e+00
Epoch 10/10
10/10 - 1s - loss: 62.7101 - loglik: -6.0927e+01 - logprior: -1.7828e+00
Fitted a model with MAP estimate = -62.5028
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 138.5877 - loglik: -5.8762e+01 - logprior: -7.9826e+01
Epoch 2/2
10/10 - 1s - loss: 79.8790 - loglik: -5.4122e+01 - logprior: -2.5757e+01
Fitted a model with MAP estimate = -68.7275
expansions: [(0, 2)]
discards: [14 32]
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 117.5076 - loglik: -5.1609e+01 - logprior: -6.5898e+01
Epoch 2/2
10/10 - 1s - loss: 74.0283 - loglik: -5.0927e+01 - logprior: -2.3101e+01
Fitted a model with MAP estimate = -65.1033
expansions: []
discards: [0]
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 117.4844 - loglik: -5.1754e+01 - logprior: -6.5731e+01
Epoch 2/10
10/10 - 1s - loss: 79.1883 - loglik: -5.1921e+01 - logprior: -2.7267e+01
Epoch 3/10
10/10 - 1s - loss: 68.1836 - loglik: -5.2215e+01 - logprior: -1.5968e+01
Epoch 4/10
10/10 - 1s - loss: 58.8185 - loglik: -5.2426e+01 - logprior: -6.3928e+00
Epoch 5/10
10/10 - 1s - loss: 55.4039 - loglik: -5.2692e+01 - logprior: -2.7122e+00
Epoch 6/10
10/10 - 1s - loss: 54.2865 - loglik: -5.2761e+01 - logprior: -1.5252e+00
Epoch 7/10
10/10 - 1s - loss: 53.6563 - loglik: -5.2718e+01 - logprior: -9.3844e-01
Epoch 8/10
10/10 - 1s - loss: 53.4318 - loglik: -5.2837e+01 - logprior: -5.9451e-01
Epoch 9/10
10/10 - 1s - loss: 53.1286 - loglik: -5.2785e+01 - logprior: -3.4335e-01
Epoch 10/10
10/10 - 1s - loss: 53.2882 - loglik: -5.3164e+01 - logprior: -1.2415e-01
Fitted a model with MAP estimate = -53.0344
Time for alignment: 30.6097
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 173.4218 - loglik: -1.1170e+02 - logprior: -6.1727e+01
Epoch 2/10
10/10 - 1s - loss: 105.9044 - loglik: -8.8706e+01 - logprior: -1.7198e+01
Epoch 3/10
10/10 - 1s - loss: 81.0128 - loglik: -7.2530e+01 - logprior: -8.4832e+00
Epoch 4/10
10/10 - 1s - loss: 69.9253 - loglik: -6.4578e+01 - logprior: -5.3472e+00
Epoch 5/10
10/10 - 1s - loss: 65.7163 - loglik: -6.1988e+01 - logprior: -3.7279e+00
Epoch 6/10
10/10 - 1s - loss: 64.1760 - loglik: -6.1265e+01 - logprior: -2.9108e+00
Epoch 7/10
10/10 - 1s - loss: 63.5131 - loglik: -6.1085e+01 - logprior: -2.4283e+00
Epoch 8/10
10/10 - 1s - loss: 63.0659 - loglik: -6.0950e+01 - logprior: -2.1159e+00
Epoch 9/10
10/10 - 1s - loss: 62.8394 - loglik: -6.0930e+01 - logprior: -1.9097e+00
Epoch 10/10
10/10 - 1s - loss: 62.6085 - loglik: -6.0811e+01 - logprior: -1.7971e+00
Fitted a model with MAP estimate = -62.4989
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 138.6539 - loglik: -5.8829e+01 - logprior: -7.9825e+01
Epoch 2/2
10/10 - 1s - loss: 79.6687 - loglik: -5.3918e+01 - logprior: -2.5750e+01
Fitted a model with MAP estimate = -68.7140
expansions: [(0, 2)]
discards: [14 32]
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 117.5169 - loglik: -5.1616e+01 - logprior: -6.5901e+01
Epoch 2/2
10/10 - 1s - loss: 74.0611 - loglik: -5.0960e+01 - logprior: -2.3101e+01
Fitted a model with MAP estimate = -65.0774
expansions: []
discards: [0]
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 117.2984 - loglik: -5.1571e+01 - logprior: -6.5728e+01
Epoch 2/10
10/10 - 1s - loss: 79.2613 - loglik: -5.1999e+01 - logprior: -2.7262e+01
Epoch 3/10
10/10 - 1s - loss: 68.2760 - loglik: -5.2303e+01 - logprior: -1.5973e+01
Epoch 4/10
10/10 - 1s - loss: 58.9865 - loglik: -5.2596e+01 - logprior: -6.3909e+00
Epoch 5/10
10/10 - 1s - loss: 55.3002 - loglik: -5.2586e+01 - logprior: -2.7143e+00
Epoch 6/10
10/10 - 1s - loss: 54.1841 - loglik: -5.2657e+01 - logprior: -1.5274e+00
Epoch 7/10
10/10 - 1s - loss: 53.7164 - loglik: -5.2775e+01 - logprior: -9.4171e-01
Epoch 8/10
10/10 - 1s - loss: 53.4799 - loglik: -5.2882e+01 - logprior: -5.9762e-01
Epoch 9/10
10/10 - 1s - loss: 53.2631 - loglik: -5.2919e+01 - logprior: -3.4387e-01
Epoch 10/10
10/10 - 1s - loss: 53.1582 - loglik: -5.3029e+01 - logprior: -1.2868e-01
Fitted a model with MAP estimate = -53.0345
Time for alignment: 31.7056
Computed alignments with likelihoods: ['-53.0381', '-53.0344', '-53.0345']
Best model has likelihood: -53.0344
SP score = 0.9203
Training of 3 independent models on file adh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20e54dfb20>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20f596eee0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f212949be80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20dc8a7f10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 8s - loss: 372.9352 - loglik: -3.6972e+02 - logprior: -3.2116e+00
Epoch 2/10
20/20 - 5s - loss: 329.4762 - loglik: -3.2818e+02 - logprior: -1.2966e+00
Epoch 3/10
20/20 - 5s - loss: 310.5984 - loglik: -3.0908e+02 - logprior: -1.5139e+00
Epoch 4/10
20/20 - 5s - loss: 305.8246 - loglik: -3.0439e+02 - logprior: -1.4300e+00
Epoch 5/10
20/20 - 4s - loss: 303.9764 - loglik: -3.0251e+02 - logprior: -1.4644e+00
Epoch 6/10
20/20 - 5s - loss: 303.5543 - loglik: -3.0210e+02 - logprior: -1.4549e+00
Epoch 7/10
20/20 - 5s - loss: 302.6509 - loglik: -3.0115e+02 - logprior: -1.4978e+00
Epoch 8/10
20/20 - 5s - loss: 302.7607 - loglik: -3.0120e+02 - logprior: -1.5650e+00
Fitted a model with MAP estimate = -291.1194
expansions: [(5, 1), (8, 2), (10, 2), (12, 2), (18, 1), (20, 1), (31, 1), (35, 1), (36, 2), (38, 1), (47, 1), (50, 1), (57, 1), (58, 1), (59, 1), (60, 2), (61, 2), (64, 2), (76, 1), (78, 2), (81, 2), (87, 1), (91, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Fitting a model of length 138 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 10s - loss: 309.3474 - loglik: -3.0546e+02 - logprior: -3.8860e+00
Epoch 2/2
20/20 - 6s - loss: 295.2816 - loglik: -2.9355e+02 - logprior: -1.7364e+00
Fitted a model with MAP estimate = -279.8740
expansions: [(0, 1), (132, 1)]
discards: [  0   8  13  78  81  87 103]
Fitting a model of length 133 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 11s - loss: 295.3814 - loglik: -2.9263e+02 - logprior: -2.7502e+00
Epoch 2/2
20/20 - 6s - loss: 290.7901 - loglik: -2.8981e+02 - logprior: -9.7708e-01
Fitted a model with MAP estimate = -277.5989
expansions: []
discards: [ 44 119]
Fitting a model of length 131 on 21331 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
28/28 - 11s - loss: 275.4937 - loglik: -2.7408e+02 - logprior: -1.4186e+00
Epoch 2/10
28/28 - 7s - loss: 272.7974 - loglik: -2.7194e+02 - logprior: -8.5964e-01
Epoch 3/10
28/28 - 8s - loss: 271.9214 - loglik: -2.7109e+02 - logprior: -8.2841e-01
Epoch 4/10
28/28 - 8s - loss: 270.3658 - loglik: -2.6954e+02 - logprior: -8.2807e-01
Epoch 5/10
28/28 - 8s - loss: 270.8940 - loglik: -2.7007e+02 - logprior: -8.2354e-01
Fitted a model with MAP estimate = -269.4849
Time for alignment: 160.7097
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 8s - loss: 373.2549 - loglik: -3.7005e+02 - logprior: -3.2096e+00
Epoch 2/10
20/20 - 5s - loss: 327.6862 - loglik: -3.2640e+02 - logprior: -1.2905e+00
Epoch 3/10
20/20 - 5s - loss: 309.7907 - loglik: -3.0828e+02 - logprior: -1.5093e+00
Epoch 4/10
20/20 - 4s - loss: 304.9481 - loglik: -3.0349e+02 - logprior: -1.4610e+00
Epoch 5/10
20/20 - 6s - loss: 303.5049 - loglik: -3.0203e+02 - logprior: -1.4702e+00
Epoch 6/10
20/20 - 5s - loss: 302.9986 - loglik: -3.0154e+02 - logprior: -1.4616e+00
Epoch 7/10
20/20 - 6s - loss: 302.4472 - loglik: -3.0095e+02 - logprior: -1.4941e+00
Epoch 8/10
20/20 - 5s - loss: 302.2066 - loglik: -3.0063e+02 - logprior: -1.5772e+00
Epoch 9/10
20/20 - 5s - loss: 301.7721 - loglik: -3.0009e+02 - logprior: -1.6818e+00
Epoch 10/10
20/20 - 5s - loss: 301.0986 - loglik: -2.9932e+02 - logprior: -1.7806e+00
Fitted a model with MAP estimate = -291.4747
expansions: [(5, 1), (8, 2), (10, 1), (13, 2), (18, 1), (20, 1), (23, 2), (35, 1), (36, 2), (40, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 1), (64, 2), (76, 1), (78, 2), (79, 1), (81, 2), (85, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Fitting a model of length 138 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 10s - loss: 309.4268 - loglik: -3.0547e+02 - logprior: -3.9537e+00
Epoch 2/2
20/20 - 7s - loss: 295.1535 - loglik: -2.9339e+02 - logprior: -1.7615e+00
Fitted a model with MAP estimate = -280.1823
expansions: [(0, 1)]
discards: [  0   8  30  74  86 102 108]
Fitting a model of length 132 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 10s - loss: 295.9825 - loglik: -2.9322e+02 - logprior: -2.7673e+00
Epoch 2/2
20/20 - 7s - loss: 291.7018 - loglik: -2.9072e+02 - logprior: -9.8229e-01
Fitted a model with MAP estimate = -278.2567
expansions: [(126, 1)]
discards: [ 44 119]
Fitting a model of length 131 on 21331 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
28/28 - 13s - loss: 275.4220 - loglik: -2.7403e+02 - logprior: -1.3950e+00
Epoch 2/10
28/28 - 8s - loss: 273.1729 - loglik: -2.7237e+02 - logprior: -8.0579e-01
Epoch 3/10
28/28 - 9s - loss: 271.9388 - loglik: -2.7119e+02 - logprior: -7.4909e-01
Epoch 4/10
28/28 - 10s - loss: 271.4527 - loglik: -2.7070e+02 - logprior: -7.5683e-01
Epoch 5/10
28/28 - 8s - loss: 270.0696 - loglik: -2.6931e+02 - logprior: -7.5684e-01
Epoch 6/10
28/28 - 10s - loss: 269.6022 - loglik: -2.6882e+02 - logprior: -7.8540e-01
Epoch 7/10
28/28 - 8s - loss: 268.7224 - loglik: -2.6789e+02 - logprior: -8.3512e-01
Epoch 8/10
28/28 - 10s - loss: 267.6089 - loglik: -2.6671e+02 - logprior: -8.9727e-01
Epoch 9/10
28/28 - 8s - loss: 267.7914 - loglik: -2.6684e+02 - logprior: -9.5473e-01
Fitted a model with MAP estimate = -266.9178
Time for alignment: 218.3894
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 8s - loss: 373.3156 - loglik: -3.7010e+02 - logprior: -3.2113e+00
Epoch 2/10
20/20 - 5s - loss: 329.8206 - loglik: -3.2852e+02 - logprior: -1.2977e+00
Epoch 3/10
20/20 - 5s - loss: 310.3483 - loglik: -3.0885e+02 - logprior: -1.4973e+00
Epoch 4/10
20/20 - 5s - loss: 305.5475 - loglik: -3.0412e+02 - logprior: -1.4311e+00
Epoch 5/10
20/20 - 5s - loss: 303.8094 - loglik: -3.0236e+02 - logprior: -1.4469e+00
Epoch 6/10
20/20 - 5s - loss: 303.3006 - loglik: -3.0186e+02 - logprior: -1.4432e+00
Epoch 7/10
20/20 - 5s - loss: 302.8220 - loglik: -3.0135e+02 - logprior: -1.4694e+00
Epoch 8/10
20/20 - 5s - loss: 302.5482 - loglik: -3.0100e+02 - logprior: -1.5483e+00
Epoch 9/10
20/20 - 6s - loss: 301.7838 - loglik: -3.0016e+02 - logprior: -1.6267e+00
Epoch 10/10
20/20 - 5s - loss: 301.8242 - loglik: -3.0009e+02 - logprior: -1.7322e+00
Fitted a model with MAP estimate = -291.4910
expansions: [(5, 1), (8, 2), (10, 1), (13, 2), (19, 1), (20, 1), (26, 2), (35, 1), (36, 2), (38, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 2), (61, 1), (62, 1), (76, 1), (78, 2), (79, 2), (81, 1), (87, 1), (91, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Fitting a model of length 138 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 10s - loss: 308.9711 - loglik: -3.0502e+02 - logprior: -3.9502e+00
Epoch 2/2
20/20 - 7s - loss: 295.3958 - loglik: -2.9363e+02 - logprior: -1.7662e+00
Fitted a model with MAP estimate = -279.9228
expansions: [(0, 1), (132, 1)]
discards: [  0   8  34  46  74  79 102 105]
Fitting a model of length 132 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 10s - loss: 295.7365 - loglik: -2.9300e+02 - logprior: -2.7357e+00
Epoch 2/2
20/20 - 7s - loss: 291.0621 - loglik: -2.9008e+02 - logprior: -9.7767e-01
Fitted a model with MAP estimate = -277.7675
expansions: []
discards: []
Fitting a model of length 132 on 21331 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
28/28 - 12s - loss: 275.4659 - loglik: -2.7406e+02 - logprior: -1.4104e+00
Epoch 2/10
28/28 - 8s - loss: 272.9999 - loglik: -2.7215e+02 - logprior: -8.5064e-01
Epoch 3/10
28/28 - 8s - loss: 271.5375 - loglik: -2.7072e+02 - logprior: -8.1949e-01
Epoch 4/10
28/28 - 8s - loss: 271.0884 - loglik: -2.7026e+02 - logprior: -8.2765e-01
Epoch 5/10
28/28 - 9s - loss: 270.1784 - loglik: -2.6936e+02 - logprior: -8.2131e-01
Epoch 6/10
28/28 - 8s - loss: 269.3742 - loglik: -2.6853e+02 - logprior: -8.4413e-01
Epoch 7/10
28/28 - 8s - loss: 268.6996 - loglik: -2.6780e+02 - logprior: -8.9522e-01
Epoch 8/10
28/28 - 8s - loss: 267.9529 - loglik: -2.6700e+02 - logprior: -9.4915e-01
Epoch 9/10
28/28 - 8s - loss: 267.5460 - loglik: -2.6654e+02 - logprior: -1.0109e+00
Epoch 10/10
28/28 - 8s - loss: 266.5793 - loglik: -2.6549e+02 - logprior: -1.0886e+00
Fitted a model with MAP estimate = -266.4653
Time for alignment: 225.2777
Computed alignments with likelihoods: ['-269.4849', '-266.9178', '-266.4653']
Best model has likelihood: -266.4653
SP score = 0.5337
Training of 3 independent models on file Sulfotransfer.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20fe947a00>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20cb1dfbb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20f6320250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20f6320220>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 198 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 14s - loss: 710.9340 - loglik: -6.8987e+02 - logprior: -2.1061e+01
Epoch 2/10
10/10 - 10s - loss: 666.3517 - loglik: -6.6256e+02 - logprior: -3.7904e+00
Epoch 3/10
10/10 - 11s - loss: 632.0801 - loglik: -6.3104e+02 - logprior: -1.0376e+00
Epoch 4/10
10/10 - 11s - loss: 603.5189 - loglik: -6.0254e+02 - logprior: -9.7575e-01
Epoch 5/10
10/10 - 11s - loss: 592.2399 - loglik: -5.9131e+02 - logprior: -9.3340e-01
Epoch 6/10
10/10 - 12s - loss: 583.9242 - loglik: -5.8308e+02 - logprior: -8.4312e-01
Epoch 7/10
10/10 - 11s - loss: 584.3719 - loglik: -5.8357e+02 - logprior: -8.0665e-01
Fitted a model with MAP estimate = -582.1924
expansions: [(25, 1), (28, 2), (30, 2), (64, 2), (88, 1), (90, 2), (105, 1), (118, 2), (120, 1), (135, 1), (136, 1), (137, 1), (146, 3), (148, 2), (153, 1), (163, 2), (167, 4), (182, 1), (184, 2), (185, 2)]
discards: [170 171]
Fitting a model of length 230 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 18s - loss: 611.0828 - loglik: -5.9539e+02 - logprior: -1.5693e+01
Epoch 2/2
10/10 - 14s - loss: 587.6166 - loglik: -5.8432e+02 - logprior: -3.2960e+00
Fitted a model with MAP estimate = -583.5624
expansions: []
discards: [ 33  34 130 165 168 185 196 197 199 200 201 202 203 204 205 213]
Fitting a model of length 214 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 16s - loss: 602.3702 - loglik: -5.8788e+02 - logprior: -1.4494e+01
Epoch 2/2
10/10 - 13s - loss: 588.6173 - loglik: -5.8571e+02 - logprior: -2.9070e+00
Fitted a model with MAP estimate = -584.9181
expansions: [(187, 1), (200, 1)]
discards: [68]
Fitting a model of length 215 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 16s - loss: 598.6964 - loglik: -5.8435e+02 - logprior: -1.4348e+01
Epoch 2/10
10/10 - 13s - loss: 585.7579 - loglik: -5.8307e+02 - logprior: -2.6896e+00
Epoch 3/10
10/10 - 13s - loss: 581.9000 - loglik: -5.8152e+02 - logprior: -3.8223e-01
Epoch 4/10
10/10 - 13s - loss: 578.3759 - loglik: -5.7882e+02 - logprior: 0.4433
Epoch 5/10
10/10 - 13s - loss: 578.6199 - loglik: -5.7933e+02 - logprior: 0.7141
Fitted a model with MAP estimate = -576.4244
Time for alignment: 241.7264
Fitting a model of length 198 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 13s - loss: 710.1898 - loglik: -6.8913e+02 - logprior: -2.1056e+01
Epoch 2/10
10/10 - 11s - loss: 667.1309 - loglik: -6.6334e+02 - logprior: -3.7923e+00
Epoch 3/10
10/10 - 10s - loss: 632.3488 - loglik: -6.3140e+02 - logprior: -9.5240e-01
Epoch 4/10
10/10 - 10s - loss: 602.7043 - loglik: -6.0194e+02 - logprior: -7.6694e-01
Epoch 5/10
10/10 - 10s - loss: 591.0187 - loglik: -5.9027e+02 - logprior: -7.4633e-01
Epoch 6/10
10/10 - 9s - loss: 586.0591 - loglik: -5.8544e+02 - logprior: -6.1499e-01
Epoch 7/10
10/10 - 10s - loss: 584.4747 - loglik: -5.8388e+02 - logprior: -5.9214e-01
Epoch 8/10
10/10 - 10s - loss: 581.6974 - loglik: -5.8107e+02 - logprior: -6.2912e-01
Epoch 9/10
10/10 - 10s - loss: 580.2744 - loglik: -5.7965e+02 - logprior: -6.2489e-01
Epoch 10/10
10/10 - 10s - loss: 579.0614 - loglik: -5.7835e+02 - logprior: -7.1606e-01
Fitted a model with MAP estimate = -579.2518
expansions: [(25, 1), (26, 1), (28, 2), (30, 2), (64, 1), (88, 1), (90, 3), (103, 1), (104, 1), (117, 2), (119, 1), (136, 1), (148, 2), (150, 3), (152, 1), (162, 1), (167, 3), (182, 1), (183, 1), (184, 2), (185, 1)]
discards: []
Fitting a model of length 230 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 16s - loss: 613.1331 - loglik: -5.9728e+02 - logprior: -1.5854e+01
Epoch 2/2
10/10 - 12s - loss: 588.3060 - loglik: -5.8490e+02 - logprior: -3.4106e+00
Fitted a model with MAP estimate = -583.2266
expansions: [(113, 5), (169, 1)]
discards: [ 27  34  35 131 166 176 177 186 191 192 197 198 199 200]
Fitting a model of length 222 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 16s - loss: 601.5154 - loglik: -5.8700e+02 - logprior: -1.4518e+01
Epoch 2/2
10/10 - 13s - loss: 585.3424 - loglik: -5.8264e+02 - logprior: -2.6982e+00
Fitted a model with MAP estimate = -582.2895
expansions: [(177, 1), (185, 1), (188, 3), (207, 1)]
discards: [112 193]
Fitting a model of length 226 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 16s - loss: 596.6365 - loglik: -5.8254e+02 - logprior: -1.4093e+01
Epoch 2/10
10/10 - 12s - loss: 583.4639 - loglik: -5.8112e+02 - logprior: -2.3463e+00
Epoch 3/10
10/10 - 12s - loss: 578.7176 - loglik: -5.7866e+02 - logprior: -5.6471e-02
Epoch 4/10
10/10 - 11s - loss: 574.2332 - loglik: -5.7498e+02 - logprior: 0.7517
Epoch 5/10
10/10 - 11s - loss: 574.9810 - loglik: -5.7605e+02 - logprior: 1.0645
Fitted a model with MAP estimate = -572.6121
Time for alignment: 252.3010
Fitting a model of length 198 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 12s - loss: 710.5325 - loglik: -6.8947e+02 - logprior: -2.1062e+01
Epoch 2/10
10/10 - 9s - loss: 667.4557 - loglik: -6.6367e+02 - logprior: -3.7889e+00
Epoch 3/10
10/10 - 9s - loss: 630.1658 - loglik: -6.2912e+02 - logprior: -1.0431e+00
Epoch 4/10
10/10 - 9s - loss: 602.8193 - loglik: -6.0187e+02 - logprior: -9.4439e-01
Epoch 5/10
10/10 - 9s - loss: 591.8546 - loglik: -5.9096e+02 - logprior: -8.9029e-01
Epoch 6/10
10/10 - 10s - loss: 585.9762 - loglik: -5.8509e+02 - logprior: -8.8816e-01
Epoch 7/10
10/10 - 10s - loss: 584.6085 - loglik: -5.8376e+02 - logprior: -8.4521e-01
Epoch 8/10
10/10 - 10s - loss: 584.7698 - loglik: -5.8396e+02 - logprior: -8.0582e-01
Fitted a model with MAP estimate = -582.3593
expansions: [(25, 1), (26, 2), (27, 2), (29, 2), (44, 1), (64, 2), (65, 2), (90, 3), (104, 1), (117, 2), (119, 2), (133, 1), (134, 1), (145, 1), (148, 1), (150, 1), (166, 3), (182, 1), (183, 1), (184, 2)]
discards: [170 171 172 173 174 175]
Fitting a model of length 224 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 16s - loss: 614.0853 - loglik: -5.9833e+02 - logprior: -1.5755e+01
Epoch 2/2
10/10 - 13s - loss: 589.4409 - loglik: -5.8608e+02 - logprior: -3.3571e+00
Fitted a model with MAP estimate = -585.2810
expansions: [(50, 3), (195, 5), (210, 1), (211, 1)]
discards: [ 34  35  75  76 134 179 180]
Fitting a model of length 227 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 15s - loss: 600.1307 - loglik: -5.8570e+02 - logprior: -1.4434e+01
Epoch 2/2
10/10 - 12s - loss: 584.0686 - loglik: -5.8157e+02 - logprior: -2.4976e+00
Fitted a model with MAP estimate = -581.2415
expansions: [(50, 1)]
discards: [195 196 197]
Fitting a model of length 225 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 14s - loss: 597.5685 - loglik: -5.8360e+02 - logprior: -1.3973e+01
Epoch 2/10
10/10 - 11s - loss: 584.6798 - loglik: -5.8236e+02 - logprior: -2.3228e+00
Epoch 3/10
10/10 - 11s - loss: 581.4736 - loglik: -5.8146e+02 - logprior: -1.2574e-02
Epoch 4/10
10/10 - 11s - loss: 576.7605 - loglik: -5.7757e+02 - logprior: 0.8046
Epoch 5/10
10/10 - 11s - loss: 575.1168 - loglik: -5.7625e+02 - logprior: 1.1285
Epoch 6/10
10/10 - 11s - loss: 572.9816 - loglik: -5.7433e+02 - logprior: 1.3472
Epoch 7/10
10/10 - 11s - loss: 572.5253 - loglik: -5.7405e+02 - logprior: 1.5271
Epoch 8/10
10/10 - 11s - loss: 570.2633 - loglik: -5.7182e+02 - logprior: 1.5569
Epoch 9/10
10/10 - 12s - loss: 570.3602 - loglik: -5.7194e+02 - logprior: 1.5838
Fitted a model with MAP estimate = -569.5476
Time for alignment: 268.4361
Computed alignments with likelihoods: ['-576.4244', '-572.6121', '-569.5476']
Best model has likelihood: -569.5476
SP score = 0.7223
Training of 3 independent models on file hom.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f202ee68eb0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20d3b0b820>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20cb5429a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20cb5bfa30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 154.4239 - loglik: -1.5099e+02 - logprior: -3.4290e+00
Epoch 2/10
19/19 - 1s - loss: 121.1925 - loglik: -1.1966e+02 - logprior: -1.5342e+00
Epoch 3/10
19/19 - 1s - loss: 107.6671 - loglik: -1.0610e+02 - logprior: -1.5702e+00
Epoch 4/10
19/19 - 1s - loss: 103.8971 - loglik: -1.0223e+02 - logprior: -1.6636e+00
Epoch 5/10
19/19 - 1s - loss: 102.7380 - loglik: -1.0118e+02 - logprior: -1.5591e+00
Epoch 6/10
19/19 - 1s - loss: 102.4019 - loglik: -1.0083e+02 - logprior: -1.5732e+00
Epoch 7/10
19/19 - 1s - loss: 102.0852 - loglik: -1.0050e+02 - logprior: -1.5816e+00
Epoch 8/10
19/19 - 1s - loss: 102.3528 - loglik: -1.0075e+02 - logprior: -1.6045e+00
Fitted a model with MAP estimate = -97.8972
expansions: [(3, 1), (5, 1), (6, 1), (7, 1), (9, 1), (13, 1), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: [0]
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 106.1315 - loglik: -1.0190e+02 - logprior: -4.2357e+00
Epoch 2/2
19/19 - 1s - loss: 96.7452 - loglik: -9.4467e+01 - logprior: -2.2781e+00
Fitted a model with MAP estimate = -91.1938
expansions: [(3, 1)]
discards: [ 0 35 38]
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 99.7769 - loglik: -9.5880e+01 - logprior: -3.8973e+00
Epoch 2/2
19/19 - 2s - loss: 95.4593 - loglik: -9.4041e+01 - logprior: -1.4180e+00
Fitted a model with MAP estimate = -90.9495
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 6s - loss: 94.7216 - loglik: -9.1071e+01 - logprior: -3.6509e+00
Epoch 2/10
21/21 - 2s - loss: 90.3629 - loglik: -8.8534e+01 - logprior: -1.8286e+00
Epoch 3/10
21/21 - 2s - loss: 89.8095 - loglik: -8.8469e+01 - logprior: -1.3406e+00
Epoch 4/10
21/21 - 2s - loss: 89.0908 - loglik: -8.7838e+01 - logprior: -1.2530e+00
Epoch 5/10
21/21 - 2s - loss: 88.7514 - loglik: -8.7501e+01 - logprior: -1.2501e+00
Epoch 6/10
21/21 - 2s - loss: 88.2508 - loglik: -8.6987e+01 - logprior: -1.2636e+00
Epoch 7/10
21/21 - 2s - loss: 88.4475 - loglik: -8.7176e+01 - logprior: -1.2713e+00
Fitted a model with MAP estimate = -88.0549
Time for alignment: 55.8677
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 154.5714 - loglik: -1.5114e+02 - logprior: -3.4336e+00
Epoch 2/10
19/19 - 1s - loss: 120.5784 - loglik: -1.1904e+02 - logprior: -1.5403e+00
Epoch 3/10
19/19 - 1s - loss: 107.0458 - loglik: -1.0551e+02 - logprior: -1.5397e+00
Epoch 4/10
19/19 - 1s - loss: 104.1743 - loglik: -1.0249e+02 - logprior: -1.6882e+00
Epoch 5/10
19/19 - 1s - loss: 103.3665 - loglik: -1.0175e+02 - logprior: -1.6117e+00
Epoch 6/10
19/19 - 1s - loss: 102.5534 - loglik: -1.0096e+02 - logprior: -1.5955e+00
Epoch 7/10
19/19 - 1s - loss: 102.7683 - loglik: -1.0115e+02 - logprior: -1.6180e+00
Fitted a model with MAP estimate = -98.0402
expansions: [(3, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 2), (31, 2), (41, 1)]
discards: [0]
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.8502 - loglik: -1.0360e+02 - logprior: -4.2550e+00
Epoch 2/2
19/19 - 1s - loss: 97.2314 - loglik: -9.4886e+01 - logprior: -2.3450e+00
Fitted a model with MAP estimate = -91.5352
expansions: [(3, 1)]
discards: [ 0 12 38 42 44]
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 99.9262 - loglik: -9.6081e+01 - logprior: -3.8454e+00
Epoch 2/2
19/19 - 1s - loss: 95.6796 - loglik: -9.4328e+01 - logprior: -1.3520e+00
Fitted a model with MAP estimate = -90.8928
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 94.5940 - loglik: -9.0969e+01 - logprior: -3.6248e+00
Epoch 2/10
21/21 - 2s - loss: 90.7528 - loglik: -8.8891e+01 - logprior: -1.8620e+00
Epoch 3/10
21/21 - 2s - loss: 89.6583 - loglik: -8.8318e+01 - logprior: -1.3406e+00
Epoch 4/10
21/21 - 2s - loss: 88.8607 - loglik: -8.7603e+01 - logprior: -1.2578e+00
Epoch 5/10
21/21 - 2s - loss: 89.0266 - loglik: -8.7793e+01 - logprior: -1.2334e+00
Fitted a model with MAP estimate = -88.4108
Time for alignment: 48.4707
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 154.4742 - loglik: -1.5104e+02 - logprior: -3.4317e+00
Epoch 2/10
19/19 - 1s - loss: 122.5134 - loglik: -1.2098e+02 - logprior: -1.5318e+00
Epoch 3/10
19/19 - 1s - loss: 107.7631 - loglik: -1.0622e+02 - logprior: -1.5393e+00
Epoch 4/10
19/19 - 1s - loss: 103.4689 - loglik: -1.0176e+02 - logprior: -1.7115e+00
Epoch 5/10
19/19 - 1s - loss: 102.6885 - loglik: -1.0108e+02 - logprior: -1.6119e+00
Epoch 6/10
19/19 - 1s - loss: 102.1389 - loglik: -1.0051e+02 - logprior: -1.6257e+00
Epoch 7/10
19/19 - 1s - loss: 101.9182 - loglik: -1.0029e+02 - logprior: -1.6306e+00
Epoch 8/10
19/19 - 1s - loss: 101.8319 - loglik: -1.0019e+02 - logprior: -1.6468e+00
Epoch 9/10
19/19 - 1s - loss: 101.6922 - loglik: -1.0004e+02 - logprior: -1.6562e+00
Epoch 10/10
19/19 - 1s - loss: 101.7250 - loglik: -1.0006e+02 - logprior: -1.6632e+00
Fitted a model with MAP estimate = -97.4901
expansions: [(3, 1), (5, 1), (6, 1), (7, 1), (9, 1), (13, 1), (21, 1), (24, 1), (28, 1), (29, 2), (30, 1), (31, 2), (33, 1)]
discards: [0]
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 106.3963 - loglik: -1.0215e+02 - logprior: -4.2488e+00
Epoch 2/2
19/19 - 1s - loss: 96.8554 - loglik: -9.4579e+01 - logprior: -2.2764e+00
Fitted a model with MAP estimate = -91.2337
expansions: [(3, 1)]
discards: [ 0 37 42]
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 99.8947 - loglik: -9.5951e+01 - logprior: -3.9436e+00
Epoch 2/2
19/19 - 1s - loss: 95.5484 - loglik: -9.4095e+01 - logprior: -1.4534e+00
Fitted a model with MAP estimate = -90.9068
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 94.7733 - loglik: -9.1121e+01 - logprior: -3.6521e+00
Epoch 2/10
21/21 - 2s - loss: 90.0875 - loglik: -8.8289e+01 - logprior: -1.7980e+00
Epoch 3/10
21/21 - 2s - loss: 89.9414 - loglik: -8.8594e+01 - logprior: -1.3470e+00
Epoch 4/10
21/21 - 2s - loss: 89.2311 - loglik: -8.7973e+01 - logprior: -1.2579e+00
Epoch 5/10
21/21 - 2s - loss: 88.8536 - loglik: -8.7598e+01 - logprior: -1.2552e+00
Epoch 6/10
21/21 - 2s - loss: 88.1831 - loglik: -8.6913e+01 - logprior: -1.2704e+00
Epoch 7/10
21/21 - 2s - loss: 88.4532 - loglik: -8.7175e+01 - logprior: -1.2781e+00
Fitted a model with MAP estimate = -88.0762
Time for alignment: 57.2240
Computed alignments with likelihoods: ['-88.0549', '-88.4108', '-88.0762']
Best model has likelihood: -88.0549
SP score = 0.9478
Training of 3 independent models on file OTCace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2153929d00>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2120f44460>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20f619a130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2153b295b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 122 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 449.9427 - loglik: -4.4070e+02 - logprior: -9.2457e+00
Epoch 2/10
13/13 - 4s - loss: 390.6466 - loglik: -3.8856e+02 - logprior: -2.0873e+00
Epoch 3/10
13/13 - 4s - loss: 345.4529 - loglik: -3.4361e+02 - logprior: -1.8403e+00
Epoch 4/10
13/13 - 4s - loss: 327.9170 - loglik: -3.2572e+02 - logprior: -2.1942e+00
Epoch 5/10
13/13 - 5s - loss: 320.7893 - loglik: -3.1867e+02 - logprior: -2.1162e+00
Epoch 6/10
13/13 - 4s - loss: 317.5323 - loglik: -3.1539e+02 - logprior: -2.1456e+00
Epoch 7/10
13/13 - 5s - loss: 316.1342 - loglik: -3.1397e+02 - logprior: -2.1661e+00
Epoch 8/10
13/13 - 5s - loss: 315.5023 - loglik: -3.1332e+02 - logprior: -2.1784e+00
Epoch 9/10
13/13 - 5s - loss: 315.2908 - loglik: -3.1309e+02 - logprior: -2.2016e+00
Epoch 10/10
13/13 - 5s - loss: 314.4889 - loglik: -3.1229e+02 - logprior: -2.2028e+00
Fitted a model with MAP estimate = -314.6464
expansions: [(0, 2), (10, 1), (19, 1), (21, 1), (24, 2), (26, 1), (33, 1), (36, 1), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 1), (67, 2), (68, 1), (69, 1), (71, 1), (72, 1), (73, 1), (78, 1), (81, 1), (94, 5), (100, 1), (104, 2), (105, 2), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Fitting a model of length 165 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 10s - loss: 322.7111 - loglik: -3.1189e+02 - logprior: -1.0821e+01
Epoch 2/2
13/13 - 7s - loss: 297.7473 - loglik: -2.9496e+02 - logprior: -2.7863e+00
Fitted a model with MAP estimate = -293.6728
expansions: []
discards: [  0  29  48  49  88 125 126 127 140 142]
Fitting a model of length 155 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 10s - loss: 305.6093 - loglik: -2.9646e+02 - logprior: -9.1509e+00
Epoch 2/2
13/13 - 7s - loss: 296.7949 - loglik: -2.9297e+02 - logprior: -3.8253e+00
Fitted a model with MAP estimate = -294.6766
expansions: [(0, 2)]
discards: [  0 148]
Fitting a model of length 155 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 9s - loss: 300.5966 - loglik: -2.9347e+02 - logprior: -7.1224e+00
Epoch 2/10
13/13 - 7s - loss: 293.6273 - loglik: -2.9197e+02 - logprior: -1.6524e+00
Epoch 3/10
13/13 - 6s - loss: 292.3348 - loglik: -2.9135e+02 - logprior: -9.8641e-01
Epoch 4/10
13/13 - 6s - loss: 291.4803 - loglik: -2.9077e+02 - logprior: -7.1384e-01
Epoch 5/10
13/13 - 7s - loss: 291.4465 - loglik: -2.9080e+02 - logprior: -6.4778e-01
Epoch 6/10
13/13 - 6s - loss: 290.5101 - loglik: -2.8992e+02 - logprior: -5.9226e-01
Epoch 7/10
13/13 - 5s - loss: 291.0555 - loglik: -2.9049e+02 - logprior: -5.6709e-01
Fitted a model with MAP estimate = -290.4385
Time for alignment: 154.2815
Fitting a model of length 122 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 9s - loss: 450.2664 - loglik: -4.4102e+02 - logprior: -9.2442e+00
Epoch 2/10
13/13 - 4s - loss: 390.6138 - loglik: -3.8853e+02 - logprior: -2.0794e+00
Epoch 3/10
13/13 - 4s - loss: 346.5743 - loglik: -3.4473e+02 - logprior: -1.8449e+00
Epoch 4/10
13/13 - 4s - loss: 328.8586 - loglik: -3.2679e+02 - logprior: -2.0656e+00
Epoch 5/10
13/13 - 4s - loss: 322.8206 - loglik: -3.2093e+02 - logprior: -1.8920e+00
Epoch 6/10
13/13 - 4s - loss: 317.7755 - loglik: -3.1579e+02 - logprior: -1.9876e+00
Epoch 7/10
13/13 - 4s - loss: 315.3773 - loglik: -3.1326e+02 - logprior: -2.1166e+00
Epoch 8/10
13/13 - 4s - loss: 313.1614 - loglik: -3.1099e+02 - logprior: -2.1750e+00
Epoch 9/10
13/13 - 4s - loss: 312.6420 - loglik: -3.1043e+02 - logprior: -2.2132e+00
Epoch 10/10
13/13 - 4s - loss: 313.0012 - loglik: -3.1079e+02 - logprior: -2.2148e+00
Fitted a model with MAP estimate = -312.3329
expansions: [(0, 2), (9, 2), (18, 1), (24, 2), (26, 1), (35, 1), (36, 1), (37, 4), (38, 1), (39, 2), (43, 1), (45, 1), (47, 1), (67, 1), (68, 1), (71, 1), (72, 1), (73, 1), (78, 1), (81, 1), (83, 2), (85, 2), (100, 1), (102, 2), (103, 1), (104, 2), (105, 1), (115, 1), (116, 1)]
discards: []
Fitting a model of length 162 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 9s - loss: 320.6748 - loglik: -3.0979e+02 - logprior: -1.0881e+01
Epoch 2/2
13/13 - 6s - loss: 296.3947 - loglik: -2.9359e+02 - logprior: -2.8073e+00
Fitted a model with MAP estimate = -292.3654
expansions: []
discards: [  0  12  29  48  54 110 114 134 139]
Fitting a model of length 153 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 9s - loss: 305.0889 - loglik: -2.9587e+02 - logprior: -9.2235e+00
Epoch 2/2
13/13 - 5s - loss: 295.8439 - loglik: -2.9194e+02 - logprior: -3.9011e+00
Fitted a model with MAP estimate = -293.8544
expansions: [(0, 2), (42, 1), (86, 1)]
discards: [0]
Fitting a model of length 156 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 298.3665 - loglik: -2.9121e+02 - logprior: -7.1596e+00
Epoch 2/10
13/13 - 6s - loss: 291.3332 - loglik: -2.8965e+02 - logprior: -1.6855e+00
Epoch 3/10
13/13 - 5s - loss: 289.0063 - loglik: -2.8801e+02 - logprior: -9.9914e-01
Epoch 4/10
13/13 - 5s - loss: 288.1013 - loglik: -2.8738e+02 - logprior: -7.1735e-01
Epoch 5/10
13/13 - 5s - loss: 288.7023 - loglik: -2.8807e+02 - logprior: -6.3091e-01
Fitted a model with MAP estimate = -287.7870
Time for alignment: 127.0933
Fitting a model of length 122 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 450.2953 - loglik: -4.4105e+02 - logprior: -9.2499e+00
Epoch 2/10
13/13 - 4s - loss: 390.1106 - loglik: -3.8801e+02 - logprior: -2.1046e+00
Epoch 3/10
13/13 - 4s - loss: 341.7534 - loglik: -3.3985e+02 - logprior: -1.8986e+00
Epoch 4/10
13/13 - 4s - loss: 325.0539 - loglik: -3.2281e+02 - logprior: -2.2413e+00
Epoch 5/10
13/13 - 4s - loss: 318.9531 - loglik: -3.1685e+02 - logprior: -2.1051e+00
Epoch 6/10
13/13 - 4s - loss: 316.2653 - loglik: -3.1412e+02 - logprior: -2.1445e+00
Epoch 7/10
13/13 - 4s - loss: 315.3322 - loglik: -3.1319e+02 - logprior: -2.1381e+00
Epoch 8/10
13/13 - 4s - loss: 314.8383 - loglik: -3.1268e+02 - logprior: -2.1565e+00
Epoch 9/10
13/13 - 4s - loss: 313.2702 - loglik: -3.1110e+02 - logprior: -2.1748e+00
Epoch 10/10
13/13 - 4s - loss: 313.8567 - loglik: -3.1165e+02 - logprior: -2.2035e+00
Fitted a model with MAP estimate = -313.4236
expansions: [(0, 2), (19, 1), (21, 1), (25, 2), (26, 1), (35, 1), (36, 2), (37, 4), (38, 1), (39, 1), (43, 1), (45, 1), (47, 1), (67, 1), (68, 1), (69, 1), (71, 1), (72, 1), (73, 1), (78, 1), (81, 1), (85, 1), (99, 1), (100, 1), (104, 2), (105, 2), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Fitting a model of length 160 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 9s - loss: 320.1422 - loglik: -3.0929e+02 - logprior: -1.0849e+01
Epoch 2/2
13/13 - 6s - loss: 296.4770 - loglik: -2.9380e+02 - logprior: -2.6811e+00
Fitted a model with MAP estimate = -292.6056
expansions: []
discards: [  0  29  48 135 137]
Fitting a model of length 155 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 304.5310 - loglik: -2.9542e+02 - logprior: -9.1136e+00
Epoch 2/2
13/13 - 5s - loss: 295.6394 - loglik: -2.9188e+02 - logprior: -3.7629e+00
Fitted a model with MAP estimate = -293.7385
expansions: [(0, 2)]
discards: [  0  45 148]
Fitting a model of length 154 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 300.7659 - loglik: -2.9372e+02 - logprior: -7.0459e+00
Epoch 2/10
13/13 - 5s - loss: 293.2908 - loglik: -2.9169e+02 - logprior: -1.6040e+00
Epoch 3/10
13/13 - 5s - loss: 291.8124 - loglik: -2.9087e+02 - logprior: -9.4233e-01
Epoch 4/10
13/13 - 5s - loss: 291.1237 - loglik: -2.9045e+02 - logprior: -6.7763e-01
Epoch 5/10
13/13 - 5s - loss: 290.8081 - loglik: -2.9020e+02 - logprior: -6.1255e-01
Epoch 6/10
13/13 - 5s - loss: 290.6966 - loglik: -2.9014e+02 - logprior: -5.5901e-01
Epoch 7/10
13/13 - 5s - loss: 289.6459 - loglik: -2.8912e+02 - logprior: -5.2682e-01
Epoch 8/10
13/13 - 5s - loss: 290.0040 - loglik: -2.8950e+02 - logprior: -5.0217e-01
Fitted a model with MAP estimate = -289.8546
Time for alignment: 136.2006
Computed alignments with likelihoods: ['-290.4385', '-287.7870', '-289.8546']
Best model has likelihood: -287.7870
SP score = 0.5558
Training of 3 independent models on file lyase_1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2139c21e20>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20a98370a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f21076d83a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20dce172e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 236 on 7632 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 20s - loss: 820.6313 - loglik: -8.1574e+02 - logprior: -4.8909e+00
Epoch 2/10
17/17 - 19s - loss: 711.8959 - loglik: -7.1091e+02 - logprior: -9.8165e-01
Epoch 3/10
17/17 - 18s - loss: 647.7484 - loglik: -6.4597e+02 - logprior: -1.7814e+00
Epoch 4/10
17/17 - 18s - loss: 627.1385 - loglik: -6.2504e+02 - logprior: -2.1033e+00
Epoch 5/10
17/17 - 19s - loss: 621.7722 - loglik: -6.1941e+02 - logprior: -2.3654e+00
Epoch 6/10
17/17 - 19s - loss: 616.9801 - loglik: -6.1449e+02 - logprior: -2.4860e+00
Epoch 7/10
17/17 - 19s - loss: 616.5640 - loglik: -6.1399e+02 - logprior: -2.5789e+00
Epoch 8/10
17/17 - 19s - loss: 615.9348 - loglik: -6.1335e+02 - logprior: -2.5858e+00
Epoch 9/10
17/17 - 19s - loss: 615.8278 - loglik: -6.1323e+02 - logprior: -2.5943e+00
Epoch 10/10
17/17 - 19s - loss: 614.8824 - loglik: -6.1224e+02 - logprior: -2.6408e+00
Fitted a model with MAP estimate = -615.0431
expansions: [(9, 1), (12, 1), (14, 1), (16, 2), (18, 3), (22, 1), (29, 2), (51, 2), (57, 1), (61, 1), (65, 1), (68, 1), (92, 1), (94, 1), (95, 2), (96, 2), (99, 1), (101, 1), (102, 1), (103, 1), (105, 1), (107, 1), (110, 1), (131, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (165, 1), (168, 1), (172, 2), (175, 1), (176, 1), (178, 1), (184, 1), (185, 1), (186, 1), (187, 3), (188, 1), (190, 1), (202, 2), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1), (230, 1)]
discards: [0]
Fitting a model of length 296 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 31s - loss: 604.1548 - loglik: -6.0038e+02 - logprior: -3.7779e+00
Epoch 2/2
34/34 - 26s - loss: 583.1473 - loglik: -5.8167e+02 - logprior: -1.4769e+00
Fitted a model with MAP estimate = -577.7055
expansions: [(179, 1)]
discards: [ 62 114 116 255]
Fitting a model of length 293 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 29s - loss: 583.2454 - loglik: -5.8107e+02 - logprior: -2.1803e+00
Epoch 2/2
34/34 - 25s - loss: 575.8982 - loglik: -5.7542e+02 - logprior: -4.8052e-01
Fitted a model with MAP estimate = -573.7461
expansions: [(19, 1), (56, 2)]
discards: [ 20 250 251 252]
Fitting a model of length 292 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 30s - loss: 583.5190 - loglik: -5.8159e+02 - logprior: -1.9257e+00
Epoch 2/10
34/34 - 27s - loss: 578.3251 - loglik: -5.7819e+02 - logprior: -1.3793e-01
Epoch 3/10
34/34 - 27s - loss: 576.2256 - loglik: -5.7614e+02 - logprior: -8.5148e-02
Epoch 4/10
34/34 - 30s - loss: 572.3172 - loglik: -5.7225e+02 - logprior: -7.0663e-02
Epoch 5/10
34/34 - 31s - loss: 573.5717 - loglik: -5.7349e+02 - logprior: -7.7089e-02
Fitted a model with MAP estimate = -572.1528
Time for alignment: 554.0145
Fitting a model of length 236 on 7632 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 24s - loss: 820.3363 - loglik: -8.1544e+02 - logprior: -4.8932e+00
Epoch 2/10
17/17 - 22s - loss: 711.6259 - loglik: -7.1063e+02 - logprior: -9.9670e-01
Epoch 3/10
17/17 - 22s - loss: 646.6732 - loglik: -6.4486e+02 - logprior: -1.8169e+00
Epoch 4/10
17/17 - 23s - loss: 628.6622 - loglik: -6.2655e+02 - logprior: -2.1079e+00
Epoch 5/10
17/17 - 22s - loss: 620.6241 - loglik: -6.1826e+02 - logprior: -2.3637e+00
Epoch 6/10
17/17 - 22s - loss: 617.7492 - loglik: -6.1530e+02 - logprior: -2.4507e+00
Epoch 7/10
17/17 - 22s - loss: 615.7418 - loglik: -6.1323e+02 - logprior: -2.5076e+00
Epoch 8/10
17/17 - 23s - loss: 615.8190 - loglik: -6.1325e+02 - logprior: -2.5706e+00
Fitted a model with MAP estimate = -614.9624
expansions: [(9, 1), (12, 1), (14, 1), (15, 3), (16, 1), (17, 5), (22, 1), (29, 1), (30, 2), (39, 1), (50, 1), (51, 1), (55, 4), (56, 1), (65, 1), (66, 1), (67, 1), (91, 1), (93, 2), (94, 2), (95, 2), (99, 1), (100, 1), (101, 1), (102, 1), (104, 1), (109, 1), (129, 1), (134, 1), (139, 1), (142, 3), (143, 1), (144, 1), (165, 1), (167, 1), (171, 2), (172, 1), (184, 1), (188, 1), (193, 1), (203, 1), (208, 1), (211, 1), (225, 1), (229, 1), (230, 1)]
discards: [0]
Fitting a model of length 297 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 36s - loss: 605.6935 - loglik: -6.0190e+02 - logprior: -3.7969e+00
Epoch 2/2
34/34 - 33s - loss: 582.4110 - loglik: -5.8081e+02 - logprior: -1.6035e+00
Fitted a model with MAP estimate = -579.7432
expansions: [(25, 1), (87, 1), (186, 1), (237, 1)]
discards: [ 18  22  23  42  73  74  75  76  88 121 122 126]
Fitting a model of length 289 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 39s - loss: 592.8212 - loglik: -5.9058e+02 - logprior: -2.2423e+00
Epoch 2/2
34/34 - 36s - loss: 585.8448 - loglik: -5.8534e+02 - logprior: -5.0904e-01
Fitted a model with MAP estimate = -584.2243
expansions: [(70, 4), (82, 2)]
discards: [80]
Fitting a model of length 294 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 40s - loss: 588.3027 - loglik: -5.8623e+02 - logprior: -2.0748e+00
Epoch 2/10
34/34 - 35s - loss: 582.1846 - loglik: -5.8188e+02 - logprior: -2.9979e-01
Epoch 3/10
34/34 - 36s - loss: 578.3355 - loglik: -5.7810e+02 - logprior: -2.3265e-01
Epoch 4/10
34/34 - 35s - loss: 577.5935 - loglik: -5.7734e+02 - logprior: -2.5834e-01
Epoch 5/10
34/34 - 35s - loss: 578.1180 - loglik: -5.7784e+02 - logprior: -2.7480e-01
Fitted a model with MAP estimate = -576.0949
Time for alignment: 662.1761
Fitting a model of length 236 on 7632 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 26s - loss: 820.0723 - loglik: -8.1518e+02 - logprior: -4.8945e+00
Epoch 2/10
17/17 - 23s - loss: 709.0931 - loglik: -7.0807e+02 - logprior: -1.0208e+00
Epoch 3/10
17/17 - 24s - loss: 645.9444 - loglik: -6.4416e+02 - logprior: -1.7882e+00
Epoch 4/10
17/17 - 23s - loss: 628.3607 - loglik: -6.2630e+02 - logprior: -2.0581e+00
Epoch 5/10
17/17 - 23s - loss: 620.6110 - loglik: -6.1828e+02 - logprior: -2.3291e+00
Epoch 6/10
17/17 - 24s - loss: 616.9711 - loglik: -6.1456e+02 - logprior: -2.4151e+00
Epoch 7/10
17/17 - 24s - loss: 617.0091 - loglik: -6.1451e+02 - logprior: -2.4991e+00
Fitted a model with MAP estimate = -615.0059
expansions: [(7, 1), (8, 1), (14, 1), (15, 3), (16, 1), (17, 5), (29, 3), (42, 1), (45, 2), (54, 1), (56, 2), (65, 1), (66, 1), (67, 1), (91, 1), (93, 1), (94, 1), (99, 1), (100, 1), (102, 1), (103, 1), (104, 1), (107, 1), (109, 1), (131, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (147, 1), (164, 4), (167, 1), (171, 1), (172, 1), (175, 1), (178, 1), (188, 1), (193, 1), (202, 2), (203, 1), (208, 1), (211, 1), (225, 1), (229, 1), (230, 1)]
discards: [0]
Fitting a model of length 297 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 36s - loss: 603.6270 - loglik: -5.9986e+02 - logprior: -3.7688e+00
Epoch 2/2
34/34 - 35s - loss: 583.7349 - loglik: -5.8219e+02 - logprior: -1.5409e+00
Fitted a model with MAP estimate = -579.8990
expansions: []
discards: [ 17  18  42  60  61  74 207 256]
Fitting a model of length 289 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 39s - loss: 591.2280 - loglik: -5.8902e+02 - logprior: -2.2041e+00
Epoch 2/2
34/34 - 37s - loss: 583.3091 - loglik: -5.8281e+02 - logprior: -4.9853e-01
Fitted a model with MAP estimate = -581.9363
expansions: [(17, 2), (177, 1), (201, 1)]
discards: []
Fitting a model of length 293 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 40s - loss: 586.2996 - loglik: -5.8420e+02 - logprior: -2.1044e+00
Epoch 2/10
34/34 - 37s - loss: 580.0046 - loglik: -5.7976e+02 - logprior: -2.4411e-01
Epoch 3/10
34/34 - 36s - loss: 579.7162 - loglik: -5.7952e+02 - logprior: -1.9431e-01
Epoch 4/10
34/34 - 37s - loss: 576.6393 - loglik: -5.7648e+02 - logprior: -1.5800e-01
Epoch 5/10
34/34 - 36s - loss: 575.5337 - loglik: -5.7523e+02 - logprior: -3.0623e-01
Epoch 6/10
34/34 - 36s - loss: 575.8234 - loglik: -5.7561e+02 - logprior: -2.0917e-01
Fitted a model with MAP estimate = -574.4148
Time for alignment: 698.1823
Computed alignments with likelihoods: ['-572.1528', '-576.0949', '-574.4148']
Best model has likelihood: -572.1528
SP score = 0.6654
Training of 3 independent models on file toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20a987fa30>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f214aaeb910>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202cc64250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2179315f10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 277.9526 - loglik: -1.8313e+02 - logprior: -9.4826e+01
Epoch 2/10
10/10 - 1s - loss: 184.8742 - loglik: -1.5970e+02 - logprior: -2.5173e+01
Epoch 3/10
10/10 - 1s - loss: 153.0658 - loglik: -1.4137e+02 - logprior: -1.1698e+01
Epoch 4/10
10/10 - 1s - loss: 138.6416 - loglik: -1.3180e+02 - logprior: -6.8440e+00
Epoch 5/10
10/10 - 1s - loss: 131.9809 - loglik: -1.2761e+02 - logprior: -4.3703e+00
Epoch 6/10
10/10 - 1s - loss: 128.4579 - loglik: -1.2545e+02 - logprior: -3.0040e+00
Epoch 7/10
10/10 - 1s - loss: 126.6002 - loglik: -1.2450e+02 - logprior: -2.0994e+00
Epoch 8/10
10/10 - 1s - loss: 125.5715 - loglik: -1.2401e+02 - logprior: -1.5585e+00
Epoch 9/10
10/10 - 1s - loss: 125.0542 - loglik: -1.2386e+02 - logprior: -1.1980e+00
Epoch 10/10
10/10 - 1s - loss: 124.7452 - loglik: -1.2385e+02 - logprior: -8.9515e-01
Fitted a model with MAP estimate = -124.6106
expansions: [(7, 1), (8, 3), (9, 2), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 223.2502 - loglik: -1.2253e+02 - logprior: -1.0072e+02
Epoch 2/2
10/10 - 1s - loss: 157.2111 - loglik: -1.1666e+02 - logprior: -4.0556e+01
Fitted a model with MAP estimate = -145.8931
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 192.3647 - loglik: -1.1339e+02 - logprior: -7.8974e+01
Epoch 2/2
10/10 - 1s - loss: 131.8232 - loglik: -1.1116e+02 - logprior: -2.0668e+01
Fitted a model with MAP estimate = -122.8899
expansions: []
discards: []
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 187.8103 - loglik: -1.1061e+02 - logprior: -7.7197e+01
Epoch 2/10
10/10 - 1s - loss: 130.3183 - loglik: -1.1030e+02 - logprior: -2.0017e+01
Epoch 3/10
10/10 - 1s - loss: 118.4016 - loglik: -1.1021e+02 - logprior: -8.1925e+00
Epoch 4/10
10/10 - 1s - loss: 113.7474 - loglik: -1.1056e+02 - logprior: -3.1843e+00
Epoch 5/10
10/10 - 1s - loss: 111.3832 - loglik: -1.1098e+02 - logprior: -3.9885e-01
Epoch 6/10
10/10 - 1s - loss: 110.0860 - loglik: -1.1132e+02 - logprior: 1.2354
Epoch 7/10
10/10 - 1s - loss: 109.3239 - loglik: -1.1155e+02 - logprior: 2.2287
Epoch 8/10
10/10 - 1s - loss: 108.8186 - loglik: -1.1169e+02 - logprior: 2.8686
Epoch 9/10
10/10 - 1s - loss: 108.4408 - loglik: -1.1179e+02 - logprior: 3.3484
Epoch 10/10
10/10 - 1s - loss: 108.1321 - loglik: -1.1186e+02 - logprior: 3.7273
Fitted a model with MAP estimate = -107.9779
Time for alignment: 35.1671
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 277.9526 - loglik: -1.8313e+02 - logprior: -9.4826e+01
Epoch 2/10
10/10 - 1s - loss: 184.8742 - loglik: -1.5970e+02 - logprior: -2.5173e+01
Epoch 3/10
10/10 - 1s - loss: 153.0658 - loglik: -1.4137e+02 - logprior: -1.1698e+01
Epoch 4/10
10/10 - 1s - loss: 138.6416 - loglik: -1.3180e+02 - logprior: -6.8440e+00
Epoch 5/10
10/10 - 1s - loss: 131.9809 - loglik: -1.2761e+02 - logprior: -4.3703e+00
Epoch 6/10
10/10 - 1s - loss: 128.4579 - loglik: -1.2545e+02 - logprior: -3.0040e+00
Epoch 7/10
10/10 - 1s - loss: 126.6002 - loglik: -1.2450e+02 - logprior: -2.0994e+00
Epoch 8/10
10/10 - 1s - loss: 125.5715 - loglik: -1.2401e+02 - logprior: -1.5585e+00
Epoch 9/10
10/10 - 1s - loss: 125.0542 - loglik: -1.2386e+02 - logprior: -1.1980e+00
Epoch 10/10
10/10 - 1s - loss: 124.7452 - loglik: -1.2385e+02 - logprior: -8.9515e-01
Fitted a model with MAP estimate = -124.6106
expansions: [(7, 1), (8, 3), (9, 2), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 223.2502 - loglik: -1.2253e+02 - logprior: -1.0072e+02
Epoch 2/2
10/10 - 1s - loss: 157.2111 - loglik: -1.1666e+02 - logprior: -4.0556e+01
Fitted a model with MAP estimate = -145.8931
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 192.3647 - loglik: -1.1339e+02 - logprior: -7.8974e+01
Epoch 2/2
10/10 - 1s - loss: 131.8232 - loglik: -1.1116e+02 - logprior: -2.0668e+01
Fitted a model with MAP estimate = -122.8899
expansions: []
discards: []
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 187.8103 - loglik: -1.1061e+02 - logprior: -7.7197e+01
Epoch 2/10
10/10 - 1s - loss: 130.3183 - loglik: -1.1030e+02 - logprior: -2.0017e+01
Epoch 3/10
10/10 - 1s - loss: 118.4016 - loglik: -1.1021e+02 - logprior: -8.1925e+00
Epoch 4/10
10/10 - 1s - loss: 113.7474 - loglik: -1.1056e+02 - logprior: -3.1843e+00
Epoch 5/10
10/10 - 1s - loss: 111.3832 - loglik: -1.1098e+02 - logprior: -3.9885e-01
Epoch 6/10
10/10 - 1s - loss: 110.0860 - loglik: -1.1132e+02 - logprior: 1.2354
Epoch 7/10
10/10 - 1s - loss: 109.3239 - loglik: -1.1155e+02 - logprior: 2.2287
Epoch 8/10
10/10 - 1s - loss: 108.8186 - loglik: -1.1169e+02 - logprior: 2.8686
Epoch 9/10
10/10 - 1s - loss: 108.4408 - loglik: -1.1179e+02 - logprior: 3.3484
Epoch 10/10
10/10 - 1s - loss: 108.1321 - loglik: -1.1186e+02 - logprior: 3.7273
Fitted a model with MAP estimate = -107.9779
Time for alignment: 34.9138
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 277.9526 - loglik: -1.8313e+02 - logprior: -9.4826e+01
Epoch 2/10
10/10 - 1s - loss: 184.8742 - loglik: -1.5970e+02 - logprior: -2.5173e+01
Epoch 3/10
10/10 - 1s - loss: 153.0658 - loglik: -1.4137e+02 - logprior: -1.1698e+01
Epoch 4/10
10/10 - 1s - loss: 138.6416 - loglik: -1.3180e+02 - logprior: -6.8440e+00
Epoch 5/10
10/10 - 1s - loss: 131.9809 - loglik: -1.2761e+02 - logprior: -4.3703e+00
Epoch 6/10
10/10 - 1s - loss: 128.4579 - loglik: -1.2545e+02 - logprior: -3.0040e+00
Epoch 7/10
10/10 - 1s - loss: 126.6002 - loglik: -1.2450e+02 - logprior: -2.0994e+00
Epoch 8/10
10/10 - 1s - loss: 125.5715 - loglik: -1.2401e+02 - logprior: -1.5585e+00
Epoch 9/10
10/10 - 1s - loss: 125.0542 - loglik: -1.2386e+02 - logprior: -1.1980e+00
Epoch 10/10
10/10 - 1s - loss: 124.7452 - loglik: -1.2385e+02 - logprior: -8.9515e-01
Fitted a model with MAP estimate = -124.6106
expansions: [(7, 1), (8, 3), (9, 2), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 223.2502 - loglik: -1.2253e+02 - logprior: -1.0072e+02
Epoch 2/2
10/10 - 1s - loss: 157.2111 - loglik: -1.1666e+02 - logprior: -4.0556e+01
Fitted a model with MAP estimate = -145.8931
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 192.3647 - loglik: -1.1339e+02 - logprior: -7.8974e+01
Epoch 2/2
10/10 - 1s - loss: 131.8232 - loglik: -1.1116e+02 - logprior: -2.0668e+01
Fitted a model with MAP estimate = -122.8899
expansions: []
discards: []
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 187.8103 - loglik: -1.1061e+02 - logprior: -7.7197e+01
Epoch 2/10
10/10 - 1s - loss: 130.3183 - loglik: -1.1030e+02 - logprior: -2.0017e+01
Epoch 3/10
10/10 - 1s - loss: 118.4016 - loglik: -1.1021e+02 - logprior: -8.1925e+00
Epoch 4/10
10/10 - 1s - loss: 113.7474 - loglik: -1.1056e+02 - logprior: -3.1843e+00
Epoch 5/10
10/10 - 1s - loss: 111.3832 - loglik: -1.1098e+02 - logprior: -3.9885e-01
Epoch 6/10
10/10 - 1s - loss: 110.0860 - loglik: -1.1132e+02 - logprior: 1.2354
Epoch 7/10
10/10 - 1s - loss: 109.3239 - loglik: -1.1155e+02 - logprior: 2.2287
Epoch 8/10
10/10 - 1s - loss: 108.8186 - loglik: -1.1169e+02 - logprior: 2.8686
Epoch 9/10
10/10 - 1s - loss: 108.4408 - loglik: -1.1179e+02 - logprior: 3.3484
Epoch 10/10
10/10 - 1s - loss: 108.1321 - loglik: -1.1186e+02 - logprior: 3.7273
Fitted a model with MAP estimate = -107.9779
Time for alignment: 33.4439
Computed alignments with likelihoods: ['-107.9779', '-107.9779', '-107.9779']
Best model has likelihood: -107.9779
SP score = 0.8411
Training of 3 independent models on file msb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2098b725e0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20d45cb4c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f211811b790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f217863a400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 317.6439 - loglik: -3.0893e+02 - logprior: -8.7110e+00
Epoch 2/10
13/13 - 2s - loss: 285.6916 - loglik: -2.8367e+02 - logprior: -2.0262e+00
Epoch 3/10
13/13 - 2s - loss: 261.9478 - loglik: -2.6025e+02 - logprior: -1.7014e+00
Epoch 4/10
13/13 - 2s - loss: 252.0323 - loglik: -2.5010e+02 - logprior: -1.9310e+00
Epoch 5/10
13/13 - 2s - loss: 249.4630 - loglik: -2.4759e+02 - logprior: -1.8709e+00
Epoch 6/10
13/13 - 2s - loss: 249.2164 - loglik: -2.4736e+02 - logprior: -1.8599e+00
Epoch 7/10
13/13 - 2s - loss: 248.2289 - loglik: -2.4628e+02 - logprior: -1.9462e+00
Epoch 8/10
13/13 - 2s - loss: 248.2146 - loglik: -2.4629e+02 - logprior: -1.9287e+00
Epoch 9/10
13/13 - 2s - loss: 247.8725 - loglik: -2.4596e+02 - logprior: -1.9078e+00
Epoch 10/10
13/13 - 2s - loss: 248.1013 - loglik: -2.4619e+02 - logprior: -1.9126e+00
Fitted a model with MAP estimate = -247.8854
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (21, 1), (27, 1), (28, 1), (29, 2), (30, 2), (31, 1), (40, 1), (43, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 258.6321 - loglik: -2.4919e+02 - logprior: -9.4468e+00
Epoch 2/2
13/13 - 2s - loss: 247.8675 - loglik: -2.4373e+02 - logprior: -4.1356e+00
Fitted a model with MAP estimate = -246.3546
expansions: [(0, 2)]
discards: [ 0 14 66 92 95]
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 249.9200 - loglik: -2.4307e+02 - logprior: -6.8548e+00
Epoch 2/2
13/13 - 2s - loss: 243.7103 - loglik: -2.4189e+02 - logprior: -1.8196e+00
Fitted a model with MAP estimate = -242.9085
expansions: []
discards: [0]
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 252.0843 - loglik: -2.4354e+02 - logprior: -8.5426e+00
Epoch 2/10
13/13 - 2s - loss: 245.8144 - loglik: -2.4321e+02 - logprior: -2.6091e+00
Epoch 3/10
13/13 - 2s - loss: 243.3629 - loglik: -2.4211e+02 - logprior: -1.2573e+00
Epoch 4/10
13/13 - 2s - loss: 242.8905 - loglik: -2.4186e+02 - logprior: -1.0279e+00
Epoch 5/10
13/13 - 2s - loss: 242.5520 - loglik: -2.4164e+02 - logprior: -9.1150e-01
Epoch 6/10
13/13 - 2s - loss: 242.0639 - loglik: -2.4112e+02 - logprior: -9.4787e-01
Epoch 7/10
13/13 - 2s - loss: 242.1193 - loglik: -2.4116e+02 - logprior: -9.5898e-01
Fitted a model with MAP estimate = -241.8222
Time for alignment: 69.9614
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 317.6643 - loglik: -3.0895e+02 - logprior: -8.7123e+00
Epoch 2/10
13/13 - 2s - loss: 285.4971 - loglik: -2.8347e+02 - logprior: -2.0306e+00
Epoch 3/10
13/13 - 2s - loss: 262.1872 - loglik: -2.6050e+02 - logprior: -1.6908e+00
Epoch 4/10
13/13 - 2s - loss: 252.2901 - loglik: -2.5043e+02 - logprior: -1.8585e+00
Epoch 5/10
13/13 - 2s - loss: 249.6308 - loglik: -2.4784e+02 - logprior: -1.7906e+00
Epoch 6/10
13/13 - 2s - loss: 248.7707 - loglik: -2.4698e+02 - logprior: -1.7931e+00
Epoch 7/10
13/13 - 2s - loss: 248.5176 - loglik: -2.4663e+02 - logprior: -1.8893e+00
Epoch 8/10
13/13 - 2s - loss: 248.1331 - loglik: -2.4627e+02 - logprior: -1.8596e+00
Epoch 9/10
13/13 - 2s - loss: 248.2214 - loglik: -2.4637e+02 - logprior: -1.8490e+00
Fitted a model with MAP estimate = -248.0347
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 4), (29, 3), (30, 1), (39, 1), (40, 1), (44, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 258.8339 - loglik: -2.4941e+02 - logprior: -9.4268e+00
Epoch 2/2
13/13 - 2s - loss: 248.1408 - loglik: -2.4402e+02 - logprior: -4.1230e+00
Fitted a model with MAP estimate = -246.3833
expansions: [(0, 2)]
discards: [ 0 14 36 66 95]
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 249.6206 - loglik: -2.4277e+02 - logprior: -6.8529e+00
Epoch 2/2
13/13 - 2s - loss: 243.6956 - loglik: -2.4189e+02 - logprior: -1.8093e+00
Fitted a model with MAP estimate = -242.9430
expansions: []
discards: [0]
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 252.3399 - loglik: -2.4381e+02 - logprior: -8.5313e+00
Epoch 2/10
13/13 - 2s - loss: 245.2110 - loglik: -2.4259e+02 - logprior: -2.6161e+00
Epoch 3/10
13/13 - 2s - loss: 243.7662 - loglik: -2.4250e+02 - logprior: -1.2662e+00
Epoch 4/10
13/13 - 2s - loss: 242.8781 - loglik: -2.4185e+02 - logprior: -1.0249e+00
Epoch 5/10
13/13 - 2s - loss: 242.4285 - loglik: -2.4152e+02 - logprior: -9.1275e-01
Epoch 6/10
13/13 - 2s - loss: 242.2937 - loglik: -2.4134e+02 - logprior: -9.5260e-01
Epoch 7/10
13/13 - 2s - loss: 242.4166 - loglik: -2.4146e+02 - logprior: -9.5964e-01
Fitted a model with MAP estimate = -241.8107
Time for alignment: 67.6038
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 317.6687 - loglik: -3.0895e+02 - logprior: -8.7163e+00
Epoch 2/10
13/13 - 2s - loss: 285.3279 - loglik: -2.8330e+02 - logprior: -2.0318e+00
Epoch 3/10
13/13 - 2s - loss: 260.8009 - loglik: -2.5912e+02 - logprior: -1.6847e+00
Epoch 4/10
13/13 - 2s - loss: 252.0189 - loglik: -2.5016e+02 - logprior: -1.8593e+00
Epoch 5/10
13/13 - 2s - loss: 249.8064 - loglik: -2.4801e+02 - logprior: -1.7920e+00
Epoch 6/10
13/13 - 2s - loss: 249.3564 - loglik: -2.4757e+02 - logprior: -1.7820e+00
Epoch 7/10
13/13 - 2s - loss: 247.2796 - loglik: -2.4538e+02 - logprior: -1.9041e+00
Epoch 8/10
13/13 - 2s - loss: 248.3998 - loglik: -2.4650e+02 - logprior: -1.9013e+00
Fitted a model with MAP estimate = -247.8762
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 1), (29, 1), (30, 3), (31, 2), (32, 2), (40, 1), (43, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 258.3178 - loglik: -2.4887e+02 - logprior: -9.4495e+00
Epoch 2/2
13/13 - 2s - loss: 247.9230 - loglik: -2.4379e+02 - logprior: -4.1322e+00
Fitted a model with MAP estimate = -246.2162
expansions: [(0, 2)]
discards: [ 0 14 67 93 96]
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 249.9733 - loglik: -2.4311e+02 - logprior: -6.8601e+00
Epoch 2/2
13/13 - 2s - loss: 243.9004 - loglik: -2.4207e+02 - logprior: -1.8316e+00
Fitted a model with MAP estimate = -242.8383
expansions: []
discards: [0]
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 252.1987 - loglik: -2.4366e+02 - logprior: -8.5380e+00
Epoch 2/10
13/13 - 2s - loss: 245.2243 - loglik: -2.4260e+02 - logprior: -2.6267e+00
Epoch 3/10
13/13 - 2s - loss: 243.6397 - loglik: -2.4238e+02 - logprior: -1.2635e+00
Epoch 4/10
13/13 - 2s - loss: 243.1685 - loglik: -2.4214e+02 - logprior: -1.0298e+00
Epoch 5/10
13/13 - 2s - loss: 241.8763 - loglik: -2.4095e+02 - logprior: -9.2458e-01
Epoch 6/10
13/13 - 2s - loss: 242.2661 - loglik: -2.4131e+02 - logprior: -9.5790e-01
Fitted a model with MAP estimate = -241.8324
Time for alignment: 64.5239
Computed alignments with likelihoods: ['-241.8222', '-241.8107', '-241.8324']
Best model has likelihood: -241.8107
SP score = 0.9262
Training of 3 independent models on file rnasemam.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f200850bdc0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f202c0c2310>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20084fdcd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2008499490>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 446.0379 - loglik: -3.4703e+02 - logprior: -9.9010e+01
Epoch 2/10
10/10 - 2s - loss: 330.6885 - loglik: -3.0791e+02 - logprior: -2.2777e+01
Epoch 3/10
10/10 - 2s - loss: 277.0834 - loglik: -2.6814e+02 - logprior: -8.9450e+00
Epoch 4/10
10/10 - 2s - loss: 247.0044 - loglik: -2.4219e+02 - logprior: -4.8130e+00
Epoch 5/10
10/10 - 2s - loss: 233.7046 - loglik: -2.3132e+02 - logprior: -2.3861e+00
Epoch 6/10
10/10 - 2s - loss: 227.5175 - loglik: -2.2647e+02 - logprior: -1.0438e+00
Epoch 7/10
10/10 - 2s - loss: 224.7113 - loglik: -2.2452e+02 - logprior: -1.9356e-01
Epoch 8/10
10/10 - 2s - loss: 223.2728 - loglik: -2.2371e+02 - logprior: 0.4330
Epoch 9/10
10/10 - 2s - loss: 222.2231 - loglik: -2.2307e+02 - logprior: 0.8472
Epoch 10/10
10/10 - 2s - loss: 221.0632 - loglik: -2.2223e+02 - logprior: 1.1667
Fitted a model with MAP estimate = -220.6370
expansions: [(13, 3), (17, 1), (29, 2), (30, 1), (41, 3), (57, 3), (61, 1), (62, 1), (63, 1), (68, 1), (79, 2), (80, 2), (81, 2), (82, 2), (89, 5)]
discards: [0]
Fitting a model of length 127 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 319.9017 - loglik: -2.1980e+02 - logprior: -1.0010e+02
Epoch 2/2
10/10 - 2s - loss: 241.2465 - loglik: -2.0335e+02 - logprior: -3.7895e+01
Fitted a model with MAP estimate = -227.2162
expansions: [(0, 2), (16, 1)]
discards: [  0  32  48  96 101 104 114 115]
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 273.9737 - loglik: -1.9834e+02 - logprior: -7.5633e+01
Epoch 2/2
10/10 - 2s - loss: 209.4174 - loglik: -1.9350e+02 - logprior: -1.5921e+01
Fitted a model with MAP estimate = -199.5420
expansions: []
discards: [0]
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 286.2272 - loglik: -1.9619e+02 - logprior: -9.0033e+01
Epoch 2/10
10/10 - 2s - loss: 217.4548 - loglik: -1.9455e+02 - logprior: -2.2906e+01
Epoch 3/10
10/10 - 2s - loss: 198.1272 - loglik: -1.9373e+02 - logprior: -4.4001e+00
Epoch 4/10
10/10 - 2s - loss: 191.1621 - loglik: -1.9331e+02 - logprior: 2.1503
Epoch 5/10
10/10 - 2s - loss: 187.9245 - loglik: -1.9326e+02 - logprior: 5.3392
Epoch 6/10
10/10 - 2s - loss: 186.1389 - loglik: -1.9333e+02 - logprior: 7.1877
Epoch 7/10
10/10 - 2s - loss: 185.0089 - loglik: -1.9337e+02 - logprior: 8.3614
Epoch 8/10
10/10 - 2s - loss: 184.2151 - loglik: -1.9342e+02 - logprior: 9.2098
Epoch 9/10
10/10 - 2s - loss: 183.5880 - loglik: -1.9351e+02 - logprior: 9.9206
Epoch 10/10
10/10 - 2s - loss: 183.0399 - loglik: -1.9361e+02 - logprior: 10.5704
Fitted a model with MAP estimate = -182.7551
Time for alignment: 61.2808
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 446.0379 - loglik: -3.4703e+02 - logprior: -9.9010e+01
Epoch 2/10
10/10 - 2s - loss: 330.6885 - loglik: -3.0791e+02 - logprior: -2.2777e+01
Epoch 3/10
10/10 - 2s - loss: 277.0834 - loglik: -2.6814e+02 - logprior: -8.9450e+00
Epoch 4/10
10/10 - 2s - loss: 247.0044 - loglik: -2.4219e+02 - logprior: -4.8130e+00
Epoch 5/10
10/10 - 2s - loss: 233.7046 - loglik: -2.3132e+02 - logprior: -2.3861e+00
Epoch 6/10
10/10 - 2s - loss: 227.5175 - loglik: -2.2647e+02 - logprior: -1.0438e+00
Epoch 7/10
10/10 - 2s - loss: 224.7113 - loglik: -2.2452e+02 - logprior: -1.9356e-01
Epoch 8/10
10/10 - 2s - loss: 223.2728 - loglik: -2.2371e+02 - logprior: 0.4330
Epoch 9/10
10/10 - 2s - loss: 222.2231 - loglik: -2.2307e+02 - logprior: 0.8472
Epoch 10/10
10/10 - 2s - loss: 221.0632 - loglik: -2.2223e+02 - logprior: 1.1667
Fitted a model with MAP estimate = -220.6370
expansions: [(13, 3), (17, 1), (29, 2), (30, 1), (41, 3), (57, 3), (61, 1), (62, 1), (63, 1), (68, 1), (79, 2), (80, 2), (81, 2), (82, 2), (89, 5)]
discards: [0]
Fitting a model of length 127 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 319.9017 - loglik: -2.1980e+02 - logprior: -1.0010e+02
Epoch 2/2
10/10 - 2s - loss: 241.2465 - loglik: -2.0335e+02 - logprior: -3.7895e+01
Fitted a model with MAP estimate = -227.2162
expansions: [(0, 2), (16, 1)]
discards: [  0  32  48  96 101 104 114 115]
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 273.9737 - loglik: -1.9834e+02 - logprior: -7.5633e+01
Epoch 2/2
10/10 - 2s - loss: 209.4174 - loglik: -1.9350e+02 - logprior: -1.5921e+01
Fitted a model with MAP estimate = -199.5420
expansions: []
discards: [0]
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 286.2272 - loglik: -1.9619e+02 - logprior: -9.0033e+01
Epoch 2/10
10/10 - 2s - loss: 217.4548 - loglik: -1.9455e+02 - logprior: -2.2906e+01
Epoch 3/10
10/10 - 2s - loss: 198.1272 - loglik: -1.9373e+02 - logprior: -4.4001e+00
Epoch 4/10
10/10 - 2s - loss: 191.1621 - loglik: -1.9331e+02 - logprior: 2.1503
Epoch 5/10
10/10 - 2s - loss: 187.9245 - loglik: -1.9326e+02 - logprior: 5.3392
Epoch 6/10
10/10 - 2s - loss: 186.1389 - loglik: -1.9333e+02 - logprior: 7.1877
Epoch 7/10
10/10 - 2s - loss: 185.0089 - loglik: -1.9337e+02 - logprior: 8.3614
Epoch 8/10
10/10 - 2s - loss: 184.2151 - loglik: -1.9342e+02 - logprior: 9.2098
Epoch 9/10
10/10 - 2s - loss: 183.5880 - loglik: -1.9351e+02 - logprior: 9.9206
Epoch 10/10
10/10 - 2s - loss: 183.0399 - loglik: -1.9361e+02 - logprior: 10.5704
Fitted a model with MAP estimate = -182.7551
Time for alignment: 61.8725
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 446.0379 - loglik: -3.4703e+02 - logprior: -9.9010e+01
Epoch 2/10
10/10 - 2s - loss: 330.6885 - loglik: -3.0791e+02 - logprior: -2.2777e+01
Epoch 3/10
10/10 - 2s - loss: 277.0834 - loglik: -2.6814e+02 - logprior: -8.9450e+00
Epoch 4/10
10/10 - 2s - loss: 247.0044 - loglik: -2.4219e+02 - logprior: -4.8130e+00
Epoch 5/10
10/10 - 2s - loss: 233.7046 - loglik: -2.3132e+02 - logprior: -2.3861e+00
Epoch 6/10
10/10 - 2s - loss: 227.5175 - loglik: -2.2647e+02 - logprior: -1.0438e+00
Epoch 7/10
10/10 - 2s - loss: 224.7113 - loglik: -2.2452e+02 - logprior: -1.9356e-01
Epoch 8/10
10/10 - 2s - loss: 223.2728 - loglik: -2.2371e+02 - logprior: 0.4330
Epoch 9/10
10/10 - 2s - loss: 222.2231 - loglik: -2.2307e+02 - logprior: 0.8472
Epoch 10/10
10/10 - 2s - loss: 221.0632 - loglik: -2.2223e+02 - logprior: 1.1667
Fitted a model with MAP estimate = -220.6370
expansions: [(13, 3), (17, 1), (29, 2), (30, 1), (41, 3), (57, 3), (61, 1), (62, 1), (63, 1), (68, 1), (79, 2), (80, 2), (81, 2), (82, 2), (89, 5)]
discards: [0]
Fitting a model of length 127 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 319.9017 - loglik: -2.1980e+02 - logprior: -1.0010e+02
Epoch 2/2
10/10 - 2s - loss: 241.2465 - loglik: -2.0335e+02 - logprior: -3.7895e+01
Fitted a model with MAP estimate = -227.2162
expansions: [(0, 2), (16, 1)]
discards: [  0  32  48  96 101 104 114 115]
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 273.9737 - loglik: -1.9834e+02 - logprior: -7.5633e+01
Epoch 2/2
10/10 - 2s - loss: 209.4174 - loglik: -1.9350e+02 - logprior: -1.5921e+01
Fitted a model with MAP estimate = -199.5419
expansions: []
discards: [0]
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 286.2272 - loglik: -1.9619e+02 - logprior: -9.0033e+01
Epoch 2/10
10/10 - 2s - loss: 217.4548 - loglik: -1.9455e+02 - logprior: -2.2906e+01
Epoch 3/10
10/10 - 2s - loss: 198.1272 - loglik: -1.9373e+02 - logprior: -4.4001e+00
Epoch 4/10
10/10 - 2s - loss: 191.1621 - loglik: -1.9331e+02 - logprior: 2.1503
Epoch 5/10
10/10 - 2s - loss: 187.9245 - loglik: -1.9326e+02 - logprior: 5.3392
Epoch 6/10
10/10 - 2s - loss: 186.1389 - loglik: -1.9333e+02 - logprior: 7.1877
Epoch 7/10
10/10 - 2s - loss: 185.0089 - loglik: -1.9337e+02 - logprior: 8.3614
Epoch 8/10
10/10 - 2s - loss: 184.2151 - loglik: -1.9342e+02 - logprior: 9.2098
Epoch 9/10
10/10 - 2s - loss: 183.5880 - loglik: -1.9351e+02 - logprior: 9.9206
Epoch 10/10
10/10 - 2s - loss: 183.0399 - loglik: -1.9361e+02 - logprior: 10.5704
Fitted a model with MAP estimate = -182.7551
Time for alignment: 63.4824
Computed alignments with likelihoods: ['-182.7551', '-182.7551', '-182.7551']
Best model has likelihood: -182.7551
SP score = 0.8901
Training of 3 independent models on file rub.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1fffc08bb0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20cb65d250>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2153288f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20cb73aa90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 169.0270 - loglik: -1.3559e+02 - logprior: -3.3437e+01
Epoch 2/10
10/10 - 1s - loss: 124.1681 - loglik: -1.1482e+02 - logprior: -9.3458e+00
Epoch 3/10
10/10 - 1s - loss: 98.2503 - loglik: -9.3220e+01 - logprior: -5.0300e+00
Epoch 4/10
10/10 - 1s - loss: 83.4773 - loglik: -7.9583e+01 - logprior: -3.8939e+00
Epoch 5/10
10/10 - 1s - loss: 78.4608 - loglik: -7.4940e+01 - logprior: -3.5209e+00
Epoch 6/10
10/10 - 1s - loss: 76.8589 - loglik: -7.3667e+01 - logprior: -3.1918e+00
Epoch 7/10
10/10 - 1s - loss: 76.1412 - loglik: -7.3376e+01 - logprior: -2.7651e+00
Epoch 8/10
10/10 - 1s - loss: 75.6388 - loglik: -7.3155e+01 - logprior: -2.4839e+00
Epoch 9/10
10/10 - 1s - loss: 75.6646 - loglik: -7.3290e+01 - logprior: -2.3743e+00
Fitted a model with MAP estimate = -75.4254
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 1), (24, 2), (26, 1)]
discards: []
Fitting a model of length 48 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 113.9006 - loglik: -7.1344e+01 - logprior: -4.2557e+01
Epoch 2/2
10/10 - 1s - loss: 77.6176 - loglik: -6.3989e+01 - logprior: -1.3629e+01
Fitted a model with MAP estimate = -70.7245
expansions: []
discards: [32]
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 90.8628 - loglik: -6.1062e+01 - logprior: -2.9801e+01
Epoch 2/2
10/10 - 1s - loss: 69.2205 - loglik: -6.0784e+01 - logprior: -8.4369e+00
Fitted a model with MAP estimate = -66.2542
expansions: []
discards: []
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 88.6215 - loglik: -6.0407e+01 - logprior: -2.8214e+01
Epoch 2/10
10/10 - 1s - loss: 68.8142 - loglik: -6.0829e+01 - logprior: -7.9852e+00
Epoch 3/10
10/10 - 1s - loss: 64.9982 - loglik: -6.1021e+01 - logprior: -3.9776e+00
Epoch 4/10
10/10 - 1s - loss: 63.6723 - loglik: -6.1205e+01 - logprior: -2.4676e+00
Epoch 5/10
10/10 - 1s - loss: 63.0309 - loglik: -6.1293e+01 - logprior: -1.7382e+00
Epoch 6/10
10/10 - 1s - loss: 62.6945 - loglik: -6.1319e+01 - logprior: -1.3754e+00
Epoch 7/10
10/10 - 1s - loss: 62.4904 - loglik: -6.1352e+01 - logprior: -1.1379e+00
Epoch 8/10
10/10 - 1s - loss: 62.3827 - loglik: -6.1452e+01 - logprior: -9.3057e-01
Epoch 9/10
10/10 - 1s - loss: 62.3219 - loglik: -6.1547e+01 - logprior: -7.7496e-01
Epoch 10/10
10/10 - 1s - loss: 62.2315 - loglik: -6.1552e+01 - logprior: -6.7979e-01
Fitted a model with MAP estimate = -62.1833
Time for alignment: 31.4611
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 169.0934 - loglik: -1.3566e+02 - logprior: -3.3437e+01
Epoch 2/10
10/10 - 1s - loss: 124.6048 - loglik: -1.1527e+02 - logprior: -9.3397e+00
Epoch 3/10
10/10 - 1s - loss: 99.6206 - loglik: -9.4638e+01 - logprior: -4.9830e+00
Epoch 4/10
10/10 - 1s - loss: 84.5056 - loglik: -8.0667e+01 - logprior: -3.8389e+00
Epoch 5/10
10/10 - 1s - loss: 79.2425 - loglik: -7.5758e+01 - logprior: -3.4845e+00
Epoch 6/10
10/10 - 1s - loss: 77.3033 - loglik: -7.4078e+01 - logprior: -3.2253e+00
Epoch 7/10
10/10 - 1s - loss: 76.3823 - loglik: -7.3580e+01 - logprior: -2.8027e+00
Epoch 8/10
10/10 - 1s - loss: 75.8864 - loglik: -7.3396e+01 - logprior: -2.4900e+00
Epoch 9/10
10/10 - 1s - loss: 75.5475 - loglik: -7.3174e+01 - logprior: -2.3731e+00
Epoch 10/10
10/10 - 1s - loss: 75.3970 - loglik: -7.3071e+01 - logprior: -2.3256e+00
Fitted a model with MAP estimate = -75.3365
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 1), (24, 2), (26, 1)]
discards: []
Fitting a model of length 48 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 113.9675 - loglik: -7.1144e+01 - logprior: -4.2824e+01
Epoch 2/2
10/10 - 1s - loss: 77.8763 - loglik: -6.4137e+01 - logprior: -1.3740e+01
Fitted a model with MAP estimate = -70.7907
expansions: []
discards: [32]
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 90.9548 - loglik: -6.1118e+01 - logprior: -2.9837e+01
Epoch 2/2
10/10 - 1s - loss: 69.1798 - loglik: -6.0746e+01 - logprior: -8.4342e+00
Fitted a model with MAP estimate = -66.2571
expansions: []
discards: []
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 88.6068 - loglik: -6.0399e+01 - logprior: -2.8208e+01
Epoch 2/10
10/10 - 1s - loss: 68.7293 - loglik: -6.0749e+01 - logprior: -7.9800e+00
Epoch 3/10
10/10 - 1s - loss: 64.9707 - loglik: -6.1000e+01 - logprior: -3.9704e+00
Epoch 4/10
10/10 - 1s - loss: 63.6566 - loglik: -6.1194e+01 - logprior: -2.4626e+00
Epoch 5/10
10/10 - 1s - loss: 63.1103 - loglik: -6.1385e+01 - logprior: -1.7252e+00
Epoch 6/10
10/10 - 1s - loss: 62.6546 - loglik: -6.1282e+01 - logprior: -1.3731e+00
Epoch 7/10
10/10 - 1s - loss: 62.5259 - loglik: -6.1394e+01 - logprior: -1.1321e+00
Epoch 8/10
10/10 - 1s - loss: 62.3897 - loglik: -6.1470e+01 - logprior: -9.1951e-01
Epoch 9/10
10/10 - 1s - loss: 62.3725 - loglik: -6.1602e+01 - logprior: -7.7021e-01
Epoch 10/10
10/10 - 1s - loss: 62.2094 - loglik: -6.1542e+01 - logprior: -6.6744e-01
Fitted a model with MAP estimate = -62.1761
Time for alignment: 32.5271
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 169.0303 - loglik: -1.3559e+02 - logprior: -3.3437e+01
Epoch 2/10
10/10 - 1s - loss: 124.2847 - loglik: -1.1493e+02 - logprior: -9.3545e+00
Epoch 3/10
10/10 - 1s - loss: 98.9619 - loglik: -9.3937e+01 - logprior: -5.0254e+00
Epoch 4/10
10/10 - 1s - loss: 84.0738 - loglik: -8.0208e+01 - logprior: -3.8663e+00
Epoch 5/10
10/10 - 1s - loss: 79.0998 - loglik: -7.5579e+01 - logprior: -3.5212e+00
Epoch 6/10
10/10 - 1s - loss: 77.1348 - loglik: -7.3929e+01 - logprior: -3.2054e+00
Epoch 7/10
10/10 - 1s - loss: 76.3844 - loglik: -7.3619e+01 - logprior: -2.7652e+00
Epoch 8/10
10/10 - 1s - loss: 75.8859 - loglik: -7.3407e+01 - logprior: -2.4794e+00
Epoch 9/10
10/10 - 1s - loss: 75.5317 - loglik: -7.3161e+01 - logprior: -2.3709e+00
Epoch 10/10
10/10 - 1s - loss: 75.4545 - loglik: -7.3136e+01 - logprior: -2.3182e+00
Fitted a model with MAP estimate = -75.3342
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 1), (24, 2), (26, 1)]
discards: []
Fitting a model of length 48 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 114.0603 - loglik: -7.1267e+01 - logprior: -4.2793e+01
Epoch 2/2
10/10 - 1s - loss: 77.8027 - loglik: -6.4070e+01 - logprior: -1.3732e+01
Fitted a model with MAP estimate = -70.7835
expansions: []
discards: [32]
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 90.9744 - loglik: -6.1136e+01 - logprior: -2.9839e+01
Epoch 2/2
10/10 - 1s - loss: 69.1665 - loglik: -6.0739e+01 - logprior: -8.4279e+00
Fitted a model with MAP estimate = -66.2593
expansions: []
discards: []
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 88.5870 - loglik: -6.0375e+01 - logprior: -2.8212e+01
Epoch 2/10
10/10 - 1s - loss: 68.7732 - loglik: -6.0794e+01 - logprior: -7.9791e+00
Epoch 3/10
10/10 - 1s - loss: 65.0375 - loglik: -6.1066e+01 - logprior: -3.9712e+00
Epoch 4/10
10/10 - 1s - loss: 63.6894 - loglik: -6.1224e+01 - logprior: -2.4656e+00
Epoch 5/10
10/10 - 1s - loss: 62.9839 - loglik: -6.1258e+01 - logprior: -1.7263e+00
Epoch 6/10
10/10 - 1s - loss: 62.7572 - loglik: -6.1385e+01 - logprior: -1.3721e+00
Epoch 7/10
10/10 - 1s - loss: 62.4451 - loglik: -6.1312e+01 - logprior: -1.1333e+00
Epoch 8/10
10/10 - 1s - loss: 62.3940 - loglik: -6.1471e+01 - logprior: -9.2291e-01
Epoch 9/10
10/10 - 1s - loss: 62.3306 - loglik: -6.1564e+01 - logprior: -7.6664e-01
Epoch 10/10
10/10 - 1s - loss: 62.1410 - loglik: -6.1468e+01 - logprior: -6.7269e-01
Fitted a model with MAP estimate = -62.1859
Time for alignment: 33.2320
Computed alignments with likelihoods: ['-62.1833', '-62.1761', '-62.1859']
Best model has likelihood: -62.1761
SP score = 0.9918
Training of 3 independent models on file cyclo.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20a9d33430>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f214ad25610>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f214ad255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f214ad25460>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 447.1508 - loglik: -4.4075e+02 - logprior: -6.4004e+00
Epoch 2/10
15/15 - 5s - loss: 360.9895 - loglik: -3.5956e+02 - logprior: -1.4343e+00
Epoch 3/10
15/15 - 5s - loss: 318.3232 - loglik: -3.1679e+02 - logprior: -1.5371e+00
Epoch 4/10
15/15 - 5s - loss: 307.8345 - loglik: -3.0621e+02 - logprior: -1.6207e+00
Epoch 5/10
15/15 - 5s - loss: 303.5459 - loglik: -3.0203e+02 - logprior: -1.5121e+00
Epoch 6/10
15/15 - 5s - loss: 301.8908 - loglik: -3.0036e+02 - logprior: -1.5288e+00
Epoch 7/10
15/15 - 5s - loss: 300.9326 - loglik: -2.9942e+02 - logprior: -1.5134e+00
Epoch 8/10
15/15 - 5s - loss: 299.2120 - loglik: -2.9771e+02 - logprior: -1.5039e+00
Epoch 9/10
15/15 - 5s - loss: 298.1352 - loglik: -2.9659e+02 - logprior: -1.5485e+00
Epoch 10/10
15/15 - 5s - loss: 297.1722 - loglik: -2.9560e+02 - logprior: -1.5717e+00
Fitted a model with MAP estimate = -297.2297
expansions: [(7, 3), (10, 1), (16, 1), (25, 1), (26, 2), (27, 1), (56, 1), (60, 1), (65, 2), (66, 2), (69, 1), (91, 2), (92, 2), (112, 1), (114, 2), (115, 1), (117, 3), (118, 1), (119, 2)]
discards: [0]
Fitting a model of length 157 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 300.6999 - loglik: -2.9389e+02 - logprior: -6.8059e+00
Epoch 2/2
15/15 - 7s - loss: 285.7047 - loglik: -2.8240e+02 - logprior: -3.3007e+00
Fitted a model with MAP estimate = -283.1556
expansions: [(0, 2)]
discards: [  0   7  32  77 109]
Fitting a model of length 154 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 10s - loss: 286.9533 - loglik: -2.8196e+02 - logprior: -4.9972e+00
Epoch 2/2
15/15 - 7s - loss: 281.7240 - loglik: -2.8030e+02 - logprior: -1.4194e+00
Fitted a model with MAP estimate = -280.4899
expansions: []
discards: [ 0 65]
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 11s - loss: 291.0678 - loglik: -2.8472e+02 - logprior: -6.3474e+00
Epoch 2/10
15/15 - 7s - loss: 284.7766 - loglik: -2.8265e+02 - logprior: -2.1268e+00
Epoch 3/10
15/15 - 7s - loss: 283.0484 - loglik: -2.8203e+02 - logprior: -1.0207e+00
Epoch 4/10
15/15 - 7s - loss: 280.4502 - loglik: -2.7958e+02 - logprior: -8.6520e-01
Epoch 5/10
15/15 - 7s - loss: 280.5738 - loglik: -2.7975e+02 - logprior: -8.2400e-01
Fitted a model with MAP estimate = -279.8717
Time for alignment: 158.5388
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 446.0643 - loglik: -4.3966e+02 - logprior: -6.3998e+00
Epoch 2/10
15/15 - 6s - loss: 367.3930 - loglik: -3.6595e+02 - logprior: -1.4427e+00
Epoch 3/10
15/15 - 5s - loss: 318.0531 - loglik: -3.1645e+02 - logprior: -1.6069e+00
Epoch 4/10
15/15 - 6s - loss: 305.6090 - loglik: -3.0390e+02 - logprior: -1.7108e+00
Epoch 5/10
15/15 - 6s - loss: 301.2847 - loglik: -2.9972e+02 - logprior: -1.5611e+00
Epoch 6/10
15/15 - 6s - loss: 300.5743 - loglik: -2.9901e+02 - logprior: -1.5667e+00
Epoch 7/10
15/15 - 6s - loss: 298.6815 - loglik: -2.9714e+02 - logprior: -1.5397e+00
Epoch 8/10
15/15 - 6s - loss: 297.6865 - loglik: -2.9615e+02 - logprior: -1.5391e+00
Epoch 9/10
15/15 - 6s - loss: 297.7397 - loglik: -2.9619e+02 - logprior: -1.5545e+00
Fitted a model with MAP estimate = -297.0919
expansions: [(7, 3), (10, 1), (16, 1), (24, 2), (25, 1), (26, 1), (55, 1), (59, 1), (64, 2), (65, 2), (69, 1), (91, 1), (92, 2), (105, 1), (112, 1), (114, 3), (116, 3), (119, 2)]
discards: [0]
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 11s - loss: 301.1602 - loglik: -2.9438e+02 - logprior: -6.7814e+00
Epoch 2/2
15/15 - 7s - loss: 285.6994 - loglik: -2.8245e+02 - logprior: -3.2502e+00
Fitted a model with MAP estimate = -284.0173
expansions: [(0, 2)]
discards: [  0   7  76  83 108]
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 287.7821 - loglik: -2.8280e+02 - logprior: -4.9828e+00
Epoch 2/2
15/15 - 6s - loss: 281.7785 - loglik: -2.8035e+02 - logprior: -1.4319e+00
Fitted a model with MAP estimate = -280.7510
expansions: []
discards: [ 0 65 66]
Fitting a model of length 150 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 292.7473 - loglik: -2.8640e+02 - logprior: -6.3428e+00
Epoch 2/10
15/15 - 6s - loss: 285.9898 - loglik: -2.8383e+02 - logprior: -2.1567e+00
Epoch 3/10
15/15 - 6s - loss: 284.8756 - loglik: -2.8386e+02 - logprior: -1.0189e+00
Epoch 4/10
15/15 - 6s - loss: 282.7675 - loglik: -2.8191e+02 - logprior: -8.5712e-01
Epoch 5/10
15/15 - 6s - loss: 281.9566 - loglik: -2.8115e+02 - logprior: -8.0425e-01
Epoch 6/10
15/15 - 6s - loss: 281.6323 - loglik: -2.8086e+02 - logprior: -7.7431e-01
Epoch 7/10
15/15 - 7s - loss: 281.0656 - loglik: -2.8033e+02 - logprior: -7.3332e-01
Epoch 8/10
15/15 - 7s - loss: 281.4825 - loglik: -2.8081e+02 - logprior: -6.7219e-01
Fitted a model with MAP estimate = -280.6932
Time for alignment: 169.8091
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 10s - loss: 446.1582 - loglik: -4.3976e+02 - logprior: -6.4009e+00
Epoch 2/10
15/15 - 5s - loss: 365.6439 - loglik: -3.6421e+02 - logprior: -1.4303e+00
Epoch 3/10
15/15 - 5s - loss: 319.0457 - loglik: -3.1746e+02 - logprior: -1.5848e+00
Epoch 4/10
15/15 - 5s - loss: 305.9505 - loglik: -3.0427e+02 - logprior: -1.6851e+00
Epoch 5/10
15/15 - 6s - loss: 303.1304 - loglik: -3.0158e+02 - logprior: -1.5479e+00
Epoch 6/10
15/15 - 6s - loss: 301.2197 - loglik: -2.9968e+02 - logprior: -1.5442e+00
Epoch 7/10
15/15 - 6s - loss: 300.3745 - loglik: -2.9885e+02 - logprior: -1.5290e+00
Epoch 8/10
15/15 - 6s - loss: 299.6066 - loglik: -2.9809e+02 - logprior: -1.5139e+00
Epoch 9/10
15/15 - 5s - loss: 298.4182 - loglik: -2.9690e+02 - logprior: -1.5184e+00
Epoch 10/10
15/15 - 5s - loss: 298.7856 - loglik: -2.9726e+02 - logprior: -1.5233e+00
Fitted a model with MAP estimate = -298.8025
expansions: [(5, 1), (7, 2), (10, 1), (16, 1), (24, 2), (25, 1), (26, 1), (55, 1), (65, 3), (69, 1), (91, 1), (92, 2), (112, 1), (114, 4), (116, 2), (118, 1), (119, 2)]
discards: [0]
Fitting a model of length 154 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 302.6854 - loglik: -2.9592e+02 - logprior: -6.7657e+00
Epoch 2/2
15/15 - 6s - loss: 288.3573 - loglik: -2.8511e+02 - logprior: -3.2486e+00
Fitted a model with MAP estimate = -286.2251
expansions: [(0, 2)]
discards: [  0   7  81 106]
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 10s - loss: 289.1991 - loglik: -2.8419e+02 - logprior: -5.0094e+00
Epoch 2/2
15/15 - 6s - loss: 285.0660 - loglik: -2.8363e+02 - logprior: -1.4377e+00
Fitted a model with MAP estimate = -283.2114
expansions: []
discards: [0]
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 292.0680 - loglik: -2.8567e+02 - logprior: -6.3930e+00
Epoch 2/10
15/15 - 6s - loss: 285.7377 - loglik: -2.8359e+02 - logprior: -2.1438e+00
Epoch 3/10
15/15 - 6s - loss: 283.5069 - loglik: -2.8244e+02 - logprior: -1.0663e+00
Epoch 4/10
15/15 - 6s - loss: 282.3446 - loglik: -2.8147e+02 - logprior: -8.7483e-01
Epoch 5/10
15/15 - 6s - loss: 282.8266 - loglik: -2.8198e+02 - logprior: -8.4646e-01
Fitted a model with MAP estimate = -281.3009
Time for alignment: 152.9676
Computed alignments with likelihoods: ['-279.8717', '-280.6932', '-281.3009']
Best model has likelihood: -279.8717
SP score = 0.9013
Training of 3 independent models on file gpdh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20a11e00a0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f202bf32640>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2178a4a310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202c804610>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 119 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 345.0903 - loglik: -3.4021e+02 - logprior: -4.8802e+00
Epoch 2/10
17/17 - 5s - loss: 254.0250 - loglik: -2.5235e+02 - logprior: -1.6714e+00
Epoch 3/10
17/17 - 6s - loss: 211.3834 - loglik: -2.0951e+02 - logprior: -1.8711e+00
Epoch 4/10
17/17 - 6s - loss: 203.8634 - loglik: -2.0203e+02 - logprior: -1.8348e+00
Epoch 5/10
17/17 - 6s - loss: 201.7133 - loglik: -1.9989e+02 - logprior: -1.8240e+00
Epoch 6/10
17/17 - 5s - loss: 201.3918 - loglik: -1.9951e+02 - logprior: -1.8862e+00
Epoch 7/10
17/17 - 5s - loss: 200.6346 - loglik: -1.9878e+02 - logprior: -1.8567e+00
Epoch 8/10
17/17 - 6s - loss: 200.6660 - loglik: -1.9873e+02 - logprior: -1.9376e+00
Fitted a model with MAP estimate = -200.3156
expansions: [(0, 2), (15, 1), (16, 2), (17, 3), (26, 2), (27, 2), (40, 1), (41, 1), (44, 1), (48, 1), (49, 1), (55, 1), (56, 2), (57, 1), (64, 1), (65, 1), (66, 1), (78, 1), (81, 1), (84, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Fitting a model of length 151 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 11s - loss: 195.5461 - loglik: -1.9050e+02 - logprior: -5.0440e+00
Epoch 2/2
17/17 - 7s - loss: 179.5459 - loglik: -1.7806e+02 - logprior: -1.4871e+00
Fitted a model with MAP estimate = -177.8169
expansions: []
discards: [ 20  35 140]
Fitting a model of length 148 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 11s - loss: 182.8963 - loglik: -1.7877e+02 - logprior: -4.1222e+00
Epoch 2/2
17/17 - 7s - loss: 177.5980 - loglik: -1.7609e+02 - logprior: -1.5041e+00
Fitted a model with MAP estimate = -177.4098
expansions: []
discards: []
Fitting a model of length 148 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 181.8784 - loglik: -1.7778e+02 - logprior: -4.0971e+00
Epoch 2/10
17/17 - 7s - loss: 178.0629 - loglik: -1.7663e+02 - logprior: -1.4367e+00
Epoch 3/10
17/17 - 7s - loss: 176.1067 - loglik: -1.7491e+02 - logprior: -1.2007e+00
Epoch 4/10
17/17 - 7s - loss: 178.3875 - loglik: -1.7724e+02 - logprior: -1.1524e+00
Fitted a model with MAP estimate = -176.2134
Time for alignment: 146.7600
Fitting a model of length 119 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 344.4736 - loglik: -3.3960e+02 - logprior: -4.8729e+00
Epoch 2/10
17/17 - 5s - loss: 258.0834 - loglik: -2.5642e+02 - logprior: -1.6666e+00
Epoch 3/10
17/17 - 5s - loss: 217.1910 - loglik: -2.1531e+02 - logprior: -1.8810e+00
Epoch 4/10
17/17 - 5s - loss: 205.7202 - loglik: -2.0386e+02 - logprior: -1.8605e+00
Epoch 5/10
17/17 - 6s - loss: 201.1440 - loglik: -1.9922e+02 - logprior: -1.9226e+00
Epoch 6/10
17/17 - 5s - loss: 201.2779 - loglik: -1.9931e+02 - logprior: -1.9646e+00
Fitted a model with MAP estimate = -200.4019
expansions: [(0, 2), (15, 1), (16, 4), (26, 2), (27, 2), (40, 1), (41, 1), (44, 1), (45, 1), (47, 1), (53, 1), (55, 1), (56, 1), (57, 1), (64, 1), (65, 1), (75, 1), (78, 1), (81, 1), (84, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Fitting a model of length 151 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 10s - loss: 194.7873 - loglik: -1.8976e+02 - logprior: -5.0297e+00
Epoch 2/2
17/17 - 7s - loss: 179.3519 - loglik: -1.7785e+02 - logprior: -1.4983e+00
Fitted a model with MAP estimate = -177.2113
expansions: []
discards: [ 34 140]
Fitting a model of length 149 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 11s - loss: 182.1867 - loglik: -1.7804e+02 - logprior: -4.1496e+00
Epoch 2/2
17/17 - 7s - loss: 177.9621 - loglik: -1.7646e+02 - logprior: -1.5023e+00
Fitted a model with MAP estimate = -177.0211
expansions: []
discards: []
Fitting a model of length 149 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 181.3528 - loglik: -1.7725e+02 - logprior: -4.1012e+00
Epoch 2/10
17/17 - 8s - loss: 177.5900 - loglik: -1.7615e+02 - logprior: -1.4413e+00
Epoch 3/10
17/17 - 8s - loss: 176.7472 - loglik: -1.7553e+02 - logprior: -1.2132e+00
Epoch 4/10
17/17 - 8s - loss: 176.8212 - loglik: -1.7566e+02 - logprior: -1.1575e+00
Fitted a model with MAP estimate = -175.7725
Time for alignment: 135.1634
Fitting a model of length 119 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 345.3507 - loglik: -3.4047e+02 - logprior: -4.8805e+00
Epoch 2/10
17/17 - 6s - loss: 259.8568 - loglik: -2.5819e+02 - logprior: -1.6705e+00
Epoch 3/10
17/17 - 6s - loss: 222.0968 - loglik: -2.2018e+02 - logprior: -1.9124e+00
Epoch 4/10
17/17 - 6s - loss: 205.8792 - loglik: -2.0390e+02 - logprior: -1.9765e+00
Epoch 5/10
17/17 - 6s - loss: 203.7127 - loglik: -2.0164e+02 - logprior: -2.0713e+00
Epoch 6/10
17/17 - 7s - loss: 202.9750 - loglik: -2.0084e+02 - logprior: -2.1400e+00
Epoch 7/10
17/17 - 6s - loss: 202.0246 - loglik: -1.9989e+02 - logprior: -2.1351e+00
Epoch 8/10
17/17 - 6s - loss: 201.6413 - loglik: -1.9952e+02 - logprior: -2.1249e+00
Epoch 9/10
17/17 - 6s - loss: 201.7095 - loglik: -1.9961e+02 - logprior: -2.1044e+00
Fitted a model with MAP estimate = -201.4099
expansions: [(0, 2), (15, 1), (16, 1), (17, 3), (26, 2), (27, 1), (28, 1), (29, 1), (42, 1), (45, 3), (48, 1), (49, 1), (55, 1), (56, 2), (57, 1), (64, 1), (65, 1), (75, 1), (78, 1), (82, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Fitting a model of length 152 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 195.5477 - loglik: -1.9034e+02 - logprior: -5.2039e+00
Epoch 2/2
17/17 - 7s - loss: 180.1911 - loglik: -1.7841e+02 - logprior: -1.7768e+00
Fitted a model with MAP estimate = -178.1154
expansions: []
discards: [ 22  34  59 141]
Fitting a model of length 148 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 10s - loss: 182.7272 - loglik: -1.7875e+02 - logprior: -3.9765e+00
Epoch 2/2
17/17 - 8s - loss: 178.1769 - loglik: -1.7676e+02 - logprior: -1.4182e+00
Fitted a model with MAP estimate = -177.3050
expansions: []
discards: []
Fitting a model of length 148 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 181.7029 - loglik: -1.7761e+02 - logprior: -4.0908e+00
Epoch 2/10
17/17 - 8s - loss: 177.7590 - loglik: -1.7631e+02 - logprior: -1.4454e+00
Epoch 3/10
17/17 - 8s - loss: 178.0692 - loglik: -1.7686e+02 - logprior: -1.2134e+00
Fitted a model with MAP estimate = -176.4893
Time for alignment: 154.8551
Computed alignments with likelihoods: ['-176.2134', '-175.7725', '-176.4893']
Best model has likelihood: -175.7725
SP score = 0.6287
Training of 3 independent models on file sodcu.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20cb609be0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20a9bda4c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f21181b1430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2153e1ea60>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 423.7281 - loglik: -3.9913e+02 - logprior: -2.4599e+01
Epoch 2/10
10/10 - 3s - loss: 364.4880 - loglik: -3.5888e+02 - logprior: -5.6071e+00
Epoch 3/10
10/10 - 3s - loss: 320.4182 - loglik: -3.1754e+02 - logprior: -2.8790e+00
Epoch 4/10
10/10 - 3s - loss: 291.4912 - loglik: -2.8922e+02 - logprior: -2.2729e+00
Epoch 5/10
10/10 - 3s - loss: 280.9212 - loglik: -2.7885e+02 - logprior: -2.0752e+00
Epoch 6/10
10/10 - 3s - loss: 276.6535 - loglik: -2.7474e+02 - logprior: -1.9174e+00
Epoch 7/10
10/10 - 3s - loss: 274.3370 - loglik: -2.7247e+02 - logprior: -1.8680e+00
Epoch 8/10
10/10 - 3s - loss: 273.9818 - loglik: -2.7216e+02 - logprior: -1.8249e+00
Epoch 9/10
10/10 - 3s - loss: 273.7230 - loglik: -2.7201e+02 - logprior: -1.7162e+00
Epoch 10/10
10/10 - 3s - loss: 273.3912 - loglik: -2.7172e+02 - logprior: -1.6703e+00
Fitted a model with MAP estimate = -273.1456
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (23, 1), (24, 4), (26, 1), (53, 1), (55, 2), (62, 1), (79, 1), (80, 2), (81, 1), (82, 2), (83, 1), (84, 1), (86, 1), (98, 1), (101, 2), (102, 1), (103, 1), (104, 1)]
discards: []
Fitting a model of length 146 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 289.2502 - loglik: -2.6937e+02 - logprior: -1.9879e+01
Epoch 2/2
10/10 - 3s - loss: 263.5499 - loglik: -2.5871e+02 - logprior: -4.8444e+00
Fitted a model with MAP estimate = -258.8030
expansions: []
discards: [ 0 31 67 96]
Fitting a model of length 142 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 282.4820 - loglik: -2.5908e+02 - logprior: -2.3406e+01
Epoch 2/2
10/10 - 3s - loss: 266.4785 - loglik: -2.5734e+02 - logprior: -9.1355e+00
Fitted a model with MAP estimate = -263.0980
expansions: [(0, 5)]
discards: [0]
Fitting a model of length 146 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 275.3281 - loglik: -2.5674e+02 - logprior: -1.8587e+01
Epoch 2/10
10/10 - 3s - loss: 258.7774 - loglik: -2.5473e+02 - logprior: -4.0491e+00
Epoch 3/10
10/10 - 3s - loss: 254.9983 - loglik: -2.5383e+02 - logprior: -1.1687e+00
Epoch 4/10
10/10 - 3s - loss: 253.5293 - loglik: -2.5341e+02 - logprior: -1.1474e-01
Epoch 5/10
10/10 - 3s - loss: 252.2797 - loglik: -2.5263e+02 - logprior: 0.3534
Epoch 6/10
10/10 - 3s - loss: 251.1012 - loglik: -2.5168e+02 - logprior: 0.5776
Epoch 7/10
10/10 - 3s - loss: 251.4315 - loglik: -2.5216e+02 - logprior: 0.7318
Fitted a model with MAP estimate = -250.7706
Time for alignment: 84.5557
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 423.6928 - loglik: -3.9910e+02 - logprior: -2.4598e+01
Epoch 2/10
10/10 - 2s - loss: 365.0883 - loglik: -3.5950e+02 - logprior: -5.5904e+00
Epoch 3/10
10/10 - 2s - loss: 320.7320 - loglik: -3.1795e+02 - logprior: -2.7832e+00
Epoch 4/10
10/10 - 3s - loss: 291.3889 - loglik: -2.8916e+02 - logprior: -2.2305e+00
Epoch 5/10
10/10 - 3s - loss: 280.6968 - loglik: -2.7867e+02 - logprior: -2.0234e+00
Epoch 6/10
10/10 - 3s - loss: 276.0954 - loglik: -2.7423e+02 - logprior: -1.8704e+00
Epoch 7/10
10/10 - 2s - loss: 274.8892 - loglik: -2.7309e+02 - logprior: -1.7977e+00
Epoch 8/10
10/10 - 2s - loss: 273.4173 - loglik: -2.7166e+02 - logprior: -1.7596e+00
Epoch 9/10
10/10 - 2s - loss: 274.1853 - loglik: -2.7251e+02 - logprior: -1.6784e+00
Fitted a model with MAP estimate = -273.1862
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (23, 1), (24, 4), (26, 1), (53, 1), (60, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (87, 1), (99, 1), (101, 4), (102, 1), (103, 1), (104, 1)]
discards: []
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 288.8453 - loglik: -2.6899e+02 - logprior: -1.9853e+01
Epoch 2/2
10/10 - 3s - loss: 262.2327 - loglik: -2.5745e+02 - logprior: -4.7838e+00
Fitted a model with MAP estimate = -257.4358
expansions: []
discards: [  0  31 125 126]
Fitting a model of length 141 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 283.2419 - loglik: -2.5986e+02 - logprior: -2.3380e+01
Epoch 2/2
10/10 - 3s - loss: 266.0434 - loglik: -2.5693e+02 - logprior: -9.1160e+00
Fitted a model with MAP estimate = -263.2140
expansions: [(0, 5)]
discards: [0]
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 274.7591 - loglik: -2.5621e+02 - logprior: -1.8553e+01
Epoch 2/10
10/10 - 3s - loss: 259.7141 - loglik: -2.5568e+02 - logprior: -4.0358e+00
Epoch 3/10
10/10 - 3s - loss: 255.0915 - loglik: -2.5393e+02 - logprior: -1.1631e+00
Epoch 4/10
10/10 - 3s - loss: 253.4891 - loglik: -2.5336e+02 - logprior: -1.2954e-01
Epoch 5/10
10/10 - 3s - loss: 251.7509 - loglik: -2.5210e+02 - logprior: 0.3482
Epoch 6/10
10/10 - 4s - loss: 252.2454 - loglik: -2.5281e+02 - logprior: 0.5696
Fitted a model with MAP estimate = -251.1822
Time for alignment: 78.8581
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 423.5458 - loglik: -3.9895e+02 - logprior: -2.4597e+01
Epoch 2/10
10/10 - 3s - loss: 364.9637 - loglik: -3.5937e+02 - logprior: -5.5937e+00
Epoch 3/10
10/10 - 3s - loss: 318.9701 - loglik: -3.1617e+02 - logprior: -2.7955e+00
Epoch 4/10
10/10 - 2s - loss: 290.6815 - loglik: -2.8840e+02 - logprior: -2.2837e+00
Epoch 5/10
10/10 - 3s - loss: 279.3444 - loglik: -2.7713e+02 - logprior: -2.2119e+00
Epoch 6/10
10/10 - 3s - loss: 276.1150 - loglik: -2.7399e+02 - logprior: -2.1269e+00
Epoch 7/10
10/10 - 3s - loss: 272.9210 - loglik: -2.7085e+02 - logprior: -2.0709e+00
Epoch 8/10
10/10 - 3s - loss: 272.2390 - loglik: -2.7024e+02 - logprior: -2.0006e+00
Epoch 9/10
10/10 - 3s - loss: 271.3323 - loglik: -2.6941e+02 - logprior: -1.9227e+00
Epoch 10/10
10/10 - 3s - loss: 271.3856 - loglik: -2.6947e+02 - logprior: -1.9174e+00
Fitted a model with MAP estimate = -270.9420
expansions: [(8, 1), (9, 1), (10, 1), (13, 1), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (55, 1), (62, 1), (79, 2), (80, 1), (81, 1), (82, 2), (85, 1), (87, 1), (99, 1), (101, 4), (102, 1), (103, 1), (104, 1)]
discards: []
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 288.6838 - loglik: -2.6891e+02 - logprior: -1.9774e+01
Epoch 2/2
10/10 - 3s - loss: 261.5469 - loglik: -2.5685e+02 - logprior: -4.7001e+00
Fitted a model with MAP estimate = -257.4719
expansions: []
discards: [  0  92 124 125]
Fitting a model of length 140 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 283.2144 - loglik: -2.5983e+02 - logprior: -2.3381e+01
Epoch 2/2
10/10 - 3s - loss: 266.5472 - loglik: -2.5740e+02 - logprior: -9.1424e+00
Fitted a model with MAP estimate = -263.6732
expansions: [(0, 6)]
discards: [0]
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 275.2305 - loglik: -2.5659e+02 - logprior: -1.8637e+01
Epoch 2/10
10/10 - 3s - loss: 259.8466 - loglik: -2.5578e+02 - logprior: -4.0715e+00
Epoch 3/10
10/10 - 3s - loss: 256.0623 - loglik: -2.5487e+02 - logprior: -1.1897e+00
Epoch 4/10
10/10 - 3s - loss: 253.0914 - loglik: -2.5294e+02 - logprior: -1.4980e-01
Epoch 5/10
10/10 - 3s - loss: 252.2620 - loglik: -2.5256e+02 - logprior: 0.2981
Epoch 6/10
10/10 - 3s - loss: 251.6552 - loglik: -2.5213e+02 - logprior: 0.4774
Epoch 7/10
10/10 - 3s - loss: 251.9365 - loglik: -2.5259e+02 - logprior: 0.6517
Fitted a model with MAP estimate = -251.1397
Time for alignment: 82.2489
Computed alignments with likelihoods: ['-250.7706', '-251.1822', '-251.1397']
Best model has likelihood: -250.7706
SP score = 0.8864
Training of 3 independent models on file DEATH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20988240a0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20a1056940>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2087945d30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2087fcd280>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 273.9638 - loglik: -2.3291e+02 - logprior: -4.1058e+01
Epoch 2/10
10/10 - 1s - loss: 232.4697 - loglik: -2.2212e+02 - logprior: -1.0346e+01
Epoch 3/10
10/10 - 1s - loss: 215.1324 - loglik: -2.1053e+02 - logprior: -4.6050e+00
Epoch 4/10
10/10 - 1s - loss: 203.8239 - loglik: -2.0113e+02 - logprior: -2.6972e+00
Epoch 5/10
10/10 - 1s - loss: 198.4304 - loglik: -1.9640e+02 - logprior: -2.0266e+00
Epoch 6/10
10/10 - 1s - loss: 195.4943 - loglik: -1.9363e+02 - logprior: -1.8645e+00
Epoch 7/10
10/10 - 1s - loss: 193.3584 - loglik: -1.9193e+02 - logprior: -1.4253e+00
Epoch 8/10
10/10 - 1s - loss: 192.1636 - loglik: -1.9126e+02 - logprior: -9.0607e-01
Epoch 9/10
10/10 - 1s - loss: 191.8580 - loglik: -1.9114e+02 - logprior: -7.1408e-01
Epoch 10/10
10/10 - 1s - loss: 191.4310 - loglik: -1.9080e+02 - logprior: -6.2774e-01
Fitted a model with MAP estimate = -191.2645
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (31, 2), (43, 2), (44, 2), (55, 1), (65, 4)]
discards: []
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 244.1500 - loglik: -1.9433e+02 - logprior: -4.9819e+01
Epoch 2/2
10/10 - 1s - loss: 202.2895 - loglik: -1.8780e+02 - logprior: -1.4490e+01
Fitted a model with MAP estimate = -194.8909
expansions: [(38, 1)]
discards: [ 0 52]
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 229.2981 - loglik: -1.8724e+02 - logprior: -4.2057e+01
Epoch 2/2
10/10 - 1s - loss: 202.2174 - loglik: -1.8623e+02 - logprior: -1.5985e+01
Fitted a model with MAP estimate = -197.6654
expansions: []
discards: [0 1]
Fitting a model of length 80 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 223.8246 - loglik: -1.8617e+02 - logprior: -3.7660e+01
Epoch 2/10
10/10 - 1s - loss: 195.0536 - loglik: -1.8547e+02 - logprior: -9.5819e+00
Epoch 3/10
10/10 - 1s - loss: 188.3781 - loglik: -1.8516e+02 - logprior: -3.2152e+00
Epoch 4/10
10/10 - 1s - loss: 185.8518 - loglik: -1.8487e+02 - logprior: -9.8008e-01
Epoch 5/10
10/10 - 1s - loss: 184.4947 - loglik: -1.8455e+02 - logprior: 0.0553
Epoch 6/10
10/10 - 1s - loss: 184.0822 - loglik: -1.8465e+02 - logprior: 0.5685
Epoch 7/10
10/10 - 1s - loss: 183.5923 - loglik: -1.8448e+02 - logprior: 0.8859
Epoch 8/10
10/10 - 1s - loss: 183.2793 - loglik: -1.8445e+02 - logprior: 1.1744
Epoch 9/10
10/10 - 1s - loss: 183.2376 - loglik: -1.8465e+02 - logprior: 1.4169
Epoch 10/10
10/10 - 1s - loss: 183.0763 - loglik: -1.8466e+02 - logprior: 1.5876
Fitted a model with MAP estimate = -182.9627
Time for alignment: 48.9137
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 273.8628 - loglik: -2.3280e+02 - logprior: -4.1059e+01
Epoch 2/10
10/10 - 1s - loss: 232.3737 - loglik: -2.2203e+02 - logprior: -1.0347e+01
Epoch 3/10
10/10 - 1s - loss: 214.8828 - loglik: -2.1029e+02 - logprior: -4.5972e+00
Epoch 4/10
10/10 - 1s - loss: 203.5990 - loglik: -2.0089e+02 - logprior: -2.7090e+00
Epoch 5/10
10/10 - 1s - loss: 198.4415 - loglik: -1.9641e+02 - logprior: -2.0323e+00
Epoch 6/10
10/10 - 1s - loss: 194.9008 - loglik: -1.9307e+02 - logprior: -1.8291e+00
Epoch 7/10
10/10 - 1s - loss: 193.3240 - loglik: -1.9165e+02 - logprior: -1.6732e+00
Epoch 8/10
10/10 - 1s - loss: 192.3723 - loglik: -1.9115e+02 - logprior: -1.2225e+00
Epoch 9/10
10/10 - 1s - loss: 192.0645 - loglik: -1.9114e+02 - logprior: -9.2724e-01
Epoch 10/10
10/10 - 1s - loss: 191.7008 - loglik: -1.9086e+02 - logprior: -8.4420e-01
Fitted a model with MAP estimate = -191.4962
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (28, 1), (31, 2), (43, 2), (45, 1), (49, 1), (55, 1), (65, 4)]
discards: []
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 244.4538 - loglik: -1.9462e+02 - logprior: -4.9829e+01
Epoch 2/2
10/10 - 1s - loss: 202.1313 - loglik: -1.8766e+02 - logprior: -1.4475e+01
Fitted a model with MAP estimate = -194.4423
expansions: [(39, 1)]
discards: [ 0 53]
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 228.7779 - loglik: -1.8683e+02 - logprior: -4.1953e+01
Epoch 2/2
10/10 - 1s - loss: 201.7878 - loglik: -1.8586e+02 - logprior: -1.5931e+01
Fitted a model with MAP estimate = -197.3885
expansions: []
discards: [0 1]
Fitting a model of length 81 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 223.5311 - loglik: -1.8592e+02 - logprior: -3.7610e+01
Epoch 2/10
10/10 - 1s - loss: 194.8208 - loglik: -1.8530e+02 - logprior: -9.5180e+00
Epoch 3/10
10/10 - 1s - loss: 188.0739 - loglik: -1.8492e+02 - logprior: -3.1547e+00
Epoch 4/10
10/10 - 1s - loss: 185.4894 - loglik: -1.8457e+02 - logprior: -9.1739e-01
Epoch 5/10
10/10 - 1s - loss: 184.3266 - loglik: -1.8446e+02 - logprior: 0.1290
Epoch 6/10
10/10 - 1s - loss: 183.6586 - loglik: -1.8429e+02 - logprior: 0.6341
Epoch 7/10
10/10 - 1s - loss: 183.1601 - loglik: -1.8411e+02 - logprior: 0.9521
Epoch 8/10
10/10 - 1s - loss: 183.2004 - loglik: -1.8444e+02 - logprior: 1.2361
Fitted a model with MAP estimate = -182.9170
Time for alignment: 45.7527
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 273.9948 - loglik: -2.3294e+02 - logprior: -4.1057e+01
Epoch 2/10
10/10 - 1s - loss: 232.2087 - loglik: -2.2187e+02 - logprior: -1.0343e+01
Epoch 3/10
10/10 - 1s - loss: 214.9220 - loglik: -2.1033e+02 - logprior: -4.5920e+00
Epoch 4/10
10/10 - 1s - loss: 203.0943 - loglik: -2.0035e+02 - logprior: -2.7444e+00
Epoch 5/10
10/10 - 1s - loss: 196.9829 - loglik: -1.9477e+02 - logprior: -2.2130e+00
Epoch 6/10
10/10 - 1s - loss: 193.9861 - loglik: -1.9197e+02 - logprior: -2.0154e+00
Epoch 7/10
10/10 - 1s - loss: 192.6004 - loglik: -1.9111e+02 - logprior: -1.4866e+00
Epoch 8/10
10/10 - 1s - loss: 192.0040 - loglik: -1.9100e+02 - logprior: -1.0084e+00
Epoch 9/10
10/10 - 1s - loss: 191.4832 - loglik: -1.9062e+02 - logprior: -8.6595e-01
Epoch 10/10
10/10 - 1s - loss: 191.1909 - loglik: -1.9040e+02 - logprior: -7.8603e-01
Fitted a model with MAP estimate = -191.1224
expansions: [(0, 3), (6, 1), (22, 1), (25, 2), (27, 2), (31, 2), (43, 1), (45, 1), (49, 1), (59, 1), (65, 4)]
discards: []
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 244.4666 - loglik: -1.9464e+02 - logprior: -4.9832e+01
Epoch 2/2
10/10 - 1s - loss: 202.2653 - loglik: -1.8779e+02 - logprior: -1.4477e+01
Fitted a model with MAP estimate = -194.6619
expansions: []
discards: [ 0 31]
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 229.3123 - loglik: -1.8734e+02 - logprior: -4.1967e+01
Epoch 2/2
10/10 - 1s - loss: 202.2442 - loglik: -1.8627e+02 - logprior: -1.5976e+01
Fitted a model with MAP estimate = -197.7365
expansions: []
discards: [0 1]
Fitting a model of length 80 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 223.8755 - loglik: -1.8622e+02 - logprior: -3.7654e+01
Epoch 2/10
10/10 - 1s - loss: 195.0705 - loglik: -1.8550e+02 - logprior: -9.5694e+00
Epoch 3/10
10/10 - 1s - loss: 188.5905 - loglik: -1.8538e+02 - logprior: -3.2055e+00
Epoch 4/10
10/10 - 1s - loss: 186.0036 - loglik: -1.8504e+02 - logprior: -9.6838e-01
Epoch 5/10
10/10 - 1s - loss: 184.6919 - loglik: -1.8476e+02 - logprior: 0.0710
Epoch 6/10
10/10 - 1s - loss: 184.0931 - loglik: -1.8467e+02 - logprior: 0.5775
Epoch 7/10
10/10 - 1s - loss: 183.7756 - loglik: -1.8467e+02 - logprior: 0.8898
Epoch 8/10
10/10 - 1s - loss: 183.2133 - loglik: -1.8439e+02 - logprior: 1.1770
Epoch 9/10
10/10 - 1s - loss: 183.4277 - loglik: -1.8485e+02 - logprior: 1.4176
Fitted a model with MAP estimate = -183.1547
Time for alignment: 47.8209
Computed alignments with likelihoods: ['-182.9627', '-182.9170', '-183.1547']
Best model has likelihood: -182.9170
SP score = 0.8381
Training of 3 independent models on file aat.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f210744b100>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2098cb6b20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20235c64f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2139e36190>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 47s - loss: 953.8530 - loglik: -9.5213e+02 - logprior: -1.7188e+00
Epoch 2/10
43/43 - 44s - loss: 838.3530 - loglik: -8.3668e+02 - logprior: -1.6699e+00
Epoch 3/10
43/43 - 44s - loss: 826.9574 - loglik: -8.2516e+02 - logprior: -1.7990e+00
Epoch 4/10
43/43 - 48s - loss: 820.9396 - loglik: -8.1910e+02 - logprior: -1.8406e+00
Epoch 5/10
43/43 - 47s - loss: 818.9801 - loglik: -8.1697e+02 - logprior: -2.0071e+00
Epoch 6/10
43/43 - 41s - loss: 815.0344 - loglik: -8.1282e+02 - logprior: -2.2161e+00
Epoch 7/10
43/43 - 41s - loss: 817.1402 - loglik: -8.1471e+02 - logprior: -2.4264e+00
Fitted a model with MAP estimate = -828.9843
expansions: [(0, 2), (16, 1), (20, 1), (21, 1), (22, 2), (23, 1), (24, 2), (30, 1), (39, 1), (40, 1), (41, 1), (42, 1), (45, 1), (46, 1), (57, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 1), (86, 2), (90, 1), (91, 1), (93, 1), (95, 1), (97, 1), (101, 1), (103, 2), (119, 1), (122, 1), (123, 1), (124, 1), (130, 1), (142, 1), (148, 2), (153, 2), (154, 1), (155, 3), (157, 2), (167, 1), (180, 1), (183, 1), (184, 1), (185, 1), (187, 1), (188, 1), (204, 1), (205, 3), (206, 2), (207, 1), (210, 1), (219, 1), (220, 1), (221, 1), (222, 1), (225, 1), (226, 2), (236, 1), (238, 2), (242, 1), (245, 1), (250, 1), (257, 1), (260, 3), (269, 3), (270, 2), (271, 2)]
discards: []
Fitting a model of length 368 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 67s - loss: 814.1548 - loglik: -8.1189e+02 - logprior: -2.2605e+00
Epoch 2/2
43/43 - 72s - loss: 793.4500 - loglik: -7.9259e+02 - logprior: -8.6052e-01
Fitted a model with MAP estimate = -794.5728
expansions: []
discards: [  0  28 110 135 205 263 264 265 291 296 340 353 355 358]
Fitting a model of length 354 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 83s - loss: 799.7924 - loglik: -7.9782e+02 - logprior: -1.9727e+00
Epoch 2/2
43/43 - 75s - loss: 795.2357 - loglik: -7.9474e+02 - logprior: -5.0006e-01
Fitted a model with MAP estimate = -795.1289
expansions: [(0, 1)]
discards: [0]
Fitting a model of length 354 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 106s - loss: 780.0008 - loglik: -7.7929e+02 - logprior: -7.0828e-01
Epoch 2/10
61/61 - 102s - loss: 772.6351 - loglik: -7.7216e+02 - logprior: -4.7917e-01
Epoch 3/10
61/61 - 96s - loss: 770.7652 - loglik: -7.7030e+02 - logprior: -4.6891e-01
Epoch 4/10
61/61 - 95s - loss: 767.7086 - loglik: -7.6721e+02 - logprior: -5.0300e-01
Epoch 5/10
61/61 - 91s - loss: 765.1993 - loglik: -7.6466e+02 - logprior: -5.3570e-01
Epoch 6/10
61/61 - 94s - loss: 760.2302 - loglik: -7.5952e+02 - logprior: -7.0620e-01
Epoch 7/10
61/61 - 90s - loss: 759.2565 - loglik: -7.5846e+02 - logprior: -7.9770e-01
Epoch 8/10
61/61 - 106s - loss: 757.7487 - loglik: -7.5678e+02 - logprior: -9.7120e-01
Epoch 9/10
61/61 - 105s - loss: 757.0839 - loglik: -7.5598e+02 - logprior: -1.1014e+00
Epoch 10/10
61/61 - 77s - loss: 756.7821 - loglik: -7.5564e+02 - logprior: -1.1425e+00
Fitted a model with MAP estimate = -753.6935
Time for alignment: 2090.4431
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 39s - loss: 953.2137 - loglik: -9.5148e+02 - logprior: -1.7349e+00
Epoch 2/10
43/43 - 37s - loss: 840.1700 - loglik: -8.3840e+02 - logprior: -1.7713e+00
Epoch 3/10
43/43 - 38s - loss: 826.7437 - loglik: -8.2486e+02 - logprior: -1.8816e+00
Epoch 4/10
43/43 - 39s - loss: 823.1675 - loglik: -8.2121e+02 - logprior: -1.9533e+00
Epoch 5/10
43/43 - 40s - loss: 820.8352 - loglik: -8.1873e+02 - logprior: -2.1071e+00
Epoch 6/10
43/43 - 41s - loss: 817.1856 - loglik: -8.1488e+02 - logprior: -2.3056e+00
Epoch 7/10
43/43 - 42s - loss: 817.0979 - loglik: -8.1455e+02 - logprior: -2.5435e+00
Epoch 8/10
43/43 - 42s - loss: 816.7334 - loglik: -8.1400e+02 - logprior: -2.7348e+00
Epoch 9/10
43/43 - 42s - loss: 816.7671 - loglik: -8.1383e+02 - logprior: -2.9370e+00
Fitted a model with MAP estimate = -830.7319
expansions: [(8, 1), (13, 1), (16, 1), (20, 2), (21, 2), (22, 1), (23, 1), (24, 2), (29, 1), (39, 1), (40, 1), (41, 1), (42, 1), (45, 1), (46, 1), (56, 1), (59, 1), (60, 1), (61, 2), (80, 1), (81, 1), (82, 1), (83, 2), (90, 1), (91, 2), (92, 1), (95, 1), (97, 2), (98, 1), (103, 3), (119, 1), (122, 1), (125, 1), (128, 2), (130, 1), (134, 1), (144, 1), (147, 1), (149, 1), (152, 2), (155, 2), (157, 2), (181, 1), (183, 1), (185, 2), (186, 2), (187, 1), (188, 2), (200, 1), (204, 1), (205, 3), (206, 2), (207, 1), (210, 1), (220, 2), (221, 1), (222, 2), (227, 2), (237, 1), (239, 3), (240, 1), (245, 1), (246, 1), (250, 1), (261, 2), (262, 1), (269, 3), (270, 2), (271, 2)]
discards: [0]
Fitting a model of length 376 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 74s - loss: 821.6044 - loglik: -8.1893e+02 - logprior: -2.6761e+00
Epoch 2/2
43/43 - 69s - loss: 795.8614 - loglik: -7.9461e+02 - logprior: -1.2473e+00
Fitted a model with MAP estimate = -794.5748
expansions: [(0, 2)]
discards: [  0  25  26  81 108 122 130 138 139 169 209 245 246 247 270 271 272 298
 304 320 362 364 366]
Fitting a model of length 355 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 57s - loss: 799.1081 - loglik: -7.9794e+02 - logprior: -1.1646e+00
Epoch 2/2
43/43 - 53s - loss: 793.8719 - loglik: -7.9348e+02 - logprior: -3.9023e-01
Fitted a model with MAP estimate = -793.1283
expansions: []
discards: [  0   1 283 303]
Fitting a model of length 351 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 87s - loss: 779.5493 - loglik: -7.7862e+02 - logprior: -9.3320e-01
Epoch 2/10
61/61 - 87s - loss: 774.7964 - loglik: -7.7440e+02 - logprior: -3.9315e-01
Epoch 3/10
61/61 - 92s - loss: 771.3798 - loglik: -7.7103e+02 - logprior: -3.4662e-01
Epoch 4/10
61/61 - 90s - loss: 768.1649 - loglik: -7.6779e+02 - logprior: -3.7272e-01
Epoch 5/10
61/61 - 89s - loss: 765.3782 - loglik: -7.6497e+02 - logprior: -4.1051e-01
Epoch 6/10
61/61 - 96s - loss: 760.5822 - loglik: -7.5999e+02 - logprior: -5.9395e-01
Epoch 7/10
61/61 - 93s - loss: 759.5660 - loglik: -7.5886e+02 - logprior: -7.1030e-01
Epoch 8/10
61/61 - 89s - loss: 758.3640 - loglik: -7.5754e+02 - logprior: -8.2278e-01
Epoch 9/10
61/61 - 91s - loss: 756.4404 - loglik: -7.5552e+02 - logprior: -9.1638e-01
Epoch 10/10
61/61 - 76s - loss: 756.9003 - loglik: -7.5593e+02 - logprior: -9.6639e-01
Fitted a model with MAP estimate = -753.7441
Time for alignment: 1916.2981
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 39s - loss: 952.1745 - loglik: -9.5043e+02 - logprior: -1.7430e+00
Epoch 2/10
43/43 - 37s - loss: 837.3536 - loglik: -8.3558e+02 - logprior: -1.7707e+00
Epoch 3/10
43/43 - 39s - loss: 824.5405 - loglik: -8.2269e+02 - logprior: -1.8543e+00
Epoch 4/10
43/43 - 43s - loss: 821.6842 - loglik: -8.1974e+02 - logprior: -1.9443e+00
Epoch 5/10
43/43 - 46s - loss: 820.6246 - loglik: -8.1852e+02 - logprior: -2.1043e+00
Epoch 6/10
43/43 - 48s - loss: 816.2815 - loglik: -8.1396e+02 - logprior: -2.3175e+00
Epoch 7/10
43/43 - 49s - loss: 815.2772 - loglik: -8.1276e+02 - logprior: -2.5147e+00
Epoch 8/10
43/43 - 51s - loss: 815.4289 - loglik: -8.1270e+02 - logprior: -2.7260e+00
Fitted a model with MAP estimate = -828.9752
expansions: [(8, 1), (13, 1), (16, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 2), (30, 1), (40, 1), (42, 1), (43, 1), (46, 1), (47, 1), (57, 1), (60, 1), (61, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 1), (86, 2), (90, 1), (91, 1), (93, 1), (95, 1), (96, 1), (101, 1), (120, 1), (121, 1), (122, 1), (125, 1), (131, 1), (132, 2), (142, 1), (148, 1), (150, 1), (153, 2), (154, 1), (155, 3), (157, 2), (181, 1), (183, 2), (185, 2), (186, 2), (187, 1), (204, 2), (205, 3), (206, 2), (207, 1), (210, 1), (219, 1), (220, 1), (221, 1), (222, 1), (225, 1), (226, 2), (239, 3), (240, 1), (242, 1), (245, 1), (248, 1), (249, 1), (260, 2), (269, 3), (270, 2), (271, 2)]
discards: [0]
Fitting a model of length 368 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 70s - loss: 815.5063 - loglik: -8.1295e+02 - logprior: -2.5540e+00
Epoch 2/2
43/43 - 75s - loss: 795.5770 - loglik: -7.9456e+02 - logprior: -1.0122e+00
Fitted a model with MAP estimate = -795.3898
expansions: [(0, 2)]
discards: [  0 108 167 203 233 239 263 264 265 291 296 312 353 355 358]
Fitting a model of length 355 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 70s - loss: 798.3466 - loglik: -7.9722e+02 - logprior: -1.1297e+00
Epoch 2/2
43/43 - 72s - loss: 795.0411 - loglik: -7.9459e+02 - logprior: -4.4925e-01
Fitted a model with MAP estimate = -793.9552
expansions: []
discards: [0 1]
Fitting a model of length 353 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 106s - loss: 780.0092 - loglik: -7.7906e+02 - logprior: -9.5263e-01
Epoch 2/10
61/61 - 111s - loss: 774.0314 - loglik: -7.7363e+02 - logprior: -4.0465e-01
Epoch 3/10
61/61 - 111s - loss: 770.5392 - loglik: -7.7015e+02 - logprior: -3.9253e-01
Epoch 4/10
61/61 - 108s - loss: 766.8402 - loglik: -7.6644e+02 - logprior: -3.9643e-01
Epoch 5/10
61/61 - 105s - loss: 765.8511 - loglik: -7.6543e+02 - logprior: -4.1903e-01
Epoch 6/10
61/61 - 108s - loss: 760.9594 - loglik: -7.6035e+02 - logprior: -6.0499e-01
Epoch 7/10
61/61 - 106s - loss: 760.8336 - loglik: -7.6008e+02 - logprior: -7.5839e-01
Epoch 8/10
61/61 - 90s - loss: 757.6899 - loglik: -7.5684e+02 - logprior: -8.5284e-01
Epoch 9/10
61/61 - 103s - loss: 757.8073 - loglik: -7.5684e+02 - logprior: -9.6932e-01
Fitted a model with MAP estimate = -755.3944
Time for alignment: 2051.3507
Computed alignments with likelihoods: ['-753.6935', '-753.7441', '-755.3944']
Best model has likelihood: -753.6935
SP score = 0.8133
Training of 3 independent models on file ins.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f21072d75b0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f21072d7280>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f21072d75e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20232ff610>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 253.9558 - loglik: -1.9324e+02 - logprior: -6.0717e+01
Epoch 2/10
10/10 - 2s - loss: 188.1105 - loglik: -1.7162e+02 - logprior: -1.6494e+01
Epoch 3/10
10/10 - 2s - loss: 162.2480 - loglik: -1.5433e+02 - logprior: -7.9215e+00
Epoch 4/10
10/10 - 1s - loss: 154.2377 - loglik: -1.4957e+02 - logprior: -4.6673e+00
Epoch 5/10
10/10 - 1s - loss: 151.2374 - loglik: -1.4815e+02 - logprior: -3.0899e+00
Epoch 6/10
10/10 - 1s - loss: 148.8864 - loglik: -1.4670e+02 - logprior: -2.1868e+00
Epoch 7/10
10/10 - 1s - loss: 149.5038 - loglik: -1.4788e+02 - logprior: -1.6276e+00
Fitted a model with MAP estimate = -148.7723
expansions: [(10, 2)]
discards: [0]
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 210.3032 - loglik: -1.4607e+02 - logprior: -6.4233e+01
Epoch 2/2
10/10 - 2s - loss: 171.9508 - loglik: -1.4497e+02 - logprior: -2.6977e+01
Fitted a model with MAP estimate = -165.8306
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 196.3307 - loglik: -1.4410e+02 - logprior: -5.2234e+01
Epoch 2/2
10/10 - 1s - loss: 156.8232 - loglik: -1.4232e+02 - logprior: -1.4504e+01
Fitted a model with MAP estimate = -152.3520
expansions: []
discards: [0]
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 206.6411 - loglik: -1.4422e+02 - logprior: -6.2423e+01
Epoch 2/10
10/10 - 2s - loss: 165.1159 - loglik: -1.4345e+02 - logprior: -2.1661e+01
Epoch 3/10
10/10 - 2s - loss: 152.7959 - loglik: -1.4416e+02 - logprior: -8.6344e+00
Epoch 4/10
10/10 - 2s - loss: 148.7121 - loglik: -1.4478e+02 - logprior: -3.9317e+00
Epoch 5/10
10/10 - 2s - loss: 146.9179 - loglik: -1.4483e+02 - logprior: -2.0909e+00
Epoch 6/10
10/10 - 1s - loss: 145.9106 - loglik: -1.4468e+02 - logprior: -1.2282e+00
Epoch 7/10
10/10 - 2s - loss: 146.3369 - loglik: -1.4557e+02 - logprior: -7.6986e-01
Fitted a model with MAP estimate = -145.4164
Time for alignment: 43.6682
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 254.2497 - loglik: -1.9353e+02 - logprior: -6.0717e+01
Epoch 2/10
10/10 - 1s - loss: 187.9187 - loglik: -1.7143e+02 - logprior: -1.6489e+01
Epoch 3/10
10/10 - 1s - loss: 162.5032 - loglik: -1.5457e+02 - logprior: -7.9290e+00
Epoch 4/10
10/10 - 2s - loss: 153.2558 - loglik: -1.4858e+02 - logprior: -4.6746e+00
Epoch 5/10
10/10 - 1s - loss: 151.0455 - loglik: -1.4796e+02 - logprior: -3.0853e+00
Epoch 6/10
10/10 - 1s - loss: 150.1359 - loglik: -1.4795e+02 - logprior: -2.1852e+00
Epoch 7/10
10/10 - 1s - loss: 148.5886 - loglik: -1.4696e+02 - logprior: -1.6295e+00
Epoch 8/10
10/10 - 1s - loss: 149.1725 - loglik: -1.4783e+02 - logprior: -1.3427e+00
Fitted a model with MAP estimate = -148.5193
expansions: [(10, 2)]
discards: [0]
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 210.3342 - loglik: -1.4608e+02 - logprior: -6.4255e+01
Epoch 2/2
10/10 - 2s - loss: 171.9643 - loglik: -1.4495e+02 - logprior: -2.7016e+01
Fitted a model with MAP estimate = -165.9319
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 196.1276 - loglik: -1.4375e+02 - logprior: -5.2373e+01
Epoch 2/2
10/10 - 1s - loss: 157.3246 - loglik: -1.4281e+02 - logprior: -1.4515e+01
Fitted a model with MAP estimate = -152.3504
expansions: []
discards: [0]
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 206.1266 - loglik: -1.4388e+02 - logprior: -6.2242e+01
Epoch 2/10
10/10 - 2s - loss: 165.0640 - loglik: -1.4393e+02 - logprior: -2.1132e+01
Epoch 3/10
10/10 - 2s - loss: 152.8786 - loglik: -1.4452e+02 - logprior: -8.3553e+00
Epoch 4/10
10/10 - 2s - loss: 148.3161 - loglik: -1.4444e+02 - logprior: -3.8752e+00
Epoch 5/10
10/10 - 2s - loss: 146.5482 - loglik: -1.4448e+02 - logprior: -2.0642e+00
Epoch 6/10
10/10 - 1s - loss: 146.1972 - loglik: -1.4499e+02 - logprior: -1.2051e+00
Epoch 7/10
10/10 - 1s - loss: 145.7894 - loglik: -1.4504e+02 - logprior: -7.4994e-01
Epoch 8/10
10/10 - 2s - loss: 145.7751 - loglik: -1.4536e+02 - logprior: -4.1819e-01
Epoch 9/10
10/10 - 2s - loss: 145.1500 - loglik: -1.4500e+02 - logprior: -1.4997e-01
Epoch 10/10
10/10 - 2s - loss: 144.2897 - loglik: -1.4434e+02 - logprior: 0.0535
Fitted a model with MAP estimate = -144.7272
Time for alignment: 49.7982
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 254.3479 - loglik: -1.9363e+02 - logprior: -6.0716e+01
Epoch 2/10
10/10 - 2s - loss: 188.7827 - loglik: -1.7228e+02 - logprior: -1.6498e+01
Epoch 3/10
10/10 - 2s - loss: 162.9641 - loglik: -1.5505e+02 - logprior: -7.9191e+00
Epoch 4/10
10/10 - 2s - loss: 153.4851 - loglik: -1.4876e+02 - logprior: -4.7240e+00
Epoch 5/10
10/10 - 2s - loss: 150.2221 - loglik: -1.4696e+02 - logprior: -3.2632e+00
Epoch 6/10
10/10 - 1s - loss: 148.1127 - loglik: -1.4574e+02 - logprior: -2.3720e+00
Epoch 7/10
10/10 - 1s - loss: 147.9928 - loglik: -1.4621e+02 - logprior: -1.7835e+00
Epoch 8/10
10/10 - 1s - loss: 148.1896 - loglik: -1.4672e+02 - logprior: -1.4709e+00
Fitted a model with MAP estimate = -147.4275
expansions: [(11, 1), (12, 2)]
discards: [0]
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 209.1462 - loglik: -1.4489e+02 - logprior: -6.4255e+01
Epoch 2/2
10/10 - 2s - loss: 169.9543 - loglik: -1.4304e+02 - logprior: -2.6912e+01
Fitted a model with MAP estimate = -163.9466
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 193.5480 - loglik: -1.4139e+02 - logprior: -5.2158e+01
Epoch 2/2
10/10 - 1s - loss: 155.7776 - loglik: -1.4136e+02 - logprior: -1.4413e+01
Fitted a model with MAP estimate = -150.1564
expansions: []
discards: [0]
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 204.1145 - loglik: -1.4177e+02 - logprior: -6.2344e+01
Epoch 2/10
10/10 - 1s - loss: 163.1629 - loglik: -1.4160e+02 - logprior: -2.1564e+01
Epoch 3/10
10/10 - 1s - loss: 150.1027 - loglik: -1.4159e+02 - logprior: -8.5109e+00
Epoch 4/10
10/10 - 1s - loss: 146.9937 - loglik: -1.4317e+02 - logprior: -3.8196e+00
Epoch 5/10
10/10 - 1s - loss: 144.8947 - loglik: -1.4291e+02 - logprior: -1.9849e+00
Epoch 6/10
10/10 - 2s - loss: 143.3870 - loglik: -1.4227e+02 - logprior: -1.1198e+00
Epoch 7/10
10/10 - 2s - loss: 143.6159 - loglik: -1.4296e+02 - logprior: -6.5809e-01
Fitted a model with MAP estimate = -143.1254
Time for alignment: 45.0309
Computed alignments with likelihoods: ['-145.4164', '-144.7272', '-143.1254']
Best model has likelihood: -143.1254
SP score = 0.9029
Training of 3 independent models on file sti.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20f6346fa0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f21399157c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2139915250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201137bee0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 553.8084 - loglik: -4.7191e+02 - logprior: -8.1901e+01
Epoch 2/10
10/10 - 3s - loss: 442.4044 - loglik: -4.2580e+02 - logprior: -1.6603e+01
Epoch 3/10
10/10 - 3s - loss: 388.2916 - loglik: -3.8264e+02 - logprior: -5.6503e+00
Epoch 4/10
10/10 - 3s - loss: 357.7970 - loglik: -3.5540e+02 - logprior: -2.4009e+00
Epoch 5/10
10/10 - 3s - loss: 342.3388 - loglik: -3.4142e+02 - logprior: -9.1450e-01
Epoch 6/10
10/10 - 3s - loss: 334.9704 - loglik: -3.3499e+02 - logprior: 0.0210
Epoch 7/10
10/10 - 3s - loss: 331.1431 - loglik: -3.3181e+02 - logprior: 0.6629
Epoch 8/10
10/10 - 3s - loss: 328.5001 - loglik: -3.2954e+02 - logprior: 1.0441
Epoch 9/10
10/10 - 3s - loss: 326.7141 - loglik: -3.2809e+02 - logprior: 1.3764
Epoch 10/10
10/10 - 3s - loss: 325.7903 - loglik: -3.2742e+02 - logprior: 1.6342
Fitted a model with MAP estimate = -325.6282
expansions: [(11, 3), (12, 2), (16, 3), (19, 1), (26, 1), (27, 1), (28, 1), (38, 2), (39, 2), (55, 1), (66, 1), (68, 1), (70, 1), (78, 2), (80, 2), (89, 1), (90, 2), (103, 1), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Fitting a model of length 175 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 405.2616 - loglik: -3.2558e+02 - logprior: -7.9686e+01
Epoch 2/2
10/10 - 4s - loss: 336.6072 - loglik: -3.0844e+02 - logprior: -2.8169e+01
Fitted a model with MAP estimate = -324.5662
expansions: [(0, 1)]
discards: [  0  11 115 164]
Fitting a model of length 172 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 363.1610 - loglik: -3.0463e+02 - logprior: -5.8527e+01
Epoch 2/2
10/10 - 4s - loss: 310.2547 - loglik: -2.9983e+02 - logprior: -1.0421e+01
Fitted a model with MAP estimate = -301.4003
expansions: [(94, 1), (126, 1)]
discards: [ 96 141]
Fitting a model of length 172 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 358.6598 - loglik: -3.0158e+02 - logprior: -5.7077e+01
Epoch 2/10
10/10 - 4s - loss: 307.4576 - loglik: -2.9805e+02 - logprior: -9.4124e+00
Epoch 3/10
10/10 - 4s - loss: 296.9825 - loglik: -2.9749e+02 - logprior: 0.5044
Epoch 4/10
10/10 - 4s - loss: 291.9315 - loglik: -2.9666e+02 - logprior: 4.7285
Epoch 5/10
10/10 - 4s - loss: 289.4179 - loglik: -2.9662e+02 - logprior: 7.2069
Epoch 6/10
10/10 - 4s - loss: 288.1840 - loglik: -2.9695e+02 - logprior: 8.7690
Epoch 7/10
10/10 - 4s - loss: 287.2409 - loglik: -2.9708e+02 - logprior: 9.8412
Epoch 8/10
10/10 - 4s - loss: 286.7139 - loglik: -2.9734e+02 - logprior: 10.6254
Epoch 9/10
10/10 - 4s - loss: 285.8935 - loglik: -2.9712e+02 - logprior: 11.2288
Epoch 10/10
10/10 - 4s - loss: 285.2588 - loglik: -2.9697e+02 - logprior: 11.7090
Fitted a model with MAP estimate = -284.9567
Time for alignment: 108.2205
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 553.7251 - loglik: -4.7182e+02 - logprior: -8.1901e+01
Epoch 2/10
10/10 - 3s - loss: 441.8268 - loglik: -4.2523e+02 - logprior: -1.6595e+01
Epoch 3/10
10/10 - 3s - loss: 387.9361 - loglik: -3.8239e+02 - logprior: -5.5504e+00
Epoch 4/10
10/10 - 3s - loss: 359.3176 - loglik: -3.5710e+02 - logprior: -2.2223e+00
Epoch 5/10
10/10 - 3s - loss: 344.8660 - loglik: -3.4431e+02 - logprior: -5.5690e-01
Epoch 6/10
10/10 - 3s - loss: 338.3203 - loglik: -3.3864e+02 - logprior: 0.3247
Epoch 7/10
10/10 - 3s - loss: 333.5519 - loglik: -3.3439e+02 - logprior: 0.8406
Epoch 8/10
10/10 - 3s - loss: 331.3195 - loglik: -3.3260e+02 - logprior: 1.2805
Epoch 9/10
10/10 - 3s - loss: 330.2358 - loglik: -3.3193e+02 - logprior: 1.6981
Epoch 10/10
10/10 - 3s - loss: 329.5075 - loglik: -3.3150e+02 - logprior: 1.9945
Fitted a model with MAP estimate = -329.1770
expansions: [(11, 3), (12, 2), (16, 3), (24, 1), (26, 1), (27, 1), (28, 1), (38, 2), (39, 1), (51, 1), (55, 1), (58, 1), (68, 1), (77, 2), (78, 2), (80, 2), (89, 1), (90, 2), (110, 2), (114, 3), (115, 1), (119, 1), (120, 1), (122, 1), (128, 3)]
discards: [0]
Fitting a model of length 175 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 408.1462 - loglik: -3.2860e+02 - logprior: -7.9548e+01
Epoch 2/2
10/10 - 4s - loss: 337.9384 - loglik: -3.0983e+02 - logprior: -2.8110e+01
Fitted a model with MAP estimate = -325.3077
expansions: [(0, 2), (130, 1)]
discards: [  0  11 116]
Fitting a model of length 175 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 362.4258 - loglik: -3.0382e+02 - logprior: -5.8611e+01
Epoch 2/2
10/10 - 4s - loss: 309.1004 - loglik: -2.9904e+02 - logprior: -1.0061e+01
Fitted a model with MAP estimate = -300.1899
expansions: []
discards: [  0 143 164]
Fitting a model of length 172 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 374.0634 - loglik: -3.0311e+02 - logprior: -7.0954e+01
Epoch 2/10
10/10 - 4s - loss: 316.8536 - loglik: -2.9986e+02 - logprior: -1.6990e+01
Epoch 3/10
10/10 - 4s - loss: 299.8011 - loglik: -2.9854e+02 - logprior: -1.2638e+00
Epoch 4/10
10/10 - 4s - loss: 292.9426 - loglik: -2.9720e+02 - logprior: 4.2527
Epoch 5/10
10/10 - 4s - loss: 290.5865 - loglik: -2.9744e+02 - logprior: 6.8499
Epoch 6/10
10/10 - 4s - loss: 288.3914 - loglik: -2.9676e+02 - logprior: 8.3690
Epoch 7/10
10/10 - 4s - loss: 287.8835 - loglik: -2.9726e+02 - logprior: 9.3770
Epoch 8/10
10/10 - 4s - loss: 286.8113 - loglik: -2.9700e+02 - logprior: 10.1904
Epoch 9/10
10/10 - 4s - loss: 286.3942 - loglik: -2.9730e+02 - logprior: 10.9068
Epoch 10/10
10/10 - 4s - loss: 285.8785 - loglik: -2.9742e+02 - logprior: 11.5440
Fitted a model with MAP estimate = -285.5151
Time for alignment: 104.1606
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 553.6012 - loglik: -4.7170e+02 - logprior: -8.1901e+01
Epoch 2/10
10/10 - 3s - loss: 442.0939 - loglik: -4.2549e+02 - logprior: -1.6600e+01
Epoch 3/10
10/10 - 3s - loss: 386.6025 - loglik: -3.8101e+02 - logprior: -5.5900e+00
Epoch 4/10
10/10 - 3s - loss: 356.6296 - loglik: -3.5433e+02 - logprior: -2.3036e+00
Epoch 5/10
10/10 - 3s - loss: 343.1917 - loglik: -3.4244e+02 - logprior: -7.5040e-01
Epoch 6/10
10/10 - 3s - loss: 335.4474 - loglik: -3.3563e+02 - logprior: 0.1797
Epoch 7/10
10/10 - 3s - loss: 331.4516 - loglik: -3.3227e+02 - logprior: 0.8200
Epoch 8/10
10/10 - 3s - loss: 329.1579 - loglik: -3.3041e+02 - logprior: 1.2516
Epoch 9/10
10/10 - 3s - loss: 328.1488 - loglik: -3.2973e+02 - logprior: 1.5777
Epoch 10/10
10/10 - 3s - loss: 327.4019 - loglik: -3.2928e+02 - logprior: 1.8789
Fitted a model with MAP estimate = -326.9346
expansions: [(11, 3), (12, 2), (16, 3), (24, 1), (26, 2), (29, 2), (38, 2), (49, 2), (55, 1), (66, 1), (68, 1), (70, 1), (78, 1), (79, 2), (80, 2), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 407.3539 - loglik: -3.2779e+02 - logprior: -7.9564e+01
Epoch 2/2
10/10 - 4s - loss: 339.7307 - loglik: -3.1142e+02 - logprior: -2.8311e+01
Fitted a model with MAP estimate = -327.2947
expansions: [(0, 1), (115, 1), (128, 1)]
discards: [  0  11  35 100 101 162]
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 366.0641 - loglik: -3.0720e+02 - logprior: -5.8859e+01
Epoch 2/2
10/10 - 4s - loss: 312.6537 - loglik: -3.0226e+02 - logprior: -1.0397e+01
Fitted a model with MAP estimate = -303.6072
expansions: []
discards: [139]
Fitting a model of length 169 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 360.3275 - loglik: -3.0325e+02 - logprior: -5.7073e+01
Epoch 2/10
10/10 - 4s - loss: 309.5693 - loglik: -3.0010e+02 - logprior: -9.4708e+00
Epoch 3/10
10/10 - 4s - loss: 298.6154 - loglik: -2.9895e+02 - logprior: 0.3301
Epoch 4/10
10/10 - 4s - loss: 294.0301 - loglik: -2.9859e+02 - logprior: 4.5573
Epoch 5/10
10/10 - 4s - loss: 292.2979 - loglik: -2.9933e+02 - logprior: 7.0292
Epoch 6/10
10/10 - 4s - loss: 290.3629 - loglik: -2.9894e+02 - logprior: 8.5759
Epoch 7/10
10/10 - 4s - loss: 289.8145 - loglik: -2.9945e+02 - logprior: 9.6365
Epoch 8/10
10/10 - 4s - loss: 288.8527 - loglik: -2.9926e+02 - logprior: 10.4112
Epoch 9/10
10/10 - 4s - loss: 288.3440 - loglik: -2.9936e+02 - logprior: 11.0165
Epoch 10/10
10/10 - 5s - loss: 287.7993 - loglik: -2.9928e+02 - logprior: 11.4781
Fitted a model with MAP estimate = -287.3931
Time for alignment: 112.3148
Computed alignments with likelihoods: ['-284.9567', '-285.5151', '-287.3931']
Best model has likelihood: -284.9567
SP score = 0.7923
Training of 3 independent models on file glob.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20c2a42e50>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20c2a42b20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20235985e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2023550790>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 293.7318 - loglik: -2.8259e+02 - logprior: -1.1145e+01
Epoch 2/10
12/12 - 2s - loss: 253.8219 - loglik: -2.5119e+02 - logprior: -2.6270e+00
Epoch 3/10
12/12 - 2s - loss: 226.7391 - loglik: -2.2496e+02 - logprior: -1.7743e+00
Epoch 4/10
12/12 - 2s - loss: 218.5222 - loglik: -2.1676e+02 - logprior: -1.7589e+00
Epoch 5/10
12/12 - 2s - loss: 213.8526 - loglik: -2.1212e+02 - logprior: -1.7280e+00
Epoch 6/10
12/12 - 2s - loss: 211.4598 - loglik: -2.0981e+02 - logprior: -1.6492e+00
Epoch 7/10
12/12 - 2s - loss: 210.4796 - loglik: -2.0884e+02 - logprior: -1.6366e+00
Epoch 8/10
12/12 - 2s - loss: 210.2886 - loglik: -2.0851e+02 - logprior: -1.7829e+00
Epoch 9/10
12/12 - 2s - loss: 208.5679 - loglik: -2.0678e+02 - logprior: -1.7901e+00
Epoch 10/10
12/12 - 2s - loss: 209.3082 - loglik: -2.0754e+02 - logprior: -1.7706e+00
Fitted a model with MAP estimate = -208.6563
expansions: [(6, 2), (8, 1), (9, 1), (10, 2), (11, 2), (21, 1), (36, 3), (49, 4), (57, 1), (58, 2), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 224.2869 - loglik: -2.1236e+02 - logprior: -1.1932e+01
Epoch 2/2
12/12 - 2s - loss: 205.0969 - loglik: -2.0019e+02 - logprior: -4.9068e+00
Fitted a model with MAP estimate = -201.0618
expansions: [(0, 7), (6, 1), (63, 1)]
discards: [ 0 46 74 77 78]
Fitting a model of length 109 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 208.2316 - loglik: -1.9885e+02 - logprior: -9.3858e+00
Epoch 2/2
12/12 - 2s - loss: 197.3601 - loglik: -1.9494e+02 - logprior: -2.4210e+00
Fitted a model with MAP estimate = -194.8395
expansions: []
discards: [ 1  2  3  4  5  6 21]
Fitting a model of length 102 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 204.6529 - loglik: -1.9586e+02 - logprior: -8.7945e+00
Epoch 2/10
12/12 - 2s - loss: 196.3320 - loglik: -1.9414e+02 - logprior: -2.1965e+00
Epoch 3/10
12/12 - 2s - loss: 194.0389 - loglik: -1.9266e+02 - logprior: -1.3768e+00
Epoch 4/10
12/12 - 2s - loss: 192.5467 - loglik: -1.9137e+02 - logprior: -1.1816e+00
Epoch 5/10
12/12 - 2s - loss: 191.3164 - loglik: -1.9034e+02 - logprior: -9.7816e-01
Epoch 6/10
12/12 - 2s - loss: 188.9314 - loglik: -1.8797e+02 - logprior: -9.6158e-01
Epoch 7/10
12/12 - 2s - loss: 189.2474 - loglik: -1.8834e+02 - logprior: -9.0339e-01
Fitted a model with MAP estimate = -188.1314
Time for alignment: 66.7676
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 293.5271 - loglik: -2.8239e+02 - logprior: -1.1139e+01
Epoch 2/10
12/12 - 2s - loss: 255.0380 - loglik: -2.5242e+02 - logprior: -2.6225e+00
Epoch 3/10
12/12 - 2s - loss: 227.5664 - loglik: -2.2578e+02 - logprior: -1.7823e+00
Epoch 4/10
12/12 - 2s - loss: 216.8018 - loglik: -2.1506e+02 - logprior: -1.7459e+00
Epoch 5/10
12/12 - 2s - loss: 213.4001 - loglik: -2.1167e+02 - logprior: -1.7258e+00
Epoch 6/10
12/12 - 2s - loss: 210.3074 - loglik: -2.0867e+02 - logprior: -1.6415e+00
Epoch 7/10
12/12 - 2s - loss: 209.3974 - loglik: -2.0778e+02 - logprior: -1.6193e+00
Epoch 8/10
12/12 - 2s - loss: 208.3793 - loglik: -2.0664e+02 - logprior: -1.7404e+00
Epoch 9/10
12/12 - 2s - loss: 208.3846 - loglik: -2.0661e+02 - logprior: -1.7762e+00
Fitted a model with MAP estimate = -208.0582
expansions: [(6, 3), (10, 3), (13, 1), (20, 1), (29, 1), (36, 4), (49, 2), (50, 3), (52, 1), (58, 3), (59, 4), (61, 1), (68, 1), (70, 1)]
discards: [0]
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 221.5818 - loglik: -2.0963e+02 - logprior: -1.1949e+01
Epoch 2/2
12/12 - 2s - loss: 201.3408 - loglik: -1.9650e+02 - logprior: -4.8430e+00
Fitted a model with MAP estimate = -197.8159
expansions: [(0, 6), (48, 1)]
discards: [ 0 62 80]
Fitting a model of length 112 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 204.7841 - loglik: -1.9537e+02 - logprior: -9.4131e+00
Epoch 2/2
12/12 - 2s - loss: 192.9946 - loglik: -1.9048e+02 - logprior: -2.5191e+00
Fitted a model with MAP estimate = -191.3088
expansions: []
discards: [1 2 3 4 5]
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 200.1566 - loglik: -1.9121e+02 - logprior: -8.9516e+00
Epoch 2/10
12/12 - 2s - loss: 193.0035 - loglik: -1.9062e+02 - logprior: -2.3857e+00
Epoch 3/10
12/12 - 2s - loss: 190.2969 - loglik: -1.8870e+02 - logprior: -1.5974e+00
Epoch 4/10
12/12 - 2s - loss: 188.0521 - loglik: -1.8674e+02 - logprior: -1.3114e+00
Epoch 5/10
12/12 - 2s - loss: 187.3459 - loglik: -1.8623e+02 - logprior: -1.1116e+00
Epoch 6/10
12/12 - 2s - loss: 185.3704 - loglik: -1.8428e+02 - logprior: -1.0931e+00
Epoch 7/10
12/12 - 2s - loss: 184.6701 - loglik: -1.8360e+02 - logprior: -1.0740e+00
Epoch 8/10
12/12 - 2s - loss: 184.0302 - loglik: -1.8295e+02 - logprior: -1.0815e+00
Epoch 9/10
12/12 - 2s - loss: 183.3685 - loglik: -1.8225e+02 - logprior: -1.1148e+00
Epoch 10/10
12/12 - 3s - loss: 183.2133 - loglik: -1.8210e+02 - logprior: -1.1182e+00
Fitted a model with MAP estimate = -182.9630
Time for alignment: 74.2265
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 293.5133 - loglik: -2.8237e+02 - logprior: -1.1143e+01
Epoch 2/10
12/12 - 2s - loss: 254.3715 - loglik: -2.5175e+02 - logprior: -2.6251e+00
Epoch 3/10
12/12 - 2s - loss: 227.7443 - loglik: -2.2594e+02 - logprior: -1.8017e+00
Epoch 4/10
12/12 - 2s - loss: 216.9604 - loglik: -2.1516e+02 - logprior: -1.8027e+00
Epoch 5/10
12/12 - 2s - loss: 212.1449 - loglik: -2.1029e+02 - logprior: -1.8544e+00
Epoch 6/10
12/12 - 2s - loss: 208.2390 - loglik: -2.0642e+02 - logprior: -1.8225e+00
Epoch 7/10
12/12 - 2s - loss: 206.3179 - loglik: -2.0452e+02 - logprior: -1.7937e+00
Epoch 8/10
12/12 - 2s - loss: 205.7432 - loglik: -2.0378e+02 - logprior: -1.9591e+00
Epoch 9/10
12/12 - 2s - loss: 205.1093 - loglik: -2.0310e+02 - logprior: -2.0105e+00
Epoch 10/10
12/12 - 2s - loss: 204.6547 - loglik: -2.0270e+02 - logprior: -1.9526e+00
Fitted a model with MAP estimate = -204.5577
expansions: [(6, 3), (10, 2), (11, 2), (12, 2), (13, 1), (21, 1), (36, 3), (49, 2), (50, 3), (52, 1), (58, 3), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Fitting a model of length 109 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 220.2682 - loglik: -2.0820e+02 - logprior: -1.2064e+01
Epoch 2/2
12/12 - 3s - loss: 199.6556 - loglik: -1.9477e+02 - logprior: -4.8891e+00
Fitted a model with MAP estimate = -196.2420
expansions: [(0, 5)]
discards: [ 0 18 63 81]
Fitting a model of length 110 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 202.0914 - loglik: -1.9272e+02 - logprior: -9.3707e+00
Epoch 2/2
12/12 - 2s - loss: 191.4877 - loglik: -1.8899e+02 - logprior: -2.5008e+00
Fitted a model with MAP estimate = -189.4640
expansions: []
discards: [ 1  2  3  4 51]
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 199.6247 - loglik: -1.9072e+02 - logprior: -8.9072e+00
Epoch 2/10
12/12 - 2s - loss: 192.0216 - loglik: -1.8967e+02 - logprior: -2.3549e+00
Epoch 3/10
12/12 - 2s - loss: 189.6887 - loglik: -1.8811e+02 - logprior: -1.5827e+00
Epoch 4/10
12/12 - 2s - loss: 187.5534 - loglik: -1.8625e+02 - logprior: -1.3056e+00
Epoch 5/10
12/12 - 2s - loss: 186.2171 - loglik: -1.8513e+02 - logprior: -1.0835e+00
Epoch 6/10
12/12 - 2s - loss: 185.3548 - loglik: -1.8430e+02 - logprior: -1.0508e+00
Epoch 7/10
12/12 - 2s - loss: 183.1879 - loglik: -1.8217e+02 - logprior: -1.0173e+00
Epoch 8/10
12/12 - 2s - loss: 182.9866 - loglik: -1.8194e+02 - logprior: -1.0491e+00
Epoch 9/10
12/12 - 2s - loss: 181.8587 - loglik: -1.8073e+02 - logprior: -1.1251e+00
Epoch 10/10
12/12 - 2s - loss: 181.4734 - loglik: -1.8039e+02 - logprior: -1.0795e+00
Fitted a model with MAP estimate = -181.5656
Time for alignment: 75.5754
Computed alignments with likelihoods: ['-188.1314', '-182.9630', '-181.5656']
Best model has likelihood: -181.5656
SP score = 0.8671
Training of 3 independent models on file az.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2087890280>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2090572310>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20126f21f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20f60078e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 334.1203 - loglik: -2.8910e+02 - logprior: -4.5019e+01
Epoch 2/10
10/10 - 1s - loss: 277.5404 - loglik: -2.6642e+02 - logprior: -1.1124e+01
Epoch 3/10
10/10 - 1s - loss: 250.3816 - loglik: -2.4587e+02 - logprior: -4.5105e+00
Epoch 4/10
10/10 - 1s - loss: 237.3429 - loglik: -2.3520e+02 - logprior: -2.1459e+00
Epoch 5/10
10/10 - 1s - loss: 232.5957 - loglik: -2.3150e+02 - logprior: -1.0935e+00
Epoch 6/10
10/10 - 1s - loss: 229.5737 - loglik: -2.2894e+02 - logprior: -6.3870e-01
Epoch 7/10
10/10 - 1s - loss: 227.7719 - loglik: -2.2737e+02 - logprior: -4.0042e-01
Epoch 8/10
10/10 - 1s - loss: 227.0449 - loglik: -2.2681e+02 - logprior: -2.3306e-01
Epoch 9/10
10/10 - 1s - loss: 226.0017 - loglik: -2.2591e+02 - logprior: -9.4586e-02
Epoch 10/10
10/10 - 1s - loss: 226.1455 - loglik: -2.2619e+02 - logprior: 0.0446
Fitted a model with MAP estimate = -225.7104
expansions: [(0, 3), (8, 3), (36, 1), (43, 12), (53, 3)]
discards: []
Fitting a model of length 98 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 276.3034 - loglik: -2.2296e+02 - logprior: -5.3348e+01
Epoch 2/2
10/10 - 2s - loss: 233.6941 - loglik: -2.1809e+02 - logprior: -1.5608e+01
Fitted a model with MAP estimate = -225.1750
expansions: []
discards: [ 0  1 55 56 73]
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 262.8551 - loglik: -2.1726e+02 - logprior: -4.5596e+01
Epoch 2/2
10/10 - 2s - loss: 234.2321 - loglik: -2.1676e+02 - logprior: -1.7472e+01
Fitted a model with MAP estimate = -229.2845
expansions: [(0, 3), (42, 1)]
discards: [ 0 52]
Fitting a model of length 95 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 256.4272 - loglik: -2.1563e+02 - logprior: -4.0801e+01
Epoch 2/10
10/10 - 2s - loss: 224.6715 - loglik: -2.1462e+02 - logprior: -1.0056e+01
Epoch 3/10
10/10 - 2s - loss: 217.5277 - loglik: -2.1433e+02 - logprior: -3.1959e+00
Epoch 4/10
10/10 - 2s - loss: 215.2557 - loglik: -2.1458e+02 - logprior: -6.7763e-01
Epoch 5/10
10/10 - 2s - loss: 214.1961 - loglik: -2.1478e+02 - logprior: 0.5834
Epoch 6/10
10/10 - 2s - loss: 213.1678 - loglik: -2.1446e+02 - logprior: 1.2970
Epoch 7/10
10/10 - 2s - loss: 213.4312 - loglik: -2.1514e+02 - logprior: 1.7101
Fitted a model with MAP estimate = -213.0143
Time for alignment: 51.6362
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 334.1340 - loglik: -2.8911e+02 - logprior: -4.5020e+01
Epoch 2/10
10/10 - 1s - loss: 277.7388 - loglik: -2.6661e+02 - logprior: -1.1127e+01
Epoch 3/10
10/10 - 1s - loss: 251.1049 - loglik: -2.4655e+02 - logprior: -4.5522e+00
Epoch 4/10
10/10 - 1s - loss: 238.1180 - loglik: -2.3583e+02 - logprior: -2.2848e+00
Epoch 5/10
10/10 - 1s - loss: 232.5886 - loglik: -2.3128e+02 - logprior: -1.3115e+00
Epoch 6/10
10/10 - 1s - loss: 229.8078 - loglik: -2.2898e+02 - logprior: -8.3158e-01
Epoch 7/10
10/10 - 1s - loss: 228.1391 - loglik: -2.2754e+02 - logprior: -6.0188e-01
Epoch 8/10
10/10 - 1s - loss: 227.3414 - loglik: -2.2686e+02 - logprior: -4.7889e-01
Epoch 9/10
10/10 - 1s - loss: 225.7771 - loglik: -2.2539e+02 - logprior: -3.8932e-01
Epoch 10/10
10/10 - 1s - loss: 225.9846 - loglik: -2.2570e+02 - logprior: -2.8330e-01
Fitted a model with MAP estimate = -225.5307
expansions: [(0, 3), (5, 2), (7, 1), (8, 1), (36, 6), (43, 7), (51, 1), (53, 3)]
discards: []
Fitting a model of length 100 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 276.6360 - loglik: -2.2319e+02 - logprior: -5.3445e+01
Epoch 2/2
10/10 - 2s - loss: 232.5982 - loglik: -2.1701e+02 - logprior: -1.5590e+01
Fitted a model with MAP estimate = -224.6427
expansions: []
discards: [ 0  1 11 43 44 45 46 47 75]
Fitting a model of length 91 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 264.3730 - loglik: -2.1864e+02 - logprior: -4.5733e+01
Epoch 2/2
10/10 - 2s - loss: 235.3147 - loglik: -2.1775e+02 - logprior: -1.7569e+01
Fitted a model with MAP estimate = -230.3807
expansions: [(0, 3)]
discards: [ 0 48 49]
Fitting a model of length 91 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 258.1599 - loglik: -2.1735e+02 - logprior: -4.0813e+01
Epoch 2/10
10/10 - 2s - loss: 226.2637 - loglik: -2.1613e+02 - logprior: -1.0131e+01
Epoch 3/10
10/10 - 2s - loss: 219.2387 - loglik: -2.1591e+02 - logprior: -3.3286e+00
Epoch 4/10
10/10 - 2s - loss: 216.7671 - loglik: -2.1595e+02 - logprior: -8.1802e-01
Epoch 5/10
10/10 - 2s - loss: 216.1062 - loglik: -2.1655e+02 - logprior: 0.4435
Epoch 6/10
10/10 - 2s - loss: 215.0578 - loglik: -2.1622e+02 - logprior: 1.1572
Epoch 7/10
10/10 - 2s - loss: 215.0816 - loglik: -2.1665e+02 - logprior: 1.5647
Fitted a model with MAP estimate = -214.8527
Time for alignment: 51.3774
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 334.0309 - loglik: -2.8901e+02 - logprior: -4.5020e+01
Epoch 2/10
10/10 - 1s - loss: 277.3701 - loglik: -2.6624e+02 - logprior: -1.1134e+01
Epoch 3/10
10/10 - 1s - loss: 249.7605 - loglik: -2.4524e+02 - logprior: -4.5216e+00
Epoch 4/10
10/10 - 1s - loss: 236.5721 - loglik: -2.3438e+02 - logprior: -2.1941e+00
Epoch 5/10
10/10 - 1s - loss: 231.7884 - loglik: -2.3063e+02 - logprior: -1.1550e+00
Epoch 6/10
10/10 - 1s - loss: 229.0731 - loglik: -2.2842e+02 - logprior: -6.5006e-01
Epoch 7/10
10/10 - 1s - loss: 227.7481 - loglik: -2.2732e+02 - logprior: -4.2489e-01
Epoch 8/10
10/10 - 1s - loss: 226.6256 - loglik: -2.2637e+02 - logprior: -2.5495e-01
Epoch 9/10
10/10 - 1s - loss: 225.9914 - loglik: -2.2580e+02 - logprior: -1.8931e-01
Epoch 10/10
10/10 - 1s - loss: 225.3591 - loglik: -2.2520e+02 - logprior: -1.5966e-01
Fitted a model with MAP estimate = -225.1630
expansions: [(0, 3), (6, 2), (7, 1), (8, 1), (43, 10), (53, 3)]
discards: []
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 275.7639 - loglik: -2.2244e+02 - logprior: -5.3320e+01
Epoch 2/2
10/10 - 2s - loss: 232.5322 - loglik: -2.1708e+02 - logprior: -1.5454e+01
Fitted a model with MAP estimate = -225.0346
expansions: []
discards: [ 0  1  9 55 71]
Fitting a model of length 91 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 264.1185 - loglik: -2.1837e+02 - logprior: -4.5750e+01
Epoch 2/2
10/10 - 2s - loss: 234.5498 - loglik: -2.1701e+02 - logprior: -1.7541e+01
Fitted a model with MAP estimate = -229.8526
expansions: [(0, 3)]
discards: [0]
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 257.0661 - loglik: -2.1618e+02 - logprior: -4.0889e+01
Epoch 2/10
10/10 - 2s - loss: 225.2601 - loglik: -2.1508e+02 - logprior: -1.0181e+01
Epoch 3/10
10/10 - 2s - loss: 218.6309 - loglik: -2.1529e+02 - logprior: -3.3450e+00
Epoch 4/10
10/10 - 2s - loss: 215.9855 - loglik: -2.1516e+02 - logprior: -8.2366e-01
Epoch 5/10
10/10 - 2s - loss: 215.2218 - loglik: -2.1567e+02 - logprior: 0.4521
Epoch 6/10
10/10 - 2s - loss: 214.9618 - loglik: -2.1612e+02 - logprior: 1.1541
Epoch 7/10
10/10 - 2s - loss: 214.2018 - loglik: -2.1577e+02 - logprior: 1.5713
Epoch 8/10
10/10 - 2s - loss: 214.2245 - loglik: -2.1605e+02 - logprior: 1.8296
Fitted a model with MAP estimate = -213.9930
Time for alignment: 51.7078
Computed alignments with likelihoods: ['-213.0143', '-214.8527', '-213.9930']
Best model has likelihood: -213.0143
SP score = 0.7436
Training of 3 independent models on file ghf11.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2008547700>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2008547f40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f200854d8b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2153d47a30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 570.1552 - loglik: -4.7337e+02 - logprior: -9.6789e+01
Epoch 2/10
10/10 - 4s - loss: 423.7115 - loglik: -4.0433e+02 - logprior: -1.9384e+01
Epoch 3/10
10/10 - 4s - loss: 342.7381 - loglik: -3.3590e+02 - logprior: -6.8364e+00
Epoch 4/10
10/10 - 4s - loss: 294.3759 - loglik: -2.8998e+02 - logprior: -4.3933e+00
Epoch 5/10
10/10 - 4s - loss: 275.9886 - loglik: -2.7238e+02 - logprior: -3.6106e+00
Epoch 6/10
10/10 - 4s - loss: 269.4320 - loglik: -2.6715e+02 - logprior: -2.2775e+00
Epoch 7/10
10/10 - 4s - loss: 266.2020 - loglik: -2.6507e+02 - logprior: -1.1366e+00
Epoch 8/10
10/10 - 4s - loss: 264.7988 - loglik: -2.6427e+02 - logprior: -5.2700e-01
Epoch 9/10
10/10 - 4s - loss: 264.3149 - loglik: -2.6419e+02 - logprior: -1.2362e-01
Epoch 10/10
10/10 - 4s - loss: 263.4882 - loglik: -2.6376e+02 - logprior: 0.2750
Fitted a model with MAP estimate = -263.2101
expansions: [(18, 1), (20, 1), (22, 1), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 2), (124, 1), (125, 1), (128, 1), (129, 3), (133, 1), (137, 1), (138, 1)]
discards: [0]
Fitting a model of length 177 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 349.1057 - loglik: -2.5625e+02 - logprior: -9.2852e+01
Epoch 2/2
10/10 - 5s - loss: 269.4832 - loglik: -2.3689e+02 - logprior: -3.2598e+01
Fitted a model with MAP estimate = -257.0610
expansions: [(0, 3), (15, 4), (86, 1)]
discards: [  0  43  53 118]
Fitting a model of length 181 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 301.1554 - loglik: -2.3069e+02 - logprior: -7.0469e+01
Epoch 2/2
10/10 - 6s - loss: 235.3987 - loglik: -2.2339e+02 - logprior: -1.2006e+01
Fitted a model with MAP estimate = -225.0575
expansions: [(16, 1), (22, 1)]
discards: [0 1]
Fitting a model of length 181 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 309.6342 - loglik: -2.2459e+02 - logprior: -8.5049e+01
Epoch 2/10
10/10 - 6s - loss: 246.9844 - loglik: -2.2140e+02 - logprior: -2.5583e+01
Epoch 3/10
10/10 - 6s - loss: 224.7105 - loglik: -2.2012e+02 - logprior: -4.5877e+00
Epoch 4/10
10/10 - 6s - loss: 214.3673 - loglik: -2.2042e+02 - logprior: 6.0496
Epoch 5/10
10/10 - 6s - loss: 210.2568 - loglik: -2.2014e+02 - logprior: 9.8835
Epoch 6/10
10/10 - 6s - loss: 208.3069 - loglik: -2.2019e+02 - logprior: 11.8817
Epoch 7/10
10/10 - 6s - loss: 207.3212 - loglik: -2.2049e+02 - logprior: 13.1643
Epoch 8/10
10/10 - 6s - loss: 206.5386 - loglik: -2.2063e+02 - logprior: 14.0903
Epoch 9/10
10/10 - 6s - loss: 205.5607 - loglik: -2.2043e+02 - logprior: 14.8736
Epoch 10/10
10/10 - 6s - loss: 205.1172 - loglik: -2.2071e+02 - logprior: 15.5972
Fitted a model with MAP estimate = -204.6874
Time for alignment: 144.0646
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 570.4743 - loglik: -4.7369e+02 - logprior: -9.6789e+01
Epoch 2/10
10/10 - 4s - loss: 422.9874 - loglik: -4.0360e+02 - logprior: -1.9385e+01
Epoch 3/10
10/10 - 4s - loss: 342.7193 - loglik: -3.3589e+02 - logprior: -6.8249e+00
Epoch 4/10
10/10 - 4s - loss: 294.1409 - loglik: -2.8977e+02 - logprior: -4.3679e+00
Epoch 5/10
10/10 - 4s - loss: 277.0132 - loglik: -2.7365e+02 - logprior: -3.3681e+00
Epoch 6/10
10/10 - 4s - loss: 269.8288 - loglik: -2.6786e+02 - logprior: -1.9642e+00
Epoch 7/10
10/10 - 4s - loss: 266.1572 - loglik: -2.6529e+02 - logprior: -8.6836e-01
Epoch 8/10
10/10 - 4s - loss: 265.5689 - loglik: -2.6517e+02 - logprior: -3.9487e-01
Epoch 9/10
10/10 - 4s - loss: 264.1547 - loglik: -2.6417e+02 - logprior: 0.0176
Epoch 10/10
10/10 - 4s - loss: 263.6919 - loglik: -2.6414e+02 - logprior: 0.4492
Fitted a model with MAP estimate = -263.4020
expansions: [(18, 1), (20, 1), (22, 1), (26, 1), (39, 2), (41, 3), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (128, 1), (129, 3), (133, 1), (137, 1), (138, 1)]
discards: [0]
Fitting a model of length 178 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 349.8058 - loglik: -2.5690e+02 - logprior: -9.2907e+01
Epoch 2/2
10/10 - 5s - loss: 270.1040 - loglik: -2.3735e+02 - logprior: -3.2752e+01
Fitted a model with MAP estimate = -257.2044
expansions: [(0, 3), (15, 3), (16, 1), (87, 1)]
discards: [  0  43  46  54 119]
Fitting a model of length 181 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 301.5610 - loglik: -2.3120e+02 - logprior: -7.0366e+01
Epoch 2/2
10/10 - 6s - loss: 235.9946 - loglik: -2.2403e+02 - logprior: -1.1964e+01
Fitted a model with MAP estimate = -225.3527
expansions: [(16, 1), (17, 1)]
discards: [0 1]
Fitting a model of length 181 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 309.7304 - loglik: -2.2465e+02 - logprior: -8.5084e+01
Epoch 2/10
10/10 - 6s - loss: 246.8232 - loglik: -2.2112e+02 - logprior: -2.5703e+01
Epoch 3/10
10/10 - 6s - loss: 225.1924 - loglik: -2.2047e+02 - logprior: -4.7204e+00
Epoch 4/10
10/10 - 6s - loss: 214.6890 - loglik: -2.2069e+02 - logprior: 6.0017
Epoch 5/10
10/10 - 6s - loss: 209.9396 - loglik: -2.1980e+02 - logprior: 9.8612
Epoch 6/10
10/10 - 6s - loss: 208.5174 - loglik: -2.2039e+02 - logprior: 11.8701
Epoch 7/10
10/10 - 6s - loss: 207.4029 - loglik: -2.2055e+02 - logprior: 13.1450
Epoch 8/10
10/10 - 6s - loss: 206.1961 - loglik: -2.2027e+02 - logprior: 14.0740
Epoch 9/10
10/10 - 6s - loss: 205.7837 - loglik: -2.2064e+02 - logprior: 14.8519
Epoch 10/10
10/10 - 6s - loss: 205.1402 - loglik: -2.2071e+02 - logprior: 15.5672
Fitted a model with MAP estimate = -204.7462
Time for alignment: 141.8411
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 570.3409 - loglik: -4.7355e+02 - logprior: -9.6789e+01
Epoch 2/10
10/10 - 4s - loss: 423.3430 - loglik: -4.0396e+02 - logprior: -1.9384e+01
Epoch 3/10
10/10 - 4s - loss: 342.4824 - loglik: -3.3564e+02 - logprior: -6.8406e+00
Epoch 4/10
10/10 - 4s - loss: 294.7658 - loglik: -2.9041e+02 - logprior: -4.3595e+00
Epoch 5/10
10/10 - 4s - loss: 276.7447 - loglik: -2.7336e+02 - logprior: -3.3844e+00
Epoch 6/10
10/10 - 4s - loss: 270.0810 - loglik: -2.6805e+02 - logprior: -2.0347e+00
Epoch 7/10
10/10 - 4s - loss: 266.9676 - loglik: -2.6598e+02 - logprior: -9.8598e-01
Epoch 8/10
10/10 - 4s - loss: 265.4319 - loglik: -2.6489e+02 - logprior: -5.4600e-01
Epoch 9/10
10/10 - 4s - loss: 264.5216 - loglik: -2.6430e+02 - logprior: -2.1887e-01
Epoch 10/10
10/10 - 4s - loss: 263.2221 - loglik: -2.6345e+02 - logprior: 0.2264
Fitted a model with MAP estimate = -263.5183
expansions: [(18, 1), (20, 1), (22, 1), (26, 1), (39, 2), (40, 1), (41, 3), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (127, 2), (128, 1), (129, 1), (133, 1), (137, 1), (138, 1)]
discards: [0]
Fitting a model of length 179 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 350.1956 - loglik: -2.5720e+02 - logprior: -9.2992e+01
Epoch 2/2
10/10 - 6s - loss: 269.4471 - loglik: -2.3669e+02 - logprior: -3.2753e+01
Fitted a model with MAP estimate = -256.6988
expansions: [(0, 3), (15, 4), (88, 1)]
discards: [  0  47  48  55 120]
Fitting a model of length 182 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 300.5184 - loglik: -2.3008e+02 - logprior: -7.0442e+01
Epoch 2/2
10/10 - 6s - loss: 235.3767 - loglik: -2.2340e+02 - logprior: -1.1976e+01
Fitted a model with MAP estimate = -224.7232
expansions: [(16, 1), (22, 1)]
discards: [0 1]
Fitting a model of length 182 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 309.0910 - loglik: -2.2404e+02 - logprior: -8.5056e+01
Epoch 2/10
10/10 - 6s - loss: 247.1425 - loglik: -2.2146e+02 - logprior: -2.5681e+01
Epoch 3/10
10/10 - 6s - loss: 224.9751 - loglik: -2.2025e+02 - logprior: -4.7247e+00
Epoch 4/10
10/10 - 6s - loss: 213.8009 - loglik: -2.1982e+02 - logprior: 6.0186
Epoch 5/10
10/10 - 6s - loss: 209.9358 - loglik: -2.1982e+02 - logprior: 9.8807
Epoch 6/10
10/10 - 6s - loss: 208.0530 - loglik: -2.1993e+02 - logprior: 11.8792
Epoch 7/10
10/10 - 6s - loss: 207.1225 - loglik: -2.2029e+02 - logprior: 13.1649
Epoch 8/10
10/10 - 6s - loss: 206.1855 - loglik: -2.2028e+02 - logprior: 14.0982
Epoch 9/10
10/10 - 6s - loss: 205.5557 - loglik: -2.2043e+02 - logprior: 14.8785
Epoch 10/10
10/10 - 6s - loss: 204.5606 - loglik: -2.2016e+02 - logprior: 15.5973
Fitted a model with MAP estimate = -204.4244
Time for alignment: 145.1025
Computed alignments with likelihoods: ['-204.6874', '-204.7462', '-204.4244']
Best model has likelihood: -204.4244
SP score = 0.9114
Training of 3 independent models on file subt.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20116e8160>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1fff7d0040>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fff7d0e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fff7d0400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 37s - loss: 812.7269 - loglik: -8.0991e+02 - logprior: -2.8194e+00
Epoch 2/10
33/33 - 32s - loss: 721.1887 - loglik: -7.2046e+02 - logprior: -7.2378e-01
Epoch 3/10
33/33 - 32s - loss: 714.1741 - loglik: -7.1342e+02 - logprior: -7.5765e-01
Epoch 4/10
33/33 - 32s - loss: 709.4634 - loglik: -7.0868e+02 - logprior: -7.8204e-01
Epoch 5/10
33/33 - 35s - loss: 708.6768 - loglik: -7.0779e+02 - logprior: -8.8264e-01
Epoch 6/10
33/33 - 36s - loss: 708.6970 - loglik: -7.0771e+02 - logprior: -9.9159e-01
Fitted a model with MAP estimate = -707.2666
expansions: [(0, 5), (7, 1), (9, 2), (34, 5), (42, 1), (44, 2), (64, 1), (71, 3), (72, 1), (78, 1), (83, 1), (113, 1), (116, 1), (118, 1), (132, 1), (155, 2), (164, 4), (185, 1), (204, 1), (217, 1), (220, 2), (221, 3), (230, 3)]
discards: [  2   3  61 225 226 227 228]
Fitting a model of length 267 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 43s - loss: 710.8936 - loglik: -7.0773e+02 - logprior: -3.1659e+00
Epoch 2/2
33/33 - 37s - loss: 701.1970 - loglik: -7.0062e+02 - logprior: -5.7754e-01
Fitted a model with MAP estimate = -698.5515
expansions: [(267, 4)]
discards: [  1   5   6  15  41  56 179 190 191 192 254 257 262 263 264 265 266]
Fitting a model of length 254 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 41s - loss: 706.1542 - loglik: -7.0410e+02 - logprior: -2.0525e+00
Epoch 2/2
33/33 - 41s - loss: 705.0628 - loglik: -7.0494e+02 - logprior: -1.2579e-01
Fitted a model with MAP estimate = -700.1542
expansions: [(0, 5), (183, 5), (248, 2), (254, 4)]
discards: [  3 250 251 252]
Fitting a model of length 266 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 40s - loss: 707.0063 - loglik: -7.0387e+02 - logprior: -3.1345e+00
Epoch 2/10
33/33 - 36s - loss: 700.0016 - loglik: -6.9982e+02 - logprior: -1.8519e-01
Epoch 3/10
33/33 - 37s - loss: 695.9702 - loglik: -6.9592e+02 - logprior: -4.8612e-02
Epoch 4/10
33/33 - 41s - loss: 697.1140 - loglik: -6.9711e+02 - logprior: -4.3499e-03
Fitted a model with MAP estimate = -694.7109
Time for alignment: 656.1516
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 37s - loss: 816.3594 - loglik: -8.1351e+02 - logprior: -2.8484e+00
Epoch 2/10
33/33 - 36s - loss: 721.1440 - loglik: -7.2032e+02 - logprior: -8.2041e-01
Epoch 3/10
33/33 - 36s - loss: 711.2500 - loglik: -7.1047e+02 - logprior: -7.8266e-01
Epoch 4/10
33/33 - 37s - loss: 708.6187 - loglik: -7.0779e+02 - logprior: -8.2407e-01
Epoch 5/10
33/33 - 37s - loss: 708.7150 - loglik: -7.0778e+02 - logprior: -9.3148e-01
Fitted a model with MAP estimate = -707.3917
expansions: [(0, 4), (6, 1), (7, 1), (9, 1), (10, 1), (34, 4), (42, 1), (44, 2), (73, 1), (74, 2), (79, 1), (92, 1), (114, 1), (117, 1), (119, 1), (135, 2), (156, 3), (164, 4), (181, 2), (207, 1), (217, 1), (220, 2), (221, 3), (230, 3)]
discards: [225 226 227 228]
Fitting a model of length 270 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 53s - loss: 709.8870 - loglik: -7.0672e+02 - logprior: -3.1681e+00
Epoch 2/2
33/33 - 50s - loss: 699.1694 - loglik: -6.9862e+02 - logprior: -5.5276e-01
Fitted a model with MAP estimate = -698.1401
expansions: [(270, 4)]
discards: [  1   2   3   7   8  43  57 182 193 194 195 214 257 265 266 267 268 269]
Fitting a model of length 256 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 48s - loss: 706.4074 - loglik: -7.0433e+02 - logprior: -2.0739e+00
Epoch 2/2
33/33 - 42s - loss: 699.4142 - loglik: -6.9930e+02 - logprior: -1.1517e-01
Fitted a model with MAP estimate = -699.5335
expansions: [(0, 5), (1, 1), (248, 1), (250, 1)]
discards: [252 253 254]
Fitting a model of length 261 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 39s - loss: 708.0959 - loglik: -7.0507e+02 - logprior: -3.0245e+00
Epoch 2/10
33/33 - 37s - loss: 699.2605 - loglik: -6.9919e+02 - logprior: -6.6823e-02
Epoch 3/10
33/33 - 37s - loss: 697.4150 - loglik: -6.9747e+02 - logprior: 0.0513
Epoch 4/10
33/33 - 36s - loss: 700.1787 - loglik: -7.0025e+02 - logprior: 0.0760
Fitted a model with MAP estimate = -696.9159
Time for alignment: 676.8291
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 34s - loss: 815.1144 - loglik: -8.1228e+02 - logprior: -2.8391e+00
Epoch 2/10
33/33 - 30s - loss: 722.6252 - loglik: -7.2189e+02 - logprior: -7.3834e-01
Epoch 3/10
33/33 - 29s - loss: 709.0463 - loglik: -7.0834e+02 - logprior: -7.0935e-01
Epoch 4/10
33/33 - 29s - loss: 708.8366 - loglik: -7.0809e+02 - logprior: -7.4752e-01
Epoch 5/10
33/33 - 29s - loss: 704.2496 - loglik: -7.0338e+02 - logprior: -8.6646e-01
Epoch 6/10
33/33 - 30s - loss: 708.8374 - loglik: -7.0785e+02 - logprior: -9.8636e-01
Fitted a model with MAP estimate = -706.0678
expansions: [(0, 5), (6, 1), (7, 1), (9, 1), (10, 2), (33, 4), (61, 1), (62, 2), (64, 1), (79, 2), (114, 1), (117, 1), (120, 2), (135, 2), (156, 3), (164, 4), (185, 1), (205, 1), (214, 1), (220, 2), (221, 4)]
discards: [  2   3 225 226 227 228]
Fitting a model of length 266 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 37s - loss: 710.2345 - loglik: -7.0710e+02 - logprior: -3.1376e+00
Epoch 2/2
33/33 - 34s - loss: 700.1382 - loglik: -6.9962e+02 - logprior: -5.1409e-01
Fitted a model with MAP estimate = -698.0687
expansions: [(266, 5)]
discards: [  1   5   6   7  42  74 141 181 192 193 194 255 262 263 264 265]
Fitting a model of length 255 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 37s - loss: 705.1518 - loglik: -7.0314e+02 - logprior: -2.0154e+00
Epoch 2/2
33/33 - 34s - loss: 701.8520 - loglik: -7.0180e+02 - logprior: -5.0376e-02
Fitted a model with MAP estimate = -699.6205
expansions: [(0, 5), (1, 1), (255, 4)]
discards: [251 252 253]
Fitting a model of length 262 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 42s - loss: 704.8737 - loglik: -7.0194e+02 - logprior: -2.9341e+00
Epoch 2/10
33/33 - 41s - loss: 702.2779 - loglik: -7.0223e+02 - logprior: -4.5914e-02
Epoch 3/10
33/33 - 41s - loss: 697.5618 - loglik: -6.9764e+02 - logprior: 0.0795
Epoch 4/10
33/33 - 39s - loss: 697.7070 - loglik: -6.9781e+02 - logprior: 0.0982
Fitted a model with MAP estimate = -696.3293
Time for alignment: 607.2946
Computed alignments with likelihoods: ['-694.7109', '-696.9159', '-696.3293']
Best model has likelihood: -694.7109
SP score = 0.7902
Training of 3 independent models on file profilin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20230af790>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f202331e250>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f210fe8db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f210fe8d400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 420.8025 - loglik: -3.4894e+02 - logprior: -7.1867e+01
Epoch 2/10
10/10 - 2s - loss: 323.3653 - loglik: -3.0690e+02 - logprior: -1.6462e+01
Epoch 3/10
10/10 - 2s - loss: 275.5802 - loglik: -2.6872e+02 - logprior: -6.8596e+00
Epoch 4/10
10/10 - 2s - loss: 249.4592 - loglik: -2.4545e+02 - logprior: -4.0115e+00
Epoch 5/10
10/10 - 2s - loss: 240.1199 - loglik: -2.3752e+02 - logprior: -2.5994e+00
Epoch 6/10
10/10 - 2s - loss: 235.6997 - loglik: -2.3400e+02 - logprior: -1.7008e+00
Epoch 7/10
10/10 - 2s - loss: 234.6488 - loglik: -2.3355e+02 - logprior: -1.0990e+00
Epoch 8/10
10/10 - 2s - loss: 232.9833 - loglik: -2.3219e+02 - logprior: -7.8970e-01
Epoch 9/10
10/10 - 2s - loss: 231.9752 - loglik: -2.3127e+02 - logprior: -7.0717e-01
Epoch 10/10
10/10 - 2s - loss: 231.3544 - loglik: -2.3082e+02 - logprior: -5.3432e-01
Fitted a model with MAP estimate = -231.4132
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 1), (40, 1), (45, 1), (55, 1), (63, 1), (64, 1), (71, 3), (78, 1), (79, 1), (83, 1), (87, 4), (88, 1)]
discards: [0]
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 304.3914 - loglik: -2.3232e+02 - logprior: -7.2072e+01
Epoch 2/2
10/10 - 2s - loss: 241.6516 - loglik: -2.1484e+02 - logprior: -2.6807e+01
Fitted a model with MAP estimate = -229.8317
expansions: [(0, 4)]
discards: [  0  87 109]
Fitting a model of length 127 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 265.7944 - loglik: -2.1090e+02 - logprior: -5.4892e+01
Epoch 2/2
10/10 - 2s - loss: 218.1367 - loglik: -2.0646e+02 - logprior: -1.1674e+01
Fitted a model with MAP estimate = -209.8442
expansions: []
discards: [ 1  2  3 16]
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 261.8570 - loglik: -2.0858e+02 - logprior: -5.3272e+01
Epoch 2/10
10/10 - 2s - loss: 218.2173 - loglik: -2.0724e+02 - logprior: -1.0976e+01
Epoch 3/10
10/10 - 2s - loss: 207.2810 - loglik: -2.0504e+02 - logprior: -2.2454e+00
Epoch 4/10
10/10 - 2s - loss: 202.3696 - loglik: -2.0379e+02 - logprior: 1.4207
Epoch 5/10
10/10 - 2s - loss: 199.2647 - loglik: -2.0273e+02 - logprior: 3.4641
Epoch 6/10
10/10 - 2s - loss: 199.0619 - loglik: -2.0370e+02 - logprior: 4.6406
Epoch 7/10
10/10 - 2s - loss: 197.0240 - loglik: -2.0238e+02 - logprior: 5.3594
Epoch 8/10
10/10 - 2s - loss: 196.9870 - loglik: -2.0281e+02 - logprior: 5.8251
Epoch 9/10
10/10 - 2s - loss: 195.9842 - loglik: -2.0220e+02 - logprior: 6.2151
Epoch 10/10
10/10 - 2s - loss: 195.8235 - loglik: -2.0248e+02 - logprior: 6.6562
Fitted a model with MAP estimate = -195.7460
Time for alignment: 69.3888
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 420.5641 - loglik: -3.4870e+02 - logprior: -7.1864e+01
Epoch 2/10
10/10 - 2s - loss: 323.5107 - loglik: -3.0703e+02 - logprior: -1.6484e+01
Epoch 3/10
10/10 - 2s - loss: 275.6413 - loglik: -2.6864e+02 - logprior: -7.0060e+00
Epoch 4/10
10/10 - 2s - loss: 248.7145 - loglik: -2.4436e+02 - logprior: -4.3553e+00
Epoch 5/10
10/10 - 2s - loss: 238.9102 - loglik: -2.3591e+02 - logprior: -3.0024e+00
Epoch 6/10
10/10 - 2s - loss: 235.2394 - loglik: -2.3322e+02 - logprior: -2.0222e+00
Epoch 7/10
10/10 - 2s - loss: 233.8888 - loglik: -2.3262e+02 - logprior: -1.2679e+00
Epoch 8/10
10/10 - 2s - loss: 232.3901 - loglik: -2.3151e+02 - logprior: -8.7736e-01
Epoch 9/10
10/10 - 2s - loss: 232.0194 - loglik: -2.3135e+02 - logprior: -6.7340e-01
Epoch 10/10
10/10 - 2s - loss: 231.8852 - loglik: -2.3145e+02 - logprior: -4.3063e-01
Fitted a model with MAP estimate = -231.2080
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (60, 2), (62, 1), (63, 2), (70, 1), (78, 1), (79, 1), (83, 1), (87, 4), (88, 1)]
discards: [0]
Fitting a model of length 127 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 304.9665 - loglik: -2.3279e+02 - logprior: -7.2178e+01
Epoch 2/2
10/10 - 2s - loss: 241.2154 - loglik: -2.1419e+02 - logprior: -2.7028e+01
Fitted a model with MAP estimate = -230.0284
expansions: [(0, 5)]
discards: [  0  73  79 110]
Fitting a model of length 128 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 266.3093 - loglik: -2.1118e+02 - logprior: -5.5124e+01
Epoch 2/2
10/10 - 2s - loss: 218.4213 - loglik: -2.0654e+02 - logprior: -1.1886e+01
Fitted a model with MAP estimate = -210.0962
expansions: []
discards: [ 1  2  3  4 17]
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 262.9546 - loglik: -2.0963e+02 - logprior: -5.3329e+01
Epoch 2/10
10/10 - 2s - loss: 217.4327 - loglik: -2.0638e+02 - logprior: -1.1054e+01
Epoch 3/10
10/10 - 2s - loss: 206.9373 - loglik: -2.0464e+02 - logprior: -2.3002e+00
Epoch 4/10
10/10 - 2s - loss: 202.5522 - loglik: -2.0391e+02 - logprior: 1.3560
Epoch 5/10
10/10 - 2s - loss: 199.6233 - loglik: -2.0300e+02 - logprior: 3.3803
Epoch 6/10
10/10 - 2s - loss: 198.3625 - loglik: -2.0289e+02 - logprior: 4.5275
Epoch 7/10
10/10 - 2s - loss: 196.9968 - loglik: -2.0220e+02 - logprior: 5.2007
Epoch 8/10
10/10 - 2s - loss: 196.7041 - loglik: -2.0239e+02 - logprior: 5.6898
Epoch 9/10
10/10 - 2s - loss: 196.0967 - loglik: -2.0227e+02 - logprior: 6.1710
Epoch 10/10
10/10 - 2s - loss: 196.0435 - loglik: -2.0268e+02 - logprior: 6.6413
Fitted a model with MAP estimate = -195.7456
Time for alignment: 65.6985
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 420.7794 - loglik: -3.4891e+02 - logprior: -7.1867e+01
Epoch 2/10
10/10 - 2s - loss: 323.4973 - loglik: -3.0702e+02 - logprior: -1.6472e+01
Epoch 3/10
10/10 - 2s - loss: 275.9598 - loglik: -2.6905e+02 - logprior: -6.9101e+00
Epoch 4/10
10/10 - 2s - loss: 250.3977 - loglik: -2.4642e+02 - logprior: -3.9759e+00
Epoch 5/10
10/10 - 2s - loss: 240.2703 - loglik: -2.3777e+02 - logprior: -2.5040e+00
Epoch 6/10
10/10 - 2s - loss: 236.5431 - loglik: -2.3502e+02 - logprior: -1.5190e+00
Epoch 7/10
10/10 - 2s - loss: 234.1170 - loglik: -2.3324e+02 - logprior: -8.7307e-01
Epoch 8/10
10/10 - 2s - loss: 233.6617 - loglik: -2.3312e+02 - logprior: -5.4540e-01
Epoch 9/10
10/10 - 2s - loss: 232.2328 - loglik: -2.3179e+02 - logprior: -4.4189e-01
Epoch 10/10
10/10 - 2s - loss: 231.6540 - loglik: -2.3135e+02 - logprior: -3.0495e-01
Fitted a model with MAP estimate = -231.5367
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 1), (71, 2), (78, 1), (79, 1), (83, 1), (87, 4), (88, 1)]
discards: []
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 286.9732 - loglik: -2.2908e+02 - logprior: -5.7888e+01
Epoch 2/2
10/10 - 2s - loss: 223.3889 - loglik: -2.1097e+02 - logprior: -1.2414e+01
Fitted a model with MAP estimate = -212.9756
expansions: []
discards: [  0 108]
Fitting a model of length 124 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 280.1909 - loglik: -2.1167e+02 - logprior: -6.8520e+01
Epoch 2/2
10/10 - 2s - loss: 234.0084 - loglik: -2.0793e+02 - logprior: -2.6077e+01
Fitted a model with MAP estimate = -226.4077
expansions: [(0, 4)]
discards: [ 0 12 16]
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 266.0139 - loglik: -2.1150e+02 - logprior: -5.4515e+01
Epoch 2/10
10/10 - 2s - loss: 217.9770 - loglik: -2.0674e+02 - logprior: -1.1237e+01
Epoch 3/10
10/10 - 2s - loss: 207.9487 - loglik: -2.0558e+02 - logprior: -2.3726e+00
Epoch 4/10
10/10 - 2s - loss: 202.6880 - loglik: -2.0390e+02 - logprior: 1.2072
Epoch 5/10
10/10 - 2s - loss: 199.8077 - loglik: -2.0297e+02 - logprior: 3.1651
Epoch 6/10
10/10 - 2s - loss: 198.5630 - loglik: -2.0299e+02 - logprior: 4.4244
Epoch 7/10
10/10 - 2s - loss: 197.9275 - loglik: -2.0320e+02 - logprior: 5.2721
Epoch 8/10
10/10 - 2s - loss: 197.5988 - loglik: -2.0347e+02 - logprior: 5.8685
Epoch 9/10
10/10 - 2s - loss: 196.4462 - loglik: -2.0277e+02 - logprior: 6.3228
Epoch 10/10
10/10 - 2s - loss: 196.3278 - loglik: -2.0303e+02 - logprior: 6.7021
Fitted a model with MAP estimate = -196.2464
Time for alignment: 67.6734
Computed alignments with likelihoods: ['-195.7460', '-195.7456', '-196.2464']
Best model has likelihood: -195.7456
SP score = 0.9386
Training of 3 independent models on file rrm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20d3cfce50>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20c2e35fa0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2153bf9910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2012040a90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 195.5930 - loglik: -1.9324e+02 - logprior: -2.3555e+00
Epoch 2/10
22/22 - 2s - loss: 162.5333 - loglik: -1.6119e+02 - logprior: -1.3419e+00
Epoch 3/10
22/22 - 2s - loss: 155.8200 - loglik: -1.5441e+02 - logprior: -1.4082e+00
Epoch 4/10
22/22 - 2s - loss: 154.4236 - loglik: -1.5311e+02 - logprior: -1.3173e+00
Epoch 5/10
22/22 - 2s - loss: 153.4155 - loglik: -1.5209e+02 - logprior: -1.3233e+00
Epoch 6/10
22/22 - 2s - loss: 153.3282 - loglik: -1.5201e+02 - logprior: -1.3186e+00
Epoch 7/10
22/22 - 2s - loss: 153.1341 - loglik: -1.5181e+02 - logprior: -1.3279e+00
Epoch 8/10
22/22 - 2s - loss: 152.7191 - loglik: -1.5138e+02 - logprior: -1.3351e+00
Epoch 9/10
22/22 - 2s - loss: 152.7989 - loglik: -1.5145e+02 - logprior: -1.3446e+00
Fitted a model with MAP estimate = -153.5256
expansions: [(8, 1), (9, 2), (12, 1), (14, 2), (17, 2), (21, 1), (22, 2), (28, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 156.5457 - loglik: -1.5355e+02 - logprior: -2.9918e+00
Epoch 2/2
22/22 - 2s - loss: 148.0753 - loglik: -1.4654e+02 - logprior: -1.5330e+00
Fitted a model with MAP estimate = -146.0457
expansions: [(0, 2)]
discards: [ 0  9 17 22 30 67]
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 7s - loss: 147.3417 - loglik: -1.4523e+02 - logprior: -2.1123e+00
Epoch 2/2
22/22 - 2s - loss: 144.8240 - loglik: -1.4382e+02 - logprior: -1.0082e+00
Fitted a model with MAP estimate = -144.7193
expansions: []
discards: [0]
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 145.2316 - loglik: -1.4403e+02 - logprior: -1.2007e+00
Epoch 2/10
32/32 - 3s - loss: 142.9605 - loglik: -1.4208e+02 - logprior: -8.8428e-01
Epoch 3/10
32/32 - 3s - loss: 142.5185 - loglik: -1.4165e+02 - logprior: -8.6555e-01
Epoch 4/10
32/32 - 3s - loss: 142.3758 - loglik: -1.4152e+02 - logprior: -8.5895e-01
Epoch 5/10
32/32 - 3s - loss: 141.8513 - loglik: -1.4099e+02 - logprior: -8.6478e-01
Epoch 6/10
32/32 - 3s - loss: 141.1799 - loglik: -1.4030e+02 - logprior: -8.7863e-01
Epoch 7/10
32/32 - 3s - loss: 141.5165 - loglik: -1.4063e+02 - logprior: -8.8323e-01
Fitted a model with MAP estimate = -141.0356
Time for alignment: 82.0083
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 6s - loss: 195.4859 - loglik: -1.9313e+02 - logprior: -2.3533e+00
Epoch 2/10
22/22 - 2s - loss: 162.8713 - loglik: -1.6154e+02 - logprior: -1.3349e+00
Epoch 3/10
22/22 - 2s - loss: 156.2429 - loglik: -1.5483e+02 - logprior: -1.4120e+00
Epoch 4/10
22/22 - 2s - loss: 154.7984 - loglik: -1.5348e+02 - logprior: -1.3181e+00
Epoch 5/10
22/22 - 2s - loss: 153.8325 - loglik: -1.5250e+02 - logprior: -1.3315e+00
Epoch 6/10
22/22 - 2s - loss: 153.5037 - loglik: -1.5217e+02 - logprior: -1.3337e+00
Epoch 7/10
22/22 - 2s - loss: 153.1203 - loglik: -1.5177e+02 - logprior: -1.3480e+00
Epoch 8/10
22/22 - 2s - loss: 153.1712 - loglik: -1.5182e+02 - logprior: -1.3504e+00
Fitted a model with MAP estimate = -153.3054
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (14, 2), (21, 1), (22, 2), (25, 1), (40, 1), (41, 1), (44, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 156.7373 - loglik: -1.5375e+02 - logprior: -2.9861e+00
Epoch 2/2
22/22 - 2s - loss: 147.9967 - loglik: -1.4647e+02 - logprior: -1.5312e+00
Fitted a model with MAP estimate = -145.9350
expansions: [(0, 2)]
discards: [ 0  9 17 19 30 67]
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 147.4741 - loglik: -1.4536e+02 - logprior: -2.1103e+00
Epoch 2/2
22/22 - 2s - loss: 144.6862 - loglik: -1.4368e+02 - logprior: -1.0083e+00
Fitted a model with MAP estimate = -144.7540
expansions: []
discards: [0]
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 7s - loss: 145.2813 - loglik: -1.4407e+02 - logprior: -1.2118e+00
Epoch 2/10
32/32 - 3s - loss: 142.8463 - loglik: -1.4196e+02 - logprior: -8.8309e-01
Epoch 3/10
32/32 - 3s - loss: 142.6410 - loglik: -1.4178e+02 - logprior: -8.5954e-01
Epoch 4/10
32/32 - 3s - loss: 142.2847 - loglik: -1.4142e+02 - logprior: -8.6410e-01
Epoch 5/10
32/32 - 3s - loss: 141.9955 - loglik: -1.4113e+02 - logprior: -8.6886e-01
Epoch 6/10
32/32 - 3s - loss: 141.5300 - loglik: -1.4065e+02 - logprior: -8.7880e-01
Epoch 7/10
32/32 - 3s - loss: 141.3451 - loglik: -1.4046e+02 - logprior: -8.8989e-01
Epoch 8/10
32/32 - 3s - loss: 141.0123 - loglik: -1.4011e+02 - logprior: -9.0670e-01
Epoch 9/10
32/32 - 3s - loss: 140.8336 - loglik: -1.3991e+02 - logprior: -9.2231e-01
Epoch 10/10
32/32 - 3s - loss: 140.8404 - loglik: -1.3990e+02 - logprior: -9.3892e-01
Fitted a model with MAP estimate = -140.6185
Time for alignment: 90.3199
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 195.7114 - loglik: -1.9336e+02 - logprior: -2.3537e+00
Epoch 2/10
22/22 - 2s - loss: 163.2326 - loglik: -1.6190e+02 - logprior: -1.3305e+00
Epoch 3/10
22/22 - 2s - loss: 156.5775 - loglik: -1.5518e+02 - logprior: -1.3996e+00
Epoch 4/10
22/22 - 2s - loss: 154.7402 - loglik: -1.5343e+02 - logprior: -1.3114e+00
Epoch 5/10
22/22 - 2s - loss: 153.7720 - loglik: -1.5244e+02 - logprior: -1.3316e+00
Epoch 6/10
22/22 - 2s - loss: 153.1458 - loglik: -1.5182e+02 - logprior: -1.3306e+00
Epoch 7/10
22/22 - 2s - loss: 153.1134 - loglik: -1.5177e+02 - logprior: -1.3422e+00
Epoch 8/10
22/22 - 2s - loss: 152.6723 - loglik: -1.5132e+02 - logprior: -1.3484e+00
Epoch 9/10
22/22 - 2s - loss: 152.7587 - loglik: -1.5139e+02 - logprior: -1.3662e+00
Fitted a model with MAP estimate = -153.6530
expansions: [(8, 1), (9, 2), (12, 1), (13, 2), (20, 2), (21, 1), (22, 2), (24, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 156.6809 - loglik: -1.5369e+02 - logprior: -2.9939e+00
Epoch 2/2
22/22 - 2s - loss: 148.2358 - loglik: -1.4670e+02 - logprior: -1.5350e+00
Fitted a model with MAP estimate = -145.8675
expansions: [(0, 2)]
discards: [ 0  9 17 25 30 67]
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 147.5970 - loglik: -1.4548e+02 - logprior: -2.1122e+00
Epoch 2/2
22/22 - 2s - loss: 144.6810 - loglik: -1.4367e+02 - logprior: -1.0066e+00
Fitted a model with MAP estimate = -144.7608
expansions: []
discards: [0]
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 145.1643 - loglik: -1.4396e+02 - logprior: -1.2059e+00
Epoch 2/10
32/32 - 3s - loss: 143.0359 - loglik: -1.4215e+02 - logprior: -8.8735e-01
Epoch 3/10
32/32 - 3s - loss: 142.5915 - loglik: -1.4173e+02 - logprior: -8.6252e-01
Epoch 4/10
32/32 - 3s - loss: 142.1774 - loglik: -1.4131e+02 - logprior: -8.6834e-01
Epoch 5/10
32/32 - 3s - loss: 141.9800 - loglik: -1.4111e+02 - logprior: -8.6825e-01
Epoch 6/10
32/32 - 3s - loss: 141.6485 - loglik: -1.4077e+02 - logprior: -8.7894e-01
Epoch 7/10
32/32 - 3s - loss: 141.1871 - loglik: -1.4030e+02 - logprior: -8.8678e-01
Epoch 8/10
32/32 - 3s - loss: 140.8726 - loglik: -1.3996e+02 - logprior: -9.0898e-01
Epoch 9/10
32/32 - 3s - loss: 140.9818 - loglik: -1.4006e+02 - logprior: -9.2439e-01
Fitted a model with MAP estimate = -140.7376
Time for alignment: 86.9157
Computed alignments with likelihoods: ['-141.0356', '-140.6185', '-140.7376']
Best model has likelihood: -140.6185
SP score = 0.8554
Training of 3 independent models on file KAS.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20fe81a760>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1fffaabf70>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20a1010af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20a1010970>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 162 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 505.2621 - loglik: -4.8079e+02 - logprior: -2.4470e+01
Epoch 2/10
10/10 - 7s - loss: 393.8807 - loglik: -3.8916e+02 - logprior: -4.7165e+00
Epoch 3/10
10/10 - 7s - loss: 308.7650 - loglik: -3.0680e+02 - logprior: -1.9652e+00
Epoch 4/10
10/10 - 7s - loss: 267.9461 - loglik: -2.6618e+02 - logprior: -1.7695e+00
Epoch 5/10
10/10 - 7s - loss: 255.0694 - loglik: -2.5363e+02 - logprior: -1.4434e+00
Epoch 6/10
10/10 - 7s - loss: 249.6248 - loglik: -2.4831e+02 - logprior: -1.3157e+00
Epoch 7/10
10/10 - 7s - loss: 249.6243 - loglik: -2.4840e+02 - logprior: -1.2259e+00
Epoch 8/10
10/10 - 7s - loss: 245.7177 - loglik: -2.4452e+02 - logprior: -1.1984e+00
Epoch 9/10
10/10 - 7s - loss: 247.7385 - loglik: -2.4657e+02 - logprior: -1.1636e+00
Fitted a model with MAP estimate = -246.6318
expansions: [(25, 1), (50, 1), (59, 1), (80, 1), (89, 1), (92, 1), (137, 1)]
discards: [0]
Fitting a model of length 168 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 279.6511 - loglik: -2.5651e+02 - logprior: -2.3141e+01
Epoch 2/2
10/10 - 8s - loss: 252.8230 - loglik: -2.4401e+02 - logprior: -8.8113e+00
Fitted a model with MAP estimate = -249.8877
expansions: [(0, 16)]
discards: [ 0 44 45]
Fitting a model of length 181 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 267.2454 - loglik: -2.4798e+02 - logprior: -1.9262e+01
Epoch 2/2
10/10 - 8s - loss: 245.5291 - loglik: -2.4098e+02 - logprior: -4.5485e+00
Fitted a model with MAP estimate = -238.5515
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 59 60]
Fitting a model of length 164 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 274.1752 - loglik: -2.5303e+02 - logprior: -2.1147e+01
Epoch 2/10
10/10 - 7s - loss: 253.3251 - loglik: -2.4800e+02 - logprior: -5.3271e+00
Epoch 3/10
10/10 - 7s - loss: 249.4238 - loglik: -2.4814e+02 - logprior: -1.2800e+00
Epoch 4/10
10/10 - 6s - loss: 247.3225 - loglik: -2.4723e+02 - logprior: -8.8366e-02
Epoch 5/10
10/10 - 7s - loss: 243.7660 - loglik: -2.4398e+02 - logprior: 0.2111
Epoch 6/10
10/10 - 7s - loss: 244.4887 - loglik: -2.4495e+02 - logprior: 0.4577
Fitted a model with MAP estimate = -243.8696
Time for alignment: 168.7777
Fitting a model of length 162 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 507.0553 - loglik: -4.8258e+02 - logprior: -2.4478e+01
Epoch 2/10
10/10 - 7s - loss: 395.3264 - loglik: -3.9076e+02 - logprior: -4.5684e+00
Epoch 3/10
10/10 - 7s - loss: 316.5041 - loglik: -3.1502e+02 - logprior: -1.4870e+00
Epoch 4/10
10/10 - 7s - loss: 276.6456 - loglik: -2.7573e+02 - logprior: -9.1777e-01
Epoch 5/10
10/10 - 7s - loss: 264.0003 - loglik: -2.6333e+02 - logprior: -6.7234e-01
Epoch 6/10
10/10 - 7s - loss: 255.6966 - loglik: -2.5533e+02 - logprior: -3.6321e-01
Epoch 7/10
10/10 - 7s - loss: 257.1021 - loglik: -2.5684e+02 - logprior: -2.6157e-01
Fitted a model with MAP estimate = -255.8089
expansions: [(0, 37), (24, 1), (25, 1), (50, 1), (60, 1), (80, 1), (89, 1), (92, 1), (136, 1)]
discards: []
Fitting a model of length 207 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 14s - loss: 281.1320 - loglik: -2.5707e+02 - logprior: -2.4062e+01
Epoch 2/2
10/10 - 10s - loss: 239.2817 - loglik: -2.3332e+02 - logprior: -5.9586e+00
Fitted a model with MAP estimate = -227.7330
expansions: [(0, 18)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32]
Fitting a model of length 192 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 12s - loss: 268.2183 - loglik: -2.4754e+02 - logprior: -2.0681e+01
Epoch 2/2
10/10 - 10s - loss: 236.8953 - loglik: -2.3268e+02 - logprior: -4.2167e+00
Fitted a model with MAP estimate = -230.5470
expansions: [(0, 27), (47, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Fitting a model of length 202 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 13s - loss: 261.6674 - loglik: -2.4269e+02 - logprior: -1.8977e+01
Epoch 2/10
10/10 - 10s - loss: 232.4570 - loglik: -2.2919e+02 - logprior: -3.2685e+00
Epoch 3/10
10/10 - 10s - loss: 218.3859 - loglik: -2.1802e+02 - logprior: -3.6965e-01
Epoch 4/10
10/10 - 10s - loss: 215.8472 - loglik: -2.1643e+02 - logprior: 0.5824
Epoch 5/10
10/10 - 10s - loss: 211.2908 - loglik: -2.1241e+02 - logprior: 1.1157
Epoch 6/10
10/10 - 10s - loss: 209.7021 - loglik: -2.1113e+02 - logprior: 1.4287
Epoch 7/10
10/10 - 11s - loss: 207.0088 - loglik: -2.0864e+02 - logprior: 1.6288
Epoch 8/10
10/10 - 10s - loss: 210.1847 - loglik: -2.1197e+02 - logprior: 1.7836
Fitted a model with MAP estimate = -207.5642
Time for alignment: 204.1800
Fitting a model of length 162 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 503.9955 - loglik: -4.7952e+02 - logprior: -2.4471e+01
Epoch 2/10
10/10 - 6s - loss: 394.5717 - loglik: -3.8985e+02 - logprior: -4.7182e+00
Epoch 3/10
10/10 - 7s - loss: 308.1523 - loglik: -3.0613e+02 - logprior: -2.0248e+00
Epoch 4/10
10/10 - 6s - loss: 269.8247 - loglik: -2.6797e+02 - logprior: -1.8585e+00
Epoch 5/10
10/10 - 7s - loss: 255.3886 - loglik: -2.5372e+02 - logprior: -1.6703e+00
Epoch 6/10
10/10 - 6s - loss: 249.4816 - loglik: -2.4808e+02 - logprior: -1.3991e+00
Epoch 7/10
10/10 - 6s - loss: 248.0918 - loglik: -2.4676e+02 - logprior: -1.3360e+00
Epoch 8/10
10/10 - 7s - loss: 246.2491 - loglik: -2.4489e+02 - logprior: -1.3604e+00
Epoch 9/10
10/10 - 7s - loss: 247.5396 - loglik: -2.4621e+02 - logprior: -1.3304e+00
Fitted a model with MAP estimate = -245.9354
expansions: [(31, 1), (50, 1), (60, 1), (82, 1), (89, 1), (96, 1), (137, 1)]
discards: [0]
Fitting a model of length 168 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 278.2990 - loglik: -2.5506e+02 - logprior: -2.3236e+01
Epoch 2/2
10/10 - 7s - loss: 252.2268 - loglik: -2.4338e+02 - logprior: -8.8458e+00
Fitted a model with MAP estimate = -248.7195
expansions: [(0, 16)]
discards: [ 0 47]
Fitting a model of length 182 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 12s - loss: 266.2910 - loglik: -2.4695e+02 - logprior: -1.9343e+01
Epoch 2/2
10/10 - 8s - loss: 242.9536 - loglik: -2.3841e+02 - logprior: -4.5476e+00
Fitted a model with MAP estimate = -238.1104
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]
Fitting a model of length 167 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 268.2548 - loglik: -2.4605e+02 - logprior: -2.2206e+01
Epoch 2/10
10/10 - 6s - loss: 247.2068 - loglik: -2.4042e+02 - logprior: -6.7837e+00
Epoch 3/10
10/10 - 6s - loss: 242.1680 - loglik: -2.4026e+02 - logprior: -1.9128e+00
Epoch 4/10
10/10 - 7s - loss: 240.5359 - loglik: -2.4030e+02 - logprior: -2.4081e-01
Epoch 5/10
10/10 - 7s - loss: 237.4963 - loglik: -2.3780e+02 - logprior: 0.3026
Epoch 6/10
10/10 - 7s - loss: 238.4911 - loglik: -2.3896e+02 - logprior: 0.4668
Fitted a model with MAP estimate = -237.0775
Time for alignment: 158.8828
Computed alignments with likelihoods: ['-238.5515', '-207.5642', '-237.0775']
Best model has likelihood: -207.5642
SP score = 0.4414
Training of 3 independent models on file GEL.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20d4144c40>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2107459be0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2107459df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2107459ca0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 247.8386 - loglik: -2.2565e+02 - logprior: -2.2189e+01
Epoch 2/10
10/10 - 1s - loss: 216.8164 - loglik: -2.1098e+02 - logprior: -5.8387e+00
Epoch 3/10
10/10 - 1s - loss: 198.8438 - loglik: -1.9581e+02 - logprior: -3.0357e+00
Epoch 4/10
10/10 - 1s - loss: 190.3066 - loglik: -1.8802e+02 - logprior: -2.2877e+00
Epoch 5/10
10/10 - 1s - loss: 187.1026 - loglik: -1.8505e+02 - logprior: -2.0529e+00
Epoch 6/10
10/10 - 1s - loss: 185.5362 - loglik: -1.8365e+02 - logprior: -1.8826e+00
Epoch 7/10
10/10 - 1s - loss: 184.5577 - loglik: -1.8296e+02 - logprior: -1.5991e+00
Epoch 8/10
10/10 - 1s - loss: 184.5304 - loglik: -1.8307e+02 - logprior: -1.4593e+00
Epoch 9/10
10/10 - 1s - loss: 183.8408 - loglik: -1.8237e+02 - logprior: -1.4661e+00
Epoch 10/10
10/10 - 1s - loss: 183.6902 - loglik: -1.8221e+02 - logprior: -1.4756e+00
Fitted a model with MAP estimate = -183.6008
expansions: [(0, 2), (7, 2), (8, 2), (23, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 2), (46, 1), (48, 1), (51, 1), (53, 1)]
discards: []
Fitting a model of length 80 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 210.3643 - loglik: -1.8321e+02 - logprior: -2.7150e+01
Epoch 2/2
10/10 - 1s - loss: 187.4042 - loglik: -1.7910e+02 - logprior: -8.3018e+00
Fitted a model with MAP estimate = -183.0729
expansions: []
discards: [ 0 10 12 53 55 63]
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.4440 - loglik: -1.8013e+02 - logprior: -2.3310e+01
Epoch 2/2
10/10 - 1s - loss: 188.4433 - loglik: -1.7932e+02 - logprior: -9.1255e+00
Fitted a model with MAP estimate = -185.6786
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 198.6709 - loglik: -1.7831e+02 - logprior: -2.0362e+01
Epoch 2/10
10/10 - 1s - loss: 182.9788 - loglik: -1.7757e+02 - logprior: -5.4115e+00
Epoch 3/10
10/10 - 1s - loss: 180.2503 - loglik: -1.7794e+02 - logprior: -2.3144e+00
Epoch 4/10
10/10 - 1s - loss: 178.6711 - loglik: -1.7736e+02 - logprior: -1.3146e+00
Epoch 5/10
10/10 - 1s - loss: 177.6578 - loglik: -1.7672e+02 - logprior: -9.3524e-01
Epoch 6/10
10/10 - 1s - loss: 177.4037 - loglik: -1.7661e+02 - logprior: -7.9278e-01
Epoch 7/10
10/10 - 1s - loss: 176.9977 - loglik: -1.7636e+02 - logprior: -6.3529e-01
Epoch 8/10
10/10 - 1s - loss: 177.0801 - loglik: -1.7664e+02 - logprior: -4.3633e-01
Fitted a model with MAP estimate = -176.7802
Time for alignment: 45.1197
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 248.0545 - loglik: -2.2586e+02 - logprior: -2.2191e+01
Epoch 2/10
10/10 - 1s - loss: 217.2210 - loglik: -2.1139e+02 - logprior: -5.8339e+00
Epoch 3/10
10/10 - 1s - loss: 199.5932 - loglik: -1.9660e+02 - logprior: -2.9917e+00
Epoch 4/10
10/10 - 1s - loss: 190.8549 - loglik: -1.8861e+02 - logprior: -2.2463e+00
Epoch 5/10
10/10 - 1s - loss: 188.0057 - loglik: -1.8599e+02 - logprior: -2.0164e+00
Epoch 6/10
10/10 - 1s - loss: 185.9483 - loglik: -1.8407e+02 - logprior: -1.8761e+00
Epoch 7/10
10/10 - 1s - loss: 185.1847 - loglik: -1.8355e+02 - logprior: -1.6314e+00
Epoch 8/10
10/10 - 1s - loss: 184.3713 - loglik: -1.8288e+02 - logprior: -1.4956e+00
Epoch 9/10
10/10 - 1s - loss: 184.2395 - loglik: -1.8272e+02 - logprior: -1.5222e+00
Epoch 10/10
10/10 - 1s - loss: 183.5566 - loglik: -1.8202e+02 - logprior: -1.5407e+00
Fitted a model with MAP estimate = -183.6759
expansions: [(0, 2), (8, 1), (9, 1), (39, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 2), (46, 1), (48, 1), (51, 1), (53, 1)]
discards: []
Fitting a model of length 78 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 209.9105 - loglik: -1.8281e+02 - logprior: -2.7097e+01
Epoch 2/2
10/10 - 1s - loss: 187.2965 - loglik: -1.7908e+02 - logprior: -8.2200e+00
Fitted a model with MAP estimate = -182.9349
expansions: []
discards: [ 0 51 53 61]
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 202.9982 - loglik: -1.7970e+02 - logprior: -2.3300e+01
Epoch 2/2
10/10 - 1s - loss: 188.2856 - loglik: -1.7918e+02 - logprior: -9.1076e+00
Fitted a model with MAP estimate = -185.4974
expansions: [(0, 2), (51, 1)]
discards: [0]
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 198.5444 - loglik: -1.7819e+02 - logprior: -2.0356e+01
Epoch 2/10
10/10 - 1s - loss: 182.6847 - loglik: -1.7731e+02 - logprior: -5.3746e+00
Epoch 3/10
10/10 - 1s - loss: 179.5816 - loglik: -1.7729e+02 - logprior: -2.2953e+00
Epoch 4/10
10/10 - 1s - loss: 178.1293 - loglik: -1.7686e+02 - logprior: -1.2704e+00
Epoch 5/10
10/10 - 1s - loss: 177.3950 - loglik: -1.7650e+02 - logprior: -8.9031e-01
Epoch 6/10
10/10 - 1s - loss: 176.7908 - loglik: -1.7605e+02 - logprior: -7.4106e-01
Epoch 7/10
10/10 - 1s - loss: 176.6483 - loglik: -1.7607e+02 - logprior: -5.7705e-01
Epoch 8/10
10/10 - 1s - loss: 176.3355 - loglik: -1.7596e+02 - logprior: -3.7223e-01
Epoch 9/10
10/10 - 1s - loss: 176.4214 - loglik: -1.7618e+02 - logprior: -2.4206e-01
Fitted a model with MAP estimate = -176.1910
Time for alignment: 46.7605
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 248.1658 - loglik: -2.2597e+02 - logprior: -2.2192e+01
Epoch 2/10
10/10 - 1s - loss: 216.4114 - loglik: -2.1057e+02 - logprior: -5.8376e+00
Epoch 3/10
10/10 - 1s - loss: 199.0059 - loglik: -1.9599e+02 - logprior: -3.0138e+00
Epoch 4/10
10/10 - 1s - loss: 190.6015 - loglik: -1.8834e+02 - logprior: -2.2656e+00
Epoch 5/10
10/10 - 1s - loss: 187.4688 - loglik: -1.8542e+02 - logprior: -2.0500e+00
Epoch 6/10
10/10 - 1s - loss: 185.4264 - loglik: -1.8348e+02 - logprior: -1.9444e+00
Epoch 7/10
10/10 - 1s - loss: 184.5559 - loglik: -1.8285e+02 - logprior: -1.7014e+00
Epoch 8/10
10/10 - 1s - loss: 184.1727 - loglik: -1.8261e+02 - logprior: -1.5655e+00
Epoch 9/10
10/10 - 1s - loss: 183.9185 - loglik: -1.8237e+02 - logprior: -1.5449e+00
Epoch 10/10
10/10 - 1s - loss: 183.8014 - loglik: -1.8227e+02 - logprior: -1.5336e+00
Fitted a model with MAP estimate = -183.7099
expansions: [(0, 2), (8, 1), (9, 1), (23, 2), (41, 2), (42, 1), (44, 1), (45, 2), (46, 1), (48, 1), (51, 1), (53, 1)]
discards: []
Fitting a model of length 78 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 210.8105 - loglik: -1.8371e+02 - logprior: -2.7099e+01
Epoch 2/2
10/10 - 1s - loss: 188.0388 - loglik: -1.7978e+02 - logprior: -8.2603e+00
Fitted a model with MAP estimate = -183.8455
expansions: []
discards: [ 0 27 53 61]
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.3741 - loglik: -1.8007e+02 - logprior: -2.3304e+01
Epoch 2/2
10/10 - 1s - loss: 188.8874 - loglik: -1.7980e+02 - logprior: -9.0906e+00
Fitted a model with MAP estimate = -186.1248
expansions: [(0, 2), (51, 1)]
discards: [0]
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 199.1902 - loglik: -1.7885e+02 - logprior: -2.0337e+01
Epoch 2/10
10/10 - 1s - loss: 183.4378 - loglik: -1.7808e+02 - logprior: -5.3550e+00
Epoch 3/10
10/10 - 1s - loss: 180.3150 - loglik: -1.7805e+02 - logprior: -2.2631e+00
Epoch 4/10
10/10 - 1s - loss: 178.7022 - loglik: -1.7746e+02 - logprior: -1.2424e+00
Epoch 5/10
10/10 - 1s - loss: 178.1004 - loglik: -1.7726e+02 - logprior: -8.4473e-01
Epoch 6/10
10/10 - 1s - loss: 177.6233 - loglik: -1.7692e+02 - logprior: -7.0262e-01
Epoch 7/10
10/10 - 1s - loss: 177.4403 - loglik: -1.7690e+02 - logprior: -5.3593e-01
Epoch 8/10
10/10 - 1s - loss: 176.6703 - loglik: -1.7634e+02 - logprior: -3.3060e-01
Epoch 9/10
10/10 - 1s - loss: 177.0329 - loglik: -1.7684e+02 - logprior: -1.9484e-01
Fitted a model with MAP estimate = -176.7475
Time for alignment: 44.4415
Computed alignments with likelihoods: ['-176.7802', '-176.1910', '-176.7475']
Best model has likelihood: -176.1910
SP score = 0.6667
Training of 3 independent models on file hr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f21535b2f10>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f201a4da370>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202e548790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202d2cf1c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 200.0077 - loglik: -1.8754e+02 - logprior: -1.2465e+01
Epoch 2/10
11/11 - 1s - loss: 157.6220 - loglik: -1.5423e+02 - logprior: -3.3948e+00
Epoch 3/10
11/11 - 1s - loss: 123.4233 - loglik: -1.2103e+02 - logprior: -2.3910e+00
Epoch 4/10
11/11 - 1s - loss: 108.6965 - loglik: -1.0639e+02 - logprior: -2.3100e+00
Epoch 5/10
11/11 - 1s - loss: 105.0582 - loglik: -1.0306e+02 - logprior: -2.0024e+00
Epoch 6/10
11/11 - 1s - loss: 103.0190 - loglik: -1.0104e+02 - logprior: -1.9765e+00
Epoch 7/10
11/11 - 1s - loss: 102.2965 - loglik: -1.0026e+02 - logprior: -2.0382e+00
Epoch 8/10
11/11 - 1s - loss: 101.9302 - loglik: -9.9895e+01 - logprior: -2.0355e+00
Epoch 9/10
11/11 - 1s - loss: 101.1045 - loglik: -9.9058e+01 - logprior: -2.0468e+00
Epoch 10/10
11/11 - 1s - loss: 101.3222 - loglik: -9.9281e+01 - logprior: -2.0414e+00
Fitted a model with MAP estimate = -101.1899
expansions: [(0, 3), (15, 1), (26, 1), (27, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (33, 1), (34, 1), (37, 1), (43, 1)]
discards: []
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 113.0374 - loglik: -9.8990e+01 - logprior: -1.4047e+01
Epoch 2/2
11/11 - 1s - loss: 95.8233 - loglik: -9.1518e+01 - logprior: -4.3057e+00
Fitted a model with MAP estimate = -93.1552
expansions: []
discards: [ 0 36 39 44]
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 105.9208 - loglik: -9.2798e+01 - logprior: -1.3123e+01
Epoch 2/2
11/11 - 1s - loss: 96.3694 - loglik: -9.0989e+01 - logprior: -5.3802e+00
Fitted a model with MAP estimate = -94.4992
expansions: [(0, 1)]
discards: [0]
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 101.9243 - loglik: -9.1119e+01 - logprior: -1.0805e+01
Epoch 2/10
11/11 - 1s - loss: 93.2976 - loglik: -9.0373e+01 - logprior: -2.9250e+00
Epoch 3/10
11/11 - 1s - loss: 91.8843 - loglik: -9.0134e+01 - logprior: -1.7506e+00
Epoch 4/10
11/11 - 1s - loss: 90.7743 - loglik: -8.9222e+01 - logprior: -1.5525e+00
Epoch 5/10
11/11 - 1s - loss: 90.9177 - loglik: -8.9438e+01 - logprior: -1.4798e+00
Fitted a model with MAP estimate = -90.4411
Time for alignment: 37.5093
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 200.0591 - loglik: -1.8760e+02 - logprior: -1.2462e+01
Epoch 2/10
11/11 - 1s - loss: 158.4861 - loglik: -1.5508e+02 - logprior: -3.4096e+00
Epoch 3/10
11/11 - 1s - loss: 124.9735 - loglik: -1.2253e+02 - logprior: -2.4447e+00
Epoch 4/10
11/11 - 1s - loss: 109.5210 - loglik: -1.0720e+02 - logprior: -2.3222e+00
Epoch 5/10
11/11 - 1s - loss: 105.0513 - loglik: -1.0304e+02 - logprior: -2.0126e+00
Epoch 6/10
11/11 - 1s - loss: 104.0463 - loglik: -1.0207e+02 - logprior: -1.9731e+00
Epoch 7/10
11/11 - 1s - loss: 102.9059 - loglik: -1.0093e+02 - logprior: -1.9776e+00
Epoch 8/10
11/11 - 1s - loss: 102.6924 - loglik: -1.0074e+02 - logprior: -1.9475e+00
Epoch 9/10
11/11 - 1s - loss: 102.5738 - loglik: -1.0063e+02 - logprior: -1.9472e+00
Epoch 10/10
11/11 - 1s - loss: 102.5054 - loglik: -1.0057e+02 - logprior: -1.9398e+00
Fitted a model with MAP estimate = -102.3519
expansions: [(0, 3), (15, 1), (26, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 1), (35, 1), (38, 2), (43, 1)]
discards: []
Fitting a model of length 72 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 111.6180 - loglik: -9.7666e+01 - logprior: -1.3952e+01
Epoch 2/2
11/11 - 1s - loss: 95.6711 - loglik: -9.1521e+01 - logprior: -4.1496e+00
Fitted a model with MAP estimate = -92.8954
expansions: []
discards: [ 0 38]
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 105.7755 - loglik: -9.2681e+01 - logprior: -1.3095e+01
Epoch 2/2
11/11 - 1s - loss: 96.3569 - loglik: -9.1003e+01 - logprior: -5.3538e+00
Fitted a model with MAP estimate = -94.4548
expansions: [(0, 1)]
discards: [0]
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 101.6280 - loglik: -9.0843e+01 - logprior: -1.0785e+01
Epoch 2/10
11/11 - 1s - loss: 93.6088 - loglik: -9.0685e+01 - logprior: -2.9237e+00
Epoch 3/10
11/11 - 1s - loss: 91.4740 - loglik: -8.9728e+01 - logprior: -1.7460e+00
Epoch 4/10
11/11 - 1s - loss: 91.3493 - loglik: -8.9804e+01 - logprior: -1.5457e+00
Epoch 5/10
11/11 - 1s - loss: 90.1779 - loglik: -8.8705e+01 - logprior: -1.4731e+00
Epoch 6/10
11/11 - 1s - loss: 90.8096 - loglik: -8.9477e+01 - logprior: -1.3329e+00
Fitted a model with MAP estimate = -90.2347
Time for alignment: 37.9253
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 199.9407 - loglik: -1.8748e+02 - logprior: -1.2462e+01
Epoch 2/10
11/11 - 1s - loss: 157.9579 - loglik: -1.5455e+02 - logprior: -3.4068e+00
Epoch 3/10
11/11 - 1s - loss: 126.1350 - loglik: -1.2371e+02 - logprior: -2.4225e+00
Epoch 4/10
11/11 - 1s - loss: 111.0306 - loglik: -1.0874e+02 - logprior: -2.2870e+00
Epoch 5/10
11/11 - 1s - loss: 106.3211 - loglik: -1.0436e+02 - logprior: -1.9607e+00
Epoch 6/10
11/11 - 1s - loss: 105.1415 - loglik: -1.0325e+02 - logprior: -1.8952e+00
Epoch 7/10
11/11 - 1s - loss: 104.0478 - loglik: -1.0216e+02 - logprior: -1.8877e+00
Epoch 8/10
11/11 - 1s - loss: 104.0601 - loglik: -1.0220e+02 - logprior: -1.8641e+00
Fitted a model with MAP estimate = -103.7762
expansions: [(0, 3), (15, 2), (28, 1), (29, 3), (30, 2), (31, 1), (32, 1), (35, 1), (38, 2), (43, 1)]
discards: []
Fitting a model of length 73 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 112.5629 - loglik: -9.8636e+01 - logprior: -1.3927e+01
Epoch 2/2
11/11 - 1s - loss: 95.7882 - loglik: -9.1593e+01 - logprior: -4.1948e+00
Fitted a model with MAP estimate = -93.1003
expansions: []
discards: [ 0 35 39]
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 105.5295 - loglik: -9.2417e+01 - logprior: -1.3112e+01
Epoch 2/2
11/11 - 1s - loss: 96.7500 - loglik: -9.1379e+01 - logprior: -5.3711e+00
Fitted a model with MAP estimate = -94.5351
expansions: [(0, 1)]
discards: [0]
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 101.6727 - loglik: -9.0878e+01 - logprior: -1.0794e+01
Epoch 2/10
11/11 - 1s - loss: 93.5560 - loglik: -9.0626e+01 - logprior: -2.9299e+00
Epoch 3/10
11/11 - 1s - loss: 91.7196 - loglik: -8.9965e+01 - logprior: -1.7541e+00
Epoch 4/10
11/11 - 1s - loss: 91.1113 - loglik: -8.9559e+01 - logprior: -1.5523e+00
Epoch 5/10
11/11 - 1s - loss: 90.8176 - loglik: -8.9330e+01 - logprior: -1.4872e+00
Epoch 6/10
11/11 - 1s - loss: 90.0826 - loglik: -8.8747e+01 - logprior: -1.3355e+00
Epoch 7/10
11/11 - 1s - loss: 90.4600 - loglik: -8.9223e+01 - logprior: -1.2368e+00
Fitted a model with MAP estimate = -90.1341
Time for alignment: 35.7768
Computed alignments with likelihoods: ['-90.4411', '-90.2347', '-90.1341']
Best model has likelihood: -90.1341
SP score = 0.9957
Training of 3 independent models on file rvp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2098bd9b50>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20cb6272b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202cace0d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202caceeb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 10s - loss: 171.1893 - loglik: -1.7003e+02 - logprior: -1.1630e+00
Epoch 2/10
42/42 - 5s - loss: 81.1363 - loglik: -8.0082e+01 - logprior: -1.0540e+00
Epoch 3/10
42/42 - 5s - loss: 78.7614 - loglik: -7.7763e+01 - logprior: -9.9820e-01
Epoch 4/10
42/42 - 5s - loss: 78.1284 - loglik: -7.7147e+01 - logprior: -9.8150e-01
Epoch 5/10
42/42 - 5s - loss: 77.5522 - loglik: -7.6605e+01 - logprior: -9.4711e-01
Epoch 6/10
42/42 - 5s - loss: 77.5310 - loglik: -7.6617e+01 - logprior: -9.1431e-01
Epoch 7/10
42/42 - 5s - loss: 78.0774 - loglik: -7.7199e+01 - logprior: -8.7850e-01
Fitted a model with MAP estimate = -76.7290
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (55, 1), (56, 1), (59, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 9s - loss: 46.9913 - loglik: -4.5643e+01 - logprior: -1.3481e+00
Epoch 2/2
42/42 - 6s - loss: 33.7141 - loglik: -3.2540e+01 - logprior: -1.1746e+00
Fitted a model with MAP estimate = -32.9365
expansions: []
discards: [0]
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 10s - loss: 37.2821 - loglik: -3.5787e+01 - logprior: -1.4956e+00
Epoch 2/2
42/42 - 6s - loss: 34.9007 - loglik: -3.3866e+01 - logprior: -1.0351e+00
Fitted a model with MAP estimate = -34.0139
expansions: []
discards: []
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 11s - loss: 33.0780 - loglik: -3.2160e+01 - logprior: -9.1831e-01
Epoch 2/10
59/59 - 8s - loss: 32.3938 - loglik: -3.1482e+01 - logprior: -9.1190e-01
Epoch 3/10
59/59 - 8s - loss: 32.7004 - loglik: -3.1801e+01 - logprior: -8.9981e-01
Fitted a model with MAP estimate = -32.0534
Time for alignment: 195.0434
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 10s - loss: 171.8396 - loglik: -1.7068e+02 - logprior: -1.1609e+00
Epoch 2/10
42/42 - 5s - loss: 81.7748 - loglik: -8.0720e+01 - logprior: -1.0543e+00
Epoch 3/10
42/42 - 5s - loss: 79.1917 - loglik: -7.8184e+01 - logprior: -1.0081e+00
Epoch 4/10
42/42 - 5s - loss: 78.8561 - loglik: -7.7877e+01 - logprior: -9.7885e-01
Epoch 5/10
42/42 - 5s - loss: 78.3791 - loglik: -7.7434e+01 - logprior: -9.4479e-01
Epoch 6/10
42/42 - 5s - loss: 78.1086 - loglik: -7.7195e+01 - logprior: -9.1380e-01
Epoch 7/10
42/42 - 5s - loss: 77.9200 - loglik: -7.7031e+01 - logprior: -8.8935e-01
Epoch 8/10
42/42 - 5s - loss: 77.8036 - loglik: -7.6960e+01 - logprior: -8.4345e-01
Epoch 9/10
42/42 - 5s - loss: 77.9778 - loglik: -7.7151e+01 - logprior: -8.2682e-01
Fitted a model with MAP estimate = -77.1322
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (29, 1), (31, 1), (47, 1), (48, 1), (49, 1), (55, 1), (56, 1), (59, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 9s - loss: 46.8891 - loglik: -4.5543e+01 - logprior: -1.3458e+00
Epoch 2/2
42/42 - 6s - loss: 33.8659 - loglik: -3.2691e+01 - logprior: -1.1747e+00
Fitted a model with MAP estimate = -32.9335
expansions: []
discards: []
Fitting a model of length 96 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 11s - loss: 32.5212 - loglik: -3.1733e+01 - logprior: -7.8771e-01
Epoch 2/10
59/59 - 8s - loss: 31.9111 - loglik: -3.1157e+01 - logprior: -7.5446e-01
Epoch 3/10
59/59 - 8s - loss: 31.9605 - loglik: -3.1217e+01 - logprior: -7.4386e-01
Fitted a model with MAP estimate = -31.5478
Time for alignment: 160.0541
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 171.8087 - loglik: -1.7065e+02 - logprior: -1.1593e+00
Epoch 2/10
42/42 - 5s - loss: 81.0864 - loglik: -8.0029e+01 - logprior: -1.0571e+00
Epoch 3/10
42/42 - 5s - loss: 78.2053 - loglik: -7.7200e+01 - logprior: -1.0054e+00
Epoch 4/10
42/42 - 5s - loss: 78.4132 - loglik: -7.7437e+01 - logprior: -9.7594e-01
Fitted a model with MAP estimate = -76.8962
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (33, 1), (47, 1), (48, 1), (49, 1), (55, 1), (56, 1), (59, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 46.5240 - loglik: -4.5173e+01 - logprior: -1.3513e+00
Epoch 2/2
42/42 - 5s - loss: 33.7437 - loglik: -3.2562e+01 - logprior: -1.1814e+00
Fitted a model with MAP estimate = -32.9464
expansions: []
discards: []
Fitting a model of length 96 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 12s - loss: 32.3951 - loglik: -3.1602e+01 - logprior: -7.9340e-01
Epoch 2/10
59/59 - 7s - loss: 31.8890 - loglik: -3.1131e+01 - logprior: -7.5770e-01
Epoch 3/10
59/59 - 8s - loss: 32.0996 - loglik: -3.1354e+01 - logprior: -7.4570e-01
Fitted a model with MAP estimate = -31.5693
Time for alignment: 134.6586
Computed alignments with likelihoods: ['-32.0534', '-31.5478', '-31.5693']
Best model has likelihood: -31.5478
SP score = 0.2936
Training of 3 independent models on file mmp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2129031a00>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1e0c17e160>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202e812ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f207f6c5670>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 124 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 491.9959 - loglik: -4.5687e+02 - logprior: -3.5127e+01
Epoch 2/10
10/10 - 4s - loss: 433.2733 - loglik: -4.2571e+02 - logprior: -7.5633e+00
Epoch 3/10
10/10 - 4s - loss: 395.3692 - loglik: -3.9250e+02 - logprior: -2.8709e+00
Epoch 4/10
10/10 - 4s - loss: 370.3162 - loglik: -3.6855e+02 - logprior: -1.7685e+00
Epoch 5/10
10/10 - 4s - loss: 361.3384 - loglik: -3.5999e+02 - logprior: -1.3464e+00
Epoch 6/10
10/10 - 4s - loss: 357.5335 - loglik: -3.5632e+02 - logprior: -1.2110e+00
Epoch 7/10
10/10 - 4s - loss: 357.5204 - loglik: -3.5646e+02 - logprior: -1.0607e+00
Epoch 8/10
10/10 - 4s - loss: 354.0850 - loglik: -3.5313e+02 - logprior: -9.5277e-01
Epoch 9/10
10/10 - 4s - loss: 355.7731 - loglik: -3.5476e+02 - logprior: -1.0135e+00
Fitted a model with MAP estimate = -354.5746
expansions: [(12, 1), (13, 1), (16, 5), (17, 1), (19, 1), (36, 1), (38, 2), (43, 1), (66, 1), (67, 1), (70, 1), (79, 5), (100, 1), (102, 1), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [ 0 45 46 47]
Fitting a model of length 150 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 394.3643 - loglik: -3.5955e+02 - logprior: -3.4811e+01
Epoch 2/2
10/10 - 5s - loss: 363.1452 - loglik: -3.4988e+02 - logprior: -1.3266e+01
Fitted a model with MAP estimate = -357.9389
expansions: [(0, 23), (56, 3), (57, 1)]
discards: [ 0 14 47]
Fitting a model of length 174 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 12s - loss: 378.4765 - loglik: -3.5041e+02 - logprior: -2.8068e+01
Epoch 2/2
10/10 - 7s - loss: 349.7358 - loglik: -3.4359e+02 - logprior: -6.1431e+00
Fitted a model with MAP estimate = -346.1315
expansions: [(77, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]
Fitting a model of length 153 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 373.3272 - loglik: -3.4250e+02 - logprior: -3.0832e+01
Epoch 2/10
10/10 - 6s - loss: 349.6513 - loglik: -3.4230e+02 - logprior: -7.3484e+00
Epoch 3/10
10/10 - 6s - loss: 342.7477 - loglik: -3.4108e+02 - logprior: -1.6675e+00
Epoch 4/10
10/10 - 6s - loss: 340.2621 - loglik: -3.4062e+02 - logprior: 0.3584
Epoch 5/10
10/10 - 6s - loss: 339.1829 - loglik: -3.4043e+02 - logprior: 1.2436
Epoch 6/10
10/10 - 6s - loss: 338.8057 - loglik: -3.4048e+02 - logprior: 1.6702
Epoch 7/10
10/10 - 6s - loss: 337.5688 - loglik: -3.3955e+02 - logprior: 1.9770
Epoch 8/10
10/10 - 6s - loss: 337.8734 - loglik: -3.4009e+02 - logprior: 2.2117
Fitted a model with MAP estimate = -337.3736
Time for alignment: 135.0303
Fitting a model of length 124 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 491.9847 - loglik: -4.5685e+02 - logprior: -3.5130e+01
Epoch 2/10
10/10 - 4s - loss: 432.6291 - loglik: -4.2505e+02 - logprior: -7.5781e+00
Epoch 3/10
10/10 - 5s - loss: 396.1587 - loglik: -3.9323e+02 - logprior: -2.9308e+00
Epoch 4/10
10/10 - 5s - loss: 371.4836 - loglik: -3.6963e+02 - logprior: -1.8533e+00
Epoch 5/10
10/10 - 4s - loss: 360.8391 - loglik: -3.5939e+02 - logprior: -1.4519e+00
Epoch 6/10
10/10 - 4s - loss: 358.1909 - loglik: -3.5699e+02 - logprior: -1.2015e+00
Epoch 7/10
10/10 - 4s - loss: 357.1606 - loglik: -3.5609e+02 - logprior: -1.0724e+00
Epoch 8/10
10/10 - 4s - loss: 356.0911 - loglik: -3.5508e+02 - logprior: -1.0102e+00
Epoch 9/10
10/10 - 4s - loss: 354.8474 - loglik: -3.5374e+02 - logprior: -1.1072e+00
Epoch 10/10
10/10 - 4s - loss: 354.8505 - loglik: -3.5356e+02 - logprior: -1.2948e+00
Fitted a model with MAP estimate = -354.4003
expansions: [(3, 1), (11, 1), (16, 5), (17, 1), (34, 1), (36, 1), (38, 2), (43, 1), (44, 1), (66, 1), (67, 2), (70, 1), (76, 1), (78, 1), (80, 2), (101, 1), (102, 3), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [ 0 45 46]
Fitting a model of length 154 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 390.7332 - loglik: -3.5587e+02 - logprior: -3.4866e+01
Epoch 2/2
10/10 - 6s - loss: 361.9088 - loglik: -3.4866e+02 - logprior: -1.3245e+01
Fitted a model with MAP estimate = -355.6235
expansions: [(0, 20), (16, 1)]
discards: [  0  47  80  81  82  83 123]
Fitting a model of length 168 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 378.2770 - loglik: -3.5068e+02 - logprior: -2.7595e+01
Epoch 2/2
10/10 - 7s - loss: 353.4618 - loglik: -3.4766e+02 - logprior: -5.7983e+00
Fitted a model with MAP estimate = -348.6242
expansions: [(75, 3)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Fitting a model of length 152 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 375.8883 - loglik: -3.4539e+02 - logprior: -3.0495e+01
Epoch 2/10
10/10 - 6s - loss: 350.4814 - loglik: -3.4343e+02 - logprior: -7.0492e+00
Epoch 3/10
10/10 - 6s - loss: 343.2600 - loglik: -3.4179e+02 - logprior: -1.4716e+00
Epoch 4/10
10/10 - 6s - loss: 341.9867 - loglik: -3.4241e+02 - logprior: 0.4280
Epoch 5/10
10/10 - 6s - loss: 340.0319 - loglik: -3.4129e+02 - logprior: 1.2610
Epoch 6/10
10/10 - 6s - loss: 338.8301 - loglik: -3.4048e+02 - logprior: 1.6544
Epoch 7/10
10/10 - 6s - loss: 339.2621 - loglik: -3.4122e+02 - logprior: 1.9589
Fitted a model with MAP estimate = -338.6521
Time for alignment: 136.6318
Fitting a model of length 124 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 492.5641 - loglik: -4.5744e+02 - logprior: -3.5126e+01
Epoch 2/10
10/10 - 5s - loss: 432.7709 - loglik: -4.2521e+02 - logprior: -7.5649e+00
Epoch 3/10
10/10 - 5s - loss: 394.3254 - loglik: -3.9148e+02 - logprior: -2.8468e+00
Epoch 4/10
10/10 - 5s - loss: 369.4448 - loglik: -3.6785e+02 - logprior: -1.5961e+00
Epoch 5/10
10/10 - 5s - loss: 359.8338 - loglik: -3.5870e+02 - logprior: -1.1345e+00
Epoch 6/10
10/10 - 5s - loss: 357.0593 - loglik: -3.5608e+02 - logprior: -9.8051e-01
Epoch 7/10
10/10 - 5s - loss: 356.8404 - loglik: -3.5602e+02 - logprior: -8.2234e-01
Epoch 8/10
10/10 - 5s - loss: 355.3553 - loglik: -3.5468e+02 - logprior: -6.7993e-01
Epoch 9/10
10/10 - 5s - loss: 355.7026 - loglik: -3.5498e+02 - logprior: -7.2399e-01
Fitted a model with MAP estimate = -354.8248
expansions: [(12, 1), (16, 5), (17, 1), (36, 3), (43, 1), (44, 1), (66, 2), (79, 6), (100, 1), (102, 1), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [ 0 45 46 47]
Fitting a model of length 149 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 394.4505 - loglik: -3.5972e+02 - logprior: -3.4735e+01
Epoch 2/2
10/10 - 6s - loss: 364.4703 - loglik: -3.5134e+02 - logprior: -1.3134e+01
Fitted a model with MAP estimate = -358.8838
expansions: [(0, 24), (54, 3), (56, 1)]
discards: [ 0 92]
Fitting a model of length 175 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 376.9675 - loglik: -3.4900e+02 - logprior: -2.7963e+01
Epoch 2/2
10/10 - 8s - loss: 350.9268 - loglik: -3.4486e+02 - logprior: -6.0652e+00
Fitted a model with MAP estimate = -346.0207
expansions: [(103, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]
Fitting a model of length 153 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 373.5743 - loglik: -3.4300e+02 - logprior: -3.0579e+01
Epoch 2/10
10/10 - 6s - loss: 348.0745 - loglik: -3.4099e+02 - logprior: -7.0822e+00
Epoch 3/10
10/10 - 6s - loss: 344.4163 - loglik: -3.4295e+02 - logprior: -1.4643e+00
Epoch 4/10
10/10 - 6s - loss: 339.5414 - loglik: -3.4004e+02 - logprior: 0.4967
Epoch 5/10
10/10 - 7s - loss: 340.3205 - loglik: -3.4168e+02 - logprior: 1.3546
Fitted a model with MAP estimate = -338.7064
Time for alignment: 125.2425
Computed alignments with likelihoods: ['-337.3736', '-338.6521', '-338.7064']
Best model has likelihood: -337.3736
SP score = 0.9702
Training of 3 independent models on file icd.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2120b19c70>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f202328e760>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2139e844f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2139e84400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 36s - loss: 856.9355 - loglik: -8.5275e+02 - logprior: -4.1879e+00
Epoch 2/10
29/29 - 35s - loss: 679.0100 - loglik: -6.7727e+02 - logprior: -1.7418e+00
Epoch 3/10
29/29 - 33s - loss: 649.9044 - loglik: -6.4790e+02 - logprior: -2.0007e+00
Epoch 4/10
29/29 - 30s - loss: 644.5881 - loglik: -6.4259e+02 - logprior: -1.9961e+00
Epoch 5/10
29/29 - 29s - loss: 642.8561 - loglik: -6.4081e+02 - logprior: -2.0489e+00
Epoch 6/10
29/29 - 27s - loss: 643.0285 - loglik: -6.4095e+02 - logprior: -2.0784e+00
Fitted a model with MAP estimate = -641.6153
expansions: [(16, 1), (22, 1), (24, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (37, 1), (39, 1), (49, 2), (50, 5), (64, 1), (77, 1), (78, 1), (89, 1), (90, 2), (94, 1), (120, 2), (121, 2), (124, 1), (125, 1), (128, 1), (142, 1), (152, 1), (153, 1), (154, 2), (163, 1), (166, 2), (173, 1), (184, 1), (185, 1), (186, 2), (192, 1), (205, 1), (216, 1), (218, 2), (219, 3), (240, 1), (248, 1), (249, 1), (251, 2), (258, 1), (260, 2), (261, 1), (262, 1), (263, 2), (264, 1), (267, 2)]
discards: [0]
Fitting a model of length 345 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 45s - loss: 625.5502 - loglik: -6.2007e+02 - logprior: -5.4811e+00
Epoch 2/2
29/29 - 39s - loss: 598.8892 - loglik: -5.9656e+02 - logprior: -2.3321e+00
Fitted a model with MAP estimate = -594.0969
expansions: [(0, 2)]
discards: [  0  62  63 146 187 202 265 331 332]
Fitting a model of length 338 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 41s - loss: 600.5934 - loglik: -5.9784e+02 - logprior: -2.7557e+00
Epoch 2/2
29/29 - 37s - loss: 595.4104 - loglik: -5.9530e+02 - logprior: -1.0775e-01
Fitted a model with MAP estimate = -592.6433
expansions: [(143, 1), (326, 2)]
discards: [  0 300]
Fitting a model of length 339 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 42s - loss: 602.9303 - loglik: -5.9866e+02 - logprior: -4.2700e+00
Epoch 2/10
29/29 - 38s - loss: 595.1851 - loglik: -5.9454e+02 - logprior: -6.4881e-01
Epoch 3/10
29/29 - 38s - loss: 592.4208 - loglik: -5.9275e+02 - logprior: 0.3326
Epoch 4/10
29/29 - 39s - loss: 591.3254 - loglik: -5.9177e+02 - logprior: 0.4441
Epoch 5/10
29/29 - 38s - loss: 589.7567 - loglik: -5.9015e+02 - logprior: 0.3946
Epoch 6/10
29/29 - 38s - loss: 590.5013 - loglik: -5.9118e+02 - logprior: 0.6769
Fitted a model with MAP estimate = -589.2632
Time for alignment: 719.4243
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 30s - loss: 857.3984 - loglik: -8.5320e+02 - logprior: -4.2010e+00
Epoch 2/10
29/29 - 28s - loss: 668.0600 - loglik: -6.6617e+02 - logprior: -1.8945e+00
Epoch 3/10
29/29 - 27s - loss: 641.8737 - loglik: -6.3972e+02 - logprior: -2.1542e+00
Epoch 4/10
29/29 - 28s - loss: 635.7242 - loglik: -6.3351e+02 - logprior: -2.2094e+00
Epoch 5/10
29/29 - 27s - loss: 633.5742 - loglik: -6.3126e+02 - logprior: -2.3162e+00
Epoch 6/10
29/29 - 28s - loss: 628.7346 - loglik: -6.2630e+02 - logprior: -2.4317e+00
Epoch 7/10
29/29 - 28s - loss: 628.2605 - loglik: -6.2582e+02 - logprior: -2.4373e+00
Epoch 8/10
29/29 - 28s - loss: 628.1591 - loglik: -6.2571e+02 - logprior: -2.4475e+00
Epoch 9/10
29/29 - 28s - loss: 629.5431 - loglik: -6.2711e+02 - logprior: -2.4314e+00
Fitted a model with MAP estimate = -628.3315
expansions: [(16, 1), (17, 1), (24, 2), (28, 1), (29, 1), (30, 1), (37, 1), (39, 1), (46, 1), (48, 2), (49, 1), (77, 1), (88, 1), (89, 2), (90, 2), (122, 2), (124, 1), (126, 1), (129, 1), (140, 1), (142, 1), (152, 1), (154, 1), (155, 1), (156, 1), (163, 1), (172, 1), (173, 1), (184, 1), (185, 1), (186, 1), (191, 1), (192, 1), (206, 1), (216, 1), (218, 2), (219, 3), (240, 1), (248, 1), (249, 1), (251, 1), (258, 1), (260, 2), (262, 1), (264, 1), (270, 5)]
discards: [0]
Fitting a model of length 337 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 41s - loss: 613.3650 - loglik: -6.0821e+02 - logprior: -5.1571e+00
Epoch 2/2
29/29 - 38s - loss: 592.4120 - loglik: -5.9046e+02 - logprior: -1.9538e+00
Fitted a model with MAP estimate = -588.6523
expansions: [(0, 2), (107, 1), (326, 2)]
discards: [  0 103 258]
Fitting a model of length 339 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 45s - loss: 591.1561 - loglik: -5.8839e+02 - logprior: -2.7631e+00
Epoch 2/2
29/29 - 43s - loss: 585.8391 - loglik: -5.8582e+02 - logprior: -2.1712e-02
Fitted a model with MAP estimate = -582.9143
expansions: []
discards: [0]
Fitting a model of length 338 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 46s - loss: 591.9923 - loglik: -5.8774e+02 - logprior: -4.2520e+00
Epoch 2/10
29/29 - 40s - loss: 585.5374 - loglik: -5.8522e+02 - logprior: -3.1932e-01
Epoch 3/10
29/29 - 41s - loss: 583.4666 - loglik: -5.8382e+02 - logprior: 0.3492
Epoch 4/10
29/29 - 40s - loss: 581.9619 - loglik: -5.8261e+02 - logprior: 0.6509
Epoch 5/10
29/29 - 41s - loss: 580.7972 - loglik: -5.8143e+02 - logprior: 0.6291
Epoch 6/10
29/29 - 40s - loss: 581.1017 - loglik: -5.8207e+02 - logprior: 0.9683
Fitted a model with MAP estimate = -579.4999
Time for alignment: 814.9485
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 32s - loss: 858.3260 - loglik: -8.5412e+02 - logprior: -4.2052e+00
Epoch 2/10
29/29 - 30s - loss: 674.4407 - loglik: -6.7264e+02 - logprior: -1.8047e+00
Epoch 3/10
29/29 - 32s - loss: 648.6469 - loglik: -6.4667e+02 - logprior: -1.9768e+00
Epoch 4/10
29/29 - 32s - loss: 642.3209 - loglik: -6.4028e+02 - logprior: -2.0443e+00
Epoch 5/10
29/29 - 33s - loss: 638.9120 - loglik: -6.3676e+02 - logprior: -2.1539e+00
Epoch 6/10
29/29 - 33s - loss: 637.1552 - loglik: -6.3499e+02 - logprior: -2.1661e+00
Epoch 7/10
29/29 - 34s - loss: 636.1277 - loglik: -6.3391e+02 - logprior: -2.2136e+00
Epoch 8/10
29/29 - 32s - loss: 636.8347 - loglik: -6.3461e+02 - logprior: -2.2242e+00
Fitted a model with MAP estimate = -635.8467
expansions: [(16, 1), (17, 1), (24, 1), (26, 1), (28, 1), (31, 2), (36, 1), (38, 1), (48, 2), (49, 1), (52, 1), (73, 1), (76, 1), (87, 1), (88, 1), (89, 1), (94, 1), (120, 2), (121, 1), (123, 1), (124, 1), (125, 1), (128, 1), (144, 1), (153, 1), (154, 1), (155, 2), (163, 1), (165, 1), (173, 1), (184, 1), (185, 1), (186, 2), (192, 1), (201, 1), (205, 1), (217, 2), (218, 2), (219, 1), (240, 1), (250, 1), (251, 3), (258, 1), (260, 1), (261, 1), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [0]
Fitting a model of length 341 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 47s - loss: 621.7109 - loglik: -6.1633e+02 - logprior: -5.3813e+00
Epoch 2/2
29/29 - 45s - loss: 600.4241 - loglik: -5.9821e+02 - logprior: -2.2106e+00
Fitted a model with MAP estimate = -594.3317
expansions: [(0, 2), (35, 1)]
discards: [  0  57 183 260 326]
Fitting a model of length 339 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 44s - loss: 598.1934 - loglik: -5.9541e+02 - logprior: -2.7865e+00
Epoch 2/2
29/29 - 42s - loss: 593.5444 - loglik: -5.9315e+02 - logprior: -3.9159e-01
Fitted a model with MAP estimate = -591.3304
expansions: [(26, 1), (325, 2)]
discards: [  0  29 296 328]
Fitting a model of length 338 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 45s - loss: 602.3427 - loglik: -5.9809e+02 - logprior: -4.2522e+00
Epoch 2/10
29/29 - 41s - loss: 594.5214 - loglik: -5.9410e+02 - logprior: -4.2488e-01
Epoch 3/10
29/29 - 41s - loss: 590.2308 - loglik: -5.9058e+02 - logprior: 0.3497
Epoch 4/10
29/29 - 44s - loss: 590.4821 - loglik: -5.9071e+02 - logprior: 0.2300
Fitted a model with MAP estimate = -589.5417
Time for alignment: 765.1556
Computed alignments with likelihoods: ['-589.2632', '-579.4999', '-589.5417']
Best model has likelihood: -579.4999
SP score = 0.8981
Training of 3 independent models on file seatoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f202dc81f10>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f21297d08e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20108b5310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2010ee6b80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 642.1591 - loglik: -1.2843e+02 - logprior: -5.1373e+02
Epoch 2/10
10/10 - 1s - loss: 247.0499 - loglik: -1.0663e+02 - logprior: -1.4042e+02
Epoch 3/10
10/10 - 1s - loss: 152.0005 - loglik: -8.7821e+01 - logprior: -6.4180e+01
Epoch 4/10
10/10 - 1s - loss: 112.5506 - loglik: -7.6943e+01 - logprior: -3.5608e+01
Epoch 5/10
10/10 - 1s - loss: 93.6418 - loglik: -7.3350e+01 - logprior: -2.0292e+01
Epoch 6/10
10/10 - 1s - loss: 83.0707 - loglik: -7.2523e+01 - logprior: -1.0548e+01
Epoch 7/10
10/10 - 1s - loss: 76.5609 - loglik: -7.2056e+01 - logprior: -4.5051e+00
Epoch 8/10
10/10 - 1s - loss: 72.7510 - loglik: -7.2080e+01 - logprior: -6.7120e-01
Epoch 9/10
10/10 - 1s - loss: 70.2951 - loglik: -7.2341e+01 - logprior: 2.0461
Epoch 10/10
10/10 - 1s - loss: 68.5368 - loglik: -7.2477e+01 - logprior: 3.9406
Fitted a model with MAP estimate = -67.7507
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 723.9979 - loglik: -6.4353e+01 - logprior: -6.5964e+02
Epoch 2/2
10/10 - 0s - loss: 260.6440 - loglik: -5.5676e+01 - logprior: -2.0497e+02
Fitted a model with MAP estimate = -172.7474
expansions: []
discards: [23]
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 505.9558 - loglik: -5.0224e+01 - logprior: -4.5573e+02
Epoch 2/2
10/10 - 0s - loss: 171.0187 - loglik: -4.9825e+01 - logprior: -1.2119e+02
Fitted a model with MAP estimate = -121.0594
expansions: []
discards: []
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 476.8864 - loglik: -4.9141e+01 - logprior: -4.2775e+02
Epoch 2/10
10/10 - 1s - loss: 163.4350 - loglik: -4.9760e+01 - logprior: -1.1367e+02
Epoch 3/10
10/10 - 1s - loss: 97.5025 - loglik: -5.0481e+01 - logprior: -4.7022e+01
Epoch 4/10
10/10 - 1s - loss: 69.5045 - loglik: -5.1052e+01 - logprior: -1.8452e+01
Epoch 5/10
10/10 - 1s - loss: 54.1105 - loglik: -5.1594e+01 - logprior: -2.5168e+00
Epoch 6/10
10/10 - 1s - loss: 45.1074 - loglik: -5.2045e+01 - logprior: 6.9379
Epoch 7/10
10/10 - 1s - loss: 39.4889 - loglik: -5.2390e+01 - logprior: 12.9008
Epoch 8/10
10/10 - 1s - loss: 35.6582 - loglik: -5.2661e+01 - logprior: 17.0031
Epoch 9/10
10/10 - 1s - loss: 32.8008 - loglik: -5.2883e+01 - logprior: 20.0822
Epoch 10/10
10/10 - 1s - loss: 30.5029 - loglik: -5.3065e+01 - logprior: 22.5622
Fitted a model with MAP estimate = -29.3840
Time for alignment: 28.3354
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 642.1591 - loglik: -1.2843e+02 - logprior: -5.1373e+02
Epoch 2/10
10/10 - 0s - loss: 247.0499 - loglik: -1.0663e+02 - logprior: -1.4042e+02
Epoch 3/10
10/10 - 0s - loss: 152.0005 - loglik: -8.7821e+01 - logprior: -6.4180e+01
Epoch 4/10
10/10 - 1s - loss: 112.5506 - loglik: -7.6943e+01 - logprior: -3.5608e+01
Epoch 5/10
10/10 - 1s - loss: 93.6418 - loglik: -7.3350e+01 - logprior: -2.0292e+01
Epoch 6/10
10/10 - 1s - loss: 83.0707 - loglik: -7.2523e+01 - logprior: -1.0548e+01
Epoch 7/10
10/10 - 1s - loss: 76.5609 - loglik: -7.2056e+01 - logprior: -4.5051e+00
Epoch 8/10
10/10 - 0s - loss: 72.7510 - loglik: -7.2080e+01 - logprior: -6.7120e-01
Epoch 9/10
10/10 - 1s - loss: 70.2951 - loglik: -7.2341e+01 - logprior: 2.0461
Epoch 10/10
10/10 - 0s - loss: 68.5368 - loglik: -7.2477e+01 - logprior: 3.9406
Fitted a model with MAP estimate = -67.7507
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 723.9979 - loglik: -6.4353e+01 - logprior: -6.5964e+02
Epoch 2/2
10/10 - 1s - loss: 260.6440 - loglik: -5.5676e+01 - logprior: -2.0497e+02
Fitted a model with MAP estimate = -172.7474
expansions: []
discards: [23]
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 505.9558 - loglik: -5.0224e+01 - logprior: -4.5573e+02
Epoch 2/2
10/10 - 1s - loss: 171.0187 - loglik: -4.9825e+01 - logprior: -1.2119e+02
Fitted a model with MAP estimate = -121.0594
expansions: []
discards: []
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 476.8864 - loglik: -4.9141e+01 - logprior: -4.2775e+02
Epoch 2/10
10/10 - 1s - loss: 163.4350 - loglik: -4.9760e+01 - logprior: -1.1367e+02
Epoch 3/10
10/10 - 1s - loss: 97.5025 - loglik: -5.0481e+01 - logprior: -4.7022e+01
Epoch 4/10
10/10 - 1s - loss: 69.5045 - loglik: -5.1052e+01 - logprior: -1.8452e+01
Epoch 5/10
10/10 - 1s - loss: 54.1105 - loglik: -5.1594e+01 - logprior: -2.5168e+00
Epoch 6/10
10/10 - 1s - loss: 45.1074 - loglik: -5.2045e+01 - logprior: 6.9379
Epoch 7/10
10/10 - 1s - loss: 39.4889 - loglik: -5.2390e+01 - logprior: 12.9008
Epoch 8/10
10/10 - 1s - loss: 35.6582 - loglik: -5.2661e+01 - logprior: 17.0031
Epoch 9/10
10/10 - 0s - loss: 32.8008 - loglik: -5.2883e+01 - logprior: 20.0822
Epoch 10/10
10/10 - 1s - loss: 30.5029 - loglik: -5.3065e+01 - logprior: 22.5622
Fitted a model with MAP estimate = -29.3840
Time for alignment: 27.6673
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 642.1591 - loglik: -1.2843e+02 - logprior: -5.1373e+02
Epoch 2/10
10/10 - 0s - loss: 247.0499 - loglik: -1.0663e+02 - logprior: -1.4042e+02
Epoch 3/10
10/10 - 1s - loss: 152.0005 - loglik: -8.7821e+01 - logprior: -6.4180e+01
Epoch 4/10
10/10 - 0s - loss: 112.5506 - loglik: -7.6943e+01 - logprior: -3.5608e+01
Epoch 5/10
10/10 - 0s - loss: 93.6418 - loglik: -7.3350e+01 - logprior: -2.0292e+01
Epoch 6/10
10/10 - 0s - loss: 83.0707 - loglik: -7.2523e+01 - logprior: -1.0548e+01
Epoch 7/10
10/10 - 0s - loss: 76.5609 - loglik: -7.2056e+01 - logprior: -4.5051e+00
Epoch 8/10
10/10 - 0s - loss: 72.7510 - loglik: -7.2080e+01 - logprior: -6.7120e-01
Epoch 9/10
10/10 - 0s - loss: 70.2951 - loglik: -7.2341e+01 - logprior: 2.0461
Epoch 10/10
10/10 - 0s - loss: 68.5368 - loglik: -7.2477e+01 - logprior: 3.9406
Fitted a model with MAP estimate = -67.7507
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 723.9979 - loglik: -6.4353e+01 - logprior: -6.5964e+02
Epoch 2/2
10/10 - 1s - loss: 260.6440 - loglik: -5.5676e+01 - logprior: -2.0497e+02
Fitted a model with MAP estimate = -172.7474
expansions: []
discards: [23]
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 505.9558 - loglik: -5.0224e+01 - logprior: -4.5573e+02
Epoch 2/2
10/10 - 0s - loss: 171.0187 - loglik: -4.9825e+01 - logprior: -1.2119e+02
Fitted a model with MAP estimate = -121.0594
expansions: []
discards: []
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 476.8864 - loglik: -4.9141e+01 - logprior: -4.2775e+02
Epoch 2/10
10/10 - 0s - loss: 163.4350 - loglik: -4.9760e+01 - logprior: -1.1367e+02
Epoch 3/10
10/10 - 0s - loss: 97.5025 - loglik: -5.0481e+01 - logprior: -4.7022e+01
Epoch 4/10
10/10 - 0s - loss: 69.5045 - loglik: -5.1052e+01 - logprior: -1.8452e+01
Epoch 5/10
10/10 - 1s - loss: 54.1105 - loglik: -5.1594e+01 - logprior: -2.5168e+00
Epoch 6/10
10/10 - 0s - loss: 45.1074 - loglik: -5.2045e+01 - logprior: 6.9379
Epoch 7/10
10/10 - 1s - loss: 39.4889 - loglik: -5.2390e+01 - logprior: 12.9008
Epoch 8/10
10/10 - 0s - loss: 35.6582 - loglik: -5.2661e+01 - logprior: 17.0031
Epoch 9/10
10/10 - 0s - loss: 32.8008 - loglik: -5.2883e+01 - logprior: 20.0822
Epoch 10/10
10/10 - 1s - loss: 30.5029 - loglik: -5.3065e+01 - logprior: 22.5622
Fitted a model with MAP estimate = -29.3840
Time for alignment: 28.2776
Computed alignments with likelihoods: ['-29.3840', '-29.3840', '-29.3840']
Best model has likelihood: -29.3840
SP score = 0.7269
Training of 3 independent models on file int.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f207f6abfd0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2008429c10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20905ed400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20cb325f40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 9s - loss: 478.0753 - loglik: -4.7298e+02 - logprior: -5.0940e+00
Epoch 2/10
16/16 - 6s - loss: 441.1542 - loglik: -4.4004e+02 - logprior: -1.1185e+00
Epoch 3/10
16/16 - 6s - loss: 416.7702 - loglik: -4.1544e+02 - logprior: -1.3278e+00
Epoch 4/10
16/16 - 6s - loss: 409.7062 - loglik: -4.0834e+02 - logprior: -1.3709e+00
Epoch 5/10
16/16 - 6s - loss: 405.9249 - loglik: -4.0452e+02 - logprior: -1.4062e+00
Epoch 6/10
16/16 - 6s - loss: 404.0641 - loglik: -4.0252e+02 - logprior: -1.5474e+00
Epoch 7/10
16/16 - 6s - loss: 401.7091 - loglik: -4.0009e+02 - logprior: -1.6240e+00
Epoch 8/10
16/16 - 6s - loss: 401.6660 - loglik: -3.9997e+02 - logprior: -1.6991e+00
Epoch 9/10
16/16 - 6s - loss: 401.2342 - loglik: -3.9948e+02 - logprior: -1.7542e+00
Epoch 10/10
16/16 - 7s - loss: 400.3761 - loglik: -3.9856e+02 - logprior: -1.8157e+00
Fitted a model with MAP estimate = -400.3517
expansions: [(13, 1), (14, 1), (23, 3), (28, 3), (29, 1), (42, 1), (44, 1), (48, 2), (51, 1), (53, 1), (56, 1), (57, 1), (69, 1), (73, 2), (94, 2), (95, 3), (99, 1), (105, 1), (116, 1), (117, 1), (119, 1), (122, 1), (125, 1), (129, 1), (139, 3)]
discards: [0]
Fitting a model of length 174 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 11s - loss: 418.4356 - loglik: -4.1265e+02 - logprior: -5.7860e+00
Epoch 2/2
16/16 - 9s - loss: 402.1108 - loglik: -3.9954e+02 - logprior: -2.5714e+00
Fitted a model with MAP estimate = -399.5506
expansions: [(0, 1), (93, 2), (94, 2)]
discards: [  0  25  32  33  34  59  73 171 172 173]
Fitting a model of length 169 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 13s - loss: 405.0396 - loglik: -4.0107e+02 - logprior: -3.9735e+00
Epoch 2/2
16/16 - 8s - loss: 399.6288 - loglik: -3.9843e+02 - logprior: -1.1988e+00
Fitted a model with MAP estimate = -397.6612
expansions: [(31, 2), (169, 3)]
discards: [114]
Fitting a model of length 173 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 12s - loss: 402.7587 - loglik: -3.9877e+02 - logprior: -3.9870e+00
Epoch 2/10
16/16 - 9s - loss: 397.8469 - loglik: -3.9669e+02 - logprior: -1.1568e+00
Epoch 3/10
16/16 - 9s - loss: 396.2282 - loglik: -3.9546e+02 - logprior: -7.6859e-01
Epoch 4/10
16/16 - 9s - loss: 393.6040 - loglik: -3.9292e+02 - logprior: -6.8407e-01
Epoch 5/10
16/16 - 9s - loss: 393.2164 - loglik: -3.9256e+02 - logprior: -6.5464e-01
Epoch 6/10
16/16 - 9s - loss: 391.5778 - loglik: -3.9084e+02 - logprior: -7.3930e-01
Epoch 7/10
16/16 - 9s - loss: 389.0241 - loglik: -3.8826e+02 - logprior: -7.6068e-01
Epoch 8/10
16/16 - 9s - loss: 388.4217 - loglik: -3.8760e+02 - logprior: -8.2537e-01
Epoch 9/10
16/16 - 9s - loss: 386.4144 - loglik: -3.8553e+02 - logprior: -8.8789e-01
Epoch 10/10
16/16 - 9s - loss: 386.9375 - loglik: -3.8600e+02 - logprior: -9.3626e-01
Fitted a model with MAP estimate = -386.3355
Time for alignment: 234.8763
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 11s - loss: 478.0674 - loglik: -4.7298e+02 - logprior: -5.0921e+00
Epoch 2/10
16/16 - 6s - loss: 439.9205 - loglik: -4.3879e+02 - logprior: -1.1292e+00
Epoch 3/10
16/16 - 6s - loss: 416.3096 - loglik: -4.1491e+02 - logprior: -1.4000e+00
Epoch 4/10
16/16 - 6s - loss: 408.1045 - loglik: -4.0669e+02 - logprior: -1.4135e+00
Epoch 5/10
16/16 - 6s - loss: 404.3713 - loglik: -4.0291e+02 - logprior: -1.4610e+00
Epoch 6/10
16/16 - 6s - loss: 404.3058 - loglik: -4.0274e+02 - logprior: -1.5663e+00
Epoch 7/10
16/16 - 6s - loss: 401.3469 - loglik: -3.9972e+02 - logprior: -1.6271e+00
Epoch 8/10
16/16 - 6s - loss: 399.8186 - loglik: -3.9810e+02 - logprior: -1.7207e+00
Epoch 9/10
16/16 - 6s - loss: 400.3296 - loglik: -3.9854e+02 - logprior: -1.7905e+00
Fitted a model with MAP estimate = -399.5272
expansions: [(13, 1), (14, 1), (23, 1), (27, 1), (28, 2), (30, 1), (43, 1), (45, 1), (49, 2), (50, 1), (51, 1), (57, 1), (58, 1), (69, 1), (72, 3), (73, 2), (94, 5), (95, 2), (98, 1), (116, 1), (117, 2), (122, 2), (125, 1), (129, 1), (139, 3)]
discards: [0]
Fitting a model of length 177 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 12s - loss: 415.5219 - loglik: -4.0977e+02 - logprior: -5.7476e+00
Epoch 2/2
16/16 - 8s - loss: 401.6825 - loglik: -3.9916e+02 - logprior: -2.5227e+00
Fitted a model with MAP estimate = -398.9230
expansions: [(0, 1)]
discards: [  0  31  32 117 146 174 175 176]
Fitting a model of length 170 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 11s - loss: 403.8816 - loglik: -3.9994e+02 - logprior: -3.9377e+00
Epoch 2/2
16/16 - 8s - loss: 398.0524 - loglik: -3.9684e+02 - logprior: -1.2139e+00
Fitted a model with MAP estimate = -397.1204
expansions: [(161, 1), (170, 2)]
discards: []
Fitting a model of length 173 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 13s - loss: 402.1589 - loglik: -3.9814e+02 - logprior: -4.0158e+00
Epoch 2/10
16/16 - 8s - loss: 397.9550 - loglik: -3.9681e+02 - logprior: -1.1475e+00
Epoch 3/10
16/16 - 8s - loss: 396.1031 - loglik: -3.9535e+02 - logprior: -7.5428e-01
Epoch 4/10
16/16 - 8s - loss: 394.1171 - loglik: -3.9345e+02 - logprior: -6.6510e-01
Epoch 5/10
16/16 - 8s - loss: 392.8835 - loglik: -3.9223e+02 - logprior: -6.5543e-01
Epoch 6/10
16/16 - 8s - loss: 391.3973 - loglik: -3.9067e+02 - logprior: -7.2761e-01
Epoch 7/10
16/16 - 9s - loss: 389.4043 - loglik: -3.8865e+02 - logprior: -7.5687e-01
Epoch 8/10
16/16 - 8s - loss: 389.3216 - loglik: -3.8849e+02 - logprior: -8.2841e-01
Epoch 9/10
16/16 - 8s - loss: 387.2702 - loglik: -3.8636e+02 - logprior: -9.0588e-01
Epoch 10/10
16/16 - 8s - loss: 386.3703 - loglik: -3.8545e+02 - logprior: -9.2437e-01
Fitted a model with MAP estimate = -386.4475
Time for alignment: 225.7206
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 9s - loss: 478.2057 - loglik: -4.7311e+02 - logprior: -5.0990e+00
Epoch 2/10
16/16 - 6s - loss: 440.8349 - loglik: -4.3971e+02 - logprior: -1.1220e+00
Epoch 3/10
16/16 - 6s - loss: 416.1017 - loglik: -4.1473e+02 - logprior: -1.3743e+00
Epoch 4/10
16/16 - 6s - loss: 407.5193 - loglik: -4.0613e+02 - logprior: -1.3926e+00
Epoch 5/10
16/16 - 6s - loss: 404.0583 - loglik: -4.0266e+02 - logprior: -1.3943e+00
Epoch 6/10
16/16 - 6s - loss: 402.2242 - loglik: -4.0070e+02 - logprior: -1.5275e+00
Epoch 7/10
16/16 - 6s - loss: 400.4377 - loglik: -3.9885e+02 - logprior: -1.5853e+00
Epoch 8/10
16/16 - 6s - loss: 399.7278 - loglik: -3.9805e+02 - logprior: -1.6789e+00
Epoch 9/10
16/16 - 6s - loss: 399.4031 - loglik: -3.9768e+02 - logprior: -1.7274e+00
Epoch 10/10
16/16 - 6s - loss: 398.9244 - loglik: -3.9713e+02 - logprior: -1.7969e+00
Fitted a model with MAP estimate = -398.5609
expansions: [(13, 1), (14, 1), (24, 1), (27, 1), (28, 3), (29, 1), (49, 3), (50, 1), (57, 3), (58, 1), (72, 1), (73, 2), (74, 2), (75, 2), (94, 2), (95, 4), (99, 1), (114, 1), (116, 1), (120, 1), (123, 1), (126, 1), (128, 1), (139, 3)]
discards: [0]
Fitting a model of length 177 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 12s - loss: 417.1602 - loglik: -4.1140e+02 - logprior: -5.7609e+00
Epoch 2/2
16/16 - 9s - loss: 401.4047 - loglik: -3.9886e+02 - logprior: -2.5418e+00
Fitted a model with MAP estimate = -398.9795
expansions: [(0, 1), (164, 1)]
discards: [  0  32  33  57  69 118 119 174 175 176]
Fitting a model of length 169 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 11s - loss: 403.8996 - loglik: -4.0000e+02 - logprior: -3.9030e+00
Epoch 2/2
16/16 - 8s - loss: 397.8261 - loglik: -3.9666e+02 - logprior: -1.1641e+00
Fitted a model with MAP estimate = -396.8081
expansions: [(55, 1), (169, 2)]
discards: [93]
Fitting a model of length 171 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 11s - loss: 402.8094 - loglik: -3.9885e+02 - logprior: -3.9606e+00
Epoch 2/10
16/16 - 9s - loss: 397.5041 - loglik: -3.9637e+02 - logprior: -1.1363e+00
Epoch 3/10
16/16 - 9s - loss: 395.9406 - loglik: -3.9519e+02 - logprior: -7.4843e-01
Epoch 4/10
16/16 - 9s - loss: 393.5643 - loglik: -3.9291e+02 - logprior: -6.5230e-01
Epoch 5/10
16/16 - 9s - loss: 394.0572 - loglik: -3.9342e+02 - logprior: -6.3563e-01
Fitted a model with MAP estimate = -391.4006
Time for alignment: 190.2157
Computed alignments with likelihoods: ['-386.3355', '-386.4475', '-391.4006']
Best model has likelihood: -386.3355
SP score = 0.8257
Training of 3 independent models on file phc.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f21297a13a0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20879e3850>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20879e37c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202e8ae850>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 293.4636 - loglik: -2.7696e+02 - logprior: -1.6505e+01
Epoch 2/10
10/10 - 2s - loss: 251.4268 - loglik: -2.4717e+02 - logprior: -4.2562e+00
Epoch 3/10
10/10 - 2s - loss: 224.8506 - loglik: -2.2253e+02 - logprior: -2.3184e+00
Epoch 4/10
10/10 - 2s - loss: 212.3489 - loglik: -2.1047e+02 - logprior: -1.8835e+00
Epoch 5/10
10/10 - 2s - loss: 207.0383 - loglik: -2.0533e+02 - logprior: -1.7051e+00
Epoch 6/10
10/10 - 2s - loss: 204.4197 - loglik: -2.0283e+02 - logprior: -1.5875e+00
Epoch 7/10
10/10 - 2s - loss: 204.3669 - loglik: -2.0287e+02 - logprior: -1.4940e+00
Epoch 8/10
10/10 - 2s - loss: 202.4843 - loglik: -2.0100e+02 - logprior: -1.4799e+00
Epoch 9/10
10/10 - 2s - loss: 203.5746 - loglik: -2.0211e+02 - logprior: -1.4688e+00
Fitted a model with MAP estimate = -202.6181
expansions: [(10, 1), (36, 2), (38, 3), (39, 2), (44, 1), (55, 2), (56, 1)]
discards: [0]
Fitting a model of length 79 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 225.4659 - loglik: -2.0786e+02 - logprior: -1.7608e+01
Epoch 2/2
10/10 - 2s - loss: 206.0660 - loglik: -1.9849e+02 - logprior: -7.5764e+00
Fitted a model with MAP estimate = -201.9574
expansions: [(0, 20), (3, 1)]
discards: [ 0 37 38 41 44]
Fitting a model of length 95 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 213.8698 - loglik: -1.9880e+02 - logprior: -1.5067e+01
Epoch 2/2
10/10 - 2s - loss: 194.9769 - loglik: -1.9045e+02 - logprior: -4.5258e+00
Fitted a model with MAP estimate = -189.1479
expansions: [(59, 2)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Fitting a model of length 79 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 209.3889 - loglik: -1.9455e+02 - logprior: -1.4843e+01
Epoch 2/10
10/10 - 2s - loss: 193.0097 - loglik: -1.8875e+02 - logprior: -4.2578e+00
Epoch 3/10
10/10 - 2s - loss: 191.9681 - loglik: -1.8981e+02 - logprior: -2.1557e+00
Epoch 4/10
10/10 - 2s - loss: 188.9632 - loglik: -1.8753e+02 - logprior: -1.4323e+00
Epoch 5/10
10/10 - 2s - loss: 188.6277 - loglik: -1.8752e+02 - logprior: -1.1121e+00
Epoch 6/10
10/10 - 2s - loss: 188.3682 - loglik: -1.8737e+02 - logprior: -9.9562e-01
Epoch 7/10
10/10 - 2s - loss: 185.4255 - loglik: -1.8448e+02 - logprior: -9.4278e-01
Epoch 8/10
10/10 - 2s - loss: 187.7306 - loglik: -1.8681e+02 - logprior: -9.2559e-01
Fitted a model with MAP estimate = -186.5094
Time for alignment: 59.2009
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 291.6169 - loglik: -2.7511e+02 - logprior: -1.6508e+01
Epoch 2/10
10/10 - 2s - loss: 252.8527 - loglik: -2.4858e+02 - logprior: -4.2690e+00
Epoch 3/10
10/10 - 2s - loss: 227.5167 - loglik: -2.2510e+02 - logprior: -2.4169e+00
Epoch 4/10
10/10 - 2s - loss: 210.0167 - loglik: -2.0788e+02 - logprior: -2.1372e+00
Epoch 5/10
10/10 - 2s - loss: 203.5403 - loglik: -2.0145e+02 - logprior: -2.0950e+00
Epoch 6/10
10/10 - 2s - loss: 201.2291 - loglik: -1.9908e+02 - logprior: -2.1540e+00
Epoch 7/10
10/10 - 2s - loss: 200.4665 - loglik: -1.9832e+02 - logprior: -2.1450e+00
Epoch 8/10
10/10 - 2s - loss: 199.7217 - loglik: -1.9768e+02 - logprior: -2.0422e+00
Epoch 9/10
10/10 - 2s - loss: 199.6713 - loglik: -1.9761e+02 - logprior: -2.0572e+00
Epoch 10/10
10/10 - 2s - loss: 197.5479 - loglik: -1.9543e+02 - logprior: -2.1197e+00
Fitted a model with MAP estimate = -198.3815
expansions: [(23, 1), (35, 2), (36, 1), (37, 1), (38, 1), (39, 1), (46, 2), (47, 1), (48, 1), (54, 1), (56, 1)]
discards: [0]
Fitting a model of length 80 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 220.4191 - loglik: -2.0282e+02 - logprior: -1.7598e+01
Epoch 2/2
10/10 - 2s - loss: 201.9022 - loglik: -1.9444e+02 - logprior: -7.4662e+00
Fitted a model with MAP estimate = -197.0592
expansions: [(0, 8), (52, 1)]
discards: [ 0 42]
Fitting a model of length 87 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 206.9077 - loglik: -1.9252e+02 - logprior: -1.4384e+01
Epoch 2/2
10/10 - 2s - loss: 188.2541 - loglik: -1.8403e+02 - logprior: -4.2288e+00
Fitted a model with MAP estimate = -186.0739
expansions: [(49, 1)]
discards: [1 2 3 4 5 6]
Fitting a model of length 82 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 201.3306 - loglik: -1.8760e+02 - logprior: -1.3734e+01
Epoch 2/10
10/10 - 2s - loss: 191.0730 - loglik: -1.8710e+02 - logprior: -3.9759e+00
Epoch 3/10
10/10 - 2s - loss: 185.8475 - loglik: -1.8375e+02 - logprior: -2.0967e+00
Epoch 4/10
10/10 - 2s - loss: 187.5707 - loglik: -1.8609e+02 - logprior: -1.4854e+00
Fitted a model with MAP estimate = -185.2482
Time for alignment: 51.7980
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 293.1994 - loglik: -2.7669e+02 - logprior: -1.6506e+01
Epoch 2/10
10/10 - 2s - loss: 251.6333 - loglik: -2.4738e+02 - logprior: -4.2512e+00
Epoch 3/10
10/10 - 2s - loss: 227.5694 - loglik: -2.2525e+02 - logprior: -2.3234e+00
Epoch 4/10
10/10 - 2s - loss: 213.9713 - loglik: -2.1199e+02 - logprior: -1.9819e+00
Epoch 5/10
10/10 - 2s - loss: 206.3043 - loglik: -2.0449e+02 - logprior: -1.8141e+00
Epoch 6/10
10/10 - 2s - loss: 204.5316 - loglik: -2.0280e+02 - logprior: -1.7308e+00
Epoch 7/10
10/10 - 2s - loss: 204.2148 - loglik: -2.0261e+02 - logprior: -1.6074e+00
Epoch 8/10
10/10 - 2s - loss: 203.0337 - loglik: -2.0147e+02 - logprior: -1.5592e+00
Epoch 9/10
10/10 - 2s - loss: 202.2310 - loglik: -2.0067e+02 - logprior: -1.5602e+00
Epoch 10/10
10/10 - 2s - loss: 201.4566 - loglik: -1.9988e+02 - logprior: -1.5727e+00
Fitted a model with MAP estimate = -201.8629
expansions: [(10, 1), (23, 2), (34, 1), (40, 4), (44, 2), (48, 1), (54, 1), (56, 1)]
discards: [0]
Fitting a model of length 80 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 227.5150 - loglik: -2.0984e+02 - logprior: -1.7677e+01
Epoch 2/2
10/10 - 2s - loss: 206.8191 - loglik: -1.9915e+02 - logprior: -7.6725e+00
Fitted a model with MAP estimate = -203.3500
expansions: [(0, 21), (3, 1)]
discards: [ 0 45 46 52]
Fitting a model of length 98 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 212.9245 - loglik: -1.9771e+02 - logprior: -1.5212e+01
Epoch 2/2
10/10 - 2s - loss: 192.9148 - loglik: -1.8821e+02 - logprior: -4.7070e+00
Fitted a model with MAP estimate = -187.8305
expansions: [(45, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Fitting a model of length 80 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 209.0670 - loglik: -1.9414e+02 - logprior: -1.4928e+01
Epoch 2/10
10/10 - 2s - loss: 196.9417 - loglik: -1.9259e+02 - logprior: -4.3564e+00
Epoch 3/10
10/10 - 2s - loss: 193.0858 - loglik: -1.9081e+02 - logprior: -2.2745e+00
Epoch 4/10
10/10 - 2s - loss: 192.6842 - loglik: -1.9114e+02 - logprior: -1.5446e+00
Epoch 5/10
10/10 - 2s - loss: 190.6758 - loglik: -1.8942e+02 - logprior: -1.2560e+00
Epoch 6/10
10/10 - 2s - loss: 190.7105 - loglik: -1.8958e+02 - logprior: -1.1330e+00
Fitted a model with MAP estimate = -190.2140
Time for alignment: 54.2129
Computed alignments with likelihoods: ['-186.5094', '-185.2482', '-187.8305']
Best model has likelihood: -185.2482
SP score = 0.6270
Training of 3 independent models on file ricin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f217835e2b0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f21077ac040>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1d741feca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202e034310>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 167 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 628.7264 - loglik: -5.6064e+02 - logprior: -6.8085e+01
Epoch 2/10
10/10 - 5s - loss: 518.6696 - loglik: -5.0653e+02 - logprior: -1.2138e+01
Epoch 3/10
10/10 - 5s - loss: 454.4129 - loglik: -4.5153e+02 - logprior: -2.8789e+00
Epoch 4/10
10/10 - 5s - loss: 419.8478 - loglik: -4.1962e+02 - logprior: -2.2634e-01
Epoch 5/10
10/10 - 5s - loss: 408.0692 - loglik: -4.0893e+02 - logprior: 0.8652
Epoch 6/10
10/10 - 6s - loss: 403.5727 - loglik: -4.0503e+02 - logprior: 1.4602
Epoch 7/10
10/10 - 5s - loss: 401.5719 - loglik: -4.0351e+02 - logprior: 1.9410
Epoch 8/10
10/10 - 6s - loss: 400.8541 - loglik: -4.0313e+02 - logprior: 2.2714
Epoch 9/10
10/10 - 5s - loss: 400.2377 - loglik: -4.0266e+02 - logprior: 2.4224
Epoch 10/10
10/10 - 6s - loss: 398.5767 - loglik: -4.0123e+02 - logprior: 2.6548
Fitted a model with MAP estimate = -399.1318
expansions: [(7, 3), (19, 1), (20, 1), (22, 1), (23, 1), (29, 5), (43, 1), (44, 2), (46, 1), (52, 2), (76, 2), (81, 1), (82, 2), (114, 2), (116, 2), (117, 1), (119, 1), (145, 4), (150, 3)]
discards: [0]
Fitting a model of length 202 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 477.5587 - loglik: -4.1396e+02 - logprior: -6.3602e+01
Epoch 2/2
10/10 - 7s - loss: 412.7921 - loglik: -3.9123e+02 - logprior: -2.1558e+01
Fitted a model with MAP estimate = -401.8943
expansions: [(0, 13)]
discards: [  0  23  36  37  67 102 136 140 183]
Fitting a model of length 206 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 443.6457 - loglik: -3.9580e+02 - logprior: -4.7849e+01
Epoch 2/2
10/10 - 7s - loss: 394.8802 - loglik: -3.8713e+02 - logprior: -7.7515e+00
Fitted a model with MAP estimate = -385.4876
expansions: [(178, 2)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12]
Fitting a model of length 196 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 435.1557 - loglik: -3.8964e+02 - logprior: -4.5511e+01
Epoch 2/10
10/10 - 6s - loss: 390.8107 - loglik: -3.8456e+02 - logprior: -6.2530e+00
Epoch 3/10
10/10 - 7s - loss: 378.7888 - loglik: -3.8066e+02 - logprior: 1.8673
Epoch 4/10
10/10 - 7s - loss: 372.2438 - loglik: -3.7762e+02 - logprior: 5.3716
Epoch 5/10
10/10 - 7s - loss: 369.6375 - loglik: -3.7698e+02 - logprior: 7.3387
Epoch 6/10
10/10 - 7s - loss: 367.0071 - loglik: -3.7553e+02 - logprior: 8.5206
Epoch 7/10
10/10 - 7s - loss: 366.4095 - loglik: -3.7578e+02 - logprior: 9.3698
Epoch 8/10
10/10 - 7s - loss: 365.3564 - loglik: -3.7539e+02 - logprior: 10.0331
Epoch 9/10
10/10 - 7s - loss: 364.7137 - loglik: -3.7530e+02 - logprior: 10.5909
Epoch 10/10
10/10 - 7s - loss: 365.1066 - loglik: -3.7618e+02 - logprior: 11.0783
Fitted a model with MAP estimate = -364.1833
Time for alignment: 174.4563
Fitting a model of length 167 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 628.9514 - loglik: -5.6087e+02 - logprior: -6.8079e+01
Epoch 2/10
10/10 - 5s - loss: 518.2501 - loglik: -5.0611e+02 - logprior: -1.2137e+01
Epoch 3/10
10/10 - 5s - loss: 453.3750 - loglik: -4.5046e+02 - logprior: -2.9136e+00
Epoch 4/10
10/10 - 6s - loss: 419.3232 - loglik: -4.1876e+02 - logprior: -5.6143e-01
Epoch 5/10
10/10 - 6s - loss: 408.3770 - loglik: -4.0890e+02 - logprior: 0.5226
Epoch 6/10
10/10 - 5s - loss: 402.5755 - loglik: -4.0381e+02 - logprior: 1.2385
Epoch 7/10
10/10 - 5s - loss: 400.4809 - loglik: -4.0212e+02 - logprior: 1.6393
Epoch 8/10
10/10 - 6s - loss: 399.2949 - loglik: -4.0127e+02 - logprior: 1.9733
Epoch 9/10
10/10 - 6s - loss: 398.7271 - loglik: -4.0091e+02 - logprior: 2.1782
Epoch 10/10
10/10 - 6s - loss: 398.2018 - loglik: -4.0058e+02 - logprior: 2.3740
Fitted a model with MAP estimate = -398.1069
expansions: [(9, 2), (15, 1), (19, 2), (20, 2), (22, 2), (23, 2), (25, 1), (30, 3), (44, 3), (46, 1), (52, 2), (71, 1), (75, 3), (81, 2), (97, 1), (104, 1), (107, 2), (113, 1), (114, 1), (116, 1), (119, 1), (145, 4), (150, 3)]
discards: []
Fitting a model of length 209 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 459.4937 - loglik: -4.0828e+02 - logprior: -5.1213e+01
Epoch 2/2
10/10 - 8s - loss: 394.1543 - loglik: -3.8572e+02 - logprior: -8.4303e+00
Fitted a model with MAP estimate = -380.8800
expansions: [(94, 1)]
discards: [  0  23  25  30  32  42  71  99 106]
Fitting a model of length 201 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 448.9221 - loglik: -3.8953e+02 - logprior: -5.9394e+01
Epoch 2/2
10/10 - 7s - loss: 402.9814 - loglik: -3.8251e+02 - logprior: -2.0467e+01
Fitted a model with MAP estimate = -394.1297
expansions: [(0, 12)]
discards: [  0  56  87 128 181]
Fitting a model of length 208 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 438.6230 - loglik: -3.9146e+02 - logprior: -4.7160e+01
Epoch 2/10
10/10 - 8s - loss: 390.6674 - loglik: -3.8374e+02 - logprior: -6.9252e+00
Epoch 3/10
10/10 - 8s - loss: 378.2294 - loglik: -3.7939e+02 - logprior: 1.1570
Epoch 4/10
10/10 - 8s - loss: 371.6613 - loglik: -3.7648e+02 - logprior: 4.8218
Epoch 5/10
10/10 - 8s - loss: 367.9438 - loglik: -3.7484e+02 - logprior: 6.8941
Epoch 6/10
10/10 - 9s - loss: 366.6983 - loglik: -3.7477e+02 - logprior: 8.0716
Epoch 7/10
10/10 - 8s - loss: 364.9246 - loglik: -3.7387e+02 - logprior: 8.9457
Epoch 8/10
10/10 - 8s - loss: 364.2352 - loglik: -3.7390e+02 - logprior: 9.6611
Epoch 9/10
10/10 - 8s - loss: 364.2469 - loglik: -3.7447e+02 - logprior: 10.2184
Fitted a model with MAP estimate = -363.6063
Time for alignment: 178.0034
Fitting a model of length 167 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 628.8182 - loglik: -5.6074e+02 - logprior: -6.8082e+01
Epoch 2/10
10/10 - 6s - loss: 518.3370 - loglik: -5.0618e+02 - logprior: -1.2158e+01
Epoch 3/10
10/10 - 5s - loss: 451.1843 - loglik: -4.4817e+02 - logprior: -3.0112e+00
Epoch 4/10
10/10 - 6s - loss: 418.4062 - loglik: -4.1780e+02 - logprior: -6.1100e-01
Epoch 5/10
10/10 - 6s - loss: 406.5162 - loglik: -4.0708e+02 - logprior: 0.5606
Epoch 6/10
10/10 - 6s - loss: 402.2391 - loglik: -4.0355e+02 - logprior: 1.3080
Epoch 7/10
10/10 - 6s - loss: 400.2685 - loglik: -4.0201e+02 - logprior: 1.7380
Epoch 8/10
10/10 - 6s - loss: 399.7513 - loglik: -4.0173e+02 - logprior: 1.9811
Epoch 9/10
10/10 - 6s - loss: 398.3192 - loglik: -4.0047e+02 - logprior: 2.1489
Epoch 10/10
10/10 - 5s - loss: 398.4839 - loglik: -4.0082e+02 - logprior: 2.3320
Fitted a model with MAP estimate = -397.9615
expansions: [(0, 3), (19, 2), (22, 1), (23, 3), (29, 3), (42, 1), (44, 1), (45, 2), (46, 1), (52, 2), (76, 2), (81, 1), (82, 2), (94, 1), (98, 2), (104, 2), (107, 2), (114, 1), (117, 3), (118, 1), (145, 3), (150, 1), (151, 2), (152, 1)]
discards: []
Fitting a model of length 210 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 482.1209 - loglik: -4.0975e+02 - logprior: -7.2366e+01
Epoch 2/2
10/10 - 8s - loss: 403.2836 - loglik: -3.8625e+02 - logprior: -1.7038e+01
Fitted a model with MAP estimate = -386.3132
expansions: [(34, 1), (181, 2)]
discards: [  0   1   2  23  29  30  38  69 104 123 124 132 150 191]
Fitting a model of length 199 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 438.9024 - loglik: -3.8986e+02 - logprior: -4.9037e+01
Epoch 2/2
10/10 - 7s - loss: 388.8470 - loglik: -3.8141e+02 - logprior: -7.4394e+00
Fitted a model with MAP estimate = -379.9971
expansions: [(0, 10)]
discards: [ 0 48]
Fitting a model of length 207 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 434.4564 - loglik: -3.8699e+02 - logprior: -4.7467e+01
Epoch 2/10
10/10 - 7s - loss: 386.6328 - loglik: -3.7991e+02 - logprior: -6.7187e+00
Epoch 3/10
10/10 - 7s - loss: 373.2955 - loglik: -3.7507e+02 - logprior: 1.7707
Epoch 4/10
10/10 - 7s - loss: 366.2470 - loglik: -3.7156e+02 - logprior: 5.3126
Epoch 5/10
10/10 - 7s - loss: 362.8358 - loglik: -3.7006e+02 - logprior: 7.2217
Epoch 6/10
10/10 - 8s - loss: 360.3831 - loglik: -3.6878e+02 - logprior: 8.3947
Epoch 7/10
10/10 - 8s - loss: 360.5903 - loglik: -3.6984e+02 - logprior: 9.2449
Fitted a model with MAP estimate = -359.5552
Time for alignment: 159.5293
Computed alignments with likelihoods: ['-364.1833', '-363.6063', '-359.5552']
Best model has likelihood: -359.5552
SP score = 0.7750
Training of 3 independent models on file PDZ.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20080a8070>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2010be8eb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2011d02790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1d94281160>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 238.8801 - loglik: -2.3548e+02 - logprior: -3.3954e+00
Epoch 2/10
19/19 - 2s - loss: 209.2274 - loglik: -2.0803e+02 - logprior: -1.1967e+00
Epoch 3/10
19/19 - 2s - loss: 198.3071 - loglik: -1.9705e+02 - logprior: -1.2559e+00
Epoch 4/10
19/19 - 2s - loss: 196.0377 - loglik: -1.9482e+02 - logprior: -1.2130e+00
Epoch 5/10
19/19 - 2s - loss: 194.8511 - loglik: -1.9364e+02 - logprior: -1.2120e+00
Epoch 6/10
19/19 - 2s - loss: 194.3632 - loglik: -1.9313e+02 - logprior: -1.2353e+00
Epoch 7/10
19/19 - 2s - loss: 194.1595 - loglik: -1.9289e+02 - logprior: -1.2706e+00
Epoch 8/10
19/19 - 2s - loss: 193.9254 - loglik: -1.9265e+02 - logprior: -1.2731e+00
Epoch 9/10
19/19 - 2s - loss: 193.8869 - loglik: -1.9259e+02 - logprior: -1.2983e+00
Epoch 10/10
19/19 - 2s - loss: 193.9194 - loglik: -1.9260e+02 - logprior: -1.3185e+00
Fitted a model with MAP estimate = -187.6707
expansions: [(0, 2), (3, 1), (4, 1), (5, 1), (6, 1), (16, 5), (17, 1), (18, 2), (23, 1), (46, 3), (47, 1), (48, 2), (49, 1), (52, 2), (55, 2), (58, 1)]
discards: []
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 196.6082 - loglik: -1.9222e+02 - logprior: -4.3845e+00
Epoch 2/2
19/19 - 2s - loss: 187.7531 - loglik: -1.8625e+02 - logprior: -1.5032e+00
Fitted a model with MAP estimate = -179.7037
expansions: [(30, 1)]
discards: [ 0  1  2 22 24 25 26 28 62 75]
Fitting a model of length 82 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 193.9379 - loglik: -1.9017e+02 - logprior: -3.7644e+00
Epoch 2/2
19/19 - 2s - loss: 189.3716 - loglik: -1.8757e+02 - logprior: -1.7992e+00
Fitted a model with MAP estimate = -180.9269
expansions: [(0, 4), (19, 1)]
discards: [0]
Fitting a model of length 86 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 181.1419 - loglik: -1.7924e+02 - logprior: -1.9022e+00
Epoch 2/10
23/23 - 3s - loss: 178.6842 - loglik: -1.7770e+02 - logprior: -9.8920e-01
Epoch 3/10
23/23 - 3s - loss: 178.4416 - loglik: -1.7749e+02 - logprior: -9.5358e-01
Epoch 4/10
23/23 - 3s - loss: 177.8173 - loglik: -1.7688e+02 - logprior: -9.3720e-01
Epoch 5/10
23/23 - 3s - loss: 177.5183 - loglik: -1.7656e+02 - logprior: -9.5737e-01
Epoch 6/10
23/23 - 3s - loss: 176.8939 - loglik: -1.7593e+02 - logprior: -9.6625e-01
Epoch 7/10
23/23 - 3s - loss: 176.4559 - loglik: -1.7548e+02 - logprior: -9.7596e-01
Epoch 8/10
23/23 - 3s - loss: 176.4530 - loglik: -1.7546e+02 - logprior: -9.9369e-01
Epoch 9/10
23/23 - 3s - loss: 176.1122 - loglik: -1.7508e+02 - logprior: -1.0283e+00
Epoch 10/10
23/23 - 3s - loss: 176.1470 - loglik: -1.7510e+02 - logprior: -1.0424e+00
Fitted a model with MAP estimate = -175.8476
Time for alignment: 94.8284
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 238.7068 - loglik: -2.3531e+02 - logprior: -3.3989e+00
Epoch 2/10
19/19 - 2s - loss: 208.6003 - loglik: -2.0740e+02 - logprior: -1.2031e+00
Epoch 3/10
19/19 - 2s - loss: 197.7903 - loglik: -1.9651e+02 - logprior: -1.2809e+00
Epoch 4/10
19/19 - 2s - loss: 195.0871 - loglik: -1.9381e+02 - logprior: -1.2782e+00
Epoch 5/10
19/19 - 2s - loss: 194.0060 - loglik: -1.9274e+02 - logprior: -1.2700e+00
Epoch 6/10
19/19 - 2s - loss: 193.4807 - loglik: -1.9219e+02 - logprior: -1.2948e+00
Epoch 7/10
19/19 - 2s - loss: 192.9874 - loglik: -1.9166e+02 - logprior: -1.3299e+00
Epoch 8/10
19/19 - 2s - loss: 193.1232 - loglik: -1.9179e+02 - logprior: -1.3360e+00
Fitted a model with MAP estimate = -186.3394
expansions: [(0, 2), (3, 1), (4, 1), (15, 5), (16, 2), (17, 2), (18, 1), (23, 1), (36, 1), (46, 2), (47, 1), (48, 2), (49, 1), (52, 1), (55, 2), (58, 1)]
discards: []
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 196.4317 - loglik: -1.9214e+02 - logprior: -4.2959e+00
Epoch 2/2
19/19 - 2s - loss: 187.8444 - loglik: -1.8638e+02 - logprior: -1.4672e+00
Fitted a model with MAP estimate = -179.5745
expansions: [(9, 2), (28, 1)]
discards: [ 0  1 23 24 25 26 63]
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 190.2199 - loglik: -1.8733e+02 - logprior: -2.8935e+00
Epoch 2/2
19/19 - 2s - loss: 186.4163 - loglik: -1.8535e+02 - logprior: -1.0693e+00
Fitted a model with MAP estimate = -178.9423
expansions: [(0, 2)]
discards: [ 8  9 19 20 21 22]
Fitting a model of length 82 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 182.2085 - loglik: -1.7988e+02 - logprior: -2.3247e+00
Epoch 2/10
23/23 - 3s - loss: 179.6647 - loglik: -1.7861e+02 - logprior: -1.0538e+00
Epoch 3/10
23/23 - 3s - loss: 178.5855 - loglik: -1.7758e+02 - logprior: -1.0068e+00
Epoch 4/10
23/23 - 3s - loss: 178.6172 - loglik: -1.7765e+02 - logprior: -9.6452e-01
Fitted a model with MAP estimate = -178.0151
Time for alignment: 73.2986
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 238.8667 - loglik: -2.3547e+02 - logprior: -3.3992e+00
Epoch 2/10
19/19 - 2s - loss: 208.1945 - loglik: -2.0699e+02 - logprior: -1.2070e+00
Epoch 3/10
19/19 - 2s - loss: 197.0996 - loglik: -1.9582e+02 - logprior: -1.2787e+00
Epoch 4/10
19/19 - 2s - loss: 194.4161 - loglik: -1.9316e+02 - logprior: -1.2562e+00
Epoch 5/10
19/19 - 2s - loss: 193.3064 - loglik: -1.9205e+02 - logprior: -1.2551e+00
Epoch 6/10
19/19 - 2s - loss: 192.7157 - loglik: -1.9143e+02 - logprior: -1.2848e+00
Epoch 7/10
19/19 - 2s - loss: 192.6820 - loglik: -1.9136e+02 - logprior: -1.3213e+00
Epoch 8/10
19/19 - 2s - loss: 192.3004 - loglik: -1.9098e+02 - logprior: -1.3235e+00
Epoch 9/10
19/19 - 2s - loss: 192.3051 - loglik: -1.9096e+02 - logprior: -1.3481e+00
Fitted a model with MAP estimate = -185.6988
expansions: [(0, 2), (3, 1), (4, 1), (16, 1), (17, 3), (18, 1), (22, 1), (24, 1), (46, 2), (47, 1), (48, 1), (49, 2), (52, 1), (55, 2), (58, 1)]
discards: []
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 196.1556 - loglik: -1.9182e+02 - logprior: -4.3365e+00
Epoch 2/2
19/19 - 2s - loss: 188.6672 - loglik: -1.8724e+02 - logprior: -1.4312e+00
Fitted a model with MAP estimate = -180.1819
expansions: [(9, 2)]
discards: [ 0 58]
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 191.2013 - loglik: -1.8736e+02 - logprior: -3.8452e+00
Epoch 2/2
19/19 - 2s - loss: 187.1033 - loglik: -1.8562e+02 - logprior: -1.4820e+00
Fitted a model with MAP estimate = -179.2294
expansions: [(24, 3), (28, 1)]
discards: [ 0  1  9 10 25 26]
Fitting a model of length 83 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 182.6759 - loglik: -1.8041e+02 - logprior: -2.2687e+00
Epoch 2/10
23/23 - 3s - loss: 179.1511 - loglik: -1.7823e+02 - logprior: -9.2223e-01
Epoch 3/10
23/23 - 3s - loss: 178.6559 - loglik: -1.7778e+02 - logprior: -8.8006e-01
Epoch 4/10
23/23 - 3s - loss: 177.8998 - loglik: -1.7706e+02 - logprior: -8.3959e-01
Epoch 5/10
23/23 - 3s - loss: 177.5913 - loglik: -1.7672e+02 - logprior: -8.7086e-01
Epoch 6/10
23/23 - 3s - loss: 177.0506 - loglik: -1.7617e+02 - logprior: -8.8032e-01
Epoch 7/10
23/23 - 3s - loss: 176.9262 - loglik: -1.7602e+02 - logprior: -9.0205e-01
Epoch 8/10
23/23 - 3s - loss: 176.6590 - loglik: -1.7573e+02 - logprior: -9.2598e-01
Epoch 9/10
23/23 - 3s - loss: 176.2503 - loglik: -1.7530e+02 - logprior: -9.5411e-01
Epoch 10/10
23/23 - 3s - loss: 176.2707 - loglik: -1.7529e+02 - logprior: -9.8083e-01
Fitted a model with MAP estimate = -176.1503
Time for alignment: 90.3434
Computed alignments with likelihoods: ['-175.8476', '-178.0151', '-176.1503']
Best model has likelihood: -175.8476
SP score = 0.8733
Training of 3 independent models on file asp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2098960bb0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f214a95d6d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2087e8ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2087e8e190>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 20s - loss: 800.3007 - loglik: -7.9161e+02 - logprior: -8.6927e+00
Epoch 2/10
22/22 - 19s - loss: 685.9068 - loglik: -6.8497e+02 - logprior: -9.3541e-01
Epoch 3/10
22/22 - 19s - loss: 645.1718 - loglik: -6.4341e+02 - logprior: -1.7648e+00
Epoch 4/10
22/22 - 20s - loss: 642.2650 - loglik: -6.4070e+02 - logprior: -1.5687e+00
Epoch 5/10
22/22 - 18s - loss: 637.7780 - loglik: -6.3614e+02 - logprior: -1.6366e+00
Epoch 6/10
22/22 - 18s - loss: 637.9946 - loglik: -6.3622e+02 - logprior: -1.7747e+00
Fitted a model with MAP estimate = -638.3685
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 2), (35, 1), (45, 2), (48, 1), (49, 1), (50, 1), (65, 1), (69, 1), (70, 1), (75, 1), (76, 2), (77, 1), (78, 1), (81, 1), (97, 1), (99, 1), (105, 2), (109, 1), (119, 1), (121, 2), (137, 1), (143, 2), (144, 1), (149, 1), (150, 1), (152, 1), (156, 1), (157, 3), (158, 1), (179, 2), (180, 4), (184, 2), (185, 3), (193, 2), (206, 1), (209, 1), (210, 1), (213, 1), (214, 2), (224, 1), (225, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Fitting a model of length 318 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 29s - loss: 642.5436 - loglik: -6.3344e+02 - logprior: -9.1068e+00
Epoch 2/2
22/22 - 24s - loss: 620.1057 - loglik: -6.1782e+02 - logprior: -2.2852e+00
Fitted a model with MAP estimate = -617.4935
expansions: [(0, 3)]
discards: [  0  34 175 195 196 197 225 232 233]
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 26s - loss: 625.4866 - loglik: -6.2060e+02 - logprior: -4.8860e+00
Epoch 2/2
22/22 - 23s - loss: 616.2981 - loglik: -6.1747e+02 - logprior: 1.1741
Fitted a model with MAP estimate = -614.4064
expansions: []
discards: [0 1 2]
Fitting a model of length 309 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 27s - loss: 629.4977 - loglik: -6.2224e+02 - logprior: -7.2580e+00
Epoch 2/10
22/22 - 22s - loss: 622.7028 - loglik: -6.2130e+02 - logprior: -1.4026e+00
Epoch 3/10
22/22 - 22s - loss: 615.5989 - loglik: -6.1587e+02 - logprior: 0.2702
Epoch 4/10
22/22 - 22s - loss: 612.3464 - loglik: -6.1492e+02 - logprior: 2.5744
Epoch 5/10
22/22 - 22s - loss: 611.1328 - loglik: -6.1387e+02 - logprior: 2.7353
Epoch 6/10
22/22 - 22s - loss: 610.5009 - loglik: -6.1336e+02 - logprior: 2.8624
Epoch 7/10
22/22 - 21s - loss: 612.0075 - loglik: -6.1491e+02 - logprior: 2.9039
Fitted a model with MAP estimate = -609.4758
Time for alignment: 435.7308
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 18s - loss: 800.1873 - loglik: -7.9147e+02 - logprior: -8.7136e+00
Epoch 2/10
22/22 - 15s - loss: 687.2904 - loglik: -6.8635e+02 - logprior: -9.3818e-01
Epoch 3/10
22/22 - 15s - loss: 645.3691 - loglik: -6.4344e+02 - logprior: -1.9247e+00
Epoch 4/10
22/22 - 15s - loss: 641.5728 - loglik: -6.3984e+02 - logprior: -1.7325e+00
Epoch 5/10
22/22 - 15s - loss: 636.7742 - loglik: -6.3497e+02 - logprior: -1.8004e+00
Epoch 6/10
22/22 - 15s - loss: 640.4503 - loglik: -6.3852e+02 - logprior: -1.9344e+00
Fitted a model with MAP estimate = -637.2709
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 2), (36, 1), (45, 3), (48, 1), (49, 1), (62, 1), (70, 1), (71, 2), (72, 1), (76, 2), (82, 1), (94, 1), (97, 1), (99, 1), (105, 2), (107, 1), (109, 1), (118, 1), (120, 2), (136, 1), (143, 1), (144, 2), (148, 1), (152, 1), (155, 2), (156, 1), (176, 1), (178, 2), (179, 1), (180, 1), (183, 2), (184, 1), (185, 1), (193, 2), (206, 1), (207, 1), (208, 1), (209, 1), (212, 1), (213, 2), (223, 2), (225, 1), (236, 1), (238, 1)]
discards: [0]
Fitting a model of length 315 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 23s - loss: 638.6127 - loglik: -6.2956e+02 - logprior: -9.0560e+00
Epoch 2/2
22/22 - 19s - loss: 620.8567 - loglik: -6.1852e+02 - logprior: -2.3344e+00
Fitted a model with MAP estimate = -617.1126
expansions: [(0, 3)]
discards: [  0  34  54 177 229 282 283]
Fitting a model of length 311 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 24s - loss: 625.4823 - loglik: -6.2052e+02 - logprior: -4.9643e+00
Epoch 2/2
22/22 - 19s - loss: 617.1801 - loglik: -6.1832e+02 - logprior: 1.1421
Fitted a model with MAP estimate = -614.6110
expansions: []
discards: [0 1 2]
Fitting a model of length 308 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 22s - loss: 631.9167 - loglik: -6.2463e+02 - logprior: -7.2880e+00
Epoch 2/10
22/22 - 19s - loss: 619.6299 - loglik: -6.1818e+02 - logprior: -1.4546e+00
Epoch 3/10
22/22 - 19s - loss: 618.6924 - loglik: -6.1891e+02 - logprior: 0.2169
Epoch 4/10
22/22 - 19s - loss: 610.8213 - loglik: -6.1331e+02 - logprior: 2.4908
Epoch 5/10
22/22 - 19s - loss: 612.3118 - loglik: -6.1502e+02 - logprior: 2.7036
Fitted a model with MAP estimate = -610.5834
Time for alignment: 324.8213
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 17s - loss: 799.2221 - loglik: -7.9052e+02 - logprior: -8.7010e+00
Epoch 2/10
22/22 - 14s - loss: 687.8804 - loglik: -6.8702e+02 - logprior: -8.6208e-01
Epoch 3/10
22/22 - 14s - loss: 645.5303 - loglik: -6.4379e+02 - logprior: -1.7408e+00
Epoch 4/10
22/22 - 14s - loss: 637.4122 - loglik: -6.3584e+02 - logprior: -1.5680e+00
Epoch 5/10
22/22 - 14s - loss: 637.7000 - loglik: -6.3609e+02 - logprior: -1.6136e+00
Fitted a model with MAP estimate = -636.7319
expansions: [(14, 1), (15, 1), (32, 1), (33, 2), (34, 3), (35, 2), (46, 1), (48, 1), (49, 1), (62, 1), (70, 1), (71, 2), (72, 1), (76, 2), (79, 1), (81, 1), (97, 1), (99, 1), (105, 2), (107, 1), (109, 1), (120, 2), (143, 2), (144, 3), (148, 1), (152, 1), (155, 2), (156, 1), (175, 1), (178, 3), (179, 1), (180, 1), (183, 2), (184, 1), (185, 1), (193, 2), (206, 1), (207, 1), (208, 1), (209, 1), (213, 3), (214, 1), (223, 2), (225, 1), (236, 1), (238, 1)]
discards: [0]
Fitting a model of length 316 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 23s - loss: 640.2637 - loglik: -6.3101e+02 - logprior: -9.2521e+00
Epoch 2/2
22/22 - 19s - loss: 621.8827 - loglik: -6.1935e+02 - logprior: -2.5300e+00
Fitted a model with MAP estimate = -617.1747
expansions: [(0, 3), (104, 1), (289, 1)]
discards: [  0  39 174 175 219 229 283 284]
Fitting a model of length 313 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 22s - loss: 626.9517 - loglik: -6.2211e+02 - logprior: -4.8370e+00
Epoch 2/2
22/22 - 19s - loss: 613.5132 - loglik: -6.1468e+02 - logprior: 1.1663
Fitted a model with MAP estimate = -613.7441
expansions: []
discards: [  0   1   2  37 100 268]
Fitting a model of length 307 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 21s - loss: 629.3419 - loglik: -6.2197e+02 - logprior: -7.3673e+00
Epoch 2/10
22/22 - 18s - loss: 621.0261 - loglik: -6.1949e+02 - logprior: -1.5314e+00
Epoch 3/10
22/22 - 19s - loss: 616.1931 - loglik: -6.1638e+02 - logprior: 0.1888
Epoch 4/10
22/22 - 18s - loss: 613.7187 - loglik: -6.1618e+02 - logprior: 2.4628
Epoch 5/10
22/22 - 19s - loss: 609.7595 - loglik: -6.1233e+02 - logprior: 2.5659
Epoch 6/10
22/22 - 18s - loss: 610.0673 - loglik: -6.1281e+02 - logprior: 2.7383
Fitted a model with MAP estimate = -609.8068
Time for alignment: 322.7774
Computed alignments with likelihoods: ['-609.4758', '-610.5834', '-609.8068']
Best model has likelihood: -609.4758
SP score = 0.9089
Training of 3 independent models on file bowman.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20d3d24820>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2153696910>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201134fc70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20085a4130>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 175.1149 - loglik: -7.9984e+01 - logprior: -9.5131e+01
Epoch 2/10
10/10 - 1s - loss: 95.7075 - loglik: -6.8512e+01 - logprior: -2.7195e+01
Epoch 3/10
10/10 - 1s - loss: 73.5449 - loglik: -6.0034e+01 - logprior: -1.3511e+01
Epoch 4/10
10/10 - 1s - loss: 64.2695 - loglik: -5.5962e+01 - logprior: -8.3072e+00
Epoch 5/10
10/10 - 1s - loss: 59.4352 - loglik: -5.3797e+01 - logprior: -5.6382e+00
Epoch 6/10
10/10 - 1s - loss: 57.3688 - loglik: -5.3216e+01 - logprior: -4.1533e+00
Epoch 7/10
10/10 - 1s - loss: 56.2866 - loglik: -5.3009e+01 - logprior: -3.2776e+00
Epoch 8/10
10/10 - 1s - loss: 55.6124 - loglik: -5.2883e+01 - logprior: -2.7298e+00
Epoch 9/10
10/10 - 1s - loss: 55.2638 - loglik: -5.2864e+01 - logprior: -2.3998e+00
Epoch 10/10
10/10 - 1s - loss: 55.0390 - loglik: -5.2865e+01 - logprior: -2.1739e+00
Fitted a model with MAP estimate = -54.9446
expansions: [(0, 3), (13, 1)]
discards: []
Fitting a model of length 24 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 174.4040 - loglik: -4.9518e+01 - logprior: -1.2489e+02
Epoch 2/2
10/10 - 1s - loss: 87.7280 - loglik: -4.7375e+01 - logprior: -4.0353e+01
Fitted a model with MAP estimate = -70.8500
expansions: [(0, 1)]
discards: []
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 148.2003 - loglik: -4.5014e+01 - logprior: -1.0319e+02
Epoch 2/2
10/10 - 1s - loss: 81.0378 - loglik: -4.4494e+01 - logprior: -3.6544e+01
Fitted a model with MAP estimate = -66.7265
expansions: []
discards: []
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 128.9282 - loglik: -4.3128e+01 - logprior: -8.5801e+01
Epoch 2/10
10/10 - 1s - loss: 68.5406 - loglik: -4.3862e+01 - logprior: -2.4679e+01
Epoch 3/10
10/10 - 1s - loss: 56.6037 - loglik: -4.4540e+01 - logprior: -1.2064e+01
Epoch 4/10
10/10 - 1s - loss: 51.9280 - loglik: -4.5008e+01 - logprior: -6.9203e+00
Epoch 5/10
10/10 - 1s - loss: 49.4551 - loglik: -4.5300e+01 - logprior: -4.1555e+00
Epoch 6/10
10/10 - 1s - loss: 47.5985 - loglik: -4.4989e+01 - logprior: -2.6092e+00
Epoch 7/10
10/10 - 1s - loss: 46.4824 - loglik: -4.4758e+01 - logprior: -1.7244e+00
Epoch 8/10
10/10 - 1s - loss: 45.8581 - loglik: -4.4660e+01 - logprior: -1.1978e+00
Epoch 9/10
10/10 - 1s - loss: 45.4533 - loglik: -4.4603e+01 - logprior: -8.5072e-01
Epoch 10/10
10/10 - 1s - loss: 45.1400 - loglik: -4.4607e+01 - logprior: -5.3274e-01
Fitted a model with MAP estimate = -44.9803
Time for alignment: 35.1997
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 175.1149 - loglik: -7.9984e+01 - logprior: -9.5131e+01
Epoch 2/10
10/10 - 1s - loss: 95.7075 - loglik: -6.8512e+01 - logprior: -2.7195e+01
Epoch 3/10
10/10 - 1s - loss: 73.5449 - loglik: -6.0034e+01 - logprior: -1.3511e+01
Epoch 4/10
10/10 - 1s - loss: 64.2695 - loglik: -5.5962e+01 - logprior: -8.3072e+00
Epoch 5/10
10/10 - 1s - loss: 59.4352 - loglik: -5.3797e+01 - logprior: -5.6382e+00
Epoch 6/10
10/10 - 1s - loss: 57.3688 - loglik: -5.3216e+01 - logprior: -4.1533e+00
Epoch 7/10
10/10 - 1s - loss: 56.2866 - loglik: -5.3009e+01 - logprior: -3.2776e+00
Epoch 8/10
10/10 - 1s - loss: 55.6124 - loglik: -5.2883e+01 - logprior: -2.7298e+00
Epoch 9/10
10/10 - 1s - loss: 55.2638 - loglik: -5.2864e+01 - logprior: -2.3998e+00
Epoch 10/10
10/10 - 1s - loss: 55.0390 - loglik: -5.2865e+01 - logprior: -2.1739e+00
Fitted a model with MAP estimate = -54.9446
expansions: [(0, 3), (13, 1)]
discards: []
Fitting a model of length 24 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 174.4040 - loglik: -4.9518e+01 - logprior: -1.2489e+02
Epoch 2/2
10/10 - 1s - loss: 87.7280 - loglik: -4.7375e+01 - logprior: -4.0353e+01
Fitted a model with MAP estimate = -70.8500
expansions: [(0, 1)]
discards: []
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 148.2003 - loglik: -4.5014e+01 - logprior: -1.0319e+02
Epoch 2/2
10/10 - 1s - loss: 81.0378 - loglik: -4.4494e+01 - logprior: -3.6544e+01
Fitted a model with MAP estimate = -66.7265
expansions: []
discards: []
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 128.9282 - loglik: -4.3128e+01 - logprior: -8.5801e+01
Epoch 2/10
10/10 - 1s - loss: 68.5406 - loglik: -4.3862e+01 - logprior: -2.4679e+01
Epoch 3/10
10/10 - 1s - loss: 56.6037 - loglik: -4.4540e+01 - logprior: -1.2064e+01
Epoch 4/10
10/10 - 1s - loss: 51.9280 - loglik: -4.5008e+01 - logprior: -6.9203e+00
Epoch 5/10
10/10 - 1s - loss: 49.4551 - loglik: -4.5300e+01 - logprior: -4.1555e+00
Epoch 6/10
10/10 - 1s - loss: 47.5985 - loglik: -4.4989e+01 - logprior: -2.6092e+00
Epoch 7/10
10/10 - 1s - loss: 46.4824 - loglik: -4.4758e+01 - logprior: -1.7244e+00
Epoch 8/10
10/10 - 1s - loss: 45.8581 - loglik: -4.4660e+01 - logprior: -1.1978e+00
Epoch 9/10
10/10 - 1s - loss: 45.4533 - loglik: -4.4603e+01 - logprior: -8.5072e-01
Epoch 10/10
10/10 - 1s - loss: 45.1400 - loglik: -4.4607e+01 - logprior: -5.3274e-01
Fitted a model with MAP estimate = -44.9803
Time for alignment: 34.1071
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 175.1149 - loglik: -7.9984e+01 - logprior: -9.5131e+01
Epoch 2/10
10/10 - 1s - loss: 95.7075 - loglik: -6.8512e+01 - logprior: -2.7195e+01
Epoch 3/10
10/10 - 1s - loss: 73.5449 - loglik: -6.0034e+01 - logprior: -1.3511e+01
Epoch 4/10
10/10 - 1s - loss: 64.2695 - loglik: -5.5962e+01 - logprior: -8.3072e+00
Epoch 5/10
10/10 - 1s - loss: 59.4352 - loglik: -5.3797e+01 - logprior: -5.6382e+00
Epoch 6/10
10/10 - 1s - loss: 57.3688 - loglik: -5.3216e+01 - logprior: -4.1533e+00
Epoch 7/10
10/10 - 1s - loss: 56.2866 - loglik: -5.3009e+01 - logprior: -3.2776e+00
Epoch 8/10
10/10 - 1s - loss: 55.6124 - loglik: -5.2883e+01 - logprior: -2.7298e+00
Epoch 9/10
10/10 - 1s - loss: 55.2638 - loglik: -5.2864e+01 - logprior: -2.3998e+00
Epoch 10/10
10/10 - 1s - loss: 55.0390 - loglik: -5.2865e+01 - logprior: -2.1739e+00
Fitted a model with MAP estimate = -54.9446
expansions: [(0, 3), (13, 1)]
discards: []
Fitting a model of length 24 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 174.4040 - loglik: -4.9518e+01 - logprior: -1.2489e+02
Epoch 2/2
10/10 - 1s - loss: 87.7280 - loglik: -4.7375e+01 - logprior: -4.0353e+01
Fitted a model with MAP estimate = -70.8500
expansions: [(0, 1)]
discards: []
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 148.2003 - loglik: -4.5014e+01 - logprior: -1.0319e+02
Epoch 2/2
10/10 - 1s - loss: 81.0378 - loglik: -4.4494e+01 - logprior: -3.6544e+01
Fitted a model with MAP estimate = -66.7265
expansions: []
discards: []
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 128.9282 - loglik: -4.3128e+01 - logprior: -8.5801e+01
Epoch 2/10
10/10 - 1s - loss: 68.5406 - loglik: -4.3862e+01 - logprior: -2.4679e+01
Epoch 3/10
10/10 - 1s - loss: 56.6037 - loglik: -4.4540e+01 - logprior: -1.2064e+01
Epoch 4/10
10/10 - 1s - loss: 51.9280 - loglik: -4.5008e+01 - logprior: -6.9203e+00
Epoch 5/10
10/10 - 1s - loss: 49.4551 - loglik: -4.5300e+01 - logprior: -4.1555e+00
Epoch 6/10
10/10 - 1s - loss: 47.5985 - loglik: -4.4989e+01 - logprior: -2.6092e+00
Epoch 7/10
10/10 - 1s - loss: 46.4824 - loglik: -4.4758e+01 - logprior: -1.7244e+00
Epoch 8/10
10/10 - 1s - loss: 45.8581 - loglik: -4.4660e+01 - logprior: -1.1978e+00
Epoch 9/10
10/10 - 1s - loss: 45.4533 - loglik: -4.4603e+01 - logprior: -8.5072e-01
Epoch 10/10
10/10 - 1s - loss: 45.1400 - loglik: -4.4607e+01 - logprior: -5.3274e-01
Fitted a model with MAP estimate = -44.9803
Time for alignment: 33.7659
Computed alignments with likelihoods: ['-44.9803', '-44.9803', '-44.9803']
Best model has likelihood: -44.9803
SP score = 0.9419
Training of 3 independent models on file oxidored_q6.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f202d764190>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2178711e20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f21787115b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2178711d00>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 8s - loss: 364.2300 - loglik: -3.5021e+02 - logprior: -1.4024e+01
Epoch 2/10
11/11 - 3s - loss: 308.8444 - loglik: -3.0556e+02 - logprior: -3.2871e+00
Epoch 3/10
11/11 - 3s - loss: 267.8126 - loglik: -2.6578e+02 - logprior: -2.0370e+00
Epoch 4/10
11/11 - 2s - loss: 249.8551 - loglik: -2.4778e+02 - logprior: -2.0769e+00
Epoch 5/10
11/11 - 3s - loss: 243.5370 - loglik: -2.4142e+02 - logprior: -2.1156e+00
Epoch 6/10
11/11 - 2s - loss: 241.9236 - loglik: -2.3985e+02 - logprior: -2.0736e+00
Epoch 7/10
11/11 - 2s - loss: 238.9305 - loglik: -2.3690e+02 - logprior: -2.0342e+00
Epoch 8/10
11/11 - 2s - loss: 238.6839 - loglik: -2.3656e+02 - logprior: -2.1213e+00
Epoch 9/10
11/11 - 2s - loss: 237.6454 - loglik: -2.3543e+02 - logprior: -2.2114e+00
Epoch 10/10
11/11 - 2s - loss: 238.8833 - loglik: -2.3665e+02 - logprior: -2.2329e+00
Fitted a model with MAP estimate = -237.8935
expansions: [(8, 2), (9, 3), (10, 1), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 3), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Fitting a model of length 117 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 254.5163 - loglik: -2.3963e+02 - logprior: -1.4884e+01
Epoch 2/2
11/11 - 3s - loss: 228.5404 - loglik: -2.2237e+02 - logprior: -6.1682e+00
Fitted a model with MAP estimate = -225.0195
expansions: [(0, 18)]
discards: [  0   8  75  76  84  87 110]
Fitting a model of length 128 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 8s - loss: 234.4807 - loglik: -2.2240e+02 - logprior: -1.2083e+01
Epoch 2/2
11/11 - 3s - loss: 217.7897 - loglik: -2.1450e+02 - logprior: -3.2851e+00
Fitted a model with MAP estimate = -215.0806
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 232.4152 - loglik: -2.1986e+02 - logprior: -1.2553e+01
Epoch 2/10
11/11 - 3s - loss: 219.6332 - loglik: -2.1665e+02 - logprior: -2.9784e+00
Epoch 3/10
11/11 - 3s - loss: 218.0340 - loglik: -2.1682e+02 - logprior: -1.2182e+00
Epoch 4/10
11/11 - 3s - loss: 215.2155 - loglik: -2.1451e+02 - logprior: -7.0982e-01
Epoch 5/10
11/11 - 3s - loss: 215.3502 - loglik: -2.1481e+02 - logprior: -5.3713e-01
Fitted a model with MAP estimate = -214.8483
Time for alignment: 79.9256
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 364.5249 - loglik: -3.5050e+02 - logprior: -1.4025e+01
Epoch 2/10
11/11 - 3s - loss: 309.3954 - loglik: -3.0610e+02 - logprior: -3.2922e+00
Epoch 3/10
11/11 - 2s - loss: 271.8077 - loglik: -2.6973e+02 - logprior: -2.0743e+00
Epoch 4/10
11/11 - 3s - loss: 253.4591 - loglik: -2.5133e+02 - logprior: -2.1285e+00
Epoch 5/10
11/11 - 2s - loss: 244.4303 - loglik: -2.4225e+02 - logprior: -2.1786e+00
Epoch 6/10
11/11 - 2s - loss: 242.5985 - loglik: -2.4041e+02 - logprior: -2.1921e+00
Epoch 7/10
11/11 - 3s - loss: 241.2012 - loglik: -2.3907e+02 - logprior: -2.1359e+00
Epoch 8/10
11/11 - 3s - loss: 239.9728 - loglik: -2.3783e+02 - logprior: -2.1435e+00
Epoch 9/10
11/11 - 3s - loss: 238.1423 - loglik: -2.3591e+02 - logprior: -2.2299e+00
Epoch 10/10
11/11 - 3s - loss: 239.0708 - loglik: -2.3682e+02 - logprior: -2.2522e+00
Fitted a model with MAP estimate = -238.6382
expansions: [(8, 2), (9, 3), (10, 2), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Fitting a model of length 117 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 8s - loss: 256.2467 - loglik: -2.4134e+02 - logprior: -1.4910e+01
Epoch 2/2
11/11 - 3s - loss: 231.8127 - loglik: -2.2564e+02 - logprior: -6.1689e+00
Fitted a model with MAP estimate = -227.0370
expansions: [(0, 22)]
discards: [  0   8  14  84  87 110]
Fitting a model of length 133 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 234.8383 - loglik: -2.2266e+02 - logprior: -1.2175e+01
Epoch 2/2
11/11 - 3s - loss: 219.0366 - loglik: -2.1576e+02 - logprior: -3.2801e+00
Fitted a model with MAP estimate = -215.4253
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 95]
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 234.4772 - loglik: -2.2194e+02 - logprior: -1.2536e+01
Epoch 2/10
11/11 - 3s - loss: 219.3465 - loglik: -2.1641e+02 - logprior: -2.9394e+00
Epoch 3/10
11/11 - 3s - loss: 218.9234 - loglik: -2.1777e+02 - logprior: -1.1534e+00
Epoch 4/10
11/11 - 3s - loss: 216.8216 - loglik: -2.1619e+02 - logprior: -6.2853e-01
Epoch 5/10
11/11 - 3s - loss: 215.4343 - loglik: -2.1497e+02 - logprior: -4.6506e-01
Epoch 6/10
11/11 - 3s - loss: 215.3450 - loglik: -2.1499e+02 - logprior: -3.5124e-01
Epoch 7/10
11/11 - 3s - loss: 216.4053 - loglik: -2.1615e+02 - logprior: -2.5276e-01
Fitted a model with MAP estimate = -214.8039
Time for alignment: 84.1464
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 364.4292 - loglik: -3.5041e+02 - logprior: -1.4023e+01
Epoch 2/10
11/11 - 3s - loss: 309.8008 - loglik: -3.0651e+02 - logprior: -3.2899e+00
Epoch 3/10
11/11 - 2s - loss: 266.8162 - loglik: -2.6477e+02 - logprior: -2.0441e+00
Epoch 4/10
11/11 - 3s - loss: 250.9485 - loglik: -2.4890e+02 - logprior: -2.0522e+00
Epoch 5/10
11/11 - 3s - loss: 243.5721 - loglik: -2.4149e+02 - logprior: -2.0812e+00
Epoch 6/10
11/11 - 3s - loss: 241.2280 - loglik: -2.3914e+02 - logprior: -2.0914e+00
Epoch 7/10
11/11 - 3s - loss: 239.7992 - loglik: -2.3774e+02 - logprior: -2.0617e+00
Epoch 8/10
11/11 - 3s - loss: 240.5123 - loglik: -2.3841e+02 - logprior: -2.1047e+00
Fitted a model with MAP estimate = -239.2460
expansions: [(8, 2), (9, 4), (10, 2), (11, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (63, 1), (64, 2), (65, 2), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Fitting a model of length 116 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 254.5706 - loglik: -2.3977e+02 - logprior: -1.4802e+01
Epoch 2/2
11/11 - 3s - loss: 229.6674 - loglik: -2.2357e+02 - logprior: -6.0934e+00
Fitted a model with MAP estimate = -224.9190
expansions: [(0, 19)]
discards: [  0   8  11  77  84 109]
Fitting a model of length 129 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 233.7092 - loglik: -2.2162e+02 - logprior: -1.2087e+01
Epoch 2/2
11/11 - 3s - loss: 217.5021 - loglik: -2.1420e+02 - logprior: -3.2987e+00
Fitted a model with MAP estimate = -214.9739
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 231.0325 - loglik: -2.1879e+02 - logprior: -1.2243e+01
Epoch 2/10
11/11 - 3s - loss: 219.4887 - loglik: -2.1661e+02 - logprior: -2.8741e+00
Epoch 3/10
11/11 - 3s - loss: 215.8467 - loglik: -2.1466e+02 - logprior: -1.1887e+00
Epoch 4/10
11/11 - 3s - loss: 214.8004 - loglik: -2.1411e+02 - logprior: -6.9376e-01
Epoch 5/10
11/11 - 3s - loss: 213.2796 - loglik: -2.1273e+02 - logprior: -5.4638e-01
Epoch 6/10
11/11 - 3s - loss: 212.5824 - loglik: -2.1209e+02 - logprior: -4.9066e-01
Epoch 7/10
11/11 - 3s - loss: 212.4811 - loglik: -2.1203e+02 - logprior: -4.5589e-01
Epoch 8/10
11/11 - 3s - loss: 211.9309 - loglik: -2.1146e+02 - logprior: -4.6809e-01
Epoch 9/10
11/11 - 3s - loss: 211.3763 - loglik: -2.1078e+02 - logprior: -6.0055e-01
Epoch 10/10
11/11 - 3s - loss: 212.2690 - loglik: -2.1170e+02 - logprior: -5.6671e-01
Fitted a model with MAP estimate = -211.3183
Time for alignment: 89.0203
Computed alignments with likelihoods: ['-214.8483', '-214.8039', '-211.3183']
Best model has likelihood: -211.3183
SP score = 0.6089
Training of 3 independent models on file aldosered.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2153dce670>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1d75a73100>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2178a0e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2178f64820>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 224 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 21s - loss: 811.6469 - loglik: -8.0819e+02 - logprior: -3.4605e+00
Epoch 2/10
19/19 - 18s - loss: 701.4102 - loglik: -7.0047e+02 - logprior: -9.4124e-01
Epoch 3/10
19/19 - 18s - loss: 639.6364 - loglik: -6.3817e+02 - logprior: -1.4688e+00
Epoch 4/10
19/19 - 18s - loss: 627.7777 - loglik: -6.2603e+02 - logprior: -1.7483e+00
Epoch 5/10
19/19 - 18s - loss: 625.2643 - loglik: -6.2340e+02 - logprior: -1.8657e+00
Epoch 6/10
19/19 - 18s - loss: 623.0623 - loglik: -6.2114e+02 - logprior: -1.9242e+00
Epoch 7/10
19/19 - 18s - loss: 621.8773 - loglik: -6.1982e+02 - logprior: -2.0616e+00
Epoch 8/10
19/19 - 17s - loss: 620.6376 - loglik: -6.1847e+02 - logprior: -2.1710e+00
Epoch 9/10
19/19 - 18s - loss: 621.2445 - loglik: -6.1895e+02 - logprior: -2.2916e+00
Fitted a model with MAP estimate = -578.2025
expansions: [(12, 3), (14, 1), (16, 1), (20, 1), (36, 1), (37, 1), (39, 1), (45, 3), (46, 1), (59, 2), (60, 1), (62, 4), (65, 1), (66, 1), (68, 1), (69, 1), (70, 1), (120, 1), (125, 1), (126, 1), (127, 4), (128, 1), (136, 1), (138, 2), (140, 1), (146, 1), (147, 1), (160, 1), (161, 1), (163, 5), (164, 1), (165, 3), (166, 1), (167, 6), (168, 2), (180, 2), (181, 1), (193, 1), (194, 1), (203, 1), (209, 2), (210, 1), (215, 1), (217, 1), (218, 1)]
discards: [0]
Fitting a model of length 294 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 615.1700 - loglik: -6.1216e+02 - logprior: -3.0090e+00
Epoch 2/2
39/39 - 27s - loss: 597.9261 - loglik: -5.9681e+02 - logprior: -1.1142e+00
Fitted a model with MAP estimate = -554.6183
expansions: [(0, 2), (206, 2), (218, 1), (219, 1)]
discards: [  0  12  77 154 172 202 203 238 274]
Fitting a model of length 291 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 601.0784 - loglik: -5.9940e+02 - logprior: -1.6736e+00
Epoch 2/2
39/39 - 27s - loss: 596.6810 - loglik: -5.9603e+02 - logprior: -6.5134e-01
Fitted a model with MAP estimate = -553.6760
expansions: []
discards: [  0 199 200]
Fitting a model of length 288 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 33s - loss: 555.8912 - loglik: -5.5413e+02 - logprior: -1.7571e+00
Epoch 2/10
45/45 - 30s - loss: 551.1392 - loglik: -5.5050e+02 - logprior: -6.4268e-01
Epoch 3/10
45/45 - 30s - loss: 548.2354 - loglik: -5.4756e+02 - logprior: -6.7835e-01
Epoch 4/10
45/45 - 30s - loss: 548.4554 - loglik: -5.4770e+02 - logprior: -7.5865e-01
Fitted a model with MAP estimate = -545.9480
Time for alignment: 535.5055
Fitting a model of length 224 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 22s - loss: 812.0547 - loglik: -8.0860e+02 - logprior: -3.4562e+00
Epoch 2/10
19/19 - 18s - loss: 707.3828 - loglik: -7.0645e+02 - logprior: -9.3196e-01
Epoch 3/10
19/19 - 18s - loss: 648.7367 - loglik: -6.4725e+02 - logprior: -1.4902e+00
Epoch 4/10
19/19 - 18s - loss: 634.3780 - loglik: -6.3257e+02 - logprior: -1.8047e+00
Epoch 5/10
19/19 - 18s - loss: 628.0030 - loglik: -6.2602e+02 - logprior: -1.9862e+00
Epoch 6/10
19/19 - 18s - loss: 625.8998 - loglik: -6.2384e+02 - logprior: -2.0595e+00
Epoch 7/10
19/19 - 17s - loss: 624.6716 - loglik: -6.2247e+02 - logprior: -2.1998e+00
Epoch 8/10
19/19 - 18s - loss: 624.2748 - loglik: -6.2197e+02 - logprior: -2.3011e+00
Epoch 9/10
19/19 - 18s - loss: 623.7595 - loglik: -6.2133e+02 - logprior: -2.4278e+00
Epoch 10/10
19/19 - 17s - loss: 622.9609 - loglik: -6.2034e+02 - logprior: -2.6175e+00
Fitted a model with MAP estimate = -582.5411
expansions: [(12, 4), (14, 1), (16, 1), (17, 1), (19, 1), (35, 1), (36, 1), (39, 1), (45, 3), (46, 2), (58, 2), (59, 1), (61, 4), (66, 1), (67, 1), (68, 1), (70, 1), (123, 1), (125, 12), (126, 3), (130, 1), (134, 1), (140, 1), (141, 1), (144, 1), (147, 2), (158, 3), (163, 1), (164, 1), (167, 1), (169, 4), (170, 3), (181, 2), (182, 1), (191, 1), (193, 1), (194, 1), (206, 1), (212, 2), (213, 2), (215, 1), (217, 1), (218, 1)]
discards: [0]
Fitting a model of length 300 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 615.3029 - loglik: -6.1211e+02 - logprior: -3.1903e+00
Epoch 2/2
39/39 - 28s - loss: 597.0187 - loglik: -5.9579e+02 - logprior: -1.2283e+00
Fitted a model with MAP estimate = -553.0619
expansions: [(0, 2), (226, 1)]
discards: [  0  11  12  57  79 156 166 194 205 206 207 243 282 284]
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 601.6954 - loglik: -6.0005e+02 - logprior: -1.6442e+00
Epoch 2/2
39/39 - 26s - loss: 597.0766 - loglik: -5.9647e+02 - logprior: -6.1038e-01
Fitted a model with MAP estimate = -553.9289
expansions: [(200, 2), (201, 2)]
discards: [0]
Fitting a model of length 292 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 34s - loss: 555.2502 - loglik: -5.5348e+02 - logprior: -1.7687e+00
Epoch 2/10
45/45 - 30s - loss: 549.9164 - loglik: -5.4927e+02 - logprior: -6.4932e-01
Epoch 3/10
45/45 - 30s - loss: 547.7896 - loglik: -5.4715e+02 - logprior: -6.3952e-01
Epoch 4/10
45/45 - 31s - loss: 546.1827 - loglik: -5.4549e+02 - logprior: -6.9146e-01
Epoch 5/10
45/45 - 30s - loss: 545.3813 - loglik: -5.4463e+02 - logprior: -7.5254e-01
Epoch 6/10
45/45 - 30s - loss: 544.3574 - loglik: -5.4348e+02 - logprior: -8.7423e-01
Epoch 7/10
45/45 - 30s - loss: 543.1725 - loglik: -5.4221e+02 - logprior: -9.5828e-01
Epoch 8/10
45/45 - 30s - loss: 543.1949 - loglik: -5.4215e+02 - logprior: -1.0404e+00
Fitted a model with MAP estimate = -542.5284
Time for alignment: 676.0960
Fitting a model of length 224 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 811.9559 - loglik: -8.0850e+02 - logprior: -3.4576e+00
Epoch 2/10
19/19 - 18s - loss: 702.7933 - loglik: -7.0188e+02 - logprior: -9.1624e-01
Epoch 3/10
19/19 - 18s - loss: 647.6844 - loglik: -6.4629e+02 - logprior: -1.3965e+00
Epoch 4/10
19/19 - 18s - loss: 635.7418 - loglik: -6.3406e+02 - logprior: -1.6785e+00
Epoch 5/10
19/19 - 18s - loss: 630.1141 - loglik: -6.2829e+02 - logprior: -1.8284e+00
Epoch 6/10
19/19 - 18s - loss: 627.8018 - loglik: -6.2588e+02 - logprior: -1.9209e+00
Epoch 7/10
19/19 - 17s - loss: 625.9622 - loglik: -6.2391e+02 - logprior: -2.0485e+00
Epoch 8/10
19/19 - 18s - loss: 625.6870 - loglik: -6.2353e+02 - logprior: -2.1523e+00
Epoch 9/10
19/19 - 18s - loss: 625.0826 - loglik: -6.2277e+02 - logprior: -2.3124e+00
Epoch 10/10
19/19 - 18s - loss: 624.7828 - loglik: -6.2229e+02 - logprior: -2.4953e+00
Fitted a model with MAP estimate = -583.8341
expansions: [(12, 4), (14, 1), (16, 1), (17, 1), (36, 1), (37, 1), (39, 1), (45, 3), (46, 1), (59, 3), (60, 1), (62, 4), (65, 1), (66, 2), (67, 2), (70, 1), (123, 1), (125, 16), (129, 2), (140, 1), (141, 1), (143, 2), (159, 3), (165, 2), (166, 2), (167, 1), (169, 4), (170, 3), (181, 2), (182, 1), (191, 1), (193, 1), (194, 1), (206, 1), (209, 2), (210, 1), (215, 1), (217, 1), (218, 1)]
discards: [0]
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 615.6031 - loglik: -6.1246e+02 - logprior: -3.1406e+00
Epoch 2/2
39/39 - 28s - loss: 596.8675 - loglik: -5.9566e+02 - logprior: -1.2051e+00
Fitted a model with MAP estimate = -552.9446
expansions: [(0, 2)]
discards: [  0  11  12  76  88 158 159 173 191 207 208 209 210 246 282]
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 601.9449 - loglik: -6.0027e+02 - logprior: -1.6776e+00
Epoch 2/2
39/39 - 26s - loss: 597.6526 - loglik: -5.9701e+02 - logprior: -6.4727e-01
Fitted a model with MAP estimate = -554.9620
expansions: [(201, 4), (219, 1)]
discards: [ 0 57 74]
Fitting a model of length 291 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 33s - loss: 555.7940 - loglik: -5.5408e+02 - logprior: -1.7146e+00
Epoch 2/10
45/45 - 30s - loss: 550.5104 - loglik: -5.4979e+02 - logprior: -7.1771e-01
Epoch 3/10
45/45 - 30s - loss: 547.5413 - loglik: -5.4685e+02 - logprior: -6.8861e-01
Epoch 4/10
45/45 - 30s - loss: 547.2623 - loglik: -5.4650e+02 - logprior: -7.6121e-01
Epoch 5/10
45/45 - 31s - loss: 545.3432 - loglik: -5.4459e+02 - logprior: -7.5422e-01
Epoch 6/10
45/45 - 30s - loss: 543.4471 - loglik: -5.4255e+02 - logprior: -9.0161e-01
Epoch 7/10
45/45 - 30s - loss: 544.3442 - loglik: -5.4332e+02 - logprior: -1.0192e+00
Fitted a model with MAP estimate = -542.9890
Time for alignment: 644.0868
Computed alignments with likelihoods: ['-545.9480', '-542.5284', '-542.9890']
Best model has likelihood: -542.5284
SP score = 0.8981
Training of 3 independent models on file sdr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20e51180a0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1fff992280>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f214aaf0a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f21290bdfa0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 11s - loss: 445.9307 - loglik: -4.4471e+02 - logprior: -1.2162e+00
Epoch 2/10
30/30 - 8s - loss: 373.9724 - loglik: -3.7282e+02 - logprior: -1.1498e+00
Epoch 3/10
30/30 - 8s - loss: 363.4972 - loglik: -3.6236e+02 - logprior: -1.1355e+00
Epoch 4/10
30/30 - 8s - loss: 361.1622 - loglik: -3.6001e+02 - logprior: -1.1501e+00
Epoch 5/10
30/30 - 8s - loss: 360.9299 - loglik: -3.5979e+02 - logprior: -1.1417e+00
Epoch 6/10
30/30 - 8s - loss: 359.9446 - loglik: -3.5878e+02 - logprior: -1.1683e+00
Epoch 7/10
30/30 - 8s - loss: 359.6006 - loglik: -3.5841e+02 - logprior: -1.1921e+00
Epoch 8/10
30/30 - 8s - loss: 358.6652 - loglik: -3.5744e+02 - logprior: -1.2284e+00
Epoch 9/10
30/30 - 8s - loss: 358.9386 - loglik: -3.5765e+02 - logprior: -1.2903e+00
Fitted a model with MAP estimate = -350.2673
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (55, 1), (56, 1), (72, 2), (73, 2), (75, 1), (78, 2), (82, 1), (92, 1), (93, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (128, 2)]
discards: [0]
Fitting a model of length 178 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 16s - loss: 357.3068 - loglik: -3.5570e+02 - logprior: -1.6093e+00
Epoch 2/2
30/30 - 12s - loss: 347.2373 - loglik: -3.4628e+02 - logprior: -9.5969e-01
Fitted a model with MAP estimate = -335.0808
expansions: []
discards: [ 25  48  51  92  94 102 133 147 151 155]
Fitting a model of length 168 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 14s - loss: 347.9478 - loglik: -3.4667e+02 - logprior: -1.2732e+00
Epoch 2/2
30/30 - 11s - loss: 346.3588 - loglik: -3.4542e+02 - logprior: -9.3421e-01
Fitted a model with MAP estimate = -335.0238
expansions: []
discards: []
Fitting a model of length 168 on 50157 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 19s - loss: 333.9791 - loglik: -3.3314e+02 - logprior: -8.4396e-01
Epoch 2/10
43/43 - 15s - loss: 332.8109 - loglik: -3.3213e+02 - logprior: -6.7966e-01
Epoch 3/10
43/43 - 15s - loss: 332.1844 - loglik: -3.3151e+02 - logprior: -6.7692e-01
Epoch 4/10
43/43 - 15s - loss: 331.3064 - loglik: -3.3064e+02 - logprior: -6.6442e-01
Epoch 5/10
43/43 - 15s - loss: 330.8126 - loglik: -3.3014e+02 - logprior: -6.7582e-01
Epoch 6/10
43/43 - 15s - loss: 329.5545 - loglik: -3.2886e+02 - logprior: -6.9115e-01
Epoch 7/10
43/43 - 15s - loss: 329.2011 - loglik: -3.2850e+02 - logprior: -6.9811e-01
Epoch 8/10
43/43 - 15s - loss: 327.7946 - loglik: -3.2704e+02 - logprior: -7.5495e-01
Epoch 9/10
43/43 - 15s - loss: 328.2724 - loglik: -3.2752e+02 - logprior: -7.5514e-01
Fitted a model with MAP estimate = -327.6820
Time for alignment: 387.2698
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 11s - loss: 446.3922 - loglik: -4.4518e+02 - logprior: -1.2115e+00
Epoch 2/10
30/30 - 8s - loss: 375.3590 - loglik: -3.7422e+02 - logprior: -1.1412e+00
Epoch 3/10
30/30 - 8s - loss: 363.9389 - loglik: -3.6281e+02 - logprior: -1.1325e+00
Epoch 4/10
30/30 - 8s - loss: 361.3209 - loglik: -3.6017e+02 - logprior: -1.1534e+00
Epoch 5/10
30/30 - 8s - loss: 360.5157 - loglik: -3.5936e+02 - logprior: -1.1571e+00
Epoch 6/10
30/30 - 8s - loss: 359.2783 - loglik: -3.5809e+02 - logprior: -1.1835e+00
Epoch 7/10
30/30 - 8s - loss: 359.4889 - loglik: -3.5827e+02 - logprior: -1.2152e+00
Fitted a model with MAP estimate = -349.4518
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (54, 1), (56, 1), (72, 2), (73, 2), (75, 1), (78, 2), (89, 1), (92, 1), (93, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (126, 1), (127, 1), (128, 1)]
discards: [0]
Fitting a model of length 178 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 15s - loss: 357.7609 - loglik: -3.5618e+02 - logprior: -1.5789e+00
Epoch 2/2
30/30 - 11s - loss: 347.3199 - loglik: -3.4636e+02 - logprior: -9.5486e-01
Fitted a model with MAP estimate = -334.8742
expansions: []
discards: [ 25  51  92  94 102 133 147 151 155]
Fitting a model of length 169 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 15s - loss: 348.2173 - loglik: -3.4694e+02 - logprior: -1.2803e+00
Epoch 2/2
30/30 - 11s - loss: 345.8028 - loglik: -3.4489e+02 - logprior: -9.0858e-01
Fitted a model with MAP estimate = -334.8986
expansions: []
discards: [47]
Fitting a model of length 168 on 50157 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 18s - loss: 334.2569 - loglik: -3.3341e+02 - logprior: -8.4424e-01
Epoch 2/10
43/43 - 15s - loss: 333.0207 - loglik: -3.3235e+02 - logprior: -6.7484e-01
Epoch 3/10
43/43 - 15s - loss: 331.7627 - loglik: -3.3109e+02 - logprior: -6.7483e-01
Epoch 4/10
43/43 - 15s - loss: 331.5237 - loglik: -3.3085e+02 - logprior: -6.7542e-01
Epoch 5/10
43/43 - 15s - loss: 330.9465 - loglik: -3.3027e+02 - logprior: -6.7434e-01
Epoch 6/10
43/43 - 15s - loss: 329.3055 - loglik: -3.2861e+02 - logprior: -6.9363e-01
Epoch 7/10
43/43 - 15s - loss: 328.9485 - loglik: -3.2825e+02 - logprior: -7.0175e-01
Epoch 8/10
43/43 - 15s - loss: 328.5465 - loglik: -3.2779e+02 - logprior: -7.5508e-01
Epoch 9/10
43/43 - 15s - loss: 327.7872 - loglik: -3.2703e+02 - logprior: -7.6105e-01
Epoch 10/10
43/43 - 15s - loss: 327.5325 - loglik: -3.2672e+02 - logprior: -8.1695e-01
Fitted a model with MAP estimate = -327.1137
Time for alignment: 383.4495
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 11s - loss: 445.9345 - loglik: -4.4471e+02 - logprior: -1.2208e+00
Epoch 2/10
30/30 - 8s - loss: 376.1030 - loglik: -3.7496e+02 - logprior: -1.1382e+00
Epoch 3/10
30/30 - 8s - loss: 363.9756 - loglik: -3.6284e+02 - logprior: -1.1348e+00
Epoch 4/10
30/30 - 8s - loss: 360.7734 - loglik: -3.5960e+02 - logprior: -1.1704e+00
Epoch 5/10
30/30 - 8s - loss: 359.7240 - loglik: -3.5854e+02 - logprior: -1.1814e+00
Epoch 6/10
30/30 - 8s - loss: 358.7659 - loglik: -3.5755e+02 - logprior: -1.2179e+00
Epoch 7/10
30/30 - 8s - loss: 358.4263 - loglik: -3.5718e+02 - logprior: -1.2422e+00
Epoch 8/10
30/30 - 8s - loss: 358.1055 - loglik: -3.5682e+02 - logprior: -1.2826e+00
Epoch 9/10
30/30 - 8s - loss: 357.4237 - loglik: -3.5608e+02 - logprior: -1.3464e+00
Epoch 10/10
30/30 - 8s - loss: 357.3360 - loglik: -3.5595e+02 - logprior: -1.3894e+00
Fitted a model with MAP estimate = -349.7679
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (34, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 2), (48, 1), (51, 1), (52, 1), (54, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (88, 1), (93, 1), (94, 1), (97, 2), (99, 1), (111, 1), (113, 2), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Fitting a model of length 178 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 16s - loss: 358.4584 - loglik: -3.5681e+02 - logprior: -1.6463e+00
Epoch 2/2
30/30 - 12s - loss: 347.7513 - loglik: -3.4677e+02 - logprior: -9.7671e-01
Fitted a model with MAP estimate = -335.0165
expansions: []
discards: [ 25  48  51  57  93  95 127 148 151 155]
Fitting a model of length 168 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 14s - loss: 348.5804 - loglik: -3.4732e+02 - logprior: -1.2586e+00
Epoch 2/2
30/30 - 11s - loss: 345.3376 - loglik: -3.4441e+02 - logprior: -9.2771e-01
Fitted a model with MAP estimate = -334.7692
expansions: []
discards: []
Fitting a model of length 168 on 50157 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 18s - loss: 333.9998 - loglik: -3.3316e+02 - logprior: -8.4256e-01
Epoch 2/10
43/43 - 15s - loss: 332.8514 - loglik: -3.3218e+02 - logprior: -6.7079e-01
Epoch 3/10
43/43 - 15s - loss: 331.6671 - loglik: -3.3100e+02 - logprior: -6.6372e-01
Epoch 4/10
43/43 - 15s - loss: 331.9832 - loglik: -3.3132e+02 - logprior: -6.6040e-01
Fitted a model with MAP estimate = -330.4221
Time for alignment: 316.9163
Computed alignments with likelihoods: ['-327.6820', '-327.1137', '-330.4221']
Best model has likelihood: -327.1137
SP score = 0.6733
Training of 3 independent models on file p450.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f214aad76d0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f202ea5b5e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dd06567f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20f6142370>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 53s - loss: 1242.1713 - loglik: -1.2404e+03 - logprior: -1.7378e+00
Epoch 2/10
40/40 - 47s - loss: 1144.1118 - loglik: -1.1442e+03 - logprior: 0.1153
Epoch 3/10
40/40 - 47s - loss: 1135.7604 - loglik: -1.1359e+03 - logprior: 0.1491
Epoch 4/10
40/40 - 47s - loss: 1130.3965 - loglik: -1.1304e+03 - logprior: 0.0131
Epoch 5/10
40/40 - 47s - loss: 1122.3772 - loglik: -1.1221e+03 - logprior: -3.0510e-01
Epoch 6/10
40/40 - 47s - loss: 1117.6726 - loglik: -1.1168e+03 - logprior: -8.3527e-01
Epoch 7/10
40/40 - 47s - loss: 1112.5056 - loglik: -1.1111e+03 - logprior: -1.3569e+00
Epoch 8/10
40/40 - 47s - loss: 1108.4303 - loglik: -1.1066e+03 - logprior: -1.7885e+00
Epoch 9/10
40/40 - 47s - loss: 1104.0073 - loglik: -1.1018e+03 - logprior: -2.1752e+00
Epoch 10/10
40/40 - 47s - loss: 1100.5986 - loglik: -1.0981e+03 - logprior: -2.4896e+00
Fitted a model with MAP estimate = -849.9270
expansions: [(25, 1), (123, 1), (124, 1), (137, 1), (200, 1), (295, 2), (322, 2), (324, 67), (330, 50)]
discards: [  1 221 222]
Fitting a model of length 453 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 79s - loss: 1138.8381 - loglik: -1.1364e+03 - logprior: -2.3988e+00
Epoch 2/2
40/40 - 75s - loss: 1040.1686 - loglik: -1.0395e+03 - logprior: -6.8834e-01
Fitted a model with MAP estimate = -772.6329
expansions: [(440, 2)]
discards: [298 332 370 371 372 373 449]
Fitting a model of length 448 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 78s - loss: 1040.6490 - loglik: -1.0396e+03 - logprior: -1.0504e+00
Epoch 2/2
40/40 - 74s - loss: 1035.6136 - loglik: -1.0358e+03 - logprior: 0.1583
Fitted a model with MAP estimate = -773.1906
expansions: [(448, 2)]
discards: [430 431]
Fitting a model of length 448 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 105s - loss: 770.5031 - loglik: -7.7001e+02 - logprior: -4.9526e-01
Epoch 2/10
56/56 - 102s - loss: 764.1046 - loglik: -7.6369e+02 - logprior: -4.1089e-01
Epoch 3/10
56/56 - 102s - loss: 763.2096 - loglik: -7.6303e+02 - logprior: -1.8428e-01
Epoch 4/10
56/56 - 102s - loss: 757.2878 - loglik: -7.5689e+02 - logprior: -3.9334e-01
Epoch 5/10
56/56 - 102s - loss: 757.3978 - loglik: -7.5697e+02 - logprior: -4.2839e-01
Fitted a model with MAP estimate = -750.9618
Time for alignment: 1723.4784
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 51s - loss: 1245.8049 - loglik: -1.2441e+03 - logprior: -1.6984e+00
Epoch 2/10
40/40 - 47s - loss: 1146.4841 - loglik: -1.1466e+03 - logprior: 0.1435
Epoch 3/10
40/40 - 47s - loss: 1139.3959 - loglik: -1.1396e+03 - logprior: 0.1739
Epoch 4/10
40/40 - 47s - loss: 1134.0115 - loglik: -1.1341e+03 - logprior: 0.0877
Epoch 5/10
40/40 - 47s - loss: 1126.7499 - loglik: -1.1265e+03 - logprior: -2.1980e-01
Epoch 6/10
40/40 - 47s - loss: 1120.5120 - loglik: -1.1198e+03 - logprior: -7.4992e-01
Epoch 7/10
40/40 - 47s - loss: 1116.1415 - loglik: -1.1148e+03 - logprior: -1.2950e+00
Epoch 8/10
40/40 - 47s - loss: 1110.5037 - loglik: -1.1088e+03 - logprior: -1.7193e+00
Epoch 9/10
40/40 - 47s - loss: 1106.7413 - loglik: -1.1047e+03 - logprior: -2.0867e+00
Epoch 10/10
40/40 - 47s - loss: 1104.0509 - loglik: -1.1016e+03 - logprior: -2.4309e+00
Fitted a model with MAP estimate = -851.4447
expansions: [(18, 2), (42, 2), (117, 2), (125, 1), (127, 1), (328, 11), (329, 12), (330, 98)]
discards: [  1 191 221]
Fitting a model of length 456 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 79s - loss: 1158.0353 - loglik: -1.1550e+03 - logprior: -3.0174e+00
Epoch 2/2
40/40 - 76s - loss: 1048.7025 - loglik: -1.0473e+03 - logprior: -1.3605e+00
Fitted a model with MAP estimate = -778.9462
expansions: [(168, 1), (332, 1), (336, 1), (337, 2), (338, 3), (348, 1), (350, 2), (351, 1), (375, 5), (376, 2), (378, 1), (396, 1), (398, 1), (415, 2)]
discards: [ 17  18  44 121 432 433 434 435 436 437 438 439 440 441 442 443 444 445
 446 447 448 449 450 451 452 453 454 455]
Fitting a model of length 452 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 78s - loss: 1044.4272 - loglik: -1.0433e+03 - logprior: -1.1406e+00
Epoch 2/2
40/40 - 74s - loss: 1036.2161 - loglik: -1.0363e+03 - logprior: 0.0814
Fitted a model with MAP estimate = -774.0218
expansions: [(441, 1), (452, 2)]
discards: [334 339 355 386]
Fitting a model of length 451 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 107s - loss: 767.1738 - loglik: -7.6663e+02 - logprior: -5.3908e-01
Epoch 2/10
56/56 - 103s - loss: 769.4813 - loglik: -7.6907e+02 - logprior: -4.0761e-01
Fitted a model with MAP estimate = -761.8635
Time for alignment: 1425.2924
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 50s - loss: 1245.3751 - loglik: -1.2436e+03 - logprior: -1.7499e+00
Epoch 2/10
40/40 - 47s - loss: 1145.0287 - loglik: -1.1451e+03 - logprior: 0.0323
Epoch 3/10
40/40 - 47s - loss: 1136.6064 - loglik: -1.1366e+03 - logprior: 0.0227
Epoch 4/10
40/40 - 47s - loss: 1130.8219 - loglik: -1.1307e+03 - logprior: -1.0060e-01
Epoch 5/10
40/40 - 47s - loss: 1125.1082 - loglik: -1.1247e+03 - logprior: -4.1510e-01
Epoch 6/10
40/40 - 47s - loss: 1119.2964 - loglik: -1.1183e+03 - logprior: -9.8012e-01
Epoch 7/10
40/40 - 47s - loss: 1114.1503 - loglik: -1.1126e+03 - logprior: -1.5160e+00
Epoch 8/10
40/40 - 47s - loss: 1108.9764 - loglik: -1.1070e+03 - logprior: -1.9685e+00
Epoch 9/10
40/40 - 47s - loss: 1106.1580 - loglik: -1.1038e+03 - logprior: -2.3656e+00
Epoch 10/10
40/40 - 47s - loss: 1102.3123 - loglik: -1.0996e+03 - logprior: -2.6949e+00
Fitted a model with MAP estimate = -849.5642
expansions: [(18, 2), (122, 1), (125, 1), (138, 1), (292, 2), (293, 2), (294, 7), (295, 2), (296, 3), (297, 1), (299, 1), (302, 1), (312, 2), (329, 13), (330, 87)]
discards: [  1 194 239]
Fitting a model of length 453 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 79s - loss: 1149.2948 - loglik: -1.1470e+03 - logprior: -2.2754e+00
Epoch 2/2
40/40 - 75s - loss: 1042.3918 - loglik: -1.0415e+03 - logprior: -9.1619e-01
Fitted a model with MAP estimate = -773.6945
expansions: [(228, 1), (357, 1), (359, 1), (433, 1), (447, 1)]
discards: [ 17  18 197 297 298 307 309 390 391 451 452]
Fitting a model of length 447 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 77s - loss: 1041.3370 - loglik: -1.0402e+03 - logprior: -1.1766e+00
Epoch 2/2
40/40 - 73s - loss: 1035.6494 - loglik: -1.0357e+03 - logprior: 0.0685
Fitted a model with MAP estimate = -773.6900
expansions: [(447, 2)]
discards: [328]
Fitting a model of length 448 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 107s - loss: 768.9208 - loglik: -7.6833e+02 - logprior: -5.9492e-01
Epoch 2/10
56/56 - 102s - loss: 766.3066 - loglik: -7.6606e+02 - logprior: -2.4265e-01
Epoch 3/10
56/56 - 102s - loss: 761.2095 - loglik: -7.6088e+02 - logprior: -3.2801e-01
Epoch 4/10
56/56 - 102s - loss: 759.0508 - loglik: -7.5879e+02 - logprior: -2.5887e-01
Epoch 5/10
56/56 - 102s - loss: 755.2509 - loglik: -7.5484e+02 - logprior: -4.0980e-01
Epoch 6/10
56/56 - 102s - loss: 749.7798 - loglik: -7.4908e+02 - logprior: -6.9513e-01
Epoch 7/10
56/56 - 102s - loss: 747.0142 - loglik: -7.4614e+02 - logprior: -8.7789e-01
Epoch 8/10
56/56 - 102s - loss: 743.5106 - loglik: -7.4219e+02 - logprior: -1.3187e+00
Epoch 9/10
56/56 - 102s - loss: 740.0787 - loglik: -7.3892e+02 - logprior: -1.1631e+00
Epoch 10/10
56/56 - 102s - loss: 737.0054 - loglik: -7.3557e+02 - logprior: -1.4311e+00
Fitted a model with MAP estimate = -735.1757
Time for alignment: 2228.0007
Computed alignments with likelihoods: ['-750.9618', '-761.8635', '-735.1757']
Best model has likelihood: -735.1757
SP score = 0.7464
Training of 3 independent models on file hla.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1d76712d30>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f201a029a00>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1d757eef70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1d75e3fa00>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 449.8288 - loglik: -4.4627e+02 - logprior: -3.5616e+00
Epoch 2/10
19/19 - 5s - loss: 283.6129 - loglik: -2.8198e+02 - logprior: -1.6347e+00
Epoch 3/10
19/19 - 5s - loss: 212.4567 - loglik: -2.1053e+02 - logprior: -1.9223e+00
Epoch 4/10
19/19 - 5s - loss: 203.7612 - loglik: -2.0169e+02 - logprior: -2.0666e+00
Epoch 5/10
19/19 - 5s - loss: 200.0732 - loglik: -1.9802e+02 - logprior: -2.0568e+00
Epoch 6/10
19/19 - 5s - loss: 197.1174 - loglik: -1.9493e+02 - logprior: -2.1904e+00
Epoch 7/10
19/19 - 5s - loss: 196.1666 - loglik: -1.9385e+02 - logprior: -2.3182e+00
Epoch 8/10
19/19 - 5s - loss: 193.8754 - loglik: -1.9130e+02 - logprior: -2.5735e+00
Epoch 9/10
19/19 - 5s - loss: 192.7314 - loglik: -1.9012e+02 - logprior: -2.6102e+00
Epoch 10/10
19/19 - 5s - loss: 194.2944 - loglik: -1.9160e+02 - logprior: -2.6978e+00
Fitted a model with MAP estimate = -187.7630
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (33, 1), (34, 1), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 2), (102, 1), (104, 1), (114, 1), (120, 2), (121, 1), (122, 2), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Fitting a model of length 182 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 181.5821 - loglik: -1.7835e+02 - logprior: -3.2324e+00
Epoch 2/2
19/19 - 6s - loss: 144.8377 - loglik: -1.4362e+02 - logprior: -1.2177e+00
Fitted a model with MAP estimate = -143.0761
expansions: []
discards: [ 75 125 149 154]
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 146.7890 - loglik: -1.4379e+02 - logprior: -2.9968e+00
Epoch 2/2
19/19 - 6s - loss: 140.8335 - loglik: -1.3980e+02 - logprior: -1.0368e+00
Fitted a model with MAP estimate = -141.9475
expansions: []
discards: []
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 11s - loss: 144.4793 - loglik: -1.4230e+02 - logprior: -2.1758e+00
Epoch 2/10
22/22 - 7s - loss: 140.8209 - loglik: -1.3981e+02 - logprior: -1.0080e+00
Epoch 3/10
22/22 - 7s - loss: 138.3324 - loglik: -1.3747e+02 - logprior: -8.6732e-01
Epoch 4/10
22/22 - 7s - loss: 135.2756 - loglik: -1.3421e+02 - logprior: -1.0656e+00
Epoch 5/10
22/22 - 7s - loss: 134.4694 - loglik: -1.3338e+02 - logprior: -1.0905e+00
Epoch 6/10
22/22 - 7s - loss: 132.9035 - loglik: -1.3176e+02 - logprior: -1.1450e+00
Epoch 7/10
22/22 - 7s - loss: 129.9691 - loglik: -1.2879e+02 - logprior: -1.1806e+00
Epoch 8/10
22/22 - 7s - loss: 131.3134 - loglik: -1.3007e+02 - logprior: -1.2395e+00
Fitted a model with MAP estimate = -129.2226
Time for alignment: 188.1980
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 449.5800 - loglik: -4.4601e+02 - logprior: -3.5657e+00
Epoch 2/10
19/19 - 5s - loss: 279.6682 - loglik: -2.7805e+02 - logprior: -1.6171e+00
Epoch 3/10
19/19 - 5s - loss: 210.2195 - loglik: -2.0826e+02 - logprior: -1.9556e+00
Epoch 4/10
19/19 - 5s - loss: 201.9117 - loglik: -1.9986e+02 - logprior: -2.0555e+00
Epoch 5/10
19/19 - 5s - loss: 198.6526 - loglik: -1.9663e+02 - logprior: -2.0258e+00
Epoch 6/10
19/19 - 5s - loss: 197.5109 - loglik: -1.9530e+02 - logprior: -2.2142e+00
Epoch 7/10
19/19 - 5s - loss: 193.1683 - loglik: -1.9078e+02 - logprior: -2.3877e+00
Epoch 8/10
19/19 - 5s - loss: 194.2169 - loglik: -1.9167e+02 - logprior: -2.5460e+00
Fitted a model with MAP estimate = -187.1439
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 2), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (100, 1), (102, 1), (104, 1), (114, 1), (121, 2), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Fitting a model of length 184 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 181.7263 - loglik: -1.7810e+02 - logprior: -3.6258e+00
Epoch 2/2
19/19 - 6s - loss: 144.8528 - loglik: -1.4310e+02 - logprior: -1.7507e+00
Fitted a model with MAP estimate = -142.9413
expansions: []
discards: [ 50  70  77 152 154 159]
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 146.8792 - loglik: -1.4348e+02 - logprior: -3.3963e+00
Epoch 2/2
19/19 - 6s - loss: 142.2195 - loglik: -1.4072e+02 - logprior: -1.4946e+00
Fitted a model with MAP estimate = -142.4521
expansions: []
discards: []
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 10s - loss: 145.1887 - loglik: -1.4272e+02 - logprior: -2.4719e+00
Epoch 2/10
22/22 - 7s - loss: 140.7778 - loglik: -1.3948e+02 - logprior: -1.3027e+00
Epoch 3/10
22/22 - 7s - loss: 137.1570 - loglik: -1.3598e+02 - logprior: -1.1779e+00
Epoch 4/10
22/22 - 7s - loss: 135.8779 - loglik: -1.3475e+02 - logprior: -1.1326e+00
Epoch 5/10
22/22 - 7s - loss: 134.7708 - loglik: -1.3366e+02 - logprior: -1.1089e+00
Epoch 6/10
22/22 - 7s - loss: 133.0906 - loglik: -1.3195e+02 - logprior: -1.1417e+00
Epoch 7/10
22/22 - 7s - loss: 130.6602 - loglik: -1.2946e+02 - logprior: -1.1960e+00
Epoch 8/10
22/22 - 7s - loss: 129.5219 - loglik: -1.2828e+02 - logprior: -1.2380e+00
Epoch 9/10
22/22 - 7s - loss: 128.9961 - loglik: -1.2769e+02 - logprior: -1.3082e+00
Epoch 10/10
22/22 - 7s - loss: 128.2034 - loglik: -1.2689e+02 - logprior: -1.3138e+00
Fitted a model with MAP estimate = -128.1972
Time for alignment: 192.9735
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 449.8727 - loglik: -4.4633e+02 - logprior: -3.5441e+00
Epoch 2/10
19/19 - 5s - loss: 285.1234 - loglik: -2.8350e+02 - logprior: -1.6190e+00
Epoch 3/10
19/19 - 5s - loss: 214.8939 - loglik: -2.1300e+02 - logprior: -1.8961e+00
Epoch 4/10
19/19 - 5s - loss: 204.7447 - loglik: -2.0273e+02 - logprior: -2.0186e+00
Epoch 5/10
19/19 - 5s - loss: 200.8059 - loglik: -1.9879e+02 - logprior: -2.0162e+00
Epoch 6/10
19/19 - 5s - loss: 199.3940 - loglik: -1.9728e+02 - logprior: -2.1116e+00
Epoch 7/10
19/19 - 5s - loss: 196.9786 - loglik: -1.9469e+02 - logprior: -2.2890e+00
Epoch 8/10
19/19 - 5s - loss: 196.4473 - loglik: -1.9395e+02 - logprior: -2.4993e+00
Epoch 9/10
19/19 - 5s - loss: 194.6331 - loglik: -1.9208e+02 - logprior: -2.5497e+00
Epoch 10/10
19/19 - 5s - loss: 194.3080 - loglik: -1.9166e+02 - logprior: -2.6433e+00
Fitted a model with MAP estimate = -189.2302
expansions: [(7, 1), (8, 2), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (33, 1), (34, 1), (53, 1), (54, 1), (55, 1), (56, 2), (58, 2), (59, 3), (62, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 2), (102, 2), (104, 1), (114, 1), (121, 1), (122, 3), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Fitting a model of length 184 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 182.0468 - loglik: -1.7878e+02 - logprior: -3.2659e+00
Epoch 2/2
19/19 - 6s - loss: 145.7851 - loglik: -1.4455e+02 - logprior: -1.2389e+00
Fitted a model with MAP estimate = -143.4762
expansions: []
discards: [ 69  73  77 127 130 156]
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 147.2760 - loglik: -1.4432e+02 - logprior: -2.9587e+00
Epoch 2/2
19/19 - 6s - loss: 141.9141 - loglik: -1.4092e+02 - logprior: -9.9366e-01
Fitted a model with MAP estimate = -142.3417
expansions: []
discards: []
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 12s - loss: 144.4362 - loglik: -1.4235e+02 - logprior: -2.0903e+00
Epoch 2/10
22/22 - 7s - loss: 141.6047 - loglik: -1.4071e+02 - logprior: -8.9335e-01
Epoch 3/10
22/22 - 7s - loss: 137.2029 - loglik: -1.3641e+02 - logprior: -7.9255e-01
Epoch 4/10
22/22 - 7s - loss: 136.1144 - loglik: -1.3528e+02 - logprior: -8.3430e-01
Epoch 5/10
22/22 - 7s - loss: 135.0632 - loglik: -1.3397e+02 - logprior: -1.0935e+00
Epoch 6/10
22/22 - 7s - loss: 132.6413 - loglik: -1.3149e+02 - logprior: -1.1499e+00
Epoch 7/10
22/22 - 7s - loss: 130.7912 - loglik: -1.2961e+02 - logprior: -1.1829e+00
Epoch 8/10
22/22 - 7s - loss: 129.7784 - loglik: -1.2854e+02 - logprior: -1.2390e+00
Epoch 9/10
22/22 - 7s - loss: 129.2939 - loglik: -1.2801e+02 - logprior: -1.2802e+00
Epoch 10/10
22/22 - 7s - loss: 127.8071 - loglik: -1.2649e+02 - logprior: -1.3196e+00
Fitted a model with MAP estimate = -128.1795
Time for alignment: 204.2600
Computed alignments with likelihoods: ['-129.2226', '-128.1972', '-128.1795']
Best model has likelihood: -128.1795
SP score = 1.0000
Training of 3 independent models on file ltn.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1d75b73970>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20988d0460>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1d75f8de50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f214a906700>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 184 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 627.8781 - loglik: -5.7970e+02 - logprior: -4.8181e+01
Epoch 2/10
10/10 - 5s - loss: 541.3391 - loglik: -5.3313e+02 - logprior: -8.2073e+00
Epoch 3/10
10/10 - 5s - loss: 482.2126 - loglik: -4.8025e+02 - logprior: -1.9604e+00
Epoch 4/10
10/10 - 5s - loss: 444.2718 - loglik: -4.4347e+02 - logprior: -8.0352e-01
Epoch 5/10
10/10 - 5s - loss: 431.4821 - loglik: -4.3143e+02 - logprior: -5.5254e-02
Epoch 6/10
10/10 - 5s - loss: 426.5795 - loglik: -4.2705e+02 - logprior: 0.4682
Epoch 7/10
10/10 - 5s - loss: 424.7485 - loglik: -4.2556e+02 - logprior: 0.8103
Epoch 8/10
10/10 - 5s - loss: 422.5770 - loglik: -4.2357e+02 - logprior: 0.9896
Epoch 9/10
10/10 - 5s - loss: 421.5576 - loglik: -4.2267e+02 - logprior: 1.1130
Epoch 10/10
10/10 - 5s - loss: 421.5476 - loglik: -4.2277e+02 - logprior: 1.2272
Fitted a model with MAP estimate = -420.8653
expansions: [(11, 3), (21, 1), (31, 4), (32, 1), (41, 1), (45, 1), (60, 1), (61, 1), (62, 1), (63, 1), (75, 1), (76, 2), (77, 2), (78, 2), (86, 1), (90, 1), (92, 1), (101, 1), (102, 1), (127, 2), (129, 2), (131, 4), (138, 1), (140, 1), (146, 3), (149, 2), (153, 2), (173, 2), (174, 1), (175, 1)]
discards: [0]
Fitting a model of length 231 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 462.9501 - loglik: -4.1894e+02 - logprior: -4.4010e+01
Epoch 2/2
10/10 - 6s - loss: 421.7812 - loglik: -4.0755e+02 - logprior: -1.4231e+01
Fitted a model with MAP estimate = -414.1879
expansions: [(0, 4), (109, 1)]
discards: [  0  36  92  94 161 188 217]
Fitting a model of length 229 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 437.3738 - loglik: -4.0562e+02 - logprior: -3.1757e+01
Epoch 2/2
10/10 - 6s - loss: 406.0141 - loglik: -4.0233e+02 - logprior: -3.6834e+00
Fitted a model with MAP estimate = -400.7467
expansions: []
discards: [  1   2   3 159 160 161]
Fitting a model of length 223 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 432.5653 - loglik: -4.0226e+02 - logprior: -3.0305e+01
Epoch 2/10
10/10 - 6s - loss: 404.4615 - loglik: -4.0146e+02 - logprior: -2.9989e+00
Epoch 3/10
10/10 - 6s - loss: 397.7464 - loglik: -4.0040e+02 - logprior: 2.6546
Epoch 4/10
10/10 - 6s - loss: 395.0510 - loglik: -4.0013e+02 - logprior: 5.0776
Epoch 5/10
10/10 - 6s - loss: 392.8357 - loglik: -3.9928e+02 - logprior: 6.4418
Epoch 6/10
10/10 - 6s - loss: 391.9725 - loglik: -3.9924e+02 - logprior: 7.2685
Epoch 7/10
10/10 - 6s - loss: 392.9950 - loglik: -4.0079e+02 - logprior: 7.7974
Fitted a model with MAP estimate = -391.7647
Time for alignment: 142.7942
Fitting a model of length 184 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 627.7410 - loglik: -5.7956e+02 - logprior: -4.8181e+01
Epoch 2/10
10/10 - 5s - loss: 541.9957 - loglik: -5.3379e+02 - logprior: -8.2019e+00
Epoch 3/10
10/10 - 5s - loss: 486.7365 - loglik: -4.8478e+02 - logprior: -1.9604e+00
Epoch 4/10
10/10 - 5s - loss: 446.3853 - loglik: -4.4553e+02 - logprior: -8.5657e-01
Epoch 5/10
10/10 - 5s - loss: 431.6851 - loglik: -4.3147e+02 - logprior: -2.2000e-01
Epoch 6/10
10/10 - 5s - loss: 426.8374 - loglik: -4.2718e+02 - logprior: 0.3387
Epoch 7/10
10/10 - 5s - loss: 424.3310 - loglik: -4.2494e+02 - logprior: 0.6114
Epoch 8/10
10/10 - 5s - loss: 423.2407 - loglik: -4.2399e+02 - logprior: 0.7457
Epoch 9/10
10/10 - 5s - loss: 421.5163 - loglik: -4.2232e+02 - logprior: 0.8076
Epoch 10/10
10/10 - 5s - loss: 420.9293 - loglik: -4.2182e+02 - logprior: 0.8944
Fitted a model with MAP estimate = -420.6004
expansions: [(11, 3), (19, 1), (31, 1), (32, 2), (33, 1), (39, 1), (42, 1), (48, 1), (62, 1), (63, 1), (64, 1), (75, 1), (76, 2), (77, 2), (78, 1), (86, 1), (87, 1), (90, 1), (92, 1), (101, 1), (102, 1), (120, 2), (127, 1), (129, 4), (137, 1), (146, 3), (151, 1), (154, 1), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Fitting a model of length 227 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 462.1650 - loglik: -4.1828e+02 - logprior: -4.3881e+01
Epoch 2/2
10/10 - 6s - loss: 420.6607 - loglik: -4.0644e+02 - logprior: -1.4217e+01
Fitted a model with MAP estimate = -412.8606
expansions: [(0, 4), (153, 5)]
discards: [  0  35 158 213]
Fitting a model of length 232 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 435.9153 - loglik: -4.0401e+02 - logprior: -3.1900e+01
Epoch 2/2
10/10 - 6s - loss: 402.8703 - loglik: -3.9925e+02 - logprior: -3.6165e+00
Fitted a model with MAP estimate = -397.9524
expansions: []
discards: [  1   2   3 149 155 156 157]
Fitting a model of length 225 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 432.2218 - loglik: -4.0199e+02 - logprior: -3.0235e+01
Epoch 2/10
10/10 - 6s - loss: 402.8011 - loglik: -3.9978e+02 - logprior: -3.0206e+00
Epoch 3/10
10/10 - 6s - loss: 395.6328 - loglik: -3.9830e+02 - logprior: 2.6679
Epoch 4/10
10/10 - 6s - loss: 394.7635 - loglik: -3.9984e+02 - logprior: 5.0809
Epoch 5/10
10/10 - 6s - loss: 391.8224 - loglik: -3.9829e+02 - logprior: 6.4680
Epoch 6/10
10/10 - 6s - loss: 391.7401 - loglik: -3.9903e+02 - logprior: 7.2899
Epoch 7/10
10/10 - 6s - loss: 391.3019 - loglik: -3.9912e+02 - logprior: 7.8176
Epoch 8/10
10/10 - 6s - loss: 390.4139 - loglik: -3.9865e+02 - logprior: 8.2374
Epoch 9/10
10/10 - 6s - loss: 390.7749 - loglik: -3.9935e+02 - logprior: 8.5769
Fitted a model with MAP estimate = -390.1184
Time for alignment: 155.0729
Fitting a model of length 184 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 627.2133 - loglik: -5.7903e+02 - logprior: -4.8179e+01
Epoch 2/10
10/10 - 5s - loss: 541.8617 - loglik: -5.3366e+02 - logprior: -8.2000e+00
Epoch 3/10
10/10 - 5s - loss: 483.1839 - loglik: -4.8118e+02 - logprior: -2.0063e+00
Epoch 4/10
10/10 - 5s - loss: 444.5256 - loglik: -4.4362e+02 - logprior: -9.0420e-01
Epoch 5/10
10/10 - 5s - loss: 429.0860 - loglik: -4.2879e+02 - logprior: -2.9396e-01
Epoch 6/10
10/10 - 5s - loss: 424.3293 - loglik: -4.2454e+02 - logprior: 0.2092
Epoch 7/10
10/10 - 5s - loss: 423.4927 - loglik: -4.2402e+02 - logprior: 0.5312
Epoch 8/10
10/10 - 5s - loss: 422.1931 - loglik: -4.2289e+02 - logprior: 0.7009
Epoch 9/10
10/10 - 5s - loss: 421.7397 - loglik: -4.2258e+02 - logprior: 0.8353
Epoch 10/10
10/10 - 5s - loss: 420.3318 - loglik: -4.2122e+02 - logprior: 0.8872
Fitted a model with MAP estimate = -419.9546
expansions: [(11, 3), (19, 1), (30, 2), (31, 3), (32, 1), (41, 1), (48, 1), (60, 1), (61, 1), (62, 1), (63, 1), (75, 1), (76, 2), (77, 2), (78, 1), (86, 1), (87, 1), (90, 1), (92, 1), (101, 1), (102, 1), (121, 2), (123, 1), (126, 1), (128, 5), (145, 1), (146, 3), (148, 2), (153, 1), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Fitting a model of length 232 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 461.6469 - loglik: -4.1761e+02 - logprior: -4.4033e+01
Epoch 2/2
10/10 - 6s - loss: 418.8957 - loglik: -4.0464e+02 - logprior: -1.4252e+01
Fitted a model with MAP estimate = -411.6768
expansions: [(0, 4), (155, 5), (185, 1)]
discards: [  0  35  36  93 148 160 162 186 187 188 218]
Fitting a model of length 231 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 437.0664 - loglik: -4.0513e+02 - logprior: -3.1938e+01
Epoch 2/2
10/10 - 6s - loss: 404.6013 - loglik: -4.0097e+02 - logprior: -3.6327e+00
Fitted a model with MAP estimate = -398.4682
expansions: [(187, 2)]
discards: [  1   2   3 154 155 156]
Fitting a model of length 227 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 431.4039 - loglik: -4.0109e+02 - logprior: -3.0313e+01
Epoch 2/10
10/10 - 6s - loss: 401.9274 - loglik: -3.9887e+02 - logprior: -3.0603e+00
Epoch 3/10
10/10 - 6s - loss: 396.3219 - loglik: -3.9896e+02 - logprior: 2.6424
Epoch 4/10
10/10 - 6s - loss: 392.2881 - loglik: -3.9736e+02 - logprior: 5.0707
Epoch 5/10
10/10 - 6s - loss: 391.8642 - loglik: -3.9832e+02 - logprior: 6.4538
Epoch 6/10
10/10 - 6s - loss: 389.9457 - loglik: -3.9723e+02 - logprior: 7.2858
Epoch 7/10
10/10 - 6s - loss: 391.3770 - loglik: -3.9919e+02 - logprior: 7.8102
Fitted a model with MAP estimate = -389.8830
Time for alignment: 143.3555
Computed alignments with likelihoods: ['-391.7647', '-390.1184', '-389.8830']
Best model has likelihood: -389.8830
SP score = 0.9364
Training of 3 independent models on file aadh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2178e9cf40>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f202cbf0220>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2153beb970>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2153bebb50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 187 on 3127 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 647.9117 - loglik: -6.3105e+02 - logprior: -1.6864e+01
Epoch 2/10
10/10 - 8s - loss: 573.6879 - loglik: -5.7042e+02 - logprior: -3.2686e+00
Epoch 3/10
10/10 - 7s - loss: 509.2134 - loglik: -5.0740e+02 - logprior: -1.8129e+00
Epoch 4/10
10/10 - 7s - loss: 463.7205 - loglik: -4.6145e+02 - logprior: -2.2725e+00
Epoch 5/10
10/10 - 7s - loss: 443.3574 - loglik: -4.4063e+02 - logprior: -2.7304e+00
Epoch 6/10
10/10 - 8s - loss: 435.1059 - loglik: -4.3218e+02 - logprior: -2.9226e+00
Epoch 7/10
10/10 - 7s - loss: 430.6378 - loglik: -4.2781e+02 - logprior: -2.8261e+00
Epoch 8/10
10/10 - 8s - loss: 428.6261 - loglik: -4.2592e+02 - logprior: -2.7041e+00
Epoch 9/10
10/10 - 7s - loss: 424.8398 - loglik: -4.2204e+02 - logprior: -2.7960e+00
Epoch 10/10
10/10 - 8s - loss: 426.0919 - loglik: -4.2322e+02 - logprior: -2.8672e+00
Fitted a model with MAP estimate = -424.7336
expansions: [(7, 1), (17, 1), (18, 1), (19, 1), (20, 3), (21, 3), (22, 1), (35, 1), (37, 1), (38, 1), (39, 1), (40, 1), (46, 1), (62, 1), (63, 3), (76, 7), (78, 1), (96, 1), (97, 1), (99, 1), (118, 2), (119, 1), (120, 1), (121, 1), (130, 1), (142, 1), (145, 1), (146, 1), (149, 1), (160, 1), (163, 2), (164, 1), (165, 1), (169, 2), (170, 1), (171, 1), (178, 1), (181, 1)]
discards: [0]
Fitting a model of length 239 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 14s - loss: 423.2276 - loglik: -4.1263e+02 - logprior: -1.0596e+01
Epoch 2/2
21/21 - 10s - loss: 395.8154 - loglik: -3.9280e+02 - logprior: -3.0163e+00
Fitted a model with MAP estimate = -391.6951
expansions: []
discards: [ 24  28  81 150 215]
Fitting a model of length 234 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 15s - loss: 400.7595 - loglik: -3.9397e+02 - logprior: -6.7884e+00
Epoch 2/2
21/21 - 10s - loss: 389.4735 - loglik: -3.8972e+02 - logprior: 0.2417
Fitted a model with MAP estimate = -388.0061
expansions: []
discards: []
Fitting a model of length 234 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 13s - loss: 397.0897 - loglik: -3.9102e+02 - logprior: -6.0711e+00
Epoch 2/10
21/21 - 10s - loss: 389.8401 - loglik: -3.9041e+02 - logprior: 0.5724
Epoch 3/10
21/21 - 10s - loss: 387.2262 - loglik: -3.8856e+02 - logprior: 1.3357
Epoch 4/10
21/21 - 10s - loss: 385.9681 - loglik: -3.8768e+02 - logprior: 1.7106
Epoch 5/10
21/21 - 10s - loss: 385.5627 - loglik: -3.8742e+02 - logprior: 1.8606
Epoch 6/10
21/21 - 10s - loss: 382.3611 - loglik: -3.8432e+02 - logprior: 1.9603
Epoch 7/10
21/21 - 10s - loss: 384.9962 - loglik: -3.8703e+02 - logprior: 2.0354
Fitted a model with MAP estimate = -383.1763
Time for alignment: 231.4719
Fitting a model of length 187 on 3127 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 648.8869 - loglik: -6.3201e+02 - logprior: -1.6873e+01
Epoch 2/10
10/10 - 7s - loss: 572.4440 - loglik: -5.6916e+02 - logprior: -3.2799e+00
Epoch 3/10
10/10 - 8s - loss: 506.3000 - loglik: -5.0449e+02 - logprior: -1.8076e+00
Epoch 4/10
10/10 - 7s - loss: 461.3346 - loglik: -4.5910e+02 - logprior: -2.2392e+00
Epoch 5/10
10/10 - 7s - loss: 442.7278 - loglik: -4.4009e+02 - logprior: -2.6373e+00
Epoch 6/10
10/10 - 7s - loss: 436.1754 - loglik: -4.3348e+02 - logprior: -2.6955e+00
Epoch 7/10
10/10 - 8s - loss: 433.6181 - loglik: -4.3110e+02 - logprior: -2.5221e+00
Epoch 8/10
10/10 - 7s - loss: 431.8123 - loglik: -4.2941e+02 - logprior: -2.4053e+00
Epoch 9/10
10/10 - 7s - loss: 429.8901 - loglik: -4.2744e+02 - logprior: -2.4509e+00
Epoch 10/10
10/10 - 7s - loss: 429.4072 - loglik: -4.2694e+02 - logprior: -2.4642e+00
Fitted a model with MAP estimate = -429.2769
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (46, 1), (62, 1), (63, 3), (76, 7), (78, 1), (96, 1), (97, 1), (99, 1), (119, 2), (121, 1), (122, 1), (144, 1), (146, 2), (147, 2), (154, 1), (160, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (178, 1), (179, 1)]
discards: [0]
Fitting a model of length 240 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 15s - loss: 427.3878 - loglik: -4.1680e+02 - logprior: -1.0584e+01
Epoch 2/2
21/21 - 10s - loss: 400.4181 - loglik: -3.9732e+02 - logprior: -3.0966e+00
Fitted a model with MAP estimate = -395.7779
expansions: []
discards: [ 13  18  28  50  81 150 216]
Fitting a model of length 233 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 406.8832 - loglik: -4.0002e+02 - logprior: -6.8628e+00
Epoch 2/2
21/21 - 10s - loss: 395.1801 - loglik: -3.9541e+02 - logprior: 0.2259
Fitted a model with MAP estimate = -393.8204
expansions: []
discards: []
Fitting a model of length 233 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 13s - loss: 403.6051 - loglik: -3.9749e+02 - logprior: -6.1155e+00
Epoch 2/10
21/21 - 10s - loss: 395.2919 - loglik: -3.9583e+02 - logprior: 0.5399
Epoch 3/10
21/21 - 10s - loss: 393.1243 - loglik: -3.9442e+02 - logprior: 1.2918
Epoch 4/10
21/21 - 10s - loss: 391.2068 - loglik: -3.9287e+02 - logprior: 1.6646
Epoch 5/10
21/21 - 10s - loss: 390.0341 - loglik: -3.9184e+02 - logprior: 1.8104
Epoch 6/10
21/21 - 11s - loss: 390.1747 - loglik: -3.9208e+02 - logprior: 1.9042
Fitted a model with MAP estimate = -389.4343
Time for alignment: 219.1247
Fitting a model of length 187 on 3127 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 12s - loss: 647.5352 - loglik: -6.3069e+02 - logprior: -1.6844e+01
Epoch 2/10
10/10 - 7s - loss: 572.5140 - loglik: -5.6926e+02 - logprior: -3.2509e+00
Epoch 3/10
10/10 - 7s - loss: 513.0311 - loglik: -5.1126e+02 - logprior: -1.7696e+00
Epoch 4/10
10/10 - 7s - loss: 473.7052 - loglik: -4.7155e+02 - logprior: -2.1560e+00
Epoch 5/10
10/10 - 7s - loss: 456.1232 - loglik: -4.5363e+02 - logprior: -2.4970e+00
Epoch 6/10
10/10 - 7s - loss: 449.9146 - loglik: -4.4738e+02 - logprior: -2.5325e+00
Epoch 7/10
10/10 - 7s - loss: 446.2344 - loglik: -4.4387e+02 - logprior: -2.3649e+00
Epoch 8/10
10/10 - 7s - loss: 443.8394 - loglik: -4.4160e+02 - logprior: -2.2380e+00
Epoch 9/10
10/10 - 7s - loss: 442.8828 - loglik: -4.4068e+02 - logprior: -2.2020e+00
Epoch 10/10
10/10 - 7s - loss: 442.4879 - loglik: -4.4019e+02 - logprior: -2.2931e+00
Fitted a model with MAP estimate = -441.4631
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 2), (22, 2), (37, 1), (38, 1), (39, 3), (40, 2), (50, 1), (60, 1), (63, 3), (65, 1), (66, 1), (75, 8), (96, 1), (97, 1), (100, 1), (119, 1), (120, 1), (121, 1), (122, 1), (138, 1), (146, 1), (148, 1), (150, 1), (158, 1), (160, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (177, 2)]
discards: [0]
Fitting a model of length 243 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 14s - loss: 440.2914 - loglik: -4.2968e+02 - logprior: -1.0613e+01
Epoch 2/2
21/21 - 11s - loss: 413.0294 - loglik: -4.0989e+02 - logprior: -3.1378e+00
Fitted a model with MAP estimate = -407.3024
expansions: []
discards: [ 13  18  30  32  55  84 219]
Fitting a model of length 236 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 416.7371 - loglik: -4.0984e+02 - logprior: -6.9020e+00
Epoch 2/2
21/21 - 10s - loss: 405.5157 - loglik: -4.0565e+02 - logprior: 0.1388
Fitted a model with MAP estimate = -402.6738
expansions: []
discards: [82]
Fitting a model of length 235 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 15s - loss: 411.5926 - loglik: -4.0538e+02 - logprior: -6.2154e+00
Epoch 2/10
21/21 - 10s - loss: 404.6042 - loglik: -4.0503e+02 - logprior: 0.4217
Epoch 3/10
21/21 - 10s - loss: 400.7667 - loglik: -4.0194e+02 - logprior: 1.1729
Epoch 4/10
21/21 - 10s - loss: 399.5987 - loglik: -4.0115e+02 - logprior: 1.5559
Epoch 5/10
21/21 - 10s - loss: 399.7318 - loglik: -4.0144e+02 - logprior: 1.7044
Fitted a model with MAP estimate = -398.3400
Time for alignment: 211.9104
Computed alignments with likelihoods: ['-383.1763', '-389.4343', '-398.3400']
Best model has likelihood: -383.1763
SP score = 0.6180
Training of 3 independent models on file gluts.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1d76498580>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1d761a7280>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f179bf9c910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2011ff66a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 284.5745 - loglik: -2.8123e+02 - logprior: -3.3408e+00
Epoch 2/10
19/19 - 3s - loss: 254.4282 - loglik: -2.5351e+02 - logprior: -9.2011e-01
Epoch 3/10
19/19 - 3s - loss: 243.8619 - loglik: -2.4290e+02 - logprior: -9.5734e-01
Epoch 4/10
19/19 - 3s - loss: 241.9309 - loglik: -2.4106e+02 - logprior: -8.6695e-01
Epoch 5/10
19/19 - 3s - loss: 240.3651 - loglik: -2.3950e+02 - logprior: -8.6433e-01
Epoch 6/10
19/19 - 3s - loss: 240.2135 - loglik: -2.3934e+02 - logprior: -8.7463e-01
Epoch 7/10
19/19 - 3s - loss: 238.9436 - loglik: -2.3805e+02 - logprior: -8.9569e-01
Epoch 8/10
19/19 - 3s - loss: 238.4560 - loglik: -2.3750e+02 - logprior: -9.6072e-01
Epoch 9/10
19/19 - 3s - loss: 237.9660 - loglik: -2.3695e+02 - logprior: -1.0174e+00
Epoch 10/10
19/19 - 3s - loss: 237.3831 - loglik: -2.3632e+02 - logprior: -1.0663e+00
Fitted a model with MAP estimate = -236.4735
expansions: [(0, 10), (12, 1), (16, 2), (54, 1), (55, 1), (57, 2), (58, 2), (59, 3), (61, 2)]
discards: []
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 249.1496 - loglik: -2.4510e+02 - logprior: -4.0511e+00
Epoch 2/2
19/19 - 4s - loss: 239.9065 - loglik: -2.3873e+02 - logprior: -1.1722e+00
Fitted a model with MAP estimate = -237.2623
expansions: [(0, 8), (34, 2)]
discards: [ 1  2  3  4  5  6  7  8  9 27 75 76 80]
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 243.7055 - loglik: -2.3991e+02 - logprior: -3.7927e+00
Epoch 2/2
19/19 - 3s - loss: 239.0242 - loglik: -2.3776e+02 - logprior: -1.2614e+00
Fitted a model with MAP estimate = -236.9526
expansions: [(0, 7), (31, 1), (33, 1)]
discards: [ 0  1  2  3  4  5  6  7  8 79]
Fitting a model of length 97 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 240.6878 - loglik: -2.3750e+02 - logprior: -3.1918e+00
Epoch 2/10
19/19 - 3s - loss: 236.9982 - loglik: -2.3608e+02 - logprior: -9.1774e-01
Epoch 3/10
19/19 - 4s - loss: 235.4520 - loglik: -2.3467e+02 - logprior: -7.8096e-01
Epoch 4/10
19/19 - 4s - loss: 234.5092 - loglik: -2.3376e+02 - logprior: -7.4811e-01
Epoch 5/10
19/19 - 4s - loss: 233.6235 - loglik: -2.3290e+02 - logprior: -7.2281e-01
Epoch 6/10
19/19 - 4s - loss: 232.3638 - loglik: -2.3165e+02 - logprior: -7.1160e-01
Epoch 7/10
19/19 - 3s - loss: 231.3367 - loglik: -2.3059e+02 - logprior: -7.4733e-01
Epoch 8/10
19/19 - 4s - loss: 231.8388 - loglik: -2.3107e+02 - logprior: -7.6524e-01
Fitted a model with MAP estimate = -230.6346
Time for alignment: 110.5143
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 284.2280 - loglik: -2.8089e+02 - logprior: -3.3339e+00
Epoch 2/10
19/19 - 3s - loss: 252.6519 - loglik: -2.5174e+02 - logprior: -9.0914e-01
Epoch 3/10
19/19 - 3s - loss: 243.6697 - loglik: -2.4276e+02 - logprior: -9.1159e-01
Epoch 4/10
19/19 - 3s - loss: 241.6507 - loglik: -2.4082e+02 - logprior: -8.3106e-01
Epoch 5/10
19/19 - 3s - loss: 240.6495 - loglik: -2.3982e+02 - logprior: -8.2994e-01
Epoch 6/10
19/19 - 3s - loss: 239.1477 - loglik: -2.3829e+02 - logprior: -8.5601e-01
Epoch 7/10
19/19 - 3s - loss: 238.4259 - loglik: -2.3755e+02 - logprior: -8.8075e-01
Epoch 8/10
19/19 - 3s - loss: 238.4047 - loglik: -2.3745e+02 - logprior: -9.5394e-01
Epoch 9/10
19/19 - 3s - loss: 237.0732 - loglik: -2.3605e+02 - logprior: -1.0213e+00
Epoch 10/10
19/19 - 3s - loss: 237.1381 - loglik: -2.3607e+02 - logprior: -1.0722e+00
Fitted a model with MAP estimate = -235.8936
expansions: [(0, 10), (12, 1), (14, 2), (15, 2), (16, 1), (19, 1), (55, 1), (57, 2), (58, 2), (59, 3)]
discards: []
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 248.4776 - loglik: -2.4443e+02 - logprior: -4.0519e+00
Epoch 2/2
19/19 - 4s - loss: 238.5272 - loglik: -2.3732e+02 - logprior: -1.2033e+00
Fitted a model with MAP estimate = -236.0151
expansions: [(0, 8), (27, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 28 78 79 83]
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 242.4251 - loglik: -2.3866e+02 - logprior: -3.7609e+00
Epoch 2/2
19/19 - 4s - loss: 238.1514 - loglik: -2.3697e+02 - logprior: -1.1849e+00
Fitted a model with MAP estimate = -236.2408
expansions: [(0, 8)]
discards: [0 1 2 3 4 5 6 7 8]
Fitting a model of length 97 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 240.1833 - loglik: -2.3701e+02 - logprior: -3.1718e+00
Epoch 2/10
19/19 - 4s - loss: 236.3882 - loglik: -2.3546e+02 - logprior: -9.3264e-01
Epoch 3/10
19/19 - 3s - loss: 235.5636 - loglik: -2.3477e+02 - logprior: -7.9825e-01
Epoch 4/10
19/19 - 3s - loss: 234.2875 - loglik: -2.3351e+02 - logprior: -7.7535e-01
Epoch 5/10
19/19 - 4s - loss: 233.4725 - loglik: -2.3271e+02 - logprior: -7.6512e-01
Epoch 6/10
19/19 - 4s - loss: 232.4446 - loglik: -2.3168e+02 - logprior: -7.6107e-01
Epoch 7/10
19/19 - 4s - loss: 231.5860 - loglik: -2.3080e+02 - logprior: -7.8641e-01
Epoch 8/10
19/19 - 3s - loss: 230.6326 - loglik: -2.2981e+02 - logprior: -8.1981e-01
Epoch 9/10
19/19 - 4s - loss: 230.8191 - loglik: -2.2995e+02 - logprior: -8.6542e-01
Fitted a model with MAP estimate = -230.1129
Time for alignment: 114.9486
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 284.5141 - loglik: -2.8116e+02 - logprior: -3.3511e+00
Epoch 2/10
19/19 - 3s - loss: 254.4803 - loglik: -2.5355e+02 - logprior: -9.3068e-01
Epoch 3/10
19/19 - 3s - loss: 244.5224 - loglik: -2.4358e+02 - logprior: -9.4060e-01
Epoch 4/10
19/19 - 3s - loss: 242.5233 - loglik: -2.4166e+02 - logprior: -8.6590e-01
Epoch 5/10
19/19 - 3s - loss: 240.8548 - loglik: -2.4001e+02 - logprior: -8.4316e-01
Epoch 6/10
19/19 - 3s - loss: 240.2354 - loglik: -2.3939e+02 - logprior: -8.4982e-01
Epoch 7/10
19/19 - 3s - loss: 239.4233 - loglik: -2.3855e+02 - logprior: -8.6989e-01
Epoch 8/10
19/19 - 3s - loss: 238.4231 - loglik: -2.3748e+02 - logprior: -9.3965e-01
Epoch 9/10
19/19 - 4s - loss: 238.6907 - loglik: -2.3769e+02 - logprior: -1.0022e+00
Fitted a model with MAP estimate = -237.1154
expansions: [(0, 10), (16, 2), (20, 4), (35, 1), (57, 2), (58, 1), (59, 3), (61, 2)]
discards: []
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 248.0535 - loglik: -2.4401e+02 - logprior: -4.0414e+00
Epoch 2/2
19/19 - 4s - loss: 239.1170 - loglik: -2.3791e+02 - logprior: -1.2026e+00
Fitted a model with MAP estimate = -236.5912
expansions: [(0, 8), (35, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 26 76 79 80 81]
Fitting a model of length 97 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 242.7128 - loglik: -2.3892e+02 - logprior: -3.7899e+00
Epoch 2/2
19/19 - 4s - loss: 238.3806 - loglik: -2.3712e+02 - logprior: -1.2649e+00
Fitted a model with MAP estimate = -236.5476
expansions: [(0, 8)]
discards: [0 1 2 3 4 5 6 7 8]
Fitting a model of length 96 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 240.4526 - loglik: -2.3720e+02 - logprior: -3.2528e+00
Epoch 2/10
19/19 - 4s - loss: 237.0895 - loglik: -2.3603e+02 - logprior: -1.0557e+00
Epoch 3/10
19/19 - 3s - loss: 235.8632 - loglik: -2.3501e+02 - logprior: -8.5336e-01
Epoch 4/10
19/19 - 3s - loss: 235.1029 - loglik: -2.3431e+02 - logprior: -7.8872e-01
Epoch 5/10
19/19 - 4s - loss: 233.7478 - loglik: -2.3298e+02 - logprior: -7.6945e-01
Epoch 6/10
19/19 - 3s - loss: 232.8791 - loglik: -2.3209e+02 - logprior: -7.8946e-01
Epoch 7/10
19/19 - 3s - loss: 232.3312 - loglik: -2.3154e+02 - logprior: -7.9485e-01
Epoch 8/10
19/19 - 3s - loss: 232.0306 - loglik: -2.3119e+02 - logprior: -8.3621e-01
Epoch 9/10
19/19 - 4s - loss: 231.2536 - loglik: -2.3037e+02 - logprior: -8.8014e-01
Epoch 10/10
19/19 - 3s - loss: 230.4432 - loglik: -2.2951e+02 - logprior: -9.3594e-01
Fitted a model with MAP estimate = -230.3217
Time for alignment: 112.6620
Computed alignments with likelihoods: ['-230.6346', '-230.1129', '-230.3217']
Best model has likelihood: -230.1129
SP score = 0.7904
Training of 3 independent models on file tms.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20e5700670>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2118425be0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dd0081730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201159e760>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 219 on 2118 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 14s - loss: 776.8749 - loglik: -7.5198e+02 - logprior: -2.4893e+01
Epoch 2/10
10/10 - 9s - loss: 661.6034 - loglik: -6.5723e+02 - logprior: -4.3713e+00
Epoch 3/10
10/10 - 9s - loss: 552.3270 - loglik: -5.5027e+02 - logprior: -2.0569e+00
Epoch 4/10
10/10 - 9s - loss: 474.1993 - loglik: -4.7148e+02 - logprior: -2.7183e+00
Epoch 5/10
10/10 - 9s - loss: 447.0219 - loglik: -4.4377e+02 - logprior: -3.2552e+00
Epoch 6/10
10/10 - 9s - loss: 440.0079 - loglik: -4.3660e+02 - logprior: -3.4094e+00
Epoch 7/10
10/10 - 9s - loss: 434.0034 - loglik: -4.3069e+02 - logprior: -3.3151e+00
Epoch 8/10
10/10 - 9s - loss: 430.5932 - loglik: -4.2751e+02 - logprior: -3.0880e+00
Epoch 9/10
10/10 - 9s - loss: 432.1301 - loglik: -4.2920e+02 - logprior: -2.9282e+00
Fitted a model with MAP estimate = -429.6980
expansions: [(8, 1), (9, 1), (11, 1), (13, 1), (15, 1), (24, 1), (33, 4), (45, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (85, 1), (87, 3), (95, 2), (97, 1), (100, 1), (114, 2), (115, 3), (129, 2), (131, 1), (140, 1), (141, 1), (160, 1), (161, 1), (163, 1), (177, 1), (179, 1), (181, 1), (182, 2), (184, 1), (191, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Fitting a model of length 269 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 14s - loss: 432.2464 - loglik: -4.1526e+02 - logprior: -1.6984e+01
Epoch 2/2
17/17 - 10s - loss: 391.5926 - loglik: -3.8693e+02 - logprior: -4.6610e+00
Fitted a model with MAP estimate = -387.3689
expansions: [(0, 9)]
discards: [  0  40 104 137 157 221]
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 14s - loss: 398.3425 - loglik: -3.8737e+02 - logprior: -1.0972e+01
Epoch 2/2
17/17 - 10s - loss: 382.2312 - loglik: -3.8258e+02 - logprior: 0.3448
Fitted a model with MAP estimate = -379.6846
expansions: []
discards: [1 2 3 4 5 6 7 8]
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 15s - loss: 394.9962 - loglik: -3.8509e+02 - logprior: -9.9098e+00
Epoch 2/10
17/17 - 10s - loss: 382.2610 - loglik: -3.8356e+02 - logprior: 1.2997
Epoch 3/10
17/17 - 10s - loss: 377.6175 - loglik: -3.8046e+02 - logprior: 2.8443
Epoch 4/10
17/17 - 10s - loss: 376.9221 - loglik: -3.8050e+02 - logprior: 3.5824
Epoch 5/10
17/17 - 10s - loss: 374.1725 - loglik: -3.7803e+02 - logprior: 3.8585
Epoch 6/10
17/17 - 10s - loss: 374.1033 - loglik: -3.7821e+02 - logprior: 4.1067
Epoch 7/10
17/17 - 10s - loss: 372.4643 - loglik: -3.7684e+02 - logprior: 4.3764
Epoch 8/10
17/17 - 10s - loss: 372.4647 - loglik: -3.7718e+02 - logprior: 4.7203
Fitted a model with MAP estimate = -371.8447
Time for alignment: 245.8982
Fitting a model of length 219 on 2118 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 12s - loss: 777.2990 - loglik: -7.5241e+02 - logprior: -2.4890e+01
Epoch 2/10
10/10 - 8s - loss: 661.4056 - loglik: -6.5703e+02 - logprior: -4.3784e+00
Epoch 3/10
10/10 - 9s - loss: 551.2508 - loglik: -5.4912e+02 - logprior: -2.1280e+00
Epoch 4/10
10/10 - 9s - loss: 473.3007 - loglik: -4.7036e+02 - logprior: -2.9450e+00
Epoch 5/10
10/10 - 9s - loss: 446.7834 - loglik: -4.4320e+02 - logprior: -3.5885e+00
Epoch 6/10
10/10 - 9s - loss: 438.8807 - loglik: -4.3512e+02 - logprior: -3.7574e+00
Epoch 7/10
10/10 - 9s - loss: 434.1955 - loglik: -4.3054e+02 - logprior: -3.6548e+00
Epoch 8/10
10/10 - 9s - loss: 431.2422 - loglik: -4.2779e+02 - logprior: -3.4537e+00
Epoch 9/10
10/10 - 9s - loss: 431.1374 - loglik: -4.2784e+02 - logprior: -3.3000e+00
Epoch 10/10
10/10 - 9s - loss: 428.8615 - loglik: -4.2550e+02 - logprior: -3.3565e+00
Fitted a model with MAP estimate = -428.8647
expansions: [(8, 1), (9, 1), (11, 1), (13, 1), (15, 1), (22, 1), (23, 1), (26, 1), (32, 1), (55, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (63, 1), (86, 1), (87, 1), (95, 2), (97, 1), (100, 1), (114, 2), (115, 3), (129, 2), (131, 1), (140, 1), (141, 1), (155, 1), (160, 1), (162, 1), (163, 1), (179, 1), (180, 1), (181, 1), (184, 1), (191, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 14s - loss: 431.1068 - loglik: -4.1423e+02 - logprior: -1.6881e+01
Epoch 2/2
17/17 - 10s - loss: 391.7027 - loglik: -3.8718e+02 - logprior: -4.5195e+00
Fitted a model with MAP estimate = -387.4942
expansions: [(0, 9)]
discards: [  0 135 155]
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 398.6604 - loglik: -3.8762e+02 - logprior: -1.1039e+01
Epoch 2/2
17/17 - 10s - loss: 382.1426 - loglik: -3.8246e+02 - logprior: 0.3145
Fitted a model with MAP estimate = -379.5799
expansions: []
discards: [1 2 3 4 5 6 7 8]
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 395.1344 - loglik: -3.8509e+02 - logprior: -1.0049e+01
Epoch 2/10
17/17 - 10s - loss: 382.5567 - loglik: -3.8377e+02 - logprior: 1.2136
Epoch 3/10
17/17 - 10s - loss: 378.6523 - loglik: -3.8146e+02 - logprior: 2.8049
Epoch 4/10
17/17 - 10s - loss: 375.7683 - loglik: -3.7939e+02 - logprior: 3.6225
Epoch 5/10
17/17 - 10s - loss: 374.2356 - loglik: -3.7820e+02 - logprior: 3.9672
Epoch 6/10
17/17 - 10s - loss: 373.2698 - loglik: -3.7749e+02 - logprior: 4.2199
Epoch 7/10
17/17 - 10s - loss: 371.8679 - loglik: -3.7632e+02 - logprior: 4.4491
Epoch 8/10
17/17 - 10s - loss: 375.0329 - loglik: -3.7976e+02 - logprior: 4.7292
Fitted a model with MAP estimate = -372.0229
Time for alignment: 250.7199
Fitting a model of length 219 on 2118 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 776.9383 - loglik: -7.5203e+02 - logprior: -2.4907e+01
Epoch 2/10
10/10 - 9s - loss: 662.0958 - loglik: -6.5772e+02 - logprior: -4.3775e+00
Epoch 3/10
10/10 - 9s - loss: 551.4218 - loglik: -5.4930e+02 - logprior: -2.1240e+00
Epoch 4/10
10/10 - 9s - loss: 476.3171 - loglik: -4.7339e+02 - logprior: -2.9263e+00
Epoch 5/10
10/10 - 9s - loss: 446.5433 - loglik: -4.4291e+02 - logprior: -3.6313e+00
Epoch 6/10
10/10 - 9s - loss: 438.7849 - loglik: -4.3490e+02 - logprior: -3.8823e+00
Epoch 7/10
10/10 - 9s - loss: 433.6381 - loglik: -4.2984e+02 - logprior: -3.7971e+00
Epoch 8/10
10/10 - 9s - loss: 431.9635 - loglik: -4.2846e+02 - logprior: -3.5041e+00
Epoch 9/10
10/10 - 9s - loss: 431.1122 - loglik: -4.2781e+02 - logprior: -3.2990e+00
Epoch 10/10
10/10 - 9s - loss: 429.8267 - loglik: -4.2651e+02 - logprior: -3.3144e+00
Fitted a model with MAP estimate = -429.6713
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (26, 1), (32, 1), (42, 1), (45, 1), (56, 1), (58, 1), (59, 1), (60, 1), (61, 2), (62, 2), (86, 2), (95, 2), (96, 1), (97, 1), (109, 1), (113, 1), (114, 1), (115, 2), (116, 1), (128, 2), (130, 1), (141, 1), (150, 1), (159, 1), (160, 1), (162, 1), (176, 1), (178, 1), (180, 1), (184, 1), (191, 1), (192, 1), (195, 1), (196, 1), (197, 1), (200, 1), (209, 1), (212, 1)]
discards: [0]
Fitting a model of length 268 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 14s - loss: 432.5062 - loglik: -4.1553e+02 - logprior: -1.6973e+01
Epoch 2/2
17/17 - 10s - loss: 392.4931 - loglik: -3.8792e+02 - logprior: -4.5689e+00
Fitted a model with MAP estimate = -387.4313
expansions: [(0, 9)]
discards: [  0  74 103 141 157]
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 398.7425 - loglik: -3.8779e+02 - logprior: -1.0949e+01
Epoch 2/2
17/17 - 10s - loss: 382.2717 - loglik: -3.8263e+02 - logprior: 0.3575
Fitted a model with MAP estimate = -379.4936
expansions: []
discards: [1 2 3 4 5 6 7 8]
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 395.4823 - loglik: -3.8556e+02 - logprior: -9.9208e+00
Epoch 2/10
17/17 - 10s - loss: 381.2128 - loglik: -3.8250e+02 - logprior: 1.2838
Epoch 3/10
17/17 - 10s - loss: 377.2832 - loglik: -3.8011e+02 - logprior: 2.8285
Epoch 4/10
17/17 - 10s - loss: 376.6364 - loglik: -3.8027e+02 - logprior: 3.6296
Epoch 5/10
17/17 - 10s - loss: 374.1453 - loglik: -3.7813e+02 - logprior: 3.9872
Epoch 6/10
17/17 - 10s - loss: 374.3307 - loglik: -3.7813e+02 - logprior: 3.8039
Fitted a model with MAP estimate = -372.8911
Time for alignment: 229.8248
Computed alignments with likelihoods: ['-371.8447', '-372.0229', '-372.8911']
Best model has likelihood: -371.8447
SP score = 0.9452
Training of 3 independent models on file kunitz.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1d76090070>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f17c05fe0a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20120a7970>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f179bd29d60>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 180.8787 - loglik: -1.5959e+02 - logprior: -2.1284e+01
Epoch 2/10
10/10 - 1s - loss: 148.1631 - loglik: -1.4217e+02 - logprior: -5.9912e+00
Epoch 3/10
10/10 - 1s - loss: 130.4067 - loglik: -1.2708e+02 - logprior: -3.3284e+00
Epoch 4/10
10/10 - 1s - loss: 119.8839 - loglik: -1.1724e+02 - logprior: -2.6475e+00
Epoch 5/10
10/10 - 1s - loss: 115.7861 - loglik: -1.1334e+02 - logprior: -2.4469e+00
Epoch 6/10
10/10 - 1s - loss: 114.3904 - loglik: -1.1207e+02 - logprior: -2.3226e+00
Epoch 7/10
10/10 - 1s - loss: 113.6153 - loglik: -1.1143e+02 - logprior: -2.1878e+00
Epoch 8/10
10/10 - 1s - loss: 113.3533 - loglik: -1.1130e+02 - logprior: -2.0582e+00
Epoch 9/10
10/10 - 1s - loss: 113.1231 - loglik: -1.1114e+02 - logprior: -1.9850e+00
Epoch 10/10
10/10 - 1s - loss: 113.0075 - loglik: -1.1103e+02 - logprior: -1.9747e+00
Fitted a model with MAP estimate = -112.9177
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (19, 1), (22, 1), (35, 4), (36, 1)]
discards: [0]
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 136.0698 - loglik: -1.1314e+02 - logprior: -2.2925e+01
Epoch 2/2
10/10 - 1s - loss: 116.7682 - loglik: -1.0703e+02 - logprior: -9.7401e+00
Fitted a model with MAP estimate = -113.1994
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 51]
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 122.2619 - loglik: -1.0437e+02 - logprior: -1.7888e+01
Epoch 2/2
10/10 - 1s - loss: 108.2649 - loglik: -1.0338e+02 - logprior: -4.8877e+00
Fitted a model with MAP estimate = -106.2452
expansions: []
discards: [0]
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 125.0302 - loglik: -1.0479e+02 - logprior: -2.0240e+01
Epoch 2/10
10/10 - 1s - loss: 110.1588 - loglik: -1.0437e+02 - logprior: -5.7892e+00
Epoch 3/10
10/10 - 1s - loss: 106.7006 - loglik: -1.0381e+02 - logprior: -2.8937e+00
Epoch 4/10
10/10 - 1s - loss: 105.8373 - loglik: -1.0379e+02 - logprior: -2.0468e+00
Epoch 5/10
10/10 - 1s - loss: 105.2975 - loglik: -1.0386e+02 - logprior: -1.4360e+00
Epoch 6/10
10/10 - 1s - loss: 104.7704 - loglik: -1.0359e+02 - logprior: -1.1801e+00
Epoch 7/10
10/10 - 1s - loss: 104.8677 - loglik: -1.0378e+02 - logprior: -1.0836e+00
Fitted a model with MAP estimate = -104.6382
Time for alignment: 34.0063
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 180.8832 - loglik: -1.5960e+02 - logprior: -2.1284e+01
Epoch 2/10
10/10 - 1s - loss: 148.2565 - loglik: -1.4226e+02 - logprior: -5.9980e+00
Epoch 3/10
10/10 - 1s - loss: 131.0238 - loglik: -1.2764e+02 - logprior: -3.3857e+00
Epoch 4/10
10/10 - 1s - loss: 119.2801 - loglik: -1.1645e+02 - logprior: -2.8315e+00
Epoch 5/10
10/10 - 1s - loss: 114.0273 - loglik: -1.1129e+02 - logprior: -2.7352e+00
Epoch 6/10
10/10 - 1s - loss: 112.6817 - loglik: -1.1005e+02 - logprior: -2.6355e+00
Epoch 7/10
10/10 - 1s - loss: 111.6250 - loglik: -1.0911e+02 - logprior: -2.5161e+00
Epoch 8/10
10/10 - 1s - loss: 110.9501 - loglik: -1.0856e+02 - logprior: -2.3868e+00
Epoch 9/10
10/10 - 1s - loss: 110.8441 - loglik: -1.0855e+02 - logprior: -2.2974e+00
Epoch 10/10
10/10 - 1s - loss: 110.7236 - loglik: -1.0845e+02 - logprior: -2.2688e+00
Fitted a model with MAP estimate = -110.6332
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (17, 1), (18, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 135.3707 - loglik: -1.1242e+02 - logprior: -2.2946e+01
Epoch 2/2
10/10 - 1s - loss: 116.2621 - loglik: -1.0655e+02 - logprior: -9.7077e+00
Fitted a model with MAP estimate = -112.8706
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 121.7904 - loglik: -1.0394e+02 - logprior: -1.7853e+01
Epoch 2/2
10/10 - 1s - loss: 107.7899 - loglik: -1.0291e+02 - logprior: -4.8790e+00
Fitted a model with MAP estimate = -106.1500
expansions: []
discards: [0]
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 124.7195 - loglik: -1.0449e+02 - logprior: -2.0228e+01
Epoch 2/10
10/10 - 1s - loss: 110.3155 - loglik: -1.0454e+02 - logprior: -5.7771e+00
Epoch 3/10
10/10 - 1s - loss: 106.8626 - loglik: -1.0397e+02 - logprior: -2.8931e+00
Epoch 4/10
10/10 - 1s - loss: 105.6883 - loglik: -1.0365e+02 - logprior: -2.0424e+00
Epoch 5/10
10/10 - 1s - loss: 105.1081 - loglik: -1.0368e+02 - logprior: -1.4257e+00
Epoch 6/10
10/10 - 1s - loss: 105.0795 - loglik: -1.0391e+02 - logprior: -1.1706e+00
Epoch 7/10
10/10 - 1s - loss: 104.6256 - loglik: -1.0355e+02 - logprior: -1.0765e+00
Epoch 8/10
10/10 - 1s - loss: 104.7020 - loglik: -1.0378e+02 - logprior: -9.2605e-01
Fitted a model with MAP estimate = -104.5474
Time for alignment: 32.6317
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 180.8880 - loglik: -1.5960e+02 - logprior: -2.1284e+01
Epoch 2/10
10/10 - 1s - loss: 148.4180 - loglik: -1.4242e+02 - logprior: -5.9941e+00
Epoch 3/10
10/10 - 1s - loss: 129.9528 - loglik: -1.2657e+02 - logprior: -3.3827e+00
Epoch 4/10
10/10 - 1s - loss: 117.8709 - loglik: -1.1503e+02 - logprior: -2.8423e+00
Epoch 5/10
10/10 - 1s - loss: 113.5592 - loglik: -1.1080e+02 - logprior: -2.7625e+00
Epoch 6/10
10/10 - 1s - loss: 112.3817 - loglik: -1.0972e+02 - logprior: -2.6663e+00
Epoch 7/10
10/10 - 1s - loss: 111.7622 - loglik: -1.0925e+02 - logprior: -2.5106e+00
Epoch 8/10
10/10 - 1s - loss: 111.1626 - loglik: -1.0879e+02 - logprior: -2.3719e+00
Epoch 9/10
10/10 - 1s - loss: 111.0729 - loglik: -1.0876e+02 - logprior: -2.3147e+00
Epoch 10/10
10/10 - 1s - loss: 110.7082 - loglik: -1.0841e+02 - logprior: -2.3024e+00
Fitted a model with MAP estimate = -110.7286
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (19, 1), (20, 1), (22, 2), (23, 1), (34, 1), (35, 1), (36, 1)]
discards: [0]
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 135.6965 - loglik: -1.1275e+02 - logprior: -2.2945e+01
Epoch 2/2
10/10 - 1s - loss: 116.3270 - loglik: -1.0660e+02 - logprior: -9.7242e+00
Fitted a model with MAP estimate = -112.9598
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 32]
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 121.5542 - loglik: -1.0369e+02 - logprior: -1.7862e+01
Epoch 2/2
10/10 - 1s - loss: 108.2136 - loglik: -1.0333e+02 - logprior: -4.8863e+00
Fitted a model with MAP estimate = -106.1314
expansions: []
discards: [0]
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 124.8310 - loglik: -1.0458e+02 - logprior: -2.0250e+01
Epoch 2/10
10/10 - 1s - loss: 110.3243 - loglik: -1.0453e+02 - logprior: -5.7949e+00
Epoch 3/10
10/10 - 1s - loss: 106.7486 - loglik: -1.0384e+02 - logprior: -2.9041e+00
Epoch 4/10
10/10 - 1s - loss: 105.7998 - loglik: -1.0374e+02 - logprior: -2.0556e+00
Epoch 5/10
10/10 - 1s - loss: 105.1907 - loglik: -1.0374e+02 - logprior: -1.4462e+00
Epoch 6/10
10/10 - 1s - loss: 104.9437 - loglik: -1.0376e+02 - logprior: -1.1821e+00
Epoch 7/10
10/10 - 1s - loss: 104.7317 - loglik: -1.0365e+02 - logprior: -1.0831e+00
Epoch 8/10
10/10 - 1s - loss: 104.5577 - loglik: -1.0363e+02 - logprior: -9.3182e-01
Epoch 9/10
10/10 - 1s - loss: 104.6507 - loglik: -1.0378e+02 - logprior: -8.6877e-01
Fitted a model with MAP estimate = -104.4788
Time for alignment: 33.2291
Computed alignments with likelihoods: ['-104.6382', '-104.5474', '-104.4788']
Best model has likelihood: -104.4788
SP score = 0.9929
Training of 3 independent models on file rhv.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2118421250>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f213a6256d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20cb2df100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20d39d0b20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 468.1449 - loglik: -4.6604e+02 - logprior: -2.1067e+00
Epoch 2/10
39/39 - 11s - loss: 388.1710 - loglik: -3.8709e+02 - logprior: -1.0801e+00
Epoch 3/10
39/39 - 10s - loss: 379.5724 - loglik: -3.7848e+02 - logprior: -1.0956e+00
Epoch 4/10
39/39 - 10s - loss: 377.7690 - loglik: -3.7668e+02 - logprior: -1.0895e+00
Epoch 5/10
39/39 - 10s - loss: 376.4106 - loglik: -3.7527e+02 - logprior: -1.1384e+00
Epoch 6/10
39/39 - 10s - loss: 375.2739 - loglik: -3.7410e+02 - logprior: -1.1770e+00
Epoch 7/10
39/39 - 11s - loss: 374.8287 - loglik: -3.7359e+02 - logprior: -1.2356e+00
Epoch 8/10
39/39 - 11s - loss: 374.6014 - loglik: -3.7331e+02 - logprior: -1.2877e+00
Epoch 9/10
39/39 - 10s - loss: 374.0523 - loglik: -3.7275e+02 - logprior: -1.3072e+00
Epoch 10/10
39/39 - 11s - loss: 374.3014 - loglik: -3.7298e+02 - logprior: -1.3229e+00
Fitted a model with MAP estimate = -308.8567
expansions: [(0, 11), (10, 1), (11, 1), (22, 1), (28, 1), (30, 1), (31, 3), (33, 2), (34, 2), (37, 1), (44, 1), (45, 1), (56, 1), (71, 1), (88, 3), (89, 1), (90, 1), (102, 1), (107, 1), (126, 8), (130, 3)]
discards: []
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 361.0921 - loglik: -3.5805e+02 - logprior: -3.0468e+00
Epoch 2/2
39/39 - 12s - loss: 345.8374 - loglik: -3.4441e+02 - logprior: -1.4287e+00
Fitted a model with MAP estimate = -286.6298
expansions: [(164, 4)]
discards: [ 1  2  3  4  5  6  7  8  9 47 48 49 50 51]
Fitting a model of length 170 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 351.4206 - loglik: -3.4927e+02 - logprior: -2.1485e+00
Epoch 2/2
39/39 - 13s - loss: 346.2759 - loglik: -3.4537e+02 - logprior: -9.0714e-01
Fitted a model with MAP estimate = -289.0597
expansions: [(0, 15), (38, 2), (39, 2), (156, 1)]
discards: []
Fitting a model of length 190 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 18s - loss: 284.3072 - loglik: -2.8249e+02 - logprior: -1.8199e+00
Epoch 2/10
52/52 - 16s - loss: 280.0546 - loglik: -2.7864e+02 - logprior: -1.4153e+00
Epoch 3/10
52/52 - 15s - loss: 278.3702 - loglik: -2.7708e+02 - logprior: -1.2947e+00
Epoch 4/10
52/52 - 16s - loss: 276.3769 - loglik: -2.7514e+02 - logprior: -1.2369e+00
Epoch 5/10
52/52 - 15s - loss: 276.2256 - loglik: -2.7501e+02 - logprior: -1.2121e+00
Epoch 6/10
52/52 - 14s - loss: 274.8191 - loglik: -2.7360e+02 - logprior: -1.2172e+00
Epoch 7/10
52/52 - 16s - loss: 277.1178 - loglik: -2.7592e+02 - logprior: -1.2000e+00
Fitted a model with MAP estimate = -274.9319
Time for alignment: 362.7409
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 467.4438 - loglik: -4.6538e+02 - logprior: -2.0682e+00
Epoch 2/10
39/39 - 11s - loss: 385.6272 - loglik: -3.8459e+02 - logprior: -1.0406e+00
Epoch 3/10
39/39 - 10s - loss: 378.4796 - loglik: -3.7748e+02 - logprior: -1.0008e+00
Epoch 4/10
39/39 - 11s - loss: 376.7632 - loglik: -3.7580e+02 - logprior: -9.6814e-01
Epoch 5/10
39/39 - 11s - loss: 375.9407 - loglik: -3.7493e+02 - logprior: -1.0099e+00
Epoch 6/10
39/39 - 11s - loss: 375.1008 - loglik: -3.7403e+02 - logprior: -1.0706e+00
Epoch 7/10
39/39 - 11s - loss: 374.8593 - loglik: -3.7373e+02 - logprior: -1.1278e+00
Epoch 8/10
39/39 - 11s - loss: 374.2007 - loglik: -3.7305e+02 - logprior: -1.1542e+00
Epoch 9/10
39/39 - 10s - loss: 374.2939 - loglik: -3.7307e+02 - logprior: -1.2245e+00
Fitted a model with MAP estimate = -309.5982
expansions: [(0, 14), (10, 1), (15, 1), (18, 1), (28, 1), (29, 2), (30, 1), (31, 3), (32, 1), (37, 1), (42, 1), (43, 1), (44, 1), (49, 1), (70, 2), (87, 1), (88, 1), (90, 1), (102, 3), (107, 2), (125, 7), (126, 2), (134, 4)]
discards: []
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 358.7182 - loglik: -3.5581e+02 - logprior: -2.9081e+00
Epoch 2/2
39/39 - 13s - loss: 342.5114 - loglik: -3.4106e+02 - logprior: -1.4520e+00
Fitted a model with MAP estimate = -284.1746
expansions: [(169, 2)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  48 138 145 174]
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 348.7729 - loglik: -3.4676e+02 - logprior: -2.0167e+00
Epoch 2/2
39/39 - 12s - loss: 343.9461 - loglik: -3.4314e+02 - logprior: -8.0158e-01
Fitted a model with MAP estimate = -287.1817
expansions: [(0, 18)]
discards: []
Fitting a model of length 191 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 18s - loss: 283.6912 - loglik: -2.8201e+02 - logprior: -1.6799e+00
Epoch 2/10
52/52 - 16s - loss: 279.2816 - loglik: -2.7795e+02 - logprior: -1.3344e+00
Epoch 3/10
52/52 - 16s - loss: 275.2401 - loglik: -2.7415e+02 - logprior: -1.0852e+00
Epoch 4/10
52/52 - 15s - loss: 276.4391 - loglik: -2.7551e+02 - logprior: -9.2938e-01
Fitted a model with MAP estimate = -274.6533
Time for alignment: 309.6879
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 468.4355 - loglik: -4.6635e+02 - logprior: -2.0820e+00
Epoch 2/10
39/39 - 10s - loss: 388.1924 - loglik: -3.8725e+02 - logprior: -9.3845e-01
Epoch 3/10
39/39 - 10s - loss: 380.3113 - loglik: -3.7935e+02 - logprior: -9.5749e-01
Epoch 4/10
39/39 - 10s - loss: 377.9509 - loglik: -3.7694e+02 - logprior: -1.0094e+00
Epoch 5/10
39/39 - 11s - loss: 377.2596 - loglik: -3.7615e+02 - logprior: -1.1107e+00
Epoch 6/10
39/39 - 10s - loss: 376.0957 - loglik: -3.7489e+02 - logprior: -1.2036e+00
Epoch 7/10
39/39 - 10s - loss: 375.9565 - loglik: -3.7466e+02 - logprior: -1.2916e+00
Epoch 8/10
39/39 - 9s - loss: 374.7985 - loglik: -3.7342e+02 - logprior: -1.3738e+00
Epoch 9/10
39/39 - 10s - loss: 375.4995 - loglik: -3.7403e+02 - logprior: -1.4648e+00
Fitted a model with MAP estimate = -309.6552
expansions: [(0, 6), (11, 1), (15, 1), (18, 1), (28, 1), (30, 1), (32, 2), (33, 1), (34, 1), (36, 1), (37, 1), (38, 2), (43, 1), (44, 2), (45, 1), (56, 1), (70, 3), (71, 1), (88, 1), (89, 1), (90, 1), (91, 1), (103, 1), (107, 1), (108, 1), (112, 1), (125, 1), (127, 8), (134, 4)]
discards: []
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 356.4882 - loglik: -3.5336e+02 - logprior: -3.1300e+00
Epoch 2/2
39/39 - 13s - loss: 338.9264 - loglik: -3.3752e+02 - logprior: -1.4058e+00
Fitted a model with MAP estimate = -281.7467
expansions: []
discards: [ 0  1  2  3  4  5 44 63 64 65 94 95]
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 349.2911 - loglik: -3.4714e+02 - logprior: -2.1537e+00
Epoch 2/2
39/39 - 11s - loss: 344.6365 - loglik: -3.4385e+02 - logprior: -7.8621e-01
Fitted a model with MAP estimate = -287.3176
expansions: [(0, 15), (56, 2)]
discards: []
Fitting a model of length 188 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 18s - loss: 283.2913 - loglik: -2.8173e+02 - logprior: -1.5604e+00
Epoch 2/10
52/52 - 15s - loss: 275.9164 - loglik: -2.7476e+02 - logprior: -1.1568e+00
Epoch 3/10
52/52 - 17s - loss: 276.2128 - loglik: -2.7514e+02 - logprior: -1.0741e+00
Fitted a model with MAP estimate = -274.6565
Time for alignment: 286.4703
Computed alignments with likelihoods: ['-274.9319', '-274.6533', '-274.6565']
Best model has likelihood: -274.6533
SP score = 0.2087
Training of 3 independent models on file blm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20a126dfd0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f202ccb0820>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202ccb0850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202326f5e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 28s - loss: 817.8483 - loglik: -8.1565e+02 - logprior: -2.2030e+00
Epoch 2/10
37/37 - 23s - loss: 731.6248 - loglik: -7.3107e+02 - logprior: -5.5049e-01
Epoch 3/10
37/37 - 23s - loss: 721.4955 - loglik: -7.2094e+02 - logprior: -5.5417e-01
Epoch 4/10
37/37 - 23s - loss: 717.5166 - loglik: -7.1692e+02 - logprior: -5.9798e-01
Epoch 5/10
37/37 - 23s - loss: 714.3428 - loglik: -7.1356e+02 - logprior: -7.8464e-01
Epoch 6/10
37/37 - 23s - loss: 713.1225 - loglik: -7.1195e+02 - logprior: -1.1702e+00
Epoch 7/10
37/37 - 23s - loss: 708.5123 - loglik: -7.0707e+02 - logprior: -1.4379e+00
Epoch 8/10
37/37 - 23s - loss: 711.4127 - loglik: -7.0963e+02 - logprior: -1.7806e+00
Fitted a model with MAP estimate = -708.1739
expansions: [(0, 4), (26, 1), (30, 1), (32, 4), (33, 2), (35, 1), (73, 4), (105, 1), (106, 1), (107, 2), (138, 1), (197, 13), (229, 1), (247, 1)]
discards: [ 99 100 151 188 189]
Fitting a model of length 286 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 30s - loss: 729.3254 - loglik: -7.2647e+02 - logprior: -2.8582e+00
Epoch 2/2
37/37 - 27s - loss: 709.2714 - loglik: -7.0861e+02 - logprior: -6.6290e-01
Fitted a model with MAP estimate = -706.0070
expansions: [(0, 2), (40, 1), (45, 1), (126, 1), (226, 2)]
discards: [  0 172 173 174 227 283]
Fitting a model of length 287 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 30s - loss: 713.6212 - loglik: -7.1167e+02 - logprior: -1.9489e+00
Epoch 2/2
37/37 - 27s - loss: 708.7295 - loglik: -7.0828e+02 - logprior: -4.5317e-01
Fitted a model with MAP estimate = -705.5148
expansions: [(0, 2), (176, 3), (285, 1)]
discards: [  1   2   3 229]
Fitting a model of length 289 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 32s - loss: 711.0769 - loglik: -7.0893e+02 - logprior: -2.1505e+00
Epoch 2/10
37/37 - 28s - loss: 705.8842 - loglik: -7.0562e+02 - logprior: -2.6091e-01
Epoch 3/10
37/37 - 28s - loss: 702.7962 - loglik: -7.0267e+02 - logprior: -1.2822e-01
Epoch 4/10
37/37 - 28s - loss: 699.4382 - loglik: -6.9935e+02 - logprior: -9.1697e-02
Epoch 5/10
37/37 - 27s - loss: 696.4447 - loglik: -6.9626e+02 - logprior: -1.8564e-01
Epoch 6/10
37/37 - 27s - loss: 695.0463 - loglik: -6.9472e+02 - logprior: -3.2941e-01
Epoch 7/10
37/37 - 27s - loss: 693.1281 - loglik: -6.9261e+02 - logprior: -5.2003e-01
Epoch 8/10
37/37 - 27s - loss: 691.9711 - loglik: -6.9131e+02 - logprior: -6.5752e-01
Epoch 9/10
37/37 - 27s - loss: 691.5382 - loglik: -6.9056e+02 - logprior: -9.7889e-01
Epoch 10/10
37/37 - 28s - loss: 691.3507 - loglik: -6.9033e+02 - logprior: -1.0176e+00
Fitted a model with MAP estimate = -689.1840
Time for alignment: 710.2389
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 26s - loss: 818.3470 - loglik: -8.1613e+02 - logprior: -2.2121e+00
Epoch 2/10
37/37 - 23s - loss: 738.5087 - loglik: -7.3792e+02 - logprior: -5.8596e-01
Epoch 3/10
37/37 - 23s - loss: 724.4228 - loglik: -7.2377e+02 - logprior: -6.5097e-01
Epoch 4/10
37/37 - 23s - loss: 720.9850 - loglik: -7.2029e+02 - logprior: -6.9027e-01
Epoch 5/10
37/37 - 23s - loss: 720.0912 - loglik: -7.1909e+02 - logprior: -1.0002e+00
Epoch 6/10
37/37 - 23s - loss: 717.2498 - loglik: -7.1607e+02 - logprior: -1.1763e+00
Epoch 7/10
37/37 - 23s - loss: 715.9028 - loglik: -7.1430e+02 - logprior: -1.6078e+00
Epoch 8/10
37/37 - 23s - loss: 712.4929 - loglik: -7.1055e+02 - logprior: -1.9446e+00
Epoch 9/10
37/37 - 23s - loss: 711.8795 - loglik: -7.0952e+02 - logprior: -2.3604e+00
Epoch 10/10
37/37 - 23s - loss: 713.3705 - loglik: -7.1078e+02 - logprior: -2.5942e+00
Fitted a model with MAP estimate = -710.6562
expansions: [(0, 4), (30, 1), (32, 4), (33, 1), (34, 1), (36, 1), (66, 1), (92, 8), (104, 2), (137, 7), (179, 1), (198, 13), (206, 7), (240, 1), (241, 1)]
discards: [ 97  98  99 100 101 102 106 189 190]
Fitting a model of length 298 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 31s - loss: 736.7659 - loglik: -7.3361e+02 - logprior: -3.1509e+00
Epoch 2/2
37/37 - 29s - loss: 710.7762 - loglik: -7.1001e+02 - logprior: -7.7082e-01
Fitted a model with MAP estimate = -707.2474
expansions: [(0, 2), (39, 1), (111, 1), (118, 1), (119, 1), (154, 1), (167, 1), (171, 9), (232, 2)]
discards: [  0 176 177 178 233 248 249 250]
Fitting a model of length 309 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 35s - loss: 713.4233 - loglik: -7.1137e+02 - logprior: -2.0576e+00
Epoch 2/2
37/37 - 30s - loss: 705.2937 - loglik: -7.0481e+02 - logprior: -4.8817e-01
Fitted a model with MAP estimate = -703.3043
expansions: [(0, 2), (260, 1)]
discards: [  1   2 186 187 188 189 190 200 247]
Fitting a model of length 303 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 33s - loss: 711.5060 - loglik: -7.0931e+02 - logprior: -2.1919e+00
Epoch 2/10
37/37 - 29s - loss: 707.5764 - loglik: -7.0708e+02 - logprior: -4.9754e-01
Epoch 3/10
37/37 - 29s - loss: 703.6130 - loglik: -7.0332e+02 - logprior: -2.9688e-01
Epoch 4/10
37/37 - 29s - loss: 698.8699 - loglik: -6.9852e+02 - logprior: -3.4739e-01
Epoch 5/10
37/37 - 29s - loss: 697.3463 - loglik: -6.9698e+02 - logprior: -3.7097e-01
Epoch 6/10
37/37 - 29s - loss: 695.8201 - loglik: -6.9530e+02 - logprior: -5.1721e-01
Epoch 7/10
37/37 - 29s - loss: 692.3496 - loglik: -6.9161e+02 - logprior: -7.3703e-01
Epoch 8/10
37/37 - 29s - loss: 691.0998 - loglik: -6.9017e+02 - logprior: -9.3415e-01
Epoch 9/10
37/37 - 29s - loss: 691.3082 - loglik: -6.9016e+02 - logprior: -1.1508e+00
Fitted a model with MAP estimate = -689.3615
Time for alignment: 755.4755
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 26s - loss: 818.8264 - loglik: -8.1660e+02 - logprior: -2.2221e+00
Epoch 2/10
37/37 - 23s - loss: 732.6644 - loglik: -7.3204e+02 - logprior: -6.2583e-01
Epoch 3/10
37/37 - 23s - loss: 722.6234 - loglik: -7.2196e+02 - logprior: -6.6030e-01
Epoch 4/10
37/37 - 23s - loss: 716.0521 - loglik: -7.1532e+02 - logprior: -7.3506e-01
Epoch 5/10
37/37 - 23s - loss: 714.9463 - loglik: -7.1402e+02 - logprior: -9.2249e-01
Epoch 6/10
37/37 - 23s - loss: 712.2852 - loglik: -7.1102e+02 - logprior: -1.2687e+00
Epoch 7/10
37/37 - 23s - loss: 709.7343 - loglik: -7.0792e+02 - logprior: -1.8116e+00
Epoch 8/10
37/37 - 23s - loss: 708.9977 - loglik: -7.0702e+02 - logprior: -1.9733e+00
Epoch 9/10
37/37 - 23s - loss: 706.3236 - loglik: -7.0400e+02 - logprior: -2.3196e+00
Epoch 10/10
37/37 - 23s - loss: 709.6967 - loglik: -7.0716e+02 - logprior: -2.5378e+00
Fitted a model with MAP estimate = -706.4621
expansions: [(0, 4), (30, 1), (32, 1), (33, 3), (34, 2), (66, 1), (90, 11), (99, 6), (100, 3), (101, 1), (121, 2), (136, 1), (140, 3), (156, 2), (197, 13), (210, 3), (229, 1), (239, 1), (247, 1)]
discards: [105 188 189]
Fitting a model of length 311 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 34s - loss: 733.1188 - loglik: -7.2978e+02 - logprior: -3.3359e+00
Epoch 2/2
37/37 - 30s - loss: 703.3008 - loglik: -7.0218e+02 - logprior: -1.1236e+00
Fitted a model with MAP estimate = -702.2424
expansions: [(0, 2), (43, 1), (44, 1), (111, 2), (123, 3), (177, 10), (247, 2)]
discards: [  0 113 114 115 116 117 118 136 178 179 180 181 182 183 184 185 186 187
 188 189 190 191 192 193 248 298 308]
Fitting a model of length 305 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 33s - loss: 714.5909 - loglik: -7.1249e+02 - logprior: -2.1031e+00
Epoch 2/2
37/37 - 30s - loss: 706.4932 - loglik: -7.0588e+02 - logprior: -6.0946e-01
Fitted a model with MAP estimate = -704.0564
expansions: [(0, 2), (177, 4), (190, 3), (191, 6), (259, 2)]
discards: [  1   2 183 254 255]
Fitting a model of length 317 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 34s - loss: 710.5737 - loglik: -7.0824e+02 - logprior: -2.3318e+00
Epoch 2/10
37/37 - 31s - loss: 702.3159 - loglik: -7.0184e+02 - logprior: -4.7162e-01
Epoch 3/10
37/37 - 32s - loss: 699.7953 - loglik: -6.9947e+02 - logprior: -3.2585e-01
Epoch 4/10
37/37 - 31s - loss: 695.8918 - loglik: -6.9558e+02 - logprior: -3.0708e-01
Epoch 5/10
37/37 - 31s - loss: 692.9943 - loglik: -6.9259e+02 - logprior: -3.9984e-01
Epoch 6/10
37/37 - 31s - loss: 688.0799 - loglik: -6.8742e+02 - logprior: -6.6332e-01
Epoch 7/10
37/37 - 31s - loss: 689.8154 - loglik: -6.8872e+02 - logprior: -1.0908e+00
Fitted a model with MAP estimate = -687.2957
Time for alignment: 712.2878
Computed alignments with likelihoods: ['-689.1840', '-689.3615', '-687.2957']
Best model has likelihood: -687.2957
SP score = 0.8900
Training of 3 independent models on file cyt3.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20f67b9be0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20b27643d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20128d6a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20d3a63730>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 399.7772 - loglik: -2.7314e+02 - logprior: -1.2663e+02
Epoch 2/10
10/10 - 1s - loss: 292.1139 - loglik: -2.6188e+02 - logprior: -3.0236e+01
Epoch 3/10
10/10 - 1s - loss: 261.0912 - loglik: -2.5027e+02 - logprior: -1.0824e+01
Epoch 4/10
10/10 - 1s - loss: 245.7373 - loglik: -2.4178e+02 - logprior: -3.9606e+00
Epoch 5/10
10/10 - 1s - loss: 237.5977 - loglik: -2.3691e+02 - logprior: -6.9077e-01
Epoch 6/10
10/10 - 1s - loss: 232.6326 - loglik: -2.3380e+02 - logprior: 1.1709
Epoch 7/10
10/10 - 1s - loss: 229.7955 - loglik: -2.3202e+02 - logprior: 2.2198
Epoch 8/10
10/10 - 1s - loss: 228.1252 - loglik: -2.3109e+02 - logprior: 2.9670
Epoch 9/10
10/10 - 1s - loss: 227.0620 - loglik: -2.3061e+02 - logprior: 3.5468
Epoch 10/10
10/10 - 1s - loss: 226.2949 - loglik: -2.3027e+02 - logprior: 3.9727
Fitted a model with MAP estimate = -225.9635
expansions: [(0, 6), (51, 3), (65, 2)]
discards: []
Fitting a model of length 87 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 381.0488 - loglik: -2.2842e+02 - logprior: -1.5263e+02
Epoch 2/2
10/10 - 1s - loss: 270.0835 - loglik: -2.2682e+02 - logprior: -4.3263e+01
Fitted a model with MAP estimate = -249.5774
expansions: [(0, 4)]
discards: [0 1 2 3 4 5]
Fitting a model of length 85 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 346.1979 - loglik: -2.2642e+02 - logprior: -1.1978e+02
Epoch 2/2
10/10 - 1s - loss: 257.1390 - loglik: -2.2624e+02 - logprior: -3.0902e+01
Fitted a model with MAP estimate = -241.9972
expansions: [(0, 4)]
discards: []
Fitting a model of length 89 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 353.0142 - loglik: -2.2546e+02 - logprior: -1.2755e+02
Epoch 2/10
10/10 - 1s - loss: 261.9543 - loglik: -2.2551e+02 - logprior: -3.6448e+01
Epoch 3/10
10/10 - 1s - loss: 237.8252 - loglik: -2.2615e+02 - logprior: -1.1676e+01
Epoch 4/10
10/10 - 1s - loss: 227.9491 - loglik: -2.2670e+02 - logprior: -1.2455e+00
Epoch 5/10
10/10 - 1s - loss: 223.5406 - loglik: -2.2706e+02 - logprior: 3.5185
Epoch 6/10
10/10 - 1s - loss: 221.1569 - loglik: -2.2721e+02 - logprior: 6.0499
Epoch 7/10
10/10 - 1s - loss: 219.6631 - loglik: -2.2724e+02 - logprior: 7.5754
Epoch 8/10
10/10 - 1s - loss: 218.6166 - loglik: -2.2721e+02 - logprior: 8.5894
Epoch 9/10
10/10 - 1s - loss: 217.8171 - loglik: -2.2716e+02 - logprior: 9.3405
Epoch 10/10
10/10 - 1s - loss: 217.1618 - loglik: -2.2713e+02 - logprior: 9.9730
Fitted a model with MAP estimate = -216.8363
Time for alignment: 47.0954
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 399.7772 - loglik: -2.7314e+02 - logprior: -1.2663e+02
Epoch 2/10
10/10 - 1s - loss: 292.1139 - loglik: -2.6188e+02 - logprior: -3.0236e+01
Epoch 3/10
10/10 - 1s - loss: 261.0912 - loglik: -2.5027e+02 - logprior: -1.0824e+01
Epoch 4/10
10/10 - 1s - loss: 245.7373 - loglik: -2.4178e+02 - logprior: -3.9606e+00
Epoch 5/10
10/10 - 1s - loss: 237.5977 - loglik: -2.3691e+02 - logprior: -6.9077e-01
Epoch 6/10
10/10 - 1s - loss: 232.6326 - loglik: -2.3380e+02 - logprior: 1.1709
Epoch 7/10
10/10 - 1s - loss: 229.7955 - loglik: -2.3202e+02 - logprior: 2.2198
Epoch 8/10
10/10 - 1s - loss: 228.1252 - loglik: -2.3109e+02 - logprior: 2.9670
Epoch 9/10
10/10 - 1s - loss: 227.0620 - loglik: -2.3061e+02 - logprior: 3.5468
Epoch 10/10
10/10 - 1s - loss: 226.2949 - loglik: -2.3027e+02 - logprior: 3.9727
Fitted a model with MAP estimate = -225.9634
expansions: [(0, 6), (51, 3), (65, 2)]
discards: []
Fitting a model of length 87 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 381.0488 - loglik: -2.2842e+02 - logprior: -1.5263e+02
Epoch 2/2
10/10 - 1s - loss: 270.0835 - loglik: -2.2682e+02 - logprior: -4.3263e+01
Fitted a model with MAP estimate = -249.5774
expansions: [(0, 4)]
discards: [0 1 2 3 4 5]
Fitting a model of length 85 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 346.1979 - loglik: -2.2642e+02 - logprior: -1.1978e+02
Epoch 2/2
10/10 - 1s - loss: 257.1390 - loglik: -2.2624e+02 - logprior: -3.0902e+01
Fitted a model with MAP estimate = -241.9972
expansions: [(0, 4)]
discards: []
Fitting a model of length 89 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 353.0142 - loglik: -2.2546e+02 - logprior: -1.2755e+02
Epoch 2/10
10/10 - 1s - loss: 261.9543 - loglik: -2.2551e+02 - logprior: -3.6448e+01
Epoch 3/10
10/10 - 1s - loss: 237.8252 - loglik: -2.2615e+02 - logprior: -1.1676e+01
Epoch 4/10
10/10 - 1s - loss: 227.9491 - loglik: -2.2670e+02 - logprior: -1.2455e+00
Epoch 5/10
10/10 - 1s - loss: 223.5406 - loglik: -2.2706e+02 - logprior: 3.5185
Epoch 6/10
10/10 - 1s - loss: 221.1569 - loglik: -2.2721e+02 - logprior: 6.0499
Epoch 7/10
10/10 - 1s - loss: 219.6631 - loglik: -2.2724e+02 - logprior: 7.5754
Epoch 8/10
10/10 - 1s - loss: 218.6166 - loglik: -2.2721e+02 - logprior: 8.5894
Epoch 9/10
10/10 - 1s - loss: 217.8171 - loglik: -2.2716e+02 - logprior: 9.3405
Epoch 10/10
10/10 - 1s - loss: 217.1618 - loglik: -2.2713e+02 - logprior: 9.9730
Fitted a model with MAP estimate = -216.8363
Time for alignment: 45.0146
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 399.7772 - loglik: -2.7314e+02 - logprior: -1.2663e+02
Epoch 2/10
10/10 - 1s - loss: 292.1139 - loglik: -2.6188e+02 - logprior: -3.0236e+01
Epoch 3/10
10/10 - 1s - loss: 261.0912 - loglik: -2.5027e+02 - logprior: -1.0824e+01
Epoch 4/10
10/10 - 1s - loss: 245.7373 - loglik: -2.4178e+02 - logprior: -3.9606e+00
Epoch 5/10
10/10 - 1s - loss: 237.5977 - loglik: -2.3691e+02 - logprior: -6.9077e-01
Epoch 6/10
10/10 - 1s - loss: 232.6326 - loglik: -2.3380e+02 - logprior: 1.1709
Epoch 7/10
10/10 - 1s - loss: 229.7955 - loglik: -2.3202e+02 - logprior: 2.2198
Epoch 8/10
10/10 - 1s - loss: 228.1252 - loglik: -2.3109e+02 - logprior: 2.9670
Epoch 9/10
10/10 - 1s - loss: 227.0620 - loglik: -2.3061e+02 - logprior: 3.5468
Epoch 10/10
10/10 - 1s - loss: 226.2949 - loglik: -2.3027e+02 - logprior: 3.9727
Fitted a model with MAP estimate = -225.9634
expansions: [(0, 6), (51, 3), (65, 2)]
discards: []
Fitting a model of length 87 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 381.0488 - loglik: -2.2842e+02 - logprior: -1.5263e+02
Epoch 2/2
10/10 - 1s - loss: 270.0835 - loglik: -2.2682e+02 - logprior: -4.3263e+01
Fitted a model with MAP estimate = -249.5774
expansions: [(0, 4)]
discards: [0 1 2 3 4 5]
Fitting a model of length 85 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 346.1979 - loglik: -2.2642e+02 - logprior: -1.1978e+02
Epoch 2/2
10/10 - 1s - loss: 257.1390 - loglik: -2.2624e+02 - logprior: -3.0902e+01
Fitted a model with MAP estimate = -241.9972
expansions: [(0, 4)]
discards: []
Fitting a model of length 89 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 353.0142 - loglik: -2.2546e+02 - logprior: -1.2755e+02
Epoch 2/10
10/10 - 1s - loss: 261.9543 - loglik: -2.2551e+02 - logprior: -3.6448e+01
Epoch 3/10
10/10 - 1s - loss: 237.8252 - loglik: -2.2615e+02 - logprior: -1.1676e+01
Epoch 4/10
10/10 - 1s - loss: 227.9491 - loglik: -2.2670e+02 - logprior: -1.2455e+00
Epoch 5/10
10/10 - 1s - loss: 223.5406 - loglik: -2.2706e+02 - logprior: 3.5185
Epoch 6/10
10/10 - 1s - loss: 221.1569 - loglik: -2.2721e+02 - logprior: 6.0499
Epoch 7/10
10/10 - 1s - loss: 219.6631 - loglik: -2.2724e+02 - logprior: 7.5754
Epoch 8/10
10/10 - 1s - loss: 218.6166 - loglik: -2.2721e+02 - logprior: 8.5894
Epoch 9/10
10/10 - 1s - loss: 217.8171 - loglik: -2.2716e+02 - logprior: 9.3405
Epoch 10/10
10/10 - 1s - loss: 217.1618 - loglik: -2.2713e+02 - logprior: 9.9730
Fitted a model with MAP estimate = -216.8363
Time for alignment: 44.3242
Computed alignments with likelihoods: ['-216.8363', '-216.8363', '-216.8363']
Best model has likelihood: -216.8363
SP score = 0.7078
Training of 3 independent models on file mofe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f21074c4cd0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f17aa6cba30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20f5c4d7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1799fb2580>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 23s - loss: 1004.9366 - loglik: -9.9202e+02 - logprior: -1.2921e+01
Epoch 2/10
19/19 - 21s - loss: 883.2837 - loglik: -8.8436e+02 - logprior: 1.0784
Epoch 3/10
19/19 - 21s - loss: 820.9304 - loglik: -8.2187e+02 - logprior: 0.9349
Epoch 4/10
19/19 - 21s - loss: 809.2923 - loglik: -8.1024e+02 - logprior: 0.9521
Epoch 5/10
19/19 - 20s - loss: 803.7521 - loglik: -8.0481e+02 - logprior: 1.0621
Epoch 6/10
19/19 - 21s - loss: 800.5851 - loglik: -8.0155e+02 - logprior: 0.9602
Epoch 7/10
19/19 - 21s - loss: 801.0511 - loglik: -8.0193e+02 - logprior: 0.8824
Fitted a model with MAP estimate = -799.4979
expansions: [(33, 1), (40, 2), (68, 1), (71, 1), (96, 3), (117, 4), (120, 5), (167, 11), (169, 1), (173, 1), (174, 3), (189, 4), (196, 1), (203, 1), (211, 1), (221, 1), (222, 2), (223, 1), (226, 1), (241, 1), (242, 1), (264, 9), (301, 1), (302, 3), (311, 3)]
discards: [0]
Fitting a model of length 382 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 32s - loss: 817.0349 - loglik: -8.0495e+02 - logprior: -1.2084e+01
Epoch 2/2
19/19 - 25s - loss: 780.3096 - loglik: -7.7727e+02 - logprior: -3.0376e+00
Fitted a model with MAP estimate = -769.3390
expansions: [(0, 2), (101, 1), (126, 1), (133, 1), (206, 1), (376, 1)]
discards: [  0  41 186 187 188 189 190 191 314 358]
Fitting a model of length 379 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 29s - loss: 773.4577 - loglik: -7.6714e+02 - logprior: -6.3159e+00
Epoch 2/2
19/19 - 25s - loss: 754.8369 - loglik: -7.5739e+02 - logprior: 2.5493
Fitted a model with MAP estimate = -751.4823
expansions: [(99, 1), (138, 1), (141, 2)]
discards: [0]
Fitting a model of length 382 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 29s - loss: 771.6918 - loglik: -7.6250e+02 - logprior: -9.1871e+00
Epoch 2/10
19/19 - 25s - loss: 755.3867 - loglik: -7.5638e+02 - logprior: 0.9933
Epoch 3/10
19/19 - 25s - loss: 747.4233 - loglik: -7.5173e+02 - logprior: 4.3051
Epoch 4/10
19/19 - 26s - loss: 748.4303 - loglik: -7.5338e+02 - logprior: 4.9514
Fitted a model with MAP estimate = -743.1646
Time for alignment: 429.8286
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 25s - loss: 1003.7811 - loglik: -9.9087e+02 - logprior: -1.2910e+01
Epoch 2/10
19/19 - 20s - loss: 876.3633 - loglik: -8.7686e+02 - logprior: 0.4921
Epoch 3/10
19/19 - 20s - loss: 815.0522 - loglik: -8.1472e+02 - logprior: -3.3707e-01
Epoch 4/10
19/19 - 20s - loss: 798.5351 - loglik: -7.9786e+02 - logprior: -6.7434e-01
Epoch 5/10
19/19 - 20s - loss: 792.3666 - loglik: -7.9174e+02 - logprior: -6.2284e-01
Epoch 6/10
19/19 - 21s - loss: 793.1762 - loglik: -7.9214e+02 - logprior: -1.0406e+00
Fitted a model with MAP estimate = -790.4597
expansions: [(14, 1), (29, 1), (57, 1), (68, 1), (98, 1), (99, 3), (107, 1), (113, 1), (114, 1), (117, 1), (118, 1), (119, 1), (121, 3), (123, 2), (125, 2), (143, 1), (145, 1), (158, 1), (165, 1), (167, 1), (168, 1), (169, 1), (170, 1), (173, 1), (176, 1), (177, 1), (178, 1), (190, 2), (191, 2), (199, 1), (202, 1), (205, 1), (212, 1), (220, 1), (221, 1), (222, 2), (224, 1), (259, 1), (262, 1), (273, 1), (282, 1), (293, 1), (296, 1), (302, 2), (303, 2), (304, 1), (311, 3)]
discards: [0]
Fitting a model of length 379 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 28s - loss: 804.1623 - loglik: -7.9184e+02 - logprior: -1.2324e+01
Epoch 2/2
19/19 - 25s - loss: 771.6885 - loglik: -7.6936e+02 - logprior: -2.3268e+00
Fitted a model with MAP estimate = -765.4901
expansions: [(0, 3), (286, 2), (310, 2), (373, 1)]
discards: [  0  29 103 104 225]
Fitting a model of length 382 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 29s - loss: 774.4872 - loglik: -7.6785e+02 - logprior: -6.6340e+00
Epoch 2/2
19/19 - 25s - loss: 758.5398 - loglik: -7.6071e+02 - logprior: 2.1729
Fitted a model with MAP estimate = -754.4529
expansions: [(311, 1), (313, 1)]
discards: [  0   1 144]
Fitting a model of length 381 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 30s - loss: 775.8992 - loglik: -7.6647e+02 - logprior: -9.4298e+00
Epoch 2/10
19/19 - 25s - loss: 759.4421 - loglik: -7.5965e+02 - logprior: 0.2106
Epoch 3/10
19/19 - 25s - loss: 754.7756 - loglik: -7.5857e+02 - logprior: 3.7896
Epoch 4/10
19/19 - 25s - loss: 749.1219 - loglik: -7.5318e+02 - logprior: 4.0559
Epoch 5/10
19/19 - 26s - loss: 746.1008 - loglik: -7.5052e+02 - logprior: 4.4169
Epoch 6/10
19/19 - 25s - loss: 746.0103 - loglik: -7.5089e+02 - logprior: 4.8793
Epoch 7/10
19/19 - 26s - loss: 745.1255 - loglik: -7.5029e+02 - logprior: 5.1635
Epoch 8/10
19/19 - 25s - loss: 743.7816 - loglik: -7.4925e+02 - logprior: 5.4691
Epoch 9/10
19/19 - 25s - loss: 742.3383 - loglik: -7.4798e+02 - logprior: 5.6369
Epoch 10/10
19/19 - 26s - loss: 743.8754 - loglik: -7.4971e+02 - logprior: 5.8384
Fitted a model with MAP estimate = -741.9520
Time for alignment: 560.8116
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 24s - loss: 1005.2724 - loglik: -9.9235e+02 - logprior: -1.2918e+01
Epoch 2/10
19/19 - 20s - loss: 874.4295 - loglik: -8.7507e+02 - logprior: 0.6447
Epoch 3/10
19/19 - 20s - loss: 813.8413 - loglik: -8.1375e+02 - logprior: -8.8222e-02
Epoch 4/10
19/19 - 20s - loss: 801.2072 - loglik: -8.0083e+02 - logprior: -3.7813e-01
Epoch 5/10
19/19 - 20s - loss: 791.4745 - loglik: -7.9109e+02 - logprior: -3.8426e-01
Epoch 6/10
19/19 - 20s - loss: 795.4589 - loglik: -7.9501e+02 - logprior: -4.4423e-01
Fitted a model with MAP estimate = -790.6821
expansions: [(14, 1), (40, 1), (68, 1), (115, 1), (116, 2), (120, 1), (121, 1), (122, 1), (126, 2), (127, 3), (145, 1), (147, 1), (160, 1), (166, 2), (168, 1), (169, 1), (170, 2), (171, 1), (174, 1), (177, 2), (178, 1), (179, 1), (191, 2), (192, 2), (199, 1), (200, 1), (202, 1), (213, 1), (215, 1), (220, 1), (221, 2), (223, 1), (224, 2), (226, 1), (239, 1), (240, 1), (241, 1), (263, 1), (264, 4), (294, 3), (301, 6), (303, 1)]
discards: [0]
Fitting a model of length 382 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 28s - loss: 806.9310 - loglik: -7.9471e+02 - logprior: -1.2217e+01
Epoch 2/2
19/19 - 25s - loss: 771.5116 - loglik: -7.6887e+02 - logprior: -2.6388e+00
Fitted a model with MAP estimate = -766.1664
expansions: [(0, 2), (132, 5), (313, 1), (314, 1)]
discards: [  0 119 190 201 347 348]
Fitting a model of length 385 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 30s - loss: 776.3012 - loglik: -7.6979e+02 - logprior: -6.5092e+00
Epoch 2/2
19/19 - 26s - loss: 754.7648 - loglik: -7.5737e+02 - logprior: 2.6080
Fitted a model with MAP estimate = -754.9384
expansions: []
discards: [  0 135 144 145 225]
Fitting a model of length 380 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 29s - loss: 777.8525 - loglik: -7.6845e+02 - logprior: -9.4073e+00
Epoch 2/10
19/19 - 25s - loss: 760.1537 - loglik: -7.6101e+02 - logprior: 0.8585
Epoch 3/10
19/19 - 25s - loss: 755.1942 - loglik: -7.5919e+02 - logprior: 3.9949
Epoch 4/10
19/19 - 25s - loss: 752.4111 - loglik: -7.5686e+02 - logprior: 4.4504
Epoch 5/10
19/19 - 25s - loss: 750.4126 - loglik: -7.5521e+02 - logprior: 4.8023
Epoch 6/10
19/19 - 25s - loss: 747.1297 - loglik: -7.5221e+02 - logprior: 5.0808
Epoch 7/10
19/19 - 25s - loss: 746.8058 - loglik: -7.5225e+02 - logprior: 5.4429
Epoch 8/10
19/19 - 25s - loss: 746.8726 - loglik: -7.5258e+02 - logprior: 5.7066
Fitted a model with MAP estimate = -746.1459
Time for alignment: 507.7087
Computed alignments with likelihoods: ['-743.1646', '-741.9520', '-746.1459']
Best model has likelihood: -741.9520
SP score = 0.6683
Training of 3 independent models on file ghf22.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1799edab80>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1799eda9a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202d0a21f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202d0a2eb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 377.7294 - loglik: -3.1284e+02 - logprior: -6.4887e+01
Epoch 2/10
10/10 - 1s - loss: 291.4710 - loglik: -2.7623e+02 - logprior: -1.5245e+01
Epoch 3/10
10/10 - 1s - loss: 240.5105 - loglik: -2.3395e+02 - logprior: -6.5595e+00
Epoch 4/10
10/10 - 1s - loss: 212.4048 - loglik: -2.0836e+02 - logprior: -4.0400e+00
Epoch 5/10
10/10 - 1s - loss: 202.4490 - loglik: -1.9965e+02 - logprior: -2.7981e+00
Epoch 6/10
10/10 - 1s - loss: 197.6280 - loglik: -1.9540e+02 - logprior: -2.2249e+00
Epoch 7/10
10/10 - 1s - loss: 196.0143 - loglik: -1.9424e+02 - logprior: -1.7775e+00
Epoch 8/10
10/10 - 1s - loss: 194.9686 - loglik: -1.9355e+02 - logprior: -1.4173e+00
Epoch 9/10
10/10 - 1s - loss: 194.7404 - loglik: -1.9361e+02 - logprior: -1.1332e+00
Epoch 10/10
10/10 - 1s - loss: 193.6721 - loglik: -1.9273e+02 - logprior: -9.3929e-01
Fitted a model with MAP estimate = -193.9282
expansions: [(13, 4), (17, 2), (31, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (60, 1), (62, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 252.9108 - loglik: -1.8684e+02 - logprior: -6.6074e+01
Epoch 2/2
10/10 - 2s - loss: 200.0128 - loglik: -1.7475e+02 - logprior: -2.5265e+01
Fitted a model with MAP estimate = -190.8027
expansions: [(0, 2), (14, 1), (69, 1)]
discards: [ 0 20 21 98]
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 221.6937 - loglik: -1.7132e+02 - logprior: -5.0372e+01
Epoch 2/2
10/10 - 1s - loss: 179.0446 - loglik: -1.6797e+02 - logprior: -1.1073e+01
Fitted a model with MAP estimate = -172.7407
expansions: []
discards: [0]
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 230.1711 - loglik: -1.6976e+02 - logprior: -6.0415e+01
Epoch 2/10
10/10 - 1s - loss: 186.4326 - loglik: -1.6930e+02 - logprior: -1.7129e+01
Epoch 3/10
10/10 - 1s - loss: 172.6228 - loglik: -1.6851e+02 - logprior: -4.1162e+00
Epoch 4/10
10/10 - 1s - loss: 168.0641 - loglik: -1.6850e+02 - logprior: 0.4338
Epoch 5/10
10/10 - 1s - loss: 166.1766 - loglik: -1.6860e+02 - logprior: 2.4197
Epoch 6/10
10/10 - 1s - loss: 164.8538 - loglik: -1.6833e+02 - logprior: 3.4762
Epoch 7/10
10/10 - 1s - loss: 164.4472 - loglik: -1.6863e+02 - logprior: 4.1835
Epoch 8/10
10/10 - 1s - loss: 163.8366 - loglik: -1.6859e+02 - logprior: 4.7578
Epoch 9/10
10/10 - 1s - loss: 163.2943 - loglik: -1.6854e+02 - logprior: 5.2445
Epoch 10/10
10/10 - 2s - loss: 163.3041 - loglik: -1.6894e+02 - logprior: 5.6376
Fitted a model with MAP estimate = -162.9874
Time for alignment: 51.2206
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 377.7498 - loglik: -3.1286e+02 - logprior: -6.4888e+01
Epoch 2/10
10/10 - 1s - loss: 291.8723 - loglik: -2.7663e+02 - logprior: -1.5242e+01
Epoch 3/10
10/10 - 1s - loss: 242.0715 - loglik: -2.3553e+02 - logprior: -6.5418e+00
Epoch 4/10
10/10 - 1s - loss: 213.3702 - loglik: -2.0920e+02 - logprior: -4.1694e+00
Epoch 5/10
10/10 - 1s - loss: 202.7187 - loglik: -1.9973e+02 - logprior: -2.9875e+00
Epoch 6/10
10/10 - 1s - loss: 198.4176 - loglik: -1.9619e+02 - logprior: -2.2255e+00
Epoch 7/10
10/10 - 1s - loss: 195.9935 - loglik: -1.9423e+02 - logprior: -1.7611e+00
Epoch 8/10
10/10 - 1s - loss: 195.2497 - loglik: -1.9375e+02 - logprior: -1.5031e+00
Epoch 9/10
10/10 - 1s - loss: 194.2707 - loglik: -1.9298e+02 - logprior: -1.2889e+00
Epoch 10/10
10/10 - 1s - loss: 193.8356 - loglik: -1.9276e+02 - logprior: -1.0772e+00
Fitted a model with MAP estimate = -193.7977
expansions: [(11, 4), (25, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (60, 1), (62, 1), (78, 4), (81, 1), (86, 1), (87, 1), (90, 1), (91, 2)]
discards: [0]
Fitting a model of length 122 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 253.4376 - loglik: -1.8745e+02 - logprior: -6.5989e+01
Epoch 2/2
10/10 - 1s - loss: 201.1982 - loglik: -1.7599e+02 - logprior: -2.5208e+01
Fitted a model with MAP estimate = -192.2384
expansions: [(0, 2), (12, 1), (67, 1)]
discards: [ 0 96]
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 221.6402 - loglik: -1.7130e+02 - logprior: -5.0342e+01
Epoch 2/2
10/10 - 1s - loss: 179.0463 - loglik: -1.6805e+02 - logprior: -1.1001e+01
Fitted a model with MAP estimate = -173.0085
expansions: []
discards: [0]
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 230.4899 - loglik: -1.7018e+02 - logprior: -6.0306e+01
Epoch 2/10
10/10 - 1s - loss: 186.3505 - loglik: -1.6939e+02 - logprior: -1.6958e+01
Epoch 3/10
10/10 - 1s - loss: 173.0468 - loglik: -1.6903e+02 - logprior: -4.0162e+00
Epoch 4/10
10/10 - 2s - loss: 168.3461 - loglik: -1.6883e+02 - logprior: 0.4822
Epoch 5/10
10/10 - 1s - loss: 166.1903 - loglik: -1.6861e+02 - logprior: 2.4229
Epoch 6/10
10/10 - 1s - loss: 165.3424 - loglik: -1.6881e+02 - logprior: 3.4645
Epoch 7/10
10/10 - 1s - loss: 164.8020 - loglik: -1.6898e+02 - logprior: 4.1754
Epoch 8/10
10/10 - 2s - loss: 163.9939 - loglik: -1.6874e+02 - logprior: 4.7481
Epoch 9/10
10/10 - 2s - loss: 164.0609 - loglik: -1.6929e+02 - logprior: 5.2328
Fitted a model with MAP estimate = -163.6247
Time for alignment: 48.7587
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 377.2984 - loglik: -3.1241e+02 - logprior: -6.4889e+01
Epoch 2/10
10/10 - 1s - loss: 292.4273 - loglik: -2.7718e+02 - logprior: -1.5248e+01
Epoch 3/10
10/10 - 1s - loss: 241.0755 - loglik: -2.3455e+02 - logprior: -6.5247e+00
Epoch 4/10
10/10 - 1s - loss: 214.5123 - loglik: -2.1052e+02 - logprior: -3.9946e+00
Epoch 5/10
10/10 - 1s - loss: 204.0674 - loglik: -2.0145e+02 - logprior: -2.6146e+00
Epoch 6/10
10/10 - 1s - loss: 198.2531 - loglik: -1.9622e+02 - logprior: -2.0350e+00
Epoch 7/10
10/10 - 1s - loss: 195.9219 - loglik: -1.9424e+02 - logprior: -1.6849e+00
Epoch 8/10
10/10 - 1s - loss: 195.1054 - loglik: -1.9369e+02 - logprior: -1.4162e+00
Epoch 9/10
10/10 - 1s - loss: 194.2938 - loglik: -1.9312e+02 - logprior: -1.1704e+00
Epoch 10/10
10/10 - 1s - loss: 194.1934 - loglik: -1.9322e+02 - logprior: -9.7307e-01
Fitted a model with MAP estimate = -193.7889
expansions: [(13, 5), (17, 2), (31, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (60, 1), (62, 1), (78, 4), (81, 1), (86, 1), (87, 1), (90, 1), (91, 2)]
discards: [0]
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 252.3014 - loglik: -1.8631e+02 - logprior: -6.5990e+01
Epoch 2/2
10/10 - 2s - loss: 198.5741 - loglik: -1.7355e+02 - logprior: -2.5026e+01
Fitted a model with MAP estimate = -189.5415
expansions: [(0, 2)]
discards: [ 0 21 22 99]
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 221.0081 - loglik: -1.7081e+02 - logprior: -5.0195e+01
Epoch 2/2
10/10 - 1s - loss: 179.2365 - loglik: -1.6814e+02 - logprior: -1.1100e+01
Fitted a model with MAP estimate = -173.4112
expansions: [(69, 1)]
discards: [0]
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 231.6113 - loglik: -1.7068e+02 - logprior: -6.0931e+01
Epoch 2/10
10/10 - 2s - loss: 187.8418 - loglik: -1.6952e+02 - logprior: -1.8326e+01
Epoch 3/10
10/10 - 1s - loss: 173.7747 - loglik: -1.6918e+02 - logprior: -4.5929e+00
Epoch 4/10
10/10 - 1s - loss: 168.5757 - loglik: -1.6905e+02 - logprior: 0.4789
Epoch 5/10
10/10 - 2s - loss: 166.2156 - loglik: -1.6874e+02 - logprior: 2.5231
Epoch 6/10
10/10 - 2s - loss: 164.8478 - loglik: -1.6844e+02 - logprior: 3.5952
Epoch 7/10
10/10 - 2s - loss: 164.5710 - loglik: -1.6885e+02 - logprior: 4.2814
Epoch 8/10
10/10 - 2s - loss: 164.0575 - loglik: -1.6891e+02 - logprior: 4.8548
Epoch 9/10
10/10 - 2s - loss: 163.8694 - loglik: -1.6922e+02 - logprior: 5.3511
Epoch 10/10
10/10 - 2s - loss: 162.9461 - loglik: -1.6870e+02 - logprior: 5.7512
Fitted a model with MAP estimate = -163.1122
Time for alignment: 50.8987
Computed alignments with likelihoods: ['-162.9874', '-163.6247', '-163.1122']
Best model has likelihood: -162.9874
SP score = 0.9194
Training of 3 independent models on file flav.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20f5eeba90>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f202e408ac0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20a14dfa00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20a14dfcd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 398.4088 - loglik: -3.8899e+02 - logprior: -9.4206e+00
Epoch 2/10
13/13 - 3s - loss: 354.2314 - loglik: -3.5220e+02 - logprior: -2.0303e+00
Epoch 3/10
13/13 - 3s - loss: 325.9322 - loglik: -3.2434e+02 - logprior: -1.5873e+00
Epoch 4/10
13/13 - 3s - loss: 312.6455 - loglik: -3.1088e+02 - logprior: -1.7679e+00
Epoch 5/10
13/13 - 3s - loss: 308.6279 - loglik: -3.0690e+02 - logprior: -1.7296e+00
Epoch 6/10
13/13 - 3s - loss: 306.4000 - loglik: -3.0471e+02 - logprior: -1.6871e+00
Epoch 7/10
13/13 - 3s - loss: 305.8271 - loglik: -3.0406e+02 - logprior: -1.7720e+00
Epoch 8/10
13/13 - 3s - loss: 305.4492 - loglik: -3.0357e+02 - logprior: -1.8802e+00
Epoch 9/10
13/13 - 3s - loss: 304.7884 - loglik: -3.0285e+02 - logprior: -1.9337e+00
Epoch 10/10
13/13 - 3s - loss: 304.4751 - loglik: -3.0251e+02 - logprior: -1.9680e+00
Fitted a model with MAP estimate = -304.3552
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (33, 1), (51, 1), (52, 1), (55, 1), (58, 2), (64, 2), (80, 1), (81, 1), (82, 2), (83, 1), (93, 1), (99, 3)]
discards: [0]
Fitting a model of length 135 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 317.7488 - loglik: -3.0787e+02 - logprior: -9.8814e+00
Epoch 2/2
13/13 - 3s - loss: 300.2418 - loglik: -2.9593e+02 - logprior: -4.3084e+00
Fitted a model with MAP estimate = -297.5732
expansions: [(0, 2)]
discards: [  0  36  80 100]
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 301.6771 - loglik: -2.9456e+02 - logprior: -7.1208e+00
Epoch 2/2
13/13 - 3s - loss: 293.9373 - loglik: -2.9220e+02 - logprior: -1.7362e+00
Fitted a model with MAP estimate = -292.2609
expansions: []
discards: [ 0 73]
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 304.3549 - loglik: -2.9560e+02 - logprior: -8.7559e+00
Epoch 2/10
13/13 - 3s - loss: 295.3632 - loglik: -2.9292e+02 - logprior: -2.4462e+00
Epoch 3/10
13/13 - 3s - loss: 292.5327 - loglik: -2.9152e+02 - logprior: -1.0135e+00
Epoch 4/10
13/13 - 3s - loss: 291.1395 - loglik: -2.9048e+02 - logprior: -6.6152e-01
Epoch 5/10
13/13 - 3s - loss: 290.2602 - loglik: -2.8976e+02 - logprior: -5.0092e-01
Epoch 6/10
13/13 - 3s - loss: 289.7524 - loglik: -2.8930e+02 - logprior: -4.4912e-01
Epoch 7/10
13/13 - 3s - loss: 289.4954 - loglik: -2.8906e+02 - logprior: -4.3474e-01
Epoch 8/10
13/13 - 3s - loss: 289.1331 - loglik: -2.8869e+02 - logprior: -4.4497e-01
Epoch 9/10
13/13 - 3s - loss: 288.9888 - loglik: -2.8855e+02 - logprior: -4.3757e-01
Epoch 10/10
13/13 - 3s - loss: 288.7827 - loglik: -2.8828e+02 - logprior: -5.0107e-01
Fitted a model with MAP estimate = -288.6615
Time for alignment: 100.6457
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 398.3058 - loglik: -3.8889e+02 - logprior: -9.4147e+00
Epoch 2/10
13/13 - 3s - loss: 355.6618 - loglik: -3.5364e+02 - logprior: -2.0246e+00
Epoch 3/10
13/13 - 3s - loss: 328.3007 - loglik: -3.2673e+02 - logprior: -1.5675e+00
Epoch 4/10
13/13 - 3s - loss: 314.6817 - loglik: -3.1291e+02 - logprior: -1.7746e+00
Epoch 5/10
13/13 - 3s - loss: 308.2875 - loglik: -3.0655e+02 - logprior: -1.7380e+00
Epoch 6/10
13/13 - 3s - loss: 305.3214 - loglik: -3.0362e+02 - logprior: -1.7034e+00
Epoch 7/10
13/13 - 3s - loss: 305.3502 - loglik: -3.0361e+02 - logprior: -1.7442e+00
Fitted a model with MAP estimate = -304.5698
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 2), (26, 2), (27, 1), (28, 1), (33, 1), (51, 1), (52, 1), (55, 1), (59, 3), (64, 2), (80, 1), (81, 1), (82, 2), (83, 2), (99, 2)]
discards: [0]
Fitting a model of length 136 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 317.6937 - loglik: -3.0789e+02 - logprior: -9.8052e+00
Epoch 2/2
13/13 - 3s - loss: 300.5040 - loglik: -2.9625e+02 - logprior: -4.2587e+00
Fitted a model with MAP estimate = -297.3221
expansions: [(0, 2), (126, 1)]
discards: [  0  31  37  75  82 102]
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 301.6431 - loglik: -2.9454e+02 - logprior: -7.1023e+00
Epoch 2/2
13/13 - 3s - loss: 293.5004 - loglik: -2.9175e+02 - logprior: -1.7489e+00
Fitted a model with MAP estimate = -291.7928
expansions: []
discards: [0]
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 303.4478 - loglik: -2.9465e+02 - logprior: -8.7961e+00
Epoch 2/10
13/13 - 3s - loss: 294.7787 - loglik: -2.9229e+02 - logprior: -2.4846e+00
Epoch 3/10
13/13 - 3s - loss: 292.1978 - loglik: -2.9117e+02 - logprior: -1.0323e+00
Epoch 4/10
13/13 - 3s - loss: 290.0260 - loglik: -2.8933e+02 - logprior: -6.9917e-01
Epoch 5/10
13/13 - 3s - loss: 289.0932 - loglik: -2.8856e+02 - logprior: -5.2981e-01
Epoch 6/10
13/13 - 3s - loss: 288.8275 - loglik: -2.8834e+02 - logprior: -4.8943e-01
Epoch 7/10
13/13 - 3s - loss: 288.4323 - loglik: -2.8795e+02 - logprior: -4.8024e-01
Epoch 8/10
13/13 - 3s - loss: 287.9449 - loglik: -2.8746e+02 - logprior: -4.8404e-01
Epoch 9/10
13/13 - 3s - loss: 288.0298 - loglik: -2.8755e+02 - logprior: -4.7675e-01
Fitted a model with MAP estimate = -287.7483
Time for alignment: 87.1622
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 398.3628 - loglik: -3.8894e+02 - logprior: -9.4188e+00
Epoch 2/10
13/13 - 3s - loss: 355.2878 - loglik: -3.5326e+02 - logprior: -2.0303e+00
Epoch 3/10
13/13 - 3s - loss: 326.2348 - loglik: -3.2462e+02 - logprior: -1.6173e+00
Epoch 4/10
13/13 - 3s - loss: 313.3111 - loglik: -3.1152e+02 - logprior: -1.7948e+00
Epoch 5/10
13/13 - 3s - loss: 309.0523 - loglik: -3.0739e+02 - logprior: -1.6611e+00
Epoch 6/10
13/13 - 3s - loss: 306.4795 - loglik: -3.0488e+02 - logprior: -1.6044e+00
Epoch 7/10
13/13 - 3s - loss: 305.6817 - loglik: -3.0401e+02 - logprior: -1.6728e+00
Epoch 8/10
13/13 - 3s - loss: 305.1794 - loglik: -3.0344e+02 - logprior: -1.7440e+00
Epoch 9/10
13/13 - 3s - loss: 304.8928 - loglik: -3.0317e+02 - logprior: -1.7234e+00
Epoch 10/10
13/13 - 3s - loss: 304.4379 - loglik: -3.0265e+02 - logprior: -1.7860e+00
Fitted a model with MAP estimate = -304.3695
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (33, 1), (51, 2), (52, 1), (55, 1), (57, 1), (64, 2), (80, 1), (81, 4), (82, 1), (100, 2), (101, 2)]
discards: [0]
Fitting a model of length 136 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 318.0945 - loglik: -3.0824e+02 - logprior: -9.8513e+00
Epoch 2/2
13/13 - 3s - loss: 300.9897 - loglik: -2.9671e+02 - logprior: -4.2821e+00
Fitted a model with MAP estimate = -297.8582
expansions: [(0, 2)]
discards: [  0  36  62  80 102]
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 302.0871 - loglik: -2.9500e+02 - logprior: -7.0854e+00
Epoch 2/2
13/13 - 3s - loss: 293.4314 - loglik: -2.9173e+02 - logprior: -1.7023e+00
Fitted a model with MAP estimate = -292.2679
expansions: [(122, 1)]
discards: [0]
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 304.3331 - loglik: -2.9545e+02 - logprior: -8.8798e+00
Epoch 2/10
13/13 - 3s - loss: 296.0594 - loglik: -2.9349e+02 - logprior: -2.5648e+00
Epoch 3/10
13/13 - 3s - loss: 291.9819 - loglik: -2.9101e+02 - logprior: -9.7663e-01
Epoch 4/10
13/13 - 3s - loss: 291.1594 - loglik: -2.9050e+02 - logprior: -6.5891e-01
Epoch 5/10
13/13 - 3s - loss: 290.2922 - loglik: -2.8982e+02 - logprior: -4.7672e-01
Epoch 6/10
13/13 - 3s - loss: 289.2326 - loglik: -2.8878e+02 - logprior: -4.4929e-01
Epoch 7/10
13/13 - 3s - loss: 288.6252 - loglik: -2.8817e+02 - logprior: -4.5405e-01
Epoch 8/10
13/13 - 3s - loss: 288.9206 - loglik: -2.8847e+02 - logprior: -4.5413e-01
Fitted a model with MAP estimate = -288.4800
Time for alignment: 91.3376
Computed alignments with likelihoods: ['-288.6615', '-287.7483', '-288.4800']
Best model has likelihood: -287.7483
SP score = 0.8598
Training of 3 independent models on file sodfe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20a100a4f0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2098960e20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2011a58f40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2011a58400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 223.3430 - loglik: -2.1391e+02 - logprior: -9.4321e+00
Epoch 2/10
13/13 - 2s - loss: 175.6133 - loglik: -1.7323e+02 - logprior: -2.3795e+00
Epoch 3/10
13/13 - 2s - loss: 146.3776 - loglik: -1.4445e+02 - logprior: -1.9250e+00
Epoch 4/10
13/13 - 2s - loss: 136.7068 - loglik: -1.3489e+02 - logprior: -1.8184e+00
Epoch 5/10
13/13 - 2s - loss: 133.9552 - loglik: -1.3228e+02 - logprior: -1.6767e+00
Epoch 6/10
13/13 - 2s - loss: 133.2708 - loglik: -1.3162e+02 - logprior: -1.6556e+00
Epoch 7/10
13/13 - 2s - loss: 132.6415 - loglik: -1.3102e+02 - logprior: -1.6193e+00
Epoch 8/10
13/13 - 2s - loss: 132.2154 - loglik: -1.3057e+02 - logprior: -1.6443e+00
Epoch 9/10
13/13 - 2s - loss: 131.9974 - loglik: -1.3039e+02 - logprior: -1.6039e+00
Epoch 10/10
13/13 - 2s - loss: 132.2830 - loglik: -1.3067e+02 - logprior: -1.6173e+00
Fitted a model with MAP estimate = -132.0280
expansions: [(0, 4), (13, 1), (16, 1), (33, 1), (35, 2), (36, 1), (37, 2), (38, 2), (43, 3), (44, 2)]
discards: []
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 138.6508 - loglik: -1.2816e+02 - logprior: -1.0487e+01
Epoch 2/2
13/13 - 2s - loss: 122.0840 - loglik: -1.1894e+02 - logprior: -3.1448e+00
Fitted a model with MAP estimate = -119.1765
expansions: [(0, 2)]
discards: [43 47 50]
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 127.8672 - loglik: -1.1778e+02 - logprior: -1.0091e+01
Epoch 2/2
13/13 - 2s - loss: 118.0639 - loglik: -1.1486e+02 - logprior: -3.2035e+00
Fitted a model with MAP estimate = -116.6065
expansions: []
discards: []
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 122.8761 - loglik: -1.1493e+02 - logprior: -7.9417e+00
Epoch 2/10
13/13 - 2s - loss: 116.5231 - loglik: -1.1428e+02 - logprior: -2.2434e+00
Epoch 3/10
13/13 - 2s - loss: 115.6680 - loglik: -1.1397e+02 - logprior: -1.6947e+00
Epoch 4/10
13/13 - 2s - loss: 114.8430 - loglik: -1.1350e+02 - logprior: -1.3457e+00
Epoch 5/10
13/13 - 2s - loss: 114.7278 - loglik: -1.1346e+02 - logprior: -1.2690e+00
Epoch 6/10
13/13 - 2s - loss: 114.5581 - loglik: -1.1333e+02 - logprior: -1.2254e+00
Epoch 7/10
13/13 - 2s - loss: 114.5108 - loglik: -1.1329e+02 - logprior: -1.2178e+00
Epoch 8/10
13/13 - 2s - loss: 114.2441 - loglik: -1.1306e+02 - logprior: -1.1799e+00
Epoch 9/10
13/13 - 2s - loss: 114.2902 - loglik: -1.1313e+02 - logprior: -1.1624e+00
Fitted a model with MAP estimate = -114.2030
Time for alignment: 66.7466
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 223.6665 - loglik: -2.1423e+02 - logprior: -9.4354e+00
Epoch 2/10
13/13 - 2s - loss: 173.1798 - loglik: -1.7080e+02 - logprior: -2.3784e+00
Epoch 3/10
13/13 - 2s - loss: 143.5848 - loglik: -1.4168e+02 - logprior: -1.9044e+00
Epoch 4/10
13/13 - 2s - loss: 136.3306 - loglik: -1.3453e+02 - logprior: -1.8010e+00
Epoch 5/10
13/13 - 2s - loss: 133.6628 - loglik: -1.3199e+02 - logprior: -1.6680e+00
Epoch 6/10
13/13 - 2s - loss: 133.2326 - loglik: -1.3158e+02 - logprior: -1.6496e+00
Epoch 7/10
13/13 - 2s - loss: 132.5336 - loglik: -1.3091e+02 - logprior: -1.6268e+00
Epoch 8/10
13/13 - 2s - loss: 132.3216 - loglik: -1.3069e+02 - logprior: -1.6314e+00
Epoch 9/10
13/13 - 2s - loss: 132.1412 - loglik: -1.3052e+02 - logprior: -1.6239e+00
Epoch 10/10
13/13 - 2s - loss: 132.0764 - loglik: -1.3046e+02 - logprior: -1.6132e+00
Fitted a model with MAP estimate = -131.9846
expansions: [(0, 4), (13, 1), (16, 1), (33, 1), (35, 2), (36, 1), (37, 2), (38, 2), (43, 3), (44, 2)]
discards: []
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 138.3801 - loglik: -1.2786e+02 - logprior: -1.0516e+01
Epoch 2/2
13/13 - 2s - loss: 122.2893 - loglik: -1.1913e+02 - logprior: -3.1554e+00
Fitted a model with MAP estimate = -119.0859
expansions: [(0, 2)]
discards: [43 47 50]
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 127.4599 - loglik: -1.1738e+02 - logprior: -1.0085e+01
Epoch 2/2
13/13 - 2s - loss: 118.4025 - loglik: -1.1519e+02 - logprior: -3.2164e+00
Fitted a model with MAP estimate = -116.6236
expansions: []
discards: []
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 122.8744 - loglik: -1.1493e+02 - logprior: -7.9438e+00
Epoch 2/10
13/13 - 2s - loss: 116.5484 - loglik: -1.1429e+02 - logprior: -2.2571e+00
Epoch 3/10
13/13 - 2s - loss: 115.8038 - loglik: -1.1410e+02 - logprior: -1.7003e+00
Epoch 4/10
13/13 - 2s - loss: 114.7972 - loglik: -1.1345e+02 - logprior: -1.3519e+00
Epoch 5/10
13/13 - 2s - loss: 114.6599 - loglik: -1.1338e+02 - logprior: -1.2832e+00
Epoch 6/10
13/13 - 2s - loss: 114.5658 - loglik: -1.1334e+02 - logprior: -1.2282e+00
Epoch 7/10
13/13 - 2s - loss: 114.6621 - loglik: -1.1343e+02 - logprior: -1.2294e+00
Fitted a model with MAP estimate = -114.3219
Time for alignment: 64.7272
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 223.3234 - loglik: -2.1389e+02 - logprior: -9.4354e+00
Epoch 2/10
13/13 - 2s - loss: 171.1384 - loglik: -1.6879e+02 - logprior: -2.3467e+00
Epoch 3/10
13/13 - 2s - loss: 143.5889 - loglik: -1.4184e+02 - logprior: -1.7520e+00
Epoch 4/10
13/13 - 2s - loss: 136.5077 - loglik: -1.3493e+02 - logprior: -1.5814e+00
Epoch 5/10
13/13 - 2s - loss: 135.0244 - loglik: -1.3359e+02 - logprior: -1.4380e+00
Epoch 6/10
13/13 - 2s - loss: 133.7231 - loglik: -1.3226e+02 - logprior: -1.4630e+00
Epoch 7/10
13/13 - 2s - loss: 133.1941 - loglik: -1.3176e+02 - logprior: -1.4355e+00
Epoch 8/10
13/13 - 2s - loss: 133.3882 - loglik: -1.3193e+02 - logprior: -1.4575e+00
Fitted a model with MAP estimate = -132.9724
expansions: [(0, 4), (13, 1), (37, 4), (38, 1), (43, 3), (44, 2), (45, 3)]
discards: []
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 137.6656 - loglik: -1.2730e+02 - logprior: -1.0364e+01
Epoch 2/2
13/13 - 2s - loss: 121.1537 - loglik: -1.1798e+02 - logprior: -3.1747e+00
Fitted a model with MAP estimate = -117.8661
expansions: [(0, 2), (45, 1), (60, 1), (61, 1)]
discards: []
Fitting a model of length 88 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 125.0118 - loglik: -1.1479e+02 - logprior: -1.0218e+01
Epoch 2/2
13/13 - 2s - loss: 114.7977 - loglik: -1.1147e+02 - logprior: -3.3250e+00
Fitted a model with MAP estimate = -112.8253
expansions: []
discards: []
Fitting a model of length 88 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 118.7725 - loglik: -1.1069e+02 - logprior: -8.0869e+00
Epoch 2/10
13/13 - 2s - loss: 112.8755 - loglik: -1.1047e+02 - logprior: -2.4075e+00
Epoch 3/10
13/13 - 2s - loss: 111.6016 - loglik: -1.0975e+02 - logprior: -1.8504e+00
Epoch 4/10
13/13 - 2s - loss: 111.0830 - loglik: -1.0958e+02 - logprior: -1.5063e+00
Epoch 5/10
13/13 - 2s - loss: 110.7037 - loglik: -1.0927e+02 - logprior: -1.4326e+00
Epoch 6/10
13/13 - 2s - loss: 110.8073 - loglik: -1.0941e+02 - logprior: -1.3968e+00
Fitted a model with MAP estimate = -110.4888
Time for alignment: 57.4892
Computed alignments with likelihoods: ['-114.2030', '-114.3219', '-110.4888']
Best model has likelihood: -110.4888
SP score = 0.4991
Training of 3 independent models on file zf-CCHH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f179b532b80>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1fffc24250>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202e8e4d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202c672400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.3930 - loglik: -5.8571e+01 - logprior: -8.2244e-01
Epoch 2/10
41/41 - 2s - loss: 46.2000 - loglik: -4.5417e+01 - logprior: -7.8274e-01
Epoch 3/10
41/41 - 2s - loss: 45.5553 - loglik: -4.4820e+01 - logprior: -7.3477e-01
Epoch 4/10
41/41 - 2s - loss: 45.3647 - loglik: -4.4633e+01 - logprior: -7.3178e-01
Epoch 5/10
41/41 - 2s - loss: 45.2252 - loglik: -4.4493e+01 - logprior: -7.3264e-01
Epoch 6/10
41/41 - 2s - loss: 45.0258 - loglik: -4.4293e+01 - logprior: -7.3273e-01
Epoch 7/10
41/41 - 2s - loss: 44.8800 - loglik: -4.4141e+01 - logprior: -7.3912e-01
Epoch 8/10
41/41 - 2s - loss: 44.8469 - loglik: -4.4104e+01 - logprior: -7.4303e-01
Epoch 9/10
41/41 - 2s - loss: 44.7302 - loglik: -4.3983e+01 - logprior: -7.4742e-01
Epoch 10/10
41/41 - 2s - loss: 44.7280 - loglik: -4.3976e+01 - logprior: -7.5240e-01
Fitted a model with MAP estimate = -44.8570
expansions: [(4, 1), (8, 2), (9, 2), (10, 1), (11, 1), (12, 1)]
discards: [0]
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 47.2771 - loglik: -4.6125e+01 - logprior: -1.1518e+00
Epoch 2/2
41/41 - 2s - loss: 43.5060 - loglik: -4.2605e+01 - logprior: -9.0073e-01
Fitted a model with MAP estimate = -42.8152
expansions: []
discards: [ 8 11]
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 7s - loss: 44.1640 - loglik: -4.3074e+01 - logprior: -1.0899e+00
Epoch 2/2
41/41 - 2s - loss: 43.4440 - loglik: -4.2567e+01 - logprior: -8.7711e-01
Fitted a model with MAP estimate = -42.7955
expansions: []
discards: []
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 6s - loss: 42.7053 - loglik: -4.1999e+01 - logprior: -7.0644e-01
Epoch 2/10
58/58 - 2s - loss: 42.2143 - loglik: -4.1604e+01 - logprior: -6.0988e-01
Epoch 3/10
58/58 - 2s - loss: 42.2416 - loglik: -4.1640e+01 - logprior: -6.0189e-01
Fitted a model with MAP estimate = -41.9911
Time for alignment: 76.9614
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 5s - loss: 59.5041 - loglik: -5.8666e+01 - logprior: -8.3800e-01
Epoch 2/10
41/41 - 2s - loss: 46.2544 - loglik: -4.5470e+01 - logprior: -7.8397e-01
Epoch 3/10
41/41 - 2s - loss: 45.5255 - loglik: -4.4792e+01 - logprior: -7.3350e-01
Epoch 4/10
41/41 - 2s - loss: 45.3320 - loglik: -4.4601e+01 - logprior: -7.3102e-01
Epoch 5/10
41/41 - 2s - loss: 45.2442 - loglik: -4.4512e+01 - logprior: -7.3254e-01
Epoch 6/10
41/41 - 2s - loss: 45.1133 - loglik: -4.4381e+01 - logprior: -7.3230e-01
Epoch 7/10
41/41 - 2s - loss: 44.8689 - loglik: -4.4130e+01 - logprior: -7.3907e-01
Epoch 8/10
41/41 - 2s - loss: 44.8307 - loglik: -4.4088e+01 - logprior: -7.4267e-01
Epoch 9/10
41/41 - 2s - loss: 44.7706 - loglik: -4.4023e+01 - logprior: -7.4778e-01
Epoch 10/10
41/41 - 2s - loss: 44.6403 - loglik: -4.3889e+01 - logprior: -7.5176e-01
Fitted a model with MAP estimate = -44.8406
expansions: [(4, 1), (8, 2), (9, 2), (10, 2), (11, 1), (12, 1)]
discards: [0]
Fitting a model of length 26 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 47.4420 - loglik: -4.6282e+01 - logprior: -1.1601e+00
Epoch 2/2
41/41 - 2s - loss: 43.6225 - loglik: -4.2714e+01 - logprior: -9.0811e-01
Fitted a model with MAP estimate = -42.7307
expansions: []
discards: [ 8 12 14]
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 44.0797 - loglik: -4.2988e+01 - logprior: -1.0915e+00
Epoch 2/2
41/41 - 2s - loss: 43.5697 - loglik: -4.2694e+01 - logprior: -8.7602e-01
Fitted a model with MAP estimate = -42.8459
expansions: []
discards: []
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 42.6616 - loglik: -4.1954e+01 - logprior: -7.0734e-01
Epoch 2/10
58/58 - 2s - loss: 42.2337 - loglik: -4.1626e+01 - logprior: -6.0761e-01
Epoch 3/10
58/58 - 2s - loss: 42.2719 - loglik: -4.1669e+01 - logprior: -6.0333e-01
Fitted a model with MAP estimate = -42.0051
Time for alignment: 75.6835
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.6675 - loglik: -5.8799e+01 - logprior: -8.6806e-01
Epoch 2/10
41/41 - 2s - loss: 46.3740 - loglik: -4.5414e+01 - logprior: -9.5994e-01
Epoch 3/10
41/41 - 2s - loss: 45.5275 - loglik: -4.4607e+01 - logprior: -9.2068e-01
Epoch 4/10
41/41 - 2s - loss: 45.4150 - loglik: -4.4497e+01 - logprior: -9.1769e-01
Epoch 5/10
41/41 - 2s - loss: 45.1864 - loglik: -4.4266e+01 - logprior: -9.2090e-01
Epoch 6/10
41/41 - 2s - loss: 45.1085 - loglik: -4.4189e+01 - logprior: -9.1923e-01
Epoch 7/10
41/41 - 2s - loss: 45.0131 - loglik: -4.4089e+01 - logprior: -9.2448e-01
Epoch 8/10
41/41 - 2s - loss: 44.8301 - loglik: -4.3899e+01 - logprior: -9.3062e-01
Epoch 9/10
41/41 - 2s - loss: 44.7656 - loglik: -4.3829e+01 - logprior: -9.3623e-01
Epoch 10/10
41/41 - 2s - loss: 44.6752 - loglik: -4.3734e+01 - logprior: -9.4155e-01
Fitted a model with MAP estimate = -44.8931
expansions: [(8, 2), (9, 2), (10, 1), (11, 2), (12, 1)]
discards: []
Fitting a model of length 26 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 45.7384 - loglik: -4.4604e+01 - logprior: -1.1341e+00
Epoch 2/2
41/41 - 2s - loss: 43.6145 - loglik: -4.2702e+01 - logprior: -9.1259e-01
Fitted a model with MAP estimate = -42.8917
expansions: []
discards: [ 8 12 16]
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 44.1955 - loglik: -4.3104e+01 - logprior: -1.0914e+00
Epoch 2/2
41/41 - 2s - loss: 43.4798 - loglik: -4.2602e+01 - logprior: -8.7729e-01
Fitted a model with MAP estimate = -42.8302
expansions: []
discards: []
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 42.6201 - loglik: -4.1913e+01 - logprior: -7.0710e-01
Epoch 2/10
58/58 - 2s - loss: 42.3027 - loglik: -4.1694e+01 - logprior: -6.0870e-01
Epoch 3/10
58/58 - 2s - loss: 42.2753 - loglik: -4.1672e+01 - logprior: -6.0333e-01
Epoch 4/10
58/58 - 2s - loss: 42.0008 - loglik: -4.1397e+01 - logprior: -6.0336e-01
Epoch 5/10
58/58 - 2s - loss: 41.8915 - loglik: -4.1291e+01 - logprior: -6.0030e-01
Epoch 6/10
58/58 - 2s - loss: 41.9259 - loglik: -4.1327e+01 - logprior: -5.9940e-01
Fitted a model with MAP estimate = -41.5867
Time for alignment: 82.8635
Computed alignments with likelihoods: ['-41.9911', '-42.0051', '-41.5867']
Best model has likelihood: -41.5867
SP score = 0.9665
Training of 3 independent models on file scorptoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1d767c77c0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f179a15ab50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1d74592910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2012de9490>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 289.2194 - loglik: -1.5695e+02 - logprior: -1.3227e+02
Epoch 2/10
10/10 - 1s - loss: 170.4642 - loglik: -1.3490e+02 - logprior: -3.5560e+01
Epoch 3/10
10/10 - 1s - loss: 134.4527 - loglik: -1.1780e+02 - logprior: -1.6655e+01
Epoch 4/10
10/10 - 1s - loss: 118.1802 - loglik: -1.0821e+02 - logprior: -9.9665e+00
Epoch 5/10
10/10 - 1s - loss: 110.1309 - loglik: -1.0376e+02 - logprior: -6.3663e+00
Epoch 6/10
10/10 - 1s - loss: 105.6076 - loglik: -1.0122e+02 - logprior: -4.3857e+00
Epoch 7/10
10/10 - 1s - loss: 103.4575 - loglik: -1.0036e+02 - logprior: -3.0984e+00
Epoch 8/10
10/10 - 1s - loss: 102.3790 - loglik: -1.0017e+02 - logprior: -2.2127e+00
Epoch 9/10
10/10 - 1s - loss: 101.6231 - loglik: -1.0004e+02 - logprior: -1.5854e+00
Epoch 10/10
10/10 - 1s - loss: 101.1233 - loglik: -9.9931e+01 - logprior: -1.1926e+00
Fitted a model with MAP estimate = -100.9188
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (19, 1), (22, 2), (31, 1), (32, 2)]
discards: [0]
Fitting a model of length 55 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 241.0870 - loglik: -9.9562e+01 - logprior: -1.4152e+02
Epoch 2/2
10/10 - 1s - loss: 151.9378 - loglik: -9.3979e+01 - logprior: -5.7958e+01
Fitted a model with MAP estimate = -137.3634
expansions: [(0, 2)]
discards: [ 0  8 20 29]
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 202.9397 - loglik: -9.0882e+01 - logprior: -1.1206e+02
Epoch 2/2
10/10 - 1s - loss: 119.9746 - loglik: -8.9916e+01 - logprior: -3.0059e+01
Fitted a model with MAP estimate = -107.8321
expansions: []
discards: [0]
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 220.5238 - loglik: -9.1682e+01 - logprior: -1.2884e+02
Epoch 2/10
10/10 - 1s - loss: 127.9814 - loglik: -9.1468e+01 - logprior: -3.6514e+01
Epoch 3/10
10/10 - 1s - loss: 105.7911 - loglik: -9.1696e+01 - logprior: -1.4095e+01
Epoch 4/10
10/10 - 1s - loss: 97.9659 - loglik: -9.2027e+01 - logprior: -5.9385e+00
Epoch 5/10
10/10 - 1s - loss: 94.1414 - loglik: -9.2336e+01 - logprior: -1.8057e+00
Epoch 6/10
10/10 - 1s - loss: 92.0030 - loglik: -9.2548e+01 - logprior: 0.5454
Epoch 7/10
10/10 - 1s - loss: 90.7254 - loglik: -9.2681e+01 - logprior: 1.9552
Epoch 8/10
10/10 - 1s - loss: 89.8997 - loglik: -9.2781e+01 - logprior: 2.8816
Epoch 9/10
10/10 - 1s - loss: 89.3082 - loglik: -9.2875e+01 - logprior: 3.5667
Epoch 10/10
10/10 - 1s - loss: 88.8406 - loglik: -9.2972e+01 - logprior: 4.1315
Fitted a model with MAP estimate = -88.6125
Time for alignment: 34.3429
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 289.2194 - loglik: -1.5695e+02 - logprior: -1.3227e+02
Epoch 2/10
10/10 - 1s - loss: 170.4642 - loglik: -1.3490e+02 - logprior: -3.5560e+01
Epoch 3/10
10/10 - 1s - loss: 134.4527 - loglik: -1.1780e+02 - logprior: -1.6655e+01
Epoch 4/10
10/10 - 1s - loss: 118.1802 - loglik: -1.0821e+02 - logprior: -9.9665e+00
Epoch 5/10
10/10 - 1s - loss: 110.1309 - loglik: -1.0376e+02 - logprior: -6.3663e+00
Epoch 6/10
10/10 - 1s - loss: 105.6076 - loglik: -1.0122e+02 - logprior: -4.3857e+00
Epoch 7/10
10/10 - 1s - loss: 103.4575 - loglik: -1.0036e+02 - logprior: -3.0984e+00
Epoch 8/10
10/10 - 1s - loss: 102.3790 - loglik: -1.0017e+02 - logprior: -2.2127e+00
Epoch 9/10
10/10 - 1s - loss: 101.6231 - loglik: -1.0004e+02 - logprior: -1.5854e+00
Epoch 10/10
10/10 - 1s - loss: 101.1233 - loglik: -9.9931e+01 - logprior: -1.1926e+00
Fitted a model with MAP estimate = -100.9188
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (19, 1), (22, 2), (31, 1), (32, 2)]
discards: [0]
Fitting a model of length 55 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 241.0870 - loglik: -9.9562e+01 - logprior: -1.4152e+02
Epoch 2/2
10/10 - 1s - loss: 151.9378 - loglik: -9.3979e+01 - logprior: -5.7958e+01
Fitted a model with MAP estimate = -137.3634
expansions: [(0, 2)]
discards: [ 0  8 20 29]
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 202.9397 - loglik: -9.0882e+01 - logprior: -1.1206e+02
Epoch 2/2
10/10 - 1s - loss: 119.9746 - loglik: -8.9916e+01 - logprior: -3.0059e+01
Fitted a model with MAP estimate = -107.8321
expansions: []
discards: [0]
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 220.5238 - loglik: -9.1682e+01 - logprior: -1.2884e+02
Epoch 2/10
10/10 - 1s - loss: 127.9814 - loglik: -9.1468e+01 - logprior: -3.6514e+01
Epoch 3/10
10/10 - 1s - loss: 105.7911 - loglik: -9.1696e+01 - logprior: -1.4095e+01
Epoch 4/10
10/10 - 1s - loss: 97.9659 - loglik: -9.2027e+01 - logprior: -5.9385e+00
Epoch 5/10
10/10 - 1s - loss: 94.1414 - loglik: -9.2336e+01 - logprior: -1.8057e+00
Epoch 6/10
10/10 - 1s - loss: 92.0030 - loglik: -9.2548e+01 - logprior: 0.5454
Epoch 7/10
10/10 - 1s - loss: 90.7254 - loglik: -9.2681e+01 - logprior: 1.9552
Epoch 8/10
10/10 - 1s - loss: 89.8997 - loglik: -9.2781e+01 - logprior: 2.8816
Epoch 9/10
10/10 - 1s - loss: 89.3082 - loglik: -9.2875e+01 - logprior: 3.5667
Epoch 10/10
10/10 - 1s - loss: 88.8406 - loglik: -9.2972e+01 - logprior: 4.1315
Fitted a model with MAP estimate = -88.6125
Time for alignment: 33.5818
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 289.2194 - loglik: -1.5695e+02 - logprior: -1.3227e+02
Epoch 2/10
10/10 - 1s - loss: 170.4642 - loglik: -1.3490e+02 - logprior: -3.5560e+01
Epoch 3/10
10/10 - 1s - loss: 134.4527 - loglik: -1.1780e+02 - logprior: -1.6655e+01
Epoch 4/10
10/10 - 1s - loss: 118.1802 - loglik: -1.0821e+02 - logprior: -9.9665e+00
Epoch 5/10
10/10 - 1s - loss: 110.1309 - loglik: -1.0376e+02 - logprior: -6.3663e+00
Epoch 6/10
10/10 - 1s - loss: 105.6076 - loglik: -1.0122e+02 - logprior: -4.3857e+00
Epoch 7/10
10/10 - 1s - loss: 103.4575 - loglik: -1.0036e+02 - logprior: -3.0984e+00
Epoch 8/10
10/10 - 1s - loss: 102.3790 - loglik: -1.0017e+02 - logprior: -2.2127e+00
Epoch 9/10
10/10 - 1s - loss: 101.6231 - loglik: -1.0004e+02 - logprior: -1.5854e+00
Epoch 10/10
10/10 - 1s - loss: 101.1233 - loglik: -9.9931e+01 - logprior: -1.1926e+00
Fitted a model with MAP estimate = -100.9188
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (19, 1), (22, 2), (31, 1), (32, 2)]
discards: [0]
Fitting a model of length 55 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 241.0870 - loglik: -9.9562e+01 - logprior: -1.4152e+02
Epoch 2/2
10/10 - 1s - loss: 151.9378 - loglik: -9.3979e+01 - logprior: -5.7958e+01
Fitted a model with MAP estimate = -137.3634
expansions: [(0, 2)]
discards: [ 0  8 20 29]
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 202.9397 - loglik: -9.0882e+01 - logprior: -1.1206e+02
Epoch 2/2
10/10 - 1s - loss: 119.9746 - loglik: -8.9916e+01 - logprior: -3.0059e+01
Fitted a model with MAP estimate = -107.8321
expansions: []
discards: [0]
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 220.5238 - loglik: -9.1682e+01 - logprior: -1.2884e+02
Epoch 2/10
10/10 - 1s - loss: 127.9814 - loglik: -9.1468e+01 - logprior: -3.6514e+01
Epoch 3/10
10/10 - 1s - loss: 105.7911 - loglik: -9.1696e+01 - logprior: -1.4095e+01
Epoch 4/10
10/10 - 1s - loss: 97.9659 - loglik: -9.2027e+01 - logprior: -5.9385e+00
Epoch 5/10
10/10 - 1s - loss: 94.1414 - loglik: -9.2336e+01 - logprior: -1.8057e+00
Epoch 6/10
10/10 - 1s - loss: 92.0030 - loglik: -9.2548e+01 - logprior: 0.5454
Epoch 7/10
10/10 - 1s - loss: 90.7254 - loglik: -9.2681e+01 - logprior: 1.9552
Epoch 8/10
10/10 - 1s - loss: 89.8997 - loglik: -9.2781e+01 - logprior: 2.8816
Epoch 9/10
10/10 - 1s - loss: 89.3082 - loglik: -9.2875e+01 - logprior: 3.5667
Epoch 10/10
10/10 - 1s - loss: 88.8406 - loglik: -9.2972e+01 - logprior: 4.1315
Fitted a model with MAP estimate = -88.6125
Time for alignment: 33.4058
Computed alignments with likelihoods: ['-88.6125', '-88.6125', '-88.6125']
Best model has likelihood: -88.6125
SP score = 0.8723
Training of 3 independent models on file biotin_lipoyl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f179be685b0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2011ff6880>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1d74b0a9d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f210f8c4d90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 195.5759 - loglik: -1.9220e+02 - logprior: -3.3783e+00
Epoch 2/10
19/19 - 2s - loss: 155.5309 - loglik: -1.5427e+02 - logprior: -1.2613e+00
Epoch 3/10
19/19 - 2s - loss: 141.8148 - loglik: -1.4048e+02 - logprior: -1.3384e+00
Epoch 4/10
19/19 - 2s - loss: 139.8440 - loglik: -1.3853e+02 - logprior: -1.3157e+00
Epoch 5/10
19/19 - 2s - loss: 139.1615 - loglik: -1.3789e+02 - logprior: -1.2737e+00
Epoch 6/10
19/19 - 2s - loss: 138.8657 - loglik: -1.3762e+02 - logprior: -1.2479e+00
Epoch 7/10
19/19 - 2s - loss: 138.8300 - loglik: -1.3760e+02 - logprior: -1.2291e+00
Epoch 8/10
19/19 - 2s - loss: 138.5083 - loglik: -1.3729e+02 - logprior: -1.2168e+00
Epoch 9/10
19/19 - 2s - loss: 138.5666 - loglik: -1.3736e+02 - logprior: -1.2065e+00
Fitted a model with MAP estimate = -138.9644
expansions: [(0, 3), (12, 1), (13, 1), (14, 1), (15, 1), (20, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 142.2008 - loglik: -1.3780e+02 - logprior: -4.3970e+00
Epoch 2/2
19/19 - 2s - loss: 132.2685 - loglik: -1.3073e+02 - logprior: -1.5406e+00
Fitted a model with MAP estimate = -132.0886
expansions: [(0, 2)]
discards: [27 49 60 65]
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 134.0275 - loglik: -1.3009e+02 - logprior: -3.9416e+00
Epoch 2/2
19/19 - 2s - loss: 129.2247 - loglik: -1.2787e+02 - logprior: -1.3560e+00
Fitted a model with MAP estimate = -129.6123
expansions: []
discards: [0]
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 6s - loss: 133.5122 - loglik: -1.3007e+02 - logprior: -3.4463e+00
Epoch 2/10
21/21 - 2s - loss: 130.5402 - loglik: -1.2886e+02 - logprior: -1.6753e+00
Epoch 3/10
21/21 - 2s - loss: 129.0698 - loglik: -1.2790e+02 - logprior: -1.1718e+00
Epoch 4/10
21/21 - 2s - loss: 128.7148 - loglik: -1.2760e+02 - logprior: -1.1165e+00
Epoch 5/10
21/21 - 2s - loss: 128.3580 - loglik: -1.2724e+02 - logprior: -1.1194e+00
Epoch 6/10
21/21 - 2s - loss: 128.2068 - loglik: -1.2709e+02 - logprior: -1.1157e+00
Epoch 7/10
21/21 - 2s - loss: 127.9856 - loglik: -1.2687e+02 - logprior: -1.1164e+00
Epoch 8/10
21/21 - 2s - loss: 128.0168 - loglik: -1.2690e+02 - logprior: -1.1122e+00
Fitted a model with MAP estimate = -127.9197
Time for alignment: 64.0951
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 195.5211 - loglik: -1.9214e+02 - logprior: -3.3764e+00
Epoch 2/10
19/19 - 2s - loss: 155.7314 - loglik: -1.5448e+02 - logprior: -1.2516e+00
Epoch 3/10
19/19 - 2s - loss: 142.1690 - loglik: -1.4084e+02 - logprior: -1.3304e+00
Epoch 4/10
19/19 - 2s - loss: 139.9375 - loglik: -1.3863e+02 - logprior: -1.3069e+00
Epoch 5/10
19/19 - 2s - loss: 139.3583 - loglik: -1.3810e+02 - logprior: -1.2554e+00
Epoch 6/10
19/19 - 2s - loss: 139.0813 - loglik: -1.3785e+02 - logprior: -1.2305e+00
Epoch 7/10
19/19 - 2s - loss: 138.8591 - loglik: -1.3765e+02 - logprior: -1.2085e+00
Epoch 8/10
19/19 - 2s - loss: 138.6164 - loglik: -1.3742e+02 - logprior: -1.1962e+00
Epoch 9/10
19/19 - 2s - loss: 139.0447 - loglik: -1.3785e+02 - logprior: -1.1906e+00
Fitted a model with MAP estimate = -139.0921
expansions: [(0, 3), (13, 2), (14, 2), (15, 1), (20, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 142.7473 - loglik: -1.3834e+02 - logprior: -4.4077e+00
Epoch 2/2
19/19 - 2s - loss: 132.1763 - loglik: -1.3061e+02 - logprior: -1.5700e+00
Fitted a model with MAP estimate = -131.8409
expansions: [(0, 2)]
discards: [19 28 50 61 66]
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 133.9095 - loglik: -1.2998e+02 - logprior: -3.9249e+00
Epoch 2/2
19/19 - 2s - loss: 128.7554 - loglik: -1.2732e+02 - logprior: -1.4400e+00
Fitted a model with MAP estimate = -128.8623
expansions: []
discards: [0]
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 132.7719 - loglik: -1.2932e+02 - logprior: -3.4532e+00
Epoch 2/10
21/21 - 2s - loss: 129.9868 - loglik: -1.2814e+02 - logprior: -1.8508e+00
Epoch 3/10
21/21 - 2s - loss: 128.5789 - loglik: -1.2735e+02 - logprior: -1.2292e+00
Epoch 4/10
21/21 - 2s - loss: 127.8142 - loglik: -1.2670e+02 - logprior: -1.1114e+00
Epoch 5/10
21/21 - 2s - loss: 127.5046 - loglik: -1.2638e+02 - logprior: -1.1290e+00
Epoch 6/10
21/21 - 2s - loss: 127.6127 - loglik: -1.2649e+02 - logprior: -1.1200e+00
Fitted a model with MAP estimate = -127.2639
Time for alignment: 58.3817
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 195.7009 - loglik: -1.9232e+02 - logprior: -3.3801e+00
Epoch 2/10
19/19 - 2s - loss: 156.5492 - loglik: -1.5529e+02 - logprior: -1.2599e+00
Epoch 3/10
19/19 - 2s - loss: 143.8294 - loglik: -1.4253e+02 - logprior: -1.3026e+00
Epoch 4/10
19/19 - 2s - loss: 141.5490 - loglik: -1.4028e+02 - logprior: -1.2738e+00
Epoch 5/10
19/19 - 2s - loss: 140.7124 - loglik: -1.3948e+02 - logprior: -1.2281e+00
Epoch 6/10
19/19 - 2s - loss: 140.4449 - loglik: -1.3923e+02 - logprior: -1.2170e+00
Epoch 7/10
19/19 - 2s - loss: 140.3672 - loglik: -1.3917e+02 - logprior: -1.1986e+00
Epoch 8/10
19/19 - 2s - loss: 140.0937 - loglik: -1.3891e+02 - logprior: -1.1831e+00
Epoch 9/10
19/19 - 2s - loss: 140.0763 - loglik: -1.3889e+02 - logprior: -1.1845e+00
Epoch 10/10
19/19 - 2s - loss: 140.1354 - loglik: -1.3894e+02 - logprior: -1.1910e+00
Fitted a model with MAP estimate = -140.3680
expansions: [(0, 3), (12, 1), (13, 1), (14, 1), (15, 1), (20, 2), (33, 1), (39, 2), (46, 1), (47, 1), (52, 2)]
discards: []
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 141.3734 - loglik: -1.3696e+02 - logprior: -4.4151e+00
Epoch 2/2
19/19 - 2s - loss: 131.9360 - loglik: -1.3045e+02 - logprior: -1.4830e+00
Fitted a model with MAP estimate = -132.1510
expansions: [(0, 2)]
discards: [27 49]
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 133.9199 - loglik: -1.3000e+02 - logprior: -3.9176e+00
Epoch 2/2
19/19 - 2s - loss: 129.1552 - loglik: -1.2776e+02 - logprior: -1.3983e+00
Fitted a model with MAP estimate = -129.6778
expansions: []
discards: [0]
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 133.4659 - loglik: -1.3002e+02 - logprior: -3.4435e+00
Epoch 2/10
21/21 - 2s - loss: 130.6151 - loglik: -1.2885e+02 - logprior: -1.7620e+00
Epoch 3/10
21/21 - 2s - loss: 129.3086 - loglik: -1.2812e+02 - logprior: -1.1849e+00
Epoch 4/10
21/21 - 2s - loss: 128.6010 - loglik: -1.2749e+02 - logprior: -1.1085e+00
Epoch 5/10
21/21 - 2s - loss: 128.3290 - loglik: -1.2721e+02 - logprior: -1.1207e+00
Epoch 6/10
21/21 - 2s - loss: 128.2099 - loglik: -1.2709e+02 - logprior: -1.1157e+00
Epoch 7/10
21/21 - 2s - loss: 127.9716 - loglik: -1.2684e+02 - logprior: -1.1272e+00
Epoch 8/10
21/21 - 2s - loss: 128.1554 - loglik: -1.2704e+02 - logprior: -1.1131e+00
Fitted a model with MAP estimate = -127.9079
Time for alignment: 62.2461
Computed alignments with likelihoods: ['-127.9197', '-127.2639', '-127.9079']
Best model has likelihood: -127.2639
SP score = 0.9488
Training of 3 independent models on file uce.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2007fb8a60>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f212961afd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17aa43a6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201a4d73d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 392.7547 - loglik: -3.8325e+02 - logprior: -9.4999e+00
Epoch 2/10
13/13 - 2s - loss: 336.8402 - loglik: -3.3475e+02 - logprior: -2.0889e+00
Epoch 3/10
13/13 - 2s - loss: 295.1078 - loglik: -2.9327e+02 - logprior: -1.8377e+00
Epoch 4/10
13/13 - 2s - loss: 282.4814 - loglik: -2.8042e+02 - logprior: -2.0629e+00
Epoch 5/10
13/13 - 2s - loss: 280.0838 - loglik: -2.7800e+02 - logprior: -2.0810e+00
Epoch 6/10
13/13 - 2s - loss: 278.6538 - loglik: -2.7667e+02 - logprior: -1.9831e+00
Epoch 7/10
13/13 - 2s - loss: 277.7545 - loglik: -2.7576e+02 - logprior: -1.9953e+00
Epoch 8/10
13/13 - 2s - loss: 276.9447 - loglik: -2.7486e+02 - logprior: -2.0839e+00
Epoch 9/10
13/13 - 2s - loss: 276.3587 - loglik: -2.7426e+02 - logprior: -2.0990e+00
Epoch 10/10
13/13 - 2s - loss: 277.9817 - loglik: -2.7580e+02 - logprior: -2.1790e+00
Fitted a model with MAP estimate = -276.7403
expansions: [(7, 2), (8, 3), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 2), (53, 1), (70, 3), (75, 2), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Fitting a model of length 143 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 284.7461 - loglik: -2.7472e+02 - logprior: -1.0028e+01
Epoch 2/2
13/13 - 3s - loss: 265.9244 - loglik: -2.6184e+02 - logprior: -4.0859e+00
Fitted a model with MAP estimate = -262.6250
expansions: [(0, 3)]
discards: [ 0  9 66 88 89 95]
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 267.0083 - loglik: -2.5967e+02 - logprior: -7.3422e+00
Epoch 2/2
13/13 - 3s - loss: 259.5918 - loglik: -2.5790e+02 - logprior: -1.6943e+00
Fitted a model with MAP estimate = -257.1658
expansions: []
discards: [0 2]
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 269.6068 - loglik: -2.6045e+02 - logprior: -9.1571e+00
Epoch 2/10
13/13 - 3s - loss: 260.6979 - loglik: -2.5800e+02 - logprior: -2.7002e+00
Epoch 3/10
13/13 - 3s - loss: 257.1220 - loglik: -2.5612e+02 - logprior: -1.0032e+00
Epoch 4/10
13/13 - 3s - loss: 256.0685 - loglik: -2.5567e+02 - logprior: -3.9824e-01
Epoch 5/10
13/13 - 3s - loss: 256.1229 - loglik: -2.5583e+02 - logprior: -2.8926e-01
Fitted a model with MAP estimate = -254.8882
Time for alignment: 80.4953
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 392.1336 - loglik: -3.8262e+02 - logprior: -9.5134e+00
Epoch 2/10
13/13 - 2s - loss: 338.8094 - loglik: -3.3671e+02 - logprior: -2.0959e+00
Epoch 3/10
13/13 - 2s - loss: 296.3811 - loglik: -2.9457e+02 - logprior: -1.8128e+00
Epoch 4/10
13/13 - 2s - loss: 284.5884 - loglik: -2.8257e+02 - logprior: -2.0202e+00
Epoch 5/10
13/13 - 2s - loss: 279.6089 - loglik: -2.7757e+02 - logprior: -2.0390e+00
Epoch 6/10
13/13 - 2s - loss: 279.2887 - loglik: -2.7730e+02 - logprior: -1.9843e+00
Epoch 7/10
13/13 - 2s - loss: 277.0117 - loglik: -2.7500e+02 - logprior: -2.0072e+00
Epoch 8/10
13/13 - 2s - loss: 277.4236 - loglik: -2.7534e+02 - logprior: -2.0808e+00
Fitted a model with MAP estimate = -277.2881
expansions: [(7, 2), (8, 2), (11, 1), (14, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (53, 1), (70, 2), (75, 2), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 284.5498 - loglik: -2.7462e+02 - logprior: -9.9258e+00
Epoch 2/2
13/13 - 3s - loss: 265.3494 - loglik: -2.6139e+02 - logprior: -3.9640e+00
Fitted a model with MAP estimate = -262.8242
expansions: [(0, 3)]
discards: [ 0 87 93]
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 267.7898 - loglik: -2.6044e+02 - logprior: -7.3489e+00
Epoch 2/2
13/13 - 3s - loss: 258.1360 - loglik: -2.5642e+02 - logprior: -1.7111e+00
Fitted a model with MAP estimate = -256.9996
expansions: []
discards: [0 2]
Fitting a model of length 139 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 269.2280 - loglik: -2.6004e+02 - logprior: -9.1851e+00
Epoch 2/10
13/13 - 3s - loss: 260.7024 - loglik: -2.5798e+02 - logprior: -2.7245e+00
Epoch 3/10
13/13 - 3s - loss: 257.4153 - loglik: -2.5639e+02 - logprior: -1.0240e+00
Epoch 4/10
13/13 - 3s - loss: 255.9878 - loglik: -2.5555e+02 - logprior: -4.4026e-01
Epoch 5/10
13/13 - 3s - loss: 255.1649 - loglik: -2.5485e+02 - logprior: -3.1219e-01
Epoch 6/10
13/13 - 3s - loss: 254.8045 - loglik: -2.5451e+02 - logprior: -2.9496e-01
Epoch 7/10
13/13 - 3s - loss: 254.3347 - loglik: -2.5401e+02 - logprior: -3.2759e-01
Epoch 8/10
13/13 - 3s - loss: 253.8874 - loglik: -2.5353e+02 - logprior: -3.6195e-01
Epoch 9/10
13/13 - 3s - loss: 253.8342 - loglik: -2.5348e+02 - logprior: -3.5472e-01
Epoch 10/10
13/13 - 3s - loss: 253.5287 - loglik: -2.5317e+02 - logprior: -3.6342e-01
Fitted a model with MAP estimate = -253.4972
Time for alignment: 89.8298
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 392.9919 - loglik: -3.8347e+02 - logprior: -9.5244e+00
Epoch 2/10
13/13 - 2s - loss: 337.3194 - loglik: -3.3521e+02 - logprior: -2.1051e+00
Epoch 3/10
13/13 - 2s - loss: 295.3571 - loglik: -2.9353e+02 - logprior: -1.8303e+00
Epoch 4/10
13/13 - 2s - loss: 284.0088 - loglik: -2.8196e+02 - logprior: -2.0483e+00
Epoch 5/10
13/13 - 2s - loss: 279.4775 - loglik: -2.7740e+02 - logprior: -2.0739e+00
Epoch 6/10
13/13 - 2s - loss: 277.3199 - loglik: -2.7526e+02 - logprior: -2.0607e+00
Epoch 7/10
13/13 - 2s - loss: 276.5948 - loglik: -2.7450e+02 - logprior: -2.0988e+00
Epoch 8/10
13/13 - 2s - loss: 275.7083 - loglik: -2.7354e+02 - logprior: -2.1669e+00
Epoch 9/10
13/13 - 2s - loss: 277.2334 - loglik: -2.7506e+02 - logprior: -2.1782e+00
Fitted a model with MAP estimate = -275.9048
expansions: [(7, 2), (8, 2), (11, 1), (14, 1), (16, 2), (21, 1), (22, 1), (23, 1), (24, 1), (38, 1), (39, 1), (51, 1), (52, 2), (53, 1), (70, 2), (75, 1), (76, 1), (77, 2), (79, 1), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 283.9641 - loglik: -2.7399e+02 - logprior: -9.9730e+00
Epoch 2/2
13/13 - 3s - loss: 265.4924 - loglik: -2.6150e+02 - logprior: -3.9891e+00
Fitted a model with MAP estimate = -262.9335
expansions: [(0, 3)]
discards: [ 0 66]
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 266.8140 - loglik: -2.5942e+02 - logprior: -7.3981e+00
Epoch 2/2
13/13 - 3s - loss: 258.4261 - loglik: -2.5668e+02 - logprior: -1.7487e+00
Fitted a model with MAP estimate = -256.6105
expansions: []
discards: [ 0  2 89]
Fitting a model of length 139 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 270.0724 - loglik: -2.6089e+02 - logprior: -9.1803e+00
Epoch 2/10
13/13 - 3s - loss: 260.4912 - loglik: -2.5779e+02 - logprior: -2.7060e+00
Epoch 3/10
13/13 - 3s - loss: 257.3391 - loglik: -2.5632e+02 - logprior: -1.0239e+00
Epoch 4/10
13/13 - 3s - loss: 256.0368 - loglik: -2.5561e+02 - logprior: -4.2737e-01
Epoch 5/10
13/13 - 3s - loss: 255.8114 - loglik: -2.5549e+02 - logprior: -3.1975e-01
Epoch 6/10
13/13 - 3s - loss: 254.2151 - loglik: -2.5395e+02 - logprior: -2.6851e-01
Epoch 7/10
13/13 - 3s - loss: 253.8024 - loglik: -2.5349e+02 - logprior: -3.1558e-01
Epoch 8/10
13/13 - 3s - loss: 254.2974 - loglik: -2.5393e+02 - logprior: -3.6920e-01
Fitted a model with MAP estimate = -253.7351
Time for alignment: 88.9135
Computed alignments with likelihoods: ['-254.8882', '-253.4972', '-253.7351']
Best model has likelihood: -253.4972
SP score = 0.9454
Training of 3 independent models on file ghf1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f17aa4ebf40>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20c2bf6ca0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17998885e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1d749b7730>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 36s - loss: 1083.9673 - loglik: -1.0777e+03 - logprior: -6.3115e+00
Epoch 2/10
25/25 - 33s - loss: 825.3722 - loglik: -8.2442e+02 - logprior: -9.5621e-01
Epoch 3/10
25/25 - 33s - loss: 774.8928 - loglik: -7.7293e+02 - logprior: -1.9622e+00
Epoch 4/10
25/25 - 32s - loss: 761.5645 - loglik: -7.5947e+02 - logprior: -2.0923e+00
Epoch 5/10
25/25 - 32s - loss: 758.8960 - loglik: -7.5663e+02 - logprior: -2.2616e+00
Epoch 6/10
25/25 - 32s - loss: 756.9664 - loglik: -7.5468e+02 - logprior: -2.2890e+00
Epoch 7/10
25/25 - 32s - loss: 756.6937 - loglik: -7.5430e+02 - logprior: -2.3913e+00
Epoch 8/10
25/25 - 32s - loss: 755.0558 - loglik: -7.5265e+02 - logprior: -2.4054e+00
Epoch 9/10
25/25 - 32s - loss: 755.7721 - loglik: -7.5335e+02 - logprior: -2.4179e+00
Fitted a model with MAP estimate = -754.9138
expansions: [(107, 1), (141, 1), (144, 1), (147, 1), (164, 1), (165, 2), (177, 2), (178, 1), (179, 1), (192, 1), (193, 3), (194, 8), (197, 2), (198, 2), (199, 1), (201, 1), (202, 1), (204, 1), (206, 1), (208, 1), (209, 1), (210, 1), (211, 1), (212, 1), (217, 1), (223, 1), (224, 1), (225, 2), (226, 1), (227, 1), (228, 2), (229, 1), (230, 1), (237, 2), (238, 1), (249, 1), (251, 1), (255, 4), (256, 2), (258, 5), (281, 1), (282, 1), (283, 1), (299, 1), (301, 1), (302, 2), (304, 1), (318, 2), (319, 2), (321, 2), (325, 2), (327, 2), (352, 2), (354, 1), (358, 1), (360, 1), (363, 1)]
discards: []
Fitting a model of length 458 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 49s - loss: 732.0508 - loglik: -7.2698e+02 - logprior: -5.0663e+00
Epoch 2/2
25/25 - 46s - loss: 702.4864 - loglik: -7.0244e+02 - logprior: -4.8390e-02
Fitted a model with MAP estimate = -697.7015
expansions: [(206, 1)]
discards: [170 221 313 314 318 319 434 435]
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 48s - loss: 710.0967 - loglik: -7.0730e+02 - logprior: -2.7981e+00
Epoch 2/2
25/25 - 45s - loss: 700.8831 - loglik: -7.0180e+02 - logprior: 0.9186
Fitted a model with MAP estimate = -698.3432
expansions: []
discards: [184 393]
Fitting a model of length 449 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 49s - loss: 707.1154 - loglik: -7.0513e+02 - logprior: -1.9832e+00
Epoch 2/10
25/25 - 45s - loss: 701.8842 - loglik: -7.0384e+02 - logprior: 1.9533
Epoch 3/10
25/25 - 45s - loss: 698.9165 - loglik: -7.0128e+02 - logprior: 2.3657
Epoch 4/10
25/25 - 45s - loss: 694.0449 - loglik: -6.9669e+02 - logprior: 2.6457
Epoch 5/10
25/25 - 45s - loss: 693.9710 - loglik: -6.9678e+02 - logprior: 2.8070
Epoch 6/10
25/25 - 45s - loss: 695.1318 - loglik: -6.9811e+02 - logprior: 2.9754
Fitted a model with MAP estimate = -693.3960
Time for alignment: 907.1657
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 36s - loss: 1086.2913 - loglik: -1.0800e+03 - logprior: -6.2430e+00
Epoch 2/10
25/25 - 32s - loss: 828.6484 - loglik: -8.2759e+02 - logprior: -1.0595e+00
Epoch 3/10
25/25 - 32s - loss: 776.0491 - loglik: -7.7411e+02 - logprior: -1.9382e+00
Epoch 4/10
25/25 - 32s - loss: 763.9235 - loglik: -7.6168e+02 - logprior: -2.2411e+00
Epoch 5/10
25/25 - 32s - loss: 762.2186 - loglik: -7.5991e+02 - logprior: -2.3055e+00
Epoch 6/10
25/25 - 32s - loss: 760.4872 - loglik: -7.5815e+02 - logprior: -2.3365e+00
Epoch 7/10
25/25 - 32s - loss: 762.9327 - loglik: -7.6038e+02 - logprior: -2.5564e+00
Fitted a model with MAP estimate = -761.2390
expansions: [(46, 1), (125, 1), (134, 1), (143, 1), (144, 1), (162, 1), (163, 2), (169, 1), (173, 5), (174, 2), (175, 2), (176, 1), (186, 1), (188, 3), (189, 5), (190, 2), (195, 1), (196, 1), (197, 1), (199, 1), (200, 1), (204, 1), (206, 1), (207, 1), (210, 1), (214, 1), (217, 1), (222, 1), (223, 1), (224, 2), (225, 1), (226, 1), (228, 1), (229, 3), (234, 1), (236, 1), (237, 1), (248, 1), (250, 1), (254, 3), (255, 2), (257, 1), (258, 1), (281, 1), (282, 1), (283, 1), (300, 1), (301, 1), (302, 2), (304, 1), (318, 2), (319, 1), (322, 2), (325, 2), (327, 2), (353, 1), (358, 1), (360, 1), (363, 1)]
discards: []
Fitting a model of length 454 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 49s - loss: 740.6750 - loglik: -7.3585e+02 - logprior: -4.8244e+00
Epoch 2/2
25/25 - 46s - loss: 709.1065 - loglik: -7.0873e+02 - logprior: -3.7687e-01
Fitted a model with MAP estimate = -703.0346
expansions: [(247, 1), (401, 1)]
discards: [184 185 186 215 312 397]
Fitting a model of length 450 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 49s - loss: 714.3224 - loglik: -7.1150e+02 - logprior: -2.8181e+00
Epoch 2/2
25/25 - 45s - loss: 702.0942 - loglik: -7.0299e+02 - logprior: 0.9005
Fitted a model with MAP estimate = -701.3207
expansions: []
discards: []
Fitting a model of length 450 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 48s - loss: 708.1033 - loglik: -7.0602e+02 - logprior: -2.0793e+00
Epoch 2/10
25/25 - 45s - loss: 704.4478 - loglik: -7.0633e+02 - logprior: 1.8847
Epoch 3/10
25/25 - 45s - loss: 700.3870 - loglik: -7.0275e+02 - logprior: 2.3585
Epoch 4/10
25/25 - 45s - loss: 699.2383 - loglik: -7.0182e+02 - logprior: 2.5814
Epoch 5/10
25/25 - 45s - loss: 698.5368 - loglik: -7.0130e+02 - logprior: 2.7661
Epoch 6/10
25/25 - 45s - loss: 696.8226 - loglik: -6.9978e+02 - logprior: 2.9564
Epoch 7/10
25/25 - 45s - loss: 697.2645 - loglik: -7.0031e+02 - logprior: 3.0419
Fitted a model with MAP estimate = -696.1342
Time for alignment: 885.8153
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 35s - loss: 1080.3999 - loglik: -1.0741e+03 - logprior: -6.2564e+00
Epoch 2/10
25/25 - 32s - loss: 821.5641 - loglik: -8.2047e+02 - logprior: -1.0956e+00
Epoch 3/10
25/25 - 32s - loss: 767.9765 - loglik: -7.6606e+02 - logprior: -1.9147e+00
Epoch 4/10
25/25 - 32s - loss: 757.7168 - loglik: -7.5553e+02 - logprior: -2.1842e+00
Epoch 5/10
25/25 - 32s - loss: 751.2381 - loglik: -7.4896e+02 - logprior: -2.2764e+00
Epoch 6/10
25/25 - 32s - loss: 756.5002 - loglik: -7.5417e+02 - logprior: -2.3265e+00
Fitted a model with MAP estimate = -752.1597
expansions: [(133, 1), (145, 1), (148, 1), (165, 1), (166, 1), (172, 1), (176, 4), (177, 2), (178, 2), (181, 1), (192, 4), (193, 2), (194, 1), (195, 2), (198, 1), (199, 3), (200, 1), (202, 1), (203, 2), (204, 1), (206, 1), (208, 1), (209, 1), (211, 1), (212, 1), (214, 1), (216, 1), (222, 1), (223, 1), (224, 2), (225, 1), (226, 1), (227, 2), (228, 1), (229, 1), (236, 2), (237, 1), (248, 1), (252, 1), (254, 3), (255, 2), (256, 1), (258, 5), (266, 1), (280, 1), (282, 1), (299, 1), (301, 2), (302, 2), (304, 1), (318, 2), (319, 2), (321, 2), (324, 1), (325, 1), (327, 2), (350, 1), (353, 1), (358, 1), (360, 1), (366, 1)]
discards: []
Fitting a model of length 460 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 50s - loss: 736.2581 - loglik: -7.3098e+02 - logprior: -5.2780e+00
Epoch 2/2
25/25 - 46s - loss: 703.0719 - loglik: -7.0170e+02 - logprior: -1.3724e+00
Fitted a model with MAP estimate = -698.7853
expansions: [(246, 1), (309, 1), (310, 1), (438, 1)]
discards: [185 186 209 311 312 313 319 320 372 400 440]
Fitting a model of length 453 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 49s - loss: 709.2369 - loglik: -7.0606e+02 - logprior: -3.1742e+00
Epoch 2/2
25/25 - 45s - loss: 699.0256 - loglik: -6.9977e+02 - logprior: 0.7397
Fitted a model with MAP estimate = -696.9387
expansions: []
discards: [310 431]
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 48s - loss: 706.4208 - loglik: -7.0405e+02 - logprior: -2.3687e+00
Epoch 2/10
25/25 - 45s - loss: 700.8911 - loglik: -7.0250e+02 - logprior: 1.6124
Epoch 3/10
25/25 - 45s - loss: 695.7238 - loglik: -6.9793e+02 - logprior: 2.2020
Epoch 4/10
25/25 - 45s - loss: 696.4415 - loglik: -6.9889e+02 - logprior: 2.4491
Fitted a model with MAP estimate = -694.3059
Time for alignment: 721.1947
Computed alignments with likelihoods: ['-693.3960', '-696.1342', '-694.3059']
Best model has likelihood: -693.3960
SP score = 0.9043
Training of 3 independent models on file ldh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1d74b37a60>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20fed4b8b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17994624f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1d747e2fd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 327.3974 - loglik: -3.2214e+02 - logprior: -5.2590e+00
Epoch 2/10
16/16 - 4s - loss: 257.8325 - loglik: -2.5625e+02 - logprior: -1.5810e+00
Epoch 3/10
16/16 - 4s - loss: 226.4251 - loglik: -2.2454e+02 - logprior: -1.8868e+00
Epoch 4/10
16/16 - 4s - loss: 215.9669 - loglik: -2.1418e+02 - logprior: -1.7910e+00
Epoch 5/10
16/16 - 4s - loss: 208.7001 - loglik: -2.0694e+02 - logprior: -1.7555e+00
Epoch 6/10
16/16 - 4s - loss: 205.2692 - loglik: -2.0337e+02 - logprior: -1.8949e+00
Epoch 7/10
16/16 - 4s - loss: 202.5387 - loglik: -2.0051e+02 - logprior: -2.0290e+00
Epoch 8/10
16/16 - 4s - loss: 201.8093 - loglik: -1.9972e+02 - logprior: -2.0881e+00
Epoch 9/10
16/16 - 4s - loss: 200.0062 - loglik: -1.9784e+02 - logprior: -2.1644e+00
Epoch 10/10
16/16 - 4s - loss: 199.0894 - loglik: -1.9686e+02 - logprior: -2.2294e+00
Fitted a model with MAP estimate = -199.2410
expansions: [(10, 1), (12, 1), (15, 2), (16, 2), (17, 1), (18, 1), (70, 1), (72, 1), (94, 1), (97, 1)]
discards: [0 2]
Fitting a model of length 122 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 211.1917 - loglik: -2.0524e+02 - logprior: -5.9501e+00
Epoch 2/2
16/16 - 5s - loss: 199.1181 - loglik: -1.9628e+02 - logprior: -2.8338e+00
Fitted a model with MAP estimate = -197.3986
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 123 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 199.0289 - loglik: -1.9450e+02 - logprior: -4.5261e+00
Epoch 2/2
16/16 - 5s - loss: 195.0560 - loglik: -1.9330e+02 - logprior: -1.7581e+00
Fitted a model with MAP estimate = -192.9713
expansions: [(20, 1)]
discards: []
Fitting a model of length 124 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 9s - loss: 197.2885 - loglik: -1.9270e+02 - logprior: -4.5840e+00
Epoch 2/10
16/16 - 5s - loss: 193.9582 - loglik: -1.9224e+02 - logprior: -1.7197e+00
Epoch 3/10
16/16 - 5s - loss: 192.4017 - loglik: -1.9095e+02 - logprior: -1.4522e+00
Epoch 4/10
16/16 - 5s - loss: 190.2753 - loglik: -1.8888e+02 - logprior: -1.3934e+00
Epoch 5/10
16/16 - 5s - loss: 190.5623 - loglik: -1.8920e+02 - logprior: -1.3597e+00
Fitted a model with MAP estimate = -189.8412
Time for alignment: 122.0009
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 327.5449 - loglik: -3.2229e+02 - logprior: -5.2562e+00
Epoch 2/10
16/16 - 4s - loss: 255.9763 - loglik: -2.5442e+02 - logprior: -1.5562e+00
Epoch 3/10
16/16 - 4s - loss: 220.8921 - loglik: -2.1902e+02 - logprior: -1.8755e+00
Epoch 4/10
16/16 - 4s - loss: 209.1730 - loglik: -2.0741e+02 - logprior: -1.7642e+00
Epoch 5/10
16/16 - 4s - loss: 205.2578 - loglik: -2.0353e+02 - logprior: -1.7283e+00
Epoch 6/10
16/16 - 5s - loss: 201.2327 - loglik: -1.9941e+02 - logprior: -1.8262e+00
Epoch 7/10
16/16 - 4s - loss: 201.9323 - loglik: -2.0005e+02 - logprior: -1.8830e+00
Fitted a model with MAP estimate = -200.8812
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 2), (17, 1), (18, 1), (69, 1), (94, 1), (97, 1)]
discards: [0 2]
Fitting a model of length 121 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 211.0087 - loglik: -2.0519e+02 - logprior: -5.8184e+00
Epoch 2/2
16/16 - 5s - loss: 202.6936 - loglik: -1.9987e+02 - logprior: -2.8279e+00
Fitted a model with MAP estimate = -200.4887
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 122 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 9s - loss: 202.8656 - loglik: -1.9834e+02 - logprior: -4.5285e+00
Epoch 2/2
16/16 - 5s - loss: 196.4157 - loglik: -1.9467e+02 - logprior: -1.7431e+00
Fitted a model with MAP estimate = -195.8131
expansions: [(20, 1)]
discards: []
Fitting a model of length 123 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 199.9367 - loglik: -1.9538e+02 - logprior: -4.5589e+00
Epoch 2/10
16/16 - 4s - loss: 194.8643 - loglik: -1.9319e+02 - logprior: -1.6783e+00
Epoch 3/10
16/16 - 4s - loss: 193.6182 - loglik: -1.9219e+02 - logprior: -1.4272e+00
Epoch 4/10
16/16 - 5s - loss: 192.1148 - loglik: -1.9074e+02 - logprior: -1.3702e+00
Epoch 5/10
16/16 - 5s - loss: 191.1260 - loglik: -1.8979e+02 - logprior: -1.3358e+00
Epoch 6/10
16/16 - 5s - loss: 190.6533 - loglik: -1.8936e+02 - logprior: -1.2963e+00
Epoch 7/10
16/16 - 5s - loss: 190.5404 - loglik: -1.8928e+02 - logprior: -1.2574e+00
Epoch 8/10
16/16 - 5s - loss: 189.0677 - loglik: -1.8781e+02 - logprior: -1.2602e+00
Epoch 9/10
16/16 - 5s - loss: 189.9406 - loglik: -1.8862e+02 - logprior: -1.3247e+00
Fitted a model with MAP estimate = -189.5090
Time for alignment: 126.5537
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 328.1371 - loglik: -3.2289e+02 - logprior: -5.2466e+00
Epoch 2/10
16/16 - 4s - loss: 254.4784 - loglik: -2.5293e+02 - logprior: -1.5457e+00
Epoch 3/10
16/16 - 4s - loss: 217.3270 - loglik: -2.1540e+02 - logprior: -1.9251e+00
Epoch 4/10
16/16 - 4s - loss: 207.7130 - loglik: -2.0583e+02 - logprior: -1.8808e+00
Epoch 5/10
16/16 - 4s - loss: 203.8659 - loglik: -2.0204e+02 - logprior: -1.8231e+00
Epoch 6/10
16/16 - 4s - loss: 202.0223 - loglik: -2.0014e+02 - logprior: -1.8780e+00
Epoch 7/10
16/16 - 4s - loss: 200.6033 - loglik: -1.9871e+02 - logprior: -1.8920e+00
Epoch 8/10
16/16 - 4s - loss: 201.2786 - loglik: -1.9927e+02 - logprior: -2.0052e+00
Fitted a model with MAP estimate = -200.8089
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 2), (17, 2), (18, 1), (25, 4), (69, 1), (89, 1), (97, 1)]
discards: [0 2]
Fitting a model of length 126 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 211.1145 - loglik: -2.0521e+02 - logprior: -5.9008e+00
Epoch 2/2
16/16 - 5s - loss: 199.4797 - loglik: -1.9658e+02 - logprior: -2.9044e+00
Fitted a model with MAP estimate = -198.2836
expansions: [(0, 2)]
discards: [ 0 24 33 34 35]
Fitting a model of length 123 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 203.3648 - loglik: -1.9881e+02 - logprior: -4.5525e+00
Epoch 2/2
16/16 - 5s - loss: 195.3751 - loglik: -1.9356e+02 - logprior: -1.8157e+00
Fitted a model with MAP estimate = -195.6147
expansions: []
discards: []
Fitting a model of length 123 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 200.3007 - loglik: -1.9566e+02 - logprior: -4.6374e+00
Epoch 2/10
16/16 - 4s - loss: 196.1336 - loglik: -1.9437e+02 - logprior: -1.7623e+00
Epoch 3/10
16/16 - 5s - loss: 193.9738 - loglik: -1.9245e+02 - logprior: -1.5224e+00
Epoch 4/10
16/16 - 4s - loss: 193.4674 - loglik: -1.9202e+02 - logprior: -1.4464e+00
Epoch 5/10
16/16 - 5s - loss: 192.8822 - loglik: -1.9147e+02 - logprior: -1.4163e+00
Epoch 6/10
16/16 - 5s - loss: 191.9252 - loglik: -1.9055e+02 - logprior: -1.3740e+00
Epoch 7/10
16/16 - 5s - loss: 192.5115 - loglik: -1.9120e+02 - logprior: -1.3138e+00
Fitted a model with MAP estimate = -191.2430
Time for alignment: 120.6669
Computed alignments with likelihoods: ['-189.8412', '-189.5090', '-191.2430']
Best model has likelihood: -189.5090
SP score = 0.5148
Training of 3 independent models on file Rhodanese.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f179b0dc760>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f179aecefa0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f179a798580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f179a7e4c10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 308.9840 - loglik: -3.0551e+02 - logprior: -3.4714e+00
Epoch 2/10
19/19 - 3s - loss: 277.1388 - loglik: -2.7594e+02 - logprior: -1.1956e+00
Epoch 3/10
19/19 - 3s - loss: 262.8002 - loglik: -2.6135e+02 - logprior: -1.4505e+00
Epoch 4/10
19/19 - 3s - loss: 259.1261 - loglik: -2.5772e+02 - logprior: -1.4032e+00
Epoch 5/10
19/19 - 3s - loss: 257.9812 - loglik: -2.5658e+02 - logprior: -1.4054e+00
Epoch 6/10
19/19 - 3s - loss: 256.8238 - loglik: -2.5537e+02 - logprior: -1.4554e+00
Epoch 7/10
19/19 - 3s - loss: 256.7129 - loglik: -2.5522e+02 - logprior: -1.4968e+00
Epoch 8/10
19/19 - 3s - loss: 255.5303 - loglik: -2.5399e+02 - logprior: -1.5370e+00
Epoch 9/10
19/19 - 3s - loss: 256.2990 - loglik: -2.5471e+02 - logprior: -1.5886e+00
Fitted a model with MAP estimate = -243.1161
expansions: [(6, 3), (7, 2), (10, 1), (34, 10), (39, 2), (40, 1), (43, 2), (58, 1), (59, 1), (60, 1), (63, 2), (66, 3), (67, 1), (73, 1)]
discards: [0]
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 261.5543 - loglik: -2.5735e+02 - logprior: -4.2028e+00
Epoch 2/2
19/19 - 3s - loss: 251.3469 - loglik: -2.4921e+02 - logprior: -2.1395e+00
Fitted a model with MAP estimate = -235.8136
expansions: [(0, 2)]
discards: [ 0  8 45 46 47 54 87 92]
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 252.3613 - loglik: -2.4946e+02 - logprior: -2.8978e+00
Epoch 2/2
19/19 - 3s - loss: 248.9537 - loglik: -2.4783e+02 - logprior: -1.1255e+00
Fitted a model with MAP estimate = -234.5315
expansions: [(45, 3)]
discards: [0]
Fitting a model of length 104 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 236.4878 - loglik: -2.3389e+02 - logprior: -2.6015e+00
Epoch 2/10
23/23 - 4s - loss: 233.7983 - loglik: -2.3268e+02 - logprior: -1.1166e+00
Epoch 3/10
23/23 - 3s - loss: 230.7849 - loglik: -2.2971e+02 - logprior: -1.0733e+00
Epoch 4/10
23/23 - 4s - loss: 231.7131 - loglik: -2.3066e+02 - logprior: -1.0553e+00
Fitted a model with MAP estimate = -230.6554
Time for alignment: 89.4147
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 309.1203 - loglik: -3.0565e+02 - logprior: -3.4679e+00
Epoch 2/10
19/19 - 3s - loss: 277.0824 - loglik: -2.7589e+02 - logprior: -1.1963e+00
Epoch 3/10
19/19 - 3s - loss: 261.7481 - loglik: -2.6024e+02 - logprior: -1.5066e+00
Epoch 4/10
19/19 - 3s - loss: 258.0026 - loglik: -2.5655e+02 - logprior: -1.4533e+00
Epoch 5/10
19/19 - 3s - loss: 257.1033 - loglik: -2.5564e+02 - logprior: -1.4632e+00
Epoch 6/10
19/19 - 3s - loss: 255.9581 - loglik: -2.5448e+02 - logprior: -1.4754e+00
Epoch 7/10
19/19 - 3s - loss: 255.5978 - loglik: -2.5410e+02 - logprior: -1.5018e+00
Epoch 8/10
19/19 - 3s - loss: 256.2490 - loglik: -2.5473e+02 - logprior: -1.5234e+00
Fitted a model with MAP estimate = -242.0428
expansions: [(6, 3), (7, 2), (10, 1), (21, 2), (33, 9), (38, 2), (43, 2), (55, 1), (58, 2), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 261.6219 - loglik: -2.5746e+02 - logprior: -4.1585e+00
Epoch 2/2
19/19 - 3s - loss: 251.8903 - loglik: -2.4973e+02 - logprior: -2.1587e+00
Fitted a model with MAP estimate = -236.2734
expansions: [(0, 2)]
discards: [ 0  9 27 54 80 87]
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 251.8400 - loglik: -2.4892e+02 - logprior: -2.9197e+00
Epoch 2/2
19/19 - 3s - loss: 248.2750 - loglik: -2.4712e+02 - logprior: -1.1567e+00
Fitted a model with MAP estimate = -234.3042
expansions: [(45, 2)]
discards: [0]
Fitting a model of length 105 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 236.7169 - loglik: -2.3411e+02 - logprior: -2.6092e+00
Epoch 2/10
23/23 - 4s - loss: 232.1437 - loglik: -2.3101e+02 - logprior: -1.1300e+00
Epoch 3/10
23/23 - 4s - loss: 232.0551 - loglik: -2.3099e+02 - logprior: -1.0655e+00
Epoch 4/10
23/23 - 4s - loss: 231.1729 - loglik: -2.3014e+02 - logprior: -1.0337e+00
Epoch 5/10
23/23 - 4s - loss: 229.9937 - loglik: -2.2893e+02 - logprior: -1.0597e+00
Epoch 6/10
23/23 - 4s - loss: 230.1986 - loglik: -2.2912e+02 - logprior: -1.0820e+00
Fitted a model with MAP estimate = -229.2537
Time for alignment: 93.3981
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 309.1684 - loglik: -3.0570e+02 - logprior: -3.4712e+00
Epoch 2/10
19/19 - 3s - loss: 277.0421 - loglik: -2.7585e+02 - logprior: -1.1942e+00
Epoch 3/10
19/19 - 3s - loss: 262.9354 - loglik: -2.6145e+02 - logprior: -1.4837e+00
Epoch 4/10
19/19 - 3s - loss: 259.9490 - loglik: -2.5853e+02 - logprior: -1.4230e+00
Epoch 5/10
19/19 - 3s - loss: 257.8899 - loglik: -2.5645e+02 - logprior: -1.4372e+00
Epoch 6/10
19/19 - 3s - loss: 257.7603 - loglik: -2.5629e+02 - logprior: -1.4674e+00
Epoch 7/10
19/19 - 3s - loss: 256.8352 - loglik: -2.5535e+02 - logprior: -1.4898e+00
Epoch 8/10
19/19 - 3s - loss: 256.9568 - loglik: -2.5543e+02 - logprior: -1.5282e+00
Fitted a model with MAP estimate = -243.0008
expansions: [(6, 3), (7, 2), (8, 1), (10, 1), (21, 2), (38, 3), (40, 1), (42, 3), (58, 2), (60, 1), (61, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 262.5486 - loglik: -2.5841e+02 - logprior: -4.1346e+00
Epoch 2/2
19/19 - 3s - loss: 253.3242 - loglik: -2.5119e+02 - logprior: -2.1346e+00
Fitted a model with MAP estimate = -237.4939
expansions: [(0, 2), (48, 1), (53, 1)]
discards: [ 0  8 11 28 55 56 74 82]
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 254.3337 - loglik: -2.5144e+02 - logprior: -2.8949e+00
Epoch 2/2
19/19 - 3s - loss: 250.3096 - loglik: -2.4921e+02 - logprior: -1.1016e+00
Fitted a model with MAP estimate = -235.8972
expansions: [(39, 6), (56, 2)]
discards: [ 0 42]
Fitting a model of length 105 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 238.0925 - loglik: -2.3545e+02 - logprior: -2.6429e+00
Epoch 2/10
23/23 - 4s - loss: 233.4529 - loglik: -2.3229e+02 - logprior: -1.1647e+00
Epoch 3/10
23/23 - 4s - loss: 232.0248 - loglik: -2.3096e+02 - logprior: -1.0631e+00
Epoch 4/10
23/23 - 4s - loss: 231.2595 - loglik: -2.3021e+02 - logprior: -1.0458e+00
Epoch 5/10
23/23 - 4s - loss: 230.4994 - loglik: -2.2943e+02 - logprior: -1.0673e+00
Epoch 6/10
23/23 - 4s - loss: 229.6387 - loglik: -2.2853e+02 - logprior: -1.1101e+00
Epoch 7/10
23/23 - 4s - loss: 229.2646 - loglik: -2.2812e+02 - logprior: -1.1439e+00
Epoch 8/10
23/23 - 4s - loss: 228.4902 - loglik: -2.2732e+02 - logprior: -1.1707e+00
Epoch 9/10
23/23 - 4s - loss: 228.6483 - loglik: -2.2743e+02 - logprior: -1.2172e+00
Fitted a model with MAP estimate = -228.1514
Time for alignment: 104.9517
Computed alignments with likelihoods: ['-230.6554', '-229.2537', '-228.1514']
Best model has likelihood: -228.1514
SP score = 0.7820
Training of 3 independent models on file slectin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1d764986a0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20f5a34fa0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fff7c7040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2118473340>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 439.5986 - loglik: -3.8642e+02 - logprior: -5.3180e+01
Epoch 2/10
10/10 - 2s - loss: 372.2188 - loglik: -3.6024e+02 - logprior: -1.1981e+01
Epoch 3/10
10/10 - 2s - loss: 333.7888 - loglik: -3.2909e+02 - logprior: -4.6997e+00
Epoch 4/10
10/10 - 2s - loss: 311.8841 - loglik: -3.0949e+02 - logprior: -2.3917e+00
Epoch 5/10
10/10 - 2s - loss: 303.4600 - loglik: -3.0208e+02 - logprior: -1.3756e+00
Epoch 6/10
10/10 - 2s - loss: 299.5125 - loglik: -2.9872e+02 - logprior: -7.8976e-01
Epoch 7/10
10/10 - 2s - loss: 297.8368 - loglik: -2.9746e+02 - logprior: -3.7182e-01
Epoch 8/10
10/10 - 2s - loss: 297.0826 - loglik: -2.9699e+02 - logprior: -8.9605e-02
Epoch 9/10
10/10 - 2s - loss: 296.5602 - loglik: -2.9662e+02 - logprior: 0.0563
Epoch 10/10
10/10 - 2s - loss: 296.1660 - loglik: -2.9631e+02 - logprior: 0.1405
Fitted a model with MAP estimate = -295.8496
expansions: [(11, 3), (12, 1), (15, 1), (17, 2), (19, 1), (26, 1), (27, 3), (48, 1), (58, 2), (59, 1), (62, 2), (64, 1), (73, 1), (82, 1), (90, 2), (91, 6)]
discards: [0]
Fitting a model of length 133 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 349.6321 - loglik: -2.9684e+02 - logprior: -5.2796e+01
Epoch 2/2
10/10 - 3s - loss: 306.0114 - loglik: -2.8632e+02 - logprior: -1.9687e+01
Fitted a model with MAP estimate = -298.0341
expansions: [(10, 1), (13, 1)]
discards: [ 22  35  71  77 111]
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 331.7113 - loglik: -2.8313e+02 - logprior: -4.8580e+01
Epoch 2/2
10/10 - 3s - loss: 292.9020 - loglik: -2.7992e+02 - logprior: -1.2983e+01
Fitted a model with MAP estimate = -284.5783
expansions: []
discards: []
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 318.5278 - loglik: -2.7873e+02 - logprior: -3.9794e+01
Epoch 2/10
10/10 - 3s - loss: 287.0175 - loglik: -2.7937e+02 - logprior: -7.6430e+00
Epoch 3/10
10/10 - 3s - loss: 279.5357 - loglik: -2.7846e+02 - logprior: -1.0757e+00
Epoch 4/10
10/10 - 3s - loss: 276.2911 - loglik: -2.7786e+02 - logprior: 1.5645
Epoch 5/10
10/10 - 3s - loss: 274.6934 - loglik: -2.7760e+02 - logprior: 2.9018
Epoch 6/10
10/10 - 3s - loss: 273.7238 - loglik: -2.7733e+02 - logprior: 3.6027
Epoch 7/10
10/10 - 3s - loss: 273.5128 - loglik: -2.7761e+02 - logprior: 4.0986
Epoch 8/10
10/10 - 3s - loss: 272.9193 - loglik: -2.7748e+02 - logprior: 4.5649
Epoch 9/10
10/10 - 3s - loss: 272.8184 - loglik: -2.7778e+02 - logprior: 4.9578
Epoch 10/10
10/10 - 3s - loss: 272.1948 - loglik: -2.7745e+02 - logprior: 5.2536
Fitted a model with MAP estimate = -272.2971
Time for alignment: 81.2951
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 440.4033 - loglik: -3.8722e+02 - logprior: -5.3179e+01
Epoch 2/10
10/10 - 2s - loss: 371.5826 - loglik: -3.5960e+02 - logprior: -1.1983e+01
Epoch 3/10
10/10 - 2s - loss: 334.5302 - loglik: -3.2979e+02 - logprior: -4.7361e+00
Epoch 4/10
10/10 - 2s - loss: 311.7834 - loglik: -3.0927e+02 - logprior: -2.5181e+00
Epoch 5/10
10/10 - 2s - loss: 301.4938 - loglik: -2.9999e+02 - logprior: -1.5005e+00
Epoch 6/10
10/10 - 2s - loss: 298.8200 - loglik: -2.9794e+02 - logprior: -8.7795e-01
Epoch 7/10
10/10 - 2s - loss: 296.5579 - loglik: -2.9613e+02 - logprior: -4.2390e-01
Epoch 8/10
10/10 - 2s - loss: 296.4791 - loglik: -2.9634e+02 - logprior: -1.4289e-01
Epoch 9/10
10/10 - 2s - loss: 295.6843 - loglik: -2.9568e+02 - logprior: -6.6015e-03
Epoch 10/10
10/10 - 2s - loss: 295.0626 - loglik: -2.9514e+02 - logprior: 0.0775
Fitted a model with MAP estimate = -295.0306
expansions: [(7, 2), (10, 2), (17, 1), (18, 1), (26, 1), (28, 3), (42, 1), (58, 2), (59, 1), (62, 1), (65, 1), (73, 1), (82, 1), (84, 1), (91, 6)]
discards: [0]
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 347.9097 - loglik: -2.9537e+02 - logprior: -5.2540e+01
Epoch 2/2
10/10 - 3s - loss: 305.5161 - loglik: -2.8616e+02 - logprior: -1.9356e+01
Fitted a model with MAP estimate = -299.1614
expansions: [(9, 2)]
discards: [ 0 11 69]
Fitting a model of length 128 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 335.5311 - loglik: -2.8569e+02 - logprior: -4.9837e+01
Epoch 2/2
10/10 - 3s - loss: 299.8184 - loglik: -2.8291e+02 - logprior: -1.6912e+01
Fitted a model with MAP estimate = -291.7385
expansions: [(0, 3), (9, 1)]
discards: [ 0 34]
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 320.3913 - loglik: -2.8132e+02 - logprior: -3.9067e+01
Epoch 2/10
10/10 - 3s - loss: 285.8017 - loglik: -2.7836e+02 - logprior: -7.4407e+00
Epoch 3/10
10/10 - 3s - loss: 278.4761 - loglik: -2.7752e+02 - logprior: -9.5721e-01
Epoch 4/10
10/10 - 3s - loss: 275.5292 - loglik: -2.7722e+02 - logprior: 1.6862
Epoch 5/10
10/10 - 3s - loss: 274.7148 - loglik: -2.7784e+02 - logprior: 3.1280
Epoch 6/10
10/10 - 3s - loss: 272.8766 - loglik: -2.7684e+02 - logprior: 3.9650
Epoch 7/10
10/10 - 3s - loss: 273.1141 - loglik: -2.7759e+02 - logprior: 4.4717
Fitted a model with MAP estimate = -272.7411
Time for alignment: 70.5217
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 440.2776 - loglik: -3.8710e+02 - logprior: -5.3181e+01
Epoch 2/10
10/10 - 2s - loss: 371.6216 - loglik: -3.5964e+02 - logprior: -1.1980e+01
Epoch 3/10
10/10 - 2s - loss: 332.4532 - loglik: -3.2777e+02 - logprior: -4.6817e+00
Epoch 4/10
10/10 - 2s - loss: 310.3109 - loglik: -3.0797e+02 - logprior: -2.3378e+00
Epoch 5/10
10/10 - 2s - loss: 302.1656 - loglik: -3.0091e+02 - logprior: -1.2578e+00
Epoch 6/10
10/10 - 2s - loss: 298.7123 - loglik: -2.9817e+02 - logprior: -5.4185e-01
Epoch 7/10
10/10 - 2s - loss: 297.5397 - loglik: -2.9751e+02 - logprior: -2.7055e-02
Epoch 8/10
10/10 - 2s - loss: 296.5610 - loglik: -2.9677e+02 - logprior: 0.2052
Epoch 9/10
10/10 - 2s - loss: 296.5103 - loglik: -2.9682e+02 - logprior: 0.3104
Epoch 10/10
10/10 - 2s - loss: 295.5423 - loglik: -2.9593e+02 - logprior: 0.3917
Fitted a model with MAP estimate = -295.4572
expansions: [(7, 2), (10, 1), (17, 1), (18, 1), (27, 1), (28, 4), (49, 2), (58, 2), (59, 1), (62, 2), (73, 1), (85, 1), (90, 2), (91, 6)]
discards: [0]
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 347.7508 - loglik: -2.9515e+02 - logprior: -5.2600e+01
Epoch 2/2
10/10 - 3s - loss: 305.3743 - loglik: -2.8597e+02 - logprior: -1.9400e+01
Fitted a model with MAP estimate = -298.5143
expansions: [(9, 3)]
discards: [ 58  70 109]
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 330.9549 - loglik: -2.8227e+02 - logprior: -4.8687e+01
Epoch 2/2
10/10 - 3s - loss: 293.2862 - loglik: -2.8009e+02 - logprior: -1.3193e+01
Fitted a model with MAP estimate = -284.2885
expansions: []
discards: [36]
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 319.1406 - loglik: -2.7927e+02 - logprior: -3.9872e+01
Epoch 2/10
10/10 - 3s - loss: 286.5967 - loglik: -2.7892e+02 - logprior: -7.6764e+00
Epoch 3/10
10/10 - 3s - loss: 279.0494 - loglik: -2.7796e+02 - logprior: -1.0874e+00
Epoch 4/10
10/10 - 3s - loss: 276.3765 - loglik: -2.7794e+02 - logprior: 1.5642
Epoch 5/10
10/10 - 3s - loss: 274.4670 - loglik: -2.7737e+02 - logprior: 2.9045
Epoch 6/10
10/10 - 3s - loss: 273.7366 - loglik: -2.7737e+02 - logprior: 3.6288
Epoch 7/10
10/10 - 3s - loss: 273.2735 - loglik: -2.7742e+02 - logprior: 4.1496
Epoch 8/10
10/10 - 3s - loss: 272.7143 - loglik: -2.7733e+02 - logprior: 4.6138
Epoch 9/10
10/10 - 3s - loss: 272.7652 - loglik: -2.7777e+02 - logprior: 5.0008
Fitted a model with MAP estimate = -272.3823
Time for alignment: 74.2695
Computed alignments with likelihoods: ['-272.2971', '-272.7411', '-272.3823']
Best model has likelihood: -272.2971
SP score = 0.8979
Training of 3 independent models on file hip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f202d822220>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2178330f10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202edc5970>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202e519e20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 474.7150 - loglik: -1.8613e+02 - logprior: -2.8858e+02
Epoch 2/10
10/10 - 1s - loss: 234.9914 - loglik: -1.6034e+02 - logprior: -7.4654e+01
Epoch 3/10
10/10 - 1s - loss: 172.1373 - loglik: -1.3978e+02 - logprior: -3.2357e+01
Epoch 4/10
10/10 - 1s - loss: 143.9787 - loglik: -1.2661e+02 - logprior: -1.7373e+01
Epoch 5/10
10/10 - 1s - loss: 130.6446 - loglik: -1.2150e+02 - logprior: -9.1417e+00
Epoch 6/10
10/10 - 1s - loss: 123.7441 - loglik: -1.2008e+02 - logprior: -3.6661e+00
Epoch 7/10
10/10 - 1s - loss: 119.4317 - loglik: -1.1919e+02 - logprior: -2.4149e-01
Epoch 8/10
10/10 - 1s - loss: 116.2886 - loglik: -1.1819e+02 - logprior: 1.9000
Epoch 9/10
10/10 - 1s - loss: 114.0563 - loglik: -1.1765e+02 - logprior: 3.5969
Epoch 10/10
10/10 - 1s - loss: 112.7272 - loglik: -1.1753e+02 - logprior: 4.8051
Fitted a model with MAP estimate = -112.1704
expansions: [(0, 3), (10, 1), (17, 2), (23, 3), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 473.7624 - loglik: -1.1111e+02 - logprior: -3.6265e+02
Epoch 2/2
10/10 - 1s - loss: 207.7720 - loglik: -9.9147e+01 - logprior: -1.0862e+02
Fitted a model with MAP estimate = -158.2468
expansions: [(21, 1)]
discards: [ 0 29 36 47 52]
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 399.9658 - loglik: -9.8161e+01 - logprior: -3.0180e+02
Epoch 2/2
10/10 - 1s - loss: 210.3531 - loglik: -9.5943e+01 - logprior: -1.1441e+02
Fitted a model with MAP estimate = -180.2169
expansions: []
discards: []
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 369.4995 - loglik: -9.4998e+01 - logprior: -2.7450e+02
Epoch 2/10
10/10 - 1s - loss: 166.9257 - loglik: -9.4191e+01 - logprior: -7.2735e+01
Epoch 3/10
10/10 - 1s - loss: 116.5477 - loglik: -9.4427e+01 - logprior: -2.2120e+01
Epoch 4/10
10/10 - 1s - loss: 98.5188 - loglik: -9.4902e+01 - logprior: -3.6169e+00
Epoch 5/10
10/10 - 1s - loss: 89.4158 - loglik: -9.5343e+01 - logprior: 5.9274
Epoch 6/10
10/10 - 1s - loss: 84.1596 - loglik: -9.5714e+01 - logprior: 11.5541
Epoch 7/10
10/10 - 1s - loss: 80.8281 - loglik: -9.5976e+01 - logprior: 15.1477
Epoch 8/10
10/10 - 1s - loss: 78.4884 - loglik: -9.6145e+01 - logprior: 17.6569
Epoch 9/10
10/10 - 1s - loss: 76.6947 - loglik: -9.6288e+01 - logprior: 19.5929
Epoch 10/10
10/10 - 1s - loss: 75.2092 - loglik: -9.6409e+01 - logprior: 21.1997
Fitted a model with MAP estimate = -74.4705
Time for alignment: 35.3254
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 474.7150 - loglik: -1.8613e+02 - logprior: -2.8858e+02
Epoch 2/10
10/10 - 1s - loss: 234.9914 - loglik: -1.6034e+02 - logprior: -7.4654e+01
Epoch 3/10
10/10 - 1s - loss: 172.1373 - loglik: -1.3978e+02 - logprior: -3.2357e+01
Epoch 4/10
10/10 - 1s - loss: 143.9787 - loglik: -1.2661e+02 - logprior: -1.7373e+01
Epoch 5/10
10/10 - 1s - loss: 130.6446 - loglik: -1.2150e+02 - logprior: -9.1417e+00
Epoch 6/10
10/10 - 1s - loss: 123.7441 - loglik: -1.2008e+02 - logprior: -3.6661e+00
Epoch 7/10
10/10 - 1s - loss: 119.4317 - loglik: -1.1919e+02 - logprior: -2.4149e-01
Epoch 8/10
10/10 - 1s - loss: 116.2886 - loglik: -1.1819e+02 - logprior: 1.9000
Epoch 9/10
10/10 - 1s - loss: 114.0563 - loglik: -1.1765e+02 - logprior: 3.5969
Epoch 10/10
10/10 - 1s - loss: 112.7272 - loglik: -1.1753e+02 - logprior: 4.8051
Fitted a model with MAP estimate = -112.1704
expansions: [(0, 3), (10, 1), (17, 2), (23, 3), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 473.7624 - loglik: -1.1111e+02 - logprior: -3.6265e+02
Epoch 2/2
10/10 - 1s - loss: 207.7720 - loglik: -9.9147e+01 - logprior: -1.0862e+02
Fitted a model with MAP estimate = -158.2468
expansions: [(21, 1)]
discards: [ 0 29 36 47 52]
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 399.9658 - loglik: -9.8161e+01 - logprior: -3.0180e+02
Epoch 2/2
10/10 - 1s - loss: 210.3531 - loglik: -9.5943e+01 - logprior: -1.1441e+02
Fitted a model with MAP estimate = -180.2169
expansions: []
discards: []
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 369.4995 - loglik: -9.4998e+01 - logprior: -2.7450e+02
Epoch 2/10
10/10 - 1s - loss: 166.9257 - loglik: -9.4191e+01 - logprior: -7.2735e+01
Epoch 3/10
10/10 - 1s - loss: 116.5477 - loglik: -9.4427e+01 - logprior: -2.2120e+01
Epoch 4/10
10/10 - 1s - loss: 98.5188 - loglik: -9.4902e+01 - logprior: -3.6169e+00
Epoch 5/10
10/10 - 1s - loss: 89.4158 - loglik: -9.5343e+01 - logprior: 5.9274
Epoch 6/10
10/10 - 1s - loss: 84.1596 - loglik: -9.5714e+01 - logprior: 11.5541
Epoch 7/10
10/10 - 1s - loss: 80.8281 - loglik: -9.5976e+01 - logprior: 15.1477
Epoch 8/10
10/10 - 1s - loss: 78.4884 - loglik: -9.6145e+01 - logprior: 17.6569
Epoch 9/10
10/10 - 1s - loss: 76.6947 - loglik: -9.6288e+01 - logprior: 19.5929
Epoch 10/10
10/10 - 1s - loss: 75.2092 - loglik: -9.6409e+01 - logprior: 21.1997
Fitted a model with MAP estimate = -74.4705
Time for alignment: 34.4951
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 474.7150 - loglik: -1.8613e+02 - logprior: -2.8858e+02
Epoch 2/10
10/10 - 1s - loss: 234.9914 - loglik: -1.6034e+02 - logprior: -7.4654e+01
Epoch 3/10
10/10 - 1s - loss: 172.1373 - loglik: -1.3978e+02 - logprior: -3.2357e+01
Epoch 4/10
10/10 - 1s - loss: 143.9787 - loglik: -1.2661e+02 - logprior: -1.7373e+01
Epoch 5/10
10/10 - 1s - loss: 130.6446 - loglik: -1.2150e+02 - logprior: -9.1417e+00
Epoch 6/10
10/10 - 1s - loss: 123.7441 - loglik: -1.2008e+02 - logprior: -3.6661e+00
Epoch 7/10
10/10 - 1s - loss: 119.4317 - loglik: -1.1919e+02 - logprior: -2.4149e-01
Epoch 8/10
10/10 - 1s - loss: 116.2886 - loglik: -1.1819e+02 - logprior: 1.9000
Epoch 9/10
10/10 - 1s - loss: 114.0563 - loglik: -1.1765e+02 - logprior: 3.5969
Epoch 10/10
10/10 - 1s - loss: 112.7272 - loglik: -1.1753e+02 - logprior: 4.8051
Fitted a model with MAP estimate = -112.1704
expansions: [(0, 3), (10, 1), (17, 2), (23, 3), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 473.7624 - loglik: -1.1111e+02 - logprior: -3.6265e+02
Epoch 2/2
10/10 - 1s - loss: 207.7720 - loglik: -9.9147e+01 - logprior: -1.0862e+02
Fitted a model with MAP estimate = -158.2468
expansions: [(21, 1)]
discards: [ 0 29 36 47 52]
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 399.9658 - loglik: -9.8161e+01 - logprior: -3.0180e+02
Epoch 2/2
10/10 - 1s - loss: 210.3531 - loglik: -9.5943e+01 - logprior: -1.1441e+02
Fitted a model with MAP estimate = -180.2169
expansions: []
discards: []
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 369.4995 - loglik: -9.4998e+01 - logprior: -2.7450e+02
Epoch 2/10
10/10 - 1s - loss: 166.9257 - loglik: -9.4191e+01 - logprior: -7.2735e+01
Epoch 3/10
10/10 - 1s - loss: 116.5477 - loglik: -9.4427e+01 - logprior: -2.2120e+01
Epoch 4/10
10/10 - 1s - loss: 98.5188 - loglik: -9.4902e+01 - logprior: -3.6169e+00
Epoch 5/10
10/10 - 1s - loss: 89.4158 - loglik: -9.5343e+01 - logprior: 5.9274
Epoch 6/10
10/10 - 1s - loss: 84.1596 - loglik: -9.5714e+01 - logprior: 11.5541
Epoch 7/10
10/10 - 1s - loss: 80.8281 - loglik: -9.5976e+01 - logprior: 15.1477
Epoch 8/10
10/10 - 1s - loss: 78.4884 - loglik: -9.6145e+01 - logprior: 17.6569
Epoch 9/10
10/10 - 1s - loss: 76.6947 - loglik: -9.6288e+01 - logprior: 19.5929
Epoch 10/10
10/10 - 1s - loss: 75.2092 - loglik: -9.6409e+01 - logprior: 21.1997
Fitted a model with MAP estimate = -74.4705
Time for alignment: 35.0101
Computed alignments with likelihoods: ['-74.4705', '-74.4705', '-74.4705']
Best model has likelihood: -74.4705
SP score = 0.8293
Training of 3 independent models on file peroxidase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f202c1a7d30>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f20a166c0a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20a166c490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20a166c1c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 200 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 12s - loss: 736.1011 - loglik: -7.2650e+02 - logprior: -9.5966e+00
Epoch 2/10
13/13 - 9s - loss: 643.9839 - loglik: -6.4254e+02 - logprior: -1.4480e+00
Epoch 3/10
13/13 - 9s - loss: 590.7602 - loglik: -5.8942e+02 - logprior: -1.3363e+00
Epoch 4/10
13/13 - 9s - loss: 577.1210 - loglik: -5.7552e+02 - logprior: -1.6027e+00
Epoch 5/10
13/13 - 9s - loss: 573.6934 - loglik: -5.7229e+02 - logprior: -1.4064e+00
Epoch 6/10
13/13 - 9s - loss: 570.5137 - loglik: -5.6911e+02 - logprior: -1.4067e+00
Epoch 7/10
13/13 - 9s - loss: 568.1078 - loglik: -5.6660e+02 - logprior: -1.5076e+00
Epoch 8/10
13/13 - 9s - loss: 568.6412 - loglik: -5.6708e+02 - logprior: -1.5594e+00
Fitted a model with MAP estimate = -568.1862
expansions: [(31, 1), (57, 1), (110, 1), (163, 2), (173, 1)]
discards: [0]
Fitting a model of length 205 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 13s - loss: 582.4087 - loglik: -5.7271e+02 - logprior: -9.7027e+00
Epoch 2/2
13/13 - 10s - loss: 573.7994 - loglik: -5.6989e+02 - logprior: -3.9112e+00
Fitted a model with MAP estimate = -571.1702
expansions: [(0, 7)]
discards: [0]
Fitting a model of length 211 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 15s - loss: 576.3123 - loglik: -5.6873e+02 - logprior: -7.5844e+00
Epoch 2/2
13/13 - 10s - loss: 567.8885 - loglik: -5.6612e+02 - logprior: -1.7723e+00
Fitted a model with MAP estimate = -566.2141
expansions: [(65, 1)]
discards: [  0   1   2   3   4   5 171]
Fitting a model of length 205 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 13s - loss: 578.6683 - loglik: -5.6933e+02 - logprior: -9.3349e+00
Epoch 2/10
13/13 - 10s - loss: 570.5572 - loglik: -5.6728e+02 - logprior: -3.2770e+00
Epoch 3/10
13/13 - 10s - loss: 566.9988 - loglik: -5.6541e+02 - logprior: -1.5884e+00
Epoch 4/10
13/13 - 10s - loss: 565.9500 - loglik: -5.6520e+02 - logprior: -7.5407e-01
Epoch 5/10
13/13 - 10s - loss: 565.3854 - loglik: -5.6469e+02 - logprior: -6.9515e-01
Epoch 6/10
13/13 - 10s - loss: 563.7283 - loglik: -5.6300e+02 - logprior: -7.2661e-01
Epoch 7/10
13/13 - 10s - loss: 563.5021 - loglik: -5.6277e+02 - logprior: -7.3641e-01
Epoch 8/10
13/13 - 10s - loss: 563.9598 - loglik: -5.6326e+02 - logprior: -6.9890e-01
Fitted a model with MAP estimate = -563.0399
Time for alignment: 244.1227
Fitting a model of length 200 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 12s - loss: 735.3798 - loglik: -7.2579e+02 - logprior: -9.5914e+00
Epoch 2/10
13/13 - 9s - loss: 645.6530 - loglik: -6.4423e+02 - logprior: -1.4212e+00
Epoch 3/10
13/13 - 9s - loss: 593.2029 - loglik: -5.9206e+02 - logprior: -1.1473e+00
Epoch 4/10
13/13 - 9s - loss: 579.4261 - loglik: -5.7809e+02 - logprior: -1.3370e+00
Epoch 5/10
13/13 - 9s - loss: 574.2299 - loglik: -5.7308e+02 - logprior: -1.1534e+00
Epoch 6/10
13/13 - 9s - loss: 572.7391 - loglik: -5.7159e+02 - logprior: -1.1486e+00
Epoch 7/10
13/13 - 9s - loss: 571.8673 - loglik: -5.7053e+02 - logprior: -1.3402e+00
Epoch 8/10
13/13 - 9s - loss: 568.2475 - loglik: -5.6667e+02 - logprior: -1.5737e+00
Epoch 9/10
13/13 - 9s - loss: 568.1881 - loglik: -5.6662e+02 - logprior: -1.5690e+00
Epoch 10/10
13/13 - 9s - loss: 566.0376 - loglik: -5.6439e+02 - logprior: -1.6505e+00
Fitted a model with MAP estimate = -566.3875
expansions: [(10, 1), (108, 4), (161, 2), (163, 1), (173, 1)]
discards: [0]
Fitting a model of length 208 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 13s - loss: 580.9471 - loglik: -5.7113e+02 - logprior: -9.8169e+00
Epoch 2/2
13/13 - 10s - loss: 569.4888 - loglik: -5.6539e+02 - logprior: -4.1029e+00
Fitted a model with MAP estimate = -567.0873
expansions: [(0, 7), (120, 1)]
discards: [  0 166 167]
Fitting a model of length 213 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 15s - loss: 573.9372 - loglik: -5.6621e+02 - logprior: -7.7266e+00
Epoch 2/2
13/13 - 10s - loss: 565.2822 - loglik: -5.6333e+02 - logprior: -1.9548e+00
Fitted a model with MAP estimate = -563.0816
expansions: [(173, 2)]
discards: [  1   2   3   4   5   6 116 117]
Fitting a model of length 207 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 13s - loss: 572.1591 - loglik: -5.6486e+02 - logprior: -7.3030e+00
Epoch 2/10
13/13 - 10s - loss: 567.5349 - loglik: -5.6577e+02 - logprior: -1.7668e+00
Epoch 3/10
13/13 - 10s - loss: 562.0071 - loglik: -5.6083e+02 - logprior: -1.1731e+00
Epoch 4/10
13/13 - 10s - loss: 565.1906 - loglik: -5.6425e+02 - logprior: -9.4290e-01
Fitted a model with MAP estimate = -562.3264
Time for alignment: 224.6798
Fitting a model of length 200 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 12s - loss: 735.3214 - loglik: -7.2574e+02 - logprior: -9.5816e+00
Epoch 2/10
13/13 - 9s - loss: 642.1721 - loglik: -6.4076e+02 - logprior: -1.4119e+00
Epoch 3/10
13/13 - 9s - loss: 592.5167 - loglik: -5.9117e+02 - logprior: -1.3439e+00
Epoch 4/10
13/13 - 9s - loss: 579.3468 - loglik: -5.7779e+02 - logprior: -1.5618e+00
Epoch 5/10
13/13 - 9s - loss: 575.7589 - loglik: -5.7440e+02 - logprior: -1.3612e+00
Epoch 6/10
13/13 - 9s - loss: 574.2759 - loglik: -5.7300e+02 - logprior: -1.2761e+00
Epoch 7/10
13/13 - 9s - loss: 573.0758 - loglik: -5.7170e+02 - logprior: -1.3801e+00
Epoch 8/10
13/13 - 9s - loss: 572.3634 - loglik: -5.7096e+02 - logprior: -1.4055e+00
Epoch 9/10
13/13 - 9s - loss: 571.6879 - loglik: -5.7024e+02 - logprior: -1.4447e+00
Epoch 10/10
13/13 - 9s - loss: 572.1052 - loglik: -5.7063e+02 - logprior: -1.4725e+00
Fitted a model with MAP estimate = -571.6043
expansions: [(173, 1)]
discards: []
Fitting a model of length 201 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 12s - loss: 581.4077 - loglik: -5.7401e+02 - logprior: -7.3996e+00
Epoch 2/2
13/13 - 9s - loss: 576.1363 - loglik: -5.7433e+02 - logprior: -1.8105e+00
Fitted a model with MAP estimate = -573.3690
expansions: [(137, 1)]
discards: []
Fitting a model of length 202 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 14s - loss: 580.2753 - loglik: -5.7312e+02 - logprior: -7.1523e+00
Epoch 2/2
13/13 - 9s - loss: 573.1164 - loglik: -5.7149e+02 - logprior: -1.6280e+00
Fitted a model with MAP estimate = -572.0443
expansions: []
discards: []
Fitting a model of length 202 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 12s - loss: 579.2510 - loglik: -5.7222e+02 - logprior: -7.0282e+00
Epoch 2/10
13/13 - 9s - loss: 572.9542 - loglik: -5.7145e+02 - logprior: -1.5028e+00
Epoch 3/10
13/13 - 10s - loss: 571.3865 - loglik: -5.7040e+02 - logprior: -9.8555e-01
Epoch 4/10
13/13 - 9s - loss: 570.1335 - loglik: -5.6942e+02 - logprior: -7.1135e-01
Epoch 5/10
13/13 - 9s - loss: 570.1662 - loglik: -5.6950e+02 - logprior: -6.6652e-01
Fitted a model with MAP estimate = -569.3491
Time for alignment: 229.0014
Computed alignments with likelihoods: ['-563.0399', '-562.3264', '-569.3491']
Best model has likelihood: -562.3264
SP score = 0.5974
Training of 3 independent models on file TNF.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1791b92e80>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1791b92130>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f178822d400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1798672670>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 434.1614 - loglik: -3.4551e+02 - logprior: -8.8654e+01
Epoch 2/10
10/10 - 2s - loss: 339.1028 - loglik: -3.1891e+02 - logprior: -2.0196e+01
Epoch 3/10
10/10 - 2s - loss: 298.8327 - loglik: -2.9156e+02 - logprior: -7.2757e+00
Epoch 4/10
10/10 - 2s - loss: 278.4282 - loglik: -2.7555e+02 - logprior: -2.8766e+00
Epoch 5/10
10/10 - 2s - loss: 270.6681 - loglik: -2.6997e+02 - logprior: -6.9308e-01
Epoch 6/10
10/10 - 2s - loss: 267.2345 - loglik: -2.6777e+02 - logprior: 0.5372
Epoch 7/10
10/10 - 2s - loss: 264.8380 - loglik: -2.6618e+02 - logprior: 1.3451
Epoch 8/10
10/10 - 2s - loss: 263.4468 - loglik: -2.6529e+02 - logprior: 1.8430
Epoch 9/10
10/10 - 2s - loss: 262.2512 - loglik: -2.6447e+02 - logprior: 2.2187
Epoch 10/10
10/10 - 2s - loss: 261.5061 - loglik: -2.6403e+02 - logprior: 2.5228
Fitted a model with MAP estimate = -261.2450
expansions: [(9, 2), (10, 2), (12, 4), (18, 2), (36, 4), (45, 3), (54, 2), (64, 3), (83, 4), (86, 1), (89, 1)]
discards: [0]
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 351.3691 - loglik: -2.6228e+02 - logprior: -8.9094e+01
Epoch 2/2
10/10 - 2s - loss: 282.9169 - loglik: -2.4954e+02 - logprior: -3.3376e+01
Fitted a model with MAP estimate = -271.2946
expansions: [(0, 3), (82, 2), (113, 3)]
discards: [  0  45  46  71 106 107]
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 315.8582 - loglik: -2.4731e+02 - logprior: -6.8543e+01
Epoch 2/2
10/10 - 2s - loss: 255.9552 - loglik: -2.4157e+02 - logprior: -1.4386e+01
Fitted a model with MAP estimate = -246.5027
expansions: [(116, 3)]
discards: [ 0  1 80]
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 325.9408 - loglik: -2.4292e+02 - logprior: -8.3021e+01
Epoch 2/10
10/10 - 2s - loss: 266.0036 - loglik: -2.3998e+02 - logprior: -2.6023e+01
Epoch 3/10
10/10 - 2s - loss: 244.8024 - loglik: -2.3840e+02 - logprior: -6.3992e+00
Epoch 4/10
10/10 - 2s - loss: 235.8467 - loglik: -2.3780e+02 - logprior: 1.9540
Epoch 5/10
10/10 - 2s - loss: 232.1163 - loglik: -2.3721e+02 - logprior: 5.0894
Epoch 6/10
10/10 - 2s - loss: 230.5052 - loglik: -2.3722e+02 - logprior: 6.7119
Epoch 7/10
10/10 - 2s - loss: 229.5966 - loglik: -2.3731e+02 - logprior: 7.7155
Epoch 8/10
10/10 - 2s - loss: 228.8876 - loglik: -2.3732e+02 - logprior: 8.4298
Epoch 9/10
10/10 - 2s - loss: 228.1668 - loglik: -2.3718e+02 - logprior: 9.0118
Epoch 10/10
10/10 - 2s - loss: 227.8820 - loglik: -2.3742e+02 - logprior: 9.5355
Fitted a model with MAP estimate = -227.6424
Time for alignment: 60.1680
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 433.9772 - loglik: -3.4533e+02 - logprior: -8.8651e+01
Epoch 2/10
10/10 - 2s - loss: 339.2647 - loglik: -3.1906e+02 - logprior: -2.0202e+01
Epoch 3/10
10/10 - 2s - loss: 298.1460 - loglik: -2.9080e+02 - logprior: -7.3420e+00
Epoch 4/10
10/10 - 2s - loss: 278.1474 - loglik: -2.7520e+02 - logprior: -2.9473e+00
Epoch 5/10
10/10 - 2s - loss: 271.1987 - loglik: -2.7043e+02 - logprior: -7.6588e-01
Epoch 6/10
10/10 - 2s - loss: 267.7935 - loglik: -2.6818e+02 - logprior: 0.3858
Epoch 7/10
10/10 - 2s - loss: 265.7327 - loglik: -2.6689e+02 - logprior: 1.1600
Epoch 8/10
10/10 - 2s - loss: 264.2161 - loglik: -2.6593e+02 - logprior: 1.7126
Epoch 9/10
10/10 - 2s - loss: 263.1113 - loglik: -2.6516e+02 - logprior: 2.0514
Epoch 10/10
10/10 - 2s - loss: 262.8887 - loglik: -2.6523e+02 - logprior: 2.3379
Fitted a model with MAP estimate = -262.5321
expansions: [(9, 2), (10, 2), (12, 4), (18, 2), (36, 4), (45, 2), (50, 1), (55, 1), (83, 4), (86, 1), (89, 1)]
discards: [0]
Fitting a model of length 120 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 352.6906 - loglik: -2.6365e+02 - logprior: -8.9043e+01
Epoch 2/2
10/10 - 2s - loss: 286.2863 - loglik: -2.5267e+02 - logprior: -3.3614e+01
Fitted a model with MAP estimate = -274.5280
expansions: [(0, 3), (60, 1), (81, 4), (109, 3), (110, 2)]
discards: [  0 102 103]
Fitting a model of length 130 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 317.0564 - loglik: -2.4767e+02 - logprior: -6.9384e+01
Epoch 2/2
10/10 - 2s - loss: 254.7002 - loglik: -2.4039e+02 - logprior: -1.4308e+01
Fitted a model with MAP estimate = -244.4802
expansions: [(118, 2)]
discards: [ 0  1 47]
Fitting a model of length 129 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 323.8824 - loglik: -2.4128e+02 - logprior: -8.2606e+01
Epoch 2/10
10/10 - 2s - loss: 263.2027 - loglik: -2.3821e+02 - logprior: -2.4992e+01
Epoch 3/10
10/10 - 2s - loss: 241.5989 - loglik: -2.3600e+02 - logprior: -5.6031e+00
Epoch 4/10
10/10 - 2s - loss: 232.8789 - loglik: -2.3490e+02 - logprior: 2.0214
Epoch 5/10
10/10 - 2s - loss: 229.7354 - loglik: -2.3481e+02 - logprior: 5.0794
Epoch 6/10
10/10 - 2s - loss: 228.1757 - loglik: -2.3489e+02 - logprior: 6.7130
Epoch 7/10
10/10 - 2s - loss: 227.1542 - loglik: -2.3488e+02 - logprior: 7.7259
Epoch 8/10
10/10 - 2s - loss: 225.8431 - loglik: -2.3428e+02 - logprior: 8.4341
Epoch 9/10
10/10 - 2s - loss: 225.5980 - loglik: -2.3461e+02 - logprior: 9.0073
Epoch 10/10
10/10 - 2s - loss: 225.1186 - loglik: -2.3465e+02 - logprior: 9.5289
Fitted a model with MAP estimate = -224.8589
Time for alignment: 60.2812
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 434.2464 - loglik: -3.4559e+02 - logprior: -8.8656e+01
Epoch 2/10
10/10 - 2s - loss: 339.2673 - loglik: -3.1907e+02 - logprior: -2.0196e+01
Epoch 3/10
10/10 - 2s - loss: 299.3027 - loglik: -2.9194e+02 - logprior: -7.3624e+00
Epoch 4/10
10/10 - 2s - loss: 278.3963 - loglik: -2.7542e+02 - logprior: -2.9777e+00
Epoch 5/10
10/10 - 2s - loss: 271.3170 - loglik: -2.7049e+02 - logprior: -8.2226e-01
Epoch 6/10
10/10 - 2s - loss: 268.3361 - loglik: -2.6876e+02 - logprior: 0.4253
Epoch 7/10
10/10 - 2s - loss: 267.1173 - loglik: -2.6837e+02 - logprior: 1.2481
Epoch 8/10
10/10 - 2s - loss: 266.1143 - loglik: -2.6792e+02 - logprior: 1.8104
Epoch 9/10
10/10 - 2s - loss: 265.3331 - loglik: -2.6749e+02 - logprior: 2.1526
Epoch 10/10
10/10 - 2s - loss: 265.0104 - loglik: -2.6738e+02 - logprior: 2.3662
Fitted a model with MAP estimate = -264.5149
expansions: [(5, 1), (6, 1), (9, 1), (10, 2), (12, 3), (19, 2), (36, 4), (45, 1), (46, 1), (79, 2), (84, 3), (86, 1), (88, 3), (89, 1)]
discards: [0]
Fitting a model of length 122 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 355.4339 - loglik: -2.6642e+02 - logprior: -8.9010e+01
Epoch 2/2
10/10 - 2s - loss: 287.7924 - loglik: -2.5434e+02 - logprior: -3.3456e+01
Fitted a model with MAP estimate = -275.5183
expansions: [(0, 3), (17, 1), (108, 3)]
discards: [  0  10  45  46 102]
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 316.5913 - loglik: -2.4835e+02 - logprior: -6.8244e+01
Epoch 2/2
10/10 - 2s - loss: 255.8873 - loglik: -2.4182e+02 - logprior: -1.4065e+01
Fitted a model with MAP estimate = -246.3690
expansions: []
discards: [0 1]
Fitting a model of length 122 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 323.9146 - loglik: -2.4298e+02 - logprior: -8.0930e+01
Epoch 2/10
10/10 - 2s - loss: 261.9970 - loglik: -2.4081e+02 - logprior: -2.1189e+01
Epoch 3/10
10/10 - 2s - loss: 243.5293 - loglik: -2.3959e+02 - logprior: -3.9369e+00
Epoch 4/10
10/10 - 2s - loss: 236.9618 - loglik: -2.3908e+02 - logprior: 2.1181
Epoch 5/10
10/10 - 2s - loss: 234.2308 - loglik: -2.3920e+02 - logprior: 4.9654
Epoch 6/10
10/10 - 2s - loss: 232.6661 - loglik: -2.3923e+02 - logprior: 6.5600
Epoch 7/10
10/10 - 2s - loss: 231.2375 - loglik: -2.3878e+02 - logprior: 7.5382
Epoch 8/10
10/10 - 2s - loss: 230.8010 - loglik: -2.3903e+02 - logprior: 8.2282
Epoch 9/10
10/10 - 2s - loss: 230.8103 - loglik: -2.3962e+02 - logprior: 8.8115
Fitted a model with MAP estimate = -230.0979
Time for alignment: 57.4809
Computed alignments with likelihoods: ['-227.6424', '-224.8589', '-230.0979']
Best model has likelihood: -224.8589
SP score = 0.7820
Training of 3 independent models on file egf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1798668880>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f21293694f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17abf51bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dfc1426d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 97.3454 - loglik: -9.2767e+01 - logprior: -4.5779e+00
Epoch 2/10
17/17 - 1s - loss: 74.7858 - loglik: -7.3211e+01 - logprior: -1.5752e+00
Epoch 3/10
17/17 - 1s - loss: 64.8899 - loglik: -6.3221e+01 - logprior: -1.6685e+00
Epoch 4/10
17/17 - 1s - loss: 63.1527 - loglik: -6.1511e+01 - logprior: -1.6420e+00
Epoch 5/10
17/17 - 1s - loss: 62.9090 - loglik: -6.1331e+01 - logprior: -1.5780e+00
Epoch 6/10
17/17 - 1s - loss: 62.8288 - loglik: -6.1195e+01 - logprior: -1.6335e+00
Epoch 7/10
17/17 - 1s - loss: 62.5540 - loglik: -6.0937e+01 - logprior: -1.6168e+00
Epoch 8/10
17/17 - 1s - loss: 62.5014 - loglik: -6.0887e+01 - logprior: -1.6148e+00
Epoch 9/10
17/17 - 1s - loss: 62.5356 - loglik: -6.0929e+01 - logprior: -1.6069e+00
Fitted a model with MAP estimate = -62.4623
expansions: [(2, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 6s - loss: 69.9530 - loglik: -6.4277e+01 - logprior: -5.6760e+00
Epoch 2/2
17/17 - 1s - loss: 61.4671 - loglik: -5.8622e+01 - logprior: -2.8450e+00
Fitted a model with MAP estimate = -59.7038
expansions: [(2, 1)]
discards: [ 0 13 16]
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 64.2343 - loglik: -5.9249e+01 - logprior: -4.9851e+00
Epoch 2/2
17/17 - 1s - loss: 58.7336 - loglik: -5.6964e+01 - logprior: -1.7693e+00
Fitted a model with MAP estimate = -58.3818
expansions: []
discards: []
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 61.6203 - loglik: -5.7320e+01 - logprior: -4.3003e+00
Epoch 2/10
17/17 - 1s - loss: 58.6728 - loglik: -5.6977e+01 - logprior: -1.6959e+00
Epoch 3/10
17/17 - 1s - loss: 58.0775 - loglik: -5.6600e+01 - logprior: -1.4771e+00
Epoch 4/10
17/17 - 1s - loss: 58.0708 - loglik: -5.6646e+01 - logprior: -1.4247e+00
Epoch 5/10
17/17 - 1s - loss: 57.9164 - loglik: -5.6509e+01 - logprior: -1.4072e+00
Epoch 6/10
17/17 - 1s - loss: 58.0117 - loglik: -5.6595e+01 - logprior: -1.4165e+00
Fitted a model with MAP estimate = -57.8324
Time for alignment: 40.9179
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 97.3752 - loglik: -9.2801e+01 - logprior: -4.5743e+00
Epoch 2/10
17/17 - 1s - loss: 74.9529 - loglik: -7.3392e+01 - logprior: -1.5608e+00
Epoch 3/10
17/17 - 1s - loss: 65.2206 - loglik: -6.3541e+01 - logprior: -1.6798e+00
Epoch 4/10
17/17 - 1s - loss: 63.7217 - loglik: -6.2086e+01 - logprior: -1.6355e+00
Epoch 5/10
17/17 - 1s - loss: 63.2545 - loglik: -6.1657e+01 - logprior: -1.5974e+00
Epoch 6/10
17/17 - 1s - loss: 62.9890 - loglik: -6.1342e+01 - logprior: -1.6474e+00
Epoch 7/10
17/17 - 1s - loss: 63.0058 - loglik: -6.1374e+01 - logprior: -1.6316e+00
Fitted a model with MAP estimate = -62.8900
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 6s - loss: 70.2036 - loglik: -6.4563e+01 - logprior: -5.6402e+00
Epoch 2/2
17/17 - 1s - loss: 61.6614 - loglik: -5.9099e+01 - logprior: -2.5623e+00
Fitted a model with MAP estimate = -59.4108
expansions: []
discards: [13 16]
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 62.2294 - loglik: -5.7839e+01 - logprior: -4.3899e+00
Epoch 2/2
17/17 - 1s - loss: 58.6779 - loglik: -5.6958e+01 - logprior: -1.7195e+00
Fitted a model with MAP estimate = -58.3281
expansions: []
discards: []
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 61.5796 - loglik: -5.7280e+01 - logprior: -4.2994e+00
Epoch 2/10
17/17 - 1s - loss: 58.6771 - loglik: -5.6975e+01 - logprior: -1.7018e+00
Epoch 3/10
17/17 - 1s - loss: 58.1558 - loglik: -5.6680e+01 - logprior: -1.4759e+00
Epoch 4/10
17/17 - 1s - loss: 58.0733 - loglik: -5.6653e+01 - logprior: -1.4206e+00
Epoch 5/10
17/17 - 1s - loss: 57.9252 - loglik: -5.6502e+01 - logprior: -1.4231e+00
Epoch 6/10
17/17 - 1s - loss: 57.9387 - loglik: -5.6527e+01 - logprior: -1.4120e+00
Fitted a model with MAP estimate = -57.8330
Time for alignment: 37.8529
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 97.2581 - loglik: -9.2684e+01 - logprior: -4.5739e+00
Epoch 2/10
17/17 - 1s - loss: 74.6783 - loglik: -7.3108e+01 - logprior: -1.5703e+00
Epoch 3/10
17/17 - 1s - loss: 65.2437 - loglik: -6.3565e+01 - logprior: -1.6787e+00
Epoch 4/10
17/17 - 1s - loss: 63.6924 - loglik: -6.2061e+01 - logprior: -1.6312e+00
Epoch 5/10
17/17 - 1s - loss: 63.1561 - loglik: -6.1570e+01 - logprior: -1.5862e+00
Epoch 6/10
17/17 - 1s - loss: 63.1062 - loglik: -6.1464e+01 - logprior: -1.6417e+00
Epoch 7/10
17/17 - 1s - loss: 62.9069 - loglik: -6.1281e+01 - logprior: -1.6256e+00
Epoch 8/10
17/17 - 1s - loss: 62.8946 - loglik: -6.1269e+01 - logprior: -1.6257e+00
Epoch 9/10
17/17 - 1s - loss: 62.8076 - loglik: -6.1188e+01 - logprior: -1.6192e+00
Epoch 10/10
17/17 - 1s - loss: 62.8563 - loglik: -6.1244e+01 - logprior: -1.6124e+00
Fitted a model with MAP estimate = -62.7965
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 6s - loss: 70.3459 - loglik: -6.4709e+01 - logprior: -5.6371e+00
Epoch 2/2
17/17 - 1s - loss: 61.8913 - loglik: -5.9262e+01 - logprior: -2.6297e+00
Fitted a model with MAP estimate = -59.6955
expansions: []
discards: [13 16]
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 62.3400 - loglik: -5.7927e+01 - logprior: -4.4125e+00
Epoch 2/2
17/17 - 1s - loss: 58.7502 - loglik: -5.7029e+01 - logprior: -1.7210e+00
Fitted a model with MAP estimate = -58.3159
expansions: []
discards: []
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 61.6689 - loglik: -5.7371e+01 - logprior: -4.2980e+00
Epoch 2/10
17/17 - 1s - loss: 58.5254 - loglik: -5.6830e+01 - logprior: -1.6957e+00
Epoch 3/10
17/17 - 1s - loss: 58.2930 - loglik: -5.6815e+01 - logprior: -1.4784e+00
Epoch 4/10
17/17 - 1s - loss: 57.9598 - loglik: -5.6531e+01 - logprior: -1.4292e+00
Epoch 5/10
17/17 - 1s - loss: 57.8715 - loglik: -5.6458e+01 - logprior: -1.4133e+00
Epoch 6/10
17/17 - 1s - loss: 58.0000 - loglik: -5.6596e+01 - logprior: -1.4036e+00
Fitted a model with MAP estimate = -57.8368
Time for alignment: 41.1306
Computed alignments with likelihoods: ['-57.8324', '-57.8330', '-57.8368']
Best model has likelihood: -57.8324
SP score = 0.7673
Training of 3 independent models on file HMG_box.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1d75c75eb0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1d75c75400>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1d75c75be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f179bc7df40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 194.2581 - loglik: -1.8550e+02 - logprior: -8.7596e+00
Epoch 2/10
13/13 - 1s - loss: 163.7818 - loglik: -1.6145e+02 - logprior: -2.3305e+00
Epoch 3/10
13/13 - 1s - loss: 146.4521 - loglik: -1.4445e+02 - logprior: -2.0041e+00
Epoch 4/10
13/13 - 1s - loss: 140.0256 - loglik: -1.3791e+02 - logprior: -2.1142e+00
Epoch 5/10
13/13 - 1s - loss: 136.3664 - loglik: -1.3435e+02 - logprior: -2.0117e+00
Epoch 6/10
13/13 - 1s - loss: 134.5027 - loglik: -1.3256e+02 - logprior: -1.9406e+00
Epoch 7/10
13/13 - 1s - loss: 134.4848 - loglik: -1.3253e+02 - logprior: -1.9509e+00
Epoch 8/10
13/13 - 1s - loss: 133.4780 - loglik: -1.3154e+02 - logprior: -1.9354e+00
Epoch 9/10
13/13 - 1s - loss: 133.7226 - loglik: -1.3181e+02 - logprior: -1.9146e+00
Fitted a model with MAP estimate = -133.6307
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (34, 1), (41, 1), (42, 1), (45, 1), (46, 1), (48, 1)]
discards: [0]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 143.3803 - loglik: -1.3368e+02 - logprior: -9.7046e+00
Epoch 2/2
13/13 - 1s - loss: 129.5066 - loglik: -1.2504e+02 - logprior: -4.4634e+00
Fitted a model with MAP estimate = -127.6496
expansions: [(0, 2)]
discards: [ 0 23]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 130.7400 - loglik: -1.2341e+02 - logprior: -7.3295e+00
Epoch 2/2
13/13 - 1s - loss: 123.9073 - loglik: -1.2170e+02 - logprior: -2.2045e+00
Fitted a model with MAP estimate = -122.9021
expansions: [(17, 1)]
discards: [0]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 132.9655 - loglik: -1.2370e+02 - logprior: -9.2633e+00
Epoch 2/10
13/13 - 1s - loss: 125.6706 - loglik: -1.2216e+02 - logprior: -3.5130e+00
Epoch 3/10
13/13 - 1s - loss: 122.2854 - loglik: -1.2060e+02 - logprior: -1.6817e+00
Epoch 4/10
13/13 - 1s - loss: 121.1760 - loglik: -1.1986e+02 - logprior: -1.3166e+00
Epoch 5/10
13/13 - 1s - loss: 120.5455 - loglik: -1.1937e+02 - logprior: -1.1761e+00
Epoch 6/10
13/13 - 1s - loss: 119.8278 - loglik: -1.1863e+02 - logprior: -1.2023e+00
Epoch 7/10
13/13 - 1s - loss: 120.0188 - loglik: -1.1879e+02 - logprior: -1.2255e+00
Fitted a model with MAP estimate = -119.4789
Time for alignment: 42.1860
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 194.3258 - loglik: -1.8557e+02 - logprior: -8.7575e+00
Epoch 2/10
13/13 - 1s - loss: 164.0122 - loglik: -1.6169e+02 - logprior: -2.3203e+00
Epoch 3/10
13/13 - 1s - loss: 147.2876 - loglik: -1.4532e+02 - logprior: -1.9711e+00
Epoch 4/10
13/13 - 1s - loss: 139.9680 - loglik: -1.3792e+02 - logprior: -2.0499e+00
Epoch 5/10
13/13 - 1s - loss: 136.8158 - loglik: -1.3492e+02 - logprior: -1.8939e+00
Epoch 6/10
13/13 - 1s - loss: 135.9611 - loglik: -1.3416e+02 - logprior: -1.8000e+00
Epoch 7/10
13/13 - 1s - loss: 134.6750 - loglik: -1.3285e+02 - logprior: -1.8266e+00
Epoch 8/10
13/13 - 1s - loss: 134.9552 - loglik: -1.3314e+02 - logprior: -1.8187e+00
Fitted a model with MAP estimate = -134.7128
expansions: [(12, 1), (17, 5), (18, 1), (19, 2), (34, 1), (41, 1), (42, 1), (45, 1), (46, 2)]
discards: [0]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 142.9398 - loglik: -1.3323e+02 - logprior: -9.7062e+00
Epoch 2/2
13/13 - 1s - loss: 130.0581 - loglik: -1.2561e+02 - logprior: -4.4472e+00
Fitted a model with MAP estimate = -127.6642
expansions: [(0, 2)]
discards: [ 0 25]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 130.8726 - loglik: -1.2354e+02 - logprior: -7.3300e+00
Epoch 2/2
13/13 - 1s - loss: 124.1427 - loglik: -1.2196e+02 - logprior: -2.1806e+00
Fitted a model with MAP estimate = -123.2357
expansions: [(17, 1)]
discards: [0]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 133.5952 - loglik: -1.2432e+02 - logprior: -9.2779e+00
Epoch 2/10
13/13 - 1s - loss: 125.7218 - loglik: -1.2221e+02 - logprior: -3.5092e+00
Epoch 3/10
13/13 - 1s - loss: 122.8079 - loglik: -1.2111e+02 - logprior: -1.6934e+00
Epoch 4/10
13/13 - 1s - loss: 121.5119 - loglik: -1.2019e+02 - logprior: -1.3198e+00
Epoch 5/10
13/13 - 1s - loss: 120.8977 - loglik: -1.1970e+02 - logprior: -1.1943e+00
Epoch 6/10
13/13 - 1s - loss: 120.4266 - loglik: -1.1921e+02 - logprior: -1.2127e+00
Epoch 7/10
13/13 - 1s - loss: 120.1798 - loglik: -1.1894e+02 - logprior: -1.2370e+00
Epoch 8/10
13/13 - 1s - loss: 119.5629 - loglik: -1.1835e+02 - logprior: -1.2097e+00
Epoch 9/10
13/13 - 1s - loss: 119.6270 - loglik: -1.1844e+02 - logprior: -1.1856e+00
Fitted a model with MAP estimate = -119.5597
Time for alignment: 42.6025
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 194.4165 - loglik: -1.8566e+02 - logprior: -8.7584e+00
Epoch 2/10
13/13 - 1s - loss: 163.5580 - loglik: -1.6124e+02 - logprior: -2.3229e+00
Epoch 3/10
13/13 - 1s - loss: 146.0891 - loglik: -1.4410e+02 - logprior: -1.9921e+00
Epoch 4/10
13/13 - 1s - loss: 140.1594 - loglik: -1.3810e+02 - logprior: -2.0545e+00
Epoch 5/10
13/13 - 1s - loss: 137.1237 - loglik: -1.3524e+02 - logprior: -1.8823e+00
Epoch 6/10
13/13 - 1s - loss: 135.6897 - loglik: -1.3387e+02 - logprior: -1.8168e+00
Epoch 7/10
13/13 - 1s - loss: 134.6672 - loglik: -1.3282e+02 - logprior: -1.8473e+00
Epoch 8/10
13/13 - 1s - loss: 134.7831 - loglik: -1.3295e+02 - logprior: -1.8354e+00
Fitted a model with MAP estimate = -134.4493
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (34, 1), (41, 1), (44, 1), (45, 1), (46, 2)]
discards: [0]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 142.8736 - loglik: -1.3318e+02 - logprior: -9.6906e+00
Epoch 2/2
13/13 - 1s - loss: 129.7429 - loglik: -1.2530e+02 - logprior: -4.4449e+00
Fitted a model with MAP estimate = -127.5964
expansions: [(0, 2)]
discards: [ 0 23]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 130.6690 - loglik: -1.2333e+02 - logprior: -7.3351e+00
Epoch 2/2
13/13 - 1s - loss: 124.2979 - loglik: -1.2212e+02 - logprior: -2.1823e+00
Fitted a model with MAP estimate = -123.2242
expansions: [(17, 1)]
discards: [0]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 134.1473 - loglik: -1.2488e+02 - logprior: -9.2708e+00
Epoch 2/10
13/13 - 1s - loss: 125.5257 - loglik: -1.2202e+02 - logprior: -3.5036e+00
Epoch 3/10
13/13 - 1s - loss: 122.6734 - loglik: -1.2098e+02 - logprior: -1.6959e+00
Epoch 4/10
13/13 - 1s - loss: 121.5207 - loglik: -1.2020e+02 - logprior: -1.3176e+00
Epoch 5/10
13/13 - 1s - loss: 120.7879 - loglik: -1.1960e+02 - logprior: -1.1868e+00
Epoch 6/10
13/13 - 1s - loss: 120.2729 - loglik: -1.1906e+02 - logprior: -1.2135e+00
Epoch 7/10
13/13 - 1s - loss: 120.0201 - loglik: -1.1879e+02 - logprior: -1.2317e+00
Epoch 8/10
13/13 - 1s - loss: 119.5722 - loglik: -1.1837e+02 - logprior: -1.2069e+00
Epoch 9/10
13/13 - 1s - loss: 119.8157 - loglik: -1.1863e+02 - logprior: -1.1859e+00
Fitted a model with MAP estimate = -119.5387
Time for alignment: 42.7092
Computed alignments with likelihoods: ['-119.4789', '-119.5597', '-119.5387']
Best model has likelihood: -119.4789
SP score = 0.9731
Training of 3 independent models on file hpr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2120ddb1c0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1d74e15520>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202c311c10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f213991a310>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 242.2814 - loglik: -2.2845e+02 - logprior: -1.3828e+01
Epoch 2/10
11/11 - 1s - loss: 208.5407 - loglik: -2.0504e+02 - logprior: -3.4991e+00
Epoch 3/10
11/11 - 1s - loss: 181.4424 - loglik: -1.7928e+02 - logprior: -2.1664e+00
Epoch 4/10
11/11 - 1s - loss: 165.8856 - loglik: -1.6399e+02 - logprior: -1.8916e+00
Epoch 5/10
11/11 - 1s - loss: 161.4011 - loglik: -1.5982e+02 - logprior: -1.5858e+00
Epoch 6/10
11/11 - 1s - loss: 159.3838 - loglik: -1.5791e+02 - logprior: -1.4758e+00
Epoch 7/10
11/11 - 1s - loss: 158.1527 - loglik: -1.5686e+02 - logprior: -1.2932e+00
Epoch 8/10
11/11 - 1s - loss: 157.6467 - loglik: -1.5653e+02 - logprior: -1.1152e+00
Epoch 9/10
11/11 - 1s - loss: 156.9913 - loglik: -1.5595e+02 - logprior: -1.0421e+00
Epoch 10/10
11/11 - 1s - loss: 157.3569 - loglik: -1.5635e+02 - logprior: -1.0059e+00
Fitted a model with MAP estimate = -156.8439
expansions: [(0, 6), (22, 1), (27, 1), (29, 2), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 171.3000 - loglik: -1.5523e+02 - logprior: -1.6067e+01
Epoch 2/2
11/11 - 1s - loss: 150.9195 - loglik: -1.4645e+02 - logprior: -4.4684e+00
Fitted a model with MAP estimate = -147.4401
expansions: []
discards: [ 0 37]
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 162.5877 - loglik: -1.4848e+02 - logprior: -1.4105e+01
Epoch 2/2
11/11 - 1s - loss: 151.2899 - loglik: -1.4585e+02 - logprior: -5.4391e+00
Fitted a model with MAP estimate = -149.2382
expansions: []
discards: []
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.3643 - loglik: -1.4611e+02 - logprior: -1.2258e+01
Epoch 2/10
11/11 - 1s - loss: 147.9012 - loglik: -1.4483e+02 - logprior: -3.0701e+00
Epoch 3/10
11/11 - 1s - loss: 146.1328 - loglik: -1.4437e+02 - logprior: -1.7653e+00
Epoch 4/10
11/11 - 1s - loss: 144.6444 - loglik: -1.4352e+02 - logprior: -1.1227e+00
Epoch 5/10
11/11 - 1s - loss: 144.3291 - loglik: -1.4353e+02 - logprior: -7.9740e-01
Epoch 6/10
11/11 - 1s - loss: 143.7402 - loglik: -1.4300e+02 - logprior: -7.3851e-01
Epoch 7/10
11/11 - 1s - loss: 143.8245 - loglik: -1.4322e+02 - logprior: -6.0642e-01
Fitted a model with MAP estimate = -143.3866
Time for alignment: 43.0716
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 242.1291 - loglik: -2.2830e+02 - logprior: -1.3829e+01
Epoch 2/10
11/11 - 1s - loss: 207.8389 - loglik: -2.0434e+02 - logprior: -3.4982e+00
Epoch 3/10
11/11 - 1s - loss: 180.8804 - loglik: -1.7875e+02 - logprior: -2.1304e+00
Epoch 4/10
11/11 - 1s - loss: 165.9012 - loglik: -1.6406e+02 - logprior: -1.8455e+00
Epoch 5/10
11/11 - 1s - loss: 160.5517 - loglik: -1.5898e+02 - logprior: -1.5684e+00
Epoch 6/10
11/11 - 1s - loss: 158.4916 - loglik: -1.5706e+02 - logprior: -1.4274e+00
Epoch 7/10
11/11 - 1s - loss: 157.2935 - loglik: -1.5608e+02 - logprior: -1.2090e+00
Epoch 8/10
11/11 - 1s - loss: 157.1736 - loglik: -1.5609e+02 - logprior: -1.0874e+00
Epoch 9/10
11/11 - 1s - loss: 156.6662 - loglik: -1.5564e+02 - logprior: -1.0264e+00
Epoch 10/10
11/11 - 1s - loss: 156.6245 - loglik: -1.5563e+02 - logprior: -9.9462e-01
Fitted a model with MAP estimate = -156.3661
expansions: [(0, 6), (22, 1), (23, 2), (28, 1), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 171.4333 - loglik: -1.5537e+02 - logprior: -1.6066e+01
Epoch 2/2
11/11 - 1s - loss: 151.0626 - loglik: -1.4658e+02 - logprior: -4.4831e+00
Fitted a model with MAP estimate = -147.5426
expansions: []
discards: [ 0 30]
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 161.9293 - loglik: -1.4783e+02 - logprior: -1.4101e+01
Epoch 2/2
11/11 - 1s - loss: 151.6812 - loglik: -1.4625e+02 - logprior: -5.4322e+00
Fitted a model with MAP estimate = -148.9818
expansions: []
discards: []
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 157.9895 - loglik: -1.4574e+02 - logprior: -1.2251e+01
Epoch 2/10
11/11 - 1s - loss: 148.2831 - loglik: -1.4522e+02 - logprior: -3.0647e+00
Epoch 3/10
11/11 - 1s - loss: 145.5900 - loglik: -1.4382e+02 - logprior: -1.7715e+00
Epoch 4/10
11/11 - 1s - loss: 144.7483 - loglik: -1.4363e+02 - logprior: -1.1224e+00
Epoch 5/10
11/11 - 1s - loss: 144.1666 - loglik: -1.4337e+02 - logprior: -7.9696e-01
Epoch 6/10
11/11 - 1s - loss: 143.8685 - loglik: -1.4313e+02 - logprior: -7.3591e-01
Epoch 7/10
11/11 - 1s - loss: 143.4739 - loglik: -1.4287e+02 - logprior: -6.0769e-01
Epoch 8/10
11/11 - 1s - loss: 143.1455 - loglik: -1.4257e+02 - logprior: -5.7484e-01
Epoch 9/10
11/11 - 1s - loss: 143.3198 - loglik: -1.4278e+02 - logprior: -5.3526e-01
Fitted a model with MAP estimate = -143.1863
Time for alignment: 44.1925
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 242.1208 - loglik: -2.2829e+02 - logprior: -1.3830e+01
Epoch 2/10
11/11 - 1s - loss: 208.9608 - loglik: -2.0546e+02 - logprior: -3.5045e+00
Epoch 3/10
11/11 - 1s - loss: 185.9579 - loglik: -1.8383e+02 - logprior: -2.1314e+00
Epoch 4/10
11/11 - 1s - loss: 168.6641 - loglik: -1.6681e+02 - logprior: -1.8531e+00
Epoch 5/10
11/11 - 1s - loss: 162.7095 - loglik: -1.6112e+02 - logprior: -1.5928e+00
Epoch 6/10
11/11 - 1s - loss: 159.5307 - loglik: -1.5804e+02 - logprior: -1.4940e+00
Epoch 7/10
11/11 - 1s - loss: 158.6337 - loglik: -1.5736e+02 - logprior: -1.2757e+00
Epoch 8/10
11/11 - 1s - loss: 157.7507 - loglik: -1.5664e+02 - logprior: -1.1104e+00
Epoch 9/10
11/11 - 1s - loss: 158.0202 - loglik: -1.5697e+02 - logprior: -1.0546e+00
Fitted a model with MAP estimate = -157.5576
expansions: [(0, 6), (21, 1), (23, 2), (28, 1), (46, 1), (47, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 172.5800 - loglik: -1.5658e+02 - logprior: -1.5999e+01
Epoch 2/2
11/11 - 1s - loss: 150.8569 - loglik: -1.4641e+02 - logprior: -4.4502e+00
Fitted a model with MAP estimate = -147.6380
expansions: []
discards: [ 0 30]
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 162.3895 - loglik: -1.4829e+02 - logprior: -1.4100e+01
Epoch 2/2
11/11 - 1s - loss: 151.1217 - loglik: -1.4569e+02 - logprior: -5.4348e+00
Fitted a model with MAP estimate = -148.9904
expansions: []
discards: []
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.0186 - loglik: -1.4577e+02 - logprior: -1.2252e+01
Epoch 2/10
11/11 - 1s - loss: 148.1885 - loglik: -1.4512e+02 - logprior: -3.0717e+00
Epoch 3/10
11/11 - 1s - loss: 145.7213 - loglik: -1.4395e+02 - logprior: -1.7749e+00
Epoch 4/10
11/11 - 1s - loss: 144.4749 - loglik: -1.4335e+02 - logprior: -1.1288e+00
Epoch 5/10
11/11 - 1s - loss: 144.3732 - loglik: -1.4357e+02 - logprior: -8.0743e-01
Epoch 6/10
11/11 - 1s - loss: 143.7479 - loglik: -1.4300e+02 - logprior: -7.4327e-01
Epoch 7/10
11/11 - 1s - loss: 143.7173 - loglik: -1.4310e+02 - logprior: -6.1746e-01
Epoch 8/10
11/11 - 1s - loss: 143.4459 - loglik: -1.4286e+02 - logprior: -5.8486e-01
Epoch 9/10
11/11 - 1s - loss: 143.1718 - loglik: -1.4262e+02 - logprior: -5.4879e-01
Epoch 10/10
11/11 - 1s - loss: 143.5138 - loglik: -1.4299e+02 - logprior: -5.2144e-01
Fitted a model with MAP estimate = -143.1417
Time for alignment: 44.4444
Computed alignments with likelihoods: ['-143.3866', '-143.1863', '-143.1417']
Best model has likelihood: -143.1417
SP score = 0.9930
Training of 3 independent models on file tim.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2012bbe6a0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f17aa256520>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202ea07d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202e2d87f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 192 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 9s - loss: 575.8332 - loglik: -5.6381e+02 - logprior: -1.2020e+01
Epoch 2/10
12/12 - 6s - loss: 478.9140 - loglik: -4.7669e+02 - logprior: -2.2238e+00
Epoch 3/10
12/12 - 6s - loss: 395.0418 - loglik: -3.9289e+02 - logprior: -2.1567e+00
Epoch 4/10
12/12 - 6s - loss: 358.4775 - loglik: -3.5586e+02 - logprior: -2.6178e+00
Epoch 5/10
12/12 - 6s - loss: 350.2771 - loglik: -3.4763e+02 - logprior: -2.6475e+00
Epoch 6/10
12/12 - 6s - loss: 347.4929 - loglik: -3.4498e+02 - logprior: -2.5174e+00
Epoch 7/10
12/12 - 6s - loss: 345.3839 - loglik: -3.4295e+02 - logprior: -2.4387e+00
Epoch 8/10
12/12 - 6s - loss: 345.1397 - loglik: -3.4265e+02 - logprior: -2.4877e+00
Epoch 9/10
12/12 - 6s - loss: 343.6565 - loglik: -3.4111e+02 - logprior: -2.5494e+00
Epoch 10/10
12/12 - 6s - loss: 343.2511 - loglik: -3.4070e+02 - logprior: -2.5532e+00
Fitted a model with MAP estimate = -343.6160
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (16, 2), (17, 2), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (62, 1), (66, 1), (76, 1), (83, 1), (88, 2), (91, 1), (109, 1), (111, 1), (114, 1), (118, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (155, 1), (158, 1), (172, 1), (173, 3), (174, 1), (185, 1), (186, 1)]
discards: [0 1]
Fitting a model of length 240 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 14s - loss: 349.3462 - loglik: -3.3775e+02 - logprior: -1.1596e+01
Epoch 2/2
12/12 - 8s - loss: 322.6890 - loglik: -3.1868e+02 - logprior: -4.0061e+00
Fitted a model with MAP estimate = -318.1144
expansions: [(0, 3), (191, 1), (193, 1)]
discards: [ 0 12]
Fitting a model of length 243 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 12s - loss: 321.9327 - loglik: -3.1381e+02 - logprior: -8.1201e+00
Epoch 2/2
12/12 - 8s - loss: 311.1054 - loglik: -3.0989e+02 - logprior: -1.2161e+00
Fitted a model with MAP estimate = -308.7155
expansions: []
discards: [ 0 26]
Fitting a model of length 241 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 12s - loss: 323.0453 - loglik: -3.1287e+02 - logprior: -1.0175e+01
Epoch 2/10
12/12 - 8s - loss: 314.2651 - loglik: -3.1106e+02 - logprior: -3.2042e+00
Epoch 3/10
12/12 - 8s - loss: 310.8082 - loglik: -3.0922e+02 - logprior: -1.5899e+00
Epoch 4/10
12/12 - 8s - loss: 306.2423 - loglik: -3.0643e+02 - logprior: 0.1912
Epoch 5/10
12/12 - 8s - loss: 308.0276 - loglik: -3.0872e+02 - logprior: 0.6963
Fitted a model with MAP estimate = -306.2174
Time for alignment: 182.7835
Fitting a model of length 192 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 11s - loss: 575.9985 - loglik: -5.6403e+02 - logprior: -1.1972e+01
Epoch 2/10
12/12 - 6s - loss: 475.7933 - loglik: -4.7363e+02 - logprior: -2.1591e+00
Epoch 3/10
12/12 - 6s - loss: 395.1693 - loglik: -3.9308e+02 - logprior: -2.0866e+00
Epoch 4/10
12/12 - 6s - loss: 359.9715 - loglik: -3.5735e+02 - logprior: -2.6178e+00
Epoch 5/10
12/12 - 6s - loss: 351.9030 - loglik: -3.4924e+02 - logprior: -2.6584e+00
Epoch 6/10
12/12 - 6s - loss: 347.5368 - loglik: -3.4507e+02 - logprior: -2.4687e+00
Epoch 7/10
12/12 - 6s - loss: 345.4716 - loglik: -3.4306e+02 - logprior: -2.4123e+00
Epoch 8/10
12/12 - 6s - loss: 345.4164 - loglik: -3.4293e+02 - logprior: -2.4842e+00
Epoch 9/10
12/12 - 6s - loss: 342.9413 - loglik: -3.4036e+02 - logprior: -2.5792e+00
Epoch 10/10
12/12 - 6s - loss: 344.6553 - loglik: -3.4208e+02 - logprior: -2.5741e+00
Fitted a model with MAP estimate = -343.5143
expansions: [(12, 2), (13, 2), (16, 2), (17, 2), (18, 2), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (47, 1), (48, 1), (49, 1), (62, 1), (63, 1), (65, 1), (83, 1), (87, 1), (88, 1), (91, 1), (111, 1), (113, 1), (114, 1), (116, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (158, 1), (172, 1), (174, 3), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Fitting a model of length 238 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 12s - loss: 349.9084 - loglik: -3.3826e+02 - logprior: -1.1649e+01
Epoch 2/2
12/12 - 8s - loss: 324.2264 - loglik: -3.2020e+02 - logprior: -4.0262e+00
Fitted a model with MAP estimate = -319.7366
expansions: [(0, 3), (12, 1), (190, 1), (192, 1), (213, 1)]
discards: [ 0 22 23 24]
Fitting a model of length 241 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 11s - loss: 324.0962 - loglik: -3.1592e+02 - logprior: -8.1802e+00
Epoch 2/2
12/12 - 8s - loss: 311.7017 - loglik: -3.1048e+02 - logprior: -1.2256e+00
Fitted a model with MAP estimate = -309.5832
expansions: [(25, 2)]
discards: [0 1]
Fitting a model of length 241 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 11s - loss: 323.4116 - loglik: -3.1321e+02 - logprior: -1.0200e+01
Epoch 2/10
12/12 - 8s - loss: 315.1671 - loglik: -3.1200e+02 - logprior: -3.1683e+00
Epoch 3/10
12/12 - 8s - loss: 312.1306 - loglik: -3.1056e+02 - logprior: -1.5714e+00
Epoch 4/10
12/12 - 8s - loss: 308.5113 - loglik: -3.0870e+02 - logprior: 0.1889
Epoch 5/10
12/12 - 8s - loss: 307.5143 - loglik: -3.0827e+02 - logprior: 0.7511
Epoch 6/10
12/12 - 8s - loss: 306.9687 - loglik: -3.0773e+02 - logprior: 0.7597
Epoch 7/10
12/12 - 8s - loss: 305.6190 - loglik: -3.0618e+02 - logprior: 0.5648
Epoch 8/10
12/12 - 8s - loss: 306.1339 - loglik: -3.0668e+02 - logprior: 0.5419
Fitted a model with MAP estimate = -305.5233
Time for alignment: 205.7846
Fitting a model of length 192 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 576.5718 - loglik: -5.6458e+02 - logprior: -1.1990e+01
Epoch 2/10
12/12 - 6s - loss: 478.2269 - loglik: -4.7606e+02 - logprior: -2.1697e+00
Epoch 3/10
12/12 - 6s - loss: 391.7420 - loglik: -3.8959e+02 - logprior: -2.1502e+00
Epoch 4/10
12/12 - 6s - loss: 359.7140 - loglik: -3.5696e+02 - logprior: -2.7502e+00
Epoch 5/10
12/12 - 6s - loss: 350.4723 - loglik: -3.4767e+02 - logprior: -2.8017e+00
Epoch 6/10
12/12 - 6s - loss: 346.0321 - loglik: -3.4346e+02 - logprior: -2.5729e+00
Epoch 7/10
12/12 - 6s - loss: 345.5442 - loglik: -3.4304e+02 - logprior: -2.5033e+00
Epoch 8/10
12/12 - 6s - loss: 343.8910 - loglik: -3.4133e+02 - logprior: -2.5628e+00
Epoch 9/10
12/12 - 6s - loss: 344.6458 - loglik: -3.4200e+02 - logprior: -2.6446e+00
Fitted a model with MAP estimate = -343.2522
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (16, 2), (17, 2), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 2), (48, 1), (49, 1), (62, 1), (63, 1), (65, 1), (76, 1), (85, 1), (88, 1), (90, 1), (93, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (150, 1), (152, 2), (153, 1), (154, 3), (155, 1), (157, 1), (173, 3), (174, 2), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Fitting a model of length 242 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 12s - loss: 350.0560 - loglik: -3.3844e+02 - logprior: -1.1616e+01
Epoch 2/2
12/12 - 8s - loss: 321.4283 - loglik: -3.1741e+02 - logprior: -4.0142e+00
Fitted a model with MAP estimate = -318.1402
expansions: [(0, 3)]
discards: [  0  12 219]
Fitting a model of length 242 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 11s - loss: 323.1613 - loglik: -3.1499e+02 - logprior: -8.1739e+00
Epoch 2/2
12/12 - 8s - loss: 311.4314 - loglik: -3.1004e+02 - logprior: -1.3874e+00
Fitted a model with MAP estimate = -309.5907
expansions: [(195, 1)]
discards: [ 0  1 26]
Fitting a model of length 240 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 13s - loss: 324.0920 - loglik: -3.1387e+02 - logprior: -1.0217e+01
Epoch 2/10
12/12 - 8s - loss: 315.3011 - loglik: -3.1200e+02 - logprior: -3.3051e+00
Epoch 3/10
12/12 - 8s - loss: 312.0838 - loglik: -3.1041e+02 - logprior: -1.6694e+00
Epoch 4/10
12/12 - 8s - loss: 309.8238 - loglik: -3.0996e+02 - logprior: 0.1331
Epoch 5/10
12/12 - 8s - loss: 306.8717 - loglik: -3.0757e+02 - logprior: 0.7011
Epoch 6/10
12/12 - 8s - loss: 306.6554 - loglik: -3.0735e+02 - logprior: 0.6897
Epoch 7/10
12/12 - 8s - loss: 307.3192 - loglik: -3.0789e+02 - logprior: 0.5659
Fitted a model with MAP estimate = -306.0707
Time for alignment: 193.6306
Computed alignments with likelihoods: ['-306.2174', '-305.5233', '-306.0707']
Best model has likelihood: -305.5233
SP score = 0.9735
Training of 3 independent models on file tgfb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2153d4fa00>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f179b2d9f40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20f5ca1d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202bf2b8b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 307.5106 - loglik: -2.7691e+02 - logprior: -3.0605e+01
Epoch 2/10
10/10 - 1s - loss: 247.2384 - loglik: -2.3956e+02 - logprior: -7.6754e+00
Epoch 3/10
10/10 - 1s - loss: 208.8993 - loglik: -2.0500e+02 - logprior: -3.9036e+00
Epoch 4/10
10/10 - 1s - loss: 187.6429 - loglik: -1.8483e+02 - logprior: -2.8094e+00
Epoch 5/10
10/10 - 1s - loss: 179.0997 - loglik: -1.7668e+02 - logprior: -2.4168e+00
Epoch 6/10
10/10 - 1s - loss: 176.3019 - loglik: -1.7410e+02 - logprior: -2.1974e+00
Epoch 7/10
10/10 - 1s - loss: 174.5638 - loglik: -1.7269e+02 - logprior: -1.8774e+00
Epoch 8/10
10/10 - 1s - loss: 174.7719 - loglik: -1.7311e+02 - logprior: -1.6649e+00
Fitted a model with MAP estimate = -174.1706
expansions: [(0, 3), (11, 1), (12, 1), (24, 1), (35, 3), (36, 1), (37, 1), (48, 3), (49, 2), (69, 2), (70, 1), (72, 3), (73, 2)]
discards: []
Fitting a model of length 107 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 202.6550 - loglik: -1.6797e+02 - logprior: -3.4688e+01
Epoch 2/2
10/10 - 1s - loss: 167.5206 - loglik: -1.5746e+02 - logprior: -1.0064e+01
Fitted a model with MAP estimate = -160.7643
expansions: []
discards: [ 0 95]
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 186.8777 - loglik: -1.5616e+02 - logprior: -3.0721e+01
Epoch 2/2
10/10 - 1s - loss: 166.1025 - loglik: -1.5408e+02 - logprior: -1.2025e+01
Fitted a model with MAP estimate = -162.4840
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 183.4146 - loglik: -1.5453e+02 - logprior: -2.8885e+01
Epoch 2/10
10/10 - 1s - loss: 160.6080 - loglik: -1.5257e+02 - logprior: -8.0406e+00
Epoch 3/10
10/10 - 1s - loss: 155.5148 - loglik: -1.5268e+02 - logprior: -2.8332e+00
Epoch 4/10
10/10 - 1s - loss: 152.1462 - loglik: -1.5101e+02 - logprior: -1.1402e+00
Epoch 5/10
10/10 - 1s - loss: 151.3129 - loglik: -1.5090e+02 - logprior: -4.1048e-01
Epoch 6/10
10/10 - 1s - loss: 150.6362 - loglik: -1.5057e+02 - logprior: -7.0462e-02
Epoch 7/10
10/10 - 1s - loss: 150.6407 - loglik: -1.5076e+02 - logprior: 0.1206
Fitted a model with MAP estimate = -150.1743
Time for alignment: 41.5329
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 307.0569 - loglik: -2.7646e+02 - logprior: -3.0602e+01
Epoch 2/10
10/10 - 1s - loss: 246.9331 - loglik: -2.3927e+02 - logprior: -7.6620e+00
Epoch 3/10
10/10 - 1s - loss: 209.1225 - loglik: -2.0529e+02 - logprior: -3.8351e+00
Epoch 4/10
10/10 - 1s - loss: 189.1320 - loglik: -1.8642e+02 - logprior: -2.7166e+00
Epoch 5/10
10/10 - 1s - loss: 180.1720 - loglik: -1.7779e+02 - logprior: -2.3868e+00
Epoch 6/10
10/10 - 1s - loss: 177.2193 - loglik: -1.7502e+02 - logprior: -2.1987e+00
Epoch 7/10
10/10 - 1s - loss: 175.5387 - loglik: -1.7363e+02 - logprior: -1.9042e+00
Epoch 8/10
10/10 - 1s - loss: 175.7134 - loglik: -1.7400e+02 - logprior: -1.7161e+00
Fitted a model with MAP estimate = -174.9963
expansions: [(0, 3), (11, 1), (12, 1), (24, 1), (35, 3), (36, 3), (48, 2), (49, 2), (51, 1), (69, 1), (70, 1), (72, 3), (73, 2)]
discards: []
Fitting a model of length 107 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 204.7626 - loglik: -1.7003e+02 - logprior: -3.4737e+01
Epoch 2/2
10/10 - 1s - loss: 169.4980 - loglik: -1.5927e+02 - logprior: -1.0225e+01
Fitted a model with MAP estimate = -162.9804
expansions: []
discards: [ 0 46 95]
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 188.9117 - loglik: -1.5809e+02 - logprior: -3.0818e+01
Epoch 2/2
10/10 - 1s - loss: 169.1948 - loglik: -1.5709e+02 - logprior: -1.2103e+01
Fitted a model with MAP estimate = -164.7005
expansions: [(3, 1), (60, 1)]
discards: [0]
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 185.5190 - loglik: -1.5655e+02 - logprior: -2.8966e+01
Epoch 2/10
10/10 - 1s - loss: 162.7246 - loglik: -1.5459e+02 - logprior: -8.1315e+00
Epoch 3/10
10/10 - 1s - loss: 156.6091 - loglik: -1.5376e+02 - logprior: -2.8541e+00
Epoch 4/10
10/10 - 1s - loss: 154.6690 - loglik: -1.5355e+02 - logprior: -1.1191e+00
Epoch 5/10
10/10 - 1s - loss: 153.3498 - loglik: -1.5300e+02 - logprior: -3.5043e-01
Epoch 6/10
10/10 - 1s - loss: 152.6074 - loglik: -1.5261e+02 - logprior: 0.0042
Epoch 7/10
10/10 - 1s - loss: 152.5119 - loglik: -1.5270e+02 - logprior: 0.1910
Epoch 8/10
10/10 - 1s - loss: 152.1293 - loglik: -1.5246e+02 - logprior: 0.3301
Epoch 9/10
10/10 - 1s - loss: 152.2803 - loglik: -1.5274e+02 - logprior: 0.4615
Fitted a model with MAP estimate = -151.7875
Time for alignment: 44.2111
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 307.1926 - loglik: -2.7659e+02 - logprior: -3.0603e+01
Epoch 2/10
10/10 - 1s - loss: 247.6215 - loglik: -2.3996e+02 - logprior: -7.6662e+00
Epoch 3/10
10/10 - 1s - loss: 208.5139 - loglik: -2.0466e+02 - logprior: -3.8554e+00
Epoch 4/10
10/10 - 1s - loss: 188.3182 - loglik: -1.8557e+02 - logprior: -2.7440e+00
Epoch 5/10
10/10 - 1s - loss: 181.2885 - loglik: -1.7895e+02 - logprior: -2.3387e+00
Epoch 6/10
10/10 - 1s - loss: 177.8318 - loglik: -1.7561e+02 - logprior: -2.2243e+00
Epoch 7/10
10/10 - 1s - loss: 176.9111 - loglik: -1.7494e+02 - logprior: -1.9669e+00
Epoch 8/10
10/10 - 1s - loss: 175.7760 - loglik: -1.7402e+02 - logprior: -1.7560e+00
Epoch 9/10
10/10 - 1s - loss: 174.8228 - loglik: -1.7311e+02 - logprior: -1.7171e+00
Epoch 10/10
10/10 - 1s - loss: 174.7444 - loglik: -1.7302e+02 - logprior: -1.7212e+00
Fitted a model with MAP estimate = -174.4219
expansions: [(0, 2), (11, 2), (12, 1), (21, 1), (33, 1), (35, 4), (48, 3), (49, 2), (60, 1), (69, 1), (70, 1), (72, 2), (73, 3)]
discards: []
Fitting a model of length 107 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 204.7491 - loglik: -1.6938e+02 - logprior: -3.5371e+01
Epoch 2/2
10/10 - 1s - loss: 168.5533 - loglik: -1.5809e+02 - logprior: -1.0465e+01
Fitted a model with MAP estimate = -161.6818
expansions: []
discards: [13 92]
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 180.6669 - loglik: -1.5531e+02 - logprior: -2.5360e+01
Epoch 2/2
10/10 - 1s - loss: 159.9104 - loglik: -1.5351e+02 - logprior: -6.4015e+00
Fitted a model with MAP estimate = -156.6125
expansions: []
discards: []
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 177.2211 - loglik: -1.5315e+02 - logprior: -2.4072e+01
Epoch 2/10
10/10 - 1s - loss: 158.0601 - loglik: -1.5206e+02 - logprior: -6.0039e+00
Epoch 3/10
10/10 - 1s - loss: 154.1418 - loglik: -1.5168e+02 - logprior: -2.4626e+00
Epoch 4/10
10/10 - 1s - loss: 152.4169 - loglik: -1.5130e+02 - logprior: -1.1131e+00
Epoch 5/10
10/10 - 1s - loss: 151.6540 - loglik: -1.5118e+02 - logprior: -4.7275e-01
Epoch 6/10
10/10 - 1s - loss: 151.3092 - loglik: -1.5117e+02 - logprior: -1.4029e-01
Epoch 7/10
10/10 - 1s - loss: 150.4348 - loglik: -1.5057e+02 - logprior: 0.1364
Epoch 8/10
10/10 - 1s - loss: 150.7212 - loglik: -1.5108e+02 - logprior: 0.3597
Fitted a model with MAP estimate = -150.5296
Time for alignment: 43.8771
Computed alignments with likelihoods: ['-150.1743', '-151.7875', '-150.5296']
Best model has likelihood: -150.1743
SP score = 0.8114
Training of 3 independent models on file HLH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1d7634d580>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1d74c932e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17aa10c9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1798812d90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 151.2451 - loglik: -1.4584e+02 - logprior: -5.4006e+00
Epoch 2/10
16/16 - 2s - loss: 127.5190 - loglik: -1.2591e+02 - logprior: -1.6075e+00
Epoch 3/10
16/16 - 2s - loss: 115.7180 - loglik: -1.1400e+02 - logprior: -1.7149e+00
Epoch 4/10
16/16 - 2s - loss: 110.7845 - loglik: -1.0904e+02 - logprior: -1.7456e+00
Epoch 5/10
16/16 - 2s - loss: 109.5467 - loglik: -1.0785e+02 - logprior: -1.6942e+00
Epoch 6/10
16/16 - 2s - loss: 108.9487 - loglik: -1.0726e+02 - logprior: -1.6926e+00
Epoch 7/10
16/16 - 2s - loss: 108.7310 - loglik: -1.0703e+02 - logprior: -1.6989e+00
Epoch 8/10
16/16 - 2s - loss: 108.2485 - loglik: -1.0654e+02 - logprior: -1.7081e+00
Epoch 9/10
16/16 - 2s - loss: 108.4168 - loglik: -1.0670e+02 - logprior: -1.7195e+00
Fitted a model with MAP estimate = -108.3652
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (17, 1), (23, 6), (24, 2), (25, 2)]
discards: [0]
Fitting a model of length 57 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 114.8234 - loglik: -1.0838e+02 - logprior: -6.4432e+00
Epoch 2/2
16/16 - 2s - loss: 105.9472 - loglik: -1.0276e+02 - logprior: -3.1873e+00
Fitted a model with MAP estimate = -104.5907
expansions: [(0, 1)]
discards: [ 0 31 32 33 38]
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 106.6116 - loglik: -1.0199e+02 - logprior: -4.6240e+00
Epoch 2/2
16/16 - 2s - loss: 103.2712 - loglik: -1.0164e+02 - logprior: -1.6350e+00
Fitted a model with MAP estimate = -102.4693
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 109.2325 - loglik: -1.0303e+02 - logprior: -6.1990e+00
Epoch 2/10
16/16 - 2s - loss: 104.6775 - loglik: -1.0191e+02 - logprior: -2.7633e+00
Epoch 3/10
16/16 - 2s - loss: 102.8535 - loglik: -1.0129e+02 - logprior: -1.5659e+00
Epoch 4/10
16/16 - 2s - loss: 102.3038 - loglik: -1.0096e+02 - logprior: -1.3423e+00
Epoch 5/10
16/16 - 2s - loss: 101.9604 - loglik: -1.0062e+02 - logprior: -1.3384e+00
Epoch 6/10
16/16 - 2s - loss: 101.4807 - loglik: -1.0008e+02 - logprior: -1.4031e+00
Epoch 7/10
16/16 - 2s - loss: 101.1771 - loglik: -9.9767e+01 - logprior: -1.4098e+00
Epoch 8/10
16/16 - 2s - loss: 100.7978 - loglik: -9.9371e+01 - logprior: -1.4271e+00
Epoch 9/10
16/16 - 2s - loss: 101.0733 - loglik: -9.9654e+01 - logprior: -1.4192e+00
Fitted a model with MAP estimate = -100.7979
Time for alignment: 59.6225
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 151.5900 - loglik: -1.4619e+02 - logprior: -5.3997e+00
Epoch 2/10
16/16 - 2s - loss: 127.0472 - loglik: -1.2546e+02 - logprior: -1.5826e+00
Epoch 3/10
16/16 - 2s - loss: 116.0910 - loglik: -1.1446e+02 - logprior: -1.6340e+00
Epoch 4/10
16/16 - 2s - loss: 111.5717 - loglik: -1.0997e+02 - logprior: -1.5991e+00
Epoch 5/10
16/16 - 2s - loss: 109.6629 - loglik: -1.0803e+02 - logprior: -1.6341e+00
Epoch 6/10
16/16 - 2s - loss: 109.3728 - loglik: -1.0775e+02 - logprior: -1.6182e+00
Epoch 7/10
16/16 - 2s - loss: 108.9207 - loglik: -1.0726e+02 - logprior: -1.6637e+00
Epoch 8/10
16/16 - 2s - loss: 109.0321 - loglik: -1.0736e+02 - logprior: -1.6734e+00
Fitted a model with MAP estimate = -108.8146
expansions: [(3, 1), (6, 1), (12, 2), (15, 1), (17, 1), (23, 5), (24, 2), (25, 2)]
discards: [0]
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 114.0706 - loglik: -1.0762e+02 - logprior: -6.4492e+00
Epoch 2/2
16/16 - 2s - loss: 105.9993 - loglik: -1.0286e+02 - logprior: -3.1417e+00
Fitted a model with MAP estimate = -104.5540
expansions: [(0, 1)]
discards: [ 0 31 32 37]
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 106.8098 - loglik: -1.0219e+02 - logprior: -4.6188e+00
Epoch 2/2
16/16 - 2s - loss: 102.8837 - loglik: -1.0125e+02 - logprior: -1.6348e+00
Fitted a model with MAP estimate = -102.4253
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 109.3746 - loglik: -1.0318e+02 - logprior: -6.1932e+00
Epoch 2/10
16/16 - 2s - loss: 104.4449 - loglik: -1.0175e+02 - logprior: -2.6911e+00
Epoch 3/10
16/16 - 2s - loss: 103.2135 - loglik: -1.0168e+02 - logprior: -1.5292e+00
Epoch 4/10
16/16 - 2s - loss: 102.2564 - loglik: -1.0094e+02 - logprior: -1.3205e+00
Epoch 5/10
16/16 - 2s - loss: 101.9033 - loglik: -1.0058e+02 - logprior: -1.3246e+00
Epoch 6/10
16/16 - 2s - loss: 101.4227 - loglik: -1.0004e+02 - logprior: -1.3840e+00
Epoch 7/10
16/16 - 2s - loss: 101.0258 - loglik: -9.9636e+01 - logprior: -1.3897e+00
Epoch 8/10
16/16 - 2s - loss: 101.1718 - loglik: -9.9765e+01 - logprior: -1.4069e+00
Fitted a model with MAP estimate = -100.9635
Time for alignment: 60.3371
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 151.4935 - loglik: -1.4609e+02 - logprior: -5.3986e+00
Epoch 2/10
16/16 - 2s - loss: 125.6272 - loglik: -1.2403e+02 - logprior: -1.5957e+00
Epoch 3/10
16/16 - 2s - loss: 114.5463 - loglik: -1.1289e+02 - logprior: -1.6575e+00
Epoch 4/10
16/16 - 2s - loss: 110.8613 - loglik: -1.0917e+02 - logprior: -1.6865e+00
Epoch 5/10
16/16 - 2s - loss: 109.7237 - loglik: -1.0808e+02 - logprior: -1.6446e+00
Epoch 6/10
16/16 - 2s - loss: 109.2571 - loglik: -1.0760e+02 - logprior: -1.6577e+00
Epoch 7/10
16/16 - 2s - loss: 109.2971 - loglik: -1.0764e+02 - logprior: -1.6567e+00
Fitted a model with MAP estimate = -109.0635
expansions: [(3, 1), (6, 1), (13, 1), (15, 2), (16, 1), (23, 6), (25, 2)]
discards: [0]
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 113.6235 - loglik: -1.0724e+02 - logprior: -6.3840e+00
Epoch 2/2
16/16 - 2s - loss: 105.6940 - loglik: -1.0263e+02 - logprior: -3.0686e+00
Fitted a model with MAP estimate = -104.3715
expansions: [(0, 1)]
discards: [ 0 31 36]
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 106.8995 - loglik: -1.0229e+02 - logprior: -4.6141e+00
Epoch 2/2
16/16 - 2s - loss: 102.7074 - loglik: -1.0108e+02 - logprior: -1.6300e+00
Fitted a model with MAP estimate = -102.4655
expansions: [(3, 1)]
discards: []
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 106.0745 - loglik: -1.0146e+02 - logprior: -4.6173e+00
Epoch 2/10
16/16 - 2s - loss: 102.4058 - loglik: -1.0082e+02 - logprior: -1.5877e+00
Epoch 3/10
16/16 - 2s - loss: 101.7736 - loglik: -1.0045e+02 - logprior: -1.3267e+00
Epoch 4/10
16/16 - 2s - loss: 101.3930 - loglik: -1.0013e+02 - logprior: -1.2653e+00
Epoch 5/10
16/16 - 2s - loss: 100.9786 - loglik: -9.9725e+01 - logprior: -1.2539e+00
Epoch 6/10
16/16 - 2s - loss: 101.0407 - loglik: -9.9754e+01 - logprior: -1.2865e+00
Fitted a model with MAP estimate = -100.6554
Time for alignment: 50.9360
Computed alignments with likelihoods: ['-100.7979', '-100.9635', '-100.6554']
Best model has likelihood: -100.6554
SP score = 0.9225
Training of 3 independent models on file blmb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f17aa2405e0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f179b029190>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1788381c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1788381ca0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 155 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 610.2014 - loglik: -6.0676e+02 - logprior: -3.4365e+00
Epoch 2/10
19/19 - 9s - loss: 566.7775 - loglik: -5.6605e+02 - logprior: -7.2688e-01
Epoch 3/10
19/19 - 9s - loss: 538.4022 - loglik: -5.3736e+02 - logprior: -1.0414e+00
Epoch 4/10
19/19 - 9s - loss: 531.0708 - loglik: -5.2990e+02 - logprior: -1.1722e+00
Epoch 5/10
19/19 - 9s - loss: 528.2452 - loglik: -5.2703e+02 - logprior: -1.2143e+00
Epoch 6/10
19/19 - 9s - loss: 526.6773 - loglik: -5.2542e+02 - logprior: -1.2565e+00
Epoch 7/10
19/19 - 9s - loss: 525.4236 - loglik: -5.2411e+02 - logprior: -1.3166e+00
Epoch 8/10
19/19 - 9s - loss: 523.9131 - loglik: -5.2247e+02 - logprior: -1.4442e+00
Epoch 9/10
19/19 - 9s - loss: 523.7752 - loglik: -5.2223e+02 - logprior: -1.5501e+00
Epoch 10/10
19/19 - 9s - loss: 521.5356 - loglik: -5.1984e+02 - logprior: -1.6997e+00
Fitted a model with MAP estimate = -494.0224
expansions: [(6, 1), (7, 1), (11, 1), (13, 1), (21, 2), (24, 3), (25, 1), (55, 2), (56, 1), (70, 1), (78, 4), (108, 1), (109, 1), (111, 1), (123, 4), (124, 2), (125, 4), (146, 7)]
discards: [0]
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 541.3947 - loglik: -5.3722e+02 - logprior: -4.1771e+00
Epoch 2/2
19/19 - 12s - loss: 525.0211 - loglik: -5.2309e+02 - logprior: -1.9330e+00
Fitted a model with MAP estimate = -479.0404
expansions: [(0, 2), (91, 1), (146, 2), (147, 2), (149, 2)]
discards: [  0  29  66 155 156 157 163 164 165 176 177 178 179]
Fitting a model of length 188 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 527.6243 - loglik: -5.2475e+02 - logprior: -2.8717e+00
Epoch 2/2
19/19 - 12s - loss: 522.6388 - loglik: -5.2163e+02 - logprior: -1.0129e+00
Fitted a model with MAP estimate = -477.8865
expansions: [(35, 1), (36, 1)]
discards: [  0  25 149 153 166 167]
Fitting a model of length 184 on 17200 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 19s - loss: 475.9406 - loglik: -4.7395e+02 - logprior: -1.9930e+00
Epoch 2/10
25/25 - 15s - loss: 471.1552 - loglik: -4.7042e+02 - logprior: -7.3500e-01
Epoch 3/10
25/25 - 15s - loss: 468.6768 - loglik: -4.6803e+02 - logprior: -6.4954e-01
Epoch 4/10
25/25 - 15s - loss: 468.2604 - loglik: -4.6760e+02 - logprior: -6.6298e-01
Epoch 5/10
25/25 - 15s - loss: 466.3878 - loglik: -4.6571e+02 - logprior: -6.7825e-01
Epoch 6/10
25/25 - 15s - loss: 464.0968 - loglik: -4.6338e+02 - logprior: -7.1816e-01
Epoch 7/10
25/25 - 15s - loss: 462.7212 - loglik: -4.6194e+02 - logprior: -7.7964e-01
Epoch 8/10
25/25 - 15s - loss: 462.1001 - loglik: -4.6124e+02 - logprior: -8.6214e-01
Epoch 9/10
25/25 - 15s - loss: 460.5656 - loglik: -4.5961e+02 - logprior: -9.5100e-01
Epoch 10/10
25/25 - 15s - loss: 459.8255 - loglik: -4.5877e+02 - logprior: -1.0521e+00
Fitted a model with MAP estimate = -458.7544
Time for alignment: 373.5957
Fitting a model of length 155 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 609.8660 - loglik: -6.0643e+02 - logprior: -3.4345e+00
Epoch 2/10
19/19 - 9s - loss: 568.0288 - loglik: -5.6733e+02 - logprior: -7.0364e-01
Epoch 3/10
19/19 - 9s - loss: 542.4193 - loglik: -5.4138e+02 - logprior: -1.0347e+00
Epoch 4/10
19/19 - 9s - loss: 535.5283 - loglik: -5.3432e+02 - logprior: -1.2128e+00
Epoch 5/10
19/19 - 9s - loss: 531.7193 - loglik: -5.3045e+02 - logprior: -1.2704e+00
Epoch 6/10
19/19 - 9s - loss: 529.3702 - loglik: -5.2802e+02 - logprior: -1.3529e+00
Epoch 7/10
19/19 - 9s - loss: 526.8336 - loglik: -5.2543e+02 - logprior: -1.4060e+00
Epoch 8/10
19/19 - 9s - loss: 526.3677 - loglik: -5.2484e+02 - logprior: -1.5278e+00
Epoch 9/10
19/19 - 9s - loss: 525.3712 - loglik: -5.2371e+02 - logprior: -1.6596e+00
Epoch 10/10
19/19 - 9s - loss: 524.0048 - loglik: -5.2220e+02 - logprior: -1.8003e+00
Fitted a model with MAP estimate = -494.2218
expansions: [(6, 1), (7, 1), (10, 2), (11, 1), (20, 2), (24, 2), (25, 1), (26, 2), (55, 2), (57, 1), (70, 1), (78, 7), (79, 1), (82, 1), (112, 1), (122, 4), (125, 3), (127, 2), (146, 2)]
discards: [ 0 94 95 96]
Fitting a model of length 188 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 545.3175 - loglik: -5.4110e+02 - logprior: -4.2212e+00
Epoch 2/2
19/19 - 12s - loss: 528.5078 - loglik: -5.2653e+02 - logprior: -1.9730e+00
Fitted a model with MAP estimate = -480.4158
expansions: [(0, 2), (149, 3), (151, 2), (171, 1), (172, 9), (177, 1)]
discards: [  0  13  25  68 156 163 164 165]
Fitting a model of length 198 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 17s - loss: 528.0449 - loglik: -5.2513e+02 - logprior: -2.9132e+00
Epoch 2/2
19/19 - 13s - loss: 521.2036 - loglik: -5.2018e+02 - logprior: -1.0269e+00
Fitted a model with MAP estimate = -476.1133
expansions: [(29, 1), (149, 2)]
discards: [  0 145 156 157 159 160 161 162 163 164 165 166 167 168 169 170 171]
Fitting a model of length 184 on 17200 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 18s - loss: 475.1039 - loglik: -4.7309e+02 - logprior: -2.0107e+00
Epoch 2/10
25/25 - 15s - loss: 470.6946 - loglik: -4.6990e+02 - logprior: -7.9251e-01
Epoch 3/10
25/25 - 15s - loss: 469.0415 - loglik: -4.6841e+02 - logprior: -6.3282e-01
Epoch 4/10
25/25 - 15s - loss: 467.4865 - loglik: -4.6683e+02 - logprior: -6.5164e-01
Epoch 5/10
25/25 - 15s - loss: 465.7656 - loglik: -4.6508e+02 - logprior: -6.8586e-01
Epoch 6/10
25/25 - 15s - loss: 463.5609 - loglik: -4.6282e+02 - logprior: -7.4139e-01
Epoch 7/10
25/25 - 15s - loss: 463.1606 - loglik: -4.6237e+02 - logprior: -7.8852e-01
Epoch 8/10
25/25 - 15s - loss: 461.0664 - loglik: -4.6018e+02 - logprior: -8.8239e-01
Epoch 9/10
25/25 - 15s - loss: 459.9244 - loglik: -4.5897e+02 - logprior: -9.5256e-01
Epoch 10/10
25/25 - 15s - loss: 459.5561 - loglik: -4.5849e+02 - logprior: -1.0671e+00
Fitted a model with MAP estimate = -458.5347
Time for alignment: 370.7023
Fitting a model of length 155 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 609.7518 - loglik: -6.0631e+02 - logprior: -3.4371e+00
Epoch 2/10
19/19 - 9s - loss: 566.7512 - loglik: -5.6600e+02 - logprior: -7.4876e-01
Epoch 3/10
19/19 - 9s - loss: 542.4116 - loglik: -5.4133e+02 - logprior: -1.0800e+00
Epoch 4/10
19/19 - 9s - loss: 535.7546 - loglik: -5.3456e+02 - logprior: -1.1966e+00
Epoch 5/10
19/19 - 9s - loss: 532.6372 - loglik: -5.3139e+02 - logprior: -1.2432e+00
Epoch 6/10
19/19 - 9s - loss: 531.0464 - loglik: -5.2974e+02 - logprior: -1.3098e+00
Epoch 7/10
19/19 - 9s - loss: 528.2957 - loglik: -5.2692e+02 - logprior: -1.3728e+00
Epoch 8/10
19/19 - 9s - loss: 528.1835 - loglik: -5.2669e+02 - logprior: -1.4950e+00
Epoch 9/10
19/19 - 9s - loss: 526.0463 - loglik: -5.2444e+02 - logprior: -1.6042e+00
Epoch 10/10
19/19 - 9s - loss: 525.7720 - loglik: -5.2403e+02 - logprior: -1.7417e+00
Fitted a model with MAP estimate = -497.2256
expansions: [(6, 1), (7, 1), (11, 1), (13, 1), (21, 2), (24, 5), (25, 1), (55, 2), (57, 2), (70, 1), (77, 6), (78, 1), (81, 2), (122, 3), (125, 4), (146, 3)]
discards: [0]
Fitting a model of length 190 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 547.2184 - loglik: -5.4300e+02 - logprior: -4.2210e+00
Epoch 2/2
19/19 - 12s - loss: 529.4406 - loglik: -5.2745e+02 - logprior: -1.9932e+00
Fitted a model with MAP estimate = -482.2981
expansions: [(0, 2), (95, 2), (155, 1)]
discards: [  0  70 105 114 115 116 149 166 167 168 169 170]
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 531.6558 - loglik: -5.2880e+02 - logprior: -2.8589e+00
Epoch 2/2
19/19 - 11s - loss: 526.7563 - loglik: -5.2579e+02 - logprior: -9.6578e-01
Fitted a model with MAP estimate = -480.5740
expansions: [(147, 3), (162, 2)]
discards: [  0  30  31  32  69 164]
Fitting a model of length 182 on 17200 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 18s - loss: 477.9673 - loglik: -4.7596e+02 - logprior: -2.0043e+00
Epoch 2/10
25/25 - 15s - loss: 471.6493 - loglik: -4.7087e+02 - logprior: -7.8405e-01
Epoch 3/10
25/25 - 15s - loss: 469.4337 - loglik: -4.6877e+02 - logprior: -6.6453e-01
Epoch 4/10
25/25 - 15s - loss: 469.0588 - loglik: -4.6837e+02 - logprior: -6.9125e-01
Epoch 5/10
25/25 - 15s - loss: 467.0962 - loglik: -4.6637e+02 - logprior: -7.2419e-01
Epoch 6/10
25/25 - 15s - loss: 464.9886 - loglik: -4.6422e+02 - logprior: -7.6967e-01
Epoch 7/10
25/25 - 15s - loss: 464.7625 - loglik: -4.6395e+02 - logprior: -8.1448e-01
Epoch 8/10
25/25 - 15s - loss: 462.4926 - loglik: -4.6159e+02 - logprior: -9.0225e-01
Epoch 9/10
25/25 - 15s - loss: 460.8086 - loglik: -4.5981e+02 - logprior: -1.0020e+00
Epoch 10/10
25/25 - 15s - loss: 462.7832 - loglik: -4.6168e+02 - logprior: -1.0987e+00
Fitted a model with MAP estimate = -459.8621
Time for alignment: 364.6939
Computed alignments with likelihoods: ['-458.7544', '-458.5347', '-459.8621']
Best model has likelihood: -458.5347
SP score = 0.6519
Training of 3 independent models on file proteasome.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f26ffd2b0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f26ffdee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f26ffd6d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2580>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2190>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f1c3d27f0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2940>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2f40>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2550>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2430>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2520>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2eb0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2e20>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2ee0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d2040>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d24f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f1c3d27c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2cd0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f179a95fa00>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f17929fdf40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20c2aa4eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17995ae640>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1f1c3068b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1f1c3d2640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 492.4027 - loglik: -4.8507e+02 - logprior: -7.3361e+00
Epoch 2/10
14/14 - 4s - loss: 436.7840 - loglik: -4.3536e+02 - logprior: -1.4257e+00
Epoch 3/10
14/14 - 4s - loss: 401.6143 - loglik: -4.0030e+02 - logprior: -1.3176e+00
Epoch 4/10
14/14 - 4s - loss: 390.6235 - loglik: -3.8944e+02 - logprior: -1.1849e+00
Epoch 5/10
14/14 - 4s - loss: 385.5726 - loglik: -3.8445e+02 - logprior: -1.1192e+00
Epoch 6/10
14/14 - 4s - loss: 384.3521 - loglik: -3.8324e+02 - logprior: -1.1168e+00
Epoch 7/10
14/14 - 4s - loss: 382.8066 - loglik: -3.8166e+02 - logprior: -1.1492e+00
Epoch 8/10
14/14 - 4s - loss: 381.9860 - loglik: -3.8076e+02 - logprior: -1.2280e+00
Epoch 9/10
14/14 - 4s - loss: 382.1795 - loglik: -3.8091e+02 - logprior: -1.2688e+00
Fitted a model with MAP estimate = -381.4072
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (34, 1), (50, 1), (52, 1), (53, 1), (56, 1), (57, 1), (65, 1), (71, 4), (102, 4), (117, 1), (120, 2), (128, 1), (129, 2), (130, 2), (131, 1), (133, 2), (139, 1)]
discards: []
Fitting a model of length 184 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 390.8423 - loglik: -3.8319e+02 - logprior: -7.6520e+00
Epoch 2/2
14/14 - 6s - loss: 369.9755 - loglik: -3.6825e+02 - logprior: -1.7244e+00
Fitted a model with MAP estimate = -366.5654
expansions: [(126, 1), (127, 1), (129, 1)]
discards: [ 41 147 160 168]
Fitting a model of length 183 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 372.4652 - loglik: -3.6713e+02 - logprior: -5.3358e+00
Epoch 2/2
14/14 - 6s - loss: 365.1460 - loglik: -3.6383e+02 - logprior: -1.3130e+00
Fitted a model with MAP estimate = -363.2344
expansions: []
discards: []
Fitting a model of length 183 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 369.2971 - loglik: -3.6408e+02 - logprior: -5.2188e+00
Epoch 2/10
14/14 - 5s - loss: 363.8312 - loglik: -3.6259e+02 - logprior: -1.2422e+00
Epoch 3/10
14/14 - 6s - loss: 362.8098 - loglik: -3.6210e+02 - logprior: -7.0570e-01
Epoch 4/10
14/14 - 6s - loss: 360.9072 - loglik: -3.6030e+02 - logprior: -6.0646e-01
Epoch 5/10
14/14 - 6s - loss: 359.8225 - loglik: -3.5924e+02 - logprior: -5.8091e-01
Epoch 6/10
14/14 - 6s - loss: 358.3351 - loglik: -3.5778e+02 - logprior: -5.5339e-01
Epoch 7/10
14/14 - 6s - loss: 358.1795 - loglik: -3.5763e+02 - logprior: -5.4878e-01
Epoch 8/10
14/14 - 6s - loss: 357.3963 - loglik: -3.5683e+02 - logprior: -5.6623e-01
Epoch 9/10
14/14 - 6s - loss: 357.8494 - loglik: -3.5728e+02 - logprior: -5.6969e-01
Fitted a model with MAP estimate = -356.8419
Time for alignment: 154.0087
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 493.4785 - loglik: -4.8614e+02 - logprior: -7.3363e+00
Epoch 2/10
14/14 - 4s - loss: 435.1662 - loglik: -4.3373e+02 - logprior: -1.4345e+00
Epoch 3/10
14/14 - 4s - loss: 399.9791 - loglik: -3.9865e+02 - logprior: -1.3288e+00
Epoch 4/10
14/14 - 4s - loss: 388.9208 - loglik: -3.8772e+02 - logprior: -1.1998e+00
Epoch 5/10
14/14 - 4s - loss: 383.0996 - loglik: -3.8193e+02 - logprior: -1.1695e+00
Epoch 6/10
14/14 - 4s - loss: 382.2720 - loglik: -3.8111e+02 - logprior: -1.1660e+00
Epoch 7/10
14/14 - 4s - loss: 381.2852 - loglik: -3.8009e+02 - logprior: -1.1945e+00
Epoch 8/10
14/14 - 4s - loss: 380.5921 - loglik: -3.7934e+02 - logprior: -1.2567e+00
Epoch 9/10
14/14 - 4s - loss: 379.6387 - loglik: -3.7834e+02 - logprior: -1.2979e+00
Epoch 10/10
14/14 - 4s - loss: 380.2268 - loglik: -3.7889e+02 - logprior: -1.3414e+00
Fitted a model with MAP estimate = -379.4441
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (66, 1), (69, 1), (70, 3), (89, 1), (102, 4), (118, 1), (120, 2), (128, 1), (131, 1), (133, 1), (134, 1), (139, 1)]
discards: []
Fitting a model of length 180 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 11s - loss: 389.6795 - loglik: -3.8202e+02 - logprior: -7.6597e+00
Epoch 2/2
14/14 - 6s - loss: 370.6128 - loglik: -3.6896e+02 - logprior: -1.6540e+00
Fitted a model with MAP estimate = -367.5765
expansions: [(126, 1), (127, 1), (129, 1)]
discards: [ 41 147]
Fitting a model of length 181 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 372.3989 - loglik: -3.6712e+02 - logprior: -5.2785e+00
Epoch 2/2
14/14 - 6s - loss: 366.6093 - loglik: -3.6534e+02 - logprior: -1.2675e+00
Fitted a model with MAP estimate = -365.1568
expansions: []
discards: [0]
Fitting a model of length 180 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 375.2193 - loglik: -3.6823e+02 - logprior: -6.9874e+00
Epoch 2/10
14/14 - 6s - loss: 368.6975 - loglik: -3.6600e+02 - logprior: -2.7023e+00
Epoch 3/10
14/14 - 5s - loss: 366.0916 - loglik: -3.6459e+02 - logprior: -1.5006e+00
Epoch 4/10
14/14 - 6s - loss: 364.3823 - loglik: -3.6392e+02 - logprior: -4.5973e-01
Epoch 5/10
14/14 - 6s - loss: 363.0932 - loglik: -3.6271e+02 - logprior: -3.8619e-01
Epoch 6/10
14/14 - 6s - loss: 360.4586 - loglik: -3.6009e+02 - logprior: -3.6874e-01
Epoch 7/10
14/14 - 6s - loss: 361.1561 - loglik: -3.6079e+02 - logprior: -3.6143e-01
Fitted a model with MAP estimate = -360.2771
Time for alignment: 144.7645
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 492.5437 - loglik: -4.8522e+02 - logprior: -7.3259e+00
Epoch 2/10
14/14 - 4s - loss: 437.9063 - loglik: -4.3648e+02 - logprior: -1.4300e+00
Epoch 3/10
14/14 - 4s - loss: 402.7441 - loglik: -4.0133e+02 - logprior: -1.4122e+00
Epoch 4/10
14/14 - 4s - loss: 391.1460 - loglik: -3.8975e+02 - logprior: -1.3954e+00
Epoch 5/10
14/14 - 4s - loss: 386.5111 - loglik: -3.8518e+02 - logprior: -1.3271e+00
Epoch 6/10
14/14 - 4s - loss: 383.9667 - loglik: -3.8263e+02 - logprior: -1.3347e+00
Epoch 7/10
14/14 - 4s - loss: 382.1531 - loglik: -3.8080e+02 - logprior: -1.3517e+00
Epoch 8/10
14/14 - 4s - loss: 382.7253 - loglik: -3.8133e+02 - logprior: -1.3994e+00
Fitted a model with MAP estimate = -381.6966
expansions: [(0, 2), (9, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (61, 1), (69, 1), (71, 4), (82, 1), (89, 1), (102, 4), (117, 1), (124, 1), (128, 1), (129, 2), (130, 2), (131, 1), (133, 2), (139, 1)]
discards: []
Fitting a model of length 185 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 11s - loss: 390.5547 - loglik: -3.8307e+02 - logprior: -7.4867e+00
Epoch 2/2
14/14 - 6s - loss: 370.2692 - loglik: -3.6858e+02 - logprior: -1.6921e+00
Fitted a model with MAP estimate = -366.4428
expansions: [(128, 1), (129, 1), (131, 1)]
discards: [ 41  91  92 160 169]
Fitting a model of length 183 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 373.3878 - loglik: -3.6816e+02 - logprior: -5.2292e+00
Epoch 2/2
14/14 - 6s - loss: 365.1589 - loglik: -3.6397e+02 - logprior: -1.1852e+00
Fitted a model with MAP estimate = -364.5120
expansions: []
discards: []
Fitting a model of length 183 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 370.6321 - loglik: -3.6549e+02 - logprior: -5.1384e+00
Epoch 2/10
14/14 - 6s - loss: 364.6357 - loglik: -3.6349e+02 - logprior: -1.1462e+00
Epoch 3/10
14/14 - 6s - loss: 364.2112 - loglik: -3.6359e+02 - logprior: -6.2503e-01
Epoch 4/10
14/14 - 6s - loss: 362.2444 - loglik: -3.6173e+02 - logprior: -5.1366e-01
Epoch 5/10
14/14 - 6s - loss: 361.7838 - loglik: -3.6134e+02 - logprior: -4.4568e-01
Epoch 6/10
14/14 - 6s - loss: 359.0889 - loglik: -3.5866e+02 - logprior: -4.2909e-01
Epoch 7/10
14/14 - 6s - loss: 359.2319 - loglik: -3.5883e+02 - logprior: -4.0008e-01
Fitted a model with MAP estimate = -358.8416
Time for alignment: 137.5163
Computed alignments with likelihoods: ['-356.8419', '-360.2771', '-358.8416']
Best model has likelihood: -356.8419
SP score = 0.8196
