Training of 3 independent models on file cys.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f329c941dc0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f350b35aeb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329c9496d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34eedfa370>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34eedfa820>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34eedfa880>, <__main__.SimpleDirichletPrior object at 0x7f34eedfae20>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34eedfcb80>

Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 538.3035 - loglik: -5.3312e+02 - logprior: -5.1445e+00
Epoch 2/10
25/25 - 8s - loss: 429.5783 - loglik: -4.2730e+02 - logprior: -2.0926e+00
Epoch 3/10
25/25 - 8s - loss: 405.9830 - loglik: -4.0288e+02 - logprior: -2.4075e+00
Epoch 4/10
25/25 - 7s - loss: 404.5660 - loglik: -4.0142e+02 - logprior: -2.3149e+00
Epoch 5/10
25/25 - 7s - loss: 401.5507 - loglik: -3.9844e+02 - logprior: -2.3098e+00
Epoch 6/10
25/25 - 8s - loss: 400.0816 - loglik: -3.9697e+02 - logprior: -2.3112e+00
Epoch 7/10
25/25 - 8s - loss: 402.8958 - loglik: -3.9977e+02 - logprior: -2.3293e+00
Fitted a model with MAP estimate = -399.3420
expansions: [(9, 3), (10, 1), (11, 2), (13, 1), (31, 3), (32, 2), (34, 2), (47, 3), (56, 1), (57, 1), (59, 1), (76, 1), (80, 1), (81, 1), (82, 1), (83, 2), (91, 1), (93, 1), (96, 1), (99, 1), (102, 1), (113, 2), (115, 1), (122, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (160, 1), (162, 2), (168, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 219 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 393.5704 - loglik: -3.8539e+02 - logprior: -7.4866e+00
Epoch 2/2
25/25 - 9s - loss: 379.8832 - loglik: -3.7583e+02 - logprior: -3.2846e+00
Fitted a model with MAP estimate = -377.0524
expansions: [(0, 2), (214, 1)]
discards: [  0   9  41  45  62 106 143 177 206]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 381.9457 - loglik: -3.7631e+02 - logprior: -4.8429e+00
Epoch 2/2
25/25 - 9s - loss: 375.2921 - loglik: -3.7351e+02 - logprior: -1.0018e+00
Fitted a model with MAP estimate = -374.3999
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 12s - loss: 383.7029 - loglik: -3.7605e+02 - logprior: -6.8567e+00
Epoch 2/10
25/25 - 9s - loss: 377.2930 - loglik: -3.7494e+02 - logprior: -1.5990e+00
Epoch 3/10
25/25 - 9s - loss: 374.2489 - loglik: -3.7320e+02 - logprior: -2.8399e-01
Epoch 4/10
25/25 - 9s - loss: 375.9971 - loglik: -3.7509e+02 - logprior: -1.7302e-01
Fitted a model with MAP estimate = -373.8834
Time for alignment: 171.9255
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 539.3470 - loglik: -5.3420e+02 - logprior: -5.1126e+00
Epoch 2/10
25/25 - 8s - loss: 426.0464 - loglik: -4.2374e+02 - logprior: -2.1202e+00
Epoch 3/10
25/25 - 8s - loss: 404.2555 - loglik: -4.0101e+02 - logprior: -2.5779e+00
Epoch 4/10
25/25 - 8s - loss: 405.8081 - loglik: -4.0262e+02 - logprior: -2.4138e+00
Fitted a model with MAP estimate = -401.0265
expansions: [(9, 3), (10, 1), (11, 2), (12, 1), (31, 3), (32, 2), (34, 2), (47, 3), (57, 1), (60, 1), (62, 2), (74, 1), (75, 1), (80, 1), (82, 1), (83, 2), (87, 1), (90, 1), (92, 1), (95, 1), (98, 1), (101, 1), (112, 2), (114, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (150, 1), (159, 1), (167, 1), (168, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 220 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 393.8771 - loglik: -3.8577e+02 - logprior: -7.3758e+00
Epoch 2/2
25/25 - 9s - loss: 381.1163 - loglik: -3.7728e+02 - logprior: -3.0250e+00
Fitted a model with MAP estimate = -376.5525
expansions: [(0, 2)]
discards: [  0   9  41  45  62  82 107 144 178]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 382.0709 - loglik: -3.7632e+02 - logprior: -4.8571e+00
Epoch 2/2
25/25 - 9s - loss: 377.0646 - loglik: -3.7521e+02 - logprior: -1.0275e+00
Fitted a model with MAP estimate = -375.0375
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 12s - loss: 383.4861 - loglik: -3.7577e+02 - logprior: -6.8920e+00
Epoch 2/10
25/25 - 9s - loss: 378.5490 - loglik: -3.7615e+02 - logprior: -1.5950e+00
Epoch 3/10
25/25 - 9s - loss: 375.2320 - loglik: -3.7411e+02 - logprior: -3.2740e-01
Epoch 4/10
25/25 - 9s - loss: 375.9776 - loglik: -3.7499e+02 - logprior: -2.2160e-01
Fitted a model with MAP estimate = -374.2769
Time for alignment: 148.9623
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 538.5973 - loglik: -5.3343e+02 - logprior: -5.1247e+00
Epoch 2/10
25/25 - 8s - loss: 426.9517 - loglik: -4.2462e+02 - logprior: -2.0538e+00
Epoch 3/10
25/25 - 8s - loss: 405.2395 - loglik: -4.0196e+02 - logprior: -2.3420e+00
Epoch 4/10
25/25 - 8s - loss: 403.5753 - loglik: -4.0035e+02 - logprior: -2.3009e+00
Epoch 5/10
25/25 - 8s - loss: 399.8240 - loglik: -3.9662e+02 - logprior: -2.3287e+00
Epoch 6/10
25/25 - 8s - loss: 400.9414 - loglik: -3.9777e+02 - logprior: -2.3370e+00
Fitted a model with MAP estimate = -399.0050
expansions: [(9, 3), (10, 1), (11, 2), (31, 1), (32, 2), (33, 1), (35, 2), (36, 1), (47, 1), (48, 1), (58, 1), (60, 1), (62, 2), (76, 1), (80, 1), (81, 1), (82, 1), (83, 2), (90, 2), (92, 1), (95, 1), (98, 1), (102, 1), (112, 2), (122, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (150, 1), (159, 1), (168, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 217 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 13s - loss: 393.3032 - loglik: -3.8508e+02 - logprior: -7.4832e+00
Epoch 2/2
25/25 - 10s - loss: 380.5504 - loglik: -3.7649e+02 - logprior: -3.2403e+00
Fitted a model with MAP estimate = -376.8536
expansions: [(0, 2), (212, 1)]
discards: [  0   9  44  80 105 142 176]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 382.1370 - loglik: -3.7646e+02 - logprior: -4.8594e+00
Epoch 2/2
25/25 - 9s - loss: 374.9520 - loglik: -3.7315e+02 - logprior: -1.0077e+00
Fitted a model with MAP estimate = -374.7301
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 12s - loss: 383.1234 - loglik: -3.7546e+02 - logprior: -6.8704e+00
Epoch 2/10
25/25 - 9s - loss: 377.8390 - loglik: -3.7546e+02 - logprior: -1.6047e+00
Epoch 3/10
25/25 - 9s - loss: 376.6670 - loglik: -3.7560e+02 - logprior: -2.9611e-01
Epoch 4/10
25/25 - 9s - loss: 374.5441 - loglik: -3.7360e+02 - logprior: -1.9703e-01
Epoch 5/10
25/25 - 9s - loss: 375.0091 - loglik: -3.7418e+02 - logprior: -7.0233e-02
Fitted a model with MAP estimate = -373.8979
Time for alignment: 174.1520
Computed alignments with likelihoods: ['-373.8834', '-374.2769', '-373.8979']
Best model has likelihood: -373.8834
time for generating output: 0.2663
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cys.projection.fasta
SP score = 0.9256674014205242
Training of 3 independent models on file DMRL_synthase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f35082a9ac0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34eedfab80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34eedfa820>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34eedfa8e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34eedfa370>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f350b35aeb0>, <__main__.SimpleDirichletPrior object at 0x7f34ee7c6880>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34eedfcb80>

Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 406.6853 - loglik: -3.8622e+02 - logprior: -2.0462e+01
Epoch 2/10
10/10 - 2s - loss: 342.0368 - loglik: -3.3708e+02 - logprior: -4.9567e+00
Epoch 3/10
10/10 - 2s - loss: 289.1624 - loglik: -2.8632e+02 - logprior: -2.8263e+00
Epoch 4/10
10/10 - 2s - loss: 256.6388 - loglik: -2.5390e+02 - logprior: -2.6212e+00
Epoch 5/10
10/10 - 2s - loss: 244.7124 - loglik: -2.4192e+02 - logprior: -2.5306e+00
Epoch 6/10
10/10 - 2s - loss: 240.4143 - loglik: -2.3774e+02 - logprior: -2.2141e+00
Epoch 7/10
10/10 - 2s - loss: 238.9878 - loglik: -2.3651e+02 - logprior: -1.9838e+00
Epoch 8/10
10/10 - 2s - loss: 238.1206 - loglik: -2.3574e+02 - logprior: -1.9241e+00
Epoch 9/10
10/10 - 2s - loss: 237.2294 - loglik: -2.3490e+02 - logprior: -1.8869e+00
Epoch 10/10
10/10 - 2s - loss: 236.8086 - loglik: -2.3452e+02 - logprior: -1.8604e+00
Fitted a model with MAP estimate = -236.1400
expansions: [(0, 4), (13, 1), (14, 1), (19, 1), (20, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (98, 1), (106, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 145 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 250.9303 - loglik: -2.2327e+02 - logprior: -2.7233e+01
Epoch 2/2
10/10 - 2s - loss: 216.8837 - loglik: -2.0813e+02 - logprior: -8.2615e+00
Fitted a model with MAP estimate = -209.1215
expansions: [(131, 1)]
discards: [59 62]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 221.9834 - loglik: -2.0213e+02 - logprior: -1.9354e+01
Epoch 2/2
10/10 - 2s - loss: 204.3715 - loglik: -1.9909e+02 - logprior: -4.7891e+00
Fitted a model with MAP estimate = -201.8318
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 216.6888 - loglik: -1.9794e+02 - logprior: -1.8245e+01
Epoch 2/10
10/10 - 2s - loss: 203.7532 - loglik: -1.9880e+02 - logprior: -4.4367e+00
Epoch 3/10
10/10 - 2s - loss: 200.3096 - loglik: -1.9799e+02 - logprior: -1.7884e+00
Epoch 4/10
10/10 - 2s - loss: 199.9682 - loglik: -1.9861e+02 - logprior: -8.4537e-01
Epoch 5/10
10/10 - 2s - loss: 199.6214 - loglik: -1.9871e+02 - logprior: -3.8611e-01
Epoch 6/10
10/10 - 2s - loss: 199.0768 - loglik: -1.9851e+02 - logprior: -3.9291e-02
Epoch 7/10
10/10 - 2s - loss: 199.4076 - loglik: -1.9913e+02 - logprior: 0.2503
Fitted a model with MAP estimate = -198.4190
Time for alignment: 63.5882
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 406.7083 - loglik: -3.8624e+02 - logprior: -2.0461e+01
Epoch 2/10
10/10 - 2s - loss: 341.7994 - loglik: -3.3685e+02 - logprior: -4.9491e+00
Epoch 3/10
10/10 - 2s - loss: 287.7175 - loglik: -2.8494e+02 - logprior: -2.7654e+00
Epoch 4/10
10/10 - 2s - loss: 255.9020 - loglik: -2.5334e+02 - logprior: -2.4571e+00
Epoch 5/10
10/10 - 2s - loss: 245.6865 - loglik: -2.4312e+02 - logprior: -2.3027e+00
Epoch 6/10
10/10 - 2s - loss: 241.1183 - loglik: -2.3860e+02 - logprior: -2.0570e+00
Epoch 7/10
10/10 - 2s - loss: 238.9379 - loglik: -2.3654e+02 - logprior: -1.9186e+00
Epoch 8/10
10/10 - 2s - loss: 237.3461 - loglik: -2.3500e+02 - logprior: -1.8942e+00
Epoch 9/10
10/10 - 2s - loss: 237.8636 - loglik: -2.3555e+02 - logprior: -1.8727e+00
Fitted a model with MAP estimate = -236.4150
expansions: [(0, 4), (13, 1), (14, 2), (15, 2), (26, 1), (27, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (98, 1), (104, 3), (105, 2), (106, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 250.6679 - loglik: -2.2322e+02 - logprior: -2.7025e+01
Epoch 2/2
10/10 - 2s - loss: 215.0936 - loglik: -2.0638e+02 - logprior: -8.2366e+00
Fitted a model with MAP estimate = -207.5048
expansions: []
discards: [ 19  60  63 137]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 220.2128 - loglik: -2.0040e+02 - logprior: -1.9314e+01
Epoch 2/2
10/10 - 2s - loss: 203.8182 - loglik: -1.9853e+02 - logprior: -4.7903e+00
Fitted a model with MAP estimate = -201.6145
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 216.8774 - loglik: -1.9811e+02 - logprior: -1.8261e+01
Epoch 2/10
10/10 - 2s - loss: 203.3031 - loglik: -1.9832e+02 - logprior: -4.4633e+00
Epoch 3/10
10/10 - 2s - loss: 200.5962 - loglik: -1.9827e+02 - logprior: -1.8040e+00
Epoch 4/10
10/10 - 2s - loss: 199.8349 - loglik: -1.9845e+02 - logprior: -8.6617e-01
Epoch 5/10
10/10 - 2s - loss: 199.5534 - loglik: -1.9863e+02 - logprior: -4.0278e-01
Epoch 6/10
10/10 - 2s - loss: 199.3106 - loglik: -1.9873e+02 - logprior: -5.9256e-02
Epoch 7/10
10/10 - 2s - loss: 198.9655 - loglik: -1.9868e+02 - logprior: 0.2395
Epoch 8/10
10/10 - 2s - loss: 199.1516 - loglik: -1.9905e+02 - logprior: 0.4224
Fitted a model with MAP estimate = -198.2711
Time for alignment: 63.8043
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 406.7867 - loglik: -3.8632e+02 - logprior: -2.0464e+01
Epoch 2/10
10/10 - 2s - loss: 341.7056 - loglik: -3.3675e+02 - logprior: -4.9497e+00
Epoch 3/10
10/10 - 2s - loss: 288.1146 - loglik: -2.8536e+02 - logprior: -2.7389e+00
Epoch 4/10
10/10 - 2s - loss: 256.1779 - loglik: -2.5358e+02 - logprior: -2.4556e+00
Epoch 5/10
10/10 - 2s - loss: 245.3878 - loglik: -2.4253e+02 - logprior: -2.4190e+00
Epoch 6/10
10/10 - 2s - loss: 240.2429 - loglik: -2.3747e+02 - logprior: -2.2116e+00
Epoch 7/10
10/10 - 2s - loss: 239.0314 - loglik: -2.3653e+02 - logprior: -2.0272e+00
Epoch 8/10
10/10 - 2s - loss: 237.4899 - loglik: -2.3513e+02 - logprior: -1.9524e+00
Epoch 9/10
10/10 - 2s - loss: 237.4143 - loglik: -2.3511e+02 - logprior: -1.9019e+00
Epoch 10/10
10/10 - 2s - loss: 237.0783 - loglik: -2.3481e+02 - logprior: -1.8602e+00
Fitted a model with MAP estimate = -235.7953
expansions: [(0, 4), (13, 1), (14, 1), (15, 1), (22, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (105, 5), (106, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 146 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 251.8159 - loglik: -2.2419e+02 - logprior: -2.7220e+01
Epoch 2/2
10/10 - 2s - loss: 216.4934 - loglik: -2.0775e+02 - logprior: -8.2980e+00
Fitted a model with MAP estimate = -209.1009
expansions: [(133, 1)]
discards: [ 59  62 135]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 220.9597 - loglik: -2.0116e+02 - logprior: -1.9353e+01
Epoch 2/2
10/10 - 2s - loss: 204.8959 - loglik: -1.9966e+02 - logprior: -4.7779e+00
Fitted a model with MAP estimate = -201.8230
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 217.3180 - loglik: -1.9859e+02 - logprior: -1.8248e+01
Epoch 2/10
10/10 - 2s - loss: 203.0557 - loglik: -1.9810e+02 - logprior: -4.4439e+00
Epoch 3/10
10/10 - 2s - loss: 200.7980 - loglik: -1.9849e+02 - logprior: -1.7921e+00
Epoch 4/10
10/10 - 2s - loss: 199.9427 - loglik: -1.9858e+02 - logprior: -8.5372e-01
Epoch 5/10
10/10 - 2s - loss: 199.6315 - loglik: -1.9872e+02 - logprior: -3.9202e-01
Epoch 6/10
10/10 - 2s - loss: 199.1521 - loglik: -1.9858e+02 - logprior: -4.9488e-02
Epoch 7/10
10/10 - 2s - loss: 199.0743 - loglik: -1.9880e+02 - logprior: 0.2462
Epoch 8/10
10/10 - 2s - loss: 198.7137 - loglik: -1.9863e+02 - logprior: 0.4364
Epoch 9/10
10/10 - 2s - loss: 199.2183 - loglik: -1.9923e+02 - logprior: 0.5410
Fitted a model with MAP estimate = -198.2061
Time for alignment: 67.9440
Computed alignments with likelihoods: ['-198.4190', '-198.2711', '-198.2061']
Best model has likelihood: -198.2061
time for generating output: 0.2160
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DMRL_synthase.projection.fasta
SP score = 0.910580204778157
Training of 3 independent models on file Stap_Strp_toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34b79fc9a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34b79e0d00>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b7c22820>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34ed8a8460>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b7980ca0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34b7980bb0>, <__main__.SimpleDirichletPrior object at 0x7f348d75e160>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34ecb10670>

Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 317.6876 - loglik: -2.4847e+02 - logprior: -6.9208e+01
Epoch 2/10
10/10 - 2s - loss: 236.8132 - loglik: -2.1885e+02 - logprior: -1.7948e+01
Epoch 3/10
10/10 - 2s - loss: 203.6847 - loglik: -1.9550e+02 - logprior: -8.1006e+00
Epoch 4/10
10/10 - 2s - loss: 187.8528 - loglik: -1.8326e+02 - logprior: -4.5582e+00
Epoch 5/10
10/10 - 2s - loss: 181.1876 - loglik: -1.7859e+02 - logprior: -2.5946e+00
Epoch 6/10
10/10 - 2s - loss: 178.3002 - loglik: -1.7678e+02 - logprior: -1.4933e+00
Epoch 7/10
10/10 - 2s - loss: 176.6926 - loglik: -1.7563e+02 - logprior: -9.4429e-01
Epoch 8/10
10/10 - 2s - loss: 175.6851 - loglik: -1.7481e+02 - logprior: -6.6534e-01
Epoch 9/10
10/10 - 2s - loss: 174.7142 - loglik: -1.7400e+02 - logprior: -4.9864e-01
Epoch 10/10
10/10 - 2s - loss: 174.2422 - loglik: -1.7376e+02 - logprior: -2.8893e-01
Fitted a model with MAP estimate = -173.8964
expansions: [(9, 3), (14, 1), (15, 1), (26, 1), (37, 1), (39, 1), (58, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 250.0286 - loglik: -1.7197e+02 - logprior: -7.7873e+01
Epoch 2/2
10/10 - 2s - loss: 198.2487 - loglik: -1.6609e+02 - logprior: -3.1925e+01
Fitted a model with MAP estimate = -189.2844
expansions: [(28, 1), (32, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 242.5725 - loglik: -1.6524e+02 - logprior: -7.7078e+01
Epoch 2/2
10/10 - 2s - loss: 193.2248 - loglik: -1.6354e+02 - logprior: -2.9409e+01
Fitted a model with MAP estimate = -182.6561
expansions: [(0, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 222.8793 - loglik: -1.6087e+02 - logprior: -6.1739e+01
Epoch 2/10
10/10 - 2s - loss: 175.4437 - loglik: -1.5924e+02 - logprior: -1.5930e+01
Epoch 3/10
10/10 - 2s - loss: 165.5949 - loglik: -1.5892e+02 - logprior: -6.3885e+00
Epoch 4/10
10/10 - 2s - loss: 161.7977 - loglik: -1.5915e+02 - logprior: -2.3637e+00
Epoch 5/10
10/10 - 2s - loss: 159.7744 - loglik: -1.5937e+02 - logprior: -1.1941e-01
Epoch 6/10
10/10 - 2s - loss: 158.6480 - loglik: -1.5957e+02 - logprior: 1.2041
Epoch 7/10
10/10 - 2s - loss: 158.0176 - loglik: -1.5975e+02 - logprior: 2.0228
Epoch 8/10
10/10 - 2s - loss: 157.5756 - loglik: -1.5986e+02 - logprior: 2.5718
Epoch 9/10
10/10 - 2s - loss: 157.2379 - loglik: -1.5993e+02 - logprior: 2.9835
Epoch 10/10
10/10 - 2s - loss: 156.9603 - loglik: -1.5999e+02 - logprior: 3.3228
Fitted a model with MAP estimate = -156.4978
Time for alignment: 67.5227
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 317.6862 - loglik: -2.4847e+02 - logprior: -6.9209e+01
Epoch 2/10
10/10 - 2s - loss: 236.8573 - loglik: -2.1889e+02 - logprior: -1.7947e+01
Epoch 3/10
10/10 - 2s - loss: 204.2123 - loglik: -1.9602e+02 - logprior: -8.1060e+00
Epoch 4/10
10/10 - 2s - loss: 190.1819 - loglik: -1.8560e+02 - logprior: -4.5420e+00
Epoch 5/10
10/10 - 2s - loss: 183.9358 - loglik: -1.8135e+02 - logprior: -2.5882e+00
Epoch 6/10
10/10 - 2s - loss: 180.5009 - loglik: -1.7896e+02 - logprior: -1.5353e+00
Epoch 7/10
10/10 - 2s - loss: 178.4682 - loglik: -1.7749e+02 - logprior: -9.5203e-01
Epoch 8/10
10/10 - 2s - loss: 177.4350 - loglik: -1.7666e+02 - logprior: -6.3411e-01
Epoch 9/10
10/10 - 2s - loss: 176.9222 - loglik: -1.7628e+02 - logprior: -4.0983e-01
Epoch 10/10
10/10 - 2s - loss: 176.0559 - loglik: -1.7560e+02 - logprior: -2.5145e-01
Fitted a model with MAP estimate = -175.3601
expansions: [(9, 3), (14, 1), (15, 1), (25, 1), (26, 2), (37, 1), (39, 1), (58, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 251.2826 - loglik: -1.7330e+02 - logprior: -7.7774e+01
Epoch 2/2
10/10 - 2s - loss: 198.7722 - loglik: -1.6675e+02 - logprior: -3.1786e+01
Fitted a model with MAP estimate = -189.6102
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 241.9057 - loglik: -1.6533e+02 - logprior: -7.6322e+01
Epoch 2/2
10/10 - 2s - loss: 191.6285 - loglik: -1.6474e+02 - logprior: -2.6618e+01
Fitted a model with MAP estimate = -180.2340
expansions: [(0, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 224.4359 - loglik: -1.6262e+02 - logprior: -6.1540e+01
Epoch 2/10
10/10 - 2s - loss: 177.4228 - loglik: -1.6134e+02 - logprior: -1.5804e+01
Epoch 3/10
10/10 - 2s - loss: 167.7503 - loglik: -1.6118e+02 - logprior: -6.2759e+00
Epoch 4/10
10/10 - 2s - loss: 163.9011 - loglik: -1.6131e+02 - logprior: -2.2967e+00
Epoch 5/10
10/10 - 2s - loss: 162.0476 - loglik: -1.6170e+02 - logprior: -5.5718e-02
Epoch 6/10
10/10 - 2s - loss: 160.9191 - loglik: -1.6189e+02 - logprior: 1.2690
Epoch 7/10
10/10 - 2s - loss: 160.1014 - loglik: -1.6192e+02 - logprior: 2.1097
Epoch 8/10
10/10 - 2s - loss: 159.5952 - loglik: -1.6196e+02 - logprior: 2.6564
Epoch 9/10
10/10 - 2s - loss: 159.2326 - loglik: -1.6200e+02 - logprior: 3.0714
Epoch 10/10
10/10 - 2s - loss: 158.9403 - loglik: -1.6205e+02 - logprior: 3.4062
Fitted a model with MAP estimate = -158.4655
Time for alignment: 67.2684
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 317.7029 - loglik: -2.4849e+02 - logprior: -6.9210e+01
Epoch 2/10
10/10 - 2s - loss: 236.7509 - loglik: -2.1879e+02 - logprior: -1.7947e+01
Epoch 3/10
10/10 - 2s - loss: 204.4733 - loglik: -1.9631e+02 - logprior: -8.0744e+00
Epoch 4/10
10/10 - 2s - loss: 192.0138 - loglik: -1.8758e+02 - logprior: -4.3842e+00
Epoch 5/10
10/10 - 2s - loss: 186.4805 - loglik: -1.8406e+02 - logprior: -2.4163e+00
Epoch 6/10
10/10 - 2s - loss: 183.2006 - loglik: -1.8182e+02 - logprior: -1.3629e+00
Epoch 7/10
10/10 - 2s - loss: 180.6669 - loglik: -1.7967e+02 - logprior: -9.1232e-01
Epoch 8/10
10/10 - 2s - loss: 178.6456 - loglik: -1.7779e+02 - logprior: -6.5460e-01
Epoch 9/10
10/10 - 2s - loss: 177.2933 - loglik: -1.7665e+02 - logprior: -4.2108e-01
Epoch 10/10
10/10 - 2s - loss: 176.3018 - loglik: -1.7583e+02 - logprior: -2.7578e-01
Fitted a model with MAP estimate = -175.8109
expansions: [(14, 2), (15, 1), (26, 1), (27, 3), (41, 1), (58, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 252.3846 - loglik: -1.7423e+02 - logprior: -7.7966e+01
Epoch 2/2
10/10 - 2s - loss: 200.2298 - loglik: -1.6791e+02 - logprior: -3.2103e+01
Fitted a model with MAP estimate = -191.3471
expansions: [(9, 2), (43, 1)]
discards: [ 0 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 244.1029 - loglik: -1.6693e+02 - logprior: -7.6969e+01
Epoch 2/2
10/10 - 2s - loss: 194.3480 - loglik: -1.6500e+02 - logprior: -2.9111e+01
Fitted a model with MAP estimate = -183.8034
expansions: [(0, 3), (5, 2)]
discards: [0 8]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 226.2119 - loglik: -1.6417e+02 - logprior: -6.1799e+01
Epoch 2/10
10/10 - 2s - loss: 177.8368 - loglik: -1.6163e+02 - logprior: -1.5936e+01
Epoch 3/10
10/10 - 2s - loss: 168.0746 - loglik: -1.6143e+02 - logprior: -6.3556e+00
Epoch 4/10
10/10 - 2s - loss: 164.2609 - loglik: -1.6164e+02 - logprior: -2.3311e+00
Epoch 5/10
10/10 - 2s - loss: 162.2814 - loglik: -1.6187e+02 - logprior: -1.1664e-01
Epoch 6/10
10/10 - 2s - loss: 161.2470 - loglik: -1.6214e+02 - logprior: 1.1915
Epoch 7/10
10/10 - 2s - loss: 160.6294 - loglik: -1.6235e+02 - logprior: 2.0161
Epoch 8/10
10/10 - 2s - loss: 160.2132 - loglik: -1.6249e+02 - logprior: 2.5680
Epoch 9/10
10/10 - 2s - loss: 159.8808 - loglik: -1.6256e+02 - logprior: 2.9795
Epoch 10/10
10/10 - 2s - loss: 159.6112 - loglik: -1.6264e+02 - logprior: 3.3194
Fitted a model with MAP estimate = -159.1465
Time for alignment: 67.2893
Computed alignments with likelihoods: ['-156.4978', '-158.4655', '-159.1465']
Best model has likelihood: -156.4978
time for generating output: 0.2320
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Stap_Strp_toxin.projection.fasta
SP score = 0.36652332120410186
Training of 3 independent models on file ghf5.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3495c08a00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34b797a040>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34a6febd00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34a6a73d60>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34a6a73fd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34ed5371c0>, <__main__.SimpleDirichletPrior object at 0x7f34af678a00>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34ee928d30>

Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 15s - loss: 756.8945 - loglik: -7.4836e+02 - logprior: -8.4136e+00
Epoch 2/10
20/20 - 11s - loss: 683.9576 - loglik: -6.8288e+02 - logprior: -2.9465e-01
Epoch 3/10
20/20 - 11s - loss: 653.6782 - loglik: -6.5170e+02 - logprior: -8.4693e-01
Epoch 4/10
20/20 - 11s - loss: 645.2222 - loglik: -6.4253e+02 - logprior: -1.1709e+00
Epoch 5/10
20/20 - 11s - loss: 639.0948 - loglik: -6.3645e+02 - logprior: -1.1250e+00
Epoch 6/10
20/20 - 11s - loss: 639.6241 - loglik: -6.3717e+02 - logprior: -1.1953e+00
Fitted a model with MAP estimate = -634.8394
expansions: [(25, 1), (52, 1), (54, 2), (56, 1), (91, 1), (92, 1), (93, 1), (99, 1), (115, 2), (117, 1), (147, 1), (153, 1), (171, 11), (176, 1), (198, 4), (207, 1), (208, 1)]
discards: [217 218]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 255 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 16s - loss: 639.0997 - loglik: -6.2908e+02 - logprior: -8.8403e+00
Epoch 2/2
20/20 - 12s - loss: 625.9816 - loglik: -6.2365e+02 - logprior: -1.2235e+00
Fitted a model with MAP estimate = -621.8031
expansions: [(0, 4), (86, 5), (249, 1)]
discards: [124]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 16s - loss: 634.4750 - loglik: -6.1970e+02 - logprior: -1.3503e+01
Epoch 2/2
20/20 - 13s - loss: 618.8071 - loglik: -6.1574e+02 - logprior: -1.7501e+00
Fitted a model with MAP estimate = -615.8401
expansions: [(55, 2), (91, 1)]
discards: [2 3 4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 16s - loss: 624.6181 - loglik: -6.1475e+02 - logprior: -8.4579e+00
Epoch 2/10
20/20 - 13s - loss: 616.7792 - loglik: -6.1481e+02 - logprior: -6.2120e-01
Epoch 3/10
20/20 - 13s - loss: 612.8228 - loglik: -6.1173e+02 - logprior: 0.2481
Epoch 4/10
20/20 - 13s - loss: 612.8610 - loglik: -6.1240e+02 - logprior: 0.7974
Fitted a model with MAP estimate = -610.8553
Time for alignment: 215.1348
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 757.5642 - loglik: -7.4901e+02 - logprior: -8.4351e+00
Epoch 2/10
20/20 - 11s - loss: 681.9176 - loglik: -6.8085e+02 - logprior: -2.9523e-01
Epoch 3/10
20/20 - 11s - loss: 651.7780 - loglik: -6.4979e+02 - logprior: -8.4956e-01
Epoch 4/10
20/20 - 11s - loss: 639.8251 - loglik: -6.3720e+02 - logprior: -1.0629e+00
Epoch 5/10
20/20 - 11s - loss: 634.5115 - loglik: -6.3204e+02 - logprior: -9.6871e-01
Epoch 6/10
20/20 - 11s - loss: 635.3814 - loglik: -6.3312e+02 - logprior: -9.9340e-01
Fitted a model with MAP estimate = -632.2201
expansions: [(0, 4), (45, 5), (52, 1), (77, 1), (83, 1), (84, 1), (86, 13), (87, 1), (92, 3), (94, 1), (103, 1), (116, 2), (118, 1), (147, 1), (153, 1), (155, 1), (171, 11), (199, 5), (207, 1), (208, 1)]
discards: [217 218]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 279 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 17s - loss: 640.6733 - loglik: -6.2697e+02 - logprior: -1.2513e+01
Epoch 2/2
20/20 - 14s - loss: 620.6442 - loglik: -6.1744e+02 - logprior: -2.0144e+00
Fitted a model with MAP estimate = -614.6261
expansions: [(0, 4), (143, 4), (186, 1), (273, 1)]
discards: [  1   2   3   4  54  91  92 120 148]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 280 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 17s - loss: 630.2111 - loglik: -6.1505e+02 - logprior: -1.3752e+01
Epoch 2/2
20/20 - 14s - loss: 614.4941 - loglik: -6.1084e+02 - logprior: -2.1362e+00
Fitted a model with MAP estimate = -609.8994
expansions: [(0, 5), (90, 1)]
discards: [  4  18  19  91 141 142]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 280 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 17s - loss: 624.4772 - loglik: -6.0944e+02 - logprior: -1.3337e+01
Epoch 2/10
20/20 - 14s - loss: 610.6594 - loglik: -6.0699e+02 - logprior: -2.1077e+00
Epoch 3/10
20/20 - 14s - loss: 606.6096 - loglik: -6.0511e+02 - logprior: -3.7340e-02
Epoch 4/10
20/20 - 14s - loss: 605.9084 - loglik: -6.0519e+02 - logprior: 0.6280
Epoch 5/10
20/20 - 14s - loss: 605.1108 - loglik: -6.0489e+02 - logprior: 1.0301
Epoch 6/10
20/20 - 14s - loss: 604.5729 - loglik: -6.0468e+02 - logprior: 1.2852
Epoch 7/10
20/20 - 14s - loss: 604.1606 - loglik: -6.0464e+02 - logprior: 1.6129
Epoch 8/10
20/20 - 14s - loss: 603.2495 - loglik: -6.0403e+02 - logprior: 1.8801
Epoch 9/10
20/20 - 14s - loss: 602.8351 - loglik: -6.0398e+02 - logprior: 2.2125
Epoch 10/10
20/20 - 14s - loss: 601.1721 - loglik: -6.0263e+02 - logprior: 2.5123
Fitted a model with MAP estimate = -601.0029
Time for alignment: 310.5052
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 15s - loss: 756.1927 - loglik: -7.4765e+02 - logprior: -8.4209e+00
Epoch 2/10
20/20 - 11s - loss: 683.9415 - loglik: -6.8293e+02 - logprior: -2.3894e-01
Epoch 3/10
20/20 - 11s - loss: 651.5207 - loglik: -6.4938e+02 - logprior: -7.3838e-01
Epoch 4/10
20/20 - 10s - loss: 643.5958 - loglik: -6.4089e+02 - logprior: -8.0969e-01
Epoch 5/10
20/20 - 11s - loss: 639.4259 - loglik: -6.3714e+02 - logprior: -7.4465e-01
Epoch 6/10
20/20 - 11s - loss: 633.1152 - loglik: -6.3091e+02 - logprior: -9.4490e-01
Epoch 7/10
20/20 - 11s - loss: 635.2089 - loglik: -6.3304e+02 - logprior: -1.0081e+00
Fitted a model with MAP estimate = -632.1198
expansions: [(0, 4), (24, 1), (44, 1), (50, 1), (51, 3), (81, 1), (85, 14), (94, 1), (100, 1), (112, 2), (115, 2), (117, 1), (146, 1), (149, 1), (171, 12), (180, 1), (193, 1), (198, 3), (207, 1), (208, 1)]
discards: [217 218]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 276 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 17s - loss: 640.5159 - loglik: -6.2679e+02 - logprior: -1.2685e+01
Epoch 2/2
20/20 - 14s - loss: 618.6315 - loglik: -6.1561e+02 - logprior: -1.9306e+00
Fitted a model with MAP estimate = -614.0239
expansions: [(0, 5), (270, 1)]
discards: [ 56  88 144 246]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 278 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 18s - loss: 626.8987 - loglik: -6.1202e+02 - logprior: -1.3369e+01
Epoch 2/2
20/20 - 14s - loss: 614.1563 - loglik: -6.0996e+02 - logprior: -2.5722e+00
Fitted a model with MAP estimate = -608.0664
expansions: [(0, 3), (106, 1)]
discards: [ 0 22]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 280 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 18s - loss: 619.4537 - loglik: -6.0751e+02 - logprior: -1.0432e+01
Epoch 2/10
20/20 - 14s - loss: 606.5903 - loglik: -6.0415e+02 - logprior: -1.0494e+00
Epoch 3/10
20/20 - 14s - loss: 607.6297 - loglik: -6.0622e+02 - logprior: -1.2545e-01
Fitted a model with MAP estimate = -604.1737
Time for alignment: 222.7595
Computed alignments with likelihoods: ['-610.8553', '-601.0029', '-604.1737']
Best model has likelihood: -601.0029
time for generating output: 0.3397
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf5.projection.fasta
SP score = 0.6490066225165563
Training of 3 independent models on file myb_DNA-binding.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34b7fcdee0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34eed1e9a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34eed1e340>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34b7dac070>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b7dac310>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3440bbb4f0>, <__main__.SimpleDirichletPrior object at 0x7f3462a1e880>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34edf7a160>

Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.6886 - loglik: -1.3142e+02 - logprior: -3.2618e+00
Epoch 2/10
19/19 - 1s - loss: 114.1262 - loglik: -1.1268e+02 - logprior: -1.3906e+00
Epoch 3/10
19/19 - 1s - loss: 106.9403 - loglik: -1.0514e+02 - logprior: -1.5566e+00
Epoch 4/10
19/19 - 1s - loss: 104.6054 - loglik: -1.0284e+02 - logprior: -1.5056e+00
Epoch 5/10
19/19 - 1s - loss: 103.4493 - loglik: -1.0175e+02 - logprior: -1.4861e+00
Epoch 6/10
19/19 - 1s - loss: 102.7192 - loglik: -1.0105e+02 - logprior: -1.4824e+00
Epoch 7/10
19/19 - 1s - loss: 102.3058 - loglik: -1.0065e+02 - logprior: -1.4848e+00
Epoch 8/10
19/19 - 1s - loss: 101.8837 - loglik: -1.0024e+02 - logprior: -1.4854e+00
Epoch 9/10
19/19 - 2s - loss: 101.8395 - loglik: -1.0021e+02 - logprior: -1.4829e+00
Epoch 10/10
19/19 - 2s - loss: 101.9441 - loglik: -1.0031e+02 - logprior: -1.4807e+00
Fitted a model with MAP estimate = -100.2217
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (22, 2), (23, 1), (28, 2), (29, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 102.4166 - loglik: -9.8000e+01 - logprior: -4.2567e+00
Epoch 2/2
19/19 - 2s - loss: 94.0915 - loglik: -9.1732e+01 - logprior: -2.1799e+00
Fitted a model with MAP estimate = -91.4450
expansions: [(0, 1)]
discards: [ 0  9 31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 94.4193 - loglik: -9.0960e+01 - logprior: -3.2583e+00
Epoch 2/2
19/19 - 2s - loss: 91.4568 - loglik: -8.9716e+01 - logprior: -1.5342e+00
Fitted a model with MAP estimate = -89.7805
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 92.1193 - loglik: -8.8654e+01 - logprior: -3.2594e+00
Epoch 2/10
19/19 - 1s - loss: 90.0576 - loglik: -8.8347e+01 - logprior: -1.5038e+00
Epoch 3/10
19/19 - 1s - loss: 89.6934 - loglik: -8.8073e+01 - logprior: -1.4078e+00
Epoch 4/10
19/19 - 1s - loss: 89.5823 - loglik: -8.8009e+01 - logprior: -1.3591e+00
Epoch 5/10
19/19 - 2s - loss: 89.4854 - loglik: -8.7930e+01 - logprior: -1.3404e+00
Epoch 6/10
19/19 - 2s - loss: 89.6040 - loglik: -8.8074e+01 - logprior: -1.3145e+00
Fitted a model with MAP estimate = -89.2262
Time for alignment: 53.5797
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 134.7189 - loglik: -1.3145e+02 - logprior: -3.2625e+00
Epoch 2/10
19/19 - 1s - loss: 113.6266 - loglik: -1.1218e+02 - logprior: -1.3899e+00
Epoch 3/10
19/19 - 1s - loss: 106.9089 - loglik: -1.0500e+02 - logprior: -1.5576e+00
Epoch 4/10
19/19 - 1s - loss: 104.9653 - loglik: -1.0325e+02 - logprior: -1.4575e+00
Epoch 5/10
19/19 - 2s - loss: 104.1671 - loglik: -1.0247e+02 - logprior: -1.4444e+00
Epoch 6/10
19/19 - 1s - loss: 103.3669 - loglik: -1.0172e+02 - logprior: -1.4471e+00
Epoch 7/10
19/19 - 1s - loss: 103.0646 - loglik: -1.0144e+02 - logprior: -1.4419e+00
Epoch 8/10
19/19 - 1s - loss: 102.8963 - loglik: -1.0129e+02 - logprior: -1.4397e+00
Epoch 9/10
19/19 - 1s - loss: 102.6343 - loglik: -1.0103e+02 - logprior: -1.4387e+00
Epoch 10/10
19/19 - 1s - loss: 102.9399 - loglik: -1.0134e+02 - logprior: -1.4361e+00
Fitted a model with MAP estimate = -101.1236
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (23, 2), (28, 3), (30, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 104.6383 - loglik: -1.0018e+02 - logprior: -4.2980e+00
Epoch 2/2
19/19 - 2s - loss: 95.9386 - loglik: -9.3520e+01 - logprior: -2.2284e+00
Fitted a model with MAP estimate = -92.7227
expansions: [(0, 1)]
discards: [ 0  9 30 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 95.0910 - loglik: -9.1623e+01 - logprior: -3.2684e+00
Epoch 2/2
19/19 - 1s - loss: 91.7784 - loglik: -9.0052e+01 - logprior: -1.5384e+00
Fitted a model with MAP estimate = -90.0280
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 92.3951 - loglik: -8.8929e+01 - logprior: -3.2764e+00
Epoch 2/10
19/19 - 1s - loss: 90.1393 - loglik: -8.8428e+01 - logprior: -1.5080e+00
Epoch 3/10
19/19 - 2s - loss: 89.9699 - loglik: -8.8349e+01 - logprior: -1.4233e+00
Epoch 4/10
19/19 - 2s - loss: 89.6310 - loglik: -8.8049e+01 - logprior: -1.3770e+00
Epoch 5/10
19/19 - 2s - loss: 89.6383 - loglik: -8.8089e+01 - logprior: -1.3459e+00
Fitted a model with MAP estimate = -89.4868
Time for alignment: 51.3858
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.9487 - loglik: -1.3168e+02 - logprior: -3.2630e+00
Epoch 2/10
19/19 - 2s - loss: 113.7042 - loglik: -1.1226e+02 - logprior: -1.3967e+00
Epoch 3/10
19/19 - 1s - loss: 106.4589 - loglik: -1.0455e+02 - logprior: -1.5571e+00
Epoch 4/10
19/19 - 1s - loss: 104.0509 - loglik: -1.0227e+02 - logprior: -1.5131e+00
Epoch 5/10
19/19 - 2s - loss: 103.1222 - loglik: -1.0136e+02 - logprior: -1.4994e+00
Epoch 6/10
19/19 - 1s - loss: 102.3859 - loglik: -1.0070e+02 - logprior: -1.4924e+00
Epoch 7/10
19/19 - 1s - loss: 101.7587 - loglik: -1.0009e+02 - logprior: -1.4879e+00
Epoch 8/10
19/19 - 2s - loss: 101.5237 - loglik: -9.9865e+01 - logprior: -1.4887e+00
Epoch 9/10
19/19 - 2s - loss: 101.6724 - loglik: -1.0001e+02 - logprior: -1.4877e+00
Fitted a model with MAP estimate = -99.9249
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (23, 2), (28, 1), (29, 1), (30, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 102.1575 - loglik: -9.7721e+01 - logprior: -4.2639e+00
Epoch 2/2
19/19 - 2s - loss: 94.0709 - loglik: -9.1737e+01 - logprior: -2.1536e+00
Fitted a model with MAP estimate = -91.4314
expansions: [(0, 1)]
discards: [0 9]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 94.2135 - loglik: -9.0764e+01 - logprior: -3.2617e+00
Epoch 2/2
19/19 - 2s - loss: 91.4446 - loglik: -8.9709e+01 - logprior: -1.5410e+00
Fitted a model with MAP estimate = -89.6540
expansions: []
discards: [29]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 92.3259 - loglik: -8.8873e+01 - logprior: -3.2524e+00
Epoch 2/10
19/19 - 2s - loss: 90.1681 - loglik: -8.8452e+01 - logprior: -1.5200e+00
Epoch 3/10
19/19 - 2s - loss: 89.7736 - loglik: -8.8148e+01 - logprior: -1.4264e+00
Epoch 4/10
19/19 - 1s - loss: 89.7400 - loglik: -8.8159e+01 - logprior: -1.3790e+00
Epoch 5/10
19/19 - 2s - loss: 89.6503 - loglik: -8.8105e+01 - logprior: -1.3472e+00
Epoch 6/10
19/19 - 1s - loss: 89.7741 - loglik: -8.8241e+01 - logprior: -1.3286e+00
Fitted a model with MAP estimate = -89.4271
Time for alignment: 51.8863
Computed alignments with likelihoods: ['-89.2262', '-89.4868', '-89.4271']
Best model has likelihood: -89.2262
time for generating output: 0.1302
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/myb_DNA-binding.projection.fasta
SP score = 0.9679358717434869
Training of 3 independent models on file ghf10.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34eed38e20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f347c33f3a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34af223700>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3473d844f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3462e36f40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34ed9a1e50>, <__main__.SimpleDirichletPrior object at 0x7f34b7bb8b50>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34b7c773a0>

Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 652.0304 - loglik: -6.3246e+02 - logprior: -1.9509e+01
Epoch 2/10
15/15 - 8s - loss: 570.2273 - loglik: -5.6853e+02 - logprior: -1.4525e+00
Epoch 3/10
15/15 - 8s - loss: 506.6120 - loglik: -5.0544e+02 - logprior: -9.1784e-01
Epoch 4/10
15/15 - 8s - loss: 481.5209 - loglik: -4.7941e+02 - logprior: -1.4483e+00
Epoch 5/10
15/15 - 7s - loss: 470.7909 - loglik: -4.6824e+02 - logprior: -1.8405e+00
Epoch 6/10
15/15 - 8s - loss: 464.1901 - loglik: -4.6186e+02 - logprior: -1.7629e+00
Epoch 7/10
15/15 - 7s - loss: 466.3131 - loglik: -4.6409e+02 - logprior: -1.7088e+00
Fitted a model with MAP estimate = -464.3786
expansions: [(25, 4), (39, 1), (41, 1), (51, 1), (53, 1), (76, 3), (77, 7), (79, 1), (81, 2), (83, 1), (98, 1), (103, 1), (104, 1), (105, 1), (106, 3), (108, 2), (112, 1), (134, 2), (135, 2), (136, 4), (137, 3), (139, 1), (140, 1), (141, 1), (157, 1), (158, 5), (159, 2), (167, 1), (173, 7), (174, 2), (176, 3)]
discards: [  1   2 223]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 289 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 464.4104 - loglik: -4.4581e+02 - logprior: -1.8124e+01
Epoch 2/2
15/15 - 10s - loss: 432.6822 - loglik: -4.3075e+02 - logprior: -1.4118e+00
Fitted a model with MAP estimate = -427.0566
expansions: [(0, 2), (86, 2), (204, 1)]
discards: [135 167 175 232 233 234 235]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 287 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 453.0306 - loglik: -4.2580e+02 - logprior: -2.6675e+01
Epoch 2/2
15/15 - 10s - loss: 428.1391 - loglik: -4.2354e+02 - logprior: -4.0031e+00
Fitted a model with MAP estimate = -423.0719
expansions: [(0, 2), (236, 2)]
discards: [  0   1  89 167]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 287 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 14s - loss: 444.0661 - loglik: -4.2246e+02 - logprior: -2.1029e+01
Epoch 2/10
15/15 - 10s - loss: 423.5103 - loglik: -4.2249e+02 - logprior: -3.9943e-01
Epoch 3/10
15/15 - 10s - loss: 417.9112 - loglik: -4.2039e+02 - logprior: 3.1037
Epoch 4/10
15/15 - 10s - loss: 419.1778 - loglik: -4.2272e+02 - logprior: 4.1850
Fitted a model with MAP estimate = -417.2520
Time for alignment: 169.1314
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 11s - loss: 651.8185 - loglik: -6.3225e+02 - logprior: -1.9500e+01
Epoch 2/10
15/15 - 8s - loss: 575.2360 - loglik: -5.7352e+02 - logprior: -1.4358e+00
Epoch 3/10
15/15 - 7s - loss: 509.0182 - loglik: -5.0774e+02 - logprior: -1.0491e+00
Epoch 4/10
15/15 - 8s - loss: 479.8993 - loglik: -4.7723e+02 - logprior: -2.0579e+00
Epoch 5/10
15/15 - 8s - loss: 471.7414 - loglik: -4.6868e+02 - logprior: -2.3207e+00
Epoch 6/10
15/15 - 8s - loss: 466.6185 - loglik: -4.6390e+02 - logprior: -2.1298e+00
Epoch 7/10
15/15 - 8s - loss: 465.4801 - loglik: -4.6292e+02 - logprior: -2.0270e+00
Epoch 8/10
15/15 - 8s - loss: 465.2641 - loglik: -4.6271e+02 - logprior: -1.9837e+00
Epoch 9/10
15/15 - 8s - loss: 464.1504 - loglik: -4.6160e+02 - logprior: -1.9554e+00
Epoch 10/10
15/15 - 8s - loss: 465.8642 - loglik: -4.6336e+02 - logprior: -1.8939e+00
Fitted a model with MAP estimate = -463.1794
expansions: [(27, 1), (31, 1), (36, 1), (39, 3), (41, 1), (43, 1), (46, 1), (49, 1), (51, 1), (52, 1), (53, 1), (54, 2), (67, 1), (68, 5), (69, 2), (70, 1), (71, 2), (72, 1), (73, 1), (76, 1), (94, 1), (101, 1), (102, 5), (104, 2), (131, 2), (132, 2), (133, 1), (134, 2), (135, 3), (137, 1), (157, 1), (158, 6), (174, 9), (175, 3), (197, 1)]
discards: [  1   2 200 223]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 290 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 465.9051 - loglik: -4.4711e+02 - logprior: -1.8268e+01
Epoch 2/2
15/15 - 10s - loss: 432.6373 - loglik: -4.3056e+02 - logprior: -1.5446e+00
Fitted a model with MAP estimate = -426.2399
expansions: [(0, 2), (62, 1), (213, 2)]
discards: [ 88 138 170 178 234 235]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 289 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 450.9917 - loglik: -4.2358e+02 - logprior: -2.6845e+01
Epoch 2/2
15/15 - 10s - loss: 426.0707 - loglik: -4.2136e+02 - logprior: -4.1054e+00
Fitted a model with MAP estimate = -421.5715
expansions: [(0, 2), (237, 1)]
discards: [ 0  1 89]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 289 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 14s - loss: 442.2052 - loglik: -4.2048e+02 - logprior: -2.1121e+01
Epoch 2/10
15/15 - 10s - loss: 421.4228 - loglik: -4.2050e+02 - logprior: -2.8101e-01
Epoch 3/10
15/15 - 10s - loss: 417.6473 - loglik: -4.2033e+02 - logprior: 3.3348
Epoch 4/10
15/15 - 10s - loss: 418.8987 - loglik: -4.2266e+02 - logprior: 4.4190
Fitted a model with MAP estimate = -416.2439
Time for alignment: 192.1545
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 11s - loss: 652.4362 - loglik: -6.3286e+02 - logprior: -1.9508e+01
Epoch 2/10
15/15 - 8s - loss: 572.6835 - loglik: -5.7107e+02 - logprior: -1.3457e+00
Epoch 3/10
15/15 - 7s - loss: 506.3072 - loglik: -5.0514e+02 - logprior: -9.0590e-01
Epoch 4/10
15/15 - 8s - loss: 476.0989 - loglik: -4.7342e+02 - logprior: -2.0081e+00
Epoch 5/10
15/15 - 7s - loss: 465.2136 - loglik: -4.6245e+02 - logprior: -2.0235e+00
Epoch 6/10
15/15 - 8s - loss: 465.3910 - loglik: -4.6274e+02 - logprior: -2.0563e+00
Fitted a model with MAP estimate = -463.0328
expansions: [(25, 2), (26, 2), (38, 1), (39, 1), (41, 1), (51, 1), (53, 1), (55, 2), (57, 2), (61, 1), (67, 1), (68, 1), (69, 4), (70, 2), (71, 1), (72, 2), (73, 1), (74, 1), (76, 2), (78, 1), (98, 1), (100, 1), (101, 1), (102, 3), (130, 1), (131, 2), (132, 2), (133, 4), (134, 2), (137, 1), (154, 7), (173, 7), (176, 4)]
discards: [  1   2 223]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 288 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 463.1630 - loglik: -4.4452e+02 - logprior: -1.8174e+01
Epoch 2/2
15/15 - 10s - loss: 432.0257 - loglik: -4.3012e+02 - logprior: -1.4081e+00
Fitted a model with MAP estimate = -425.6966
expansions: [(26, 1), (206, 2), (207, 1), (237, 1)]
discards: [ 23  88 169 177 233]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 288 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 447.3091 - loglik: -4.2967e+02 - logprior: -1.7103e+01
Epoch 2/2
15/15 - 10s - loss: 418.0207 - loglik: -4.1697e+02 - logprior: -4.9331e-01
Fitted a model with MAP estimate = -421.0111
expansions: [(0, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 290 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 14s - loss: 446.9546 - loglik: -4.2080e+02 - logprior: -2.5602e+01
Epoch 2/10
15/15 - 10s - loss: 422.0826 - loglik: -4.1883e+02 - logprior: -2.6649e+00
Epoch 3/10
15/15 - 10s - loss: 421.9002 - loglik: -4.2376e+02 - logprior: 2.4513
Epoch 4/10
15/15 - 10s - loss: 417.8794 - loglik: -4.2135e+02 - logprior: 4.0788
Epoch 5/10
15/15 - 10s - loss: 417.1450 - loglik: -4.2128e+02 - logprior: 4.7534
Epoch 6/10
15/15 - 10s - loss: 415.5928 - loglik: -4.2021e+02 - logprior: 5.2415
Epoch 7/10
15/15 - 10s - loss: 418.5888 - loglik: -4.2357e+02 - logprior: 5.6187
Fitted a model with MAP estimate = -415.1726
Time for alignment: 192.9093
Computed alignments with likelihoods: ['-417.2520', '-416.2439', '-415.1726']
Best model has likelihood: -415.1726
time for generating output: 0.3860
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf10.projection.fasta
SP score = 0.9157398045163465
Training of 3 independent models on file il8.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34af6fa0d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3473d7c820>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3473bb2400>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34a6de5910>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34a6de5d60>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f342d226520>, <__main__.SimpleDirichletPrior object at 0x7f34ecc89b50>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34ee81e4c0>

Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 226.9867 - loglik: -1.8498e+02 - logprior: -4.1987e+01
Epoch 2/10
10/10 - 1s - loss: 180.5560 - loglik: -1.6903e+02 - logprior: -1.1503e+01
Epoch 3/10
10/10 - 1s - loss: 160.8337 - loglik: -1.5502e+02 - logprior: -5.8020e+00
Epoch 4/10
10/10 - 1s - loss: 151.7176 - loglik: -1.4784e+02 - logprior: -3.8261e+00
Epoch 5/10
10/10 - 1s - loss: 147.7774 - loglik: -1.4472e+02 - logprior: -2.8561e+00
Epoch 6/10
10/10 - 1s - loss: 145.5611 - loglik: -1.4290e+02 - logprior: -2.3852e+00
Epoch 7/10
10/10 - 1s - loss: 144.5896 - loglik: -1.4212e+02 - logprior: -2.2195e+00
Epoch 8/10
10/10 - 1s - loss: 143.7838 - loglik: -1.4152e+02 - logprior: -2.0208e+00
Epoch 9/10
10/10 - 1s - loss: 143.2967 - loglik: -1.4125e+02 - logprior: -1.7997e+00
Epoch 10/10
10/10 - 1s - loss: 143.0918 - loglik: -1.4117e+02 - logprior: -1.6625e+00
Fitted a model with MAP estimate = -142.7453
expansions: [(0, 2), (1, 1), (17, 1), (18, 3), (20, 1), (21, 1), (24, 1), (37, 1), (43, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 191.5300 - loglik: -1.3581e+02 - logprior: -5.5447e+01
Epoch 2/2
10/10 - 1s - loss: 149.7180 - loglik: -1.3215e+02 - logprior: -1.7278e+01
Fitted a model with MAP estimate = -141.9884
expansions: [(19, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 179.5911 - loglik: -1.3147e+02 - logprior: -4.7824e+01
Epoch 2/2
10/10 - 1s - loss: 150.5443 - loglik: -1.3121e+02 - logprior: -1.9052e+01
Fitted a model with MAP estimate = -145.5638
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 173.8169 - loglik: -1.3024e+02 - logprior: -4.3286e+01
Epoch 2/10
10/10 - 1s - loss: 142.7504 - loglik: -1.3021e+02 - logprior: -1.2235e+01
Epoch 3/10
10/10 - 1s - loss: 136.0947 - loglik: -1.3068e+02 - logprior: -5.1040e+00
Epoch 4/10
10/10 - 1s - loss: 133.9372 - loglik: -1.3107e+02 - logprior: -2.5570e+00
Epoch 5/10
10/10 - 1s - loss: 132.4143 - loglik: -1.3078e+02 - logprior: -1.3187e+00
Epoch 6/10
10/10 - 1s - loss: 131.5995 - loglik: -1.3054e+02 - logprior: -7.3539e-01
Epoch 7/10
10/10 - 1s - loss: 131.0583 - loglik: -1.3025e+02 - logprior: -4.6858e-01
Epoch 8/10
10/10 - 1s - loss: 130.8698 - loglik: -1.3033e+02 - logprior: -1.9972e-01
Epoch 9/10
10/10 - 1s - loss: 130.6729 - loglik: -1.3044e+02 - logprior: 0.1082
Epoch 10/10
10/10 - 1s - loss: 130.4581 - loglik: -1.3046e+02 - logprior: 0.3399
Fitted a model with MAP estimate = -130.0804
Time for alignment: 35.3811
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 227.1034 - loglik: -1.8510e+02 - logprior: -4.1988e+01
Epoch 2/10
10/10 - 1s - loss: 180.6374 - loglik: -1.6911e+02 - logprior: -1.1498e+01
Epoch 3/10
10/10 - 1s - loss: 161.6174 - loglik: -1.5582e+02 - logprior: -5.7921e+00
Epoch 4/10
10/10 - 1s - loss: 151.9775 - loglik: -1.4812e+02 - logprior: -3.8217e+00
Epoch 5/10
10/10 - 1s - loss: 147.6548 - loglik: -1.4463e+02 - logprior: -2.8950e+00
Epoch 6/10
10/10 - 1s - loss: 145.4340 - loglik: -1.4269e+02 - logprior: -2.4362e+00
Epoch 7/10
10/10 - 1s - loss: 144.4391 - loglik: -1.4193e+02 - logprior: -2.1954e+00
Epoch 8/10
10/10 - 1s - loss: 143.9022 - loglik: -1.4164e+02 - logprior: -1.9921e+00
Epoch 9/10
10/10 - 1s - loss: 143.4219 - loglik: -1.4137e+02 - logprior: -1.7969e+00
Epoch 10/10
10/10 - 1s - loss: 143.0757 - loglik: -1.4116e+02 - logprior: -1.6428e+00
Fitted a model with MAP estimate = -142.7181
expansions: [(0, 2), (17, 1), (18, 3), (20, 1), (21, 1), (28, 1), (38, 1), (43, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 191.7876 - loglik: -1.3595e+02 - logprior: -5.5568e+01
Epoch 2/2
10/10 - 1s - loss: 150.0478 - loglik: -1.3239e+02 - logprior: -1.7373e+01
Fitted a model with MAP estimate = -142.2309
expansions: [(3, 1), (18, 1)]
discards: [ 0 22]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 180.0445 - loglik: -1.3177e+02 - logprior: -4.8004e+01
Epoch 2/2
10/10 - 1s - loss: 150.8479 - loglik: -1.3161e+02 - logprior: -1.8963e+01
Fitted a model with MAP estimate = -145.7682
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 173.8420 - loglik: -1.3047e+02 - logprior: -4.3091e+01
Epoch 2/10
10/10 - 1s - loss: 142.6979 - loglik: -1.3031e+02 - logprior: -1.2091e+01
Epoch 3/10
10/10 - 1s - loss: 136.3343 - loglik: -1.3100e+02 - logprior: -5.0291e+00
Epoch 4/10
10/10 - 1s - loss: 133.8996 - loglik: -1.3110e+02 - logprior: -2.4865e+00
Epoch 5/10
10/10 - 1s - loss: 132.5715 - loglik: -1.3101e+02 - logprior: -1.2355e+00
Epoch 6/10
10/10 - 1s - loss: 131.9035 - loglik: -1.3093e+02 - logprior: -6.5293e-01
Epoch 7/10
10/10 - 1s - loss: 131.3862 - loglik: -1.3067e+02 - logprior: -3.9142e-01
Epoch 8/10
10/10 - 1s - loss: 131.2426 - loglik: -1.3079e+02 - logprior: -1.2587e-01
Epoch 9/10
10/10 - 1s - loss: 130.9602 - loglik: -1.3083e+02 - logprior: 0.1979
Epoch 10/10
10/10 - 1s - loss: 130.8470 - loglik: -1.3095e+02 - logprior: 0.4303
Fitted a model with MAP estimate = -130.4466
Time for alignment: 35.6901
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 227.1321 - loglik: -1.8513e+02 - logprior: -4.1986e+01
Epoch 2/10
10/10 - 1s - loss: 180.7740 - loglik: -1.6925e+02 - logprior: -1.1500e+01
Epoch 3/10
10/10 - 1s - loss: 162.1964 - loglik: -1.5643e+02 - logprior: -5.7617e+00
Epoch 4/10
10/10 - 1s - loss: 152.8030 - loglik: -1.4893e+02 - logprior: -3.8076e+00
Epoch 5/10
10/10 - 1s - loss: 148.5302 - loglik: -1.4537e+02 - logprior: -2.9262e+00
Epoch 6/10
10/10 - 1s - loss: 146.2353 - loglik: -1.4343e+02 - logprior: -2.4659e+00
Epoch 7/10
10/10 - 1s - loss: 145.4842 - loglik: -1.4298e+02 - logprior: -2.2217e+00
Epoch 8/10
10/10 - 1s - loss: 144.9483 - loglik: -1.4268e+02 - logprior: -2.0067e+00
Epoch 9/10
10/10 - 1s - loss: 144.4808 - loglik: -1.4242e+02 - logprior: -1.7981e+00
Epoch 10/10
10/10 - 1s - loss: 144.2661 - loglik: -1.4232e+02 - logprior: -1.6602e+00
Fitted a model with MAP estimate = -143.8467
expansions: [(0, 2), (1, 1), (16, 1), (17, 1), (19, 1), (20, 1), (21, 2), (27, 1), (35, 1), (43, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 193.2153 - loglik: -1.3753e+02 - logprior: -5.5410e+01
Epoch 2/2
10/10 - 1s - loss: 151.1187 - loglik: -1.3351e+02 - logprior: -1.7318e+01
Fitted a model with MAP estimate = -143.2904
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 180.5804 - loglik: -1.3237e+02 - logprior: -4.7895e+01
Epoch 2/2
10/10 - 1s - loss: 152.1138 - loglik: -1.3274e+02 - logprior: -1.9050e+01
Fitted a model with MAP estimate = -147.0391
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 175.4534 - loglik: -1.3183e+02 - logprior: -4.3302e+01
Epoch 2/10
10/10 - 1s - loss: 144.1012 - loglik: -1.3152e+02 - logprior: -1.2254e+01
Epoch 3/10
10/10 - 1s - loss: 137.4115 - loglik: -1.3195e+02 - logprior: -5.1231e+00
Epoch 4/10
10/10 - 1s - loss: 135.1728 - loglik: -1.3227e+02 - logprior: -2.5692e+00
Epoch 5/10
10/10 - 1s - loss: 133.9276 - loglik: -1.3224e+02 - logprior: -1.3397e+00
Epoch 6/10
10/10 - 1s - loss: 133.1429 - loglik: -1.3203e+02 - logprior: -7.6730e-01
Epoch 7/10
10/10 - 1s - loss: 132.9167 - loglik: -1.3208e+02 - logprior: -4.8727e-01
Epoch 8/10
10/10 - 1s - loss: 132.7131 - loglik: -1.3217e+02 - logprior: -1.9479e-01
Epoch 9/10
10/10 - 1s - loss: 132.4945 - loglik: -1.3226e+02 - logprior: 0.1140
Epoch 10/10
10/10 - 1s - loss: 132.3295 - loglik: -1.3232e+02 - logprior: 0.3356
Fitted a model with MAP estimate = -131.9107
Time for alignment: 34.9864
Computed alignments with likelihoods: ['-130.0804', '-130.4466', '-131.9107']
Best model has likelihood: -130.0804
time for generating output: 0.1247
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/il8.projection.fasta
SP score = 0.905340491664312
Training of 3 independent models on file cytb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3436526a90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34b782f2b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34ecdc7ac0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34365fb5b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3440f72700>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f342c7efeb0>, <__main__.SimpleDirichletPrior object at 0x7f34af6683d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34eedfce50>

Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 250.7842 - loglik: -2.3744e+02 - logprior: -1.3318e+01
Epoch 2/10
11/11 - 2s - loss: 220.8953 - loglik: -2.1728e+02 - logprior: -3.5490e+00
Epoch 3/10
11/11 - 2s - loss: 201.4204 - loglik: -1.9896e+02 - logprior: -2.2827e+00
Epoch 4/10
11/11 - 2s - loss: 189.9752 - loglik: -1.8737e+02 - logprior: -2.2656e+00
Epoch 5/10
11/11 - 2s - loss: 186.6635 - loglik: -1.8390e+02 - logprior: -2.2814e+00
Epoch 6/10
11/11 - 2s - loss: 185.3496 - loglik: -1.8276e+02 - logprior: -2.1522e+00
Epoch 7/10
11/11 - 2s - loss: 184.0604 - loglik: -1.8164e+02 - logprior: -2.0186e+00
Epoch 8/10
11/11 - 2s - loss: 184.4987 - loglik: -1.8213e+02 - logprior: -1.9862e+00
Fitted a model with MAP estimate = -183.2101
expansions: [(9, 1), (10, 2), (11, 2), (15, 1), (24, 1), (25, 1), (32, 1), (35, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 194.8366 - loglik: -1.7918e+02 - logprior: -1.5283e+01
Epoch 2/2
11/11 - 2s - loss: 181.3853 - loglik: -1.7440e+02 - logprior: -6.5769e+00
Fitted a model with MAP estimate = -177.5855
expansions: [(0, 2)]
discards: [ 0 11]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 183.1992 - loglik: -1.7065e+02 - logprior: -1.2098e+01
Epoch 2/2
11/11 - 2s - loss: 172.9801 - loglik: -1.6909e+02 - logprior: -3.4191e+00
Fitted a model with MAP estimate = -171.8313
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 185.3342 - loglik: -1.7060e+02 - logprior: -1.4271e+01
Epoch 2/10
11/11 - 2s - loss: 175.2008 - loglik: -1.7044e+02 - logprior: -4.2826e+00
Epoch 3/10
11/11 - 2s - loss: 172.2986 - loglik: -1.6933e+02 - logprior: -2.4794e+00
Epoch 4/10
11/11 - 2s - loss: 172.0122 - loglik: -1.6975e+02 - logprior: -1.7785e+00
Epoch 5/10
11/11 - 2s - loss: 171.9216 - loglik: -1.7006e+02 - logprior: -1.3830e+00
Epoch 6/10
11/11 - 2s - loss: 170.7431 - loglik: -1.6896e+02 - logprior: -1.2891e+00
Epoch 7/10
11/11 - 2s - loss: 171.5015 - loglik: -1.6984e+02 - logprior: -1.1626e+00
Fitted a model with MAP estimate = -170.6294
Time for alignment: 53.9364
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 251.1747 - loglik: -2.3783e+02 - logprior: -1.3319e+01
Epoch 2/10
11/11 - 2s - loss: 220.7171 - loglik: -2.1710e+02 - logprior: -3.5473e+00
Epoch 3/10
11/11 - 2s - loss: 199.6736 - loglik: -1.9715e+02 - logprior: -2.2882e+00
Epoch 4/10
11/11 - 2s - loss: 189.1375 - loglik: -1.8630e+02 - logprior: -2.2591e+00
Epoch 5/10
11/11 - 2s - loss: 186.3457 - loglik: -1.8351e+02 - logprior: -2.2702e+00
Epoch 6/10
11/11 - 2s - loss: 184.2952 - loglik: -1.8176e+02 - logprior: -2.1027e+00
Epoch 7/10
11/11 - 2s - loss: 184.1210 - loglik: -1.8176e+02 - logprior: -1.9629e+00
Epoch 8/10
11/11 - 2s - loss: 183.3701 - loglik: -1.8103e+02 - logprior: -1.9348e+00
Epoch 9/10
11/11 - 2s - loss: 183.1327 - loglik: -1.8075e+02 - logprior: -1.9602e+00
Epoch 10/10
11/11 - 2s - loss: 182.9236 - loglik: -1.8052e+02 - logprior: -1.9756e+00
Fitted a model with MAP estimate = -182.3081
expansions: [(9, 1), (10, 2), (11, 2), (12, 2), (24, 1), (25, 1), (32, 1), (35, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 195.1361 - loglik: -1.7944e+02 - logprior: -1.5307e+01
Epoch 2/2
11/11 - 2s - loss: 181.1474 - loglik: -1.7411e+02 - logprior: -6.6595e+00
Fitted a model with MAP estimate = -177.6734
expansions: [(0, 2)]
discards: [ 0 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 182.2739 - loglik: -1.6966e+02 - logprior: -1.2165e+01
Epoch 2/2
11/11 - 2s - loss: 172.7505 - loglik: -1.6879e+02 - logprior: -3.4805e+00
Fitted a model with MAP estimate = -171.4172
expansions: []
discards: [ 0 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 185.4114 - loglik: -1.7059e+02 - logprior: -1.4321e+01
Epoch 2/10
11/11 - 2s - loss: 174.2885 - loglik: -1.6947e+02 - logprior: -4.3190e+00
Epoch 3/10
11/11 - 2s - loss: 172.3375 - loglik: -1.6933e+02 - logprior: -2.4908e+00
Epoch 4/10
11/11 - 2s - loss: 171.8280 - loglik: -1.6952e+02 - logprior: -1.7951e+00
Epoch 5/10
11/11 - 2s - loss: 171.0294 - loglik: -1.6913e+02 - logprior: -1.3910e+00
Epoch 6/10
11/11 - 2s - loss: 170.5706 - loglik: -1.6876e+02 - logprior: -1.2950e+00
Epoch 7/10
11/11 - 2s - loss: 171.6832 - loglik: -1.7001e+02 - logprior: -1.1623e+00
Fitted a model with MAP estimate = -170.3902
Time for alignment: 57.7810
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 251.0370 - loglik: -2.3770e+02 - logprior: -1.3319e+01
Epoch 2/10
11/11 - 2s - loss: 220.6460 - loglik: -2.1702e+02 - logprior: -3.5557e+00
Epoch 3/10
11/11 - 2s - loss: 200.3823 - loglik: -1.9784e+02 - logprior: -2.3055e+00
Epoch 4/10
11/11 - 2s - loss: 188.5184 - loglik: -1.8566e+02 - logprior: -2.2792e+00
Epoch 5/10
11/11 - 2s - loss: 185.9968 - loglik: -1.8316e+02 - logprior: -2.2772e+00
Epoch 6/10
11/11 - 2s - loss: 184.8446 - loglik: -1.8236e+02 - logprior: -2.0878e+00
Epoch 7/10
11/11 - 2s - loss: 183.2888 - loglik: -1.8100e+02 - logprior: -1.9410e+00
Epoch 8/10
11/11 - 2s - loss: 183.3811 - loglik: -1.8113e+02 - logprior: -1.9189e+00
Fitted a model with MAP estimate = -182.7910
expansions: [(8, 1), (9, 1), (10, 3), (11, 1), (12, 2), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 196.4216 - loglik: -1.8083e+02 - logprior: -1.5270e+01
Epoch 2/2
11/11 - 2s - loss: 180.5097 - loglik: -1.7345e+02 - logprior: -6.6926e+00
Fitted a model with MAP estimate = -177.7906
expansions: [(0, 2)]
discards: [ 0  9 15 17 45]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 183.2860 - loglik: -1.7078e+02 - logprior: -1.2091e+01
Epoch 2/2
11/11 - 2s - loss: 173.4982 - loglik: -1.6963e+02 - logprior: -3.4104e+00
Fitted a model with MAP estimate = -172.1543
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 185.5648 - loglik: -1.7081e+02 - logprior: -1.4284e+01
Epoch 2/10
11/11 - 2s - loss: 175.5823 - loglik: -1.7082e+02 - logprior: -4.2731e+00
Epoch 3/10
11/11 - 2s - loss: 171.6135 - loglik: -1.6864e+02 - logprior: -2.4713e+00
Epoch 4/10
11/11 - 2s - loss: 172.4014 - loglik: -1.7012e+02 - logprior: -1.7647e+00
Fitted a model with MAP estimate = -171.0913
Time for alignment: 48.1533
Computed alignments with likelihoods: ['-170.6294', '-170.3902', '-171.0913']
Best model has likelihood: -170.3902
time for generating output: 0.1684
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cytb.projection.fasta
SP score = 0.8143213988343048
Training of 3 independent models on file kringle.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f347c3efbe0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34ec987d90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34ec9870d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34eeb00070>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34af3e0be0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34a6bf7790>, <__main__.SimpleDirichletPrior object at 0x7f34a6fe05b0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34ede7d430>

Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 264.7769 - loglik: -2.2373e+02 - logprior: -4.1025e+01
Epoch 2/10
10/10 - 1s - loss: 209.0988 - loglik: -1.9795e+02 - logprior: -1.1139e+01
Epoch 3/10
10/10 - 1s - loss: 179.6026 - loglik: -1.7370e+02 - logprior: -5.8829e+00
Epoch 4/10
10/10 - 1s - loss: 157.0741 - loglik: -1.5271e+02 - logprior: -4.3413e+00
Epoch 5/10
10/10 - 1s - loss: 147.3891 - loglik: -1.4344e+02 - logprior: -3.8521e+00
Epoch 6/10
10/10 - 1s - loss: 143.8424 - loglik: -1.3998e+02 - logprior: -3.5300e+00
Epoch 7/10
10/10 - 1s - loss: 142.1843 - loglik: -1.3859e+02 - logprior: -3.1724e+00
Epoch 8/10
10/10 - 1s - loss: 141.5230 - loglik: -1.3822e+02 - logprior: -2.9345e+00
Epoch 9/10
10/10 - 1s - loss: 141.0726 - loglik: -1.3795e+02 - logprior: -2.7813e+00
Epoch 10/10
10/10 - 1s - loss: 140.8862 - loglik: -1.3782e+02 - logprior: -2.7075e+00
Fitted a model with MAP estimate = -140.4010
expansions: [(2, 1), (5, 1), (10, 1), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 1), (52, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 170.4754 - loglik: -1.3255e+02 - logprior: -3.7565e+01
Epoch 2/2
10/10 - 1s - loss: 137.1091 - loglik: -1.2628e+02 - logprior: -1.0443e+01
Fitted a model with MAP estimate = -131.8466
expansions: []
discards: [45 58 69]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 161.5379 - loglik: -1.2453e+02 - logprior: -3.6595e+01
Epoch 2/2
10/10 - 1s - loss: 134.1999 - loglik: -1.2377e+02 - logprior: -1.0003e+01
Fitted a model with MAP estimate = -130.3098
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 173.9166 - loglik: -1.2779e+02 - logprior: -4.5685e+01
Epoch 2/10
10/10 - 1s - loss: 146.9440 - loglik: -1.2747e+02 - logprior: -1.9020e+01
Epoch 3/10
10/10 - 1s - loss: 140.9937 - loglik: -1.2703e+02 - logprior: -1.3503e+01
Epoch 4/10
10/10 - 1s - loss: 138.2657 - loglik: -1.2667e+02 - logprior: -1.1123e+01
Epoch 5/10
10/10 - 1s - loss: 135.8371 - loglik: -1.2727e+02 - logprior: -8.0856e+00
Epoch 6/10
10/10 - 1s - loss: 129.7919 - loglik: -1.2715e+02 - logprior: -2.1690e+00
Epoch 7/10
10/10 - 1s - loss: 128.1711 - loglik: -1.2755e+02 - logprior: -1.3497e-01
Epoch 8/10
10/10 - 1s - loss: 127.8208 - loglik: -1.2756e+02 - logprior: 0.2272
Epoch 9/10
10/10 - 1s - loss: 127.7278 - loglik: -1.2766e+02 - logprior: 0.4162
Epoch 10/10
10/10 - 1s - loss: 127.5148 - loglik: -1.2761e+02 - logprior: 0.5819
Fitted a model with MAP estimate = -126.9338
Time for alignment: 39.8395
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 264.7846 - loglik: -2.2374e+02 - logprior: -4.1024e+01
Epoch 2/10
10/10 - 1s - loss: 209.0322 - loglik: -1.9789e+02 - logprior: -1.1130e+01
Epoch 3/10
10/10 - 1s - loss: 178.3985 - loglik: -1.7263e+02 - logprior: -5.7494e+00
Epoch 4/10
10/10 - 1s - loss: 156.6188 - loglik: -1.5256e+02 - logprior: -4.0446e+00
Epoch 5/10
10/10 - 1s - loss: 147.6768 - loglik: -1.4412e+02 - logprior: -3.4644e+00
Epoch 6/10
10/10 - 1s - loss: 144.8258 - loglik: -1.4139e+02 - logprior: -3.1166e+00
Epoch 7/10
10/10 - 1s - loss: 143.7558 - loglik: -1.4052e+02 - logprior: -2.7995e+00
Epoch 8/10
10/10 - 1s - loss: 143.0782 - loglik: -1.4005e+02 - logprior: -2.6268e+00
Epoch 9/10
10/10 - 1s - loss: 142.4162 - loglik: -1.3954e+02 - logprior: -2.5146e+00
Epoch 10/10
10/10 - 1s - loss: 142.3414 - loglik: -1.3953e+02 - logprior: -2.4451e+00
Fitted a model with MAP estimate = -141.6536
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 1), (52, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 169.2468 - loglik: -1.3141e+02 - logprior: -3.7489e+01
Epoch 2/2
10/10 - 1s - loss: 135.9291 - loglik: -1.2530e+02 - logprior: -1.0256e+01
Fitted a model with MAP estimate = -130.9023
expansions: []
discards: [ 0 46 59 70]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 175.2001 - loglik: -1.2888e+02 - logprior: -4.5913e+01
Epoch 2/2
10/10 - 1s - loss: 147.7321 - loglik: -1.2813e+02 - logprior: -1.9177e+01
Fitted a model with MAP estimate = -143.4395
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 162.6087 - loglik: -1.2513e+02 - logprior: -3.7045e+01
Epoch 2/10
10/10 - 1s - loss: 134.9562 - loglik: -1.2480e+02 - logprior: -9.7255e+00
Epoch 3/10
10/10 - 1s - loss: 129.0020 - loglik: -1.2439e+02 - logprior: -4.1689e+00
Epoch 4/10
10/10 - 1s - loss: 127.2750 - loglik: -1.2486e+02 - logprior: -1.9665e+00
Epoch 5/10
10/10 - 1s - loss: 126.1251 - loglik: -1.2486e+02 - logprior: -8.1806e-01
Epoch 6/10
10/10 - 1s - loss: 125.6300 - loglik: -1.2497e+02 - logprior: -2.0678e-01
Epoch 7/10
10/10 - 1s - loss: 125.1323 - loglik: -1.2482e+02 - logprior: 0.1478
Epoch 8/10
10/10 - 1s - loss: 125.1952 - loglik: -1.2513e+02 - logprior: 0.3919
Fitted a model with MAP estimate = -124.5598
Time for alignment: 36.6476
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 264.7735 - loglik: -2.2373e+02 - logprior: -4.1024e+01
Epoch 2/10
10/10 - 1s - loss: 209.2312 - loglik: -1.9809e+02 - logprior: -1.1133e+01
Epoch 3/10
10/10 - 1s - loss: 178.2162 - loglik: -1.7238e+02 - logprior: -5.8228e+00
Epoch 4/10
10/10 - 1s - loss: 156.1844 - loglik: -1.5196e+02 - logprior: -4.2042e+00
Epoch 5/10
10/10 - 1s - loss: 147.1284 - loglik: -1.4337e+02 - logprior: -3.6159e+00
Epoch 6/10
10/10 - 1s - loss: 144.7015 - loglik: -1.4112e+02 - logprior: -3.1991e+00
Epoch 7/10
10/10 - 1s - loss: 143.4182 - loglik: -1.4009e+02 - logprior: -2.8862e+00
Epoch 8/10
10/10 - 1s - loss: 142.5603 - loglik: -1.3947e+02 - logprior: -2.6944e+00
Epoch 9/10
10/10 - 1s - loss: 142.1549 - loglik: -1.3923e+02 - logprior: -2.5706e+00
Epoch 10/10
10/10 - 1s - loss: 141.9059 - loglik: -1.3907e+02 - logprior: -2.4733e+00
Fitted a model with MAP estimate = -141.4717
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 1), (52, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 169.1418 - loglik: -1.3131e+02 - logprior: -3.7482e+01
Epoch 2/2
10/10 - 1s - loss: 136.0891 - loglik: -1.2547e+02 - logprior: -1.0248e+01
Fitted a model with MAP estimate = -130.9197
expansions: []
discards: [ 0 46 59 70]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 174.8974 - loglik: -1.2864e+02 - logprior: -4.5855e+01
Epoch 2/2
10/10 - 1s - loss: 147.7835 - loglik: -1.2819e+02 - logprior: -1.9164e+01
Fitted a model with MAP estimate = -143.3730
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 162.5725 - loglik: -1.2524e+02 - logprior: -3.6895e+01
Epoch 2/10
10/10 - 1s - loss: 134.8902 - loglik: -1.2475e+02 - logprior: -9.6978e+00
Epoch 3/10
10/10 - 1s - loss: 128.8672 - loglik: -1.2424e+02 - logprior: -4.1634e+00
Epoch 4/10
10/10 - 1s - loss: 127.1653 - loglik: -1.2473e+02 - logprior: -1.9651e+00
Epoch 5/10
10/10 - 1s - loss: 126.0137 - loglik: -1.2473e+02 - logprior: -8.2215e-01
Epoch 6/10
10/10 - 1s - loss: 125.6673 - loglik: -1.2499e+02 - logprior: -2.0803e-01
Epoch 7/10
10/10 - 1s - loss: 125.4099 - loglik: -1.2508e+02 - logprior: 0.1389
Epoch 8/10
10/10 - 1s - loss: 125.2295 - loglik: -1.2515e+02 - logprior: 0.3899
Epoch 9/10
10/10 - 1s - loss: 124.5342 - loglik: -1.2465e+02 - logprior: 0.5976
Epoch 10/10
10/10 - 1s - loss: 125.1222 - loglik: -1.2542e+02 - logprior: 0.7782
Fitted a model with MAP estimate = -124.2602
Time for alignment: 39.1803
Computed alignments with likelihoods: ['-126.9338', '-124.5598', '-124.2602']
Best model has likelihood: -124.2602
time for generating output: 0.1436
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kringle.projection.fasta
SP score = 0.922202486678508
Training of 3 independent models on file LIM.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34738a95e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34ed7a81c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34af31a940>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34af31adc0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34af31feb0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34ee1b4fa0>, <__main__.SimpleDirichletPrior object at 0x7f347c4b4c40>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34ed599a60>

Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.5038 - loglik: -1.6680e+02 - logprior: -5.6661e+00
Epoch 2/10
15/15 - 1s - loss: 145.6234 - loglik: -1.4378e+02 - logprior: -1.8130e+00
Epoch 3/10
15/15 - 1s - loss: 130.3625 - loglik: -1.2838e+02 - logprior: -1.8838e+00
Epoch 4/10
15/15 - 1s - loss: 126.4745 - loglik: -1.2415e+02 - logprior: -1.8764e+00
Epoch 5/10
15/15 - 1s - loss: 125.1101 - loglik: -1.2282e+02 - logprior: -1.8067e+00
Epoch 6/10
15/15 - 1s - loss: 124.6558 - loglik: -1.2237e+02 - logprior: -1.8244e+00
Epoch 7/10
15/15 - 1s - loss: 124.2604 - loglik: -1.2205e+02 - logprior: -1.7996e+00
Epoch 8/10
15/15 - 1s - loss: 123.9557 - loglik: -1.2176e+02 - logprior: -1.7771e+00
Epoch 9/10
15/15 - 1s - loss: 123.9977 - loglik: -1.2179e+02 - logprior: -1.7656e+00
Fitted a model with MAP estimate = -123.3315
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (24, 2), (27, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 5s - loss: 133.9656 - loglik: -1.2645e+02 - logprior: -7.0550e+00
Epoch 2/2
15/15 - 1s - loss: 124.7218 - loglik: -1.2065e+02 - logprior: -3.5905e+00
Fitted a model with MAP estimate = -122.5230
expansions: [(0, 2)]
discards: [ 0 12 15 31 39 46]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 125.1276 - loglik: -1.1935e+02 - logprior: -5.2633e+00
Epoch 2/2
15/15 - 1s - loss: 119.5603 - loglik: -1.1726e+02 - logprior: -1.8205e+00
Fitted a model with MAP estimate = -117.8125
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 5s - loss: 126.3274 - loglik: -1.1920e+02 - logprior: -6.6004e+00
Epoch 2/10
15/15 - 1s - loss: 119.8231 - loglik: -1.1693e+02 - logprior: -2.3281e+00
Epoch 3/10
15/15 - 1s - loss: 118.0945 - loglik: -1.1584e+02 - logprior: -1.6668e+00
Epoch 4/10
15/15 - 1s - loss: 118.0136 - loglik: -1.1592e+02 - logprior: -1.5108e+00
Epoch 5/10
15/15 - 1s - loss: 117.8991 - loglik: -1.1585e+02 - logprior: -1.4588e+00
Epoch 6/10
15/15 - 1s - loss: 117.6281 - loglik: -1.1560e+02 - logprior: -1.4278e+00
Epoch 7/10
15/15 - 1s - loss: 117.5707 - loglik: -1.1556e+02 - logprior: -1.4066e+00
Epoch 8/10
15/15 - 1s - loss: 117.4541 - loglik: -1.1546e+02 - logprior: -1.3889e+00
Epoch 9/10
15/15 - 1s - loss: 117.4500 - loglik: -1.1548e+02 - logprior: -1.3653e+00
Epoch 10/10
15/15 - 1s - loss: 117.3635 - loglik: -1.1540e+02 - logprior: -1.3524e+00
Fitted a model with MAP estimate = -116.6448
Time for alignment: 46.0893
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.5398 - loglik: -1.6684e+02 - logprior: -5.6653e+00
Epoch 2/10
15/15 - 1s - loss: 145.7381 - loglik: -1.4390e+02 - logprior: -1.8137e+00
Epoch 3/10
15/15 - 1s - loss: 132.1455 - loglik: -1.3025e+02 - logprior: -1.8623e+00
Epoch 4/10
15/15 - 1s - loss: 127.6976 - loglik: -1.2580e+02 - logprior: -1.8193e+00
Epoch 5/10
15/15 - 1s - loss: 125.5746 - loglik: -1.2342e+02 - logprior: -1.7584e+00
Epoch 6/10
15/15 - 1s - loss: 124.8970 - loglik: -1.2265e+02 - logprior: -1.7866e+00
Epoch 7/10
15/15 - 1s - loss: 124.4244 - loglik: -1.2223e+02 - logprior: -1.7749e+00
Epoch 8/10
15/15 - 1s - loss: 124.0721 - loglik: -1.2184e+02 - logprior: -1.7552e+00
Epoch 9/10
15/15 - 1s - loss: 123.9686 - loglik: -1.2173e+02 - logprior: -1.7483e+00
Epoch 10/10
15/15 - 1s - loss: 123.5435 - loglik: -1.2131e+02 - logprior: -1.7520e+00
Fitted a model with MAP estimate = -122.9968
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (15, 1), (24, 2), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 133.7028 - loglik: -1.2620e+02 - logprior: -7.0415e+00
Epoch 2/2
15/15 - 1s - loss: 124.5287 - loglik: -1.2046e+02 - logprior: -3.5906e+00
Fitted a model with MAP estimate = -122.5509
expansions: [(0, 2)]
discards: [ 0 15 31 39 46]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 124.8939 - loglik: -1.1915e+02 - logprior: -5.2588e+00
Epoch 2/2
15/15 - 1s - loss: 118.9601 - loglik: -1.1666e+02 - logprior: -1.8063e+00
Fitted a model with MAP estimate = -117.5696
expansions: []
discards: [ 0 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 126.3591 - loglik: -1.1926e+02 - logprior: -6.5673e+00
Epoch 2/10
15/15 - 1s - loss: 119.7504 - loglik: -1.1688e+02 - logprior: -2.3057e+00
Epoch 3/10
15/15 - 1s - loss: 118.2757 - loglik: -1.1604e+02 - logprior: -1.6654e+00
Epoch 4/10
15/15 - 1s - loss: 117.9971 - loglik: -1.1591e+02 - logprior: -1.5135e+00
Epoch 5/10
15/15 - 1s - loss: 117.9688 - loglik: -1.1594e+02 - logprior: -1.4515e+00
Epoch 6/10
15/15 - 1s - loss: 117.6401 - loglik: -1.1563e+02 - logprior: -1.4231e+00
Epoch 7/10
15/15 - 1s - loss: 117.7574 - loglik: -1.1575e+02 - logprior: -1.4063e+00
Fitted a model with MAP estimate = -116.8940
Time for alignment: 42.7909
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 5s - loss: 172.3319 - loglik: -1.6663e+02 - logprior: -5.6635e+00
Epoch 2/10
15/15 - 1s - loss: 144.7339 - loglik: -1.4290e+02 - logprior: -1.8099e+00
Epoch 3/10
15/15 - 1s - loss: 129.6086 - loglik: -1.2762e+02 - logprior: -1.8926e+00
Epoch 4/10
15/15 - 1s - loss: 125.9840 - loglik: -1.2368e+02 - logprior: -1.8788e+00
Epoch 5/10
15/15 - 1s - loss: 124.8419 - loglik: -1.2256e+02 - logprior: -1.8092e+00
Epoch 6/10
15/15 - 1s - loss: 124.3929 - loglik: -1.2214e+02 - logprior: -1.8188e+00
Epoch 7/10
15/15 - 1s - loss: 124.2141 - loglik: -1.2200e+02 - logprior: -1.7905e+00
Epoch 8/10
15/15 - 1s - loss: 123.9811 - loglik: -1.2178e+02 - logprior: -1.7687e+00
Epoch 9/10
15/15 - 1s - loss: 123.7222 - loglik: -1.2150e+02 - logprior: -1.7645e+00
Epoch 10/10
15/15 - 1s - loss: 123.4564 - loglik: -1.2123e+02 - logprior: -1.7554e+00
Fitted a model with MAP estimate = -122.9845
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (15, 1), (27, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 61 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 132.9014 - loglik: -1.2536e+02 - logprior: -7.0427e+00
Epoch 2/2
15/15 - 1s - loss: 124.4107 - loglik: -1.2034e+02 - logprior: -3.5674e+00
Fitted a model with MAP estimate = -122.3120
expansions: [(0, 2)]
discards: [ 0 15 38 45]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 124.5237 - loglik: -1.1872e+02 - logprior: -5.2744e+00
Epoch 2/2
15/15 - 1s - loss: 118.9812 - loglik: -1.1662e+02 - logprior: -1.8252e+00
Fitted a model with MAP estimate = -117.3871
expansions: []
discards: [ 0 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 126.2320 - loglik: -1.1907e+02 - logprior: -6.5808e+00
Epoch 2/10
15/15 - 1s - loss: 119.6104 - loglik: -1.1668e+02 - logprior: -2.3260e+00
Epoch 3/10
15/15 - 1s - loss: 118.0300 - loglik: -1.1573e+02 - logprior: -1.6850e+00
Epoch 4/10
15/15 - 1s - loss: 117.9493 - loglik: -1.1580e+02 - logprior: -1.5283e+00
Epoch 5/10
15/15 - 1s - loss: 117.6618 - loglik: -1.1558e+02 - logprior: -1.4650e+00
Epoch 6/10
15/15 - 1s - loss: 117.6035 - loglik: -1.1556e+02 - logprior: -1.4359e+00
Epoch 7/10
15/15 - 1s - loss: 117.5678 - loglik: -1.1556e+02 - logprior: -1.4058e+00
Epoch 8/10
15/15 - 1s - loss: 117.3324 - loglik: -1.1533e+02 - logprior: -1.3948e+00
Epoch 9/10
15/15 - 1s - loss: 117.2514 - loglik: -1.1528e+02 - logprior: -1.3711e+00
Epoch 10/10
15/15 - 1s - loss: 117.2804 - loglik: -1.1534e+02 - logprior: -1.3434e+00
Fitted a model with MAP estimate = -116.5803
Time for alignment: 46.6650
Computed alignments with likelihoods: ['-116.6448', '-116.8940', '-116.5803']
Best model has likelihood: -116.5803
time for generating output: 0.1438
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/LIM.projection.fasta
SP score = 0.9790322580645161
Training of 3 independent models on file annexin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f342d867c40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3473d40700>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3424737730>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3495a3d3d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34edaff850>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f348d311370>, <__main__.SimpleDirichletPrior object at 0x7f34ed07bee0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34ed599a60>

Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 197.7363 - loglik: -1.8329e+02 - logprior: -1.4440e+01
Epoch 2/10
10/10 - 2s - loss: 171.1534 - loglik: -1.6693e+02 - logprior: -4.1887e+00
Epoch 3/10
10/10 - 2s - loss: 153.7426 - loglik: -1.5098e+02 - logprior: -2.6049e+00
Epoch 4/10
10/10 - 2s - loss: 140.3611 - loglik: -1.3763e+02 - logprior: -2.4676e+00
Epoch 5/10
10/10 - 2s - loss: 135.5197 - loglik: -1.3260e+02 - logprior: -2.5903e+00
Epoch 6/10
10/10 - 2s - loss: 133.6002 - loglik: -1.3069e+02 - logprior: -2.6713e+00
Epoch 7/10
10/10 - 2s - loss: 132.4483 - loglik: -1.2968e+02 - logprior: -2.5884e+00
Epoch 8/10
10/10 - 2s - loss: 132.3580 - loglik: -1.2978e+02 - logprior: -2.4276e+00
Epoch 9/10
10/10 - 2s - loss: 131.8629 - loglik: -1.2940e+02 - logprior: -2.3251e+00
Epoch 10/10
10/10 - 2s - loss: 131.6198 - loglik: -1.2920e+02 - logprior: -2.2908e+00
Fitted a model with MAP estimate = -131.4493
expansions: [(4, 2), (5, 2), (7, 2), (8, 2), (24, 1), (27, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (40, 1), (41, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 144.8734 - loglik: -1.2851e+02 - logprior: -1.6226e+01
Epoch 2/2
10/10 - 2s - loss: 126.9374 - loglik: -1.1972e+02 - logprior: -7.0856e+00
Fitted a model with MAP estimate = -123.9786
expansions: [(0, 2)]
discards: [ 0 10 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 129.4491 - loglik: -1.1640e+02 - logprior: -1.2903e+01
Epoch 2/2
10/10 - 2s - loss: 119.4180 - loglik: -1.1554e+02 - logprior: -3.7186e+00
Fitted a model with MAP estimate = -118.2334
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 132.8216 - loglik: -1.1742e+02 - logprior: -1.5218e+01
Epoch 2/10
10/10 - 2s - loss: 121.9409 - loglik: -1.1686e+02 - logprior: -4.8848e+00
Epoch 3/10
10/10 - 2s - loss: 118.9486 - loglik: -1.1598e+02 - logprior: -2.7566e+00
Epoch 4/10
10/10 - 2s - loss: 118.6582 - loglik: -1.1647e+02 - logprior: -1.9783e+00
Epoch 5/10
10/10 - 1s - loss: 117.5055 - loglik: -1.1582e+02 - logprior: -1.4737e+00
Epoch 6/10
10/10 - 2s - loss: 118.0153 - loglik: -1.1643e+02 - logprior: -1.3619e+00
Fitted a model with MAP estimate = -117.4928
Time for alignment: 62.1730
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 197.7851 - loglik: -1.8334e+02 - logprior: -1.4440e+01
Epoch 2/10
10/10 - 2s - loss: 170.4454 - loglik: -1.6623e+02 - logprior: -4.1871e+00
Epoch 3/10
10/10 - 2s - loss: 153.0876 - loglik: -1.5036e+02 - logprior: -2.5810e+00
Epoch 4/10
10/10 - 2s - loss: 140.7924 - loglik: -1.3811e+02 - logprior: -2.4267e+00
Epoch 5/10
10/10 - 2s - loss: 136.6816 - loglik: -1.3383e+02 - logprior: -2.5257e+00
Epoch 6/10
10/10 - 2s - loss: 134.0985 - loglik: -1.3126e+02 - logprior: -2.6086e+00
Epoch 7/10
10/10 - 2s - loss: 132.8257 - loglik: -1.3005e+02 - logprior: -2.5800e+00
Epoch 8/10
10/10 - 2s - loss: 132.0776 - loglik: -1.2947e+02 - logprior: -2.4437e+00
Epoch 9/10
10/10 - 2s - loss: 132.0058 - loglik: -1.2951e+02 - logprior: -2.3542e+00
Epoch 10/10
10/10 - 2s - loss: 131.5149 - loglik: -1.2904e+02 - logprior: -2.3310e+00
Fitted a model with MAP estimate = -131.4401
expansions: [(4, 2), (5, 2), (6, 2), (8, 2), (24, 1), (30, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 145.0130 - loglik: -1.2861e+02 - logprior: -1.6261e+01
Epoch 2/2
10/10 - 2s - loss: 127.2981 - loglik: -1.2007e+02 - logprior: -7.0964e+00
Fitted a model with MAP estimate = -124.1659
expansions: [(0, 2)]
discards: [ 0  9 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 129.6941 - loglik: -1.1666e+02 - logprior: -1.2900e+01
Epoch 2/2
10/10 - 2s - loss: 119.7689 - loglik: -1.1589e+02 - logprior: -3.7183e+00
Fitted a model with MAP estimate = -118.2760
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 132.5759 - loglik: -1.1719e+02 - logprior: -1.5211e+01
Epoch 2/10
10/10 - 2s - loss: 122.2588 - loglik: -1.1718e+02 - logprior: -4.8838e+00
Epoch 3/10
10/10 - 2s - loss: 118.9959 - loglik: -1.1604e+02 - logprior: -2.7518e+00
Epoch 4/10
10/10 - 2s - loss: 118.3198 - loglik: -1.1614e+02 - logprior: -1.9780e+00
Epoch 5/10
10/10 - 2s - loss: 117.7951 - loglik: -1.1611e+02 - logprior: -1.4708e+00
Epoch 6/10
10/10 - 2s - loss: 117.8308 - loglik: -1.1626e+02 - logprior: -1.3539e+00
Fitted a model with MAP estimate = -117.5037
Time for alignment: 60.5630
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 197.7264 - loglik: -1.8328e+02 - logprior: -1.4438e+01
Epoch 2/10
10/10 - 2s - loss: 170.6188 - loglik: -1.6641e+02 - logprior: -4.1800e+00
Epoch 3/10
10/10 - 2s - loss: 153.2545 - loglik: -1.5057e+02 - logprior: -2.5209e+00
Epoch 4/10
10/10 - 2s - loss: 142.4776 - loglik: -1.3986e+02 - logprior: -2.2515e+00
Epoch 5/10
10/10 - 2s - loss: 137.3996 - loglik: -1.3473e+02 - logprior: -2.3037e+00
Epoch 6/10
10/10 - 2s - loss: 135.7718 - loglik: -1.3317e+02 - logprior: -2.3771e+00
Epoch 7/10
10/10 - 2s - loss: 134.0910 - loglik: -1.3157e+02 - logprior: -2.3476e+00
Epoch 8/10
10/10 - 2s - loss: 133.6553 - loglik: -1.3128e+02 - logprior: -2.2379e+00
Epoch 9/10
10/10 - 2s - loss: 133.5973 - loglik: -1.3132e+02 - logprior: -2.1577e+00
Epoch 10/10
10/10 - 2s - loss: 133.0462 - loglik: -1.3078e+02 - logprior: -2.1379e+00
Fitted a model with MAP estimate = -133.0295
expansions: [(4, 2), (5, 2), (6, 2), (8, 2), (24, 1), (33, 3), (35, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 144.8211 - loglik: -1.2846e+02 - logprior: -1.6228e+01
Epoch 2/2
10/10 - 2s - loss: 127.1988 - loglik: -1.1998e+02 - logprior: -7.0893e+00
Fitted a model with MAP estimate = -124.1311
expansions: [(0, 2)]
discards: [ 0  9 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 129.6462 - loglik: -1.1661e+02 - logprior: -1.2901e+01
Epoch 2/2
10/10 - 2s - loss: 119.7745 - loglik: -1.1590e+02 - logprior: -3.7165e+00
Fitted a model with MAP estimate = -118.2788
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 132.5174 - loglik: -1.1713e+02 - logprior: -1.5207e+01
Epoch 2/10
10/10 - 2s - loss: 122.1792 - loglik: -1.1711e+02 - logprior: -4.8767e+00
Epoch 3/10
10/10 - 2s - loss: 119.1522 - loglik: -1.1619e+02 - logprior: -2.7465e+00
Epoch 4/10
10/10 - 2s - loss: 118.1733 - loglik: -1.1598e+02 - logprior: -1.9808e+00
Epoch 5/10
10/10 - 2s - loss: 118.2965 - loglik: -1.1661e+02 - logprior: -1.4704e+00
Fitted a model with MAP estimate = -117.5925
Time for alignment: 59.0641
Computed alignments with likelihoods: ['-117.4928', '-117.5037', '-117.5925']
Best model has likelihood: -117.4928
time for generating output: 0.2655
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/annexin.projection.fasta
SP score = 0.9966151893378464
Training of 3 independent models on file hormone_rec.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34af3b3f70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3462c633d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3473f77eb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3473b89df0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3473b89fa0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34ed0879a0>, <__main__.SimpleDirichletPrior object at 0x7f3473eb1c40>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f347c18b670>

Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 10s - loss: 528.4050 - loglik: -5.2171e+02 - logprior: -6.5246e+00
Epoch 2/10
23/23 - 6s - loss: 453.7720 - loglik: -4.5151e+02 - logprior: -1.1513e+00
Epoch 3/10
23/23 - 6s - loss: 434.0902 - loglik: -4.3112e+02 - logprior: -1.0250e+00
Epoch 4/10
23/23 - 6s - loss: 428.1078 - loglik: -4.2547e+02 - logprior: -9.3211e-01
Epoch 5/10
23/23 - 6s - loss: 427.2615 - loglik: -4.2499e+02 - logprior: -9.8028e-01
Epoch 6/10
23/23 - 6s - loss: 423.4268 - loglik: -4.2137e+02 - logprior: -1.0460e+00
Epoch 7/10
23/23 - 6s - loss: 423.1039 - loglik: -4.2119e+02 - logprior: -1.0589e+00
Epoch 8/10
23/23 - 6s - loss: 423.0884 - loglik: -4.2129e+02 - logprior: -1.0525e+00
Epoch 9/10
23/23 - 6s - loss: 422.1263 - loglik: -4.2039e+02 - logprior: -1.0625e+00
Epoch 10/10
23/23 - 6s - loss: 422.2454 - loglik: -4.2049e+02 - logprior: -1.1288e+00
Fitted a model with MAP estimate = -420.9204
expansions: [(0, 9), (9, 3), (19, 1), (39, 1), (57, 2), (58, 1), (60, 1), (67, 1), (72, 2), (73, 1), (81, 3), (87, 1), (108, 1), (113, 1), (120, 1), (121, 1), (122, 1), (126, 1), (127, 1), (149, 2), (150, 2), (159, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 201 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 11s - loss: 422.4568 - loglik: -4.1292e+02 - logprior: -8.9351e+00
Epoch 2/2
23/23 - 7s - loss: 405.3869 - loglik: -4.0336e+02 - logprior: -1.3667e+00
Fitted a model with MAP estimate = -402.7200
expansions: [(0, 3), (18, 2), (103, 1)]
discards: [  1   2   3   4  72  92 196 197 198 199 200]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 196 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 11s - loss: 413.3897 - loglik: -4.0323e+02 - logprior: -9.4801e+00
Epoch 2/2
23/23 - 7s - loss: 402.9759 - loglik: -4.0112e+02 - logprior: -1.1922e+00
Fitted a model with MAP estimate = -401.4370
expansions: [(196, 5)]
discards: [1 2 3]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 198 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 11s - loss: 408.5439 - loglik: -4.0212e+02 - logprior: -5.7758e+00
Epoch 2/10
23/23 - 7s - loss: 401.1230 - loglik: -3.9997e+02 - logprior: -5.1585e-01
Epoch 3/10
23/23 - 7s - loss: 400.1765 - loglik: -3.9981e+02 - logprior: 0.2161
Epoch 4/10
23/23 - 7s - loss: 399.1344 - loglik: -3.9916e+02 - logprior: 0.5771
Epoch 5/10
23/23 - 7s - loss: 399.3739 - loglik: -3.9965e+02 - logprior: 0.8116
Fitted a model with MAP estimate = -398.7051
Time for alignment: 167.3662
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 529.5646 - loglik: -5.2288e+02 - logprior: -6.5120e+00
Epoch 2/10
23/23 - 6s - loss: 453.6533 - loglik: -4.5128e+02 - logprior: -1.2816e+00
Epoch 3/10
23/23 - 6s - loss: 433.2906 - loglik: -4.3023e+02 - logprior: -1.4767e+00
Epoch 4/10
23/23 - 6s - loss: 426.2023 - loglik: -4.2322e+02 - logprior: -1.3807e+00
Epoch 5/10
23/23 - 6s - loss: 424.0806 - loglik: -4.2139e+02 - logprior: -1.3800e+00
Epoch 6/10
23/23 - 6s - loss: 422.4563 - loglik: -4.1998e+02 - logprior: -1.4025e+00
Epoch 7/10
23/23 - 6s - loss: 421.4138 - loglik: -4.1910e+02 - logprior: -1.3979e+00
Epoch 8/10
23/23 - 6s - loss: 420.1746 - loglik: -4.1797e+02 - logprior: -1.4020e+00
Epoch 9/10
23/23 - 6s - loss: 421.6072 - loglik: -4.1949e+02 - logprior: -1.4074e+00
Fitted a model with MAP estimate = -419.5856
expansions: [(0, 8), (10, 3), (11, 1), (31, 1), (43, 1), (53, 1), (57, 1), (58, 2), (59, 1), (66, 1), (71, 1), (72, 3), (77, 1), (83, 1), (84, 1), (86, 1), (112, 2), (113, 1), (119, 1), (121, 1), (124, 1), (127, 1), (131, 1), (149, 1), (150, 2), (159, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 203 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 11s - loss: 420.6283 - loglik: -4.1107e+02 - logprior: -8.9294e+00
Epoch 2/2
23/23 - 8s - loss: 404.2958 - loglik: -4.0223e+02 - logprior: -1.3671e+00
Fitted a model with MAP estimate = -401.7596
expansions: [(18, 1)]
discards: [  1   2   3  93 141 198 199 200 201 202]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 194 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 10s - loss: 409.6440 - loglik: -4.0293e+02 - logprior: -5.9936e+00
Epoch 2/2
23/23 - 7s - loss: 402.9823 - loglik: -4.0167e+02 - logprior: -6.2622e-01
Fitted a model with MAP estimate = -401.2955
expansions: [(0, 4), (15, 1), (194, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 203 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 11s - loss: 412.4274 - loglik: -4.0144e+02 - logprior: -1.0340e+01
Epoch 2/10
23/23 - 8s - loss: 402.0883 - loglik: -4.0016e+02 - logprior: -1.3096e+00
Epoch 3/10
23/23 - 8s - loss: 401.1012 - loglik: -4.0063e+02 - logprior: 0.1195
Epoch 4/10
23/23 - 8s - loss: 400.4383 - loglik: -4.0036e+02 - logprior: 0.4735
Epoch 5/10
23/23 - 7s - loss: 398.9616 - loglik: -3.9913e+02 - logprior: 0.6885
Epoch 6/10
23/23 - 7s - loss: 399.2326 - loglik: -3.9960e+02 - logprior: 0.8522
Fitted a model with MAP estimate = -398.5189
Time for alignment: 168.2659
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 10s - loss: 528.4379 - loglik: -5.2175e+02 - logprior: -6.5189e+00
Epoch 2/10
23/23 - 6s - loss: 455.0851 - loglik: -4.5284e+02 - logprior: -1.1190e+00
Epoch 3/10
23/23 - 6s - loss: 431.0492 - loglik: -4.2780e+02 - logprior: -1.2234e+00
Epoch 4/10
23/23 - 6s - loss: 423.8817 - loglik: -4.2084e+02 - logprior: -1.1180e+00
Epoch 5/10
23/23 - 6s - loss: 425.0782 - loglik: -4.2254e+02 - logprior: -1.1440e+00
Fitted a model with MAP estimate = -420.7322
expansions: [(0, 8), (8, 5), (52, 1), (53, 1), (56, 2), (57, 1), (59, 2), (62, 1), (70, 1), (71, 3), (75, 1), (78, 1), (79, 1), (86, 1), (112, 2), (113, 1), (119, 1), (121, 1), (124, 1), (127, 1), (149, 2), (150, 2), (159, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 11s - loss: 420.7332 - loglik: -4.1098e+02 - logprior: -8.6955e+00
Epoch 2/2
23/23 - 7s - loss: 404.0069 - loglik: -4.0169e+02 - logprior: -1.3601e+00
Fitted a model with MAP estimate = -401.9988
expansions: []
discards: [  1   2   3  77  94 142 199 200 201 202 203]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 193 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 11s - loss: 410.5430 - loglik: -4.0356e+02 - logprior: -6.0217e+00
Epoch 2/2
23/23 - 7s - loss: 403.2008 - loglik: -4.0177e+02 - logprior: -5.9728e-01
Fitted a model with MAP estimate = -401.8369
expansions: [(0, 4), (193, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 202 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 11s - loss: 413.5462 - loglik: -4.0267e+02 - logprior: -1.0099e+01
Epoch 2/10
23/23 - 7s - loss: 402.0965 - loglik: -4.0003e+02 - logprior: -1.3281e+00
Epoch 3/10
23/23 - 8s - loss: 400.8387 - loglik: -4.0021e+02 - logprior: 0.0784
Epoch 4/10
23/23 - 7s - loss: 399.7818 - loglik: -3.9961e+02 - logprior: 0.4717
Epoch 5/10
23/23 - 8s - loss: 400.3640 - loglik: -4.0045e+02 - logprior: 0.6820
Fitted a model with MAP estimate = -398.9585
Time for alignment: 136.6673
Computed alignments with likelihoods: ['-398.7051', '-398.5189', '-398.9585']
Best model has likelihood: -398.5189
time for generating output: 0.2700
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hormone_rec.projection.fasta
SP score = 0.7071722247601645
Training of 3 independent models on file Acetyltransf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34a6fbd0a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34b7d113d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f346b5bb310>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34ee1f3c40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34af2cd2e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34af6499a0>, <__main__.SimpleDirichletPrior object at 0x7f34edf40970>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34eec28af0>

Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 8s - loss: 249.7484 - loglik: -2.4831e+02 - logprior: -1.1759e+00
Epoch 2/10
29/29 - 4s - loss: 224.4487 - loglik: -2.2261e+02 - logprior: -8.5060e-01
Epoch 3/10
29/29 - 4s - loss: 219.2892 - loglik: -2.1739e+02 - logprior: -8.4653e-01
Epoch 4/10
29/29 - 4s - loss: 217.1165 - loglik: -2.1538e+02 - logprior: -8.6640e-01
Epoch 5/10
29/29 - 4s - loss: 216.6390 - loglik: -2.1492e+02 - logprior: -8.6162e-01
Epoch 6/10
29/29 - 4s - loss: 216.1408 - loglik: -2.1447e+02 - logprior: -8.5289e-01
Epoch 7/10
29/29 - 4s - loss: 215.0282 - loglik: -2.1340e+02 - logprior: -8.5350e-01
Epoch 8/10
29/29 - 4s - loss: 214.7258 - loglik: -2.1302e+02 - logprior: -8.5412e-01
Epoch 9/10
29/29 - 4s - loss: 213.8929 - loglik: -2.1229e+02 - logprior: -8.5759e-01
Epoch 10/10
29/29 - 4s - loss: 214.1182 - loglik: -2.1249e+02 - logprior: -8.7161e-01
Fitted a model with MAP estimate = -205.1518
expansions: [(2, 1), (14, 4), (22, 1), (39, 2), (41, 1), (42, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 210.9186 - loglik: -2.0895e+02 - logprior: -1.2273e+00
Epoch 2/2
29/29 - 4s - loss: 206.9833 - loglik: -2.0516e+02 - logprior: -8.6190e-01
Fitted a model with MAP estimate = -204.8525
expansions: [(15, 1)]
discards: [45 56]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 207.0983 - loglik: -2.0486e+02 - logprior: -1.2013e+00
Epoch 2/2
29/29 - 4s - loss: 204.9777 - loglik: -2.0301e+02 - logprior: -8.2131e-01
Fitted a model with MAP estimate = -206.5827
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 9s - loss: 200.6522 - loglik: -1.9798e+02 - logprior: -7.2480e-01
Epoch 2/10
42/42 - 5s - loss: 196.1996 - loglik: -1.9333e+02 - logprior: -5.2813e-01
Epoch 3/10
42/42 - 5s - loss: 192.7474 - loglik: -1.9045e+02 - logprior: -5.0864e-01
Epoch 4/10
42/42 - 6s - loss: 191.5138 - loglik: -1.8919e+02 - logprior: -4.9879e-01
Epoch 5/10
42/42 - 6s - loss: 190.1696 - loglik: -1.8789e+02 - logprior: -4.9542e-01
Epoch 6/10
42/42 - 6s - loss: 189.4030 - loglik: -1.8726e+02 - logprior: -4.9005e-01
Epoch 7/10
42/42 - 6s - loss: 188.5629 - loglik: -1.8648e+02 - logprior: -4.8134e-01
Epoch 8/10
42/42 - 6s - loss: 188.0547 - loglik: -1.8622e+02 - logprior: -4.7706e-01
Epoch 9/10
42/42 - 6s - loss: 187.6991 - loglik: -1.8588e+02 - logprior: -4.6739e-01
Epoch 10/10
42/42 - 5s - loss: 187.4066 - loglik: -1.8583e+02 - logprior: -4.6111e-01
Fitted a model with MAP estimate = -185.7649
Time for alignment: 175.5413
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 250.4400 - loglik: -2.4899e+02 - logprior: -1.1788e+00
Epoch 2/10
29/29 - 4s - loss: 225.3524 - loglik: -2.2346e+02 - logprior: -8.5396e-01
Epoch 3/10
29/29 - 4s - loss: 219.5833 - loglik: -2.1765e+02 - logprior: -8.5044e-01
Epoch 4/10
29/29 - 4s - loss: 217.3105 - loglik: -2.1557e+02 - logprior: -8.7942e-01
Epoch 5/10
29/29 - 4s - loss: 216.7207 - loglik: -2.1501e+02 - logprior: -8.6599e-01
Epoch 6/10
29/29 - 4s - loss: 216.0762 - loglik: -2.1441e+02 - logprior: -8.6297e-01
Epoch 7/10
29/29 - 4s - loss: 215.0085 - loglik: -2.1334e+02 - logprior: -8.6278e-01
Epoch 8/10
29/29 - 4s - loss: 214.8345 - loglik: -2.1305e+02 - logprior: -8.7539e-01
Epoch 9/10
29/29 - 4s - loss: 214.2211 - loglik: -2.1257e+02 - logprior: -8.8001e-01
Epoch 10/10
29/29 - 4s - loss: 213.6194 - loglik: -2.1197e+02 - logprior: -8.8122e-01
Fitted a model with MAP estimate = -205.0928
expansions: [(2, 1), (3, 1), (14, 3), (15, 1), (22, 1), (35, 1), (39, 1), (41, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 210.1042 - loglik: -2.0814e+02 - logprior: -1.2054e+00
Epoch 2/2
29/29 - 4s - loss: 205.8303 - loglik: -2.0403e+02 - logprior: -8.3623e-01
Fitted a model with MAP estimate = -204.1222
expansions: [(20, 1)]
discards: [56]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 206.1476 - loglik: -2.0390e+02 - logprior: -1.1848e+00
Epoch 2/2
29/29 - 4s - loss: 204.6319 - loglik: -2.0265e+02 - logprior: -8.1447e-01
Fitted a model with MAP estimate = -206.3283
expansions: []
discards: [4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 201.4012 - loglik: -1.9872e+02 - logprior: -7.1440e-01
Epoch 2/10
42/42 - 6s - loss: 196.1118 - loglik: -1.9322e+02 - logprior: -5.3636e-01
Epoch 3/10
42/42 - 5s - loss: 192.8477 - loglik: -1.9054e+02 - logprior: -5.1994e-01
Epoch 4/10
42/42 - 5s - loss: 191.7340 - loglik: -1.8944e+02 - logprior: -5.0431e-01
Epoch 5/10
42/42 - 5s - loss: 190.0564 - loglik: -1.8779e+02 - logprior: -4.9420e-01
Epoch 6/10
42/42 - 5s - loss: 189.2147 - loglik: -1.8708e+02 - logprior: -4.8976e-01
Epoch 7/10
42/42 - 5s - loss: 188.8477 - loglik: -1.8677e+02 - logprior: -4.7722e-01
Epoch 8/10
42/42 - 5s - loss: 188.1222 - loglik: -1.8627e+02 - logprior: -4.7751e-01
Epoch 9/10
42/42 - 5s - loss: 187.6551 - loglik: -1.8583e+02 - logprior: -4.6555e-01
Epoch 10/10
42/42 - 5s - loss: 187.2089 - loglik: -1.8562e+02 - logprior: -4.6304e-01
Fitted a model with MAP estimate = -185.8346
Time for alignment: 173.4693
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 8s - loss: 249.8627 - loglik: -2.4842e+02 - logprior: -1.1778e+00
Epoch 2/10
29/29 - 4s - loss: 224.1696 - loglik: -2.2234e+02 - logprior: -8.5129e-01
Epoch 3/10
29/29 - 4s - loss: 218.9220 - loglik: -2.1705e+02 - logprior: -8.6884e-01
Epoch 4/10
29/29 - 4s - loss: 217.2429 - loglik: -2.1553e+02 - logprior: -8.7209e-01
Epoch 5/10
29/29 - 4s - loss: 216.9740 - loglik: -2.1528e+02 - logprior: -8.5792e-01
Epoch 6/10
29/29 - 4s - loss: 216.1147 - loglik: -2.1448e+02 - logprior: -8.4975e-01
Epoch 7/10
29/29 - 4s - loss: 215.1074 - loglik: -2.1348e+02 - logprior: -8.4848e-01
Epoch 8/10
29/29 - 4s - loss: 215.4026 - loglik: -2.1370e+02 - logprior: -8.5116e-01
Fitted a model with MAP estimate = -202.5572
expansions: [(2, 2), (14, 4), (17, 3), (27, 2), (38, 1), (41, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 211.8469 - loglik: -2.0974e+02 - logprior: -1.2147e+00
Epoch 2/2
29/29 - 4s - loss: 206.4666 - loglik: -2.0436e+02 - logprior: -8.4496e-01
Fitted a model with MAP estimate = -201.9066
expansions: []
discards: [ 2 36 59]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 207.3122 - loglik: -2.0496e+02 - logprior: -1.1467e+00
Epoch 2/2
29/29 - 4s - loss: 205.8183 - loglik: -2.0374e+02 - logprior: -8.4650e-01
Fitted a model with MAP estimate = -204.0409
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 198.9928 - loglik: -1.9637e+02 - logprior: -7.2563e-01
Epoch 2/10
42/42 - 6s - loss: 195.3497 - loglik: -1.9264e+02 - logprior: -5.2438e-01
Epoch 3/10
42/42 - 6s - loss: 192.1471 - loglik: -1.8996e+02 - logprior: -5.0857e-01
Epoch 4/10
42/42 - 6s - loss: 191.1618 - loglik: -1.8886e+02 - logprior: -5.0659e-01
Epoch 5/10
42/42 - 6s - loss: 189.5232 - loglik: -1.8725e+02 - logprior: -4.9972e-01
Epoch 6/10
42/42 - 5s - loss: 188.9054 - loglik: -1.8677e+02 - logprior: -4.9295e-01
Epoch 7/10
42/42 - 6s - loss: 188.5739 - loglik: -1.8650e+02 - logprior: -4.8246e-01
Epoch 8/10
42/42 - 6s - loss: 187.6972 - loglik: -1.8583e+02 - logprior: -4.8155e-01
Epoch 9/10
42/42 - 6s - loss: 187.3573 - loglik: -1.8551e+02 - logprior: -4.7079e-01
Epoch 10/10
42/42 - 5s - loss: 186.9771 - loglik: -1.8540e+02 - logprior: -4.6550e-01
Fitted a model with MAP estimate = -185.5042
Time for alignment: 167.2678
Computed alignments with likelihoods: ['-185.7649', '-185.8346', '-185.5042']
Best model has likelihood: -185.5042
time for generating output: 0.2116
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Acetyltransf.projection.fasta
SP score = 0.6149283242708848
Training of 3 independent models on file phoslip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34ec86cee0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3462b7c160>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34af2bc190>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34b7dadfa0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b7dadf70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f344083da00>, <__main__.SimpleDirichletPrior object at 0x7f33a3788070>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34ed764ee0>

Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 376.1157 - loglik: -3.3016e+02 - logprior: -4.5934e+01
Epoch 2/10
10/10 - 1s - loss: 299.0735 - loglik: -2.8758e+02 - logprior: -1.1482e+01
Epoch 3/10
10/10 - 1s - loss: 251.1608 - loglik: -2.4563e+02 - logprior: -5.5203e+00
Epoch 4/10
10/10 - 1s - loss: 225.2613 - loglik: -2.2173e+02 - logprior: -3.5236e+00
Epoch 5/10
10/10 - 1s - loss: 213.1592 - loglik: -2.1042e+02 - logprior: -2.7066e+00
Epoch 6/10
10/10 - 1s - loss: 205.8624 - loglik: -2.0325e+02 - logprior: -2.3708e+00
Epoch 7/10
10/10 - 1s - loss: 203.9283 - loglik: -2.0141e+02 - logprior: -2.0487e+00
Epoch 8/10
10/10 - 1s - loss: 202.6089 - loglik: -2.0027e+02 - logprior: -1.8148e+00
Epoch 9/10
10/10 - 1s - loss: 201.7864 - loglik: -1.9961e+02 - logprior: -1.7165e+00
Epoch 10/10
10/10 - 1s - loss: 201.0951 - loglik: -1.9903e+02 - logprior: -1.6570e+00
Fitted a model with MAP estimate = -200.5221
expansions: [(10, 2), (11, 3), (12, 3), (13, 2), (14, 1), (44, 2), (45, 2), (51, 2), (52, 1), (58, 2), (79, 1), (81, 2), (82, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 120 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 247.6508 - loglik: -1.9489e+02 - logprior: -5.2344e+01
Epoch 2/2
10/10 - 2s - loss: 206.3730 - loglik: -1.8450e+02 - logprior: -2.1451e+01
Fitted a model with MAP estimate = -199.0917
expansions: [(0, 2)]
discards: [  0   9  13  55  75 103 104]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 224.8671 - loglik: -1.8291e+02 - logprior: -4.1497e+01
Epoch 2/2
10/10 - 2s - loss: 191.1837 - loglik: -1.8020e+02 - logprior: -1.0510e+01
Fitted a model with MAP estimate = -185.4241
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 229.7073 - loglik: -1.8016e+02 - logprior: -4.9049e+01
Epoch 2/10
10/10 - 2s - loss: 193.9742 - loglik: -1.7979e+02 - logprior: -1.3685e+01
Epoch 3/10
10/10 - 2s - loss: 184.8698 - loglik: -1.7972e+02 - logprior: -4.6470e+00
Epoch 4/10
10/10 - 2s - loss: 181.3708 - loglik: -1.7927e+02 - logprior: -1.5979e+00
Epoch 5/10
10/10 - 2s - loss: 179.7396 - loglik: -1.7899e+02 - logprior: -2.5264e-01
Epoch 6/10
10/10 - 2s - loss: 178.8857 - loglik: -1.7893e+02 - logprior: 0.5420
Epoch 7/10
10/10 - 2s - loss: 178.3202 - loglik: -1.7903e+02 - logprior: 1.2073
Epoch 8/10
10/10 - 2s - loss: 177.5682 - loglik: -1.7878e+02 - logprior: 1.7167
Epoch 9/10
10/10 - 2s - loss: 177.2856 - loglik: -1.7884e+02 - logprior: 2.0540
Epoch 10/10
10/10 - 2s - loss: 176.8411 - loglik: -1.7866e+02 - logprior: 2.3268
Fitted a model with MAP estimate = -176.1349
Time for alignment: 53.2048
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 376.0616 - loglik: -3.3011e+02 - logprior: -4.5933e+01
Epoch 2/10
10/10 - 1s - loss: 299.0761 - loglik: -2.8758e+02 - logprior: -1.1485e+01
Epoch 3/10
10/10 - 1s - loss: 251.9416 - loglik: -2.4629e+02 - logprior: -5.6406e+00
Epoch 4/10
10/10 - 1s - loss: 226.3516 - loglik: -2.2242e+02 - logprior: -3.9184e+00
Epoch 5/10
10/10 - 1s - loss: 216.8192 - loglik: -2.1363e+02 - logprior: -3.0919e+00
Epoch 6/10
10/10 - 1s - loss: 209.9944 - loglik: -2.0711e+02 - logprior: -2.5348e+00
Epoch 7/10
10/10 - 1s - loss: 205.8462 - loglik: -2.0300e+02 - logprior: -2.3102e+00
Epoch 8/10
10/10 - 1s - loss: 203.9194 - loglik: -2.0118e+02 - logprior: -2.2047e+00
Epoch 9/10
10/10 - 1s - loss: 202.9755 - loglik: -2.0042e+02 - logprior: -2.0904e+00
Epoch 10/10
10/10 - 1s - loss: 202.2624 - loglik: -1.9984e+02 - logprior: -2.0037e+00
Fitted a model with MAP estimate = -201.6282
expansions: [(10, 2), (11, 2), (12, 2), (13, 2), (14, 2), (16, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (63, 1), (79, 1), (81, 2), (82, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 234.7093 - loglik: -1.9208e+02 - logprior: -4.2201e+01
Epoch 2/2
10/10 - 2s - loss: 192.8659 - loglik: -1.8135e+02 - logprior: -1.1081e+01
Fitted a model with MAP estimate = -185.8274
expansions: []
discards: [  0  19  21  56  57 104 105]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 236.5668 - loglik: -1.8409e+02 - logprior: -5.2010e+01
Epoch 2/2
10/10 - 2s - loss: 204.1333 - loglik: -1.8241e+02 - logprior: -2.1233e+01
Fitted a model with MAP estimate = -198.5949
expansions: [(0, 2)]
discards: [ 0 10]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 220.3626 - loglik: -1.7868e+02 - logprior: -4.1180e+01
Epoch 2/10
10/10 - 2s - loss: 189.0026 - loglik: -1.7838e+02 - logprior: -1.0113e+01
Epoch 3/10
10/10 - 2s - loss: 182.9711 - loglik: -1.7878e+02 - logprior: -3.6870e+00
Epoch 4/10
10/10 - 2s - loss: 180.1974 - loglik: -1.7865e+02 - logprior: -1.0367e+00
Epoch 5/10
10/10 - 2s - loss: 179.3692 - loglik: -1.7922e+02 - logprior: 0.3612
Epoch 6/10
10/10 - 2s - loss: 178.6163 - loglik: -1.7926e+02 - logprior: 1.1549
Epoch 7/10
10/10 - 2s - loss: 178.2348 - loglik: -1.7937e+02 - logprior: 1.6465
Epoch 8/10
10/10 - 2s - loss: 178.0103 - loglik: -1.7948e+02 - logprior: 1.9840
Epoch 9/10
10/10 - 2s - loss: 177.1575 - loglik: -1.7890e+02 - logprior: 2.2643
Epoch 10/10
10/10 - 2s - loss: 177.5495 - loglik: -1.7954e+02 - logprior: 2.5114
Fitted a model with MAP estimate = -176.7458
Time for alignment: 52.5040
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 375.9439 - loglik: -3.2999e+02 - logprior: -4.5932e+01
Epoch 2/10
10/10 - 1s - loss: 299.2176 - loglik: -2.8772e+02 - logprior: -1.1485e+01
Epoch 3/10
10/10 - 1s - loss: 252.8050 - loglik: -2.4714e+02 - logprior: -5.6501e+00
Epoch 4/10
10/10 - 1s - loss: 225.7426 - loglik: -2.2182e+02 - logprior: -3.9027e+00
Epoch 5/10
10/10 - 1s - loss: 213.3466 - loglik: -2.1016e+02 - logprior: -3.0727e+00
Epoch 6/10
10/10 - 1s - loss: 206.5091 - loglik: -2.0347e+02 - logprior: -2.6663e+00
Epoch 7/10
10/10 - 1s - loss: 203.7144 - loglik: -2.0082e+02 - logprior: -2.4026e+00
Epoch 8/10
10/10 - 1s - loss: 202.9662 - loglik: -2.0031e+02 - logprior: -2.2164e+00
Epoch 9/10
10/10 - 1s - loss: 202.5801 - loglik: -2.0012e+02 - logprior: -2.0899e+00
Epoch 10/10
10/10 - 1s - loss: 201.8830 - loglik: -1.9953e+02 - logprior: -2.0104e+00
Fitted a model with MAP estimate = -201.4433
expansions: [(10, 2), (11, 2), (12, 2), (13, 1), (14, 2), (16, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (66, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 247.8822 - loglik: -1.9517e+02 - logprior: -5.2340e+01
Epoch 2/2
10/10 - 2s - loss: 205.5007 - loglik: -1.8369e+02 - logprior: -2.1410e+01
Fitted a model with MAP estimate = -198.1659
expansions: [(0, 2)]
discards: [  0   9  20  54  55 102 103]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 223.6680 - loglik: -1.8180e+02 - logprior: -4.1436e+01
Epoch 2/2
10/10 - 2s - loss: 190.5623 - loglik: -1.7969e+02 - logprior: -1.0395e+01
Fitted a model with MAP estimate = -185.1831
expansions: [(53, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 231.9028 - loglik: -1.8052e+02 - logprior: -5.0874e+01
Epoch 2/10
10/10 - 2s - loss: 197.1592 - loglik: -1.7935e+02 - logprior: -1.7295e+01
Epoch 3/10
10/10 - 2s - loss: 185.5787 - loglik: -1.7895e+02 - logprior: -6.1217e+00
Epoch 4/10
10/10 - 2s - loss: 180.5905 - loglik: -1.7830e+02 - logprior: -1.7921e+00
Epoch 5/10
10/10 - 2s - loss: 178.7070 - loglik: -1.7793e+02 - logprior: -2.7671e-01
Epoch 6/10
10/10 - 2s - loss: 177.8761 - loglik: -1.7791e+02 - logprior: 0.5456
Epoch 7/10
10/10 - 2s - loss: 177.0580 - loglik: -1.7778e+02 - logprior: 1.2335
Epoch 8/10
10/10 - 2s - loss: 176.5628 - loglik: -1.7780e+02 - logprior: 1.7513
Epoch 9/10
10/10 - 2s - loss: 176.2793 - loglik: -1.7786e+02 - logprior: 2.0878
Epoch 10/10
10/10 - 2s - loss: 175.7971 - loglik: -1.7764e+02 - logprior: 2.3550
Fitted a model with MAP estimate = -175.1188
Time for alignment: 53.1457
Computed alignments with likelihoods: ['-176.1349', '-176.7458', '-175.1188']
Best model has likelihood: -175.1188
time for generating output: 0.1622
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phoslip.projection.fasta
SP score = 0.9329027228687566
Training of 3 independent models on file cah.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f342d3fdfd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f347c33b8e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3436762250>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f347c303fa0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f347c303f10>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f346b643fd0>, <__main__.SimpleDirichletPrior object at 0x7f3449147af0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3462e6b4c0>

Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 630.8992 - loglik: -6.0754e+02 - logprior: -2.3315e+01
Epoch 2/10
14/14 - 4s - loss: 540.2906 - loglik: -5.3732e+02 - logprior: -2.8685e+00
Epoch 3/10
14/14 - 4s - loss: 475.7865 - loglik: -4.7367e+02 - logprior: -1.9151e+00
Epoch 4/10
14/14 - 4s - loss: 459.6237 - loglik: -4.5714e+02 - logprior: -1.8466e+00
Epoch 5/10
14/14 - 4s - loss: 454.7770 - loglik: -4.5240e+02 - logprior: -1.5794e+00
Epoch 6/10
14/14 - 4s - loss: 453.2510 - loglik: -4.5111e+02 - logprior: -1.3700e+00
Epoch 7/10
14/14 - 4s - loss: 451.7626 - loglik: -4.4968e+02 - logprior: -1.3255e+00
Epoch 8/10
14/14 - 4s - loss: 451.5515 - loglik: -4.4960e+02 - logprior: -1.2432e+00
Epoch 9/10
14/14 - 4s - loss: 450.9581 - loglik: -4.4910e+02 - logprior: -1.1910e+00
Epoch 10/10
14/14 - 4s - loss: 450.2415 - loglik: -4.4847e+02 - logprior: -1.1363e+00
Fitted a model with MAP estimate = -449.7371
expansions: [(14, 1), (15, 1), (28, 1), (30, 1), (31, 2), (32, 1), (40, 2), (41, 2), (42, 1), (51, 1), (52, 1), (55, 4), (81, 1), (90, 1), (91, 7), (112, 1), (113, 3), (114, 2), (115, 2), (118, 1), (119, 1), (120, 3), (130, 2), (153, 1), (163, 1), (166, 2), (167, 8)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 237 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 472.0088 - loglik: -4.4320e+02 - logprior: -2.8219e+01
Epoch 2/2
14/14 - 5s - loss: 439.5382 - loglik: -4.2936e+02 - logprior: -9.5579e+00
Fitted a model with MAP estimate = -435.1198
expansions: [(0, 3), (69, 2), (116, 1)]
discards: [  0  72 170]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 240 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 448.3542 - loglik: -4.2667e+02 - logprior: -2.1007e+01
Epoch 2/2
14/14 - 5s - loss: 426.6964 - loglik: -4.2389e+02 - logprior: -2.1060e+00
Fitted a model with MAP estimate = -423.0125
expansions: [(37, 1)]
discards: [  0   1  51  76  77 113 217]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 234 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 10s - loss: 455.1920 - loglik: -4.2720e+02 - logprior: -2.7256e+01
Epoch 2/10
14/14 - 5s - loss: 434.2065 - loglik: -4.2576e+02 - logprior: -7.6775e+00
Epoch 3/10
14/14 - 5s - loss: 426.7515 - loglik: -4.2575e+02 - logprior: -2.5299e-01
Epoch 4/10
14/14 - 5s - loss: 422.7205 - loglik: -4.2456e+02 - logprior: 2.5884
Epoch 5/10
14/14 - 5s - loss: 422.6169 - loglik: -4.2529e+02 - logprior: 3.4110
Epoch 6/10
14/14 - 5s - loss: 421.5422 - loglik: -4.2475e+02 - logprior: 3.9479
Epoch 7/10
14/14 - 5s - loss: 420.9273 - loglik: -4.2450e+02 - logprior: 4.3007
Epoch 8/10
14/14 - 5s - loss: 419.4855 - loglik: -4.2344e+02 - logprior: 4.6916
Epoch 9/10
14/14 - 5s - loss: 419.7469 - loglik: -4.2408e+02 - logprior: 5.0799
Fitted a model with MAP estimate = -418.5755
Time for alignment: 141.9955
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 632.4095 - loglik: -6.0904e+02 - logprior: -2.3322e+01
Epoch 2/10
14/14 - 4s - loss: 540.9503 - loglik: -5.3803e+02 - logprior: -2.8107e+00
Epoch 3/10
14/14 - 4s - loss: 481.1502 - loglik: -4.7915e+02 - logprior: -1.7945e+00
Epoch 4/10
14/14 - 4s - loss: 458.5347 - loglik: -4.5595e+02 - logprior: -1.9460e+00
Epoch 5/10
14/14 - 4s - loss: 456.0491 - loglik: -4.5332e+02 - logprior: -1.8991e+00
Epoch 6/10
14/14 - 4s - loss: 451.6917 - loglik: -4.4908e+02 - logprior: -1.8141e+00
Epoch 7/10
14/14 - 4s - loss: 451.6114 - loglik: -4.4915e+02 - logprior: -1.6996e+00
Epoch 8/10
14/14 - 4s - loss: 452.0286 - loglik: -4.4969e+02 - logprior: -1.6082e+00
Fitted a model with MAP estimate = -449.8643
expansions: [(14, 1), (15, 1), (28, 1), (30, 1), (31, 2), (32, 1), (33, 1), (41, 1), (42, 1), (43, 1), (53, 1), (54, 1), (55, 4), (58, 1), (65, 1), (80, 1), (90, 1), (110, 1), (112, 1), (113, 2), (114, 2), (115, 2), (117, 1), (118, 2), (119, 1), (120, 1), (154, 1), (157, 1), (163, 1), (166, 2), (167, 8)]
discards: [ 0 72]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 229 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 473.7051 - loglik: -4.4477e+02 - logprior: -2.8281e+01
Epoch 2/2
14/14 - 5s - loss: 442.4472 - loglik: -4.3236e+02 - logprior: -9.4445e+00
Fitted a model with MAP estimate = -438.6134
expansions: [(35, 1), (136, 1)]
discards: [  0  68 149]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 228 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 459.5520 - loglik: -4.3143e+02 - logprior: -2.7442e+01
Epoch 2/2
14/14 - 5s - loss: 437.9146 - loglik: -4.2962e+02 - logprior: -7.5669e+00
Fitted a model with MAP estimate = -432.5462
expansions: [(0, 3)]
discards: [  0 205]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 229 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 449.8238 - loglik: -4.2905e+02 - logprior: -2.0055e+01
Epoch 2/10
14/14 - 5s - loss: 428.3244 - loglik: -4.2628e+02 - logprior: -1.2982e+00
Epoch 3/10
14/14 - 5s - loss: 426.7657 - loglik: -4.2787e+02 - logprior: 1.8522
Epoch 4/10
14/14 - 5s - loss: 425.3829 - loglik: -4.2778e+02 - logprior: 3.1287
Epoch 5/10
14/14 - 5s - loss: 424.2614 - loglik: -4.2723e+02 - logprior: 3.7178
Epoch 6/10
14/14 - 5s - loss: 424.5381 - loglik: -4.2793e+02 - logprior: 4.1326
Fitted a model with MAP estimate = -422.5852
Time for alignment: 114.3438
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 631.0249 - loglik: -6.0765e+02 - logprior: -2.3325e+01
Epoch 2/10
14/14 - 4s - loss: 539.9965 - loglik: -5.3710e+02 - logprior: -2.7784e+00
Epoch 3/10
14/14 - 4s - loss: 478.5473 - loglik: -4.7660e+02 - logprior: -1.6274e+00
Epoch 4/10
14/14 - 4s - loss: 460.6072 - loglik: -4.5848e+02 - logprior: -1.3021e+00
Epoch 5/10
14/14 - 4s - loss: 456.7086 - loglik: -4.5475e+02 - logprior: -1.0578e+00
Epoch 6/10
14/14 - 4s - loss: 453.0396 - loglik: -4.5126e+02 - logprior: -1.0177e+00
Epoch 7/10
14/14 - 4s - loss: 453.8814 - loglik: -4.5223e+02 - logprior: -9.9598e-01
Fitted a model with MAP estimate = -451.5974
expansions: [(14, 1), (15, 1), (28, 1), (30, 1), (31, 2), (32, 1), (42, 2), (43, 1), (52, 1), (53, 3), (54, 5), (57, 1), (90, 1), (92, 6), (110, 1), (112, 1), (114, 2), (115, 2), (116, 2), (118, 2), (119, 2), (120, 3), (161, 1), (164, 2), (166, 1), (167, 8)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 237 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 472.0899 - loglik: -4.4332e+02 - logprior: -2.8155e+01
Epoch 2/2
14/14 - 5s - loss: 439.0009 - loglik: -4.2884e+02 - logprior: -9.5249e+00
Fitted a model with MAP estimate = -434.9507
expansions: [(117, 1)]
discards: [  0  68  69 113 155]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 233 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 457.4011 - loglik: -4.2903e+02 - logprior: -2.7661e+01
Epoch 2/2
14/14 - 5s - loss: 434.9398 - loglik: -4.2634e+02 - logprior: -7.8231e+00
Fitted a model with MAP estimate = -428.7229
expansions: [(0, 3), (33, 1), (115, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 237 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 446.1444 - loglik: -4.2516e+02 - logprior: -2.0225e+01
Epoch 2/10
14/14 - 5s - loss: 423.5883 - loglik: -4.2138e+02 - logprior: -1.4516e+00
Epoch 3/10
14/14 - 5s - loss: 422.2581 - loglik: -4.2323e+02 - logprior: 1.7184
Epoch 4/10
14/14 - 5s - loss: 420.5854 - loglik: -4.2285e+02 - logprior: 3.0078
Epoch 5/10
14/14 - 5s - loss: 420.0865 - loglik: -4.2300e+02 - logprior: 3.6539
Epoch 6/10
14/14 - 5s - loss: 418.6707 - loglik: -4.2199e+02 - logprior: 4.0658
Epoch 7/10
14/14 - 5s - loss: 419.6079 - loglik: -4.2332e+02 - logprior: 4.4541
Fitted a model with MAP estimate = -417.8298
Time for alignment: 116.8488
Computed alignments with likelihoods: ['-418.5755', '-422.5852', '-417.8298']
Best model has likelihood: -417.8298
time for generating output: 0.2923
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cah.projection.fasta
SP score = 0.864853195164076
Training of 3 independent models on file trfl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f348d7e5340>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f342d519130>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f342d519e50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f348d7f8130>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f348d7f88e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34a6978f70>, <__main__.SimpleDirichletPrior object at 0x7f3473da7d30>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f348498f430>

Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 13s - loss: 739.3037 - loglik: -6.9679e+02 - logprior: -4.2482e+01
Epoch 2/10
11/11 - 10s - loss: 628.3594 - loglik: -6.2346e+02 - logprior: -4.7870e+00
Epoch 3/10
11/11 - 9s - loss: 549.3080 - loglik: -5.4865e+02 - logprior: -6.0564e-01
Epoch 4/10
11/11 - 9s - loss: 498.6192 - loglik: -4.9817e+02 - logprior: -3.5717e-01
Epoch 5/10
11/11 - 9s - loss: 477.1152 - loglik: -4.7585e+02 - logprior: -9.6559e-01
Epoch 6/10
11/11 - 10s - loss: 474.7902 - loglik: -4.7351e+02 - logprior: -7.3956e-01
Epoch 7/10
11/11 - 9s - loss: 470.1446 - loglik: -4.6927e+02 - logprior: -3.3784e-01
Epoch 8/10
11/11 - 9s - loss: 463.7249 - loglik: -4.6317e+02 - logprior: -1.2138e-01
Epoch 9/10
11/11 - 10s - loss: 466.0751 - loglik: -4.6585e+02 - logprior: 0.1692
Fitted a model with MAP estimate = -464.1718
expansions: [(19, 5), (21, 1), (22, 1), (26, 1), (36, 1), (47, 1), (52, 2), (64, 1), (66, 1), (79, 1), (80, 1), (81, 1), (92, 1), (93, 1), (104, 5), (105, 1), (107, 1), (130, 1), (137, 1), (138, 1), (161, 1), (169, 2), (180, 1), (182, 2), (183, 2), (184, 1), (198, 4), (201, 2), (202, 6), (220, 1), (222, 2), (224, 1), (225, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 297 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 16s - loss: 504.7394 - loglik: -4.5412e+02 - logprior: -5.0224e+01
Epoch 2/2
11/11 - 13s - loss: 453.3547 - loglik: -4.3716e+02 - logprior: -1.5756e+01
Fitted a model with MAP estimate = -444.7694
expansions: [(0, 3), (239, 3)]
discards: [  0  21  61 123 124]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 16s - loss: 468.3825 - loglik: -4.2979e+02 - logprior: -3.8130e+01
Epoch 2/2
11/11 - 12s - loss: 432.0406 - loglik: -4.2835e+02 - logprior: -3.1930e+00
Fitted a model with MAP estimate = -425.7493
expansions: [(250, 1)]
discards: [ 0 22]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 297 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 16s - loss: 476.8015 - loglik: -4.2829e+02 - logprior: -4.7990e+01
Epoch 2/10
11/11 - 13s - loss: 440.2034 - loglik: -4.2735e+02 - logprior: -1.2311e+01
Epoch 3/10
11/11 - 13s - loss: 427.8501 - loglik: -4.2779e+02 - logprior: 0.5052
Epoch 4/10
11/11 - 13s - loss: 421.4469 - loglik: -4.2822e+02 - logprior: 7.3538
Epoch 5/10
11/11 - 12s - loss: 419.1471 - loglik: -4.2800e+02 - logprior: 9.4474
Epoch 6/10
11/11 - 12s - loss: 416.4007 - loglik: -4.2636e+02 - logprior: 10.5563
Epoch 7/10
11/11 - 12s - loss: 416.0893 - loglik: -4.2689e+02 - logprior: 11.4158
Epoch 8/10
11/11 - 13s - loss: 416.8792 - loglik: -4.2845e+02 - logprior: 12.1837
Fitted a model with MAP estimate = -414.6483
Time for alignment: 272.5770
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 13s - loss: 738.4340 - loglik: -6.9591e+02 - logprior: -4.2486e+01
Epoch 2/10
11/11 - 9s - loss: 635.2866 - loglik: -6.3040e+02 - logprior: -4.7603e+00
Epoch 3/10
11/11 - 9s - loss: 545.7446 - loglik: -5.4495e+02 - logprior: -7.3447e-01
Epoch 4/10
11/11 - 10s - loss: 495.6565 - loglik: -4.9484e+02 - logprior: -7.2650e-01
Epoch 5/10
11/11 - 10s - loss: 475.9916 - loglik: -4.7454e+02 - logprior: -1.1742e+00
Epoch 6/10
11/11 - 10s - loss: 470.3420 - loglik: -4.6902e+02 - logprior: -7.7500e-01
Epoch 7/10
11/11 - 10s - loss: 465.2667 - loglik: -4.6428e+02 - logprior: -4.2280e-01
Epoch 8/10
11/11 - 10s - loss: 466.9731 - loglik: -4.6628e+02 - logprior: -2.2977e-01
Fitted a model with MAP estimate = -463.9337
expansions: [(19, 4), (21, 2), (22, 1), (29, 1), (36, 1), (42, 1), (49, 1), (61, 1), (63, 4), (64, 1), (77, 1), (78, 1), (80, 1), (90, 1), (92, 1), (102, 4), (103, 1), (104, 3), (105, 2), (107, 2), (135, 1), (159, 2), (161, 5), (162, 1), (179, 1), (180, 3), (181, 1), (183, 1), (196, 1), (197, 2), (198, 2), (200, 2), (201, 4), (220, 1), (222, 2), (224, 1), (225, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 309 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 16s - loss: 491.7220 - loglik: -4.5222e+02 - logprior: -3.9085e+01
Epoch 2/2
11/11 - 12s - loss: 432.3213 - loglik: -4.2757e+02 - logprior: -4.2968e+00
Fitted a model with MAP estimate = -424.5218
expansions: [(224, 1), (262, 3)]
discards: [  1  25  78 126 127 132 135 247]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 305 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 17s - loss: 463.2045 - loglik: -4.2571e+02 - logprior: -3.7047e+01
Epoch 2/2
11/11 - 14s - loss: 424.0481 - loglik: -4.1995e+02 - logprior: -3.5927e+00
Fitted a model with MAP estimate = -419.9128
expansions: []
discards: [ 21  76 132]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 302 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 16s - loss: 460.0710 - loglik: -4.2315e+02 - logprior: -3.6403e+01
Epoch 2/10
11/11 - 12s - loss: 423.6346 - loglik: -4.2075e+02 - logprior: -2.3437e+00
Epoch 3/10
11/11 - 14s - loss: 417.9751 - loglik: -4.2203e+02 - logprior: 4.6127
Epoch 4/10
11/11 - 12s - loss: 413.9525 - loglik: -4.2111e+02 - logprior: 7.7332
Epoch 5/10
11/11 - 12s - loss: 413.2467 - loglik: -4.2222e+02 - logprior: 9.5638
Epoch 6/10
11/11 - 12s - loss: 412.5688 - loglik: -4.2265e+02 - logprior: 10.6835
Epoch 7/10
11/11 - 12s - loss: 412.2772 - loglik: -4.2300e+02 - logprior: 11.3263
Epoch 8/10
11/11 - 13s - loss: 408.2948 - loglik: -4.1969e+02 - logprior: 12.0053
Epoch 9/10
11/11 - 12s - loss: 407.7397 - loglik: -4.1992e+02 - logprior: 12.8015
Epoch 10/10
11/11 - 13s - loss: 409.1442 - loglik: -4.2207e+02 - logprior: 13.5587
Fitted a model with MAP estimate = -407.0077
Time for alignment: 290.9321
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 13s - loss: 739.3785 - loglik: -6.9686e+02 - logprior: -4.2488e+01
Epoch 2/10
11/11 - 9s - loss: 634.7126 - loglik: -6.2989e+02 - logprior: -4.6901e+00
Epoch 3/10
11/11 - 9s - loss: 542.3676 - loglik: -5.4216e+02 - logprior: -1.3941e-01
Epoch 4/10
11/11 - 10s - loss: 489.0029 - loglik: -4.8881e+02 - logprior: -8.7465e-02
Epoch 5/10
11/11 - 9s - loss: 478.1602 - loglik: -4.7756e+02 - logprior: -2.3539e-01
Epoch 6/10
11/11 - 9s - loss: 469.1937 - loglik: -4.6890e+02 - logprior: 0.2647
Epoch 7/10
11/11 - 10s - loss: 466.0600 - loglik: -4.6621e+02 - logprior: 0.6637
Epoch 8/10
11/11 - 10s - loss: 465.1837 - loglik: -4.6567e+02 - logprior: 0.9274
Epoch 9/10
11/11 - 9s - loss: 467.4177 - loglik: -4.6815e+02 - logprior: 1.1700
Fitted a model with MAP estimate = -463.8969
expansions: [(22, 3), (25, 1), (37, 1), (51, 1), (53, 2), (62, 1), (64, 4), (65, 1), (78, 1), (79, 1), (80, 1), (91, 1), (92, 1), (103, 4), (104, 1), (180, 1), (181, 1), (182, 5), (198, 7), (199, 2), (200, 4), (222, 1), (224, 1), (225, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 289 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 16s - loss: 507.3573 - loglik: -4.5614e+02 - logprior: -5.0771e+01
Epoch 2/2
11/11 - 12s - loss: 456.9882 - loglik: -4.4017e+02 - logprior: -1.6335e+01
Fitted a model with MAP estimate = -448.4842
expansions: [(0, 3), (186, 6), (208, 1), (230, 1), (243, 3)]
discards: [ 0 58 74]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 300 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 16s - loss: 471.6681 - loglik: -4.3273e+02 - logprior: -3.8444e+01
Epoch 2/2
11/11 - 12s - loss: 429.4951 - loglik: -4.2543e+02 - logprior: -3.5281e+00
Fitted a model with MAP estimate = -423.8604
expansions: [(22, 2), (185, 1)]
discards: [  0  75 189]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 300 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 16s - loss: 474.8808 - loglik: -4.2638e+02 - logprior: -4.7937e+01
Epoch 2/10
11/11 - 13s - loss: 438.2461 - loglik: -4.2519e+02 - logprior: -1.2463e+01
Epoch 3/10
11/11 - 12s - loss: 423.6277 - loglik: -4.2353e+02 - logprior: 0.5010
Epoch 4/10
11/11 - 12s - loss: 416.4301 - loglik: -4.2291e+02 - logprior: 7.0930
Epoch 5/10
11/11 - 12s - loss: 415.4235 - loglik: -4.2400e+02 - logprior: 9.1989
Epoch 6/10
11/11 - 13s - loss: 413.6830 - loglik: -4.2338e+02 - logprior: 10.3266
Epoch 7/10
11/11 - 12s - loss: 410.8501 - loglik: -4.2148e+02 - logprior: 11.2711
Epoch 8/10
11/11 - 13s - loss: 413.9333 - loglik: -4.2534e+02 - logprior: 12.0535
Fitted a model with MAP estimate = -411.0208
Time for alignment: 267.8936
Computed alignments with likelihoods: ['-414.6483', '-407.0077', '-411.0208']
Best model has likelihood: -407.0077
time for generating output: 0.6275
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/trfl.projection.fasta
SP score = 0.9420894139448691
Training of 3 independent models on file serpin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f33a3517a00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f346b51e940>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3495b76250>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3495b76850>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f342daadf40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3462bb4760>, <__main__.SimpleDirichletPrior object at 0x7f34ecfc5910>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34af6b4ee0>

Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 20s - loss: 812.9683 - loglik: -8.0632e+02 - logprior: -6.5140e+00
Epoch 2/10
21/21 - 17s - loss: 696.2694 - loglik: -6.9436e+02 - logprior: -1.3441e+00
Epoch 3/10
21/21 - 17s - loss: 640.4553 - loglik: -6.3600e+02 - logprior: -3.6425e+00
Epoch 4/10
21/21 - 17s - loss: 631.2689 - loglik: -6.2674e+02 - logprior: -3.5144e+00
Epoch 5/10
21/21 - 17s - loss: 628.2792 - loglik: -6.2402e+02 - logprior: -3.3398e+00
Epoch 6/10
21/21 - 17s - loss: 632.5332 - loglik: -6.2825e+02 - logprior: -3.4451e+00
Fitted a model with MAP estimate = -627.7727
expansions: [(13, 1), (14, 2), (15, 1), (16, 1), (52, 2), (54, 3), (55, 2), (61, 1), (62, 2), (63, 1), (64, 2), (76, 1), (77, 1), (78, 1), (79, 1), (80, 3), (81, 1), (85, 1), (87, 1), (94, 1), (95, 1), (96, 1), (103, 1), (104, 1), (111, 1), (113, 1), (115, 1), (131, 1), (135, 1), (136, 1), (139, 1), (142, 2), (144, 1), (152, 1), (153, 1), (154, 1), (155, 1), (157, 1), (158, 1), (160, 1), (161, 1), (181, 1), (184, 1), (187, 1), (190, 1), (191, 1), (192, 2), (193, 1), (194, 1), (195, 1), (196, 1), (209, 1), (212, 1), (213, 1), (215, 1), (219, 1), (223, 1), (229, 2), (230, 2), (232, 1), (257, 1), (258, 3), (259, 2), (261, 1), (262, 1), (271, 1), (272, 2), (273, 2)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 30s - loss: 621.3617 - loglik: -6.1076e+02 - logprior: -9.8288e+00
Epoch 2/2
21/21 - 25s - loss: 600.1168 - loglik: -5.9577e+02 - logprior: -3.5627e+00
Fitted a model with MAP estimate = -595.1131
expansions: [(0, 3), (102, 1)]
discards: [  0  56 180 296 335]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 29s - loss: 598.4219 - loglik: -5.9153e+02 - logprior: -6.0019e+00
Epoch 2/2
21/21 - 25s - loss: 592.0374 - loglik: -5.9158e+02 - logprior: 0.4667
Fitted a model with MAP estimate = -588.4044
expansions: []
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 28s - loss: 596.9232 - loglik: -5.9020e+02 - logprior: -5.7457e+00
Epoch 2/10
21/21 - 25s - loss: 589.1872 - loglik: -5.8908e+02 - logprior: 0.8494
Epoch 3/10
21/21 - 25s - loss: 589.6665 - loglik: -5.9036e+02 - logprior: 1.6475
Fitted a model with MAP estimate = -586.2329
Time for alignment: 358.4864
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 21s - loss: 818.2592 - loglik: -8.1162e+02 - logprior: -6.5018e+00
Epoch 2/10
21/21 - 17s - loss: 693.5145 - loglik: -6.9152e+02 - logprior: -1.3953e+00
Epoch 3/10
21/21 - 17s - loss: 639.5892 - loglik: -6.3471e+02 - logprior: -3.9018e+00
Epoch 4/10
21/21 - 17s - loss: 632.6044 - loglik: -6.2760e+02 - logprior: -3.8856e+00
Epoch 5/10
21/21 - 17s - loss: 629.6177 - loglik: -6.2489e+02 - logprior: -3.7411e+00
Epoch 6/10
21/21 - 17s - loss: 632.5836 - loglik: -6.2799e+02 - logprior: -3.7239e+00
Fitted a model with MAP estimate = -627.8063
expansions: [(13, 1), (14, 2), (15, 1), (53, 3), (56, 1), (57, 1), (62, 2), (63, 2), (65, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (84, 1), (86, 1), (87, 1), (88, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (103, 1), (104, 1), (111, 1), (113, 1), (115, 1), (117, 1), (135, 1), (136, 1), (139, 1), (153, 1), (154, 1), (155, 1), (156, 1), (158, 1), (159, 1), (161, 1), (162, 2), (170, 1), (180, 1), (183, 1), (186, 1), (189, 1), (190, 1), (191, 2), (192, 1), (193, 1), (194, 1), (195, 1), (207, 1), (208, 1), (211, 1), (214, 1), (223, 1), (227, 1), (228, 1), (229, 2), (231, 1), (235, 1), (255, 1), (256, 1), (257, 1), (258, 2), (260, 2), (261, 1), (272, 2), (273, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 363 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 28s - loss: 619.4382 - loglik: -6.0887e+02 - logprior: -9.7778e+00
Epoch 2/2
21/21 - 25s - loss: 601.8102 - loglik: -5.9780e+02 - logprior: -3.2188e+00
Fitted a model with MAP estimate = -595.7308
expansions: [(0, 2), (75, 1)]
discards: [  0 331]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 28s - loss: 599.4072 - loglik: -5.9211e+02 - logprior: -6.3857e+00
Epoch 2/2
21/21 - 24s - loss: 592.3917 - loglik: -5.9178e+02 - logprior: 0.3108
Fitted a model with MAP estimate = -589.5345
expansions: [(75, 1), (184, 1)]
discards: [  1 333]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 29s - loss: 597.2956 - loglik: -5.9074e+02 - logprior: -5.5959e+00
Epoch 2/10
21/21 - 25s - loss: 590.9239 - loglik: -5.9078e+02 - logprior: 0.8133
Epoch 3/10
21/21 - 25s - loss: 588.3702 - loglik: -5.8898e+02 - logprior: 1.5456
Epoch 4/10
21/21 - 25s - loss: 588.1714 - loglik: -5.8927e+02 - logprior: 1.9543
Epoch 5/10
21/21 - 25s - loss: 586.4612 - loglik: -5.8789e+02 - logprior: 2.2275
Epoch 6/10
21/21 - 25s - loss: 584.6944 - loglik: -5.8657e+02 - logprior: 2.6243
Epoch 7/10
21/21 - 25s - loss: 588.8267 - loglik: -5.9106e+02 - logprior: 2.9437
Fitted a model with MAP estimate = -584.8207
Time for alignment: 454.2895
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 21s - loss: 817.8964 - loglik: -8.1125e+02 - logprior: -6.5116e+00
Epoch 2/10
21/21 - 17s - loss: 689.9948 - loglik: -6.8790e+02 - logprior: -1.4249e+00
Epoch 3/10
21/21 - 17s - loss: 639.2824 - loglik: -6.3406e+02 - logprior: -3.7943e+00
Epoch 4/10
21/21 - 17s - loss: 631.0657 - loglik: -6.2603e+02 - logprior: -3.6678e+00
Epoch 5/10
21/21 - 17s - loss: 628.2245 - loglik: -6.2373e+02 - logprior: -3.4992e+00
Epoch 6/10
21/21 - 17s - loss: 630.9152 - loglik: -6.2658e+02 - logprior: -3.5337e+00
Fitted a model with MAP estimate = -626.6914
expansions: [(13, 1), (14, 2), (15, 1), (46, 1), (52, 3), (54, 2), (56, 1), (61, 2), (62, 2), (64, 2), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (82, 1), (84, 1), (85, 1), (86, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (100, 1), (102, 1), (109, 1), (111, 1), (113, 1), (115, 1), (133, 1), (134, 1), (137, 1), (140, 1), (151, 1), (152, 1), (153, 1), (154, 1), (156, 3), (158, 1), (159, 1), (168, 1), (178, 1), (181, 1), (188, 1), (189, 1), (191, 1), (192, 1), (193, 2), (194, 1), (195, 1), (207, 1), (208, 1), (211, 1), (218, 1), (223, 1), (227, 1), (228, 1), (229, 2), (231, 1), (254, 1), (255, 1), (256, 1), (257, 1), (258, 2), (260, 2), (261, 1), (272, 2), (273, 1)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 29s - loss: 616.0912 - loglik: -6.0837e+02 - logprior: -6.9650e+00
Epoch 2/2
21/21 - 25s - loss: 596.2043 - loglik: -5.9508e+02 - logprior: -3.3680e-01
Fitted a model with MAP estimate = -591.5033
expansions: [(77, 1)]
discards: [ 57 251 335]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 28s - loss: 599.3073 - loglik: -5.9237e+02 - logprior: -6.0420e+00
Epoch 2/2
21/21 - 25s - loss: 589.7410 - loglik: -5.8908e+02 - logprior: 0.2439
Fitted a model with MAP estimate = -588.6631
expansions: []
discards: [334]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 29s - loss: 598.9894 - loglik: -5.9244e+02 - logprior: -5.6130e+00
Epoch 2/10
21/21 - 25s - loss: 586.4811 - loglik: -5.8638e+02 - logprior: 0.8117
Epoch 3/10
21/21 - 24s - loss: 589.7464 - loglik: -5.9043e+02 - logprior: 1.5640
Fitted a model with MAP estimate = -586.6126
Time for alignment: 356.4239
Computed alignments with likelihoods: ['-586.2329', '-584.8207', '-586.6126']
Best model has likelihood: -584.8207
time for generating output: 0.3875
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/serpin.projection.fasta
SP score = 0.9652406417112299
Training of 3 independent models on file ghf13.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34af14e100>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3424662b80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3449242250>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34b7abd6a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f347c3c43d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f342d0dfac0>, <__main__.SimpleDirichletPrior object at 0x7f34ed1d5af0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34af6b4ee0>

Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 835.8707 - loglik: -8.3368e+02 - logprior: -1.8655e+00
Epoch 2/10
39/39 - 23s - loss: 737.5895 - loglik: -7.3458e+02 - logprior: -1.7157e+00
Epoch 3/10
39/39 - 24s - loss: 722.6661 - loglik: -7.1853e+02 - logprior: -1.9541e+00
Epoch 4/10
39/39 - 24s - loss: 717.6724 - loglik: -7.1349e+02 - logprior: -2.0234e+00
Epoch 5/10
39/39 - 23s - loss: 714.4761 - loglik: -7.1048e+02 - logprior: -2.0283e+00
Epoch 6/10
39/39 - 24s - loss: 712.6634 - loglik: -7.0881e+02 - logprior: -2.0661e+00
Epoch 7/10
39/39 - 23s - loss: 711.4864 - loglik: -7.0768e+02 - logprior: -2.1007e+00
Epoch 8/10
39/39 - 24s - loss: 710.9719 - loglik: -7.0732e+02 - logprior: -2.1155e+00
Epoch 9/10
39/39 - 24s - loss: 710.3459 - loglik: -7.0682e+02 - logprior: -2.1115e+00
Epoch 10/10
39/39 - 24s - loss: 708.8659 - loglik: -7.0542e+02 - logprior: -2.1313e+00
Fitted a model with MAP estimate = -613.5304
expansions: [(14, 1), (41, 1), (57, 1), (82, 3), (83, 2), (96, 2), (98, 1), (103, 2), (106, 1), (108, 5), (111, 1), (117, 1), (119, 1), (121, 2), (122, 4), (123, 1), (152, 1), (161, 2), (162, 3), (164, 1), (165, 1), (169, 11), (170, 3), (172, 1), (173, 1), (174, 1), (175, 1), (177, 2), (178, 7), (179, 1), (180, 1), (184, 1), (185, 1), (187, 2), (189, 1), (190, 1), (204, 1), (205, 1), (244, 3)]
discards: [  0  34 142 153]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 317 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 696.5855 - loglik: -6.9180e+02 - logprior: -3.5422e+00
Epoch 2/2
39/39 - 33s - loss: 679.6782 - loglik: -6.7547e+02 - logprior: -2.5515e+00
Fitted a model with MAP estimate = -588.0812
expansions: [(0, 2), (180, 1), (317, 2)]
discards: [  0 112 119 120 121 122 123 124 125 126 127 145 211 229 251 314 315 316]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 304 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 682.4055 - loglik: -6.7793e+02 - logprior: -2.6200e+00
Epoch 2/2
39/39 - 31s - loss: 678.6360 - loglik: -6.7547e+02 - logprior: -1.3761e+00
Fitted a model with MAP estimate = -588.6796
expansions: [(119, 7), (120, 1), (205, 2), (206, 1), (304, 2)]
discards: [  0 302 303]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 314 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 39s - loss: 588.9821 - loglik: -5.8464e+02 - logprior: -2.4911e+00
Epoch 2/10
43/43 - 36s - loss: 584.0217 - loglik: -5.8105e+02 - logprior: -1.1413e+00
Epoch 3/10
43/43 - 36s - loss: 578.3090 - loglik: -5.7553e+02 - logprior: -9.8566e-01
Epoch 4/10
43/43 - 36s - loss: 581.7278 - loglik: -5.7939e+02 - logprior: -6.8094e-01
Fitted a model with MAP estimate = -576.3058
Time for alignment: 680.4428
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 838.7861 - loglik: -8.3657e+02 - logprior: -1.8502e+00
Epoch 2/10
39/39 - 23s - loss: 739.3621 - loglik: -7.3674e+02 - logprior: -1.6449e+00
Epoch 3/10
39/39 - 24s - loss: 722.2451 - loglik: -7.1839e+02 - logprior: -2.0229e+00
Epoch 4/10
39/39 - 24s - loss: 717.0688 - loglik: -7.1314e+02 - logprior: -2.0685e+00
Epoch 5/10
39/39 - 24s - loss: 715.4133 - loglik: -7.1172e+02 - logprior: -2.0595e+00
Epoch 6/10
39/39 - 24s - loss: 714.3531 - loglik: -7.1077e+02 - logprior: -2.0691e+00
Epoch 7/10
39/39 - 23s - loss: 713.3862 - loglik: -7.0992e+02 - logprior: -2.0642e+00
Epoch 8/10
39/39 - 23s - loss: 712.4712 - loglik: -7.0906e+02 - logprior: -2.0831e+00
Epoch 9/10
39/39 - 24s - loss: 712.0372 - loglik: -7.0864e+02 - logprior: -2.0981e+00
Epoch 10/10
39/39 - 23s - loss: 711.3925 - loglik: -7.0803e+02 - logprior: -2.1226e+00
Fitted a model with MAP estimate = -614.8466
expansions: [(14, 1), (41, 1), (57, 1), (77, 1), (81, 2), (82, 3), (83, 1), (85, 1), (92, 1), (93, 2), (95, 1), (100, 3), (110, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1), (151, 6), (152, 1), (154, 3), (169, 1), (170, 3), (172, 3), (173, 1), (175, 1), (176, 1), (179, 1), (181, 6), (182, 1), (186, 3), (187, 1), (188, 1), (189, 2), (190, 2), (191, 2), (192, 1), (212, 4), (225, 1), (244, 3)]
discards: [  0  34 104 105 106 107 108]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 310 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 703.2448 - loglik: -6.9844e+02 - logprior: -3.5489e+00
Epoch 2/2
39/39 - 32s - loss: 688.6558 - loglik: -6.8471e+02 - logprior: -2.3709e+00
Fitted a model with MAP estimate = -595.4657
expansions: [(0, 2), (122, 11), (244, 1), (310, 2)]
discards: [  0  86 103 116 172 173 188 189 190 191 192 193 194 195 224 270 271 272
 307 308 309]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 305 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 688.2007 - loglik: -6.8392e+02 - logprior: -2.5462e+00
Epoch 2/2
39/39 - 31s - loss: 680.3936 - loglik: -6.7735e+02 - logprior: -1.3107e+00
Fitted a model with MAP estimate = -588.9941
expansions: [(103, 2), (127, 1), (196, 2), (204, 1)]
discards: [  0 118 119 120 245 303 304]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 304 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 38s - loss: 591.3174 - loglik: -5.8706e+02 - logprior: -2.4883e+00
Epoch 2/10
43/43 - 34s - loss: 586.3344 - loglik: -5.8351e+02 - logprior: -1.0606e+00
Epoch 3/10
43/43 - 34s - loss: 584.3380 - loglik: -5.8195e+02 - logprior: -6.7899e-01
Epoch 4/10
43/43 - 35s - loss: 584.3934 - loglik: -5.8225e+02 - logprior: -5.6993e-01
Fitted a model with MAP estimate = -580.8658
Time for alignment: 671.4191
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 838.6104 - loglik: -8.3640e+02 - logprior: -1.8455e+00
Epoch 2/10
39/39 - 23s - loss: 739.9494 - loglik: -7.3696e+02 - logprior: -1.6768e+00
Epoch 3/10
39/39 - 24s - loss: 724.2374 - loglik: -7.2019e+02 - logprior: -1.9224e+00
Epoch 4/10
39/39 - 24s - loss: 719.0554 - loglik: -7.1505e+02 - logprior: -2.0091e+00
Epoch 5/10
39/39 - 24s - loss: 716.4417 - loglik: -7.1266e+02 - logprior: -2.0153e+00
Epoch 6/10
39/39 - 24s - loss: 715.3571 - loglik: -7.1182e+02 - logprior: -2.0310e+00
Epoch 7/10
39/39 - 23s - loss: 713.8159 - loglik: -7.1042e+02 - logprior: -2.0364e+00
Epoch 8/10
39/39 - 24s - loss: 713.8528 - loglik: -7.1056e+02 - logprior: -2.0512e+00
Fitted a model with MAP estimate = -616.7576
expansions: [(19, 1), (41, 1), (43, 1), (77, 1), (81, 4), (82, 2), (84, 1), (94, 2), (96, 1), (101, 2), (104, 1), (111, 1), (117, 1), (119, 1), (121, 2), (122, 4), (123, 1), (147, 6), (168, 5), (169, 2), (170, 1), (171, 2), (173, 1), (174, 1), (175, 1), (179, 7), (180, 2), (181, 2), (186, 1), (187, 1), (188, 2), (189, 3), (190, 2), (191, 1), (205, 1), (206, 1), (218, 1), (244, 3)]
discards: [  0  34 105 106 107 108 158 159 160 161 162 163]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 305 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 704.6540 - loglik: -6.9990e+02 - logprior: -3.4641e+00
Epoch 2/2
39/39 - 31s - loss: 689.5907 - loglik: -6.8557e+02 - logprior: -2.4091e+00
Fitted a model with MAP estimate = -595.2670
expansions: [(0, 2), (120, 7), (174, 4), (192, 1), (305, 2)]
discards: [  0  87 114 139 185 186 187 188 242 302 303 304]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 309 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 684.4433 - loglik: -6.8017e+02 - logprior: -2.6032e+00
Epoch 2/2
39/39 - 32s - loss: 679.2003 - loglik: -6.7626e+02 - logprior: -1.3920e+00
Fitted a model with MAP estimate = -587.7720
expansions: []
discards: [  0 119 120 121 122 123 124 125 126 127 225 226 227 307 308]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 294 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 36s - loss: 595.0401 - loglik: -5.9078e+02 - logprior: -2.4393e+00
Epoch 2/10
43/43 - 33s - loss: 591.8780 - loglik: -5.8915e+02 - logprior: -9.6791e-01
Epoch 3/10
43/43 - 33s - loss: 588.2391 - loglik: -5.8566e+02 - logprior: -8.0364e-01
Epoch 4/10
43/43 - 33s - loss: 583.6475 - loglik: -5.8137e+02 - logprior: -6.4196e-01
Epoch 5/10
43/43 - 32s - loss: 587.8795 - loglik: -5.8589e+02 - logprior: -4.3691e-01
Fitted a model with MAP estimate = -583.4828
Time for alignment: 647.9222
Computed alignments with likelihoods: ['-576.3058', '-580.8658', '-583.4828']
Best model has likelihood: -576.3058
time for generating output: 0.4719
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf13.projection.fasta
SP score = 0.30993514632950026
Training of 3 independent models on file cryst.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f342410fca0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34eda26a30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34edf90af0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34eda23ca0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34eda23f40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34eda734f0>, <__main__.SimpleDirichletPrior object at 0x7f3436f17520>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3424347790>

Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 274.8440 - loglik: -2.3632e+02 - logprior: -3.8510e+01
Epoch 2/10
10/10 - 2s - loss: 224.3197 - loglik: -2.1387e+02 - logprior: -1.0427e+01
Epoch 3/10
10/10 - 2s - loss: 198.1420 - loglik: -1.9268e+02 - logprior: -5.4564e+00
Epoch 4/10
10/10 - 2s - loss: 182.8802 - loglik: -1.7898e+02 - logprior: -3.8454e+00
Epoch 5/10
10/10 - 2s - loss: 175.5074 - loglik: -1.7226e+02 - logprior: -3.0443e+00
Epoch 6/10
10/10 - 2s - loss: 172.0925 - loglik: -1.6908e+02 - logprior: -2.6459e+00
Epoch 7/10
10/10 - 2s - loss: 170.0920 - loglik: -1.6728e+02 - logprior: -2.4594e+00
Epoch 8/10
10/10 - 2s - loss: 168.8538 - loglik: -1.6612e+02 - logprior: -2.4246e+00
Epoch 9/10
10/10 - 2s - loss: 168.0678 - loglik: -1.6532e+02 - logprior: -2.4195e+00
Epoch 10/10
10/10 - 2s - loss: 167.1586 - loglik: -1.6441e+02 - logprior: -2.4131e+00
Fitted a model with MAP estimate = -166.7335
expansions: [(7, 2), (13, 1), (15, 2), (18, 1), (20, 1), (21, 1), (23, 2), (30, 1), (32, 2), (43, 1), (55, 3), (56, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 205.8005 - loglik: -1.6192e+02 - logprior: -4.3579e+01
Epoch 2/2
10/10 - 2s - loss: 172.8790 - loglik: -1.5444e+02 - logprior: -1.8152e+01
Fitted a model with MAP estimate = -166.8087
expansions: [(0, 2)]
discards: [ 0 17 42 68]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 184.3340 - loglik: -1.4945e+02 - logprior: -3.4579e+01
Epoch 2/2
10/10 - 2s - loss: 157.6745 - loglik: -1.4800e+02 - logprior: -9.3513e+00
Fitted a model with MAP estimate = -153.4808
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 189.5805 - loglik: -1.4916e+02 - logprior: -4.0102e+01
Epoch 2/10
10/10 - 2s - loss: 161.3022 - loglik: -1.4960e+02 - logprior: -1.1373e+01
Epoch 3/10
10/10 - 2s - loss: 154.0671 - loglik: -1.4906e+02 - logprior: -4.6743e+00
Epoch 4/10
10/10 - 2s - loss: 151.9720 - loglik: -1.4918e+02 - logprior: -2.4623e+00
Epoch 5/10
10/10 - 2s - loss: 150.3060 - loglik: -1.4846e+02 - logprior: -1.5013e+00
Epoch 6/10
10/10 - 2s - loss: 150.1233 - loglik: -1.4894e+02 - logprior: -8.5322e-01
Epoch 7/10
10/10 - 2s - loss: 149.9666 - loglik: -1.4934e+02 - logprior: -2.9097e-01
Epoch 8/10
10/10 - 2s - loss: 149.4410 - loglik: -1.4917e+02 - logprior: 0.0703
Epoch 9/10
10/10 - 2s - loss: 149.7256 - loglik: -1.4965e+02 - logprior: 0.2624
Fitted a model with MAP estimate = -149.0293
Time for alignment: 57.0627
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 274.8842 - loglik: -2.3636e+02 - logprior: -3.8510e+01
Epoch 2/10
10/10 - 2s - loss: 224.0760 - loglik: -2.1362e+02 - logprior: -1.0428e+01
Epoch 3/10
10/10 - 2s - loss: 197.7970 - loglik: -1.9234e+02 - logprior: -5.4421e+00
Epoch 4/10
10/10 - 2s - loss: 181.7838 - loglik: -1.7787e+02 - logprior: -3.8392e+00
Epoch 5/10
10/10 - 2s - loss: 174.3275 - loglik: -1.7096e+02 - logprior: -3.0952e+00
Epoch 6/10
10/10 - 2s - loss: 171.4010 - loglik: -1.6829e+02 - logprior: -2.7151e+00
Epoch 7/10
10/10 - 2s - loss: 169.9017 - loglik: -1.6700e+02 - logprior: -2.5509e+00
Epoch 8/10
10/10 - 2s - loss: 168.6653 - loglik: -1.6584e+02 - logprior: -2.4955e+00
Epoch 9/10
10/10 - 2s - loss: 168.3167 - loglik: -1.6554e+02 - logprior: -2.4429e+00
Epoch 10/10
10/10 - 2s - loss: 167.5687 - loglik: -1.6484e+02 - logprior: -2.3666e+00
Fitted a model with MAP estimate = -166.9778
expansions: [(7, 2), (13, 1), (15, 2), (18, 1), (20, 2), (24, 3), (25, 1), (32, 1), (43, 1), (55, 3), (58, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 205.6032 - loglik: -1.6175e+02 - logprior: -4.3553e+01
Epoch 2/2
10/10 - 2s - loss: 172.2894 - loglik: -1.5382e+02 - logprior: -1.8166e+01
Fitted a model with MAP estimate = -166.0464
expansions: [(0, 2)]
discards: [ 0 17 25 69]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 183.6005 - loglik: -1.4874e+02 - logprior: -3.4540e+01
Epoch 2/2
10/10 - 2s - loss: 157.2281 - loglik: -1.4762e+02 - logprior: -9.2894e+00
Fitted a model with MAP estimate = -153.1245
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 189.3086 - loglik: -1.4894e+02 - logprior: -4.0041e+01
Epoch 2/10
10/10 - 2s - loss: 160.9978 - loglik: -1.4936e+02 - logprior: -1.1313e+01
Epoch 3/10
10/10 - 2s - loss: 153.5327 - loglik: -1.4859e+02 - logprior: -4.6070e+00
Epoch 4/10
10/10 - 2s - loss: 151.5885 - loglik: -1.4885e+02 - logprior: -2.3975e+00
Epoch 5/10
10/10 - 2s - loss: 150.2355 - loglik: -1.4846e+02 - logprior: -1.4290e+00
Epoch 6/10
10/10 - 2s - loss: 150.1568 - loglik: -1.4903e+02 - logprior: -7.8123e-01
Epoch 7/10
10/10 - 2s - loss: 149.4235 - loglik: -1.4886e+02 - logprior: -2.1772e-01
Epoch 8/10
10/10 - 2s - loss: 149.3560 - loglik: -1.4916e+02 - logprior: 0.1462
Epoch 9/10
10/10 - 2s - loss: 149.0979 - loglik: -1.4909e+02 - logprior: 0.3419
Epoch 10/10
10/10 - 2s - loss: 149.0621 - loglik: -1.4920e+02 - logprior: 0.4866
Fitted a model with MAP estimate = -148.5993
Time for alignment: 57.2364
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 275.2027 - loglik: -2.3667e+02 - logprior: -3.8510e+01
Epoch 2/10
10/10 - 2s - loss: 223.7653 - loglik: -2.1331e+02 - logprior: -1.0428e+01
Epoch 3/10
10/10 - 2s - loss: 197.4841 - loglik: -1.9201e+02 - logprior: -5.4661e+00
Epoch 4/10
10/10 - 2s - loss: 181.5156 - loglik: -1.7762e+02 - logprior: -3.8404e+00
Epoch 5/10
10/10 - 2s - loss: 173.0544 - loglik: -1.6963e+02 - logprior: -3.1879e+00
Epoch 6/10
10/10 - 2s - loss: 169.8130 - loglik: -1.6649e+02 - logprior: -2.9498e+00
Epoch 7/10
10/10 - 2s - loss: 168.5924 - loglik: -1.6544e+02 - logprior: -2.8177e+00
Epoch 8/10
10/10 - 2s - loss: 167.8059 - loglik: -1.6498e+02 - logprior: -2.5281e+00
Epoch 9/10
10/10 - 2s - loss: 167.2650 - loglik: -1.6470e+02 - logprior: -2.2556e+00
Epoch 10/10
10/10 - 2s - loss: 166.8362 - loglik: -1.6436e+02 - logprior: -2.1493e+00
Fitted a model with MAP estimate = -166.2291
expansions: [(0, 2), (12, 1), (15, 2), (18, 1), (20, 2), (23, 3), (30, 1), (38, 1), (46, 1), (55, 3), (58, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 209.8442 - loglik: -1.5887e+02 - logprior: -5.0694e+01
Epoch 2/2
10/10 - 2s - loss: 166.8974 - loglik: -1.5075e+02 - logprior: -1.5855e+01
Fitted a model with MAP estimate = -158.8855
expansions: []
discards: [ 0 18 70]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 194.9577 - loglik: -1.5060e+02 - logprior: -4.4047e+01
Epoch 2/2
10/10 - 2s - loss: 168.1680 - loglik: -1.5020e+02 - logprior: -1.7658e+01
Fitted a model with MAP estimate = -163.5735
expansions: []
discards: [25]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 189.2386 - loglik: -1.4863e+02 - logprior: -4.0296e+01
Epoch 2/10
10/10 - 2s - loss: 160.3354 - loglik: -1.4851e+02 - logprior: -1.1496e+01
Epoch 3/10
10/10 - 2s - loss: 153.6760 - loglik: -1.4866e+02 - logprior: -4.6794e+00
Epoch 4/10
10/10 - 2s - loss: 151.2442 - loglik: -1.4848e+02 - logprior: -2.4231e+00
Epoch 5/10
10/10 - 2s - loss: 150.1965 - loglik: -1.4840e+02 - logprior: -1.4548e+00
Epoch 6/10
10/10 - 2s - loss: 149.5431 - loglik: -1.4840e+02 - logprior: -7.9734e-01
Epoch 7/10
10/10 - 2s - loss: 149.5400 - loglik: -1.4897e+02 - logprior: -2.3038e-01
Epoch 8/10
10/10 - 2s - loss: 149.4915 - loglik: -1.4927e+02 - logprior: 0.1229
Epoch 9/10
10/10 - 2s - loss: 149.0537 - loglik: -1.4901e+02 - logprior: 0.3049
Epoch 10/10
10/10 - 2s - loss: 148.7418 - loglik: -1.4886e+02 - logprior: 0.4597
Fitted a model with MAP estimate = -148.4721
Time for alignment: 56.2916
Computed alignments with likelihoods: ['-149.0293', '-148.5993', '-148.4721']
Best model has likelihood: -148.4721
time for generating output: 0.2114
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cryst.projection.fasta
SP score = 0.9135348968635207
Training of 3 independent models on file Ald_Xan_dh_2.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34ecc28e80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f347c787a60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34af6ca340>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329008e250>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34eca50430>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34493473a0>, <__main__.SimpleDirichletPrior object at 0x7f343fe67ee0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34ee241700>

Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 303.4582 - loglik: -2.9227e+02 - logprior: -1.1187e+01
Epoch 2/10
19/19 - 7s - loss: 239.8072 - loglik: -2.3737e+02 - logprior: -2.3991e+00
Epoch 3/10
19/19 - 6s - loss: 220.5594 - loglik: -2.1811e+02 - logprior: -2.2028e+00
Epoch 4/10
19/19 - 7s - loss: 215.6018 - loglik: -2.1332e+02 - logprior: -1.9795e+00
Epoch 5/10
19/19 - 7s - loss: 216.5201 - loglik: -2.1449e+02 - logprior: -1.7798e+00
Fitted a model with MAP estimate = -215.2518
expansions: [(10, 3), (11, 2), (12, 2), (15, 1), (27, 1), (29, 1), (34, 1), (48, 5), (49, 1), (57, 1), (72, 1), (73, 4), (74, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 222.7356 - loglik: -2.0847e+02 - logprior: -1.4040e+01
Epoch 2/2
19/19 - 6s - loss: 204.2478 - loglik: -1.9918e+02 - logprior: -4.7771e+00
Fitted a model with MAP estimate = -200.2581
expansions: [(0, 2)]
discards: [ 0 12 14 16 90]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 207.2211 - loglik: -1.9692e+02 - logprior: -9.9234e+00
Epoch 2/2
19/19 - 8s - loss: 198.6902 - loglik: -1.9653e+02 - logprior: -1.7602e+00
Fitted a model with MAP estimate = -196.5539
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 209.2893 - loglik: -1.9705e+02 - logprior: -1.1793e+01
Epoch 2/10
19/19 - 7s - loss: 199.3598 - loglik: -1.9692e+02 - logprior: -1.9765e+00
Epoch 3/10
19/19 - 7s - loss: 197.8522 - loglik: -1.9661e+02 - logprior: -7.6067e-01
Epoch 4/10
19/19 - 5s - loss: 194.9627 - loglik: -1.9410e+02 - logprior: -3.8085e-01
Epoch 5/10
19/19 - 8s - loss: 197.9144 - loglik: -1.9727e+02 - logprior: -1.6225e-01
Fitted a model with MAP estimate = -195.9229
Time for alignment: 126.8945
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 303.0576 - loglik: -2.9186e+02 - logprior: -1.1190e+01
Epoch 2/10
19/19 - 6s - loss: 240.1042 - loglik: -2.3760e+02 - logprior: -2.4578e+00
Epoch 3/10
19/19 - 6s - loss: 219.3706 - loglik: -2.1677e+02 - logprior: -2.3080e+00
Epoch 4/10
19/19 - 6s - loss: 215.1922 - loglik: -2.1273e+02 - logprior: -2.1168e+00
Epoch 5/10
19/19 - 7s - loss: 215.3383 - loglik: -2.1313e+02 - logprior: -1.9298e+00
Fitted a model with MAP estimate = -214.1514
expansions: [(10, 3), (11, 2), (12, 2), (15, 1), (26, 1), (29, 1), (43, 1), (48, 5), (49, 1), (57, 1), (58, 1), (72, 4), (73, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 222.0825 - loglik: -2.0775e+02 - logprior: -1.4058e+01
Epoch 2/2
19/19 - 6s - loss: 203.3025 - loglik: -1.9826e+02 - logprior: -4.6921e+00
Fitted a model with MAP estimate = -199.8212
expansions: [(0, 2)]
discards: [ 0 12 14 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 207.3771 - loglik: -1.9699e+02 - logprior: -9.9357e+00
Epoch 2/2
19/19 - 7s - loss: 197.6235 - loglik: -1.9539e+02 - logprior: -1.7877e+00
Fitted a model with MAP estimate = -196.3821
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 209.3721 - loglik: -1.9716e+02 - logprior: -1.1742e+01
Epoch 2/10
19/19 - 9s - loss: 199.2907 - loglik: -1.9683e+02 - logprior: -1.9713e+00
Epoch 3/10
19/19 - 6s - loss: 196.4742 - loglik: -1.9521e+02 - logprior: -7.6605e-01
Epoch 4/10
19/19 - 7s - loss: 196.3832 - loglik: -1.9552e+02 - logprior: -3.7509e-01
Epoch 5/10
19/19 - 8s - loss: 197.0725 - loglik: -1.9641e+02 - logprior: -1.6446e-01
Fitted a model with MAP estimate = -195.8984
Time for alignment: 127.5847
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 302.7457 - loglik: -2.9155e+02 - logprior: -1.1190e+01
Epoch 2/10
19/19 - 6s - loss: 240.5254 - loglik: -2.3800e+02 - logprior: -2.4779e+00
Epoch 3/10
19/19 - 8s - loss: 220.7806 - loglik: -2.1815e+02 - logprior: -2.3810e+00
Epoch 4/10
19/19 - 7s - loss: 216.4683 - loglik: -2.1399e+02 - logprior: -2.1685e+00
Epoch 5/10
19/19 - 5s - loss: 213.1554 - loglik: -2.1094e+02 - logprior: -1.9638e+00
Epoch 6/10
19/19 - 8s - loss: 216.0171 - loglik: -2.1379e+02 - logprior: -1.9788e+00
Fitted a model with MAP estimate = -213.9241
expansions: [(10, 3), (11, 2), (12, 2), (15, 1), (26, 1), (29, 1), (34, 1), (48, 6), (49, 1), (57, 1), (72, 1), (73, 3), (74, 1), (76, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 220.6395 - loglik: -2.0642e+02 - logprior: -1.3987e+01
Epoch 2/2
19/19 - 7s - loss: 204.6933 - loglik: -1.9969e+02 - logprior: -4.7019e+00
Fitted a model with MAP estimate = -199.7399
expansions: [(0, 2)]
discards: [ 0 12 14 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 207.0358 - loglik: -1.9673e+02 - logprior: -9.8959e+00
Epoch 2/2
19/19 - 7s - loss: 197.3157 - loglik: -1.9517e+02 - logprior: -1.7373e+00
Fitted a model with MAP estimate = -196.1074
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 209.4794 - loglik: -1.9739e+02 - logprior: -1.1642e+01
Epoch 2/10
19/19 - 7s - loss: 198.6564 - loglik: -1.9627e+02 - logprior: -1.9204e+00
Epoch 3/10
19/19 - 7s - loss: 197.2365 - loglik: -1.9601e+02 - logprior: -7.4765e-01
Epoch 4/10
19/19 - 7s - loss: 196.5288 - loglik: -1.9571e+02 - logprior: -3.2576e-01
Epoch 5/10
19/19 - 6s - loss: 195.6904 - loglik: -1.9508e+02 - logprior: -1.2326e-01
Epoch 6/10
19/19 - 7s - loss: 196.1720 - loglik: -1.9567e+02 - logprior: -4.5329e-03
Fitted a model with MAP estimate = -195.2995
Time for alignment: 141.6681
Computed alignments with likelihoods: ['-195.9229', '-195.8984', '-195.2995']
Best model has likelihood: -195.2995
time for generating output: 0.5662
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Ald_Xan_dh_2.projection.fasta
SP score = 0.21122793841532106
Training of 3 independent models on file ace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34eccf3a30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f343fe67b50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f343fe5d310>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f33a429a0a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33a429a5b0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f33a4299f70>, <__main__.SimpleDirichletPrior object at 0x7f34ee9c0f70>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34a6b22790>

Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 53s - loss: 1105.2706 - loglik: -1.1028e+03 - logprior: -2.0348e+00
Epoch 2/10
49/49 - 48s - loss: 952.1291 - loglik: -9.4901e+02 - logprior: -1.5567e+00
Epoch 3/10
49/49 - 48s - loss: 941.5182 - loglik: -9.3721e+02 - logprior: -1.7510e+00
Epoch 4/10
49/49 - 48s - loss: 934.2640 - loglik: -9.2972e+02 - logprior: -1.9718e+00
Epoch 5/10
49/49 - 48s - loss: 934.3149 - loglik: -9.2994e+02 - logprior: -2.0402e+00
Fitted a model with MAP estimate = -928.2039
expansions: [(0, 3), (132, 1), (142, 1), (187, 1), (215, 1), (216, 1), (230, 1), (232, 5), (233, 1), (234, 1), (235, 1), (239, 1), (243, 2), (245, 1), (252, 1), (253, 1), (254, 1), (257, 1), (258, 3), (259, 5), (261, 1), (275, 1), (276, 2), (277, 3), (278, 5), (285, 1), (293, 2), (296, 2), (299, 2), (301, 7), (303, 1), (313, 2), (314, 1), (316, 3), (317, 1), (318, 1), (324, 1), (326, 1), (327, 2), (328, 2), (329, 1), (332, 1), (344, 1), (345, 5), (346, 1), (372, 1), (373, 2), (374, 1), (375, 1), (377, 1), (389, 2), (390, 2), (398, 10)]
discards: [35 36 37]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 504 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 71s - loss: 907.7302 - loglik: -9.0049e+02 - logprior: -4.8199e+00
Epoch 2/2
49/49 - 66s - loss: 890.8751 - loglik: -8.8732e+02 - logprior: -2.9553e-01
Fitted a model with MAP estimate = -882.4348
expansions: [(0, 2), (496, 4), (499, 1)]
discards: [  1   2  22  23  24 111 371 394 395 478]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 501 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 69s - loss: 889.3708 - loglik: -8.8176e+02 - logprior: -4.1880e+00
Epoch 2/2
49/49 - 66s - loss: 882.4219 - loglik: -8.7983e+02 - logprior: 0.7240
Fitted a model with MAP estimate = -875.3363
expansions: [(0, 3), (335, 5), (389, 1), (486, 1), (487, 1), (491, 1)]
discards: [500]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 512 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 72s - loss: 883.6432 - loglik: -8.7763e+02 - logprior: -3.1260e+00
Epoch 2/10
49/49 - 68s - loss: 870.3273 - loglik: -8.7035e+02 - logprior: 2.5324
Epoch 3/10
49/49 - 68s - loss: 872.1606 - loglik: -8.7340e+02 - logprior: 3.4396
Fitted a model with MAP estimate = -866.8865
Time for alignment: 905.5470
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 52s - loss: 1106.3538 - loglik: -1.1038e+03 - logprior: -2.1116e+00
Epoch 2/10
49/49 - 48s - loss: 955.1826 - loglik: -9.5227e+02 - logprior: -1.2966e+00
Epoch 3/10
49/49 - 48s - loss: 945.4036 - loglik: -9.4132e+02 - logprior: -1.5646e+00
Epoch 4/10
49/49 - 48s - loss: 937.6739 - loglik: -9.3304e+02 - logprior: -1.9278e+00
Epoch 5/10
49/49 - 48s - loss: 933.8941 - loglik: -9.2938e+02 - logprior: -2.0501e+00
Epoch 6/10
49/49 - 49s - loss: 936.8599 - loglik: -9.3253e+02 - logprior: -2.1812e+00
Fitted a model with MAP estimate = -929.5619
expansions: [(0, 3), (130, 1), (140, 1), (180, 1), (181, 1), (183, 1), (211, 1), (212, 1), (226, 1), (228, 5), (229, 3), (230, 1), (233, 2), (237, 2), (238, 2), (239, 1), (245, 1), (247, 1), (250, 1), (251, 3), (252, 5), (268, 1), (269, 1), (270, 1), (271, 6), (292, 4), (293, 1), (294, 1), (296, 1), (299, 9), (303, 1), (308, 2), (309, 1), (310, 3), (311, 1), (313, 1), (318, 1), (320, 1), (321, 1), (322, 2), (323, 1), (327, 1), (341, 3), (343, 1), (364, 4), (365, 1), (366, 1), (367, 1), (368, 1), (369, 2), (394, 5), (395, 4)]
discards: [ 35 381 382]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 503 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 70s - loss: 906.2118 - loglik: -8.9977e+02 - logprior: -4.3067e+00
Epoch 2/2
49/49 - 67s - loss: 885.4913 - loglik: -8.8216e+02 - logprior: -2.5407e-01
Fitted a model with MAP estimate = -880.0484
expansions: [(0, 2), (314, 1), (376, 1), (491, 1), (494, 2)]
discards: [  1   2   3  23  24  25 372]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 503 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 69s - loss: 886.8182 - loglik: -8.7956e+02 - logprior: -4.0320e+00
Epoch 2/2
49/49 - 66s - loss: 877.9910 - loglik: -8.7585e+02 - logprior: 0.7727
Fitted a model with MAP estimate = -873.9423
expansions: [(0, 3), (34, 3), (283, 1), (440, 1), (490, 2), (493, 1)]
discards: [469]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 513 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 73s - loss: 879.0435 - loglik: -8.7356e+02 - logprior: -2.9746e+00
Epoch 2/10
49/49 - 68s - loss: 873.7524 - loglik: -8.7400e+02 - logprior: 2.5613
Epoch 3/10
49/49 - 69s - loss: 869.0903 - loglik: -8.7051e+02 - logprior: 3.4124
Epoch 4/10
49/49 - 68s - loss: 866.3043 - loglik: -8.6871e+02 - logprior: 4.2665
Epoch 5/10
49/49 - 68s - loss: 870.0834 - loglik: -8.7361e+02 - logprior: 5.2623
Fitted a model with MAP estimate = -864.8246
Time for alignment: 1089.6524
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 52s - loss: 1094.9834 - loglik: -1.0927e+03 - logprior: -1.9000e+00
Epoch 2/10
49/49 - 48s - loss: 952.7985 - loglik: -9.5001e+02 - logprior: -1.1878e+00
Epoch 3/10
49/49 - 48s - loss: 935.3700 - loglik: -9.3146e+02 - logprior: -1.4625e+00
Epoch 4/10
49/49 - 48s - loss: 936.5172 - loglik: -9.3266e+02 - logprior: -1.4881e+00
Fitted a model with MAP estimate = -928.9269
expansions: [(0, 3), (142, 1), (216, 1), (236, 3), (237, 1), (238, 1), (239, 1), (245, 1), (249, 1), (252, 1), (259, 1), (260, 1), (261, 1), (264, 1), (265, 3), (266, 6), (280, 1), (281, 2), (282, 7), (299, 2), (300, 5), (301, 2), (302, 1), (306, 1), (309, 9), (310, 2), (314, 3), (315, 1), (316, 5), (317, 2), (318, 1), (321, 1), (323, 1), (324, 2), (325, 2), (326, 1), (344, 5), (345, 1), (363, 1), (392, 5), (394, 7), (395, 5), (404, 4)]
discards: [34 35]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 508 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 71s - loss: 912.3711 - loglik: -9.0520e+02 - logprior: -4.6595e+00
Epoch 2/2
49/49 - 68s - loss: 893.7706 - loglik: -8.8956e+02 - logprior: -1.0062e+00
Fitted a model with MAP estimate = -885.8697
expansions: [(0, 2), (308, 1), (374, 1), (428, 1), (487, 1), (488, 1)]
discards: [  1   2   3  22  23 240 398 503 504 505 506 507]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 503 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 70s - loss: 896.3360 - loglik: -8.8916e+02 - logprior: -3.9466e+00
Epoch 2/2
49/49 - 66s - loss: 881.8472 - loglik: -8.7926e+02 - logprior: 0.8636
Fitted a model with MAP estimate = -877.3603
expansions: [(0, 3), (283, 3), (503, 4)]
discards: [379]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 512 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 72s - loss: 883.9941 - loglik: -8.7737e+02 - logprior: -2.9984e+00
Epoch 2/10
49/49 - 69s - loss: 876.2380 - loglik: -8.7552e+02 - logprior: 2.3284
Epoch 3/10
49/49 - 68s - loss: 873.8109 - loglik: -8.7446e+02 - logprior: 3.1685
Epoch 4/10
49/49 - 68s - loss: 869.5406 - loglik: -8.7153e+02 - logprior: 4.1535
Epoch 5/10
49/49 - 68s - loss: 871.2912 - loglik: -8.7462e+02 - logprior: 5.3115
Fitted a model with MAP estimate = -865.6703
Time for alignment: 996.2728
Computed alignments with likelihoods: ['-866.8865', '-864.8246', '-865.6703']
Best model has likelihood: -864.8246
time for generating output: 0.4878
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ace.projection.fasta
SP score = 0.7947452654041077
Training of 3 independent models on file tRNA-synt_2b.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f338a606ac0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34243758b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f346b7bbee0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f338a548f10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34ed97e640>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f338a67afd0>, <__main__.SimpleDirichletPrior object at 0x7f33a3dbe040>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3462b4df70>

Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 477.8526 - loglik: -4.7555e+02 - logprior: -2.0504e+00
Epoch 2/10
39/39 - 12s - loss: 409.2433 - loglik: -4.0670e+02 - logprior: -1.7922e+00
Epoch 3/10
39/39 - 11s - loss: 401.9863 - loglik: -3.9933e+02 - logprior: -1.8226e+00
Epoch 4/10
39/39 - 12s - loss: 399.2998 - loglik: -3.9661e+02 - logprior: -1.8891e+00
Epoch 5/10
39/39 - 11s - loss: 397.8153 - loglik: -3.9518e+02 - logprior: -1.9241e+00
Epoch 6/10
39/39 - 12s - loss: 397.4272 - loglik: -3.9486e+02 - logprior: -1.9303e+00
Epoch 7/10
39/39 - 11s - loss: 397.2359 - loglik: -3.9472e+02 - logprior: -1.9303e+00
Epoch 8/10
39/39 - 11s - loss: 396.7626 - loglik: -3.9430e+02 - logprior: -1.9266e+00
Epoch 9/10
39/39 - 11s - loss: 396.9125 - loglik: -3.9447e+02 - logprior: -1.9310e+00
Fitted a model with MAP estimate = -397.0493
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (32, 1), (40, 2), (45, 1), (46, 1), (55, 1), (56, 1), (57, 2), (58, 2), (59, 1), (69, 2), (71, 2), (89, 2), (92, 1), (93, 2), (95, 1), (99, 1), (103, 2), (104, 1), (105, 1), (107, 1), (109, 1), (118, 1), (122, 2), (126, 1), (131, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 177 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 388.5475 - loglik: -3.8483e+02 - logprior: -3.2107e+00
Epoch 2/2
39/39 - 14s - loss: 375.6467 - loglik: -3.7374e+02 - logprior: -1.3197e+00
Fitted a model with MAP estimate = -376.7739
expansions: []
discards: [ 13  72  74  88  92 112 119 134]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 375.4072 - loglik: -3.7257e+02 - logprior: -2.2174e+00
Epoch 2/2
39/39 - 14s - loss: 373.5019 - loglik: -3.7192e+02 - logprior: -9.6206e-01
Fitted a model with MAP estimate = -376.4164
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 18s - loss: 376.5926 - loglik: -3.7407e+02 - logprior: -1.7894e+00
Epoch 2/10
41/41 - 14s - loss: 374.3540 - loglik: -3.7283e+02 - logprior: -7.2827e-01
Epoch 3/10
41/41 - 14s - loss: 372.9474 - loglik: -3.7153e+02 - logprior: -6.4505e-01
Epoch 4/10
41/41 - 14s - loss: 371.6786 - loglik: -3.7032e+02 - logprior: -5.7698e-01
Epoch 5/10
41/41 - 14s - loss: 372.5966 - loglik: -3.7133e+02 - logprior: -5.0859e-01
Fitted a model with MAP estimate = -370.4854
Time for alignment: 312.5210
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 479.7767 - loglik: -4.7751e+02 - logprior: -2.0211e+00
Epoch 2/10
39/39 - 12s - loss: 411.3488 - loglik: -4.0883e+02 - logprior: -1.7456e+00
Epoch 3/10
39/39 - 12s - loss: 403.2196 - loglik: -4.0044e+02 - logprior: -1.7603e+00
Epoch 4/10
39/39 - 12s - loss: 401.0918 - loglik: -3.9842e+02 - logprior: -1.7308e+00
Epoch 5/10
39/39 - 12s - loss: 399.3293 - loglik: -3.9675e+02 - logprior: -1.7362e+00
Epoch 6/10
39/39 - 11s - loss: 398.4564 - loglik: -3.9597e+02 - logprior: -1.7407e+00
Epoch 7/10
39/39 - 12s - loss: 398.3487 - loglik: -3.9594e+02 - logprior: -1.7455e+00
Epoch 8/10
39/39 - 12s - loss: 397.4922 - loglik: -3.9514e+02 - logprior: -1.7457e+00
Epoch 9/10
39/39 - 12s - loss: 397.8987 - loglik: -3.9558e+02 - logprior: -1.7474e+00
Fitted a model with MAP estimate = -397.9777
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (33, 1), (40, 2), (42, 1), (46, 1), (56, 2), (58, 2), (59, 1), (60, 2), (70, 2), (72, 2), (77, 1), (80, 2), (89, 2), (94, 2), (96, 1), (101, 2), (102, 1), (104, 2), (105, 3), (107, 2), (109, 1), (118, 1), (119, 1), (122, 1), (126, 1), (128, 1), (131, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 388.7267 - loglik: -3.8487e+02 - logprior: -3.3047e+00
Epoch 2/2
39/39 - 15s - loss: 375.6016 - loglik: -3.7350e+02 - logprior: -1.5053e+00
Fitted a model with MAP estimate = -376.5573
expansions: []
discards: [ 13  68  72  77  89  93 104 105 115 122 135 138 146]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 377.3659 - loglik: -3.7447e+02 - logprior: -2.2698e+00
Epoch 2/2
39/39 - 14s - loss: 375.3599 - loglik: -3.7374e+02 - logprior: -9.9545e-01
Fitted a model with MAP estimate = -378.2507
expansions: [(98, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 173 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 18s - loss: 377.4214 - loglik: -3.7480e+02 - logprior: -1.8374e+00
Epoch 2/10
41/41 - 14s - loss: 373.3794 - loglik: -3.7179e+02 - logprior: -7.5095e-01
Epoch 3/10
41/41 - 14s - loss: 373.0185 - loglik: -3.7148e+02 - logprior: -6.8605e-01
Epoch 4/10
41/41 - 14s - loss: 370.6771 - loglik: -3.6919e+02 - logprior: -6.1995e-01
Epoch 5/10
41/41 - 14s - loss: 371.7948 - loglik: -3.7041e+02 - logprior: -5.4122e-01
Fitted a model with MAP estimate = -369.5726
Time for alignment: 312.5529
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 477.2752 - loglik: -4.7501e+02 - logprior: -2.0272e+00
Epoch 2/10
39/39 - 12s - loss: 408.2611 - loglik: -4.0557e+02 - logprior: -1.7453e+00
Epoch 3/10
39/39 - 12s - loss: 401.2610 - loglik: -3.9861e+02 - logprior: -1.7716e+00
Epoch 4/10
39/39 - 11s - loss: 399.2115 - loglik: -3.9658e+02 - logprior: -1.7665e+00
Epoch 5/10
39/39 - 12s - loss: 398.1252 - loglik: -3.9555e+02 - logprior: -1.7778e+00
Epoch 6/10
39/39 - 11s - loss: 397.5809 - loglik: -3.9508e+02 - logprior: -1.7978e+00
Epoch 7/10
39/39 - 11s - loss: 396.8415 - loglik: -3.9441e+02 - logprior: -1.7970e+00
Epoch 8/10
39/39 - 12s - loss: 397.0434 - loglik: -3.9465e+02 - logprior: -1.7990e+00
Fitted a model with MAP estimate = -396.8265
expansions: [(9, 2), (10, 2), (11, 2), (12, 2), (20, 1), (22, 1), (23, 2), (25, 1), (39, 2), (40, 2), (43, 1), (44, 1), (54, 2), (56, 2), (57, 2), (58, 2), (71, 2), (89, 2), (92, 1), (93, 1), (95, 1), (99, 1), (103, 2), (104, 3), (106, 2), (118, 1), (119, 1), (122, 1), (126, 1), (131, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 389.9575 - loglik: -3.8611e+02 - logprior: -3.2632e+00
Epoch 2/2
39/39 - 14s - loss: 377.6604 - loglik: -3.7561e+02 - logprior: -1.3784e+00
Fitted a model with MAP estimate = -378.5197
expansions: []
discards: [  8  11  17  32  54  72  77  79  82  97 117 137 145]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 378.3090 - loglik: -3.7538e+02 - logprior: -2.2186e+00
Epoch 2/2
39/39 - 14s - loss: 376.3609 - loglik: -3.7475e+02 - logprior: -9.3816e-01
Fitted a model with MAP estimate = -378.8254
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 18s - loss: 379.0403 - loglik: -3.7643e+02 - logprior: -1.8007e+00
Epoch 2/10
41/41 - 14s - loss: 375.8276 - loglik: -3.7425e+02 - logprior: -7.2805e-01
Epoch 3/10
41/41 - 14s - loss: 375.5277 - loglik: -3.7406e+02 - logprior: -6.4389e-01
Epoch 4/10
41/41 - 14s - loss: 375.8470 - loglik: -3.7443e+02 - logprior: -5.8482e-01
Fitted a model with MAP estimate = -373.4419
Time for alignment: 284.6254
Computed alignments with likelihoods: ['-370.4854', '-369.5726', '-373.4419']
Best model has likelihood: -369.5726
time for generating output: 0.3297
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tRNA-synt_2b.projection.fasta
SP score = 0.4914004914004914
Training of 3 independent models on file ChtBD.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f344013aeb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f349596e1f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f349596ed60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34b7bebe50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b7be23a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34eeb2aa30>, <__main__.SimpleDirichletPrior object at 0x7f342d9ae820>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34ee80c4c0>

Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 170.8494 - loglik: -1.1165e+02 - logprior: -5.9181e+01
Epoch 2/10
10/10 - 1s - loss: 106.0847 - loglik: -8.9227e+01 - logprior: -1.6849e+01
Epoch 3/10
10/10 - 1s - loss: 80.8772 - loglik: -7.2318e+01 - logprior: -8.5241e+00
Epoch 4/10
10/10 - 1s - loss: 70.6668 - loglik: -6.5190e+01 - logprior: -5.4277e+00
Epoch 5/10
10/10 - 1s - loss: 65.7192 - loglik: -6.1889e+01 - logprior: -3.8121e+00
Epoch 6/10
10/10 - 1s - loss: 62.8932 - loglik: -5.9784e+01 - logprior: -3.0679e+00
Epoch 7/10
10/10 - 1s - loss: 61.8605 - loglik: -5.8962e+01 - logprior: -2.6975e+00
Epoch 8/10
10/10 - 1s - loss: 61.4119 - loglik: -5.8661e+01 - logprior: -2.4247e+00
Epoch 9/10
10/10 - 1s - loss: 61.0116 - loglik: -5.8475e+01 - logprior: -2.2312e+00
Epoch 10/10
10/10 - 1s - loss: 60.9406 - loglik: -5.8594e+01 - logprior: -2.0722e+00
Fitted a model with MAP estimate = -60.4942
expansions: [(0, 4), (10, 2), (26, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 136.1121 - loglik: -5.6066e+01 - logprior: -7.9778e+01
Epoch 2/2
10/10 - 1s - loss: 78.7733 - loglik: -5.2296e+01 - logprior: -2.6198e+01
Fitted a model with MAP estimate = -67.6635
expansions: [(0, 2)]
discards: [14 32]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 116.0387 - loglik: -4.9334e+01 - logprior: -6.6398e+01
Epoch 2/2
10/10 - 1s - loss: 73.2697 - loglik: -4.9301e+01 - logprior: -2.3654e+01
Fitted a model with MAP estimate = -63.9582
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 116.2299 - loglik: -4.9457e+01 - logprior: -6.6448e+01
Epoch 2/10
10/10 - 1s - loss: 78.2441 - loglik: -5.0060e+01 - logprior: -2.7858e+01
Epoch 3/10
10/10 - 1s - loss: 67.2902 - loglik: -5.0622e+01 - logprior: -1.6327e+01
Epoch 4/10
10/10 - 1s - loss: 58.0971 - loglik: -5.0909e+01 - logprior: -6.8331e+00
Epoch 5/10
10/10 - 1s - loss: 54.9741 - loglik: -5.1278e+01 - logprior: -3.3342e+00
Epoch 6/10
10/10 - 1s - loss: 53.7037 - loglik: -5.1180e+01 - logprior: -2.1706e+00
Epoch 7/10
10/10 - 1s - loss: 53.1361 - loglik: -5.1212e+01 - logprior: -1.5817e+00
Epoch 8/10
10/10 - 1s - loss: 52.9128 - loglik: -5.1348e+01 - logprior: -1.2273e+00
Epoch 9/10
10/10 - 1s - loss: 52.6593 - loglik: -5.1352e+01 - logprior: -9.7004e-01
Epoch 10/10
10/10 - 1s - loss: 52.5722 - loglik: -5.1477e+01 - logprior: -7.4961e-01
Fitted a model with MAP estimate = -52.1378
Time for alignment: 29.2614
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 170.9221 - loglik: -1.1173e+02 - logprior: -5.9180e+01
Epoch 2/10
10/10 - 1s - loss: 105.8394 - loglik: -8.8982e+01 - logprior: -1.6849e+01
Epoch 3/10
10/10 - 1s - loss: 81.1142 - loglik: -7.2552e+01 - logprior: -8.5272e+00
Epoch 4/10
10/10 - 1s - loss: 70.5262 - loglik: -6.5024e+01 - logprior: -5.4526e+00
Epoch 5/10
10/10 - 1s - loss: 66.1820 - loglik: -6.2272e+01 - logprior: -3.8926e+00
Epoch 6/10
10/10 - 1s - loss: 64.2701 - loglik: -6.1083e+01 - logprior: -3.1528e+00
Epoch 7/10
10/10 - 1s - loss: 63.3945 - loglik: -6.0458e+01 - logprior: -2.7597e+00
Epoch 8/10
10/10 - 1s - loss: 62.8209 - loglik: -6.0052e+01 - logprior: -2.4671e+00
Epoch 9/10
10/10 - 1s - loss: 62.7463 - loglik: -6.0179e+01 - logprior: -2.2831e+00
Epoch 10/10
10/10 - 1s - loss: 62.3460 - loglik: -5.9903e+01 - logprior: -2.1898e+00
Fitted a model with MAP estimate = -61.9140
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 137.1111 - loglik: -5.7283e+01 - logprior: -7.9584e+01
Epoch 2/2
10/10 - 1s - loss: 78.9742 - loglik: -5.2553e+01 - logprior: -2.6154e+01
Fitted a model with MAP estimate = -67.9434
expansions: [(0, 2)]
discards: [14 32]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 116.3193 - loglik: -4.9754e+01 - logprior: -6.6268e+01
Epoch 2/2
10/10 - 1s - loss: 73.2844 - loglik: -4.9320e+01 - logprior: -2.3643e+01
Fitted a model with MAP estimate = -64.1012
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 109.4269 - loglik: -4.9431e+01 - logprior: -5.9664e+01
Epoch 2/10
10/10 - 1s - loss: 68.1644 - loglik: -5.0622e+01 - logprior: -1.7216e+01
Epoch 3/10
10/10 - 1s - loss: 60.0302 - loglik: -5.1557e+01 - logprior: -8.1446e+00
Epoch 4/10
10/10 - 1s - loss: 57.2818 - loglik: -5.2216e+01 - logprior: -4.7361e+00
Epoch 5/10
10/10 - 1s - loss: 55.5315 - loglik: -5.2123e+01 - logprior: -3.0774e+00
Epoch 6/10
10/10 - 1s - loss: 54.5336 - loglik: -5.2066e+01 - logprior: -2.1362e+00
Epoch 7/10
10/10 - 1s - loss: 53.9962 - loglik: -5.2083e+01 - logprior: -1.5838e+00
Epoch 8/10
10/10 - 1s - loss: 53.6814 - loglik: -5.2122e+01 - logprior: -1.2285e+00
Epoch 9/10
10/10 - 1s - loss: 53.5647 - loglik: -5.2281e+01 - logprior: -9.5683e-01
Epoch 10/10
10/10 - 1s - loss: 53.2341 - loglik: -5.2180e+01 - logprior: -7.2566e-01
Fitted a model with MAP estimate = -52.9096
Time for alignment: 29.3468
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 170.8831 - loglik: -1.1169e+02 - logprior: -5.9180e+01
Epoch 2/10
10/10 - 1s - loss: 105.9708 - loglik: -8.9111e+01 - logprior: -1.6851e+01
Epoch 3/10
10/10 - 1s - loss: 80.9152 - loglik: -7.2354e+01 - logprior: -8.5263e+00
Epoch 4/10
10/10 - 1s - loss: 70.5961 - loglik: -6.5101e+01 - logprior: -5.4456e+00
Epoch 5/10
10/10 - 1s - loss: 66.0410 - loglik: -6.2135e+01 - logprior: -3.8873e+00
Epoch 6/10
10/10 - 1s - loss: 63.5131 - loglik: -6.0331e+01 - logprior: -3.1465e+00
Epoch 7/10
10/10 - 1s - loss: 62.1538 - loglik: -5.9205e+01 - logprior: -2.7625e+00
Epoch 8/10
10/10 - 1s - loss: 61.6870 - loglik: -5.8875e+01 - logprior: -2.4932e+00
Epoch 9/10
10/10 - 1s - loss: 61.3756 - loglik: -5.8797e+01 - logprior: -2.2860e+00
Epoch 10/10
10/10 - 1s - loss: 61.1780 - loglik: -5.8807e+01 - logprior: -2.1200e+00
Fitted a model with MAP estimate = -60.8016
expansions: [(0, 4), (10, 2), (21, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 136.0911 - loglik: -5.6015e+01 - logprior: -7.9821e+01
Epoch 2/2
10/10 - 1s - loss: 78.5514 - loglik: -5.2267e+01 - logprior: -2.6015e+01
Fitted a model with MAP estimate = -67.4189
expansions: [(0, 2)]
discards: [14]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 115.9680 - loglik: -4.9299e+01 - logprior: -6.6368e+01
Epoch 2/2
10/10 - 1s - loss: 73.2341 - loglik: -4.9272e+01 - logprior: -2.3647e+01
Fitted a model with MAP estimate = -63.9146
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 38 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 109.2107 - loglik: -4.9249e+01 - logprior: -5.9638e+01
Epoch 2/10
10/10 - 1s - loss: 67.9307 - loglik: -5.0377e+01 - logprior: -1.7226e+01
Epoch 3/10
10/10 - 1s - loss: 59.9756 - loglik: -5.1461e+01 - logprior: -8.1874e+00
Epoch 4/10
10/10 - 1s - loss: 57.1396 - loglik: -5.2033e+01 - logprior: -4.7798e+00
Epoch 5/10
10/10 - 1s - loss: 55.6233 - loglik: -5.2171e+01 - logprior: -3.1230e+00
Epoch 6/10
10/10 - 1s - loss: 54.5595 - loglik: -5.2041e+01 - logprior: -2.1917e+00
Epoch 7/10
10/10 - 1s - loss: 53.8598 - loglik: -5.1883e+01 - logprior: -1.6493e+00
Epoch 8/10
10/10 - 1s - loss: 53.6421 - loglik: -5.2026e+01 - logprior: -1.2907e+00
Epoch 9/10
10/10 - 1s - loss: 53.3541 - loglik: -5.2010e+01 - logprior: -1.0208e+00
Epoch 10/10
10/10 - 1s - loss: 53.2315 - loglik: -5.2117e+01 - logprior: -7.8963e-01
Fitted a model with MAP estimate = -52.8350
Time for alignment: 29.6783
Computed alignments with likelihoods: ['-52.1378', '-52.9096', '-52.8350']
Best model has likelihood: -52.1378
time for generating output: 0.1126
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ChtBD.projection.fasta
SP score = 0.966183574879227
Training of 3 independent models on file adh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f342da47f40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f33a455f160>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34ee2e7a90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34b7a2df40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b7a2d520>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f348d5b18e0>, <__main__.SimpleDirichletPrior object at 0x7f3436d95400>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34eec91a60>

Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 372.7291 - loglik: -3.6974e+02 - logprior: -2.8696e+00
Epoch 2/10
20/20 - 4s - loss: 327.1253 - loglik: -3.2510e+02 - logprior: -1.3231e+00
Epoch 3/10
20/20 - 4s - loss: 306.9749 - loglik: -3.0438e+02 - logprior: -1.6378e+00
Epoch 4/10
20/20 - 4s - loss: 302.1385 - loglik: -2.9940e+02 - logprior: -1.6265e+00
Epoch 5/10
20/20 - 4s - loss: 299.8763 - loglik: -2.9717e+02 - logprior: -1.6396e+00
Epoch 6/10
20/20 - 4s - loss: 298.6127 - loglik: -2.9604e+02 - logprior: -1.6139e+00
Epoch 7/10
20/20 - 4s - loss: 297.7152 - loglik: -2.9523e+02 - logprior: -1.6073e+00
Epoch 8/10
20/20 - 4s - loss: 297.8603 - loglik: -2.9548e+02 - logprior: -1.5871e+00
Fitted a model with MAP estimate = -288.4247
expansions: [(5, 1), (8, 2), (10, 2), (13, 2), (19, 1), (20, 1), (31, 1), (35, 1), (36, 2), (38, 1), (39, 1), (46, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 2), (64, 2), (76, 1), (78, 2), (81, 2), (87, 1), (91, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 137 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 292.4253 - loglik: -2.8870e+02 - logprior: -2.9338e+00
Epoch 2/2
40/40 - 7s - loss: 282.8033 - loglik: -2.8080e+02 - logprior: -1.1911e+00
Fitted a model with MAP estimate = -282.1228
expansions: [(137, 2)]
discards: [  8  12  46  81  85 124]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 133 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 12s - loss: 283.0784 - loglik: -2.7998e+02 - logprior: -2.2117e+00
Epoch 2/2
40/40 - 7s - loss: 280.9550 - loglik: -2.7908e+02 - logprior: -1.0598e+00
Fitted a model with MAP estimate = -284.7447
expansions: []
discards: [ 98 132]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 13s - loss: 275.9335 - loglik: -2.7322e+02 - logprior: -1.1667e+00
Epoch 2/10
57/57 - 9s - loss: 269.5947 - loglik: -2.6722e+02 - logprior: -7.7436e-01
Epoch 3/10
57/57 - 9s - loss: 266.9177 - loglik: -2.6459e+02 - logprior: -7.4577e-01
Epoch 4/10
57/57 - 9s - loss: 264.4661 - loglik: -2.6225e+02 - logprior: -7.3855e-01
Epoch 5/10
57/57 - 9s - loss: 263.9952 - loglik: -2.6192e+02 - logprior: -7.0643e-01
Epoch 6/10
57/57 - 9s - loss: 263.3762 - loglik: -2.6145e+02 - logprior: -6.7046e-01
Epoch 7/10
57/57 - 9s - loss: 262.9139 - loglik: -2.6120e+02 - logprior: -6.3764e-01
Epoch 8/10
57/57 - 10s - loss: 263.1091 - loglik: -2.6150e+02 - logprior: -5.9525e-01
Fitted a model with MAP estimate = -261.1753
Time for alignment: 202.8085
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 372.7947 - loglik: -3.6980e+02 - logprior: -2.8728e+00
Epoch 2/10
20/20 - 4s - loss: 326.3911 - loglik: -3.2435e+02 - logprior: -1.3184e+00
Epoch 3/10
20/20 - 4s - loss: 308.0158 - loglik: -3.0532e+02 - logprior: -1.5769e+00
Epoch 4/10
20/20 - 4s - loss: 302.9504 - loglik: -3.0016e+02 - logprior: -1.5722e+00
Epoch 5/10
20/20 - 4s - loss: 300.7437 - loglik: -2.9811e+02 - logprior: -1.6248e+00
Epoch 6/10
20/20 - 4s - loss: 299.2838 - loglik: -2.9681e+02 - logprior: -1.6056e+00
Epoch 7/10
20/20 - 4s - loss: 298.4464 - loglik: -2.9605e+02 - logprior: -1.5878e+00
Epoch 8/10
20/20 - 4s - loss: 298.1278 - loglik: -2.9582e+02 - logprior: -1.5823e+00
Epoch 9/10
20/20 - 5s - loss: 297.6158 - loglik: -2.9536e+02 - logprior: -1.5735e+00
Epoch 10/10
20/20 - 4s - loss: 297.5240 - loglik: -2.9533e+02 - logprior: -1.5753e+00
Fitted a model with MAP estimate = -290.1273
expansions: [(5, 1), (8, 2), (10, 2), (13, 2), (19, 1), (20, 1), (31, 1), (37, 2), (38, 2), (40, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 1), (63, 1), (76, 1), (78, 2), (79, 2), (81, 2), (83, 1), (91, 1), (94, 2), (95, 1), (96, 1)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 12s - loss: 289.4105 - loglik: -2.8648e+02 - logprior: -2.3336e+00
Epoch 2/2
40/40 - 7s - loss: 281.6615 - loglik: -2.7979e+02 - logprior: -1.2261e+00
Fitted a model with MAP estimate = -283.1694
expansions: [(139, 2)]
discards: [  8  12  47  49  74 102 105 109 126]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 282.6990 - loglik: -2.7983e+02 - logprior: -2.1772e+00
Epoch 2/2
40/40 - 7s - loss: 280.9576 - loglik: -2.7924e+02 - logprior: -1.0275e+00
Fitted a model with MAP estimate = -285.4393
expansions: [(132, 2)]
discards: [130 131]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 12s - loss: 276.4590 - loglik: -2.7386e+02 - logprior: -1.1647e+00
Epoch 2/10
57/57 - 9s - loss: 269.7676 - loglik: -2.6747e+02 - logprior: -7.8439e-01
Epoch 3/10
57/57 - 9s - loss: 266.1767 - loglik: -2.6389e+02 - logprior: -7.5746e-01
Epoch 4/10
57/57 - 10s - loss: 264.6230 - loglik: -2.6244e+02 - logprior: -7.4702e-01
Epoch 5/10
57/57 - 9s - loss: 262.9955 - loglik: -2.6091e+02 - logprior: -7.2076e-01
Epoch 6/10
57/57 - 9s - loss: 263.1536 - loglik: -2.6125e+02 - logprior: -6.7923e-01
Fitted a model with MAP estimate = -261.5433
Time for alignment: 194.4607
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 372.6883 - loglik: -3.6969e+02 - logprior: -2.8725e+00
Epoch 2/10
20/20 - 4s - loss: 327.8854 - loglik: -3.2580e+02 - logprior: -1.3380e+00
Epoch 3/10
20/20 - 4s - loss: 307.3006 - loglik: -3.0436e+02 - logprior: -1.6318e+00
Epoch 4/10
20/20 - 4s - loss: 303.0410 - loglik: -3.0032e+02 - logprior: -1.5816e+00
Epoch 5/10
20/20 - 5s - loss: 301.3394 - loglik: -2.9871e+02 - logprior: -1.6101e+00
Epoch 6/10
20/20 - 4s - loss: 299.3492 - loglik: -2.9678e+02 - logprior: -1.6118e+00
Epoch 7/10
20/20 - 4s - loss: 297.9567 - loglik: -2.9544e+02 - logprior: -1.6093e+00
Epoch 8/10
20/20 - 4s - loss: 297.7809 - loglik: -2.9536e+02 - logprior: -1.5900e+00
Epoch 9/10
20/20 - 4s - loss: 297.2741 - loglik: -2.9495e+02 - logprior: -1.5928e+00
Epoch 10/10
20/20 - 4s - loss: 296.4320 - loglik: -2.9416e+02 - logprior: -1.5906e+00
Fitted a model with MAP estimate = -290.1350
expansions: [(5, 1), (8, 2), (10, 2), (13, 2), (19, 1), (20, 1), (31, 1), (35, 1), (36, 2), (38, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 1), (64, 1), (76, 1), (78, 2), (79, 2), (81, 2), (87, 1), (91, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 292.0080 - loglik: -2.8839e+02 - logprior: -2.9604e+00
Epoch 2/2
40/40 - 7s - loss: 282.1458 - loglik: -2.8026e+02 - logprior: -1.1905e+00
Fitted a model with MAP estimate = -284.3619
expansions: [(138, 2)]
discards: [  8  12  46  73 102 104 108 125]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 282.5009 - loglik: -2.7957e+02 - logprior: -2.1847e+00
Epoch 2/2
40/40 - 7s - loss: 280.5175 - loglik: -2.7880e+02 - logprior: -1.0349e+00
Fitted a model with MAP estimate = -286.5731
expansions: [(132, 2)]
discards: [130 131]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 12s - loss: 277.0644 - loglik: -2.7450e+02 - logprior: -1.1581e+00
Epoch 2/10
57/57 - 9s - loss: 270.1035 - loglik: -2.6781e+02 - logprior: -7.8528e-01
Epoch 3/10
57/57 - 9s - loss: 267.0205 - loglik: -2.6477e+02 - logprior: -7.4770e-01
Epoch 4/10
57/57 - 9s - loss: 264.5528 - loglik: -2.6238e+02 - logprior: -7.4146e-01
Epoch 5/10
57/57 - 9s - loss: 263.5590 - loglik: -2.6149e+02 - logprior: -7.1343e-01
Epoch 6/10
57/57 - 9s - loss: 263.1599 - loglik: -2.6124e+02 - logprior: -6.8507e-01
Epoch 7/10
57/57 - 9s - loss: 262.8562 - loglik: -2.6115e+02 - logprior: -6.5115e-01
Epoch 8/10
57/57 - 9s - loss: 261.8869 - loglik: -2.6028e+02 - logprior: -6.1536e-01
Epoch 9/10
57/57 - 9s - loss: 262.9941 - loglik: -2.6150e+02 - logprior: -5.7026e-01
Fitted a model with MAP estimate = -260.9814
Time for alignment: 219.4828
Computed alignments with likelihoods: ['-261.1753', '-261.5433', '-260.9814']
Best model has likelihood: -260.9814
time for generating output: 0.3353
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/adh.projection.fasta
SP score = 0.6681879194630872
Training of 3 independent models on file Sulfotransfer.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3473ab3940>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f333538df40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f333538dac0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34af7aa250>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34af7aa100>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3335178af0>, <__main__.SimpleDirichletPrior object at 0x7f343fe94340>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f342c986a60>

Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 687.0877 - loglik: -6.7691e+02 - logprior: -1.0066e+01
Epoch 2/10
19/19 - 8s - loss: 621.1647 - loglik: -6.1922e+02 - logprior: -1.1234e+00
Epoch 3/10
19/19 - 8s - loss: 588.0959 - loglik: -5.8512e+02 - logprior: -1.6324e+00
Epoch 4/10
19/19 - 8s - loss: 576.0710 - loglik: -5.7252e+02 - logprior: -1.9005e+00
Epoch 5/10
19/19 - 8s - loss: 572.0065 - loglik: -5.6831e+02 - logprior: -1.7448e+00
Epoch 6/10
19/19 - 8s - loss: 568.8902 - loglik: -5.6519e+02 - logprior: -1.7880e+00
Epoch 7/10
19/19 - 8s - loss: 567.8096 - loglik: -5.6417e+02 - logprior: -1.8772e+00
Epoch 8/10
19/19 - 8s - loss: 564.0862 - loglik: -5.6054e+02 - logprior: -1.9649e+00
Epoch 9/10
19/19 - 8s - loss: 563.4677 - loglik: -5.6005e+02 - logprior: -1.9807e+00
Epoch 10/10
19/19 - 8s - loss: 564.1961 - loglik: -5.6092e+02 - logprior: -1.9505e+00
Fitted a model with MAP estimate = -561.9803
expansions: [(18, 1), (25, 1), (27, 1), (28, 2), (30, 2), (33, 20), (44, 4), (45, 1), (65, 3), (91, 1), (93, 2), (102, 1), (103, 2), (104, 1), (105, 1), (119, 1), (120, 1), (135, 1), (136, 1), (151, 2), (153, 1), (154, 1), (167, 5), (182, 1), (183, 1), (184, 2), (185, 1)]
discards: [  2 171 172 173 174 175 176 177 178]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 250 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 572.9122 - loglik: -5.6117e+02 - logprior: -1.0440e+01
Epoch 2/2
19/19 - 10s - loss: 550.2465 - loglik: -5.4700e+02 - logprior: -1.8542e+00
Fitted a model with MAP estimate = -544.8127
expansions: []
discards: [ 30  39  40  41  42  43  44  45 197 226 227 228 229 230 231]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 556.9429 - loglik: -5.4484e+02 - logprior: -1.0273e+01
Epoch 2/2
19/19 - 9s - loss: 546.7715 - loglik: -5.4346e+02 - logprior: -1.6563e+00
Fitted a model with MAP estimate = -543.1493
expansions: []
discards: [ 23  24  25  26 111]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 230 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 554.6179 - loglik: -5.4323e+02 - logprior: -9.8723e+00
Epoch 2/10
19/19 - 9s - loss: 546.2044 - loglik: -5.4372e+02 - logprior: -1.1145e+00
Epoch 3/10
19/19 - 9s - loss: 543.3107 - loglik: -5.4182e+02 - logprior: -1.9180e-01
Epoch 4/10
19/19 - 9s - loss: 544.1655 - loglik: -5.4340e+02 - logprior: 0.4487
Fitted a model with MAP estimate = -541.8474
Time for alignment: 195.1874
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 687.0283 - loglik: -6.7686e+02 - logprior: -1.0052e+01
Epoch 2/10
19/19 - 8s - loss: 617.3422 - loglik: -6.1555e+02 - logprior: -9.8527e-01
Epoch 3/10
19/19 - 8s - loss: 586.6489 - loglik: -5.8392e+02 - logprior: -1.2579e+00
Epoch 4/10
19/19 - 8s - loss: 576.9016 - loglik: -5.7350e+02 - logprior: -1.3525e+00
Epoch 5/10
19/19 - 8s - loss: 572.1616 - loglik: -5.6868e+02 - logprior: -1.3603e+00
Epoch 6/10
19/19 - 8s - loss: 568.4034 - loglik: -5.6485e+02 - logprior: -1.5137e+00
Epoch 7/10
19/19 - 8s - loss: 565.0867 - loglik: -5.6171e+02 - logprior: -1.5496e+00
Epoch 8/10
19/19 - 8s - loss: 563.8967 - loglik: -5.6075e+02 - logprior: -1.5747e+00
Epoch 9/10
19/19 - 8s - loss: 566.4775 - loglik: -5.6351e+02 - logprior: -1.5854e+00
Fitted a model with MAP estimate = -561.9259
expansions: [(0, 2), (25, 1), (26, 1), (28, 1), (30, 2), (43, 9), (63, 2), (64, 2), (90, 6), (102, 1), (118, 2), (119, 2), (135, 1), (162, 1), (182, 1), (183, 1), (184, 2), (185, 1), (198, 3)]
discards: [  1 152 153 176]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 572.9307 - loglik: -5.5724e+02 - logprior: -1.4367e+01
Epoch 2/2
19/19 - 9s - loss: 551.8844 - loglik: -5.4826e+02 - logprior: -2.3017e+00
Fitted a model with MAP estimate = -547.4556
expansions: [(34, 1), (109, 1), (214, 1)]
discards: [  0 145 200 201 202 203 204 206 207 232 233 234]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 226 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 566.3162 - loglik: -5.5074e+02 - logprior: -1.4011e+01
Epoch 2/2
19/19 - 9s - loss: 552.6819 - loglik: -5.4645e+02 - logprior: -4.6140e+00
Fitted a model with MAP estimate = -547.7662
expansions: [(0, 1)]
discards: [  0 103 200 201 202 203]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 221 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 559.4031 - loglik: -5.4794e+02 - logprior: -9.8942e+00
Epoch 2/10
19/19 - 8s - loss: 549.9539 - loglik: -5.4757e+02 - logprior: -9.8848e-01
Epoch 3/10
19/19 - 8s - loss: 549.2697 - loglik: -5.4791e+02 - logprior: -4.3547e-02
Epoch 4/10
19/19 - 8s - loss: 546.9182 - loglik: -5.4629e+02 - logprior: 0.6241
Epoch 5/10
19/19 - 8s - loss: 547.2359 - loglik: -5.4703e+02 - logprior: 1.0194
Fitted a model with MAP estimate = -545.2783
Time for alignment: 188.3359
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 686.7051 - loglik: -6.7655e+02 - logprior: -1.0042e+01
Epoch 2/10
19/19 - 8s - loss: 621.6403 - loglik: -6.1984e+02 - logprior: -9.8565e-01
Epoch 3/10
19/19 - 8s - loss: 588.4773 - loglik: -5.8570e+02 - logprior: -1.3161e+00
Epoch 4/10
19/19 - 8s - loss: 576.9915 - loglik: -5.7331e+02 - logprior: -1.5533e+00
Epoch 5/10
19/19 - 8s - loss: 571.4527 - loglik: -5.6770e+02 - logprior: -1.5184e+00
Epoch 6/10
19/19 - 8s - loss: 567.6555 - loglik: -5.6397e+02 - logprior: -1.6317e+00
Epoch 7/10
19/19 - 8s - loss: 567.1174 - loglik: -5.6362e+02 - logprior: -1.6781e+00
Epoch 8/10
19/19 - 8s - loss: 562.9090 - loglik: -5.5963e+02 - logprior: -1.6613e+00
Epoch 9/10
19/19 - 8s - loss: 563.7271 - loglik: -5.6068e+02 - logprior: -1.6217e+00
Fitted a model with MAP estimate = -561.1512
expansions: [(25, 1), (27, 1), (28, 2), (33, 15), (43, 6), (64, 2), (65, 2), (90, 6), (102, 1), (103, 1), (118, 1), (135, 1), (147, 2), (149, 2), (151, 1), (153, 1), (165, 5), (182, 1), (183, 1), (184, 2), (185, 1), (198, 3)]
discards: [170 171 172 173 174 175 176 177 178]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 247 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 572.1188 - loglik: -5.6060e+02 - logprior: -1.0236e+01
Epoch 2/2
19/19 - 10s - loss: 550.5103 - loglik: -5.4718e+02 - logprior: -1.9730e+00
Fitted a model with MAP estimate = -544.9196
expansions: [(216, 7), (226, 1)]
discards: [ 31  32  33  34 190 243 244 245 246]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 246 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 554.1680 - loglik: -5.4216e+02 - logprior: -1.0246e+01
Epoch 2/2
19/19 - 10s - loss: 541.8397 - loglik: -5.3805e+02 - logprior: -2.0336e+00
Fitted a model with MAP estimate = -537.3553
expansions: [(88, 1), (246, 2)]
discards: [ 23  24  25 212 213 214 215 216 217 222 223 224 226 227]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 556.0320 - loglik: -5.4445e+02 - logprior: -9.9405e+00
Epoch 2/10
19/19 - 9s - loss: 544.7562 - loglik: -5.4218e+02 - logprior: -1.1613e+00
Epoch 3/10
19/19 - 9s - loss: 543.5046 - loglik: -5.4190e+02 - logprior: -2.7146e-01
Epoch 4/10
19/19 - 9s - loss: 542.1812 - loglik: -5.4134e+02 - logprior: 0.3949
Epoch 5/10
19/19 - 9s - loss: 540.7502 - loglik: -5.4037e+02 - logprior: 0.7967
Epoch 6/10
19/19 - 9s - loss: 540.7134 - loglik: -5.4070e+02 - logprior: 1.1247
Epoch 7/10
19/19 - 9s - loss: 539.9475 - loglik: -5.4037e+02 - logprior: 1.5443
Epoch 8/10
19/19 - 9s - loss: 540.4709 - loglik: -5.4124e+02 - logprior: 1.8656
Fitted a model with MAP estimate = -538.3965
Time for alignment: 223.8003
Computed alignments with likelihoods: ['-541.8474', '-545.2783', '-537.3553']
Best model has likelihood: -537.3553
time for generating output: 0.3728
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Sulfotransfer.projection.fasta
SP score = 0.42004175365344465
Training of 3 independent models on file hom.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3424003910>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3473848d30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33a369f940>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3356b8c6a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3356b8cc70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f33a3adf9d0>, <__main__.SimpleDirichletPrior object at 0x7f3436cad0d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f342c986a60>

Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.4373 - loglik: -1.5117e+02 - logprior: -3.2584e+00
Epoch 2/10
19/19 - 1s - loss: 120.5878 - loglik: -1.1905e+02 - logprior: -1.5154e+00
Epoch 3/10
19/19 - 1s - loss: 106.0258 - loglik: -1.0408e+02 - logprior: -1.6354e+00
Epoch 4/10
19/19 - 1s - loss: 103.2153 - loglik: -1.0119e+02 - logprior: -1.7163e+00
Epoch 5/10
19/19 - 1s - loss: 102.2935 - loglik: -1.0047e+02 - logprior: -1.6036e+00
Epoch 6/10
19/19 - 1s - loss: 101.9288 - loglik: -1.0011e+02 - logprior: -1.6063e+00
Epoch 7/10
19/19 - 1s - loss: 101.2949 - loglik: -9.9508e+01 - logprior: -1.5848e+00
Epoch 8/10
19/19 - 1s - loss: 101.5467 - loglik: -9.9757e+01 - logprior: -1.5724e+00
Fitted a model with MAP estimate = -97.1228
expansions: [(3, 1), (5, 1), (6, 1), (7, 1), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 100.2087 - loglik: -9.6796e+01 - logprior: -3.1891e+00
Epoch 2/2
19/19 - 1s - loss: 92.6240 - loglik: -9.0971e+01 - logprior: -1.4310e+00
Fitted a model with MAP estimate = -88.1258
expansions: []
discards: [ 0 21 37 40]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 98.1185 - loglik: -9.3618e+01 - logprior: -4.2326e+00
Epoch 2/2
19/19 - 1s - loss: 94.6966 - loglik: -9.2060e+01 - logprior: -2.3542e+00
Fitted a model with MAP estimate = -89.4662
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 91.3811 - loglik: -8.7860e+01 - logprior: -3.2469e+00
Epoch 2/10
21/21 - 2s - loss: 87.8526 - loglik: -8.6193e+01 - logprior: -1.3856e+00
Epoch 3/10
21/21 - 2s - loss: 87.6079 - loglik: -8.6029e+01 - logprior: -1.2898e+00
Epoch 4/10
21/21 - 2s - loss: 87.3619 - loglik: -8.5894e+01 - logprior: -1.1634e+00
Epoch 5/10
21/21 - 2s - loss: 86.1151 - loglik: -8.4542e+01 - logprior: -1.2648e+00
Epoch 6/10
21/21 - 2s - loss: 86.4401 - loglik: -8.4906e+01 - logprior: -1.2148e+00
Fitted a model with MAP estimate = -85.9992
Time for alignment: 51.7400
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 154.3403 - loglik: -1.5107e+02 - logprior: -3.2604e+00
Epoch 2/10
19/19 - 1s - loss: 121.1607 - loglik: -1.1960e+02 - logprior: -1.5323e+00
Epoch 3/10
19/19 - 1s - loss: 107.0635 - loglik: -1.0524e+02 - logprior: -1.6262e+00
Epoch 4/10
19/19 - 1s - loss: 102.9731 - loglik: -1.0093e+02 - logprior: -1.7347e+00
Epoch 5/10
19/19 - 1s - loss: 102.1222 - loglik: -1.0029e+02 - logprior: -1.5972e+00
Epoch 6/10
19/19 - 1s - loss: 102.1863 - loglik: -1.0036e+02 - logprior: -1.6043e+00
Fitted a model with MAP estimate = -97.3575
expansions: [(3, 1), (5, 1), (6, 1), (7, 1), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 104.2575 - loglik: -9.9806e+01 - logprior: -4.2411e+00
Epoch 2/2
19/19 - 1s - loss: 94.8428 - loglik: -9.2239e+01 - logprior: -2.3771e+00
Fitted a model with MAP estimate = -89.1551
expansions: [(3, 1)]
discards: [ 0 20 36 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 97.2304 - loglik: -9.2963e+01 - logprior: -3.9990e+00
Epoch 2/2
19/19 - 1s - loss: 93.1335 - loglik: -9.1363e+01 - logprior: -1.5134e+00
Fitted a model with MAP estimate = -88.6269
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 92.0294 - loglik: -8.8039e+01 - logprior: -3.7195e+00
Epoch 2/10
21/21 - 2s - loss: 88.6718 - loglik: -8.6508e+01 - logprior: -1.8973e+00
Epoch 3/10
21/21 - 2s - loss: 87.2248 - loglik: -8.5472e+01 - logprior: -1.4562e+00
Epoch 4/10
21/21 - 2s - loss: 87.3687 - loglik: -8.5702e+01 - logprior: -1.3581e+00
Fitted a model with MAP estimate = -86.7879
Time for alignment: 46.2877
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.3597 - loglik: -1.5109e+02 - logprior: -3.2587e+00
Epoch 2/10
19/19 - 1s - loss: 121.7065 - loglik: -1.2015e+02 - logprior: -1.5252e+00
Epoch 3/10
19/19 - 1s - loss: 106.9750 - loglik: -1.0511e+02 - logprior: -1.6451e+00
Epoch 4/10
19/19 - 1s - loss: 103.5537 - loglik: -1.0147e+02 - logprior: -1.7677e+00
Epoch 5/10
19/19 - 1s - loss: 102.9726 - loglik: -1.0108e+02 - logprior: -1.6522e+00
Epoch 6/10
19/19 - 1s - loss: 102.3073 - loglik: -1.0044e+02 - logprior: -1.6526e+00
Epoch 7/10
19/19 - 1s - loss: 102.0874 - loglik: -1.0026e+02 - logprior: -1.6237e+00
Epoch 8/10
19/19 - 1s - loss: 102.1127 - loglik: -1.0029e+02 - logprior: -1.6197e+00
Fitted a model with MAP estimate = -97.4957
expansions: [(3, 1), (5, 1), (6, 1), (7, 1), (9, 1), (16, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 1), (31, 1), (32, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.9725 - loglik: -9.7572e+01 - logprior: -3.1926e+00
Epoch 2/2
19/19 - 2s - loss: 92.7706 - loglik: -9.1089e+01 - logprior: -1.4398e+00
Fitted a model with MAP estimate = -88.2570
expansions: []
discards: [ 0 21 39 46]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 98.0742 - loglik: -9.3558e+01 - logprior: -4.2458e+00
Epoch 2/2
19/19 - 1s - loss: 94.6880 - loglik: -9.2046e+01 - logprior: -2.3554e+00
Fitted a model with MAP estimate = -89.4560
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 91.2790 - loglik: -8.7727e+01 - logprior: -3.2694e+00
Epoch 2/10
21/21 - 2s - loss: 88.1258 - loglik: -8.6470e+01 - logprior: -1.3876e+00
Epoch 3/10
21/21 - 2s - loss: 87.5721 - loglik: -8.5996e+01 - logprior: -1.2772e+00
Epoch 4/10
21/21 - 2s - loss: 86.3936 - loglik: -8.4873e+01 - logprior: -1.2130e+00
Epoch 5/10
21/21 - 2s - loss: 86.6090 - loglik: -8.5001e+01 - logprior: -1.2979e+00
Fitted a model with MAP estimate = -86.1068
Time for alignment: 49.2130
Computed alignments with likelihoods: ['-85.9992', '-86.7879', '-86.1068']
Best model has likelihood: -85.9992
time for generating output: 0.1339
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hom.projection.fasta
SP score = 0.943979394719897
Training of 3 independent models on file OTCace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34ee667880>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3508255280>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f35082555b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34240765e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f343fe110a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34af174340>, <__main__.SimpleDirichletPrior object at 0x7f33a4512670>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34366569d0>

Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 8s - loss: 417.7543 - loglik: -4.1284e+02 - logprior: -4.8572e+00
Epoch 2/10
27/27 - 5s - loss: 331.4990 - loglik: -3.2900e+02 - logprior: -2.2017e+00
Epoch 3/10
27/27 - 5s - loss: 316.9227 - loglik: -3.1423e+02 - logprior: -2.2073e+00
Epoch 4/10
27/27 - 5s - loss: 314.2411 - loglik: -3.1167e+02 - logprior: -2.1148e+00
Epoch 5/10
27/27 - 5s - loss: 312.6795 - loglik: -3.1013e+02 - logprior: -2.0907e+00
Epoch 6/10
27/27 - 5s - loss: 312.8622 - loglik: -3.1032e+02 - logprior: -2.0793e+00
Fitted a model with MAP estimate = -311.5878
expansions: [(0, 2), (9, 2), (18, 1), (24, 2), (26, 1), (36, 2), (37, 4), (38, 1), (39, 1), (41, 1), (42, 1), (44, 1), (67, 2), (68, 1), (69, 1), (71, 2), (73, 1), (78, 1), (82, 2), (94, 2), (103, 1), (104, 2), (105, 2), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 162 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 11s - loss: 304.0824 - loglik: -2.9697e+02 - logprior: -6.6624e+00
Epoch 2/2
27/27 - 6s - loss: 286.2509 - loglik: -2.8396e+02 - logprior: -1.7628e+00
Fitted a model with MAP estimate = -284.7196
expansions: [(43, 1)]
discards: [ 12  29  47  48  86  90 110 137 139]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 291.9931 - loglik: -2.8698e+02 - logprior: -4.4610e+00
Epoch 2/2
27/27 - 6s - loss: 287.2616 - loglik: -2.8550e+02 - logprior: -1.2063e+00
Fitted a model with MAP estimate = -285.9942
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 10s - loss: 289.9128 - loglik: -2.8507e+02 - logprior: -4.2710e+00
Epoch 2/10
27/27 - 6s - loss: 286.7415 - loglik: -2.8519e+02 - logprior: -9.7302e-01
Epoch 3/10
27/27 - 6s - loss: 285.6905 - loglik: -2.8443e+02 - logprior: -6.9042e-01
Epoch 4/10
27/27 - 6s - loss: 286.0998 - loglik: -2.8496e+02 - logprior: -5.7076e-01
Fitted a model with MAP estimate = -285.0098
Time for alignment: 119.8949
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 8s - loss: 417.6674 - loglik: -4.1277e+02 - logprior: -4.8424e+00
Epoch 2/10
27/27 - 5s - loss: 333.9090 - loglik: -3.3151e+02 - logprior: -2.1547e+00
Epoch 3/10
27/27 - 5s - loss: 316.8779 - loglik: -3.1400e+02 - logprior: -2.2961e+00
Epoch 4/10
27/27 - 5s - loss: 313.2346 - loglik: -3.1040e+02 - logprior: -2.2921e+00
Epoch 5/10
27/27 - 5s - loss: 311.5229 - loglik: -3.0873e+02 - logprior: -2.2686e+00
Epoch 6/10
27/27 - 5s - loss: 310.3728 - loglik: -3.0762e+02 - logprior: -2.2647e+00
Epoch 7/10
27/27 - 5s - loss: 310.9471 - loglik: -3.0820e+02 - logprior: -2.2522e+00
Fitted a model with MAP estimate = -309.8107
expansions: [(0, 2), (10, 1), (21, 1), (25, 2), (26, 1), (36, 2), (37, 3), (38, 1), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 2), (67, 1), (68, 1), (69, 1), (71, 2), (73, 1), (78, 1), (79, 1), (81, 2), (93, 2), (99, 1), (103, 2), (104, 2), (105, 1), (115, 1), (116, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 162 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 10s - loss: 302.1352 - loglik: -2.9493e+02 - logprior: -6.7351e+00
Epoch 2/2
27/27 - 6s - loss: 286.3869 - loglik: -2.8402e+02 - logprior: -1.8124e+00
Fitted a model with MAP estimate = -283.8450
expansions: [(42, 1)]
discards: [  0  29  46  47  48  65  90 110 137 139]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 295.3021 - loglik: -2.8801e+02 - logprior: -6.7145e+00
Epoch 2/2
27/27 - 6s - loss: 287.9419 - loglik: -2.8571e+02 - logprior: -1.6562e+00
Fitted a model with MAP estimate = -286.2485
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 290.0445 - loglik: -2.8495e+02 - logprior: -4.5004e+00
Epoch 2/10
27/27 - 6s - loss: 287.2374 - loglik: -2.8548e+02 - logprior: -1.1532e+00
Epoch 3/10
27/27 - 6s - loss: 285.9152 - loglik: -2.8447e+02 - logprior: -8.4256e-01
Epoch 4/10
27/27 - 6s - loss: 286.3564 - loglik: -2.8509e+02 - logprior: -6.8523e-01
Fitted a model with MAP estimate = -284.9838
Time for alignment: 123.6323
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 417.6791 - loglik: -4.1278e+02 - logprior: -4.8406e+00
Epoch 2/10
27/27 - 5s - loss: 330.6805 - loglik: -3.2836e+02 - logprior: -2.1594e+00
Epoch 3/10
27/27 - 5s - loss: 316.8356 - loglik: -3.1409e+02 - logprior: -2.2758e+00
Epoch 4/10
27/27 - 5s - loss: 313.8586 - loglik: -3.1117e+02 - logprior: -2.2275e+00
Epoch 5/10
27/27 - 5s - loss: 313.1911 - loglik: -3.1054e+02 - logprior: -2.2210e+00
Epoch 6/10
27/27 - 6s - loss: 312.1835 - loglik: -3.0949e+02 - logprior: -2.2773e+00
Epoch 7/10
27/27 - 5s - loss: 311.2140 - loglik: -3.0849e+02 - logprior: -2.2883e+00
Epoch 8/10
27/27 - 5s - loss: 310.8099 - loglik: -3.0807e+02 - logprior: -2.2939e+00
Epoch 9/10
27/27 - 5s - loss: 311.0972 - loglik: -3.0836e+02 - logprior: -2.2926e+00
Fitted a model with MAP estimate = -310.1787
expansions: [(0, 2), (10, 1), (18, 2), (25, 2), (26, 1), (36, 2), (37, 3), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 2), (67, 1), (68, 1), (69, 1), (71, 2), (73, 1), (78, 1), (79, 1), (81, 2), (100, 1), (101, 1), (102, 1), (103, 2), (104, 2), (105, 1), (113, 1), (114, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 164 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 303.6595 - loglik: -2.9641e+02 - logprior: -6.8044e+00
Epoch 2/2
27/27 - 6s - loss: 287.0465 - loglik: -2.8480e+02 - logprior: -1.7179e+00
Fitted a model with MAP estimate = -284.6219
expansions: [(43, 1)]
discards: [  0  21  30  48  49  50  51  67 112 139 141]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 295.3061 - loglik: -2.8814e+02 - logprior: -6.6406e+00
Epoch 2/2
27/27 - 6s - loss: 288.7081 - loglik: -2.8660e+02 - logprior: -1.5590e+00
Fitted a model with MAP estimate = -286.9278
expansions: []
discards: [85]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 291.7585 - loglik: -2.8678e+02 - logprior: -4.4370e+00
Epoch 2/10
27/27 - 6s - loss: 288.4424 - loglik: -2.8681e+02 - logprior: -1.0727e+00
Epoch 3/10
27/27 - 6s - loss: 287.6769 - loglik: -2.8638e+02 - logprior: -7.5834e-01
Epoch 4/10
27/27 - 6s - loss: 286.9297 - loglik: -2.8579e+02 - logprior: -5.9408e-01
Epoch 5/10
27/27 - 5s - loss: 286.8837 - loglik: -2.8586e+02 - logprior: -4.6868e-01
Epoch 6/10
27/27 - 6s - loss: 286.6161 - loglik: -2.8574e+02 - logprior: -3.1891e-01
Epoch 7/10
27/27 - 6s - loss: 286.4906 - loglik: -2.8570e+02 - logprior: -2.3359e-01
Epoch 8/10
27/27 - 6s - loss: 286.9573 - loglik: -2.8631e+02 - logprior: -8.9405e-02
Fitted a model with MAP estimate = -285.5746
Time for alignment: 157.0251
Computed alignments with likelihoods: ['-284.7196', '-283.8450', '-284.6219']
Best model has likelihood: -283.8450
time for generating output: 0.3258
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/OTCace.projection.fasta
SP score = 0.5594684385382059
Training of 3 independent models on file lyase_1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f349e1310a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f338a2fc7f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f347c0f41f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3440c277c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3440c279a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34ed7172b0>, <__main__.SimpleDirichletPrior object at 0x7f33a336b7f0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34ed95cb80>

Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 22s - loss: 768.5497 - loglik: -7.6596e+02 - logprior: -2.3209e+00
Epoch 2/10
34/34 - 16s - loss: 643.5095 - loglik: -6.4062e+02 - logprior: -2.0600e+00
Epoch 3/10
34/34 - 16s - loss: 628.8424 - loglik: -6.2552e+02 - logprior: -2.2748e+00
Epoch 4/10
34/34 - 16s - loss: 624.6077 - loglik: -6.2161e+02 - logprior: -2.1802e+00
Epoch 5/10
34/34 - 17s - loss: 623.5951 - loglik: -6.2072e+02 - logprior: -2.1646e+00
Epoch 6/10
34/34 - 16s - loss: 622.2800 - loglik: -6.1942e+02 - logprior: -2.1800e+00
Epoch 7/10
34/34 - 16s - loss: 621.6207 - loglik: -6.1875e+02 - logprior: -2.1862e+00
Epoch 8/10
34/34 - 17s - loss: 621.4193 - loglik: -6.1854e+02 - logprior: -2.1898e+00
Epoch 9/10
34/34 - 16s - loss: 620.2475 - loglik: -6.1736e+02 - logprior: -2.1997e+00
Epoch 10/10
34/34 - 17s - loss: 621.8530 - loglik: -6.1895e+02 - logprior: -2.2098e+00
Fitted a model with MAP estimate = -619.5996
expansions: [(13, 2), (14, 1), (15, 3), (16, 1), (17, 5), (28, 3), (49, 1), (52, 2), (56, 5), (57, 1), (61, 1), (65, 1), (66, 2), (92, 1), (94, 1), (95, 2), (96, 2), (99, 1), (101, 1), (102, 1), (103, 1), (104, 1), (107, 1), (110, 1), (137, 1), (140, 1), (142, 1), (143, 3), (144, 1), (165, 1), (168, 1), (171, 2), (173, 1), (175, 1), (178, 1), (184, 1), (185, 3), (186, 2), (189, 1), (191, 1), (203, 1), (204, 1), (208, 1), (211, 1), (225, 1), (228, 1), (230, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 305 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 27s - loss: 596.4406 - loglik: -5.9177e+02 - logprior: -3.9863e+00
Epoch 2/2
34/34 - 23s - loss: 575.5718 - loglik: -5.7255e+02 - logprior: -2.2564e+00
Fitted a model with MAP estimate = -570.8804
expansions: [(74, 1)]
discards: [ 18  41  67  75  76  77  91 125 127]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 297 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 26s - loss: 580.4728 - loglik: -5.7698e+02 - logprior: -2.6655e+00
Epoch 2/2
34/34 - 22s - loss: 576.0835 - loglik: -5.7428e+02 - logprior: -9.8361e-01
Fitted a model with MAP estimate = -572.8177
expansions: [(180, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 298 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 27s - loss: 574.0696 - loglik: -5.7080e+02 - logprior: -2.4021e+00
Epoch 2/10
34/34 - 23s - loss: 572.0477 - loglik: -5.7063e+02 - logprior: -5.5603e-01
Epoch 3/10
34/34 - 22s - loss: 570.9822 - loglik: -5.6976e+02 - logprior: -3.4376e-01
Epoch 4/10
34/34 - 22s - loss: 569.6508 - loglik: -5.6863e+02 - logprior: -1.6822e-01
Epoch 5/10
34/34 - 22s - loss: 569.9383 - loglik: -5.6912e+02 - logprior: 0.0282
Fitted a model with MAP estimate = -568.7410
Time for alignment: 486.6168
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 20s - loss: 766.6588 - loglik: -7.6408e+02 - logprior: -2.3236e+00
Epoch 2/10
34/34 - 17s - loss: 642.1963 - loglik: -6.3957e+02 - logprior: -2.0005e+00
Epoch 3/10
34/34 - 16s - loss: 621.1356 - loglik: -6.1778e+02 - logprior: -2.2541e+00
Epoch 4/10
34/34 - 16s - loss: 615.2753 - loglik: -6.1221e+02 - logprior: -2.2098e+00
Epoch 5/10
34/34 - 17s - loss: 615.7043 - loglik: -6.1272e+02 - logprior: -2.2163e+00
Fitted a model with MAP estimate = -613.5170
expansions: [(9, 1), (12, 1), (14, 1), (15, 3), (16, 1), (17, 3), (18, 2), (22, 2), (29, 1), (30, 2), (51, 2), (55, 1), (56, 2), (60, 1), (64, 1), (67, 1), (91, 1), (93, 1), (94, 1), (99, 1), (100, 2), (102, 1), (104, 1), (107, 1), (110, 1), (129, 1), (134, 1), (139, 1), (142, 1), (143, 3), (144, 1), (165, 1), (166, 1), (167, 1), (171, 1), (173, 1), (175, 1), (186, 3), (188, 2), (190, 1), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1), (230, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 296 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 27s - loss: 594.5517 - loglik: -5.8978e+02 - logprior: -3.9842e+00
Epoch 2/2
34/34 - 22s - loss: 575.4065 - loglik: -5.7259e+02 - logprior: -2.0128e+00
Fitted a model with MAP estimate = -571.3793
expansions: [(68, 1), (76, 1), (237, 3)]
discards: [19 23 34 44]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 297 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 26s - loss: 575.5164 - loglik: -5.7194e+02 - logprior: -2.6533e+00
Epoch 2/2
34/34 - 22s - loss: 569.2134 - loglik: -5.6730e+02 - logprior: -1.0014e+00
Fitted a model with MAP estimate = -567.9633
expansions: [(181, 1), (238, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 299 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 27s - loss: 570.7659 - loglik: -5.6745e+02 - logprior: -2.3677e+00
Epoch 2/10
34/34 - 23s - loss: 567.0474 - loglik: -5.6549e+02 - logprior: -6.5089e-01
Epoch 3/10
34/34 - 22s - loss: 567.5308 - loglik: -5.6620e+02 - logprior: -4.2584e-01
Fitted a model with MAP estimate = -565.4966
Time for alignment: 356.7528
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 19s - loss: 765.5649 - loglik: -7.6300e+02 - logprior: -2.3127e+00
Epoch 2/10
34/34 - 17s - loss: 637.0451 - loglik: -6.3433e+02 - logprior: -2.0637e+00
Epoch 3/10
34/34 - 16s - loss: 618.4498 - loglik: -6.1519e+02 - logprior: -2.3100e+00
Epoch 4/10
34/34 - 16s - loss: 616.5328 - loglik: -6.1351e+02 - logprior: -2.2197e+00
Epoch 5/10
34/34 - 17s - loss: 615.3488 - loglik: -6.1240e+02 - logprior: -2.2154e+00
Epoch 6/10
34/34 - 16s - loss: 613.1337 - loglik: -6.1021e+02 - logprior: -2.2327e+00
Epoch 7/10
34/34 - 16s - loss: 612.4958 - loglik: -6.0955e+02 - logprior: -2.2484e+00
Epoch 8/10
34/34 - 16s - loss: 611.6186 - loglik: -6.0869e+02 - logprior: -2.2413e+00
Epoch 9/10
34/34 - 17s - loss: 612.9230 - loglik: -6.1000e+02 - logprior: -2.2433e+00
Fitted a model with MAP estimate = -611.0673
expansions: [(9, 1), (12, 1), (14, 1), (15, 3), (16, 5), (17, 2), (29, 2), (31, 1), (50, 3), (56, 1), (64, 1), (66, 1), (67, 1), (91, 1), (93, 1), (94, 2), (95, 2), (98, 1), (100, 1), (101, 1), (102, 1), (104, 1), (109, 1), (129, 1), (130, 1), (136, 1), (139, 1), (141, 1), (142, 3), (143, 1), (164, 1), (167, 1), (172, 2), (175, 1), (178, 1), (184, 3), (186, 3), (188, 1), (190, 1), (199, 1), (203, 1), (208, 1), (211, 1), (225, 1), (229, 1), (230, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 300 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 27s - loss: 592.7211 - loglik: -5.8811e+02 - logprior: -3.8694e+00
Epoch 2/2
34/34 - 23s - loss: 575.7694 - loglik: -5.7288e+02 - logprior: -2.0031e+00
Fitted a model with MAP estimate = -568.1789
expansions: [(234, 1)]
discards: [ 19  22  23 119 121 239 257 258 259]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 292 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 25s - loss: 576.2896 - loglik: -5.7285e+02 - logprior: -2.4030e+00
Epoch 2/2
34/34 - 22s - loss: 570.3160 - loglik: -5.6864e+02 - logprior: -7.4597e-01
Fitted a model with MAP estimate = -568.1242
expansions: [(69, 4), (223, 1)]
discards: [202 213]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 295 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 26s - loss: 568.8137 - loglik: -5.6563e+02 - logprior: -2.2083e+00
Epoch 2/10
34/34 - 22s - loss: 564.5881 - loglik: -5.6314e+02 - logprior: -5.0801e-01
Epoch 3/10
34/34 - 22s - loss: 564.2844 - loglik: -5.6298e+02 - logprior: -3.4091e-01
Epoch 4/10
34/34 - 22s - loss: 563.2173 - loglik: -5.6220e+02 - logprior: -1.0588e-01
Epoch 5/10
34/34 - 22s - loss: 562.8018 - loglik: -5.6196e+02 - logprior: 0.0799
Epoch 6/10
34/34 - 22s - loss: 562.9630 - loglik: -5.6233e+02 - logprior: 0.2739
Fitted a model with MAP estimate = -561.0142
Time for alignment: 486.6328
Computed alignments with likelihoods: ['-568.7410', '-565.4966', '-561.0142']
Best model has likelihood: -561.0142
time for generating output: 0.4305
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/lyase_1.projection.fasta
SP score = 0.7414814814814815
Training of 3 independent models on file toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34af462fa0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34b7a39e50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b7a39880>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3436382250>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3437555970>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f343753be20>, <__main__.SimpleDirichletPrior object at 0x7f346b6654c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f33a3c13ee0>

Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 271.9233 - loglik: -1.8302e+02 - logprior: -8.8881e+01
Epoch 2/10
10/10 - 1s - loss: 184.4494 - loglik: -1.6007e+02 - logprior: -2.4363e+01
Epoch 3/10
10/10 - 1s - loss: 153.7513 - loglik: -1.4189e+02 - logprior: -1.1846e+01
Epoch 4/10
10/10 - 1s - loss: 139.7707 - loglik: -1.3251e+02 - logprior: -7.2403e+00
Epoch 5/10
10/10 - 1s - loss: 132.2251 - loglik: -1.2728e+02 - logprior: -4.9304e+00
Epoch 6/10
10/10 - 1s - loss: 128.1924 - loglik: -1.2430e+02 - logprior: -3.7584e+00
Epoch 7/10
10/10 - 1s - loss: 126.5255 - loglik: -1.2330e+02 - logprior: -2.8568e+00
Epoch 8/10
10/10 - 1s - loss: 125.7219 - loglik: -1.2298e+02 - logprior: -2.3256e+00
Epoch 9/10
10/10 - 1s - loss: 125.2059 - loglik: -1.2286e+02 - logprior: -1.9873e+00
Epoch 10/10
10/10 - 1s - loss: 124.7839 - loglik: -1.2267e+02 - logprior: -1.7780e+00
Fitted a model with MAP estimate = -124.2362
expansions: [(7, 2), (8, 4), (16, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 219.9502 - loglik: -1.1983e+02 - logprior: -9.9762e+01
Epoch 2/2
10/10 - 1s - loss: 155.7744 - loglik: -1.1387e+02 - logprior: -4.1487e+01
Fitted a model with MAP estimate = -144.3471
expansions: [(0, 1), (48, 1)]
discards: [ 0 30 31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 189.9334 - loglik: -1.0950e+02 - logprior: -7.9971e+01
Epoch 2/2
10/10 - 1s - loss: 129.9662 - loglik: -1.0752e+02 - logprior: -2.1961e+01
Fitted a model with MAP estimate = -120.5716
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 184.7579 - loglik: -1.0568e+02 - logprior: -7.8579e+01
Epoch 2/10
10/10 - 1s - loss: 128.1842 - loglik: -1.0634e+02 - logprior: -2.1336e+01
Epoch 3/10
10/10 - 1s - loss: 116.9087 - loglik: -1.0700e+02 - logprior: -9.3933e+00
Epoch 4/10
10/10 - 1s - loss: 112.4888 - loglik: -1.0765e+02 - logprior: -4.3167e+00
Epoch 5/10
10/10 - 1s - loss: 110.2010 - loglik: -1.0817e+02 - logprior: -1.4954e+00
Epoch 6/10
10/10 - 1s - loss: 108.9263 - loglik: -1.0851e+02 - logprior: 0.1257
Epoch 7/10
10/10 - 1s - loss: 108.1535 - loglik: -1.0871e+02 - logprior: 1.1130
Epoch 8/10
10/10 - 1s - loss: 107.6296 - loglik: -1.0886e+02 - logprior: 1.7846
Epoch 9/10
10/10 - 1s - loss: 107.2343 - loglik: -1.0897e+02 - logprior: 2.2965
Epoch 10/10
10/10 - 1s - loss: 106.8991 - loglik: -1.0902e+02 - logprior: 2.6910
Fitted a model with MAP estimate = -106.1643
Time for alignment: 35.4798
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 271.9233 - loglik: -1.8302e+02 - logprior: -8.8881e+01
Epoch 2/10
10/10 - 1s - loss: 184.4494 - loglik: -1.6007e+02 - logprior: -2.4363e+01
Epoch 3/10
10/10 - 1s - loss: 153.7513 - loglik: -1.4189e+02 - logprior: -1.1846e+01
Epoch 4/10
10/10 - 1s - loss: 139.7709 - loglik: -1.3251e+02 - logprior: -7.2403e+00
Epoch 5/10
10/10 - 1s - loss: 132.3178 - loglik: -1.2741e+02 - logprior: -4.9004e+00
Epoch 6/10
10/10 - 1s - loss: 128.3945 - loglik: -1.2467e+02 - logprior: -3.6820e+00
Epoch 7/10
10/10 - 1s - loss: 126.3969 - loglik: -1.2330e+02 - logprior: -2.8654e+00
Epoch 8/10
10/10 - 1s - loss: 125.5583 - loglik: -1.2282e+02 - logprior: -2.3464e+00
Epoch 9/10
10/10 - 1s - loss: 125.0200 - loglik: -1.2263e+02 - logprior: -2.0160e+00
Epoch 10/10
10/10 - 1s - loss: 124.6505 - loglik: -1.2253e+02 - logprior: -1.7977e+00
Fitted a model with MAP estimate = -124.1877
expansions: [(7, 2), (8, 4), (16, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 219.9179 - loglik: -1.1980e+02 - logprior: -9.9765e+01
Epoch 2/2
10/10 - 1s - loss: 155.7589 - loglik: -1.1385e+02 - logprior: -4.1483e+01
Fitted a model with MAP estimate = -144.3305
expansions: [(0, 1), (48, 1)]
discards: [ 0 30 31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 189.9425 - loglik: -1.0951e+02 - logprior: -7.9971e+01
Epoch 2/2
10/10 - 1s - loss: 129.9758 - loglik: -1.0753e+02 - logprior: -2.1964e+01
Fitted a model with MAP estimate = -120.5752
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 184.7521 - loglik: -1.0567e+02 - logprior: -7.8583e+01
Epoch 2/10
10/10 - 1s - loss: 128.1747 - loglik: -1.0632e+02 - logprior: -2.1344e+01
Epoch 3/10
10/10 - 1s - loss: 116.9042 - loglik: -1.0698e+02 - logprior: -9.4018e+00
Epoch 4/10
10/10 - 1s - loss: 112.4900 - loglik: -1.0764e+02 - logprior: -4.3242e+00
Epoch 5/10
10/10 - 1s - loss: 110.2046 - loglik: -1.0817e+02 - logprior: -1.5023e+00
Epoch 6/10
10/10 - 1s - loss: 108.9295 - loglik: -1.0850e+02 - logprior: 0.1179
Epoch 7/10
10/10 - 1s - loss: 108.1544 - loglik: -1.0870e+02 - logprior: 1.1033
Epoch 8/10
10/10 - 1s - loss: 107.6338 - loglik: -1.0885e+02 - logprior: 1.7742
Epoch 9/10
10/10 - 1s - loss: 107.2356 - loglik: -1.0895e+02 - logprior: 2.2859
Epoch 10/10
10/10 - 1s - loss: 106.9052 - loglik: -1.0901e+02 - logprior: 2.6794
Fitted a model with MAP estimate = -106.1723
Time for alignment: 35.0934
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 271.9233 - loglik: -1.8302e+02 - logprior: -8.8881e+01
Epoch 2/10
10/10 - 1s - loss: 184.4495 - loglik: -1.6007e+02 - logprior: -2.4363e+01
Epoch 3/10
10/10 - 1s - loss: 153.7513 - loglik: -1.4189e+02 - logprior: -1.1846e+01
Epoch 4/10
10/10 - 1s - loss: 139.7707 - loglik: -1.3251e+02 - logprior: -7.2403e+00
Epoch 5/10
10/10 - 1s - loss: 132.1993 - loglik: -1.2725e+02 - logprior: -4.9302e+00
Epoch 6/10
10/10 - 1s - loss: 128.0534 - loglik: -1.2417e+02 - logprior: -3.7548e+00
Epoch 7/10
10/10 - 1s - loss: 126.3599 - loglik: -1.2313e+02 - logprior: -2.8880e+00
Epoch 8/10
10/10 - 1s - loss: 125.5417 - loglik: -1.2281e+02 - logprior: -2.3413e+00
Epoch 9/10
10/10 - 1s - loss: 125.0231 - loglik: -1.2270e+02 - logprior: -2.0013e+00
Epoch 10/10
10/10 - 1s - loss: 124.6711 - loglik: -1.2261e+02 - logprior: -1.7711e+00
Fitted a model with MAP estimate = -124.2334
expansions: [(7, 2), (8, 4), (16, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 219.9441 - loglik: -1.1987e+02 - logprior: -9.9744e+01
Epoch 2/2
10/10 - 1s - loss: 155.8151 - loglik: -1.1400e+02 - logprior: -4.1444e+01
Fitted a model with MAP estimate = -144.4137
expansions: [(0, 1), (48, 1)]
discards: [ 0 30 31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 189.8072 - loglik: -1.0944e+02 - logprior: -7.9944e+01
Epoch 2/2
10/10 - 1s - loss: 129.8629 - loglik: -1.0742e+02 - logprior: -2.1957e+01
Fitted a model with MAP estimate = -120.5097
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 184.7412 - loglik: -1.0567e+02 - logprior: -7.8577e+01
Epoch 2/10
10/10 - 1s - loss: 128.1882 - loglik: -1.0636e+02 - logprior: -2.1328e+01
Epoch 3/10
10/10 - 1s - loss: 116.9188 - loglik: -1.0701e+02 - logprior: -9.3934e+00
Epoch 4/10
10/10 - 1s - loss: 112.4946 - loglik: -1.0766e+02 - logprior: -4.3162e+00
Epoch 5/10
10/10 - 1s - loss: 110.2049 - loglik: -1.0818e+02 - logprior: -1.4997e+00
Epoch 6/10
10/10 - 1s - loss: 108.9327 - loglik: -1.0851e+02 - logprior: 0.1196
Epoch 7/10
10/10 - 1s - loss: 108.1660 - loglik: -1.0873e+02 - logprior: 1.1058
Epoch 8/10
10/10 - 1s - loss: 107.6447 - loglik: -1.0887e+02 - logprior: 1.7784
Epoch 9/10
10/10 - 1s - loss: 107.2522 - loglik: -1.0898e+02 - logprior: 2.2869
Epoch 10/10
10/10 - 1s - loss: 106.9230 - loglik: -1.0904e+02 - logprior: 2.6800
Fitted a model with MAP estimate = -106.1875
Time for alignment: 34.8726
Computed alignments with likelihoods: ['-106.1643', '-106.1723', '-106.1875']
Best model has likelihood: -106.1643
time for generating output: 0.1180
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/toxin.projection.fasta
SP score = 0.916136074222303
Training of 3 independent models on file msb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f33a4482e80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3495ba9130>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3436c1f250>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34ecf1da30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34ecf1d070>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3436beaf40>, <__main__.SimpleDirichletPrior object at 0x7f343f8fa0a0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34edf4c820>

Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 316.2025 - loglik: -3.0835e+02 - logprior: -7.8118e+00
Epoch 2/10
13/13 - 2s - loss: 285.0364 - loglik: -2.8290e+02 - logprior: -1.9818e+00
Epoch 3/10
13/13 - 2s - loss: 260.9788 - loglik: -2.5893e+02 - logprior: -1.7809e+00
Epoch 4/10
13/13 - 2s - loss: 250.5982 - loglik: -2.4791e+02 - logprior: -2.0395e+00
Epoch 5/10
13/13 - 2s - loss: 248.0382 - loglik: -2.4540e+02 - logprior: -1.9746e+00
Epoch 6/10
13/13 - 2s - loss: 246.5915 - loglik: -2.4409e+02 - logprior: -1.9060e+00
Epoch 7/10
13/13 - 2s - loss: 245.6775 - loglik: -2.4315e+02 - logprior: -1.9400e+00
Epoch 8/10
13/13 - 2s - loss: 244.9248 - loglik: -2.4241e+02 - logprior: -1.9480e+00
Epoch 9/10
13/13 - 2s - loss: 244.7258 - loglik: -2.4225e+02 - logprior: -1.9187e+00
Epoch 10/10
13/13 - 2s - loss: 244.2384 - loglik: -2.4178e+02 - logprior: -1.9155e+00
Fitted a model with MAP estimate = -243.6250
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 1), (29, 1), (30, 2), (31, 2), (32, 3), (40, 1), (43, 1), (52, 2), (63, 2), (64, 1), (68, 1), (69, 5), (70, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 253.2685 - loglik: -2.4343e+02 - logprior: -9.3301e+00
Epoch 2/2
13/13 - 2s - loss: 242.2134 - loglik: -2.3724e+02 - logprior: -4.4164e+00
Fitted a model with MAP estimate = -239.8267
expansions: [(0, 2)]
discards: [ 0 14 67 93 96]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 242.9492 - loglik: -2.3510e+02 - logprior: -7.1712e+00
Epoch 2/2
13/13 - 2s - loss: 236.6896 - loglik: -2.3366e+02 - logprior: -2.2309e+00
Fitted a model with MAP estimate = -234.8788
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 240.8158 - loglik: -2.3279e+02 - logprior: -7.1613e+00
Epoch 2/10
13/13 - 2s - loss: 235.3010 - loglik: -2.3219e+02 - logprior: -2.2430e+00
Epoch 3/10
13/13 - 2s - loss: 235.2093 - loglik: -2.3271e+02 - logprior: -1.5634e+00
Epoch 4/10
13/13 - 2s - loss: 233.6329 - loglik: -2.3126e+02 - logprior: -1.3535e+00
Epoch 5/10
13/13 - 2s - loss: 233.0359 - loglik: -2.3051e+02 - logprior: -1.2574e+00
Epoch 6/10
13/13 - 2s - loss: 231.9967 - loglik: -2.2933e+02 - logprior: -1.2411e+00
Epoch 7/10
13/13 - 2s - loss: 231.7200 - loglik: -2.2915e+02 - logprior: -1.2168e+00
Epoch 8/10
13/13 - 2s - loss: 230.3209 - loglik: -2.2781e+02 - logprior: -1.1834e+00
Epoch 9/10
13/13 - 2s - loss: 230.6404 - loglik: -2.2830e+02 - logprior: -1.1418e+00
Fitted a model with MAP estimate = -229.1284
Time for alignment: 70.0647
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 316.5699 - loglik: -3.0872e+02 - logprior: -7.8134e+00
Epoch 2/10
13/13 - 2s - loss: 284.9592 - loglik: -2.8281e+02 - logprior: -1.9967e+00
Epoch 3/10
13/13 - 2s - loss: 261.9211 - loglik: -2.5986e+02 - logprior: -1.8428e+00
Epoch 4/10
13/13 - 2s - loss: 250.7652 - loglik: -2.4811e+02 - logprior: -2.1659e+00
Epoch 5/10
13/13 - 2s - loss: 248.3180 - loglik: -2.4556e+02 - logprior: -2.1311e+00
Epoch 6/10
13/13 - 2s - loss: 246.8363 - loglik: -2.4424e+02 - logprior: -2.0168e+00
Epoch 7/10
13/13 - 2s - loss: 245.6465 - loglik: -2.4304e+02 - logprior: -2.0220e+00
Epoch 8/10
13/13 - 2s - loss: 245.8503 - loglik: -2.4325e+02 - logprior: -2.0277e+00
Fitted a model with MAP estimate = -244.7003
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (19, 1), (27, 1), (28, 1), (29, 1), (30, 3), (31, 2), (39, 1), (40, 1), (44, 1), (56, 1), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 249.4136 - loglik: -2.4154e+02 - logprior: -7.3096e+00
Epoch 2/2
13/13 - 2s - loss: 239.7315 - loglik: -2.3683e+02 - logprior: -2.3103e+00
Fitted a model with MAP estimate = -237.3402
expansions: []
discards: [ 0 15 91 96]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 247.3848 - loglik: -2.3741e+02 - logprior: -9.2622e+00
Epoch 2/2
13/13 - 2s - loss: 240.3245 - loglik: -2.3503e+02 - logprior: -4.4491e+00
Fitted a model with MAP estimate = -238.4900
expansions: [(0, 2)]
discards: [ 0 37 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 241.6266 - loglik: -2.3354e+02 - logprior: -7.1850e+00
Epoch 2/10
13/13 - 2s - loss: 236.5803 - loglik: -2.3346e+02 - logprior: -2.2315e+00
Epoch 3/10
13/13 - 2s - loss: 235.5815 - loglik: -2.3311e+02 - logprior: -1.5888e+00
Epoch 4/10
13/13 - 2s - loss: 234.4929 - loglik: -2.3228e+02 - logprior: -1.3479e+00
Epoch 5/10
13/13 - 2s - loss: 234.6856 - loglik: -2.3258e+02 - logprior: -1.2393e+00
Fitted a model with MAP estimate = -233.2079
Time for alignment: 56.4629
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 316.5695 - loglik: -3.0872e+02 - logprior: -7.8152e+00
Epoch 2/10
13/13 - 2s - loss: 285.6494 - loglik: -2.8351e+02 - logprior: -1.9917e+00
Epoch 3/10
13/13 - 2s - loss: 262.1607 - loglik: -2.6012e+02 - logprior: -1.8229e+00
Epoch 4/10
13/13 - 2s - loss: 251.0355 - loglik: -2.4844e+02 - logprior: -2.1156e+00
Epoch 5/10
13/13 - 2s - loss: 248.2833 - loglik: -2.4558e+02 - logprior: -2.0625e+00
Epoch 6/10
13/13 - 2s - loss: 247.2772 - loglik: -2.4472e+02 - logprior: -1.9491e+00
Epoch 7/10
13/13 - 2s - loss: 246.2504 - loglik: -2.4369e+02 - logprior: -1.9748e+00
Epoch 8/10
13/13 - 2s - loss: 245.4288 - loglik: -2.4285e+02 - logprior: -1.9801e+00
Epoch 9/10
13/13 - 2s - loss: 245.1428 - loglik: -2.4260e+02 - logprior: -1.9674e+00
Epoch 10/10
13/13 - 2s - loss: 244.5424 - loglik: -2.4204e+02 - logprior: -1.9686e+00
Fitted a model with MAP estimate = -243.9226
expansions: [(9, 1), (10, 1), (13, 1), (15, 1), (28, 1), (29, 1), (30, 4), (31, 2), (32, 2), (40, 1), (43, 1), (44, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 252.9556 - loglik: -2.4310e+02 - logprior: -9.3336e+00
Epoch 2/2
13/13 - 2s - loss: 242.4554 - loglik: -2.3753e+02 - logprior: -4.3752e+00
Fitted a model with MAP estimate = -239.7932
expansions: [(0, 2)]
discards: [ 0 43 67 96]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 242.7870 - loglik: -2.3492e+02 - logprior: -7.1731e+00
Epoch 2/2
13/13 - 2s - loss: 236.8459 - loglik: -2.3381e+02 - logprior: -2.2401e+00
Fitted a model with MAP estimate = -234.7629
expansions: []
discards: [ 0 39 90]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 244.8500 - loglik: -2.3505e+02 - logprior: -8.9517e+00
Epoch 2/10
13/13 - 2s - loss: 237.4726 - loglik: -2.3356e+02 - logprior: -3.0641e+00
Epoch 3/10
13/13 - 2s - loss: 235.8952 - loglik: -2.3332e+02 - logprior: -1.7083e+00
Epoch 4/10
13/13 - 2s - loss: 235.4594 - loglik: -2.3309e+02 - logprior: -1.4681e+00
Epoch 5/10
13/13 - 2s - loss: 233.9991 - loglik: -2.3166e+02 - logprior: -1.3253e+00
Epoch 6/10
13/13 - 2s - loss: 233.3540 - loglik: -2.3080e+02 - logprior: -1.2896e+00
Epoch 7/10
13/13 - 2s - loss: 233.0409 - loglik: -2.3037e+02 - logprior: -1.2795e+00
Epoch 8/10
13/13 - 2s - loss: 231.8042 - loglik: -2.2926e+02 - logprior: -1.2449e+00
Epoch 9/10
13/13 - 2s - loss: 231.6005 - loglik: -2.2911e+02 - logprior: -1.2157e+00
Epoch 10/10
13/13 - 2s - loss: 231.1574 - loglik: -2.2881e+02 - logprior: -1.1661e+00
Fitted a model with MAP estimate = -230.0236
Time for alignment: 68.6287
Computed alignments with likelihoods: ['-229.1284', '-233.2079', '-230.0236']
Best model has likelihood: -229.1284
time for generating output: 0.1833
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/msb.projection.fasta
SP score = 0.9239821882951654
Training of 3 independent models on file rnasemam.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3473ceed30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f342d017700>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f342d017460>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34af3a4f10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34af3a41c0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f346b191820>, <__main__.SimpleDirichletPrior object at 0x7f34b7cc2370>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f347c2b1280>

Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 433.8089 - loglik: -3.4692e+02 - logprior: -8.6875e+01
Epoch 2/10
10/10 - 1s - loss: 329.3674 - loglik: -3.0824e+02 - logprior: -2.1123e+01
Epoch 3/10
10/10 - 1s - loss: 278.4340 - loglik: -2.6912e+02 - logprior: -9.2992e+00
Epoch 4/10
10/10 - 1s - loss: 247.0998 - loglik: -2.4130e+02 - logprior: -5.7943e+00
Epoch 5/10
10/10 - 1s - loss: 232.2598 - loglik: -2.2819e+02 - logprior: -3.9976e+00
Epoch 6/10
10/10 - 1s - loss: 225.9519 - loglik: -2.2273e+02 - logprior: -2.9026e+00
Epoch 7/10
10/10 - 1s - loss: 222.7810 - loglik: -2.2030e+02 - logprior: -2.0431e+00
Epoch 8/10
10/10 - 1s - loss: 221.1963 - loglik: -2.1942e+02 - logprior: -1.3968e+00
Epoch 9/10
10/10 - 1s - loss: 220.3376 - loglik: -2.1908e+02 - logprior: -9.4040e-01
Epoch 10/10
10/10 - 1s - loss: 219.7615 - loglik: -2.1886e+02 - logprior: -5.9175e-01
Fitted a model with MAP estimate = -219.1774
expansions: [(12, 1), (13, 2), (14, 2), (24, 1), (29, 1), (30, 1), (40, 1), (41, 1), (55, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 307.3861 - loglik: -2.0926e+02 - logprior: -9.7744e+01
Epoch 2/2
10/10 - 2s - loss: 233.8771 - loglik: -1.9522e+02 - logprior: -3.8232e+01
Fitted a model with MAP estimate = -220.8215
expansions: [(0, 2)]
discards: [  0 111 112]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 266.2487 - loglik: -1.8883e+02 - logprior: -7.7003e+01
Epoch 2/2
10/10 - 2s - loss: 205.6397 - loglik: -1.8730e+02 - logprior: -1.7893e+01
Fitted a model with MAP estimate = -196.4535
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 280.4573 - loglik: -1.8865e+02 - logprior: -9.1356e+01
Epoch 2/10
10/10 - 2s - loss: 213.4838 - loglik: -1.8913e+02 - logprior: -2.3897e+01
Epoch 3/10
10/10 - 2s - loss: 196.1031 - loglik: -1.8957e+02 - logprior: -6.0731e+00
Epoch 4/10
10/10 - 2s - loss: 189.9477 - loglik: -1.8980e+02 - logprior: 0.3154
Epoch 5/10
10/10 - 2s - loss: 186.8464 - loglik: -1.8992e+02 - logprior: 3.5444
Epoch 6/10
10/10 - 2s - loss: 185.0719 - loglik: -1.9001e+02 - logprior: 5.4131
Epoch 7/10
10/10 - 2s - loss: 183.9427 - loglik: -1.9009e+02 - logprior: 6.6166
Epoch 8/10
10/10 - 2s - loss: 183.1340 - loglik: -1.9017e+02 - logprior: 7.5014
Epoch 9/10
10/10 - 2s - loss: 182.4798 - loglik: -1.9026e+02 - logprior: 8.2510
Epoch 10/10
10/10 - 2s - loss: 181.9027 - loglik: -1.9037e+02 - logprior: 8.9367
Fitted a model with MAP estimate = -181.1347
Time for alignment: 54.9678
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 433.8088 - loglik: -3.4692e+02 - logprior: -8.6875e+01
Epoch 2/10
10/10 - 1s - loss: 329.3672 - loglik: -3.0824e+02 - logprior: -2.1123e+01
Epoch 3/10
10/10 - 1s - loss: 278.4342 - loglik: -2.6912e+02 - logprior: -9.2993e+00
Epoch 4/10
10/10 - 1s - loss: 247.1705 - loglik: -2.4139e+02 - logprior: -5.7766e+00
Epoch 5/10
10/10 - 1s - loss: 233.0858 - loglik: -2.2925e+02 - logprior: -3.8285e+00
Epoch 6/10
10/10 - 1s - loss: 226.1681 - loglik: -2.2320e+02 - logprior: -2.8069e+00
Epoch 7/10
10/10 - 1s - loss: 222.7446 - loglik: -2.2035e+02 - logprior: -1.9985e+00
Epoch 8/10
10/10 - 1s - loss: 221.2019 - loglik: -2.1941e+02 - logprior: -1.3687e+00
Epoch 9/10
10/10 - 1s - loss: 220.3094 - loglik: -2.1906e+02 - logprior: -9.0115e-01
Epoch 10/10
10/10 - 1s - loss: 219.7013 - loglik: -2.1885e+02 - logprior: -5.4277e-01
Fitted a model with MAP estimate = -219.1105
expansions: [(13, 3), (14, 1), (24, 1), (29, 1), (30, 1), (40, 1), (41, 1), (55, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 307.2733 - loglik: -2.0912e+02 - logprior: -9.7777e+01
Epoch 2/2
10/10 - 2s - loss: 234.5201 - loglik: -1.9587e+02 - logprior: -3.8233e+01
Fitted a model with MAP estimate = -221.7102
expansions: [(0, 2)]
discards: [  0 110 111]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 267.2244 - loglik: -1.8980e+02 - logprior: -7.7009e+01
Epoch 2/2
10/10 - 2s - loss: 206.6838 - loglik: -1.8828e+02 - logprior: -1.7956e+01
Fitted a model with MAP estimate = -197.5038
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 281.3360 - loglik: -1.8965e+02 - logprior: -9.1238e+01
Epoch 2/10
10/10 - 2s - loss: 214.3215 - loglik: -1.9011e+02 - logprior: -2.3758e+01
Epoch 3/10
10/10 - 2s - loss: 197.1492 - loglik: -1.9058e+02 - logprior: -6.1102e+00
Epoch 4/10
10/10 - 2s - loss: 191.0437 - loglik: -1.9082e+02 - logprior: 0.2419
Epoch 5/10
10/10 - 2s - loss: 187.9727 - loglik: -1.9095e+02 - logprior: 3.4411
Epoch 6/10
10/10 - 2s - loss: 186.2068 - loglik: -1.9105e+02 - logprior: 5.3060
Epoch 7/10
10/10 - 2s - loss: 185.0804 - loglik: -1.9111e+02 - logprior: 6.4993
Epoch 8/10
10/10 - 2s - loss: 184.2761 - loglik: -1.9118e+02 - logprior: 7.3726
Epoch 9/10
10/10 - 2s - loss: 183.6285 - loglik: -1.9127e+02 - logprior: 8.1108
Epoch 10/10
10/10 - 2s - loss: 183.0583 - loglik: -1.9138e+02 - logprior: 8.7904
Fitted a model with MAP estimate = -182.2921
Time for alignment: 54.5891
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 433.8089 - loglik: -3.4692e+02 - logprior: -8.6875e+01
Epoch 2/10
10/10 - 1s - loss: 329.3674 - loglik: -3.0824e+02 - logprior: -2.1123e+01
Epoch 3/10
10/10 - 1s - loss: 278.4340 - loglik: -2.6912e+02 - logprior: -9.2993e+00
Epoch 4/10
10/10 - 1s - loss: 247.0310 - loglik: -2.4122e+02 - logprior: -5.8085e+00
Epoch 5/10
10/10 - 1s - loss: 232.0689 - loglik: -2.2793e+02 - logprior: -4.0369e+00
Epoch 6/10
10/10 - 1s - loss: 225.9419 - loglik: -2.2267e+02 - logprior: -2.9194e+00
Epoch 7/10
10/10 - 1s - loss: 222.7917 - loglik: -2.2031e+02 - logprior: -2.0516e+00
Epoch 8/10
10/10 - 1s - loss: 221.1982 - loglik: -2.1943e+02 - logprior: -1.4027e+00
Epoch 9/10
10/10 - 1s - loss: 220.3474 - loglik: -2.1910e+02 - logprior: -9.4318e-01
Epoch 10/10
10/10 - 1s - loss: 219.7858 - loglik: -2.1890e+02 - logprior: -5.8891e-01
Fitted a model with MAP estimate = -219.2142
expansions: [(12, 1), (13, 2), (14, 2), (24, 1), (29, 1), (30, 1), (40, 1), (41, 1), (55, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 307.3541 - loglik: -2.0926e+02 - logprior: -9.7730e+01
Epoch 2/2
10/10 - 2s - loss: 233.8416 - loglik: -1.9519e+02 - logprior: -3.8232e+01
Fitted a model with MAP estimate = -220.7900
expansions: [(0, 2)]
discards: [  0 111 112]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 266.2301 - loglik: -1.8881e+02 - logprior: -7.7003e+01
Epoch 2/2
10/10 - 2s - loss: 205.6359 - loglik: -1.8730e+02 - logprior: -1.7898e+01
Fitted a model with MAP estimate = -196.4524
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 280.4568 - loglik: -1.8865e+02 - logprior: -9.1358e+01
Epoch 2/10
10/10 - 2s - loss: 213.4775 - loglik: -1.8911e+02 - logprior: -2.3912e+01
Epoch 3/10
10/10 - 2s - loss: 196.1090 - loglik: -1.8955e+02 - logprior: -6.0967e+00
Epoch 4/10
10/10 - 2s - loss: 189.9579 - loglik: -1.8980e+02 - logprior: 0.3126
Epoch 5/10
10/10 - 2s - loss: 186.8558 - loglik: -1.8992e+02 - logprior: 3.5310
Epoch 6/10
10/10 - 2s - loss: 185.0832 - loglik: -1.9001e+02 - logprior: 5.3971
Epoch 7/10
10/10 - 2s - loss: 183.9558 - loglik: -1.9009e+02 - logprior: 6.6011
Epoch 8/10
10/10 - 2s - loss: 183.1489 - loglik: -1.9017e+02 - logprior: 7.4843
Epoch 9/10
10/10 - 2s - loss: 182.4961 - loglik: -1.9026e+02 - logprior: 8.2324
Epoch 10/10
10/10 - 2s - loss: 181.9213 - loglik: -1.9037e+02 - logprior: 8.9152
Fitted a model with MAP estimate = -181.1572
Time for alignment: 55.4892
Computed alignments with likelihoods: ['-181.1347', '-182.2921', '-181.1572']
Best model has likelihood: -181.1347
time for generating output: 0.1706
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rnasemam.projection.fasta
SP score = 0.8740298507462687
Training of 3 independent models on file rub.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34b7b1c0a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34ede84af0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3324510520>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f33a3fec880>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33a3fec730>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f348d72ba00>, <__main__.SimpleDirichletPrior object at 0x7f34a6bfd460>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3335519e50>

Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 167.3933 - loglik: -1.3553e+02 - logprior: -3.1847e+01
Epoch 2/10
10/10 - 1s - loss: 124.2845 - loglik: -1.1515e+02 - logprior: -9.1284e+00
Epoch 3/10
10/10 - 1s - loss: 99.1550 - loglik: -9.4078e+01 - logprior: -5.0458e+00
Epoch 4/10
10/10 - 1s - loss: 83.8940 - loglik: -7.9932e+01 - logprior: -3.9445e+00
Epoch 5/10
10/10 - 1s - loss: 78.7913 - loglik: -7.5104e+01 - logprior: -3.6464e+00
Epoch 6/10
10/10 - 1s - loss: 76.6704 - loglik: -7.3057e+01 - logprior: -3.4174e+00
Epoch 7/10
10/10 - 1s - loss: 75.8726 - loglik: -7.2593e+01 - logprior: -3.0090e+00
Epoch 8/10
10/10 - 1s - loss: 75.4380 - loglik: -7.2476e+01 - logprior: -2.7255e+00
Epoch 9/10
10/10 - 1s - loss: 75.1973 - loglik: -7.2359e+01 - logprior: -2.6216e+00
Epoch 10/10
10/10 - 1s - loss: 74.9453 - loglik: -7.2137e+01 - logprior: -2.5756e+00
Fitted a model with MAP estimate = -74.7027
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 113.5360 - loglik: -7.0634e+01 - logprior: -4.2651e+01
Epoch 2/2
10/10 - 1s - loss: 77.4563 - loglik: -6.3019e+01 - logprior: -1.4179e+01
Fitted a model with MAP estimate = -70.1922
expansions: []
discards: [30 33]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 89.8921 - loglik: -5.9393e+01 - logprior: -3.0236e+01
Epoch 2/2
10/10 - 1s - loss: 68.4901 - loglik: -5.9393e+01 - logprior: -8.8320e+00
Fitted a model with MAP estimate = -65.3130
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 87.4298 - loglik: -5.8529e+01 - logprior: -2.8625e+01
Epoch 2/10
10/10 - 1s - loss: 67.8922 - loglik: -5.9234e+01 - logprior: -8.3879e+00
Epoch 3/10
10/10 - 1s - loss: 64.3650 - loglik: -5.9711e+01 - logprior: -4.3729e+00
Epoch 4/10
10/10 - 1s - loss: 63.2093 - loglik: -6.0082e+01 - logprior: -2.8484e+00
Epoch 5/10
10/10 - 1s - loss: 62.4894 - loglik: -6.0097e+01 - logprior: -2.1110e+00
Epoch 6/10
10/10 - 1s - loss: 62.3425 - loglik: -6.0313e+01 - logprior: -1.7490e+00
Epoch 7/10
10/10 - 1s - loss: 62.0285 - loglik: -6.0241e+01 - logprior: -1.5085e+00
Epoch 8/10
10/10 - 1s - loss: 62.0607 - loglik: -6.0474e+01 - logprior: -1.3041e+00
Fitted a model with MAP estimate = -61.6329
Time for alignment: 30.4404
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 167.4783 - loglik: -1.3562e+02 - logprior: -3.1848e+01
Epoch 2/10
10/10 - 1s - loss: 124.2474 - loglik: -1.1511e+02 - logprior: -9.1283e+00
Epoch 3/10
10/10 - 1s - loss: 99.8832 - loglik: -9.4817e+01 - logprior: -5.0349e+00
Epoch 4/10
10/10 - 1s - loss: 84.9447 - loglik: -8.1018e+01 - logprior: -3.9103e+00
Epoch 5/10
10/10 - 1s - loss: 79.2674 - loglik: -7.5648e+01 - logprior: -3.6095e+00
Epoch 6/10
10/10 - 1s - loss: 77.3529 - loglik: -7.3824e+01 - logprior: -3.4249e+00
Epoch 7/10
10/10 - 1s - loss: 76.0979 - loglik: -7.2808e+01 - logprior: -3.0232e+00
Epoch 8/10
10/10 - 1s - loss: 75.6968 - loglik: -7.2681e+01 - logprior: -2.7280e+00
Epoch 9/10
10/10 - 1s - loss: 75.2924 - loglik: -7.2437e+01 - logprior: -2.6142e+00
Epoch 10/10
10/10 - 1s - loss: 75.1087 - loglik: -7.2292e+01 - logprior: -2.5794e+00
Fitted a model with MAP estimate = -74.7867
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 113.5681 - loglik: -7.0686e+01 - logprior: -4.2627e+01
Epoch 2/2
10/10 - 1s - loss: 77.3427 - loglik: -6.2930e+01 - logprior: -1.4163e+01
Fitted a model with MAP estimate = -70.1729
expansions: []
discards: [30 33]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 89.8499 - loglik: -5.9360e+01 - logprior: -3.0233e+01
Epoch 2/2
10/10 - 1s - loss: 68.5890 - loglik: -5.9498e+01 - logprior: -8.8289e+00
Fitted a model with MAP estimate = -65.3156
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 87.3223 - loglik: -5.8427e+01 - logprior: -2.8625e+01
Epoch 2/10
10/10 - 1s - loss: 67.9982 - loglik: -5.9340e+01 - logprior: -8.3860e+00
Epoch 3/10
10/10 - 1s - loss: 64.4378 - loglik: -5.9791e+01 - logprior: -4.3705e+00
Epoch 4/10
10/10 - 1s - loss: 63.1588 - loglik: -6.0037e+01 - logprior: -2.8445e+00
Epoch 5/10
10/10 - 1s - loss: 62.5527 - loglik: -6.0160e+01 - logprior: -2.1102e+00
Epoch 6/10
10/10 - 1s - loss: 62.1175 - loglik: -6.0089e+01 - logprior: -1.7505e+00
Epoch 7/10
10/10 - 1s - loss: 62.2045 - loglik: -6.0414e+01 - logprior: -1.5106e+00
Fitted a model with MAP estimate = -61.7266
Time for alignment: 29.8424
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 167.4375 - loglik: -1.3558e+02 - logprior: -3.1847e+01
Epoch 2/10
10/10 - 1s - loss: 124.0641 - loglik: -1.1493e+02 - logprior: -9.1250e+00
Epoch 3/10
10/10 - 1s - loss: 98.2777 - loglik: -9.3195e+01 - logprior: -5.0501e+00
Epoch 4/10
10/10 - 1s - loss: 83.6701 - loglik: -7.9693e+01 - logprior: -3.9589e+00
Epoch 5/10
10/10 - 1s - loss: 78.6103 - loglik: -7.4965e+01 - logprior: -3.6369e+00
Epoch 6/10
10/10 - 1s - loss: 76.6976 - loglik: -7.3227e+01 - logprior: -3.3783e+00
Epoch 7/10
10/10 - 1s - loss: 75.9942 - loglik: -7.2780e+01 - logprior: -2.9775e+00
Epoch 8/10
10/10 - 1s - loss: 75.5657 - loglik: -7.2594e+01 - logprior: -2.7052e+00
Epoch 9/10
10/10 - 1s - loss: 75.2268 - loglik: -7.2384e+01 - logprior: -2.6070e+00
Epoch 10/10
10/10 - 1s - loss: 75.1222 - loglik: -7.2336e+01 - logprior: -2.5559e+00
Fitted a model with MAP estimate = -74.8152
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 113.6149 - loglik: -7.0740e+01 - logprior: -4.2618e+01
Epoch 2/2
10/10 - 1s - loss: 77.4768 - loglik: -6.3054e+01 - logprior: -1.4143e+01
Fitted a model with MAP estimate = -70.1934
expansions: []
discards: [30 33]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 89.8972 - loglik: -5.9420e+01 - logprior: -3.0220e+01
Epoch 2/2
10/10 - 1s - loss: 68.5445 - loglik: -5.9454e+01 - logprior: -8.8346e+00
Fitted a model with MAP estimate = -65.3356
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 87.4035 - loglik: -5.8518e+01 - logprior: -2.8616e+01
Epoch 2/10
10/10 - 1s - loss: 67.8945 - loglik: -5.9236e+01 - logprior: -8.3893e+00
Epoch 3/10
10/10 - 1s - loss: 64.3402 - loglik: -5.9694e+01 - logprior: -4.3706e+00
Epoch 4/10
10/10 - 1s - loss: 63.1827 - loglik: -6.0066e+01 - logprior: -2.8394e+00
Epoch 5/10
10/10 - 1s - loss: 62.6505 - loglik: -6.0262e+01 - logprior: -2.1115e+00
Epoch 6/10
10/10 - 1s - loss: 62.3658 - loglik: -6.0343e+01 - logprior: -1.7454e+00
Epoch 7/10
10/10 - 1s - loss: 61.9388 - loglik: -6.0152e+01 - logprior: -1.5076e+00
Epoch 8/10
10/10 - 1s - loss: 61.9374 - loglik: -6.0353e+01 - logprior: -1.3035e+00
Epoch 9/10
10/10 - 1s - loss: 62.0086 - loglik: -6.0585e+01 - logprior: -1.1441e+00
Fitted a model with MAP estimate = -61.5552
Time for alignment: 30.9770
Computed alignments with likelihoods: ['-61.6329', '-61.7266', '-61.5552']
Best model has likelihood: -61.5552
time for generating output: 0.1079
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rub.projection.fasta
SP score = 0.9918367346938776
Training of 3 independent models on file cyclo.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f343fd7efa0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f347c7b6cd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34af205be0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34af2051f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34738a8c10>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3424662b50>, <__main__.SimpleDirichletPrior object at 0x7f3356fdeb80>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34243b8ee0>

Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 445.5705 - loglik: -4.4010e+02 - logprior: -5.4644e+00
Epoch 2/10
15/15 - 4s - loss: 367.5418 - loglik: -3.6610e+02 - logprior: -1.4187e+00
Epoch 3/10
15/15 - 4s - loss: 324.1473 - loglik: -3.2246e+02 - logprior: -1.6701e+00
Epoch 4/10
15/15 - 4s - loss: 308.9944 - loglik: -3.0696e+02 - logprior: -1.8695e+00
Epoch 5/10
15/15 - 4s - loss: 303.5972 - loglik: -3.0154e+02 - logprior: -1.7748e+00
Epoch 6/10
15/15 - 4s - loss: 300.9536 - loglik: -2.9891e+02 - logprior: -1.7793e+00
Epoch 7/10
15/15 - 4s - loss: 299.7655 - loglik: -2.9775e+02 - logprior: -1.7680e+00
Epoch 8/10
15/15 - 4s - loss: 299.5544 - loglik: -2.9757e+02 - logprior: -1.7473e+00
Epoch 9/10
15/15 - 4s - loss: 298.2012 - loglik: -2.9622e+02 - logprior: -1.7437e+00
Epoch 10/10
15/15 - 4s - loss: 297.8716 - loglik: -2.9589e+02 - logprior: -1.7460e+00
Fitted a model with MAP estimate = -297.5483
expansions: [(7, 3), (10, 1), (16, 1), (24, 2), (26, 1), (49, 2), (60, 1), (66, 2), (67, 1), (69, 1), (91, 1), (104, 1), (112, 1), (114, 4), (116, 3), (119, 1), (120, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 154 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 297.2912 - loglik: -2.9030e+02 - logprior: -6.7760e+00
Epoch 2/2
15/15 - 5s - loss: 283.6318 - loglik: -2.7998e+02 - logprior: -3.4344e+00
Fitted a model with MAP estimate = -282.0313
expansions: [(0, 2)]
discards: [ 0  7 57 77]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 285.2504 - loglik: -2.7981e+02 - logprior: -5.2002e+00
Epoch 2/2
15/15 - 5s - loss: 279.7736 - loglik: -2.7783e+02 - logprior: -1.7064e+00
Fitted a model with MAP estimate = -279.4672
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 286.4074 - loglik: -2.7952e+02 - logprior: -6.6245e+00
Epoch 2/10
15/15 - 5s - loss: 281.2098 - loglik: -2.7865e+02 - logprior: -2.2973e+00
Epoch 3/10
15/15 - 5s - loss: 279.8719 - loglik: -2.7830e+02 - logprior: -1.2900e+00
Epoch 4/10
15/15 - 5s - loss: 279.3266 - loglik: -2.7793e+02 - logprior: -1.1038e+00
Epoch 5/10
15/15 - 5s - loss: 278.9265 - loglik: -2.7758e+02 - logprior: -1.0494e+00
Epoch 6/10
15/15 - 5s - loss: 278.7352 - loglik: -2.7742e+02 - logprior: -1.0162e+00
Epoch 7/10
15/15 - 5s - loss: 279.0906 - loglik: -2.7781e+02 - logprior: -9.8177e-01
Fitted a model with MAP estimate = -278.3292
Time for alignment: 139.1999
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 446.0558 - loglik: -4.4058e+02 - logprior: -5.4689e+00
Epoch 2/10
15/15 - 4s - loss: 365.9077 - loglik: -3.6448e+02 - logprior: -1.4091e+00
Epoch 3/10
15/15 - 4s - loss: 318.8635 - loglik: -3.1720e+02 - logprior: -1.6479e+00
Epoch 4/10
15/15 - 4s - loss: 305.6665 - loglik: -3.0364e+02 - logprior: -1.8629e+00
Epoch 5/10
15/15 - 4s - loss: 302.0410 - loglik: -3.0002e+02 - logprior: -1.7490e+00
Epoch 6/10
15/15 - 4s - loss: 298.9347 - loglik: -2.9694e+02 - logprior: -1.7255e+00
Epoch 7/10
15/15 - 4s - loss: 300.9078 - loglik: -2.9897e+02 - logprior: -1.7060e+00
Fitted a model with MAP estimate = -298.5583
expansions: [(7, 3), (10, 1), (14, 1), (24, 3), (25, 1), (49, 2), (55, 1), (59, 1), (66, 2), (69, 1), (91, 2), (92, 1), (112, 1), (114, 4), (116, 3), (117, 1), (118, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 298.8094 - loglik: -2.9186e+02 - logprior: -6.7416e+00
Epoch 2/2
15/15 - 5s - loss: 283.0915 - loglik: -2.7944e+02 - logprior: -3.4383e+00
Fitted a model with MAP estimate = -281.5312
expansions: [(0, 2)]
discards: [  0   7  29  58 107]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 284.8738 - loglik: -2.7944e+02 - logprior: -5.1786e+00
Epoch 2/2
15/15 - 5s - loss: 279.0824 - loglik: -2.7714e+02 - logprior: -1.6856e+00
Fitted a model with MAP estimate = -278.3531
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 286.4306 - loglik: -2.7956e+02 - logprior: -6.5963e+00
Epoch 2/10
15/15 - 5s - loss: 279.4071 - loglik: -2.7687e+02 - logprior: -2.2682e+00
Epoch 3/10
15/15 - 5s - loss: 279.6856 - loglik: -2.7813e+02 - logprior: -1.2736e+00
Fitted a model with MAP estimate = -278.3550
Time for alignment: 106.5209
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 445.0401 - loglik: -4.3957e+02 - logprior: -5.4634e+00
Epoch 2/10
15/15 - 4s - loss: 365.1141 - loglik: -3.6368e+02 - logprior: -1.4140e+00
Epoch 3/10
15/15 - 4s - loss: 316.6342 - loglik: -3.1492e+02 - logprior: -1.6982e+00
Epoch 4/10
15/15 - 4s - loss: 304.2551 - loglik: -3.0211e+02 - logprior: -1.9789e+00
Epoch 5/10
15/15 - 4s - loss: 301.7034 - loglik: -2.9956e+02 - logprior: -1.8565e+00
Epoch 6/10
15/15 - 4s - loss: 299.1751 - loglik: -2.9706e+02 - logprior: -1.8453e+00
Epoch 7/10
15/15 - 4s - loss: 298.4196 - loglik: -2.9633e+02 - logprior: -1.8442e+00
Epoch 8/10
15/15 - 4s - loss: 298.1451 - loglik: -2.9604e+02 - logprior: -1.8480e+00
Epoch 9/10
15/15 - 4s - loss: 296.1954 - loglik: -2.9406e+02 - logprior: -1.8640e+00
Epoch 10/10
15/15 - 4s - loss: 296.9096 - loglik: -2.9478e+02 - logprior: -1.8619e+00
Fitted a model with MAP estimate = -296.1095
expansions: [(5, 1), (7, 2), (10, 1), (16, 1), (24, 2), (27, 1), (49, 2), (60, 1), (66, 3), (67, 1), (69, 1), (91, 1), (92, 2), (108, 1), (112, 1), (114, 1), (115, 1), (116, 4), (119, 1), (120, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 10s - loss: 297.6269 - loglik: -2.9060e+02 - logprior: -6.7997e+00
Epoch 2/2
15/15 - 5s - loss: 283.1132 - loglik: -2.7945e+02 - logprior: -3.4275e+00
Fitted a model with MAP estimate = -280.8254
expansions: [(0, 2)]
discards: [  0   7  57 108]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 154 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 282.6739 - loglik: -2.7722e+02 - logprior: -5.1958e+00
Epoch 2/2
15/15 - 5s - loss: 278.2755 - loglik: -2.7630e+02 - logprior: -1.6995e+00
Fitted a model with MAP estimate = -276.8800
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 10s - loss: 284.5241 - loglik: -2.7761e+02 - logprior: -6.6356e+00
Epoch 2/10
15/15 - 5s - loss: 278.8781 - loglik: -2.7628e+02 - logprior: -2.3170e+00
Epoch 3/10
15/15 - 5s - loss: 277.4658 - loglik: -2.7589e+02 - logprior: -1.2708e+00
Epoch 4/10
15/15 - 5s - loss: 277.1715 - loglik: -2.7579e+02 - logprior: -1.0824e+00
Epoch 5/10
15/15 - 5s - loss: 276.9793 - loglik: -2.7565e+02 - logprior: -1.0240e+00
Epoch 6/10
15/15 - 5s - loss: 276.4287 - loglik: -2.7512e+02 - logprior: -9.9899e-01
Epoch 7/10
15/15 - 5s - loss: 277.0471 - loglik: -2.7576e+02 - logprior: -9.6250e-01
Fitted a model with MAP estimate = -276.3452
Time for alignment: 139.4962
Computed alignments with likelihoods: ['-278.3292', '-278.3531', '-276.3452']
Best model has likelihood: -276.3452
time for generating output: 0.2247
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyclo.projection.fasta
SP score = 0.8821070234113713
Training of 3 independent models on file gpdh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34375e2580>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f33a452eaf0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33a452e4f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f33a452ebb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33a452e730>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f347c4d2520>, <__main__.SimpleDirichletPrior object at 0x7f332442ed00>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f33a35c29d0>

Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 9s - loss: 299.4241 - loglik: -2.9641e+02 - logprior: -2.9262e+00
Epoch 2/10
34/34 - 6s - loss: 207.1334 - loglik: -2.0510e+02 - logprior: -1.8837e+00
Epoch 3/10
34/34 - 7s - loss: 200.3997 - loglik: -1.9825e+02 - logprior: -1.8872e+00
Epoch 4/10
34/34 - 6s - loss: 198.3544 - loglik: -1.9622e+02 - logprior: -1.8626e+00
Epoch 5/10
34/34 - 6s - loss: 198.4830 - loglik: -1.9636e+02 - logprior: -1.8446e+00
Fitted a model with MAP estimate = -197.8183
expansions: [(0, 2), (15, 1), (16, 4), (26, 2), (27, 2), (40, 1), (41, 1), (44, 1), (45, 1), (47, 1), (53, 1), (55, 1), (56, 1), (57, 1), (64, 1), (65, 1), (79, 1), (82, 1), (85, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 150 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 183.5944 - loglik: -1.8005e+02 - logprior: -3.2681e+00
Epoch 2/2
34/34 - 7s - loss: 175.5365 - loglik: -1.7371e+02 - logprior: -1.5099e+00
Fitted a model with MAP estimate = -173.7617
expansions: []
discards: [ 34 139]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 176.3929 - loglik: -1.7318e+02 - logprior: -2.8869e+00
Epoch 2/2
34/34 - 7s - loss: 174.0932 - loglik: -1.7241e+02 - logprior: -1.3391e+00
Fitted a model with MAP estimate = -173.8223
expansions: [(123, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 175.9979 - loglik: -1.7289e+02 - logprior: -2.7676e+00
Epoch 2/10
34/34 - 7s - loss: 174.1148 - loglik: -1.7256e+02 - logprior: -1.2004e+00
Epoch 3/10
34/34 - 7s - loss: 172.6101 - loglik: -1.7119e+02 - logprior: -1.0649e+00
Epoch 4/10
34/34 - 7s - loss: 173.8888 - loglik: -1.7256e+02 - logprior: -9.7428e-01
Fitted a model with MAP estimate = -172.7929
Time for alignment: 136.4923
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 300.6393 - loglik: -2.9760e+02 - logprior: -2.9506e+00
Epoch 2/10
34/34 - 6s - loss: 211.2861 - loglik: -2.0917e+02 - logprior: -2.0552e+00
Epoch 3/10
34/34 - 7s - loss: 202.9141 - loglik: -2.0049e+02 - logprior: -2.1522e+00
Epoch 4/10
34/34 - 6s - loss: 200.2830 - loglik: -1.9788e+02 - logprior: -2.1264e+00
Epoch 5/10
34/34 - 7s - loss: 200.7775 - loglik: -1.9836e+02 - logprior: -2.1203e+00
Fitted a model with MAP estimate = -199.4769
expansions: [(0, 2), (15, 1), (16, 1), (17, 3), (26, 1), (27, 1), (28, 1), (42, 1), (43, 1), (45, 1), (48, 1), (49, 1), (55, 1), (56, 2), (57, 1), (64, 1), (65, 1), (75, 1), (78, 1), (81, 1), (84, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 150 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 185.4928 - loglik: -1.8173e+02 - logprior: -3.4503e+00
Epoch 2/2
34/34 - 7s - loss: 174.6959 - loglik: -1.7279e+02 - logprior: -1.5768e+00
Fitted a model with MAP estimate = -174.2889
expansions: []
discards: [139]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 176.5731 - loglik: -1.7335e+02 - logprior: -2.8698e+00
Epoch 2/2
34/34 - 6s - loss: 174.2782 - loglik: -1.7247e+02 - logprior: -1.4387e+00
Fitted a model with MAP estimate = -173.7968
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 175.9704 - loglik: -1.7280e+02 - logprior: -2.8023e+00
Epoch 2/10
34/34 - 7s - loss: 175.0230 - loglik: -1.7335e+02 - logprior: -1.2993e+00
Epoch 3/10
34/34 - 7s - loss: 173.1738 - loglik: -1.7160e+02 - logprior: -1.1971e+00
Epoch 4/10
34/34 - 7s - loss: 173.8262 - loglik: -1.7235e+02 - logprior: -1.1057e+00
Fitted a model with MAP estimate = -173.0841
Time for alignment: 137.4711
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 11s - loss: 301.4132 - loglik: -2.9839e+02 - logprior: -2.9378e+00
Epoch 2/10
34/34 - 6s - loss: 214.0680 - loglik: -2.1212e+02 - logprior: -1.8851e+00
Epoch 3/10
34/34 - 6s - loss: 202.7879 - loglik: -2.0063e+02 - logprior: -1.8781e+00
Epoch 4/10
34/34 - 7s - loss: 200.3577 - loglik: -1.9822e+02 - logprior: -1.8740e+00
Epoch 5/10
34/34 - 6s - loss: 199.9510 - loglik: -1.9780e+02 - logprior: -1.8810e+00
Epoch 6/10
34/34 - 7s - loss: 199.9202 - loglik: -1.9775e+02 - logprior: -1.8827e+00
Epoch 7/10
34/34 - 6s - loss: 199.6466 - loglik: -1.9745e+02 - logprior: -1.8979e+00
Epoch 8/10
34/34 - 6s - loss: 200.1210 - loglik: -1.9791e+02 - logprior: -1.8976e+00
Fitted a model with MAP estimate = -199.0057
expansions: [(0, 2), (15, 1), (16, 4), (26, 2), (27, 2), (40, 1), (41, 1), (42, 1), (48, 1), (49, 1), (55, 1), (56, 2), (57, 1), (64, 1), (65, 1), (75, 1), (78, 1), (81, 1), (84, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 184.6449 - loglik: -1.8098e+02 - logprior: -3.3857e+00
Epoch 2/2
34/34 - 7s - loss: 172.9055 - loglik: -1.7120e+02 - logprior: -1.3834e+00
Fitted a model with MAP estimate = -173.1574
expansions: []
discards: [ 34 140]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 11s - loss: 175.2930 - loglik: -1.7210e+02 - logprior: -2.8690e+00
Epoch 2/2
34/34 - 7s - loss: 174.5352 - loglik: -1.7287e+02 - logprior: -1.3285e+00
Fitted a model with MAP estimate = -173.2109
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 175.1315 - loglik: -1.7205e+02 - logprior: -2.7615e+00
Epoch 2/10
34/34 - 7s - loss: 174.2345 - loglik: -1.7271e+02 - logprior: -1.1962e+00
Epoch 3/10
34/34 - 7s - loss: 173.9173 - loglik: -1.7251e+02 - logprior: -1.0729e+00
Epoch 4/10
34/34 - 7s - loss: 172.3588 - loglik: -1.7105e+02 - logprior: -9.7818e-01
Epoch 5/10
34/34 - 7s - loss: 173.5051 - loglik: -1.7228e+02 - logprior: -8.7978e-01
Fitted a model with MAP estimate = -172.6539
Time for alignment: 164.8060
Computed alignments with likelihoods: ['-172.7929', '-173.0841', '-172.6539']
Best model has likelihood: -172.6539
time for generating output: 0.3744
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gpdh.projection.fasta
SP score = 0.6246625634380737
Training of 3 independent models on file sodcu.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f333df034f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34ed34fa00>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3462a86250>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3436036a90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3436036be0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f33a3be0d60>, <__main__.SimpleDirichletPrior object at 0x7f33a4676100>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34373f5e50>

Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 420.5430 - loglik: -3.9945e+02 - logprior: -2.1082e+01
Epoch 2/10
10/10 - 2s - loss: 364.9868 - loglik: -3.5986e+02 - logprior: -5.1223e+00
Epoch 3/10
10/10 - 2s - loss: 322.7901 - loglik: -3.1986e+02 - logprior: -2.9270e+00
Epoch 4/10
10/10 - 2s - loss: 292.5206 - loglik: -2.8989e+02 - logprior: -2.5424e+00
Epoch 5/10
10/10 - 2s - loss: 280.8475 - loglik: -2.7797e+02 - logprior: -2.6677e+00
Epoch 6/10
10/10 - 2s - loss: 276.0554 - loglik: -2.7299e+02 - logprior: -2.6745e+00
Epoch 7/10
10/10 - 2s - loss: 273.3980 - loglik: -2.7033e+02 - logprior: -2.5894e+00
Epoch 8/10
10/10 - 2s - loss: 273.0854 - loglik: -2.7014e+02 - logprior: -2.5229e+00
Epoch 9/10
10/10 - 2s - loss: 271.3225 - loglik: -2.6847e+02 - logprior: -2.4860e+00
Epoch 10/10
10/10 - 2s - loss: 271.0931 - loglik: -2.6821e+02 - logprior: -2.5229e+00
Fitted a model with MAP estimate = -270.3869
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (55, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (87, 1), (97, 4), (98, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 281.8675 - loglik: -2.6200e+02 - logprior: -1.9524e+01
Epoch 2/2
10/10 - 2s - loss: 258.6945 - loglik: -2.5305e+02 - logprior: -5.2525e+00
Fitted a model with MAP estimate = -253.6978
expansions: []
discards: [118 119 120]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 141 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 271.6776 - loglik: -2.5207e+02 - logprior: -1.9188e+01
Epoch 2/2
10/10 - 2s - loss: 256.6035 - loglik: -2.5118e+02 - logprior: -4.9958e+00
Fitted a model with MAP estimate = -254.1738
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 141 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 269.9076 - loglik: -2.5044e+02 - logprior: -1.9042e+01
Epoch 2/10
10/10 - 2s - loss: 255.8712 - loglik: -2.5058e+02 - logprior: -4.8573e+00
Epoch 3/10
10/10 - 2s - loss: 253.5363 - loglik: -2.5107e+02 - logprior: -2.0201e+00
Epoch 4/10
10/10 - 2s - loss: 252.3529 - loglik: -2.5096e+02 - logprior: -9.4807e-01
Epoch 5/10
10/10 - 2s - loss: 252.2609 - loglik: -2.5134e+02 - logprior: -4.6739e-01
Epoch 6/10
10/10 - 2s - loss: 251.8804 - loglik: -2.5121e+02 - logprior: -2.1062e-01
Epoch 7/10
10/10 - 2s - loss: 251.2039 - loglik: -2.5074e+02 - logprior: 7.6648e-04
Epoch 8/10
10/10 - 2s - loss: 252.1311 - loglik: -2.5186e+02 - logprior: 0.1966
Fitted a model with MAP estimate = -250.9159
Time for alignment: 72.5027
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 420.2302 - loglik: -3.9914e+02 - logprior: -2.1082e+01
Epoch 2/10
10/10 - 2s - loss: 364.9645 - loglik: -3.5983e+02 - logprior: -5.1285e+00
Epoch 3/10
10/10 - 2s - loss: 322.8518 - loglik: -3.1987e+02 - logprior: -2.9768e+00
Epoch 4/10
10/10 - 2s - loss: 295.0823 - loglik: -2.9239e+02 - logprior: -2.6027e+00
Epoch 5/10
10/10 - 2s - loss: 283.3432 - loglik: -2.8040e+02 - logprior: -2.5926e+00
Epoch 6/10
10/10 - 2s - loss: 278.3193 - loglik: -2.7527e+02 - logprior: -2.5504e+00
Epoch 7/10
10/10 - 2s - loss: 276.7991 - loglik: -2.7390e+02 - logprior: -2.4604e+00
Epoch 8/10
10/10 - 2s - loss: 274.6694 - loglik: -2.7189e+02 - logprior: -2.4101e+00
Epoch 9/10
10/10 - 2s - loss: 275.1649 - loglik: -2.7243e+02 - logprior: -2.3954e+00
Fitted a model with MAP estimate = -273.8993
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (23, 1), (24, 4), (26, 1), (53, 1), (60, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (84, 2), (86, 2), (87, 1), (95, 3), (98, 1), (102, 1), (109, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 147 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 285.5048 - loglik: -2.6536e+02 - logprior: -1.9822e+01
Epoch 2/2
10/10 - 3s - loss: 259.3575 - loglik: -2.5334e+02 - logprior: -5.6645e+00
Fitted a model with MAP estimate = -255.0240
expansions: []
discards: [  0  31 104 108]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 277.2169 - loglik: -2.5235e+02 - logprior: -2.4485e+01
Epoch 2/2
10/10 - 2s - loss: 261.3604 - loglik: -2.5063e+02 - logprior: -1.0294e+01
Fitted a model with MAP estimate = -258.4154
expansions: [(0, 4), (118, 1), (129, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 268.8685 - loglik: -2.4900e+02 - logprior: -1.9417e+01
Epoch 2/10
10/10 - 3s - loss: 252.3949 - loglik: -2.4685e+02 - logprior: -5.0809e+00
Epoch 3/10
10/10 - 3s - loss: 249.9018 - loglik: -2.4716e+02 - logprior: -2.2570e+00
Epoch 4/10
10/10 - 3s - loss: 248.1606 - loglik: -2.4649e+02 - logprior: -1.1913e+00
Epoch 5/10
10/10 - 3s - loss: 247.9527 - loglik: -2.4676e+02 - logprior: -7.1123e-01
Epoch 6/10
10/10 - 3s - loss: 247.1622 - loglik: -2.4624e+02 - logprior: -4.4062e-01
Epoch 7/10
10/10 - 3s - loss: 247.5243 - loglik: -2.4680e+02 - logprior: -2.3660e-01
Fitted a model with MAP estimate = -246.5507
Time for alignment: 69.2123
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 420.1388 - loglik: -3.9904e+02 - logprior: -2.1084e+01
Epoch 2/10
10/10 - 2s - loss: 365.1390 - loglik: -3.6003e+02 - logprior: -5.1082e+00
Epoch 3/10
10/10 - 2s - loss: 322.1267 - loglik: -3.1923e+02 - logprior: -2.8877e+00
Epoch 4/10
10/10 - 2s - loss: 290.4305 - loglik: -2.8766e+02 - logprior: -2.6952e+00
Epoch 5/10
10/10 - 2s - loss: 279.7067 - loglik: -2.7666e+02 - logprior: -2.8386e+00
Epoch 6/10
10/10 - 2s - loss: 274.1792 - loglik: -2.7101e+02 - logprior: -2.7570e+00
Epoch 7/10
10/10 - 2s - loss: 273.2282 - loglik: -2.7010e+02 - logprior: -2.6525e+00
Epoch 8/10
10/10 - 2s - loss: 271.5805 - loglik: -2.6850e+02 - logprior: -2.6384e+00
Epoch 9/10
10/10 - 2s - loss: 272.0882 - loglik: -2.6908e+02 - logprior: -2.5908e+00
Fitted a model with MAP estimate = -270.7365
expansions: [(8, 1), (9, 1), (10, 1), (13, 1), (17, 1), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (60, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (87, 1), (97, 4), (98, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 282.8519 - loglik: -2.6295e+02 - logprior: -1.9532e+01
Epoch 2/2
10/10 - 2s - loss: 257.9302 - loglik: -2.5223e+02 - logprior: -5.2892e+00
Fitted a model with MAP estimate = -253.5488
expansions: []
discards: [  0 119 120 121 122]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 279.9163 - loglik: -2.5534e+02 - logprior: -2.4146e+01
Epoch 2/2
10/10 - 2s - loss: 264.1123 - loglik: -2.5371e+02 - logprior: -9.9487e+00
Fitted a model with MAP estimate = -260.9941
expansions: [(0, 4), (121, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 146 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 270.7010 - loglik: -2.5102e+02 - logprior: -1.9262e+01
Epoch 2/10
10/10 - 2s - loss: 253.9116 - loglik: -2.4859e+02 - logprior: -4.8770e+00
Epoch 3/10
10/10 - 2s - loss: 250.1944 - loglik: -2.4760e+02 - logprior: -2.1435e+00
Epoch 4/10
10/10 - 3s - loss: 248.0012 - loglik: -2.4636e+02 - logprior: -1.1764e+00
Epoch 5/10
10/10 - 3s - loss: 247.5812 - loglik: -2.4640e+02 - logprior: -7.1464e-01
Epoch 6/10
10/10 - 3s - loss: 246.8392 - loglik: -2.4593e+02 - logprior: -4.4437e-01
Epoch 7/10
10/10 - 3s - loss: 247.3089 - loglik: -2.4658e+02 - logprior: -2.4723e-01
Fitted a model with MAP estimate = -246.4231
Time for alignment: 67.9708
Computed alignments with likelihoods: ['-250.9159', '-246.5507', '-246.4231']
Best model has likelihood: -246.4231
time for generating output: 0.1958
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodcu.projection.fasta
SP score = 0.8972332015810277
Training of 3 independent models on file DEATH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34ee6f0640>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f342d161940>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3440dce220>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f342cabac10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f342caba220>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3484d08520>, <__main__.SimpleDirichletPrior object at 0x7f33a3f65fa0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34b798d3a0>

Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 270.3741 - loglik: -2.3268e+02 - logprior: -3.7670e+01
Epoch 2/10
10/10 - 1s - loss: 230.8533 - loglik: -2.2073e+02 - logprior: -9.8970e+00
Epoch 3/10
10/10 - 1s - loss: 213.9837 - loglik: -2.0879e+02 - logprior: -4.7017e+00
Epoch 4/10
10/10 - 1s - loss: 203.2695 - loglik: -1.9984e+02 - logprior: -3.0283e+00
Epoch 5/10
10/10 - 1s - loss: 197.0153 - loglik: -1.9402e+02 - logprior: -2.6068e+00
Epoch 6/10
10/10 - 1s - loss: 193.3917 - loglik: -1.9033e+02 - logprior: -2.5904e+00
Epoch 7/10
10/10 - 1s - loss: 191.7304 - loglik: -1.8906e+02 - logprior: -2.2494e+00
Epoch 8/10
10/10 - 1s - loss: 190.9579 - loglik: -1.8888e+02 - logprior: -1.7441e+00
Epoch 9/10
10/10 - 1s - loss: 190.4307 - loglik: -1.8853e+02 - logprior: -1.5865e+00
Epoch 10/10
10/10 - 1s - loss: 189.7728 - loglik: -1.8793e+02 - logprior: -1.5290e+00
Fitted a model with MAP estimate = -189.4326
expansions: [(0, 3), (6, 1), (22, 1), (32, 2), (34, 1), (41, 1), (45, 1), (49, 1), (57, 1), (65, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 235.9828 - loglik: -1.8573e+02 - logprior: -4.9954e+01
Epoch 2/2
10/10 - 1s - loss: 197.8154 - loglik: -1.8228e+02 - logprior: -1.5240e+01
Fitted a model with MAP estimate = -190.3903
expansions: [(37, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 224.9024 - loglik: -1.8153e+02 - logprior: -4.3052e+01
Epoch 2/2
10/10 - 1s - loss: 198.8544 - loglik: -1.8155e+02 - logprior: -1.6958e+01
Fitted a model with MAP estimate = -194.1448
expansions: [(32, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 220.9168 - loglik: -1.8058e+02 - logprior: -3.9968e+01
Epoch 2/10
10/10 - 1s - loss: 191.8716 - loglik: -1.7976e+02 - logprior: -1.1714e+01
Epoch 3/10
10/10 - 1s - loss: 184.8553 - loglik: -1.7999e+02 - logprior: -4.4427e+00
Epoch 4/10
10/10 - 1s - loss: 182.2422 - loglik: -1.7972e+02 - logprior: -2.0528e+00
Epoch 5/10
10/10 - 1s - loss: 180.8988 - loglik: -1.7927e+02 - logprior: -1.0586e+00
Epoch 6/10
10/10 - 1s - loss: 179.9885 - loglik: -1.7876e+02 - logprior: -5.2888e-01
Epoch 7/10
10/10 - 1s - loss: 179.2749 - loglik: -1.7837e+02 - logprior: -1.1231e-01
Epoch 8/10
10/10 - 1s - loss: 178.7633 - loglik: -1.7823e+02 - logprior: 0.2404
Epoch 9/10
10/10 - 1s - loss: 178.3287 - loglik: -1.7797e+02 - logprior: 0.4623
Epoch 10/10
10/10 - 1s - loss: 178.0746 - loglik: -1.7787e+02 - logprior: 0.6085
Fitted a model with MAP estimate = -177.1114
Time for alignment: 48.0975
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 270.2465 - loglik: -2.3255e+02 - logprior: -3.7672e+01
Epoch 2/10
10/10 - 1s - loss: 230.8109 - loglik: -2.2068e+02 - logprior: -9.8991e+00
Epoch 3/10
10/10 - 1s - loss: 213.4328 - loglik: -2.0830e+02 - logprior: -4.6512e+00
Epoch 4/10
10/10 - 1s - loss: 202.7306 - loglik: -1.9948e+02 - logprior: -2.8901e+00
Epoch 5/10
10/10 - 1s - loss: 196.3332 - loglik: -1.9358e+02 - logprior: -2.4511e+00
Epoch 6/10
10/10 - 1s - loss: 193.3933 - loglik: -1.9056e+02 - logprior: -2.4254e+00
Epoch 7/10
10/10 - 1s - loss: 191.7995 - loglik: -1.8941e+02 - logprior: -1.9639e+00
Epoch 8/10
10/10 - 1s - loss: 190.8357 - loglik: -1.8901e+02 - logprior: -1.4860e+00
Epoch 9/10
10/10 - 1s - loss: 190.3432 - loglik: -1.8867e+02 - logprior: -1.3561e+00
Epoch 10/10
10/10 - 1s - loss: 189.8420 - loglik: -1.8824e+02 - logprior: -1.2822e+00
Fitted a model with MAP estimate = -189.3545
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (30, 3), (40, 2), (43, 1), (57, 1), (65, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 235.1867 - loglik: -1.8509e+02 - logprior: -4.9794e+01
Epoch 2/2
10/10 - 1s - loss: 196.6265 - loglik: -1.8122e+02 - logprior: -1.5118e+01
Fitted a model with MAP estimate = -189.1455
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 223.4942 - loglik: -1.8031e+02 - logprior: -4.2890e+01
Epoch 2/2
10/10 - 1s - loss: 197.6139 - loglik: -1.8036e+02 - logprior: -1.6927e+01
Fitted a model with MAP estimate = -192.9579
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 218.2087 - loglik: -1.7850e+02 - logprior: -3.9349e+01
Epoch 2/10
10/10 - 1s - loss: 190.2231 - loglik: -1.7879e+02 - logprior: -1.1050e+01
Epoch 3/10
10/10 - 1s - loss: 183.5589 - loglik: -1.7894e+02 - logprior: -4.2192e+00
Epoch 4/10
10/10 - 1s - loss: 181.3942 - loglik: -1.7907e+02 - logprior: -1.9066e+00
Epoch 5/10
10/10 - 1s - loss: 180.0573 - loglik: -1.7870e+02 - logprior: -8.9909e-01
Epoch 6/10
10/10 - 1s - loss: 179.4174 - loglik: -1.7849e+02 - logprior: -3.7507e-01
Epoch 7/10
10/10 - 1s - loss: 178.8548 - loglik: -1.7822e+02 - logprior: 0.0522
Epoch 8/10
10/10 - 1s - loss: 178.3818 - loglik: -1.7803e+02 - logprior: 0.4000
Epoch 9/10
10/10 - 1s - loss: 178.0062 - loglik: -1.7789e+02 - logprior: 0.6269
Epoch 10/10
10/10 - 1s - loss: 177.8395 - loglik: -1.7788e+02 - logprior: 0.7747
Fitted a model with MAP estimate = -176.9202
Time for alignment: 48.3465
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 270.3425 - loglik: -2.3264e+02 - logprior: -3.7672e+01
Epoch 2/10
10/10 - 1s - loss: 230.6165 - loglik: -2.2049e+02 - logprior: -9.8953e+00
Epoch 3/10
10/10 - 1s - loss: 213.2645 - loglik: -2.0814e+02 - logprior: -4.6573e+00
Epoch 4/10
10/10 - 1s - loss: 202.7042 - loglik: -1.9945e+02 - logprior: -2.9117e+00
Epoch 5/10
10/10 - 1s - loss: 196.9055 - loglik: -1.9411e+02 - logprior: -2.4113e+00
Epoch 6/10
10/10 - 1s - loss: 193.4926 - loglik: -1.9060e+02 - logprior: -2.3607e+00
Epoch 7/10
10/10 - 1s - loss: 191.7271 - loglik: -1.8937e+02 - logprior: -1.9268e+00
Epoch 8/10
10/10 - 1s - loss: 190.7880 - loglik: -1.8905e+02 - logprior: -1.4182e+00
Epoch 9/10
10/10 - 1s - loss: 190.2325 - loglik: -1.8866e+02 - logprior: -1.2969e+00
Epoch 10/10
10/10 - 1s - loss: 189.6430 - loglik: -1.8807e+02 - logprior: -1.2885e+00
Fitted a model with MAP estimate = -189.1805
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (28, 1), (31, 2), (40, 2), (43, 1), (56, 1), (65, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 234.8904 - loglik: -1.8480e+02 - logprior: -4.9787e+01
Epoch 2/2
10/10 - 1s - loss: 196.3673 - loglik: -1.8084e+02 - logprior: -1.5185e+01
Fitted a model with MAP estimate = -189.0226
expansions: [(39, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 223.3239 - loglik: -1.8009e+02 - logprior: -4.2877e+01
Epoch 2/2
10/10 - 1s - loss: 197.1868 - loglik: -1.7988e+02 - logprior: -1.6931e+01
Fitted a model with MAP estimate = -192.6563
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 219.2832 - loglik: -1.7897e+02 - logprior: -3.9921e+01
Epoch 2/10
10/10 - 1s - loss: 190.9926 - loglik: -1.7897e+02 - logprior: -1.1620e+01
Epoch 3/10
10/10 - 1s - loss: 183.7356 - loglik: -1.7895e+02 - logprior: -4.3515e+00
Epoch 4/10
10/10 - 1s - loss: 181.1915 - loglik: -1.7874e+02 - logprior: -1.9640e+00
Epoch 5/10
10/10 - 1s - loss: 179.9663 - loglik: -1.7839e+02 - logprior: -9.6211e-01
Epoch 6/10
10/10 - 1s - loss: 179.0168 - loglik: -1.7783e+02 - logprior: -4.5193e-01
Epoch 7/10
10/10 - 1s - loss: 178.3823 - loglik: -1.7762e+02 - logprior: -4.2010e-03
Epoch 8/10
10/10 - 1s - loss: 178.2672 - loglik: -1.7788e+02 - logprior: 0.3721
Epoch 9/10
10/10 - 1s - loss: 177.8159 - loglik: -1.7767e+02 - logprior: 0.6135
Epoch 10/10
10/10 - 1s - loss: 177.7796 - loglik: -1.7779e+02 - logprior: 0.7697
Fitted a model with MAP estimate = -176.7315
Time for alignment: 46.5918
Computed alignments with likelihoods: ['-177.1114', '-176.9202', '-176.7315']
Best model has likelihood: -176.7315
time for generating output: 0.1784
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DEATH.projection.fasta
SP score = 0.7483028720626632
Training of 3 independent models on file aat.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345a60c460>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3368343f40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34247529a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f349e0fa9a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f343ff04100>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34ecf4fca0>, <__main__.SimpleDirichletPrior object at 0x7f346b58f2b0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f346b64cd30>

Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 36s - loss: 950.3375 - loglik: -9.4829e+02 - logprior: -1.4239e+00
Epoch 2/10
43/43 - 31s - loss: 832.0026 - loglik: -8.2761e+02 - logprior: -2.0842e+00
Epoch 3/10
43/43 - 31s - loss: 817.3193 - loglik: -8.1265e+02 - logprior: -2.1652e+00
Epoch 4/10
43/43 - 31s - loss: 814.9104 - loglik: -8.1068e+02 - logprior: -2.2397e+00
Epoch 5/10
43/43 - 31s - loss: 812.8799 - loglik: -8.0895e+02 - logprior: -2.1965e+00
Epoch 6/10
43/43 - 31s - loss: 811.6808 - loglik: -8.0782e+02 - logprior: -2.2856e+00
Epoch 7/10
43/43 - 31s - loss: 811.0365 - loglik: -8.0734e+02 - logprior: -2.2376e+00
Epoch 8/10
43/43 - 31s - loss: 810.5745 - loglik: -8.0694e+02 - logprior: -2.2597e+00
Epoch 9/10
43/43 - 31s - loss: 810.0865 - loglik: -8.0654e+02 - logprior: -2.2570e+00
Epoch 10/10
43/43 - 31s - loss: 808.9674 - loglik: -8.0549e+02 - logprior: -2.2659e+00
Fitted a model with MAP estimate = -821.5013
expansions: [(7, 2), (16, 1), (20, 2), (21, 1), (22, 1), (23, 1), (24, 3), (25, 1), (29, 1), (39, 1), (41, 1), (42, 1), (45, 2), (46, 1), (56, 1), (59, 1), (60, 1), (61, 1), (78, 1), (79, 1), (80, 1), (81, 1), (90, 1), (91, 2), (92, 2), (94, 1), (95, 1), (100, 1), (119, 1), (120, 1), (121, 1), (125, 1), (130, 2), (132, 2), (144, 1), (147, 1), (149, 1), (152, 2), (155, 3), (156, 2), (167, 1), (180, 1), (183, 1), (184, 1), (185, 1), (187, 1), (188, 2), (203, 1), (205, 3), (206, 1), (207, 1), (210, 1), (219, 1), (220, 1), (221, 1), (225, 1), (226, 2), (239, 5), (242, 1), (244, 1), (245, 2), (249, 1), (260, 2), (261, 1), (269, 2), (270, 1), (271, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 369 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 51s - loss: 788.7764 - loglik: -7.8484e+02 - logprior: -2.6899e+00
Epoch 2/2
43/43 - 47s - loss: 773.2191 - loglik: -7.7021e+02 - logprior: -1.5157e+00
Fitted a model with MAP estimate = -805.1712
expansions: [(0, 2)]
discards: [  0  29  31  60 121 169 172 197 206 246 297 313 314 324 356]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 356 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 48s - loss: 770.7764 - loglik: -7.6759e+02 - logprior: -1.6362e+00
Epoch 2/2
43/43 - 45s - loss: 771.5718 - loglik: -7.6919e+02 - logprior: -8.4096e-01
Fitted a model with MAP estimate = -805.0505
expansions: []
discards: [  0   1 258 259 304]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 351 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 65s - loss: 778.4047 - loglik: -7.7448e+02 - logprior: -1.0893e+00
Epoch 2/10
61/61 - 60s - loss: 763.2621 - loglik: -7.5916e+02 - logprior: -4.9534e-01
Epoch 3/10
61/61 - 60s - loss: 753.1071 - loglik: -7.4918e+02 - logprior: -4.4505e-01
Epoch 4/10
61/61 - 60s - loss: 750.0480 - loglik: -7.4639e+02 - logprior: -3.8577e-01
Epoch 5/10
61/61 - 60s - loss: 747.4705 - loglik: -7.4408e+02 - logprior: -3.1240e-01
Epoch 6/10
61/61 - 61s - loss: 747.0912 - loglik: -7.4428e+02 - logprior: -1.8865e-01
Epoch 7/10
61/61 - 60s - loss: 748.8046 - loglik: -7.4619e+02 - logprior: -1.0758e-01
Fitted a model with MAP estimate = -744.2941
Time for alignment: 1245.5325
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 35s - loss: 952.3353 - loglik: -9.5029e+02 - logprior: -1.4390e+00
Epoch 2/10
43/43 - 31s - loss: 832.1059 - loglik: -8.2824e+02 - logprior: -2.1572e+00
Epoch 3/10
43/43 - 32s - loss: 818.1092 - loglik: -8.1395e+02 - logprior: -2.1540e+00
Epoch 4/10
43/43 - 31s - loss: 814.9144 - loglik: -8.1091e+02 - logprior: -2.1274e+00
Epoch 5/10
43/43 - 31s - loss: 813.8854 - loglik: -8.0995e+02 - logprior: -2.1574e+00
Epoch 6/10
43/43 - 31s - loss: 811.9520 - loglik: -8.0808e+02 - logprior: -2.2111e+00
Epoch 7/10
43/43 - 32s - loss: 812.4120 - loglik: -8.0868e+02 - logprior: -2.1658e+00
Fitted a model with MAP estimate = -815.2525
expansions: [(14, 3), (16, 1), (21, 3), (22, 2), (23, 1), (24, 1), (29, 1), (40, 1), (41, 1), (42, 1), (43, 1), (45, 2), (46, 1), (57, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 1), (86, 2), (90, 1), (91, 1), (93, 1), (95, 1), (97, 1), (101, 1), (104, 2), (121, 1), (122, 1), (125, 1), (131, 1), (132, 2), (142, 1), (148, 1), (150, 1), (153, 2), (154, 1), (155, 3), (157, 2), (181, 1), (183, 2), (185, 3), (186, 1), (187, 1), (188, 2), (203, 1), (205, 4), (206, 1), (207, 1), (208, 1), (209, 1), (219, 1), (221, 1), (225, 1), (226, 2), (238, 3), (242, 1), (245, 2), (248, 1), (249, 1), (260, 2), (269, 2), (270, 2), (271, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 371 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 52s - loss: 794.4564 - loglik: -7.8998e+02 - logprior: -2.8210e+00
Epoch 2/2
43/43 - 47s - loss: 773.2188 - loglik: -7.6978e+02 - logprior: -1.6567e+00
Fitted a model with MAP estimate = -797.7281
expansions: [(0, 2)]
discards: [  0  14  28  29  60 111 171 198 207 236 241 248 270 300 316 325 359]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 356 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 48s - loss: 774.6929 - loglik: -7.7089e+02 - logprior: -1.8402e+00
Epoch 2/2
43/43 - 45s - loss: 772.3456 - loglik: -7.6946e+02 - logprior: -9.4034e-01
Fitted a model with MAP estimate = -797.8491
expansions: []
discards: [  0   1 345]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 353 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 65s - loss: 774.4211 - loglik: -7.7002e+02 - logprior: -1.1279e+00
Epoch 2/10
61/61 - 61s - loss: 759.6450 - loglik: -7.5520e+02 - logprior: -5.2493e-01
Epoch 3/10
61/61 - 61s - loss: 751.7257 - loglik: -7.4745e+02 - logprior: -4.4765e-01
Epoch 4/10
61/61 - 60s - loss: 749.3835 - loglik: -7.4553e+02 - logprior: -3.6821e-01
Epoch 5/10
61/61 - 60s - loss: 747.5131 - loglik: -7.4395e+02 - logprior: -2.8905e-01
Epoch 6/10
61/61 - 61s - loss: 746.9018 - loglik: -7.4388e+02 - logprior: -2.0235e-01
Epoch 7/10
61/61 - 61s - loss: 745.9829 - loglik: -7.4320e+02 - logprior: -9.1394e-02
Epoch 8/10
61/61 - 61s - loss: 746.8243 - loglik: -7.4435e+02 - logprior: 0.0115
Fitted a model with MAP estimate = -743.1577
Time for alignment: 1214.1139
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 34s - loss: 949.6088 - loglik: -9.4755e+02 - logprior: -1.4263e+00
Epoch 2/10
43/43 - 31s - loss: 831.7010 - loglik: -8.2754e+02 - logprior: -2.1303e+00
Epoch 3/10
43/43 - 31s - loss: 817.7329 - loglik: -8.1327e+02 - logprior: -2.2867e+00
Epoch 4/10
43/43 - 31s - loss: 814.0208 - loglik: -8.0985e+02 - logprior: -2.2064e+00
Epoch 5/10
43/43 - 31s - loss: 812.8629 - loglik: -8.0888e+02 - logprior: -2.2370e+00
Epoch 6/10
43/43 - 31s - loss: 810.5510 - loglik: -8.0672e+02 - logprior: -2.2539e+00
Epoch 7/10
43/43 - 31s - loss: 811.9664 - loglik: -8.0819e+02 - logprior: -2.3061e+00
Fitted a model with MAP estimate = -814.5601
expansions: [(7, 2), (16, 1), (21, 3), (22, 2), (23, 1), (24, 2), (29, 1), (36, 1), (39, 1), (41, 1), (42, 1), (45, 2), (46, 1), (56, 1), (59, 1), (60, 1), (61, 1), (78, 1), (79, 1), (80, 1), (81, 1), (90, 1), (91, 2), (92, 1), (95, 1), (97, 2), (98, 1), (103, 2), (119, 1), (120, 1), (121, 1), (124, 1), (130, 2), (132, 2), (142, 1), (147, 1), (149, 1), (152, 2), (155, 3), (156, 2), (167, 1), (180, 1), (183, 1), (184, 2), (186, 1), (187, 1), (188, 2), (203, 1), (205, 3), (206, 2), (207, 1), (210, 1), (219, 1), (220, 1), (221, 1), (225, 1), (226, 2), (239, 3), (242, 1), (245, 2), (248, 1), (249, 1), (260, 2), (269, 2), (270, 2), (271, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 372 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 52s - loss: 794.1268 - loglik: -7.8974e+02 - logprior: -2.7987e+00
Epoch 2/2
43/43 - 48s - loss: 775.4617 - loglik: -7.7199e+02 - logprior: -1.6817e+00
Fitted a model with MAP estimate = -797.0276
expansions: [(0, 2)]
discards: [  0  27  28  32  60 121 128 138 171 174 200 208 241 249 271 301 317 326
 360 361]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 354 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 47s - loss: 774.3410 - loglik: -7.7077e+02 - logprior: -1.6204e+00
Epoch 2/2
43/43 - 45s - loss: 772.0841 - loglik: -7.6930e+02 - logprior: -8.6359e-01
Fitted a model with MAP estimate = -796.8978
expansions: []
discards: [  0   1 258]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 351 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 65s - loss: 774.1574 - loglik: -7.6969e+02 - logprior: -1.1071e+00
Epoch 2/10
61/61 - 60s - loss: 761.1217 - loglik: -7.5665e+02 - logprior: -5.2727e-01
Epoch 3/10
61/61 - 60s - loss: 753.3696 - loglik: -7.4903e+02 - logprior: -5.1278e-01
Epoch 4/10
61/61 - 60s - loss: 750.2609 - loglik: -7.4641e+02 - logprior: -4.0026e-01
Epoch 5/10
61/61 - 60s - loss: 750.3398 - loglik: -7.4682e+02 - logprior: -3.2721e-01
Fitted a model with MAP estimate = -745.4321
Time for alignment: 1027.2840
Computed alignments with likelihoods: ['-744.2941', '-743.1577', '-745.4321']
Best model has likelihood: -743.1577
time for generating output: 0.4289
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aat.projection.fasta
SP score = 0.8020796616143814
Training of 3 independent models on file ins.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34eceb2610>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f342d6c9220>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f344962e100>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f344962ea60>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34738843d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34370163a0>, <__main__.SimpleDirichletPrior object at 0x7f33247d94c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f346b64cd30>

Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 250.2644 - loglik: -1.9318e+02 - logprior: -5.7061e+01
Epoch 2/10
10/10 - 1s - loss: 188.9011 - loglik: -1.7291e+02 - logprior: -1.5957e+01
Epoch 3/10
10/10 - 2s - loss: 162.3941 - loglik: -1.5441e+02 - logprior: -7.9787e+00
Epoch 4/10
10/10 - 1s - loss: 153.0594 - loglik: -1.4796e+02 - logprior: -5.0332e+00
Epoch 5/10
10/10 - 2s - loss: 148.6925 - loglik: -1.4466e+02 - logprior: -3.7647e+00
Epoch 6/10
10/10 - 1s - loss: 148.5203 - loglik: -1.4522e+02 - logprior: -2.9413e+00
Epoch 7/10
10/10 - 1s - loss: 146.7113 - loglik: -1.4399e+02 - logprior: -2.4037e+00
Epoch 8/10
10/10 - 1s - loss: 146.8909 - loglik: -1.4444e+02 - logprior: -2.1437e+00
Fitted a model with MAP estimate = -145.8580
expansions: [(11, 1), (12, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 205.2961 - loglik: -1.4018e+02 - logprior: -6.4726e+01
Epoch 2/2
10/10 - 2s - loss: 167.6242 - loglik: -1.3953e+02 - logprior: -2.7644e+01
Fitted a model with MAP estimate = -161.3480
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 190.6108 - loglik: -1.3732e+02 - logprior: -5.2832e+01
Epoch 2/2
10/10 - 2s - loss: 153.2672 - loglik: -1.3762e+02 - logprior: -1.5156e+01
Fitted a model with MAP estimate = -147.6720
expansions: []
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 190.3158 - loglik: -1.3817e+02 - logprior: -5.1649e+01
Epoch 2/10
10/10 - 1s - loss: 153.3893 - loglik: -1.3777e+02 - logprior: -1.5105e+01
Epoch 3/10
10/10 - 1s - loss: 146.7811 - loglik: -1.3866e+02 - logprior: -7.6051e+00
Epoch 4/10
10/10 - 2s - loss: 143.9349 - loglik: -1.3895e+02 - logprior: -4.4688e+00
Epoch 5/10
10/10 - 2s - loss: 142.5743 - loglik: -1.3933e+02 - logprior: -2.7188e+00
Epoch 6/10
10/10 - 2s - loss: 142.7559 - loglik: -1.4052e+02 - logprior: -1.7108e+00
Fitted a model with MAP estimate = -141.4288
Time for alignment: 44.4927
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 250.9898 - loglik: -1.9391e+02 - logprior: -5.7061e+01
Epoch 2/10
10/10 - 1s - loss: 187.6874 - loglik: -1.7169e+02 - logprior: -1.5968e+01
Epoch 3/10
10/10 - 2s - loss: 162.5133 - loglik: -1.5451e+02 - logprior: -7.9982e+00
Epoch 4/10
10/10 - 2s - loss: 152.9073 - loglik: -1.4790e+02 - logprior: -4.9682e+00
Epoch 5/10
10/10 - 2s - loss: 150.5150 - loglik: -1.4675e+02 - logprior: -3.5199e+00
Epoch 6/10
10/10 - 2s - loss: 148.6097 - loglik: -1.4563e+02 - logprior: -2.5811e+00
Epoch 7/10
10/10 - 2s - loss: 148.8396 - loglik: -1.4646e+02 - logprior: -2.0246e+00
Fitted a model with MAP estimate = -147.7276
expansions: [(11, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 207.8637 - loglik: -1.4284e+02 - logprior: -6.4647e+01
Epoch 2/2
10/10 - 1s - loss: 170.3967 - loglik: -1.4240e+02 - logprior: -2.7595e+01
Fitted a model with MAP estimate = -163.9749
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 192.7980 - loglik: -1.3957e+02 - logprior: -5.2841e+01
Epoch 2/2
10/10 - 2s - loss: 155.9019 - loglik: -1.4035e+02 - logprior: -1.5149e+01
Fitted a model with MAP estimate = -150.0130
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 203.5703 - loglik: -1.3993e+02 - logprior: -6.3214e+01
Epoch 2/10
10/10 - 1s - loss: 163.9116 - loglik: -1.4075e+02 - logprior: -2.2734e+01
Epoch 3/10
10/10 - 1s - loss: 151.2186 - loglik: -1.4129e+02 - logprior: -9.4744e+00
Epoch 4/10
10/10 - 1s - loss: 146.5432 - loglik: -1.4153e+02 - logprior: -4.5467e+00
Epoch 5/10
10/10 - 1s - loss: 145.7285 - loglik: -1.4258e+02 - logprior: -2.6685e+00
Epoch 6/10
10/10 - 1s - loss: 143.8045 - loglik: -1.4151e+02 - logprior: -1.7968e+00
Epoch 7/10
10/10 - 2s - loss: 145.1008 - loglik: -1.4326e+02 - logprior: -1.3354e+00
Fitted a model with MAP estimate = -143.2666
Time for alignment: 44.0192
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 250.1214 - loglik: -1.9304e+02 - logprior: -5.7062e+01
Epoch 2/10
10/10 - 2s - loss: 189.1585 - loglik: -1.7317e+02 - logprior: -1.5959e+01
Epoch 3/10
10/10 - 2s - loss: 163.2771 - loglik: -1.5529e+02 - logprior: -7.9835e+00
Epoch 4/10
10/10 - 1s - loss: 154.1319 - loglik: -1.4922e+02 - logprior: -4.9029e+00
Epoch 5/10
10/10 - 2s - loss: 150.4182 - loglik: -1.4677e+02 - logprior: -3.4994e+00
Epoch 6/10
10/10 - 1s - loss: 148.8633 - loglik: -1.4587e+02 - logprior: -2.6141e+00
Epoch 7/10
10/10 - 1s - loss: 148.6788 - loglik: -1.4620e+02 - logprior: -2.1016e+00
Epoch 8/10
10/10 - 1s - loss: 148.4439 - loglik: -1.4630e+02 - logprior: -1.8359e+00
Epoch 9/10
10/10 - 2s - loss: 147.5361 - loglik: -1.4553e+02 - logprior: -1.6803e+00
Epoch 10/10
10/10 - 1s - loss: 147.4035 - loglik: -1.4551e+02 - logprior: -1.5416e+00
Fitted a model with MAP estimate = -147.1609
expansions: [(10, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 207.9232 - loglik: -1.4284e+02 - logprior: -6.4705e+01
Epoch 2/2
10/10 - 1s - loss: 169.2871 - loglik: -1.4124e+02 - logprior: -2.7650e+01
Fitted a model with MAP estimate = -163.8028
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 193.1347 - loglik: -1.3976e+02 - logprior: -5.2994e+01
Epoch 2/2
10/10 - 2s - loss: 155.3117 - loglik: -1.3976e+02 - logprior: -1.5143e+01
Fitted a model with MAP estimate = -149.9553
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 203.3641 - loglik: -1.3992e+02 - logprior: -6.3014e+01
Epoch 2/10
10/10 - 2s - loss: 163.1637 - loglik: -1.4055e+02 - logprior: -2.2170e+01
Epoch 3/10
10/10 - 1s - loss: 150.5762 - loglik: -1.4100e+02 - logprior: -9.1257e+00
Epoch 4/10
10/10 - 1s - loss: 147.0376 - loglik: -1.4212e+02 - logprior: -4.4508e+00
Epoch 5/10
10/10 - 1s - loss: 145.7134 - loglik: -1.4263e+02 - logprior: -2.6055e+00
Epoch 6/10
10/10 - 2s - loss: 144.0966 - loglik: -1.4186e+02 - logprior: -1.7424e+00
Epoch 7/10
10/10 - 1s - loss: 144.4482 - loglik: -1.4267e+02 - logprior: -1.2830e+00
Fitted a model with MAP estimate = -143.2250
Time for alignment: 50.0497
Computed alignments with likelihoods: ['-141.4288', '-143.2666', '-143.2250']
Best model has likelihood: -141.4288
time for generating output: 0.1187
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ins.projection.fasta
SP score = 0.9070735090152566
Training of 3 independent models on file sti.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f336860a7c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f33355e8c10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34ede2dc10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34ede2db20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3392cc9eb0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3356a225e0>, <__main__.SimpleDirichletPrior object at 0x7f347c1fe190>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34eda47790>

Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 540.0037 - loglik: -4.7177e+02 - logprior: -6.8214e+01
Epoch 2/10
10/10 - 2s - loss: 440.9064 - loglik: -4.2616e+02 - logprior: -1.4741e+01
Epoch 3/10
10/10 - 2s - loss: 387.9084 - loglik: -3.8203e+02 - logprior: -5.8762e+00
Epoch 4/10
10/10 - 2s - loss: 357.0619 - loglik: -3.5393e+02 - logprior: -3.1315e+00
Epoch 5/10
10/10 - 2s - loss: 342.4355 - loglik: -3.4030e+02 - logprior: -2.0463e+00
Epoch 6/10
10/10 - 2s - loss: 335.5676 - loglik: -3.3384e+02 - logprior: -1.4007e+00
Epoch 7/10
10/10 - 2s - loss: 332.5413 - loglik: -3.3129e+02 - logprior: -8.1167e-01
Epoch 8/10
10/10 - 2s - loss: 330.2445 - loglik: -3.2929e+02 - logprior: -5.3294e-01
Epoch 9/10
10/10 - 2s - loss: 329.5352 - loglik: -3.2882e+02 - logprior: -2.8847e-01
Epoch 10/10
10/10 - 2s - loss: 328.2926 - loglik: -3.2771e+02 - logprior: -1.2666e-01
Fitted a model with MAP estimate = -327.1850
expansions: [(11, 3), (13, 1), (19, 2), (26, 1), (27, 1), (28, 1), (36, 2), (38, 2), (39, 2), (55, 1), (66, 1), (68, 2), (77, 2), (78, 2), (80, 2), (89, 1), (90, 2), (103, 1), (110, 2), (114, 2), (115, 1), (120, 2), (122, 1), (127, 1), (128, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 176 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 381.5649 - loglik: -3.1877e+02 - logprior: -6.2324e+01
Epoch 2/2
10/10 - 3s - loss: 318.8849 - loglik: -3.0409e+02 - logprior: -1.4296e+01
Fitted a model with MAP estimate = -308.0869
expansions: [(130, 1)]
discards: [ 0 12 23 45 49 52 85]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 379.9398 - loglik: -3.0240e+02 - logprior: -7.7006e+01
Epoch 2/2
10/10 - 3s - loss: 329.8883 - loglik: -2.9982e+02 - logprior: -2.9496e+01
Fitted a model with MAP estimate = -321.6708
expansions: [(0, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 358.0514 - loglik: -2.9706e+02 - logprior: -6.0398e+01
Epoch 2/10
10/10 - 3s - loss: 310.2658 - loglik: -2.9722e+02 - logprior: -1.2444e+01
Epoch 3/10
10/10 - 3s - loss: 300.2599 - loglik: -2.9734e+02 - logprior: -2.3226e+00
Epoch 4/10
10/10 - 3s - loss: 296.4232 - loglik: -2.9792e+02 - logprior: 2.0944
Epoch 5/10
10/10 - 3s - loss: 293.6465 - loglik: -2.9770e+02 - logprior: 4.6520
Epoch 6/10
10/10 - 3s - loss: 292.9436 - loglik: -2.9858e+02 - logprior: 6.2373
Epoch 7/10
10/10 - 3s - loss: 291.7917 - loglik: -2.9849e+02 - logprior: 7.2949
Epoch 8/10
10/10 - 3s - loss: 291.4433 - loglik: -2.9893e+02 - logprior: 8.0784
Epoch 9/10
10/10 - 3s - loss: 290.4461 - loglik: -2.9858e+02 - logprior: 8.7289
Epoch 10/10
10/10 - 3s - loss: 290.2023 - loglik: -2.9887e+02 - logprior: 9.2716
Fitted a model with MAP estimate = -289.2437
Time for alignment: 88.7567
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 540.3627 - loglik: -4.7213e+02 - logprior: -6.8217e+01
Epoch 2/10
10/10 - 2s - loss: 440.3715 - loglik: -4.2565e+02 - logprior: -1.4721e+01
Epoch 3/10
10/10 - 2s - loss: 388.6587 - loglik: -3.8298e+02 - logprior: -5.6691e+00
Epoch 4/10
10/10 - 2s - loss: 360.4898 - loglik: -3.5773e+02 - logprior: -2.7508e+00
Epoch 5/10
10/10 - 2s - loss: 345.4349 - loglik: -3.4357e+02 - logprior: -1.6733e+00
Epoch 6/10
10/10 - 2s - loss: 338.4376 - loglik: -3.3714e+02 - logprior: -8.1418e-01
Epoch 7/10
10/10 - 2s - loss: 334.5832 - loglik: -3.3388e+02 - logprior: -1.7105e-01
Epoch 8/10
10/10 - 2s - loss: 332.5206 - loglik: -3.3224e+02 - logprior: 0.1855
Epoch 9/10
10/10 - 2s - loss: 331.3553 - loglik: -3.3138e+02 - logprior: 0.4763
Epoch 10/10
10/10 - 2s - loss: 330.5041 - loglik: -3.3071e+02 - logprior: 0.6718
Fitted a model with MAP estimate = -329.4964
expansions: [(11, 2), (12, 1), (19, 2), (26, 1), (27, 1), (28, 1), (36, 1), (38, 1), (40, 1), (49, 1), (51, 1), (69, 2), (79, 6), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (127, 1), (128, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 401.2420 - loglik: -3.2300e+02 - logprior: -7.7735e+01
Epoch 2/2
10/10 - 3s - loss: 340.2305 - loglik: -3.0972e+02 - logprior: -2.9963e+01
Fitted a model with MAP estimate = -329.5030
expansions: [(0, 1), (123, 1)]
discards: [ 0 22 81]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 167 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 365.6140 - loglik: -3.0385e+02 - logprior: -6.1192e+01
Epoch 2/2
10/10 - 3s - loss: 315.9918 - loglik: -3.0193e+02 - logprior: -1.3469e+01
Fitted a model with MAP estimate = -307.8890
expansions: [(48, 1), (108, 3), (120, 1)]
discards: [135]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 361.7523 - loglik: -3.0112e+02 - logprior: -6.0038e+01
Epoch 2/10
10/10 - 3s - loss: 313.0989 - loglik: -2.9997e+02 - logprior: -1.2520e+01
Epoch 3/10
10/10 - 3s - loss: 302.5430 - loglik: -2.9951e+02 - logprior: -2.4131e+00
Epoch 4/10
10/10 - 3s - loss: 298.7539 - loglik: -3.0011e+02 - logprior: 1.9806
Epoch 5/10
10/10 - 3s - loss: 296.5432 - loglik: -3.0046e+02 - logprior: 4.5435
Epoch 6/10
10/10 - 3s - loss: 294.7396 - loglik: -3.0022e+02 - logprior: 6.1101
Epoch 7/10
10/10 - 3s - loss: 294.4207 - loglik: -3.0095e+02 - logprior: 7.1707
Epoch 8/10
10/10 - 3s - loss: 293.6730 - loglik: -3.0100e+02 - logprior: 7.9594
Epoch 9/10
10/10 - 3s - loss: 292.6584 - loglik: -3.0061e+02 - logprior: 8.5883
Epoch 10/10
10/10 - 3s - loss: 292.1159 - loglik: -3.0056e+02 - logprior: 9.0833
Fitted a model with MAP estimate = -291.2033
Time for alignment: 88.5063
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 540.1156 - loglik: -4.7189e+02 - logprior: -6.8213e+01
Epoch 2/10
10/10 - 2s - loss: 440.5791 - loglik: -4.2583e+02 - logprior: -1.4742e+01
Epoch 3/10
10/10 - 2s - loss: 386.4605 - loglik: -3.8065e+02 - logprior: -5.8057e+00
Epoch 4/10
10/10 - 2s - loss: 355.3824 - loglik: -3.5226e+02 - logprior: -3.1050e+00
Epoch 5/10
10/10 - 2s - loss: 342.1726 - loglik: -3.3993e+02 - logprior: -2.0655e+00
Epoch 6/10
10/10 - 2s - loss: 336.0485 - loglik: -3.3443e+02 - logprior: -1.2290e+00
Epoch 7/10
10/10 - 2s - loss: 332.1330 - loglik: -3.3102e+02 - logprior: -6.9610e-01
Epoch 8/10
10/10 - 2s - loss: 330.3393 - loglik: -3.2960e+02 - logprior: -3.7155e-01
Epoch 9/10
10/10 - 2s - loss: 328.8076 - loglik: -3.2822e+02 - logprior: -2.3947e-01
Epoch 10/10
10/10 - 2s - loss: 327.6150 - loglik: -3.2714e+02 - logprior: -6.9395e-02
Fitted a model with MAP estimate = -326.9148
expansions: [(11, 2), (13, 2), (19, 2), (26, 1), (27, 1), (28, 1), (36, 2), (38, 2), (39, 2), (55, 1), (66, 1), (68, 1), (77, 3), (79, 3), (89, 1), (103, 1), (110, 2), (114, 2), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 172 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 399.9482 - loglik: -3.2164e+02 - logprior: -7.7862e+01
Epoch 2/2
10/10 - 3s - loss: 338.4872 - loglik: -3.0756e+02 - logprior: -3.0433e+01
Fitted a model with MAP estimate = -327.4256
expansions: [(0, 1), (114, 1), (126, 1)]
discards: [  0  14  23  44  48  51 162]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 363.8439 - loglik: -3.0203e+02 - logprior: -6.1279e+01
Epoch 2/2
10/10 - 3s - loss: 313.6449 - loglik: -2.9959e+02 - logprior: -1.3485e+01
Fitted a model with MAP estimate = -305.0703
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 358.0624 - loglik: -2.9725e+02 - logprior: -6.0230e+01
Epoch 2/10
10/10 - 3s - loss: 310.2984 - loglik: -2.9707e+02 - logprior: -1.2620e+01
Epoch 3/10
10/10 - 3s - loss: 300.4581 - loglik: -2.9734e+02 - logprior: -2.5016e+00
Epoch 4/10
10/10 - 3s - loss: 297.1196 - loglik: -2.9838e+02 - logprior: 1.8854
Epoch 5/10
10/10 - 3s - loss: 294.3570 - loglik: -2.9815e+02 - logprior: 4.4223
Epoch 6/10
10/10 - 3s - loss: 293.2334 - loglik: -2.9861e+02 - logprior: 6.0074
Epoch 7/10
10/10 - 3s - loss: 292.4817 - loglik: -2.9889e+02 - logprior: 7.0433
Epoch 8/10
10/10 - 3s - loss: 291.7636 - loglik: -2.9896e+02 - logprior: 7.8339
Epoch 9/10
10/10 - 3s - loss: 290.9924 - loglik: -2.9881e+02 - logprior: 8.4527
Epoch 10/10
10/10 - 3s - loss: 290.5410 - loglik: -2.9884e+02 - logprior: 8.9348
Fitted a model with MAP estimate = -289.4183
Time for alignment: 86.2529
Computed alignments with likelihoods: ['-289.2437', '-291.2033', '-289.4183']
Best model has likelihood: -289.2437
time for generating output: 0.2115
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sti.projection.fasta
SP score = 0.8760074395536268
Training of 3 independent models on file glob.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3462a36430>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34edcd7250>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34af21bdf0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34a6fbe970>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33a3948fa0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34af7a7610>, <__main__.SimpleDirichletPrior object at 0x7f3381e621c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34ee88e160>

Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 292.6302 - loglik: -2.8257e+02 - logprior: -1.0051e+01
Epoch 2/10
12/12 - 2s - loss: 254.4993 - loglik: -2.5191e+02 - logprior: -2.5440e+00
Epoch 3/10
12/12 - 2s - loss: 226.6633 - loglik: -2.2455e+02 - logprior: -1.8541e+00
Epoch 4/10
12/12 - 2s - loss: 213.8136 - loglik: -2.1143e+02 - logprior: -1.9344e+00
Epoch 5/10
12/12 - 2s - loss: 208.8164 - loglik: -2.0615e+02 - logprior: -1.9945e+00
Epoch 6/10
12/12 - 2s - loss: 208.0163 - loglik: -2.0546e+02 - logprior: -1.9011e+00
Epoch 7/10
12/12 - 2s - loss: 206.5105 - loglik: -2.0414e+02 - logprior: -1.8298e+00
Epoch 8/10
12/12 - 2s - loss: 206.5665 - loglik: -2.0429e+02 - logprior: -1.8179e+00
Fitted a model with MAP estimate = -205.5127
expansions: [(6, 3), (10, 3), (11, 2), (20, 1), (29, 1), (36, 3), (49, 1), (50, 3), (52, 1), (59, 5), (61, 1), (64, 1), (70, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 211.6387 - loglik: -1.9947e+02 - logprior: -1.1722e+01
Epoch 2/2
12/12 - 2s - loss: 192.8798 - loglik: -1.8715e+02 - logprior: -5.3123e+00
Fitted a model with MAP estimate = -189.1136
expansions: [(0, 2)]
discards: [ 0 16 37 78]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 192.6766 - loglik: -1.8283e+02 - logprior: -9.3655e+00
Epoch 2/2
12/12 - 2s - loss: 183.4023 - loglik: -1.8012e+02 - logprior: -2.8126e+00
Fitted a model with MAP estimate = -181.6828
expansions: [(15, 1)]
discards: [ 0 44]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 195.0576 - loglik: -1.8302e+02 - logprior: -1.1568e+01
Epoch 2/10
12/12 - 2s - loss: 185.9539 - loglik: -1.8143e+02 - logprior: -4.0697e+00
Epoch 3/10
12/12 - 2s - loss: 182.7011 - loglik: -1.7999e+02 - logprior: -2.2582e+00
Epoch 4/10
12/12 - 2s - loss: 181.5506 - loglik: -1.7958e+02 - logprior: -1.5440e+00
Epoch 5/10
12/12 - 2s - loss: 182.0867 - loglik: -1.8026e+02 - logprior: -1.4065e+00
Fitted a model with MAP estimate = -181.1869
Time for alignment: 54.2931
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 292.6034 - loglik: -2.8254e+02 - logprior: -1.0051e+01
Epoch 2/10
12/12 - 2s - loss: 253.9617 - loglik: -2.5138e+02 - logprior: -2.5308e+00
Epoch 3/10
12/12 - 2s - loss: 227.1851 - loglik: -2.2507e+02 - logprior: -1.8404e+00
Epoch 4/10
12/12 - 2s - loss: 214.5547 - loglik: -2.1185e+02 - logprior: -1.9548e+00
Epoch 5/10
12/12 - 2s - loss: 209.2084 - loglik: -2.0618e+02 - logprior: -2.0544e+00
Epoch 6/10
12/12 - 2s - loss: 206.8149 - loglik: -2.0413e+02 - logprior: -1.9504e+00
Epoch 7/10
12/12 - 2s - loss: 205.3089 - loglik: -2.0281e+02 - logprior: -1.9288e+00
Epoch 8/10
12/12 - 2s - loss: 203.8655 - loglik: -2.0137e+02 - logprior: -1.9803e+00
Epoch 9/10
12/12 - 2s - loss: 203.5735 - loglik: -2.0110e+02 - logprior: -1.9971e+00
Epoch 10/10
12/12 - 2s - loss: 202.7818 - loglik: -2.0034e+02 - logprior: -1.9841e+00
Fitted a model with MAP estimate = -202.3460
expansions: [(8, 1), (9, 1), (10, 5), (11, 2), (21, 1), (36, 3), (49, 1), (50, 3), (58, 3), (59, 5), (61, 1), (64, 1), (70, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 208.7443 - loglik: -1.9645e+02 - logprior: -1.1821e+01
Epoch 2/2
12/12 - 2s - loss: 188.2113 - loglik: -1.8229e+02 - logprior: -5.4203e+00
Fitted a model with MAP estimate = -184.4411
expansions: [(0, 2)]
discards: [ 0 47 75 76 79]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 189.1604 - loglik: -1.7940e+02 - logprior: -9.2813e+00
Epoch 2/2
12/12 - 2s - loss: 180.9301 - loglik: -1.7777e+02 - logprior: -2.7012e+00
Fitted a model with MAP estimate = -179.2267
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 191.8175 - loglik: -1.7990e+02 - logprior: -1.1463e+01
Epoch 2/10
12/12 - 2s - loss: 182.5712 - loglik: -1.7825e+02 - logprior: -3.8978e+00
Epoch 3/10
12/12 - 2s - loss: 180.1035 - loglik: -1.7748e+02 - logprior: -2.1996e+00
Epoch 4/10
12/12 - 2s - loss: 179.4644 - loglik: -1.7747e+02 - logprior: -1.5603e+00
Epoch 5/10
12/12 - 2s - loss: 178.5750 - loglik: -1.7670e+02 - logprior: -1.4375e+00
Epoch 6/10
12/12 - 2s - loss: 178.9421 - loglik: -1.7720e+02 - logprior: -1.2896e+00
Fitted a model with MAP estimate = -178.1394
Time for alignment: 59.1942
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 292.5636 - loglik: -2.8250e+02 - logprior: -1.0051e+01
Epoch 2/10
12/12 - 2s - loss: 254.1739 - loglik: -2.5158e+02 - logprior: -2.5382e+00
Epoch 3/10
12/12 - 2s - loss: 225.9689 - loglik: -2.2382e+02 - logprior: -1.8466e+00
Epoch 4/10
12/12 - 2s - loss: 213.1863 - loglik: -2.1068e+02 - logprior: -2.0193e+00
Epoch 5/10
12/12 - 2s - loss: 208.0980 - loglik: -2.0528e+02 - logprior: -2.1138e+00
Epoch 6/10
12/12 - 2s - loss: 206.2717 - loglik: -2.0357e+02 - logprior: -2.0009e+00
Epoch 7/10
12/12 - 2s - loss: 204.7377 - loglik: -2.0218e+02 - logprior: -1.9472e+00
Epoch 8/10
12/12 - 2s - loss: 205.0687 - loglik: -2.0261e+02 - logprior: -1.9328e+00
Fitted a model with MAP estimate = -203.6704
expansions: [(6, 3), (10, 4), (11, 2), (21, 1), (36, 3), (49, 2), (50, 3), (52, 1), (58, 3), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 209.5579 - loglik: -1.9724e+02 - logprior: -1.1825e+01
Epoch 2/2
12/12 - 2s - loss: 190.8182 - loglik: -1.8498e+02 - logprior: -5.3650e+00
Fitted a model with MAP estimate = -186.4557
expansions: [(0, 2)]
discards: [ 0 47 62 80]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 191.1100 - loglik: -1.8120e+02 - logprior: -9.4085e+00
Epoch 2/2
12/12 - 2s - loss: 181.9801 - loglik: -1.7861e+02 - logprior: -2.8902e+00
Fitted a model with MAP estimate = -180.1642
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 192.3032 - loglik: -1.8014e+02 - logprior: -1.1693e+01
Epoch 2/10
12/12 - 2s - loss: 184.5220 - loglik: -1.7991e+02 - logprior: -4.1626e+00
Epoch 3/10
12/12 - 2s - loss: 180.9316 - loglik: -1.7809e+02 - logprior: -2.4054e+00
Epoch 4/10
12/12 - 2s - loss: 180.2934 - loglik: -1.7812e+02 - logprior: -1.7401e+00
Epoch 5/10
12/12 - 2s - loss: 179.8150 - loglik: -1.7776e+02 - logprior: -1.6091e+00
Epoch 6/10
12/12 - 2s - loss: 179.3778 - loglik: -1.7746e+02 - logprior: -1.4746e+00
Epoch 7/10
12/12 - 2s - loss: 179.6136 - loglik: -1.7776e+02 - logprior: -1.4268e+00
Fitted a model with MAP estimate = -179.0307
Time for alignment: 57.6232
Computed alignments with likelihoods: ['-181.1869', '-178.1394', '-179.0307']
Best model has likelihood: -178.1394
time for generating output: 0.2288
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/glob.projection.fasta
SP score = 0.8889548020134521
Training of 3 independent models on file az.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f342df355e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f32f8cd7370>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32f8cd74f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f32eeae2a60>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b7bb8430>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34af7a7b20>, <__main__.SimpleDirichletPrior object at 0x7f32f95b1310>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f335693a8b0>

Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 329.4805 - loglik: -2.8876e+02 - logprior: -4.0701e+01
Epoch 2/10
10/10 - 1s - loss: 276.8321 - loglik: -2.6630e+02 - logprior: -1.0524e+01
Epoch 3/10
10/10 - 1s - loss: 250.5039 - loglik: -2.4587e+02 - logprior: -4.6237e+00
Epoch 4/10
10/10 - 1s - loss: 237.0919 - loglik: -2.3447e+02 - logprior: -2.5565e+00
Epoch 5/10
10/10 - 1s - loss: 231.6662 - loglik: -2.2973e+02 - logprior: -1.6555e+00
Epoch 6/10
10/10 - 1s - loss: 228.2453 - loglik: -2.2661e+02 - logprior: -1.2162e+00
Epoch 7/10
10/10 - 1s - loss: 226.3418 - loglik: -2.2490e+02 - logprior: -1.0623e+00
Epoch 8/10
10/10 - 1s - loss: 225.2778 - loglik: -2.2399e+02 - logprior: -9.2955e-01
Epoch 9/10
10/10 - 1s - loss: 224.4531 - loglik: -2.2325e+02 - logprior: -8.3277e-01
Epoch 10/10
10/10 - 1s - loss: 223.9853 - loglik: -2.2287e+02 - logprior: -7.5893e-01
Fitted a model with MAP estimate = -223.4901
expansions: [(0, 3), (5, 1), (8, 1), (36, 1), (37, 2), (43, 12), (53, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 272.3235 - loglik: -2.1912e+02 - logprior: -5.2845e+01
Epoch 2/2
10/10 - 1s - loss: 230.4939 - loglik: -2.1371e+02 - logprior: -1.6390e+01
Fitted a model with MAP estimate = -222.5120
expansions: [(7, 1)]
discards: [ 0  1 56 57 58 74]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 94 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 260.9333 - loglik: -2.1379e+02 - logprior: -4.6701e+01
Epoch 2/2
10/10 - 1s - loss: 231.7889 - loglik: -2.1274e+02 - logprior: -1.8611e+01
Fitted a model with MAP estimate = -227.0545
expansions: [(0, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 253.6474 - loglik: -2.1143e+02 - logprior: -4.1767e+01
Epoch 2/10
10/10 - 1s - loss: 222.3165 - loglik: -2.1071e+02 - logprior: -1.1143e+01
Epoch 3/10
10/10 - 1s - loss: 216.3497 - loglik: -2.1161e+02 - logprior: -4.2767e+00
Epoch 4/10
10/10 - 1s - loss: 214.0449 - loglik: -2.1182e+02 - logprior: -1.7516e+00
Epoch 5/10
10/10 - 1s - loss: 212.6989 - loglik: -2.1178e+02 - logprior: -4.4207e-01
Epoch 6/10
10/10 - 1s - loss: 212.6527 - loglik: -2.1244e+02 - logprior: 0.2733
Epoch 7/10
10/10 - 1s - loss: 212.2100 - loglik: -2.1241e+02 - logprior: 0.6860
Epoch 8/10
10/10 - 1s - loss: 211.9201 - loglik: -2.1238e+02 - logprior: 0.9455
Epoch 9/10
10/10 - 1s - loss: 211.8571 - loglik: -2.1251e+02 - logprior: 1.1412
Epoch 10/10
10/10 - 1s - loss: 211.5933 - loglik: -2.1243e+02 - logprior: 1.3208
Fitted a model with MAP estimate = -210.9889
Time for alignment: 53.2289
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 329.6345 - loglik: -2.8892e+02 - logprior: -4.0699e+01
Epoch 2/10
10/10 - 1s - loss: 276.6944 - loglik: -2.6616e+02 - logprior: -1.0522e+01
Epoch 3/10
10/10 - 1s - loss: 250.6225 - loglik: -2.4599e+02 - logprior: -4.6236e+00
Epoch 4/10
10/10 - 1s - loss: 237.1105 - loglik: -2.3448e+02 - logprior: -2.5663e+00
Epoch 5/10
10/10 - 1s - loss: 231.5164 - loglik: -2.2948e+02 - logprior: -1.7508e+00
Epoch 6/10
10/10 - 1s - loss: 228.4747 - loglik: -2.2676e+02 - logprior: -1.2893e+00
Epoch 7/10
10/10 - 1s - loss: 227.0275 - loglik: -2.2556e+02 - logprior: -1.0730e+00
Epoch 8/10
10/10 - 1s - loss: 225.9099 - loglik: -2.2455e+02 - logprior: -9.7413e-01
Epoch 9/10
10/10 - 1s - loss: 225.0995 - loglik: -2.2375e+02 - logprior: -9.3380e-01
Epoch 10/10
10/10 - 1s - loss: 224.5080 - loglik: -2.2320e+02 - logprior: -9.0369e-01
Fitted a model with MAP estimate = -223.9878
expansions: [(0, 3), (6, 2), (7, 1), (8, 1), (36, 1), (37, 2), (43, 12), (53, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 101 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 272.1848 - loglik: -2.1921e+02 - logprior: -5.2598e+01
Epoch 2/2
10/10 - 2s - loss: 229.6988 - loglik: -2.1300e+02 - logprior: -1.6286e+01
Fitted a model with MAP estimate = -221.4462
expansions: []
discards: [ 0  1  9 76]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 97 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 258.5159 - loglik: -2.1166e+02 - logprior: -4.6432e+01
Epoch 2/2
10/10 - 1s - loss: 230.8015 - loglik: -2.1182e+02 - logprior: -1.8542e+01
Fitted a model with MAP estimate = -225.7192
expansions: [(0, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 252.4597 - loglik: -2.1026e+02 - logprior: -4.1758e+01
Epoch 2/10
10/10 - 2s - loss: 221.2022 - loglik: -2.0965e+02 - logprior: -1.1112e+01
Epoch 3/10
10/10 - 1s - loss: 215.4013 - loglik: -2.1072e+02 - logprior: -4.2432e+00
Epoch 4/10
10/10 - 2s - loss: 213.2402 - loglik: -2.1111e+02 - logprior: -1.6875e+00
Epoch 5/10
10/10 - 2s - loss: 211.9490 - loglik: -2.1111e+02 - logprior: -3.8972e-01
Epoch 6/10
10/10 - 2s - loss: 211.6397 - loglik: -2.1153e+02 - logprior: 0.3348
Epoch 7/10
10/10 - 2s - loss: 211.0834 - loglik: -2.1139e+02 - logprior: 0.7573
Epoch 8/10
10/10 - 2s - loss: 210.7515 - loglik: -2.1132e+02 - logprior: 1.0201
Epoch 9/10
10/10 - 1s - loss: 211.3335 - loglik: -2.1210e+02 - logprior: 1.2204
Fitted a model with MAP estimate = -210.2163
Time for alignment: 52.5673
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 329.5507 - loglik: -2.8883e+02 - logprior: -4.0702e+01
Epoch 2/10
10/10 - 1s - loss: 277.0916 - loglik: -2.6654e+02 - logprior: -1.0538e+01
Epoch 3/10
10/10 - 1s - loss: 251.0094 - loglik: -2.4635e+02 - logprior: -4.6479e+00
Epoch 4/10
10/10 - 1s - loss: 236.8257 - loglik: -2.3417e+02 - logprior: -2.5896e+00
Epoch 5/10
10/10 - 1s - loss: 231.1658 - loglik: -2.2916e+02 - logprior: -1.7240e+00
Epoch 6/10
10/10 - 1s - loss: 228.9487 - loglik: -2.2723e+02 - logprior: -1.3000e+00
Epoch 7/10
10/10 - 1s - loss: 226.6926 - loglik: -2.2524e+02 - logprior: -1.0697e+00
Epoch 8/10
10/10 - 1s - loss: 225.8430 - loglik: -2.2451e+02 - logprior: -9.6634e-01
Epoch 9/10
10/10 - 1s - loss: 224.7723 - loglik: -2.2348e+02 - logprior: -8.9298e-01
Epoch 10/10
10/10 - 1s - loss: 224.6212 - loglik: -2.2342e+02 - logprior: -7.9785e-01
Fitted a model with MAP estimate = -224.0143
expansions: [(0, 3), (5, 2), (7, 1), (8, 1), (36, 1), (37, 2), (43, 12), (53, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 101 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 272.0311 - loglik: -2.1911e+02 - logprior: -5.2548e+01
Epoch 2/2
10/10 - 1s - loss: 230.7560 - loglik: -2.1414e+02 - logprior: -1.6225e+01
Fitted a model with MAP estimate = -222.0141
expansions: []
discards: [ 0  1 58 59 60 76]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 259.9998 - loglik: -2.1309e+02 - logprior: -4.6470e+01
Epoch 2/2
10/10 - 1s - loss: 231.4799 - loglik: -2.1250e+02 - logprior: -1.8527e+01
Fitted a model with MAP estimate = -226.6547
expansions: [(0, 3)]
discards: [0 7]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 253.6566 - loglik: -2.1148e+02 - logprior: -4.1718e+01
Epoch 2/10
10/10 - 1s - loss: 222.4448 - loglik: -2.1086e+02 - logprior: -1.1122e+01
Epoch 3/10
10/10 - 1s - loss: 215.9561 - loglik: -2.1121e+02 - logprior: -4.2770e+00
Epoch 4/10
10/10 - 1s - loss: 214.0741 - loglik: -2.1187e+02 - logprior: -1.7249e+00
Epoch 5/10
10/10 - 1s - loss: 213.2217 - loglik: -2.1230e+02 - logprior: -4.3495e-01
Epoch 6/10
10/10 - 1s - loss: 212.4126 - loglik: -2.1220e+02 - logprior: 0.2803
Epoch 7/10
10/10 - 1s - loss: 212.1529 - loglik: -2.1237e+02 - logprior: 0.6978
Epoch 8/10
10/10 - 1s - loss: 212.0622 - loglik: -2.1252e+02 - logprior: 0.9455
Epoch 9/10
10/10 - 1s - loss: 211.1952 - loglik: -2.1185e+02 - logprior: 1.1422
Epoch 10/10
10/10 - 1s - loss: 212.0726 - loglik: -2.1290e+02 - logprior: 1.3171
Fitted a model with MAP estimate = -210.9777
Time for alignment: 51.5051
Computed alignments with likelihoods: ['-210.9889', '-210.2163', '-210.9777']
Best model has likelihood: -210.2163
time for generating output: 0.1782
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/az.projection.fasta
SP score = 0.7249134497748393
Training of 3 independent models on file ghf11.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f349e2ac580>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f346b673a90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f342cf0b520>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f33a40fceb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3484ad05b0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f33a406fe80>, <__main__.SimpleDirichletPrior object at 0x7f342d8f1af0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f342ce0ea60>

Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 553.2573 - loglik: -4.7363e+02 - logprior: -7.9620e+01
Epoch 2/10
10/10 - 3s - loss: 421.1769 - loglik: -4.0409e+02 - logprior: -1.7062e+01
Epoch 3/10
10/10 - 3s - loss: 342.4986 - loglik: -3.3521e+02 - logprior: -7.1994e+00
Epoch 4/10
10/10 - 3s - loss: 297.0388 - loglik: -2.9192e+02 - logprior: -5.0656e+00
Epoch 5/10
10/10 - 3s - loss: 278.5644 - loglik: -2.7429e+02 - logprior: -4.2469e+00
Epoch 6/10
10/10 - 3s - loss: 270.0994 - loglik: -2.6634e+02 - logprior: -3.6590e+00
Epoch 7/10
10/10 - 3s - loss: 266.6253 - loglik: -2.6329e+02 - logprior: -3.0373e+00
Epoch 8/10
10/10 - 3s - loss: 264.8216 - loglik: -2.6200e+02 - logprior: -2.4338e+00
Epoch 9/10
10/10 - 3s - loss: 263.3558 - loglik: -2.6082e+02 - logprior: -2.1949e+00
Epoch 10/10
10/10 - 3s - loss: 263.2778 - loglik: -2.6109e+02 - logprior: -1.8930e+00
Fitted a model with MAP estimate = -262.4454
expansions: [(18, 1), (20, 1), (22, 1), (26, 1), (39, 2), (40, 3), (41, 2), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 2), (90, 1), (91, 1), (98, 2), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 345.9416 - loglik: -2.5461e+02 - logprior: -9.1026e+01
Epoch 2/2
10/10 - 2s - loss: 270.1374 - loglik: -2.3487e+02 - logprior: -3.4958e+01
Fitted a model with MAP estimate = -258.3434
expansions: [(0, 3), (15, 3), (89, 1)]
discards: [  0  43  45  46  49  56  96 119 123]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 180 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 300.4352 - loglik: -2.2811e+02 - logprior: -7.2047e+01
Epoch 2/2
10/10 - 2s - loss: 237.4516 - loglik: -2.2254e+02 - logprior: -1.4622e+01
Fitted a model with MAP estimate = -227.7304
expansions: [(16, 1), (17, 1), (21, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 309.2774 - loglik: -2.2130e+02 - logprior: -8.7694e+01
Epoch 2/10
10/10 - 2s - loss: 248.7773 - loglik: -2.1970e+02 - logprior: -2.8784e+01
Epoch 3/10
10/10 - 2s - loss: 227.2944 - loglik: -2.1902e+02 - logprior: -7.9713e+00
Epoch 4/10
10/10 - 2s - loss: 215.4506 - loglik: -2.1829e+02 - logprior: 3.1436
Epoch 5/10
10/10 - 2s - loss: 211.3985 - loglik: -2.1816e+02 - logprior: 7.0565
Epoch 6/10
10/10 - 2s - loss: 210.5975 - loglik: -2.1934e+02 - logprior: 9.0445
Epoch 7/10
10/10 - 3s - loss: 209.4227 - loglik: -2.1942e+02 - logprior: 10.2941
Epoch 8/10
10/10 - 2s - loss: 208.3969 - loglik: -2.1930e+02 - logprior: 11.2084
Epoch 9/10
10/10 - 2s - loss: 207.9987 - loglik: -2.1968e+02 - logprior: 11.9801
Epoch 10/10
10/10 - 2s - loss: 206.9028 - loglik: -2.1929e+02 - logprior: 12.6825
Fitted a model with MAP estimate = -206.2295
Time for alignment: 86.6487
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 553.3011 - loglik: -4.7368e+02 - logprior: -7.9619e+01
Epoch 2/10
10/10 - 3s - loss: 420.8849 - loglik: -4.0380e+02 - logprior: -1.7063e+01
Epoch 3/10
10/10 - 3s - loss: 343.1705 - loglik: -3.3586e+02 - logprior: -7.2175e+00
Epoch 4/10
10/10 - 3s - loss: 295.9134 - loglik: -2.9062e+02 - logprior: -5.2403e+00
Epoch 5/10
10/10 - 3s - loss: 278.2712 - loglik: -2.7372e+02 - logprior: -4.5299e+00
Epoch 6/10
10/10 - 3s - loss: 271.2393 - loglik: -2.6742e+02 - logprior: -3.7862e+00
Epoch 7/10
10/10 - 3s - loss: 267.5582 - loglik: -2.6435e+02 - logprior: -3.1072e+00
Epoch 8/10
10/10 - 3s - loss: 265.2293 - loglik: -2.6227e+02 - logprior: -2.6716e+00
Epoch 9/10
10/10 - 3s - loss: 264.2300 - loglik: -2.6154e+02 - logprior: -2.3198e+00
Epoch 10/10
10/10 - 3s - loss: 263.6904 - loglik: -2.6146e+02 - logprior: -1.9150e+00
Fitted a model with MAP estimate = -262.9180
expansions: [(18, 1), (20, 1), (22, 1), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 2), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (127, 1), (129, 3), (133, 1), (137, 1), (138, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 344.1583 - loglik: -2.5280e+02 - logprior: -9.1074e+01
Epoch 2/2
10/10 - 2s - loss: 269.1853 - loglik: -2.3435e+02 - logprior: -3.4538e+01
Fitted a model with MAP estimate = -257.6216
expansions: [(0, 3), (15, 3), (86, 1)]
discards: [  0  53  93 119]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 299.6393 - loglik: -2.2728e+02 - logprior: -7.2084e+01
Epoch 2/2
10/10 - 2s - loss: 237.5704 - loglik: -2.2272e+02 - logprior: -1.4571e+01
Fitted a model with MAP estimate = -227.5010
expansions: [(16, 1), (17, 1), (21, 1)]
discards: [ 0  1 48]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 309.6603 - loglik: -2.2171e+02 - logprior: -8.7664e+01
Epoch 2/10
10/10 - 2s - loss: 248.3717 - loglik: -2.1940e+02 - logprior: -2.8675e+01
Epoch 3/10
10/10 - 2s - loss: 227.1115 - loglik: -2.1900e+02 - logprior: -7.8066e+00
Epoch 4/10
10/10 - 2s - loss: 216.1376 - loglik: -2.1902e+02 - logprior: 3.1904
Epoch 5/10
10/10 - 2s - loss: 211.2981 - loglik: -2.1804e+02 - logprior: 7.0490
Epoch 6/10
10/10 - 3s - loss: 210.7254 - loglik: -2.1944e+02 - logprior: 9.0243
Epoch 7/10
10/10 - 2s - loss: 208.6219 - loglik: -2.1860e+02 - logprior: 10.2807
Epoch 8/10
10/10 - 2s - loss: 208.3981 - loglik: -2.1928e+02 - logprior: 11.1913
Epoch 9/10
10/10 - 2s - loss: 207.8471 - loglik: -2.1950e+02 - logprior: 11.9542
Epoch 10/10
10/10 - 3s - loss: 206.9827 - loglik: -2.1934e+02 - logprior: 12.6577
Fitted a model with MAP estimate = -206.2498
Time for alignment: 86.5216
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 553.1207 - loglik: -4.7350e+02 - logprior: -7.9619e+01
Epoch 2/10
10/10 - 3s - loss: 421.3228 - loglik: -4.0424e+02 - logprior: -1.7058e+01
Epoch 3/10
10/10 - 3s - loss: 343.0751 - loglik: -3.3580e+02 - logprior: -7.1857e+00
Epoch 4/10
10/10 - 3s - loss: 295.1915 - loglik: -2.8987e+02 - logprior: -5.2674e+00
Epoch 5/10
10/10 - 3s - loss: 277.6328 - loglik: -2.7284e+02 - logprior: -4.7744e+00
Epoch 6/10
10/10 - 3s - loss: 270.2635 - loglik: -2.6657e+02 - logprior: -3.6647e+00
Epoch 7/10
10/10 - 3s - loss: 267.4730 - loglik: -2.6470e+02 - logprior: -2.7173e+00
Epoch 8/10
10/10 - 3s - loss: 265.1184 - loglik: -2.6249e+02 - logprior: -2.4071e+00
Epoch 9/10
10/10 - 3s - loss: 264.4876 - loglik: -2.6196e+02 - logprior: -2.1611e+00
Epoch 10/10
10/10 - 3s - loss: 263.5376 - loglik: -2.6145e+02 - logprior: -1.7507e+00
Fitted a model with MAP estimate = -263.0390
expansions: [(21, 1), (23, 2), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 2), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 343.7597 - loglik: -2.5238e+02 - logprior: -9.1098e+01
Epoch 2/2
10/10 - 2s - loss: 269.8636 - loglik: -2.3500e+02 - logprior: -3.4571e+01
Fitted a model with MAP estimate = -257.4398
expansions: [(0, 3), (15, 3), (16, 2), (86, 1)]
discards: [ 53  93 119]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 297.4335 - loglik: -2.2504e+02 - logprior: -7.2116e+01
Epoch 2/2
10/10 - 3s - loss: 234.9530 - loglik: -2.1999e+02 - logprior: -1.4670e+01
Fitted a model with MAP estimate = -224.8776
expansions: []
discards: [ 0  1  2 22 50]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 179 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 309.7549 - loglik: -2.2125e+02 - logprior: -8.8211e+01
Epoch 2/10
10/10 - 2s - loss: 251.4166 - loglik: -2.2056e+02 - logprior: -3.0548e+01
Epoch 3/10
10/10 - 2s - loss: 232.6989 - loglik: -2.2116e+02 - logprior: -1.1241e+01
Epoch 4/10
10/10 - 2s - loss: 218.2921 - loglik: -2.1999e+02 - logprior: 2.0031
Epoch 5/10
10/10 - 2s - loss: 215.0451 - loglik: -2.2138e+02 - logprior: 6.6461
Epoch 6/10
10/10 - 2s - loss: 211.3059 - loglik: -2.1969e+02 - logprior: 8.6937
Epoch 7/10
10/10 - 2s - loss: 211.3965 - loglik: -2.2104e+02 - logprior: 9.9460
Fitted a model with MAP estimate = -210.1640
Time for alignment: 75.9690
Computed alignments with likelihoods: ['-206.2295', '-206.2498', '-210.1640']
Best model has likelihood: -206.2295
time for generating output: 0.2228
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf11.projection.fasta
SP score = 0.9142376681614349
Training of 3 independent models on file subt.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f339296dd90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f343fa7af40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34ee8125e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f343fb74d00>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f343fb74c70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f33a376a9a0>, <__main__.SimpleDirichletPrior object at 0x7f345a6abb20>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3484b733a0>

Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 29s - loss: 811.0724 - loglik: -8.0858e+02 - logprior: -2.1960e+00
Epoch 2/10
33/33 - 24s - loss: 717.2821 - loglik: -7.1564e+02 - logprior: -8.8487e-01
Epoch 3/10
33/33 - 24s - loss: 707.3386 - loglik: -7.0519e+02 - logprior: -9.1109e-01
Epoch 4/10
33/33 - 25s - loss: 706.0211 - loglik: -7.0383e+02 - logprior: -9.2005e-01
Epoch 5/10
33/33 - 25s - loss: 703.5220 - loglik: -7.0136e+02 - logprior: -9.1233e-01
Epoch 6/10
33/33 - 25s - loss: 701.2643 - loglik: -6.9909e+02 - logprior: -9.2432e-01
Epoch 7/10
33/33 - 25s - loss: 700.7504 - loglik: -6.9862e+02 - logprior: -9.2303e-01
Epoch 8/10
33/33 - 25s - loss: 699.2127 - loglik: -6.9712e+02 - logprior: -9.2365e-01
Epoch 9/10
33/33 - 25s - loss: 698.6920 - loglik: -6.9664e+02 - logprior: -9.2961e-01
Epoch 10/10
33/33 - 24s - loss: 697.7080 - loglik: -6.9570e+02 - logprior: -9.5034e-01
Fitted a model with MAP estimate = -697.0542
expansions: [(0, 6), (10, 1), (33, 9), (61, 1), (62, 2), (71, 1), (72, 2), (83, 1), (110, 2), (113, 1), (118, 1), (135, 1), (155, 4), (163, 4), (184, 1), (210, 1), (224, 2), (230, 4)]
discards: [226 227 228]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 271 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 34s - loss: 694.4912 - loglik: -6.8980e+02 - logprior: -3.6287e+00
Epoch 2/2
33/33 - 30s - loss: 686.4291 - loglik: -6.8394e+02 - logprior: -1.1855e+00
Fitted a model with MAP estimate = -682.7564
expansions: [(134, 4)]
discards: [  1   2  16  42  43  44  45  46  77 183 184 195 196 267 268 269 270]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 258 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 32s - loss: 688.0441 - loglik: -6.8395e+02 - logprior: -2.6590e+00
Epoch 2/2
33/33 - 28s - loss: 682.9145 - loglik: -6.8062e+02 - logprior: -7.3510e-01
Fitted a model with MAP estimate = -681.7720
expansions: [(0, 3), (14, 1), (188, 4), (255, 2), (258, 3)]
discards: [124 125]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 269 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 35s - loss: 687.4305 - loglik: -6.8203e+02 - logprior: -3.8564e+00
Epoch 2/10
33/33 - 30s - loss: 680.6127 - loglik: -6.7832e+02 - logprior: -7.3709e-01
Epoch 3/10
33/33 - 29s - loss: 680.9428 - loglik: -6.7868e+02 - logprior: -5.5509e-01
Fitted a model with MAP estimate = -677.7749
Time for alignment: 577.3610
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 28s - loss: 814.9498 - loglik: -8.1246e+02 - logprior: -2.1888e+00
Epoch 2/10
33/33 - 25s - loss: 720.5986 - loglik: -7.1883e+02 - logprior: -9.5552e-01
Epoch 3/10
33/33 - 24s - loss: 707.6056 - loglik: -7.0532e+02 - logprior: -9.9054e-01
Epoch 4/10
33/33 - 24s - loss: 706.1195 - loglik: -7.0379e+02 - logprior: -9.8440e-01
Epoch 5/10
33/33 - 24s - loss: 702.0107 - loglik: -6.9969e+02 - logprior: -9.9121e-01
Epoch 6/10
33/33 - 25s - loss: 703.8431 - loglik: -7.0154e+02 - logprior: -9.9434e-01
Fitted a model with MAP estimate = -700.5343
expansions: [(0, 5), (9, 1), (34, 6), (45, 2), (72, 1), (73, 3), (78, 1), (111, 1), (114, 1), (116, 1), (118, 1), (135, 1), (155, 4), (163, 4), (180, 2), (203, 1), (220, 2), (221, 4)]
discards: [226 227 228]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 268 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 33s - loss: 697.7505 - loglik: -6.9279e+02 - logprior: -3.5910e+00
Epoch 2/2
33/33 - 29s - loss: 689.2295 - loglik: -6.8663e+02 - logprior: -1.1609e+00
Fitted a model with MAP estimate = -684.9515
expansions: [(90, 1)]
discards: [  1   6   7   8  42  43  57 179 180 192 213 256 265 266]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 255 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 31s - loss: 690.7029 - loglik: -6.8640e+02 - logprior: -2.7229e+00
Epoch 2/2
33/33 - 28s - loss: 688.7334 - loglik: -6.8652e+02 - logprior: -7.3968e-01
Fitted a model with MAP estimate = -684.8144
expansions: [(0, 4), (1, 1), (124, 5), (255, 3)]
discards: [183 253]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 266 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 32s - loss: 690.0652 - loglik: -6.8464e+02 - logprior: -3.7821e+00
Epoch 2/10
33/33 - 30s - loss: 683.7703 - loglik: -6.8136e+02 - logprior: -7.7315e-01
Epoch 3/10
33/33 - 29s - loss: 682.4327 - loglik: -6.8013e+02 - logprior: -5.1640e-01
Epoch 4/10
33/33 - 30s - loss: 683.9098 - loglik: -6.8191e+02 - logprior: -4.1302e-01
Fitted a model with MAP estimate = -679.6702
Time for alignment: 501.8952
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 29s - loss: 813.1364 - loglik: -8.1061e+02 - logprior: -2.2145e+00
Epoch 2/10
33/33 - 24s - loss: 718.7172 - loglik: -7.1681e+02 - logprior: -9.8874e-01
Epoch 3/10
33/33 - 25s - loss: 706.5405 - loglik: -7.0408e+02 - logprior: -1.0288e+00
Epoch 4/10
33/33 - 24s - loss: 706.2925 - loglik: -7.0377e+02 - logprior: -1.0695e+00
Epoch 5/10
33/33 - 24s - loss: 701.4764 - loglik: -6.9901e+02 - logprior: -1.0822e+00
Epoch 6/10
33/33 - 25s - loss: 698.7920 - loglik: -6.9637e+02 - logprior: -1.1121e+00
Epoch 7/10
33/33 - 25s - loss: 703.0563 - loglik: -7.0072e+02 - logprior: -1.1173e+00
Fitted a model with MAP estimate = -698.2070
expansions: [(0, 4), (7, 1), (9, 2), (35, 3), (67, 1), (73, 1), (78, 1), (79, 3), (114, 1), (117, 1), (119, 1), (136, 1), (156, 4), (164, 4), (177, 1), (204, 1), (217, 1), (220, 2), (221, 4)]
discards: [226 227 228]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 33s - loss: 694.4986 - loglik: -6.8967e+02 - logprior: -3.5561e+00
Epoch 2/2
33/33 - 29s - loss: 687.7564 - loglik: -6.8539e+02 - logprior: -1.0380e+00
Fitted a model with MAP estimate = -684.9052
expansions: [(75, 1), (127, 5), (264, 3)]
discards: [  1   2   3   7   8  73 176 177 189 190 191 252 261 262]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 259 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 33s - loss: 690.2915 - loglik: -6.8608e+02 - logprior: -2.7244e+00
Epoch 2/2
33/33 - 28s - loss: 684.2186 - loglik: -6.8185e+02 - logprior: -8.1609e-01
Fitted a model with MAP estimate = -683.5607
expansions: [(0, 4), (1, 1), (187, 4), (259, 3)]
discards: [122 123 254 255 256 257]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 265 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 32s - loss: 689.5406 - loglik: -6.8407e+02 - logprior: -3.7779e+00
Epoch 2/10
33/33 - 29s - loss: 683.6588 - loglik: -6.8122e+02 - logprior: -7.9418e-01
Epoch 3/10
33/33 - 29s - loss: 681.3547 - loglik: -6.7908e+02 - logprior: -5.5484e-01
Epoch 4/10
33/33 - 29s - loss: 684.1215 - loglik: -6.8216e+02 - logprior: -3.8087e-01
Fitted a model with MAP estimate = -678.5329
Time for alignment: 526.3357
Computed alignments with likelihoods: ['-677.7749', '-679.6702', '-678.5329']
Best model has likelihood: -677.7749
time for generating output: 0.3835
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/subt.projection.fasta
SP score = 0.75815142304504
Training of 3 independent models on file profilin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3462d32d30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f338a449760>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b7dad6a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f33246bbbe0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33246bb4c0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f342d1ec1c0>, <__main__.SimpleDirichletPrior object at 0x7f349e057fd0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f32f8a0e670>

Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 411.7331 - loglik: -3.4884e+02 - logprior: -6.2885e+01
Epoch 2/10
10/10 - 2s - loss: 322.1222 - loglik: -3.0687e+02 - logprior: -1.5249e+01
Epoch 3/10
10/10 - 2s - loss: 274.7313 - loglik: -2.6753e+02 - logprior: -7.1347e+00
Epoch 4/10
10/10 - 2s - loss: 247.9675 - loglik: -2.4259e+02 - logprior: -5.1580e+00
Epoch 5/10
10/10 - 2s - loss: 238.2149 - loglik: -2.3353e+02 - logprior: -4.2210e+00
Epoch 6/10
10/10 - 2s - loss: 233.8418 - loglik: -2.2993e+02 - logprior: -3.3962e+00
Epoch 7/10
10/10 - 2s - loss: 232.6054 - loglik: -2.2955e+02 - logprior: -2.7207e+00
Epoch 8/10
10/10 - 2s - loss: 231.3936 - loglik: -2.2880e+02 - logprior: -2.3288e+00
Epoch 9/10
10/10 - 2s - loss: 230.4333 - loglik: -2.2806e+02 - logprior: -2.1217e+00
Epoch 10/10
10/10 - 2s - loss: 229.5627 - loglik: -2.2741e+02 - logprior: -1.9292e+00
Fitted a model with MAP estimate = -229.5357
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (60, 1), (62, 1), (63, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 3), (88, 1), (91, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 290.8568 - loglik: -2.1950e+02 - logprior: -7.1115e+01
Epoch 2/2
10/10 - 2s - loss: 234.0756 - loglik: -2.0569e+02 - logprior: -2.8110e+01
Fitted a model with MAP estimate = -223.4608
expansions: [(0, 2)]
discards: [  0  12 108]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 255.1311 - loglik: -1.9879e+02 - logprior: -5.6063e+01
Epoch 2/2
10/10 - 2s - loss: 210.4648 - loglik: -1.9680e+02 - logprior: -1.3340e+01
Fitted a model with MAP estimate = -203.8860
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 265.1843 - loglik: -1.9794e+02 - logprior: -6.6915e+01
Epoch 2/10
10/10 - 2s - loss: 217.1132 - loglik: -1.9862e+02 - logprior: -1.8145e+01
Epoch 3/10
10/10 - 2s - loss: 203.8373 - loglik: -1.9836e+02 - logprior: -5.1254e+00
Epoch 4/10
10/10 - 2s - loss: 199.7118 - loglik: -1.9883e+02 - logprior: -5.2305e-01
Epoch 5/10
10/10 - 2s - loss: 197.4469 - loglik: -1.9879e+02 - logprior: 1.7080
Epoch 6/10
10/10 - 2s - loss: 196.0452 - loglik: -1.9859e+02 - logprior: 2.9191
Epoch 7/10
10/10 - 2s - loss: 195.2137 - loglik: -1.9854e+02 - logprior: 3.7091
Epoch 8/10
10/10 - 2s - loss: 194.4573 - loglik: -1.9842e+02 - logprior: 4.3461
Epoch 9/10
10/10 - 2s - loss: 194.5537 - loglik: -1.9907e+02 - logprior: 4.9009
Fitted a model with MAP estimate = -193.8023
Time for alignment: 59.3546
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 411.5416 - loglik: -3.4865e+02 - logprior: -6.2884e+01
Epoch 2/10
10/10 - 2s - loss: 321.9011 - loglik: -3.0664e+02 - logprior: -1.5257e+01
Epoch 3/10
10/10 - 2s - loss: 273.8590 - loglik: -2.6664e+02 - logprior: -7.1590e+00
Epoch 4/10
10/10 - 2s - loss: 248.6124 - loglik: -2.4347e+02 - logprior: -4.9721e+00
Epoch 5/10
10/10 - 2s - loss: 238.1037 - loglik: -2.3384e+02 - logprior: -3.9660e+00
Epoch 6/10
10/10 - 2s - loss: 235.0085 - loglik: -2.3138e+02 - logprior: -3.1963e+00
Epoch 7/10
10/10 - 2s - loss: 232.3923 - loglik: -2.2949e+02 - logprior: -2.5366e+00
Epoch 8/10
10/10 - 2s - loss: 231.2934 - loglik: -2.2884e+02 - logprior: -2.1663e+00
Epoch 9/10
10/10 - 2s - loss: 230.7386 - loglik: -2.2851e+02 - logprior: -1.9442e+00
Epoch 10/10
10/10 - 2s - loss: 230.2342 - loglik: -2.2819e+02 - logprior: -1.7444e+00
Fitted a model with MAP estimate = -229.7287
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 1), (71, 2), (78, 1), (79, 1), (83, 1), (87, 3), (88, 1), (91, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 290.4334 - loglik: -2.1898e+02 - logprior: -7.1170e+01
Epoch 2/2
10/10 - 2s - loss: 233.8488 - loglik: -2.0546e+02 - logprior: -2.8115e+01
Fitted a model with MAP estimate = -223.4278
expansions: [(0, 2)]
discards: [  0  12 108]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 255.0402 - loglik: -1.9869e+02 - logprior: -5.6067e+01
Epoch 2/2
10/10 - 2s - loss: 210.7301 - loglik: -1.9708e+02 - logprior: -1.3334e+01
Fitted a model with MAP estimate = -203.9311
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 265.3281 - loglik: -1.9814e+02 - logprior: -6.6865e+01
Epoch 2/10
10/10 - 2s - loss: 216.8687 - loglik: -1.9842e+02 - logprior: -1.8108e+01
Epoch 3/10
10/10 - 2s - loss: 204.1244 - loglik: -1.9870e+02 - logprior: -5.0856e+00
Epoch 4/10
10/10 - 2s - loss: 198.8500 - loglik: -1.9801e+02 - logprior: -4.8715e-01
Epoch 5/10
10/10 - 2s - loss: 198.2770 - loglik: -1.9965e+02 - logprior: 1.7347
Epoch 6/10
10/10 - 2s - loss: 195.2499 - loglik: -1.9783e+02 - logprior: 2.9452
Epoch 7/10
10/10 - 2s - loss: 195.9894 - loglik: -1.9936e+02 - logprior: 3.7354
Fitted a model with MAP estimate = -194.6901
Time for alignment: 53.4237
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 411.7870 - loglik: -3.4889e+02 - logprior: -6.2886e+01
Epoch 2/10
10/10 - 2s - loss: 321.8590 - loglik: -3.0660e+02 - logprior: -1.5252e+01
Epoch 3/10
10/10 - 2s - loss: 275.2432 - loglik: -2.6806e+02 - logprior: -7.1171e+00
Epoch 4/10
10/10 - 2s - loss: 249.9277 - loglik: -2.4483e+02 - logprior: -4.9280e+00
Epoch 5/10
10/10 - 2s - loss: 239.4378 - loglik: -2.3517e+02 - logprior: -4.0506e+00
Epoch 6/10
10/10 - 2s - loss: 235.4431 - loglik: -2.3182e+02 - logprior: -3.2382e+00
Epoch 7/10
10/10 - 2s - loss: 233.0713 - loglik: -2.3017e+02 - logprior: -2.5085e+00
Epoch 8/10
10/10 - 2s - loss: 232.1660 - loglik: -2.2968e+02 - logprior: -2.1777e+00
Epoch 9/10
10/10 - 2s - loss: 230.7385 - loglik: -2.2834e+02 - logprior: -2.1009e+00
Epoch 10/10
10/10 - 2s - loss: 229.9327 - loglik: -2.2765e+02 - logprior: -1.9529e+00
Fitted a model with MAP estimate = -229.5429
expansions: [(11, 1), (12, 2), (13, 5), (37, 2), (38, 1), (39, 2), (45, 1), (51, 1), (60, 1), (62, 1), (63, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 3), (88, 1), (91, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 291.8645 - loglik: -2.2032e+02 - logprior: -7.1224e+01
Epoch 2/2
10/10 - 2s - loss: 234.6637 - loglik: -2.0608e+02 - logprior: -2.8284e+01
Fitted a model with MAP estimate = -223.9093
expansions: [(0, 2)]
discards: [  0  12  47 109]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 255.0631 - loglik: -1.9865e+02 - logprior: -5.6112e+01
Epoch 2/2
10/10 - 2s - loss: 210.7271 - loglik: -1.9702e+02 - logprior: -1.3365e+01
Fitted a model with MAP estimate = -203.9222
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 265.4291 - loglik: -1.9813e+02 - logprior: -6.6956e+01
Epoch 2/10
10/10 - 2s - loss: 216.9257 - loglik: -1.9838e+02 - logprior: -1.8196e+01
Epoch 3/10
10/10 - 2s - loss: 204.3846 - loglik: -1.9889e+02 - logprior: -5.1370e+00
Epoch 4/10
10/10 - 2s - loss: 199.1695 - loglik: -1.9829e+02 - logprior: -5.2102e-01
Epoch 5/10
10/10 - 2s - loss: 197.2678 - loglik: -1.9861e+02 - logprior: 1.7109
Epoch 6/10
10/10 - 2s - loss: 196.4150 - loglik: -1.9897e+02 - logprior: 2.9269
Epoch 7/10
10/10 - 2s - loss: 194.9808 - loglik: -1.9831e+02 - logprior: 3.7107
Epoch 8/10
10/10 - 2s - loss: 195.0121 - loglik: -1.9898e+02 - logprior: 4.3516
Fitted a model with MAP estimate = -194.2032
Time for alignment: 57.0521
Computed alignments with likelihoods: ['-193.8023', '-194.6901', '-194.2032']
Best model has likelihood: -193.8023
time for generating output: 0.1868
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/profilin.projection.fasta
SP score = 0.9394184168012925
Training of 3 independent models on file rrm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f33a3a0d610>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34ed002e50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34ec8a0130>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345a7882b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f348d252730>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3462ed1040>, <__main__.SimpleDirichletPrior object at 0x7f342d1bd490>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3495c45280>

Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 195.5571 - loglik: -1.9329e+02 - logprior: -2.2228e+00
Epoch 2/10
22/22 - 2s - loss: 162.3820 - loglik: -1.6057e+02 - logprior: -1.3730e+00
Epoch 3/10
22/22 - 2s - loss: 154.8747 - loglik: -1.5282e+02 - logprior: -1.4791e+00
Epoch 4/10
22/22 - 2s - loss: 153.4680 - loglik: -1.5171e+02 - logprior: -1.3595e+00
Epoch 5/10
22/22 - 2s - loss: 152.7319 - loglik: -1.5103e+02 - logprior: -1.3618e+00
Epoch 6/10
22/22 - 2s - loss: 152.3409 - loglik: -1.5070e+02 - logprior: -1.3441e+00
Epoch 7/10
22/22 - 2s - loss: 152.2485 - loglik: -1.5063e+02 - logprior: -1.3329e+00
Epoch 8/10
22/22 - 2s - loss: 151.9160 - loglik: -1.5028e+02 - logprior: -1.3272e+00
Epoch 9/10
22/22 - 2s - loss: 151.7995 - loglik: -1.5017e+02 - logprior: -1.3213e+00
Epoch 10/10
22/22 - 2s - loss: 151.9340 - loglik: -1.5030e+02 - logprior: -1.3205e+00
Fitted a model with MAP estimate = -153.5367
expansions: [(8, 1), (9, 2), (11, 1), (14, 2), (20, 2), (21, 2), (22, 2), (28, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 77 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 7s - loss: 154.1958 - loglik: -1.5089e+02 - logprior: -2.9962e+00
Epoch 2/2
22/22 - 2s - loss: 145.0772 - loglik: -1.4307e+02 - logprior: -1.6655e+00
Fitted a model with MAP estimate = -148.6883
expansions: [(0, 2)]
discards: [ 0  9 17 26 29 31 68]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 144.0504 - loglik: -1.4144e+02 - logprior: -2.2003e+00
Epoch 2/2
22/22 - 2s - loss: 141.4041 - loglik: -1.3987e+02 - logprior: -1.1039e+00
Fitted a model with MAP estimate = -148.0442
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 146.4477 - loglik: -1.4454e+02 - logprior: -1.2147e+00
Epoch 2/10
32/32 - 3s - loss: 142.8045 - loglik: -1.4131e+02 - logprior: -8.9736e-01
Epoch 3/10
32/32 - 3s - loss: 141.1580 - loglik: -1.3972e+02 - logprior: -8.8386e-01
Epoch 4/10
32/32 - 3s - loss: 140.4662 - loglik: -1.3892e+02 - logprior: -8.8987e-01
Epoch 5/10
32/32 - 3s - loss: 140.0982 - loglik: -1.3846e+02 - logprior: -8.9590e-01
Epoch 6/10
32/32 - 3s - loss: 139.4775 - loglik: -1.3785e+02 - logprior: -8.9193e-01
Epoch 7/10
32/32 - 3s - loss: 139.1098 - loglik: -1.3744e+02 - logprior: -8.9427e-01
Epoch 8/10
32/32 - 3s - loss: 138.9438 - loglik: -1.3742e+02 - logprior: -8.7934e-01
Epoch 9/10
32/32 - 3s - loss: 138.7154 - loglik: -1.3721e+02 - logprior: -8.7506e-01
Epoch 10/10
32/32 - 3s - loss: 138.7413 - loglik: -1.3728e+02 - logprior: -8.7388e-01
Fitted a model with MAP estimate = -137.9107
Time for alignment: 90.8379
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 6s - loss: 195.4306 - loglik: -1.9316e+02 - logprior: -2.2228e+00
Epoch 2/10
22/22 - 2s - loss: 162.5830 - loglik: -1.6090e+02 - logprior: -1.3639e+00
Epoch 3/10
22/22 - 2s - loss: 154.8521 - loglik: -1.5280e+02 - logprior: -1.4950e+00
Epoch 4/10
22/22 - 2s - loss: 153.3145 - loglik: -1.5145e+02 - logprior: -1.4015e+00
Epoch 5/10
22/22 - 2s - loss: 152.8240 - loglik: -1.5104e+02 - logprior: -1.3942e+00
Epoch 6/10
22/22 - 2s - loss: 152.4040 - loglik: -1.5067e+02 - logprior: -1.3654e+00
Epoch 7/10
22/22 - 2s - loss: 152.0408 - loglik: -1.5033e+02 - logprior: -1.3560e+00
Epoch 8/10
22/22 - 2s - loss: 152.0248 - loglik: -1.5032e+02 - logprior: -1.3456e+00
Epoch 9/10
22/22 - 2s - loss: 151.6124 - loglik: -1.4988e+02 - logprior: -1.3440e+00
Epoch 10/10
22/22 - 2s - loss: 151.6825 - loglik: -1.4995e+02 - logprior: -1.3459e+00
Fitted a model with MAP estimate = -153.9031
expansions: [(8, 1), (9, 2), (12, 1), (13, 2), (20, 2), (21, 2), (22, 1), (25, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 153.4128 - loglik: -1.5004e+02 - logprior: -3.0033e+00
Epoch 2/2
22/22 - 2s - loss: 144.8185 - loglik: -1.4278e+02 - logprior: -1.6456e+00
Fitted a model with MAP estimate = -149.2682
expansions: [(0, 2)]
discards: [ 0  9 16 26 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 73 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 143.6826 - loglik: -1.4101e+02 - logprior: -2.2212e+00
Epoch 2/2
22/22 - 2s - loss: 141.0403 - loglik: -1.3945e+02 - logprior: -1.1317e+00
Fitted a model with MAP estimate = -148.8169
expansions: []
discards: [ 0 26]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 147.1106 - loglik: -1.4512e+02 - logprior: -1.2108e+00
Epoch 2/10
32/32 - 3s - loss: 143.0964 - loglik: -1.4147e+02 - logprior: -8.9106e-01
Epoch 3/10
32/32 - 3s - loss: 141.1257 - loglik: -1.3967e+02 - logprior: -9.0120e-01
Epoch 4/10
32/32 - 3s - loss: 140.5120 - loglik: -1.3896e+02 - logprior: -8.9475e-01
Epoch 5/10
32/32 - 3s - loss: 140.0797 - loglik: -1.3844e+02 - logprior: -8.9292e-01
Epoch 6/10
32/32 - 3s - loss: 139.3136 - loglik: -1.3766e+02 - logprior: -8.9719e-01
Epoch 7/10
32/32 - 3s - loss: 139.1867 - loglik: -1.3751e+02 - logprior: -8.9524e-01
Epoch 8/10
32/32 - 3s - loss: 139.0053 - loglik: -1.3746e+02 - logprior: -8.9227e-01
Epoch 9/10
32/32 - 3s - loss: 138.6923 - loglik: -1.3716e+02 - logprior: -8.8020e-01
Epoch 10/10
32/32 - 3s - loss: 138.6778 - loglik: -1.3720e+02 - logprior: -8.7680e-01
Fitted a model with MAP estimate = -137.8712
Time for alignment: 91.5675
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 195.3525 - loglik: -1.9309e+02 - logprior: -2.2259e+00
Epoch 2/10
22/22 - 2s - loss: 162.4865 - loglik: -1.6075e+02 - logprior: -1.3540e+00
Epoch 3/10
22/22 - 2s - loss: 155.2244 - loglik: -1.5324e+02 - logprior: -1.4790e+00
Epoch 4/10
22/22 - 2s - loss: 153.4483 - loglik: -1.5163e+02 - logprior: -1.3977e+00
Epoch 5/10
22/22 - 2s - loss: 152.9219 - loglik: -1.5118e+02 - logprior: -1.4012e+00
Epoch 6/10
22/22 - 2s - loss: 152.6681 - loglik: -1.5097e+02 - logprior: -1.3682e+00
Epoch 7/10
22/22 - 2s - loss: 152.1700 - loglik: -1.5049e+02 - logprior: -1.3570e+00
Epoch 8/10
22/22 - 2s - loss: 152.0923 - loglik: -1.5040e+02 - logprior: -1.3518e+00
Epoch 9/10
22/22 - 2s - loss: 151.9910 - loglik: -1.5029e+02 - logprior: -1.3502e+00
Epoch 10/10
22/22 - 2s - loss: 151.7189 - loglik: -1.5001e+02 - logprior: -1.3491e+00
Fitted a model with MAP estimate = -153.8102
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (17, 2), (21, 1), (22, 2), (25, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 153.8763 - loglik: -1.5055e+02 - logprior: -2.9957e+00
Epoch 2/2
22/22 - 2s - loss: 144.7054 - loglik: -1.4268e+02 - logprior: -1.6477e+00
Fitted a model with MAP estimate = -148.8608
expansions: [(0, 2)]
discards: [ 0  9 16 22 30 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 143.8517 - loglik: -1.4124e+02 - logprior: -2.1977e+00
Epoch 2/2
22/22 - 2s - loss: 141.3752 - loglik: -1.3983e+02 - logprior: -1.1044e+00
Fitted a model with MAP estimate = -148.3849
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 146.8819 - loglik: -1.4496e+02 - logprior: -1.2124e+00
Epoch 2/10
32/32 - 3s - loss: 142.6637 - loglik: -1.4115e+02 - logprior: -8.9052e-01
Epoch 3/10
32/32 - 3s - loss: 141.2746 - loglik: -1.3985e+02 - logprior: -8.9238e-01
Epoch 4/10
32/32 - 3s - loss: 140.5184 - loglik: -1.3898e+02 - logprior: -8.9384e-01
Epoch 5/10
32/32 - 3s - loss: 140.0022 - loglik: -1.3837e+02 - logprior: -8.9802e-01
Epoch 6/10
32/32 - 3s - loss: 139.3751 - loglik: -1.3776e+02 - logprior: -8.9118e-01
Epoch 7/10
32/32 - 3s - loss: 139.1930 - loglik: -1.3753e+02 - logprior: -8.9464e-01
Epoch 8/10
32/32 - 3s - loss: 138.8653 - loglik: -1.3734e+02 - logprior: -8.8371e-01
Epoch 9/10
32/32 - 3s - loss: 138.8770 - loglik: -1.3736e+02 - logprior: -8.8416e-01
Fitted a model with MAP estimate = -137.9986
Time for alignment: 86.4567
Computed alignments with likelihoods: ['-137.9107', '-137.8712', '-137.9986']
Best model has likelihood: -137.8712
time for generating output: 0.1382
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rrm.projection.fasta
SP score = 0.8421273912129493
Training of 3 independent models on file KAS.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3473e78df0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3449162d30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3437330280>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f33242ad670>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3436fca5b0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34ece20550>, <__main__.SimpleDirichletPrior object at 0x7f335fe86850>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f342d408b80>

Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 463.6185 - loglik: -4.5019e+02 - logprior: -1.3422e+01
Epoch 2/10
17/17 - 5s - loss: 319.6762 - loglik: -3.1792e+02 - logprior: -1.6879e+00
Epoch 3/10
17/17 - 5s - loss: 267.2653 - loglik: -2.6618e+02 - logprior: -1.0561e+00
Epoch 4/10
17/17 - 5s - loss: 257.4077 - loglik: -2.5631e+02 - logprior: -9.1301e-01
Epoch 5/10
17/17 - 5s - loss: 251.7698 - loglik: -2.5050e+02 - logprior: -9.0588e-01
Epoch 6/10
17/17 - 5s - loss: 251.3479 - loglik: -2.5007e+02 - logprior: -8.8707e-01
Epoch 7/10
17/17 - 5s - loss: 251.2941 - loglik: -2.4997e+02 - logprior: -9.1605e-01
Epoch 8/10
17/17 - 6s - loss: 250.7087 - loglik: -2.4937e+02 - logprior: -9.2995e-01
Epoch 9/10
17/17 - 5s - loss: 249.2148 - loglik: -2.4790e+02 - logprior: -9.1357e-01
Epoch 10/10
17/17 - 5s - loss: 249.6990 - loglik: -2.4840e+02 - logprior: -9.0831e-01
Fitted a model with MAP estimate = -249.0444
expansions: [(0, 33), (24, 2), (25, 1), (50, 1), (60, 1), (80, 1), (91, 1), (108, 1), (137, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 204 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 10s - loss: 251.9934 - loglik: -2.3357e+02 - logprior: -1.8037e+01
Epoch 2/2
17/17 - 6s - loss: 214.6830 - loglik: -2.1096e+02 - logprior: -3.3388e+00
Fitted a model with MAP estimate = -206.0653
expansions: [(0, 11), (11, 1), (12, 2), (13, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 220 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 10s - loss: 214.8577 - loglik: -1.9632e+02 - logprior: -1.8132e+01
Epoch 2/2
17/17 - 7s - loss: 189.8158 - loglik: -1.8543e+02 - logprior: -3.9552e+00
Fitted a model with MAP estimate = -187.3069
expansions: [(0, 4), (13, 1), (26, 1), (30, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 216 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 207.6533 - loglik: -1.9197e+02 - logprior: -1.5247e+01
Epoch 2/10
17/17 - 7s - loss: 190.1184 - loglik: -1.8845e+02 - logprior: -1.2358e+00
Epoch 3/10
17/17 - 7s - loss: 188.9288 - loglik: -1.8911e+02 - logprior: 0.6274
Epoch 4/10
17/17 - 7s - loss: 187.6133 - loglik: -1.8823e+02 - logprior: 1.0755
Epoch 5/10
17/17 - 6s - loss: 187.2203 - loglik: -1.8825e+02 - logprior: 1.4953
Epoch 6/10
17/17 - 7s - loss: 188.4708 - loglik: -1.8981e+02 - logprior: 1.8075
Fitted a model with MAP estimate = -186.6506
Time for alignment: 151.8979
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 464.2938 - loglik: -4.5087e+02 - logprior: -1.3419e+01
Epoch 2/10
17/17 - 5s - loss: 323.4408 - loglik: -3.2160e+02 - logprior: -1.7714e+00
Epoch 3/10
17/17 - 5s - loss: 269.7102 - loglik: -2.6832e+02 - logprior: -1.3531e+00
Epoch 4/10
17/17 - 5s - loss: 257.5306 - loglik: -2.5597e+02 - logprior: -1.2603e+00
Epoch 5/10
17/17 - 5s - loss: 257.3596 - loglik: -2.5578e+02 - logprior: -1.1667e+00
Epoch 6/10
17/17 - 5s - loss: 254.7022 - loglik: -2.5319e+02 - logprior: -1.1267e+00
Epoch 7/10
17/17 - 5s - loss: 251.1672 - loglik: -2.4958e+02 - logprior: -1.1790e+00
Epoch 8/10
17/17 - 5s - loss: 252.6056 - loglik: -2.5098e+02 - logprior: -1.2042e+00
Fitted a model with MAP estimate = -251.5693
expansions: [(0, 31), (2, 1), (8, 1), (25, 2), (26, 1), (51, 1), (60, 1), (81, 1), (83, 1), (93, 1), (117, 1), (137, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 205 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 10s - loss: 253.3885 - loglik: -2.3506e+02 - logprior: -1.7935e+01
Epoch 2/2
17/17 - 6s - loss: 213.9461 - loglik: -2.1032e+02 - logprior: -3.2365e+00
Fitted a model with MAP estimate = -207.3777
expansions: [(0, 11), (11, 1), (12, 1), (13, 2), (14, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 221 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 11s - loss: 216.9608 - loglik: -1.9850e+02 - logprior: -1.8041e+01
Epoch 2/2
17/17 - 7s - loss: 192.3362 - loglik: -1.8800e+02 - logprior: -3.8927e+00
Fitted a model with MAP estimate = -189.3713
expansions: [(0, 4), (13, 1), (14, 1), (24, 1), (25, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 40 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 216 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 211.6924 - loglik: -1.9603e+02 - logprior: -1.5222e+01
Epoch 2/10
17/17 - 7s - loss: 193.9805 - loglik: -1.9227e+02 - logprior: -1.2575e+00
Epoch 3/10
17/17 - 7s - loss: 192.7241 - loglik: -1.9282e+02 - logprior: 0.5546
Epoch 4/10
17/17 - 7s - loss: 190.9524 - loglik: -1.9149e+02 - logprior: 1.0039
Epoch 5/10
17/17 - 7s - loss: 191.5046 - loglik: -1.9245e+02 - logprior: 1.4300
Fitted a model with MAP estimate = -190.3961
Time for alignment: 135.7150
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 464.1979 - loglik: -4.5077e+02 - logprior: -1.3425e+01
Epoch 2/10
17/17 - 5s - loss: 321.9630 - loglik: -3.2016e+02 - logprior: -1.7350e+00
Epoch 3/10
17/17 - 5s - loss: 270.1023 - loglik: -2.6889e+02 - logprior: -1.1793e+00
Epoch 4/10
17/17 - 5s - loss: 258.6990 - loglik: -2.5742e+02 - logprior: -9.9308e-01
Epoch 5/10
17/17 - 5s - loss: 253.1385 - loglik: -2.5177e+02 - logprior: -9.5810e-01
Epoch 6/10
17/17 - 5s - loss: 253.8812 - loglik: -2.5261e+02 - logprior: -9.3114e-01
Fitted a model with MAP estimate = -252.6542
expansions: [(0, 33), (7, 2), (24, 2), (25, 1), (39, 1), (59, 1), (80, 1), (87, 1), (96, 1), (137, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 206 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 10s - loss: 252.2586 - loglik: -2.3455e+02 - logprior: -1.7361e+01
Epoch 2/2
17/17 - 6s - loss: 213.1084 - loglik: -2.0972e+02 - logprior: -3.0282e+00
Fitted a model with MAP estimate = -203.9650
expansions: [(0, 11), (13, 1), (14, 2), (15, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 221 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 10s - loss: 214.0420 - loglik: -1.9548e+02 - logprior: -1.8186e+01
Epoch 2/2
17/17 - 6s - loss: 189.7036 - loglik: -1.8543e+02 - logprior: -3.8597e+00
Fitted a model with MAP estimate = -186.3997
expansions: [(0, 4), (15, 1), (23, 1), (24, 1), (25, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 40 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 216 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 209.6464 - loglik: -1.9406e+02 - logprior: -1.5160e+01
Epoch 2/10
17/17 - 7s - loss: 191.1047 - loglik: -1.8941e+02 - logprior: -1.2608e+00
Epoch 3/10
17/17 - 6s - loss: 188.2913 - loglik: -1.8839e+02 - logprior: 0.5283
Epoch 4/10
17/17 - 7s - loss: 190.9729 - loglik: -1.9148e+02 - logprior: 0.9484
Fitted a model with MAP estimate = -188.2592
Time for alignment: 115.7363
Computed alignments with likelihoods: ['-186.6506', '-189.3713', '-186.3997']
Best model has likelihood: -186.3997
time for generating output: 0.3946
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/KAS.projection.fasta
SP score = 0.5045063927897715
Training of 3 independent models on file GEL.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3473b4a970>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3378cb8130>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3378cb8bb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34ee379d90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34ee3790d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3484957d00>, <__main__.SimpleDirichletPrior object at 0x7f335fcd08b0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34ecb10310>

Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 246.1017 - loglik: -2.2563e+02 - logprior: -2.0450e+01
Epoch 2/10
10/10 - 1s - loss: 216.1893 - loglik: -2.1041e+02 - logprior: -5.6069e+00
Epoch 3/10
10/10 - 1s - loss: 198.3676 - loglik: -1.9502e+02 - logprior: -3.0647e+00
Epoch 4/10
10/10 - 1s - loss: 190.0329 - loglik: -1.8718e+02 - logprior: -2.4829e+00
Epoch 5/10
10/10 - 1s - loss: 186.7797 - loglik: -1.8411e+02 - logprior: -2.3205e+00
Epoch 6/10
10/10 - 1s - loss: 185.0907 - loglik: -1.8269e+02 - logprior: -2.1673e+00
Epoch 7/10
10/10 - 1s - loss: 184.3293 - loglik: -1.8224e+02 - logprior: -1.8834e+00
Epoch 8/10
10/10 - 1s - loss: 183.9934 - loglik: -1.8202e+02 - logprior: -1.7778e+00
Epoch 9/10
10/10 - 1s - loss: 183.5083 - loglik: -1.8152e+02 - logprior: -1.7970e+00
Epoch 10/10
10/10 - 1s - loss: 183.4299 - loglik: -1.8146e+02 - logprior: -1.7695e+00
Fitted a model with MAP estimate = -183.0473
expansions: [(0, 2), (8, 1), (9, 1), (23, 1), (41, 2), (42, 2), (43, 1), (44, 1), (46, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 206.0597 - loglik: -1.7884e+02 - logprior: -2.7015e+01
Epoch 2/2
10/10 - 1s - loss: 184.5565 - loglik: -1.7594e+02 - logprior: -8.3984e+00
Fitted a model with MAP estimate = -180.6093
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 199.3135 - loglik: -1.7554e+02 - logprior: -2.3512e+01
Epoch 2/2
10/10 - 1s - loss: 185.1448 - loglik: -1.7544e+02 - logprior: -9.4048e+00
Fitted a model with MAP estimate = -182.4680
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 195.1206 - loglik: -1.7423e+02 - logprior: -2.0585e+01
Epoch 2/10
10/10 - 1s - loss: 180.2156 - loglik: -1.7421e+02 - logprior: -5.6990e+00
Epoch 3/10
10/10 - 1s - loss: 177.3932 - loglik: -1.7446e+02 - logprior: -2.6199e+00
Epoch 4/10
10/10 - 1s - loss: 176.4646 - loglik: -1.7453e+02 - logprior: -1.6057e+00
Epoch 5/10
10/10 - 1s - loss: 176.3104 - loglik: -1.7479e+02 - logprior: -1.1914e+00
Epoch 6/10
10/10 - 1s - loss: 175.6583 - loglik: -1.7429e+02 - logprior: -1.0274e+00
Epoch 7/10
10/10 - 1s - loss: 175.6099 - loglik: -1.7435e+02 - logprior: -9.0646e-01
Epoch 8/10
10/10 - 1s - loss: 175.6851 - loglik: -1.7455e+02 - logprior: -7.7100e-01
Fitted a model with MAP estimate = -175.0906
Time for alignment: 44.1271
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 246.1372 - loglik: -2.2566e+02 - logprior: -2.0449e+01
Epoch 2/10
10/10 - 1s - loss: 216.2301 - loglik: -2.1045e+02 - logprior: -5.6049e+00
Epoch 3/10
10/10 - 1s - loss: 198.7014 - loglik: -1.9537e+02 - logprior: -3.0438e+00
Epoch 4/10
10/10 - 1s - loss: 189.4499 - loglik: -1.8670e+02 - logprior: -2.4414e+00
Epoch 5/10
10/10 - 1s - loss: 186.8314 - loglik: -1.8418e+02 - logprior: -2.3023e+00
Epoch 6/10
10/10 - 1s - loss: 185.0628 - loglik: -1.8265e+02 - logprior: -2.1297e+00
Epoch 7/10
10/10 - 1s - loss: 183.9488 - loglik: -1.8189e+02 - logprior: -1.8203e+00
Epoch 8/10
10/10 - 1s - loss: 184.3133 - loglik: -1.8238e+02 - logprior: -1.6946e+00
Fitted a model with MAP estimate = -183.3836
expansions: [(0, 2), (7, 2), (8, 2), (23, 1), (41, 2), (42, 3), (43, 1), (44, 2), (46, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 80 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 206.5078 - loglik: -1.7962e+02 - logprior: -2.6687e+01
Epoch 2/2
10/10 - 1s - loss: 184.5313 - loglik: -1.7590e+02 - logprior: -8.4162e+00
Fitted a model with MAP estimate = -180.5033
expansions: []
discards: [ 0  9 10 51 55 57]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 200.0040 - loglik: -1.7621e+02 - logprior: -2.3548e+01
Epoch 2/2
10/10 - 1s - loss: 185.0885 - loglik: -1.7537e+02 - logprior: -9.4457e+00
Fitted a model with MAP estimate = -182.4894
expansions: [(0, 2), (51, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 194.9405 - loglik: -1.7414e+02 - logprior: -2.0515e+01
Epoch 2/10
10/10 - 1s - loss: 179.5871 - loglik: -1.7360e+02 - logprior: -5.6671e+00
Epoch 3/10
10/10 - 1s - loss: 176.8020 - loglik: -1.7386e+02 - logprior: -2.6280e+00
Epoch 4/10
10/10 - 1s - loss: 175.7887 - loglik: -1.7386e+02 - logprior: -1.6122e+00
Epoch 5/10
10/10 - 1s - loss: 175.4576 - loglik: -1.7392e+02 - logprior: -1.2095e+00
Epoch 6/10
10/10 - 1s - loss: 175.1605 - loglik: -1.7378e+02 - logprior: -1.0482e+00
Epoch 7/10
10/10 - 1s - loss: 174.9525 - loglik: -1.7368e+02 - logprior: -9.3174e-01
Epoch 8/10
10/10 - 1s - loss: 174.7732 - loglik: -1.7363e+02 - logprior: -7.9941e-01
Epoch 9/10
10/10 - 1s - loss: 174.8363 - loglik: -1.7382e+02 - logprior: -6.6546e-01
Fitted a model with MAP estimate = -174.2722
Time for alignment: 43.1479
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 246.2475 - loglik: -2.2577e+02 - logprior: -2.0450e+01
Epoch 2/10
10/10 - 1s - loss: 216.0910 - loglik: -2.1031e+02 - logprior: -5.6052e+00
Epoch 3/10
10/10 - 1s - loss: 198.7692 - loglik: -1.9544e+02 - logprior: -3.0535e+00
Epoch 4/10
10/10 - 1s - loss: 190.3503 - loglik: -1.8766e+02 - logprior: -2.4179e+00
Epoch 5/10
10/10 - 1s - loss: 186.9493 - loglik: -1.8437e+02 - logprior: -2.2623e+00
Epoch 6/10
10/10 - 1s - loss: 185.0939 - loglik: -1.8270e+02 - logprior: -2.1363e+00
Epoch 7/10
10/10 - 1s - loss: 184.3151 - loglik: -1.8224e+02 - logprior: -1.8453e+00
Epoch 8/10
10/10 - 1s - loss: 183.8566 - loglik: -1.8193e+02 - logprior: -1.7172e+00
Epoch 9/10
10/10 - 1s - loss: 183.4306 - loglik: -1.8150e+02 - logprior: -1.7265e+00
Epoch 10/10
10/10 - 1s - loss: 183.2725 - loglik: -1.8135e+02 - logprior: -1.7149e+00
Fitted a model with MAP estimate = -182.9742
expansions: [(0, 2), (7, 1), (8, 1), (23, 1), (41, 2), (42, 3), (44, 2), (48, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 77 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 205.9497 - loglik: -1.7871e+02 - logprior: -2.7052e+01
Epoch 2/2
10/10 - 1s - loss: 184.5362 - loglik: -1.7586e+02 - logprior: -8.4844e+00
Fitted a model with MAP estimate = -180.3142
expansions: []
discards: [ 0 49]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 199.0613 - loglik: -1.7528e+02 - logprior: -2.3546e+01
Epoch 2/2
10/10 - 1s - loss: 184.5443 - loglik: -1.7484e+02 - logprior: -9.4417e+00
Fitted a model with MAP estimate = -182.0398
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 194.5143 - loglik: -1.7363e+02 - logprior: -2.0611e+01
Epoch 2/10
10/10 - 1s - loss: 179.7803 - loglik: -1.7377e+02 - logprior: -5.7215e+00
Epoch 3/10
10/10 - 1s - loss: 177.0060 - loglik: -1.7406e+02 - logprior: -2.6441e+00
Epoch 4/10
10/10 - 1s - loss: 176.0052 - loglik: -1.7407e+02 - logprior: -1.6194e+00
Epoch 5/10
10/10 - 1s - loss: 175.7369 - loglik: -1.7420e+02 - logprior: -1.2140e+00
Epoch 6/10
10/10 - 1s - loss: 175.2704 - loglik: -1.7388e+02 - logprior: -1.0537e+00
Epoch 7/10
10/10 - 1s - loss: 175.1755 - loglik: -1.7389e+02 - logprior: -9.3733e-01
Epoch 8/10
10/10 - 1s - loss: 175.1229 - loglik: -1.7397e+02 - logprior: -7.8808e-01
Epoch 9/10
10/10 - 1s - loss: 174.6693 - loglik: -1.7364e+02 - logprior: -6.4525e-01
Epoch 10/10
10/10 - 1s - loss: 174.5483 - loglik: -1.7359e+02 - logprior: -5.7204e-01
Fitted a model with MAP estimate = -174.2291
Time for alignment: 45.5752
Computed alignments with likelihoods: ['-175.0906', '-174.2722', '-174.2291']
Best model has likelihood: -174.2291
time for generating output: 0.1656
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/GEL.projection.fasta
SP score = 0.690547263681592
Training of 3 independent models on file hr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34ee48e400>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f337974af70>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34ecb7e3d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3356b3f850>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3356b3f550>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3495c2a6d0>, <__main__.SimpleDirichletPrior object at 0x7f338a47e2b0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34ecb10310>

Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 199.3731 - loglik: -1.8777e+02 - logprior: -1.1585e+01
Epoch 2/10
11/11 - 1s - loss: 158.0922 - loglik: -1.5478e+02 - logprior: -3.3038e+00
Epoch 3/10
11/11 - 1s - loss: 125.9385 - loglik: -1.2344e+02 - logprior: -2.4495e+00
Epoch 4/10
11/11 - 1s - loss: 110.3341 - loglik: -1.0793e+02 - logprior: -2.3351e+00
Epoch 5/10
11/11 - 1s - loss: 104.5189 - loglik: -1.0227e+02 - logprior: -2.0707e+00
Epoch 6/10
11/11 - 1s - loss: 103.0949 - loglik: -1.0071e+02 - logprior: -2.0663e+00
Epoch 7/10
11/11 - 1s - loss: 101.9246 - loglik: -9.9552e+01 - logprior: -2.0562e+00
Epoch 8/10
11/11 - 1s - loss: 101.7657 - loglik: -9.9464e+01 - logprior: -1.9930e+00
Epoch 9/10
11/11 - 1s - loss: 101.1123 - loglik: -9.8787e+01 - logprior: -1.9997e+00
Epoch 10/10
11/11 - 1s - loss: 101.1705 - loglik: -9.8864e+01 - logprior: -1.9917e+00
Fitted a model with MAP estimate = -100.7083
expansions: [(0, 3), (15, 2), (28, 1), (29, 3), (30, 2), (31, 1), (32, 2), (33, 1), (34, 1), (37, 1), (51, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 109.4780 - loglik: -9.5206e+01 - logprior: -1.3943e+01
Epoch 2/2
11/11 - 1s - loss: 93.0035 - loglik: -8.8174e+01 - logprior: -4.4799e+00
Fitted a model with MAP estimate = -90.2120
expansions: []
discards: [ 0 35 39 44]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 102.4165 - loglik: -8.8700e+01 - logprior: -1.3354e+01
Epoch 2/2
11/11 - 1s - loss: 93.7270 - loglik: -8.7715e+01 - logprior: -5.6221e+00
Fitted a model with MAP estimate = -91.3894
expansions: [(0, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 98.3230 - loglik: -8.6900e+01 - logprior: -1.1025e+01
Epoch 2/10
11/11 - 1s - loss: 90.4456 - loglik: -8.6881e+01 - logprior: -3.1680e+00
Epoch 3/10
11/11 - 1s - loss: 89.1077 - loglik: -8.6699e+01 - logprior: -1.9986e+00
Epoch 4/10
11/11 - 1s - loss: 88.7369 - loglik: -8.6533e+01 - logprior: -1.7971e+00
Epoch 5/10
11/11 - 1s - loss: 88.6551 - loglik: -8.6534e+01 - logprior: -1.7071e+00
Epoch 6/10
11/11 - 1s - loss: 88.3524 - loglik: -8.6396e+01 - logprior: -1.5425e+00
Epoch 7/10
11/11 - 1s - loss: 88.3836 - loglik: -8.6535e+01 - logprior: -1.4337e+00
Fitted a model with MAP estimate = -87.9609
Time for alignment: 39.7925
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 199.0402 - loglik: -1.8744e+02 - logprior: -1.1585e+01
Epoch 2/10
11/11 - 1s - loss: 157.6852 - loglik: -1.5437e+02 - logprior: -3.3051e+00
Epoch 3/10
11/11 - 1s - loss: 123.0902 - loglik: -1.2057e+02 - logprior: -2.4705e+00
Epoch 4/10
11/11 - 1s - loss: 108.4340 - loglik: -1.0603e+02 - logprior: -2.3543e+00
Epoch 5/10
11/11 - 1s - loss: 104.4485 - loglik: -1.0232e+02 - logprior: -2.0896e+00
Epoch 6/10
11/11 - 1s - loss: 102.9972 - loglik: -1.0069e+02 - logprior: -2.1003e+00
Epoch 7/10
11/11 - 1s - loss: 101.9055 - loglik: -9.9437e+01 - logprior: -2.1054e+00
Epoch 8/10
11/11 - 1s - loss: 101.6664 - loglik: -9.9221e+01 - logprior: -2.0696e+00
Epoch 9/10
11/11 - 1s - loss: 100.7330 - loglik: -9.8282e+01 - logprior: -2.0892e+00
Epoch 10/10
11/11 - 1s - loss: 101.0110 - loglik: -9.8560e+01 - logprior: -2.0808e+00
Fitted a model with MAP estimate = -100.3563
expansions: [(0, 3), (15, 1), (26, 1), (28, 1), (29, 3), (30, 2), (31, 1), (32, 1), (33, 1), (34, 1), (37, 1), (43, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 73 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 109.5006 - loglik: -9.5236e+01 - logprior: -1.3894e+01
Epoch 2/2
11/11 - 1s - loss: 93.4044 - loglik: -8.8613e+01 - logprior: -4.3971e+00
Fitted a model with MAP estimate = -90.4202
expansions: []
discards: [35 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 97.6154 - loglik: -8.6443e+01 - logprior: -1.0788e+01
Epoch 2/2
11/11 - 1s - loss: 89.9744 - loglik: -8.6398e+01 - logprior: -3.1998e+00
Fitted a model with MAP estimate = -88.4711
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 101.4773 - loglik: -8.7825e+01 - logprior: -1.3273e+01
Epoch 2/10
11/11 - 1s - loss: 93.7871 - loglik: -8.7667e+01 - logprior: -5.7167e+00
Epoch 3/10
11/11 - 1s - loss: 91.3334 - loglik: -8.7275e+01 - logprior: -3.6601e+00
Epoch 4/10
11/11 - 1s - loss: 89.0107 - loglik: -8.6604e+01 - logprior: -2.0020e+00
Epoch 5/10
11/11 - 1s - loss: 89.1324 - loglik: -8.7137e+01 - logprior: -1.5806e+00
Fitted a model with MAP estimate = -88.4225
Time for alignment: 38.5364
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 199.1401 - loglik: -1.8753e+02 - logprior: -1.1587e+01
Epoch 2/10
11/11 - 1s - loss: 158.6282 - loglik: -1.5530e+02 - logprior: -3.3186e+00
Epoch 3/10
11/11 - 1s - loss: 127.2718 - loglik: -1.2474e+02 - logprior: -2.4907e+00
Epoch 4/10
11/11 - 1s - loss: 111.2512 - loglik: -1.0884e+02 - logprior: -2.3571e+00
Epoch 5/10
11/11 - 1s - loss: 105.9941 - loglik: -1.0375e+02 - logprior: -2.0702e+00
Epoch 6/10
11/11 - 1s - loss: 104.1068 - loglik: -1.0173e+02 - logprior: -2.0505e+00
Epoch 7/10
11/11 - 1s - loss: 103.3671 - loglik: -1.0103e+02 - logprior: -2.0075e+00
Epoch 8/10
11/11 - 1s - loss: 102.6586 - loglik: -1.0041e+02 - logprior: -1.9414e+00
Epoch 9/10
11/11 - 1s - loss: 102.4973 - loglik: -1.0023e+02 - logprior: -1.9483e+00
Epoch 10/10
11/11 - 1s - loss: 102.1572 - loglik: -9.9910e+01 - logprior: -1.9315e+00
Fitted a model with MAP estimate = -101.8666
expansions: [(0, 3), (15, 2), (28, 1), (29, 3), (30, 2), (31, 1), (32, 2), (33, 1), (38, 2), (51, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 109.9139 - loglik: -9.5632e+01 - logprior: -1.3955e+01
Epoch 2/2
11/11 - 1s - loss: 93.8883 - loglik: -8.9018e+01 - logprior: -4.5107e+00
Fitted a model with MAP estimate = -90.9375
expansions: []
discards: [ 0 35 39 44]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 103.4949 - loglik: -8.9750e+01 - logprior: -1.3382e+01
Epoch 2/2
11/11 - 1s - loss: 94.2002 - loglik: -8.8149e+01 - logprior: -5.6568e+00
Fitted a model with MAP estimate = -92.1404
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 100.8885 - loglik: -8.8403e+01 - logprior: -1.2095e+01
Epoch 2/10
11/11 - 1s - loss: 92.2935 - loglik: -8.8206e+01 - logprior: -3.6744e+00
Epoch 3/10
11/11 - 1s - loss: 90.7055 - loglik: -8.7876e+01 - logprior: -2.4193e+00
Epoch 4/10
11/11 - 1s - loss: 90.3560 - loglik: -8.8139e+01 - logprior: -1.7974e+00
Epoch 5/10
11/11 - 1s - loss: 90.2394 - loglik: -8.8301e+01 - logprior: -1.5242e+00
Epoch 6/10
11/11 - 1s - loss: 89.7714 - loglik: -8.7887e+01 - logprior: -1.4520e+00
Epoch 7/10
11/11 - 1s - loss: 89.4732 - loglik: -8.7693e+01 - logprior: -1.3614e+00
Epoch 8/10
11/11 - 1s - loss: 89.1395 - loglik: -8.7363e+01 - logprior: -1.3568e+00
Epoch 9/10
11/11 - 1s - loss: 89.0992 - loglik: -8.7362e+01 - logprior: -1.3154e+00
Epoch 10/10
11/11 - 1s - loss: 89.1132 - loglik: -8.7410e+01 - logprior: -1.2865e+00
Fitted a model with MAP estimate = -88.5967
Time for alignment: 40.9183
Computed alignments with likelihoods: ['-87.9609', '-88.4225', '-88.5967']
Best model has likelihood: -87.9609
time for generating output: 0.1280
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hr.projection.fasta
SP score = 0.9957142857142857
Training of 3 independent models on file rvp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3495f193d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f332c9cc640>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f337910c580>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34ecc089a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f332cbc4b80>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f332cd576a0>, <__main__.SimpleDirichletPrior object at 0x7f3392d66ac0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f333de08280>

Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 10s - loss: 171.4837 - loglik: -1.7054e+02 - logprior: -7.3069e-01
Epoch 2/10
42/42 - 4s - loss: 81.5720 - loglik: -8.0658e+01 - logprior: -6.4051e-01
Epoch 3/10
42/42 - 5s - loss: 78.5966 - loglik: -7.7737e+01 - logprior: -6.4823e-01
Epoch 4/10
42/42 - 4s - loss: 78.3292 - loglik: -7.7511e+01 - logprior: -6.2810e-01
Epoch 5/10
42/42 - 5s - loss: 78.0802 - loglik: -7.7269e+01 - logprior: -6.1849e-01
Epoch 6/10
42/42 - 5s - loss: 77.8654 - loglik: -7.7060e+01 - logprior: -6.0744e-01
Epoch 7/10
42/42 - 4s - loss: 77.4463 - loglik: -7.6640e+01 - logprior: -6.0840e-01
Epoch 8/10
42/42 - 4s - loss: 77.4647 - loglik: -7.6659e+01 - logprior: -6.0805e-01
Fitted a model with MAP estimate = -76.6761
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 45.5570 - loglik: -4.4527e+01 - logprior: -8.4832e-01
Epoch 2/2
42/42 - 5s - loss: 32.7426 - loglik: -3.1920e+01 - logprior: -6.3775e-01
Fitted a model with MAP estimate = -32.4319
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 9s - loss: 35.8519 - loglik: -3.4715e+01 - logprior: -9.6422e-01
Epoch 2/2
42/42 - 5s - loss: 33.1176 - loglik: -3.2147e+01 - logprior: -8.2373e-01
Fitted a model with MAP estimate = -32.6951
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 10s - loss: 32.9626 - loglik: -3.2139e+01 - logprior: -6.6737e-01
Epoch 2/10
59/59 - 7s - loss: 31.9806 - loglik: -3.1235e+01 - logprior: -5.7415e-01
Epoch 3/10
59/59 - 7s - loss: 31.4683 - loglik: -3.0728e+01 - logprior: -5.6270e-01
Epoch 4/10
59/59 - 7s - loss: 32.0515 - loglik: -3.1343e+01 - logprior: -5.5298e-01
Fitted a model with MAP estimate = -31.2169
Time for alignment: 194.4422
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 171.6520 - loglik: -1.7071e+02 - logprior: -7.3251e-01
Epoch 2/10
42/42 - 5s - loss: 81.5426 - loglik: -8.0626e+01 - logprior: -6.3776e-01
Epoch 3/10
42/42 - 4s - loss: 79.0606 - loglik: -7.8173e+01 - logprior: -6.3241e-01
Epoch 4/10
42/42 - 4s - loss: 78.1719 - loglik: -7.7330e+01 - logprior: -6.2537e-01
Epoch 5/10
42/42 - 4s - loss: 77.8949 - loglik: -7.7108e+01 - logprior: -6.1977e-01
Epoch 6/10
42/42 - 4s - loss: 77.1521 - loglik: -7.6368e+01 - logprior: -6.1570e-01
Epoch 7/10
42/42 - 4s - loss: 77.6863 - loglik: -7.6915e+01 - logprior: -6.0285e-01
Fitted a model with MAP estimate = -76.5500
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (33, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 9s - loss: 45.6775 - loglik: -4.4661e+01 - logprior: -8.4662e-01
Epoch 2/2
42/42 - 5s - loss: 32.5683 - loglik: -3.1759e+01 - logprior: -6.3720e-01
Fitted a model with MAP estimate = -32.4233
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 35.9610 - loglik: -3.4829e+01 - logprior: -9.7069e-01
Epoch 2/2
42/42 - 5s - loss: 33.2386 - loglik: -3.2453e+01 - logprior: -6.3691e-01
Fitted a model with MAP estimate = -32.7810
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 10s - loss: 32.6350 - loglik: -3.1827e+01 - logprior: -6.5871e-01
Epoch 2/10
59/59 - 7s - loss: 31.7997 - loglik: -3.1050e+01 - logprior: -5.7332e-01
Epoch 3/10
59/59 - 7s - loss: 32.1080 - loglik: -3.1369e+01 - logprior: -5.6212e-01
Fitted a model with MAP estimate = -31.3750
Time for alignment: 180.5937
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 170.8985 - loglik: -1.6995e+02 - logprior: -7.4089e-01
Epoch 2/10
42/42 - 5s - loss: 81.3886 - loglik: -8.0431e+01 - logprior: -6.5145e-01
Epoch 3/10
42/42 - 4s - loss: 78.8710 - loglik: -7.8027e+01 - logprior: -6.4628e-01
Epoch 4/10
42/42 - 4s - loss: 78.4714 - loglik: -7.7648e+01 - logprior: -6.2509e-01
Epoch 5/10
42/42 - 5s - loss: 77.6810 - loglik: -7.6857e+01 - logprior: -6.2295e-01
Epoch 6/10
42/42 - 5s - loss: 77.7310 - loglik: -7.6926e+01 - logprior: -6.0965e-01
Fitted a model with MAP estimate = -76.8236
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 45.6662 - loglik: -4.4642e+01 - logprior: -8.4195e-01
Epoch 2/2
42/42 - 5s - loss: 32.8078 - loglik: -3.1978e+01 - logprior: -6.3992e-01
Fitted a model with MAP estimate = -32.4438
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 11s - loss: 31.9026 - loglik: -3.1246e+01 - logprior: -4.7573e-01
Epoch 2/10
59/59 - 7s - loss: 31.3927 - loglik: -3.0797e+01 - logprior: -4.1107e-01
Epoch 3/10
59/59 - 7s - loss: 31.8216 - loglik: -3.1232e+01 - logprior: -3.9826e-01
Fitted a model with MAP estimate = -31.0245
Time for alignment: 135.3392
Computed alignments with likelihoods: ['-31.2169', '-31.3750', '-31.0245']
Best model has likelihood: -31.0245
time for generating output: 0.1735
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rvp.projection.fasta
SP score = 0.32949932341001353
Training of 3 independent models on file mmp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f343fffc040>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f33a385a7f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33569f4f70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f344049a1f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32f90370d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34af7f8d90>, <__main__.SimpleDirichletPrior object at 0x7f33a3bc3a00>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f333de08280>

Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 473.7280 - loglik: -4.4981e+02 - logprior: -2.3894e+01
Epoch 2/10
14/14 - 4s - loss: 411.7224 - loglik: -4.0765e+02 - logprior: -4.0278e+00
Epoch 3/10
14/14 - 4s - loss: 372.0320 - loglik: -3.6971e+02 - logprior: -2.1949e+00
Epoch 4/10
14/14 - 4s - loss: 358.8819 - loglik: -3.5621e+02 - logprior: -2.2400e+00
Epoch 5/10
14/14 - 4s - loss: 355.6499 - loglik: -3.5281e+02 - logprior: -2.1547e+00
Epoch 6/10
14/14 - 4s - loss: 351.9130 - loglik: -3.4924e+02 - logprior: -2.0529e+00
Epoch 7/10
14/14 - 4s - loss: 353.1321 - loglik: -3.5057e+02 - logprior: -1.9771e+00
Fitted a model with MAP estimate = -351.0016
expansions: [(10, 1), (11, 1), (16, 4), (18, 1), (35, 1), (36, 3), (42, 1), (43, 1), (44, 2), (45, 1), (66, 1), (67, 2), (76, 1), (78, 2), (79, 4), (100, 1), (102, 2), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 159 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 374.0341 - loglik: -3.4461e+02 - logprior: -2.8846e+01
Epoch 2/2
14/14 - 5s - loss: 346.5240 - loglik: -3.3523e+02 - logprior: -1.0684e+01
Fitted a model with MAP estimate = -343.0798
expansions: [(0, 8), (19, 1)]
discards: [  0  44  98 101 128]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 163 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 356.5666 - loglik: -3.3403e+02 - logprior: -2.1923e+01
Epoch 2/2
14/14 - 5s - loss: 337.5275 - loglik: -3.3315e+02 - logprior: -3.7624e+00
Fitted a model with MAP estimate = -334.3249
expansions: []
discards: [1 2 3 4 5 6 7]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 156 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 353.6839 - loglik: -3.3177e+02 - logprior: -2.1300e+01
Epoch 2/10
14/14 - 4s - loss: 336.6290 - loglik: -3.3274e+02 - logprior: -3.2610e+00
Epoch 3/10
14/14 - 4s - loss: 334.7679 - loglik: -3.3386e+02 - logprior: -2.8391e-01
Epoch 4/10
14/14 - 5s - loss: 332.0788 - loglik: -3.3231e+02 - logprior: 0.8781
Epoch 5/10
14/14 - 4s - loss: 331.6363 - loglik: -3.3233e+02 - logprior: 1.3334
Epoch 6/10
14/14 - 4s - loss: 331.6526 - loglik: -3.3267e+02 - logprior: 1.6739
Fitted a model with MAP estimate = -330.5591
Time for alignment: 99.6694
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 475.0623 - loglik: -4.5114e+02 - logprior: -2.3893e+01
Epoch 2/10
14/14 - 4s - loss: 409.5941 - loglik: -4.0551e+02 - logprior: -4.0466e+00
Epoch 3/10
14/14 - 4s - loss: 370.6240 - loglik: -3.6819e+02 - logprior: -2.2977e+00
Epoch 4/10
14/14 - 4s - loss: 358.3884 - loglik: -3.5565e+02 - logprior: -2.2719e+00
Epoch 5/10
14/14 - 4s - loss: 355.4558 - loglik: -3.5267e+02 - logprior: -2.0954e+00
Epoch 6/10
14/14 - 4s - loss: 351.1857 - loglik: -3.4862e+02 - logprior: -1.9339e+00
Epoch 7/10
14/14 - 4s - loss: 351.5622 - loglik: -3.4910e+02 - logprior: -1.8537e+00
Fitted a model with MAP estimate = -350.2602
expansions: [(10, 1), (11, 1), (16, 5), (17, 1), (36, 3), (38, 1), (41, 1), (44, 1), (45, 1), (66, 1), (67, 1), (74, 1), (79, 6), (100, 1), (102, 2), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 157 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 374.2635 - loglik: -3.4477e+02 - logprior: -2.8894e+01
Epoch 2/2
14/14 - 5s - loss: 346.3649 - loglik: -3.3500e+02 - logprior: -1.0752e+01
Fitted a model with MAP estimate = -343.5691
expansions: [(0, 8), (16, 1), (57, 1)]
discards: [  0  44  98 126]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 163 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 358.0459 - loglik: -3.3559e+02 - logprior: -2.1866e+01
Epoch 2/2
14/14 - 5s - loss: 336.2055 - loglik: -3.3188e+02 - logprior: -3.7267e+00
Fitted a model with MAP estimate = -334.2214
expansions: []
discards: [1 2 3 4 5 6 7]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 156 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 354.3728 - loglik: -3.3252e+02 - logprior: -2.1258e+01
Epoch 2/10
14/14 - 4s - loss: 335.7676 - loglik: -3.3197e+02 - logprior: -3.2067e+00
Epoch 3/10
14/14 - 5s - loss: 332.5976 - loglik: -3.3177e+02 - logprior: -2.3603e-01
Epoch 4/10
14/14 - 5s - loss: 333.2774 - loglik: -3.3362e+02 - logprior: 0.9423
Fitted a model with MAP estimate = -331.3338
Time for alignment: 90.9498
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 475.0685 - loglik: -4.5114e+02 - logprior: -2.3901e+01
Epoch 2/10
14/14 - 4s - loss: 410.8518 - loglik: -4.0674e+02 - logprior: -4.0698e+00
Epoch 3/10
14/14 - 4s - loss: 371.9545 - loglik: -3.6939e+02 - logprior: -2.3545e+00
Epoch 4/10
14/14 - 4s - loss: 358.4234 - loglik: -3.5538e+02 - logprior: -2.3673e+00
Epoch 5/10
14/14 - 4s - loss: 354.5196 - loglik: -3.5146e+02 - logprior: -2.3188e+00
Epoch 6/10
14/14 - 4s - loss: 352.3598 - loglik: -3.4947e+02 - logprior: -2.2690e+00
Epoch 7/10
14/14 - 4s - loss: 350.5005 - loglik: -3.4759e+02 - logprior: -2.3115e+00
Epoch 8/10
14/14 - 4s - loss: 349.8820 - loglik: -3.4697e+02 - logprior: -2.3161e+00
Epoch 9/10
14/14 - 4s - loss: 349.0610 - loglik: -3.4619e+02 - logprior: -2.2750e+00
Epoch 10/10
14/14 - 4s - loss: 349.3684 - loglik: -3.4653e+02 - logprior: -2.2482e+00
Fitted a model with MAP estimate = -348.1277
expansions: [(5, 1), (11, 1), (16, 4), (18, 1), (35, 1), (36, 3), (38, 2), (41, 1), (42, 1), (44, 1), (57, 1), (65, 1), (66, 1), (76, 1), (78, 2), (79, 4), (100, 1), (102, 1), (107, 1), (110, 3), (111, 1), (113, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 157 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 370.5526 - loglik: -3.4103e+02 - logprior: -2.8932e+01
Epoch 2/2
14/14 - 4s - loss: 347.5964 - loglik: -3.3626e+02 - logprior: -1.0719e+01
Fitted a model with MAP estimate = -343.4126
expansions: [(0, 7), (60, 1)]
discards: [ 0 44 48 98]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 161 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 357.4890 - loglik: -3.3501e+02 - logprior: -2.1853e+01
Epoch 2/2
14/14 - 5s - loss: 337.0285 - loglik: -3.3264e+02 - logprior: -3.7534e+00
Fitted a model with MAP estimate = -334.4555
expansions: [(22, 1)]
discards: [1 2 3 4 5 6]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 156 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 354.0046 - loglik: -3.3211e+02 - logprior: -2.1283e+01
Epoch 2/10
14/14 - 4s - loss: 335.9649 - loglik: -3.3210e+02 - logprior: -3.2270e+00
Epoch 3/10
14/14 - 4s - loss: 333.8874 - loglik: -3.3298e+02 - logprior: -2.5846e-01
Epoch 4/10
14/14 - 4s - loss: 332.0683 - loglik: -3.3234e+02 - logprior: 0.9138
Epoch 5/10
14/14 - 4s - loss: 331.9733 - loglik: -3.3268e+02 - logprior: 1.3638
Epoch 6/10
14/14 - 4s - loss: 331.4877 - loglik: -3.3249e+02 - logprior: 1.6761
Epoch 7/10
14/14 - 4s - loss: 330.4359 - loglik: -3.3187e+02 - logprior: 2.1151
Epoch 8/10
14/14 - 5s - loss: 331.0016 - loglik: -3.3269e+02 - logprior: 2.3911
Fitted a model with MAP estimate = -329.5925
Time for alignment: 118.1166
Computed alignments with likelihoods: ['-330.5591', '-331.3338', '-329.5925']
Best model has likelihood: -329.5925
time for generating output: 0.2207
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mmp.projection.fasta
SP score = 0.9794031105506515
Training of 3 independent models on file icd.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f342d2c1f40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f337947cd60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f333df01700>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f333df01be0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f343771eb20>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f343fdc2eb0>, <__main__.SimpleDirichletPrior object at 0x7f32ef26e400>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3186410d30>

Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 27s - loss: 857.9186 - loglik: -8.5478e+02 - logprior: -3.0132e+00
Epoch 2/10
29/29 - 22s - loss: 675.7629 - loglik: -6.7335e+02 - logprior: -2.0860e+00
Epoch 3/10
29/29 - 21s - loss: 648.6331 - loglik: -6.4551e+02 - logprior: -2.4753e+00
Epoch 4/10
29/29 - 21s - loss: 644.7221 - loglik: -6.4167e+02 - logprior: -2.4529e+00
Epoch 5/10
29/29 - 22s - loss: 641.8195 - loglik: -6.3877e+02 - logprior: -2.4893e+00
Epoch 6/10
29/29 - 21s - loss: 640.4878 - loglik: -6.3739e+02 - logprior: -2.5281e+00
Epoch 7/10
29/29 - 22s - loss: 640.2957 - loglik: -6.3715e+02 - logprior: -2.5754e+00
Epoch 8/10
29/29 - 22s - loss: 636.9867 - loglik: -6.3379e+02 - logprior: -2.6174e+00
Epoch 9/10
29/29 - 21s - loss: 639.8209 - loglik: -6.3660e+02 - logprior: -2.6454e+00
Fitted a model with MAP estimate = -637.3148
expansions: [(16, 1), (25, 1), (29, 1), (30, 1), (31, 1), (37, 1), (39, 1), (49, 2), (50, 5), (64, 1), (87, 1), (89, 1), (90, 2), (120, 2), (121, 1), (123, 1), (124, 1), (125, 1), (127, 1), (144, 1), (151, 1), (152, 1), (154, 1), (155, 1), (162, 1), (173, 1), (174, 1), (181, 1), (184, 1), (185, 1), (190, 1), (191, 1), (204, 1), (217, 2), (218, 2), (219, 1), (232, 1), (248, 1), (249, 1), (251, 1), (258, 1), (260, 2), (261, 1), (262, 1), (263, 2), (264, 1), (267, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 337 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 32s - loss: 618.6050 - loglik: -6.1248e+02 - logprior: -5.6287e+00
Epoch 2/2
29/29 - 28s - loss: 597.1822 - loglik: -5.9393e+02 - logprior: -2.7007e+00
Fitted a model with MAP estimate = -594.2785
expansions: [(0, 2), (26, 1), (36, 1)]
discards: [  0  59  60 259 324]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 33s - loss: 598.6824 - loglik: -5.9458e+02 - logprior: -3.5340e+00
Epoch 2/2
29/29 - 28s - loss: 590.9752 - loglik: -5.8954e+02 - logprior: -8.5821e-01
Fitted a model with MAP estimate = -590.4532
expansions: [(88, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 32s - loss: 597.4247 - loglik: -5.9188e+02 - logprior: -4.9516e+00
Epoch 2/10
29/29 - 28s - loss: 590.7793 - loglik: -5.8904e+02 - logprior: -1.1242e+00
Epoch 3/10
29/29 - 28s - loss: 589.1176 - loglik: -5.8820e+02 - logprior: -3.1204e-01
Epoch 4/10
29/29 - 28s - loss: 589.2739 - loglik: -5.8862e+02 - logprior: -4.6811e-02
Fitted a model with MAP estimate = -588.0269
Time for alignment: 542.6755
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 27s - loss: 858.3559 - loglik: -8.5521e+02 - logprior: -3.0095e+00
Epoch 2/10
29/29 - 21s - loss: 676.1763 - loglik: -6.7392e+02 - logprior: -1.9813e+00
Epoch 3/10
29/29 - 22s - loss: 638.6666 - loglik: -6.3551e+02 - logprior: -2.4689e+00
Epoch 4/10
29/29 - 21s - loss: 630.9844 - loglik: -6.2775e+02 - logprior: -2.5655e+00
Epoch 5/10
29/29 - 22s - loss: 628.7515 - loglik: -6.2561e+02 - logprior: -2.5408e+00
Epoch 6/10
29/29 - 22s - loss: 627.5881 - loglik: -6.2446e+02 - logprior: -2.5405e+00
Epoch 7/10
29/29 - 22s - loss: 627.1484 - loglik: -6.2405e+02 - logprior: -2.5530e+00
Epoch 8/10
29/29 - 21s - loss: 626.4650 - loglik: -6.2340e+02 - logprior: -2.5534e+00
Epoch 9/10
29/29 - 22s - loss: 624.1085 - loglik: -6.2104e+02 - logprior: -2.5644e+00
Epoch 10/10
29/29 - 22s - loss: 626.0664 - loglik: -6.2300e+02 - logprior: -2.5658e+00
Fitted a model with MAP estimate = -624.5428
expansions: [(16, 1), (22, 1), (24, 1), (28, 1), (29, 1), (30, 1), (31, 2), (42, 1), (49, 1), (66, 1), (73, 1), (77, 1), (88, 1), (89, 1), (95, 2), (120, 2), (121, 2), (124, 1), (125, 2), (128, 1), (141, 1), (152, 1), (153, 1), (155, 1), (156, 1), (163, 1), (174, 1), (185, 1), (186, 1), (187, 2), (193, 1), (205, 1), (206, 1), (216, 1), (218, 2), (219, 3), (245, 1), (250, 1), (251, 3), (258, 1), (260, 1), (261, 1), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 340 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 32s - loss: 605.0168 - loglik: -5.9878e+02 - logprior: -5.7667e+00
Epoch 2/2
29/29 - 29s - loss: 584.5010 - loglik: -5.8092e+02 - logprior: -3.0198e+00
Fitted a model with MAP estimate = -580.1258
expansions: [(0, 2)]
discards: [  0 136 139 146 165 258 297 323 325]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 333 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 33s - loss: 585.8953 - loglik: -5.8155e+02 - logprior: -3.7730e+00
Epoch 2/2
29/29 - 28s - loss: 581.9650 - loglik: -5.8030e+02 - logprior: -1.1013e+00
Fitted a model with MAP estimate = -579.4220
expansions: [(319, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 334 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 31s - loss: 585.7166 - loglik: -5.8007e+02 - logprior: -5.1013e+00
Epoch 2/10
29/29 - 28s - loss: 582.5929 - loglik: -5.8093e+02 - logprior: -1.1050e+00
Epoch 3/10
29/29 - 28s - loss: 579.1012 - loglik: -5.7813e+02 - logprior: -4.1669e-01
Epoch 4/10
29/29 - 28s - loss: 578.9683 - loglik: -5.7848e+02 - logprior: 0.0721
Epoch 5/10
29/29 - 28s - loss: 578.3150 - loglik: -5.7775e+02 - logprior: -2.4981e-03
Epoch 6/10
29/29 - 28s - loss: 579.1959 - loglik: -5.7915e+02 - logprior: 0.5195
Fitted a model with MAP estimate = -577.4652
Time for alignment: 619.4194
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 25s - loss: 855.9983 - loglik: -8.5288e+02 - logprior: -3.0000e+00
Epoch 2/10
29/29 - 22s - loss: 671.2794 - loglik: -6.6887e+02 - logprior: -2.0942e+00
Epoch 3/10
29/29 - 22s - loss: 641.1166 - loglik: -6.3797e+02 - logprior: -2.4928e+00
Epoch 4/10
29/29 - 22s - loss: 633.6881 - loglik: -6.3048e+02 - logprior: -2.5835e+00
Epoch 5/10
29/29 - 22s - loss: 632.0153 - loglik: -6.2885e+02 - logprior: -2.5673e+00
Epoch 6/10
29/29 - 22s - loss: 629.7372 - loglik: -6.2659e+02 - logprior: -2.6011e+00
Epoch 7/10
29/29 - 22s - loss: 628.2616 - loglik: -6.2511e+02 - logprior: -2.6181e+00
Epoch 8/10
29/29 - 22s - loss: 629.9738 - loglik: -6.2679e+02 - logprior: -2.6466e+00
Fitted a model with MAP estimate = -628.1495
expansions: [(16, 1), (22, 1), (24, 2), (30, 1), (31, 1), (36, 1), (38, 1), (48, 3), (61, 1), (72, 1), (87, 1), (88, 1), (89, 2), (117, 1), (118, 2), (119, 1), (120, 2), (123, 1), (124, 2), (142, 1), (148, 1), (151, 1), (153, 1), (155, 1), (163, 1), (172, 1), (173, 1), (184, 1), (185, 1), (186, 2), (190, 1), (191, 1), (205, 1), (217, 2), (218, 2), (219, 1), (232, 1), (248, 1), (249, 1), (251, 2), (258, 1), (260, 1), (261, 1), (262, 2), (263, 2), (264, 1), (267, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 339 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 34s - loss: 607.6951 - loglik: -6.0147e+02 - logprior: -5.7166e+00
Epoch 2/2
29/29 - 28s - loss: 584.2740 - loglik: -5.8087e+02 - logprior: -2.8678e+00
Fitted a model with MAP estimate = -581.2608
expansions: [(0, 2), (26, 1)]
discards: [  0  56  57 140 260 299 326]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 335 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 32s - loss: 586.1824 - loglik: -5.8203e+02 - logprior: -3.5941e+00
Epoch 2/2
29/29 - 28s - loss: 580.5366 - loglik: -5.7906e+02 - logprior: -9.0142e-01
Fitted a model with MAP estimate = -578.7096
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 334 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 31s - loss: 585.5351 - loglik: -5.7994e+02 - logprior: -5.0146e+00
Epoch 2/10
29/29 - 28s - loss: 580.3121 - loglik: -5.7864e+02 - logprior: -1.0663e+00
Epoch 3/10
29/29 - 28s - loss: 580.6876 - loglik: -5.7968e+02 - logprior: -3.9913e-01
Fitted a model with MAP estimate = -577.7485
Time for alignment: 490.7914
Computed alignments with likelihoods: ['-588.0269', '-577.4652', '-577.7485']
Best model has likelihood: -577.4652
time for generating output: 0.4557
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/icd.projection.fasta
SP score = 0.8662807525325615
Training of 3 independent models on file seatoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f343727c820>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f344950e0a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3392d7d160>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f335f9f39d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3323ae3fd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f345a7d70d0>, <__main__.SimpleDirichletPrior object at 0x7f335744af70>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3186410d30>

Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 619.6025 - loglik: -1.2835e+02 - logprior: -4.9123e+02
Epoch 2/10
10/10 - 0s - loss: 244.2986 - loglik: -1.0685e+02 - logprior: -1.3744e+02
Epoch 3/10
10/10 - 1s - loss: 151.4831 - loglik: -8.6722e+01 - logprior: -6.4726e+01
Epoch 4/10
10/10 - 1s - loss: 110.7553 - loglik: -7.3483e+01 - logprior: -3.7201e+01
Epoch 5/10
10/10 - 1s - loss: 92.3559 - loglik: -6.9824e+01 - logprior: -2.2485e+01
Epoch 6/10
10/10 - 1s - loss: 82.4428 - loglik: -6.9612e+01 - logprior: -1.2798e+01
Epoch 7/10
10/10 - 0s - loss: 76.6507 - loglik: -7.0107e+01 - logprior: -6.5186e+00
Epoch 8/10
10/10 - 1s - loss: 73.1245 - loglik: -7.0407e+01 - logprior: -2.6994e+00
Epoch 9/10
10/10 - 1s - loss: 70.7828 - loglik: -7.0593e+01 - logprior: -1.7940e-01
Epoch 10/10
10/10 - 0s - loss: 69.1118 - loglik: -7.0788e+01 - logprior: 1.6818
Fitted a model with MAP estimate = -68.3576
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 716.4342 - loglik: -6.1213e+01 - logprior: -6.5522e+02
Epoch 2/2
10/10 - 0s - loss: 260.5723 - loglik: -5.4135e+01 - logprior: -2.0644e+02
Fitted a model with MAP estimate = -174.2199
expansions: []
discards: [23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 507.2491 - loglik: -4.8862e+01 - logprior: -4.5839e+02
Epoch 2/2
10/10 - 0s - loss: 173.1636 - loglik: -4.9149e+01 - logprior: -1.2401e+02
Fitted a model with MAP estimate = -123.3553
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 478.3856 - loglik: -4.7836e+01 - logprior: -4.3055e+02
Epoch 2/10
10/10 - 0s - loss: 165.4346 - loglik: -4.9053e+01 - logprior: -1.1638e+02
Epoch 3/10
10/10 - 1s - loss: 99.7017 - loglik: -5.0073e+01 - logprior: -4.9629e+01
Epoch 4/10
10/10 - 1s - loss: 71.8134 - loglik: -5.0829e+01 - logprior: -2.0985e+01
Epoch 5/10
10/10 - 0s - loss: 56.4235 - loglik: -5.1424e+01 - logprior: -4.9995e+00
Epoch 6/10
10/10 - 1s - loss: 47.3870 - loglik: -5.1884e+01 - logprior: 4.4971
Epoch 7/10
10/10 - 1s - loss: 41.7289 - loglik: -5.2240e+01 - logprior: 10.5116
Epoch 8/10
10/10 - 0s - loss: 37.8592 - loglik: -5.2520e+01 - logprior: 14.6613
Epoch 9/10
10/10 - 0s - loss: 34.9657 - loglik: -5.2745e+01 - logprior: 17.7791
Epoch 10/10
10/10 - 0s - loss: 32.6356 - loglik: -5.2930e+01 - logprior: 20.2945
Fitted a model with MAP estimate = -31.5000
Time for alignment: 27.6229
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 619.6025 - loglik: -1.2835e+02 - logprior: -4.9123e+02
Epoch 2/10
10/10 - 1s - loss: 244.2982 - loglik: -1.0685e+02 - logprior: -1.3744e+02
Epoch 3/10
10/10 - 1s - loss: 151.4826 - loglik: -8.6722e+01 - logprior: -6.4726e+01
Epoch 4/10
10/10 - 1s - loss: 110.7559 - loglik: -7.3483e+01 - logprior: -3.7202e+01
Epoch 5/10
10/10 - 1s - loss: 92.3563 - loglik: -6.9824e+01 - logprior: -2.2485e+01
Epoch 6/10
10/10 - 1s - loss: 82.4429 - loglik: -6.9612e+01 - logprior: -1.2798e+01
Epoch 7/10
10/10 - 1s - loss: 76.6507 - loglik: -7.0107e+01 - logprior: -6.5186e+00
Epoch 8/10
10/10 - 0s - loss: 73.1245 - loglik: -7.0407e+01 - logprior: -2.6995e+00
Epoch 9/10
10/10 - 1s - loss: 70.7828 - loglik: -7.0593e+01 - logprior: -1.7941e-01
Epoch 10/10
10/10 - 1s - loss: 69.1118 - loglik: -7.0788e+01 - logprior: 1.6818
Fitted a model with MAP estimate = -68.3576
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 716.4344 - loglik: -6.1213e+01 - logprior: -6.5522e+02
Epoch 2/2
10/10 - 1s - loss: 260.5724 - loglik: -5.4135e+01 - logprior: -2.0644e+02
Fitted a model with MAP estimate = -174.2198
expansions: []
discards: [23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 507.2500 - loglik: -4.8864e+01 - logprior: -4.5839e+02
Epoch 2/2
10/10 - 1s - loss: 173.1663 - loglik: -4.9151e+01 - logprior: -1.2402e+02
Fitted a model with MAP estimate = -123.3588
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 478.3909 - loglik: -4.7840e+01 - logprior: -4.3055e+02
Epoch 2/10
10/10 - 1s - loss: 165.4429 - loglik: -4.9058e+01 - logprior: -1.1638e+02
Epoch 3/10
10/10 - 1s - loss: 99.7123 - loglik: -5.0077e+01 - logprior: -4.9635e+01
Epoch 4/10
10/10 - 0s - loss: 71.8257 - loglik: -5.0833e+01 - logprior: -2.0993e+01
Epoch 5/10
10/10 - 0s - loss: 56.4372 - loglik: -5.1428e+01 - logprior: -5.0088e+00
Epoch 6/10
10/10 - 0s - loss: 47.4019 - loglik: -5.1888e+01 - logprior: 4.4864
Epoch 7/10
10/10 - 0s - loss: 41.7448 - loglik: -5.2245e+01 - logprior: 10.4998
Epoch 8/10
10/10 - 0s - loss: 37.8759 - loglik: -5.2525e+01 - logprior: 14.6488
Epoch 9/10
10/10 - 0s - loss: 34.9832 - loglik: -5.2749e+01 - logprior: 17.7659
Epoch 10/10
10/10 - 0s - loss: 32.6536 - loglik: -5.2934e+01 - logprior: 20.2807
Fitted a model with MAP estimate = -31.5184
Time for alignment: 27.4818
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 619.6027 - loglik: -1.2835e+02 - logprior: -4.9123e+02
Epoch 2/10
10/10 - 0s - loss: 244.2987 - loglik: -1.0685e+02 - logprior: -1.3744e+02
Epoch 3/10
10/10 - 0s - loss: 151.4827 - loglik: -8.6722e+01 - logprior: -6.4726e+01
Epoch 4/10
10/10 - 0s - loss: 110.7552 - loglik: -7.3483e+01 - logprior: -3.7201e+01
Epoch 5/10
10/10 - 0s - loss: 92.3559 - loglik: -6.9824e+01 - logprior: -2.2485e+01
Epoch 6/10
10/10 - 0s - loss: 82.4428 - loglik: -6.9612e+01 - logprior: -1.2798e+01
Epoch 7/10
10/10 - 0s - loss: 76.6507 - loglik: -7.0107e+01 - logprior: -6.5186e+00
Epoch 8/10
10/10 - 0s - loss: 73.1245 - loglik: -7.0407e+01 - logprior: -2.6994e+00
Epoch 9/10
10/10 - 0s - loss: 70.7828 - loglik: -7.0593e+01 - logprior: -1.7940e-01
Epoch 10/10
10/10 - 0s - loss: 69.1118 - loglik: -7.0788e+01 - logprior: 1.6818
Fitted a model with MAP estimate = -68.3575
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 716.4343 - loglik: -6.1213e+01 - logprior: -6.5522e+02
Epoch 2/2
10/10 - 1s - loss: 260.5723 - loglik: -5.4135e+01 - logprior: -2.0644e+02
Fitted a model with MAP estimate = -174.2199
expansions: []
discards: [23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 507.2499 - loglik: -4.8864e+01 - logprior: -4.5839e+02
Epoch 2/2
10/10 - 1s - loss: 173.1663 - loglik: -4.9151e+01 - logprior: -1.2402e+02
Fitted a model with MAP estimate = -123.3589
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 478.3921 - loglik: -4.7840e+01 - logprior: -4.3055e+02
Epoch 2/10
10/10 - 0s - loss: 165.4468 - loglik: -4.9058e+01 - logprior: -1.1639e+02
Epoch 3/10
10/10 - 0s - loss: 99.7186 - loglik: -5.0077e+01 - logprior: -4.9641e+01
Epoch 4/10
10/10 - 0s - loss: 71.8341 - loglik: -5.0833e+01 - logprior: -2.1001e+01
Epoch 5/10
10/10 - 0s - loss: 56.4474 - loglik: -5.1429e+01 - logprior: -5.0185e+00
Epoch 6/10
10/10 - 1s - loss: 47.4137 - loglik: -5.1889e+01 - logprior: 4.4750
Epoch 7/10
10/10 - 1s - loss: 41.7580 - loglik: -5.2245e+01 - logprior: 10.4871
Epoch 8/10
10/10 - 1s - loss: 37.8903 - loglik: -5.2525e+01 - logprior: 14.6348
Epoch 9/10
10/10 - 1s - loss: 34.9988 - loglik: -5.2750e+01 - logprior: 17.7507
Epoch 10/10
10/10 - 1s - loss: 32.6704 - loglik: -5.2935e+01 - logprior: 20.2644
Fitted a model with MAP estimate = -31.5357
Time for alignment: 27.6197
Computed alignments with likelihoods: ['-31.5000', '-31.5184', '-31.5357']
Best model has likelihood: -31.5000
time for generating output: 0.1071
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/seatoxin.projection.fasta
SP score = 0.7083333333333334
Training of 3 independent models on file int.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f32f97b9760>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f33792f0730>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33792f04c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34af3ea340>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3323cc7d00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34629f57f0>, <__main__.SimpleDirichletPrior object at 0x7f34a68bab80>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f342d3e6d30>

Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 10s - loss: 477.0317 - loglik: -4.7265e+02 - logprior: -4.3005e+00
Epoch 2/10
16/16 - 5s - loss: 439.0626 - loglik: -4.3733e+02 - logprior: -1.1371e+00
Epoch 3/10
16/16 - 5s - loss: 413.0435 - loglik: -4.1046e+02 - logprior: -1.5495e+00
Epoch 4/10
16/16 - 5s - loss: 402.3057 - loglik: -3.9937e+02 - logprior: -1.7206e+00
Epoch 5/10
16/16 - 5s - loss: 399.7982 - loglik: -3.9692e+02 - logprior: -1.6659e+00
Epoch 6/10
16/16 - 5s - loss: 398.0433 - loglik: -3.9534e+02 - logprior: -1.6904e+00
Epoch 7/10
16/16 - 5s - loss: 397.4933 - loglik: -3.9492e+02 - logprior: -1.6789e+00
Epoch 8/10
16/16 - 5s - loss: 396.8570 - loglik: -3.9433e+02 - logprior: -1.7177e+00
Epoch 9/10
16/16 - 5s - loss: 394.9994 - loglik: -3.9251e+02 - logprior: -1.7470e+00
Epoch 10/10
16/16 - 5s - loss: 396.0169 - loglik: -3.9354e+02 - logprior: -1.7724e+00
Fitted a model with MAP estimate = -394.2049
expansions: [(13, 1), (14, 1), (22, 1), (23, 1), (28, 3), (29, 1), (42, 1), (43, 1), (48, 2), (50, 2), (56, 1), (58, 1), (72, 1), (73, 2), (74, 2), (75, 2), (93, 1), (94, 5), (95, 1), (99, 1), (114, 1), (117, 2), (120, 1), (123, 1), (126, 1), (128, 1), (129, 1), (139, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 179 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 390.8283 - loglik: -3.8572e+02 - logprior: -4.3371e+00
Epoch 2/2
33/33 - 9s - loss: 382.7549 - loglik: -3.8021e+02 - logprior: -1.7410e+00
Fitted a model with MAP estimate = -379.1403
expansions: [(179, 2)]
discards: [  0  32  89  95  96 117 118 148 177 178]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 13s - loss: 388.1334 - loglik: -3.8304e+02 - logprior: -4.1902e+00
Epoch 2/2
33/33 - 9s - loss: 383.5996 - loglik: -3.8129e+02 - logprior: -1.4738e+00
Fitted a model with MAP estimate = -381.2723
expansions: [(0, 1), (171, 2)]
discards: [ 30  60 169 170]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 170 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 12s - loss: 384.6402 - loglik: -3.8097e+02 - logprior: -2.8446e+00
Epoch 2/10
33/33 - 8s - loss: 382.2619 - loglik: -3.8045e+02 - logprior: -1.0273e+00
Epoch 3/10
33/33 - 8s - loss: 380.8404 - loglik: -3.7927e+02 - logprior: -8.0877e-01
Epoch 4/10
33/33 - 8s - loss: 381.2788 - loglik: -3.7989e+02 - logprior: -7.1077e-01
Fitted a model with MAP estimate = -380.1086
Time for alignment: 172.2363
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 477.0366 - loglik: -4.7265e+02 - logprior: -4.3036e+00
Epoch 2/10
16/16 - 5s - loss: 439.6588 - loglik: -4.3792e+02 - logprior: -1.1464e+00
Epoch 3/10
16/16 - 5s - loss: 412.4988 - loglik: -4.0988e+02 - logprior: -1.5803e+00
Epoch 4/10
16/16 - 5s - loss: 403.0793 - loglik: -4.0018e+02 - logprior: -1.7250e+00
Epoch 5/10
16/16 - 5s - loss: 400.5779 - loglik: -3.9773e+02 - logprior: -1.6461e+00
Epoch 6/10
16/16 - 5s - loss: 398.5232 - loglik: -3.9587e+02 - logprior: -1.6765e+00
Epoch 7/10
16/16 - 5s - loss: 397.5735 - loglik: -3.9510e+02 - logprior: -1.6471e+00
Epoch 8/10
16/16 - 5s - loss: 397.2516 - loglik: -3.9483e+02 - logprior: -1.6844e+00
Epoch 9/10
16/16 - 5s - loss: 396.4086 - loglik: -3.9400e+02 - logprior: -1.7035e+00
Epoch 10/10
16/16 - 5s - loss: 396.0378 - loglik: -3.9367e+02 - logprior: -1.7034e+00
Fitted a model with MAP estimate = -395.0952
expansions: [(13, 1), (14, 1), (24, 1), (27, 1), (28, 2), (30, 1), (41, 1), (45, 1), (48, 4), (50, 2), (57, 1), (58, 1), (72, 1), (73, 2), (81, 1), (95, 3), (96, 1), (99, 2), (113, 1), (116, 1), (117, 2), (122, 1), (124, 1), (125, 1), (139, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 175 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 13s - loss: 393.1650 - loglik: -3.8807e+02 - logprior: -4.3703e+00
Epoch 2/2
33/33 - 9s - loss: 385.2317 - loglik: -3.8267e+02 - logprior: -1.7865e+00
Fitted a model with MAP estimate = -381.7263
expansions: [(113, 1)]
discards: [  0  31  32  59  63  90 124 145 174]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 167 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 390.3340 - loglik: -3.8528e+02 - logprior: -4.1678e+00
Epoch 2/2
33/33 - 8s - loss: 386.6109 - loglik: -3.8435e+02 - logprior: -1.4629e+00
Fitted a model with MAP estimate = -383.6274
expansions: [(0, 1), (30, 2), (167, 2)]
discards: [165 166]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 170 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 12s - loss: 385.3564 - loglik: -3.8168e+02 - logprior: -2.8561e+00
Epoch 2/10
33/33 - 9s - loss: 382.6030 - loglik: -3.8080e+02 - logprior: -1.0163e+00
Epoch 3/10
33/33 - 8s - loss: 381.1743 - loglik: -3.7960e+02 - logprior: -7.9323e-01
Epoch 4/10
33/33 - 8s - loss: 381.0285 - loglik: -3.7968e+02 - logprior: -6.6626e-01
Epoch 5/10
33/33 - 8s - loss: 381.6467 - loglik: -3.8043e+02 - logprior: -5.6569e-01
Fitted a model with MAP estimate = -380.0804
Time for alignment: 178.3271
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 476.6969 - loglik: -4.7230e+02 - logprior: -4.3140e+00
Epoch 2/10
16/16 - 5s - loss: 440.3409 - loglik: -4.3856e+02 - logprior: -1.1665e+00
Epoch 3/10
16/16 - 5s - loss: 415.8112 - loglik: -4.1305e+02 - logprior: -1.5397e+00
Epoch 4/10
16/16 - 5s - loss: 405.1280 - loglik: -4.0196e+02 - logprior: -1.6275e+00
Epoch 5/10
16/16 - 5s - loss: 401.1584 - loglik: -3.9809e+02 - logprior: -1.5923e+00
Epoch 6/10
16/16 - 5s - loss: 399.5067 - loglik: -3.9675e+02 - logprior: -1.6371e+00
Epoch 7/10
16/16 - 5s - loss: 397.9259 - loglik: -3.9534e+02 - logprior: -1.6500e+00
Epoch 8/10
16/16 - 5s - loss: 396.0353 - loglik: -3.9352e+02 - logprior: -1.6934e+00
Epoch 9/10
16/16 - 5s - loss: 397.2611 - loglik: -3.9473e+02 - logprior: -1.7174e+00
Fitted a model with MAP estimate = -394.8081
expansions: [(13, 1), (14, 1), (17, 1), (23, 1), (28, 3), (29, 1), (42, 1), (43, 1), (48, 2), (49, 2), (53, 1), (56, 1), (57, 2), (69, 1), (72, 2), (74, 3), (95, 3), (96, 1), (99, 2), (105, 1), (116, 3), (119, 1), (122, 1), (125, 1), (129, 1), (130, 1), (131, 1), (139, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 180 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 392.2115 - loglik: -3.8696e+02 - logprior: -4.3712e+00
Epoch 2/2
33/33 - 9s - loss: 381.1949 - loglik: -3.7852e+02 - logprior: -1.7922e+00
Fitted a model with MAP estimate = -378.8599
expansions: [(180, 2)]
discards: [  0  32  60  74  91  97 126 147 178 179]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 172 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 14s - loss: 388.0240 - loglik: -3.8288e+02 - logprior: -4.1760e+00
Epoch 2/2
33/33 - 8s - loss: 384.2071 - loglik: -3.8188e+02 - logprior: -1.4638e+00
Fitted a model with MAP estimate = -381.0964
expansions: [(0, 1), (71, 1)]
discards: [ 30  69 170 171]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 170 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 12s - loss: 384.1189 - loglik: -3.8044e+02 - logprior: -2.8325e+00
Epoch 2/10
33/33 - 8s - loss: 381.3946 - loglik: -3.7966e+02 - logprior: -9.5518e-01
Epoch 3/10
33/33 - 8s - loss: 380.5849 - loglik: -3.7908e+02 - logprior: -7.4682e-01
Epoch 4/10
33/33 - 8s - loss: 379.8769 - loglik: -3.7856e+02 - logprior: -6.3776e-01
Epoch 5/10
33/33 - 8s - loss: 381.3621 - loglik: -3.8018e+02 - logprior: -5.3059e-01
Fitted a model with MAP estimate = -379.3118
Time for alignment: 173.4946
Computed alignments with likelihoods: ['-379.1403', '-380.0804', '-378.8599']
Best model has likelihood: -378.8599
time for generating output: 0.2455
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/int.projection.fasta
SP score = 0.7817371937639198
Training of 3 independent models on file phc.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f33238e5910>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f348d060970>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3323cc7d00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34af3ea340>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32f97b9760>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f33792f0730>, <__main__.SimpleDirichletPrior object at 0x7f3323e7c0a0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f342d3e6d30>

Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 292.0421 - loglik: -2.7695e+02 - logprior: -1.5086e+01
Epoch 2/10
10/10 - 2s - loss: 251.9288 - loglik: -2.4786e+02 - logprior: -4.0593e+00
Epoch 3/10
10/10 - 2s - loss: 226.9920 - loglik: -2.2456e+02 - logprior: -2.4256e+00
Epoch 4/10
10/10 - 2s - loss: 211.1066 - loglik: -2.0885e+02 - logprior: -2.1167e+00
Epoch 5/10
10/10 - 2s - loss: 207.1420 - loglik: -2.0481e+02 - logprior: -2.0878e+00
Epoch 6/10
10/10 - 2s - loss: 202.0310 - loglik: -1.9960e+02 - logprior: -2.1482e+00
Epoch 7/10
10/10 - 2s - loss: 203.6516 - loglik: -2.0144e+02 - logprior: -1.9892e+00
Fitted a model with MAP estimate = -202.3217
expansions: [(20, 1), (34, 1), (37, 4), (45, 1), (48, 3), (54, 1), (56, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 79 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 217.9037 - loglik: -2.0008e+02 - logprior: -1.7581e+01
Epoch 2/2
10/10 - 2s - loss: 200.8017 - loglik: -1.9269e+02 - logprior: -7.8904e+00
Fitted a model with MAP estimate = -196.8047
expansions: [(0, 5), (40, 1)]
discards: [ 0 34 55]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 203.9307 - loglik: -1.8925e+02 - logprior: -1.4490e+01
Epoch 2/2
10/10 - 2s - loss: 190.1174 - loglik: -1.8525e+02 - logprior: -4.6666e+00
Fitted a model with MAP estimate = -186.8540
expansions: [(55, 1)]
discards: [0 1 2 3]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 79 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 204.5354 - loglik: -1.8717e+02 - logprior: -1.7180e+01
Epoch 2/10
10/10 - 2s - loss: 192.0849 - loglik: -1.8581e+02 - logprior: -6.0827e+00
Epoch 3/10
10/10 - 2s - loss: 187.8634 - loglik: -1.8455e+02 - logprior: -3.1296e+00
Epoch 4/10
10/10 - 2s - loss: 187.2875 - loglik: -1.8479e+02 - logprior: -2.3308e+00
Epoch 5/10
10/10 - 2s - loss: 187.7078 - loglik: -1.8564e+02 - logprior: -1.8872e+00
Fitted a model with MAP estimate = -186.6511
Time for alignment: 47.6745
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 291.5439 - loglik: -2.7645e+02 - logprior: -1.5087e+01
Epoch 2/10
10/10 - 2s - loss: 253.4436 - loglik: -2.4938e+02 - logprior: -4.0576e+00
Epoch 3/10
10/10 - 2s - loss: 227.6577 - loglik: -2.2519e+02 - logprior: -2.4519e+00
Epoch 4/10
10/10 - 2s - loss: 209.1141 - loglik: -2.0670e+02 - logprior: -2.2538e+00
Epoch 5/10
10/10 - 2s - loss: 203.0452 - loglik: -2.0032e+02 - logprior: -2.3409e+00
Epoch 6/10
10/10 - 2s - loss: 201.4373 - loglik: -1.9861e+02 - logprior: -2.4145e+00
Epoch 7/10
10/10 - 2s - loss: 198.9906 - loglik: -1.9645e+02 - logprior: -2.2950e+00
Epoch 8/10
10/10 - 2s - loss: 198.5627 - loglik: -1.9618e+02 - logprior: -2.2102e+00
Epoch 9/10
10/10 - 2s - loss: 199.2635 - loglik: -1.9693e+02 - logprior: -2.1865e+00
Fitted a model with MAP estimate = -197.8904
expansions: [(20, 1), (25, 1), (34, 1), (36, 1), (39, 1), (46, 1), (47, 1), (48, 2), (54, 1), (56, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 214.4690 - loglik: -1.9674e+02 - logprior: -1.7596e+01
Epoch 2/2
10/10 - 2s - loss: 196.8222 - loglik: -1.8890e+02 - logprior: -7.7688e+00
Fitted a model with MAP estimate = -193.5038
expansions: [(0, 5)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 198.7274 - loglik: -1.8424e+02 - logprior: -1.4327e+01
Epoch 2/2
10/10 - 2s - loss: 187.2923 - loglik: -1.8260e+02 - logprior: -4.4988e+00
Fitted a model with MAP estimate = -183.6784
expansions: []
discards: [1 2 3]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 79 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 196.5948 - loglik: -1.8228e+02 - logprior: -1.4117e+01
Epoch 2/10
10/10 - 2s - loss: 185.9040 - loglik: -1.8125e+02 - logprior: -4.4506e+00
Epoch 3/10
10/10 - 2s - loss: 185.2615 - loglik: -1.8246e+02 - logprior: -2.5926e+00
Epoch 4/10
10/10 - 2s - loss: 183.8717 - loglik: -1.8164e+02 - logprior: -2.0202e+00
Epoch 5/10
10/10 - 2s - loss: 182.9618 - loglik: -1.8092e+02 - logprior: -1.8294e+00
Epoch 6/10
10/10 - 2s - loss: 183.9026 - loglik: -1.8201e+02 - logprior: -1.6841e+00
Fitted a model with MAP estimate = -183.2968
Time for alignment: 53.3555
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 292.0154 - loglik: -2.7693e+02 - logprior: -1.5081e+01
Epoch 2/10
10/10 - 2s - loss: 252.3241 - loglik: -2.4827e+02 - logprior: -4.0440e+00
Epoch 3/10
10/10 - 2s - loss: 227.0760 - loglik: -2.2468e+02 - logprior: -2.3855e+00
Epoch 4/10
10/10 - 2s - loss: 212.5578 - loglik: -2.1032e+02 - logprior: -2.1037e+00
Epoch 5/10
10/10 - 2s - loss: 208.4404 - loglik: -2.0607e+02 - logprior: -2.1362e+00
Epoch 6/10
10/10 - 2s - loss: 203.4502 - loglik: -2.0094e+02 - logprior: -2.2268e+00
Epoch 7/10
10/10 - 2s - loss: 204.9063 - loglik: -2.0255e+02 - logprior: -2.1234e+00
Fitted a model with MAP estimate = -203.5190
expansions: [(26, 2), (34, 1), (37, 1), (46, 2), (47, 1), (48, 1), (54, 1), (56, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 77 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 220.7659 - loglik: -2.0294e+02 - logprior: -1.7634e+01
Epoch 2/2
10/10 - 2s - loss: 203.4902 - loglik: -1.9540e+02 - logprior: -7.8859e+00
Fitted a model with MAP estimate = -199.6125
expansions: [(0, 5), (3, 1), (42, 1), (49, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 205.9836 - loglik: -1.9127e+02 - logprior: -1.4524e+01
Epoch 2/2
10/10 - 2s - loss: 191.6558 - loglik: -1.8684e+02 - logprior: -4.5967e+00
Fitted a model with MAP estimate = -188.7420
expansions: []
discards: [ 0  1  2  3  7 31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 209.0757 - loglik: -1.9169e+02 - logprior: -1.7171e+01
Epoch 2/10
10/10 - 2s - loss: 196.2297 - loglik: -1.8982e+02 - logprior: -6.1925e+00
Epoch 3/10
10/10 - 2s - loss: 193.4278 - loglik: -1.9010e+02 - logprior: -3.1178e+00
Epoch 4/10
10/10 - 2s - loss: 192.6603 - loglik: -1.9019e+02 - logprior: -2.2557e+00
Epoch 5/10
10/10 - 2s - loss: 192.3614 - loglik: -1.9029e+02 - logprior: -1.8443e+00
Epoch 6/10
10/10 - 2s - loss: 190.4867 - loglik: -1.8872e+02 - logprior: -1.5370e+00
Epoch 7/10
10/10 - 2s - loss: 191.0099 - loglik: -1.8932e+02 - logprior: -1.4467e+00
Fitted a model with MAP estimate = -190.9742
Time for alignment: 49.1819
Computed alignments with likelihoods: ['-186.6511', '-183.2968', '-188.7420']
Best model has likelihood: -183.2968
time for generating output: 0.2361
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phc.projection.fasta
SP score = 0.7860508953817154
Training of 3 independent models on file ricin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3451b28130>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3451e5da90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f31860d6fd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f344053d0d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f344053d7f0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f33229b9460>, <__main__.SimpleDirichletPrior object at 0x7f34a6ffab50>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34375068b0>

Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 615.4411 - loglik: -5.6113e+02 - logprior: -5.4295e+01
Epoch 2/10
10/10 - 3s - loss: 519.1278 - loglik: -5.0883e+02 - logprior: -1.0275e+01
Epoch 3/10
10/10 - 3s - loss: 456.4146 - loglik: -4.5291e+02 - logprior: -3.4009e+00
Epoch 4/10
10/10 - 3s - loss: 422.2704 - loglik: -4.2013e+02 - logprior: -1.8691e+00
Epoch 5/10
10/10 - 3s - loss: 407.2684 - loglik: -4.0535e+02 - logprior: -1.4170e+00
Epoch 6/10
10/10 - 3s - loss: 401.1158 - loglik: -3.9959e+02 - logprior: -9.0798e-01
Epoch 7/10
10/10 - 3s - loss: 397.6597 - loglik: -3.9643e+02 - logprior: -6.3925e-01
Epoch 8/10
10/10 - 3s - loss: 396.6779 - loglik: -3.9566e+02 - logprior: -4.8532e-01
Epoch 9/10
10/10 - 3s - loss: 395.4408 - loglik: -3.9466e+02 - logprior: -2.7164e-01
Epoch 10/10
10/10 - 3s - loss: 393.8430 - loglik: -3.9331e+02 - logprior: -3.6775e-02
Fitted a model with MAP estimate = -394.0760
expansions: [(9, 2), (19, 2), (20, 2), (22, 1), (23, 1), (25, 1), (30, 1), (32, 1), (43, 1), (44, 3), (45, 1), (46, 1), (52, 1), (80, 2), (82, 2), (94, 1), (104, 2), (105, 2), (114, 1), (117, 3), (119, 1), (144, 4), (150, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 203 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 450.4404 - loglik: -3.8678e+02 - logprior: -6.3181e+01
Epoch 2/2
10/10 - 3s - loss: 395.4448 - loglik: -3.7067e+02 - logprior: -2.4266e+01
Fitted a model with MAP estimate = -384.7243
expansions: [(0, 3), (9, 1)]
discards: [  0  21  24  56  97 101 127 145]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 199 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 414.9691 - loglik: -3.6506e+02 - logprior: -4.9393e+01
Epoch 2/2
10/10 - 3s - loss: 373.2378 - loglik: -3.6247e+02 - logprior: -1.0224e+01
Fitted a model with MAP estimate = -365.5915
expansions: [(41, 1), (57, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 199 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 423.2598 - loglik: -3.6155e+02 - logprior: -6.1170e+01
Epoch 2/10
10/10 - 3s - loss: 381.0551 - loglik: -3.6142e+02 - logprior: -1.9083e+01
Epoch 3/10
10/10 - 3s - loss: 365.7510 - loglik: -3.6092e+02 - logprior: -4.2898e+00
Epoch 4/10
10/10 - 3s - loss: 358.4090 - loglik: -3.5988e+02 - logprior: 2.0244
Epoch 5/10
10/10 - 3s - loss: 357.3461 - loglik: -3.6129e+02 - logprior: 4.4926
Epoch 6/10
10/10 - 3s - loss: 356.0739 - loglik: -3.6130e+02 - logprior: 5.7858
Epoch 7/10
10/10 - 3s - loss: 354.4636 - loglik: -3.6053e+02 - logprior: 6.6306
Epoch 8/10
10/10 - 3s - loss: 353.7424 - loglik: -3.6052e+02 - logprior: 7.3338
Epoch 9/10
10/10 - 3s - loss: 354.6886 - loglik: -3.6214e+02 - logprior: 8.0100
Fitted a model with MAP estimate = -352.6555
Time for alignment: 92.4376
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 614.6042 - loglik: -5.6029e+02 - logprior: -5.4297e+01
Epoch 2/10
10/10 - 3s - loss: 519.3599 - loglik: -5.0905e+02 - logprior: -1.0286e+01
Epoch 3/10
10/10 - 3s - loss: 455.5268 - loglik: -4.5220e+02 - logprior: -3.2271e+00
Epoch 4/10
10/10 - 3s - loss: 418.6076 - loglik: -4.1691e+02 - logprior: -1.3993e+00
Epoch 5/10
10/10 - 3s - loss: 403.8387 - loglik: -4.0199e+02 - logprior: -1.2148e+00
Epoch 6/10
10/10 - 3s - loss: 400.1715 - loglik: -3.9861e+02 - logprior: -8.2960e-01
Epoch 7/10
10/10 - 3s - loss: 396.9623 - loglik: -3.9603e+02 - logprior: -3.8508e-01
Epoch 8/10
10/10 - 3s - loss: 395.5714 - loglik: -3.9488e+02 - logprior: -2.4000e-01
Epoch 9/10
10/10 - 3s - loss: 394.9953 - loglik: -3.9431e+02 - logprior: -2.2710e-01
Epoch 10/10
10/10 - 3s - loss: 392.4820 - loglik: -3.9190e+02 - logprior: -8.2903e-02
Fitted a model with MAP estimate = -392.9356
expansions: [(7, 1), (8, 2), (19, 2), (22, 2), (23, 1), (25, 1), (29, 3), (44, 1), (45, 2), (46, 1), (52, 2), (74, 1), (75, 2), (81, 2), (94, 1), (106, 1), (114, 1), (116, 2), (117, 1), (118, 1), (144, 3), (146, 1), (149, 2), (150, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 204 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 447.2667 - loglik: -3.8386e+02 - logprior: -6.2911e+01
Epoch 2/2
10/10 - 3s - loss: 392.8152 - loglik: -3.6829e+02 - logprior: -2.3966e+01
Fitted a model with MAP estimate = -381.9701
expansions: [(0, 3)]
discards: [  0  22  26  37  67 101 183]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 200 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 415.1783 - loglik: -3.6510e+02 - logprior: -4.9502e+01
Epoch 2/2
10/10 - 3s - loss: 373.0436 - loglik: -3.6187e+02 - logprior: -1.0561e+01
Fitted a model with MAP estimate = -366.0402
expansions: [(181, 1)]
discards: [  0   2  56 138]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 197 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 425.7582 - loglik: -3.6398e+02 - logprior: -6.1168e+01
Epoch 2/10
10/10 - 3s - loss: 380.9142 - loglik: -3.6208e+02 - logprior: -1.8208e+01
Epoch 3/10
10/10 - 3s - loss: 365.9847 - loglik: -3.6163e+02 - logprior: -3.7389e+00
Epoch 4/10
10/10 - 3s - loss: 361.0319 - loglik: -3.6235e+02 - logprior: 1.9295
Epoch 5/10
10/10 - 3s - loss: 359.0413 - loglik: -3.6271e+02 - logprior: 4.2889
Epoch 6/10
10/10 - 3s - loss: 358.7352 - loglik: -3.6367e+02 - logprior: 5.5443
Epoch 7/10
10/10 - 3s - loss: 355.6862 - loglik: -3.6147e+02 - logprior: 6.4031
Epoch 8/10
10/10 - 3s - loss: 356.9458 - loglik: -3.6344e+02 - logprior: 7.1125
Fitted a model with MAP estimate = -355.0868
Time for alignment: 86.9186
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 615.3517 - loglik: -5.6104e+02 - logprior: -5.4299e+01
Epoch 2/10
10/10 - 3s - loss: 517.7396 - loglik: -5.0745e+02 - logprior: -1.0268e+01
Epoch 3/10
10/10 - 3s - loss: 456.1499 - loglik: -4.5281e+02 - logprior: -3.2429e+00
Epoch 4/10
10/10 - 3s - loss: 417.9193 - loglik: -4.1605e+02 - logprior: -1.5622e+00
Epoch 5/10
10/10 - 3s - loss: 402.2613 - loglik: -4.0052e+02 - logprior: -1.0753e+00
Epoch 6/10
10/10 - 3s - loss: 397.3393 - loglik: -3.9597e+02 - logprior: -5.5939e-01
Epoch 7/10
10/10 - 3s - loss: 394.7491 - loglik: -3.9393e+02 - logprior: -2.5085e-01
Epoch 8/10
10/10 - 3s - loss: 392.5597 - loglik: -3.9200e+02 - logprior: -1.1203e-01
Epoch 9/10
10/10 - 3s - loss: 391.5953 - loglik: -3.9111e+02 - logprior: 0.0126
Epoch 10/10
10/10 - 3s - loss: 390.9739 - loglik: -3.9055e+02 - logprior: 0.1253
Fitted a model with MAP estimate = -389.8401
expansions: [(0, 3), (19, 1), (20, 2), (22, 2), (23, 1), (25, 1), (31, 2), (44, 3), (45, 1), (46, 1), (52, 1), (75, 3), (81, 2), (94, 1), (99, 1), (104, 2), (105, 1), (106, 2), (113, 1), (114, 2), (142, 4), (150, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 208 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 450.1837 - loglik: -3.7881e+02 - logprior: -7.0868e+01
Epoch 2/2
10/10 - 3s - loss: 381.5651 - loglik: -3.6226e+02 - logprior: -1.8753e+01
Fitted a model with MAP estimate = -366.3382
expansions: [(178, 1)]
discards: [  0   1   2  24  28 102 123 134 188 189]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 199 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 414.3459 - loglik: -3.6165e+02 - logprior: -5.2130e+01
Epoch 2/2
10/10 - 3s - loss: 368.6068 - loglik: -3.5692e+02 - logprior: -1.1090e+01
Fitted a model with MAP estimate = -362.6191
expansions: [(117, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 200 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 406.1250 - loglik: -3.5677e+02 - logprior: -4.8753e+01
Epoch 2/10
10/10 - 3s - loss: 367.2652 - loglik: -3.5711e+02 - logprior: -9.5473e+00
Epoch 3/10
10/10 - 3s - loss: 358.6002 - loglik: -3.5680e+02 - logprior: -1.2106e+00
Epoch 4/10
10/10 - 3s - loss: 354.6418 - loglik: -3.5654e+02 - logprior: 2.4866
Epoch 5/10
10/10 - 3s - loss: 353.9972 - loglik: -3.5801e+02 - logprior: 4.5980
Epoch 6/10
10/10 - 3s - loss: 352.2710 - loglik: -3.5760e+02 - logprior: 5.9270
Epoch 7/10
10/10 - 3s - loss: 352.1900 - loglik: -3.5842e+02 - logprior: 6.8226
Epoch 8/10
10/10 - 3s - loss: 351.5704 - loglik: -3.5845e+02 - logprior: 7.4845
Epoch 9/10
10/10 - 3s - loss: 351.1934 - loglik: -3.5865e+02 - logprior: 8.0566
Epoch 10/10
10/10 - 3s - loss: 350.3284 - loglik: -3.5828e+02 - logprior: 8.5608
Fitted a model with MAP estimate = -349.1859
Time for alignment: 94.7500
Computed alignments with likelihoods: ['-352.6555', '-355.0868', '-349.1859']
Best model has likelihood: -349.1859
time for generating output: 0.3051
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ricin.projection.fasta
SP score = 0.7793197810789679
Training of 3 independent models on file PDZ.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f33a447e3d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f342d3d5760>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3473de1d00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345a01c400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34404ee580>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f342d076d30>, <__main__.SimpleDirichletPrior object at 0x7f31860e8f70>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3484d38dc0>

Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 238.3895 - loglik: -2.3517e+02 - logprior: -3.1659e+00
Epoch 2/10
19/19 - 2s - loss: 209.2949 - loglik: -2.0768e+02 - logprior: -1.2448e+00
Epoch 3/10
19/19 - 2s - loss: 197.2566 - loglik: -1.9528e+02 - logprior: -1.4146e+00
Epoch 4/10
19/19 - 2s - loss: 194.3219 - loglik: -1.9250e+02 - logprior: -1.3585e+00
Epoch 5/10
19/19 - 2s - loss: 193.0369 - loglik: -1.9132e+02 - logprior: -1.3243e+00
Epoch 6/10
19/19 - 2s - loss: 192.4092 - loglik: -1.9077e+02 - logprior: -1.3072e+00
Epoch 7/10
19/19 - 2s - loss: 192.2808 - loglik: -1.9068e+02 - logprior: -1.2755e+00
Epoch 8/10
19/19 - 2s - loss: 191.7949 - loglik: -1.9019e+02 - logprior: -1.2855e+00
Epoch 9/10
19/19 - 2s - loss: 191.4712 - loglik: -1.8988e+02 - logprior: -1.2755e+00
Epoch 10/10
19/19 - 2s - loss: 191.8802 - loglik: -1.9030e+02 - logprior: -1.2697e+00
Fitted a model with MAP estimate = -185.7632
expansions: [(0, 2), (3, 1), (6, 1), (16, 1), (17, 4), (18, 3), (23, 1), (36, 1), (46, 2), (47, 1), (48, 1), (49, 2), (52, 1), (55, 2), (58, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 88 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 192.0628 - loglik: -1.8733e+02 - logprior: -4.4115e+00
Epoch 2/2
19/19 - 2s - loss: 184.0076 - loglik: -1.8198e+02 - logprior: -1.6343e+00
Fitted a model with MAP estimate = -179.4611
expansions: []
discards: [ 0  1 60]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 184.7937 - loglik: -1.8127e+02 - logprior: -3.0672e+00
Epoch 2/2
19/19 - 2s - loss: 182.4181 - loglik: -1.8071e+02 - logprior: -1.2311e+00
Fitted a model with MAP estimate = -179.7371
expansions: [(0, 2), (22, 1)]
discards: [23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 180.5264 - loglik: -1.7741e+02 - logprior: -2.4746e+00
Epoch 2/10
23/23 - 3s - loss: 176.9838 - loglik: -1.7526e+02 - logprior: -1.1363e+00
Epoch 3/10
23/23 - 3s - loss: 175.8622 - loglik: -1.7416e+02 - logprior: -1.0913e+00
Epoch 4/10
23/23 - 3s - loss: 174.8534 - loglik: -1.7316e+02 - logprior: -1.0481e+00
Epoch 5/10
23/23 - 3s - loss: 174.4858 - loglik: -1.7279e+02 - logprior: -1.0340e+00
Epoch 6/10
23/23 - 3s - loss: 173.8619 - loglik: -1.7218e+02 - logprior: -1.0193e+00
Epoch 7/10
23/23 - 3s - loss: 173.9486 - loglik: -1.7230e+02 - logprior: -1.0115e+00
Fitted a model with MAP estimate = -172.9410
Time for alignment: 82.7606
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 237.9486 - loglik: -2.3474e+02 - logprior: -3.1591e+00
Epoch 2/10
19/19 - 2s - loss: 204.7821 - loglik: -2.0327e+02 - logprior: -1.2358e+00
Epoch 3/10
19/19 - 2s - loss: 195.5503 - loglik: -1.9365e+02 - logprior: -1.3935e+00
Epoch 4/10
19/19 - 2s - loss: 193.7242 - loglik: -1.9204e+02 - logprior: -1.3249e+00
Epoch 5/10
19/19 - 2s - loss: 192.9981 - loglik: -1.9139e+02 - logprior: -1.2897e+00
Epoch 6/10
19/19 - 2s - loss: 192.3578 - loglik: -1.9082e+02 - logprior: -1.2693e+00
Epoch 7/10
19/19 - 2s - loss: 192.1892 - loglik: -1.9067e+02 - logprior: -1.2582e+00
Epoch 8/10
19/19 - 2s - loss: 191.7877 - loglik: -1.9026e+02 - logprior: -1.2619e+00
Epoch 9/10
19/19 - 2s - loss: 191.9860 - loglik: -1.9046e+02 - logprior: -1.2562e+00
Fitted a model with MAP estimate = -185.6104
expansions: [(3, 1), (4, 1), (5, 1), (6, 1), (15, 2), (16, 5), (17, 2), (22, 1), (46, 1), (47, 1), (48, 2), (49, 1), (52, 1), (55, 2), (58, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 190.3167 - loglik: -1.8712e+02 - logprior: -2.9226e+00
Epoch 2/2
19/19 - 2s - loss: 184.1728 - loglik: -1.8254e+02 - logprior: -1.2670e+00
Fitted a model with MAP estimate = -179.7599
expansions: [(20, 1), (21, 1)]
discards: [ 0 23 24 25 26]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 188.6764 - loglik: -1.8428e+02 - logprior: -3.9904e+00
Epoch 2/2
19/19 - 2s - loss: 185.3682 - loglik: -1.8282e+02 - logprior: -2.1180e+00
Fitted a model with MAP estimate = -181.1333
expansions: [(0, 3), (27, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 88 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 180.2373 - loglik: -1.7780e+02 - logprior: -1.8829e+00
Epoch 2/10
23/23 - 3s - loss: 177.5486 - loglik: -1.7602e+02 - logprior: -9.9157e-01
Epoch 3/10
23/23 - 3s - loss: 176.4247 - loglik: -1.7491e+02 - logprior: -9.4580e-01
Epoch 4/10
23/23 - 3s - loss: 175.4517 - loglik: -1.7389e+02 - logprior: -9.6032e-01
Epoch 5/10
23/23 - 3s - loss: 175.0484 - loglik: -1.7345e+02 - logprior: -9.6709e-01
Epoch 6/10
23/23 - 3s - loss: 174.9970 - loglik: -1.7343e+02 - logprior: -9.5031e-01
Epoch 7/10
23/23 - 3s - loss: 174.2354 - loglik: -1.7269e+02 - logprior: -9.4071e-01
Epoch 8/10
23/23 - 3s - loss: 174.0196 - loglik: -1.7248e+02 - logprior: -9.2835e-01
Epoch 9/10
23/23 - 3s - loss: 174.2637 - loglik: -1.7278e+02 - logprior: -9.0930e-01
Fitted a model with MAP estimate = -173.2208
Time for alignment: 84.9605
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 238.0516 - loglik: -2.3484e+02 - logprior: -3.1590e+00
Epoch 2/10
19/19 - 2s - loss: 205.8170 - loglik: -2.0424e+02 - logprior: -1.2353e+00
Epoch 3/10
19/19 - 2s - loss: 196.0400 - loglik: -1.9416e+02 - logprior: -1.3920e+00
Epoch 4/10
19/19 - 2s - loss: 193.5479 - loglik: -1.9186e+02 - logprior: -1.3158e+00
Epoch 5/10
19/19 - 2s - loss: 192.3615 - loglik: -1.9076e+02 - logprior: -1.3045e+00
Epoch 6/10
19/19 - 2s - loss: 192.1404 - loglik: -1.9058e+02 - logprior: -1.2754e+00
Epoch 7/10
19/19 - 2s - loss: 191.9391 - loglik: -1.9040e+02 - logprior: -1.2769e+00
Epoch 8/10
19/19 - 2s - loss: 191.6293 - loglik: -1.9010e+02 - logprior: -1.2624e+00
Epoch 9/10
19/19 - 2s - loss: 191.6453 - loglik: -1.9013e+02 - logprior: -1.2597e+00
Fitted a model with MAP estimate = -185.3601
expansions: [(3, 1), (4, 1), (5, 1), (6, 1), (15, 2), (16, 5), (17, 2), (24, 1), (46, 2), (47, 1), (48, 1), (49, 2), (52, 1), (55, 2), (58, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 88 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 190.9818 - loglik: -1.8778e+02 - logprior: -2.9350e+00
Epoch 2/2
19/19 - 2s - loss: 184.3516 - loglik: -1.8271e+02 - logprior: -1.2912e+00
Fitted a model with MAP estimate = -179.8231
expansions: [(20, 1), (21, 1)]
discards: [ 0 23 24 25 26 61]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 189.0797 - loglik: -1.8469e+02 - logprior: -3.9929e+00
Epoch 2/2
19/19 - 2s - loss: 185.2249 - loglik: -1.8268e+02 - logprior: -2.1123e+00
Fitted a model with MAP estimate = -181.0746
expansions: [(0, 3), (27, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 88 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 180.5264 - loglik: -1.7810e+02 - logprior: -1.8748e+00
Epoch 2/10
23/23 - 3s - loss: 177.1928 - loglik: -1.7567e+02 - logprior: -9.9767e-01
Epoch 3/10
23/23 - 3s - loss: 176.3062 - loglik: -1.7476e+02 - logprior: -9.6654e-01
Epoch 4/10
23/23 - 3s - loss: 175.5025 - loglik: -1.7393e+02 - logprior: -9.7816e-01
Epoch 5/10
23/23 - 3s - loss: 174.8081 - loglik: -1.7321e+02 - logprior: -9.7311e-01
Epoch 6/10
23/23 - 3s - loss: 174.6674 - loglik: -1.7309e+02 - logprior: -9.5764e-01
Epoch 7/10
23/23 - 3s - loss: 174.1769 - loglik: -1.7262e+02 - logprior: -9.4674e-01
Epoch 8/10
23/23 - 3s - loss: 174.0750 - loglik: -1.7255e+02 - logprior: -9.2592e-01
Epoch 9/10
23/23 - 3s - loss: 174.2365 - loglik: -1.7275e+02 - logprior: -9.1226e-01
Fitted a model with MAP estimate = -173.1685
Time for alignment: 85.7215
Computed alignments with likelihoods: ['-172.9410', '-173.2208', '-173.1685']
Best model has likelihood: -172.9410
time for generating output: 0.1469
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PDZ.projection.fasta
SP score = 0.8556067588325653
Training of 3 independent models on file asp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f334679f160>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3323c45910>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34403776d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f32f8f110a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f332cdef250>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f32f92d6790>, <__main__.SimpleDirichletPrior object at 0x7f33357d5df0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34b7c770d0>

Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 17s - loss: 796.6220 - loglik: -7.9025e+02 - logprior: -6.2415e+00
Epoch 2/10
22/22 - 14s - loss: 683.4673 - loglik: -6.8189e+02 - logprior: -1.0971e+00
Epoch 3/10
22/22 - 14s - loss: 642.8330 - loglik: -6.3942e+02 - logprior: -2.3572e+00
Epoch 4/10
22/22 - 14s - loss: 638.3657 - loglik: -6.3516e+02 - logprior: -2.1846e+00
Epoch 5/10
22/22 - 14s - loss: 636.4962 - loglik: -6.3349e+02 - logprior: -2.2023e+00
Epoch 6/10
22/22 - 14s - loss: 634.2912 - loglik: -6.3118e+02 - logprior: -2.3004e+00
Epoch 7/10
22/22 - 14s - loss: 633.5682 - loglik: -6.3042e+02 - logprior: -2.3140e+00
Epoch 8/10
22/22 - 14s - loss: 629.6235 - loglik: -6.2648e+02 - logprior: -2.3442e+00
Epoch 9/10
22/22 - 14s - loss: 635.3299 - loglik: -6.3221e+02 - logprior: -2.3599e+00
Fitted a model with MAP estimate = -631.0948
expansions: [(14, 1), (15, 1), (32, 1), (33, 3), (34, 3), (36, 1), (46, 1), (48, 2), (49, 1), (50, 1), (70, 1), (71, 2), (72, 1), (76, 2), (79, 1), (81, 1), (83, 1), (99, 1), (104, 1), (105, 2), (107, 1), (108, 1), (118, 1), (120, 2), (132, 1), (143, 1), (144, 1), (148, 1), (152, 1), (156, 1), (157, 3), (158, 1), (179, 2), (180, 4), (183, 2), (184, 1), (185, 1), (193, 2), (209, 1), (210, 1), (214, 3), (215, 1), (224, 2), (225, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 318 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 22s - loss: 629.0388 - loglik: -6.1878e+02 - logprior: -9.5541e+00
Epoch 2/2
22/22 - 19s - loss: 605.9538 - loglik: -6.0138e+02 - logprior: -3.7645e+00
Fitted a model with MAP estimate = -604.7866
expansions: [(0, 3), (248, 1)]
discards: [  0  35  40 195 196 226 285]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 315 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 23s - loss: 609.6379 - loglik: -6.0264e+02 - logprior: -6.0751e+00
Epoch 2/2
22/22 - 19s - loss: 604.2639 - loglik: -6.0312e+02 - logprior: -2.0073e-01
Fitted a model with MAP estimate = -599.9221
expansions: []
discards: [0 1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 22s - loss: 614.3575 - loglik: -6.0477e+02 - logprior: -8.6414e+00
Epoch 2/10
22/22 - 19s - loss: 607.3055 - loglik: -6.0365e+02 - logprior: -2.7440e+00
Epoch 3/10
22/22 - 19s - loss: 602.6979 - loglik: -6.0074e+02 - logprior: -1.0725e+00
Epoch 4/10
22/22 - 19s - loss: 600.0095 - loglik: -6.0065e+02 - logprior: 1.4895
Epoch 5/10
22/22 - 19s - loss: 599.0353 - loglik: -5.9997e+02 - logprior: 1.8016
Epoch 6/10
22/22 - 19s - loss: 598.6537 - loglik: -5.9980e+02 - logprior: 2.0180
Epoch 7/10
22/22 - 19s - loss: 598.5445 - loglik: -5.9990e+02 - logprior: 2.2543
Epoch 8/10
22/22 - 19s - loss: 598.7419 - loglik: -6.0037e+02 - logprior: 2.5606
Fitted a model with MAP estimate = -596.5692
Time for alignment: 421.2158
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 19s - loss: 796.5992 - loglik: -7.9025e+02 - logprior: -6.2233e+00
Epoch 2/10
22/22 - 14s - loss: 687.2763 - loglik: -6.8567e+02 - logprior: -1.0747e+00
Epoch 3/10
22/22 - 14s - loss: 642.9480 - loglik: -6.3939e+02 - logprior: -2.3664e+00
Epoch 4/10
22/22 - 14s - loss: 637.0173 - loglik: -6.3369e+02 - logprior: -2.2647e+00
Epoch 5/10
22/22 - 14s - loss: 635.0501 - loglik: -6.3179e+02 - logprior: -2.3142e+00
Epoch 6/10
22/22 - 14s - loss: 630.4243 - loglik: -6.2717e+02 - logprior: -2.3256e+00
Epoch 7/10
22/22 - 14s - loss: 634.3195 - loglik: -6.3111e+02 - logprior: -2.3249e+00
Fitted a model with MAP estimate = -630.7215
expansions: [(14, 1), (15, 1), (32, 1), (33, 3), (34, 3), (36, 1), (46, 1), (48, 2), (49, 1), (50, 1), (70, 1), (71, 2), (72, 1), (76, 2), (79, 1), (81, 1), (83, 1), (97, 1), (99, 1), (104, 1), (105, 2), (107, 1), (109, 1), (118, 1), (120, 2), (143, 2), (144, 3), (148, 1), (149, 1), (156, 1), (157, 3), (158, 1), (179, 2), (180, 4), (183, 3), (184, 3), (192, 2), (206, 1), (207, 1), (208, 1), (212, 1), (214, 1), (225, 1), (236, 1), (238, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 319 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 23s - loss: 628.0084 - loglik: -6.1765e+02 - logprior: -9.5354e+00
Epoch 2/2
22/22 - 20s - loss: 610.1624 - loglik: -6.0554e+02 - logprior: -3.7557e+00
Fitted a model with MAP estimate = -605.7347
expansions: [(0, 3), (252, 1)]
discards: [  0  35  40 129 178 179 198 199 229 235]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 313 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 24s - loss: 610.2504 - loglik: -6.0307e+02 - logprior: -6.1829e+00
Epoch 2/2
22/22 - 19s - loss: 602.9116 - loglik: -6.0170e+02 - logprior: -2.3482e-01
Fitted a model with MAP estimate = -600.7915
expansions: []
discards: [0 1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 22s - loss: 614.1578 - loglik: -6.0446e+02 - logprior: -8.7487e+00
Epoch 2/10
22/22 - 19s - loss: 609.2991 - loglik: -6.0564e+02 - logprior: -2.7587e+00
Epoch 3/10
22/22 - 19s - loss: 602.1424 - loglik: -6.0031e+02 - logprior: -9.4476e-01
Epoch 4/10
22/22 - 19s - loss: 601.5917 - loglik: -6.0225e+02 - logprior: 1.4839
Epoch 5/10
22/22 - 19s - loss: 599.6450 - loglik: -6.0058e+02 - logprior: 1.7559
Epoch 6/10
22/22 - 19s - loss: 601.9387 - loglik: -6.0301e+02 - logprior: 1.8925
Fitted a model with MAP estimate = -598.5189
Time for alignment: 356.6131
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 17s - loss: 798.3065 - loglik: -7.9194e+02 - logprior: -6.2377e+00
Epoch 2/10
22/22 - 14s - loss: 685.2104 - loglik: -6.8362e+02 - logprior: -1.1094e+00
Epoch 3/10
22/22 - 14s - loss: 642.6627 - loglik: -6.3908e+02 - logprior: -2.6868e+00
Epoch 4/10
22/22 - 14s - loss: 636.4238 - loglik: -6.3279e+02 - logprior: -2.6515e+00
Epoch 5/10
22/22 - 14s - loss: 636.0842 - loglik: -6.3254e+02 - logprior: -2.6377e+00
Epoch 6/10
22/22 - 14s - loss: 631.9683 - loglik: -6.2849e+02 - logprior: -2.6135e+00
Epoch 7/10
22/22 - 14s - loss: 632.3666 - loglik: -6.2890e+02 - logprior: -2.6297e+00
Fitted a model with MAP estimate = -630.1152
expansions: [(12, 1), (13, 1), (32, 1), (33, 3), (34, 3), (36, 1), (45, 1), (48, 2), (49, 1), (50, 1), (65, 1), (69, 1), (70, 1), (72, 1), (76, 2), (82, 1), (99, 2), (105, 2), (107, 1), (108, 1), (118, 1), (120, 2), (143, 2), (144, 1), (149, 1), (150, 1), (152, 1), (155, 1), (158, 1), (175, 1), (178, 3), (179, 1), (180, 1), (183, 3), (184, 1), (185, 1), (193, 2), (207, 1), (208, 1), (209, 1), (214, 2), (215, 1), (216, 2), (222, 1), (224, 1), (236, 1), (238, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 315 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 24s - loss: 630.3901 - loglik: -6.1996e+02 - logprior: -9.5886e+00
Epoch 2/2
22/22 - 19s - loss: 608.8944 - loglik: -6.0426e+02 - logprior: -3.8000e+00
Fitted a model with MAP estimate = -606.4823
expansions: [(0, 3), (119, 1), (245, 1)]
discards: [  0  35  40 173 218 228 273]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 313 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 22s - loss: 612.8226 - loglik: -6.0560e+02 - logprior: -6.2543e+00
Epoch 2/2
22/22 - 19s - loss: 603.0796 - loglik: -6.0181e+02 - logprior: -2.9228e-01
Fitted a model with MAP estimate = -601.4317
expansions: []
discards: [0 1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 22s - loss: 615.8326 - loglik: -6.0609e+02 - logprior: -8.7660e+00
Epoch 2/10
22/22 - 19s - loss: 605.9841 - loglik: -6.0226e+02 - logprior: -2.8117e+00
Epoch 3/10
22/22 - 19s - loss: 604.8314 - loglik: -6.0284e+02 - logprior: -1.1142e+00
Epoch 4/10
22/22 - 19s - loss: 603.1511 - loglik: -6.0366e+02 - logprior: 1.3464
Epoch 5/10
22/22 - 18s - loss: 598.5923 - loglik: -5.9948e+02 - logprior: 1.6806
Epoch 6/10
22/22 - 19s - loss: 601.0613 - loglik: -6.0213e+02 - logprior: 1.8801
Fitted a model with MAP estimate = -598.6763
Time for alignment: 351.4446
Computed alignments with likelihoods: ['-596.5692', '-598.5189', '-598.6763']
Best model has likelihood: -596.5692
time for generating output: 0.3863
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/asp.projection.fasta
SP score = 0.9064965383996136
Training of 3 independent models on file bowman.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f342c8cbd90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3346354520>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3357288af0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f335fb823d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f347c028a90>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f317c2ce5e0>, <__main__.SimpleDirichletPrior object at 0x7f335faae070>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f32ee51fe50>

Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 172.5583 - loglik: -7.9870e+01 - logprior: -9.2667e+01
Epoch 2/10
10/10 - 1s - loss: 95.4988 - loglik: -6.8621e+01 - logprior: -2.6855e+01
Epoch 3/10
10/10 - 1s - loss: 73.4782 - loglik: -5.9895e+01 - logprior: -1.3573e+01
Epoch 4/10
10/10 - 1s - loss: 64.3451 - loglik: -5.5874e+01 - logprior: -8.4487e+00
Epoch 5/10
10/10 - 1s - loss: 59.6148 - loglik: -5.3772e+01 - logprior: -5.8257e+00
Epoch 6/10
10/10 - 1s - loss: 57.4461 - loglik: -5.3012e+01 - logprior: -4.3963e+00
Epoch 7/10
10/10 - 1s - loss: 56.3130 - loglik: -5.2603e+01 - logprior: -3.5681e+00
Epoch 8/10
10/10 - 1s - loss: 55.6848 - loglik: -5.2458e+01 - logprior: -3.0385e+00
Epoch 9/10
10/10 - 1s - loss: 55.3466 - loglik: -5.2485e+01 - logprior: -2.7121e+00
Epoch 10/10
10/10 - 1s - loss: 55.1249 - loglik: -5.2505e+01 - logprior: -2.4811e+00
Fitted a model with MAP estimate = -54.8885
expansions: [(0, 4), (13, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 172.3840 - loglik: -4.8131e+01 - logprior: -1.2409e+02
Epoch 2/2
10/10 - 1s - loss: 85.9824 - loglik: -4.5239e+01 - logprior: -4.0572e+01
Fitted a model with MAP estimate = -69.0066
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 129.5649 - loglik: -4.1963e+01 - logprior: -8.7395e+01
Epoch 2/10
10/10 - 1s - loss: 68.2067 - loglik: -4.2455e+01 - logprior: -2.5523e+01
Epoch 3/10
10/10 - 1s - loss: 56.1485 - loglik: -4.3144e+01 - logprior: -1.2761e+01
Epoch 4/10
10/10 - 1s - loss: 51.4707 - loglik: -4.3592e+01 - logprior: -7.6215e+00
Epoch 5/10
10/10 - 1s - loss: 49.0691 - loglik: -4.3955e+01 - logprior: -4.8519e+00
Epoch 6/10
10/10 - 1s - loss: 47.7414 - loglik: -4.4186e+01 - logprior: -3.2909e+00
Epoch 7/10
10/10 - 1s - loss: 46.9514 - loglik: -4.4281e+01 - logprior: -2.4002e+00
Epoch 8/10
10/10 - 1s - loss: 46.2361 - loglik: -4.4086e+01 - logprior: -1.8701e+00
Epoch 9/10
10/10 - 1s - loss: 45.3972 - loglik: -4.3585e+01 - logprior: -1.5310e+00
Epoch 10/10
10/10 - 1s - loss: 44.9723 - loglik: -4.3462e+01 - logprior: -1.2401e+00
Fitted a model with MAP estimate = -44.5407
Time for alignment: 32.5121
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 172.5583 - loglik: -7.9870e+01 - logprior: -9.2667e+01
Epoch 2/10
10/10 - 1s - loss: 95.4988 - loglik: -6.8621e+01 - logprior: -2.6855e+01
Epoch 3/10
10/10 - 1s - loss: 73.4782 - loglik: -5.9895e+01 - logprior: -1.3573e+01
Epoch 4/10
10/10 - 1s - loss: 64.3450 - loglik: -5.5874e+01 - logprior: -8.4486e+00
Epoch 5/10
10/10 - 1s - loss: 59.6174 - loglik: -5.3776e+01 - logprior: -5.8241e+00
Epoch 6/10
10/10 - 1s - loss: 57.5111 - loglik: -5.3126e+01 - logprior: -4.3686e+00
Epoch 7/10
10/10 - 1s - loss: 56.3122 - loglik: -5.2685e+01 - logprior: -3.5626e+00
Epoch 8/10
10/10 - 1s - loss: 55.6412 - loglik: -5.2426e+01 - logprior: -3.0444e+00
Epoch 9/10
10/10 - 1s - loss: 55.3043 - loglik: -5.2414e+01 - logprior: -2.7266e+00
Epoch 10/10
10/10 - 1s - loss: 55.0754 - loglik: -5.2437e+01 - logprior: -2.4969e+00
Fitted a model with MAP estimate = -54.8354
expansions: [(0, 4), (13, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 172.3276 - loglik: -4.8042e+01 - logprior: -1.2412e+02
Epoch 2/2
10/10 - 1s - loss: 85.9609 - loglik: -4.5192e+01 - logprior: -4.0590e+01
Fitted a model with MAP estimate = -68.9842
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 129.5745 - loglik: -4.1957e+01 - logprior: -8.7404e+01
Epoch 2/10
10/10 - 1s - loss: 68.2383 - loglik: -4.2476e+01 - logprior: -2.5529e+01
Epoch 3/10
10/10 - 1s - loss: 56.1739 - loglik: -4.3164e+01 - logprior: -1.2765e+01
Epoch 4/10
10/10 - 1s - loss: 51.5066 - loglik: -4.3634e+01 - logprior: -7.6195e+00
Epoch 5/10
10/10 - 1s - loss: 49.0982 - loglik: -4.3996e+01 - logprior: -4.8450e+00
Epoch 6/10
10/10 - 1s - loss: 47.7516 - loglik: -4.4207e+01 - logprior: -3.2847e+00
Epoch 7/10
10/10 - 1s - loss: 46.9521 - loglik: -4.4290e+01 - logprior: -2.3950e+00
Epoch 8/10
10/10 - 1s - loss: 45.9952 - loglik: -4.3844e+01 - logprior: -1.8682e+00
Epoch 9/10
10/10 - 1s - loss: 45.3015 - loglik: -4.3487e+01 - logprior: -1.5380e+00
Epoch 10/10
10/10 - 1s - loss: 44.9329 - loglik: -4.3423e+01 - logprior: -1.2436e+00
Fitted a model with MAP estimate = -44.5137
Time for alignment: 29.0681
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 172.5583 - loglik: -7.9870e+01 - logprior: -9.2667e+01
Epoch 2/10
10/10 - 1s - loss: 95.4988 - loglik: -6.8621e+01 - logprior: -2.6855e+01
Epoch 3/10
10/10 - 1s - loss: 73.4782 - loglik: -5.9895e+01 - logprior: -1.3573e+01
Epoch 4/10
10/10 - 1s - loss: 64.3450 - loglik: -5.5874e+01 - logprior: -8.4486e+00
Epoch 5/10
10/10 - 1s - loss: 59.6175 - loglik: -5.3776e+01 - logprior: -5.8242e+00
Epoch 6/10
10/10 - 1s - loss: 57.5277 - loglik: -5.3153e+01 - logprior: -4.3601e+00
Epoch 7/10
10/10 - 1s - loss: 56.4065 - loglik: -5.2861e+01 - logprior: -3.5258e+00
Epoch 8/10
10/10 - 1s - loss: 55.6586 - loglik: -5.2528e+01 - logprior: -3.0295e+00
Epoch 9/10
10/10 - 1s - loss: 55.3215 - loglik: -5.2432e+01 - logprior: -2.7202e+00
Epoch 10/10
10/10 - 1s - loss: 55.0895 - loglik: -5.2451e+01 - logprior: -2.4952e+00
Fitted a model with MAP estimate = -54.8606
expansions: [(0, 4), (13, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 172.3540 - loglik: -4.8086e+01 - logprior: -1.2411e+02
Epoch 2/2
10/10 - 1s - loss: 85.9766 - loglik: -4.5220e+01 - logprior: -4.0587e+01
Fitted a model with MAP estimate = -68.9937
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 129.5958 - loglik: -4.1987e+01 - logprior: -8.7403e+01
Epoch 2/10
10/10 - 1s - loss: 68.2809 - loglik: -4.2533e+01 - logprior: -2.5522e+01
Epoch 3/10
10/10 - 1s - loss: 56.1987 - loglik: -4.3207e+01 - logprior: -1.2759e+01
Epoch 4/10
10/10 - 1s - loss: 51.5358 - loglik: -4.3677e+01 - logprior: -7.6148e+00
Epoch 5/10
10/10 - 1s - loss: 49.1398 - loglik: -4.4053e+01 - logprior: -4.8386e+00
Epoch 6/10
10/10 - 1s - loss: 47.7701 - loglik: -4.4236e+01 - logprior: -3.2805e+00
Epoch 7/10
10/10 - 1s - loss: 46.9237 - loglik: -4.4266e+01 - logprior: -2.3956e+00
Epoch 8/10
10/10 - 1s - loss: 45.8496 - loglik: -4.3689e+01 - logprior: -1.8784e+00
Epoch 9/10
10/10 - 1s - loss: 45.2586 - loglik: -4.3441e+01 - logprior: -1.5443e+00
Epoch 10/10
10/10 - 1s - loss: 44.9152 - loglik: -4.3400e+01 - logprior: -1.2498e+00
Fitted a model with MAP estimate = -44.5092
Time for alignment: 31.5245
Computed alignments with likelihoods: ['-44.5407', '-44.5137', '-44.5092']
Best model has likelihood: -44.5092
time for generating output: 0.1195
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/bowman.projection.fasta
SP score = 0.9302325581395349
Training of 3 independent models on file oxidored_q6.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f335facbeb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34739ed280>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f346b27db80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f32ef5a37c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3451c07550>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f344038b880>, <__main__.SimpleDirichletPrior object at 0x7f3392afba30>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f347c5b7280>

Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 8s - loss: 362.8657 - loglik: -3.5036e+02 - logprior: -1.2485e+01
Epoch 2/10
11/11 - 2s - loss: 310.1813 - loglik: -3.0703e+02 - logprior: -3.1240e+00
Epoch 3/10
11/11 - 3s - loss: 269.2881 - loglik: -2.6711e+02 - logprior: -2.1519e+00
Epoch 4/10
11/11 - 2s - loss: 248.0665 - loglik: -2.4555e+02 - logprior: -2.3790e+00
Epoch 5/10
11/11 - 2s - loss: 242.9712 - loglik: -2.4005e+02 - logprior: -2.5292e+00
Epoch 6/10
11/11 - 3s - loss: 239.2790 - loglik: -2.3623e+02 - logprior: -2.5112e+00
Epoch 7/10
11/11 - 2s - loss: 238.1048 - loglik: -2.3512e+02 - logprior: -2.4735e+00
Epoch 8/10
11/11 - 2s - loss: 237.2062 - loglik: -2.3427e+02 - logprior: -2.4472e+00
Epoch 9/10
11/11 - 2s - loss: 235.2334 - loglik: -2.3231e+02 - logprior: -2.4501e+00
Epoch 10/10
11/11 - 3s - loss: 236.8952 - loglik: -2.3397e+02 - logprior: -2.4644e+00
Fitted a model with MAP estimate = -235.3145
expansions: [(8, 2), (9, 3), (10, 2), (12, 2), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 2), (62, 2), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 118 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 245.9751 - loglik: -2.3089e+02 - logprior: -1.4624e+01
Epoch 2/2
11/11 - 3s - loss: 222.7136 - loglik: -2.1554e+02 - logprior: -6.6277e+00
Fitted a model with MAP estimate = -217.8841
expansions: [(0, 2)]
discards: [ 0 10 14 18 75 86 89]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 113 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 8s - loss: 221.5303 - loglik: -2.0942e+02 - logprior: -1.1601e+01
Epoch 2/2
11/11 - 3s - loss: 209.5913 - loglik: -2.0595e+02 - logprior: -3.1622e+00
Fitted a model with MAP estimate = -207.9349
expansions: []
discards: [ 0 75]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 221.0806 - loglik: -2.0689e+02 - logprior: -1.3692e+01
Epoch 2/10
11/11 - 3s - loss: 212.5340 - loglik: -2.0789e+02 - logprior: -4.1496e+00
Epoch 3/10
11/11 - 3s - loss: 206.8761 - loglik: -2.0397e+02 - logprior: -2.3978e+00
Epoch 4/10
11/11 - 3s - loss: 207.8617 - loglik: -2.0567e+02 - logprior: -1.6975e+00
Fitted a model with MAP estimate = -206.0306
Time for alignment: 77.5860
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 362.5487 - loglik: -3.5004e+02 - logprior: -1.2488e+01
Epoch 2/10
11/11 - 3s - loss: 309.7722 - loglik: -3.0662e+02 - logprior: -3.1278e+00
Epoch 3/10
11/11 - 2s - loss: 266.4411 - loglik: -2.6426e+02 - logprior: -2.1527e+00
Epoch 4/10
11/11 - 2s - loss: 247.6391 - loglik: -2.4508e+02 - logprior: -2.3265e+00
Epoch 5/10
11/11 - 3s - loss: 241.3571 - loglik: -2.3835e+02 - logprior: -2.4208e+00
Epoch 6/10
11/11 - 3s - loss: 239.5594 - loglik: -2.3653e+02 - logprior: -2.3849e+00
Epoch 7/10
11/11 - 2s - loss: 237.5140 - loglik: -2.3462e+02 - logprior: -2.3270e+00
Epoch 8/10
11/11 - 2s - loss: 235.6194 - loglik: -2.3281e+02 - logprior: -2.3024e+00
Epoch 9/10
11/11 - 3s - loss: 237.7047 - loglik: -2.3494e+02 - logprior: -2.3100e+00
Fitted a model with MAP estimate = -235.5530
expansions: [(8, 2), (9, 3), (10, 2), (12, 2), (17, 1), (33, 1), (34, 1), (38, 3), (61, 1), (62, 3), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 118 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 8s - loss: 247.5254 - loglik: -2.3248e+02 - logprior: -1.4595e+01
Epoch 2/2
11/11 - 3s - loss: 222.3894 - loglik: -2.1522e+02 - logprior: -6.6526e+00
Fitted a model with MAP estimate = -216.5500
expansions: [(0, 2)]
discards: [ 0 10 14 18 77 78 86 89]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 112 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 220.8034 - loglik: -2.0872e+02 - logprior: -1.1588e+01
Epoch 2/2
11/11 - 3s - loss: 210.3099 - loglik: -2.0669e+02 - logprior: -3.1513e+00
Fitted a model with MAP estimate = -207.4777
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 220.9107 - loglik: -2.0678e+02 - logprior: -1.3655e+01
Epoch 2/10
11/11 - 3s - loss: 210.0759 - loglik: -2.0550e+02 - logprior: -4.1069e+00
Epoch 3/10
11/11 - 3s - loss: 208.2798 - loglik: -2.0544e+02 - logprior: -2.3835e+00
Epoch 4/10
11/11 - 3s - loss: 205.6497 - loglik: -2.0352e+02 - logprior: -1.6800e+00
Epoch 5/10
11/11 - 3s - loss: 207.9250 - loglik: -2.0618e+02 - logprior: -1.2978e+00
Fitted a model with MAP estimate = -205.7227
Time for alignment: 75.3448
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 362.7440 - loglik: -3.5024e+02 - logprior: -1.2484e+01
Epoch 2/10
11/11 - 2s - loss: 310.2847 - loglik: -3.0713e+02 - logprior: -3.1252e+00
Epoch 3/10
11/11 - 3s - loss: 267.0569 - loglik: -2.6487e+02 - logprior: -2.1583e+00
Epoch 4/10
11/11 - 2s - loss: 247.1836 - loglik: -2.4462e+02 - logprior: -2.4198e+00
Epoch 5/10
11/11 - 2s - loss: 240.8024 - loglik: -2.3784e+02 - logprior: -2.5672e+00
Epoch 6/10
11/11 - 2s - loss: 237.3778 - loglik: -2.3423e+02 - logprior: -2.6088e+00
Epoch 7/10
11/11 - 2s - loss: 236.2415 - loglik: -2.3305e+02 - logprior: -2.6682e+00
Epoch 8/10
11/11 - 3s - loss: 234.7934 - loglik: -2.3160e+02 - logprior: -2.6880e+00
Epoch 9/10
11/11 - 2s - loss: 234.8228 - loglik: -2.3168e+02 - logprior: -2.6791e+00
Fitted a model with MAP estimate = -233.8663
expansions: [(8, 2), (9, 3), (10, 2), (12, 2), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (63, 1), (65, 1), (66, 1), (67, 1), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 115 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 244.2203 - loglik: -2.2915e+02 - logprior: -1.4603e+01
Epoch 2/2
11/11 - 3s - loss: 220.3869 - loglik: -2.1338e+02 - logprior: -6.4593e+00
Fitted a model with MAP estimate = -215.7951
expansions: [(0, 2)]
discards: [ 0 10 14 18 77]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 112 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 219.7575 - loglik: -2.0765e+02 - logprior: -1.1578e+01
Epoch 2/2
11/11 - 3s - loss: 208.9726 - loglik: -2.0529e+02 - logprior: -3.1812e+00
Fitted a model with MAP estimate = -206.8403
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 112 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 8s - loss: 214.9669 - loglik: -2.0310e+02 - logprior: -1.1365e+01
Epoch 2/10
11/11 - 3s - loss: 207.4688 - loglik: -2.0388e+02 - logprior: -3.1009e+00
Epoch 3/10
11/11 - 3s - loss: 204.4129 - loglik: -2.0208e+02 - logprior: -1.8032e+00
Epoch 4/10
11/11 - 3s - loss: 205.8100 - loglik: -2.0383e+02 - logprior: -1.4779e+00
Fitted a model with MAP estimate = -203.9773
Time for alignment: 73.1647
Computed alignments with likelihoods: ['-206.0306', '-205.7227', '-203.9773']
Best model has likelihood: -203.9773
time for generating output: 0.3038
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/oxidored_q6.projection.fasta
SP score = 0.7158914728682171
Training of 3 independent models on file aldosered.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3436bacac0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34ee10ebb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3323923550>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3495c03fa0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3495c03d30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f32eef25af0>, <__main__.SimpleDirichletPrior object at 0x7f3381979940>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3381fcfa60>

Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 756.0312 - loglik: -7.5374e+02 - logprior: -1.8198e+00
Epoch 2/10
39/39 - 19s - loss: 633.2293 - loglik: -6.2992e+02 - logprior: -1.9451e+00
Epoch 3/10
39/39 - 19s - loss: 621.3655 - loglik: -6.1779e+02 - logprior: -2.0596e+00
Epoch 4/10
39/39 - 19s - loss: 619.1071 - loglik: -6.1579e+02 - logprior: -2.0289e+00
Epoch 5/10
39/39 - 19s - loss: 618.0812 - loglik: -6.1488e+02 - logprior: -2.0348e+00
Epoch 6/10
39/39 - 19s - loss: 617.1105 - loglik: -6.1403e+02 - logprior: -2.0431e+00
Epoch 7/10
39/39 - 19s - loss: 616.4934 - loglik: -6.1344e+02 - logprior: -2.0599e+00
Epoch 8/10
39/39 - 19s - loss: 615.9843 - loglik: -6.1292e+02 - logprior: -2.0675e+00
Epoch 9/10
39/39 - 19s - loss: 615.7193 - loglik: -6.1265e+02 - logprior: -2.0833e+00
Epoch 10/10
39/39 - 19s - loss: 615.0029 - loglik: -6.1197e+02 - logprior: -2.0840e+00
Fitted a model with MAP estimate = -576.3572
expansions: [(12, 4), (13, 1), (16, 1), (17, 1), (36, 1), (37, 1), (39, 1), (45, 3), (46, 1), (58, 6), (59, 4), (60, 1), (66, 1), (90, 1), (119, 1), (124, 1), (125, 1), (126, 4), (127, 1), (128, 1), (137, 3), (138, 1), (139, 1), (140, 3), (141, 2), (142, 2), (144, 2), (145, 1), (157, 1), (158, 2), (159, 2), (160, 1), (164, 3), (165, 1), (166, 2), (167, 3), (168, 2), (169, 1), (180, 2), (191, 1), (193, 1), (194, 1), (203, 1), (209, 1), (211, 2), (212, 2), (217, 1), (218, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 305 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 601.9895 - loglik: -5.9768e+02 - logprior: -3.3338e+00
Epoch 2/2
39/39 - 28s - loss: 583.4178 - loglik: -5.8011e+02 - logprior: -1.8525e+00
Fitted a model with MAP estimate = -549.6093
expansions: [(0, 2), (230, 1)]
discards: [  0  11  12  76 155 173 178 185 187 191 212 221 248 287 289]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 293 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 582.0378 - loglik: -5.7816e+02 - logprior: -2.1536e+00
Epoch 2/2
39/39 - 26s - loss: 579.4153 - loglik: -5.7658e+02 - logprior: -1.1414e+00
Fitted a model with MAP estimate = -547.7902
expansions: []
discards: [  0 198 199 200 201]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 288 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 33s - loss: 547.5944 - loglik: -5.4332e+02 - logprior: -1.9916e+00
Epoch 2/10
45/45 - 29s - loss: 540.1757 - loglik: -5.3710e+02 - logprior: -9.5862e-01
Epoch 3/10
45/45 - 29s - loss: 538.3623 - loglik: -5.3549e+02 - logprior: -8.6500e-01
Epoch 4/10
45/45 - 30s - loss: 536.2905 - loglik: -5.3361e+02 - logprior: -7.8013e-01
Epoch 5/10
45/45 - 30s - loss: 536.8891 - loglik: -5.3445e+02 - logprior: -6.7379e-01
Fitted a model with MAP estimate = -533.3078
Time for alignment: 604.1920
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 753.5068 - loglik: -7.5125e+02 - logprior: -1.7879e+00
Epoch 2/10
39/39 - 19s - loss: 633.1642 - loglik: -6.3030e+02 - logprior: -1.6275e+00
Epoch 3/10
39/39 - 19s - loss: 621.8586 - loglik: -6.1853e+02 - logprior: -1.7515e+00
Epoch 4/10
39/39 - 19s - loss: 617.7343 - loglik: -6.1466e+02 - logprior: -1.7599e+00
Epoch 5/10
39/39 - 19s - loss: 616.2788 - loglik: -6.1333e+02 - logprior: -1.7663e+00
Epoch 6/10
39/39 - 19s - loss: 615.5770 - loglik: -6.1267e+02 - logprior: -1.7772e+00
Epoch 7/10
39/39 - 19s - loss: 614.7621 - loglik: -6.1188e+02 - logprior: -1.8008e+00
Epoch 8/10
39/39 - 19s - loss: 613.5323 - loglik: -6.1062e+02 - logprior: -1.8530e+00
Epoch 9/10
39/39 - 19s - loss: 613.2888 - loglik: -6.1036e+02 - logprior: -1.8635e+00
Epoch 10/10
39/39 - 19s - loss: 612.4121 - loglik: -6.0951e+02 - logprior: -1.8714e+00
Fitted a model with MAP estimate = -573.2335
expansions: [(12, 5), (13, 1), (14, 1), (15, 1), (19, 1), (35, 1), (36, 1), (45, 3), (46, 1), (58, 5), (60, 3), (68, 1), (69, 1), (70, 1), (119, 1), (125, 2), (126, 5), (128, 1), (129, 1), (141, 1), (148, 1), (162, 1), (163, 1), (164, 6), (165, 1), (166, 7), (167, 1), (168, 1), (169, 5), (180, 2), (191, 2), (193, 1), (194, 1), (210, 1), (212, 2), (213, 2), (215, 1), (217, 1), (218, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 298 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 600.1609 - loglik: -5.9580e+02 - logprior: -3.3242e+00
Epoch 2/2
39/39 - 27s - loss: 582.9780 - loglik: -5.7970e+02 - logprior: -1.8121e+00
Fitted a model with MAP estimate = -549.5651
expansions: [(0, 2), (152, 1), (206, 1), (207, 2), (218, 1)]
discards: [  0  11  12 154 155 156 202 203 204 240 253 280 282]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 292 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 32s - loss: 583.5714 - loglik: -5.7974e+02 - logprior: -2.1591e+00
Epoch 2/2
39/39 - 27s - loss: 580.2247 - loglik: -5.7746e+02 - logprior: -1.1492e+00
Fitted a model with MAP estimate = -548.7647
expansions: []
discards: [  0 198 199 200]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 288 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 33s - loss: 548.9804 - loglik: -5.4479e+02 - logprior: -2.0001e+00
Epoch 2/10
45/45 - 29s - loss: 542.0610 - loglik: -5.3912e+02 - logprior: -9.4928e-01
Epoch 3/10
45/45 - 30s - loss: 536.4037 - loglik: -5.3368e+02 - logprior: -8.2380e-01
Epoch 4/10
45/45 - 30s - loss: 538.7119 - loglik: -5.3619e+02 - logprior: -7.5740e-01
Fitted a model with MAP estimate = -534.2239
Time for alignment: 573.7091
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 756.3199 - loglik: -7.5404e+02 - logprior: -1.7968e+00
Epoch 2/10
39/39 - 19s - loss: 635.1846 - loglik: -6.3206e+02 - logprior: -1.7065e+00
Epoch 3/10
39/39 - 19s - loss: 622.0714 - loglik: -6.1852e+02 - logprior: -1.8726e+00
Epoch 4/10
39/39 - 19s - loss: 619.0534 - loglik: -6.1580e+02 - logprior: -1.8719e+00
Epoch 5/10
39/39 - 19s - loss: 617.4056 - loglik: -6.1424e+02 - logprior: -1.8908e+00
Epoch 6/10
39/39 - 19s - loss: 616.4457 - loglik: -6.1330e+02 - logprior: -1.9275e+00
Epoch 7/10
39/39 - 19s - loss: 615.5682 - loglik: -6.1247e+02 - logprior: -1.9326e+00
Epoch 8/10
39/39 - 19s - loss: 614.8123 - loglik: -6.1173e+02 - logprior: -1.9719e+00
Epoch 9/10
39/39 - 19s - loss: 614.2598 - loglik: -6.1119e+02 - logprior: -1.9890e+00
Epoch 10/10
39/39 - 19s - loss: 613.9731 - loglik: -6.1091e+02 - logprior: -1.9995e+00
Fitted a model with MAP estimate = -574.9032
expansions: [(12, 4), (13, 1), (16, 1), (17, 1), (19, 1), (35, 1), (36, 1), (46, 2), (47, 2), (59, 1), (60, 2), (62, 4), (65, 1), (66, 1), (67, 1), (68, 1), (70, 1), (98, 2), (119, 2), (126, 5), (127, 1), (128, 2), (138, 2), (147, 1), (162, 1), (163, 1), (164, 6), (165, 2), (167, 4), (168, 4), (169, 2), (179, 1), (182, 1), (191, 1), (193, 1), (194, 1), (202, 1), (209, 1), (211, 2), (212, 2), (217, 1), (218, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 297 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 32s - loss: 601.7640 - loglik: -5.9739e+02 - logprior: -3.3048e+00
Epoch 2/2
39/39 - 28s - loss: 584.1381 - loglik: -5.8094e+02 - logprior: -1.7758e+00
Fitted a model with MAP estimate = -551.5195
expansions: [(0, 2), (213, 1), (216, 3), (222, 1)]
discards: [  0  11  12  79 123 146 161 176 205 206 207 279 281]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 291 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 583.7852 - loglik: -5.7985e+02 - logprior: -2.2703e+00
Epoch 2/2
39/39 - 27s - loss: 580.3124 - loglik: -5.7748e+02 - logprior: -1.2055e+00
Fitted a model with MAP estimate = -550.5960
expansions: [(201, 2)]
discards: [  0 153]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 291 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 33s - loss: 548.2559 - loglik: -5.4403e+02 - logprior: -2.0434e+00
Epoch 2/10
45/45 - 30s - loss: 541.0460 - loglik: -5.3813e+02 - logprior: -9.0871e-01
Epoch 3/10
45/45 - 30s - loss: 538.2310 - loglik: -5.3545e+02 - logprior: -8.6902e-01
Epoch 4/10
45/45 - 30s - loss: 536.4243 - loglik: -5.3388e+02 - logprior: -7.5729e-01
Epoch 5/10
45/45 - 30s - loss: 536.6431 - loglik: -5.3428e+02 - logprior: -6.8243e-01
Fitted a model with MAP estimate = -533.2319
Time for alignment: 603.2459
Computed alignments with likelihoods: ['-533.3078', '-534.2239', '-533.2319']
Best model has likelihood: -533.2319
time for generating output: 0.3277
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aldosered.projection.fasta
SP score = 0.8844363152643832
Training of 3 independent models on file sdr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34eccf3e20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f348d24a100>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f343fb37790>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f343fb37310>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34edbba700>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34ed6e1730>, <__main__.SimpleDirichletPrior object at 0x7f34ee8745e0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f33a3e49670>

Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 14s - loss: 445.7614 - loglik: -4.4440e+02 - logprior: -1.0883e+00
Epoch 2/10
30/30 - 8s - loss: 374.1907 - loglik: -3.7196e+02 - logprior: -1.2079e+00
Epoch 3/10
30/30 - 8s - loss: 360.0859 - loglik: -3.5779e+02 - logprior: -1.2394e+00
Epoch 4/10
30/30 - 8s - loss: 358.4554 - loglik: -3.5628e+02 - logprior: -1.2271e+00
Epoch 5/10
30/30 - 8s - loss: 358.0120 - loglik: -3.5594e+02 - logprior: -1.2002e+00
Epoch 6/10
30/30 - 8s - loss: 356.5659 - loglik: -3.5459e+02 - logprior: -1.1876e+00
Epoch 7/10
30/30 - 8s - loss: 356.9337 - loglik: -3.5496e+02 - logprior: -1.1766e+00
Fitted a model with MAP estimate = -346.1321
expansions: [(14, 1), (15, 1), (16, 1), (23, 2), (26, 2), (29, 1), (34, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (54, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (88, 1), (93, 1), (94, 1), (97, 1), (99, 1), (113, 3), (114, 1), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 176 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 18s - loss: 345.2654 - loglik: -3.4299e+02 - logprior: -1.3163e+00
Epoch 2/2
61/61 - 14s - loss: 338.6769 - loglik: -3.3651e+02 - logprior: -1.0205e+00
Fitted a model with MAP estimate = -333.6237
expansions: []
discards: [ 25  30  49  52  93  95 146 152]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 19s - loss: 338.9522 - loglik: -3.3661e+02 - logprior: -1.1999e+00
Epoch 2/2
61/61 - 14s - loss: 338.8817 - loglik: -3.3678e+02 - logprior: -9.1569e-01
Fitted a model with MAP estimate = -333.8389
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 22s - loss: 330.2632 - loglik: -3.2781e+02 - logprior: -7.7411e-01
Epoch 2/10
87/87 - 19s - loss: 329.5067 - loglik: -3.2740e+02 - logprior: -6.4358e-01
Epoch 3/10
87/87 - 19s - loss: 327.4667 - loglik: -3.2547e+02 - logprior: -6.1866e-01
Epoch 4/10
87/87 - 19s - loss: 326.5892 - loglik: -3.2454e+02 - logprior: -5.9452e-01
Epoch 5/10
87/87 - 19s - loss: 325.8684 - loglik: -3.2398e+02 - logprior: -5.6266e-01
Epoch 6/10
87/87 - 19s - loss: 325.4698 - loglik: -3.2375e+02 - logprior: -5.3492e-01
Epoch 7/10
87/87 - 19s - loss: 325.0097 - loglik: -3.2335e+02 - logprior: -5.0567e-01
Epoch 8/10
87/87 - 19s - loss: 324.2911 - loglik: -3.2266e+02 - logprior: -4.7911e-01
Epoch 9/10
87/87 - 19s - loss: 324.2304 - loglik: -3.2256e+02 - logprior: -4.5074e-01
Epoch 10/10
87/87 - 19s - loss: 324.3661 - loglik: -3.2292e+02 - logprior: -4.1677e-01
Fitted a model with MAP estimate = -322.6742
Time for alignment: 475.8222
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 11s - loss: 446.0061 - loglik: -4.4466e+02 - logprior: -1.0807e+00
Epoch 2/10
30/30 - 8s - loss: 375.3571 - loglik: -3.7314e+02 - logprior: -1.1811e+00
Epoch 3/10
30/30 - 8s - loss: 362.8077 - loglik: -3.6055e+02 - logprior: -1.1780e+00
Epoch 4/10
30/30 - 8s - loss: 359.9357 - loglik: -3.5775e+02 - logprior: -1.1854e+00
Epoch 5/10
30/30 - 8s - loss: 358.9236 - loglik: -3.5690e+02 - logprior: -1.1697e+00
Epoch 6/10
30/30 - 8s - loss: 358.1442 - loglik: -3.5623e+02 - logprior: -1.1558e+00
Epoch 7/10
30/30 - 8s - loss: 358.0127 - loglik: -3.5610e+02 - logprior: -1.1448e+00
Epoch 8/10
30/30 - 8s - loss: 357.2567 - loglik: -3.5539e+02 - logprior: -1.1420e+00
Epoch 9/10
30/30 - 8s - loss: 357.0981 - loglik: -3.5530e+02 - logprior: -1.1400e+00
Epoch 10/10
30/30 - 8s - loss: 356.8367 - loglik: -3.5503e+02 - logprior: -1.1392e+00
Fitted a model with MAP estimate = -348.8579
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (35, 1), (36, 1), (39, 1), (41, 1), (42, 3), (43, 2), (48, 1), (52, 1), (55, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (92, 1), (93, 1), (94, 1), (97, 1), (99, 1), (101, 2), (113, 2), (114, 2), (116, 2), (117, 1), (125, 1), (128, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 177 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 19s - loss: 344.2006 - loglik: -3.4201e+02 - logprior: -1.3341e+00
Epoch 2/2
61/61 - 14s - loss: 338.4907 - loglik: -3.3634e+02 - logprior: -1.0248e+00
Fitted a model with MAP estimate = -335.8869
expansions: []
discards: [ 25  53  56  92  94 147 150 153]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 17s - loss: 338.7903 - loglik: -3.3645e+02 - logprior: -1.1958e+00
Epoch 2/2
61/61 - 14s - loss: 336.9365 - loglik: -3.3484e+02 - logprior: -9.1438e-01
Fitted a model with MAP estimate = -336.3380
expansions: []
discards: [128]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 23s - loss: 331.3289 - loglik: -3.2878e+02 - logprior: -7.6576e-01
Epoch 2/10
87/87 - 19s - loss: 330.2360 - loglik: -3.2812e+02 - logprior: -6.4057e-01
Epoch 3/10
87/87 - 19s - loss: 327.3541 - loglik: -3.2535e+02 - logprior: -6.0991e-01
Epoch 4/10
87/87 - 19s - loss: 325.9462 - loglik: -3.2384e+02 - logprior: -5.9044e-01
Epoch 5/10
87/87 - 19s - loss: 325.8287 - loglik: -3.2389e+02 - logprior: -5.6291e-01
Epoch 6/10
87/87 - 19s - loss: 324.8451 - loglik: -3.2312e+02 - logprior: -5.3436e-01
Epoch 7/10
87/87 - 19s - loss: 324.5710 - loglik: -3.2289e+02 - logprior: -5.0341e-01
Epoch 8/10
87/87 - 19s - loss: 324.3384 - loglik: -3.2270e+02 - logprior: -4.7378e-01
Epoch 9/10
87/87 - 18s - loss: 323.9706 - loglik: -3.2232e+02 - logprior: -4.4590e-01
Epoch 10/10
87/87 - 19s - loss: 323.9681 - loglik: -3.2250e+02 - logprior: -4.1182e-01
Fitted a model with MAP estimate = -322.4680
Time for alignment: 497.5920
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 11s - loss: 445.4445 - loglik: -4.4408e+02 - logprior: -1.0857e+00
Epoch 2/10
30/30 - 8s - loss: 375.0092 - loglik: -3.7285e+02 - logprior: -1.1723e+00
Epoch 3/10
30/30 - 8s - loss: 362.0749 - loglik: -3.5993e+02 - logprior: -1.1783e+00
Epoch 4/10
30/30 - 8s - loss: 359.5372 - loglik: -3.5736e+02 - logprior: -1.1844e+00
Epoch 5/10
30/30 - 8s - loss: 357.7603 - loglik: -3.5574e+02 - logprior: -1.1816e+00
Epoch 6/10
30/30 - 8s - loss: 357.2363 - loglik: -3.5534e+02 - logprior: -1.1653e+00
Epoch 7/10
30/30 - 8s - loss: 356.8052 - loglik: -3.5492e+02 - logprior: -1.1544e+00
Epoch 8/10
30/30 - 8s - loss: 356.6223 - loglik: -3.5479e+02 - logprior: -1.1482e+00
Epoch 9/10
30/30 - 8s - loss: 356.3152 - loglik: -3.5455e+02 - logprior: -1.1450e+00
Epoch 10/10
30/30 - 8s - loss: 356.5297 - loglik: -3.5477e+02 - logprior: -1.1421e+00
Fitted a model with MAP estimate = -348.0717
expansions: [(14, 1), (15, 1), (16, 2), (20, 1), (26, 2), (29, 1), (36, 1), (38, 1), (39, 2), (41, 2), (42, 3), (43, 2), (48, 1), (52, 1), (54, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (88, 1), (93, 1), (94, 1), (97, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 180 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 17s - loss: 345.0979 - loglik: -3.4292e+02 - logprior: -1.3505e+00
Epoch 2/2
61/61 - 14s - loss: 337.8915 - loglik: -3.3570e+02 - logprior: -1.0540e+00
Fitted a model with MAP estimate = -335.3741
expansions: []
discards: [ 17  30  48  52  57  59  95  97 149 153 156]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 19s - loss: 338.8134 - loglik: -3.3644e+02 - logprior: -1.1863e+00
Epoch 2/2
61/61 - 13s - loss: 337.8217 - loglik: -3.3573e+02 - logprior: -9.0665e-01
Fitted a model with MAP estimate = -335.8257
expansions: []
discards: [128]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 22s - loss: 331.7355 - loglik: -3.2915e+02 - logprior: -7.6924e-01
Epoch 2/10
87/87 - 19s - loss: 329.8194 - loglik: -3.2769e+02 - logprior: -6.3121e-01
Epoch 3/10
87/87 - 19s - loss: 327.3813 - loglik: -3.2536e+02 - logprior: -6.1556e-01
Epoch 4/10
87/87 - 19s - loss: 326.4589 - loglik: -3.2432e+02 - logprior: -5.8901e-01
Epoch 5/10
87/87 - 19s - loss: 325.3109 - loglik: -3.2337e+02 - logprior: -5.6376e-01
Epoch 6/10
87/87 - 19s - loss: 325.2885 - loglik: -3.2356e+02 - logprior: -5.2999e-01
Epoch 7/10
87/87 - 18s - loss: 324.6683 - loglik: -3.2298e+02 - logprior: -5.0089e-01
Epoch 8/10
87/87 - 19s - loss: 324.4240 - loglik: -3.2280e+02 - logprior: -4.6925e-01
Epoch 9/10
87/87 - 19s - loss: 323.9868 - loglik: -3.2232e+02 - logprior: -4.4570e-01
Epoch 10/10
87/87 - 19s - loss: 323.7025 - loglik: -3.2223e+02 - logprior: -4.1066e-01
Fitted a model with MAP estimate = -322.4933
Time for alignment: 497.0342
Computed alignments with likelihoods: ['-322.6742', '-322.4680', '-322.4933']
Best model has likelihood: -322.4680
time for generating output: 0.2748
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sdr.projection.fasta
SP score = 0.6278633327813655
Training of 3 independent models on file p450.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f33232436d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3451f95430>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32efd5efd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3323ed6c70>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33234270a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f32efc1c490>, <__main__.SimpleDirichletPrior object at 0x7f32ed9e76a0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3473aaf9d0>

Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 47s - loss: 1231.1549 - loglik: -1.2288e+03 - logprior: -1.8292e+00
Epoch 2/10
40/40 - 44s - loss: 1097.9851 - loglik: -1.0927e+03 - logprior: -2.9918e+00
Epoch 3/10
40/40 - 44s - loss: 1074.6014 - loglik: -1.0682e+03 - logprior: -3.1870e+00
Epoch 4/10
40/40 - 44s - loss: 1063.7427 - loglik: -1.0563e+03 - logprior: -3.2588e+00
Epoch 5/10
40/40 - 44s - loss: 1057.1440 - loglik: -1.0492e+03 - logprior: -3.3877e+00
Epoch 6/10
40/40 - 44s - loss: 1053.0808 - loglik: -1.0453e+03 - logprior: -3.4336e+00
Epoch 7/10
40/40 - 44s - loss: 1050.2383 - loglik: -1.0429e+03 - logprior: -3.4980e+00
Epoch 8/10
40/40 - 44s - loss: 1047.9780 - loglik: -1.0409e+03 - logprior: -3.5672e+00
Epoch 9/10
40/40 - 44s - loss: 1046.3558 - loglik: -1.0396e+03 - logprior: -3.5562e+00
Epoch 10/10
40/40 - 45s - loss: 1043.7592 - loglik: -1.0371e+03 - logprior: -3.6024e+00
Fitted a model with MAP estimate = -807.9379
expansions: [(122, 1), (153, 1), (175, 1), (231, 7), (330, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 123 159 160 161 162 163 164 180 181 182 183 184 185 186 187
 188 189 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249
 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267
 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285
 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303
 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321
 322 323 324 325 326 327 328 329]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 118 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 22s - loss: 1174.5653 - loglik: -1.1638e+03 - logprior: -6.1417e+00
Epoch 2/2
40/40 - 17s - loss: 1126.9664 - loglik: -1.1157e+03 - logprior: -4.3900e+00
Fitted a model with MAP estimate = -880.9760
expansions: [(0, 2), (118, 3)]
discards: [38 39 40 41 44 46 47 48 49]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 20s - loss: 1116.9685 - loglik: -1.1057e+03 - logprior: -4.9754e+00
Epoch 2/2
40/40 - 17s - loss: 1106.1368 - loglik: -1.0975e+03 - logprior: -4.2471e+00
Fitted a model with MAP estimate = -878.6375
expansions: [(0, 2), (114, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 118 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 26s - loss: 863.4073 - loglik: -8.5836e+02 - logprior: -2.2918e+00
Epoch 2/10
56/56 - 23s - loss: 860.8085 - loglik: -8.5655e+02 - logprior: -1.8926e+00
Epoch 3/10
56/56 - 24s - loss: 856.1771 - loglik: -8.5230e+02 - logprior: -1.8210e+00
Epoch 4/10
56/56 - 24s - loss: 827.4986 - loglik: -8.1724e+02 - logprior: -2.7774e+00
Epoch 5/10
56/56 - 23s - loss: 815.8771 - loglik: -8.0166e+02 - logprior: -2.9419e+00
Epoch 6/10
56/56 - 23s - loss: 808.9374 - loglik: -7.9945e+02 - logprior: -2.9328e+00
Epoch 7/10
56/56 - 23s - loss: 800.2615 - loglik: -7.9326e+02 - logprior: -2.8550e+00
Epoch 8/10
56/56 - 24s - loss: 803.9585 - loglik: -7.9770e+02 - logprior: -2.8013e+00
Fitted a model with MAP estimate = -798.2057
Time for alignment: 914.4660
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 48s - loss: 1231.1268 - loglik: -1.2290e+03 - logprior: -1.6243e+00
Epoch 2/10
40/40 - 44s - loss: 1100.5276 - loglik: -1.0956e+03 - logprior: -2.6090e+00
Epoch 3/10
40/40 - 44s - loss: 1079.5573 - loglik: -1.0724e+03 - logprior: -2.8468e+00
Epoch 4/10
40/40 - 44s - loss: 1065.8948 - loglik: -1.0571e+03 - logprior: -3.2196e+00
Epoch 5/10
40/40 - 44s - loss: 1056.7935 - loglik: -1.0479e+03 - logprior: -3.4345e+00
Epoch 6/10
40/40 - 44s - loss: 1050.9341 - loglik: -1.0424e+03 - logprior: -3.6249e+00
Epoch 7/10
40/40 - 44s - loss: 1048.2810 - loglik: -1.0404e+03 - logprior: -3.6671e+00
Epoch 8/10
40/40 - 44s - loss: 1045.1683 - loglik: -1.0378e+03 - logprior: -3.6962e+00
Epoch 9/10
40/40 - 44s - loss: 1043.6382 - loglik: -1.0363e+03 - logprior: -3.7438e+00
Epoch 10/10
40/40 - 44s - loss: 1040.6106 - loglik: -1.0328e+03 - logprior: -3.7911e+00
Fitted a model with MAP estimate = -811.5354
expansions: [(125, 1), (330, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 159 160 161 162 163 180 181 182 183 184 185 186 187 188 212
 213 214 215 216 217 218 219 247 248 249 250 251 252 253 254 255 256 257
 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275
 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293
 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311
 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 117 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 20s - loss: 1164.5334 - loglik: -1.1521e+03 - logprior: -6.5982e+00
Epoch 2/2
40/40 - 17s - loss: 1125.3726 - loglik: -1.1122e+03 - logprior: -6.3854e+00
Fitted a model with MAP estimate = -896.0869
expansions: [(0, 2), (117, 1)]
discards: [ 14  15  16  21  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  65  66  92 108 109 110 111]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 94 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 21s - loss: 1118.1610 - loglik: -1.1069e+03 - logprior: -6.6044e+00
Epoch 2/2
40/40 - 17s - loss: 1113.2344 - loglik: -1.1043e+03 - logprior: -5.8912e+00
Fitted a model with MAP estimate = -875.1019
expansions: [(0, 2), (94, 1)]
discards: [93]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 26s - loss: 864.8760 - loglik: -8.5923e+02 - logprior: -3.4625e+00
Epoch 2/10
56/56 - 23s - loss: 862.2773 - loglik: -8.5697e+02 - logprior: -3.4018e+00
Epoch 3/10
56/56 - 23s - loss: 857.5164 - loglik: -8.5255e+02 - logprior: -3.3166e+00
Epoch 4/10
56/56 - 23s - loss: 842.0909 - loglik: -8.3174e+02 - logprior: -3.7678e+00
Epoch 5/10
56/56 - 23s - loss: 820.3836 - loglik: -8.0541e+02 - logprior: -3.7980e+00
Epoch 6/10
56/56 - 22s - loss: 814.0778 - loglik: -8.0361e+02 - logprior: -3.7671e+00
Epoch 7/10
56/56 - 22s - loss: 813.4467 - loglik: -8.0548e+02 - logprior: -3.7424e+00
Epoch 8/10
56/56 - 23s - loss: 807.2864 - loglik: -8.0016e+02 - logprior: -3.6769e+00
Epoch 9/10
56/56 - 23s - loss: 812.0375 - loglik: -8.0545e+02 - logprior: -3.6758e+00
Fitted a model with MAP estimate = -805.4468
Time for alignment: 923.9007
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 47s - loss: 1224.0481 - loglik: -1.2216e+03 - logprior: -1.8890e+00
Epoch 2/10
40/40 - 44s - loss: 1089.4139 - loglik: -1.0844e+03 - logprior: -3.0607e+00
Epoch 3/10
40/40 - 44s - loss: 1069.3223 - loglik: -1.0628e+03 - logprior: -3.1064e+00
Epoch 4/10
40/40 - 44s - loss: 1059.8694 - loglik: -1.0524e+03 - logprior: -3.1871e+00
Epoch 5/10
40/40 - 44s - loss: 1054.8015 - loglik: -1.0470e+03 - logprior: -3.2900e+00
Epoch 6/10
40/40 - 44s - loss: 1051.1378 - loglik: -1.0435e+03 - logprior: -3.3684e+00
Epoch 7/10
40/40 - 44s - loss: 1048.5596 - loglik: -1.0414e+03 - logprior: -3.3823e+00
Epoch 8/10
40/40 - 44s - loss: 1046.2113 - loglik: -1.0394e+03 - logprior: -3.4175e+00
Epoch 9/10
40/40 - 44s - loss: 1044.6111 - loglik: -1.0379e+03 - logprior: -3.4254e+00
Epoch 10/10
40/40 - 44s - loss: 1042.3942 - loglik: -1.0356e+03 - logprior: -3.4583e+00
Fitted a model with MAP estimate = -809.5863
expansions: [(125, 1), (175, 2), (224, 1), (228, 8), (330, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 123 159 160 161 162 163 179 180 181 182 183 184 185 186 187
 188 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247
 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265
 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283
 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301
 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319
 320 321 322 323 324 325 326 327 328 329]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 118 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 21s - loss: 1172.7329 - loglik: -1.1615e+03 - logprior: -6.3642e+00
Epoch 2/2
40/40 - 17s - loss: 1124.8834 - loglik: -1.1129e+03 - logprior: -4.7641e+00
Fitted a model with MAP estimate = -881.2194
expansions: [(0, 3), (118, 3)]
discards: [20 39 43 45 46 47 48 49 65]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 115 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 20s - loss: 1112.7649 - loglik: -1.1023e+03 - logprior: -5.0113e+00
Epoch 2/2
40/40 - 17s - loss: 1104.0181 - loglik: -1.0957e+03 - logprior: -4.4750e+00
Fitted a model with MAP estimate = -872.4239
expansions: [(0, 2), (115, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 118 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 29s - loss: 864.2015 - loglik: -8.5944e+02 - logprior: -2.2394e+00
Epoch 2/10
56/56 - 23s - loss: 856.8430 - loglik: -8.5270e+02 - logprior: -1.9069e+00
Epoch 3/10
56/56 - 24s - loss: 856.8690 - loglik: -8.5298e+02 - logprior: -1.8233e+00
Fitted a model with MAP estimate = -846.0769
Time for alignment: 795.9705
Computed alignments with likelihoods: ['-798.2057', '-805.4468', '-809.5863']
Best model has likelihood: -798.2057
time for generating output: 0.4044
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/p450.projection.fasta
SP score = 0.03055726917540613
Training of 3 independent models on file hla.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f335fd08670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f33a32be820>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f337962b490>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34ecb0c1c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34ecb0ca90>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34849b5c40>, <__main__.SimpleDirichletPrior object at 0x7f3451b5fd60>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f33240ea8b0>

Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 449.2306 - loglik: -4.4623e+02 - logprior: -2.9855e+00
Epoch 2/10
19/19 - 5s - loss: 282.5152 - loglik: -2.8085e+02 - logprior: -1.4353e+00
Epoch 3/10
19/19 - 5s - loss: 214.7738 - loglik: -2.1284e+02 - logprior: -1.8243e+00
Epoch 4/10
19/19 - 5s - loss: 202.3516 - loglik: -2.0005e+02 - logprior: -2.0926e+00
Epoch 5/10
19/19 - 5s - loss: 197.3271 - loglik: -1.9467e+02 - logprior: -2.2357e+00
Epoch 6/10
19/19 - 5s - loss: 195.0752 - loglik: -1.9214e+02 - logprior: -2.4433e+00
Epoch 7/10
19/19 - 5s - loss: 193.4368 - loglik: -1.9060e+02 - logprior: -2.3797e+00
Epoch 8/10
19/19 - 5s - loss: 191.7899 - loglik: -1.8896e+02 - logprior: -2.3778e+00
Epoch 9/10
19/19 - 5s - loss: 192.0314 - loglik: -1.8924e+02 - logprior: -2.3793e+00
Fitted a model with MAP estimate = -186.2262
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (33, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 1), (102, 1), (111, 2), (114, 1), (121, 1), (122, 3), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 164.8369 - loglik: -1.6111e+02 - logprior: -3.3661e+00
Epoch 2/2
19/19 - 6s - loss: 131.0748 - loglik: -1.2913e+02 - logprior: -1.5991e+00
Fitted a model with MAP estimate = -136.6923
expansions: []
discards: [ 50  76 138 154]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 130.1947 - loglik: -1.2655e+02 - logprior: -3.2643e+00
Epoch 2/2
19/19 - 6s - loss: 126.3425 - loglik: -1.2451e+02 - logprior: -1.4431e+00
Fitted a model with MAP estimate = -136.2386
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 11s - loss: 136.1725 - loglik: -1.3350e+02 - logprior: -2.0879e+00
Epoch 2/10
22/22 - 7s - loss: 132.1934 - loglik: -1.3053e+02 - logprior: -1.0511e+00
Epoch 3/10
22/22 - 7s - loss: 128.9450 - loglik: -1.2718e+02 - logprior: -1.1633e+00
Epoch 4/10
22/22 - 7s - loss: 129.1228 - loglik: -1.2730e+02 - logprior: -1.1805e+00
Fitted a model with MAP estimate = -126.8310
Time for alignment: 154.0845
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 449.4425 - loglik: -4.4644e+02 - logprior: -2.9800e+00
Epoch 2/10
19/19 - 5s - loss: 281.4371 - loglik: -2.7975e+02 - logprior: -1.4449e+00
Epoch 3/10
19/19 - 5s - loss: 212.5014 - loglik: -2.1058e+02 - logprior: -1.8266e+00
Epoch 4/10
19/19 - 5s - loss: 201.2428 - loglik: -1.9896e+02 - logprior: -2.1000e+00
Epoch 5/10
19/19 - 5s - loss: 195.1213 - loglik: -1.9243e+02 - logprior: -2.3130e+00
Epoch 6/10
19/19 - 5s - loss: 193.1017 - loglik: -1.9022e+02 - logprior: -2.4450e+00
Epoch 7/10
19/19 - 5s - loss: 190.9188 - loglik: -1.8810e+02 - logprior: -2.3793e+00
Epoch 8/10
19/19 - 5s - loss: 191.1230 - loglik: -1.8834e+02 - logprior: -2.3802e+00
Fitted a model with MAP estimate = -184.6380
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (71, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (100, 1), (102, 1), (104, 1), (114, 1), (120, 2), (121, 1), (122, 1), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 164.2617 - loglik: -1.6055e+02 - logprior: -3.3505e+00
Epoch 2/2
19/19 - 6s - loss: 130.8605 - loglik: -1.2896e+02 - logprior: -1.5694e+00
Fitted a model with MAP estimate = -136.3935
expansions: []
discards: [ 50  76 149]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 130.1110 - loglik: -1.2646e+02 - logprior: -3.2659e+00
Epoch 2/2
19/19 - 6s - loss: 126.2518 - loglik: -1.2440e+02 - logprior: -1.4417e+00
Fitted a model with MAP estimate = -136.0727
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 10s - loss: 136.0754 - loglik: -1.3338e+02 - logprior: -2.1021e+00
Epoch 2/10
22/22 - 7s - loss: 132.2986 - loglik: -1.3060e+02 - logprior: -1.0711e+00
Epoch 3/10
22/22 - 7s - loss: 129.5878 - loglik: -1.2778e+02 - logprior: -1.1893e+00
Epoch 4/10
22/22 - 7s - loss: 126.6226 - loglik: -1.2480e+02 - logprior: -1.1702e+00
Epoch 5/10
22/22 - 7s - loss: 128.3452 - loglik: -1.2651e+02 - logprior: -1.1426e+00
Fitted a model with MAP estimate = -126.2336
Time for alignment: 154.8151
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 449.2137 - loglik: -4.4622e+02 - logprior: -2.9795e+00
Epoch 2/10
19/19 - 5s - loss: 285.1802 - loglik: -2.8351e+02 - logprior: -1.4341e+00
Epoch 3/10
19/19 - 5s - loss: 214.1086 - loglik: -2.1216e+02 - logprior: -1.8458e+00
Epoch 4/10
19/19 - 5s - loss: 201.3594 - loglik: -1.9900e+02 - logprior: -2.1365e+00
Epoch 5/10
19/19 - 5s - loss: 196.6024 - loglik: -1.9393e+02 - logprior: -2.2475e+00
Epoch 6/10
19/19 - 5s - loss: 193.3722 - loglik: -1.9037e+02 - logprior: -2.5327e+00
Epoch 7/10
19/19 - 5s - loss: 191.9891 - loglik: -1.8910e+02 - logprior: -2.4112e+00
Epoch 8/10
19/19 - 5s - loss: 191.4591 - loglik: -1.8860e+02 - logprior: -2.4131e+00
Epoch 9/10
19/19 - 5s - loss: 190.7582 - loglik: -1.8792e+02 - logprior: -2.4153e+00
Epoch 10/10
19/19 - 5s - loss: 190.6786 - loglik: -1.8785e+02 - logprior: -2.4061e+00
Fitted a model with MAP estimate = -185.5017
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (62, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (99, 1), (101, 1), (111, 2), (114, 2), (121, 1), (122, 2), (123, 1), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 165.7398 - loglik: -1.6197e+02 - logprior: -3.3850e+00
Epoch 2/2
19/19 - 6s - loss: 130.6995 - loglik: -1.2876e+02 - logprior: -1.6030e+00
Fitted a model with MAP estimate = -136.7773
expansions: []
discards: [ 50  76 138 143 154]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 129.9072 - loglik: -1.2627e+02 - logprior: -3.2625e+00
Epoch 2/2
19/19 - 6s - loss: 127.0753 - loglik: -1.2525e+02 - logprior: -1.4380e+00
Fitted a model with MAP estimate = -136.5807
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 12s - loss: 136.1955 - loglik: -1.3356e+02 - logprior: -2.0912e+00
Epoch 2/10
22/22 - 7s - loss: 132.3516 - loglik: -1.3070e+02 - logprior: -1.0359e+00
Epoch 3/10
22/22 - 7s - loss: 130.9891 - loglik: -1.2922e+02 - logprior: -1.1738e+00
Epoch 4/10
22/22 - 7s - loss: 126.6866 - loglik: -1.2488e+02 - logprior: -1.1717e+00
Epoch 5/10
22/22 - 7s - loss: 127.3504 - loglik: -1.2556e+02 - logprior: -1.1363e+00
Fitted a model with MAP estimate = -126.2761
Time for alignment: 166.6717
Computed alignments with likelihoods: ['-126.8310', '-126.2336', '-126.2761']
Best model has likelihood: -126.2336
time for generating output: 0.2282
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hla.projection.fasta
SP score = 1.0
Training of 3 independent models on file ltn.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f333de9dd60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f342d3729d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f342d3729a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f332cf56760>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f335f902c40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f347c5095b0>, <__main__.SimpleDirichletPrior object at 0x7f334674b5e0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f347c66d8b0>

Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 608.0959 - loglik: -5.7483e+02 - logprior: -3.3248e+01
Epoch 2/10
12/12 - 4s - loss: 524.1031 - loglik: -5.1956e+02 - logprior: -4.5144e+00
Epoch 3/10
12/12 - 4s - loss: 464.0013 - loglik: -4.6194e+02 - logprior: -1.9068e+00
Epoch 4/10
12/12 - 4s - loss: 436.4419 - loglik: -4.3449e+02 - logprior: -1.4089e+00
Epoch 5/10
12/12 - 4s - loss: 429.0935 - loglik: -4.2746e+02 - logprior: -1.0121e+00
Epoch 6/10
12/12 - 4s - loss: 426.2717 - loglik: -4.2497e+02 - logprior: -8.1039e-01
Epoch 7/10
12/12 - 4s - loss: 422.3563 - loglik: -4.2125e+02 - logprior: -6.9073e-01
Epoch 8/10
12/12 - 4s - loss: 421.8241 - loglik: -4.2074e+02 - logprior: -6.9026e-01
Epoch 9/10
12/12 - 4s - loss: 420.1988 - loglik: -4.1909e+02 - logprior: -6.9541e-01
Epoch 10/10
12/12 - 4s - loss: 419.7571 - loglik: -4.1870e+02 - logprior: -6.1667e-01
Fitted a model with MAP estimate = -418.6012
expansions: [(11, 3), (21, 1), (32, 2), (33, 1), (39, 1), (41, 1), (44, 1), (60, 1), (61, 1), (62, 1), (63, 1), (75, 3), (76, 1), (77, 2), (86, 1), (90, 1), (91, 1), (92, 1), (101, 1), (102, 1), (127, 6), (129, 1), (131, 1), (138, 1), (140, 1), (147, 3), (149, 2), (151, 1), (154, 1), (173, 1), (174, 2), (175, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 230 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 449.9869 - loglik: -4.1050e+02 - logprior: -3.9041e+01
Epoch 2/2
12/12 - 4s - loss: 412.9097 - loglik: -3.9922e+02 - logprior: -1.3180e+01
Fitted a model with MAP estimate = -407.1557
expansions: [(0, 2), (207, 1)]
discards: [  0  94 185 187 217]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 228 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 425.7899 - loglik: -3.9612e+02 - logprior: -2.9099e+01
Epoch 2/2
12/12 - 4s - loss: 398.5518 - loglik: -3.9484e+02 - logprior: -3.0974e+00
Fitted a model with MAP estimate = -393.4578
expansions: [(184, 1)]
discards: [  1 152 153]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 226 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 423.9486 - loglik: -3.9500e+02 - logprior: -2.8328e+01
Epoch 2/10
12/12 - 4s - loss: 397.4843 - loglik: -3.9421e+02 - logprior: -2.6554e+00
Epoch 3/10
12/12 - 4s - loss: 392.6128 - loglik: -3.9419e+02 - logprior: 2.2182
Epoch 4/10
12/12 - 4s - loss: 389.8915 - loglik: -3.9352e+02 - logprior: 4.2867
Epoch 5/10
12/12 - 4s - loss: 388.1924 - loglik: -3.9296e+02 - logprior: 5.4359
Epoch 6/10
12/12 - 4s - loss: 388.7653 - loglik: -3.9418e+02 - logprior: 6.0854
Fitted a model with MAP estimate = -387.5590
Time for alignment: 106.7360
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 608.5083 - loglik: -5.7526e+02 - logprior: -3.3227e+01
Epoch 2/10
12/12 - 4s - loss: 524.4468 - loglik: -5.1990e+02 - logprior: -4.5222e+00
Epoch 3/10
12/12 - 4s - loss: 461.6958 - loglik: -4.5964e+02 - logprior: -1.9542e+00
Epoch 4/10
12/12 - 4s - loss: 434.3745 - loglik: -4.3247e+02 - logprior: -1.6097e+00
Epoch 5/10
12/12 - 4s - loss: 425.9198 - loglik: -4.2420e+02 - logprior: -1.1949e+00
Epoch 6/10
12/12 - 4s - loss: 421.4121 - loglik: -4.2021e+02 - logprior: -6.9892e-01
Epoch 7/10
12/12 - 4s - loss: 421.3329 - loglik: -4.2028e+02 - logprior: -6.0129e-01
Epoch 8/10
12/12 - 4s - loss: 420.3604 - loglik: -4.1944e+02 - logprior: -4.8419e-01
Epoch 9/10
12/12 - 4s - loss: 419.6255 - loglik: -4.1882e+02 - logprior: -3.9569e-01
Epoch 10/10
12/12 - 4s - loss: 418.7667 - loglik: -4.1797e+02 - logprior: -3.6963e-01
Fitted a model with MAP estimate = -417.9771
expansions: [(11, 3), (19, 1), (30, 1), (31, 2), (32, 1), (41, 1), (44, 1), (58, 1), (59, 1), (61, 2), (62, 2), (76, 2), (77, 2), (86, 1), (90, 1), (91, 1), (92, 1), (101, 1), (102, 1), (127, 4), (129, 3), (137, 1), (145, 1), (146, 3), (148, 2), (153, 1), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 229 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 9s - loss: 449.5405 - loglik: -4.1012e+02 - logprior: -3.9014e+01
Epoch 2/2
12/12 - 4s - loss: 412.4584 - loglik: -3.9894e+02 - logprior: -1.3068e+01
Fitted a model with MAP estimate = -406.5624
expansions: [(0, 3)]
discards: [  0  75  92 183 184 185 215]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 225 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 426.7531 - loglik: -3.9712e+02 - logprior: -2.9131e+01
Epoch 2/2
12/12 - 4s - loss: 400.6496 - loglik: -3.9700e+02 - logprior: -3.1188e+00
Fitted a model with MAP estimate = -395.4191
expansions: [(182, 2)]
discards: [1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 225 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 424.1913 - loglik: -3.9530e+02 - logprior: -2.8348e+01
Epoch 2/10
12/12 - 4s - loss: 396.2433 - loglik: -3.9305e+02 - logprior: -2.6227e+00
Epoch 3/10
12/12 - 4s - loss: 394.1409 - loglik: -3.9576e+02 - logprior: 2.1842
Epoch 4/10
12/12 - 4s - loss: 389.7871 - loglik: -3.9342e+02 - logprior: 4.2296
Epoch 5/10
12/12 - 4s - loss: 392.0680 - loglik: -3.9687e+02 - logprior: 5.3864
Fitted a model with MAP estimate = -389.0699
Time for alignment: 100.7756
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 9s - loss: 606.8600 - loglik: -5.7360e+02 - logprior: -3.3241e+01
Epoch 2/10
12/12 - 4s - loss: 524.0438 - loglik: -5.1951e+02 - logprior: -4.5021e+00
Epoch 3/10
12/12 - 4s - loss: 461.9452 - loglik: -4.5995e+02 - logprior: -1.8528e+00
Epoch 4/10
12/12 - 4s - loss: 433.1497 - loglik: -4.3119e+02 - logprior: -1.4099e+00
Epoch 5/10
12/12 - 4s - loss: 428.2012 - loglik: -4.2657e+02 - logprior: -9.6409e-01
Epoch 6/10
12/12 - 4s - loss: 424.5591 - loglik: -4.2345e+02 - logprior: -5.9431e-01
Epoch 7/10
12/12 - 4s - loss: 419.5569 - loglik: -4.1862e+02 - logprior: -5.2252e-01
Epoch 8/10
12/12 - 4s - loss: 419.8049 - loglik: -4.1890e+02 - logprior: -5.0274e-01
Fitted a model with MAP estimate = -418.7016
expansions: [(11, 3), (25, 1), (31, 4), (32, 1), (41, 1), (59, 1), (62, 2), (63, 2), (75, 2), (76, 2), (77, 2), (86, 1), (90, 1), (92, 1), (101, 1), (102, 1), (121, 2), (126, 5), (127, 1), (129, 2), (130, 2), (137, 1), (146, 3), (149, 2), (153, 1), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 233 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 451.8492 - loglik: -4.1228e+02 - logprior: -3.9206e+01
Epoch 2/2
12/12 - 4s - loss: 413.8289 - loglik: -3.9990e+02 - logprior: -1.3499e+01
Fitted a model with MAP estimate = -406.7109
expansions: [(0, 2)]
discards: [  0  36 146 155 165 190 219]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 228 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 426.4341 - loglik: -3.9653e+02 - logprior: -2.9394e+01
Epoch 2/2
12/12 - 4s - loss: 396.4697 - loglik: -3.9256e+02 - logprior: -3.3425e+00
Fitted a model with MAP estimate = -393.8205
expansions: [(108, 1), (183, 2)]
discards: [  1  89  93 152 153 184 185]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 224 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 9s - loss: 425.4246 - loglik: -3.9637e+02 - logprior: -2.8478e+01
Epoch 2/10
12/12 - 4s - loss: 398.2164 - loglik: -3.9481e+02 - logprior: -2.8191e+00
Epoch 3/10
12/12 - 4s - loss: 392.5359 - loglik: -3.9396e+02 - logprior: 2.0233
Epoch 4/10
12/12 - 4s - loss: 391.5653 - loglik: -3.9509e+02 - logprior: 4.1313
Epoch 5/10
12/12 - 4s - loss: 389.3802 - loglik: -3.9405e+02 - logprior: 5.3010
Epoch 6/10
12/12 - 4s - loss: 390.4067 - loglik: -3.9570e+02 - logprior: 5.9319
Fitted a model with MAP estimate = -388.6966
Time for alignment: 99.8460
Computed alignments with likelihoods: ['-387.5590', '-389.0699', '-388.6966']
Best model has likelihood: -387.5590
time for generating output: 0.2953
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ltn.projection.fasta
SP score = 0.9420959630284084
Training of 3 independent models on file aadh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f32ee86ed90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f32ee861f40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f31186feee0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3335612bb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3335612a30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3451afec40>, <__main__.SimpleDirichletPrior object at 0x7f3392dbdd00>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3484a045e0>

Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 11s - loss: 605.4180 - loglik: -5.9769e+02 - logprior: -7.7020e+00
Epoch 2/10
21/21 - 8s - loss: 477.9836 - loglik: -4.7559e+02 - logprior: -2.3529e+00
Epoch 3/10
21/21 - 8s - loss: 433.0963 - loglik: -4.2926e+02 - logprior: -3.3092e+00
Epoch 4/10
21/21 - 8s - loss: 426.0967 - loglik: -4.2230e+02 - logprior: -3.2352e+00
Epoch 5/10
21/21 - 9s - loss: 423.2979 - loglik: -4.1963e+02 - logprior: -3.1567e+00
Epoch 6/10
21/21 - 8s - loss: 421.8492 - loglik: -4.1817e+02 - logprior: -3.1641e+00
Epoch 7/10
21/21 - 8s - loss: 421.2090 - loglik: -4.1751e+02 - logprior: -3.1675e+00
Epoch 8/10
21/21 - 8s - loss: 421.5145 - loglik: -4.1781e+02 - logprior: -3.1669e+00
Fitted a model with MAP estimate = -420.0644
expansions: [(15, 1), (16, 3), (19, 1), (20, 2), (21, 3), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (40, 1), (50, 1), (60, 1), (63, 4), (65, 1), (75, 5), (76, 2), (78, 1), (97, 1), (100, 2), (102, 1), (118, 2), (119, 1), (120, 1), (121, 1), (143, 1), (145, 2), (147, 1), (154, 1), (160, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (175, 1), (177, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 243 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 14s - loss: 413.0055 - loglik: -4.0179e+02 - logprior: -1.0692e+01
Epoch 2/2
21/21 - 11s - loss: 388.4167 - loglik: -3.8345e+02 - logprior: -4.3413e+00
Fitted a model with MAP estimate = -383.5347
expansions: [(0, 2)]
discards: [  0  17  28  82  86 133 153 219]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 16s - loss: 389.5745 - loglik: -3.8176e+02 - logprior: -7.1658e+00
Epoch 2/2
21/21 - 10s - loss: 379.5577 - loglik: -3.7803e+02 - logprior: -8.8748e-01
Fitted a model with MAP estimate = -378.4425
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 13s - loss: 391.7320 - loglik: -3.8137e+02 - logprior: -9.7179e+00
Epoch 2/10
21/21 - 10s - loss: 381.6870 - loglik: -3.7980e+02 - logprior: -1.2100e+00
Epoch 3/10
21/21 - 10s - loss: 378.7350 - loglik: -3.7842e+02 - logprior: 0.3530
Epoch 4/10
21/21 - 10s - loss: 379.4506 - loglik: -3.7949e+02 - logprior: 0.7090
Fitted a model with MAP estimate = -377.5986
Time for alignment: 196.4590
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 13s - loss: 604.5718 - loglik: -5.9690e+02 - logprior: -7.6475e+00
Epoch 2/10
21/21 - 8s - loss: 478.2371 - loglik: -4.7591e+02 - logprior: -2.2778e+00
Epoch 3/10
21/21 - 8s - loss: 439.1915 - loglik: -4.3557e+02 - logprior: -3.1920e+00
Epoch 4/10
21/21 - 8s - loss: 433.1721 - loglik: -4.2961e+02 - logprior: -3.0649e+00
Epoch 5/10
21/21 - 9s - loss: 430.9250 - loglik: -4.2743e+02 - logprior: -2.9840e+00
Epoch 6/10
21/21 - 8s - loss: 429.3205 - loglik: -4.2583e+02 - logprior: -2.9819e+00
Epoch 7/10
21/21 - 8s - loss: 429.9391 - loglik: -4.2644e+02 - logprior: -2.9668e+00
Fitted a model with MAP estimate = -428.0026
expansions: [(14, 1), (17, 2), (20, 1), (21, 2), (22, 3), (23, 1), (37, 1), (38, 1), (40, 2), (41, 1), (47, 1), (50, 1), (60, 1), (63, 3), (66, 1), (76, 7), (78, 1), (97, 1), (98, 1), (100, 1), (115, 1), (116, 1), (119, 1), (121, 1), (122, 1), (144, 1), (147, 1), (149, 1), (151, 1), (159, 1), (161, 1), (162, 1), (164, 1), (165, 2), (166, 2), (167, 1), (169, 2), (170, 1), (171, 1), (177, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 243 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 14s - loss: 420.7858 - loglik: -4.0951e+02 - logprior: -1.0752e+01
Epoch 2/2
21/21 - 10s - loss: 396.4735 - loglik: -3.9145e+02 - logprior: -4.3918e+00
Fitted a model with MAP estimate = -391.5659
expansions: [(0, 2), (98, 1)]
discards: [  0  17  28  86 211 219]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 240 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 14s - loss: 395.6651 - loglik: -3.8767e+02 - logprior: -7.2899e+00
Epoch 2/2
21/21 - 10s - loss: 387.1978 - loglik: -3.8544e+02 - logprior: -1.0494e+00
Fitted a model with MAP estimate = -384.9193
expansions: []
discards: [ 0 79]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 238 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 15s - loss: 398.8124 - loglik: -3.8820e+02 - logprior: -9.9015e+00
Epoch 2/10
21/21 - 10s - loss: 388.1861 - loglik: -3.8605e+02 - logprior: -1.4275e+00
Epoch 3/10
21/21 - 10s - loss: 384.7964 - loglik: -3.8425e+02 - logprior: 0.1547
Epoch 4/10
21/21 - 10s - loss: 385.3813 - loglik: -3.8524e+02 - logprior: 0.5425
Fitted a model with MAP estimate = -383.7166
Time for alignment: 191.3208
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 604.4909 - loglik: -5.9681e+02 - logprior: -7.6604e+00
Epoch 2/10
21/21 - 9s - loss: 483.2174 - loglik: -4.8097e+02 - logprior: -2.2244e+00
Epoch 3/10
21/21 - 8s - loss: 442.5511 - loglik: -4.3909e+02 - logprior: -3.0997e+00
Epoch 4/10
21/21 - 8s - loss: 435.3585 - loglik: -4.3194e+02 - logprior: -2.9288e+00
Epoch 5/10
21/21 - 8s - loss: 431.6740 - loglik: -4.2830e+02 - logprior: -2.8589e+00
Epoch 6/10
21/21 - 8s - loss: 431.0772 - loglik: -4.2764e+02 - logprior: -2.9208e+00
Epoch 7/10
21/21 - 9s - loss: 431.7178 - loglik: -4.2827e+02 - logprior: -2.9178e+00
Fitted a model with MAP estimate = -429.6622
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (35, 1), (38, 2), (39, 2), (40, 1), (50, 1), (60, 1), (63, 3), (66, 1), (76, 8), (78, 1), (80, 2), (96, 1), (99, 1), (115, 1), (116, 1), (119, 1), (121, 1), (122, 1), (144, 1), (146, 2), (147, 1), (150, 1), (158, 1), (160, 1), (163, 2), (164, 1), (165, 1), (166, 1), (167, 1), (168, 2), (171, 1), (178, 1), (181, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 245 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 14s - loss: 422.2823 - loglik: -4.1114e+02 - logprior: -1.0658e+01
Epoch 2/2
21/21 - 11s - loss: 398.2817 - loglik: -3.9330e+02 - logprior: -4.4151e+00
Fitted a model with MAP estimate = -392.5637
expansions: [(0, 2)]
discards: [  0  18  28  49  82  87 112 221]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 239 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 15s - loss: 398.4358 - loglik: -3.9053e+02 - logprior: -7.2992e+00
Epoch 2/2
21/21 - 10s - loss: 388.6433 - loglik: -3.8698e+02 - logprior: -1.0401e+00
Fitted a model with MAP estimate = -387.1953
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 238 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 13s - loss: 399.8811 - loglik: -3.8940e+02 - logprior: -9.8320e+00
Epoch 2/10
21/21 - 11s - loss: 390.0059 - loglik: -3.8807e+02 - logprior: -1.2828e+00
Epoch 3/10
21/21 - 10s - loss: 388.0944 - loglik: -3.8773e+02 - logprior: 0.2897
Epoch 4/10
21/21 - 10s - loss: 387.1783 - loglik: -3.8718e+02 - logprior: 0.6618
Epoch 5/10
21/21 - 10s - loss: 387.3406 - loglik: -3.8758e+02 - logprior: 0.9006
Fitted a model with MAP estimate = -385.5284
Time for alignment: 199.8625
Computed alignments with likelihoods: ['-377.5986', '-383.7166', '-385.5284']
Best model has likelihood: -377.5986
time for generating output: 0.4766
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aadh.projection.fasta
SP score = 0.6152325362455281
Training of 3 independent models on file gluts.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3356f10c70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3495be3a30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f332354d940>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34ecbaae20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34ecbaa940>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f332cf5a790>, <__main__.SimpleDirichletPrior object at 0x7f335fbb5dc0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34af217d30>

Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 283.5221 - loglik: -2.8035e+02 - logprior: -3.0578e+00
Epoch 2/10
19/19 - 3s - loss: 253.1819 - loglik: -2.5158e+02 - logprior: -9.2275e-01
Epoch 3/10
19/19 - 3s - loss: 243.7426 - loglik: -2.4185e+02 - logprior: -9.6235e-01
Epoch 4/10
19/19 - 3s - loss: 239.7410 - loglik: -2.3766e+02 - logprior: -9.1605e-01
Epoch 5/10
19/19 - 3s - loss: 237.9142 - loglik: -2.3599e+02 - logprior: -8.9630e-01
Epoch 6/10
19/19 - 3s - loss: 236.6123 - loglik: -2.3484e+02 - logprior: -8.9226e-01
Epoch 7/10
19/19 - 3s - loss: 235.6875 - loglik: -2.3400e+02 - logprior: -8.9169e-01
Epoch 8/10
19/19 - 3s - loss: 234.5947 - loglik: -2.3294e+02 - logprior: -8.8833e-01
Epoch 9/10
19/19 - 3s - loss: 234.2792 - loglik: -2.3259e+02 - logprior: -8.8372e-01
Epoch 10/10
19/19 - 3s - loss: 233.5854 - loglik: -2.3185e+02 - logprior: -8.8488e-01
Fitted a model with MAP estimate = -231.3583
expansions: [(0, 12), (16, 2), (17, 1), (19, 2), (30, 2), (57, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 234.9606 - loglik: -2.2992e+02 - logprior: -4.0909e+00
Epoch 2/2
19/19 - 4s - loss: 227.4599 - loglik: -2.2497e+02 - logprior: -1.4706e+00
Fitted a model with MAP estimate = -224.1549
expansions: [(0, 5)]
discards: [ 0  1  2  3 34 47]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 228.3412 - loglik: -2.2359e+02 - logprior: -3.4238e+00
Epoch 2/2
19/19 - 4s - loss: 224.1501 - loglik: -2.2136e+02 - logprior: -1.2894e+00
Fitted a model with MAP estimate = -221.0917
expansions: []
discards: [ 1  2  3  4 77]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 94 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 225.3141 - loglik: -2.2092e+02 - logprior: -2.8751e+00
Epoch 2/10
19/19 - 4s - loss: 222.5751 - loglik: -2.2017e+02 - logprior: -1.0479e+00
Epoch 3/10
19/19 - 3s - loss: 222.0726 - loglik: -2.1987e+02 - logprior: -9.7496e-01
Epoch 4/10
19/19 - 3s - loss: 221.4937 - loglik: -2.1949e+02 - logprior: -9.1804e-01
Epoch 5/10
19/19 - 4s - loss: 221.1346 - loglik: -2.1928e+02 - logprior: -8.6510e-01
Epoch 6/10
19/19 - 4s - loss: 220.8272 - loglik: -2.1908e+02 - logprior: -8.3215e-01
Epoch 7/10
19/19 - 3s - loss: 220.7503 - loglik: -2.1909e+02 - logprior: -7.9005e-01
Epoch 8/10
19/19 - 3s - loss: 221.2656 - loglik: -2.1968e+02 - logprior: -7.5465e-01
Fitted a model with MAP estimate = -219.7963
Time for alignment: 111.0661
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 283.8789 - loglik: -2.8071e+02 - logprior: -3.0577e+00
Epoch 2/10
19/19 - 3s - loss: 256.3055 - loglik: -2.5467e+02 - logprior: -9.3400e-01
Epoch 3/10
19/19 - 3s - loss: 242.7762 - loglik: -2.4057e+02 - logprior: -1.0211e+00
Epoch 4/10
19/19 - 3s - loss: 239.0693 - loglik: -2.3680e+02 - logprior: -9.6624e-01
Epoch 5/10
19/19 - 3s - loss: 237.4153 - loglik: -2.3541e+02 - logprior: -9.4404e-01
Epoch 6/10
19/19 - 3s - loss: 236.3910 - loglik: -2.3460e+02 - logprior: -9.3177e-01
Epoch 7/10
19/19 - 3s - loss: 235.1127 - loglik: -2.3342e+02 - logprior: -9.2939e-01
Epoch 8/10
19/19 - 3s - loss: 235.2403 - loglik: -2.3355e+02 - logprior: -9.0906e-01
Fitted a model with MAP estimate = -232.8916
expansions: [(0, 11), (12, 1), (16, 1), (20, 3), (30, 2), (41, 1), (57, 3), (59, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 236.1286 - loglik: -2.3130e+02 - logprior: -3.9571e+00
Epoch 2/2
19/19 - 4s - loss: 228.0848 - loglik: -2.2562e+02 - logprior: -1.4348e+00
Fitted a model with MAP estimate = -224.5901
expansions: [(0, 5)]
discards: [ 0  1  2  3 34 35 47 81 82]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 230.2052 - loglik: -2.2563e+02 - logprior: -3.3387e+00
Epoch 2/2
19/19 - 4s - loss: 226.4875 - loglik: -2.2392e+02 - logprior: -1.2325e+00
Fitted a model with MAP estimate = -223.1519
expansions: []
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 97 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 226.4228 - loglik: -2.2198e+02 - logprior: -2.8818e+00
Epoch 2/10
19/19 - 4s - loss: 223.4436 - loglik: -2.2075e+02 - logprior: -1.0949e+00
Epoch 3/10
19/19 - 4s - loss: 222.1593 - loglik: -2.1954e+02 - logprior: -1.0074e+00
Epoch 4/10
19/19 - 4s - loss: 221.2253 - loglik: -2.1884e+02 - logprior: -9.5892e-01
Epoch 5/10
19/19 - 4s - loss: 220.6738 - loglik: -2.1852e+02 - logprior: -9.1439e-01
Epoch 6/10
19/19 - 4s - loss: 220.5604 - loglik: -2.1856e+02 - logprior: -8.8508e-01
Epoch 7/10
19/19 - 4s - loss: 220.5779 - loglik: -2.1870e+02 - logprior: -8.4948e-01
Fitted a model with MAP estimate = -219.2170
Time for alignment: 103.6527
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 283.6042 - loglik: -2.8043e+02 - logprior: -3.0582e+00
Epoch 2/10
19/19 - 3s - loss: 253.0014 - loglik: -2.5141e+02 - logprior: -9.2520e-01
Epoch 3/10
19/19 - 3s - loss: 241.6088 - loglik: -2.3974e+02 - logprior: -9.8030e-01
Epoch 4/10
19/19 - 3s - loss: 238.8357 - loglik: -2.3682e+02 - logprior: -8.9935e-01
Epoch 5/10
19/19 - 3s - loss: 237.3819 - loglik: -2.3557e+02 - logprior: -8.7477e-01
Epoch 6/10
19/19 - 3s - loss: 235.9967 - loglik: -2.3431e+02 - logprior: -8.8952e-01
Epoch 7/10
19/19 - 3s - loss: 235.5151 - loglik: -2.3392e+02 - logprior: -8.8239e-01
Epoch 8/10
19/19 - 3s - loss: 234.4038 - loglik: -2.3280e+02 - logprior: -8.7811e-01
Epoch 9/10
19/19 - 3s - loss: 234.2023 - loglik: -2.3255e+02 - logprior: -8.7175e-01
Epoch 10/10
19/19 - 3s - loss: 232.9418 - loglik: -2.3121e+02 - logprior: -8.8588e-01
Fitted a model with MAP estimate = -231.0829
expansions: [(0, 11), (11, 3), (15, 3), (55, 1), (57, 4), (59, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 234.2284 - loglik: -2.2924e+02 - logprior: -4.0669e+00
Epoch 2/2
19/19 - 4s - loss: 226.9167 - loglik: -2.2448e+02 - logprior: -1.4011e+00
Fitted a model with MAP estimate = -223.4432
expansions: []
discards: [ 1  2 29 30 81 82]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 227.7098 - loglik: -2.2338e+02 - logprior: -2.9250e+00
Epoch 2/2
19/19 - 4s - loss: 224.3695 - loglik: -2.2177e+02 - logprior: -1.1031e+00
Fitted a model with MAP estimate = -221.5798
expansions: [(0, 6)]
discards: [20]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 101 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 226.6155 - loglik: -2.2059e+02 - logprior: -4.4655e+00
Epoch 2/10
19/19 - 4s - loss: 222.5538 - loglik: -2.1947e+02 - logprior: -1.6727e+00
Epoch 3/10
19/19 - 4s - loss: 221.0626 - loglik: -2.1852e+02 - logprior: -1.2270e+00
Epoch 4/10
19/19 - 4s - loss: 220.0452 - loglik: -2.1779e+02 - logprior: -1.0698e+00
Epoch 5/10
19/19 - 4s - loss: 220.2196 - loglik: -2.1813e+02 - logprior: -1.0117e+00
Fitted a model with MAP estimate = -218.6097
Time for alignment: 99.5322
Computed alignments with likelihoods: ['-219.7963', '-219.2170', '-218.6097']
Best model has likelihood: -218.6097
time for generating output: 0.2779
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gluts.projection.fasta
SP score = 0.472
Training of 3 independent models on file tms.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f317c37ca60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3462a60f10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34a6fbe940>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f33236cb940>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33236cbc40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f335fbed850>, <__main__.SimpleDirichletPrior object at 0x7f2b25b25520>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f349e298430>

Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 732.4494 - loglik: -7.1987e+02 - logprior: -1.2560e+01
Epoch 2/10
17/17 - 8s - loss: 551.9551 - loglik: -5.4954e+02 - logprior: -2.3917e+00
Epoch 3/10
17/17 - 8s - loss: 453.8283 - loglik: -4.4985e+02 - logprior: -3.9424e+00
Epoch 4/10
17/17 - 8s - loss: 432.8910 - loglik: -4.2795e+02 - logprior: -4.6233e+00
Epoch 5/10
17/17 - 8s - loss: 427.8284 - loglik: -4.2281e+02 - logprior: -4.4797e+00
Epoch 6/10
17/17 - 8s - loss: 424.8796 - loglik: -4.2005e+02 - logprior: -4.2924e+00
Epoch 7/10
17/17 - 8s - loss: 423.7793 - loglik: -4.1898e+02 - logprior: -4.2908e+00
Epoch 8/10
17/17 - 8s - loss: 421.8973 - loglik: -4.1710e+02 - logprior: -4.2880e+00
Epoch 9/10
17/17 - 8s - loss: 423.4216 - loglik: -4.1864e+02 - logprior: -4.2653e+00
Fitted a model with MAP estimate = -421.7362
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (26, 1), (32, 2), (45, 1), (54, 1), (56, 1), (58, 1), (60, 1), (61, 1), (63, 1), (86, 1), (87, 1), (95, 2), (97, 1), (100, 1), (114, 1), (115, 1), (116, 1), (123, 1), (129, 3), (130, 2), (141, 1), (155, 1), (160, 1), (162, 1), (163, 1), (176, 1), (180, 1), (181, 1), (184, 1), (191, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 417.0811 - loglik: -3.9958e+02 - logprior: -1.7026e+01
Epoch 2/2
17/17 - 10s - loss: 382.3625 - loglik: -3.7595e+02 - logprior: -5.8952e+00
Fitted a model with MAP estimate = -378.9430
expansions: [(0, 3)]
discards: [  0 155 158]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 15s - loss: 384.8085 - loglik: -3.7255e+02 - logprior: -1.1750e+01
Epoch 2/2
17/17 - 10s - loss: 371.2402 - loglik: -3.6999e+02 - logprior: -6.9470e-01
Fitted a model with MAP estimate = -369.9670
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 389.9736 - loglik: -3.7331e+02 - logprior: -1.6100e+01
Epoch 2/10
17/17 - 10s - loss: 377.7480 - loglik: -3.7263e+02 - logprior: -4.5465e+00
Epoch 3/10
17/17 - 10s - loss: 374.7343 - loglik: -3.7358e+02 - logprior: -5.6763e-01
Epoch 4/10
17/17 - 10s - loss: 367.7641 - loglik: -3.6936e+02 - logprior: 2.1975
Epoch 5/10
17/17 - 10s - loss: 369.8179 - loglik: -3.7191e+02 - logprior: 2.6972
Fitted a model with MAP estimate = -368.2830
Time for alignment: 205.2345
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 731.9555 - loglik: -7.1939e+02 - logprior: -1.2550e+01
Epoch 2/10
17/17 - 8s - loss: 552.7134 - loglik: -5.5034e+02 - logprior: -2.3414e+00
Epoch 3/10
17/17 - 8s - loss: 453.5515 - loglik: -4.4968e+02 - logprior: -3.8207e+00
Epoch 4/10
17/17 - 8s - loss: 433.8450 - loglik: -4.2911e+02 - logprior: -4.3323e+00
Epoch 5/10
17/17 - 8s - loss: 430.0085 - loglik: -4.2550e+02 - logprior: -3.9119e+00
Epoch 6/10
17/17 - 8s - loss: 426.2654 - loglik: -4.2202e+02 - logprior: -3.7168e+00
Epoch 7/10
17/17 - 8s - loss: 425.1383 - loglik: -4.2094e+02 - logprior: -3.7351e+00
Epoch 8/10
17/17 - 8s - loss: 426.5759 - loglik: -4.2228e+02 - logprior: -3.8158e+00
Fitted a model with MAP estimate = -424.0232
expansions: [(8, 1), (9, 1), (11, 1), (13, 3), (22, 1), (26, 1), (32, 1), (42, 1), (45, 1), (56, 1), (58, 1), (59, 1), (60, 1), (61, 1), (63, 1), (86, 2), (95, 2), (97, 1), (100, 1), (114, 2), (115, 3), (129, 3), (130, 2), (141, 1), (156, 1), (160, 1), (161, 1), (162, 1), (179, 1), (180, 1), (181, 1), (184, 1), (188, 1), (192, 1), (195, 1), (196, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 268 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 14s - loss: 418.3719 - loglik: -4.0087e+02 - logprior: -1.7068e+01
Epoch 2/2
17/17 - 10s - loss: 383.1064 - loglik: -3.7648e+02 - logprior: -6.1269e+00
Fitted a model with MAP estimate = -379.5879
expansions: [(0, 4)]
discards: [  0  16 136 157 160]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 267 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 385.4639 - loglik: -3.7307e+02 - logprior: -1.1905e+01
Epoch 2/2
17/17 - 10s - loss: 372.3169 - loglik: -3.7103e+02 - logprior: -7.5241e-01
Fitted a model with MAP estimate = -370.1670
expansions: []
discards: [0 1 3]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 15s - loss: 390.1014 - loglik: -3.7341e+02 - logprior: -1.6163e+01
Epoch 2/10
17/17 - 10s - loss: 378.2952 - loglik: -3.7322e+02 - logprior: -4.5372e+00
Epoch 3/10
17/17 - 10s - loss: 372.0016 - loglik: -3.7097e+02 - logprior: -4.6849e-01
Epoch 4/10
17/17 - 10s - loss: 370.0430 - loglik: -3.7165e+02 - logprior: 2.1730
Epoch 5/10
17/17 - 10s - loss: 368.7440 - loglik: -3.7083e+02 - logprior: 2.6617
Epoch 6/10
17/17 - 10s - loss: 368.7434 - loglik: -3.7119e+02 - logprior: 3.0423
Epoch 7/10
17/17 - 10s - loss: 368.4341 - loglik: -3.7116e+02 - logprior: 3.3264
Epoch 8/10
17/17 - 10s - loss: 369.4967 - loglik: -3.7251e+02 - logprior: 3.6152
Fitted a model with MAP estimate = -367.1063
Time for alignment: 227.9882
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 733.1805 - loglik: -7.2061e+02 - logprior: -1.2554e+01
Epoch 2/10
17/17 - 8s - loss: 552.3594 - loglik: -5.4999e+02 - logprior: -2.3426e+00
Epoch 3/10
17/17 - 8s - loss: 454.6487 - loglik: -4.5061e+02 - logprior: -3.9939e+00
Epoch 4/10
17/17 - 8s - loss: 435.9313 - loglik: -4.3095e+02 - logprior: -4.6275e+00
Epoch 5/10
17/17 - 8s - loss: 430.5549 - loglik: -4.2575e+02 - logprior: -4.2647e+00
Epoch 6/10
17/17 - 8s - loss: 427.4862 - loglik: -4.2294e+02 - logprior: -4.0129e+00
Epoch 7/10
17/17 - 8s - loss: 427.5529 - loglik: -4.2301e+02 - logprior: -4.0125e+00
Fitted a model with MAP estimate = -426.2968
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (31, 1), (32, 2), (45, 1), (54, 1), (56, 1), (57, 1), (58, 1), (59, 1), (63, 1), (71, 2), (86, 1), (95, 2), (97, 1), (100, 1), (114, 2), (115, 3), (129, 2), (131, 1), (150, 1), (151, 1), (160, 1), (161, 1), (163, 1), (177, 1), (179, 1), (181, 2), (184, 1), (191, 1), (192, 1), (197, 1), (198, 1), (201, 1), (207, 1), (209, 1), (212, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 267 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 418.4366 - loglik: -4.0089e+02 - logprior: -1.7061e+01
Epoch 2/2
17/17 - 10s - loss: 384.5668 - loglik: -3.7803e+02 - logprior: -6.0349e+00
Fitted a model with MAP estimate = -379.1900
expansions: [(0, 3)]
discards: [  0  86 136 156]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 15s - loss: 386.0284 - loglik: -3.7374e+02 - logprior: -1.1784e+01
Epoch 2/2
17/17 - 10s - loss: 371.8554 - loglik: -3.7058e+02 - logprior: -7.4736e-01
Fitted a model with MAP estimate = -370.3543
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 389.9643 - loglik: -3.7334e+02 - logprior: -1.6098e+01
Epoch 2/10
17/17 - 10s - loss: 378.2559 - loglik: -3.7323e+02 - logprior: -4.4517e+00
Epoch 3/10
17/17 - 10s - loss: 372.1000 - loglik: -3.7161e+02 - logprior: 0.0760
Epoch 4/10
17/17 - 10s - loss: 370.0597 - loglik: -3.7164e+02 - logprior: 2.1557
Epoch 5/10
17/17 - 10s - loss: 370.7056 - loglik: -3.7274e+02 - logprior: 2.6187
Fitted a model with MAP estimate = -368.4319
Time for alignment: 187.2998
Computed alignments with likelihoods: ['-368.2830', '-367.1063', '-368.4319']
Best model has likelihood: -367.1063
time for generating output: 0.3338
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tms.projection.fasta
SP score = 0.9421677518192263
Training of 3 independent models on file kunitz.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f33568d51f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f32eda06190>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f330a601a60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f30f8f325e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f330a721310>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f348d7b35e0>, <__main__.SimpleDirichletPrior object at 0x7f345a6175e0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f33791ee790>

Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 179.5686 - loglik: -1.5941e+02 - logprior: -2.0139e+01
Epoch 2/10
10/10 - 1s - loss: 148.1942 - loglik: -1.4235e+02 - logprior: -5.8266e+00
Epoch 3/10
10/10 - 1s - loss: 131.4338 - loglik: -1.2809e+02 - logprior: -3.3439e+00
Epoch 4/10
10/10 - 1s - loss: 120.5758 - loglik: -1.1781e+02 - logprior: -2.7187e+00
Epoch 5/10
10/10 - 1s - loss: 115.5117 - loglik: -1.1267e+02 - logprior: -2.5912e+00
Epoch 6/10
10/10 - 1s - loss: 113.6357 - loglik: -1.1074e+02 - logprior: -2.5091e+00
Epoch 7/10
10/10 - 1s - loss: 113.0711 - loglik: -1.1036e+02 - logprior: -2.3878e+00
Epoch 8/10
10/10 - 1s - loss: 112.3790 - loglik: -1.0982e+02 - logprior: -2.2662e+00
Epoch 9/10
10/10 - 1s - loss: 112.2959 - loglik: -1.0980e+02 - logprior: -2.1892e+00
Epoch 10/10
10/10 - 1s - loss: 111.9589 - loglik: -1.0949e+02 - logprior: -2.1750e+00
Fitted a model with MAP estimate = -111.6351
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (19, 1), (22, 1), (35, 4), (36, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 133.9159 - loglik: -1.1093e+02 - logprior: -2.2681e+01
Epoch 2/2
10/10 - 1s - loss: 114.9032 - loglik: -1.0461e+02 - logprior: -9.9346e+00
Fitted a model with MAP estimate = -111.0976
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 51]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 120.2383 - loglik: -1.0173e+02 - logprior: -1.8115e+01
Epoch 2/2
10/10 - 1s - loss: 106.0122 - loglik: -1.0043e+02 - logprior: -5.1814e+00
Fitted a model with MAP estimate = -103.9961
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 122.7672 - loglik: -1.0179e+02 - logprior: -2.0539e+01
Epoch 2/10
10/10 - 1s - loss: 108.1588 - loglik: -1.0163e+02 - logprior: -6.0881e+00
Epoch 3/10
10/10 - 1s - loss: 104.8162 - loglik: -1.0115e+02 - logprior: -3.2011e+00
Epoch 4/10
10/10 - 1s - loss: 103.9340 - loglik: -1.0106e+02 - logprior: -2.3826e+00
Epoch 5/10
10/10 - 1s - loss: 103.6438 - loglik: -1.0134e+02 - logprior: -1.7944e+00
Epoch 6/10
10/10 - 1s - loss: 102.9435 - loglik: -1.0094e+02 - logprior: -1.4797e+00
Epoch 7/10
10/10 - 1s - loss: 102.7052 - loglik: -1.0082e+02 - logprior: -1.3466e+00
Epoch 8/10
10/10 - 1s - loss: 102.8904 - loglik: -1.0112e+02 - logprior: -1.2274e+00
Fitted a model with MAP estimate = -102.1947
Time for alignment: 34.2997
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.6100 - loglik: -1.5945e+02 - logprior: -2.0139e+01
Epoch 2/10
10/10 - 1s - loss: 148.4931 - loglik: -1.4264e+02 - logprior: -5.8307e+00
Epoch 3/10
10/10 - 1s - loss: 130.7089 - loglik: -1.2730e+02 - logprior: -3.4036e+00
Epoch 4/10
10/10 - 1s - loss: 120.0993 - loglik: -1.1718e+02 - logprior: -2.9171e+00
Epoch 5/10
10/10 - 1s - loss: 115.2294 - loglik: -1.1229e+02 - logprior: -2.8711e+00
Epoch 6/10
10/10 - 1s - loss: 113.0360 - loglik: -1.0996e+02 - logprior: -2.8160e+00
Epoch 7/10
10/10 - 1s - loss: 111.6626 - loglik: -1.0859e+02 - logprior: -2.6744e+00
Epoch 8/10
10/10 - 1s - loss: 110.8071 - loglik: -1.0791e+02 - logprior: -2.5502e+00
Epoch 9/10
10/10 - 1s - loss: 110.0714 - loglik: -1.0730e+02 - logprior: -2.4891e+00
Epoch 10/10
10/10 - 1s - loss: 110.0109 - loglik: -1.0725e+02 - logprior: -2.4679e+00
Fitted a model with MAP estimate = -109.4792
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (19, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 133.7157 - loglik: -1.1071e+02 - logprior: -2.2693e+01
Epoch 2/2
10/10 - 1s - loss: 114.2681 - loglik: -1.0402e+02 - logprior: -9.8984e+00
Fitted a model with MAP estimate = -110.8299
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 119.6704 - loglik: -1.0121e+02 - logprior: -1.8083e+01
Epoch 2/2
10/10 - 1s - loss: 106.0278 - loglik: -1.0045e+02 - logprior: -5.1748e+00
Fitted a model with MAP estimate = -103.9242
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 122.5365 - loglik: -1.0157e+02 - logprior: -2.0538e+01
Epoch 2/10
10/10 - 1s - loss: 108.2700 - loglik: -1.0174e+02 - logprior: -6.0872e+00
Epoch 3/10
10/10 - 1s - loss: 104.8944 - loglik: -1.0124e+02 - logprior: -3.1936e+00
Epoch 4/10
10/10 - 1s - loss: 103.9226 - loglik: -1.0109e+02 - logprior: -2.3476e+00
Epoch 5/10
10/10 - 1s - loss: 103.5857 - loglik: -1.0134e+02 - logprior: -1.7509e+00
Epoch 6/10
10/10 - 1s - loss: 103.2660 - loglik: -1.0127e+02 - logprior: -1.4847e+00
Epoch 7/10
10/10 - 1s - loss: 102.7421 - loglik: -1.0085e+02 - logprior: -1.3625e+00
Epoch 8/10
10/10 - 1s - loss: 103.0747 - loglik: -1.0131e+02 - logprior: -1.2228e+00
Fitted a model with MAP estimate = -102.2565
Time for alignment: 33.6119
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 179.6445 - loglik: -1.5949e+02 - logprior: -2.0140e+01
Epoch 2/10
10/10 - 1s - loss: 148.2596 - loglik: -1.4240e+02 - logprior: -5.8348e+00
Epoch 3/10
10/10 - 1s - loss: 130.2619 - loglik: -1.2685e+02 - logprior: -3.4109e+00
Epoch 4/10
10/10 - 1s - loss: 118.4210 - loglik: -1.1552e+02 - logprior: -2.8983e+00
Epoch 5/10
10/10 - 1s - loss: 113.9323 - loglik: -1.1106e+02 - logprior: -2.8323e+00
Epoch 6/10
10/10 - 1s - loss: 112.0611 - loglik: -1.0914e+02 - logprior: -2.8149e+00
Epoch 7/10
10/10 - 1s - loss: 111.1573 - loglik: -1.0811e+02 - logprior: -2.7369e+00
Epoch 8/10
10/10 - 1s - loss: 110.3824 - loglik: -1.0742e+02 - logprior: -2.5952e+00
Epoch 9/10
10/10 - 1s - loss: 109.9515 - loglik: -1.0715e+02 - logprior: -2.5023e+00
Epoch 10/10
10/10 - 1s - loss: 110.0790 - loglik: -1.0733e+02 - logprior: -2.4813e+00
Fitted a model with MAP estimate = -109.5604
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (17, 1), (19, 1), (22, 2), (23, 1), (34, 1), (35, 1), (36, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 133.7685 - loglik: -1.1079e+02 - logprior: -2.2688e+01
Epoch 2/2
10/10 - 1s - loss: 114.6634 - loglik: -1.0446e+02 - logprior: -9.8914e+00
Fitted a model with MAP estimate = -110.9779
expansions: [(0, 2)]
discards: [ 0  6 13 16 18 32]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 119.9083 - loglik: -1.0149e+02 - logprior: -1.8075e+01
Epoch 2/2
10/10 - 1s - loss: 106.2183 - loglik: -1.0069e+02 - logprior: -5.1738e+00
Fitted a model with MAP estimate = -104.0496
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 122.6544 - loglik: -1.0175e+02 - logprior: -2.0517e+01
Epoch 2/10
10/10 - 1s - loss: 108.1887 - loglik: -1.0172e+02 - logprior: -6.0698e+00
Epoch 3/10
10/10 - 1s - loss: 105.0757 - loglik: -1.0145e+02 - logprior: -3.1904e+00
Epoch 4/10
10/10 - 1s - loss: 104.1262 - loglik: -1.0131e+02 - logprior: -2.3621e+00
Epoch 5/10
10/10 - 1s - loss: 103.6943 - loglik: -1.0146e+02 - logprior: -1.7680e+00
Epoch 6/10
10/10 - 1s - loss: 102.8435 - loglik: -1.0089e+02 - logprior: -1.4636e+00
Epoch 7/10
10/10 - 1s - loss: 103.3377 - loglik: -1.0150e+02 - logprior: -1.3341e+00
Fitted a model with MAP estimate = -102.4435
Time for alignment: 34.9701
Computed alignments with likelihoods: ['-102.1947', '-102.2565', '-102.4435']
Best model has likelihood: -102.1947
time for generating output: 0.1318
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kunitz.projection.fasta
SP score = 0.9928712871287129
Training of 3 independent models on file rhv.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2b258d6f40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34ecd06a90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34ee127ca0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f33a33494c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2b2c331ee0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f346b10cc70>, <__main__.SimpleDirichletPrior object at 0x7f34ed36bcd0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f33576b03a0>

Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 469.2968 - loglik: -4.6740e+02 - logprior: -1.8689e+00
Epoch 2/10
39/39 - 10s - loss: 386.1819 - loglik: -3.8473e+02 - logprior: -1.2312e+00
Epoch 3/10
39/39 - 11s - loss: 377.2103 - loglik: -3.7545e+02 - logprior: -1.2561e+00
Epoch 4/10
39/39 - 11s - loss: 375.0449 - loglik: -3.7331e+02 - logprior: -1.2063e+00
Epoch 5/10
39/39 - 11s - loss: 374.1665 - loglik: -3.7246e+02 - logprior: -1.1962e+00
Epoch 6/10
39/39 - 11s - loss: 372.5652 - loglik: -3.7082e+02 - logprior: -1.2339e+00
Epoch 7/10
39/39 - 11s - loss: 371.1536 - loglik: -3.6935e+02 - logprior: -1.2659e+00
Epoch 8/10
39/39 - 11s - loss: 371.1298 - loglik: -3.6933e+02 - logprior: -1.2675e+00
Epoch 9/10
39/39 - 11s - loss: 370.7642 - loglik: -3.6897e+02 - logprior: -1.2658e+00
Epoch 10/10
39/39 - 10s - loss: 370.1599 - loglik: -3.6836e+02 - logprior: -1.2627e+00
Fitted a model with MAP estimate = -306.2783
expansions: [(0, 17), (10, 1), (15, 1), (18, 1), (28, 1), (29, 1), (37, 2), (43, 1), (44, 2), (71, 1), (88, 4), (90, 1), (102, 2), (108, 1), (112, 1), (124, 9), (126, 3), (130, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 186 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 348.9119 - loglik: -3.4543e+02 - logprior: -3.0285e+00
Epoch 2/2
39/39 - 14s - loss: 334.5009 - loglik: -3.3198e+02 - logprior: -1.9396e+00
Fitted a model with MAP estimate = -282.2535
expansions: [(52, 2), (54, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  69 135 181]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 172 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 338.9993 - loglik: -3.3535e+02 - logprior: -3.0174e+00
Epoch 2/2
39/39 - 13s - loss: 335.4956 - loglik: -3.3358e+02 - logprior: -1.2585e+00
Fitted a model with MAP estimate = -282.8843
expansions: [(0, 3), (39, 1), (58, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 177 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 19s - loss: 277.8689 - loglik: -2.7583e+02 - logprior: -1.4725e+00
Epoch 2/10
52/52 - 16s - loss: 273.8208 - loglik: -2.7229e+02 - logprior: -9.7988e-01
Epoch 3/10
52/52 - 15s - loss: 273.3835 - loglik: -2.7187e+02 - logprior: -9.3247e-01
Epoch 4/10
52/52 - 16s - loss: 270.5488 - loglik: -2.6905e+02 - logprior: -8.6724e-01
Epoch 5/10
52/52 - 16s - loss: 270.1496 - loglik: -2.6868e+02 - logprior: -8.0102e-01
Epoch 6/10
52/52 - 15s - loss: 269.9250 - loglik: -2.6853e+02 - logprior: -7.3653e-01
Epoch 7/10
52/52 - 16s - loss: 271.1329 - loglik: -2.6981e+02 - logprior: -6.7634e-01
Fitted a model with MAP estimate = -269.1496
Time for alignment: 370.8414
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 471.1669 - loglik: -4.6925e+02 - logprior: -1.8912e+00
Epoch 2/10
39/39 - 11s - loss: 389.7118 - loglik: -3.8813e+02 - logprior: -1.3389e+00
Epoch 3/10
39/39 - 10s - loss: 380.4305 - loglik: -3.7857e+02 - logprior: -1.4357e+00
Epoch 4/10
39/39 - 11s - loss: 377.7286 - loglik: -3.7581e+02 - logprior: -1.4119e+00
Epoch 5/10
39/39 - 11s - loss: 375.5920 - loglik: -3.7368e+02 - logprior: -1.4093e+00
Epoch 6/10
39/39 - 11s - loss: 375.0812 - loglik: -3.7320e+02 - logprior: -1.4049e+00
Epoch 7/10
39/39 - 11s - loss: 374.4977 - loglik: -3.7267e+02 - logprior: -1.3841e+00
Epoch 8/10
39/39 - 11s - loss: 374.2191 - loglik: -3.7238e+02 - logprior: -1.3931e+00
Epoch 9/10
39/39 - 10s - loss: 373.2007 - loglik: -3.7138e+02 - logprior: -1.3950e+00
Epoch 10/10
39/39 - 11s - loss: 373.3723 - loglik: -3.7155e+02 - logprior: -1.4085e+00
Fitted a model with MAP estimate = -308.9407
expansions: [(0, 5), (11, 1), (15, 1), (25, 1), (28, 1), (30, 1), (33, 3), (34, 2), (43, 1), (44, 4), (67, 4), (70, 2), (87, 1), (88, 1), (89, 1), (90, 1), (102, 1), (107, 2), (125, 7), (126, 2), (130, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 351.5139 - loglik: -3.4728e+02 - logprior: -3.7760e+00
Epoch 2/2
39/39 - 13s - loss: 335.5637 - loglik: -3.3331e+02 - logprior: -1.7339e+00
Fitted a model with MAP estimate = -281.0179
expansions: [(47, 1), (162, 3)]
discards: [  0  61  88  89  90  94 138 173]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 176 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 339.1821 - loglik: -3.3549e+02 - logprior: -3.1735e+00
Epoch 2/2
39/39 - 13s - loss: 334.4712 - loglik: -3.3264e+02 - logprior: -1.3031e+00
Fitted a model with MAP estimate = -282.6206
expansions: [(157, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 19s - loss: 279.5225 - loglik: -2.7776e+02 - logprior: -1.2799e+00
Epoch 2/10
52/52 - 15s - loss: 275.1495 - loglik: -2.7382e+02 - logprior: -8.7904e-01
Epoch 3/10
52/52 - 15s - loss: 272.5743 - loglik: -2.7116e+02 - logprior: -9.3543e-01
Epoch 4/10
52/52 - 16s - loss: 274.3394 - loglik: -2.7289e+02 - logprior: -9.3556e-01
Fitted a model with MAP estimate = -272.7882
Time for alignment: 322.4104
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 468.0583 - loglik: -4.6614e+02 - logprior: -1.8945e+00
Epoch 2/10
39/39 - 11s - loss: 384.7686 - loglik: -3.8320e+02 - logprior: -1.3538e+00
Epoch 3/10
39/39 - 11s - loss: 375.8799 - loglik: -3.7407e+02 - logprior: -1.4009e+00
Epoch 4/10
39/39 - 11s - loss: 372.9863 - loglik: -3.7104e+02 - logprior: -1.4116e+00
Epoch 5/10
39/39 - 11s - loss: 371.5866 - loglik: -3.6969e+02 - logprior: -1.4005e+00
Epoch 6/10
39/39 - 11s - loss: 371.0251 - loglik: -3.6915e+02 - logprior: -1.4058e+00
Epoch 7/10
39/39 - 11s - loss: 370.5689 - loglik: -3.6872e+02 - logprior: -1.3999e+00
Epoch 8/10
39/39 - 11s - loss: 370.4528 - loglik: -3.6859e+02 - logprior: -1.4211e+00
Epoch 9/10
39/39 - 10s - loss: 369.4405 - loglik: -3.6755e+02 - logprior: -1.4452e+00
Epoch 10/10
39/39 - 10s - loss: 369.5761 - loglik: -3.6765e+02 - logprior: -1.4566e+00
Fitted a model with MAP estimate = -305.9504
expansions: [(0, 14), (10, 1), (18, 1), (28, 1), (30, 2), (31, 1), (32, 2), (33, 1), (38, 1), (43, 1), (44, 1), (45, 1), (56, 1), (71, 2), (88, 1), (89, 1), (90, 1), (91, 1), (103, 1), (104, 2), (107, 2), (112, 1), (127, 9), (130, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 348.7788 - loglik: -3.4524e+02 - logprior: -3.1403e+00
Epoch 2/2
39/39 - 14s - loss: 332.8593 - loglik: -3.3024e+02 - logprior: -2.1290e+00
Fitted a model with MAP estimate = -279.8267
expansions: [(38, 1), (170, 2), (171, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  52 139 144]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 177 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 335.1996 - loglik: -3.3161e+02 - logprior: -3.1127e+00
Epoch 2/2
39/39 - 13s - loss: 330.7197 - loglik: -3.2879e+02 - logprior: -1.3852e+00
Fitted a model with MAP estimate = -280.5343
expansions: [(0, 4)]
discards: [157]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 180 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 20s - loss: 276.3548 - loglik: -2.7429e+02 - logprior: -1.5880e+00
Epoch 2/10
52/52 - 16s - loss: 269.8345 - loglik: -2.6826e+02 - logprior: -1.1273e+00
Epoch 3/10
52/52 - 16s - loss: 269.9887 - loglik: -2.6838e+02 - logprior: -1.0809e+00
Fitted a model with MAP estimate = -268.0594
Time for alignment: 309.1679
Computed alignments with likelihoods: ['-269.1496', '-272.7882', '-268.0594']
Best model has likelihood: -268.0594
time for generating output: 0.6811
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rhv.projection.fasta
SP score = 0.23097437851223926
Training of 3 independent models on file blm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3322ef8820>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34ee8a5160>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f335f937c40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f32f0148400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32f0148b50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34ed80f610>, <__main__.SimpleDirichletPrior object at 0x7f32ee1c0f10>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f349e298f70>

Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 26s - loss: 817.3452 - loglik: -8.1517e+02 - logprior: -1.7054e+00
Epoch 2/10
37/37 - 23s - loss: 727.1989 - loglik: -7.2449e+02 - logprior: -9.3174e-01
Epoch 3/10
37/37 - 23s - loss: 709.4113 - loglik: -7.0599e+02 - logprior: -9.9149e-01
Epoch 4/10
37/37 - 23s - loss: 705.1371 - loglik: -7.0192e+02 - logprior: -1.0230e+00
Epoch 5/10
37/37 - 23s - loss: 700.3879 - loglik: -6.9743e+02 - logprior: -1.0985e+00
Epoch 6/10
37/37 - 23s - loss: 700.4510 - loglik: -6.9781e+02 - logprior: -1.1158e+00
Fitted a model with MAP estimate = -697.0504
expansions: [(0, 4), (30, 1), (32, 4), (33, 2), (35, 1), (65, 1), (91, 5), (92, 12), (93, 5), (133, 1), (186, 1), (198, 8), (206, 3), (240, 1), (242, 1)]
discards: [100 104 122 155]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 300 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 32s - loss: 693.3852 - loglik: -6.8879e+02 - logprior: -3.1040e+00
Epoch 2/2
37/37 - 29s - loss: 680.4261 - loglik: -6.7743e+02 - logprior: -1.4796e+00
Fitted a model with MAP estimate = -677.6417
expansions: [(0, 2), (39, 1), (44, 1), (112, 4), (249, 1)]
discards: [  0 106 136]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 306 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 36s - loss: 680.4471 - loglik: -6.7606e+02 - logprior: -2.6674e+00
Epoch 2/2
37/37 - 30s - loss: 678.0335 - loglik: -6.7510e+02 - logprior: -1.1807e+00
Fitted a model with MAP estimate = -673.9802
expansions: []
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 305 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 33s - loss: 677.5873 - loglik: -6.7340e+02 - logprior: -2.3516e+00
Epoch 2/10
37/37 - 30s - loss: 676.0818 - loglik: -6.7344e+02 - logprior: -9.0175e-01
Epoch 3/10
37/37 - 30s - loss: 673.1782 - loglik: -6.7069e+02 - logprior: -6.8221e-01
Epoch 4/10
37/37 - 30s - loss: 673.4904 - loglik: -6.7133e+02 - logprior: -5.6385e-01
Fitted a model with MAP estimate = -670.9052
Time for alignment: 520.6042
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 27s - loss: 813.7836 - loglik: -8.1162e+02 - logprior: -1.6934e+00
Epoch 2/10
37/37 - 23s - loss: 724.0057 - loglik: -7.2143e+02 - logprior: -6.9407e-01
Epoch 3/10
37/37 - 23s - loss: 707.5739 - loglik: -7.0443e+02 - logprior: -7.2885e-01
Epoch 4/10
37/37 - 23s - loss: 704.3146 - loglik: -7.0142e+02 - logprior: -7.7409e-01
Epoch 5/10
37/37 - 23s - loss: 700.8912 - loglik: -6.9832e+02 - logprior: -8.3897e-01
Epoch 6/10
37/37 - 23s - loss: 699.9156 - loglik: -6.9758e+02 - logprior: -8.7226e-01
Epoch 7/10
37/37 - 23s - loss: 698.2170 - loglik: -6.9604e+02 - logprior: -8.8971e-01
Epoch 8/10
37/37 - 23s - loss: 699.8148 - loglik: -6.9776e+02 - logprior: -9.0132e-01
Fitted a model with MAP estimate = -696.0732
expansions: [(0, 4), (26, 1), (33, 1), (34, 3), (35, 2), (36, 1), (91, 21), (141, 1), (197, 7), (210, 2), (239, 1), (240, 1)]
discards: [101]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 298 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 32s - loss: 691.7610 - loglik: -6.8753e+02 - logprior: -3.0837e+00
Epoch 2/2
37/37 - 29s - loss: 682.1202 - loglik: -6.7939e+02 - logprior: -1.4033e+00
Fitted a model with MAP estimate = -679.0098
expansions: [(0, 2), (44, 1), (45, 1), (251, 1)]
discards: [  0 184 185]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 300 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 32s - loss: 683.3328 - loglik: -6.7901e+02 - logprior: -2.6445e+00
Epoch 2/2
37/37 - 29s - loss: 678.4725 - loglik: -6.7567e+02 - logprior: -1.1557e+00
Fitted a model with MAP estimate = -676.3282
expansions: [(106, 4), (186, 2), (235, 1), (236, 1)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 307 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 35s - loss: 678.5433 - loglik: -6.7459e+02 - logprior: -2.2114e+00
Epoch 2/10
37/37 - 30s - loss: 676.0396 - loglik: -6.7339e+02 - logprior: -8.7244e-01
Epoch 3/10
37/37 - 30s - loss: 674.2748 - loglik: -6.7175e+02 - logprior: -6.8509e-01
Epoch 4/10
37/37 - 30s - loss: 672.4420 - loglik: -6.7033e+02 - logprior: -5.1954e-01
Epoch 5/10
37/37 - 30s - loss: 672.8467 - loglik: -6.7105e+02 - logprior: -3.2050e-01
Fitted a model with MAP estimate = -670.6947
Time for alignment: 595.9181
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 27s - loss: 815.7951 - loglik: -8.1362e+02 - logprior: -1.7048e+00
Epoch 2/10
37/37 - 23s - loss: 725.5989 - loglik: -7.2305e+02 - logprior: -8.4589e-01
Epoch 3/10
37/37 - 23s - loss: 710.0425 - loglik: -7.0696e+02 - logprior: -9.9579e-01
Epoch 4/10
37/37 - 23s - loss: 706.5239 - loglik: -7.0363e+02 - logprior: -9.9333e-01
Epoch 5/10
37/37 - 23s - loss: 703.0844 - loglik: -7.0042e+02 - logprior: -1.0255e+00
Epoch 6/10
37/37 - 23s - loss: 701.7899 - loglik: -6.9922e+02 - logprior: -1.0950e+00
Epoch 7/10
37/37 - 23s - loss: 700.7716 - loglik: -6.9821e+02 - logprior: -1.1906e+00
Epoch 8/10
37/37 - 23s - loss: 700.7159 - loglik: -6.9845e+02 - logprior: -1.0405e+00
Epoch 9/10
37/37 - 23s - loss: 698.6072 - loglik: -6.9645e+02 - logprior: -1.0367e+00
Epoch 10/10
37/37 - 23s - loss: 697.7889 - loglik: -6.9571e+02 - logprior: -1.0400e+00
Fitted a model with MAP estimate = -697.3048
expansions: [(0, 4), (34, 3), (35, 1), (36, 2), (39, 1), (67, 1), (93, 20), (143, 8), (199, 12), (240, 1), (242, 1)]
discards: [104 105 152 189 190]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 303 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 32s - loss: 692.6815 - loglik: -6.8851e+02 - logprior: -3.1057e+00
Epoch 2/2
37/37 - 29s - loss: 681.2240 - loglik: -6.7854e+02 - logprior: -1.4142e+00
Fitted a model with MAP estimate = -677.8949
expansions: [(40, 1), (111, 6), (239, 5)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 314 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 36s - loss: 681.3182 - loglik: -6.7633e+02 - logprior: -3.5789e+00
Epoch 2/2
37/37 - 31s - loss: 675.4855 - loglik: -6.7256e+02 - logprior: -1.3170e+00
Fitted a model with MAP estimate = -672.1549
expansions: [(0, 2), (266, 4)]
discards: [110]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 319 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 36s - loss: 675.8340 - loglik: -6.7139e+02 - logprior: -2.7115e+00
Epoch 2/10
37/37 - 32s - loss: 671.5473 - loglik: -6.6878e+02 - logprior: -1.0253e+00
Epoch 3/10
37/37 - 32s - loss: 670.3861 - loglik: -6.6788e+02 - logprior: -7.5613e-01
Epoch 4/10
37/37 - 32s - loss: 670.8971 - loglik: -6.6883e+02 - logprior: -5.5970e-01
Fitted a model with MAP estimate = -668.0242
Time for alignment: 631.0328
Computed alignments with likelihoods: ['-670.9052', '-670.6947', '-668.0242']
Best model has likelihood: -668.0242
time for generating output: 0.3301
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blm.projection.fasta
SP score = 0.8828114126652749
Training of 3 independent models on file cyt3.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f344012edc0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34af65b070>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34eccb1250>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3324736a90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3392f08b20>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f333554a1f0>, <__main__.SimpleDirichletPrior object at 0x7f34ed558c70>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f342d408b80>

Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 387.2053 - loglik: -2.7272e+02 - logprior: -1.1446e+02
Epoch 2/10
10/10 - 1s - loss: 290.3582 - loglik: -2.6162e+02 - logprior: -2.8564e+01
Epoch 3/10
10/10 - 1s - loss: 262.0306 - loglik: -2.5083e+02 - logprior: -1.1027e+01
Epoch 4/10
10/10 - 1s - loss: 246.6733 - loglik: -2.4188e+02 - logprior: -4.7293e+00
Epoch 5/10
10/10 - 1s - loss: 237.3148 - loglik: -2.3532e+02 - logprior: -1.8007e+00
Epoch 6/10
10/10 - 1s - loss: 231.7439 - loglik: -2.3100e+02 - logprior: -2.1111e-01
Epoch 7/10
10/10 - 1s - loss: 228.5133 - loglik: -2.2841e+02 - logprior: 0.6530
Epoch 8/10
10/10 - 1s - loss: 226.5432 - loglik: -2.2702e+02 - logprior: 1.2458
Epoch 9/10
10/10 - 1s - loss: 225.3366 - loglik: -2.2631e+02 - logprior: 1.6573
Epoch 10/10
10/10 - 1s - loss: 224.5273 - loglik: -2.2593e+02 - logprior: 2.0406
Fitted a model with MAP estimate = -223.5329
expansions: [(0, 6), (51, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 376.3028 - loglik: -2.2307e+02 - logprior: -1.5254e+02
Epoch 2/2
10/10 - 1s - loss: 267.3524 - loglik: -2.2185e+02 - logprior: -4.4773e+01
Fitted a model with MAP estimate = -246.2104
expansions: [(43, 5), (51, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 326.5782 - loglik: -2.1970e+02 - logprior: -1.0611e+02
Epoch 2/2
10/10 - 1s - loss: 245.5291 - loglik: -2.1893e+02 - logprior: -2.5799e+01
Fitted a model with MAP estimate = -232.6998
expansions: [(43, 3)]
discards: [56]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 97 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 318.0215 - loglik: -2.1735e+02 - logprior: -9.9828e+01
Epoch 2/10
10/10 - 1s - loss: 242.0630 - loglik: -2.1718e+02 - logprior: -2.3980e+01
Epoch 3/10
10/10 - 1s - loss: 226.3496 - loglik: -2.1752e+02 - logprior: -7.8452e+00
Epoch 4/10
10/10 - 1s - loss: 219.6638 - loglik: -2.1768e+02 - logprior: -9.4898e-01
Epoch 5/10
10/10 - 1s - loss: 215.9141 - loglik: -2.1783e+02 - logprior: 2.9864
Epoch 6/10
10/10 - 1s - loss: 213.5491 - loglik: -2.1778e+02 - logprior: 5.3447
Epoch 7/10
10/10 - 1s - loss: 211.9606 - loglik: -2.1771e+02 - logprior: 6.8768
Epoch 8/10
10/10 - 1s - loss: 210.7948 - loglik: -2.1759e+02 - logprior: 7.9506
Epoch 9/10
10/10 - 1s - loss: 209.7941 - loglik: -2.1739e+02 - logprior: 8.7866
Epoch 10/10
10/10 - 1s - loss: 208.9009 - loglik: -2.1720e+02 - logprior: 9.5071
Fitted a model with MAP estimate = -207.2269
Time for alignment: 46.3597
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 387.2053 - loglik: -2.7272e+02 - logprior: -1.1446e+02
Epoch 2/10
10/10 - 1s - loss: 290.3581 - loglik: -2.6162e+02 - logprior: -2.8564e+01
Epoch 3/10
10/10 - 1s - loss: 262.0305 - loglik: -2.5083e+02 - logprior: -1.1027e+01
Epoch 4/10
10/10 - 1s - loss: 246.5770 - loglik: -2.4176e+02 - logprior: -4.7524e+00
Epoch 5/10
10/10 - 1s - loss: 237.2639 - loglik: -2.3516e+02 - logprior: -1.9095e+00
Epoch 6/10
10/10 - 1s - loss: 231.8898 - loglik: -2.3105e+02 - logprior: -2.7986e-01
Epoch 7/10
10/10 - 1s - loss: 228.7097 - loglik: -2.2847e+02 - logprior: 0.6012
Epoch 8/10
10/10 - 1s - loss: 226.8506 - loglik: -2.2713e+02 - logprior: 1.1820
Epoch 9/10
10/10 - 1s - loss: 225.7022 - loglik: -2.2645e+02 - logprior: 1.6023
Epoch 10/10
10/10 - 1s - loss: 224.8501 - loglik: -2.2605e+02 - logprior: 1.9967
Fitted a model with MAP estimate = -223.6636
expansions: [(0, 6), (51, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 376.6206 - loglik: -2.2318e+02 - logprior: -1.5268e+02
Epoch 2/2
10/10 - 1s - loss: 267.4794 - loglik: -2.2193e+02 - logprior: -4.4806e+01
Fitted a model with MAP estimate = -246.3263
expansions: [(43, 6), (51, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 326.5064 - loglik: -2.1969e+02 - logprior: -1.0604e+02
Epoch 2/2
10/10 - 1s - loss: 245.2987 - loglik: -2.1890e+02 - logprior: -2.5602e+01
Fitted a model with MAP estimate = -232.5186
expansions: []
discards: [57]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 317.7724 - loglik: -2.1719e+02 - logprior: -9.9744e+01
Epoch 2/10
10/10 - 1s - loss: 242.2508 - loglik: -2.1732e+02 - logprior: -2.4010e+01
Epoch 3/10
10/10 - 1s - loss: 226.6746 - loglik: -2.1777e+02 - logprior: -7.9115e+00
Epoch 4/10
10/10 - 1s - loss: 220.1340 - loglik: -2.1814e+02 - logprior: -9.5073e-01
Epoch 5/10
10/10 - 1s - loss: 216.4172 - loglik: -2.1826e+02 - logprior: 2.9463
Epoch 6/10
10/10 - 1s - loss: 214.0789 - loglik: -2.1821e+02 - logprior: 5.2704
Epoch 7/10
10/10 - 1s - loss: 212.4924 - loglik: -2.1805e+02 - logprior: 6.7285
Epoch 8/10
10/10 - 1s - loss: 211.3547 - loglik: -2.1793e+02 - logprior: 7.7645
Epoch 9/10
10/10 - 1s - loss: 210.4789 - loglik: -2.1787e+02 - logprior: 8.5929
Epoch 10/10
10/10 - 1s - loss: 209.7303 - loglik: -2.1783e+02 - logprior: 9.3164
Fitted a model with MAP estimate = -208.1221
Time for alignment: 46.0687
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 387.2053 - loglik: -2.7272e+02 - logprior: -1.1446e+02
Epoch 2/10
10/10 - 1s - loss: 290.3580 - loglik: -2.6162e+02 - logprior: -2.8563e+01
Epoch 3/10
10/10 - 1s - loss: 262.0307 - loglik: -2.5083e+02 - logprior: -1.1028e+01
Epoch 4/10
10/10 - 1s - loss: 246.5851 - loglik: -2.4177e+02 - logprior: -4.7504e+00
Epoch 5/10
10/10 - 1s - loss: 237.2751 - loglik: -2.3518e+02 - logprior: -1.9078e+00
Epoch 6/10
10/10 - 1s - loss: 231.8958 - loglik: -2.3106e+02 - logprior: -2.8056e-01
Epoch 7/10
10/10 - 1s - loss: 228.7125 - loglik: -2.2847e+02 - logprior: 0.6030
Epoch 8/10
10/10 - 1s - loss: 226.8533 - loglik: -2.2713e+02 - logprior: 1.1849
Epoch 9/10
10/10 - 1s - loss: 225.7198 - loglik: -2.2648e+02 - logprior: 1.6113
Epoch 10/10
10/10 - 1s - loss: 224.9671 - loglik: -2.2621e+02 - logprior: 2.0523
Fitted a model with MAP estimate = -223.8540
expansions: [(0, 6), (51, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 376.9492 - loglik: -2.2353e+02 - logprior: -1.5258e+02
Epoch 2/2
10/10 - 1s - loss: 267.9053 - loglik: -2.2234e+02 - logprior: -4.4707e+01
Fitted a model with MAP estimate = -246.5519
expansions: [(43, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 326.7231 - loglik: -2.1978e+02 - logprior: -1.0613e+02
Epoch 2/2
10/10 - 1s - loss: 246.1458 - loglik: -2.1929e+02 - logprior: -2.6041e+01
Fitted a model with MAP estimate = -233.4626
expansions: [(43, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 318.4753 - loglik: -2.1766e+02 - logprior: -9.9993e+01
Epoch 2/10
10/10 - 1s - loss: 242.6155 - loglik: -2.1764e+02 - logprior: -2.4104e+01
Epoch 3/10
10/10 - 1s - loss: 227.0064 - loglik: -2.1809e+02 - logprior: -7.9842e+00
Epoch 4/10
10/10 - 1s - loss: 220.4357 - loglik: -2.1838e+02 - logprior: -1.0441e+00
Epoch 5/10
10/10 - 1s - loss: 216.6669 - loglik: -2.1839e+02 - logprior: 2.8076
Epoch 6/10
10/10 - 1s - loss: 214.3021 - loglik: -2.1829e+02 - logprior: 5.1141
Epoch 7/10
10/10 - 1s - loss: 212.6244 - loglik: -2.1796e+02 - logprior: 6.5280
Epoch 8/10
10/10 - 1s - loss: 211.3898 - loglik: -2.1772e+02 - logprior: 7.5415
Epoch 9/10
10/10 - 1s - loss: 210.4733 - loglik: -2.1763e+02 - logprior: 8.3687
Epoch 10/10
10/10 - 1s - loss: 209.6561 - loglik: -2.1748e+02 - logprior: 9.0615
Fitted a model with MAP estimate = -208.0173
Time for alignment: 46.0021
Computed alignments with likelihoods: ['-207.2269', '-208.1221', '-208.0173']
Best model has likelihood: -207.2269
time for generating output: 0.1702
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyt3.projection.fasta
SP score = 0.7242284963887065
Training of 3 independent models on file mofe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f33355bbac0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3436143550>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f348d192460>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f30f8faca00>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f30f8facac0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3495df2b20>, <__main__.SimpleDirichletPrior object at 0x7f33572fbfd0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34b798dee0>

Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 25s - loss: 996.2695 - loglik: -9.8783e+02 - logprior: -8.3229e+00
Epoch 2/10
19/19 - 20s - loss: 873.1391 - loglik: -8.7287e+02 - logprior: 0.3143
Epoch 3/10
19/19 - 20s - loss: 812.3113 - loglik: -8.1023e+02 - logprior: -1.3119e+00
Epoch 4/10
19/19 - 19s - loss: 794.1984 - loglik: -7.9133e+02 - logprior: -1.8714e+00
Epoch 5/10
19/19 - 20s - loss: 787.0795 - loglik: -7.8424e+02 - logprior: -1.9119e+00
Epoch 6/10
19/19 - 20s - loss: 785.0081 - loglik: -7.8206e+02 - logprior: -2.1198e+00
Epoch 7/10
19/19 - 20s - loss: 784.8738 - loglik: -7.8186e+02 - logprior: -2.1974e+00
Epoch 8/10
19/19 - 20s - loss: 778.5831 - loglik: -7.7558e+02 - logprior: -2.1976e+00
Epoch 9/10
19/19 - 20s - loss: 782.8565 - loglik: -7.7985e+02 - logprior: -2.2262e+00
Fitted a model with MAP estimate = -779.5548
expansions: [(14, 1), (30, 1), (65, 1), (67, 1), (100, 2), (114, 1), (115, 2), (119, 1), (120, 1), (121, 1), (123, 8), (124, 1), (125, 2), (143, 1), (145, 1), (158, 1), (163, 1), (164, 1), (167, 1), (169, 1), (170, 1), (174, 1), (177, 2), (178, 1), (179, 1), (180, 1), (189, 1), (191, 2), (199, 1), (205, 1), (211, 1), (212, 1), (222, 1), (223, 2), (225, 1), (238, 2), (239, 2), (260, 1), (263, 1), (264, 6), (300, 1), (301, 1), (302, 2), (303, 1), (309, 1), (310, 1), (311, 1), (312, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 388 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 30s - loss: 777.5411 - loglik: -7.6460e+02 - logprior: -1.2204e+01
Epoch 2/2
19/19 - 25s - loss: 749.1658 - loglik: -7.4454e+02 - logprior: -3.8880e+00
Fitted a model with MAP estimate = -744.3939
expansions: [(0, 2)]
discards: [  0 103 228]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 387 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 29s - loss: 748.9609 - loglik: -7.4054e+02 - logprior: -7.7152e+00
Epoch 2/2
19/19 - 25s - loss: 737.8744 - loglik: -7.3777e+02 - logprior: 0.6027
Fitted a model with MAP estimate = -735.3633
expansions: []
discards: [  0 119]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 385 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 29s - loss: 752.5604 - loglik: -7.4076e+02 - logprior: -1.1101e+01
Epoch 2/10
19/19 - 26s - loss: 741.7577 - loglik: -7.4007e+02 - logprior: -9.5887e-01
Epoch 3/10
19/19 - 25s - loss: 733.3930 - loglik: -7.3510e+02 - logprior: 2.3922
Epoch 4/10
19/19 - 26s - loss: 736.5386 - loglik: -7.3907e+02 - logprior: 3.1785
Fitted a model with MAP estimate = -733.4840
Time for alignment: 466.9103
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 23s - loss: 997.4438 - loglik: -9.8903e+02 - logprior: -8.3026e+00
Epoch 2/10
19/19 - 20s - loss: 873.2092 - loglik: -8.7281e+02 - logprior: 0.2166
Epoch 3/10
19/19 - 20s - loss: 806.3553 - loglik: -8.0399e+02 - logprior: -1.3189e+00
Epoch 4/10
19/19 - 20s - loss: 790.9744 - loglik: -7.8776e+02 - logprior: -1.8406e+00
Epoch 5/10
19/19 - 20s - loss: 786.8442 - loglik: -7.8393e+02 - logprior: -1.8285e+00
Epoch 6/10
19/19 - 20s - loss: 780.8660 - loglik: -7.7799e+02 - logprior: -2.0242e+00
Epoch 7/10
19/19 - 20s - loss: 780.3467 - loglik: -7.7740e+02 - logprior: -2.1697e+00
Epoch 8/10
19/19 - 20s - loss: 776.2387 - loglik: -7.7322e+02 - logprior: -2.2499e+00
Epoch 9/10
19/19 - 20s - loss: 776.4266 - loglik: -7.7342e+02 - logprior: -2.2657e+00
Fitted a model with MAP estimate = -775.7157
expansions: [(32, 1), (61, 1), (97, 1), (98, 1), (99, 2), (107, 1), (113, 1), (114, 1), (117, 1), (118, 1), (119, 1), (121, 9), (123, 2), (157, 1), (164, 1), (166, 1), (167, 1), (168, 1), (169, 1), (176, 2), (177, 1), (178, 1), (190, 4), (198, 1), (204, 1), (218, 3), (219, 1), (220, 1), (221, 1), (222, 1), (223, 2), (236, 1), (237, 1), (241, 1), (259, 1), (262, 3), (273, 1), (281, 1), (282, 1), (296, 3), (301, 1), (302, 2), (311, 3), (312, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 387 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 30s - loss: 772.0677 - loglik: -7.5863e+02 - logprior: -1.2722e+01
Epoch 2/2
19/19 - 25s - loss: 745.1323 - loglik: -7.4011e+02 - logprior: -4.3101e+00
Fitted a model with MAP estimate = -738.5818
expansions: [(0, 2), (315, 3), (354, 1)]
discards: [  0 102 144 145 204 257 267]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 386 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 29s - loss: 742.6386 - loglik: -7.3392e+02 - logprior: -7.9373e+00
Epoch 2/2
19/19 - 25s - loss: 730.5477 - loglik: -7.3023e+02 - logprior: 0.4438
Fitted a model with MAP estimate = -727.5121
expansions: [(361, 1), (364, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 387 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 29s - loss: 744.9300 - loglik: -7.3254e+02 - logprior: -1.1639e+01
Epoch 2/10
19/19 - 26s - loss: 734.2379 - loglik: -7.3207e+02 - logprior: -1.4169e+00
Epoch 3/10
19/19 - 25s - loss: 724.4493 - loglik: -7.2597e+02 - logprior: 2.2160
Epoch 4/10
19/19 - 25s - loss: 723.5946 - loglik: -7.2595e+02 - logprior: 3.0129
Epoch 5/10
19/19 - 25s - loss: 724.3982 - loglik: -7.2726e+02 - logprior: 3.4957
Fitted a model with MAP estimate = -723.8515
Time for alignment: 489.6524
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 25s - loss: 998.9094 - loglik: -9.9050e+02 - logprior: -8.2996e+00
Epoch 2/10
19/19 - 20s - loss: 870.3155 - loglik: -8.7009e+02 - logprior: 0.3511
Epoch 3/10
19/19 - 20s - loss: 805.3231 - loglik: -8.0335e+02 - logprior: -1.0004e+00
Epoch 4/10
19/19 - 20s - loss: 789.8809 - loglik: -7.8712e+02 - logprior: -1.4641e+00
Epoch 5/10
19/19 - 20s - loss: 788.5607 - loglik: -7.8594e+02 - logprior: -1.5607e+00
Epoch 6/10
19/19 - 20s - loss: 776.6439 - loglik: -7.7376e+02 - logprior: -1.9824e+00
Epoch 7/10
19/19 - 20s - loss: 783.0107 - loglik: -7.8014e+02 - logprior: -2.0556e+00
Fitted a model with MAP estimate = -779.3710
expansions: [(14, 1), (30, 1), (68, 1), (69, 1), (94, 1), (95, 1), (96, 2), (97, 2), (111, 2), (112, 3), (115, 1), (117, 1), (120, 3), (123, 5), (142, 1), (143, 1), (157, 1), (163, 2), (165, 1), (166, 1), (169, 1), (173, 1), (174, 1), (175, 1), (176, 1), (177, 1), (189, 4), (204, 1), (218, 1), (220, 1), (221, 1), (222, 2), (223, 1), (226, 1), (239, 1), (240, 1), (241, 1), (265, 7), (302, 3), (303, 1), (311, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 386 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 29s - loss: 774.5355 - loglik: -7.6133e+02 - logprior: -1.2454e+01
Epoch 2/2
19/19 - 26s - loss: 750.6252 - loglik: -7.4557e+02 - logprior: -4.3553e+00
Fitted a model with MAP estimate = -742.1768
expansions: [(0, 2), (338, 3), (380, 1)]
discards: [  0 121 145]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 389 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 29s - loss: 745.3765 - loglik: -7.3652e+02 - logprior: -8.1342e+00
Epoch 2/2
19/19 - 26s - loss: 730.8904 - loglik: -7.3036e+02 - logprior: 0.1759
Fitted a model with MAP estimate = -730.2954
expansions: [(340, 2)]
discards: [  0 106 121]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 388 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 31s - loss: 745.8914 - loglik: -7.3369e+02 - logprior: -1.1499e+01
Epoch 2/10
19/19 - 26s - loss: 738.5596 - loglik: -7.3638e+02 - logprior: -1.4900e+00
Epoch 3/10
19/19 - 26s - loss: 727.9343 - loglik: -7.2924e+02 - logprior: 1.9672
Epoch 4/10
19/19 - 26s - loss: 729.0060 - loglik: -7.3093e+02 - logprior: 2.5415
Fitted a model with MAP estimate = -727.3520
Time for alignment: 428.9766
Computed alignments with likelihoods: ['-733.4840', '-723.8515', '-727.3520']
Best model has likelihood: -723.8515
time for generating output: 0.4817
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mofe.projection.fasta
SP score = 0.7328725961538461
Training of 3 independent models on file ghf22.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2b1c8a6ee0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34a6839fa0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34a6839910>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f2b1cdbe2e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3357297a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f2b1c843f10>, <__main__.SimpleDirichletPrior object at 0x7f32ee1f3580>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3495c95310>

Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 369.4217 - loglik: -3.1231e+02 - logprior: -5.7095e+01
Epoch 2/10
10/10 - 1s - loss: 290.8414 - loglik: -2.7663e+02 - logprior: -1.4192e+01
Epoch 3/10
10/10 - 1s - loss: 241.2043 - loglik: -2.3446e+02 - logprior: -6.7140e+00
Epoch 4/10
10/10 - 1s - loss: 213.9254 - loglik: -2.0920e+02 - logprior: -4.6679e+00
Epoch 5/10
10/10 - 1s - loss: 203.7319 - loglik: -2.0010e+02 - logprior: -3.5879e+00
Epoch 6/10
10/10 - 1s - loss: 199.3357 - loglik: -1.9623e+02 - logprior: -2.9532e+00
Epoch 7/10
10/10 - 1s - loss: 197.3963 - loglik: -1.9455e+02 - logprior: -2.5532e+00
Epoch 8/10
10/10 - 1s - loss: 195.7760 - loglik: -1.9312e+02 - logprior: -2.3653e+00
Epoch 9/10
10/10 - 1s - loss: 194.3478 - loglik: -1.9188e+02 - logprior: -2.2258e+00
Epoch 10/10
10/10 - 1s - loss: 194.2414 - loglik: -1.9197e+02 - logprior: -2.0452e+00
Fitted a model with MAP estimate = -193.6570
expansions: [(13, 3), (17, 3), (25, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (59, 1), (62, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 249.1128 - loglik: -1.8416e+02 - logprior: -6.4716e+01
Epoch 2/2
10/10 - 2s - loss: 199.0028 - loglik: -1.7276e+02 - logprior: -2.5994e+01
Fitted a model with MAP estimate = -190.3640
expansions: [(0, 2), (69, 1)]
discards: [ 0 20 98]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 219.4950 - loglik: -1.6803e+02 - logprior: -5.1206e+01
Epoch 2/2
10/10 - 2s - loss: 178.4046 - loglik: -1.6581e+02 - logprior: -1.2315e+01
Fitted a model with MAP estimate = -172.4519
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 229.2601 - loglik: -1.6735e+02 - logprior: -6.1632e+01
Epoch 2/10
10/10 - 2s - loss: 185.7396 - loglik: -1.6722e+02 - logprior: -1.8234e+01
Epoch 3/10
10/10 - 2s - loss: 173.3646 - loglik: -1.6772e+02 - logprior: -5.3580e+00
Epoch 4/10
10/10 - 2s - loss: 168.4557 - loglik: -1.6733e+02 - logprior: -8.3756e-01
Epoch 5/10
10/10 - 2s - loss: 166.7483 - loglik: -1.6758e+02 - logprior: 1.1236
Epoch 6/10
10/10 - 2s - loss: 165.7029 - loglik: -1.6758e+02 - logprior: 2.1733
Epoch 7/10
10/10 - 2s - loss: 164.7441 - loglik: -1.6733e+02 - logprior: 2.8809
Epoch 8/10
10/10 - 2s - loss: 164.6748 - loglik: -1.6784e+02 - logprior: 3.4659
Epoch 9/10
10/10 - 2s - loss: 164.1712 - loglik: -1.6783e+02 - logprior: 3.9641
Epoch 10/10
10/10 - 2s - loss: 163.7547 - loglik: -1.6781e+02 - logprior: 4.3572
Fitted a model with MAP estimate = -163.3900
Time for alignment: 52.8765
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 369.5587 - loglik: -3.1244e+02 - logprior: -5.7094e+01
Epoch 2/10
10/10 - 1s - loss: 290.6682 - loglik: -2.7646e+02 - logprior: -1.4192e+01
Epoch 3/10
10/10 - 1s - loss: 242.9710 - loglik: -2.3619e+02 - logprior: -6.7522e+00
Epoch 4/10
10/10 - 1s - loss: 215.7769 - loglik: -2.1101e+02 - logprior: -4.7126e+00
Epoch 5/10
10/10 - 1s - loss: 203.7023 - loglik: -1.9999e+02 - logprior: -3.6701e+00
Epoch 6/10
10/10 - 1s - loss: 198.2871 - loglik: -1.9505e+02 - logprior: -3.0746e+00
Epoch 7/10
10/10 - 1s - loss: 196.4554 - loglik: -1.9347e+02 - logprior: -2.6647e+00
Epoch 8/10
10/10 - 1s - loss: 195.3119 - loglik: -1.9263e+02 - logprior: -2.3473e+00
Epoch 9/10
10/10 - 1s - loss: 194.8387 - loglik: -1.9241e+02 - logprior: -2.1406e+00
Epoch 10/10
10/10 - 1s - loss: 194.7245 - loglik: -1.9250e+02 - logprior: -1.9645e+00
Fitted a model with MAP estimate = -194.1555
expansions: [(13, 5), (17, 2), (19, 1), (32, 1), (33, 1), (34, 1), (53, 1), (55, 2), (56, 2), (57, 1), (58, 1), (62, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 248.7170 - loglik: -1.8379e+02 - logprior: -6.4658e+01
Epoch 2/2
10/10 - 2s - loss: 197.7693 - loglik: -1.7159e+02 - logprior: -2.5927e+01
Fitted a model with MAP estimate = -189.2511
expansions: [(0, 2), (69, 1)]
discards: [ 0 21 22 98]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 219.6349 - loglik: -1.6808e+02 - logprior: -5.1281e+01
Epoch 2/2
10/10 - 2s - loss: 179.1429 - loglik: -1.6641e+02 - logprior: -1.2448e+01
Fitted a model with MAP estimate = -172.7408
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 229.4856 - loglik: -1.6733e+02 - logprior: -6.1856e+01
Epoch 2/10
10/10 - 1s - loss: 186.4870 - loglik: -1.6776e+02 - logprior: -1.8431e+01
Epoch 3/10
10/10 - 1s - loss: 173.0183 - loglik: -1.6718e+02 - logprior: -5.5322e+00
Epoch 4/10
10/10 - 2s - loss: 168.6991 - loglik: -1.6738e+02 - logprior: -1.0215e+00
Epoch 5/10
10/10 - 2s - loss: 167.0285 - loglik: -1.6766e+02 - logprior: 0.9400
Epoch 6/10
10/10 - 1s - loss: 165.7043 - loglik: -1.6739e+02 - logprior: 1.9898
Epoch 7/10
10/10 - 2s - loss: 165.2172 - loglik: -1.6759e+02 - logprior: 2.6820
Epoch 8/10
10/10 - 2s - loss: 165.0547 - loglik: -1.6800e+02 - logprior: 3.2521
Epoch 9/10
10/10 - 2s - loss: 164.1133 - loglik: -1.6754e+02 - logprior: 3.7424
Epoch 10/10
10/10 - 2s - loss: 164.3837 - loglik: -1.6822e+02 - logprior: 4.1440
Fitted a model with MAP estimate = -163.6074
Time for alignment: 52.3432
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 369.5904 - loglik: -3.1247e+02 - logprior: -5.7097e+01
Epoch 2/10
10/10 - 1s - loss: 291.6590 - loglik: -2.7745e+02 - logprior: -1.4190e+01
Epoch 3/10
10/10 - 1s - loss: 241.4211 - loglik: -2.3462e+02 - logprior: -6.7702e+00
Epoch 4/10
10/10 - 1s - loss: 214.7138 - loglik: -2.1004e+02 - logprior: -4.6087e+00
Epoch 5/10
10/10 - 1s - loss: 204.6566 - loglik: -2.0120e+02 - logprior: -3.4217e+00
Epoch 6/10
10/10 - 1s - loss: 199.9556 - loglik: -1.9708e+02 - logprior: -2.8336e+00
Epoch 7/10
10/10 - 1s - loss: 197.0578 - loglik: -1.9445e+02 - logprior: -2.4783e+00
Epoch 8/10
10/10 - 1s - loss: 195.4622 - loglik: -1.9299e+02 - logprior: -2.2238e+00
Epoch 9/10
10/10 - 1s - loss: 194.9215 - loglik: -1.9259e+02 - logprior: -2.0741e+00
Epoch 10/10
10/10 - 1s - loss: 193.9001 - loglik: -1.9172e+02 - logprior: -1.9550e+00
Fitted a model with MAP estimate = -193.6608
expansions: [(13, 5), (17, 2), (18, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (60, 1), (62, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 248.7316 - loglik: -1.8386e+02 - logprior: -6.4652e+01
Epoch 2/2
10/10 - 2s - loss: 198.0408 - loglik: -1.7197e+02 - logprior: -2.5836e+01
Fitted a model with MAP estimate = -189.0420
expansions: [(0, 2), (70, 1)]
discards: [ 0 14 99]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 217.8621 - loglik: -1.6648e+02 - logprior: -5.1117e+01
Epoch 2/2
10/10 - 2s - loss: 176.9865 - loglik: -1.6449e+02 - logprior: -1.2211e+01
Fitted a model with MAP estimate = -170.7803
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 227.6042 - loglik: -1.6574e+02 - logprior: -6.1560e+01
Epoch 2/10
10/10 - 2s - loss: 184.0485 - loglik: -1.6553e+02 - logprior: -1.8219e+01
Epoch 3/10
10/10 - 2s - loss: 171.2227 - loglik: -1.6561e+02 - logprior: -5.3071e+00
Epoch 4/10
10/10 - 2s - loss: 166.7690 - loglik: -1.6567e+02 - logprior: -7.8630e-01
Epoch 5/10
10/10 - 2s - loss: 164.5102 - loglik: -1.6539e+02 - logprior: 1.1901
Epoch 6/10
10/10 - 2s - loss: 163.9908 - loglik: -1.6591e+02 - logprior: 2.2391
Epoch 7/10
10/10 - 2s - loss: 162.9264 - loglik: -1.6552e+02 - logprior: 2.9211
Epoch 8/10
10/10 - 2s - loss: 162.6216 - loglik: -1.6579e+02 - logprior: 3.4979
Epoch 9/10
10/10 - 2s - loss: 162.2354 - loglik: -1.6589e+02 - logprior: 3.9852
Epoch 10/10
10/10 - 2s - loss: 162.1057 - loglik: -1.6615e+02 - logprior: 4.3821
Fitted a model with MAP estimate = -161.4081
Time for alignment: 52.2969
Computed alignments with likelihoods: ['-163.3900', '-163.6074', '-161.4081']
Best model has likelihood: -161.4081
time for generating output: 0.1793
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf22.projection.fasta
SP score = 0.9311943393924607
Training of 3 independent models on file flav.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f342d686100>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f32f9ec3cd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32f045cfd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f335fa041f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f342d71e070>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f338a755430>, <__main__.SimpleDirichletPrior object at 0x7f34b7965e80>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f346b6e9e50>

Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 396.9826 - loglik: -3.8871e+02 - logprior: -8.2232e+00
Epoch 2/10
13/13 - 3s - loss: 355.3618 - loglik: -3.5307e+02 - logprior: -1.9830e+00
Epoch 3/10
13/13 - 3s - loss: 328.3726 - loglik: -3.2605e+02 - logprior: -1.7364e+00
Epoch 4/10
13/13 - 3s - loss: 314.2219 - loglik: -3.1126e+02 - logprior: -2.0009e+00
Epoch 5/10
13/13 - 3s - loss: 308.2832 - loglik: -3.0551e+02 - logprior: -1.9286e+00
Epoch 6/10
13/13 - 3s - loss: 304.6883 - loglik: -3.0221e+02 - logprior: -1.8872e+00
Epoch 7/10
13/13 - 3s - loss: 303.8170 - loglik: -3.0139e+02 - logprior: -1.9441e+00
Epoch 8/10
13/13 - 3s - loss: 302.6549 - loglik: -3.0031e+02 - logprior: -1.9116e+00
Epoch 9/10
13/13 - 3s - loss: 302.4505 - loglik: -3.0012e+02 - logprior: -1.8947e+00
Epoch 10/10
13/13 - 3s - loss: 301.9655 - loglik: -2.9962e+02 - logprior: -1.9041e+00
Fitted a model with MAP estimate = -301.4170
expansions: [(6, 1), (19, 1), (20, 1), (21, 2), (23, 1), (24, 2), (25, 2), (26, 1), (27, 2), (28, 1), (45, 1), (51, 1), (52, 1), (58, 2), (64, 2), (79, 1), (80, 1), (92, 7), (99, 1), (100, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 311.6647 - loglik: -3.0145e+02 - logprior: -9.8124e+00
Epoch 2/2
13/13 - 3s - loss: 294.8178 - loglik: -2.8962e+02 - logprior: -4.8078e+00
Fitted a model with MAP estimate = -291.3839
expansions: [(0, 2)]
discards: [  0  23  29  37  82 118 119 120]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 294.4104 - loglik: -2.8647e+02 - logprior: -7.5080e+00
Epoch 2/2
13/13 - 3s - loss: 286.3645 - loglik: -2.8375e+02 - logprior: -2.1880e+00
Fitted a model with MAP estimate = -285.0363
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 295.9123 - loglik: -2.8622e+02 - logprior: -9.2619e+00
Epoch 2/10
13/13 - 3s - loss: 287.9710 - loglik: -2.8455e+02 - logprior: -3.0011e+00
Epoch 3/10
13/13 - 3s - loss: 285.6071 - loglik: -2.8368e+02 - logprior: -1.4991e+00
Epoch 4/10
13/13 - 3s - loss: 284.8056 - loglik: -2.8327e+02 - logprior: -1.1157e+00
Epoch 5/10
13/13 - 3s - loss: 284.4413 - loglik: -2.8313e+02 - logprior: -8.9907e-01
Epoch 6/10
13/13 - 3s - loss: 284.6750 - loglik: -2.8346e+02 - logprior: -8.0104e-01
Fitted a model with MAP estimate = -283.8232
Time for alignment: 88.4746
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 396.6214 - loglik: -3.8834e+02 - logprior: -8.2308e+00
Epoch 2/10
13/13 - 3s - loss: 354.9841 - loglik: -3.5268e+02 - logprior: -1.9853e+00
Epoch 3/10
13/13 - 3s - loss: 326.8741 - loglik: -3.2471e+02 - logprior: -1.7097e+00
Epoch 4/10
13/13 - 3s - loss: 313.6563 - loglik: -3.1101e+02 - logprior: -2.0271e+00
Epoch 5/10
13/13 - 3s - loss: 309.0363 - loglik: -3.0637e+02 - logprior: -1.9464e+00
Epoch 6/10
13/13 - 3s - loss: 305.5804 - loglik: -3.0313e+02 - logprior: -1.8679e+00
Epoch 7/10
13/13 - 3s - loss: 303.7407 - loglik: -3.0133e+02 - logprior: -1.9252e+00
Epoch 8/10
13/13 - 3s - loss: 303.1111 - loglik: -3.0067e+02 - logprior: -1.9832e+00
Epoch 9/10
13/13 - 3s - loss: 301.2834 - loglik: -2.9885e+02 - logprior: -2.0018e+00
Epoch 10/10
13/13 - 3s - loss: 301.1535 - loglik: -2.9874e+02 - logprior: -2.0201e+00
Fitted a model with MAP estimate = -300.5827
expansions: [(6, 1), (19, 1), (20, 1), (21, 2), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (33, 1), (51, 1), (52, 1), (55, 1), (58, 3), (64, 2), (79, 1), (80, 1), (92, 1), (96, 1), (100, 3), (101, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 137 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 309.2141 - loglik: -2.9904e+02 - logprior: -9.8200e+00
Epoch 2/2
13/13 - 3s - loss: 294.1326 - loglik: -2.8905e+02 - logprior: -4.7146e+00
Fitted a model with MAP estimate = -290.8455
expansions: [(0, 2)]
discards: [ 0 23 37 74 75 82]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 293.8366 - loglik: -2.8599e+02 - logprior: -7.4640e+00
Epoch 2/2
13/13 - 3s - loss: 286.7449 - loglik: -2.8422e+02 - logprior: -2.1380e+00
Fitted a model with MAP estimate = -285.2010
expansions: [(121, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 296.0218 - loglik: -2.8630e+02 - logprior: -9.3339e+00
Epoch 2/10
13/13 - 3s - loss: 289.0001 - loglik: -2.8555e+02 - logprior: -3.0645e+00
Epoch 3/10
13/13 - 3s - loss: 285.8387 - loglik: -2.8404e+02 - logprior: -1.4161e+00
Epoch 4/10
13/13 - 3s - loss: 285.3028 - loglik: -2.8387e+02 - logprior: -1.0479e+00
Epoch 5/10
13/13 - 3s - loss: 284.3379 - loglik: -2.8312e+02 - logprior: -8.3738e-01
Epoch 6/10
13/13 - 3s - loss: 284.6203 - loglik: -2.8352e+02 - logprior: -7.2623e-01
Fitted a model with MAP estimate = -284.1957
Time for alignment: 89.8115
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 396.3257 - loglik: -3.8806e+02 - logprior: -8.2184e+00
Epoch 2/10
13/13 - 3s - loss: 354.4594 - loglik: -3.5218e+02 - logprior: -1.9774e+00
Epoch 3/10
13/13 - 3s - loss: 326.4590 - loglik: -3.2430e+02 - logprior: -1.7324e+00
Epoch 4/10
13/13 - 3s - loss: 312.2900 - loglik: -3.0959e+02 - logprior: -2.1213e+00
Epoch 5/10
13/13 - 3s - loss: 306.4453 - loglik: -3.0363e+02 - logprior: -2.1320e+00
Epoch 6/10
13/13 - 3s - loss: 304.0090 - loglik: -3.0143e+02 - logprior: -2.0416e+00
Epoch 7/10
13/13 - 3s - loss: 302.8616 - loglik: -3.0039e+02 - logprior: -2.0167e+00
Epoch 8/10
13/13 - 3s - loss: 302.8187 - loglik: -3.0040e+02 - logprior: -1.9801e+00
Epoch 9/10
13/13 - 3s - loss: 302.1821 - loglik: -2.9980e+02 - logprior: -1.9611e+00
Epoch 10/10
13/13 - 3s - loss: 301.8061 - loglik: -2.9942e+02 - logprior: -1.9706e+00
Fitted a model with MAP estimate = -301.4557
expansions: [(6, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 2), (25, 2), (26, 1), (27, 2), (28, 2), (32, 1), (52, 1), (55, 1), (58, 2), (64, 2), (80, 1), (81, 1), (82, 2), (83, 1), (100, 3), (101, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 310.7271 - loglik: -3.0055e+02 - logprior: -9.8083e+00
Epoch 2/2
13/13 - 3s - loss: 294.7322 - loglik: -2.8959e+02 - logprior: -4.7687e+00
Fitted a model with MAP estimate = -291.5438
expansions: [(0, 2)]
discards: [  0  28  37  39  82 102]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 294.7193 - loglik: -2.8681e+02 - logprior: -7.5112e+00
Epoch 2/2
13/13 - 3s - loss: 286.5668 - loglik: -2.8399e+02 - logprior: -2.1608e+00
Fitted a model with MAP estimate = -285.4546
expansions: []
discards: [  0  73 125]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 297.1105 - loglik: -2.8752e+02 - logprior: -9.1914e+00
Epoch 2/10
13/13 - 3s - loss: 289.0085 - loglik: -2.8570e+02 - logprior: -2.9288e+00
Epoch 3/10
13/13 - 3s - loss: 287.1015 - loglik: -2.8529e+02 - logprior: -1.4373e+00
Epoch 4/10
13/13 - 3s - loss: 286.0797 - loglik: -2.8466e+02 - logprior: -1.0496e+00
Epoch 5/10
13/13 - 3s - loss: 286.2252 - loglik: -2.8501e+02 - logprior: -8.5349e-01
Fitted a model with MAP estimate = -285.4780
Time for alignment: 83.8700
Computed alignments with likelihoods: ['-283.8232', '-284.1957', '-285.4546']
Best model has likelihood: -283.8232
time for generating output: 0.2295
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/flav.projection.fasta
SP score = 0.796617712505563
Training of 3 independent models on file sodfe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34ee913d30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3356bfb2e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2b243dc3a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f33237249d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33237246d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3356ffd940>, <__main__.SimpleDirichletPrior object at 0x7f32f083d5b0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3495cc4d30>

Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 222.7144 - loglik: -2.1403e+02 - logprior: -8.6842e+00
Epoch 2/10
13/13 - 2s - loss: 171.7602 - loglik: -1.6940e+02 - logprior: -2.3003e+00
Epoch 3/10
13/13 - 2s - loss: 143.4529 - loglik: -1.4156e+02 - logprior: -1.8138e+00
Epoch 4/10
13/13 - 2s - loss: 136.4236 - loglik: -1.3463e+02 - logprior: -1.7174e+00
Epoch 5/10
13/13 - 2s - loss: 134.1518 - loglik: -1.3231e+02 - logprior: -1.6257e+00
Epoch 6/10
13/13 - 2s - loss: 133.3051 - loglik: -1.3142e+02 - logprior: -1.6371e+00
Epoch 7/10
13/13 - 2s - loss: 132.9643 - loglik: -1.3114e+02 - logprior: -1.6019e+00
Epoch 8/10
13/13 - 2s - loss: 132.5061 - loglik: -1.3069e+02 - logprior: -1.5971e+00
Epoch 9/10
13/13 - 2s - loss: 132.4080 - loglik: -1.3059e+02 - logprior: -1.5885e+00
Epoch 10/10
13/13 - 2s - loss: 132.2076 - loglik: -1.3040e+02 - logprior: -1.5760e+00
Fitted a model with MAP estimate = -131.9251
expansions: [(0, 4), (13, 1), (36, 4), (37, 2), (38, 2), (43, 3), (44, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 135.5585 - loglik: -1.2497e+02 - logprior: -1.0373e+01
Epoch 2/2
13/13 - 2s - loss: 121.0301 - loglik: -1.1754e+02 - logprior: -3.2552e+00
Fitted a model with MAP estimate = -117.5857
expansions: [(0, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 124.6672 - loglik: -1.1412e+02 - logprior: -1.0288e+01
Epoch 2/2
13/13 - 2s - loss: 116.1762 - loglik: -1.1246e+02 - logprior: -3.4482e+00
Fitted a model with MAP estimate = -114.2486
expansions: []
discards: [49]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 120.8785 - loglik: -1.1245e+02 - logprior: -8.1521e+00
Epoch 2/10
13/13 - 2s - loss: 114.4319 - loglik: -1.1166e+02 - logprior: -2.4900e+00
Epoch 3/10
13/13 - 2s - loss: 113.8079 - loglik: -1.1156e+02 - logprior: -1.9589e+00
Epoch 4/10
13/13 - 2s - loss: 113.6737 - loglik: -1.1179e+02 - logprior: -1.5890e+00
Epoch 5/10
13/13 - 2s - loss: 113.1633 - loglik: -1.1136e+02 - logprior: -1.5015e+00
Epoch 6/10
13/13 - 2s - loss: 113.5208 - loglik: -1.1178e+02 - logprior: -1.4409e+00
Fitted a model with MAP estimate = -112.8872
Time for alignment: 63.0264
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 223.0123 - loglik: -2.1432e+02 - logprior: -8.6827e+00
Epoch 2/10
13/13 - 2s - loss: 174.5321 - loglik: -1.7214e+02 - logprior: -2.3366e+00
Epoch 3/10
13/13 - 2s - loss: 146.5787 - loglik: -1.4449e+02 - logprior: -2.0240e+00
Epoch 4/10
13/13 - 2s - loss: 136.9972 - loglik: -1.3495e+02 - logprior: -2.0242e+00
Epoch 5/10
13/13 - 2s - loss: 135.3210 - loglik: -1.3325e+02 - logprior: -1.8954e+00
Epoch 6/10
13/13 - 2s - loss: 134.2876 - loglik: -1.3217e+02 - logprior: -1.8882e+00
Epoch 7/10
13/13 - 2s - loss: 133.8200 - loglik: -1.3179e+02 - logprior: -1.8336e+00
Epoch 8/10
13/13 - 2s - loss: 133.8651 - loglik: -1.3182e+02 - logprior: -1.8326e+00
Fitted a model with MAP estimate = -133.4154
expansions: [(0, 3), (13, 1), (14, 3), (16, 1), (34, 1), (35, 2), (36, 1), (37, 1), (38, 2), (43, 3), (44, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 135.3490 - loglik: -1.2483e+02 - logprior: -1.0317e+01
Epoch 2/2
13/13 - 2s - loss: 118.7176 - loglik: -1.1528e+02 - logprior: -3.1937e+00
Fitted a model with MAP estimate = -115.9841
expansions: []
discards: [51]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 121.0389 - loglik: -1.1267e+02 - logprior: -8.1115e+00
Epoch 2/2
13/13 - 2s - loss: 114.9620 - loglik: -1.1221e+02 - logprior: -2.5042e+00
Fitted a model with MAP estimate = -113.7162
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 119.7275 - loglik: -1.1147e+02 - logprior: -8.0058e+00
Epoch 2/10
13/13 - 2s - loss: 114.3635 - loglik: -1.1159e+02 - logprior: -2.4990e+00
Epoch 3/10
13/13 - 2s - loss: 113.5333 - loglik: -1.1138e+02 - logprior: -1.8827e+00
Epoch 4/10
13/13 - 2s - loss: 113.3681 - loglik: -1.1155e+02 - logprior: -1.5418e+00
Epoch 5/10
13/13 - 2s - loss: 113.2641 - loglik: -1.1152e+02 - logprior: -1.4572e+00
Epoch 6/10
13/13 - 2s - loss: 112.8740 - loglik: -1.1120e+02 - logprior: -1.3907e+00
Epoch 7/10
13/13 - 2s - loss: 112.3233 - loglik: -1.1063e+02 - logprior: -1.3951e+00
Epoch 8/10
13/13 - 2s - loss: 112.4801 - loglik: -1.1082e+02 - logprior: -1.3735e+00
Fitted a model with MAP estimate = -111.9400
Time for alignment: 61.7960
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 223.2114 - loglik: -2.1452e+02 - logprior: -8.6908e+00
Epoch 2/10
13/13 - 2s - loss: 173.3613 - loglik: -1.7097e+02 - logprior: -2.3299e+00
Epoch 3/10
13/13 - 2s - loss: 145.7125 - loglik: -1.4373e+02 - logprior: -1.9087e+00
Epoch 4/10
13/13 - 2s - loss: 136.3135 - loglik: -1.3444e+02 - logprior: -1.8066e+00
Epoch 5/10
13/13 - 2s - loss: 133.8179 - loglik: -1.3195e+02 - logprior: -1.6611e+00
Epoch 6/10
13/13 - 2s - loss: 132.7993 - loglik: -1.3089e+02 - logprior: -1.6804e+00
Epoch 7/10
13/13 - 2s - loss: 132.4486 - loglik: -1.3060e+02 - logprior: -1.6449e+00
Epoch 8/10
13/13 - 2s - loss: 131.9769 - loglik: -1.3013e+02 - logprior: -1.6555e+00
Epoch 9/10
13/13 - 2s - loss: 131.8635 - loglik: -1.3002e+02 - logprior: -1.6403e+00
Epoch 10/10
13/13 - 2s - loss: 131.8105 - loglik: -1.2997e+02 - logprior: -1.6233e+00
Fitted a model with MAP estimate = -131.4228
expansions: [(0, 4), (13, 1), (16, 1), (36, 2), (37, 3), (38, 2), (43, 3), (44, 2), (45, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 135.3595 - loglik: -1.2475e+02 - logprior: -1.0399e+01
Epoch 2/2
13/13 - 2s - loss: 118.0538 - loglik: -1.1447e+02 - logprior: -3.3475e+00
Fitted a model with MAP estimate = -114.7853
expansions: [(0, 2)]
discards: [49 61 62 63 64]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 125.7052 - loglik: -1.1518e+02 - logprior: -1.0270e+01
Epoch 2/2
13/13 - 2s - loss: 117.1527 - loglik: -1.1345e+02 - logprior: -3.4430e+00
Fitted a model with MAP estimate = -115.1091
expansions: [(62, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 120.5163 - loglik: -1.1211e+02 - logprior: -8.1444e+00
Epoch 2/10
13/13 - 2s - loss: 113.4718 - loglik: -1.1069e+02 - logprior: -2.5048e+00
Epoch 3/10
13/13 - 2s - loss: 111.5635 - loglik: -1.0924e+02 - logprior: -2.0361e+00
Epoch 4/10
13/13 - 2s - loss: 111.0146 - loglik: -1.0901e+02 - logprior: -1.7074e+00
Epoch 5/10
13/13 - 2s - loss: 111.0166 - loglik: -1.0909e+02 - logprior: -1.6297e+00
Fitted a model with MAP estimate = -110.3907
Time for alignment: 61.6709
Computed alignments with likelihoods: ['-112.8872', '-111.9400', '-110.3907']
Best model has likelihood: -110.3907
time for generating output: 0.2113
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodfe.projection.fasta
SP score = 0.5131727512231841
Training of 3 independent models on file zf-CCHH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34401687f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3346204760>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f30f8f9fa90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f30f8f9f670>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3118309ca0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f349e156520>, <__main__.SimpleDirichletPrior object at 0x7f3462a984f0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3323d0b310>

Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 5s - loss: 59.4315 - loglik: -5.8532e+01 - logprior: -8.7708e-01
Epoch 2/10
41/41 - 2s - loss: 46.3214 - loglik: -4.5259e+01 - logprior: -9.0484e-01
Epoch 3/10
41/41 - 2s - loss: 45.5045 - loglik: -4.4494e+01 - logprior: -8.8950e-01
Epoch 4/10
41/41 - 2s - loss: 45.3486 - loglik: -4.4314e+01 - logprior: -8.8411e-01
Epoch 5/10
41/41 - 2s - loss: 45.1858 - loglik: -4.4129e+01 - logprior: -8.8149e-01
Epoch 6/10
41/41 - 2s - loss: 44.9353 - loglik: -4.3867e+01 - logprior: -8.7816e-01
Epoch 7/10
41/41 - 2s - loss: 44.8696 - loglik: -4.3793e+01 - logprior: -8.7771e-01
Epoch 8/10
41/41 - 2s - loss: 44.8922 - loglik: -4.3813e+01 - logprior: -8.7766e-01
Fitted a model with MAP estimate = -44.6903
expansions: [(4, 1), (9, 2), (10, 1), (11, 2), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.4697 - loglik: -4.3173e+01 - logprior: -1.0875e+00
Epoch 2/2
41/41 - 1s - loss: 42.7145 - loglik: -4.1623e+01 - logprior: -8.7435e-01
Fitted a model with MAP estimate = -42.8292
expansions: []
discards: [11 15]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 43.1748 - loglik: -4.1892e+01 - logprior: -1.0600e+00
Epoch 2/2
41/41 - 2s - loss: 42.5178 - loglik: -4.1455e+01 - logprior: -8.4378e-01
Fitted a model with MAP estimate = -42.8927
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 42.5652 - loglik: -4.1666e+01 - logprior: -6.8184e-01
Epoch 2/10
58/58 - 2s - loss: 42.0087 - loglik: -4.1250e+01 - logprior: -5.8484e-01
Epoch 3/10
58/58 - 2s - loss: 42.0629 - loglik: -4.1315e+01 - logprior: -5.7728e-01
Fitted a model with MAP estimate = -41.5849
Time for alignment: 72.1173
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 7s - loss: 59.7543 - loglik: -5.8873e+01 - logprior: -8.7092e-01
Epoch 2/10
41/41 - 2s - loss: 46.4272 - loglik: -4.5508e+01 - logprior: -9.0218e-01
Epoch 3/10
41/41 - 2s - loss: 45.6008 - loglik: -4.4657e+01 - logprior: -8.8454e-01
Epoch 4/10
41/41 - 2s - loss: 45.4520 - loglik: -4.4442e+01 - logprior: -8.8320e-01
Epoch 5/10
41/41 - 2s - loss: 45.1595 - loglik: -4.4132e+01 - logprior: -8.7840e-01
Epoch 6/10
41/41 - 2s - loss: 45.1630 - loglik: -4.4111e+01 - logprior: -8.7791e-01
Fitted a model with MAP estimate = -44.6296
expansions: [(4, 1), (9, 2), (10, 1), (11, 2), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 44.7384 - loglik: -4.3459e+01 - logprior: -1.0879e+00
Epoch 2/2
41/41 - 1s - loss: 42.8729 - loglik: -4.1800e+01 - logprior: -8.7145e-01
Fitted a model with MAP estimate = -42.6579
expansions: []
discards: [11 15]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 43.3602 - loglik: -4.2092e+01 - logprior: -1.0577e+00
Epoch 2/2
41/41 - 2s - loss: 42.5646 - loglik: -4.1508e+01 - logprior: -8.4515e-01
Fitted a model with MAP estimate = -42.6751
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 7s - loss: 42.4894 - loglik: -4.1595e+01 - logprior: -6.8346e-01
Epoch 2/10
58/58 - 2s - loss: 41.9836 - loglik: -4.1221e+01 - logprior: -5.8643e-01
Epoch 3/10
58/58 - 2s - loss: 41.9857 - loglik: -4.1242e+01 - logprior: -5.7770e-01
Fitted a model with MAP estimate = -41.5796
Time for alignment: 73.7128
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 5s - loss: 59.6484 - loglik: -5.8764e+01 - logprior: -8.7353e-01
Epoch 2/10
41/41 - 2s - loss: 46.6994 - loglik: -4.5678e+01 - logprior: -8.9996e-01
Epoch 3/10
41/41 - 2s - loss: 45.6601 - loglik: -4.4671e+01 - logprior: -8.8894e-01
Epoch 4/10
41/41 - 2s - loss: 45.4273 - loglik: -4.4414e+01 - logprior: -8.8416e-01
Epoch 5/10
41/41 - 2s - loss: 45.2009 - loglik: -4.4167e+01 - logprior: -8.8025e-01
Epoch 6/10
41/41 - 2s - loss: 45.1543 - loglik: -4.4101e+01 - logprior: -8.7925e-01
Epoch 7/10
41/41 - 2s - loss: 44.9316 - loglik: -4.3877e+01 - logprior: -8.7824e-01
Epoch 8/10
41/41 - 2s - loss: 44.9404 - loglik: -4.3881e+01 - logprior: -8.7526e-01
Fitted a model with MAP estimate = -44.6384
expansions: [(4, 1), (7, 2), (10, 2), (11, 2), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 26 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.8389 - loglik: -4.3551e+01 - logprior: -1.0933e+00
Epoch 2/2
41/41 - 1s - loss: 42.7372 - loglik: -4.1659e+01 - logprior: -8.7661e-01
Fitted a model with MAP estimate = -42.7536
expansions: []
discards: [ 8 14 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 43.1704 - loglik: -4.1901e+01 - logprior: -1.0583e+00
Epoch 2/2
41/41 - 2s - loss: 42.6030 - loglik: -4.1547e+01 - logprior: -8.4342e-01
Fitted a model with MAP estimate = -42.7794
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 6s - loss: 42.5088 - loglik: -4.1626e+01 - logprior: -6.8213e-01
Epoch 2/10
58/58 - 2s - loss: 42.0280 - loglik: -4.1272e+01 - logprior: -5.8485e-01
Epoch 3/10
58/58 - 3s - loss: 42.0198 - loglik: -4.1277e+01 - logprior: -5.7777e-01
Epoch 4/10
58/58 - 2s - loss: 41.7681 - loglik: -4.1001e+01 - logprior: -5.7444e-01
Epoch 5/10
58/58 - 2s - loss: 41.7250 - loglik: -4.0950e+01 - logprior: -5.7345e-01
Epoch 6/10
58/58 - 3s - loss: 41.6808 - loglik: -4.0909e+01 - logprior: -5.7163e-01
Epoch 7/10
58/58 - 3s - loss: 41.5917 - loglik: -4.0819e+01 - logprior: -5.6754e-01
Epoch 8/10
58/58 - 2s - loss: 41.4481 - loglik: -4.0673e+01 - logprior: -5.6776e-01
Epoch 9/10
58/58 - 2s - loss: 41.4651 - loglik: -4.0691e+01 - logprior: -5.6833e-01
Fitted a model with MAP estimate = -41.2251
Time for alignment: 89.0806
Computed alignments with likelihoods: ['-41.5849', '-41.5796', '-41.2251']
Best model has likelihood: -41.2251
time for generating output: 0.1007
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/zf-CCHH.projection.fasta
SP score = 0.966464502318944
Training of 3 independent models on file scorptoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2b25911610>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f33572b8a60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3424171f70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34ee60b070>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34ee60b520>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34eeb2a340>, <__main__.SimpleDirichletPrior object at 0x7f2b148b5fa0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34eda7a820>

Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 281.8616 - loglik: -1.5687e+02 - logprior: -1.2497e+02
Epoch 2/10
10/10 - 1s - loss: 169.6813 - loglik: -1.3509e+02 - logprior: -3.4582e+01
Epoch 3/10
10/10 - 1s - loss: 134.7862 - loglik: -1.1794e+02 - logprior: -1.6825e+01
Epoch 4/10
10/10 - 1s - loss: 118.9485 - loglik: -1.0853e+02 - logprior: -1.0396e+01
Epoch 5/10
10/10 - 1s - loss: 110.9344 - loglik: -1.0408e+02 - logprior: -6.8517e+00
Epoch 6/10
10/10 - 1s - loss: 107.0313 - loglik: -1.0214e+02 - logprior: -4.8533e+00
Epoch 7/10
10/10 - 1s - loss: 105.0287 - loglik: -1.0121e+02 - logprior: -3.5885e+00
Epoch 8/10
10/10 - 1s - loss: 104.0843 - loglik: -1.0100e+02 - logprior: -2.7220e+00
Epoch 9/10
10/10 - 1s - loss: 103.5024 - loglik: -1.0105e+02 - logprior: -2.1472e+00
Epoch 10/10
10/10 - 1s - loss: 103.0423 - loglik: -1.0102e+02 - logprior: -1.7446e+00
Fitted a model with MAP estimate = -102.5618
expansions: [(9, 2), (13, 1), (15, 3), (16, 2), (23, 3), (31, 3), (32, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 240.1793 - loglik: -9.9604e+01 - logprior: -1.4032e+02
Epoch 2/2
10/10 - 1s - loss: 152.7963 - loglik: -9.3745e+01 - logprior: -5.8772e+01
Fitted a model with MAP estimate = -138.0213
expansions: [(0, 2)]
discards: [ 0  8 18 21 30]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 202.9127 - loglik: -8.9537e+01 - logprior: -1.1307e+02
Epoch 2/2
10/10 - 1s - loss: 119.9146 - loglik: -8.8278e+01 - logprior: -3.1333e+01
Fitted a model with MAP estimate = -107.4924
expansions: []
discards: [ 0 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 221.2171 - loglik: -9.0301e+01 - logprior: -1.3061e+02
Epoch 2/10
10/10 - 1s - loss: 128.6880 - loglik: -9.0031e+01 - logprior: -3.8360e+01
Epoch 3/10
10/10 - 1s - loss: 106.2246 - loglik: -9.0414e+01 - logprior: -1.5505e+01
Epoch 4/10
10/10 - 1s - loss: 98.3318 - loglik: -9.0824e+01 - logprior: -7.1983e+00
Epoch 5/10
10/10 - 1s - loss: 94.4664 - loglik: -9.1105e+01 - logprior: -3.0329e+00
Epoch 6/10
10/10 - 1s - loss: 92.2927 - loglik: -9.1247e+01 - logprior: -7.0923e-01
Epoch 7/10
10/10 - 1s - loss: 91.0045 - loglik: -9.1363e+01 - logprior: 0.6988
Epoch 8/10
10/10 - 1s - loss: 90.1721 - loglik: -9.1480e+01 - logprior: 1.6487
Epoch 9/10
10/10 - 1s - loss: 89.5680 - loglik: -9.1583e+01 - logprior: 2.3514
Epoch 10/10
10/10 - 1s - loss: 89.0954 - loglik: -9.1682e+01 - logprior: 2.9230
Fitted a model with MAP estimate = -88.5303
Time for alignment: 33.0645
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 281.8616 - loglik: -1.5687e+02 - logprior: -1.2497e+02
Epoch 2/10
10/10 - 1s - loss: 169.6813 - loglik: -1.3509e+02 - logprior: -3.4582e+01
Epoch 3/10
10/10 - 1s - loss: 134.7861 - loglik: -1.1794e+02 - logprior: -1.6825e+01
Epoch 4/10
10/10 - 1s - loss: 118.9485 - loglik: -1.0853e+02 - logprior: -1.0396e+01
Epoch 5/10
10/10 - 1s - loss: 110.9343 - loglik: -1.0408e+02 - logprior: -6.8517e+00
Epoch 6/10
10/10 - 1s - loss: 107.0303 - loglik: -1.0214e+02 - logprior: -4.8535e+00
Epoch 7/10
10/10 - 1s - loss: 105.0247 - loglik: -1.0120e+02 - logprior: -3.5922e+00
Epoch 8/10
10/10 - 1s - loss: 104.0478 - loglik: -1.0094e+02 - logprior: -2.7456e+00
Epoch 9/10
10/10 - 1s - loss: 103.4190 - loglik: -1.0092e+02 - logprior: -2.1869e+00
Epoch 10/10
10/10 - 1s - loss: 103.0022 - loglik: -1.0097e+02 - logprior: -1.7583e+00
Fitted a model with MAP estimate = -102.5753
expansions: [(9, 2), (13, 1), (15, 3), (16, 2), (23, 3), (31, 3), (32, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 240.1741 - loglik: -9.9597e+01 - logprior: -1.4032e+02
Epoch 2/2
10/10 - 1s - loss: 152.7825 - loglik: -9.3727e+01 - logprior: -5.8775e+01
Fitted a model with MAP estimate = -137.9920
expansions: [(0, 2)]
discards: [ 0  8 18 21 30]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 202.8767 - loglik: -8.9499e+01 - logprior: -1.1308e+02
Epoch 2/2
10/10 - 1s - loss: 119.9067 - loglik: -8.8267e+01 - logprior: -3.1336e+01
Fitted a model with MAP estimate = -107.4941
expansions: []
discards: [ 0 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 221.2139 - loglik: -9.0292e+01 - logprior: -1.3062e+02
Epoch 2/10
10/10 - 1s - loss: 128.6869 - loglik: -9.0030e+01 - logprior: -3.8360e+01
Epoch 3/10
10/10 - 1s - loss: 106.2255 - loglik: -9.0419e+01 - logprior: -1.5501e+01
Epoch 4/10
10/10 - 1s - loss: 98.3311 - loglik: -9.0829e+01 - logprior: -7.1932e+00
Epoch 5/10
10/10 - 1s - loss: 94.4605 - loglik: -9.1103e+01 - logprior: -3.0289e+00
Epoch 6/10
10/10 - 1s - loss: 92.2876 - loglik: -9.1247e+01 - logprior: -7.0525e-01
Epoch 7/10
10/10 - 1s - loss: 90.9919 - loglik: -9.1368e+01 - logprior: 0.7161
Epoch 8/10
10/10 - 1s - loss: 90.1548 - loglik: -9.1480e+01 - logprior: 1.6648
Epoch 9/10
10/10 - 1s - loss: 89.5583 - loglik: -9.1581e+01 - logprior: 2.3581
Epoch 10/10
10/10 - 1s - loss: 89.0893 - loglik: -9.1680e+01 - logprior: 2.9273
Fitted a model with MAP estimate = -88.5245
Time for alignment: 32.2255
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 281.8616 - loglik: -1.5687e+02 - logprior: -1.2497e+02
Epoch 2/10
10/10 - 1s - loss: 169.6814 - loglik: -1.3509e+02 - logprior: -3.4582e+01
Epoch 3/10
10/10 - 1s - loss: 134.7860 - loglik: -1.1794e+02 - logprior: -1.6825e+01
Epoch 4/10
10/10 - 1s - loss: 118.9485 - loglik: -1.0853e+02 - logprior: -1.0396e+01
Epoch 5/10
10/10 - 1s - loss: 110.9384 - loglik: -1.0408e+02 - logprior: -6.8495e+00
Epoch 6/10
10/10 - 1s - loss: 107.1282 - loglik: -1.0229e+02 - logprior: -4.8179e+00
Epoch 7/10
10/10 - 1s - loss: 105.0181 - loglik: -1.0126e+02 - logprior: -3.6010e+00
Epoch 8/10
10/10 - 1s - loss: 104.0244 - loglik: -1.0096e+02 - logprior: -2.7431e+00
Epoch 9/10
10/10 - 1s - loss: 103.4271 - loglik: -1.0098e+02 - logprior: -2.1631e+00
Epoch 10/10
10/10 - 1s - loss: 103.0194 - loglik: -1.0105e+02 - logprior: -1.7367e+00
Fitted a model with MAP estimate = -102.6294
expansions: [(9, 2), (13, 1), (15, 3), (16, 2), (23, 3), (31, 3), (32, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 240.1861 - loglik: -9.9629e+01 - logprior: -1.4031e+02
Epoch 2/2
10/10 - 1s - loss: 152.7512 - loglik: -9.3694e+01 - logprior: -5.8774e+01
Fitted a model with MAP estimate = -137.8509
expansions: [(0, 2)]
discards: [ 0  8 18 21 30]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 202.7440 - loglik: -8.9365e+01 - logprior: -1.1308e+02
Epoch 2/2
10/10 - 1s - loss: 119.8609 - loglik: -8.8222e+01 - logprior: -3.1336e+01
Fitted a model with MAP estimate = -107.4575
expansions: []
discards: [ 0 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 221.2037 - loglik: -9.0289e+01 - logprior: -1.3061e+02
Epoch 2/10
10/10 - 1s - loss: 128.6817 - loglik: -9.0019e+01 - logprior: -3.8363e+01
Epoch 3/10
10/10 - 1s - loss: 106.2263 - loglik: -9.0412e+01 - logprior: -1.5504e+01
Epoch 4/10
10/10 - 1s - loss: 98.3297 - loglik: -9.0825e+01 - logprior: -7.1889e+00
Epoch 5/10
10/10 - 1s - loss: 94.4496 - loglik: -9.1077e+01 - logprior: -3.0380e+00
Epoch 6/10
10/10 - 1s - loss: 92.2837 - loglik: -9.1224e+01 - logprior: -7.1625e-01
Epoch 7/10
10/10 - 1s - loss: 90.9980 - loglik: -9.1349e+01 - logprior: 0.7003
Epoch 8/10
10/10 - 1s - loss: 90.1702 - loglik: -9.1472e+01 - logprior: 1.6438
Epoch 9/10
10/10 - 1s - loss: 89.5742 - loglik: -9.1576e+01 - logprior: 2.3395
Epoch 10/10
10/10 - 1s - loss: 89.1050 - loglik: -9.1684e+01 - logprior: 2.9161
Fitted a model with MAP estimate = -88.5360
Time for alignment: 31.3322
Computed alignments with likelihoods: ['-88.5303', '-88.5245', '-88.5360']
Best model has likelihood: -88.5245
time for generating output: 0.1255
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/scorptoxin.projection.fasta
SP score = 0.8942992874109263
Training of 3 independent models on file biotin_lipoyl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2b24624160>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34364788b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f313845e6a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3484a277c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3484a274c0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f2b1cf82160>, <__main__.SimpleDirichletPrior object at 0x7f3118395040>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f342c87c5e0>

Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 195.3334 - loglik: -1.9216e+02 - logprior: -3.1599e+00
Epoch 2/10
19/19 - 2s - loss: 155.4052 - loglik: -1.5411e+02 - logprior: -1.2708e+00
Epoch 3/10
19/19 - 2s - loss: 141.6957 - loglik: -1.4017e+02 - logprior: -1.3692e+00
Epoch 4/10
19/19 - 2s - loss: 139.7892 - loglik: -1.3832e+02 - logprior: -1.3463e+00
Epoch 5/10
19/19 - 2s - loss: 139.0987 - loglik: -1.3771e+02 - logprior: -1.2894e+00
Epoch 6/10
19/19 - 2s - loss: 138.9187 - loglik: -1.3754e+02 - logprior: -1.2652e+00
Epoch 7/10
19/19 - 2s - loss: 138.6586 - loglik: -1.3730e+02 - logprior: -1.2411e+00
Epoch 8/10
19/19 - 2s - loss: 138.6203 - loglik: -1.3728e+02 - logprior: -1.2254e+00
Epoch 9/10
19/19 - 2s - loss: 138.5075 - loglik: -1.3717e+02 - logprior: -1.2162e+00
Epoch 10/10
19/19 - 2s - loss: 138.4462 - loglik: -1.3711e+02 - logprior: -1.2085e+00
Fitted a model with MAP estimate = -138.9524
expansions: [(0, 3), (13, 2), (14, 2), (15, 1), (20, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 140.8780 - loglik: -1.3630e+02 - logprior: -4.4454e+00
Epoch 2/2
19/19 - 2s - loss: 130.8171 - loglik: -1.2901e+02 - logprior: -1.6601e+00
Fitted a model with MAP estimate = -131.9051
expansions: [(0, 2)]
discards: [19 28 50 61 66]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 132.5109 - loglik: -1.2835e+02 - logprior: -3.9920e+00
Epoch 2/2
19/19 - 2s - loss: 128.4675 - loglik: -1.2674e+02 - logprior: -1.5431e+00
Fitted a model with MAP estimate = -129.7448
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 132.7751 - loglik: -1.2912e+02 - logprior: -3.4149e+00
Epoch 2/10
21/21 - 2s - loss: 130.0689 - loglik: -1.2798e+02 - logprior: -1.8646e+00
Epoch 3/10
21/21 - 2s - loss: 128.5037 - loglik: -1.2689e+02 - logprior: -1.3907e+00
Epoch 4/10
21/21 - 2s - loss: 127.7298 - loglik: -1.2624e+02 - logprior: -1.2422e+00
Epoch 5/10
21/21 - 2s - loss: 127.6730 - loglik: -1.2624e+02 - logprior: -1.1747e+00
Epoch 6/10
21/21 - 2s - loss: 127.3678 - loglik: -1.2596e+02 - logprior: -1.1481e+00
Epoch 7/10
21/21 - 2s - loss: 127.3200 - loglik: -1.2594e+02 - logprior: -1.1242e+00
Epoch 8/10
21/21 - 2s - loss: 127.1090 - loglik: -1.2576e+02 - logprior: -1.1013e+00
Epoch 9/10
21/21 - 2s - loss: 127.1235 - loglik: -1.2580e+02 - logprior: -1.0793e+00
Fitted a model with MAP estimate = -126.8423
Time for alignment: 66.7918
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 195.4304 - loglik: -1.9226e+02 - logprior: -3.1574e+00
Epoch 2/10
19/19 - 2s - loss: 155.7009 - loglik: -1.5440e+02 - logprior: -1.2754e+00
Epoch 3/10
19/19 - 2s - loss: 141.1062 - loglik: -1.3952e+02 - logprior: -1.3758e+00
Epoch 4/10
19/19 - 2s - loss: 139.0797 - loglik: -1.3763e+02 - logprior: -1.3403e+00
Epoch 5/10
19/19 - 2s - loss: 138.6798 - loglik: -1.3729e+02 - logprior: -1.2855e+00
Epoch 6/10
19/19 - 2s - loss: 138.2583 - loglik: -1.3689e+02 - logprior: -1.2662e+00
Epoch 7/10
19/19 - 2s - loss: 137.9276 - loglik: -1.3659e+02 - logprior: -1.2378e+00
Epoch 8/10
19/19 - 2s - loss: 138.0642 - loglik: -1.3673e+02 - logprior: -1.2266e+00
Fitted a model with MAP estimate = -138.4951
expansions: [(0, 3), (13, 2), (14, 1), (19, 1), (20, 1), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 139.4828 - loglik: -1.3502e+02 - logprior: -4.3404e+00
Epoch 2/2
19/19 - 2s - loss: 130.7587 - loglik: -1.2903e+02 - logprior: -1.5889e+00
Fitted a model with MAP estimate = -131.8479
expansions: [(0, 2)]
discards: [48 59 64]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 132.4336 - loglik: -1.2829e+02 - logprior: -3.9679e+00
Epoch 2/2
19/19 - 2s - loss: 128.2950 - loglik: -1.2659e+02 - logprior: -1.5241e+00
Fitted a model with MAP estimate = -129.7005
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 133.0960 - loglik: -1.2939e+02 - logprior: -3.4742e+00
Epoch 2/10
21/21 - 2s - loss: 130.3264 - loglik: -1.2820e+02 - logprior: -1.9072e+00
Epoch 3/10
21/21 - 2s - loss: 128.8085 - loglik: -1.2725e+02 - logprior: -1.3142e+00
Epoch 4/10
21/21 - 2s - loss: 128.2071 - loglik: -1.2679e+02 - logprior: -1.1743e+00
Epoch 5/10
21/21 - 2s - loss: 128.0998 - loglik: -1.2669e+02 - logprior: -1.1528e+00
Epoch 6/10
21/21 - 2s - loss: 127.7972 - loglik: -1.2641e+02 - logprior: -1.1389e+00
Epoch 7/10
21/21 - 2s - loss: 127.7978 - loglik: -1.2645e+02 - logprior: -1.1042e+00
Fitted a model with MAP estimate = -127.4932
Time for alignment: 58.6545
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 195.6819 - loglik: -1.9251e+02 - logprior: -3.1621e+00
Epoch 2/10
19/19 - 2s - loss: 157.2130 - loglik: -1.5591e+02 - logprior: -1.2837e+00
Epoch 3/10
19/19 - 2s - loss: 142.2897 - loglik: -1.4075e+02 - logprior: -1.3769e+00
Epoch 4/10
19/19 - 2s - loss: 140.0524 - loglik: -1.3855e+02 - logprior: -1.3472e+00
Epoch 5/10
19/19 - 2s - loss: 139.2844 - loglik: -1.3786e+02 - logprior: -1.3022e+00
Epoch 6/10
19/19 - 2s - loss: 139.0185 - loglik: -1.3762e+02 - logprior: -1.2764e+00
Epoch 7/10
19/19 - 1s - loss: 138.8910 - loglik: -1.3750e+02 - logprior: -1.2583e+00
Epoch 8/10
19/19 - 2s - loss: 138.5716 - loglik: -1.3719e+02 - logprior: -1.2486e+00
Epoch 9/10
19/19 - 2s - loss: 138.5623 - loglik: -1.3718e+02 - logprior: -1.2422e+00
Epoch 10/10
19/19 - 2s - loss: 138.5572 - loglik: -1.3718e+02 - logprior: -1.2385e+00
Fitted a model with MAP estimate = -139.2013
expansions: [(0, 3), (12, 1), (13, 1), (14, 1), (19, 1), (33, 1), (34, 1), (44, 1), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 139.1501 - loglik: -1.3459e+02 - logprior: -4.4270e+00
Epoch 2/2
19/19 - 2s - loss: 130.3696 - loglik: -1.2864e+02 - logprior: -1.5726e+00
Fitted a model with MAP estimate = -131.4303
expansions: [(0, 2)]
discards: [58 63]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 131.8403 - loglik: -1.2769e+02 - logprior: -3.9710e+00
Epoch 2/2
19/19 - 2s - loss: 128.0997 - loglik: -1.2638e+02 - logprior: -1.5331e+00
Fitted a model with MAP estimate = -129.6276
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 132.8718 - loglik: -1.2920e+02 - logprior: -3.4349e+00
Epoch 2/10
21/21 - 2s - loss: 130.2510 - loglik: -1.2817e+02 - logprior: -1.8748e+00
Epoch 3/10
21/21 - 2s - loss: 128.9138 - loglik: -1.2731e+02 - logprior: -1.3890e+00
Epoch 4/10
21/21 - 2s - loss: 128.2016 - loglik: -1.2675e+02 - logprior: -1.2086e+00
Epoch 5/10
21/21 - 2s - loss: 127.8429 - loglik: -1.2642e+02 - logprior: -1.1743e+00
Epoch 6/10
21/21 - 2s - loss: 127.9592 - loglik: -1.2657e+02 - logprior: -1.1417e+00
Fitted a model with MAP estimate = -127.4292
Time for alignment: 60.5076
Computed alignments with likelihoods: ['-126.8423', '-127.4932', '-127.4292']
Best model has likelihood: -126.8423
time for generating output: 0.1450
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/biotin_lipoyl.projection.fasta
SP score = 0.9444791016843419
Training of 3 independent models on file uce.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2b1cb37eb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34ecfe90a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34ecfe9310>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345190d940>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345190d280>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f332314fd60>, <__main__.SimpleDirichletPrior object at 0x7f345a744d60>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f335ff6f310>

Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 390.5923 - loglik: -3.8228e+02 - logprior: -8.2747e+00
Epoch 2/10
13/13 - 2s - loss: 339.3705 - loglik: -3.3720e+02 - logprior: -2.0390e+00
Epoch 3/10
13/13 - 2s - loss: 296.6071 - loglik: -2.9426e+02 - logprior: -1.9643e+00
Epoch 4/10
13/13 - 3s - loss: 282.4685 - loglik: -2.7933e+02 - logprior: -2.3361e+00
Epoch 5/10
13/13 - 2s - loss: 278.3580 - loglik: -2.7530e+02 - logprior: -2.3804e+00
Epoch 6/10
13/13 - 3s - loss: 276.7278 - loglik: -2.7398e+02 - logprior: -2.2877e+00
Epoch 7/10
13/13 - 2s - loss: 275.8708 - loglik: -2.7321e+02 - logprior: -2.2671e+00
Epoch 8/10
13/13 - 2s - loss: 275.1343 - loglik: -2.7251e+02 - logprior: -2.2819e+00
Epoch 9/10
13/13 - 2s - loss: 275.4673 - loglik: -2.7286e+02 - logprior: -2.2962e+00
Fitted a model with MAP estimate = -274.5310
expansions: [(7, 2), (8, 2), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (61, 2), (70, 3), (75, 1), (76, 4), (97, 1), (98, 2), (99, 3), (100, 1), (101, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 277.1856 - loglik: -2.6704e+02 - logprior: -9.8468e+00
Epoch 2/2
13/13 - 3s - loss: 259.7946 - loglik: -2.5496e+02 - logprior: -4.5306e+00
Fitted a model with MAP estimate = -257.0975
expansions: [(0, 2)]
discards: [  0  87  88 127]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 260.0120 - loglik: -2.5208e+02 - logprior: -7.5732e+00
Epoch 2/2
13/13 - 3s - loss: 252.9272 - loglik: -2.5043e+02 - logprior: -2.1180e+00
Fitted a model with MAP estimate = -251.5716
expansions: []
discards: [ 0 76]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 262.5184 - loglik: -2.5257e+02 - logprior: -9.5651e+00
Epoch 2/10
13/13 - 3s - loss: 256.2837 - loglik: -2.5235e+02 - logprior: -3.5457e+00
Epoch 3/10
13/13 - 3s - loss: 252.3888 - loglik: -2.5044e+02 - logprior: -1.5664e+00
Epoch 4/10
13/13 - 3s - loss: 251.7112 - loglik: -2.5027e+02 - logprior: -1.0616e+00
Epoch 5/10
13/13 - 3s - loss: 252.3344 - loglik: -2.5109e+02 - logprior: -8.6572e-01
Fitted a model with MAP estimate = -251.1813
Time for alignment: 81.2950
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 390.9228 - loglik: -3.8262e+02 - logprior: -8.2707e+00
Epoch 2/10
13/13 - 2s - loss: 339.2606 - loglik: -3.3709e+02 - logprior: -2.0299e+00
Epoch 3/10
13/13 - 2s - loss: 295.9230 - loglik: -2.9370e+02 - logprior: -1.9592e+00
Epoch 4/10
13/13 - 2s - loss: 281.7478 - loglik: -2.7891e+02 - logprior: -2.3336e+00
Epoch 5/10
13/13 - 3s - loss: 277.4155 - loglik: -2.7448e+02 - logprior: -2.3871e+00
Epoch 6/10
13/13 - 2s - loss: 277.0833 - loglik: -2.7430e+02 - logprior: -2.3162e+00
Epoch 7/10
13/13 - 2s - loss: 276.1077 - loglik: -2.7338e+02 - logprior: -2.3086e+00
Epoch 8/10
13/13 - 2s - loss: 275.0327 - loglik: -2.7232e+02 - logprior: -2.3089e+00
Epoch 9/10
13/13 - 2s - loss: 274.9698 - loglik: -2.7232e+02 - logprior: -2.2925e+00
Epoch 10/10
13/13 - 2s - loss: 274.8926 - loglik: -2.7226e+02 - logprior: -2.2802e+00
Fitted a model with MAP estimate = -274.3233
expansions: [(7, 2), (8, 2), (11, 1), (15, 1), (16, 2), (21, 1), (22, 1), (23, 1), (24, 1), (38, 1), (39, 1), (51, 1), (52, 1), (53, 2), (70, 3), (75, 1), (76, 4), (97, 1), (98, 2), (99, 3), (100, 1), (101, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 143 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 276.9345 - loglik: -2.6669e+02 - logprior: -9.8968e+00
Epoch 2/2
13/13 - 3s - loss: 259.6751 - loglik: -2.5473e+02 - logprior: -4.6016e+00
Fitted a model with MAP estimate = -256.7691
expansions: [(0, 2)]
discards: [  0  68  88  89 128]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 259.9112 - loglik: -2.5191e+02 - logprior: -7.6032e+00
Epoch 2/2
13/13 - 3s - loss: 253.0141 - loglik: -2.5046e+02 - logprior: -2.1469e+00
Fitted a model with MAP estimate = -251.3029
expansions: []
discards: [ 0 23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 262.5267 - loglik: -2.5251e+02 - logprior: -9.6019e+00
Epoch 2/10
13/13 - 3s - loss: 255.4827 - loglik: -2.5151e+02 - logprior: -3.5849e+00
Epoch 3/10
13/13 - 3s - loss: 253.2922 - loglik: -2.5130e+02 - logprior: -1.5950e+00
Epoch 4/10
13/13 - 3s - loss: 250.5844 - loglik: -2.4912e+02 - logprior: -1.0809e+00
Epoch 5/10
13/13 - 3s - loss: 252.0683 - loglik: -2.5079e+02 - logprior: -8.9609e-01
Fitted a model with MAP estimate = -250.9722
Time for alignment: 80.2932
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 391.2820 - loglik: -3.8297e+02 - logprior: -8.2797e+00
Epoch 2/10
13/13 - 2s - loss: 337.9037 - loglik: -3.3574e+02 - logprior: -2.0434e+00
Epoch 3/10
13/13 - 2s - loss: 296.0569 - loglik: -2.9386e+02 - logprior: -1.9623e+00
Epoch 4/10
13/13 - 2s - loss: 282.4785 - loglik: -2.7964e+02 - logprior: -2.3467e+00
Epoch 5/10
13/13 - 2s - loss: 278.3175 - loglik: -2.7540e+02 - logprior: -2.3913e+00
Epoch 6/10
13/13 - 2s - loss: 277.6899 - loglik: -2.7500e+02 - logprior: -2.2616e+00
Epoch 7/10
13/13 - 2s - loss: 276.1534 - loglik: -2.7353e+02 - logprior: -2.2213e+00
Epoch 8/10
13/13 - 2s - loss: 276.5323 - loglik: -2.7396e+02 - logprior: -2.1951e+00
Fitted a model with MAP estimate = -275.3857
expansions: [(7, 2), (8, 2), (14, 1), (15, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (50, 1), (51, 1), (52, 1), (61, 2), (71, 1), (75, 2), (76, 4), (99, 5), (100, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 277.4187 - loglik: -2.6723e+02 - logprior: -9.8432e+00
Epoch 2/2
13/13 - 3s - loss: 260.0833 - loglik: -2.5519e+02 - logprior: -4.5470e+00
Fitted a model with MAP estimate = -257.1973
expansions: [(0, 2)]
discards: [ 0 93]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 260.1146 - loglik: -2.5212e+02 - logprior: -7.6106e+00
Epoch 2/2
13/13 - 3s - loss: 252.3076 - loglik: -2.4978e+02 - logprior: -2.1291e+00
Fitted a model with MAP estimate = -251.3107
expansions: []
discards: [ 0 23 77]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 262.9503 - loglik: -2.5296e+02 - logprior: -9.5848e+00
Epoch 2/10
13/13 - 3s - loss: 255.8279 - loglik: -2.5189e+02 - logprior: -3.5560e+00
Epoch 3/10
13/13 - 3s - loss: 252.1118 - loglik: -2.5015e+02 - logprior: -1.5731e+00
Epoch 4/10
13/13 - 3s - loss: 252.5460 - loglik: -2.5110e+02 - logprior: -1.0685e+00
Fitted a model with MAP estimate = -251.4186
Time for alignment: 71.9318
Computed alignments with likelihoods: ['-251.1813', '-250.9722', '-251.3107']
Best model has likelihood: -250.9722
time for generating output: 0.1939
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/uce.projection.fasta
SP score = 0.9479338842975207
Training of 3 independent models on file ghf1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f349598f970>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f332cbaa4c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2b15edd070>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3451bb18b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2b2c9823d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3462a56cd0>, <__main__.SimpleDirichletPrior object at 0x7f34365b94c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3440dba310>

Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 35s - loss: 1082.1525 - loglik: -1.0782e+03 - logprior: -3.9334e+00
Epoch 2/10
25/25 - 32s - loss: 825.6656 - loglik: -8.2390e+02 - logprior: -1.6530e+00
Epoch 3/10
25/25 - 32s - loss: 771.3273 - loglik: -7.6792e+02 - logprior: -2.8188e+00
Epoch 4/10
25/25 - 32s - loss: 763.9519 - loglik: -7.6016e+02 - logprior: -3.0766e+00
Epoch 5/10
25/25 - 32s - loss: 760.4733 - loglik: -7.5655e+02 - logprior: -3.1072e+00
Epoch 6/10
25/25 - 32s - loss: 757.2648 - loglik: -7.5332e+02 - logprior: -3.1124e+00
Epoch 7/10
25/25 - 32s - loss: 756.1889 - loglik: -7.5218e+02 - logprior: -3.1703e+00
Epoch 8/10
25/25 - 32s - loss: 755.4542 - loglik: -7.5131e+02 - logprior: -3.3270e+00
Epoch 9/10
25/25 - 32s - loss: 752.1266 - loglik: -7.4813e+02 - logprior: -3.1953e+00
Epoch 10/10
25/25 - 32s - loss: 753.5197 - loglik: -7.4924e+02 - logprior: -3.4995e+00
Fitted a model with MAP estimate = -750.8884
expansions: [(46, 1), (88, 2), (131, 1), (134, 1), (163, 1), (164, 3), (169, 1), (173, 5), (174, 2), (175, 2), (176, 1), (189, 1), (190, 1), (191, 4), (192, 1), (193, 1), (195, 1), (196, 1), (197, 2), (198, 1), (200, 1), (201, 1), (203, 1), (204, 1), (206, 1), (207, 1), (210, 1), (211, 1), (213, 1), (215, 1), (221, 1), (222, 1), (223, 2), (224, 2), (225, 3), (226, 1), (227, 1), (232, 1), (234, 2), (235, 1), (245, 1), (252, 4), (253, 2), (255, 1), (256, 1), (264, 1), (278, 1), (280, 1), (281, 1), (297, 1), (299, 2), (312, 1), (313, 1), (315, 1), (321, 2), (324, 2), (326, 1), (354, 3), (358, 1), (360, 1), (363, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 458 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 51s - loss: 728.2365 - loglik: -7.2173e+02 - logprior: -5.8181e+00
Epoch 2/2
25/25 - 45s - loss: 695.1841 - loglik: -6.9203e+02 - logprior: -2.3768e+00
Fitted a model with MAP estimate = -691.4014
expansions: [(3, 1), (248, 1), (403, 1)]
discards: [ 89 185 186 187 214 399 436 437]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 453 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 48s - loss: 698.3776 - loglik: -6.9359e+02 - logprior: -3.9887e+00
Epoch 2/2
25/25 - 44s - loss: 689.3536 - loglik: -6.8851e+02 - logprior: -3.5691e-02
Fitted a model with MAP estimate = -688.3012
expansions: []
discards: [311]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 47s - loss: 695.8622 - loglik: -6.9161e+02 - logprior: -3.4483e+00
Epoch 2/10
25/25 - 44s - loss: 688.1703 - loglik: -6.8794e+02 - logprior: 0.5593
Epoch 3/10
25/25 - 44s - loss: 690.5446 - loglik: -6.9092e+02 - logprior: 1.1764
Fitted a model with MAP estimate = -687.1042
Time for alignment: 794.8575
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 36s - loss: 1082.4639 - loglik: -1.0785e+03 - logprior: -3.8719e+00
Epoch 2/10
25/25 - 32s - loss: 821.7407 - loglik: -8.2011e+02 - logprior: -1.3696e+00
Epoch 3/10
25/25 - 32s - loss: 770.2023 - loglik: -7.6695e+02 - logprior: -2.3440e+00
Epoch 4/10
25/25 - 32s - loss: 759.5316 - loglik: -7.5593e+02 - logprior: -2.6673e+00
Epoch 5/10
25/25 - 32s - loss: 754.8315 - loglik: -7.5121e+02 - logprior: -2.6888e+00
Epoch 6/10
25/25 - 32s - loss: 751.9464 - loglik: -7.4833e+02 - logprior: -2.7675e+00
Epoch 7/10
25/25 - 32s - loss: 752.3967 - loglik: -7.4869e+02 - logprior: -2.8567e+00
Fitted a model with MAP estimate = -750.0550
expansions: [(50, 1), (132, 1), (144, 3), (164, 2), (169, 1), (177, 1), (191, 1), (194, 6), (195, 2), (197, 1), (198, 5), (200, 1), (201, 1), (203, 1), (204, 1), (206, 1), (207, 1), (210, 1), (211, 1), (217, 1), (222, 1), (223, 4), (224, 1), (225, 1), (226, 2), (227, 1), (228, 1), (236, 2), (248, 1), (252, 1), (255, 6), (258, 1), (266, 1), (281, 1), (282, 3), (299, 1), (300, 1), (301, 2), (303, 1), (304, 1), (316, 1), (317, 1), (318, 1), (324, 1), (325, 1), (327, 2), (350, 1), (355, 1), (360, 1), (366, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 447 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 47s - loss: 720.3915 - loglik: -7.1446e+02 - logprior: -5.1903e+00
Epoch 2/2
25/25 - 43s - loss: 694.4006 - loglik: -6.9236e+02 - logprior: -1.2015e+00
Fitted a model with MAP estimate = -690.4778
expansions: [(168, 1), (212, 1), (220, 1), (239, 1), (427, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 50s - loss: 694.3245 - loglik: -6.8876e+02 - logprior: -4.7045e+00
Epoch 2/2
25/25 - 44s - loss: 688.1516 - loglik: -6.8691e+02 - logprior: -3.9737e-01
Fitted a model with MAP estimate = -685.5682
expansions: [(3, 1)]
discards: [432]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 48s - loss: 691.4177 - loglik: -6.8693e+02 - logprior: -3.6365e+00
Epoch 2/10
25/25 - 44s - loss: 687.7130 - loglik: -6.8717e+02 - logprior: 0.3007
Epoch 3/10
25/25 - 44s - loss: 684.4429 - loglik: -6.8465e+02 - logprior: 1.0275
Epoch 4/10
25/25 - 44s - loss: 687.8691 - loglik: -6.8842e+02 - logprior: 1.3355
Fitted a model with MAP estimate = -684.1043
Time for alignment: 737.8748
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 35s - loss: 1081.8857 - loglik: -1.0779e+03 - logprior: -3.8914e+00
Epoch 2/10
25/25 - 32s - loss: 820.5560 - loglik: -8.1875e+02 - logprior: -1.7035e+00
Epoch 3/10
25/25 - 32s - loss: 767.2507 - loglik: -7.6343e+02 - logprior: -3.2288e+00
Epoch 4/10
25/25 - 32s - loss: 758.4156 - loglik: -7.5440e+02 - logprior: -3.2867e+00
Epoch 5/10
25/25 - 32s - loss: 754.1973 - loglik: -7.5011e+02 - logprior: -3.2629e+00
Epoch 6/10
25/25 - 32s - loss: 749.3854 - loglik: -7.4513e+02 - logprior: -3.3641e+00
Epoch 7/10
25/25 - 32s - loss: 750.2786 - loglik: -7.4610e+02 - logprior: -3.3076e+00
Fitted a model with MAP estimate = -748.1045
expansions: [(3, 1), (52, 1), (135, 1), (144, 1), (163, 2), (164, 1), (169, 1), (175, 6), (176, 2), (177, 1), (178, 1), (189, 1), (190, 1), (191, 1), (192, 1), (193, 2), (194, 1), (196, 1), (197, 1), (198, 2), (199, 1), (201, 1), (202, 1), (204, 1), (206, 1), (208, 1), (209, 1), (212, 1), (213, 1), (218, 1), (219, 1), (223, 1), (224, 1), (225, 2), (226, 1), (227, 1), (229, 1), (230, 3), (237, 2), (238, 1), (249, 1), (253, 2), (255, 3), (257, 1), (259, 5), (267, 1), (284, 1), (299, 1), (301, 1), (302, 4), (304, 1), (305, 1), (317, 1), (320, 1), (326, 1), (327, 1), (328, 2), (355, 1), (356, 1), (358, 1), (360, 1), (363, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 456 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 50s - loss: 722.2090 - loglik: -7.1647e+02 - logprior: -5.0336e+00
Epoch 2/2
25/25 - 45s - loss: 691.5843 - loglik: -6.8959e+02 - logprior: -1.1682e+00
Fitted a model with MAP estimate = -688.1100
expansions: [(128, 1), (409, 1), (434, 2)]
discards: [168 185 186 215]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 456 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 48s - loss: 693.9252 - loglik: -6.8832e+02 - logprior: -4.7870e+00
Epoch 2/2
25/25 - 45s - loss: 683.1449 - loglik: -6.8184e+02 - logprior: -4.6742e-01
Fitted a model with MAP estimate = -682.7636
expansions: [(431, 1)]
discards: [432 433]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 455 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 48s - loss: 690.7060 - loglik: -6.8582e+02 - logprior: -4.0541e+00
Epoch 2/10
25/25 - 44s - loss: 686.3814 - loglik: -6.8579e+02 - logprior: 0.2371
Epoch 3/10
25/25 - 44s - loss: 682.5415 - loglik: -6.8265e+02 - logprior: 0.9350
Epoch 4/10
25/25 - 45s - loss: 683.1604 - loglik: -6.8366e+02 - logprior: 1.2746
Fitted a model with MAP estimate = -681.9225
Time for alignment: 744.5417
Computed alignments with likelihoods: ['-687.1042', '-684.1043', '-681.9225']
Best model has likelihood: -681.9225
time for generating output: 0.5053
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf1.projection.fasta
SP score = 0.9079302283761913
Training of 3 independent models on file ldh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f343fefcf10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f347c509460>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2b253d9bb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f30f8d72b20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2b3c573460>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f2b242e7e80>, <__main__.SimpleDirichletPrior object at 0x7f311848aeb0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f33233300d0>

Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 327.7860 - loglik: -3.2319e+02 - logprior: -4.5874e+00
Epoch 2/10
16/16 - 4s - loss: 254.1529 - loglik: -2.5259e+02 - logprior: -1.5496e+00
Epoch 3/10
16/16 - 4s - loss: 223.8054 - loglik: -2.2169e+02 - logprior: -1.8161e+00
Epoch 4/10
16/16 - 4s - loss: 212.5609 - loglik: -2.1024e+02 - logprior: -1.7113e+00
Epoch 5/10
16/16 - 4s - loss: 208.9372 - loglik: -2.0660e+02 - logprior: -1.6849e+00
Epoch 6/10
16/16 - 5s - loss: 203.3509 - loglik: -2.0096e+02 - logprior: -1.7105e+00
Epoch 7/10
16/16 - 4s - loss: 200.4626 - loglik: -1.9809e+02 - logprior: -1.7317e+00
Epoch 8/10
16/16 - 5s - loss: 199.8451 - loglik: -1.9751e+02 - logprior: -1.7731e+00
Epoch 9/10
16/16 - 4s - loss: 198.2175 - loglik: -1.9592e+02 - logprior: -1.7527e+00
Epoch 10/10
16/16 - 4s - loss: 197.3833 - loglik: -1.9506e+02 - logprior: -1.7879e+00
Fitted a model with MAP estimate = -197.0344
expansions: [(0, 1), (11, 1), (15, 1), (16, 2), (17, 1), (18, 1), (71, 1), (96, 1), (97, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 193.5570 - loglik: -1.8928e+02 - logprior: -3.7372e+00
Epoch 2/2
33/33 - 7s - loss: 189.0226 - loglik: -1.8679e+02 - logprior: -1.7415e+00
Fitted a model with MAP estimate = -187.0178
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 189.0311 - loglik: -1.8523e+02 - logprior: -3.2605e+00
Epoch 2/10
33/33 - 6s - loss: 186.7639 - loglik: -1.8465e+02 - logprior: -1.5886e+00
Epoch 3/10
33/33 - 7s - loss: 188.0770 - loglik: -1.8609e+02 - logprior: -1.4644e+00
Fitted a model with MAP estimate = -186.0386
Time for alignment: 110.1083
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 326.5167 - loglik: -3.2192e+02 - logprior: -4.5875e+00
Epoch 2/10
16/16 - 5s - loss: 254.9967 - loglik: -2.5342e+02 - logprior: -1.5650e+00
Epoch 3/10
16/16 - 4s - loss: 223.2150 - loglik: -2.2105e+02 - logprior: -2.0091e+00
Epoch 4/10
16/16 - 5s - loss: 209.7321 - loglik: -2.0726e+02 - logprior: -2.0148e+00
Epoch 5/10
16/16 - 4s - loss: 202.1750 - loglik: -1.9966e+02 - logprior: -1.9193e+00
Epoch 6/10
16/16 - 4s - loss: 201.7729 - loglik: -1.9925e+02 - logprior: -1.9144e+00
Epoch 7/10
16/16 - 4s - loss: 199.2952 - loglik: -1.9680e+02 - logprior: -1.9188e+00
Epoch 8/10
16/16 - 5s - loss: 198.6626 - loglik: -1.9619e+02 - logprior: -1.9033e+00
Epoch 9/10
16/16 - 4s - loss: 196.3331 - loglik: -1.9389e+02 - logprior: -1.9233e+00
Epoch 10/10
16/16 - 5s - loss: 196.0176 - loglik: -1.9358e+02 - logprior: -1.9184e+00
Fitted a model with MAP estimate = -195.7173
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 3), (57, 1), (86, 1), (97, 1)]
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 120 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 10s - loss: 199.9228 - loglik: -1.9346e+02 - logprior: -5.9445e+00
Epoch 2/2
16/16 - 5s - loss: 191.8609 - loglik: -1.8832e+02 - logprior: -3.0626e+00
Fitted a model with MAP estimate = -190.7563
expansions: [(0, 2), (19, 1), (23, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 189.2083 - loglik: -1.8545e+02 - logprior: -3.2336e+00
Epoch 2/2
33/33 - 6s - loss: 185.3539 - loglik: -1.8318e+02 - logprior: -1.6500e+00
Fitted a model with MAP estimate = -185.4819
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 187.5132 - loglik: -1.8380e+02 - logprior: -3.1786e+00
Epoch 2/10
33/33 - 6s - loss: 186.4459 - loglik: -1.8443e+02 - logprior: -1.4909e+00
Epoch 3/10
33/33 - 6s - loss: 185.7687 - loglik: -1.8388e+02 - logprior: -1.3717e+00
Epoch 4/10
33/33 - 7s - loss: 185.4097 - loglik: -1.8362e+02 - logprior: -1.2774e+00
Epoch 5/10
33/33 - 7s - loss: 184.8999 - loglik: -1.8322e+02 - logprior: -1.1800e+00
Epoch 6/10
33/33 - 7s - loss: 185.9221 - loglik: -1.8433e+02 - logprior: -1.0959e+00
Fitted a model with MAP estimate = -184.6414
Time for alignment: 147.7052
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 10s - loss: 327.4312 - loglik: -3.2284e+02 - logprior: -4.5879e+00
Epoch 2/10
16/16 - 5s - loss: 254.1065 - loglik: -2.5255e+02 - logprior: -1.5516e+00
Epoch 3/10
16/16 - 4s - loss: 222.7120 - loglik: -2.2067e+02 - logprior: -1.9092e+00
Epoch 4/10
16/16 - 4s - loss: 212.5276 - loglik: -2.1016e+02 - logprior: -1.9425e+00
Epoch 5/10
16/16 - 5s - loss: 203.3976 - loglik: -2.0098e+02 - logprior: -1.8398e+00
Epoch 6/10
16/16 - 4s - loss: 199.3527 - loglik: -1.9690e+02 - logprior: -1.8551e+00
Epoch 7/10
16/16 - 5s - loss: 201.1437 - loglik: -1.9873e+02 - logprior: -1.8505e+00
Fitted a model with MAP estimate = -197.5436
expansions: [(10, 1), (15, 1), (16, 3), (69, 1), (94, 1), (97, 1)]
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 118 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 202.6355 - loglik: -1.9615e+02 - logprior: -5.9419e+00
Epoch 2/2
16/16 - 5s - loss: 193.8832 - loglik: -1.9033e+02 - logprior: -3.0759e+00
Fitted a model with MAP estimate = -193.2813
expansions: [(0, 2), (11, 2), (17, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 190.4532 - loglik: -1.8669e+02 - logprior: -3.2451e+00
Epoch 2/2
33/33 - 6s - loss: 188.2318 - loglik: -1.8604e+02 - logprior: -1.6599e+00
Fitted a model with MAP estimate = -186.3342
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 188.3673 - loglik: -1.8462e+02 - logprior: -3.1942e+00
Epoch 2/10
33/33 - 6s - loss: 186.5646 - loglik: -1.8452e+02 - logprior: -1.5103e+00
Epoch 3/10
33/33 - 7s - loss: 187.1214 - loglik: -1.8519e+02 - logprior: -1.3877e+00
Fitted a model with MAP estimate = -185.5860
Time for alignment: 114.8213
Computed alignments with likelihoods: ['-186.0386', '-184.6414', '-185.5860']
Best model has likelihood: -184.6414
time for generating output: 0.3391
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ldh.projection.fasta
SP score = 0.5419439771813063
Training of 3 independent models on file Rhodanese.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2b1c37cf10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f2b04537880>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f330a829a30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f2b3c3b72e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2b14a17fd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f318618bdc0>, <__main__.SimpleDirichletPrior object at 0x7f33248664c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f33354d9790>

Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 308.3990 - loglik: -3.0511e+02 - logprior: -3.1869e+00
Epoch 2/10
19/19 - 3s - loss: 276.8543 - loglik: -2.7497e+02 - logprior: -1.2305e+00
Epoch 3/10
19/19 - 3s - loss: 260.2112 - loglik: -2.5769e+02 - logprior: -1.6296e+00
Epoch 4/10
19/19 - 3s - loss: 256.1351 - loglik: -2.5370e+02 - logprior: -1.5809e+00
Epoch 5/10
19/19 - 3s - loss: 255.4399 - loglik: -2.5320e+02 - logprior: -1.5155e+00
Epoch 6/10
19/19 - 3s - loss: 254.6798 - loglik: -2.5255e+02 - logprior: -1.4891e+00
Epoch 7/10
19/19 - 3s - loss: 254.0440 - loglik: -2.5197e+02 - logprior: -1.4644e+00
Epoch 8/10
19/19 - 3s - loss: 253.5267 - loglik: -2.5149e+02 - logprior: -1.4553e+00
Epoch 9/10
19/19 - 3s - loss: 252.7998 - loglik: -2.5078e+02 - logprior: -1.4519e+00
Epoch 10/10
19/19 - 3s - loss: 253.1588 - loglik: -2.5115e+02 - logprior: -1.4526e+00
Fitted a model with MAP estimate = -240.3601
expansions: [(6, 3), (7, 2), (10, 2), (34, 10), (39, 2), (40, 3), (55, 2), (58, 2), (60, 1), (62, 2), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 252.3090 - loglik: -2.4854e+02 - logprior: -3.2151e+00
Epoch 2/2
19/19 - 3s - loss: 242.3058 - loglik: -2.4020e+02 - logprior: -1.4908e+00
Fitted a model with MAP estimate = -232.9891
expansions: [(47, 2)]
discards: [ 0 11 15 57 77 82 90 93]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 245.9155 - loglik: -2.4096e+02 - logprior: -4.2742e+00
Epoch 2/2
19/19 - 3s - loss: 242.1665 - loglik: -2.3906e+02 - logprior: -2.3872e+00
Fitted a model with MAP estimate = -234.7338
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 8s - loss: 232.4839 - loglik: -2.2931e+02 - logprior: -2.1438e+00
Epoch 2/10
23/23 - 4s - loss: 227.7259 - loglik: -2.2526e+02 - logprior: -1.1486e+00
Epoch 3/10
23/23 - 4s - loss: 226.1499 - loglik: -2.2375e+02 - logprior: -1.1259e+00
Epoch 4/10
23/23 - 4s - loss: 225.3007 - loglik: -2.2308e+02 - logprior: -1.0778e+00
Epoch 5/10
23/23 - 4s - loss: 224.2587 - loglik: -2.2210e+02 - logprior: -1.0643e+00
Epoch 6/10
23/23 - 4s - loss: 224.0211 - loglik: -2.2198e+02 - logprior: -1.0463e+00
Epoch 7/10
23/23 - 4s - loss: 223.9087 - loglik: -2.2197e+02 - logprior: -1.0280e+00
Epoch 8/10
23/23 - 4s - loss: 223.0702 - loglik: -2.2123e+02 - logprior: -1.0082e+00
Epoch 9/10
23/23 - 4s - loss: 224.0653 - loglik: -2.2231e+02 - logprior: -9.8505e-01
Fitted a model with MAP estimate = -222.4729
Time for alignment: 116.2648
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 308.6136 - loglik: -3.0533e+02 - logprior: -3.1843e+00
Epoch 2/10
19/19 - 3s - loss: 277.2903 - loglik: -2.7539e+02 - logprior: -1.2165e+00
Epoch 3/10
19/19 - 3s - loss: 261.0627 - loglik: -2.5841e+02 - logprior: -1.5764e+00
Epoch 4/10
19/19 - 3s - loss: 256.3853 - loglik: -2.5395e+02 - logprior: -1.5565e+00
Epoch 5/10
19/19 - 3s - loss: 254.6794 - loglik: -2.5242e+02 - logprior: -1.5295e+00
Epoch 6/10
19/19 - 3s - loss: 253.6515 - loglik: -2.5149e+02 - logprior: -1.5177e+00
Epoch 7/10
19/19 - 3s - loss: 253.3082 - loglik: -2.5121e+02 - logprior: -1.4939e+00
Epoch 8/10
19/19 - 3s - loss: 252.6961 - loglik: -2.5063e+02 - logprior: -1.4855e+00
Epoch 9/10
19/19 - 3s - loss: 252.5907 - loglik: -2.5055e+02 - logprior: -1.4796e+00
Epoch 10/10
19/19 - 3s - loss: 252.4465 - loglik: -2.5043e+02 - logprior: -1.4799e+00
Fitted a model with MAP estimate = -240.5039
expansions: [(6, 3), (7, 2), (10, 2), (21, 2), (33, 10), (38, 2), (39, 1), (42, 2), (58, 2), (60, 1), (62, 2), (63, 2), (65, 1), (67, 1), (73, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 254.8060 - loglik: -2.5008e+02 - logprior: -4.2090e+00
Epoch 2/2
19/19 - 3s - loss: 244.5509 - loglik: -2.4155e+02 - logprior: -2.4013e+00
Fitted a model with MAP estimate = -234.6079
expansions: [(0, 2), (47, 1)]
discards: [ 0  9 14 28 57 81 89 92]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 243.3886 - loglik: -2.3959e+02 - logprior: -3.1545e+00
Epoch 2/2
19/19 - 3s - loss: 240.1119 - loglik: -2.3809e+02 - logprior: -1.3588e+00
Fitted a model with MAP estimate = -233.5370
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 234.1248 - loglik: -2.3044e+02 - logprior: -2.7132e+00
Epoch 2/10
23/23 - 4s - loss: 228.8670 - loglik: -2.2640e+02 - logprior: -1.2190e+00
Epoch 3/10
23/23 - 4s - loss: 226.1227 - loglik: -2.2373e+02 - logprior: -1.1844e+00
Epoch 4/10
23/23 - 4s - loss: 226.2942 - loglik: -2.2406e+02 - logprior: -1.1468e+00
Fitted a model with MAP estimate = -224.1616
Time for alignment: 93.7180
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 308.6039 - loglik: -3.0532e+02 - logprior: -3.1825e+00
Epoch 2/10
19/19 - 3s - loss: 275.5456 - loglik: -2.7368e+02 - logprior: -1.2139e+00
Epoch 3/10
19/19 - 3s - loss: 259.6522 - loglik: -2.5714e+02 - logprior: -1.6331e+00
Epoch 4/10
19/19 - 3s - loss: 255.8027 - loglik: -2.5335e+02 - logprior: -1.5965e+00
Epoch 5/10
19/19 - 3s - loss: 255.0215 - loglik: -2.5278e+02 - logprior: -1.5326e+00
Epoch 6/10
19/19 - 3s - loss: 253.6808 - loglik: -2.5152e+02 - logprior: -1.5091e+00
Epoch 7/10
19/19 - 3s - loss: 253.8130 - loglik: -2.5171e+02 - logprior: -1.4835e+00
Fitted a model with MAP estimate = -239.8216
expansions: [(6, 3), (7, 2), (10, 2), (21, 2), (33, 10), (38, 2), (39, 1), (42, 2), (58, 2), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 255.0288 - loglik: -2.5023e+02 - logprior: -4.1889e+00
Epoch 2/2
19/19 - 3s - loss: 244.8276 - loglik: -2.4181e+02 - logprior: -2.3509e+00
Fitted a model with MAP estimate = -233.3176
expansions: [(0, 2), (47, 2)]
discards: [ 0  9 14 28 56 81 90]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 243.8309 - loglik: -2.3996e+02 - logprior: -3.1578e+00
Epoch 2/2
19/19 - 3s - loss: 240.5865 - loglik: -2.3853e+02 - logprior: -1.3459e+00
Fitted a model with MAP estimate = -232.0536
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 233.2018 - loglik: -2.2944e+02 - logprior: -2.7341e+00
Epoch 2/10
23/23 - 4s - loss: 227.6520 - loglik: -2.2514e+02 - logprior: -1.2431e+00
Epoch 3/10
23/23 - 4s - loss: 226.9013 - loglik: -2.2447e+02 - logprior: -1.1955e+00
Epoch 4/10
23/23 - 4s - loss: 225.9075 - loglik: -2.2362e+02 - logprior: -1.1589e+00
Epoch 5/10
23/23 - 4s - loss: 224.9873 - loglik: -2.2275e+02 - logprior: -1.1520e+00
Epoch 6/10
23/23 - 4s - loss: 224.6806 - loglik: -2.2258e+02 - logprior: -1.1308e+00
Epoch 7/10
23/23 - 4s - loss: 224.3899 - loglik: -2.2238e+02 - logprior: -1.1123e+00
Epoch 8/10
23/23 - 4s - loss: 223.7245 - loglik: -2.2181e+02 - logprior: -1.0986e+00
Epoch 9/10
23/23 - 4s - loss: 224.5962 - loglik: -2.2277e+02 - logprior: -1.0748e+00
Fitted a model with MAP estimate = -223.1222
Time for alignment: 102.9552
Computed alignments with likelihoods: ['-222.4729', '-224.1616', '-223.1222']
Best model has likelihood: -222.4729
time for generating output: 0.1860
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Rhodanese.projection.fasta
SP score = 0.7174620390455532
Training of 3 independent models on file slectin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34ee10e700>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3356edc580>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f342cb285b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34eed61250>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3462ad0580>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f2b253abd60>, <__main__.SimpleDirichletPrior object at 0x7f342daf8730>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f2b25929430>

Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 433.2686 - loglik: -3.8701e+02 - logprior: -4.6228e+01
Epoch 2/10
10/10 - 2s - loss: 371.1165 - loglik: -3.5996e+02 - logprior: -1.1028e+01
Epoch 3/10
10/10 - 2s - loss: 334.8010 - loglik: -3.2978e+02 - logprior: -4.8956e+00
Epoch 4/10
10/10 - 2s - loss: 311.9946 - loglik: -3.0873e+02 - logprior: -3.1295e+00
Epoch 5/10
10/10 - 2s - loss: 302.9826 - loglik: -3.0031e+02 - logprior: -2.3478e+00
Epoch 6/10
10/10 - 2s - loss: 299.0253 - loglik: -2.9676e+02 - logprior: -1.8725e+00
Epoch 7/10
10/10 - 2s - loss: 297.1628 - loglik: -2.9532e+02 - logprior: -1.4761e+00
Epoch 8/10
10/10 - 2s - loss: 296.6401 - loglik: -2.9511e+02 - logprior: -1.1684e+00
Epoch 9/10
10/10 - 2s - loss: 295.6424 - loglik: -2.9426e+02 - logprior: -1.0102e+00
Epoch 10/10
10/10 - 2s - loss: 295.0434 - loglik: -2.9370e+02 - logprior: -9.5556e-01
Fitted a model with MAP estimate = -294.7348
expansions: [(10, 4), (17, 1), (20, 1), (27, 1), (28, 2), (41, 2), (42, 1), (48, 2), (59, 2), (62, 2), (73, 1), (85, 1), (90, 2), (91, 3), (92, 2), (93, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 342.4066 - loglik: -2.9002e+02 - logprior: -5.2020e+01
Epoch 2/2
10/10 - 3s - loss: 303.5369 - loglik: -2.8245e+02 - logprior: -2.0698e+01
Fitted a model with MAP estimate = -296.0094
expansions: [(10, 1), (11, 1)]
discards: [  0  59  72 110]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 330.8307 - loglik: -2.7937e+02 - logprior: -5.1031e+01
Epoch 2/2
10/10 - 3s - loss: 295.4132 - loglik: -2.7680e+02 - logprior: -1.8164e+01
Fitted a model with MAP estimate = -287.4424
expansions: [(0, 2), (2, 2)]
discards: [ 0 51]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 314.9564 - loglik: -2.7423e+02 - logprior: -4.0268e+01
Epoch 2/10
10/10 - 3s - loss: 282.6108 - loglik: -2.7318e+02 - logprior: -8.9460e+00
Epoch 3/10
10/10 - 3s - loss: 276.1017 - loglik: -2.7311e+02 - logprior: -2.4928e+00
Epoch 4/10
10/10 - 3s - loss: 273.9501 - loglik: -2.7365e+02 - logprior: 0.2062
Epoch 5/10
10/10 - 3s - loss: 272.3419 - loglik: -2.7354e+02 - logprior: 1.7047
Epoch 6/10
10/10 - 3s - loss: 272.0150 - loglik: -2.7405e+02 - logprior: 2.5531
Epoch 7/10
10/10 - 3s - loss: 270.9120 - loglik: -2.7346e+02 - logprior: 3.0810
Epoch 8/10
10/10 - 3s - loss: 271.3042 - loglik: -2.7421e+02 - logprior: 3.4455
Fitted a model with MAP estimate = -270.2564
Time for alignment: 74.4485
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 432.9702 - loglik: -3.8672e+02 - logprior: -4.6229e+01
Epoch 2/10
10/10 - 2s - loss: 371.0831 - loglik: -3.5993e+02 - logprior: -1.1033e+01
Epoch 3/10
10/10 - 2s - loss: 337.5709 - loglik: -3.3252e+02 - logprior: -4.9306e+00
Epoch 4/10
10/10 - 2s - loss: 314.3517 - loglik: -3.1107e+02 - logprior: -3.0747e+00
Epoch 5/10
10/10 - 2s - loss: 303.0033 - loglik: -3.0018e+02 - logprior: -2.3426e+00
Epoch 6/10
10/10 - 2s - loss: 299.1364 - loglik: -2.9687e+02 - logprior: -1.8150e+00
Epoch 7/10
10/10 - 2s - loss: 297.2118 - loglik: -2.9547e+02 - logprior: -1.4188e+00
Epoch 8/10
10/10 - 2s - loss: 295.8433 - loglik: -2.9441e+02 - logprior: -1.1507e+00
Epoch 9/10
10/10 - 2s - loss: 295.6674 - loglik: -2.9433e+02 - logprior: -1.0477e+00
Epoch 10/10
10/10 - 2s - loss: 294.7848 - loglik: -2.9349e+02 - logprior: -9.8359e-01
Fitted a model with MAP estimate = -294.4059
expansions: [(8, 2), (10, 2), (13, 1), (18, 1), (19, 1), (26, 1), (48, 2), (58, 2), (59, 2), (62, 1), (64, 1), (73, 1), (82, 1), (90, 2), (91, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 343.5783 - loglik: -2.9111e+02 - logprior: -5.2142e+01
Epoch 2/2
10/10 - 3s - loss: 304.3818 - loglik: -2.8319e+02 - logprior: -2.0822e+01
Fitted a model with MAP estimate = -296.6231
expansions: [(6, 2), (8, 1), (34, 3)]
discards: [  0  11  55  68  70 108]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 331.4384 - loglik: -2.7985e+02 - logprior: -5.1163e+01
Epoch 2/2
10/10 - 3s - loss: 296.4628 - loglik: -2.7727e+02 - logprior: -1.8722e+01
Fitted a model with MAP estimate = -289.2873
expansions: [(0, 3)]
discards: [ 0 35]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 315.1693 - loglik: -2.7446e+02 - logprior: -4.0228e+01
Epoch 2/10
10/10 - 3s - loss: 282.7918 - loglik: -2.7341e+02 - logprior: -8.9000e+00
Epoch 3/10
10/10 - 3s - loss: 275.9665 - loglik: -2.7309e+02 - logprior: -2.3821e+00
Epoch 4/10
10/10 - 3s - loss: 274.2591 - loglik: -2.7407e+02 - logprior: 0.3136
Epoch 5/10
10/10 - 3s - loss: 272.7969 - loglik: -2.7410e+02 - logprior: 1.8085
Epoch 6/10
10/10 - 3s - loss: 272.2581 - loglik: -2.7443e+02 - logprior: 2.6818
Epoch 7/10
10/10 - 3s - loss: 271.6922 - loglik: -2.7440e+02 - logprior: 3.2256
Epoch 8/10
10/10 - 3s - loss: 271.2624 - loglik: -2.7433e+02 - logprior: 3.6002
Epoch 9/10
10/10 - 3s - loss: 270.5563 - loglik: -2.7394e+02 - logprior: 3.9237
Epoch 10/10
10/10 - 3s - loss: 270.7464 - loglik: -2.7441e+02 - logprior: 4.2050
Fitted a model with MAP estimate = -269.8651
Time for alignment: 79.2997
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 433.1548 - loglik: -3.8690e+02 - logprior: -4.6232e+01
Epoch 2/10
10/10 - 2s - loss: 371.1819 - loglik: -3.6003e+02 - logprior: -1.1024e+01
Epoch 3/10
10/10 - 2s - loss: 334.5136 - loglik: -3.2953e+02 - logprior: -4.8621e+00
Epoch 4/10
10/10 - 2s - loss: 312.6169 - loglik: -3.0938e+02 - logprior: -3.0463e+00
Epoch 5/10
10/10 - 2s - loss: 302.4476 - loglik: -2.9968e+02 - logprior: -2.3133e+00
Epoch 6/10
10/10 - 2s - loss: 298.6172 - loglik: -2.9629e+02 - logprior: -1.8606e+00
Epoch 7/10
10/10 - 2s - loss: 296.9222 - loglik: -2.9503e+02 - logprior: -1.5503e+00
Epoch 8/10
10/10 - 2s - loss: 295.5551 - loglik: -2.9398e+02 - logprior: -1.2933e+00
Epoch 9/10
10/10 - 2s - loss: 294.9632 - loglik: -2.9357e+02 - logprior: -1.1143e+00
Epoch 10/10
10/10 - 2s - loss: 294.6115 - loglik: -2.9328e+02 - logprior: -1.0486e+00
Fitted a model with MAP estimate = -294.1884
expansions: [(10, 4), (17, 1), (18, 1), (19, 1), (28, 3), (48, 2), (58, 2), (59, 1), (62, 1), (64, 1), (73, 1), (85, 1), (90, 2), (91, 3), (92, 2), (93, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 342.7458 - loglik: -2.9043e+02 - logprior: -5.2041e+01
Epoch 2/2
10/10 - 3s - loss: 303.4346 - loglik: -2.8252e+02 - logprior: -2.0599e+01
Fitted a model with MAP estimate = -296.3502
expansions: [(10, 2)]
discards: [  0  57  70 109]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 330.9023 - loglik: -2.7949e+02 - logprior: -5.1023e+01
Epoch 2/2
10/10 - 3s - loss: 295.5478 - loglik: -2.7701e+02 - logprior: -1.8112e+01
Fitted a model with MAP estimate = -288.0344
expansions: [(0, 2), (2, 2)]
discards: [ 0 37]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 316.0669 - loglik: -2.7535e+02 - logprior: -4.0279e+01
Epoch 2/10
10/10 - 3s - loss: 283.1696 - loglik: -2.7375e+02 - logprior: -8.9618e+00
Epoch 3/10
10/10 - 3s - loss: 276.9936 - loglik: -2.7402e+02 - logprior: -2.4931e+00
Epoch 4/10
10/10 - 3s - loss: 274.6758 - loglik: -2.7443e+02 - logprior: 0.2415
Epoch 5/10
10/10 - 3s - loss: 273.2723 - loglik: -2.7454e+02 - logprior: 1.7511
Epoch 6/10
10/10 - 3s - loss: 272.7304 - loglik: -2.7484e+02 - logprior: 2.6064
Epoch 7/10
10/10 - 3s - loss: 272.5907 - loglik: -2.7523e+02 - logprior: 3.1328
Epoch 8/10
10/10 - 3s - loss: 271.6950 - loglik: -2.7472e+02 - logprior: 3.5229
Epoch 9/10
10/10 - 3s - loss: 271.7318 - loglik: -2.7507e+02 - logprior: 3.8389
Fitted a model with MAP estimate = -271.0153
Time for alignment: 76.6602
Computed alignments with likelihoods: ['-270.2564', '-269.8651', '-271.0153']
Best model has likelihood: -269.8651
time for generating output: 0.2112
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/slectin.projection.fasta
SP score = 0.9149265274555298
Training of 3 independent models on file hip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f32eef77190>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f33231640d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3323164b80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f33685a5ca0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34a68f6760>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34ede2b2e0>, <__main__.SimpleDirichletPrior object at 0x7f345a79b3d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34b7c77700>

Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 455.1663 - loglik: -1.8612e+02 - logprior: -2.6904e+02
Epoch 2/10
10/10 - 1s - loss: 232.4906 - loglik: -1.6043e+02 - logprior: -7.2059e+01
Epoch 3/10
10/10 - 1s - loss: 172.7878 - loglik: -1.3991e+02 - logprior: -3.2881e+01
Epoch 4/10
10/10 - 1s - loss: 144.8595 - loglik: -1.2604e+02 - logprior: -1.8823e+01
Epoch 5/10
10/10 - 1s - loss: 131.1155 - loglik: -1.1991e+02 - logprior: -1.1189e+01
Epoch 6/10
10/10 - 1s - loss: 123.3010 - loglik: -1.1697e+02 - logprior: -6.1323e+00
Epoch 7/10
10/10 - 1s - loss: 118.3564 - loglik: -1.1508e+02 - logprior: -2.9034e+00
Epoch 8/10
10/10 - 1s - loss: 115.5467 - loglik: -1.1449e+02 - logprior: -7.1435e-01
Epoch 9/10
10/10 - 1s - loss: 113.9493 - loglik: -1.1454e+02 - logprior: 0.8759
Epoch 10/10
10/10 - 1s - loss: 112.8779 - loglik: -1.1468e+02 - logprior: 2.0824
Fitted a model with MAP estimate = -112.1106
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 464.1901 - loglik: -1.0580e+02 - logprior: -3.5809e+02
Epoch 2/2
10/10 - 1s - loss: 204.9127 - loglik: -9.4838e+01 - logprior: -1.0976e+02
Fitted a model with MAP estimate = -156.5947
expansions: []
discards: [ 0 21 37 48 53]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 397.7818 - loglik: -9.3789e+01 - logprior: -3.0368e+02
Epoch 2/2
10/10 - 1s - loss: 211.4619 - loglik: -9.3880e+01 - logprior: -1.1727e+02
Fitted a model with MAP estimate = -181.6406
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 370.0223 - loglik: -9.1950e+01 - logprior: -2.7776e+02
Epoch 2/10
10/10 - 1s - loss: 168.3927 - loglik: -9.2199e+01 - logprior: -7.5888e+01
Epoch 3/10
10/10 - 1s - loss: 118.0449 - loglik: -9.2870e+01 - logprior: -2.4869e+01
Epoch 4/10
10/10 - 1s - loss: 99.9935 - loglik: -9.3514e+01 - logprior: -6.1700e+00
Epoch 5/10
10/10 - 1s - loss: 90.8598 - loglik: -9.4018e+01 - logprior: 3.4645
Epoch 6/10
10/10 - 1s - loss: 85.5585 - loglik: -9.4407e+01 - logprior: 9.1558
Epoch 7/10
10/10 - 1s - loss: 82.1802 - loglik: -9.4692e+01 - logprior: 12.8192
Epoch 8/10
10/10 - 1s - loss: 79.7915 - loglik: -9.4898e+01 - logprior: 15.4135
Epoch 9/10
10/10 - 1s - loss: 77.9336 - loglik: -9.5051e+01 - logprior: 17.4249
Epoch 10/10
10/10 - 1s - loss: 76.3773 - loglik: -9.5173e+01 - logprior: 19.1026
Fitted a model with MAP estimate = -75.2912
Time for alignment: 37.2379
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 455.1665 - loglik: -1.8612e+02 - logprior: -2.6904e+02
Epoch 2/10
10/10 - 1s - loss: 232.4904 - loglik: -1.6043e+02 - logprior: -7.2059e+01
Epoch 3/10
10/10 - 1s - loss: 172.7876 - loglik: -1.3991e+02 - logprior: -3.2880e+01
Epoch 4/10
10/10 - 1s - loss: 144.8596 - loglik: -1.2604e+02 - logprior: -1.8823e+01
Epoch 5/10
10/10 - 1s - loss: 131.1890 - loglik: -1.2002e+02 - logprior: -1.1158e+01
Epoch 6/10
10/10 - 1s - loss: 123.3582 - loglik: -1.1711e+02 - logprior: -6.0944e+00
Epoch 7/10
10/10 - 1s - loss: 118.3714 - loglik: -1.1512e+02 - logprior: -2.8859e+00
Epoch 8/10
10/10 - 1s - loss: 115.5541 - loglik: -1.1449e+02 - logprior: -7.1200e-01
Epoch 9/10
10/10 - 1s - loss: 113.9511 - loglik: -1.1453e+02 - logprior: 0.8747
Epoch 10/10
10/10 - 1s - loss: 112.8772 - loglik: -1.1467e+02 - logprior: 2.0750
Fitted a model with MAP estimate = -112.1091
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 464.1786 - loglik: -1.0578e+02 - logprior: -3.5810e+02
Epoch 2/2
10/10 - 1s - loss: 204.9038 - loglik: -9.4816e+01 - logprior: -1.0977e+02
Fitted a model with MAP estimate = -156.5873
expansions: []
discards: [ 0 21 37 48 53]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 397.7849 - loglik: -9.3784e+01 - logprior: -3.0369e+02
Epoch 2/2
10/10 - 1s - loss: 211.4728 - loglik: -9.3882e+01 - logprior: -1.1728e+02
Fitted a model with MAP estimate = -181.6546
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 370.0378 - loglik: -9.1947e+01 - logprior: -2.7778e+02
Epoch 2/10
10/10 - 1s - loss: 168.4077 - loglik: -9.2197e+01 - logprior: -7.5905e+01
Epoch 3/10
10/10 - 1s - loss: 118.0624 - loglik: -9.2873e+01 - logprior: -2.4884e+01
Epoch 4/10
10/10 - 1s - loss: 100.0113 - loglik: -9.3510e+01 - logprior: -6.1919e+00
Epoch 5/10
10/10 - 1s - loss: 90.8792 - loglik: -9.4015e+01 - logprior: 3.4426
Epoch 6/10
10/10 - 1s - loss: 85.5792 - loglik: -9.4408e+01 - logprior: 9.1351
Epoch 7/10
10/10 - 1s - loss: 82.2020 - loglik: -9.4691e+01 - logprior: 12.7960
Epoch 8/10
10/10 - 1s - loss: 79.8147 - loglik: -9.4896e+01 - logprior: 15.3883
Epoch 9/10
10/10 - 1s - loss: 77.9575 - loglik: -9.5050e+01 - logprior: 17.3995
Epoch 10/10
10/10 - 1s - loss: 76.4023 - loglik: -9.5171e+01 - logprior: 19.0758
Fitted a model with MAP estimate = -75.3171
Time for alignment: 36.8245
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 455.1663 - loglik: -1.8612e+02 - logprior: -2.6904e+02
Epoch 2/10
10/10 - 1s - loss: 232.4904 - loglik: -1.6043e+02 - logprior: -7.2059e+01
Epoch 3/10
10/10 - 1s - loss: 172.7879 - loglik: -1.3991e+02 - logprior: -3.2881e+01
Epoch 4/10
10/10 - 1s - loss: 144.8597 - loglik: -1.2604e+02 - logprior: -1.8823e+01
Epoch 5/10
10/10 - 1s - loss: 131.1381 - loglik: -1.1993e+02 - logprior: -1.1189e+01
Epoch 6/10
10/10 - 1s - loss: 123.3777 - loglik: -1.1706e+02 - logprior: -6.1055e+00
Epoch 7/10
10/10 - 1s - loss: 118.4876 - loglik: -1.1519e+02 - logprior: -2.8842e+00
Epoch 8/10
10/10 - 1s - loss: 115.6897 - loglik: -1.1463e+02 - logprior: -7.1468e-01
Epoch 9/10
10/10 - 1s - loss: 114.1008 - loglik: -1.1470e+02 - logprior: 0.8975
Epoch 10/10
10/10 - 1s - loss: 112.9679 - loglik: -1.1473e+02 - logprior: 2.0792
Fitted a model with MAP estimate = -112.1211
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 464.1739 - loglik: -1.0577e+02 - logprior: -3.5809e+02
Epoch 2/2
10/10 - 1s - loss: 204.8885 - loglik: -9.4811e+01 - logprior: -1.0976e+02
Fitted a model with MAP estimate = -156.5739
expansions: []
discards: [ 0 21 37 48 53]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 397.7710 - loglik: -9.3780e+01 - logprior: -3.0368e+02
Epoch 2/2
10/10 - 1s - loss: 211.4519 - loglik: -9.3873e+01 - logprior: -1.1727e+02
Fitted a model with MAP estimate = -181.6344
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 370.0217 - loglik: -9.1949e+01 - logprior: -2.7776e+02
Epoch 2/10
10/10 - 1s - loss: 168.3962 - loglik: -9.2200e+01 - logprior: -7.5891e+01
Epoch 3/10
10/10 - 1s - loss: 118.0550 - loglik: -9.2868e+01 - logprior: -2.4881e+01
Epoch 4/10
10/10 - 1s - loss: 100.0057 - loglik: -9.3512e+01 - logprior: -6.1851e+00
Epoch 5/10
10/10 - 1s - loss: 90.8733 - loglik: -9.4016e+01 - logprior: 3.4497
Epoch 6/10
10/10 - 1s - loss: 85.5731 - loglik: -9.4407e+01 - logprior: 9.1404
Epoch 7/10
10/10 - 1s - loss: 82.1955 - loglik: -9.4692e+01 - logprior: 12.8030
Epoch 8/10
10/10 - 1s - loss: 79.8075 - loglik: -9.4897e+01 - logprior: 15.3956
Epoch 9/10
10/10 - 1s - loss: 77.9497 - loglik: -9.5051e+01 - logprior: 17.4081
Epoch 10/10
10/10 - 1s - loss: 76.3932 - loglik: -9.5172e+01 - logprior: 19.0859
Fitted a model with MAP estimate = -75.3072
Time for alignment: 36.5980
Computed alignments with likelihoods: ['-75.2912', '-75.3171', '-75.3072']
Best model has likelihood: -75.2912
time for generating output: 0.1355
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hip.projection.fasta
SP score = 0.8444108761329305
Training of 3 independent models on file peroxidase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f32f0501160>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3356f9b1f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32ee4b3b20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34ee4c1640>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f347c37f670>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f33a3f707f0>, <__main__.SimpleDirichletPrior object at 0x7f338a285790>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f346b6e9e50>

Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 685.6777 - loglik: -6.8129e+02 - logprior: -4.2906e+00
Epoch 2/10
26/26 - 11s - loss: 586.4868 - loglik: -5.8476e+02 - logprior: -1.5979e+00
Epoch 3/10
26/26 - 11s - loss: 570.7618 - loglik: -5.6862e+02 - logprior: -1.6367e+00
Epoch 4/10
26/26 - 11s - loss: 568.7556 - loglik: -5.6648e+02 - logprior: -1.7019e+00
Epoch 5/10
26/26 - 11s - loss: 567.1939 - loglik: -5.6483e+02 - logprior: -1.7700e+00
Epoch 6/10
26/26 - 11s - loss: 565.6367 - loglik: -5.6324e+02 - logprior: -1.8050e+00
Epoch 7/10
26/26 - 11s - loss: 564.6910 - loglik: -5.6223e+02 - logprior: -1.8335e+00
Epoch 8/10
26/26 - 11s - loss: 564.8336 - loglik: -5.6239e+02 - logprior: -1.8334e+00
Fitted a model with MAP estimate = -563.2492
expansions: [(9, 1), (173, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 202 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 15s - loss: 566.2849 - loglik: -5.6049e+02 - logprior: -5.1873e+00
Epoch 2/2
26/26 - 11s - loss: 563.3500 - loglik: -5.6097e+02 - logprior: -1.7605e+00
Fitted a model with MAP estimate = -561.2683
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 202 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 16s - loss: 565.3369 - loglik: -5.5973e+02 - logprior: -4.9802e+00
Epoch 2/10
26/26 - 11s - loss: 561.3376 - loglik: -5.5919e+02 - logprior: -1.5047e+00
Epoch 3/10
26/26 - 11s - loss: 560.8297 - loglik: -5.5891e+02 - logprior: -1.2648e+00
Epoch 4/10
26/26 - 11s - loss: 562.1430 - loglik: -5.6032e+02 - logprior: -1.1745e+00
Fitted a model with MAP estimate = -560.2437
Time for alignment: 200.1777
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 691.4138 - loglik: -6.8699e+02 - logprior: -4.3080e+00
Epoch 2/10
26/26 - 11s - loss: 593.5401 - loglik: -5.9209e+02 - logprior: -1.3327e+00
Epoch 3/10
26/26 - 11s - loss: 578.0342 - loglik: -5.7611e+02 - logprior: -1.4217e+00
Epoch 4/10
26/26 - 11s - loss: 575.7450 - loglik: -5.7365e+02 - logprior: -1.4709e+00
Epoch 5/10
26/26 - 11s - loss: 572.7982 - loglik: -5.7068e+02 - logprior: -1.4944e+00
Epoch 6/10
26/26 - 11s - loss: 572.4177 - loglik: -5.7030e+02 - logprior: -1.5113e+00
Epoch 7/10
26/26 - 11s - loss: 571.8962 - loglik: -5.6976e+02 - logprior: -1.5156e+00
Epoch 8/10
26/26 - 11s - loss: 570.2861 - loglik: -5.6813e+02 - logprior: -1.5319e+00
Epoch 9/10
26/26 - 11s - loss: 571.1584 - loglik: -5.6898e+02 - logprior: -1.5430e+00
Fitted a model with MAP estimate = -570.2197
expansions: [(0, 3), (11, 1), (95, 2), (107, 7), (173, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 214 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 15s - loss: 571.3375 - loglik: -5.6377e+02 - logprior: -6.9110e+00
Epoch 2/2
26/26 - 11s - loss: 562.9066 - loglik: -5.6000e+02 - logprior: -2.2062e+00
Fitted a model with MAP estimate = -560.8311
expansions: []
discards: [ 1  2  3 99]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 210 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 15s - loss: 567.0667 - loglik: -5.6142e+02 - logprior: -4.9043e+00
Epoch 2/2
26/26 - 12s - loss: 561.8979 - loglik: -5.5974e+02 - logprior: -1.4033e+00
Fitted a model with MAP estimate = -560.7503
expansions: [(0, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 213 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 17s - loss: 566.7556 - loglik: -5.5934e+02 - logprior: -6.6486e+00
Epoch 2/10
26/26 - 11s - loss: 561.5724 - loglik: -5.5909e+02 - logprior: -1.6974e+00
Epoch 3/10
26/26 - 11s - loss: 561.9430 - loglik: -5.5986e+02 - logprior: -1.2995e+00
Fitted a model with MAP estimate = -559.3027
Time for alignment: 240.1494
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 689.9177 - loglik: -6.8548e+02 - logprior: -4.3251e+00
Epoch 2/10
26/26 - 11s - loss: 587.1007 - loglik: -5.8556e+02 - logprior: -1.3929e+00
Epoch 3/10
26/26 - 11s - loss: 573.9504 - loglik: -5.7199e+02 - logprior: -1.4205e+00
Epoch 4/10
26/26 - 11s - loss: 568.7203 - loglik: -5.6651e+02 - logprior: -1.5527e+00
Epoch 5/10
26/26 - 11s - loss: 566.9880 - loglik: -5.6469e+02 - logprior: -1.5981e+00
Epoch 6/10
26/26 - 11s - loss: 566.8184 - loglik: -5.6449e+02 - logprior: -1.6096e+00
Epoch 7/10
26/26 - 11s - loss: 564.5652 - loglik: -5.6219e+02 - logprior: -1.6416e+00
Epoch 8/10
26/26 - 11s - loss: 566.4523 - loglik: -5.6408e+02 - logprior: -1.6462e+00
Fitted a model with MAP estimate = -563.6752
expansions: [(11, 2), (51, 1), (82, 1), (96, 1), (130, 1), (156, 1), (175, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 208 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 15s - loss: 569.5937 - loglik: -5.6154e+02 - logprior: -7.3375e+00
Epoch 2/2
26/26 - 12s - loss: 562.2974 - loglik: -5.5829e+02 - logprior: -3.2272e+00
Fitted a model with MAP estimate = -558.3685
expansions: [(0, 3)]
discards: [ 0 11]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 209 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 17s - loss: 564.2505 - loglik: -5.5857e+02 - logprior: -4.9019e+00
Epoch 2/2
26/26 - 12s - loss: 557.6267 - loglik: -5.5531e+02 - logprior: -1.5288e+00
Fitted a model with MAP estimate = -557.0944
expansions: []
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 208 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 560.3622 - loglik: -5.5495e+02 - logprior: -4.6206e+00
Epoch 2/10
26/26 - 12s - loss: 559.6928 - loglik: -5.5770e+02 - logprior: -1.2079e+00
Epoch 3/10
26/26 - 12s - loss: 557.2647 - loglik: -5.5565e+02 - logprior: -8.2891e-01
Epoch 4/10
26/26 - 12s - loss: 557.1842 - loglik: -5.5569e+02 - logprior: -7.2593e-01
Epoch 5/10
26/26 - 12s - loss: 558.0067 - loglik: -5.5665e+02 - logprior: -5.9148e-01
Fitted a model with MAP estimate = -555.9037
Time for alignment: 252.7350
Computed alignments with likelihoods: ['-560.2437', '-559.3027', '-555.9037']
Best model has likelihood: -555.9037
time for generating output: 0.3363
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/peroxidase.projection.fasta
SP score = 0.7116129032258065
Training of 3 independent models on file TNF.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f343fe98eb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3335181490>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2b1cb3b370>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f32f05418b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32f0541ca0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f32f05419a0>, <__main__.SimpleDirichletPrior object at 0x7f349e48bcd0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34243b8430>

Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 422.9499 - loglik: -3.4503e+02 - logprior: -7.7897e+01
Epoch 2/10
10/10 - 2s - loss: 338.0409 - loglik: -3.1915e+02 - logprior: -1.8758e+01
Epoch 3/10
10/10 - 2s - loss: 301.1442 - loglik: -2.9336e+02 - logprior: -7.6734e+00
Epoch 4/10
10/10 - 2s - loss: 281.0764 - loglik: -2.7690e+02 - logprior: -3.9584e+00
Epoch 5/10
10/10 - 2s - loss: 272.2128 - loglik: -2.6972e+02 - logprior: -1.9987e+00
Epoch 6/10
10/10 - 2s - loss: 268.3464 - loglik: -2.6706e+02 - logprior: -8.2158e-01
Epoch 7/10
10/10 - 2s - loss: 265.4437 - loglik: -2.6500e+02 - logprior: -1.2742e-01
Epoch 8/10
10/10 - 2s - loss: 263.2406 - loglik: -2.6325e+02 - logprior: 0.3026
Epoch 9/10
10/10 - 2s - loss: 262.1479 - loglik: -2.6249e+02 - logprior: 0.6936
Epoch 10/10
10/10 - 2s - loss: 261.5319 - loglik: -2.6211e+02 - logprior: 0.9734
Fitted a model with MAP estimate = -260.6287
expansions: [(9, 4), (10, 2), (30, 1), (36, 4), (45, 3), (48, 1), (54, 2), (64, 3), (78, 2), (83, 3), (86, 1), (88, 4), (89, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 344.6812 - loglik: -2.5690e+02 - logprior: -8.7405e+01
Epoch 2/2
10/10 - 2s - loss: 281.3904 - loglik: -2.4637e+02 - logprior: -3.4643e+01
Fitted a model with MAP estimate = -269.8307
expansions: [(0, 2), (10, 2), (13, 1), (115, 2)]
discards: [  0  42  43  97 105]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 129 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 308.2255 - loglik: -2.3868e+02 - logprior: -6.9149e+01
Epoch 2/2
10/10 - 2s - loss: 250.4561 - loglik: -2.3422e+02 - logprior: -1.5803e+01
Fitted a model with MAP estimate = -241.3401
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 128 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 317.4734 - loglik: -2.3382e+02 - logprior: -8.3188e+01
Epoch 2/10
10/10 - 2s - loss: 257.4790 - loglik: -2.3328e+02 - logprior: -2.3706e+01
Epoch 3/10
10/10 - 2s - loss: 240.0920 - loglik: -2.3363e+02 - logprior: -5.9606e+00
Epoch 4/10
10/10 - 2s - loss: 233.8034 - loglik: -2.3361e+02 - logprior: 0.3287
Epoch 5/10
10/10 - 2s - loss: 231.2225 - loglik: -2.3391e+02 - logprior: 3.2302
Epoch 6/10
10/10 - 2s - loss: 229.0586 - loglik: -2.3339e+02 - logprior: 4.8781
Epoch 7/10
10/10 - 2s - loss: 228.6896 - loglik: -2.3405e+02 - logprior: 5.9095
Epoch 8/10
10/10 - 2s - loss: 227.5011 - loglik: -2.3359e+02 - logprior: 6.6471
Epoch 9/10
10/10 - 2s - loss: 227.1473 - loglik: -2.3384e+02 - logprior: 7.2527
Epoch 10/10
10/10 - 2s - loss: 226.5170 - loglik: -2.3375e+02 - logprior: 7.8043
Fitted a model with MAP estimate = -225.7969
Time for alignment: 62.5671
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 423.0316 - loglik: -3.4511e+02 - logprior: -7.7896e+01
Epoch 2/10
10/10 - 2s - loss: 337.9887 - loglik: -3.1910e+02 - logprior: -1.8768e+01
Epoch 3/10
10/10 - 2s - loss: 298.1894 - loglik: -2.9039e+02 - logprior: -7.7000e+00
Epoch 4/10
10/10 - 2s - loss: 278.8158 - loglik: -2.7457e+02 - logprior: -4.0584e+00
Epoch 5/10
10/10 - 2s - loss: 270.2278 - loglik: -2.6738e+02 - logprior: -2.3756e+00
Epoch 6/10
10/10 - 2s - loss: 266.0031 - loglik: -2.6412e+02 - logprior: -1.3735e+00
Epoch 7/10
10/10 - 2s - loss: 263.7602 - loglik: -2.6275e+02 - logprior: -6.3435e-01
Epoch 8/10
10/10 - 2s - loss: 262.2505 - loglik: -2.6188e+02 - logprior: -5.9003e-02
Epoch 9/10
10/10 - 2s - loss: 261.8545 - loglik: -2.6186e+02 - logprior: 0.3126
Epoch 10/10
10/10 - 2s - loss: 260.9689 - loglik: -2.6130e+02 - logprior: 0.6457
Fitted a model with MAP estimate = -260.4818
expansions: [(10, 1), (11, 4), (12, 1), (19, 2), (36, 4), (45, 3), (48, 1), (55, 1), (59, 1), (79, 2), (83, 3), (86, 1), (88, 4), (89, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 341.7021 - loglik: -2.5417e+02 - logprior: -8.7202e+01
Epoch 2/2
10/10 - 2s - loss: 278.6409 - loglik: -2.4401e+02 - logprior: -3.4256e+01
Fitted a model with MAP estimate = -267.8127
expansions: [(0, 2), (11, 1), (113, 2)]
discards: [  0  43  96 103]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 306.4976 - loglik: -2.3709e+02 - logprior: -6.8991e+01
Epoch 2/2
10/10 - 2s - loss: 250.3260 - loglik: -2.3411e+02 - logprior: -1.5766e+01
Fitted a model with MAP estimate = -241.5380
expansions: [(12, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 318.5241 - loglik: -2.3422e+02 - logprior: -8.3823e+01
Epoch 2/10
10/10 - 2s - loss: 259.2807 - loglik: -2.3365e+02 - logprior: -2.5126e+01
Epoch 3/10
10/10 - 2s - loss: 241.3157 - loglik: -2.3436e+02 - logprior: -6.4411e+00
Epoch 4/10
10/10 - 2s - loss: 234.0616 - loglik: -2.3399e+02 - logprior: 0.4444
Epoch 5/10
10/10 - 2s - loss: 231.2778 - loglik: -2.3420e+02 - logprior: 3.4376
Epoch 6/10
10/10 - 2s - loss: 229.7469 - loglik: -2.3430e+02 - logprior: 5.0832
Epoch 7/10
10/10 - 2s - loss: 228.7121 - loglik: -2.3430e+02 - logprior: 6.1138
Epoch 8/10
10/10 - 2s - loss: 228.0622 - loglik: -2.3439e+02 - logprior: 6.8533
Epoch 9/10
10/10 - 2s - loss: 227.8317 - loglik: -2.3477e+02 - logprior: 7.4628
Epoch 10/10
10/10 - 2s - loss: 227.0106 - loglik: -2.3451e+02 - logprior: 8.0187
Fitted a model with MAP estimate = -226.3727
Time for alignment: 61.1386
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 423.0921 - loglik: -3.4517e+02 - logprior: -7.7897e+01
Epoch 2/10
10/10 - 2s - loss: 338.0477 - loglik: -3.1917e+02 - logprior: -1.8754e+01
Epoch 3/10
10/10 - 2s - loss: 299.5885 - loglik: -2.9186e+02 - logprior: -7.6285e+00
Epoch 4/10
10/10 - 2s - loss: 279.0344 - loglik: -2.7479e+02 - logprior: -4.0239e+00
Epoch 5/10
10/10 - 2s - loss: 268.4886 - loglik: -2.6531e+02 - logprior: -2.6417e+00
Epoch 6/10
10/10 - 2s - loss: 264.0714 - loglik: -2.6189e+02 - logprior: -1.6359e+00
Epoch 7/10
10/10 - 2s - loss: 261.8954 - loglik: -2.6061e+02 - logprior: -8.7424e-01
Epoch 8/10
10/10 - 2s - loss: 260.8955 - loglik: -2.6022e+02 - logprior: -3.2293e-01
Epoch 9/10
10/10 - 2s - loss: 259.4837 - loglik: -2.5907e+02 - logprior: -3.1300e-02
Epoch 10/10
10/10 - 2s - loss: 259.0670 - loglik: -2.5889e+02 - logprior: 0.2540
Fitted a model with MAP estimate = -258.1756
expansions: [(5, 1), (6, 1), (10, 2), (11, 2), (12, 1), (19, 2), (36, 4), (45, 1), (46, 1), (55, 3), (63, 4), (82, 1), (86, 4), (88, 3), (89, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 336.9458 - loglik: -2.4929e+02 - logprior: -8.7242e+01
Epoch 2/2
10/10 - 2s - loss: 272.6307 - loglik: -2.3812e+02 - logprior: -3.4070e+01
Fitted a model with MAP estimate = -261.2222
expansions: [(0, 2), (69, 1)]
discards: [ 0 44 45 76 77 78]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 303.1555 - loglik: -2.3362e+02 - logprior: -6.9095e+01
Epoch 2/2
10/10 - 2s - loss: 247.4823 - loglik: -2.3093e+02 - logprior: -1.6090e+01
Fitted a model with MAP estimate = -238.5273
expansions: [(13, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 315.4892 - loglik: -2.3094e+02 - logprior: -8.4060e+01
Epoch 2/10
10/10 - 2s - loss: 256.8073 - loglik: -2.3080e+02 - logprior: -2.5499e+01
Epoch 3/10
10/10 - 2s - loss: 238.1582 - loglik: -2.3090e+02 - logprior: -6.7428e+00
Epoch 4/10
10/10 - 2s - loss: 231.2143 - loglik: -2.3089e+02 - logprior: 0.1991
Epoch 5/10
10/10 - 2s - loss: 228.5658 - loglik: -2.3125e+02 - logprior: 3.2152
Epoch 6/10
10/10 - 2s - loss: 227.1310 - loglik: -2.3142e+02 - logprior: 4.8344
Epoch 7/10
10/10 - 2s - loss: 226.0057 - loglik: -2.3132e+02 - logprior: 5.8614
Epoch 8/10
10/10 - 2s - loss: 225.3787 - loglik: -2.3144e+02 - logprior: 6.6065
Epoch 9/10
10/10 - 2s - loss: 224.8885 - loglik: -2.3156e+02 - logprior: 7.2244
Epoch 10/10
10/10 - 2s - loss: 224.5012 - loglik: -2.3173e+02 - logprior: 7.7786
Fitted a model with MAP estimate = -223.6218
Time for alignment: 61.1973
Computed alignments with likelihoods: ['-225.7969', '-226.3727', '-223.6218']
Best model has likelihood: -223.6218
time for generating output: 0.1961
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/TNF.projection.fasta
SP score = 0.8474820143884892
Training of 3 independent models on file egf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2b2ca3f220>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f343ffacd30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32f04e9e20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f32edc7c700>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32edc7ce80>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f332395ef10>, <__main__.SimpleDirichletPrior object at 0x7f330a7dc460>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f32f99be9d0>

Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 97.2182 - loglik: -9.2754e+01 - logprior: -4.4439e+00
Epoch 2/10
17/17 - 1s - loss: 75.4707 - loglik: -7.3900e+01 - logprior: -1.5585e+00
Epoch 3/10
17/17 - 1s - loss: 65.4915 - loglik: -6.3816e+01 - logprior: -1.6708e+00
Epoch 4/10
17/17 - 1s - loss: 63.3581 - loglik: -6.1625e+01 - logprior: -1.6678e+00
Epoch 5/10
17/17 - 1s - loss: 63.2142 - loglik: -6.1517e+01 - logprior: -1.5751e+00
Epoch 6/10
17/17 - 1s - loss: 62.9990 - loglik: -6.1289e+01 - logprior: -1.5928e+00
Epoch 7/10
17/17 - 1s - loss: 62.7023 - loglik: -6.0983e+01 - logprior: -1.5777e+00
Epoch 8/10
17/17 - 1s - loss: 62.7269 - loglik: -6.1001e+01 - logprior: -1.5641e+00
Fitted a model with MAP estimate = -62.4886
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 6s - loss: 69.6012 - loglik: -6.3833e+01 - logprior: -5.5820e+00
Epoch 2/2
17/17 - 1s - loss: 61.3157 - loglik: -5.8534e+01 - logprior: -2.5760e+00
Fitted a model with MAP estimate = -58.9058
expansions: []
discards: [12 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 61.6457 - loglik: -5.7059e+01 - logprior: -4.3826e+00
Epoch 2/2
17/17 - 1s - loss: 58.0832 - loglik: -5.6164e+01 - logprior: -1.7086e+00
Fitted a model with MAP estimate = -57.6436
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 61.0173 - loglik: -5.6525e+01 - logprior: -4.2742e+00
Epoch 2/10
17/17 - 1s - loss: 58.0120 - loglik: -5.6090e+01 - logprior: -1.6985e+00
Epoch 3/10
17/17 - 1s - loss: 57.8055 - loglik: -5.6114e+01 - logprior: -1.4623e+00
Epoch 4/10
17/17 - 1s - loss: 57.5838 - loglik: -5.5948e+01 - logprior: -1.4021e+00
Epoch 5/10
17/17 - 1s - loss: 57.3680 - loglik: -5.5748e+01 - logprior: -1.3741e+00
Epoch 6/10
17/17 - 1s - loss: 57.4731 - loglik: -5.5882e+01 - logprior: -1.3411e+00
Fitted a model with MAP estimate = -57.1466
Time for alignment: 39.2827
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 97.1946 - loglik: -9.2732e+01 - logprior: -4.4423e+00
Epoch 2/10
17/17 - 1s - loss: 75.3132 - loglik: -7.3739e+01 - logprior: -1.5608e+00
Epoch 3/10
17/17 - 1s - loss: 65.6432 - loglik: -6.3959e+01 - logprior: -1.6796e+00
Epoch 4/10
17/17 - 1s - loss: 63.8213 - loglik: -6.2167e+01 - logprior: -1.6505e+00
Epoch 5/10
17/17 - 1s - loss: 63.1316 - loglik: -6.1484e+01 - logprior: -1.5713e+00
Epoch 6/10
17/17 - 1s - loss: 62.9241 - loglik: -6.1216e+01 - logprior: -1.5936e+00
Epoch 7/10
17/17 - 1s - loss: 62.9120 - loglik: -6.1224e+01 - logprior: -1.5706e+00
Epoch 8/10
17/17 - 1s - loss: 62.7332 - loglik: -6.1029e+01 - logprior: -1.5623e+00
Epoch 9/10
17/17 - 1s - loss: 62.6701 - loglik: -6.0966e+01 - logprior: -1.5555e+00
Epoch 10/10
17/17 - 1s - loss: 62.6156 - loglik: -6.0907e+01 - logprior: -1.5464e+00
Fitted a model with MAP estimate = -62.4416
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 6s - loss: 69.7037 - loglik: -6.3935e+01 - logprior: -5.5830e+00
Epoch 2/2
17/17 - 1s - loss: 61.5822 - loglik: -5.8760e+01 - logprior: -2.6136e+00
Fitted a model with MAP estimate = -59.0842
expansions: []
discards: [12 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 61.6136 - loglik: -5.7008e+01 - logprior: -4.3990e+00
Epoch 2/2
17/17 - 1s - loss: 58.1853 - loglik: -5.6274e+01 - logprior: -1.7095e+00
Fitted a model with MAP estimate = -57.6712
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 60.9768 - loglik: -5.6484e+01 - logprior: -4.2766e+00
Epoch 2/10
17/17 - 1s - loss: 57.9837 - loglik: -5.6064e+01 - logprior: -1.6988e+00
Epoch 3/10
17/17 - 1s - loss: 57.7660 - loglik: -5.6082e+01 - logprior: -1.4581e+00
Epoch 4/10
17/17 - 1s - loss: 57.6291 - loglik: -5.5993e+01 - logprior: -1.4048e+00
Epoch 5/10
17/17 - 1s - loss: 57.5742 - loglik: -5.5967e+01 - logprior: -1.3655e+00
Epoch 6/10
17/17 - 1s - loss: 57.5656 - loglik: -5.5971e+01 - logprior: -1.3448e+00
Epoch 7/10
17/17 - 1s - loss: 57.2932 - loglik: -5.5717e+01 - logprior: -1.3288e+00
Epoch 8/10
17/17 - 1s - loss: 57.5125 - loglik: -5.5941e+01 - logprior: -1.3063e+00
Fitted a model with MAP estimate = -57.1298
Time for alignment: 41.9699
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 97.1956 - loglik: -9.2728e+01 - logprior: -4.4474e+00
Epoch 2/10
17/17 - 1s - loss: 76.8457 - loglik: -7.5266e+01 - logprior: -1.5653e+00
Epoch 3/10
17/17 - 1s - loss: 66.4031 - loglik: -6.4728e+01 - logprior: -1.6626e+00
Epoch 4/10
17/17 - 1s - loss: 63.7968 - loglik: -6.1986e+01 - logprior: -1.6919e+00
Epoch 5/10
17/17 - 1s - loss: 62.9691 - loglik: -6.1245e+01 - logprior: -1.5815e+00
Epoch 6/10
17/17 - 1s - loss: 62.9549 - loglik: -6.1216e+01 - logprior: -1.6036e+00
Epoch 7/10
17/17 - 1s - loss: 62.9606 - loglik: -6.1216e+01 - logprior: -1.5862e+00
Fitted a model with MAP estimate = -62.6265
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 69.6553 - loglik: -6.3887e+01 - logprior: -5.5837e+00
Epoch 2/2
17/17 - 1s - loss: 61.2470 - loglik: -5.8483e+01 - logprior: -2.5554e+00
Fitted a model with MAP estimate = -58.9022
expansions: []
discards: [12 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 61.5307 - loglik: -5.6950e+01 - logprior: -4.3718e+00
Epoch 2/2
17/17 - 1s - loss: 58.2599 - loglik: -5.6350e+01 - logprior: -1.7094e+00
Fitted a model with MAP estimate = -57.6680
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 60.9759 - loglik: -5.6494e+01 - logprior: -4.2760e+00
Epoch 2/10
17/17 - 1s - loss: 58.0832 - loglik: -5.6176e+01 - logprior: -1.6940e+00
Epoch 3/10
17/17 - 1s - loss: 57.7330 - loglik: -5.6049e+01 - logprior: -1.4638e+00
Epoch 4/10
17/17 - 1s - loss: 57.6408 - loglik: -5.6014e+01 - logprior: -1.4002e+00
Epoch 5/10
17/17 - 1s - loss: 57.3730 - loglik: -5.5772e+01 - logprior: -1.3656e+00
Epoch 6/10
17/17 - 1s - loss: 57.5598 - loglik: -5.5973e+01 - logprior: -1.3439e+00
Fitted a model with MAP estimate = -57.1628
Time for alignment: 38.1399
Computed alignments with likelihoods: ['-57.1466', '-57.1298', '-57.1628']
Best model has likelihood: -57.1298
time for generating output: 0.1282
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/egf.projection.fasta
SP score = 0.7630258587417985
Training of 3 independent models on file HMG_box.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2b15952430>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f2b2592cf40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3322ced2e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3322ced3a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3322ced580>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f2b2dfcbfa0>, <__main__.SimpleDirichletPrior object at 0x7f3381f83e80>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f32f99be9d0>

Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 193.7817 - loglik: -1.8561e+02 - logprior: -8.1667e+00
Epoch 2/10
13/13 - 1s - loss: 163.9156 - loglik: -1.6163e+02 - logprior: -2.2871e+00
Epoch 3/10
13/13 - 1s - loss: 146.2853 - loglik: -1.4425e+02 - logprior: -1.9627e+00
Epoch 4/10
13/13 - 1s - loss: 138.8660 - loglik: -1.3644e+02 - logprior: -2.1045e+00
Epoch 5/10
13/13 - 1s - loss: 136.1802 - loglik: -1.3383e+02 - logprior: -2.0229e+00
Epoch 6/10
13/13 - 1s - loss: 134.6494 - loglik: -1.3247e+02 - logprior: -1.9457e+00
Epoch 7/10
13/13 - 1s - loss: 134.1889 - loglik: -1.3201e+02 - logprior: -1.9901e+00
Epoch 8/10
13/13 - 1s - loss: 133.9014 - loglik: -1.3174e+02 - logprior: -1.9792e+00
Epoch 9/10
13/13 - 1s - loss: 133.5681 - loglik: -1.3143e+02 - logprior: -1.9572e+00
Epoch 10/10
13/13 - 1s - loss: 133.7618 - loglik: -1.3164e+02 - logprior: -1.9493e+00
Fitted a model with MAP estimate = -133.3218
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (34, 1), (41, 1), (44, 1), (45, 2), (46, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 139.6964 - loglik: -1.2978e+02 - logprior: -9.7249e+00
Epoch 2/2
13/13 - 1s - loss: 126.1263 - loglik: -1.2122e+02 - logprior: -4.7451e+00
Fitted a model with MAP estimate = -124.4487
expansions: [(0, 2)]
discards: [ 0 23 56]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 126.4933 - loglik: -1.1879e+02 - logprior: -7.5277e+00
Epoch 2/2
13/13 - 1s - loss: 120.8454 - loglik: -1.1825e+02 - logprior: -2.4226e+00
Fitted a model with MAP estimate = -119.5829
expansions: [(17, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 129.3658 - loglik: -1.1966e+02 - logprior: -9.5062e+00
Epoch 2/10
13/13 - 1s - loss: 123.2901 - loglik: -1.1932e+02 - logprior: -3.7759e+00
Epoch 3/10
13/13 - 1s - loss: 119.9701 - loglik: -1.1782e+02 - logprior: -1.9551e+00
Epoch 4/10
13/13 - 1s - loss: 119.8809 - loglik: -1.1809e+02 - logprior: -1.5927e+00
Epoch 5/10
13/13 - 1s - loss: 119.6870 - loglik: -1.1806e+02 - logprior: -1.4335e+00
Epoch 6/10
13/13 - 1s - loss: 119.3445 - loglik: -1.1772e+02 - logprior: -1.4318e+00
Epoch 7/10
13/13 - 1s - loss: 119.5608 - loglik: -1.1793e+02 - logprior: -1.4400e+00
Fitted a model with MAP estimate = -119.1124
Time for alignment: 44.4832
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 193.8721 - loglik: -1.8570e+02 - logprior: -8.1679e+00
Epoch 2/10
13/13 - 1s - loss: 164.0827 - loglik: -1.6179e+02 - logprior: -2.2907e+00
Epoch 3/10
13/13 - 1s - loss: 147.0825 - loglik: -1.4500e+02 - logprior: -2.0125e+00
Epoch 4/10
13/13 - 1s - loss: 140.1798 - loglik: -1.3778e+02 - logprior: -2.1545e+00
Epoch 5/10
13/13 - 1s - loss: 136.8553 - loglik: -1.3452e+02 - logprior: -2.0419e+00
Epoch 6/10
13/13 - 1s - loss: 135.6446 - loglik: -1.3345e+02 - logprior: -1.9629e+00
Epoch 7/10
13/13 - 1s - loss: 135.0254 - loglik: -1.3285e+02 - logprior: -1.9814e+00
Epoch 8/10
13/13 - 1s - loss: 135.0540 - loglik: -1.3291e+02 - logprior: -1.9689e+00
Fitted a model with MAP estimate = -134.5327
expansions: [(12, 1), (13, 1), (17, 2), (18, 1), (21, 5), (34, 1), (41, 1), (44, 1), (45, 1), (46, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 139.0997 - loglik: -1.2927e+02 - logprior: -9.6621e+00
Epoch 2/2
13/13 - 1s - loss: 125.2112 - loglik: -1.2040e+02 - logprior: -4.6358e+00
Fitted a model with MAP estimate = -123.5072
expansions: [(0, 1)]
discards: [ 0 18]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 126.6122 - loglik: -1.1893e+02 - logprior: -7.4737e+00
Epoch 2/2
13/13 - 1s - loss: 120.4737 - loglik: -1.1807e+02 - logprior: -2.2041e+00
Fitted a model with MAP estimate = -119.5946
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 124.5553 - loglik: -1.1694e+02 - logprior: -7.4161e+00
Epoch 2/10
13/13 - 1s - loss: 120.3944 - loglik: -1.1803e+02 - logprior: -2.1754e+00
Epoch 3/10
13/13 - 1s - loss: 119.6805 - loglik: -1.1770e+02 - logprior: -1.7859e+00
Epoch 4/10
13/13 - 1s - loss: 118.8020 - loglik: -1.1703e+02 - logprior: -1.5862e+00
Epoch 5/10
13/13 - 1s - loss: 119.1964 - loglik: -1.1759e+02 - logprior: -1.4140e+00
Fitted a model with MAP estimate = -118.7658
Time for alignment: 41.5987
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 193.6969 - loglik: -1.8552e+02 - logprior: -8.1676e+00
Epoch 2/10
13/13 - 1s - loss: 163.8707 - loglik: -1.6158e+02 - logprior: -2.2916e+00
Epoch 3/10
13/13 - 1s - loss: 146.6465 - loglik: -1.4454e+02 - logprior: -2.0246e+00
Epoch 4/10
13/13 - 1s - loss: 138.8597 - loglik: -1.3642e+02 - logprior: -2.1761e+00
Epoch 5/10
13/13 - 1s - loss: 135.8447 - loglik: -1.3348e+02 - logprior: -2.0647e+00
Epoch 6/10
13/13 - 1s - loss: 134.5262 - loglik: -1.3230e+02 - logprior: -1.9704e+00
Epoch 7/10
13/13 - 1s - loss: 134.0925 - loglik: -1.3190e+02 - logprior: -1.9830e+00
Epoch 8/10
13/13 - 1s - loss: 133.8634 - loglik: -1.3170e+02 - logprior: -1.9735e+00
Epoch 9/10
13/13 - 1s - loss: 133.7471 - loglik: -1.3161e+02 - logprior: -1.9520e+00
Epoch 10/10
13/13 - 1s - loss: 133.5092 - loglik: -1.3138e+02 - logprior: -1.9505e+00
Fitted a model with MAP estimate = -133.3206
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (34, 1), (41, 1), (44, 1), (45, 2), (46, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 139.5809 - loglik: -1.2967e+02 - logprior: -9.7280e+00
Epoch 2/2
13/13 - 1s - loss: 126.2855 - loglik: -1.2136e+02 - logprior: -4.7450e+00
Fitted a model with MAP estimate = -124.4484
expansions: [(0, 2)]
discards: [ 0 23 56]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 126.9702 - loglik: -1.1926e+02 - logprior: -7.5270e+00
Epoch 2/2
13/13 - 1s - loss: 120.0553 - loglik: -1.1744e+02 - logprior: -2.4221e+00
Fitted a model with MAP estimate = -119.5782
expansions: [(17, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 129.7804 - loglik: -1.2008e+02 - logprior: -9.4969e+00
Epoch 2/10
13/13 - 1s - loss: 122.7873 - loglik: -1.1883e+02 - logprior: -3.7612e+00
Epoch 3/10
13/13 - 1s - loss: 120.2592 - loglik: -1.1810e+02 - logprior: -1.9600e+00
Epoch 4/10
13/13 - 1s - loss: 119.5285 - loglik: -1.1775e+02 - logprior: -1.5873e+00
Epoch 5/10
13/13 - 1s - loss: 119.7274 - loglik: -1.1810e+02 - logprior: -1.4396e+00
Fitted a model with MAP estimate = -119.2205
Time for alignment: 40.6802
Computed alignments with likelihoods: ['-119.1124', '-118.7658', '-119.2205']
Best model has likelihood: -118.7658
time for generating output: 0.1457
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HMG_box.projection.fasta
SP score = 0.9575070821529745
Training of 3 independent models on file hpr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2b3c0763a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f2b14a76fd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f343f9f67f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34ecd06070>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33a3a1b910>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f2b14253f40>, <__main__.SimpleDirichletPrior object at 0x7f349e735bb0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f34375060d0>

Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 241.0685 - loglik: -2.2839e+02 - logprior: -1.2671e+01
Epoch 2/10
11/11 - 1s - loss: 208.4784 - loglik: -2.0509e+02 - logprior: -3.3814e+00
Epoch 3/10
11/11 - 1s - loss: 183.9447 - loglik: -1.8161e+02 - logprior: -2.2065e+00
Epoch 4/10
11/11 - 1s - loss: 166.7495 - loglik: -1.6442e+02 - logprior: -2.0354e+00
Epoch 5/10
11/11 - 1s - loss: 160.8183 - loglik: -1.5857e+02 - logprior: -1.8161e+00
Epoch 6/10
11/11 - 1s - loss: 158.0943 - loglik: -1.5603e+02 - logprior: -1.6486e+00
Epoch 7/10
11/11 - 1s - loss: 157.2823 - loglik: -1.5558e+02 - logprior: -1.4198e+00
Epoch 8/10
11/11 - 1s - loss: 156.6050 - loglik: -1.5510e+02 - logprior: -1.2798e+00
Epoch 9/10
11/11 - 1s - loss: 156.0898 - loglik: -1.5467e+02 - logprior: -1.1896e+00
Epoch 10/10
11/11 - 1s - loss: 155.7621 - loglik: -1.5438e+02 - logprior: -1.1587e+00
Fitted a model with MAP estimate = -155.5417
expansions: [(0, 6), (21, 1), (23, 2), (28, 1), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 167.1326 - loglik: -1.5098e+02 - logprior: -1.5934e+01
Epoch 2/2
11/11 - 1s - loss: 147.7767 - loglik: -1.4285e+02 - logprior: -4.7128e+00
Fitted a model with MAP estimate = -144.5040
expansions: []
discards: [ 0 30]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 157.9750 - loglik: -1.4333e+02 - logprior: -1.4405e+01
Epoch 2/2
11/11 - 1s - loss: 148.4594 - loglik: -1.4241e+02 - logprior: -5.7873e+00
Fitted a model with MAP estimate = -145.7151
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 154.0664 - loglik: -1.4122e+02 - logprior: -1.2576e+01
Epoch 2/10
11/11 - 1s - loss: 144.7697 - loglik: -1.4104e+02 - logprior: -3.4584e+00
Epoch 3/10
11/11 - 1s - loss: 142.7567 - loglik: -1.4033e+02 - logprior: -2.1552e+00
Epoch 4/10
11/11 - 1s - loss: 142.1645 - loglik: -1.4046e+02 - logprior: -1.4155e+00
Epoch 5/10
11/11 - 1s - loss: 142.3374 - loglik: -1.4093e+02 - logprior: -1.1219e+00
Fitted a model with MAP estimate = -141.6972
Time for alignment: 40.5989
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 240.9597 - loglik: -2.2828e+02 - logprior: -1.2674e+01
Epoch 2/10
11/11 - 1s - loss: 208.1567 - loglik: -2.0477e+02 - logprior: -3.3785e+00
Epoch 3/10
11/11 - 1s - loss: 182.3890 - loglik: -1.8006e+02 - logprior: -2.1964e+00
Epoch 4/10
11/11 - 1s - loss: 166.8203 - loglik: -1.6442e+02 - logprior: -1.9840e+00
Epoch 5/10
11/11 - 1s - loss: 161.0092 - loglik: -1.5869e+02 - logprior: -1.7563e+00
Epoch 6/10
11/11 - 1s - loss: 158.8429 - loglik: -1.5684e+02 - logprior: -1.6017e+00
Epoch 7/10
11/11 - 1s - loss: 157.4852 - loglik: -1.5581e+02 - logprior: -1.4073e+00
Epoch 8/10
11/11 - 1s - loss: 156.9048 - loglik: -1.5537e+02 - logprior: -1.2983e+00
Epoch 9/10
11/11 - 1s - loss: 156.3651 - loglik: -1.5493e+02 - logprior: -1.2087e+00
Epoch 10/10
11/11 - 1s - loss: 156.1475 - loglik: -1.5475e+02 - logprior: -1.1671e+00
Fitted a model with MAP estimate = -155.7946
expansions: [(0, 6), (22, 1), (27, 1), (28, 1), (32, 1), (45, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 166.3039 - loglik: -1.5015e+02 - logprior: -1.5925e+01
Epoch 2/2
11/11 - 1s - loss: 147.6700 - loglik: -1.4276e+02 - logprior: -4.6595e+00
Fitted a model with MAP estimate = -144.3040
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 157.9020 - loglik: -1.4326e+02 - logprior: -1.4397e+01
Epoch 2/2
11/11 - 1s - loss: 148.3464 - loglik: -1.4227e+02 - logprior: -5.7958e+00
Fitted a model with MAP estimate = -145.6705
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 153.9975 - loglik: -1.4113e+02 - logprior: -1.2588e+01
Epoch 2/10
11/11 - 1s - loss: 144.5279 - loglik: -1.4078e+02 - logprior: -3.4647e+00
Epoch 3/10
11/11 - 1s - loss: 143.0224 - loglik: -1.4058e+02 - logprior: -2.1543e+00
Epoch 4/10
11/11 - 1s - loss: 142.0199 - loglik: -1.4031e+02 - logprior: -1.4248e+00
Epoch 5/10
11/11 - 1s - loss: 142.4628 - loglik: -1.4105e+02 - logprior: -1.1266e+00
Fitted a model with MAP estimate = -141.6638
Time for alignment: 39.8416
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 241.0932 - loglik: -2.2841e+02 - logprior: -1.2676e+01
Epoch 2/10
11/11 - 1s - loss: 208.4836 - loglik: -2.0510e+02 - logprior: -3.3840e+00
Epoch 3/10
11/11 - 1s - loss: 182.7730 - loglik: -1.8045e+02 - logprior: -2.1911e+00
Epoch 4/10
11/11 - 1s - loss: 167.1271 - loglik: -1.6479e+02 - logprior: -1.9520e+00
Epoch 5/10
11/11 - 1s - loss: 160.5568 - loglik: -1.5829e+02 - logprior: -1.7530e+00
Epoch 6/10
11/11 - 1s - loss: 157.6473 - loglik: -1.5557e+02 - logprior: -1.6575e+00
Epoch 7/10
11/11 - 1s - loss: 157.1286 - loglik: -1.5538e+02 - logprior: -1.4512e+00
Epoch 8/10
11/11 - 1s - loss: 156.2337 - loglik: -1.5470e+02 - logprior: -1.3052e+00
Epoch 9/10
11/11 - 1s - loss: 155.9363 - loglik: -1.5449e+02 - logprior: -1.2070e+00
Epoch 10/10
11/11 - 1s - loss: 155.5777 - loglik: -1.5417e+02 - logprior: -1.1696e+00
Fitted a model with MAP estimate = -155.3736
expansions: [(0, 6), (22, 1), (27, 1), (28, 1), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 166.3522 - loglik: -1.5020e+02 - logprior: -1.5928e+01
Epoch 2/2
11/11 - 1s - loss: 147.5724 - loglik: -1.4268e+02 - logprior: -4.6594e+00
Fitted a model with MAP estimate = -144.3219
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 158.0559 - loglik: -1.4341e+02 - logprior: -1.4393e+01
Epoch 2/2
11/11 - 1s - loss: 147.9394 - loglik: -1.4189e+02 - logprior: -5.7896e+00
Fitted a model with MAP estimate = -145.6517
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 153.9617 - loglik: -1.4111e+02 - logprior: -1.2574e+01
Epoch 2/10
11/11 - 1s - loss: 144.9872 - loglik: -1.4125e+02 - logprior: -3.4626e+00
Epoch 3/10
11/11 - 1s - loss: 142.5138 - loglik: -1.4006e+02 - logprior: -2.1627e+00
Epoch 4/10
11/11 - 1s - loss: 142.2838 - loglik: -1.4058e+02 - logprior: -1.4191e+00
Epoch 5/10
11/11 - 1s - loss: 142.0815 - loglik: -1.4066e+02 - logprior: -1.1308e+00
Epoch 6/10
11/11 - 1s - loss: 141.9964 - loglik: -1.4070e+02 - logprior: -1.0038e+00
Epoch 7/10
11/11 - 1s - loss: 142.0062 - loglik: -1.4082e+02 - logprior: -8.9330e-01
Fitted a model with MAP estimate = -141.5077
Time for alignment: 42.1432
Computed alignments with likelihoods: ['-141.6972', '-141.6638', '-141.5077']
Best model has likelihood: -141.5077
time for generating output: 0.1399
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hpr.projection.fasta
SP score = 0.993006993006993
Training of 3 independent models on file tim.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2af8701790>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3335206ee0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3335206370>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f34af38a1f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f333dd36310>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34ecd06070>, <__main__.SimpleDirichletPrior object at 0x7f2b2dc01a60>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f2b3c2cb790>

Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 11s - loss: 527.4599 - loglik: -5.2178e+02 - logprior: -5.6651e+00
Epoch 2/10
24/24 - 8s - loss: 378.4527 - loglik: -3.7585e+02 - logprior: -2.5696e+00
Epoch 3/10
24/24 - 8s - loss: 347.1128 - loglik: -3.4371e+02 - logprior: -3.0577e+00
Epoch 4/10
24/24 - 7s - loss: 343.1265 - loglik: -3.3979e+02 - logprior: -2.8662e+00
Epoch 5/10
24/24 - 7s - loss: 341.8575 - loglik: -3.3855e+02 - logprior: -2.8818e+00
Epoch 6/10
24/24 - 8s - loss: 341.4269 - loglik: -3.3812e+02 - logprior: -2.8929e+00
Epoch 7/10
24/24 - 7s - loss: 339.9348 - loglik: -3.3661e+02 - logprior: -2.9113e+00
Epoch 8/10
24/24 - 7s - loss: 339.9309 - loglik: -3.3660e+02 - logprior: -2.9217e+00
Epoch 9/10
24/24 - 7s - loss: 339.7181 - loglik: -3.3640e+02 - logprior: -2.9179e+00
Epoch 10/10
24/24 - 8s - loss: 339.8424 - loglik: -3.3652e+02 - logprior: -2.9236e+00
Fitted a model with MAP estimate = -339.3053
expansions: [(11, 1), (12, 3), (16, 1), (17, 2), (18, 1), (20, 2), (34, 1), (36, 1), (37, 1), (38, 1), (39, 2), (48, 1), (49, 1), (62, 1), (63, 1), (65, 1), (76, 1), (82, 1), (87, 1), (90, 1), (93, 1), (110, 1), (113, 1), (118, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 3), (155, 1), (156, 1), (158, 1), (172, 1), (173, 3), (174, 1), (175, 1), (187, 1), (189, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 240 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 12s - loss: 328.4997 - loglik: -3.2008e+02 - logprior: -8.0234e+00
Epoch 2/2
24/24 - 9s - loss: 310.2089 - loglik: -3.0617e+02 - logprior: -3.5457e+00
Fitted a model with MAP estimate = -307.8522
expansions: [(0, 2), (12, 1), (13, 1), (190, 1)]
discards: [ 0 21]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 243 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 16s - loss: 307.7913 - loglik: -3.0207e+02 - logprior: -5.2099e+00
Epoch 2/2
24/24 - 9s - loss: 302.6284 - loglik: -3.0130e+02 - logprior: -8.2172e-01
Fitted a model with MAP estimate = -301.0487
expansions: [(152, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 244 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 13s - loss: 305.5984 - loglik: -3.0019e+02 - logprior: -4.8829e+00
Epoch 2/10
24/24 - 9s - loss: 302.6470 - loglik: -3.0165e+02 - logprior: -4.6451e-01
Epoch 3/10
24/24 - 9s - loss: 299.6749 - loglik: -2.9921e+02 - logprior: 0.0690
Epoch 4/10
24/24 - 9s - loss: 300.3875 - loglik: -3.0017e+02 - logprior: 0.3079
Fitted a model with MAP estimate = -299.5172
Time for alignment: 197.5120
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 11s - loss: 527.8735 - loglik: -5.2224e+02 - logprior: -5.6238e+00
Epoch 2/10
24/24 - 7s - loss: 381.1156 - loglik: -3.7853e+02 - logprior: -2.5131e+00
Epoch 3/10
24/24 - 7s - loss: 347.3573 - loglik: -3.4401e+02 - logprior: -2.9353e+00
Epoch 4/10
24/24 - 8s - loss: 344.8056 - loglik: -3.4155e+02 - logprior: -2.7891e+00
Epoch 5/10
24/24 - 7s - loss: 343.3312 - loglik: -3.4016e+02 - logprior: -2.7535e+00
Epoch 6/10
24/24 - 8s - loss: 340.8770 - loglik: -3.3772e+02 - logprior: -2.7541e+00
Epoch 7/10
24/24 - 7s - loss: 342.8458 - loglik: -3.3965e+02 - logprior: -2.7927e+00
Fitted a model with MAP estimate = -340.8330
expansions: [(11, 1), (12, 3), (16, 1), (17, 2), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 2), (49, 2), (63, 1), (65, 1), (76, 1), (82, 1), (88, 1), (90, 1), (93, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 3), (122, 1), (150, 1), (153, 1), (154, 4), (158, 1), (172, 1), (173, 2), (174, 2), (175, 1), (186, 1), (187, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 239 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 12s - loss: 331.4510 - loglik: -3.2294e+02 - logprior: -8.1194e+00
Epoch 2/2
24/24 - 9s - loss: 311.5323 - loglik: -3.0731e+02 - logprior: -3.7477e+00
Fitted a model with MAP estimate = -309.8043
expansions: [(0, 2), (12, 1), (13, 1), (190, 1), (192, 1)]
discards: [  0  22  61 146]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 13s - loss: 310.4329 - loglik: -3.0470e+02 - logprior: -5.2401e+00
Epoch 2/2
24/24 - 9s - loss: 303.3151 - loglik: -3.0199e+02 - logprior: -8.1907e-01
Fitted a model with MAP estimate = -302.2652
expansions: [(152, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 12s - loss: 306.4945 - loglik: -3.0104e+02 - logprior: -4.9372e+00
Epoch 2/10
24/24 - 9s - loss: 303.0537 - loglik: -3.0205e+02 - logprior: -4.8163e-01
Epoch 3/10
24/24 - 9s - loss: 301.3035 - loglik: -3.0078e+02 - logprior: -3.8500e-03
Epoch 4/10
24/24 - 9s - loss: 301.6905 - loglik: -3.0144e+02 - logprior: 0.2577
Fitted a model with MAP estimate = -300.4292
Time for alignment: 173.1588
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 10s - loss: 525.5368 - loglik: -5.1989e+02 - logprior: -5.6279e+00
Epoch 2/10
24/24 - 8s - loss: 383.6834 - loglik: -3.8118e+02 - logprior: -2.4653e+00
Epoch 3/10
24/24 - 8s - loss: 351.5633 - loglik: -3.4830e+02 - logprior: -2.9196e+00
Epoch 4/10
24/24 - 8s - loss: 344.0387 - loglik: -3.4067e+02 - logprior: -2.8619e+00
Epoch 5/10
24/24 - 7s - loss: 345.3894 - loglik: -3.4207e+02 - logprior: -2.8617e+00
Fitted a model with MAP estimate = -342.8425
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (16, 2), (17, 2), (18, 2), (34, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 2), (49, 2), (63, 1), (65, 1), (78, 1), (85, 1), (88, 1), (90, 1), (111, 1), (113, 1), (114, 1), (118, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (155, 1), (158, 1), (171, 1), (173, 2), (174, 2), (175, 1), (186, 1), (187, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 243 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 15s - loss: 331.4375 - loglik: -3.2287e+02 - logprior: -8.1283e+00
Epoch 2/2
24/24 - 9s - loss: 311.4912 - loglik: -3.0734e+02 - logprior: -3.6546e+00
Fitted a model with MAP estimate = -307.7929
expansions: [(0, 2), (193, 1), (195, 1)]
discards: [  0  12  24  26  65 220]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 13s - loss: 309.1659 - loglik: -3.0339e+02 - logprior: -5.2617e+00
Epoch 2/2
24/24 - 9s - loss: 302.7079 - loglik: -3.0131e+02 - logprior: -8.6974e-01
Fitted a model with MAP estimate = -301.6205
expansions: [(150, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 12s - loss: 306.8570 - loglik: -3.0135e+02 - logprior: -4.9672e+00
Epoch 2/10
24/24 - 9s - loss: 300.8897 - loglik: -2.9982e+02 - logprior: -5.3460e-01
Epoch 3/10
24/24 - 9s - loss: 301.6989 - loglik: -3.0114e+02 - logprior: -3.0158e-02
Fitted a model with MAP estimate = -300.0764
Time for alignment: 149.9748
Computed alignments with likelihoods: ['-299.5172', '-300.4292', '-300.0764']
Best model has likelihood: -299.5172
time for generating output: 0.2714
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tim.projection.fasta
SP score = 0.9732223629455401
Training of 3 independent models on file tgfb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f34af657730>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f2b15885b80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2b24c22d30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f2b146bafd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2b146baee0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f2b146baa60>, <__main__.SimpleDirichletPrior object at 0x7f2b1c365ca0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f342cdd2ee0>

Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 304.1428 - loglik: -2.7671e+02 - logprior: -2.7414e+01
Epoch 2/10
10/10 - 1s - loss: 247.5915 - loglik: -2.4035e+02 - logprior: -7.2269e+00
Epoch 3/10
10/10 - 1s - loss: 210.4395 - loglik: -2.0646e+02 - logprior: -3.9792e+00
Epoch 4/10
10/10 - 1s - loss: 188.2383 - loglik: -1.8522e+02 - logprior: -3.0127e+00
Epoch 5/10
10/10 - 1s - loss: 179.8596 - loglik: -1.7711e+02 - logprior: -2.7135e+00
Epoch 6/10
10/10 - 1s - loss: 176.4309 - loglik: -1.7365e+02 - logprior: -2.6458e+00
Epoch 7/10
10/10 - 1s - loss: 175.3341 - loglik: -1.7261e+02 - logprior: -2.3998e+00
Epoch 8/10
10/10 - 1s - loss: 174.0482 - loglik: -1.7148e+02 - logprior: -2.2030e+00
Epoch 9/10
10/10 - 1s - loss: 174.1902 - loglik: -1.7171e+02 - logprior: -2.1605e+00
Fitted a model with MAP estimate = -173.5407
expansions: [(0, 3), (11, 1), (12, 1), (24, 1), (35, 4), (36, 2), (50, 2), (60, 1), (69, 1), (70, 1), (72, 2), (73, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 200.6798 - loglik: -1.6580e+02 - logprior: -3.4586e+01
Epoch 2/2
10/10 - 1s - loss: 167.5224 - loglik: -1.5654e+02 - logprior: -1.0638e+01
Fitted a model with MAP estimate = -160.4960
expansions: []
discards: [ 0 43 89]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 186.2594 - loglik: -1.5435e+02 - logprior: -3.1526e+01
Epoch 2/2
10/10 - 1s - loss: 167.0896 - loglik: -1.5376e+02 - logprior: -1.2913e+01
Fitted a model with MAP estimate = -163.1183
expansions: [(3, 1), (61, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 182.4694 - loglik: -1.5239e+02 - logprior: -2.9651e+01
Epoch 2/10
10/10 - 1s - loss: 160.5892 - loglik: -1.5124e+02 - logprior: -8.9052e+00
Epoch 3/10
10/10 - 1s - loss: 154.7595 - loglik: -1.5058e+02 - logprior: -3.6984e+00
Epoch 4/10
10/10 - 1s - loss: 151.9597 - loglik: -1.4951e+02 - logprior: -1.9609e+00
Epoch 5/10
10/10 - 1s - loss: 151.9470 - loglik: -1.5024e+02 - logprior: -1.1999e+00
Epoch 6/10
10/10 - 1s - loss: 151.4190 - loglik: -1.5006e+02 - logprior: -8.4963e-01
Epoch 7/10
10/10 - 1s - loss: 151.1696 - loglik: -1.5000e+02 - logprior: -6.4951e-01
Epoch 8/10
10/10 - 1s - loss: 151.2130 - loglik: -1.5022e+02 - logprior: -4.6731e-01
Fitted a model with MAP estimate = -150.4150
Time for alignment: 44.2049
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 304.1711 - loglik: -2.7674e+02 - logprior: -2.7413e+01
Epoch 2/10
10/10 - 1s - loss: 247.6174 - loglik: -2.4037e+02 - logprior: -7.2265e+00
Epoch 3/10
10/10 - 1s - loss: 209.1072 - loglik: -2.0517e+02 - logprior: -3.9314e+00
Epoch 4/10
10/10 - 1s - loss: 187.8949 - loglik: -1.8487e+02 - logprior: -2.9931e+00
Epoch 5/10
10/10 - 1s - loss: 178.7903 - loglik: -1.7577e+02 - logprior: -2.8006e+00
Epoch 6/10
10/10 - 1s - loss: 175.8478 - loglik: -1.7275e+02 - logprior: -2.7123e+00
Epoch 7/10
10/10 - 1s - loss: 174.1610 - loglik: -1.7133e+02 - logprior: -2.4729e+00
Epoch 8/10
10/10 - 1s - loss: 174.1216 - loglik: -1.7151e+02 - logprior: -2.3136e+00
Epoch 9/10
10/10 - 1s - loss: 173.3340 - loglik: -1.7080e+02 - logprior: -2.2430e+00
Epoch 10/10
10/10 - 1s - loss: 173.6619 - loglik: -1.7116e+02 - logprior: -2.2015e+00
Fitted a model with MAP estimate = -172.8581
expansions: [(0, 3), (11, 1), (12, 1), (24, 1), (35, 4), (36, 2), (50, 3), (51, 1), (69, 1), (70, 1), (72, 2), (73, 1), (74, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 200.4693 - loglik: -1.6536e+02 - logprior: -3.4825e+01
Epoch 2/2
10/10 - 1s - loss: 167.7471 - loglik: -1.5669e+02 - logprior: -1.0747e+01
Fitted a model with MAP estimate = -160.7188
expansions: []
discards: [ 0 44 90]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 186.5867 - loglik: -1.5471e+02 - logprior: -3.1519e+01
Epoch 2/2
10/10 - 1s - loss: 165.9568 - loglik: -1.5265e+02 - logprior: -1.2881e+01
Fitted a model with MAP estimate = -161.9736
expansions: [(3, 1), (62, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 181.0126 - loglik: -1.5092e+02 - logprior: -2.9646e+01
Epoch 2/10
10/10 - 1s - loss: 159.6291 - loglik: -1.5025e+02 - logprior: -8.9108e+00
Epoch 3/10
10/10 - 1s - loss: 153.4452 - loglik: -1.4929e+02 - logprior: -3.6791e+00
Epoch 4/10
10/10 - 1s - loss: 151.5651 - loglik: -1.4915e+02 - logprior: -1.9253e+00
Epoch 5/10
10/10 - 1s - loss: 150.9466 - loglik: -1.4928e+02 - logprior: -1.1621e+00
Epoch 6/10
10/10 - 1s - loss: 150.8306 - loglik: -1.4951e+02 - logprior: -8.1135e-01
Epoch 7/10
10/10 - 1s - loss: 150.5210 - loglik: -1.4940e+02 - logprior: -6.0751e-01
Epoch 8/10
10/10 - 1s - loss: 149.9916 - loglik: -1.4905e+02 - logprior: -4.2771e-01
Epoch 9/10
10/10 - 1s - loss: 150.2301 - loglik: -1.4945e+02 - logprior: -2.5518e-01
Fitted a model with MAP estimate = -149.5171
Time for alignment: 45.6224
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 304.2193 - loglik: -2.7679e+02 - logprior: -2.7413e+01
Epoch 2/10
10/10 - 1s - loss: 247.1422 - loglik: -2.3991e+02 - logprior: -7.2192e+00
Epoch 3/10
10/10 - 1s - loss: 210.1741 - loglik: -2.0626e+02 - logprior: -3.9130e+00
Epoch 4/10
10/10 - 1s - loss: 191.0063 - loglik: -1.8808e+02 - logprior: -2.9211e+00
Epoch 5/10
10/10 - 1s - loss: 180.4032 - loglik: -1.7759e+02 - logprior: -2.7161e+00
Epoch 6/10
10/10 - 1s - loss: 176.6557 - loglik: -1.7373e+02 - logprior: -2.6244e+00
Epoch 7/10
10/10 - 1s - loss: 174.9919 - loglik: -1.7217e+02 - logprior: -2.4120e+00
Epoch 8/10
10/10 - 1s - loss: 174.0497 - loglik: -1.7139e+02 - logprior: -2.2675e+00
Epoch 9/10
10/10 - 1s - loss: 173.5449 - loglik: -1.7097e+02 - logprior: -2.2240e+00
Epoch 10/10
10/10 - 1s - loss: 173.0299 - loglik: -1.7045e+02 - logprior: -2.2245e+00
Fitted a model with MAP estimate = -172.4265
expansions: [(0, 3), (5, 1), (10, 1), (34, 1), (35, 3), (36, 1), (37, 1), (48, 3), (49, 2), (60, 1), (69, 1), (70, 1), (72, 2), (73, 1), (74, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 198.3484 - loglik: -1.6328e+02 - logprior: -3.4744e+01
Epoch 2/2
10/10 - 1s - loss: 165.1265 - loglik: -1.5410e+02 - logprior: -1.0652e+01
Fitted a model with MAP estimate = -158.0883
expansions: []
discards: [ 0 91]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 184.1062 - loglik: -1.5228e+02 - logprior: -3.1425e+01
Epoch 2/2
10/10 - 1s - loss: 163.9843 - loglik: -1.5077e+02 - logprior: -1.2800e+01
Fitted a model with MAP estimate = -159.8948
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.2712 - loglik: -1.4931e+02 - logprior: -2.9532e+01
Epoch 2/10
10/10 - 1s - loss: 157.4756 - loglik: -1.4828e+02 - logprior: -8.7512e+00
Epoch 3/10
10/10 - 1s - loss: 152.5793 - loglik: -1.4859e+02 - logprior: -3.5225e+00
Epoch 4/10
10/10 - 1s - loss: 150.2714 - loglik: -1.4798e+02 - logprior: -1.8191e+00
Epoch 5/10
10/10 - 1s - loss: 149.5554 - loglik: -1.4801e+02 - logprior: -1.0644e+00
Epoch 6/10
10/10 - 1s - loss: 149.1577 - loglik: -1.4795e+02 - logprior: -7.1305e-01
Epoch 7/10
10/10 - 1s - loss: 149.4308 - loglik: -1.4841e+02 - logprior: -5.2461e-01
Fitted a model with MAP estimate = -148.3669
Time for alignment: 44.6206
Computed alignments with likelihoods: ['-150.4150', '-149.5171', '-148.3669']
Best model has likelihood: -148.3669
time for generating output: 0.1620
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tgfb.projection.fasta
SP score = 0.8464135021097047
Training of 3 independent models on file HLH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f332cc4db50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f330a8075b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f330a807e80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f2b2d42ce50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2b146bd040>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f34a6933550>, <__main__.SimpleDirichletPrior object at 0x7f2b047ddbe0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3323e96700>

Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 151.0073 - loglik: -1.4587e+02 - logprior: -5.1341e+00
Epoch 2/10
16/16 - 2s - loss: 125.9853 - loglik: -1.2432e+02 - logprior: -1.6032e+00
Epoch 3/10
16/16 - 2s - loss: 114.7182 - loglik: -1.1261e+02 - logprior: -1.7309e+00
Epoch 4/10
16/16 - 2s - loss: 110.3970 - loglik: -1.0833e+02 - logprior: -1.8082e+00
Epoch 5/10
16/16 - 2s - loss: 108.7031 - loglik: -1.0662e+02 - logprior: -1.7753e+00
Epoch 6/10
16/16 - 2s - loss: 108.3431 - loglik: -1.0629e+02 - logprior: -1.7492e+00
Epoch 7/10
16/16 - 2s - loss: 107.4973 - loglik: -1.0550e+02 - logprior: -1.7136e+00
Epoch 8/10
16/16 - 2s - loss: 107.7055 - loglik: -1.0570e+02 - logprior: -1.7031e+00
Fitted a model with MAP estimate = -107.1299
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (16, 1), (23, 6), (25, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 111.7363 - loglik: -1.0505e+02 - logprior: -6.3797e+00
Epoch 2/2
16/16 - 2s - loss: 103.3803 - loglik: -9.9860e+01 - logprior: -3.1807e+00
Fitted a model with MAP estimate = -101.3413
expansions: [(0, 1)]
discards: [ 0 30 36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 103.5527 - loglik: -9.8453e+01 - logprior: -4.7383e+00
Epoch 2/2
16/16 - 2s - loss: 99.8485 - loglik: -9.7733e+01 - logprior: -1.7585e+00
Fitted a model with MAP estimate = -99.1139
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 105.6647 - loglik: -9.8937e+01 - logprior: -6.3426e+00
Epoch 2/10
16/16 - 2s - loss: 101.5827 - loglik: -9.8296e+01 - logprior: -2.9075e+00
Epoch 3/10
16/16 - 2s - loss: 99.7537 - loglik: -9.7693e+01 - logprior: -1.6872e+00
Epoch 4/10
16/16 - 2s - loss: 99.9888 - loglik: -9.8157e+01 - logprior: -1.4578e+00
Fitted a model with MAP estimate = -98.8351
Time for alignment: 54.6557
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 151.1617 - loglik: -1.4603e+02 - logprior: -5.1322e+00
Epoch 2/10
16/16 - 2s - loss: 126.8677 - loglik: -1.2517e+02 - logprior: -1.6313e+00
Epoch 3/10
16/16 - 2s - loss: 114.8887 - loglik: -1.1269e+02 - logprior: -1.8361e+00
Epoch 4/10
16/16 - 2s - loss: 111.2595 - loglik: -1.0909e+02 - logprior: -1.8953e+00
Epoch 5/10
16/16 - 2s - loss: 109.7560 - loglik: -1.0764e+02 - logprior: -1.8451e+00
Epoch 6/10
16/16 - 2s - loss: 108.6126 - loglik: -1.0655e+02 - logprior: -1.8014e+00
Epoch 7/10
16/16 - 2s - loss: 109.0947 - loglik: -1.0707e+02 - logprior: -1.7548e+00
Fitted a model with MAP estimate = -108.1340
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (16, 1), (24, 2), (25, 3), (26, 2), (29, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 113.4464 - loglik: -1.0679e+02 - logprior: -6.3812e+00
Epoch 2/2
16/16 - 2s - loss: 104.2508 - loglik: -1.0078e+02 - logprior: -3.1706e+00
Fitted a model with MAP estimate = -102.9376
expansions: [(0, 1)]
discards: [ 0 33 36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 104.8070 - loglik: -9.9770e+01 - logprior: -4.7271e+00
Epoch 2/2
16/16 - 2s - loss: 101.7057 - loglik: -9.9627e+01 - logprior: -1.7480e+00
Fitted a model with MAP estimate = -100.5002
expansions: [(3, 1)]
discards: [ 0 30]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 107.0827 - loglik: -1.0039e+02 - logprior: -6.3453e+00
Epoch 2/10
16/16 - 2s - loss: 103.1706 - loglik: -9.9923e+01 - logprior: -2.8984e+00
Epoch 3/10
16/16 - 2s - loss: 101.3988 - loglik: -9.9353e+01 - logprior: -1.6864e+00
Epoch 4/10
16/16 - 2s - loss: 100.6963 - loglik: -9.8881e+01 - logprior: -1.4572e+00
Epoch 5/10
16/16 - 2s - loss: 100.5749 - loglik: -9.8768e+01 - logprior: -1.4427e+00
Epoch 6/10
16/16 - 2s - loss: 100.1001 - loglik: -9.8315e+01 - logprior: -1.4161e+00
Epoch 7/10
16/16 - 2s - loss: 100.0789 - loglik: -9.8324e+01 - logprior: -1.3865e+00
Epoch 8/10
16/16 - 2s - loss: 100.4190 - loglik: -9.8688e+01 - logprior: -1.3612e+00
Fitted a model with MAP estimate = -99.6650
Time for alignment: 57.0942
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 151.1823 - loglik: -1.4605e+02 - logprior: -5.1314e+00
Epoch 2/10
16/16 - 2s - loss: 126.1977 - loglik: -1.2456e+02 - logprior: -1.5797e+00
Epoch 3/10
16/16 - 2s - loss: 115.9976 - loglik: -1.1410e+02 - logprior: -1.6366e+00
Epoch 4/10
16/16 - 2s - loss: 112.2399 - loglik: -1.1023e+02 - logprior: -1.6465e+00
Epoch 5/10
16/16 - 2s - loss: 111.1854 - loglik: -1.0928e+02 - logprior: -1.6096e+00
Epoch 6/10
16/16 - 2s - loss: 110.8820 - loglik: -1.0900e+02 - logprior: -1.5864e+00
Epoch 7/10
16/16 - 2s - loss: 110.7136 - loglik: -1.0888e+02 - logprior: -1.5500e+00
Epoch 8/10
16/16 - 2s - loss: 109.2060 - loglik: -1.0738e+02 - logprior: -1.5463e+00
Epoch 9/10
16/16 - 2s - loss: 109.5798 - loglik: -1.0775e+02 - logprior: -1.5396e+00
Fitted a model with MAP estimate = -108.9795
expansions: [(3, 1), (6, 1), (12, 3), (17, 1), (23, 7)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 111.5047 - loglik: -1.0484e+02 - logprior: -6.3925e+00
Epoch 2/2
16/16 - 2s - loss: 103.0858 - loglik: -9.9585e+01 - logprior: -3.2042e+00
Fitted a model with MAP estimate = -101.6501
expansions: [(0, 1)]
discards: [ 0 30]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 103.3060 - loglik: -9.8263e+01 - logprior: -4.7409e+00
Epoch 2/2
16/16 - 2s - loss: 100.4037 - loglik: -9.8296e+01 - logprior: -1.7715e+00
Fitted a model with MAP estimate = -99.2473
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 105.6461 - loglik: -9.8985e+01 - logprior: -6.3126e+00
Epoch 2/10
16/16 - 2s - loss: 101.5232 - loglik: -9.8376e+01 - logprior: -2.7807e+00
Epoch 3/10
16/16 - 2s - loss: 100.1159 - loglik: -9.8130e+01 - logprior: -1.6271e+00
Epoch 4/10
16/16 - 2s - loss: 99.3055 - loglik: -9.7462e+01 - logprior: -1.4646e+00
Epoch 5/10
16/16 - 2s - loss: 99.0245 - loglik: -9.7206e+01 - logprior: -1.4416e+00
Epoch 6/10
16/16 - 2s - loss: 99.0890 - loglik: -9.7299e+01 - logprior: -1.4081e+00
Fitted a model with MAP estimate = -98.4854
Time for alignment: 56.8548
Computed alignments with likelihoods: ['-98.8351', '-99.6650', '-98.4854']
Best model has likelihood: -98.4854
time for generating output: 0.1369
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HLH.projection.fasta
SP score = 0.9369951534733441
Training of 3 independent models on file blmb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f33a33f73d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f335fb81580>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34361efe20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f343733b8e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f343733bcd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f2b2548df10>, <__main__.SimpleDirichletPrior object at 0x7f2af85d8670>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3323e96700>

Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 587.7423 - loglik: -5.8547e+02 - logprior: -1.7722e+00
Epoch 2/10
39/39 - 13s - loss: 538.0564 - loglik: -5.3522e+02 - logprior: -1.0942e+00
Epoch 3/10
39/39 - 12s - loss: 526.7869 - loglik: -5.2315e+02 - logprior: -1.1724e+00
Epoch 4/10
39/39 - 12s - loss: 520.0130 - loglik: -5.1638e+02 - logprior: -1.2561e+00
Epoch 5/10
39/39 - 12s - loss: 516.8954 - loglik: -5.1342e+02 - logprior: -1.2783e+00
Epoch 6/10
39/39 - 12s - loss: 514.6747 - loglik: -5.1139e+02 - logprior: -1.3033e+00
Epoch 7/10
39/39 - 12s - loss: 513.5116 - loglik: -5.1046e+02 - logprior: -1.3487e+00
Epoch 8/10
39/39 - 13s - loss: 512.3813 - loglik: -5.0948e+02 - logprior: -1.3724e+00
Epoch 9/10
39/39 - 13s - loss: 511.6895 - loglik: -5.0893e+02 - logprior: -1.3758e+00
Epoch 10/10
39/39 - 13s - loss: 510.9052 - loglik: -5.0824e+02 - logprior: -1.3764e+00
Fitted a model with MAP estimate = -492.0440
expansions: [(6, 1), (7, 1), (11, 1), (13, 1), (21, 2), (25, 1), (28, 1), (31, 1), (54, 1), (55, 1), (57, 1), (70, 1), (78, 1), (82, 1), (94, 2), (112, 2), (113, 1), (114, 1), (123, 6), (124, 2), (125, 3), (138, 8)]
discards: [2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 194 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 502.6029 - loglik: -4.9911e+02 - logprior: -2.1977e+00
Epoch 2/2
39/39 - 15s - loss: 494.2220 - loglik: -4.9139e+02 - logprior: -1.1702e+00
Fitted a model with MAP estimate = -487.9224
expansions: [(92, 11)]
discards: [153 167 168 169 170]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 200 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 494.8270 - loglik: -4.9072e+02 - logprior: -2.1607e+00
Epoch 2/2
39/39 - 15s - loss: 490.4278 - loglik: -4.8721e+02 - logprior: -1.1167e+00
Fitted a model with MAP estimate = -490.5892
expansions: []
discards: [164]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 199 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 25s - loss: 475.6874 - loglik: -4.7092e+02 - logprior: -1.4207e+00
Epoch 2/10
51/51 - 20s - loss: 461.0027 - loglik: -4.5624e+02 - logprior: -1.0114e+00
Epoch 3/10
51/51 - 19s - loss: 456.5920 - loglik: -4.5209e+02 - logprior: -8.9014e-01
Epoch 4/10
51/51 - 19s - loss: 452.0957 - loglik: -4.4794e+02 - logprior: -7.5005e-01
Epoch 5/10
51/51 - 19s - loss: 448.0818 - loglik: -4.4449e+02 - logprior: -6.7098e-01
Epoch 6/10
51/51 - 19s - loss: 445.0396 - loglik: -4.4163e+02 - logprior: -5.9432e-01
Epoch 7/10
51/51 - 19s - loss: 444.8669 - loglik: -4.4120e+02 - logprior: -5.4076e-01
Epoch 8/10
51/51 - 19s - loss: 443.4397 - loglik: -4.3970e+02 - logprior: -4.7129e-01
Epoch 9/10
51/51 - 19s - loss: 440.3824 - loglik: -4.3734e+02 - logprior: -4.0597e-01
Epoch 10/10
51/51 - 19s - loss: 440.9649 - loglik: -4.3827e+02 - logprior: -3.2427e-01
Fitted a model with MAP estimate = -436.5776
Time for alignment: 486.6377
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 586.6626 - loglik: -5.8440e+02 - logprior: -1.7730e+00
Epoch 2/10
39/39 - 13s - loss: 536.9487 - loglik: -5.3418e+02 - logprior: -1.1100e+00
Epoch 3/10
39/39 - 13s - loss: 526.6594 - loglik: -5.2309e+02 - logprior: -1.1741e+00
Epoch 4/10
39/39 - 12s - loss: 521.6022 - loglik: -5.1823e+02 - logprior: -1.2256e+00
Epoch 5/10
39/39 - 13s - loss: 519.2147 - loglik: -5.1615e+02 - logprior: -1.2430e+00
Epoch 6/10
39/39 - 13s - loss: 517.4591 - loglik: -5.1455e+02 - logprior: -1.2722e+00
Epoch 7/10
39/39 - 12s - loss: 516.0850 - loglik: -5.1328e+02 - logprior: -1.2953e+00
Epoch 8/10
39/39 - 13s - loss: 515.4019 - loglik: -5.1273e+02 - logprior: -1.3117e+00
Epoch 9/10
39/39 - 13s - loss: 514.7551 - loglik: -5.1215e+02 - logprior: -1.3196e+00
Epoch 10/10
39/39 - 13s - loss: 514.0365 - loglik: -5.1144e+02 - logprior: -1.3381e+00
Fitted a model with MAP estimate = -491.4511
expansions: [(6, 1), (7, 1), (11, 1), (13, 1), (21, 6), (28, 1), (30, 3), (54, 1), (55, 1), (70, 2), (74, 2), (75, 17), (106, 1), (121, 8), (122, 1), (125, 2), (127, 1)]
discards: [  2 135 136 137]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 201 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 506.0612 - loglik: -5.0244e+02 - logprior: -2.2077e+00
Epoch 2/2
39/39 - 15s - loss: 494.5573 - loglik: -4.9146e+02 - logprior: -1.2168e+00
Fitted a model with MAP estimate = -487.4700
expansions: [(164, 3), (184, 1)]
discards: [ 26  42 126 127 128 171]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 199 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 495.0981 - loglik: -4.9083e+02 - logprior: -2.1881e+00
Epoch 2/2
39/39 - 15s - loss: 491.9884 - loglik: -4.8890e+02 - logprior: -1.0518e+00
Fitted a model with MAP estimate = -491.6154
expansions: [(26, 2), (124, 1)]
discards: [127]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 201 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 23s - loss: 477.1848 - loglik: -4.7259e+02 - logprior: -1.4676e+00
Epoch 2/10
51/51 - 19s - loss: 462.1698 - loglik: -4.5755e+02 - logprior: -1.0064e+00
Epoch 3/10
51/51 - 20s - loss: 456.1375 - loglik: -4.5171e+02 - logprior: -8.3539e-01
Epoch 4/10
51/51 - 19s - loss: 450.2876 - loglik: -4.4641e+02 - logprior: -7.0118e-01
Epoch 5/10
51/51 - 20s - loss: 447.8455 - loglik: -4.4439e+02 - logprior: -6.0749e-01
Epoch 6/10
51/51 - 19s - loss: 445.3941 - loglik: -4.4224e+02 - logprior: -5.2476e-01
Epoch 7/10
51/51 - 20s - loss: 444.3418 - loglik: -4.4123e+02 - logprior: -4.4674e-01
Epoch 8/10
51/51 - 20s - loss: 442.6287 - loglik: -4.3948e+02 - logprior: -3.7791e-01
Epoch 9/10
51/51 - 19s - loss: 441.0537 - loglik: -4.3814e+02 - logprior: -3.2122e-01
Epoch 10/10
51/51 - 19s - loss: 439.8663 - loglik: -4.3718e+02 - logprior: -2.5346e-01
Fitted a model with MAP estimate = -436.1494
Time for alignment: 489.5648
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 587.1236 - loglik: -5.8483e+02 - logprior: -1.7835e+00
Epoch 2/10
39/39 - 12s - loss: 534.4058 - loglik: -5.3147e+02 - logprior: -1.1919e+00
Epoch 3/10
39/39 - 12s - loss: 524.3698 - loglik: -5.2072e+02 - logprior: -1.1935e+00
Epoch 4/10
39/39 - 12s - loss: 520.5801 - loglik: -5.1703e+02 - logprior: -1.1809e+00
Epoch 5/10
39/39 - 12s - loss: 517.5461 - loglik: -5.1412e+02 - logprior: -1.2011e+00
Epoch 6/10
39/39 - 12s - loss: 514.3383 - loglik: -5.1103e+02 - logprior: -1.2469e+00
Epoch 7/10
39/39 - 12s - loss: 512.7961 - loglik: -5.0973e+02 - logprior: -1.2797e+00
Epoch 8/10
39/39 - 12s - loss: 511.7313 - loglik: -5.0888e+02 - logprior: -1.2926e+00
Epoch 9/10
39/39 - 12s - loss: 511.1110 - loglik: -5.0845e+02 - logprior: -1.2959e+00
Epoch 10/10
39/39 - 12s - loss: 510.4105 - loglik: -5.0784e+02 - logprior: -1.3001e+00
Fitted a model with MAP estimate = -490.9960
expansions: [(6, 1), (7, 1), (11, 1), (13, 1), (21, 3), (25, 2), (56, 1), (71, 2), (77, 8), (78, 1), (81, 1), (113, 2), (122, 5), (123, 1), (124, 2), (126, 1), (146, 2)]
discards: [2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 504.0243 - loglik: -5.0059e+02 - logprior: -2.1870e+00
Epoch 2/2
39/39 - 15s - loss: 496.6570 - loglik: -4.9386e+02 - logprior: -1.1491e+00
Fitted a model with MAP estimate = -489.2936
expansions: [(25, 1), (92, 9), (147, 1), (148, 3), (179, 1)]
discards: [135]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 203 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 494.5589 - loglik: -4.9043e+02 - logprior: -2.1476e+00
Epoch 2/2
39/39 - 15s - loss: 490.0573 - loglik: -4.8688e+02 - logprior: -1.1089e+00
Fitted a model with MAP estimate = -489.0171
expansions: [(24, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 208 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 23s - loss: 474.4637 - loglik: -4.6975e+02 - logprior: -1.4585e+00
Epoch 2/10
51/51 - 20s - loss: 458.9834 - loglik: -4.5412e+02 - logprior: -9.8000e-01
Epoch 3/10
51/51 - 21s - loss: 454.7613 - loglik: -4.5005e+02 - logprior: -8.2543e-01
Epoch 4/10
51/51 - 21s - loss: 449.2378 - loglik: -4.4514e+02 - logprior: -6.8474e-01
Epoch 5/10
51/51 - 20s - loss: 445.8782 - loglik: -4.4237e+02 - logprior: -6.0055e-01
Epoch 6/10
51/51 - 20s - loss: 444.6480 - loglik: -4.4133e+02 - logprior: -5.2272e-01
Epoch 7/10
51/51 - 20s - loss: 443.3479 - loglik: -4.3998e+02 - logprior: -4.6354e-01
Epoch 8/10
51/51 - 20s - loss: 441.5282 - loglik: -4.3805e+02 - logprior: -4.0793e-01
Epoch 9/10
51/51 - 20s - loss: 439.2014 - loglik: -4.3606e+02 - logprior: -3.3362e-01
Epoch 10/10
51/51 - 20s - loss: 437.6782 - loglik: -4.3485e+02 - logprior: -2.6053e-01
Fitted a model with MAP estimate = -434.4703
Time for alignment: 491.8784
Computed alignments with likelihoods: ['-436.5776', '-436.1494', '-434.4703']
Best model has likelihood: -434.4703
time for generating output: 0.3255
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blmb.projection.fasta
SP score = 0.5133637548891786
Training of 3 independent models on file proteasome.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f32900a7280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f32900a7ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f32900a7820>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a0a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a8e0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004adc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f329004a340>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a2e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a280>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a850>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a820>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a5e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a580>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a4f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a220>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a160>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f329004aac0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f2b2db11e50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f2b2c7edc10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34ee6aa400>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f32ed4c0790>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f32900afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004a1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f329004af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32ed4c0070>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f350829f670> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f33a79ecaf0>, <function make_default_emission_matrix at 0x7f33a79ecaf0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f335700d130>, <__main__.SimpleDirichletPrior object at 0x7f32efbcf280>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3440f675e0>

Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 491.0150 - loglik: -4.8485e+02 - logprior: -6.1032e+00
Epoch 2/10
14/14 - 4s - loss: 435.0518 - loglik: -4.3322e+02 - logprior: -1.3915e+00
Epoch 3/10
14/14 - 4s - loss: 398.4645 - loglik: -3.9613e+02 - logprior: -1.4481e+00
Epoch 4/10
14/14 - 4s - loss: 386.2146 - loglik: -3.8355e+02 - logprior: -1.4627e+00
Epoch 5/10
14/14 - 4s - loss: 381.9727 - loglik: -3.7944e+02 - logprior: -1.4511e+00
Epoch 6/10
14/14 - 4s - loss: 381.0833 - loglik: -3.7883e+02 - logprior: -1.4441e+00
Epoch 7/10
14/14 - 4s - loss: 379.0565 - loglik: -3.7696e+02 - logprior: -1.4401e+00
Epoch 8/10
14/14 - 4s - loss: 378.2904 - loglik: -3.7628e+02 - logprior: -1.4494e+00
Epoch 9/10
14/14 - 4s - loss: 378.3531 - loglik: -3.7637e+02 - logprior: -1.4457e+00
Fitted a model with MAP estimate = -377.0770
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (51, 2), (52, 2), (53, 1), (58, 1), (61, 1), (69, 1), (70, 4), (89, 1), (102, 5), (117, 1), (120, 2), (128, 1), (129, 1), (130, 2), (131, 2), (133, 2), (136, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 187 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 10s - loss: 367.8011 - loglik: -3.6245e+02 - logprior: -4.8155e+00
Epoch 2/2
29/29 - 7s - loss: 354.3152 - loglik: -3.5210e+02 - logprior: -1.6469e+00
Fitted a model with MAP estimate = -352.0356
expansions: [(128, 1), (130, 1)]
discards: [  0  41  65  92 150 171]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 13s - loss: 358.9548 - loglik: -3.5312e+02 - logprior: -5.2354e+00
Epoch 2/2
29/29 - 7s - loss: 354.7334 - loglik: -3.5280e+02 - logprior: -1.2973e+00
Fitted a model with MAP estimate = -351.9028
expansions: [(1, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 11s - loss: 355.1200 - loglik: -3.5113e+02 - logprior: -3.3333e+00
Epoch 2/10
29/29 - 7s - loss: 351.4095 - loglik: -3.4977e+02 - logprior: -9.8682e-01
Epoch 3/10
29/29 - 7s - loss: 352.1046 - loglik: -3.5073e+02 - logprior: -7.3058e-01
Fitted a model with MAP estimate = -350.1640
Time for alignment: 135.4652
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 491.2607 - loglik: -4.8510e+02 - logprior: -6.1010e+00
Epoch 2/10
14/14 - 4s - loss: 435.2825 - loglik: -4.3341e+02 - logprior: -1.4187e+00
Epoch 3/10
14/14 - 4s - loss: 398.5275 - loglik: -3.9607e+02 - logprior: -1.5143e+00
Epoch 4/10
14/14 - 4s - loss: 386.3581 - loglik: -3.8372e+02 - logprior: -1.5507e+00
Epoch 5/10
14/14 - 4s - loss: 381.6079 - loglik: -3.7905e+02 - logprior: -1.5762e+00
Epoch 6/10
14/14 - 4s - loss: 379.3341 - loglik: -3.7700e+02 - logprior: -1.5797e+00
Epoch 7/10
14/14 - 4s - loss: 378.3804 - loglik: -3.7620e+02 - logprior: -1.5254e+00
Epoch 8/10
14/14 - 4s - loss: 377.4879 - loglik: -3.7537e+02 - logprior: -1.5116e+00
Epoch 9/10
14/14 - 4s - loss: 376.9535 - loglik: -3.7488e+02 - logprior: -1.5027e+00
Epoch 10/10
14/14 - 4s - loss: 376.9040 - loglik: -3.7487e+02 - logprior: -1.5054e+00
Fitted a model with MAP estimate = -376.1865
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (53, 1), (56, 1), (57, 1), (58, 1), (66, 1), (69, 1), (70, 3), (89, 1), (102, 5), (118, 1), (123, 1), (128, 1), (129, 1), (130, 2), (131, 2), (133, 2), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 13s - loss: 366.9142 - loglik: -3.6159e+02 - logprior: -4.8109e+00
Epoch 2/2
29/29 - 7s - loss: 356.2484 - loglik: -3.5416e+02 - logprior: -1.5476e+00
Fitted a model with MAP estimate = -353.2871
expansions: [(126, 1), (128, 1)]
discards: [ 41 168]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 10s - loss: 355.8923 - loglik: -3.5171e+02 - logprior: -3.6099e+00
Epoch 2/2
29/29 - 7s - loss: 353.9051 - loglik: -3.5221e+02 - logprior: -1.1321e+00
Fitted a model with MAP estimate = -351.6415
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 10s - loss: 358.7770 - loglik: -3.5306e+02 - logprior: -5.1606e+00
Epoch 2/10
29/29 - 7s - loss: 354.3049 - loglik: -3.5162e+02 - logprior: -2.1180e+00
Epoch 3/10
29/29 - 7s - loss: 353.1157 - loglik: -3.5190e+02 - logprior: -6.4197e-01
Epoch 4/10
29/29 - 7s - loss: 351.7194 - loglik: -3.5069e+02 - logprior: -4.7528e-01
Epoch 5/10
29/29 - 7s - loss: 351.8085 - loglik: -3.5091e+02 - logprior: -3.6706e-01
Fitted a model with MAP estimate = -351.0048
Time for alignment: 154.3694
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 491.6537 - loglik: -4.8549e+02 - logprior: -6.1046e+00
Epoch 2/10
14/14 - 4s - loss: 434.8262 - loglik: -4.3298e+02 - logprior: -1.4080e+00
Epoch 3/10
14/14 - 4s - loss: 398.5410 - loglik: -3.9619e+02 - logprior: -1.5239e+00
Epoch 4/10
14/14 - 4s - loss: 385.9082 - loglik: -3.8338e+02 - logprior: -1.5823e+00
Epoch 5/10
14/14 - 4s - loss: 381.0045 - loglik: -3.7854e+02 - logprior: -1.5845e+00
Epoch 6/10
14/14 - 4s - loss: 380.1834 - loglik: -3.7795e+02 - logprior: -1.5453e+00
Epoch 7/10
14/14 - 4s - loss: 377.5222 - loglik: -3.7547e+02 - logprior: -1.4903e+00
Epoch 8/10
14/14 - 4s - loss: 377.7961 - loglik: -3.7580e+02 - logprior: -1.4941e+00
Fitted a model with MAP estimate = -376.6863
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (51, 1), (53, 1), (56, 1), (58, 2), (61, 1), (66, 1), (69, 1), (70, 3), (102, 5), (118, 1), (120, 2), (127, 2), (128, 1), (131, 1), (133, 1), (134, 1), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 12s - loss: 368.5537 - loglik: -3.6331e+02 - logprior: -4.7514e+00
Epoch 2/2
29/29 - 7s - loss: 356.1296 - loglik: -3.5408e+02 - logprior: -1.5350e+00
Fitted a model with MAP estimate = -354.1130
expansions: [(128, 1), (130, 1), (158, 1)]
discards: [  0  41  77 148]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 10s - loss: 360.6366 - loglik: -3.5485e+02 - logprior: -5.2080e+00
Epoch 2/2
29/29 - 7s - loss: 355.1144 - loglik: -3.5323e+02 - logprior: -1.3036e+00
Fitted a model with MAP estimate = -352.9628
expansions: [(1, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 10s - loss: 356.3406 - loglik: -3.5247e+02 - logprior: -3.2993e+00
Epoch 2/10
29/29 - 7s - loss: 353.4276 - loglik: -3.5192e+02 - logprior: -9.4823e-01
Epoch 3/10
29/29 - 7s - loss: 352.5422 - loglik: -3.5130e+02 - logprior: -6.8963e-01
Epoch 4/10
29/29 - 7s - loss: 351.7721 - loglik: -3.5067e+02 - logprior: -5.8802e-01
Epoch 5/10
29/29 - 7s - loss: 352.5992 - loglik: -3.5166e+02 - logprior: -4.5103e-01
Fitted a model with MAP estimate = -351.2438
Time for alignment: 144.2831
Computed alignments with likelihoods: ['-350.1640', '-351.0048', '-351.2438']
Best model has likelihood: -350.1640
time for generating output: 0.2843
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/proteasome.projection.fasta
SP score = 0.8288947308263802
