Training of 3 independent models on file cys.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbbb59357f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbb946eb9d0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 172 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 575.0486 - loglik: -5.6662e+02 - logprior: -8.4315e+00
Epoch 2/10
12/12 - 5s - loss: 505.7701 - loglik: -5.0418e+02 - logprior: -1.5869e+00
Epoch 3/10
12/12 - 5s - loss: 441.2559 - loglik: -4.3974e+02 - logprior: -1.5138e+00
Epoch 4/10
12/12 - 5s - loss: 418.6449 - loglik: -4.1665e+02 - logprior: -1.9985e+00
Epoch 5/10
12/12 - 5s - loss: 407.9468 - loglik: -4.0589e+02 - logprior: -2.0521e+00
Epoch 6/10
12/12 - 5s - loss: 405.4450 - loglik: -4.0352e+02 - logprior: -1.9223e+00
Epoch 7/10
12/12 - 5s - loss: 406.1101 - loglik: -4.0426e+02 - logprior: -1.8456e+00
Fitted a model with MAP estimate = -404.5416
expansions: [(9, 3), (10, 1), (11, 2), (31, 1), (32, 2), (33, 2), (35, 2), (36, 1), (57, 1), (58, 1), (60, 1), (62, 2), (76, 1), (81, 2), (82, 2), (83, 2), (90, 1), (93, 1), (96, 1), (99, 1), (102, 1), (112, 2), (115, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (160, 1), (162, 2), (163, 1), (167, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 219 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 407.2096 - loglik: -3.9739e+02 - logprior: -9.8206e+00
Epoch 2/2
12/12 - 7s - loss: 389.0262 - loglik: -3.8500e+02 - logprior: -4.0294e+00
Fitted a model with MAP estimate = -386.3761
expansions: [(0, 2), (60, 2)]
discards: [  0   9  41  45 104 106 176 205]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 215 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 9s - loss: 390.2495 - loglik: -3.8275e+02 - logprior: -7.5008e+00
Epoch 2/2
12/12 - 7s - loss: 381.4264 - loglik: -3.8004e+02 - logprior: -1.3836e+00
Fitted a model with MAP estimate = -380.7012
expansions: []
discards: [ 0 60 80]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 212 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 391.0305 - loglik: -3.8149e+02 - logprior: -9.5438e+00
Epoch 2/10
12/12 - 7s - loss: 387.6725 - loglik: -3.8433e+02 - logprior: -3.3467e+00
Epoch 3/10
12/12 - 7s - loss: 381.6024 - loglik: -3.8010e+02 - logprior: -1.4979e+00
Epoch 4/10
12/12 - 7s - loss: 381.8863 - loglik: -3.8176e+02 - logprior: -1.3060e-01
Fitted a model with MAP estimate = -381.1756
Time for alignment: 128.5368
Fitting a model of length 172 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 576.1514 - loglik: -5.6770e+02 - logprior: -8.4472e+00
Epoch 2/10
12/12 - 5s - loss: 504.4821 - loglik: -5.0286e+02 - logprior: -1.6243e+00
Epoch 3/10
12/12 - 5s - loss: 443.9263 - loglik: -4.4239e+02 - logprior: -1.5363e+00
Epoch 4/10
12/12 - 5s - loss: 418.7585 - loglik: -4.1679e+02 - logprior: -1.9644e+00
Epoch 5/10
12/12 - 5s - loss: 408.1295 - loglik: -4.0608e+02 - logprior: -2.0493e+00
Epoch 6/10
12/12 - 5s - loss: 405.9378 - loglik: -4.0397e+02 - logprior: -1.9700e+00
Epoch 7/10
12/12 - 5s - loss: 403.4843 - loglik: -4.0157e+02 - logprior: -1.9167e+00
Epoch 8/10
12/12 - 5s - loss: 404.3092 - loglik: -4.0241e+02 - logprior: -1.8958e+00
Fitted a model with MAP estimate = -402.8938
expansions: [(9, 2), (10, 1), (11, 3), (16, 1), (31, 1), (32, 1), (33, 3), (34, 2), (47, 3), (58, 1), (60, 1), (62, 2), (74, 1), (80, 1), (81, 1), (82, 1), (83, 2), (87, 1), (90, 1), (92, 1), (95, 1), (98, 1), (101, 1), (112, 2), (115, 1), (123, 2), (126, 2), (137, 1), (140, 1), (150, 1), (160, 1), (162, 2), (167, 1), (168, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 220 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 404.3272 - loglik: -3.9453e+02 - logprior: -9.7950e+00
Epoch 2/2
12/12 - 8s - loss: 389.1715 - loglik: -3.8520e+02 - logprior: -3.9715e+00
Fitted a model with MAP estimate = -385.2257
expansions: [(0, 2)]
discards: [  0  14  42  45 144]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 217 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 388.2768 - loglik: -3.8082e+02 - logprior: -7.4590e+00
Epoch 2/2
12/12 - 8s - loss: 381.7146 - loglik: -3.8037e+02 - logprior: -1.3409e+00
Fitted a model with MAP estimate = -380.3706
expansions: []
discards: [  0  60  80 106 156 203]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 211 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 391.7750 - loglik: -3.8225e+02 - logprior: -9.5242e+00
Epoch 2/10
12/12 - 8s - loss: 387.5249 - loglik: -3.8420e+02 - logprior: -3.3250e+00
Epoch 3/10
12/12 - 8s - loss: 383.4400 - loglik: -3.8202e+02 - logprior: -1.4202e+00
Epoch 4/10
12/12 - 8s - loss: 382.2266 - loglik: -3.8211e+02 - logprior: -1.2102e-01
Epoch 5/10
12/12 - 8s - loss: 381.0122 - loglik: -3.8112e+02 - logprior: 0.1072
Epoch 6/10
12/12 - 8s - loss: 379.8766 - loglik: -3.8010e+02 - logprior: 0.2234
Epoch 7/10
12/12 - 8s - loss: 382.3096 - loglik: -3.8255e+02 - logprior: 0.2359
Fitted a model with MAP estimate = -380.9053
Time for alignment: 167.5991
Fitting a model of length 172 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 575.2087 - loglik: -5.6677e+02 - logprior: -8.4430e+00
Epoch 2/10
12/12 - 6s - loss: 507.1670 - loglik: -5.0555e+02 - logprior: -1.6127e+00
Epoch 3/10
12/12 - 6s - loss: 442.9015 - loglik: -4.4140e+02 - logprior: -1.4979e+00
Epoch 4/10
12/12 - 6s - loss: 416.3196 - loglik: -4.1438e+02 - logprior: -1.9414e+00
Epoch 5/10
12/12 - 6s - loss: 410.1582 - loglik: -4.0816e+02 - logprior: -1.9946e+00
Epoch 6/10
12/12 - 6s - loss: 408.8532 - loglik: -4.0696e+02 - logprior: -1.8971e+00
Epoch 7/10
12/12 - 6s - loss: 404.6037 - loglik: -4.0276e+02 - logprior: -1.8467e+00
Epoch 8/10
12/12 - 6s - loss: 403.8805 - loglik: -4.0205e+02 - logprior: -1.8261e+00
Epoch 9/10
12/12 - 6s - loss: 406.0728 - loglik: -4.0427e+02 - logprior: -1.7987e+00
Fitted a model with MAP estimate = -404.0592
expansions: [(9, 3), (10, 1), (11, 2), (12, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (48, 2), (50, 1), (58, 1), (60, 1), (62, 2), (76, 1), (81, 2), (82, 2), (83, 2), (90, 2), (92, 1), (96, 1), (102, 1), (112, 2), (122, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (149, 1), (159, 1), (167, 1), (168, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 218 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 11s - loss: 406.3860 - loglik: -3.9658e+02 - logprior: -9.8096e+00
Epoch 2/2
12/12 - 9s - loss: 388.1104 - loglik: -3.8415e+02 - logprior: -3.9642e+00
Fitted a model with MAP estimate = -385.7891
expansions: [(0, 2)]
discards: [  0   9  60 104 106 176]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 214 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 12s - loss: 389.7017 - loglik: -3.8223e+02 - logprior: -7.4699e+00
Epoch 2/2
12/12 - 9s - loss: 381.3196 - loglik: -3.7995e+02 - logprior: -1.3685e+00
Fitted a model with MAP estimate = -380.9354
expansions: []
discards: [  0  79 153]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 211 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 12s - loss: 391.7030 - loglik: -3.8217e+02 - logprior: -9.5301e+00
Epoch 2/10
12/12 - 9s - loss: 387.6021 - loglik: -3.8427e+02 - logprior: -3.3338e+00
Epoch 3/10
12/12 - 9s - loss: 381.7753 - loglik: -3.8032e+02 - logprior: -1.4540e+00
Epoch 4/10
12/12 - 9s - loss: 382.5937 - loglik: -3.8247e+02 - logprior: -1.1945e-01
Fitted a model with MAP estimate = -381.3018
Time for alignment: 166.2407
Computed alignments with likelihoods: ['-380.7012', '-380.3706', '-380.9354']
Best model has likelihood: -380.3706
SP score = 0.9319
Training of 3 independent models on file DMRL_synthase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbbb59357f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbbc4455ac0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 406.1949 - loglik: -3.8610e+02 - logprior: -2.0100e+01
Epoch 2/10
10/10 - 2s - loss: 341.0743 - loglik: -3.3649e+02 - logprior: -4.5892e+00
Epoch 3/10
10/10 - 2s - loss: 285.8329 - loglik: -2.8344e+02 - logprior: -2.3909e+00
Epoch 4/10
10/10 - 2s - loss: 255.7544 - loglik: -2.5363e+02 - logprior: -2.1269e+00
Epoch 5/10
10/10 - 2s - loss: 245.6469 - loglik: -2.4371e+02 - logprior: -1.9335e+00
Epoch 6/10
10/10 - 2s - loss: 242.1840 - loglik: -2.4060e+02 - logprior: -1.5882e+00
Epoch 7/10
10/10 - 2s - loss: 241.3156 - loglik: -2.3996e+02 - logprior: -1.3515e+00
Epoch 8/10
10/10 - 2s - loss: 239.4107 - loglik: -2.3817e+02 - logprior: -1.2363e+00
Epoch 9/10
10/10 - 2s - loss: 239.4974 - loglik: -2.3834e+02 - logprior: -1.1615e+00
Fitted a model with MAP estimate = -239.2138
expansions: [(0, 4), (13, 1), (14, 1), (15, 1), (24, 1), (26, 1), (27, 1), (31, 1), (43, 1), (46, 2), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (105, 1), (106, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 253.2828 - loglik: -2.2687e+02 - logprior: -2.6417e+01
Epoch 2/2
10/10 - 2s - loss: 218.7641 - loglik: -2.1133e+02 - logprior: -7.4332e+00
Fitted a model with MAP estimate = -211.8905
expansions: [(132, 1)]
discards: [61]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 224.0116 - loglik: -2.0543e+02 - logprior: -1.8585e+01
Epoch 2/2
10/10 - 2s - loss: 206.9310 - loglik: -2.0292e+02 - logprior: -4.0158e+00
Fitted a model with MAP estimate = -205.3022
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 219.7795 - loglik: -2.0232e+02 - logprior: -1.7459e+01
Epoch 2/10
10/10 - 2s - loss: 206.7828 - loglik: -2.0314e+02 - logprior: -3.6448e+00
Epoch 3/10
10/10 - 2s - loss: 203.4074 - loglik: -2.0243e+02 - logprior: -9.7791e-01
Epoch 4/10
10/10 - 2s - loss: 203.7347 - loglik: -2.0370e+02 - logprior: -3.6574e-02
Fitted a model with MAP estimate = -202.8194
Time for alignment: 52.3174
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 406.1144 - loglik: -3.8601e+02 - logprior: -2.0100e+01
Epoch 2/10
10/10 - 2s - loss: 342.1637 - loglik: -3.3757e+02 - logprior: -4.5974e+00
Epoch 3/10
10/10 - 2s - loss: 290.9786 - loglik: -2.8854e+02 - logprior: -2.4412e+00
Epoch 4/10
10/10 - 2s - loss: 260.1424 - loglik: -2.5797e+02 - logprior: -2.1687e+00
Epoch 5/10
10/10 - 2s - loss: 246.6818 - loglik: -2.4459e+02 - logprior: -2.0920e+00
Epoch 6/10
10/10 - 2s - loss: 241.9427 - loglik: -2.4004e+02 - logprior: -1.9040e+00
Epoch 7/10
10/10 - 2s - loss: 240.5431 - loglik: -2.3881e+02 - logprior: -1.7349e+00
Epoch 8/10
10/10 - 2s - loss: 239.2811 - loglik: -2.3762e+02 - logprior: -1.6573e+00
Epoch 9/10
10/10 - 2s - loss: 238.6687 - loglik: -2.3707e+02 - logprior: -1.6017e+00
Epoch 10/10
10/10 - 2s - loss: 238.4817 - loglik: -2.3692e+02 - logprior: -1.5591e+00
Fitted a model with MAP estimate = -238.2825
expansions: [(0, 4), (13, 1), (14, 1), (15, 1), (20, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 1), (47, 1), (48, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (98, 2), (104, 3), (105, 2), (106, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 146 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 252.5872 - loglik: -2.2606e+02 - logprior: -2.6530e+01
Epoch 2/2
10/10 - 2s - loss: 215.9809 - loglik: -2.0860e+02 - logprior: -7.3850e+00
Fitted a model with MAP estimate = -209.9151
expansions: []
discards: [ 62 121]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 222.3995 - loglik: -2.0386e+02 - logprior: -1.8540e+01
Epoch 2/2
10/10 - 2s - loss: 207.1669 - loglik: -2.0319e+02 - logprior: -3.9818e+00
Fitted a model with MAP estimate = -205.1204
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 219.9382 - loglik: -2.0250e+02 - logprior: -1.7440e+01
Epoch 2/10
10/10 - 2s - loss: 206.1868 - loglik: -2.0257e+02 - logprior: -3.6150e+00
Epoch 3/10
10/10 - 2s - loss: 204.0672 - loglik: -2.0312e+02 - logprior: -9.5103e-01
Epoch 4/10
10/10 - 2s - loss: 203.7013 - loglik: -2.0369e+02 - logprior: -9.1856e-03
Epoch 5/10
10/10 - 2s - loss: 202.5757 - loglik: -2.0304e+02 - logprior: 0.4609
Epoch 6/10
10/10 - 2s - loss: 202.7539 - loglik: -2.0356e+02 - logprior: 0.8029
Fitted a model with MAP estimate = -202.3449
Time for alignment: 59.2377
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 406.0847 - loglik: -3.8599e+02 - logprior: -2.0099e+01
Epoch 2/10
10/10 - 2s - loss: 341.8311 - loglik: -3.3724e+02 - logprior: -4.5904e+00
Epoch 3/10
10/10 - 2s - loss: 289.4452 - loglik: -2.8703e+02 - logprior: -2.4117e+00
Epoch 4/10
10/10 - 2s - loss: 257.1758 - loglik: -2.5498e+02 - logprior: -2.1920e+00
Epoch 5/10
10/10 - 2s - loss: 245.4160 - loglik: -2.4330e+02 - logprior: -2.1151e+00
Epoch 6/10
10/10 - 2s - loss: 242.2388 - loglik: -2.4033e+02 - logprior: -1.9058e+00
Epoch 7/10
10/10 - 2s - loss: 239.6330 - loglik: -2.3790e+02 - logprior: -1.7307e+00
Epoch 8/10
10/10 - 2s - loss: 239.2462 - loglik: -2.3760e+02 - logprior: -1.6461e+00
Epoch 9/10
10/10 - 2s - loss: 238.6361 - loglik: -2.3705e+02 - logprior: -1.5907e+00
Epoch 10/10
10/10 - 2s - loss: 238.5335 - loglik: -2.3698e+02 - logprior: -1.5563e+00
Fitted a model with MAP estimate = -238.2371
expansions: [(0, 4), (13, 1), (14, 1), (15, 1), (20, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 1), (47, 1), (48, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (98, 2), (104, 3), (105, 2), (106, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 146 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 252.1840 - loglik: -2.2567e+02 - logprior: -2.6517e+01
Epoch 2/2
10/10 - 2s - loss: 216.2755 - loglik: -2.0889e+02 - logprior: -7.3859e+00
Fitted a model with MAP estimate = -209.9352
expansions: []
discards: [ 62 121]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 222.3402 - loglik: -2.0380e+02 - logprior: -1.8543e+01
Epoch 2/2
10/10 - 2s - loss: 207.2374 - loglik: -2.0325e+02 - logprior: -3.9876e+00
Fitted a model with MAP estimate = -205.1316
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 219.9280 - loglik: -2.0249e+02 - logprior: -1.7442e+01
Epoch 2/10
10/10 - 2s - loss: 206.3405 - loglik: -2.0271e+02 - logprior: -3.6273e+00
Epoch 3/10
10/10 - 2s - loss: 204.3345 - loglik: -2.0338e+02 - logprior: -9.5472e-01
Epoch 4/10
10/10 - 2s - loss: 202.9155 - loglik: -2.0290e+02 - logprior: -1.2024e-02
Epoch 5/10
10/10 - 2s - loss: 202.3926 - loglik: -2.0284e+02 - logprior: 0.4524
Epoch 6/10
10/10 - 2s - loss: 202.6238 - loglik: -2.0342e+02 - logprior: 0.7981
Fitted a model with MAP estimate = -202.3493
Time for alignment: 59.9045
Computed alignments with likelihoods: ['-202.8194', '-202.3449', '-202.3493']
Best model has likelihood: -202.3449
SP score = 0.9058
Training of 3 independent models on file Stap_Strp_toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbb9483bfa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cd3a69a0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 316.9228 - loglik: -2.4845e+02 - logprior: -6.8470e+01
Epoch 2/10
10/10 - 2s - loss: 236.2929 - loglik: -2.1909e+02 - logprior: -1.7204e+01
Epoch 3/10
10/10 - 2s - loss: 203.7585 - loglik: -1.9640e+02 - logprior: -7.3635e+00
Epoch 4/10
10/10 - 2s - loss: 189.8810 - loglik: -1.8602e+02 - logprior: -3.8578e+00
Epoch 5/10
10/10 - 2s - loss: 182.6578 - loglik: -1.8067e+02 - logprior: -1.9857e+00
Epoch 6/10
10/10 - 1s - loss: 178.2429 - loglik: -1.7721e+02 - logprior: -1.0343e+00
Epoch 7/10
10/10 - 1s - loss: 175.9698 - loglik: -1.7549e+02 - logprior: -4.8367e-01
Epoch 8/10
10/10 - 2s - loss: 174.7931 - loglik: -1.7469e+02 - logprior: -9.9720e-02
Epoch 9/10
10/10 - 2s - loss: 174.2778 - loglik: -1.7441e+02 - logprior: 0.1350
Epoch 10/10
10/10 - 2s - loss: 173.9625 - loglik: -1.7428e+02 - logprior: 0.3222
Fitted a model with MAP estimate = -173.8210
expansions: [(14, 2), (15, 2), (25, 1), (26, 2), (27, 2), (37, 1), (52, 1), (58, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 249.6109 - loglik: -1.7278e+02 - logprior: -7.6833e+01
Epoch 2/2
10/10 - 2s - loss: 196.5583 - loglik: -1.6566e+02 - logprior: -3.0896e+01
Fitted a model with MAP estimate = -187.7348
expansions: []
discards: [ 0 14 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 240.2535 - loglik: -1.6494e+02 - logprior: -7.5314e+01
Epoch 2/2
10/10 - 2s - loss: 189.8363 - loglik: -1.6406e+02 - logprior: -2.5779e+01
Fitted a model with MAP estimate = -178.8426
expansions: [(0, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 223.1336 - loglik: -1.6274e+02 - logprior: -6.0390e+01
Epoch 2/10
10/10 - 2s - loss: 175.8591 - loglik: -1.6125e+02 - logprior: -1.4607e+01
Epoch 3/10
10/10 - 2s - loss: 166.3020 - loglik: -1.6125e+02 - logprior: -5.0533e+00
Epoch 4/10
10/10 - 2s - loss: 162.6046 - loglik: -1.6154e+02 - logprior: -1.0626e+00
Epoch 5/10
10/10 - 2s - loss: 160.7472 - loglik: -1.6190e+02 - logprior: 1.1542
Epoch 6/10
10/10 - 2s - loss: 159.7563 - loglik: -1.6223e+02 - logprior: 2.4721
Epoch 7/10
10/10 - 2s - loss: 159.1380 - loglik: -1.6244e+02 - logprior: 3.2975
Epoch 8/10
10/10 - 2s - loss: 158.7192 - loglik: -1.6258e+02 - logprior: 3.8587
Epoch 9/10
10/10 - 2s - loss: 158.3974 - loglik: -1.6267e+02 - logprior: 4.2756
Epoch 10/10
10/10 - 2s - loss: 158.1263 - loglik: -1.6275e+02 - logprior: 4.6189
Fitted a model with MAP estimate = -157.9599
Time for alignment: 55.7074
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 316.8963 - loglik: -2.4843e+02 - logprior: -6.8469e+01
Epoch 2/10
10/10 - 2s - loss: 236.1100 - loglik: -2.1890e+02 - logprior: -1.7208e+01
Epoch 3/10
10/10 - 2s - loss: 203.7661 - loglik: -1.9643e+02 - logprior: -7.3366e+00
Epoch 4/10
10/10 - 2s - loss: 190.5162 - loglik: -1.8683e+02 - logprior: -3.6895e+00
Epoch 5/10
10/10 - 2s - loss: 185.1877 - loglik: -1.8348e+02 - logprior: -1.7074e+00
Epoch 6/10
10/10 - 2s - loss: 181.2965 - loglik: -1.8066e+02 - logprior: -6.3354e-01
Epoch 7/10
10/10 - 2s - loss: 178.6625 - loglik: -1.7853e+02 - logprior: -1.3499e-01
Epoch 8/10
10/10 - 2s - loss: 177.1678 - loglik: -1.7737e+02 - logprior: 0.2069
Epoch 9/10
10/10 - 2s - loss: 176.3782 - loglik: -1.7686e+02 - logprior: 0.4797
Epoch 10/10
10/10 - 2s - loss: 175.9947 - loglik: -1.7666e+02 - logprior: 0.6644
Fitted a model with MAP estimate = -175.8142
expansions: [(14, 2), (26, 2), (37, 1), (52, 2), (58, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 252.0291 - loglik: -1.7497e+02 - logprior: -7.7064e+01
Epoch 2/2
10/10 - 2s - loss: 199.9249 - loglik: -1.6887e+02 - logprior: -3.1057e+01
Fitted a model with MAP estimate = -191.2724
expansions: [(25, 1)]
discards: [ 0 56]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 243.6460 - loglik: -1.6783e+02 - logprior: -7.5818e+01
Epoch 2/2
10/10 - 2s - loss: 193.2017 - loglik: -1.6630e+02 - logprior: -2.6905e+01
Fitted a model with MAP estimate = -182.2124
expansions: [(0, 3), (7, 2)]
discards: [ 0 27]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 226.1266 - loglik: -1.6527e+02 - logprior: -6.0858e+01
Epoch 2/10
10/10 - 2s - loss: 178.0126 - loglik: -1.6303e+02 - logprior: -1.4985e+01
Epoch 3/10
10/10 - 2s - loss: 168.2775 - loglik: -1.6291e+02 - logprior: -5.3665e+00
Epoch 4/10
10/10 - 2s - loss: 164.4037 - loglik: -1.6307e+02 - logprior: -1.3289e+00
Epoch 5/10
10/10 - 2s - loss: 162.4120 - loglik: -1.6331e+02 - logprior: 0.8994
Epoch 6/10
10/10 - 2s - loss: 161.3624 - loglik: -1.6359e+02 - logprior: 2.2282
Epoch 7/10
10/10 - 2s - loss: 160.7354 - loglik: -1.6379e+02 - logprior: 3.0497
Epoch 8/10
10/10 - 2s - loss: 160.3094 - loglik: -1.6391e+02 - logprior: 3.5986
Epoch 9/10
10/10 - 2s - loss: 159.9885 - loglik: -1.6400e+02 - logprior: 4.0147
Epoch 10/10
10/10 - 2s - loss: 159.7072 - loglik: -1.6406e+02 - logprior: 4.3532
Fitted a model with MAP estimate = -159.5477
Time for alignment: 56.2850
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 316.9095 - loglik: -2.4844e+02 - logprior: -6.8469e+01
Epoch 2/10
10/10 - 2s - loss: 236.1895 - loglik: -2.1898e+02 - logprior: -1.7207e+01
Epoch 3/10
10/10 - 2s - loss: 203.1123 - loglik: -1.9573e+02 - logprior: -7.3781e+00
Epoch 4/10
10/10 - 2s - loss: 188.9792 - loglik: -1.8511e+02 - logprior: -3.8671e+00
Epoch 5/10
10/10 - 2s - loss: 183.0306 - loglik: -1.8116e+02 - logprior: -1.8670e+00
Epoch 6/10
10/10 - 2s - loss: 178.8947 - loglik: -1.7803e+02 - logprior: -8.6107e-01
Epoch 7/10
10/10 - 2s - loss: 176.3887 - loglik: -1.7606e+02 - logprior: -3.3359e-01
Epoch 8/10
10/10 - 2s - loss: 174.9706 - loglik: -1.7499e+02 - logprior: 0.0202
Epoch 9/10
10/10 - 2s - loss: 174.2215 - loglik: -1.7450e+02 - logprior: 0.2790
Epoch 10/10
10/10 - 2s - loss: 173.7585 - loglik: -1.7420e+02 - logprior: 0.4404
Fitted a model with MAP estimate = -173.5540
expansions: [(9, 3), (14, 2), (15, 1), (25, 2), (26, 2), (37, 1), (52, 1), (58, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 249.1602 - loglik: -1.7247e+02 - logprior: -7.6695e+01
Epoch 2/2
10/10 - 2s - loss: 195.9349 - loglik: -1.6518e+02 - logprior: -3.0756e+01
Fitted a model with MAP estimate = -187.0332
expansions: []
discards: [16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 234.8996 - loglik: -1.6199e+02 - logprior: -7.2914e+01
Epoch 2/2
10/10 - 2s - loss: 181.7763 - loglik: -1.6108e+02 - logprior: -2.0701e+01
Fitted a model with MAP estimate = -171.3030
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 220.3401 - loglik: -1.5920e+02 - logprior: -6.1139e+01
Epoch 2/10
10/10 - 2s - loss: 174.2566 - loglik: -1.5950e+02 - logprior: -1.4760e+01
Epoch 3/10
10/10 - 2s - loss: 165.2189 - loglik: -1.6014e+02 - logprior: -5.0810e+00
Epoch 4/10
10/10 - 2s - loss: 161.5603 - loglik: -1.6051e+02 - logprior: -1.0472e+00
Epoch 5/10
10/10 - 2s - loss: 159.5837 - loglik: -1.6075e+02 - logprior: 1.1646
Epoch 6/10
10/10 - 2s - loss: 158.4836 - loglik: -1.6092e+02 - logprior: 2.4314
Epoch 7/10
10/10 - 2s - loss: 157.8171 - loglik: -1.6104e+02 - logprior: 3.2188
Epoch 8/10
10/10 - 2s - loss: 157.3821 - loglik: -1.6115e+02 - logprior: 3.7678
Epoch 9/10
10/10 - 2s - loss: 157.0343 - loglik: -1.6126e+02 - logprior: 4.2211
Epoch 10/10
10/10 - 2s - loss: 156.7360 - loglik: -1.6135e+02 - logprior: 4.6101
Fitted a model with MAP estimate = -156.5573
Time for alignment: 58.6347
Computed alignments with likelihoods: ['-157.9599', '-159.5477', '-156.5573']
Best model has likelihood: -156.5573
SP score = 0.3404
Training of 3 independent models on file ghf5.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbbb59357c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9de2c5970>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 225 on 2717 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 15s - loss: 777.3218 - loglik: -7.6348e+02 - logprior: -1.3846e+01
Epoch 2/10
10/10 - 14s - loss: 735.7440 - loglik: -7.3383e+02 - logprior: -1.9124e+00
Epoch 3/10
10/10 - 15s - loss: 699.0825 - loglik: -6.9920e+02 - logprior: 0.1175
Epoch 4/10
10/10 - 15s - loss: 672.0986 - loglik: -6.7250e+02 - logprior: 0.4032
Epoch 5/10
10/10 - 16s - loss: 659.2941 - loglik: -6.5947e+02 - logprior: 0.1778
Epoch 6/10
10/10 - 16s - loss: 652.9960 - loglik: -6.5297e+02 - logprior: -2.8004e-02
Epoch 7/10
10/10 - 15s - loss: 651.1072 - loglik: -6.5113e+02 - logprior: 0.0232
Epoch 8/10
10/10 - 16s - loss: 645.7599 - loglik: -6.4588e+02 - logprior: 0.1208
Epoch 9/10
10/10 - 16s - loss: 646.6705 - loglik: -6.4683e+02 - logprior: 0.1547
Fitted a model with MAP estimate = -645.6867
expansions: [(0, 4), (18, 3), (25, 3), (45, 1), (52, 1), (54, 2), (56, 1), (81, 3), (85, 12), (86, 1), (94, 1), (111, 4), (112, 2), (115, 2), (117, 1), (150, 1), (153, 1), (168, 1), (199, 6), (201, 1), (207, 2), (208, 1)]
discards: [217 218]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 277 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 24s - loss: 652.9672 - loglik: -6.4170e+02 - logprior: -1.1267e+01
Epoch 2/2
20/20 - 22s - loss: 635.4907 - loglik: -6.3484e+02 - logprior: -6.5077e-01
Fitted a model with MAP estimate = -630.8345
expansions: []
discards: [  1   2   3  23  33  97  98 106 107 145 146 147 153 245 246 258]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 261 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 22s - loss: 639.9582 - loglik: -6.3278e+02 - logprior: -7.1803e+00
Epoch 2/2
20/20 - 19s - loss: 632.7374 - loglik: -6.3325e+02 - logprior: 0.5078
Fitted a model with MAP estimate = -630.6131
expansions: [(201, 10)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 271 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 23s - loss: 637.3134 - loglik: -6.3048e+02 - logprior: -6.8345e+00
Epoch 2/10
20/20 - 21s - loss: 630.3596 - loglik: -6.3132e+02 - logprior: 0.9635
Epoch 3/10
20/20 - 21s - loss: 627.0457 - loglik: -6.2882e+02 - logprior: 1.7753
Epoch 4/10
20/20 - 21s - loss: 627.2438 - loglik: -6.2965e+02 - logprior: 2.4078
Fitted a model with MAP estimate = -626.5297
Time for alignment: 359.4815
Fitting a model of length 225 on 2717 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 18s - loss: 778.0198 - loglik: -7.6417e+02 - logprior: -1.3846e+01
Epoch 2/10
10/10 - 16s - loss: 732.7079 - loglik: -7.3079e+02 - logprior: -1.9171e+00
Epoch 3/10
10/10 - 16s - loss: 698.8113 - loglik: -6.9890e+02 - logprior: 0.0893
Epoch 4/10
10/10 - 16s - loss: 674.8544 - loglik: -6.7523e+02 - logprior: 0.3718
Epoch 5/10
10/10 - 16s - loss: 657.7833 - loglik: -6.5796e+02 - logprior: 0.1767
Epoch 6/10
10/10 - 16s - loss: 657.2286 - loglik: -6.5723e+02 - logprior: -9.0077e-04
Epoch 7/10
10/10 - 15s - loss: 648.9089 - loglik: -6.4903e+02 - logprior: 0.1212
Epoch 8/10
10/10 - 14s - loss: 647.7037 - loglik: -6.4791e+02 - logprior: 0.2099
Epoch 9/10
10/10 - 15s - loss: 648.5726 - loglik: -6.4880e+02 - logprior: 0.2232
Fitted a model with MAP estimate = -646.9734
expansions: [(0, 3), (25, 1), (52, 1), (54, 2), (56, 1), (86, 11), (92, 1), (94, 3), (112, 2), (115, 2), (117, 1), (147, 1), (153, 2), (180, 1), (199, 4), (208, 1)]
discards: [192 217 218]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 259 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 19s - loss: 654.8676 - loglik: -6.4368e+02 - logprior: -1.1187e+01
Epoch 2/2
20/20 - 17s - loss: 636.3080 - loglik: -6.3577e+02 - logprior: -5.3564e-01
Fitted a model with MAP estimate = -634.4410
expansions: [(0, 4), (232, 1), (242, 1)]
discards: [  1   2   3   4   5   6 115 140 183]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 256 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 21s - loss: 647.6799 - loglik: -6.3570e+02 - logprior: -1.1981e+01
Epoch 2/2
20/20 - 18s - loss: 636.3063 - loglik: -6.3578e+02 - logprior: -5.2615e-01
Fitted a model with MAP estimate = -633.2179
expansions: [(0, 3), (197, 10)]
discards: [1 2 3]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 266 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 21s - loss: 646.7020 - loglik: -6.3547e+02 - logprior: -1.1229e+01
Epoch 2/10
20/20 - 17s - loss: 632.6735 - loglik: -6.3207e+02 - logprior: -6.0078e-01
Epoch 3/10
20/20 - 17s - loss: 630.9419 - loglik: -6.3253e+02 - logprior: 1.5852
Epoch 4/10
20/20 - 17s - loss: 630.4062 - loglik: -6.3274e+02 - logprior: 2.3339
Epoch 5/10
20/20 - 18s - loss: 630.5216 - loglik: -6.3319e+02 - logprior: 2.6672
Fitted a model with MAP estimate = -629.3711
Time for alignment: 349.4724
Fitting a model of length 225 on 2717 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 17s - loss: 776.5667 - loglik: -7.6272e+02 - logprior: -1.3844e+01
Epoch 2/10
10/10 - 15s - loss: 738.3715 - loglik: -7.3647e+02 - logprior: -1.8972e+00
Epoch 3/10
10/10 - 15s - loss: 698.8529 - loglik: -6.9899e+02 - logprior: 0.1331
Epoch 4/10
10/10 - 15s - loss: 676.1096 - loglik: -6.7643e+02 - logprior: 0.3236
Epoch 5/10
10/10 - 14s - loss: 656.2568 - loglik: -6.5635e+02 - logprior: 0.0942
Epoch 6/10
10/10 - 14s - loss: 653.1340 - loglik: -6.5305e+02 - logprior: -8.4269e-02
Epoch 7/10
10/10 - 13s - loss: 651.0261 - loglik: -6.5106e+02 - logprior: 0.0375
Epoch 8/10
10/10 - 13s - loss: 647.8104 - loglik: -6.4801e+02 - logprior: 0.1950
Epoch 9/10
10/10 - 14s - loss: 645.8870 - loglik: -6.4608e+02 - logprior: 0.1963
Epoch 10/10
10/10 - 14s - loss: 646.3690 - loglik: -6.4658e+02 - logprior: 0.2119
Fitted a model with MAP estimate = -644.8718
expansions: [(0, 4), (24, 1), (51, 2), (62, 1), (86, 12), (92, 1), (94, 1), (100, 1), (112, 2), (115, 2), (148, 1), (154, 2), (155, 1), (171, 10), (176, 1), (201, 1), (208, 1)]
discards: [217 218]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 267 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 21s - loss: 653.5092 - loglik: -6.4236e+02 - logprior: -1.1152e+01
Epoch 2/2
20/20 - 20s - loss: 635.0668 - loglik: -6.3456e+02 - logprior: -5.0230e-01
Fitted a model with MAP estimate = -633.8056
expansions: [(0, 3), (240, 3), (250, 1)]
discards: [  1   2   3   4   5  42  43  97  98 140 183 202 203 204]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 260 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 22s - loss: 650.0702 - loglik: -6.3800e+02 - logprior: -1.2072e+01
Epoch 2/2
20/20 - 21s - loss: 635.3658 - loglik: -6.3479e+02 - logprior: -5.7658e-01
Fitted a model with MAP estimate = -634.2385
expansions: [(0, 3), (23, 5)]
discards: [3 4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 266 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 25s - loss: 645.3463 - loglik: -6.3373e+02 - logprior: -1.1611e+01
Epoch 2/10
20/20 - 21s - loss: 634.9092 - loglik: -6.3442e+02 - logprior: -4.8723e-01
Epoch 3/10
20/20 - 21s - loss: 630.6626 - loglik: -6.3231e+02 - logprior: 1.6435
Epoch 4/10
20/20 - 20s - loss: 630.5739 - loglik: -6.3291e+02 - logprior: 2.3350
Epoch 5/10
20/20 - 20s - loss: 630.6981 - loglik: -6.3333e+02 - logprior: 2.6287
Fitted a model with MAP estimate = -629.7832
Time for alignment: 383.4544
Computed alignments with likelihoods: ['-626.5297', '-629.3711', '-629.7832']
Best model has likelihood: -626.5297
SP score = 0.6497
Training of 3 independent models on file myb_DNA-binding.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c2e28820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c2e02340>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.6447 - loglik: -1.3140e+02 - logprior: -3.2422e+00
Epoch 2/10
19/19 - 1s - loss: 114.2896 - loglik: -1.1289e+02 - logprior: -1.3947e+00
Epoch 3/10
19/19 - 1s - loss: 106.7075 - loglik: -1.0518e+02 - logprior: -1.5290e+00
Epoch 4/10
19/19 - 1s - loss: 104.0596 - loglik: -1.0258e+02 - logprior: -1.4804e+00
Epoch 5/10
19/19 - 1s - loss: 103.3587 - loglik: -1.0189e+02 - logprior: -1.4695e+00
Epoch 6/10
19/19 - 1s - loss: 102.7617 - loglik: -1.0130e+02 - logprior: -1.4638e+00
Epoch 7/10
19/19 - 1s - loss: 102.6897 - loglik: -1.0123e+02 - logprior: -1.4550e+00
Epoch 8/10
19/19 - 1s - loss: 102.3165 - loglik: -1.0086e+02 - logprior: -1.4556e+00
Epoch 9/10
19/19 - 1s - loss: 102.3529 - loglik: -1.0091e+02 - logprior: -1.4468e+00
Fitted a model with MAP estimate = -100.9199
expansions: [(6, 1), (10, 1), (12, 1), (13, 1), (14, 1), (20, 1), (22, 2), (23, 1), (28, 2), (30, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 103.7546 - loglik: -9.9552e+01 - logprior: -4.2022e+00
Epoch 2/2
19/19 - 1s - loss: 95.1657 - loglik: -9.3064e+01 - logprior: -2.1017e+00
Fitted a model with MAP estimate = -92.5884
expansions: [(0, 1)]
discards: [ 0 29 36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 95.4677 - loglik: -9.2392e+01 - logprior: -3.0761e+00
Epoch 2/2
19/19 - 1s - loss: 92.5550 - loglik: -9.1094e+01 - logprior: -1.4609e+00
Fitted a model with MAP estimate = -91.1072
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 93.0584 - loglik: -8.9905e+01 - logprior: -3.1538e+00
Epoch 2/10
19/19 - 1s - loss: 90.9922 - loglik: -8.9547e+01 - logprior: -1.4448e+00
Epoch 3/10
19/19 - 1s - loss: 90.6152 - loglik: -8.9278e+01 - logprior: -1.3373e+00
Epoch 4/10
19/19 - 1s - loss: 90.7466 - loglik: -8.9438e+01 - logprior: -1.3084e+00
Fitted a model with MAP estimate = -90.4837
Time for alignment: 40.3058
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.6631 - loglik: -1.3142e+02 - logprior: -3.2402e+00
Epoch 2/10
19/19 - 1s - loss: 114.4250 - loglik: -1.1307e+02 - logprior: -1.3511e+00
Epoch 3/10
19/19 - 1s - loss: 107.4758 - loglik: -1.0603e+02 - logprior: -1.4469e+00
Epoch 4/10
19/19 - 1s - loss: 105.0252 - loglik: -1.0358e+02 - logprior: -1.4420e+00
Epoch 5/10
19/19 - 1s - loss: 103.8273 - loglik: -1.0238e+02 - logprior: -1.4483e+00
Epoch 6/10
19/19 - 1s - loss: 103.0621 - loglik: -1.0163e+02 - logprior: -1.4359e+00
Epoch 7/10
19/19 - 1s - loss: 102.5623 - loglik: -1.0113e+02 - logprior: -1.4358e+00
Epoch 8/10
19/19 - 1s - loss: 102.6564 - loglik: -1.0122e+02 - logprior: -1.4329e+00
Fitted a model with MAP estimate = -101.1277
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (22, 2), (23, 1), (28, 2), (29, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 103.2797 - loglik: -9.9069e+01 - logprior: -4.2111e+00
Epoch 2/2
19/19 - 1s - loss: 95.0086 - loglik: -9.2922e+01 - logprior: -2.0863e+00
Fitted a model with MAP estimate = -92.5183
expansions: [(0, 1)]
discards: [ 0  9 30]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 95.4695 - loglik: -9.2401e+01 - logprior: -3.0684e+00
Epoch 2/2
19/19 - 1s - loss: 92.5342 - loglik: -9.1075e+01 - logprior: -1.4593e+00
Fitted a model with MAP estimate = -91.0724
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 93.0137 - loglik: -8.9865e+01 - logprior: -3.1491e+00
Epoch 2/10
19/19 - 1s - loss: 91.0305 - loglik: -8.9586e+01 - logprior: -1.4443e+00
Epoch 3/10
19/19 - 1s - loss: 90.6946 - loglik: -8.9357e+01 - logprior: -1.3374e+00
Epoch 4/10
19/19 - 1s - loss: 90.6528 - loglik: -8.9343e+01 - logprior: -1.3096e+00
Epoch 5/10
19/19 - 1s - loss: 90.4724 - loglik: -8.9190e+01 - logprior: -1.2819e+00
Epoch 6/10
19/19 - 1s - loss: 90.2461 - loglik: -8.8993e+01 - logprior: -1.2529e+00
Epoch 7/10
19/19 - 1s - loss: 90.5490 - loglik: -8.9311e+01 - logprior: -1.2382e+00
Fitted a model with MAP estimate = -90.4023
Time for alignment: 40.7437
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.6940 - loglik: -1.3146e+02 - logprior: -3.2383e+00
Epoch 2/10
19/19 - 1s - loss: 114.0604 - loglik: -1.1270e+02 - logprior: -1.3587e+00
Epoch 3/10
19/19 - 1s - loss: 105.9755 - loglik: -1.0455e+02 - logprior: -1.4287e+00
Epoch 4/10
19/19 - 1s - loss: 103.7021 - loglik: -1.0234e+02 - logprior: -1.3628e+00
Epoch 5/10
19/19 - 1s - loss: 102.6914 - loglik: -1.0133e+02 - logprior: -1.3616e+00
Epoch 6/10
19/19 - 1s - loss: 102.0312 - loglik: -1.0073e+02 - logprior: -1.3052e+00
Epoch 7/10
19/19 - 1s - loss: 101.9641 - loglik: -1.0071e+02 - logprior: -1.2496e+00
Epoch 8/10
19/19 - 1s - loss: 101.7280 - loglik: -1.0048e+02 - logprior: -1.2453e+00
Epoch 9/10
19/19 - 1s - loss: 101.8542 - loglik: -1.0060e+02 - logprior: -1.2494e+00
Fitted a model with MAP estimate = -100.2304
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (22, 3), (35, 1), (36, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 103.2172 - loglik: -9.8974e+01 - logprior: -4.2430e+00
Epoch 2/2
19/19 - 1s - loss: 95.8258 - loglik: -9.3691e+01 - logprior: -2.1347e+00
Fitted a model with MAP estimate = -93.1542
expansions: [(0, 1)]
discards: [ 0 11 46]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 96.1091 - loglik: -9.2918e+01 - logprior: -3.1912e+00
Epoch 2/2
19/19 - 1s - loss: 92.9563 - loglik: -9.1472e+01 - logprior: -1.4845e+00
Fitted a model with MAP estimate = -91.1273
expansions: []
discards: [29]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 93.4486 - loglik: -9.0265e+01 - logprior: -3.1835e+00
Epoch 2/10
19/19 - 1s - loss: 91.0587 - loglik: -8.9622e+01 - logprior: -1.4363e+00
Epoch 3/10
19/19 - 1s - loss: 90.8197 - loglik: -8.9472e+01 - logprior: -1.3477e+00
Epoch 4/10
19/19 - 1s - loss: 90.7403 - loglik: -8.9431e+01 - logprior: -1.3088e+00
Epoch 5/10
19/19 - 1s - loss: 90.6872 - loglik: -8.9405e+01 - logprior: -1.2818e+00
Epoch 6/10
19/19 - 1s - loss: 90.5039 - loglik: -8.9247e+01 - logprior: -1.2568e+00
Epoch 7/10
19/19 - 1s - loss: 90.9280 - loglik: -8.9687e+01 - logprior: -1.2407e+00
Fitted a model with MAP estimate = -90.5854
Time for alignment: 42.2256
Computed alignments with likelihoods: ['-90.4837', '-90.4023', '-90.5854']
Best model has likelihood: -90.4023
SP score = 0.9439
Training of 3 independent models on file ghf10.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9f791b910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cb661b20>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 225 on 1502 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 14s - loss: 668.7604 - loglik: -6.4377e+02 - logprior: -2.4995e+01
Epoch 2/10
10/10 - 12s - loss: 607.1649 - loglik: -6.0356e+02 - logprior: -3.6066e+00
Epoch 3/10
10/10 - 13s - loss: 552.4194 - loglik: -5.5251e+02 - logprior: 0.0937
Epoch 4/10
10/10 - 13s - loss: 509.6201 - loglik: -5.0966e+02 - logprior: 0.0403
Epoch 5/10
10/10 - 13s - loss: 487.0682 - loglik: -4.8652e+02 - logprior: -5.5300e-01
Epoch 6/10
10/10 - 13s - loss: 475.5715 - loglik: -4.7458e+02 - logprior: -9.8679e-01
Epoch 7/10
10/10 - 13s - loss: 471.2334 - loglik: -4.7016e+02 - logprior: -1.0766e+00
Epoch 8/10
10/10 - 13s - loss: 465.6171 - loglik: -4.6469e+02 - logprior: -9.2989e-01
Epoch 9/10
10/10 - 13s - loss: 468.0961 - loglik: -4.6707e+02 - logprior: -1.0258e+00
Fitted a model with MAP estimate = -466.0818
expansions: [(23, 1), (24, 2), (25, 2), (27, 1), (36, 1), (37, 1), (39, 1), (41, 1), (48, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (65, 1), (66, 1), (68, 2), (69, 3), (71, 1), (72, 1), (73, 2), (76, 1), (93, 1), (94, 1), (100, 1), (101, 2), (102, 3), (104, 2), (131, 2), (132, 1), (133, 1), (134, 2), (135, 3), (137, 1), (157, 1), (158, 7), (173, 8)]
discards: [  1   2 223]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 286 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 17s - loss: 470.2179 - loglik: -4.5354e+02 - logprior: -1.6682e+01
Epoch 2/2
15/15 - 16s - loss: 433.8066 - loglik: -4.3385e+02 - logprior: 0.0424
Fitted a model with MAP estimate = -431.2024
expansions: [(26, 1), (91, 1), (206, 2), (207, 1)]
discards: [ 23  84 130 138 177 235]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 18s - loss: 446.7300 - loglik: -4.3140e+02 - logprior: -1.5327e+01
Epoch 2/2
15/15 - 14s - loss: 430.4431 - loglik: -4.3185e+02 - logprior: 1.4076
Fitted a model with MAP estimate = -427.2976
expansions: [(0, 2)]
discards: [165 203]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 17s - loss: 454.2522 - loglik: -4.3073e+02 - logprior: -2.3523e+01
Epoch 2/10
15/15 - 13s - loss: 429.3733 - loglik: -4.2846e+02 - logprior: -9.1767e-01
Epoch 3/10
15/15 - 14s - loss: 424.4001 - loglik: -4.2873e+02 - logprior: 4.3338
Epoch 4/10
15/15 - 14s - loss: 424.4102 - loglik: -4.3043e+02 - logprior: 6.0171
Fitted a model with MAP estimate = -422.1966
Time for alignment: 269.2493
Fitting a model of length 225 on 1502 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 15s - loss: 668.9716 - loglik: -6.4398e+02 - logprior: -2.4991e+01
Epoch 2/10
10/10 - 14s - loss: 608.6347 - loglik: -6.0504e+02 - logprior: -3.5909e+00
Epoch 3/10
10/10 - 14s - loss: 555.5668 - loglik: -5.5574e+02 - logprior: 0.1725
Epoch 4/10
10/10 - 14s - loss: 512.9005 - loglik: -5.1327e+02 - logprior: 0.3678
Epoch 5/10
10/10 - 14s - loss: 488.6649 - loglik: -4.8883e+02 - logprior: 0.1655
Epoch 6/10
10/10 - 14s - loss: 476.7730 - loglik: -4.7674e+02 - logprior: -3.4789e-02
Epoch 7/10
10/10 - 13s - loss: 473.2845 - loglik: -4.7311e+02 - logprior: -1.7702e-01
Epoch 8/10
10/10 - 13s - loss: 468.8722 - loglik: -4.6890e+02 - logprior: 0.0300
Epoch 9/10
10/10 - 12s - loss: 467.6732 - loglik: -4.6767e+02 - logprior: -2.3609e-03
Epoch 10/10
10/10 - 13s - loss: 469.4574 - loglik: -4.6961e+02 - logprior: 0.1513
Fitted a model with MAP estimate = -467.4436
expansions: [(23, 1), (25, 4), (39, 1), (41, 1), (43, 1), (50, 1), (52, 1), (54, 2), (55, 2), (70, 5), (71, 3), (72, 2), (73, 1), (74, 1), (76, 1), (77, 1), (94, 1), (101, 1), (102, 5), (132, 2), (133, 2), (134, 4), (135, 2), (137, 1), (157, 1), (158, 6), (174, 8), (197, 1)]
discards: [  1   2   6 200 223]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 282 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 17s - loss: 470.0522 - loglik: -4.5365e+02 - logprior: -1.6406e+01
Epoch 2/2
15/15 - 14s - loss: 434.7623 - loglik: -4.3517e+02 - logprior: 0.4052
Fitted a model with MAP estimate = -432.2534
expansions: [(0, 2), (61, 1), (202, 1)]
discards: [ 85 167 230 231 232]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 281 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 17s - loss: 457.8709 - loglik: -4.3305e+02 - logprior: -2.4817e+01
Epoch 2/2
15/15 - 15s - loss: 434.4005 - loglik: -4.3230e+02 - logprior: -2.0990e+00
Fitted a model with MAP estimate = -429.3948
expansions: [(0, 2), (206, 1), (241, 4)]
discards: [  0   1 166]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 19s - loss: 448.7540 - loglik: -4.2971e+02 - logprior: -1.9049e+01
Epoch 2/10
15/15 - 17s - loss: 426.1773 - loglik: -4.2770e+02 - logprior: 1.5232
Epoch 3/10
15/15 - 17s - loss: 424.7589 - loglik: -4.2969e+02 - logprior: 4.9280
Epoch 4/10
15/15 - 17s - loss: 422.4645 - loglik: -4.2843e+02 - logprior: 5.9639
Epoch 5/10
15/15 - 17s - loss: 418.1767 - loglik: -4.2461e+02 - logprior: 6.4335
Epoch 6/10
15/15 - 15s - loss: 423.8014 - loglik: -4.3072e+02 - logprior: 6.9149
Fitted a model with MAP estimate = -420.5764
Time for alignment: 326.4370
Fitting a model of length 225 on 1502 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 14s - loss: 667.9098 - loglik: -6.4293e+02 - logprior: -2.4984e+01
Epoch 2/10
10/10 - 12s - loss: 609.1167 - loglik: -6.0557e+02 - logprior: -3.5501e+00
Epoch 3/10
10/10 - 11s - loss: 552.8153 - loglik: -5.5299e+02 - logprior: 0.1780
Epoch 4/10
10/10 - 11s - loss: 516.3595 - loglik: -5.1676e+02 - logprior: 0.3985
Epoch 5/10
10/10 - 10s - loss: 487.3758 - loglik: -4.8725e+02 - logprior: -1.2541e-01
Epoch 6/10
10/10 - 10s - loss: 476.2940 - loglik: -4.7585e+02 - logprior: -4.4105e-01
Epoch 7/10
10/10 - 10s - loss: 473.2933 - loglik: -4.7275e+02 - logprior: -5.4231e-01
Epoch 8/10
10/10 - 10s - loss: 468.8038 - loglik: -4.6844e+02 - logprior: -3.6635e-01
Epoch 9/10
10/10 - 10s - loss: 465.9173 - loglik: -4.6539e+02 - logprior: -5.2237e-01
Epoch 10/10
10/10 - 10s - loss: 467.4311 - loglik: -4.6701e+02 - logprior: -4.1739e-01
Fitted a model with MAP estimate = -466.1891
expansions: [(23, 1), (24, 2), (25, 2), (39, 2), (41, 1), (43, 1), (47, 1), (49, 1), (50, 2), (51, 2), (54, 1), (66, 1), (67, 5), (68, 2), (71, 1), (73, 1), (76, 2), (78, 1), (94, 1), (100, 1), (101, 1), (102, 3), (130, 1), (131, 2), (132, 1), (133, 1), (134, 2), (135, 3), (137, 1), (155, 1), (156, 1), (157, 6), (173, 8)]
discards: [  1   2 223]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 284 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 466.5828 - loglik: -4.4996e+02 - logprior: -1.6619e+01
Epoch 2/2
15/15 - 11s - loss: 437.1811 - loglik: -4.3737e+02 - logprior: 0.1912
Fitted a model with MAP estimate = -431.1731
expansions: [(0, 2), (58, 1), (66, 1), (205, 1)]
discards: [175]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 288 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 453.1546 - loglik: -4.2895e+02 - logprior: -2.4205e+01
Epoch 2/2
15/15 - 12s - loss: 428.0766 - loglik: -4.2649e+02 - logprior: -1.5902e+00
Fitted a model with MAP estimate = -424.1313
expansions: [(0, 2), (207, 2), (237, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 291 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 15s - loss: 444.1720 - loglik: -4.2571e+02 - logprior: -1.8462e+01
Epoch 2/10
15/15 - 12s - loss: 422.3392 - loglik: -4.2421e+02 - logprior: 1.8728
Epoch 3/10
15/15 - 12s - loss: 420.6936 - loglik: -4.2582e+02 - logprior: 5.1277
Epoch 4/10
15/15 - 13s - loss: 421.4546 - loglik: -4.2763e+02 - logprior: 6.1757
Fitted a model with MAP estimate = -418.8243
Time for alignment: 231.7662
Computed alignments with likelihoods: ['-422.1966', '-420.5764', '-418.8243']
Best model has likelihood: -418.8243
SP score = 0.9262
Training of 3 independent models on file il8.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cd877f40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cc8a6610>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 226.8165 - loglik: -1.8515e+02 - logprior: -4.1663e+01
Epoch 2/10
10/10 - 1s - loss: 180.0937 - loglik: -1.6891e+02 - logprior: -1.1188e+01
Epoch 3/10
10/10 - 1s - loss: 161.3473 - loglik: -1.5584e+02 - logprior: -5.5095e+00
Epoch 4/10
10/10 - 1s - loss: 151.7964 - loglik: -1.4832e+02 - logprior: -3.4753e+00
Epoch 5/10
10/10 - 1s - loss: 147.8249 - loglik: -1.4535e+02 - logprior: -2.4708e+00
Epoch 6/10
10/10 - 1s - loss: 145.9013 - loglik: -1.4394e+02 - logprior: -1.9630e+00
Epoch 7/10
10/10 - 1s - loss: 145.0269 - loglik: -1.4330e+02 - logprior: -1.7316e+00
Epoch 8/10
10/10 - 1s - loss: 144.3768 - loglik: -1.4286e+02 - logprior: -1.5121e+00
Epoch 9/10
10/10 - 1s - loss: 144.2299 - loglik: -1.4294e+02 - logprior: -1.2942e+00
Epoch 10/10
10/10 - 1s - loss: 143.9568 - loglik: -1.4283e+02 - logprior: -1.1284e+00
Fitted a model with MAP estimate = -143.9485
expansions: [(0, 2), (17, 1), (18, 3), (20, 1), (24, 1), (28, 1), (37, 1), (43, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 192.1726 - loglik: -1.3724e+02 - logprior: -5.4931e+01
Epoch 2/2
10/10 - 1s - loss: 149.9825 - loglik: -1.3324e+02 - logprior: -1.6746e+01
Fitted a model with MAP estimate = -142.7662
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 179.9577 - loglik: -1.3252e+02 - logprior: -4.7435e+01
Epoch 2/2
10/10 - 1s - loss: 151.8976 - loglik: -1.3342e+02 - logprior: -1.8481e+01
Fitted a model with MAP estimate = -147.0245
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 174.9453 - loglik: -1.3232e+02 - logprior: -4.2629e+01
Epoch 2/10
10/10 - 1s - loss: 144.2586 - loglik: -1.3264e+02 - logprior: -1.1616e+01
Epoch 3/10
10/10 - 1s - loss: 137.4084 - loglik: -1.3286e+02 - logprior: -4.5449e+00
Epoch 4/10
10/10 - 1s - loss: 135.1460 - loglik: -1.3314e+02 - logprior: -2.0029e+00
Epoch 5/10
10/10 - 1s - loss: 134.4522 - loglik: -1.3371e+02 - logprior: -7.3980e-01
Epoch 6/10
10/10 - 1s - loss: 133.5880 - loglik: -1.3349e+02 - logprior: -9.8033e-02
Epoch 7/10
10/10 - 1s - loss: 133.2590 - loglik: -1.3343e+02 - logprior: 0.1676
Epoch 8/10
10/10 - 1s - loss: 133.0973 - loglik: -1.3345e+02 - logprior: 0.3501
Epoch 9/10
10/10 - 1s - loss: 132.9988 - loglik: -1.3363e+02 - logprior: 0.6267
Epoch 10/10
10/10 - 1s - loss: 132.6586 - loglik: -1.3355e+02 - logprior: 0.8880
Fitted a model with MAP estimate = -132.6161
Time for alignment: 28.6232
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 226.7253 - loglik: -1.8506e+02 - logprior: -4.1662e+01
Epoch 2/10
10/10 - 1s - loss: 179.7329 - loglik: -1.6855e+02 - logprior: -1.1183e+01
Epoch 3/10
10/10 - 1s - loss: 160.6130 - loglik: -1.5512e+02 - logprior: -5.4947e+00
Epoch 4/10
10/10 - 1s - loss: 151.2079 - loglik: -1.4770e+02 - logprior: -3.5042e+00
Epoch 5/10
10/10 - 1s - loss: 147.7268 - loglik: -1.4524e+02 - logprior: -2.4889e+00
Epoch 6/10
10/10 - 1s - loss: 145.9858 - loglik: -1.4401e+02 - logprior: -1.9772e+00
Epoch 7/10
10/10 - 1s - loss: 145.1932 - loglik: -1.4348e+02 - logprior: -1.7138e+00
Epoch 8/10
10/10 - 1s - loss: 144.4634 - loglik: -1.4296e+02 - logprior: -1.5026e+00
Epoch 9/10
10/10 - 1s - loss: 144.1543 - loglik: -1.4284e+02 - logprior: -1.3120e+00
Epoch 10/10
10/10 - 1s - loss: 143.9332 - loglik: -1.4277e+02 - logprior: -1.1589e+00
Fitted a model with MAP estimate = -143.7538
expansions: [(0, 2), (1, 1), (17, 1), (18, 3), (20, 1), (21, 1), (28, 1), (38, 1), (43, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 192.0584 - loglik: -1.3722e+02 - logprior: -5.4834e+01
Epoch 2/2
10/10 - 1s - loss: 150.2757 - loglik: -1.3359e+02 - logprior: -1.6690e+01
Fitted a model with MAP estimate = -142.7393
expansions: [(19, 1)]
discards: [ 0 22]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 180.0237 - loglik: -1.3273e+02 - logprior: -4.7290e+01
Epoch 2/2
10/10 - 1s - loss: 151.4762 - loglik: -1.3310e+02 - logprior: -1.8375e+01
Fitted a model with MAP estimate = -146.7524
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 174.6302 - loglik: -1.3202e+02 - logprior: -4.2615e+01
Epoch 2/10
10/10 - 1s - loss: 143.8811 - loglik: -1.3233e+02 - logprior: -1.1556e+01
Epoch 3/10
10/10 - 1s - loss: 136.9010 - loglik: -1.3247e+02 - logprior: -4.4285e+00
Epoch 4/10
10/10 - 1s - loss: 134.8142 - loglik: -1.3294e+02 - logprior: -1.8704e+00
Epoch 5/10
10/10 - 1s - loss: 133.4522 - loglik: -1.3282e+02 - logprior: -6.3323e-01
Epoch 6/10
10/10 - 1s - loss: 133.1349 - loglik: -1.3307e+02 - logprior: -6.4354e-02
Epoch 7/10
10/10 - 1s - loss: 132.5356 - loglik: -1.3274e+02 - logprior: 0.2090
Epoch 8/10
10/10 - 1s - loss: 132.1810 - loglik: -1.3267e+02 - logprior: 0.4902
Epoch 9/10
10/10 - 1s - loss: 132.0628 - loglik: -1.3287e+02 - logprior: 0.8041
Epoch 10/10
10/10 - 1s - loss: 131.8163 - loglik: -1.3284e+02 - logprior: 1.0218
Fitted a model with MAP estimate = -131.8785
Time for alignment: 28.6530
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 226.7663 - loglik: -1.8510e+02 - logprior: -4.1663e+01
Epoch 2/10
10/10 - 1s - loss: 180.3763 - loglik: -1.6919e+02 - logprior: -1.1187e+01
Epoch 3/10
10/10 - 1s - loss: 162.1926 - loglik: -1.5669e+02 - logprior: -5.5018e+00
Epoch 4/10
10/10 - 1s - loss: 152.4280 - loglik: -1.4894e+02 - logprior: -3.4892e+00
Epoch 5/10
10/10 - 1s - loss: 147.8289 - loglik: -1.4537e+02 - logprior: -2.4558e+00
Epoch 6/10
10/10 - 1s - loss: 146.0755 - loglik: -1.4414e+02 - logprior: -1.9376e+00
Epoch 7/10
10/10 - 1s - loss: 145.0863 - loglik: -1.4337e+02 - logprior: -1.7162e+00
Epoch 8/10
10/10 - 1s - loss: 144.5208 - loglik: -1.4296e+02 - logprior: -1.5611e+00
Epoch 9/10
10/10 - 1s - loss: 144.2868 - loglik: -1.4296e+02 - logprior: -1.3230e+00
Epoch 10/10
10/10 - 1s - loss: 143.9849 - loglik: -1.4283e+02 - logprior: -1.1582e+00
Fitted a model with MAP estimate = -143.8538
expansions: [(0, 2), (17, 1), (19, 2), (20, 1), (21, 1), (27, 1), (35, 1), (43, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 192.3174 - loglik: -1.3737e+02 - logprior: -5.4950e+01
Epoch 2/2
10/10 - 1s - loss: 150.4152 - loglik: -1.3379e+02 - logprior: -1.6625e+01
Fitted a model with MAP estimate = -143.0977
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 180.3814 - loglik: -1.3294e+02 - logprior: -4.7444e+01
Epoch 2/2
10/10 - 1s - loss: 151.6420 - loglik: -1.3328e+02 - logprior: -1.8360e+01
Fitted a model with MAP estimate = -146.9052
expansions: [(18, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 174.9148 - loglik: -1.3244e+02 - logprior: -4.2472e+01
Epoch 2/10
10/10 - 1s - loss: 143.6328 - loglik: -1.3214e+02 - logprior: -1.1497e+01
Epoch 3/10
10/10 - 1s - loss: 137.0651 - loglik: -1.3262e+02 - logprior: -4.4463e+00
Epoch 4/10
10/10 - 1s - loss: 134.9111 - loglik: -1.3298e+02 - logprior: -1.9352e+00
Epoch 5/10
10/10 - 1s - loss: 133.9281 - loglik: -1.3325e+02 - logprior: -6.7348e-01
Epoch 6/10
10/10 - 1s - loss: 133.1967 - loglik: -1.3320e+02 - logprior: 0.0015
Epoch 7/10
10/10 - 1s - loss: 132.8283 - loglik: -1.3321e+02 - logprior: 0.3819
Epoch 8/10
10/10 - 1s - loss: 132.3508 - loglik: -1.3296e+02 - logprior: 0.6072
Epoch 9/10
10/10 - 1s - loss: 132.1393 - loglik: -1.3286e+02 - logprior: 0.7158
Epoch 10/10
10/10 - 1s - loss: 131.9831 - loglik: -1.3282e+02 - logprior: 0.8363
Fitted a model with MAP estimate = -131.7901
Time for alignment: 29.0189
Computed alignments with likelihoods: ['-132.6161', '-131.8785', '-131.7901']
Best model has likelihood: -131.7901
SP score = 0.9017
Training of 3 independent models on file cytb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9f7d2d3a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9f7a641f0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 250.5372 - loglik: -2.3734e+02 - logprior: -1.3193e+01
Epoch 2/10
11/11 - 1s - loss: 221.6882 - loglik: -2.1827e+02 - logprior: -3.4207e+00
Epoch 3/10
11/11 - 1s - loss: 201.9001 - loglik: -1.9979e+02 - logprior: -2.1070e+00
Epoch 4/10
11/11 - 1s - loss: 191.6439 - loglik: -1.8965e+02 - logprior: -1.9987e+00
Epoch 5/10
11/11 - 1s - loss: 186.5688 - loglik: -1.8459e+02 - logprior: -1.9836e+00
Epoch 6/10
11/11 - 1s - loss: 186.8013 - loglik: -1.8493e+02 - logprior: -1.8705e+00
Fitted a model with MAP estimate = -185.4004
expansions: [(8, 2), (9, 1), (10, 2), (11, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 1), (44, 1), (48, 1), (50, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 196.4991 - loglik: -1.8153e+02 - logprior: -1.4967e+01
Epoch 2/2
11/11 - 1s - loss: 182.7376 - loglik: -1.7649e+02 - logprior: -6.2523e+00
Fitted a model with MAP estimate = -179.5651
expansions: [(0, 2)]
discards: [ 0 15]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 185.2824 - loglik: -1.7350e+02 - logprior: -1.1785e+01
Epoch 2/2
11/11 - 1s - loss: 175.6156 - loglik: -1.7253e+02 - logprior: -3.0873e+00
Fitted a model with MAP estimate = -174.8861
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 188.4568 - loglik: -1.7448e+02 - logprior: -1.3973e+01
Epoch 2/10
11/11 - 1s - loss: 177.3456 - loglik: -1.7337e+02 - logprior: -3.9707e+00
Epoch 3/10
11/11 - 1s - loss: 175.6569 - loglik: -1.7353e+02 - logprior: -2.1271e+00
Epoch 4/10
11/11 - 1s - loss: 174.8690 - loglik: -1.7345e+02 - logprior: -1.4168e+00
Epoch 5/10
11/11 - 1s - loss: 174.7612 - loglik: -1.7374e+02 - logprior: -1.0178e+00
Epoch 6/10
11/11 - 1s - loss: 173.9973 - loglik: -1.7307e+02 - logprior: -9.2492e-01
Epoch 7/10
11/11 - 1s - loss: 174.6602 - loglik: -1.7388e+02 - logprior: -7.8511e-01
Fitted a model with MAP estimate = -174.2680
Time for alignment: 39.9522
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 250.7148 - loglik: -2.3752e+02 - logprior: -1.3196e+01
Epoch 2/10
11/11 - 1s - loss: 221.3432 - loglik: -2.1792e+02 - logprior: -3.4265e+00
Epoch 3/10
11/11 - 1s - loss: 198.7016 - loglik: -1.9655e+02 - logprior: -2.1514e+00
Epoch 4/10
11/11 - 1s - loss: 189.6898 - loglik: -1.8761e+02 - logprior: -2.0804e+00
Epoch 5/10
11/11 - 1s - loss: 186.6340 - loglik: -1.8461e+02 - logprior: -2.0281e+00
Epoch 6/10
11/11 - 1s - loss: 186.6631 - loglik: -1.8481e+02 - logprior: -1.8549e+00
Fitted a model with MAP estimate = -185.5952
expansions: [(8, 2), (9, 1), (10, 2), (12, 1), (24, 1), (25, 1), (32, 1), (35, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 196.0704 - loglik: -1.8111e+02 - logprior: -1.4957e+01
Epoch 2/2
11/11 - 1s - loss: 183.0582 - loglik: -1.7683e+02 - logprior: -6.2324e+00
Fitted a model with MAP estimate = -180.0412
expansions: [(0, 2)]
discards: [ 0 10]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 185.5601 - loglik: -1.7378e+02 - logprior: -1.1779e+01
Epoch 2/2
11/11 - 1s - loss: 176.3800 - loglik: -1.7330e+02 - logprior: -3.0828e+00
Fitted a model with MAP estimate = -175.0618
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 188.6226 - loglik: -1.7466e+02 - logprior: -1.3965e+01
Epoch 2/10
11/11 - 1s - loss: 178.1967 - loglik: -1.7424e+02 - logprior: -3.9518e+00
Epoch 3/10
11/11 - 1s - loss: 174.9838 - loglik: -1.7288e+02 - logprior: -2.1084e+00
Epoch 4/10
11/11 - 1s - loss: 175.4533 - loglik: -1.7405e+02 - logprior: -1.4057e+00
Fitted a model with MAP estimate = -174.8594
Time for alignment: 36.3445
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 250.6401 - loglik: -2.3744e+02 - logprior: -1.3197e+01
Epoch 2/10
11/11 - 1s - loss: 221.6449 - loglik: -2.1821e+02 - logprior: -3.4357e+00
Epoch 3/10
11/11 - 1s - loss: 199.4980 - loglik: -1.9736e+02 - logprior: -2.1411e+00
Epoch 4/10
11/11 - 1s - loss: 189.9390 - loglik: -1.8792e+02 - logprior: -2.0224e+00
Epoch 5/10
11/11 - 1s - loss: 186.8978 - loglik: -1.8493e+02 - logprior: -1.9697e+00
Epoch 6/10
11/11 - 1s - loss: 185.7892 - loglik: -1.8399e+02 - logprior: -1.8033e+00
Epoch 7/10
11/11 - 1s - loss: 185.5481 - loglik: -1.8390e+02 - logprior: -1.6517e+00
Epoch 8/10
11/11 - 1s - loss: 184.9535 - loglik: -1.8334e+02 - logprior: -1.6102e+00
Epoch 9/10
11/11 - 1s - loss: 185.1068 - loglik: -1.8348e+02 - logprior: -1.6314e+00
Fitted a model with MAP estimate = -184.4849
expansions: [(11, 5), (17, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 196.9271 - loglik: -1.8195e+02 - logprior: -1.4973e+01
Epoch 2/2
11/11 - 1s - loss: 181.7470 - loglik: -1.7543e+02 - logprior: -6.3136e+00
Fitted a model with MAP estimate = -180.0406
expansions: [(0, 2)]
discards: [ 0 12 43]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 185.4175 - loglik: -1.7367e+02 - logprior: -1.1751e+01
Epoch 2/2
11/11 - 1s - loss: 175.5601 - loglik: -1.7251e+02 - logprior: -3.0521e+00
Fitted a model with MAP estimate = -174.9681
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 188.2496 - loglik: -1.7432e+02 - logprior: -1.3931e+01
Epoch 2/10
11/11 - 1s - loss: 177.5624 - loglik: -1.7364e+02 - logprior: -3.9268e+00
Epoch 3/10
11/11 - 1s - loss: 176.4897 - loglik: -1.7441e+02 - logprior: -2.0805e+00
Epoch 4/10
11/11 - 1s - loss: 174.5455 - loglik: -1.7318e+02 - logprior: -1.3633e+00
Epoch 5/10
11/11 - 1s - loss: 174.5300 - loglik: -1.7356e+02 - logprior: -9.7258e-01
Epoch 6/10
11/11 - 1s - loss: 175.1517 - loglik: -1.7429e+02 - logprior: -8.6247e-01
Fitted a model with MAP estimate = -174.5700
Time for alignment: 41.3088
Computed alignments with likelihoods: ['-174.2680', '-174.8594', '-174.5700']
Best model has likelihood: -174.2680
SP score = 0.8243
Training of 3 independent models on file kringle.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbbad238df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbba4ecfd60>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 264.3705 - loglik: -2.2373e+02 - logprior: -4.0644e+01
Epoch 2/10
10/10 - 1s - loss: 208.5756 - loglik: -1.9781e+02 - logprior: -1.0763e+01
Epoch 3/10
10/10 - 1s - loss: 177.7558 - loglik: -1.7229e+02 - logprior: -5.4634e+00
Epoch 4/10
10/10 - 1s - loss: 156.0171 - loglik: -1.5222e+02 - logprior: -3.8015e+00
Epoch 5/10
10/10 - 1s - loss: 147.6622 - loglik: -1.4456e+02 - logprior: -3.1038e+00
Epoch 6/10
10/10 - 1s - loss: 145.4610 - loglik: -1.4286e+02 - logprior: -2.5978e+00
Epoch 7/10
10/10 - 1s - loss: 144.0278 - loglik: -1.4177e+02 - logprior: -2.2560e+00
Epoch 8/10
10/10 - 1s - loss: 143.0927 - loglik: -1.4102e+02 - logprior: -2.0709e+00
Epoch 9/10
10/10 - 1s - loss: 142.7751 - loglik: -1.4081e+02 - logprior: -1.9626e+00
Epoch 10/10
10/10 - 1s - loss: 142.4575 - loglik: -1.4058e+02 - logprior: -1.8788e+00
Fitted a model with MAP estimate = -142.3889
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 169.1140 - loglik: -1.3233e+02 - logprior: -3.6781e+01
Epoch 2/2
10/10 - 1s - loss: 136.6069 - loglik: -1.2704e+02 - logprior: -9.5651e+00
Fitted a model with MAP estimate = -131.8155
expansions: []
discards: [ 0 46 59 66]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 175.5673 - loglik: -1.3038e+02 - logprior: -4.5191e+01
Epoch 2/2
10/10 - 1s - loss: 148.6245 - loglik: -1.3018e+02 - logprior: -1.8447e+01
Fitted a model with MAP estimate = -144.7034
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 163.6269 - loglik: -1.2737e+02 - logprior: -3.6253e+01
Epoch 2/10
10/10 - 1s - loss: 135.1747 - loglik: -1.2616e+02 - logprior: -9.0148e+00
Epoch 3/10
10/10 - 1s - loss: 129.6542 - loglik: -1.2618e+02 - logprior: -3.4740e+00
Epoch 4/10
10/10 - 1s - loss: 127.7363 - loglik: -1.2648e+02 - logprior: -1.2557e+00
Epoch 5/10
10/10 - 1s - loss: 126.8438 - loglik: -1.2674e+02 - logprior: -1.0827e-01
Epoch 6/10
10/10 - 1s - loss: 126.1802 - loglik: -1.2669e+02 - logprior: 0.5115
Epoch 7/10
10/10 - 1s - loss: 125.8934 - loglik: -1.2675e+02 - logprior: 0.8574
Epoch 8/10
10/10 - 1s - loss: 125.7556 - loglik: -1.2686e+02 - logprior: 1.1027
Epoch 9/10
10/10 - 1s - loss: 125.7167 - loglik: -1.2702e+02 - logprior: 1.3082
Epoch 10/10
10/10 - 1s - loss: 125.5748 - loglik: -1.2706e+02 - logprior: 1.4821
Fitted a model with MAP estimate = -125.4610
Time for alignment: 33.4787
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 264.3485 - loglik: -2.2370e+02 - logprior: -4.0646e+01
Epoch 2/10
10/10 - 1s - loss: 208.4036 - loglik: -1.9764e+02 - logprior: -1.0759e+01
Epoch 3/10
10/10 - 1s - loss: 177.2402 - loglik: -1.7182e+02 - logprior: -5.4216e+00
Epoch 4/10
10/10 - 1s - loss: 156.8410 - loglik: -1.5313e+02 - logprior: -3.7106e+00
Epoch 5/10
10/10 - 1s - loss: 148.2128 - loglik: -1.4526e+02 - logprior: -2.9523e+00
Epoch 6/10
10/10 - 1s - loss: 145.7628 - loglik: -1.4322e+02 - logprior: -2.5399e+00
Epoch 7/10
10/10 - 1s - loss: 144.3595 - loglik: -1.4214e+02 - logprior: -2.2180e+00
Epoch 8/10
10/10 - 1s - loss: 144.0134 - loglik: -1.4198e+02 - logprior: -2.0332e+00
Epoch 9/10
10/10 - 1s - loss: 142.9910 - loglik: -1.4109e+02 - logprior: -1.9005e+00
Epoch 10/10
10/10 - 1s - loss: 142.9826 - loglik: -1.4116e+02 - logprior: -1.8267e+00
Fitted a model with MAP estimate = -142.6500
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 1), (32, 2), (34, 2), (44, 2), (51, 2), (52, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 169.6111 - loglik: -1.3280e+02 - logprior: -3.6813e+01
Epoch 2/2
10/10 - 1s - loss: 136.4608 - loglik: -1.2689e+02 - logprior: -9.5727e+00
Fitted a model with MAP estimate = -131.8564
expansions: []
discards: [ 0 46 59 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 175.7585 - loglik: -1.3052e+02 - logprior: -4.5240e+01
Epoch 2/2
10/10 - 1s - loss: 148.7885 - loglik: -1.3033e+02 - logprior: -1.8456e+01
Fitted a model with MAP estimate = -144.7905
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 163.7247 - loglik: -1.2737e+02 - logprior: -3.6354e+01
Epoch 2/10
10/10 - 1s - loss: 135.2361 - loglik: -1.2619e+02 - logprior: -9.0434e+00
Epoch 3/10
10/10 - 1s - loss: 129.9145 - loglik: -1.2642e+02 - logprior: -3.4931e+00
Epoch 4/10
10/10 - 1s - loss: 127.6029 - loglik: -1.2633e+02 - logprior: -1.2700e+00
Epoch 5/10
10/10 - 1s - loss: 126.7164 - loglik: -1.2660e+02 - logprior: -1.2100e-01
Epoch 6/10
10/10 - 1s - loss: 126.3565 - loglik: -1.2686e+02 - logprior: 0.5000
Epoch 7/10
10/10 - 1s - loss: 126.2220 - loglik: -1.2707e+02 - logprior: 0.8486
Epoch 8/10
10/10 - 1s - loss: 126.1095 - loglik: -1.2720e+02 - logprior: 1.0955
Epoch 9/10
10/10 - 1s - loss: 125.5487 - loglik: -1.2685e+02 - logprior: 1.2979
Epoch 10/10
10/10 - 1s - loss: 125.5883 - loglik: -1.2706e+02 - logprior: 1.4692
Fitted a model with MAP estimate = -125.5222
Time for alignment: 31.4397
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 264.4864 - loglik: -2.2384e+02 - logprior: -4.0643e+01
Epoch 2/10
10/10 - 1s - loss: 208.6259 - loglik: -1.9787e+02 - logprior: -1.0756e+01
Epoch 3/10
10/10 - 1s - loss: 179.1829 - loglik: -1.7371e+02 - logprior: -5.4698e+00
Epoch 4/10
10/10 - 1s - loss: 157.6123 - loglik: -1.5383e+02 - logprior: -3.7812e+00
Epoch 5/10
10/10 - 1s - loss: 148.4001 - loglik: -1.4532e+02 - logprior: -3.0795e+00
Epoch 6/10
10/10 - 1s - loss: 146.1408 - loglik: -1.4355e+02 - logprior: -2.5890e+00
Epoch 7/10
10/10 - 1s - loss: 144.4972 - loglik: -1.4225e+02 - logprior: -2.2449e+00
Epoch 8/10
10/10 - 1s - loss: 143.6450 - loglik: -1.4157e+02 - logprior: -2.0732e+00
Epoch 9/10
10/10 - 1s - loss: 143.4559 - loglik: -1.4150e+02 - logprior: -1.9558e+00
Epoch 10/10
10/10 - 1s - loss: 143.1989 - loglik: -1.4133e+02 - logprior: -1.8651e+00
Fitted a model with MAP estimate = -143.0530
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (51, 2), (52, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 169.7103 - loglik: -1.3292e+02 - logprior: -3.6786e+01
Epoch 2/2
10/10 - 1s - loss: 136.7802 - loglik: -1.2720e+02 - logprior: -9.5760e+00
Fitted a model with MAP estimate = -131.8585
expansions: []
discards: [ 0 46 59 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 175.8268 - loglik: -1.3061e+02 - logprior: -4.5214e+01
Epoch 2/2
10/10 - 1s - loss: 148.5251 - loglik: -1.3007e+02 - logprior: -1.8452e+01
Fitted a model with MAP estimate = -144.7553
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 163.4258 - loglik: -1.2709e+02 - logprior: -3.6331e+01
Epoch 2/10
10/10 - 1s - loss: 135.6463 - loglik: -1.2662e+02 - logprior: -9.0296e+00
Epoch 3/10
10/10 - 1s - loss: 129.5889 - loglik: -1.2610e+02 - logprior: -3.4888e+00
Epoch 4/10
10/10 - 1s - loss: 127.5651 - loglik: -1.2630e+02 - logprior: -1.2647e+00
Epoch 5/10
10/10 - 1s - loss: 126.8206 - loglik: -1.2670e+02 - logprior: -1.2004e-01
Epoch 6/10
10/10 - 1s - loss: 126.3747 - loglik: -1.2688e+02 - logprior: 0.5019
Epoch 7/10
10/10 - 1s - loss: 126.1083 - loglik: -1.2695e+02 - logprior: 0.8456
Epoch 8/10
10/10 - 1s - loss: 125.8483 - loglik: -1.2694e+02 - logprior: 1.0893
Epoch 9/10
10/10 - 1s - loss: 125.5958 - loglik: -1.2689e+02 - logprior: 1.2934
Epoch 10/10
10/10 - 1s - loss: 125.6757 - loglik: -1.2714e+02 - logprior: 1.4668
Fitted a model with MAP estimate = -125.4921
Time for alignment: 32.3341
Computed alignments with likelihoods: ['-125.4610', '-125.5222', '-125.4921']
Best model has likelihood: -125.4610
SP score = 0.8540
Training of 3 independent models on file LIM.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c2a172e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9f74501c0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.4100 - loglik: -1.6679e+02 - logprior: -5.6174e+00
Epoch 2/10
15/15 - 1s - loss: 144.7516 - loglik: -1.4299e+02 - logprior: -1.7644e+00
Epoch 3/10
15/15 - 1s - loss: 130.6290 - loglik: -1.2883e+02 - logprior: -1.7966e+00
Epoch 4/10
15/15 - 1s - loss: 127.0279 - loglik: -1.2528e+02 - logprior: -1.7489e+00
Epoch 5/10
15/15 - 1s - loss: 126.2455 - loglik: -1.2458e+02 - logprior: -1.6612e+00
Epoch 6/10
15/15 - 1s - loss: 125.9228 - loglik: -1.2424e+02 - logprior: -1.6817e+00
Epoch 7/10
15/15 - 1s - loss: 125.5602 - loglik: -1.2391e+02 - logprior: -1.6542e+00
Epoch 8/10
15/15 - 1s - loss: 125.4051 - loglik: -1.2377e+02 - logprior: -1.6319e+00
Epoch 9/10
15/15 - 1s - loss: 125.5077 - loglik: -1.2388e+02 - logprior: -1.6232e+00
Fitted a model with MAP estimate = -125.3910
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (25, 2), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 135.7839 - loglik: -1.2887e+02 - logprior: -6.9146e+00
Epoch 2/2
15/15 - 1s - loss: 126.6361 - loglik: -1.2321e+02 - logprior: -3.4289e+00
Fitted a model with MAP estimate = -124.9616
expansions: [(0, 1)]
discards: [ 0 12 15 31 46]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 3s - loss: 128.0558 - loglik: -1.2284e+02 - logprior: -5.2155e+00
Epoch 2/2
15/15 - 1s - loss: 122.1128 - loglik: -1.2014e+02 - logprior: -1.9745e+00
Fitted a model with MAP estimate = -121.1353
expansions: []
discards: [ 9 36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 125.8480 - loglik: -1.2049e+02 - logprior: -5.3533e+00
Epoch 2/10
15/15 - 1s - loss: 121.9536 - loglik: -1.2004e+02 - logprior: -1.9143e+00
Epoch 3/10
15/15 - 1s - loss: 121.1711 - loglik: -1.1970e+02 - logprior: -1.4721e+00
Epoch 4/10
15/15 - 1s - loss: 121.3205 - loglik: -1.2000e+02 - logprior: -1.3155e+00
Fitted a model with MAP estimate = -120.9702
Time for alignment: 32.4186
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 172.3361 - loglik: -1.6672e+02 - logprior: -5.6204e+00
Epoch 2/10
15/15 - 1s - loss: 144.5720 - loglik: -1.4281e+02 - logprior: -1.7664e+00
Epoch 3/10
15/15 - 1s - loss: 130.7638 - loglik: -1.2897e+02 - logprior: -1.7965e+00
Epoch 4/10
15/15 - 1s - loss: 127.0286 - loglik: -1.2528e+02 - logprior: -1.7502e+00
Epoch 5/10
15/15 - 1s - loss: 125.9479 - loglik: -1.2427e+02 - logprior: -1.6742e+00
Epoch 6/10
15/15 - 1s - loss: 125.8610 - loglik: -1.2418e+02 - logprior: -1.6834e+00
Epoch 7/10
15/15 - 1s - loss: 125.4636 - loglik: -1.2381e+02 - logprior: -1.6560e+00
Epoch 8/10
15/15 - 1s - loss: 125.6476 - loglik: -1.2401e+02 - logprior: -1.6345e+00
Fitted a model with MAP estimate = -125.4827
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (25, 2), (27, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 135.7787 - loglik: -1.2886e+02 - logprior: -6.9181e+00
Epoch 2/2
15/15 - 1s - loss: 126.6789 - loglik: -1.2325e+02 - logprior: -3.4254e+00
Fitted a model with MAP estimate = -124.7560
expansions: []
discards: [12 15 31 46]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 3s - loss: 127.5195 - loglik: -1.2154e+02 - logprior: -5.9757e+00
Epoch 2/2
15/15 - 1s - loss: 121.7996 - loglik: -1.1979e+02 - logprior: -2.0127e+00
Fitted a model with MAP estimate = -121.0684
expansions: []
discards: [ 9 36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 125.8200 - loglik: -1.2046e+02 - logprior: -5.3565e+00
Epoch 2/10
15/15 - 1s - loss: 122.0571 - loglik: -1.2014e+02 - logprior: -1.9185e+00
Epoch 3/10
15/15 - 1s - loss: 121.2157 - loglik: -1.1975e+02 - logprior: -1.4673e+00
Epoch 4/10
15/15 - 1s - loss: 121.0867 - loglik: -1.1977e+02 - logprior: -1.3189e+00
Epoch 5/10
15/15 - 1s - loss: 120.9727 - loglik: -1.1971e+02 - logprior: -1.2598e+00
Epoch 6/10
15/15 - 1s - loss: 120.9716 - loglik: -1.1975e+02 - logprior: -1.2202e+00
Epoch 7/10
15/15 - 1s - loss: 120.9120 - loglik: -1.1971e+02 - logprior: -1.2045e+00
Epoch 8/10
15/15 - 1s - loss: 120.9682 - loglik: -1.1979e+02 - logprior: -1.1790e+00
Fitted a model with MAP estimate = -120.8266
Time for alignment: 34.0081
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 172.4370 - loglik: -1.6682e+02 - logprior: -5.6183e+00
Epoch 2/10
15/15 - 1s - loss: 144.7896 - loglik: -1.4303e+02 - logprior: -1.7614e+00
Epoch 3/10
15/15 - 1s - loss: 130.9408 - loglik: -1.2915e+02 - logprior: -1.7904e+00
Epoch 4/10
15/15 - 1s - loss: 127.4476 - loglik: -1.2571e+02 - logprior: -1.7385e+00
Epoch 5/10
15/15 - 1s - loss: 126.2755 - loglik: -1.2461e+02 - logprior: -1.6608e+00
Epoch 6/10
15/15 - 1s - loss: 125.8361 - loglik: -1.2416e+02 - logprior: -1.6803e+00
Epoch 7/10
15/15 - 1s - loss: 125.6847 - loglik: -1.2403e+02 - logprior: -1.6541e+00
Epoch 8/10
15/15 - 1s - loss: 125.5436 - loglik: -1.2391e+02 - logprior: -1.6355e+00
Epoch 9/10
15/15 - 1s - loss: 125.5098 - loglik: -1.2388e+02 - logprior: -1.6275e+00
Epoch 10/10
15/15 - 1s - loss: 125.4336 - loglik: -1.2382e+02 - logprior: -1.6169e+00
Fitted a model with MAP estimate = -125.3943
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (25, 2), (27, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 135.7494 - loglik: -1.2884e+02 - logprior: -6.9130e+00
Epoch 2/2
15/15 - 1s - loss: 126.9162 - loglik: -1.2347e+02 - logprior: -3.4424e+00
Fitted a model with MAP estimate = -125.1206
expansions: [(0, 2)]
discards: [ 0 12 15 31 46]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 3s - loss: 126.9194 - loglik: -1.2180e+02 - logprior: -5.1189e+00
Epoch 2/2
15/15 - 1s - loss: 121.2400 - loglik: -1.1957e+02 - logprior: -1.6685e+00
Fitted a model with MAP estimate = -120.4162
expansions: []
discards: [ 0 10 37]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 129.4169 - loglik: -1.2294e+02 - logprior: -6.4744e+00
Epoch 2/10
15/15 - 1s - loss: 122.5752 - loglik: -1.2042e+02 - logprior: -2.1514e+00
Epoch 3/10
15/15 - 1s - loss: 121.3239 - loglik: -1.1984e+02 - logprior: -1.4859e+00
Epoch 4/10
15/15 - 1s - loss: 121.2474 - loglik: -1.1993e+02 - logprior: -1.3171e+00
Epoch 5/10
15/15 - 1s - loss: 120.9745 - loglik: -1.1973e+02 - logprior: -1.2474e+00
Epoch 6/10
15/15 - 1s - loss: 120.9965 - loglik: -1.1978e+02 - logprior: -1.2213e+00
Fitted a model with MAP estimate = -120.8819
Time for alignment: 33.3729
Computed alignments with likelihoods: ['-120.9702', '-120.8266', '-120.4162']
Best model has likelihood: -120.4162
SP score = 0.9790
Training of 3 independent models on file annexin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cb54d5b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbb947c4940>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 197.2527 - loglik: -1.8292e+02 - logprior: -1.4330e+01
Epoch 2/10
10/10 - 2s - loss: 171.2321 - loglik: -1.6715e+02 - logprior: -4.0853e+00
Epoch 3/10
10/10 - 2s - loss: 153.5270 - loglik: -1.5107e+02 - logprior: -2.4609e+00
Epoch 4/10
10/10 - 2s - loss: 141.7749 - loglik: -1.3955e+02 - logprior: -2.2230e+00
Epoch 5/10
10/10 - 1s - loss: 136.8472 - loglik: -1.3458e+02 - logprior: -2.2635e+00
Epoch 6/10
10/10 - 2s - loss: 134.7535 - loglik: -1.3244e+02 - logprior: -2.3148e+00
Epoch 7/10
10/10 - 2s - loss: 133.4092 - loglik: -1.3116e+02 - logprior: -2.2528e+00
Epoch 8/10
10/10 - 2s - loss: 132.7086 - loglik: -1.3056e+02 - logprior: -2.1535e+00
Epoch 9/10
10/10 - 2s - loss: 132.4512 - loglik: -1.3035e+02 - logprior: -2.1018e+00
Epoch 10/10
10/10 - 2s - loss: 132.2297 - loglik: -1.3013e+02 - logprior: -2.0986e+00
Fitted a model with MAP estimate = -132.3589
expansions: [(4, 2), (5, 2), (6, 1), (7, 1), (8, 2), (24, 1), (30, 1), (32, 1), (33, 1), (38, 1), (39, 1), (40, 2), (47, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 145.5089 - loglik: -1.2948e+02 - logprior: -1.6025e+01
Epoch 2/2
10/10 - 2s - loss: 128.0076 - loglik: -1.2111e+02 - logprior: -6.8993e+00
Fitted a model with MAP estimate = -124.7919
expansions: [(0, 2)]
discards: [ 0  9 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 130.1588 - loglik: -1.1744e+02 - logprior: -1.2717e+01
Epoch 2/2
10/10 - 2s - loss: 120.6849 - loglik: -1.1716e+02 - logprior: -3.5204e+00
Fitted a model with MAP estimate = -118.9866
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 133.1954 - loglik: -1.1819e+02 - logprior: -1.5005e+01
Epoch 2/10
10/10 - 1s - loss: 122.6588 - loglik: -1.1799e+02 - logprior: -4.6722e+00
Epoch 3/10
10/10 - 1s - loss: 119.8465 - loglik: -1.1730e+02 - logprior: -2.5416e+00
Epoch 4/10
10/10 - 1s - loss: 118.5542 - loglik: -1.1679e+02 - logprior: -1.7610e+00
Epoch 5/10
10/10 - 1s - loss: 118.7999 - loglik: -1.1754e+02 - logprior: -1.2641e+00
Fitted a model with MAP estimate = -118.4503
Time for alignment: 47.8108
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 197.7599 - loglik: -1.8343e+02 - logprior: -1.4329e+01
Epoch 2/10
10/10 - 2s - loss: 170.3125 - loglik: -1.6624e+02 - logprior: -4.0746e+00
Epoch 3/10
10/10 - 1s - loss: 153.1423 - loglik: -1.5069e+02 - logprior: -2.4486e+00
Epoch 4/10
10/10 - 2s - loss: 141.2879 - loglik: -1.3904e+02 - logprior: -2.2496e+00
Epoch 5/10
10/10 - 2s - loss: 137.3981 - loglik: -1.3505e+02 - logprior: -2.3523e+00
Epoch 6/10
10/10 - 2s - loss: 134.3326 - loglik: -1.3190e+02 - logprior: -2.4302e+00
Epoch 7/10
10/10 - 1s - loss: 132.5027 - loglik: -1.3011e+02 - logprior: -2.3903e+00
Epoch 8/10
10/10 - 2s - loss: 132.2919 - loglik: -1.2999e+02 - logprior: -2.3022e+00
Epoch 9/10
10/10 - 2s - loss: 131.9647 - loglik: -1.2972e+02 - logprior: -2.2431e+00
Epoch 10/10
10/10 - 2s - loss: 131.7963 - loglik: -1.2957e+02 - logprior: -2.2264e+00
Fitted a model with MAP estimate = -131.7115
expansions: [(4, 2), (5, 1), (6, 1), (7, 2), (8, 2), (24, 1), (30, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 145.8333 - loglik: -1.2980e+02 - logprior: -1.6030e+01
Epoch 2/2
10/10 - 2s - loss: 127.7594 - loglik: -1.2088e+02 - logprior: -6.8795e+00
Fitted a model with MAP estimate = -124.5757
expansions: [(0, 2)]
discards: [ 0 10 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 129.6655 - loglik: -1.1697e+02 - logprior: -1.2692e+01
Epoch 2/2
10/10 - 1s - loss: 120.4554 - loglik: -1.1694e+02 - logprior: -3.5153e+00
Fitted a model with MAP estimate = -118.9143
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 133.0363 - loglik: -1.1804e+02 - logprior: -1.5001e+01
Epoch 2/10
10/10 - 2s - loss: 122.5366 - loglik: -1.1787e+02 - logprior: -4.6619e+00
Epoch 3/10
10/10 - 2s - loss: 119.9043 - loglik: -1.1737e+02 - logprior: -2.5374e+00
Epoch 4/10
10/10 - 1s - loss: 118.6316 - loglik: -1.1687e+02 - logprior: -1.7659e+00
Epoch 5/10
10/10 - 1s - loss: 118.8116 - loglik: -1.1755e+02 - logprior: -1.2623e+00
Fitted a model with MAP estimate = -118.4479
Time for alignment: 48.5741
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 197.4120 - loglik: -1.8308e+02 - logprior: -1.4328e+01
Epoch 2/10
10/10 - 2s - loss: 170.4952 - loglik: -1.6643e+02 - logprior: -4.0686e+00
Epoch 3/10
10/10 - 2s - loss: 153.2975 - loglik: -1.5087e+02 - logprior: -2.4271e+00
Epoch 4/10
10/10 - 1s - loss: 141.7340 - loglik: -1.3954e+02 - logprior: -2.1944e+00
Epoch 5/10
10/10 - 2s - loss: 137.8951 - loglik: -1.3566e+02 - logprior: -2.2377e+00
Epoch 6/10
10/10 - 2s - loss: 135.6538 - loglik: -1.3340e+02 - logprior: -2.2513e+00
Epoch 7/10
10/10 - 2s - loss: 134.4963 - loglik: -1.3236e+02 - logprior: -2.1360e+00
Epoch 8/10
10/10 - 1s - loss: 134.0073 - loglik: -1.3200e+02 - logprior: -2.0084e+00
Epoch 9/10
10/10 - 1s - loss: 133.7127 - loglik: -1.3176e+02 - logprior: -1.9518e+00
Epoch 10/10
10/10 - 2s - loss: 133.2491 - loglik: -1.3129e+02 - logprior: -1.9581e+00
Fitted a model with MAP estimate = -133.1732
expansions: [(4, 2), (5, 2), (7, 2), (8, 2), (24, 1), (30, 1), (32, 1), (33, 1), (38, 3), (39, 1), (41, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 144.6443 - loglik: -1.2863e+02 - logprior: -1.6017e+01
Epoch 2/2
10/10 - 1s - loss: 127.4982 - loglik: -1.2062e+02 - logprior: -6.8755e+00
Fitted a model with MAP estimate = -124.3894
expansions: [(0, 2)]
discards: [ 0 10 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 129.4744 - loglik: -1.1678e+02 - logprior: -1.2697e+01
Epoch 2/2
10/10 - 2s - loss: 120.3039 - loglik: -1.1679e+02 - logprior: -3.5160e+00
Fitted a model with MAP estimate = -118.9217
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 133.1159 - loglik: -1.1810e+02 - logprior: -1.5014e+01
Epoch 2/10
10/10 - 2s - loss: 122.7590 - loglik: -1.1807e+02 - logprior: -4.6849e+00
Epoch 3/10
10/10 - 2s - loss: 119.6461 - loglik: -1.1710e+02 - logprior: -2.5413e+00
Epoch 4/10
10/10 - 1s - loss: 118.6553 - loglik: -1.1690e+02 - logprior: -1.7598e+00
Epoch 5/10
10/10 - 2s - loss: 119.0695 - loglik: -1.1781e+02 - logprior: -1.2630e+00
Fitted a model with MAP estimate = -118.4535
Time for alignment: 45.9022
Computed alignments with likelihoods: ['-118.4503', '-118.4479', '-118.4535']
Best model has likelihood: -118.4479
SP score = 0.9786
Training of 3 independent models on file hormone_rec.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb7ec533370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c2f495b0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 159 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 551.3705 - loglik: -5.4036e+02 - logprior: -1.1011e+01
Epoch 2/10
11/11 - 4s - loss: 507.8147 - loglik: -5.0593e+02 - logprior: -1.8891e+00
Epoch 3/10
11/11 - 4s - loss: 468.3519 - loglik: -4.6752e+02 - logprior: -8.2839e-01
Epoch 4/10
11/11 - 4s - loss: 448.4677 - loglik: -4.4767e+02 - logprior: -7.9467e-01
Epoch 5/10
11/11 - 4s - loss: 439.8807 - loglik: -4.3910e+02 - logprior: -7.7659e-01
Epoch 6/10
11/11 - 5s - loss: 433.7636 - loglik: -4.3311e+02 - logprior: -6.5357e-01
Epoch 7/10
11/11 - 5s - loss: 430.5009 - loglik: -4.2995e+02 - logprior: -5.5389e-01
Epoch 8/10
11/11 - 5s - loss: 429.3698 - loglik: -4.2887e+02 - logprior: -5.0323e-01
Epoch 9/10
11/11 - 5s - loss: 428.8887 - loglik: -4.2841e+02 - logprior: -4.8271e-01
Epoch 10/10
11/11 - 5s - loss: 427.1044 - loglik: -4.2661e+02 - logprior: -4.9396e-01
Fitted a model with MAP estimate = -427.0505
expansions: [(0, 8), (9, 3), (15, 1), (31, 1), (45, 1), (56, 2), (57, 2), (58, 2), (70, 1), (71, 1), (72, 2), (79, 2), (80, 1), (83, 1), (107, 1), (114, 1), (120, 1), (121, 1), (122, 1), (124, 1), (127, 1), (131, 1), (149, 1), (150, 2), (159, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 203 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 10s - loss: 436.5695 - loglik: -4.2311e+02 - logprior: -1.3464e+01
Epoch 2/2
11/11 - 7s - loss: 415.4477 - loglik: -4.1219e+02 - logprior: -3.2587e+00
Fitted a model with MAP estimate = -412.8999
expansions: [(0, 5)]
discards: [  1   2   3   4   5  76 104 198 199 200 201 202]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 196 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 9s - loss: 425.9176 - loglik: -4.1340e+02 - logprior: -1.2517e+01
Epoch 2/2
11/11 - 6s - loss: 412.8286 - loglik: -4.0957e+02 - logprior: -3.2561e+00
Fitted a model with MAP estimate = -411.9838
expansions: [(0, 5), (17, 2), (196, 5)]
discards: [ 0  5 93]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 205 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 10s - loss: 421.8678 - loglik: -4.1086e+02 - logprior: -1.1006e+01
Epoch 2/10
11/11 - 7s - loss: 410.7888 - loglik: -4.0874e+02 - logprior: -2.0498e+00
Epoch 3/10
11/11 - 7s - loss: 409.4008 - loglik: -4.0898e+02 - logprior: -4.2546e-01
Epoch 4/10
11/11 - 7s - loss: 407.3188 - loglik: -4.0731e+02 - logprior: -4.2900e-03
Epoch 5/10
11/11 - 8s - loss: 408.0258 - loglik: -4.0834e+02 - logprior: 0.3145
Fitted a model with MAP estimate = -407.3153
Time for alignment: 143.1445
Fitting a model of length 159 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 8s - loss: 551.2500 - loglik: -5.4022e+02 - logprior: -1.1032e+01
Epoch 2/10
11/11 - 5s - loss: 509.3505 - loglik: -5.0745e+02 - logprior: -1.8996e+00
Epoch 3/10
11/11 - 5s - loss: 469.1356 - loglik: -4.6830e+02 - logprior: -8.3241e-01
Epoch 4/10
11/11 - 5s - loss: 446.9167 - loglik: -4.4611e+02 - logprior: -8.0633e-01
Epoch 5/10
11/11 - 5s - loss: 438.4356 - loglik: -4.3766e+02 - logprior: -7.8041e-01
Epoch 6/10
11/11 - 6s - loss: 433.4105 - loglik: -4.3276e+02 - logprior: -6.4768e-01
Epoch 7/10
11/11 - 5s - loss: 430.0887 - loglik: -4.2953e+02 - logprior: -5.6363e-01
Epoch 8/10
11/11 - 6s - loss: 429.4966 - loglik: -4.2900e+02 - logprior: -4.9468e-01
Epoch 9/10
11/11 - 6s - loss: 427.4398 - loglik: -4.2700e+02 - logprior: -4.4261e-01
Epoch 10/10
11/11 - 6s - loss: 428.0847 - loglik: -4.2764e+02 - logprior: -4.4856e-01
Fitted a model with MAP estimate = -427.5086
expansions: [(0, 9), (19, 1), (42, 1), (51, 1), (56, 2), (57, 1), (59, 2), (66, 1), (71, 2), (72, 1), (74, 1), (75, 2), (76, 1), (87, 1), (107, 1), (112, 2), (113, 1), (115, 1), (121, 1), (124, 1), (127, 1), (149, 2), (150, 1), (159, 6)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 202 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 11s - loss: 439.1684 - loglik: -4.2566e+02 - logprior: -1.3513e+01
Epoch 2/2
11/11 - 8s - loss: 417.9111 - loglik: -4.1452e+02 - logprior: -3.3903e+00
Fitted a model with MAP estimate = -415.1354
expansions: [(0, 5), (18, 2), (186, 1)]
discards: [  1   2   3   4  69  74  89 140 196 197 198 199 200 201]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 196 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 10s - loss: 427.4867 - loglik: -4.1500e+02 - logprior: -1.2492e+01
Epoch 2/2
11/11 - 8s - loss: 415.4364 - loglik: -4.1220e+02 - logprior: -3.2316e+00
Fitted a model with MAP estimate = -413.8009
expansions: [(196, 5)]
discards: [0 1 2 3 4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 196 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 11s - loss: 423.6983 - loglik: -4.1293e+02 - logprior: -1.0771e+01
Epoch 2/10
11/11 - 8s - loss: 412.2854 - loglik: -4.1037e+02 - logprior: -1.9204e+00
Epoch 3/10
11/11 - 8s - loss: 410.5700 - loglik: -4.1016e+02 - logprior: -4.0805e-01
Epoch 4/10
11/11 - 8s - loss: 410.4194 - loglik: -4.1045e+02 - logprior: 0.0329
Epoch 5/10
11/11 - 7s - loss: 410.4777 - loglik: -4.1093e+02 - logprior: 0.4526
Fitted a model with MAP estimate = -409.5070
Time for alignment: 160.5072
Fitting a model of length 159 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 8s - loss: 551.4519 - loglik: -5.4044e+02 - logprior: -1.1008e+01
Epoch 2/10
11/11 - 5s - loss: 507.6517 - loglik: -5.0577e+02 - logprior: -1.8843e+00
Epoch 3/10
11/11 - 5s - loss: 468.4464 - loglik: -4.6764e+02 - logprior: -8.0187e-01
Epoch 4/10
11/11 - 5s - loss: 446.5146 - loglik: -4.4565e+02 - logprior: -8.5998e-01
Epoch 5/10
11/11 - 5s - loss: 438.7439 - loglik: -4.3793e+02 - logprior: -8.1416e-01
Epoch 6/10
11/11 - 5s - loss: 433.0661 - loglik: -4.3239e+02 - logprior: -6.7291e-01
Epoch 7/10
11/11 - 5s - loss: 430.0760 - loglik: -4.2948e+02 - logprior: -5.9934e-01
Epoch 8/10
11/11 - 5s - loss: 428.2841 - loglik: -4.2775e+02 - logprior: -5.3743e-01
Epoch 9/10
11/11 - 6s - loss: 427.7889 - loglik: -4.2730e+02 - logprior: -4.8665e-01
Epoch 10/10
11/11 - 5s - loss: 427.6191 - loglik: -4.2714e+02 - logprior: -4.7804e-01
Fitted a model with MAP estimate = -426.9988
expansions: [(0, 8), (8, 4), (31, 1), (52, 1), (56, 1), (57, 2), (58, 2), (71, 1), (72, 1), (76, 2), (77, 1), (83, 1), (84, 1), (111, 1), (112, 2), (113, 1), (115, 1), (124, 1), (127, 1), (128, 1), (149, 2), (150, 1), (159, 6)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 202 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 11s - loss: 437.5739 - loglik: -4.2406e+02 - logprior: -1.3510e+01
Epoch 2/2
11/11 - 8s - loss: 418.8346 - loglik: -4.1550e+02 - logprior: -3.3350e+00
Fitted a model with MAP estimate = -414.3799
expansions: [(186, 1)]
discards: [  1   2   3   4   5  75  97 140 196 197 198 199 200 201]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 189 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 10s - loss: 424.1660 - loglik: -4.1419e+02 - logprior: -9.9788e+00
Epoch 2/2
11/11 - 7s - loss: 414.3808 - loglik: -4.1265e+02 - logprior: -1.7317e+00
Fitted a model with MAP estimate = -412.6791
expansions: [(0, 5), (12, 2), (189, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 201 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 10s - loss: 427.6797 - loglik: -4.1206e+02 - logprior: -1.5624e+01
Epoch 2/10
11/11 - 7s - loss: 414.6194 - loglik: -4.1049e+02 - logprior: -4.1249e+00
Epoch 3/10
11/11 - 7s - loss: 410.4913 - loglik: -4.0905e+02 - logprior: -1.4381e+00
Epoch 4/10
11/11 - 8s - loss: 408.8110 - loglik: -4.0842e+02 - logprior: -3.9489e-01
Epoch 5/10
11/11 - 8s - loss: 407.7466 - loglik: -4.0801e+02 - logprior: 0.2645
Epoch 6/10
11/11 - 8s - loss: 408.2065 - loglik: -4.0887e+02 - logprior: 0.6596
Fitted a model with MAP estimate = -407.8650
Time for alignment: 165.0519
Computed alignments with likelihoods: ['-407.3153', '-409.5070', '-407.8650']
Best model has likelihood: -407.3153
SP score = 0.6651
Training of 3 independent models on file Acetyltransf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbb82cd3730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fba10bc74f0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 6s - loss: 250.1178 - loglik: -2.4895e+02 - logprior: -1.1637e+00
Epoch 2/10
29/29 - 3s - loss: 225.1784 - loglik: -2.2433e+02 - logprior: -8.5044e-01
Epoch 3/10
29/29 - 3s - loss: 220.5726 - loglik: -2.1975e+02 - logprior: -8.2316e-01
Epoch 4/10
29/29 - 3s - loss: 219.0375 - loglik: -2.1821e+02 - logprior: -8.2883e-01
Epoch 5/10
29/29 - 3s - loss: 218.7915 - loglik: -2.1797e+02 - logprior: -8.2297e-01
Epoch 6/10
29/29 - 3s - loss: 218.1699 - loglik: -2.1735e+02 - logprior: -8.1529e-01
Epoch 7/10
29/29 - 3s - loss: 217.7773 - loglik: -2.1696e+02 - logprior: -8.1337e-01
Epoch 8/10
29/29 - 3s - loss: 216.9761 - loglik: -2.1616e+02 - logprior: -8.1215e-01
Epoch 9/10
29/29 - 3s - loss: 216.8915 - loglik: -2.1608e+02 - logprior: -8.1372e-01
Epoch 10/10
29/29 - 3s - loss: 216.9617 - loglik: -2.1615e+02 - logprior: -8.1099e-01
Fitted a model with MAP estimate = -203.6699
expansions: [(11, 1), (13, 3), (15, 1), (17, 2), (27, 2), (38, 1), (41, 1), (43, 1), (46, 2), (47, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 215.5227 - loglik: -2.1436e+02 - logprior: -1.1581e+00
Epoch 2/2
29/29 - 3s - loss: 212.1736 - loglik: -2.1139e+02 - logprior: -7.8845e-01
Fitted a model with MAP estimate = -201.9015
expansions: []
discards: [34 59 65]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 212.4601 - loglik: -2.1136e+02 - logprior: -1.1040e+00
Epoch 2/2
29/29 - 4s - loss: 212.1172 - loglik: -2.1139e+02 - logprior: -7.2487e-01
Fitted a model with MAP estimate = -203.1646
expansions: [(18, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 199.0043 - loglik: -1.9835e+02 - logprior: -6.5806e-01
Epoch 2/10
42/42 - 5s - loss: 196.3284 - loglik: -1.9585e+02 - logprior: -4.8149e-01
Epoch 3/10
42/42 - 5s - loss: 194.8084 - loglik: -1.9434e+02 - logprior: -4.7022e-01
Epoch 4/10
42/42 - 5s - loss: 194.9299 - loglik: -1.9447e+02 - logprior: -4.6374e-01
Fitted a model with MAP estimate = -194.1007
Time for alignment: 113.1423
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 5s - loss: 250.6199 - loglik: -2.4946e+02 - logprior: -1.1625e+00
Epoch 2/10
29/29 - 3s - loss: 225.4251 - loglik: -2.2458e+02 - logprior: -8.4091e-01
Epoch 3/10
29/29 - 3s - loss: 220.7053 - loglik: -2.1988e+02 - logprior: -8.2544e-01
Epoch 4/10
29/29 - 3s - loss: 219.5449 - loglik: -2.1872e+02 - logprior: -8.2721e-01
Epoch 5/10
29/29 - 3s - loss: 218.9575 - loglik: -2.1814e+02 - logprior: -8.1627e-01
Epoch 6/10
29/29 - 3s - loss: 218.1536 - loglik: -2.1735e+02 - logprior: -8.0716e-01
Epoch 7/10
29/29 - 3s - loss: 217.8917 - loglik: -2.1709e+02 - logprior: -8.0194e-01
Epoch 8/10
29/29 - 3s - loss: 217.1974 - loglik: -2.1640e+02 - logprior: -8.0148e-01
Epoch 9/10
29/29 - 3s - loss: 217.1258 - loglik: -2.1632e+02 - logprior: -8.0176e-01
Epoch 10/10
29/29 - 3s - loss: 217.3336 - loglik: -2.1653e+02 - logprior: -8.0125e-01
Fitted a model with MAP estimate = -203.8049
expansions: [(3, 1), (5, 1), (13, 3), (14, 2), (15, 1), (22, 1), (35, 1), (38, 2), (41, 1), (43, 1), (44, 2), (45, 1), (46, 1), (47, 1), (48, 2), (50, 1), (52, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 215.0754 - loglik: -2.1394e+02 - logprior: -1.1347e+00
Epoch 2/2
29/29 - 3s - loss: 211.5636 - loglik: -2.1081e+02 - logprior: -7.5765e-01
Fitted a model with MAP estimate = -201.6989
expansions: []
discards: [48 59 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 212.2660 - loglik: -2.1118e+02 - logprior: -1.0856e+00
Epoch 2/2
29/29 - 3s - loss: 211.7020 - loglik: -2.1100e+02 - logprior: -7.0678e-01
Fitted a model with MAP estimate = -203.2922
expansions: []
discards: [ 2 17]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 199.4841 - loglik: -1.9884e+02 - logprior: -6.4404e-01
Epoch 2/10
42/42 - 4s - loss: 196.2955 - loglik: -1.9581e+02 - logprior: -4.8332e-01
Epoch 3/10
42/42 - 4s - loss: 195.1448 - loglik: -1.9467e+02 - logprior: -4.7485e-01
Epoch 4/10
42/42 - 4s - loss: 195.0791 - loglik: -1.9461e+02 - logprior: -4.6615e-01
Epoch 5/10
42/42 - 4s - loss: 194.2372 - loglik: -1.9377e+02 - logprior: -4.6500e-01
Epoch 6/10
42/42 - 4s - loss: 193.8987 - loglik: -1.9344e+02 - logprior: -4.5868e-01
Epoch 7/10
42/42 - 4s - loss: 193.1297 - loglik: -1.9268e+02 - logprior: -4.5379e-01
Epoch 8/10
42/42 - 5s - loss: 193.1889 - loglik: -1.9274e+02 - logprior: -4.5006e-01
Fitted a model with MAP estimate = -192.7899
Time for alignment: 127.7844
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 5s - loss: 250.2876 - loglik: -2.4913e+02 - logprior: -1.1607e+00
Epoch 2/10
29/29 - 3s - loss: 224.9399 - loglik: -2.2411e+02 - logprior: -8.3182e-01
Epoch 3/10
29/29 - 3s - loss: 220.9807 - loglik: -2.2017e+02 - logprior: -8.1211e-01
Epoch 4/10
29/29 - 3s - loss: 219.9132 - loglik: -2.1910e+02 - logprior: -8.1048e-01
Epoch 5/10
29/29 - 3s - loss: 218.7788 - loglik: -2.1797e+02 - logprior: -8.1239e-01
Epoch 6/10
29/29 - 3s - loss: 218.2387 - loglik: -2.1743e+02 - logprior: -8.0396e-01
Epoch 7/10
29/29 - 3s - loss: 217.4210 - loglik: -2.1662e+02 - logprior: -8.0432e-01
Epoch 8/10
29/29 - 3s - loss: 217.7867 - loglik: -2.1699e+02 - logprior: -7.9998e-01
Fitted a model with MAP estimate = -202.3174
expansions: [(1, 1), (2, 1), (13, 3), (14, 3), (27, 2), (38, 1), (41, 1), (45, 2), (46, 2), (47, 1), (48, 1), (49, 2), (51, 1), (53, 1), (55, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 215.6502 - loglik: -2.1451e+02 - logprior: -1.1379e+00
Epoch 2/2
29/29 - 3s - loss: 211.9711 - loglik: -2.1120e+02 - logprior: -7.7189e-01
Fitted a model with MAP estimate = -200.7992
expansions: []
discards: [35 58 60 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 212.9538 - loglik: -2.1186e+02 - logprior: -1.0890e+00
Epoch 2/2
29/29 - 3s - loss: 211.5719 - loglik: -2.1086e+02 - logprior: -7.1107e-01
Fitted a model with MAP estimate = -201.8097
expansions: []
discards: [2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 198.4013 - loglik: -1.9776e+02 - logprior: -6.4368e-01
Epoch 2/10
42/42 - 4s - loss: 196.2228 - loglik: -1.9575e+02 - logprior: -4.7520e-01
Epoch 3/10
42/42 - 4s - loss: 195.0392 - loglik: -1.9457e+02 - logprior: -4.7125e-01
Epoch 4/10
42/42 - 5s - loss: 194.8316 - loglik: -1.9437e+02 - logprior: -4.6190e-01
Epoch 5/10
42/42 - 4s - loss: 194.3070 - loglik: -1.9385e+02 - logprior: -4.6023e-01
Epoch 6/10
42/42 - 4s - loss: 193.6680 - loglik: -1.9321e+02 - logprior: -4.5468e-01
Epoch 7/10
42/42 - 4s - loss: 193.5323 - loglik: -1.9308e+02 - logprior: -4.5187e-01
Epoch 8/10
42/42 - 5s - loss: 193.3016 - loglik: -1.9286e+02 - logprior: -4.4638e-01
Epoch 9/10
42/42 - 4s - loss: 192.6696 - loglik: -1.9223e+02 - logprior: -4.3977e-01
Epoch 10/10
42/42 - 4s - loss: 192.5077 - loglik: -1.9207e+02 - logprior: -4.3785e-01
Fitted a model with MAP estimate = -192.4962
Time for alignment: 128.9543
Computed alignments with likelihoods: ['-194.1007', '-192.7899', '-192.4962']
Best model has likelihood: -192.4962
SP score = 0.6105
Training of 3 independent models on file phoslip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fba10b8f430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbb93979f70>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 375.5763 - loglik: -3.3032e+02 - logprior: -4.5254e+01
Epoch 2/10
10/10 - 1s - loss: 298.5865 - loglik: -2.8777e+02 - logprior: -1.0816e+01
Epoch 3/10
10/10 - 1s - loss: 251.8776 - loglik: -2.4704e+02 - logprior: -4.8337e+00
Epoch 4/10
10/10 - 1s - loss: 224.8032 - loglik: -2.2192e+02 - logprior: -2.8809e+00
Epoch 5/10
10/10 - 1s - loss: 212.1613 - loglik: -2.1002e+02 - logprior: -2.1430e+00
Epoch 6/10
10/10 - 1s - loss: 207.5011 - loglik: -2.0567e+02 - logprior: -1.8291e+00
Epoch 7/10
10/10 - 1s - loss: 205.6727 - loglik: -2.0414e+02 - logprior: -1.5374e+00
Epoch 8/10
10/10 - 1s - loss: 204.3569 - loglik: -2.0315e+02 - logprior: -1.2102e+00
Epoch 9/10
10/10 - 1s - loss: 204.3226 - loglik: -2.0332e+02 - logprior: -1.0067e+00
Epoch 10/10
10/10 - 1s - loss: 203.8272 - loglik: -2.0291e+02 - logprior: -9.1372e-01
Fitted a model with MAP estimate = -203.5464
expansions: [(10, 2), (11, 2), (12, 2), (13, 2), (16, 1), (30, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (66, 1), (67, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 118 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 248.8315 - loglik: -1.9753e+02 - logprior: -5.1298e+01
Epoch 2/2
10/10 - 1s - loss: 207.5849 - loglik: -1.8740e+02 - logprior: -2.0186e+01
Fitted a model with MAP estimate = -200.4195
expansions: [(0, 2)]
discards: [ 0  9 18]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 221.7549 - loglik: -1.8134e+02 - logprior: -4.0411e+01
Epoch 2/2
10/10 - 1s - loss: 188.9336 - loglik: -1.7977e+02 - logprior: -9.1659e+00
Fitted a model with MAP estimate = -184.0855
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 116 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 229.6546 - loglik: -1.8108e+02 - logprior: -4.8574e+01
Epoch 2/10
10/10 - 1s - loss: 194.0137 - loglik: -1.8066e+02 - logprior: -1.3350e+01
Epoch 3/10
10/10 - 1s - loss: 183.9790 - loglik: -1.8041e+02 - logprior: -3.5680e+00
Epoch 4/10
10/10 - 1s - loss: 180.8459 - loglik: -1.8057e+02 - logprior: -2.7920e-01
Epoch 5/10
10/10 - 1s - loss: 179.1025 - loglik: -1.8023e+02 - logprior: 1.1275
Epoch 6/10
10/10 - 2s - loss: 178.0431 - loglik: -1.7996e+02 - logprior: 1.9178
Epoch 7/10
10/10 - 1s - loss: 177.5229 - loglik: -1.8010e+02 - logprior: 2.5816
Epoch 8/10
10/10 - 1s - loss: 176.8841 - loglik: -1.7999e+02 - logprior: 3.1062
Epoch 9/10
10/10 - 1s - loss: 176.7985 - loglik: -1.8025e+02 - logprior: 3.4505
Epoch 10/10
10/10 - 1s - loss: 176.0734 - loglik: -1.7980e+02 - logprior: 3.7250
Fitted a model with MAP estimate = -175.9300
Time for alignment: 49.2910
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 375.4407 - loglik: -3.3018e+02 - logprior: -4.5259e+01
Epoch 2/10
10/10 - 1s - loss: 298.5354 - loglik: -2.8772e+02 - logprior: -1.0814e+01
Epoch 3/10
10/10 - 1s - loss: 251.7778 - loglik: -2.4684e+02 - logprior: -4.9350e+00
Epoch 4/10
10/10 - 1s - loss: 224.5778 - loglik: -2.2142e+02 - logprior: -3.1532e+00
Epoch 5/10
10/10 - 1s - loss: 211.6539 - loglik: -2.0941e+02 - logprior: -2.2406e+00
Epoch 6/10
10/10 - 1s - loss: 207.1496 - loglik: -2.0542e+02 - logprior: -1.7302e+00
Epoch 7/10
10/10 - 1s - loss: 204.4960 - loglik: -2.0303e+02 - logprior: -1.4627e+00
Epoch 8/10
10/10 - 1s - loss: 204.1203 - loglik: -2.0283e+02 - logprior: -1.2919e+00
Epoch 9/10
10/10 - 1s - loss: 203.2000 - loglik: -2.0201e+02 - logprior: -1.1884e+00
Epoch 10/10
10/10 - 1s - loss: 202.8034 - loglik: -2.0169e+02 - logprior: -1.1152e+00
Fitted a model with MAP estimate = -202.6059
expansions: [(10, 2), (11, 2), (12, 2), (13, 1), (14, 2), (16, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (66, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 248.5647 - loglik: -1.9730e+02 - logprior: -5.1260e+01
Epoch 2/2
10/10 - 2s - loss: 206.2769 - loglik: -1.8599e+02 - logprior: -2.0287e+01
Fitted a model with MAP estimate = -199.2638
expansions: [(0, 2)]
discards: [  0  20 102 103]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 223.2889 - loglik: -1.8278e+02 - logprior: -4.0506e+01
Epoch 2/2
10/10 - 1s - loss: 190.3478 - loglik: -1.8098e+02 - logprior: -9.3663e+00
Fitted a model with MAP estimate = -185.7689
expansions: []
discards: [ 0 11]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 230.8312 - loglik: -1.8232e+02 - logprior: -4.8510e+01
Epoch 2/10
10/10 - 1s - loss: 194.7367 - loglik: -1.8154e+02 - logprior: -1.3198e+01
Epoch 3/10
10/10 - 1s - loss: 185.4884 - loglik: -1.8193e+02 - logprior: -3.5552e+00
Epoch 4/10
10/10 - 1s - loss: 181.5070 - loglik: -1.8119e+02 - logprior: -3.1810e-01
Epoch 5/10
10/10 - 1s - loss: 179.9801 - loglik: -1.8104e+02 - logprior: 1.0574
Epoch 6/10
10/10 - 1s - loss: 178.9301 - loglik: -1.8077e+02 - logprior: 1.8432
Epoch 7/10
10/10 - 1s - loss: 178.4836 - loglik: -1.8101e+02 - logprior: 2.5232
Epoch 8/10
10/10 - 2s - loss: 177.6955 - loglik: -1.8075e+02 - logprior: 3.0531
Epoch 9/10
10/10 - 1s - loss: 177.1888 - loglik: -1.8060e+02 - logprior: 3.4102
Epoch 10/10
10/10 - 1s - loss: 176.7061 - loglik: -1.8040e+02 - logprior: 3.6951
Fitted a model with MAP estimate = -176.7614
Time for alignment: 47.6536
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 375.5079 - loglik: -3.3025e+02 - logprior: -4.5257e+01
Epoch 2/10
10/10 - 1s - loss: 297.7321 - loglik: -2.8692e+02 - logprior: -1.0814e+01
Epoch 3/10
10/10 - 1s - loss: 251.0951 - loglik: -2.4613e+02 - logprior: -4.9602e+00
Epoch 4/10
10/10 - 1s - loss: 225.9348 - loglik: -2.2279e+02 - logprior: -3.1492e+00
Epoch 5/10
10/10 - 1s - loss: 214.6604 - loglik: -2.1251e+02 - logprior: -2.1526e+00
Epoch 6/10
10/10 - 1s - loss: 208.8062 - loglik: -2.0718e+02 - logprior: -1.6243e+00
Epoch 7/10
10/10 - 1s - loss: 206.0808 - loglik: -2.0471e+02 - logprior: -1.3695e+00
Epoch 8/10
10/10 - 1s - loss: 204.7413 - loglik: -2.0352e+02 - logprior: -1.2216e+00
Epoch 9/10
10/10 - 1s - loss: 203.7307 - loglik: -2.0259e+02 - logprior: -1.1386e+00
Epoch 10/10
10/10 - 1s - loss: 203.1571 - loglik: -2.0210e+02 - logprior: -1.0557e+00
Fitted a model with MAP estimate = -202.6304
expansions: [(10, 2), (11, 2), (12, 2), (13, 1), (14, 2), (16, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (58, 2), (79, 1), (81, 2), (82, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 120 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 248.9220 - loglik: -1.9761e+02 - logprior: -5.1317e+01
Epoch 2/2
10/10 - 2s - loss: 206.9201 - loglik: -1.8645e+02 - logprior: -2.0468e+01
Fitted a model with MAP estimate = -199.5408
expansions: [(0, 2)]
discards: [  0  10  20  75 103 104]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 116 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 223.3621 - loglik: -1.8288e+02 - logprior: -4.0479e+01
Epoch 2/2
10/10 - 1s - loss: 190.6109 - loglik: -1.8136e+02 - logprior: -9.2495e+00
Fitted a model with MAP estimate = -185.6750
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 230.9860 - loglik: -1.8243e+02 - logprior: -4.8557e+01
Epoch 2/10
10/10 - 1s - loss: 194.9106 - loglik: -1.8163e+02 - logprior: -1.3282e+01
Epoch 3/10
10/10 - 1s - loss: 184.9787 - loglik: -1.8141e+02 - logprior: -3.5727e+00
Epoch 4/10
10/10 - 1s - loss: 181.6518 - loglik: -1.8134e+02 - logprior: -3.1160e-01
Epoch 5/10
10/10 - 1s - loss: 179.6760 - loglik: -1.8074e+02 - logprior: 1.0638
Epoch 6/10
10/10 - 1s - loss: 179.1299 - loglik: -1.8097e+02 - logprior: 1.8442
Epoch 7/10
10/10 - 1s - loss: 177.8904 - loglik: -1.8041e+02 - logprior: 2.5178
Epoch 8/10
10/10 - 1s - loss: 177.7650 - loglik: -1.8081e+02 - logprior: 3.0406
Epoch 9/10
10/10 - 1s - loss: 176.9895 - loglik: -1.8038e+02 - logprior: 3.3876
Epoch 10/10
10/10 - 1s - loss: 177.1156 - loglik: -1.8078e+02 - logprior: 3.6665
Fitted a model with MAP estimate = -176.7568
Time for alignment: 48.8971
Computed alignments with likelihoods: ['-175.9300', '-176.7614', '-176.7568']
Best model has likelihood: -175.9300
SP score = 0.9197
Training of 3 independent models on file cah.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9d5babf10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9de32ba90>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 184 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 643.6937 - loglik: -6.1524e+02 - logprior: -2.8452e+01
Epoch 2/10
10/10 - 5s - loss: 577.1796 - loglik: -5.7237e+02 - logprior: -4.8075e+00
Epoch 3/10
10/10 - 5s - loss: 518.4633 - loglik: -5.1740e+02 - logprior: -1.0678e+00
Epoch 4/10
10/10 - 5s - loss: 479.3901 - loglik: -4.7868e+02 - logprior: -7.1122e-01
Epoch 5/10
10/10 - 6s - loss: 465.8074 - loglik: -4.6541e+02 - logprior: -3.9987e-01
Epoch 6/10
10/10 - 6s - loss: 460.6681 - loglik: -4.6044e+02 - logprior: -2.2872e-01
Epoch 7/10
10/10 - 6s - loss: 458.5574 - loglik: -4.5856e+02 - logprior: -9.0069e-04
Epoch 8/10
10/10 - 6s - loss: 456.6410 - loglik: -4.5681e+02 - logprior: 0.1701
Epoch 9/10
10/10 - 6s - loss: 457.5820 - loglik: -4.5783e+02 - logprior: 0.2439
Fitted a model with MAP estimate = -456.3832
expansions: [(15, 2), (16, 2), (28, 1), (30, 1), (31, 2), (32, 2), (40, 2), (41, 2), (42, 1), (51, 1), (52, 1), (53, 1), (54, 5), (80, 1), (89, 1), (106, 2), (112, 1), (114, 2), (115, 2), (116, 3), (118, 1), (119, 2), (120, 2), (154, 1), (157, 1), (163, 1), (166, 8), (167, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 483.5324 - loglik: -4.5171e+02 - logprior: -3.1825e+01
Epoch 2/2
10/10 - 8s - loss: 448.7720 - loglik: -4.3819e+02 - logprior: -1.0585e+01
Fitted a model with MAP estimate = -445.0499
expansions: [(72, 2)]
discards: [  0  19  50  76  77  78 215]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 230 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 473.3661 - loglik: -4.4218e+02 - logprior: -3.1190e+01
Epoch 2/2
10/10 - 8s - loss: 449.9005 - loglik: -4.4009e+02 - logprior: -9.8150e+00
Fitted a model with MAP estimate = -445.1326
expansions: [(0, 4), (12, 1)]
discards: [  0  13  14  69  70 126 152]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 228 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 463.2319 - loglik: -4.3921e+02 - logprior: -2.4018e+01
Epoch 2/10
10/10 - 8s - loss: 439.9842 - loglik: -4.3702e+02 - logprior: -2.9622e+00
Epoch 3/10
10/10 - 8s - loss: 435.1131 - loglik: -4.3652e+02 - logprior: 1.4029
Epoch 4/10
10/10 - 8s - loss: 432.4939 - loglik: -4.3574e+02 - logprior: 3.2422
Epoch 5/10
10/10 - 8s - loss: 432.0080 - loglik: -4.3625e+02 - logprior: 4.2402
Epoch 6/10
10/10 - 8s - loss: 431.7975 - loglik: -4.3661e+02 - logprior: 4.8152
Epoch 7/10
10/10 - 8s - loss: 429.7637 - loglik: -4.3495e+02 - logprior: 5.1826
Epoch 8/10
10/10 - 8s - loss: 430.4539 - loglik: -4.3592e+02 - logprior: 5.4700
Fitted a model with MAP estimate = -429.9666
Time for alignment: 174.7860
Fitting a model of length 184 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 644.0410 - loglik: -6.1560e+02 - logprior: -2.8438e+01
Epoch 2/10
10/10 - 6s - loss: 575.6722 - loglik: -5.7088e+02 - logprior: -4.7960e+00
Epoch 3/10
10/10 - 6s - loss: 515.4108 - loglik: -5.1432e+02 - logprior: -1.0859e+00
Epoch 4/10
10/10 - 6s - loss: 479.3710 - loglik: -4.7874e+02 - logprior: -6.3213e-01
Epoch 5/10
10/10 - 6s - loss: 466.4202 - loglik: -4.6613e+02 - logprior: -2.9279e-01
Epoch 6/10
10/10 - 6s - loss: 460.7629 - loglik: -4.6072e+02 - logprior: -3.9240e-02
Epoch 7/10
10/10 - 6s - loss: 458.7130 - loglik: -4.5888e+02 - logprior: 0.1663
Epoch 8/10
10/10 - 6s - loss: 457.1901 - loglik: -4.5748e+02 - logprior: 0.2904
Epoch 9/10
10/10 - 6s - loss: 456.7165 - loglik: -4.5707e+02 - logprior: 0.3567
Epoch 10/10
10/10 - 7s - loss: 456.6614 - loglik: -4.5710e+02 - logprior: 0.4349
Fitted a model with MAP estimate = -455.8058
expansions: [(15, 1), (16, 2), (28, 1), (30, 1), (31, 2), (32, 2), (33, 1), (42, 2), (43, 1), (51, 1), (52, 1), (55, 4), (81, 1), (90, 1), (112, 1), (114, 2), (115, 2), (116, 3), (118, 2), (119, 2), (120, 3), (130, 2), (164, 1), (166, 9), (167, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 232 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 13s - loss: 483.8703 - loglik: -4.5194e+02 - logprior: -3.1929e+01
Epoch 2/2
10/10 - 10s - loss: 451.9675 - loglik: -4.4117e+02 - logprior: -1.0793e+01
Fitted a model with MAP estimate = -446.5576
expansions: [(70, 2), (137, 1)]
discards: [  0  18  73 142 149 152 212]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 228 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 13s - loss: 472.5731 - loglik: -4.4141e+02 - logprior: -3.1161e+01
Epoch 2/2
10/10 - 11s - loss: 448.6749 - loglik: -4.3885e+02 - logprior: -9.8200e+00
Fitted a model with MAP estimate = -443.7721
expansions: [(0, 4)]
discards: [  0  49  73  74 163]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 227 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 12s - loss: 463.2075 - loglik: -4.3925e+02 - logprior: -2.3953e+01
Epoch 2/10
10/10 - 10s - loss: 440.3364 - loglik: -4.3746e+02 - logprior: -2.8769e+00
Epoch 3/10
10/10 - 11s - loss: 435.6471 - loglik: -4.3717e+02 - logprior: 1.5270
Epoch 4/10
10/10 - 11s - loss: 434.0065 - loglik: -4.3733e+02 - logprior: 3.3284
Epoch 5/10
10/10 - 11s - loss: 431.7331 - loglik: -4.3605e+02 - logprior: 4.3122
Epoch 6/10
10/10 - 11s - loss: 432.3502 - loglik: -4.3724e+02 - logprior: 4.8872
Fitted a model with MAP estimate = -431.1707
Time for alignment: 192.1598
Fitting a model of length 184 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 645.1927 - loglik: -6.1675e+02 - logprior: -2.8447e+01
Epoch 2/10
10/10 - 8s - loss: 575.0545 - loglik: -5.7025e+02 - logprior: -4.8011e+00
Epoch 3/10
10/10 - 7s - loss: 515.5475 - loglik: -5.1452e+02 - logprior: -1.0254e+00
Epoch 4/10
10/10 - 8s - loss: 477.4666 - loglik: -4.7707e+02 - logprior: -3.9443e-01
Epoch 5/10
10/10 - 8s - loss: 464.3806 - loglik: -4.6436e+02 - logprior: -1.7069e-02
Epoch 6/10
10/10 - 7s - loss: 460.4283 - loglik: -4.6059e+02 - logprior: 0.1633
Epoch 7/10
10/10 - 8s - loss: 459.5295 - loglik: -4.5993e+02 - logprior: 0.3972
Epoch 8/10
10/10 - 8s - loss: 457.5943 - loglik: -4.5810e+02 - logprior: 0.5007
Epoch 9/10
10/10 - 8s - loss: 456.5357 - loglik: -4.5705e+02 - logprior: 0.5173
Epoch 10/10
10/10 - 8s - loss: 455.8796 - loglik: -4.5647e+02 - logprior: 0.5919
Fitted a model with MAP estimate = -455.8530
expansions: [(15, 2), (16, 2), (28, 1), (29, 2), (30, 2), (31, 2), (41, 2), (42, 1), (51, 1), (52, 1), (55, 4), (91, 6), (92, 2), (110, 1), (114, 1), (115, 1), (116, 4), (118, 2), (119, 2), (120, 1), (131, 2), (162, 1), (164, 2), (166, 2), (167, 8)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 238 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 14s - loss: 483.0643 - loglik: -4.5120e+02 - logprior: -3.1862e+01
Epoch 2/2
10/10 - 10s - loss: 449.1090 - loglik: -4.3823e+02 - logprior: -1.0878e+01
Fitted a model with MAP estimate = -444.7839
expansions: [(146, 1)]
discards: [ 0 18 19 52]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 12s - loss: 468.2649 - loglik: -4.3670e+02 - logprior: -3.1567e+01
Epoch 2/2
10/10 - 10s - loss: 443.7471 - loglik: -4.3343e+02 - logprior: -1.0321e+01
Fitted a model with MAP estimate = -440.1254
expansions: [(0, 4), (12, 1), (67, 2), (68, 1), (111, 1), (113, 1)]
discards: [  0  13  70  71  72 107 108 109 153 168 208]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 234 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 12s - loss: 464.8633 - loglik: -4.4094e+02 - logprior: -2.3923e+01
Epoch 2/10
10/10 - 9s - loss: 437.3192 - loglik: -4.3437e+02 - logprior: -2.9535e+00
Epoch 3/10
10/10 - 9s - loss: 433.3963 - loglik: -4.3476e+02 - logprior: 1.3644
Epoch 4/10
10/10 - 10s - loss: 430.5162 - loglik: -4.3369e+02 - logprior: 3.1715
Epoch 5/10
10/10 - 10s - loss: 428.8141 - loglik: -4.3293e+02 - logprior: 4.1142
Epoch 6/10
10/10 - 10s - loss: 429.0938 - loglik: -4.3378e+02 - logprior: 4.6858
Fitted a model with MAP estimate = -427.8634
Time for alignment: 206.4298
Computed alignments with likelihoods: ['-429.9666', '-431.1707', '-427.8634']
Best model has likelihood: -427.8634
SP score = 0.8882
Training of 3 independent models on file trfl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9f72441c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c3103670>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 16s - loss: 736.0920 - loglik: -6.9551e+02 - logprior: -4.0577e+01
Epoch 2/10
11/11 - 12s - loss: 629.0354 - loglik: -6.2612e+02 - logprior: -2.9112e+00
Epoch 3/10
11/11 - 14s - loss: 538.6926 - loglik: -5.4007e+02 - logprior: 1.3744
Epoch 4/10
11/11 - 13s - loss: 487.4060 - loglik: -4.8943e+02 - logprior: 2.0278
Epoch 5/10
11/11 - 13s - loss: 478.7450 - loglik: -4.8097e+02 - logprior: 2.2230
Epoch 6/10
11/11 - 13s - loss: 465.5942 - loglik: -4.6807e+02 - logprior: 2.4771
Epoch 7/10
11/11 - 12s - loss: 466.4936 - loglik: -4.6933e+02 - logprior: 2.8342
Fitted a model with MAP estimate = -465.3028
expansions: [(19, 5), (21, 1), (22, 1), (24, 1), (28, 1), (35, 1), (46, 1), (51, 2), (63, 1), (65, 1), (78, 1), (79, 1), (80, 1), (91, 1), (93, 1), (103, 5), (104, 1), (106, 3), (121, 2), (161, 2), (162, 5), (181, 1), (182, 3), (196, 1), (197, 2), (198, 4), (200, 2), (201, 5), (220, 1), (222, 2), (224, 1), (225, 1)]
discards: [  0 208]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 303 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 20s - loss: 504.5689 - loglik: -4.5712e+02 - logprior: -4.7453e+01
Epoch 2/2
11/11 - 16s - loss: 447.4041 - loglik: -4.3443e+02 - logprior: -1.2976e+01
Fitted a model with MAP estimate = -439.7312
expansions: [(0, 2), (200, 2), (201, 1), (220, 1), (248, 1)]
discards: [  0  21  22  62 124 125 132 149]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 302 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 19s - loss: 466.0743 - loglik: -4.3132e+02 - logprior: -3.4755e+01
Epoch 2/2
11/11 - 16s - loss: 425.7763 - loglik: -4.2555e+02 - logprior: -2.2465e-01
Fitted a model with MAP estimate = -420.7637
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 301 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 22s - loss: 469.4622 - loglik: -4.2447e+02 - logprior: -4.4997e+01
Epoch 2/10
11/11 - 17s - loss: 434.6621 - loglik: -4.2511e+02 - logprior: -9.5552e+00
Epoch 3/10
11/11 - 17s - loss: 425.8816 - loglik: -4.2866e+02 - logprior: 2.7780
Epoch 4/10
11/11 - 17s - loss: 414.1026 - loglik: -4.2431e+02 - logprior: 10.2071
Epoch 5/10
11/11 - 16s - loss: 416.0541 - loglik: -4.2864e+02 - logprior: 12.5889
Fitted a model with MAP estimate = -412.6599
Time for alignment: 281.9695
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 13s - loss: 738.5305 - loglik: -6.9797e+02 - logprior: -4.0563e+01
Epoch 2/10
11/11 - 11s - loss: 629.7292 - loglik: -6.2689e+02 - logprior: -2.8406e+00
Epoch 3/10
11/11 - 11s - loss: 544.8113 - loglik: -5.4618e+02 - logprior: 1.3645
Epoch 4/10
11/11 - 12s - loss: 494.5371 - loglik: -4.9618e+02 - logprior: 1.6411
Epoch 5/10
11/11 - 10s - loss: 479.9471 - loglik: -4.8159e+02 - logprior: 1.6446
Epoch 6/10
11/11 - 14s - loss: 470.9191 - loglik: -4.7270e+02 - logprior: 1.7768
Epoch 7/10
11/11 - 14s - loss: 470.0996 - loglik: -4.7213e+02 - logprior: 2.0273
Epoch 8/10
11/11 - 13s - loss: 468.3623 - loglik: -4.7063e+02 - logprior: 2.2678
Epoch 9/10
11/11 - 13s - loss: 467.1719 - loglik: -4.6978e+02 - logprior: 2.6101
Epoch 10/10
11/11 - 13s - loss: 462.9104 - loglik: -4.6579e+02 - logprior: 2.8750
Fitted a model with MAP estimate = -465.4318
expansions: [(22, 3), (23, 1), (25, 1), (36, 1), (43, 1), (50, 1), (53, 2), (62, 1), (64, 4), (65, 1), (78, 1), (79, 1), (80, 1), (91, 1), (92, 1), (103, 3), (104, 1), (105, 2), (106, 1), (129, 1), (134, 1), (137, 1), (150, 1), (151, 1), (163, 5), (180, 1), (181, 1), (182, 3), (196, 1), (197, 2), (198, 4), (200, 2), (201, 3), (220, 1), (222, 2), (224, 1), (225, 1)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 303 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 22s - loss: 492.6901 - loglik: -4.5691e+02 - logprior: -3.5780e+01
Epoch 2/2
11/11 - 20s - loss: 434.1651 - loglik: -4.3317e+02 - logprior: -9.9026e-01
Fitted a model with MAP estimate = -427.2345
expansions: [(221, 1), (256, 2)]
discards: [ 61  62  77 160]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 302 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 22s - loss: 467.0705 - loglik: -4.3304e+02 - logprior: -3.4026e+01
Epoch 2/2
11/11 - 21s - loss: 425.1980 - loglik: -4.2519e+02 - logprior: -1.0130e-02
Fitted a model with MAP estimate = -423.0136
expansions: [(246, 1)]
discards: [ 75 121 122 279]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 24s - loss: 462.2282 - loglik: -4.2903e+02 - logprior: -3.3197e+01
Epoch 2/10
11/11 - 20s - loss: 429.1604 - loglik: -4.3012e+02 - logprior: 0.9570
Epoch 3/10
11/11 - 19s - loss: 421.3528 - loglik: -4.2918e+02 - logprior: 7.8279
Epoch 4/10
11/11 - 20s - loss: 415.6917 - loglik: -4.2666e+02 - logprior: 10.9719
Epoch 5/10
11/11 - 20s - loss: 417.4501 - loglik: -4.3032e+02 - logprior: 12.8701
Fitted a model with MAP estimate = -414.3655
Time for alignment: 340.6193
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 16s - loss: 736.5485 - loglik: -6.9598e+02 - logprior: -4.0564e+01
Epoch 2/10
11/11 - 13s - loss: 630.9808 - loglik: -6.2811e+02 - logprior: -2.8756e+00
Epoch 3/10
11/11 - 15s - loss: 542.7051 - loglik: -5.4399e+02 - logprior: 1.2874
Epoch 4/10
11/11 - 13s - loss: 492.6481 - loglik: -4.9448e+02 - logprior: 1.8302
Epoch 5/10
11/11 - 15s - loss: 478.3364 - loglik: -4.8019e+02 - logprior: 1.8582
Epoch 6/10
11/11 - 14s - loss: 471.8649 - loglik: -4.7401e+02 - logprior: 2.1500
Epoch 7/10
11/11 - 14s - loss: 469.4049 - loglik: -4.7187e+02 - logprior: 2.4607
Epoch 8/10
11/11 - 15s - loss: 467.2220 - loglik: -4.6989e+02 - logprior: 2.6634
Epoch 9/10
11/11 - 14s - loss: 463.2390 - loglik: -4.6614e+02 - logprior: 2.8979
Epoch 10/10
11/11 - 15s - loss: 466.5687 - loglik: -4.6973e+02 - logprior: 3.1582
Fitted a model with MAP estimate = -464.5586
expansions: [(19, 4), (21, 1), (22, 1), (26, 1), (36, 1), (47, 1), (49, 1), (52, 2), (63, 2), (64, 2), (65, 2), (77, 1), (78, 2), (80, 1), (90, 1), (92, 1), (102, 3), (103, 1), (104, 1), (106, 4), (107, 1), (136, 1), (161, 4), (168, 2), (181, 1), (182, 3), (185, 2), (196, 3), (197, 1), (198, 2), (200, 2), (201, 5), (220, 1), (222, 2), (224, 1), (225, 1)]
discards: [  1 208]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 307 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 21s - loss: 490.7131 - loglik: -4.5497e+02 - logprior: -3.5744e+01
Epoch 2/2
11/11 - 23s - loss: 435.8496 - loglik: -4.3485e+02 - logprior: -1.0028e+00
Fitted a model with MAP estimate = -427.2338
expansions: [(198, 2), (223, 1)]
discards: [ 21  61  78  96 126 127 134 135]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 302 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 22s - loss: 464.9096 - loglik: -4.3087e+02 - logprior: -3.4036e+01
Epoch 2/2
11/11 - 23s - loss: 428.3834 - loglik: -4.2810e+02 - logprior: -2.8157e-01
Fitted a model with MAP estimate = -421.7440
expansions: [(247, 1)]
discards: [ 76  77 190 279]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 22s - loss: 463.0546 - loglik: -4.2966e+02 - logprior: -3.3394e+01
Epoch 2/10
11/11 - 17s - loss: 426.0153 - loglik: -4.2671e+02 - logprior: 0.6976
Epoch 3/10
11/11 - 19s - loss: 419.0233 - loglik: -4.2661e+02 - logprior: 7.5913
Epoch 4/10
11/11 - 16s - loss: 415.5698 - loglik: -4.2636e+02 - logprior: 10.7889
Epoch 5/10
11/11 - 16s - loss: 416.1693 - loglik: -4.2884e+02 - logprior: 12.6734
Fitted a model with MAP estimate = -414.0507
Time for alignment: 354.1032
Computed alignments with likelihoods: ['-412.6599', '-414.3655', '-414.0507']
Best model has likelihood: -412.6599
SP score = 0.9217
Training of 3 independent models on file serpin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c2b890d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb80c4b9af0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 29s - loss: 814.8425 - loglik: -8.0890e+02 - logprior: -5.9376e+00
Epoch 2/10
21/21 - 26s - loss: 695.6630 - loglik: -6.9507e+02 - logprior: -5.9209e-01
Epoch 3/10
21/21 - 26s - loss: 645.4675 - loglik: -6.4303e+02 - logprior: -2.4394e+00
Epoch 4/10
21/21 - 24s - loss: 634.2327 - loglik: -6.3184e+02 - logprior: -2.3957e+00
Epoch 5/10
21/21 - 23s - loss: 634.2447 - loglik: -6.3199e+02 - logprior: -2.2565e+00
Fitted a model with MAP estimate = -632.7769
expansions: [(13, 1), (14, 2), (15, 1), (53, 2), (55, 2), (56, 1), (62, 2), (63, 2), (64, 1), (66, 1), (67, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 2), (82, 1), (84, 1), (86, 1), (87, 1), (94, 1), (95, 1), (96, 1), (102, 1), (104, 1), (111, 1), (113, 1), (115, 1), (117, 1), (136, 1), (138, 1), (139, 1), (142, 2), (154, 1), (155, 1), (156, 1), (158, 3), (159, 1), (160, 1), (161, 1), (167, 1), (169, 1), (179, 1), (182, 1), (186, 1), (188, 1), (189, 1), (190, 2), (191, 1), (192, 1), (193, 1), (194, 1), (207, 1), (211, 1), (213, 1), (221, 1), (222, 2), (228, 2), (229, 2), (231, 1), (235, 1), (255, 1), (256, 1), (257, 1), (258, 2), (260, 2), (261, 1), (272, 2), (273, 1)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 368 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 41s - loss: 623.9597 - loglik: -6.1827e+02 - logprior: -5.6938e+00
Epoch 2/2
21/21 - 41s - loss: 601.5063 - loglik: -6.0209e+02 - logprior: 0.5849
Fitted a model with MAP estimate = -599.0844
expansions: [(76, 1)]
discards: [181 205 287 297 336 337]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 363 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 38s - loss: 605.6681 - loglik: -6.0098e+02 - logprior: -4.6886e+00
Epoch 2/2
21/21 - 36s - loss: 599.4597 - loglik: -6.0118e+02 - logprior: 1.7249
Fitted a model with MAP estimate = -597.3954
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 363 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 45s - loss: 604.2731 - loglik: -5.9990e+02 - logprior: -4.3693e+00
Epoch 2/10
21/21 - 44s - loss: 597.2994 - loglik: -5.9950e+02 - logprior: 2.2055
Epoch 3/10
21/21 - 45s - loss: 596.7429 - loglik: -5.9981e+02 - logprior: 3.0689
Epoch 4/10
21/21 - 46s - loss: 597.4522 - loglik: -6.0098e+02 - logprior: 3.5316
Fitted a model with MAP estimate = -595.4277
Time for alignment: 555.3064
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 31s - loss: 814.0587 - loglik: -8.0815e+02 - logprior: -5.9123e+00
Epoch 2/10
21/21 - 28s - loss: 700.1864 - loglik: -6.9966e+02 - logprior: -5.2724e-01
Epoch 3/10
21/21 - 28s - loss: 645.3628 - loglik: -6.4312e+02 - logprior: -2.2382e+00
Epoch 4/10
21/21 - 28s - loss: 636.7331 - loglik: -6.3442e+02 - logprior: -2.3152e+00
Epoch 5/10
21/21 - 28s - loss: 634.8622 - loglik: -6.3273e+02 - logprior: -2.1362e+00
Epoch 6/10
21/21 - 29s - loss: 633.4608 - loglik: -6.3135e+02 - logprior: -2.1133e+00
Epoch 7/10
21/21 - 28s - loss: 633.5702 - loglik: -6.3145e+02 - logprior: -2.1206e+00
Fitted a model with MAP estimate = -632.8821
expansions: [(13, 1), (14, 2), (15, 1), (46, 1), (52, 4), (53, 2), (55, 1), (60, 5), (61, 1), (63, 1), (74, 2), (75, 4), (78, 1), (79, 1), (80, 1), (84, 1), (85, 1), (90, 1), (91, 1), (93, 1), (99, 1), (100, 1), (109, 1), (128, 1), (130, 1), (135, 1), (138, 1), (141, 1), (152, 1), (153, 1), (154, 1), (158, 1), (160, 1), (161, 3), (170, 1), (180, 1), (183, 1), (187, 1), (189, 1), (190, 1), (191, 1), (193, 2), (194, 1), (195, 1), (196, 1), (210, 1), (213, 1), (215, 1), (219, 1), (224, 1), (228, 1), (229, 2), (230, 2), (232, 1), (236, 1), (256, 1), (257, 1), (258, 1), (259, 2), (261, 2), (262, 1), (271, 1), (272, 2), (273, 1)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 48s - loss: 624.0337 - loglik: -6.1818e+02 - logprior: -5.8556e+00
Epoch 2/2
21/21 - 45s - loss: 602.7765 - loglik: -6.0328e+02 - logprior: 0.5076
Fitted a model with MAP estimate = -599.3408
expansions: [(77, 1), (93, 1)]
discards: [ 96 249 296 335 336]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 42s - loss: 604.9408 - loglik: -6.0036e+02 - logprior: -4.5827e+00
Epoch 2/2
21/21 - 39s - loss: 596.9684 - loglik: -5.9874e+02 - logprior: 1.7714
Fitted a model with MAP estimate = -596.8677
expansions: [(147, 1)]
discards: [57]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 46s - loss: 603.9354 - loglik: -5.9986e+02 - logprior: -4.0803e+00
Epoch 2/10
21/21 - 36s - loss: 596.4870 - loglik: -5.9895e+02 - logprior: 2.4585
Epoch 3/10
21/21 - 34s - loss: 597.8595 - loglik: -6.0104e+02 - logprior: 3.1774
Fitted a model with MAP estimate = -595.1013
Time for alignment: 591.1817
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 25s - loss: 814.5912 - loglik: -8.0866e+02 - logprior: -5.9288e+00
Epoch 2/10
21/21 - 25s - loss: 695.5421 - loglik: -6.9478e+02 - logprior: -7.6372e-01
Epoch 3/10
21/21 - 26s - loss: 645.2762 - loglik: -6.4273e+02 - logprior: -2.5455e+00
Epoch 4/10
21/21 - 26s - loss: 638.7131 - loglik: -6.3639e+02 - logprior: -2.3207e+00
Epoch 5/10
21/21 - 24s - loss: 632.9232 - loglik: -6.3070e+02 - logprior: -2.2278e+00
Epoch 6/10
21/21 - 23s - loss: 637.0593 - loglik: -6.3486e+02 - logprior: -2.2006e+00
Fitted a model with MAP estimate = -634.2379
expansions: [(13, 1), (14, 2), (15, 1), (34, 1), (52, 2), (54, 3), (55, 1), (61, 2), (62, 2), (64, 2), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 2), (82, 2), (86, 1), (87, 1), (91, 1), (94, 1), (95, 1), (96, 1), (103, 1), (104, 1), (111, 1), (113, 1), (115, 1), (117, 1), (135, 1), (136, 1), (139, 1), (153, 1), (154, 1), (155, 1), (156, 1), (158, 1), (159, 1), (161, 1), (162, 2), (181, 1), (184, 1), (191, 3), (192, 2), (193, 1), (194, 1), (195, 1), (209, 1), (210, 1), (213, 1), (215, 1), (219, 1), (224, 1), (228, 1), (229, 2), (230, 2), (232, 1), (236, 1), (256, 1), (257, 1), (258, 1), (259, 2), (261, 2), (262, 1), (271, 1), (272, 2), (273, 1)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 41s - loss: 621.9854 - loglik: -6.1622e+02 - logprior: -5.7673e+00
Epoch 2/2
21/21 - 40s - loss: 602.0640 - loglik: -6.0297e+02 - logprior: 0.9012
Fitted a model with MAP estimate = -598.7419
expansions: [(187, 1)]
discards: [ 57 105 296 335 336]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 363 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 51s - loss: 606.4073 - loglik: -6.0168e+02 - logprior: -4.7249e+00
Epoch 2/2
21/21 - 42s - loss: 596.6503 - loglik: -5.9852e+02 - logprior: 1.8733
Fitted a model with MAP estimate = -596.8966
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 363 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 44s - loss: 603.9098 - loglik: -5.9957e+02 - logprior: -4.3381e+00
Epoch 2/10
21/21 - 39s - loss: 598.5593 - loglik: -6.0094e+02 - logprior: 2.3771
Epoch 3/10
21/21 - 38s - loss: 597.1211 - loglik: -6.0027e+02 - logprior: 3.1517
Epoch 4/10
21/21 - 38s - loss: 594.3160 - loglik: -5.9794e+02 - logprior: 3.6233
Epoch 5/10
21/21 - 37s - loss: 594.1400 - loglik: -5.9807e+02 - logprior: 3.9326
Epoch 6/10
21/21 - 39s - loss: 597.9890 - loglik: -6.0216e+02 - logprior: 4.1716
Fitted a model with MAP estimate = -594.7704
Time for alignment: 659.8760
Computed alignments with likelihoods: ['-595.4277', '-595.1013', '-594.7704']
Best model has likelihood: -594.7704
SP score = 0.9654
Training of 3 independent models on file ghf13.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fba10d58d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbb82e0cfd0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 840.1221 - loglik: -8.3843e+02 - logprior: -1.6901e+00
Epoch 2/10
39/39 - 36s - loss: 740.3486 - loglik: -7.3892e+02 - logprior: -1.4246e+00
Epoch 3/10
39/39 - 36s - loss: 729.3813 - loglik: -7.2793e+02 - logprior: -1.4528e+00
Epoch 4/10
39/39 - 35s - loss: 726.3690 - loglik: -7.2493e+02 - logprior: -1.4371e+00
Epoch 5/10
39/39 - 34s - loss: 724.3793 - loglik: -7.2294e+02 - logprior: -1.4354e+00
Epoch 6/10
39/39 - 33s - loss: 723.4175 - loglik: -7.2197e+02 - logprior: -1.4497e+00
Epoch 7/10
39/39 - 32s - loss: 723.1752 - loglik: -7.2170e+02 - logprior: -1.4790e+00
Epoch 8/10
39/39 - 33s - loss: 722.8546 - loglik: -7.2138e+02 - logprior: -1.4711e+00
Epoch 9/10
39/39 - 34s - loss: 722.9452 - loglik: -7.2147e+02 - logprior: -1.4767e+00
Fitted a model with MAP estimate = -624.0676
expansions: [(14, 1), (19, 1), (40, 1), (55, 1), (80, 6), (81, 2), (91, 2), (93, 1), (98, 2), (101, 1), (108, 1), (117, 1), (119, 1), (121, 3), (122, 2), (149, 7), (160, 1), (161, 1), (162, 3), (164, 1), (169, 1), (171, 1), (172, 1), (177, 1), (178, 1), (179, 3), (180, 1), (185, 2), (187, 2), (188, 1), (191, 2), (192, 2), (208, 1), (209, 1), (213, 3), (215, 1), (225, 1), (244, 3)]
discards: [  0 102 103 104 105 106 166 167]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 303 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 56s - loss: 715.0588 - loglik: -7.1209e+02 - logprior: -2.9695e+00
Epoch 2/2
39/39 - 50s - loss: 703.8546 - loglik: -7.0215e+02 - logprior: -1.7015e+00
Fitted a model with MAP estimate = -607.6077
expansions: [(0, 2), (119, 7), (122, 3), (194, 1), (303, 2)]
discards: [  0 112 140 225 237 266 300 301 302]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 309 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 699.4465 - loglik: -6.9758e+02 - logprior: -1.8620e+00
Epoch 2/2
39/39 - 50s - loss: 694.4773 - loglik: -6.9366e+02 - logprior: -8.1612e-01
Fitted a model with MAP estimate = -601.0454
expansions: [(130, 1)]
discards: [  0 119 120 121 126 127 128 274 307 308]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 300 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 44s - loss: 604.8133 - loglik: -6.0280e+02 - logprior: -2.0128e+00
Epoch 2/10
43/43 - 40s - loss: 599.9945 - loglik: -5.9954e+02 - logprior: -4.5038e-01
Epoch 3/10
43/43 - 40s - loss: 598.2629 - loglik: -5.9805e+02 - logprior: -2.1627e-01
Epoch 4/10
43/43 - 40s - loss: 594.8915 - loglik: -5.9479e+02 - logprior: -1.0103e-01
Epoch 5/10
43/43 - 38s - loss: 597.5294 - loglik: -5.9753e+02 - logprior: 0.0036
Fitted a model with MAP estimate = -595.9154
Time for alignment: 932.2224
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 837.2726 - loglik: -8.3556e+02 - logprior: -1.7144e+00
Epoch 2/10
39/39 - 25s - loss: 742.5316 - loglik: -7.4102e+02 - logprior: -1.5153e+00
Epoch 3/10
39/39 - 25s - loss: 730.3644 - loglik: -7.2872e+02 - logprior: -1.6434e+00
Epoch 4/10
39/39 - 25s - loss: 726.4913 - loglik: -7.2484e+02 - logprior: -1.6560e+00
Epoch 5/10
39/39 - 27s - loss: 725.6231 - loglik: -7.2397e+02 - logprior: -1.6511e+00
Epoch 6/10
39/39 - 29s - loss: 724.5754 - loglik: -7.2294e+02 - logprior: -1.6332e+00
Epoch 7/10
39/39 - 30s - loss: 725.0931 - loglik: -7.2347e+02 - logprior: -1.6214e+00
Fitted a model with MAP estimate = -624.9245
expansions: [(14, 1), (45, 1), (56, 1), (81, 4), (82, 2), (83, 2), (93, 2), (95, 2), (103, 1), (110, 1), (116, 1), (118, 1), (120, 3), (121, 2), (122, 1), (151, 1), (152, 1), (155, 3), (170, 2), (171, 7), (173, 2), (174, 2), (175, 2), (176, 1), (181, 3), (186, 2), (187, 2), (189, 3), (190, 2), (193, 2), (195, 1), (206, 1), (208, 2), (209, 1), (213, 3), (215, 1), (218, 1), (244, 3)]
discards: [  0 159 160 161]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 313 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 711.7204 - loglik: -7.0866e+02 - logprior: -3.0605e+00
Epoch 2/2
39/39 - 47s - loss: 698.6776 - loglik: -6.9684e+02 - logprior: -1.8372e+00
Fitted a model with MAP estimate = -603.3204
expansions: [(0, 2), (121, 3), (188, 6), (190, 1)]
discards: [  0  91 119 175 176 177 178 179 180 181 182 183 211 212 240 310 311 312]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 307 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 51s - loss: 698.0866 - loglik: -6.9614e+02 - logprior: -1.9472e+00
Epoch 2/2
39/39 - 50s - loss: 693.9451 - loglik: -6.9319e+02 - logprior: -7.5830e-01
Fitted a model with MAP estimate = -600.6764
expansions: [(120, 2), (121, 1), (307, 3)]
discards: [  0 173 174 175 176 177 245]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 306 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 53s - loss: 602.4084 - loglik: -6.0030e+02 - logprior: -2.1123e+00
Epoch 2/10
43/43 - 50s - loss: 597.0912 - loglik: -5.9644e+02 - logprior: -6.5603e-01
Epoch 3/10
43/43 - 52s - loss: 595.7852 - loglik: -5.9512e+02 - logprior: -6.6069e-01
Epoch 4/10
43/43 - 54s - loss: 593.9887 - loglik: -5.9359e+02 - logprior: -3.9848e-01
Epoch 5/10
43/43 - 55s - loss: 596.0342 - loglik: -5.9581e+02 - logprior: -2.2313e-01
Fitted a model with MAP estimate = -594.7793
Time for alignment: 871.6085
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 843.2015 - loglik: -8.4150e+02 - logprior: -1.7024e+00
Epoch 2/10
39/39 - 34s - loss: 741.6845 - loglik: -7.4023e+02 - logprior: -1.4511e+00
Epoch 3/10
39/39 - 33s - loss: 728.6716 - loglik: -7.2714e+02 - logprior: -1.5290e+00
Epoch 4/10
39/39 - 32s - loss: 724.9594 - loglik: -7.2341e+02 - logprior: -1.5466e+00
Epoch 5/10
39/39 - 32s - loss: 723.9291 - loglik: -7.2235e+02 - logprior: -1.5803e+00
Epoch 6/10
39/39 - 32s - loss: 722.6542 - loglik: -7.2106e+02 - logprior: -1.5920e+00
Epoch 7/10
39/39 - 33s - loss: 723.3807 - loglik: -7.2179e+02 - logprior: -1.5937e+00
Fitted a model with MAP estimate = -623.7105
expansions: [(13, 1), (14, 1), (29, 1), (40, 1), (43, 1), (54, 1), (73, 1), (78, 5), (80, 2), (92, 1), (97, 2), (100, 2), (102, 2), (103, 3), (107, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (145, 8), (167, 3), (169, 2), (170, 2), (172, 1), (173, 1), (174, 1), (179, 1), (180, 3), (181, 3), (182, 1), (186, 2), (187, 1), (189, 2), (191, 4), (207, 1), (211, 1), (212, 2), (215, 1), (225, 1), (244, 3)]
discards: [  0 157 158 159 160 161 162 163 164 165]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 309 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 711.8441 - loglik: -7.0894e+02 - logprior: -2.8995e+00
Epoch 2/2
39/39 - 37s - loss: 698.7259 - loglik: -6.9673e+02 - logprior: -1.9928e+00
Fitted a model with MAP estimate = -602.3069
expansions: [(0, 2), (179, 1), (195, 1), (196, 1)]
discards: [  0  91 111 117 118 119 120 121 122 123 124 125 126 127 128 223 224 225
 233 241 272 273 306 307 308]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 706.8956 - loglik: -7.0499e+02 - logprior: -1.9092e+00
Epoch 2/2
39/39 - 34s - loss: 703.0303 - loglik: -7.0236e+02 - logprior: -6.7086e-01
Fitted a model with MAP estimate = -607.1719
expansions: [(103, 2), (111, 1), (116, 8), (189, 2), (194, 1), (227, 1), (289, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 306 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 45s - loss: 603.1955 - loglik: -6.0108e+02 - logprior: -2.1203e+00
Epoch 2/10
43/43 - 43s - loss: 598.5503 - loglik: -5.9776e+02 - logprior: -7.8773e-01
Epoch 3/10
43/43 - 44s - loss: 598.6819 - loglik: -5.9803e+02 - logprior: -6.5211e-01
Fitted a model with MAP estimate = -596.2416
Time for alignment: 695.8709
Computed alignments with likelihoods: ['-595.9154', '-594.7793', '-596.2416']
Best model has likelihood: -594.7793
SP score = 0.5225
Training of 3 independent models on file cryst.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c2b62070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbbad57ab20>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 274.6305 - loglik: -2.3650e+02 - logprior: -3.8135e+01
Epoch 2/10
10/10 - 1s - loss: 223.3400 - loglik: -2.1328e+02 - logprior: -1.0063e+01
Epoch 3/10
10/10 - 1s - loss: 196.4308 - loglik: -1.9134e+02 - logprior: -5.0943e+00
Epoch 4/10
10/10 - 1s - loss: 180.7811 - loglik: -1.7741e+02 - logprior: -3.3709e+00
Epoch 5/10
10/10 - 1s - loss: 173.7129 - loglik: -1.7121e+02 - logprior: -2.5074e+00
Epoch 6/10
10/10 - 1s - loss: 170.7263 - loglik: -1.6849e+02 - logprior: -2.2364e+00
Epoch 7/10
10/10 - 1s - loss: 169.8107 - loglik: -1.6772e+02 - logprior: -2.0898e+00
Epoch 8/10
10/10 - 1s - loss: 168.9455 - loglik: -1.6720e+02 - logprior: -1.7482e+00
Epoch 9/10
10/10 - 1s - loss: 168.1849 - loglik: -1.6669e+02 - logprior: -1.4905e+00
Epoch 10/10
10/10 - 1s - loss: 168.1100 - loglik: -1.6668e+02 - logprior: -1.4308e+00
Fitted a model with MAP estimate = -167.7710
expansions: [(0, 2), (12, 1), (15, 3), (20, 2), (21, 1), (23, 2), (30, 1), (32, 1), (43, 1), (55, 2), (58, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 209.7883 - loglik: -1.5971e+02 - logprior: -5.0074e+01
Epoch 2/2
10/10 - 1s - loss: 167.3201 - loglik: -1.5224e+02 - logprior: -1.5084e+01
Fitted a model with MAP estimate = -159.6866
expansions: []
discards: [ 0 19 27 29]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 196.2068 - loglik: -1.5273e+02 - logprior: -4.3479e+01
Epoch 2/2
10/10 - 1s - loss: 169.8689 - loglik: -1.5287e+02 - logprior: -1.6996e+01
Fitted a model with MAP estimate = -165.2527
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 190.4859 - loglik: -1.5078e+02 - logprior: -3.9708e+01
Epoch 2/10
10/10 - 1s - loss: 162.0511 - loglik: -1.5117e+02 - logprior: -1.0881e+01
Epoch 3/10
10/10 - 1s - loss: 155.2968 - loglik: -1.5126e+02 - logprior: -4.0343e+00
Epoch 4/10
10/10 - 1s - loss: 152.6232 - loglik: -1.5083e+02 - logprior: -1.7899e+00
Epoch 5/10
10/10 - 1s - loss: 152.0859 - loglik: -1.5126e+02 - logprior: -8.2204e-01
Epoch 6/10
10/10 - 1s - loss: 151.3392 - loglik: -1.5118e+02 - logprior: -1.5799e-01
Epoch 7/10
10/10 - 1s - loss: 151.0723 - loglik: -1.5147e+02 - logprior: 0.4026
Epoch 8/10
10/10 - 1s - loss: 150.7379 - loglik: -1.5149e+02 - logprior: 0.7503
Epoch 9/10
10/10 - 1s - loss: 151.0016 - loglik: -1.5194e+02 - logprior: 0.9369
Fitted a model with MAP estimate = -150.5778
Time for alignment: 46.0633
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 274.6086 - loglik: -2.3647e+02 - logprior: -3.8137e+01
Epoch 2/10
10/10 - 1s - loss: 223.1607 - loglik: -2.1310e+02 - logprior: -1.0060e+01
Epoch 3/10
10/10 - 1s - loss: 196.3427 - loglik: -1.9127e+02 - logprior: -5.0686e+00
Epoch 4/10
10/10 - 1s - loss: 180.6648 - loglik: -1.7726e+02 - logprior: -3.4081e+00
Epoch 5/10
10/10 - 1s - loss: 174.2545 - loglik: -1.7174e+02 - logprior: -2.5118e+00
Epoch 6/10
10/10 - 1s - loss: 171.7877 - loglik: -1.6972e+02 - logprior: -2.0637e+00
Epoch 7/10
10/10 - 1s - loss: 170.1500 - loglik: -1.6825e+02 - logprior: -1.8961e+00
Epoch 8/10
10/10 - 1s - loss: 169.4021 - loglik: -1.6759e+02 - logprior: -1.8156e+00
Epoch 9/10
10/10 - 1s - loss: 168.8228 - loglik: -1.6710e+02 - logprior: -1.7262e+00
Epoch 10/10
10/10 - 1s - loss: 168.0674 - loglik: -1.6639e+02 - logprior: -1.6763e+00
Fitted a model with MAP estimate = -168.1197
expansions: [(7, 2), (12, 1), (15, 2), (18, 1), (20, 2), (24, 3), (30, 1), (32, 1), (43, 1), (55, 3), (58, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 205.7843 - loglik: -1.6298e+02 - logprior: -4.2802e+01
Epoch 2/2
10/10 - 1s - loss: 172.5375 - loglik: -1.5508e+02 - logprior: -1.7461e+01
Fitted a model with MAP estimate = -166.9782
expansions: [(0, 1)]
discards: [ 0 17 25 69]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 185.1709 - loglik: -1.5130e+02 - logprior: -3.3870e+01
Epoch 2/2
10/10 - 1s - loss: 159.6523 - loglik: -1.5091e+02 - logprior: -8.7401e+00
Fitted a model with MAP estimate = -155.6196
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 183.0822 - loglik: -1.4958e+02 - logprior: -3.3507e+01
Epoch 2/10
10/10 - 1s - loss: 158.2675 - loglik: -1.4975e+02 - logprior: -8.5151e+00
Epoch 3/10
10/10 - 1s - loss: 153.8628 - loglik: -1.5044e+02 - logprior: -3.4240e+00
Epoch 4/10
10/10 - 1s - loss: 152.2461 - loglik: -1.5087e+02 - logprior: -1.3725e+00
Epoch 5/10
10/10 - 1s - loss: 151.6661 - loglik: -1.5138e+02 - logprior: -2.8940e-01
Epoch 6/10
10/10 - 1s - loss: 151.0999 - loglik: -1.5137e+02 - logprior: 0.2743
Epoch 7/10
10/10 - 1s - loss: 150.7398 - loglik: -1.5131e+02 - logprior: 0.5738
Epoch 8/10
10/10 - 1s - loss: 150.6353 - loglik: -1.5141e+02 - logprior: 0.7792
Epoch 9/10
10/10 - 1s - loss: 150.5351 - loglik: -1.5149e+02 - logprior: 0.9535
Epoch 10/10
10/10 - 1s - loss: 150.4809 - loglik: -1.5160e+02 - logprior: 1.1194
Fitted a model with MAP estimate = -150.2892
Time for alignment: 47.1292
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 274.8907 - loglik: -2.3675e+02 - logprior: -3.8136e+01
Epoch 2/10
10/10 - 1s - loss: 223.4338 - loglik: -2.1337e+02 - logprior: -1.0064e+01
Epoch 3/10
10/10 - 1s - loss: 197.0775 - loglik: -1.9197e+02 - logprior: -5.1036e+00
Epoch 4/10
10/10 - 1s - loss: 182.4466 - loglik: -1.7907e+02 - logprior: -3.3782e+00
Epoch 5/10
10/10 - 1s - loss: 175.5999 - loglik: -1.7314e+02 - logprior: -2.4565e+00
Epoch 6/10
10/10 - 1s - loss: 172.8977 - loglik: -1.7085e+02 - logprior: -2.0526e+00
Epoch 7/10
10/10 - 1s - loss: 172.0718 - loglik: -1.7025e+02 - logprior: -1.8206e+00
Epoch 8/10
10/10 - 1s - loss: 171.0721 - loglik: -1.6942e+02 - logprior: -1.6495e+00
Epoch 9/10
10/10 - 1s - loss: 170.4375 - loglik: -1.6887e+02 - logprior: -1.5632e+00
Epoch 10/10
10/10 - 1s - loss: 169.8390 - loglik: -1.6823e+02 - logprior: -1.6047e+00
Fitted a model with MAP estimate = -169.3539
expansions: [(7, 2), (12, 1), (14, 2), (15, 1), (20, 1), (23, 2), (30, 1), (38, 1), (46, 1), (55, 2), (58, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 205.5603 - loglik: -1.6274e+02 - logprior: -4.2817e+01
Epoch 2/2
10/10 - 1s - loss: 172.8888 - loglik: -1.5550e+02 - logprior: -1.7393e+01
Fitted a model with MAP estimate = -167.1491
expansions: [(0, 1)]
discards: [ 0 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 185.3605 - loglik: -1.5140e+02 - logprior: -3.3960e+01
Epoch 2/2
10/10 - 1s - loss: 160.0873 - loglik: -1.5127e+02 - logprior: -8.8183e+00
Fitted a model with MAP estimate = -156.2276
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 183.3012 - loglik: -1.4970e+02 - logprior: -3.3606e+01
Epoch 2/10
10/10 - 1s - loss: 159.3629 - loglik: -1.5074e+02 - logprior: -8.6230e+00
Epoch 3/10
10/10 - 1s - loss: 154.7079 - loglik: -1.5118e+02 - logprior: -3.5314e+00
Epoch 4/10
10/10 - 1s - loss: 152.9827 - loglik: -1.5150e+02 - logprior: -1.4795e+00
Epoch 5/10
10/10 - 1s - loss: 152.2129 - loglik: -1.5181e+02 - logprior: -3.9828e-01
Epoch 6/10
10/10 - 1s - loss: 151.7699 - loglik: -1.5193e+02 - logprior: 0.1644
Epoch 7/10
10/10 - 1s - loss: 151.5408 - loglik: -1.5200e+02 - logprior: 0.4622
Epoch 8/10
10/10 - 1s - loss: 151.2539 - loglik: -1.5191e+02 - logprior: 0.6600
Epoch 9/10
10/10 - 1s - loss: 151.4387 - loglik: -1.5227e+02 - logprior: 0.8350
Fitted a model with MAP estimate = -151.1563
Time for alignment: 46.0772
Computed alignments with likelihoods: ['-150.5778', '-150.2892', '-151.1563']
Best model has likelihood: -150.2892
SP score = 0.7994
Training of 3 independent models on file Ald_Xan_dh_2.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c04c3190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cbe9b370>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 87 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 323.0325 - loglik: -3.0620e+02 - logprior: -1.6829e+01
Epoch 2/10
10/10 - 6s - loss: 279.0040 - loglik: -2.7472e+02 - logprior: -4.2873e+00
Epoch 3/10
10/10 - 5s - loss: 246.4325 - loglik: -2.4413e+02 - logprior: -2.2988e+00
Epoch 4/10
10/10 - 6s - loss: 229.9611 - loglik: -2.2799e+02 - logprior: -1.9731e+00
Epoch 5/10
10/10 - 5s - loss: 222.1791 - loglik: -2.2030e+02 - logprior: -1.8777e+00
Epoch 6/10
10/10 - 5s - loss: 219.2703 - loglik: -2.1751e+02 - logprior: -1.7647e+00
Epoch 7/10
10/10 - 5s - loss: 219.5009 - loglik: -2.1791e+02 - logprior: -1.5869e+00
Fitted a model with MAP estimate = -218.1649
expansions: [(10, 3), (11, 3), (26, 1), (29, 1), (33, 1), (43, 1), (47, 4), (57, 1), (73, 4), (74, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 231.7763 - loglik: -2.1298e+02 - logprior: -1.8793e+01
Epoch 2/2
10/10 - 6s - loss: 211.1940 - loglik: -2.0359e+02 - logprior: -7.6032e+00
Fitted a model with MAP estimate = -207.7046
expansions: [(0, 2)]
discards: [ 0 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 214.6964 - loglik: -1.9987e+02 - logprior: -1.4822e+01
Epoch 2/2
10/10 - 6s - loss: 201.8095 - loglik: -1.9821e+02 - logprior: -3.5967e+00
Fitted a model with MAP estimate = -200.0402
expansions: [(56, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 218.3091 - loglik: -2.0021e+02 - logprior: -1.8095e+01
Epoch 2/10
10/10 - 6s - loss: 204.4171 - loglik: -1.9885e+02 - logprior: -5.5700e+00
Epoch 3/10
10/10 - 6s - loss: 200.7858 - loglik: -1.9860e+02 - logprior: -2.1820e+00
Epoch 4/10
10/10 - 6s - loss: 198.9638 - loglik: -1.9783e+02 - logprior: -1.1328e+00
Epoch 5/10
10/10 - 6s - loss: 198.7693 - loglik: -1.9831e+02 - logprior: -4.5969e-01
Epoch 6/10
10/10 - 6s - loss: 198.1195 - loglik: -1.9797e+02 - logprior: -1.4641e-01
Epoch 7/10
10/10 - 7s - loss: 198.8838 - loglik: -1.9886e+02 - logprior: -2.8421e-02
Fitted a model with MAP estimate = -198.1275
Time for alignment: 129.0691
Fitting a model of length 87 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 323.1570 - loglik: -3.0633e+02 - logprior: -1.6830e+01
Epoch 2/10
10/10 - 5s - loss: 278.3480 - loglik: -2.7406e+02 - logprior: -4.2837e+00
Epoch 3/10
10/10 - 5s - loss: 246.1944 - loglik: -2.4388e+02 - logprior: -2.3098e+00
Epoch 4/10
10/10 - 5s - loss: 228.0915 - loglik: -2.2607e+02 - logprior: -2.0238e+00
Epoch 5/10
10/10 - 5s - loss: 221.0804 - loglik: -2.1916e+02 - logprior: -1.9221e+00
Epoch 6/10
10/10 - 4s - loss: 217.1565 - loglik: -2.1532e+02 - logprior: -1.8415e+00
Epoch 7/10
10/10 - 5s - loss: 216.6628 - loglik: -2.1494e+02 - logprior: -1.7196e+00
Epoch 8/10
10/10 - 5s - loss: 215.8869 - loglik: -2.1431e+02 - logprior: -1.5809e+00
Epoch 9/10
10/10 - 5s - loss: 215.0870 - loglik: -2.1359e+02 - logprior: -1.4947e+00
Epoch 10/10
10/10 - 5s - loss: 214.9367 - loglik: -2.1345e+02 - logprior: -1.4879e+00
Fitted a model with MAP estimate = -215.0015
expansions: [(10, 3), (11, 2), (12, 1), (15, 1), (26, 1), (29, 1), (34, 1), (47, 4), (48, 4), (49, 3), (60, 1), (72, 4), (73, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 113 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 230.8935 - loglik: -2.1214e+02 - logprior: -1.8751e+01
Epoch 2/2
10/10 - 7s - loss: 210.4492 - loglik: -2.0287e+02 - logprior: -7.5837e+00
Fitted a model with MAP estimate = -206.7610
expansions: [(0, 2)]
discards: [ 0 13 14 58 59]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 214.2515 - loglik: -1.9947e+02 - logprior: -1.4785e+01
Epoch 2/2
10/10 - 6s - loss: 201.2425 - loglik: -1.9764e+02 - logprior: -3.6002e+00
Fitted a model with MAP estimate = -199.2701
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 216.8457 - loglik: -1.9935e+02 - logprior: -1.7496e+01
Epoch 2/10
10/10 - 6s - loss: 203.6206 - loglik: -1.9881e+02 - logprior: -4.8132e+00
Epoch 3/10
10/10 - 6s - loss: 199.6881 - loglik: -1.9762e+02 - logprior: -2.0726e+00
Epoch 4/10
10/10 - 7s - loss: 198.9655 - loglik: -1.9771e+02 - logprior: -1.2549e+00
Epoch 5/10
10/10 - 7s - loss: 197.8127 - loglik: -1.9724e+02 - logprior: -5.7202e-01
Epoch 6/10
10/10 - 7s - loss: 198.6021 - loglik: -1.9832e+02 - logprior: -2.8272e-01
Fitted a model with MAP estimate = -197.8854
Time for alignment: 141.1567
Fitting a model of length 87 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 323.0098 - loglik: -3.0618e+02 - logprior: -1.6829e+01
Epoch 2/10
10/10 - 5s - loss: 278.3131 - loglik: -2.7404e+02 - logprior: -4.2776e+00
Epoch 3/10
10/10 - 5s - loss: 245.3961 - loglik: -2.4314e+02 - logprior: -2.2572e+00
Epoch 4/10
10/10 - 5s - loss: 227.9651 - loglik: -2.2603e+02 - logprior: -1.9383e+00
Epoch 5/10
10/10 - 6s - loss: 221.6123 - loglik: -2.1976e+02 - logprior: -1.8540e+00
Epoch 6/10
10/10 - 4s - loss: 218.6464 - loglik: -2.1690e+02 - logprior: -1.7459e+00
Epoch 7/10
10/10 - 4s - loss: 217.7080 - loglik: -2.1611e+02 - logprior: -1.6018e+00
Epoch 8/10
10/10 - 6s - loss: 217.7377 - loglik: -2.1626e+02 - logprior: -1.4822e+00
Fitted a model with MAP estimate = -217.0226
expansions: [(10, 3), (11, 2), (12, 1), (15, 1), (26, 1), (27, 1), (43, 1), (47, 4), (48, 4), (49, 1), (50, 2), (72, 1), (73, 3), (74, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 113 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 231.5710 - loglik: -2.1281e+02 - logprior: -1.8758e+01
Epoch 2/2
10/10 - 6s - loss: 210.4322 - loglik: -2.0285e+02 - logprior: -7.5797e+00
Fitted a model with MAP estimate = -207.0077
expansions: [(0, 2)]
discards: [ 0 13 14 59 60 68]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 214.4259 - loglik: -1.9964e+02 - logprior: -1.4787e+01
Epoch 2/2
10/10 - 6s - loss: 201.6359 - loglik: -1.9803e+02 - logprior: -3.6043e+00
Fitted a model with MAP estimate = -199.6718
expansions: []
discards: [ 0 57]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 217.6394 - loglik: -2.0015e+02 - logprior: -1.7492e+01
Epoch 2/10
10/10 - 7s - loss: 204.2137 - loglik: -1.9939e+02 - logprior: -4.8245e+00
Epoch 3/10
10/10 - 6s - loss: 200.3357 - loglik: -1.9826e+02 - logprior: -2.0731e+00
Epoch 4/10
10/10 - 6s - loss: 199.4710 - loglik: -1.9822e+02 - logprior: -1.2484e+00
Epoch 5/10
10/10 - 5s - loss: 198.3302 - loglik: -1.9777e+02 - logprior: -5.5707e-01
Epoch 6/10
10/10 - 5s - loss: 198.6638 - loglik: -1.9839e+02 - logprior: -2.7500e-01
Fitted a model with MAP estimate = -198.5052
Time for alignment: 126.3177
Computed alignments with likelihoods: ['-198.1275', '-197.8854', '-198.5052']
Best model has likelihood: -197.8854
SP score = 0.1709
Training of 3 independent models on file ace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c2a868b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cc6cda30>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 404 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 73s - loss: 1198.4369 - loglik: -1.1949e+03 - logprior: -3.4977e+00
Epoch 2/10
24/24 - 81s - loss: 1004.6011 - loglik: -1.0053e+03 - logprior: 0.7073
Epoch 3/10
24/24 - 78s - loss: 962.8047 - loglik: -9.6297e+02 - logprior: 0.1617
Epoch 4/10
24/24 - 80s - loss: 955.5281 - loglik: -9.5568e+02 - logprior: 0.1542
Epoch 5/10
24/24 - 75s - loss: 952.2708 - loglik: -9.5241e+02 - logprior: 0.1376
Epoch 6/10
24/24 - 75s - loss: 954.1135 - loglik: -9.5432e+02 - logprior: 0.2050
Fitted a model with MAP estimate = -952.6847
expansions: [(0, 5), (141, 1), (163, 2), (182, 1), (210, 1), (212, 1), (213, 1), (227, 1), (229, 4), (230, 3), (231, 3), (233, 2), (237, 2), (238, 2), (239, 1), (248, 1), (251, 1), (252, 6), (253, 2), (254, 2), (270, 2), (271, 7), (291, 2), (292, 5), (296, 2), (299, 6), (304, 1), (308, 1), (312, 3), (321, 1), (323, 1), (324, 2), (325, 2), (326, 1), (344, 3), (346, 1), (374, 2), (375, 2), (391, 3), (392, 1), (394, 10), (404, 6)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 35 84]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 491 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 107s - loss: 945.5536 - loglik: -9.3983e+02 - logprior: -5.7224e+00
Epoch 2/2
24/24 - 100s - loss: 928.9579 - loglik: -9.2914e+02 - logprior: 0.1773
Fitted a model with MAP estimate = -923.2368
expansions: [(0, 6), (23, 3), (298, 1), (358, 1), (438, 1), (471, 1), (476, 5)]
discards: [  1   2   3   4   5   6 150 327 379 480 481 482 483 484 485 486 487 488
 489 490]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 489 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 115s - loss: 935.7020 - loglik: -9.3025e+02 - logprior: -5.4518e+00
Epoch 2/2
24/24 - 113s - loss: 919.2299 - loglik: -9.2060e+02 - logprior: 1.3671
Fitted a model with MAP estimate = -920.0585
expansions: [(0, 6), (324, 1), (325, 1), (472, 1), (483, 1), (484, 1), (489, 5)]
discards: [  0   1   2   3   4   5  23  24  25 272 274]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 494 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 106s - loss: 931.6430 - loglik: -9.2792e+02 - logprior: -3.7267e+00
Epoch 2/10
24/24 - 103s - loss: 914.6678 - loglik: -9.1656e+02 - logprior: 1.8881
Epoch 3/10
24/24 - 99s - loss: 917.6355 - loglik: -9.2057e+02 - logprior: 2.9318
Fitted a model with MAP estimate = -916.5668
Time for alignment: 1546.7257
Fitting a model of length 404 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 80s - loss: 1193.1372 - loglik: -1.1897e+03 - logprior: -3.4720e+00
Epoch 2/10
24/24 - 82s - loss: 1009.0237 - loglik: -1.0097e+03 - logprior: 0.6979
Epoch 3/10
24/24 - 78s - loss: 955.4612 - loglik: -9.5536e+02 - logprior: -9.7107e-02
Epoch 4/10
24/24 - 83s - loss: 951.8635 - loglik: -9.5184e+02 - logprior: -2.2151e-02
Epoch 5/10
24/24 - 85s - loss: 947.7335 - loglik: -9.4778e+02 - logprior: 0.0453
Epoch 6/10
24/24 - 83s - loss: 946.7293 - loglik: -9.4677e+02 - logprior: 0.0448
Epoch 7/10
24/24 - 72s - loss: 945.7857 - loglik: -9.4574e+02 - logprior: -4.9233e-02
Epoch 8/10
24/24 - 72s - loss: 951.1028 - loglik: -9.5117e+02 - logprior: 0.0693
Fitted a model with MAP estimate = -946.5321
expansions: [(0, 5), (132, 1), (140, 1), (193, 1), (213, 1), (214, 1), (232, 3), (233, 2), (234, 4), (236, 1), (241, 2), (243, 1), (244, 1), (245, 1), (252, 1), (255, 1), (256, 3), (257, 4), (258, 2), (259, 1), (275, 1), (276, 1), (277, 2), (278, 1), (279, 2), (296, 2), (297, 2), (298, 2), (299, 5), (300, 3), (301, 8), (303, 1), (304, 1), (308, 1), (309, 5), (322, 1), (323, 1), (324, 1), (346, 5), (347, 1), (348, 1), (350, 1), (364, 1), (377, 1), (395, 1), (399, 11), (400, 2), (404, 6)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 33 34 35]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 493 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 96s - loss: 943.8282 - loglik: -9.3837e+02 - logprior: -5.4585e+00
Epoch 2/2
24/24 - 102s - loss: 924.0334 - loglik: -9.2497e+02 - logprior: 0.9358
Fitted a model with MAP estimate = -919.4826
expansions: [(0, 6), (328, 2), (358, 1), (474, 1)]
discards: [  1   2   3   4   5   6   7 274 407 487 488 489 490 491 492]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 488 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 120s - loss: 931.3599 - loglik: -9.2592e+02 - logprior: -5.4371e+00
Epoch 2/2
24/24 - 119s - loss: 922.0200 - loglik: -9.2355e+02 - logprior: 1.5331
Fitted a model with MAP estimate = -919.3436
expansions: [(0, 6), (7, 3)]
discards: [  0   1   2   3   4   5 326 327 483 484 485 486 487]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 484 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 122s - loss: 929.1953 - loglik: -9.2559e+02 - logprior: -3.6056e+00
Epoch 2/10
24/24 - 105s - loss: 922.7820 - loglik: -9.2538e+02 - logprior: 2.5933
Epoch 3/10
24/24 - 105s - loss: 922.1873 - loglik: -9.2567e+02 - logprior: 3.4812
Epoch 4/10
24/24 - 95s - loss: 918.1970 - loglik: -9.2218e+02 - logprior: 3.9821
Epoch 5/10
24/24 - 106s - loss: 919.4333 - loglik: -9.2380e+02 - logprior: 4.3647
Fitted a model with MAP estimate = -918.3081
Time for alignment: 1936.3673
Fitting a model of length 404 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 67s - loss: 1192.0211 - loglik: -1.1885e+03 - logprior: -3.5134e+00
Epoch 2/10
24/24 - 74s - loss: 1009.0120 - loglik: -1.0098e+03 - logprior: 0.7426
Epoch 3/10
24/24 - 84s - loss: 956.6955 - loglik: -9.5663e+02 - logprior: -6.7986e-02
Epoch 4/10
24/24 - 82s - loss: 949.0882 - loglik: -9.4908e+02 - logprior: -1.0890e-02
Epoch 5/10
24/24 - 86s - loss: 950.0974 - loglik: -9.5015e+02 - logprior: 0.0529
Fitted a model with MAP estimate = -948.9739
expansions: [(0, 5), (142, 1), (184, 1), (215, 1), (216, 1), (230, 1), (233, 3), (234, 2), (235, 2), (236, 1), (240, 1), (248, 1), (255, 2), (256, 1), (257, 1), (258, 2), (259, 7), (260, 2), (276, 1), (277, 1), (278, 2), (279, 1), (280, 2), (292, 1), (296, 2), (297, 2), (298, 5), (299, 3), (300, 3), (301, 7), (302, 2), (312, 4), (322, 1), (323, 2), (324, 1), (325, 1), (346, 5), (347, 1), (348, 1), (364, 1), (365, 2), (376, 1), (391, 2), (392, 4), (404, 6)]
discards: [ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 34 35 36 83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 484 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 104s - loss: 943.8537 - loglik: -9.3852e+02 - logprior: -5.3303e+00
Epoch 2/2
24/24 - 102s - loss: 928.9541 - loglik: -9.3009e+02 - logprior: 1.1371
Fitted a model with MAP estimate = -922.5810
expansions: [(0, 6), (271, 1), (358, 2), (474, 2), (482, 3)]
discards: [  1   2   3   4   5   6   7 228 462]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 489 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 103s - loss: 932.4175 - loglik: -9.2684e+02 - logprior: -5.5747e+00
Epoch 2/2
24/24 - 104s - loss: 919.9563 - loglik: -9.2125e+02 - logprior: 1.2951
Fitted a model with MAP estimate = -919.0077
expansions: [(0, 6), (7, 2), (472, 1)]
discards: [  0   1   2   3   4   5 269 484 485 486 487 488]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 486 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 125s - loss: 929.5720 - loglik: -9.2629e+02 - logprior: -3.2814e+00
Epoch 2/10
24/24 - 93s - loss: 921.3397 - loglik: -9.2385e+02 - logprior: 2.5126
Epoch 3/10
24/24 - 77s - loss: 919.8271 - loglik: -9.2324e+02 - logprior: 3.4114
Epoch 4/10
24/24 - 74s - loss: 918.8811 - loglik: -9.2262e+02 - logprior: 3.7406
Epoch 5/10
24/24 - 76s - loss: 919.3216 - loglik: -9.2353e+02 - logprior: 4.2108
Fitted a model with MAP estimate = -917.8364
Time for alignment: 1597.1100
Computed alignments with likelihoods: ['-916.5668', '-918.3081', '-917.8364']
Best model has likelihood: -916.5668
SP score = 0.7959
Training of 3 independent models on file tRNA-synt_2b.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb80c570880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9f7d6a190>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 136 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 513.5637 - loglik: -5.1071e+02 - logprior: -2.8509e+00
Epoch 2/10
19/19 - 8s - loss: 445.4763 - loglik: -4.4438e+02 - logprior: -1.0919e+00
Epoch 3/10
19/19 - 8s - loss: 412.7004 - loglik: -4.1110e+02 - logprior: -1.6048e+00
Epoch 4/10
19/19 - 8s - loss: 403.8779 - loglik: -4.0221e+02 - logprior: -1.6671e+00
Epoch 5/10
19/19 - 8s - loss: 399.8441 - loglik: -3.9811e+02 - logprior: -1.7330e+00
Epoch 6/10
19/19 - 8s - loss: 398.5764 - loglik: -3.9687e+02 - logprior: -1.7066e+00
Epoch 7/10
19/19 - 8s - loss: 399.2937 - loglik: -3.9759e+02 - logprior: -1.7062e+00
Fitted a model with MAP estimate = -397.5981
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 2), (23, 1), (26, 1), (31, 1), (38, 2), (39, 1), (40, 1), (43, 1), (44, 1), (54, 2), (56, 2), (57, 1), (58, 2), (71, 2), (79, 1), (89, 2), (90, 2), (93, 1), (95, 1), (99, 1), (103, 1), (104, 2), (106, 1), (109, 1), (118, 1), (119, 1), (126, 2), (131, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 179 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 398.8244 - loglik: -3.9483e+02 - logprior: -3.9930e+00
Epoch 2/2
19/19 - 11s - loss: 382.0922 - loglik: -3.8006e+02 - logprior: -2.0307e+00
Fitted a model with MAP estimate = -382.1790
expansions: [(0, 2)]
discards: [  0  13  28  49  71  74  79  94 116 118]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 381.1966 - loglik: -3.7837e+02 - logprior: -2.8249e+00
Epoch 2/2
19/19 - 10s - loss: 377.9431 - loglik: -3.7694e+02 - logprior: -1.0058e+00
Fitted a model with MAP estimate = -379.9077
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 170 on 11293 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 13s - loss: 382.7480 - loglik: -3.7988e+02 - logprior: -2.8726e+00
Epoch 2/10
20/20 - 11s - loss: 378.5847 - loglik: -3.7771e+02 - logprior: -8.7532e-01
Epoch 3/10
20/20 - 11s - loss: 377.1385 - loglik: -3.7645e+02 - logprior: -6.8971e-01
Epoch 4/10
20/20 - 11s - loss: 375.8109 - loglik: -3.7514e+02 - logprior: -6.6954e-01
Epoch 5/10
20/20 - 12s - loss: 376.7639 - loglik: -3.7613e+02 - logprior: -6.3359e-01
Fitted a model with MAP estimate = -375.9523
Time for alignment: 216.7122
Fitting a model of length 136 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 513.6865 - loglik: -5.1083e+02 - logprior: -2.8526e+00
Epoch 2/10
19/19 - 8s - loss: 446.2713 - loglik: -4.4517e+02 - logprior: -1.1019e+00
Epoch 3/10
19/19 - 8s - loss: 416.5870 - loglik: -4.1501e+02 - logprior: -1.5798e+00
Epoch 4/10
19/19 - 9s - loss: 408.2992 - loglik: -4.0669e+02 - logprior: -1.6112e+00
Epoch 5/10
19/19 - 9s - loss: 404.5243 - loglik: -4.0285e+02 - logprior: -1.6719e+00
Epoch 6/10
19/19 - 9s - loss: 403.7739 - loglik: -4.0212e+02 - logprior: -1.6512e+00
Epoch 7/10
19/19 - 9s - loss: 402.8214 - loglik: -4.0118e+02 - logprior: -1.6415e+00
Epoch 8/10
19/19 - 9s - loss: 401.3809 - loglik: -3.9975e+02 - logprior: -1.6289e+00
Epoch 9/10
19/19 - 10s - loss: 400.0807 - loglik: -3.9847e+02 - logprior: -1.6155e+00
Epoch 10/10
19/19 - 10s - loss: 399.1400 - loglik: -3.9753e+02 - logprior: -1.6127e+00
Fitted a model with MAP estimate = -399.7925
expansions: [(9, 2), (10, 2), (11, 2), (12, 2), (22, 2), (23, 2), (26, 1), (39, 1), (40, 1), (41, 1), (44, 1), (45, 1), (55, 2), (56, 1), (57, 1), (58, 1), (59, 1), (71, 2), (80, 2), (88, 2), (93, 1), (95, 1), (99, 1), (103, 1), (104, 1), (105, 1), (106, 1), (109, 1), (118, 1), (119, 1), (122, 1), (126, 1), (131, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 398.7897 - loglik: -3.9484e+02 - logprior: -3.9529e+00
Epoch 2/2
19/19 - 13s - loss: 383.3142 - loglik: -3.8133e+02 - logprior: -1.9810e+00
Fitted a model with MAP estimate = -383.5072
expansions: [(0, 2)]
discards: [  0  12  17  30  32  73  94 115]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 172 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 381.0621 - loglik: -3.7826e+02 - logprior: -2.8007e+00
Epoch 2/2
19/19 - 13s - loss: 378.0332 - loglik: -3.7705e+02 - logprior: -9.8387e-01
Fitted a model with MAP estimate = -380.4432
expansions: []
discards: [  0   9 101]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 11293 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 16s - loss: 384.5852 - loglik: -3.8177e+02 - logprior: -2.8128e+00
Epoch 2/10
20/20 - 13s - loss: 379.7729 - loglik: -3.7895e+02 - logprior: -8.2626e-01
Epoch 3/10
20/20 - 13s - loss: 378.7616 - loglik: -3.7810e+02 - logprior: -6.6660e-01
Epoch 4/10
20/20 - 13s - loss: 377.1086 - loglik: -3.7649e+02 - logprior: -6.1486e-01
Epoch 5/10
20/20 - 14s - loss: 377.2170 - loglik: -3.7663e+02 - logprior: -5.8790e-01
Fitted a model with MAP estimate = -376.1506
Time for alignment: 278.2271
Fitting a model of length 136 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 513.1084 - loglik: -5.1027e+02 - logprior: -2.8396e+00
Epoch 2/10
19/19 - 9s - loss: 441.9510 - loglik: -4.4089e+02 - logprior: -1.0602e+00
Epoch 3/10
19/19 - 9s - loss: 412.2662 - loglik: -4.1070e+02 - logprior: -1.5705e+00
Epoch 4/10
19/19 - 9s - loss: 403.9006 - loglik: -4.0229e+02 - logprior: -1.6139e+00
Epoch 5/10
19/19 - 9s - loss: 399.8811 - loglik: -3.9820e+02 - logprior: -1.6840e+00
Epoch 6/10
19/19 - 9s - loss: 399.5170 - loglik: -3.9784e+02 - logprior: -1.6731e+00
Epoch 7/10
19/19 - 9s - loss: 398.4897 - loglik: -3.9683e+02 - logprior: -1.6635e+00
Epoch 8/10
19/19 - 9s - loss: 396.9653 - loglik: -3.9532e+02 - logprior: -1.6488e+00
Epoch 9/10
19/19 - 10s - loss: 398.2161 - loglik: -3.9657e+02 - logprior: -1.6501e+00
Fitted a model with MAP estimate = -397.4912
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 2), (23, 2), (26, 1), (39, 1), (40, 1), (41, 2), (44, 1), (45, 1), (55, 1), (57, 2), (58, 3), (71, 2), (80, 1), (89, 2), (92, 1), (93, 2), (95, 1), (99, 1), (103, 1), (104, 1), (105, 1), (106, 1), (109, 1), (118, 1), (122, 1), (126, 2), (133, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 179 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 17s - loss: 398.8081 - loglik: -3.9489e+02 - logprior: -3.9212e+00
Epoch 2/2
19/19 - 15s - loss: 382.3043 - loglik: -3.8034e+02 - logprior: -1.9605e+00
Fitted a model with MAP estimate = -382.0351
expansions: [(0, 2)]
discards: [  0  13  28  30  53  75  93 114 121 166]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 17s - loss: 380.1287 - loglik: -3.7734e+02 - logprior: -2.7869e+00
Epoch 2/2
19/19 - 14s - loss: 377.7735 - loglik: -3.7682e+02 - logprior: -9.5099e-01
Fitted a model with MAP estimate = -379.2947
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 170 on 11293 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 18s - loss: 382.2202 - loglik: -3.7935e+02 - logprior: -2.8654e+00
Epoch 2/10
20/20 - 15s - loss: 378.5914 - loglik: -3.7766e+02 - logprior: -9.3080e-01
Epoch 3/10
20/20 - 15s - loss: 375.7091 - loglik: -3.7497e+02 - logprior: -7.3807e-01
Epoch 4/10
20/20 - 16s - loss: 376.3602 - loglik: -3.7568e+02 - logprior: -6.8270e-01
Fitted a model with MAP estimate = -375.8154
Time for alignment: 273.1129
Computed alignments with likelihoods: ['-375.9523', '-376.1506', '-375.8154']
Best model has likelihood: -375.8154
SP score = 0.4758
Training of 3 independent models on file ChtBD.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbb93f4f430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fba1169abb0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 170.5121 - loglik: -1.1161e+02 - logprior: -5.8900e+01
Epoch 2/10
10/10 - 0s - loss: 105.5428 - loglik: -8.8971e+01 - logprior: -1.6572e+01
Epoch 3/10
10/10 - 0s - loss: 80.7120 - loglik: -7.2460e+01 - logprior: -8.2518e+00
Epoch 4/10
10/10 - 0s - loss: 69.8539 - loglik: -6.4658e+01 - logprior: -5.1959e+00
Epoch 5/10
10/10 - 0s - loss: 65.7423 - loglik: -6.2102e+01 - logprior: -3.6401e+00
Epoch 6/10
10/10 - 0s - loss: 63.9899 - loglik: -6.1153e+01 - logprior: -2.8372e+00
Epoch 7/10
10/10 - 0s - loss: 63.3823 - loglik: -6.1002e+01 - logprior: -2.3806e+00
Epoch 8/10
10/10 - 0s - loss: 63.1198 - loglik: -6.1045e+01 - logprior: -2.0748e+00
Epoch 9/10
10/10 - 0s - loss: 62.6927 - loglik: -6.0826e+01 - logprior: -1.8667e+00
Epoch 10/10
10/10 - 0s - loss: 62.6677 - loglik: -6.0916e+01 - logprior: -1.7521e+00
Fitted a model with MAP estimate = -62.4313
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 137.0696 - loglik: -5.8026e+01 - logprior: -7.9043e+01
Epoch 2/2
10/10 - 0s - loss: 79.1674 - loglik: -5.3559e+01 - logprior: -2.5608e+01
Fitted a model with MAP estimate = -68.4247
expansions: [(0, 2)]
discards: [14 32]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 116.5003 - loglik: -5.0778e+01 - logprior: -6.5722e+01
Epoch 2/2
10/10 - 0s - loss: 73.7183 - loglik: -5.0632e+01 - logprior: -2.3086e+01
Fitted a model with MAP estimate = -64.8202
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 109.8349 - loglik: -5.0721e+01 - logprior: -5.9114e+01
Epoch 2/10
10/10 - 0s - loss: 68.4118 - loglik: -5.1751e+01 - logprior: -1.6661e+01
Epoch 3/10
10/10 - 0s - loss: 60.4128 - loglik: -5.2825e+01 - logprior: -7.5874e+00
Epoch 4/10
10/10 - 0s - loss: 57.6456 - loglik: -5.3433e+01 - logprior: -4.2126e+00
Epoch 5/10
10/10 - 0s - loss: 55.9201 - loglik: -5.3418e+01 - logprior: -2.5025e+00
Epoch 6/10
10/10 - 0s - loss: 54.8638 - loglik: -5.3319e+01 - logprior: -1.5446e+00
Epoch 7/10
10/10 - 0s - loss: 54.3748 - loglik: -5.3382e+01 - logprior: -9.9277e-01
Epoch 8/10
10/10 - 0s - loss: 54.0047 - loglik: -5.3355e+01 - logprior: -6.4944e-01
Epoch 9/10
10/10 - 0s - loss: 53.8282 - loglik: -5.3432e+01 - logprior: -3.9595e-01
Epoch 10/10
10/10 - 0s - loss: 53.6593 - loglik: -5.3485e+01 - logprior: -1.7406e-01
Fitted a model with MAP estimate = -53.6352
Time for alignment: 25.2724
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 170.5021 - loglik: -1.1160e+02 - logprior: -5.8899e+01
Epoch 2/10
10/10 - 0s - loss: 105.5249 - loglik: -8.8955e+01 - logprior: -1.6570e+01
Epoch 3/10
10/10 - 0s - loss: 80.7468 - loglik: -7.2498e+01 - logprior: -8.2491e+00
Epoch 4/10
10/10 - 0s - loss: 69.9481 - loglik: -6.4755e+01 - logprior: -5.1934e+00
Epoch 5/10
10/10 - 0s - loss: 65.7015 - loglik: -6.2063e+01 - logprior: -3.6390e+00
Epoch 6/10
10/10 - 0s - loss: 64.1362 - loglik: -6.1300e+01 - logprior: -2.8359e+00
Epoch 7/10
10/10 - 0s - loss: 63.3952 - loglik: -6.1016e+01 - logprior: -2.3796e+00
Epoch 8/10
10/10 - 0s - loss: 62.9801 - loglik: -6.0908e+01 - logprior: -2.0717e+00
Epoch 9/10
10/10 - 0s - loss: 62.7561 - loglik: -6.0890e+01 - logprior: -1.8664e+00
Epoch 10/10
10/10 - 0s - loss: 62.6029 - loglik: -6.0855e+01 - logprior: -1.7480e+00
Fitted a model with MAP estimate = -62.4318
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 137.1077 - loglik: -5.8066e+01 - logprior: -7.9042e+01
Epoch 2/2
10/10 - 0s - loss: 79.1445 - loglik: -5.3538e+01 - logprior: -2.5606e+01
Fitted a model with MAP estimate = -68.4131
expansions: [(0, 2)]
discards: [14 32]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 116.5254 - loglik: -5.0803e+01 - logprior: -6.5722e+01
Epoch 2/2
10/10 - 0s - loss: 73.6361 - loglik: -5.0550e+01 - logprior: -2.3086e+01
Fitted a model with MAP estimate = -64.8071
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 116.5936 - loglik: -5.0829e+01 - logprior: -6.5764e+01
Epoch 2/10
10/10 - 0s - loss: 78.9243 - loglik: -5.1635e+01 - logprior: -2.7289e+01
Epoch 3/10
10/10 - 0s - loss: 67.9142 - loglik: -5.1958e+01 - logprior: -1.5956e+01
Epoch 4/10
10/10 - 0s - loss: 58.7582 - loglik: -5.2398e+01 - logprior: -6.3607e+00
Epoch 5/10
10/10 - 0s - loss: 55.2761 - loglik: -5.2578e+01 - logprior: -2.6976e+00
Epoch 6/10
10/10 - 0s - loss: 54.1286 - loglik: -5.2621e+01 - logprior: -1.5073e+00
Epoch 7/10
10/10 - 0s - loss: 53.5953 - loglik: -5.2673e+01 - logprior: -9.2254e-01
Epoch 8/10
10/10 - 0s - loss: 53.4244 - loglik: -5.2843e+01 - logprior: -5.8147e-01
Epoch 9/10
10/10 - 0s - loss: 53.1506 - loglik: -5.2826e+01 - logprior: -3.2510e-01
Epoch 10/10
10/10 - 0s - loss: 53.0920 - loglik: -5.2984e+01 - logprior: -1.0806e-01
Fitted a model with MAP estimate = -52.9802
Time for alignment: 23.7602
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 170.6009 - loglik: -1.1170e+02 - logprior: -5.8899e+01
Epoch 2/10
10/10 - 0s - loss: 105.5116 - loglik: -8.8941e+01 - logprior: -1.6570e+01
Epoch 3/10
10/10 - 0s - loss: 80.6011 - loglik: -7.2351e+01 - logprior: -8.2506e+00
Epoch 4/10
10/10 - 0s - loss: 69.8517 - loglik: -6.4650e+01 - logprior: -5.2021e+00
Epoch 5/10
10/10 - 0s - loss: 65.5393 - loglik: -6.1896e+01 - logprior: -3.6435e+00
Epoch 6/10
10/10 - 0s - loss: 64.1293 - loglik: -6.1291e+01 - logprior: -2.8382e+00
Epoch 7/10
10/10 - 0s - loss: 63.2536 - loglik: -6.0878e+01 - logprior: -2.3759e+00
Epoch 8/10
10/10 - 0s - loss: 63.0368 - loglik: -6.0969e+01 - logprior: -2.0682e+00
Epoch 9/10
10/10 - 0s - loss: 62.8677 - loglik: -6.1003e+01 - logprior: -1.8651e+00
Epoch 10/10
10/10 - 0s - loss: 62.4230 - loglik: -6.0672e+01 - logprior: -1.7512e+00
Fitted a model with MAP estimate = -62.4273
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 137.1033 - loglik: -5.8065e+01 - logprior: -7.9038e+01
Epoch 2/2
10/10 - 0s - loss: 79.1490 - loglik: -5.3545e+01 - logprior: -2.5604e+01
Fitted a model with MAP estimate = -68.4117
expansions: [(0, 2)]
discards: [14 32]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 116.5083 - loglik: -5.0786e+01 - logprior: -6.5722e+01
Epoch 2/2
10/10 - 0s - loss: 73.5727 - loglik: -5.0482e+01 - logprior: -2.3091e+01
Fitted a model with MAP estimate = -64.8134
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 116.6059 - loglik: -5.0845e+01 - logprior: -6.5761e+01
Epoch 2/10
10/10 - 0s - loss: 78.9202 - loglik: -5.1629e+01 - logprior: -2.7291e+01
Epoch 3/10
10/10 - 0s - loss: 67.9352 - loglik: -5.1982e+01 - logprior: -1.5953e+01
Epoch 4/10
10/10 - 0s - loss: 58.7705 - loglik: -5.2409e+01 - logprior: -6.3618e+00
Epoch 5/10
10/10 - 0s - loss: 55.2381 - loglik: -5.2544e+01 - logprior: -2.6938e+00
Epoch 6/10
10/10 - 0s - loss: 54.1158 - loglik: -5.2605e+01 - logprior: -1.5105e+00
Epoch 7/10
10/10 - 0s - loss: 53.6277 - loglik: -5.2705e+01 - logprior: -9.2298e-01
Epoch 8/10
10/10 - 0s - loss: 53.4258 - loglik: -5.2845e+01 - logprior: -5.8053e-01
Epoch 9/10
10/10 - 0s - loss: 53.2395 - loglik: -5.2912e+01 - logprior: -3.2774e-01
Epoch 10/10
10/10 - 0s - loss: 53.0771 - loglik: -5.2966e+01 - logprior: -1.1107e-01
Fitted a model with MAP estimate = -52.9812
Time for alignment: 24.0375
Computed alignments with likelihoods: ['-53.6352', '-52.9802', '-52.9812']
Best model has likelihood: -52.9802
SP score = 0.9203
Training of 3 independent models on file adh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cc2d7370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbbad4727c0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 372.8974 - loglik: -3.7009e+02 - logprior: -2.8124e+00
Epoch 2/10
20/20 - 4s - loss: 329.1774 - loglik: -3.2792e+02 - logprior: -1.2530e+00
Epoch 3/10
20/20 - 4s - loss: 309.8954 - loglik: -3.0842e+02 - logprior: -1.4784e+00
Epoch 4/10
20/20 - 4s - loss: 305.2990 - loglik: -3.0387e+02 - logprior: -1.4278e+00
Epoch 5/10
20/20 - 4s - loss: 303.8317 - loglik: -3.0238e+02 - logprior: -1.4498e+00
Epoch 6/10
20/20 - 4s - loss: 302.9582 - loglik: -3.0153e+02 - logprior: -1.4296e+00
Epoch 7/10
20/20 - 4s - loss: 302.4626 - loglik: -3.0103e+02 - logprior: -1.4300e+00
Epoch 8/10
20/20 - 4s - loss: 302.3480 - loglik: -3.0093e+02 - logprior: -1.4208e+00
Epoch 9/10
20/20 - 4s - loss: 302.3549 - loglik: -3.0093e+02 - logprior: -1.4206e+00
Fitted a model with MAP estimate = -290.5569
expansions: [(5, 1), (8, 2), (11, 2), (13, 2), (19, 1), (20, 1), (31, 1), (35, 1), (36, 2), (38, 1), (39, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 2), (62, 1), (76, 1), (78, 2), (79, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 9s - loss: 302.7955 - loglik: -2.9911e+02 - logprior: -3.6833e+00
Epoch 2/2
20/20 - 6s - loss: 290.9498 - loglik: -2.8919e+02 - logprior: -1.7626e+00
Fitted a model with MAP estimate = -283.5168
expansions: [(0, 1)]
discards: [  0   8  13  46  74  82 105 109]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 10s - loss: 290.7947 - loglik: -2.8801e+02 - logprior: -2.7844e+00
Epoch 2/2
20/20 - 6s - loss: 287.6447 - loglik: -2.8660e+02 - logprior: -1.0440e+00
Fitted a model with MAP estimate = -283.7638
expansions: []
discards: [97]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 21331 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
28/28 - 10s - loss: 278.1779 - loglik: -2.7676e+02 - logprior: -1.4177e+00
Epoch 2/10
28/28 - 7s - loss: 273.3034 - loglik: -2.7253e+02 - logprior: -7.7645e-01
Epoch 3/10
28/28 - 7s - loss: 271.7362 - loglik: -2.7101e+02 - logprior: -7.2548e-01
Epoch 4/10
28/28 - 8s - loss: 270.5670 - loglik: -2.6985e+02 - logprior: -7.1987e-01
Epoch 5/10
28/28 - 7s - loss: 270.2569 - loglik: -2.6956e+02 - logprior: -6.9465e-01
Epoch 6/10
28/28 - 8s - loss: 270.0329 - loglik: -2.6934e+02 - logprior: -6.9181e-01
Epoch 7/10
28/28 - 7s - loss: 268.7269 - loglik: -2.6803e+02 - logprior: -6.9228e-01
Epoch 8/10
28/28 - 9s - loss: 269.1807 - loglik: -2.6850e+02 - logprior: -6.7603e-01
Fitted a model with MAP estimate = -268.6156
Time for alignment: 174.1514
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 372.8684 - loglik: -3.7006e+02 - logprior: -2.8091e+00
Epoch 2/10
20/20 - 5s - loss: 328.7258 - loglik: -3.2747e+02 - logprior: -1.2593e+00
Epoch 3/10
20/20 - 4s - loss: 309.6149 - loglik: -3.0812e+02 - logprior: -1.4974e+00
Epoch 4/10
20/20 - 4s - loss: 304.4963 - loglik: -3.0304e+02 - logprior: -1.4601e+00
Epoch 5/10
20/20 - 4s - loss: 303.0357 - loglik: -3.0156e+02 - logprior: -1.4801e+00
Epoch 6/10
20/20 - 4s - loss: 302.5135 - loglik: -3.0106e+02 - logprior: -1.4521e+00
Epoch 7/10
20/20 - 4s - loss: 301.8668 - loglik: -3.0043e+02 - logprior: -1.4392e+00
Epoch 8/10
20/20 - 4s - loss: 302.0459 - loglik: -3.0062e+02 - logprior: -1.4281e+00
Fitted a model with MAP estimate = -289.9934
expansions: [(5, 1), (8, 2), (10, 1), (13, 2), (19, 1), (20, 1), (31, 1), (35, 1), (36, 2), (38, 1), (39, 1), (46, 1), (57, 1), (58, 1), (59, 1), (60, 2), (61, 2), (62, 2), (63, 2), (76, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 10s - loss: 303.5140 - loglik: -2.9984e+02 - logprior: -3.6721e+00
Epoch 2/2
20/20 - 6s - loss: 291.8358 - loglik: -2.9005e+02 - logprior: -1.7814e+00
Fitted a model with MAP estimate = -284.0101
expansions: [(0, 1)]
discards: [  0   8  45  77  83  84  86 108]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 9s - loss: 290.9963 - loglik: -2.8822e+02 - logprior: -2.7779e+00
Epoch 2/2
20/20 - 6s - loss: 287.6625 - loglik: -2.8664e+02 - logprior: -1.0267e+00
Fitted a model with MAP estimate = -283.7040
expansions: []
discards: [96]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 21331 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
28/28 - 11s - loss: 278.1104 - loglik: -2.7669e+02 - logprior: -1.4193e+00
Epoch 2/10
28/28 - 8s - loss: 273.8420 - loglik: -2.7306e+02 - logprior: -7.8508e-01
Epoch 3/10
28/28 - 7s - loss: 271.5925 - loglik: -2.7085e+02 - logprior: -7.3846e-01
Epoch 4/10
28/28 - 8s - loss: 270.6343 - loglik: -2.6991e+02 - logprior: -7.2392e-01
Epoch 5/10
28/28 - 8s - loss: 269.9441 - loglik: -2.6923e+02 - logprior: -7.1315e-01
Epoch 6/10
28/28 - 8s - loss: 269.9691 - loglik: -2.6927e+02 - logprior: -7.0126e-01
Fitted a model with MAP estimate = -269.1201
Time for alignment: 159.2322
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 8s - loss: 372.7501 - loglik: -3.6994e+02 - logprior: -2.8093e+00
Epoch 2/10
20/20 - 4s - loss: 328.3224 - loglik: -3.2707e+02 - logprior: -1.2539e+00
Epoch 3/10
20/20 - 5s - loss: 309.7065 - loglik: -3.0825e+02 - logprior: -1.4609e+00
Epoch 4/10
20/20 - 4s - loss: 304.9589 - loglik: -3.0355e+02 - logprior: -1.4067e+00
Epoch 5/10
20/20 - 5s - loss: 303.3212 - loglik: -3.0188e+02 - logprior: -1.4394e+00
Epoch 6/10
20/20 - 5s - loss: 302.7479 - loglik: -3.0134e+02 - logprior: -1.4081e+00
Epoch 7/10
20/20 - 4s - loss: 302.5016 - loglik: -3.0109e+02 - logprior: -1.4083e+00
Epoch 8/10
20/20 - 5s - loss: 302.0180 - loglik: -3.0062e+02 - logprior: -1.3953e+00
Epoch 9/10
20/20 - 5s - loss: 302.2884 - loglik: -3.0088e+02 - logprior: -1.4069e+00
Fitted a model with MAP estimate = -290.6876
expansions: [(5, 1), (8, 2), (10, 2), (13, 2), (19, 1), (20, 1), (31, 1), (35, 1), (36, 2), (38, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 2), (62, 1), (76, 1), (78, 2), (79, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 10s - loss: 302.9756 - loglik: -2.9929e+02 - logprior: -3.6831e+00
Epoch 2/2
20/20 - 7s - loss: 291.2581 - loglik: -2.8949e+02 - logprior: -1.7674e+00
Fitted a model with MAP estimate = -283.9580
expansions: [(0, 1)]
discards: [  0   8  13  46  74  82 105 109]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 10s - loss: 291.0187 - loglik: -2.8823e+02 - logprior: -2.7926e+00
Epoch 2/2
20/20 - 8s - loss: 287.7701 - loglik: -2.8673e+02 - logprior: -1.0409e+00
Fitted a model with MAP estimate = -283.3029
expansions: []
discards: [97]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 21331 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
28/28 - 11s - loss: 278.4774 - loglik: -2.7707e+02 - logprior: -1.4093e+00
Epoch 2/10
28/28 - 8s - loss: 273.0320 - loglik: -2.7226e+02 - logprior: -7.6715e-01
Epoch 3/10
28/28 - 8s - loss: 271.6302 - loglik: -2.7091e+02 - logprior: -7.2175e-01
Epoch 4/10
28/28 - 7s - loss: 270.6215 - loglik: -2.6991e+02 - logprior: -7.1355e-01
Epoch 5/10
28/28 - 8s - loss: 270.0991 - loglik: -2.6939e+02 - logprior: -7.0532e-01
Epoch 6/10
28/28 - 8s - loss: 269.8603 - loglik: -2.6918e+02 - logprior: -6.8461e-01
Epoch 7/10
28/28 - 9s - loss: 269.2480 - loglik: -2.6856e+02 - logprior: -6.8426e-01
Epoch 8/10
28/28 - 8s - loss: 268.6053 - loglik: -2.6793e+02 - logprior: -6.7286e-01
Epoch 9/10
28/28 - 8s - loss: 269.0682 - loglik: -2.6840e+02 - logprior: -6.6580e-01
Fitted a model with MAP estimate = -268.4418
Time for alignment: 198.6914
Computed alignments with likelihoods: ['-268.6156', '-269.1201', '-268.4418']
Best model has likelihood: -268.4418
SP score = 0.6328
Training of 3 independent models on file Sulfotransfer.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c019a670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fba11484f70>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 198 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 12s - loss: 703.9020 - loglik: -6.8827e+02 - logprior: -1.5636e+01
Epoch 2/10
10/10 - 9s - loss: 667.5627 - loglik: -6.6494e+02 - logprior: -2.6260e+00
Epoch 3/10
10/10 - 10s - loss: 629.2850 - loglik: -6.2876e+02 - logprior: -5.2476e-01
Epoch 4/10
10/10 - 9s - loss: 600.0931 - loglik: -5.9955e+02 - logprior: -5.3955e-01
Epoch 5/10
10/10 - 9s - loss: 589.5850 - loglik: -5.8891e+02 - logprior: -6.7577e-01
Epoch 6/10
10/10 - 10s - loss: 582.9677 - loglik: -5.8241e+02 - logprior: -5.5321e-01
Epoch 7/10
10/10 - 9s - loss: 582.0715 - loglik: -5.8159e+02 - logprior: -4.8244e-01
Epoch 8/10
10/10 - 10s - loss: 581.5179 - loglik: -5.8115e+02 - logprior: -3.6606e-01
Epoch 9/10
10/10 - 10s - loss: 579.9149 - loglik: -5.7963e+02 - logprior: -2.8576e-01
Epoch 10/10
10/10 - 10s - loss: 580.0465 - loglik: -5.7979e+02 - logprior: -2.5401e-01
Fitted a model with MAP estimate = -579.7186
expansions: [(25, 1), (27, 1), (28, 1), (33, 2), (43, 4), (44, 1), (64, 2), (65, 2), (90, 2), (101, 1), (102, 2), (103, 1), (104, 1), (120, 1), (135, 1), (136, 1), (148, 1), (150, 3), (152, 1), (162, 2), (165, 2), (183, 1), (184, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 234 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 16s - loss: 590.8103 - loglik: -5.7665e+02 - logprior: -1.4159e+01
Epoch 2/2
10/10 - 13s - loss: 572.1681 - loglik: -5.6940e+02 - logprior: -2.7671e+00
Fitted a model with MAP estimate = -570.4783
expansions: [(175, 1), (221, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 15s - loss: 581.9520 - loglik: -5.6794e+02 - logprior: -1.4016e+01
Epoch 2/2
10/10 - 13s - loss: 569.2333 - loglik: -5.6676e+02 - logprior: -2.4685e+00
Fitted a model with MAP estimate = -567.3490
expansions: [(106, 2), (218, 1)]
discards: [203]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 238 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 16s - loss: 581.0889 - loglik: -5.6733e+02 - logprior: -1.3759e+01
Epoch 2/10
10/10 - 14s - loss: 568.0188 - loglik: -5.6586e+02 - logprior: -2.1542e+00
Epoch 3/10
10/10 - 14s - loss: 566.6490 - loglik: -5.6673e+02 - logprior: 0.0815
Epoch 4/10
10/10 - 14s - loss: 564.8719 - loglik: -5.6576e+02 - logprior: 0.8846
Epoch 5/10
10/10 - 15s - loss: 563.8156 - loglik: -5.6503e+02 - logprior: 1.2179
Epoch 6/10
10/10 - 14s - loss: 565.3453 - loglik: -5.6678e+02 - logprior: 1.4310
Fitted a model with MAP estimate = -564.2091
Time for alignment: 273.2104
Fitting a model of length 198 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 13s - loss: 704.7723 - loglik: -6.8914e+02 - logprior: -1.5636e+01
Epoch 2/10
10/10 - 10s - loss: 664.6614 - loglik: -6.6204e+02 - logprior: -2.6199e+00
Epoch 3/10
10/10 - 10s - loss: 629.9259 - loglik: -6.2929e+02 - logprior: -6.3394e-01
Epoch 4/10
10/10 - 11s - loss: 600.5348 - loglik: -5.9989e+02 - logprior: -6.4869e-01
Epoch 5/10
10/10 - 11s - loss: 591.5753 - loglik: -5.9092e+02 - logprior: -6.5600e-01
Epoch 6/10
10/10 - 11s - loss: 585.3265 - loglik: -5.8470e+02 - logprior: -6.3110e-01
Epoch 7/10
10/10 - 11s - loss: 582.0721 - loglik: -5.8153e+02 - logprior: -5.3845e-01
Epoch 8/10
10/10 - 11s - loss: 583.3293 - loglik: -5.8293e+02 - logprior: -4.0079e-01
Fitted a model with MAP estimate = -581.3711
expansions: [(18, 1), (25, 1), (27, 1), (28, 1), (40, 4), (65, 3), (90, 2), (91, 2), (93, 1), (101, 1), (102, 2), (103, 1), (104, 1), (117, 1), (119, 1), (135, 1), (147, 2), (150, 3), (152, 1), (166, 5), (182, 1), (183, 1), (184, 2), (185, 1)]
discards: [2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 237 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 18s - loss: 592.0941 - loglik: -5.7795e+02 - logprior: -1.4144e+01
Epoch 2/2
10/10 - 15s - loss: 573.5779 - loglik: -5.7088e+02 - logprior: -2.7026e+00
Fitted a model with MAP estimate = -570.6049
expansions: [(220, 1)]
discards: [204 205]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 18s - loss: 582.9930 - loglik: -5.6903e+02 - logprior: -1.3965e+01
Epoch 2/2
10/10 - 16s - loss: 570.8887 - loglik: -5.6855e+02 - logprior: -2.3353e+00
Fitted a model with MAP estimate = -568.5185
expansions: [(44, 2)]
discards: [103 197 204 205 206 207 208]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 231 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 17s - loss: 584.4316 - loglik: -5.7064e+02 - logprior: -1.3788e+01
Epoch 2/10
10/10 - 15s - loss: 572.0797 - loglik: -5.6986e+02 - logprior: -2.2237e+00
Epoch 3/10
10/10 - 15s - loss: 568.3429 - loglik: -5.6837e+02 - logprior: 0.0321
Epoch 4/10
10/10 - 15s - loss: 567.7299 - loglik: -5.6855e+02 - logprior: 0.8199
Epoch 5/10
10/10 - 15s - loss: 566.4867 - loglik: -5.6765e+02 - logprior: 1.1629
Epoch 6/10
10/10 - 15s - loss: 565.2931 - loglik: -5.6666e+02 - logprior: 1.3695
Epoch 7/10
10/10 - 14s - loss: 566.9471 - loglik: -5.6864e+02 - logprior: 1.6886
Fitted a model with MAP estimate = -565.9028
Time for alignment: 298.8952
Fitting a model of length 198 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 13s - loss: 704.6243 - loglik: -6.8899e+02 - logprior: -1.5631e+01
Epoch 2/10
10/10 - 12s - loss: 664.6478 - loglik: -6.6203e+02 - logprior: -2.6177e+00
Epoch 3/10
10/10 - 10s - loss: 631.7355 - loglik: -6.3111e+02 - logprior: -6.2695e-01
Epoch 4/10
10/10 - 11s - loss: 600.0009 - loglik: -5.9934e+02 - logprior: -6.5744e-01
Epoch 5/10
10/10 - 10s - loss: 588.5979 - loglik: -5.8790e+02 - logprior: -6.9475e-01
Epoch 6/10
10/10 - 10s - loss: 585.7058 - loglik: -5.8502e+02 - logprior: -6.8168e-01
Epoch 7/10
10/10 - 10s - loss: 583.7465 - loglik: -5.8314e+02 - logprior: -6.0384e-01
Epoch 8/10
10/10 - 9s - loss: 579.0766 - loglik: -5.7859e+02 - logprior: -4.8354e-01
Epoch 9/10
10/10 - 9s - loss: 580.4949 - loglik: -5.8007e+02 - logprior: -4.2338e-01
Fitted a model with MAP estimate = -579.7467
expansions: [(18, 1), (25, 1), (26, 1), (28, 2), (64, 2), (65, 2), (90, 2), (91, 5), (95, 1), (103, 1), (104, 1), (117, 1), (119, 1), (135, 1), (147, 2), (150, 3), (162, 1), (163, 1), (182, 1), (183, 1), (184, 2), (185, 1)]
discards: [  2 170]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 230 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 15s - loss: 592.3946 - loglik: -5.7814e+02 - logprior: -1.4255e+01
Epoch 2/2
10/10 - 12s - loss: 574.2186 - loglik: -5.7136e+02 - logprior: -2.8567e+00
Fitted a model with MAP estimate = -571.6217
expansions: [(195, 4), (213, 1)]
discards: [99]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 234 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 16s - loss: 583.2056 - loglik: -5.6909e+02 - logprior: -1.4117e+01
Epoch 2/2
10/10 - 14s - loss: 569.9127 - loglik: -5.6736e+02 - logprior: -2.5495e+00
Fitted a model with MAP estimate = -568.1910
expansions: []
discards: [ 31 195]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 232 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 16s - loss: 580.8140 - loglik: -5.6679e+02 - logprior: -1.4024e+01
Epoch 2/10
10/10 - 13s - loss: 568.7645 - loglik: -5.6642e+02 - logprior: -2.3492e+00
Epoch 3/10
10/10 - 12s - loss: 567.1353 - loglik: -5.6706e+02 - logprior: -7.9315e-02
Epoch 4/10
10/10 - 12s - loss: 566.1896 - loglik: -5.6693e+02 - logprior: 0.7443
Epoch 5/10
10/10 - 11s - loss: 567.0751 - loglik: -5.6817e+02 - logprior: 1.0982
Fitted a model with MAP estimate = -565.5478
Time for alignment: 246.1359
Computed alignments with likelihoods: ['-564.2091', '-565.9028', '-565.5478']
Best model has likelihood: -564.2091
SP score = 0.7608
Training of 3 independent models on file hom.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbb82b6ed00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9f77701c0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.2759 - loglik: -1.5105e+02 - logprior: -3.2253e+00
Epoch 2/10
19/19 - 1s - loss: 120.6702 - loglik: -1.1917e+02 - logprior: -1.4964e+00
Epoch 3/10
19/19 - 1s - loss: 106.9740 - loglik: -1.0541e+02 - logprior: -1.5661e+00
Epoch 4/10
19/19 - 1s - loss: 103.8458 - loglik: -1.0221e+02 - logprior: -1.6380e+00
Epoch 5/10
19/19 - 1s - loss: 102.8527 - loglik: -1.0133e+02 - logprior: -1.5197e+00
Epoch 6/10
19/19 - 1s - loss: 102.2048 - loglik: -1.0067e+02 - logprior: -1.5360e+00
Epoch 7/10
19/19 - 1s - loss: 102.4071 - loglik: -1.0089e+02 - logprior: -1.5141e+00
Fitted a model with MAP estimate = -97.9892
expansions: [(3, 1), (5, 1), (6, 1), (7, 1), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 104.8622 - loglik: -1.0070e+02 - logprior: -4.1660e+00
Epoch 2/2
19/19 - 1s - loss: 95.5297 - loglik: -9.3191e+01 - logprior: -2.3382e+00
Fitted a model with MAP estimate = -90.1516
expansions: [(3, 1)]
discards: [ 0 20 36 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 97.9234 - loglik: -9.4019e+01 - logprior: -3.9040e+00
Epoch 2/2
19/19 - 1s - loss: 94.0010 - loglik: -9.2570e+01 - logprior: -1.4315e+00
Fitted a model with MAP estimate = -89.8391
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 92.9986 - loglik: -8.9319e+01 - logprior: -3.6796e+00
Epoch 2/10
21/21 - 1s - loss: 89.5399 - loglik: -8.7689e+01 - logprior: -1.8511e+00
Epoch 3/10
21/21 - 1s - loss: 88.5602 - loglik: -8.7158e+01 - logprior: -1.4018e+00
Epoch 4/10
21/21 - 1s - loss: 88.3571 - loglik: -8.7058e+01 - logprior: -1.2994e+00
Epoch 5/10
21/21 - 1s - loss: 88.1477 - loglik: -8.6881e+01 - logprior: -1.2670e+00
Epoch 6/10
21/21 - 1s - loss: 88.2923 - loglik: -8.7046e+01 - logprior: -1.2465e+00
Fitted a model with MAP estimate = -88.1832
Time for alignment: 39.9511
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.1788 - loglik: -1.5095e+02 - logprior: -3.2295e+00
Epoch 2/10
19/19 - 1s - loss: 119.6821 - loglik: -1.1818e+02 - logprior: -1.5023e+00
Epoch 3/10
19/19 - 1s - loss: 106.1729 - loglik: -1.0460e+02 - logprior: -1.5697e+00
Epoch 4/10
19/19 - 1s - loss: 103.5113 - loglik: -1.0187e+02 - logprior: -1.6435e+00
Epoch 5/10
19/19 - 1s - loss: 102.7785 - loglik: -1.0126e+02 - logprior: -1.5190e+00
Epoch 6/10
19/19 - 1s - loss: 102.5705 - loglik: -1.0104e+02 - logprior: -1.5314e+00
Epoch 7/10
19/19 - 1s - loss: 101.8849 - loglik: -1.0037e+02 - logprior: -1.5189e+00
Epoch 8/10
19/19 - 1s - loss: 101.8875 - loglik: -1.0038e+02 - logprior: -1.5025e+00
Fitted a model with MAP estimate = -97.8695
expansions: [(3, 1), (5, 1), (6, 1), (7, 1), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 104.7589 - loglik: -1.0059e+02 - logprior: -4.1712e+00
Epoch 2/2
19/19 - 1s - loss: 95.5182 - loglik: -9.3174e+01 - logprior: -2.3439e+00
Fitted a model with MAP estimate = -90.1681
expansions: [(3, 1)]
discards: [ 0 20 36 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 98.0232 - loglik: -9.4098e+01 - logprior: -3.9252e+00
Epoch 2/2
19/19 - 1s - loss: 94.0414 - loglik: -9.2586e+01 - logprior: -1.4552e+00
Fitted a model with MAP estimate = -89.8592
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 92.8985 - loglik: -8.9216e+01 - logprior: -3.6822e+00
Epoch 2/10
21/21 - 1s - loss: 89.5199 - loglik: -8.7684e+01 - logprior: -1.8358e+00
Epoch 3/10
21/21 - 1s - loss: 88.6279 - loglik: -8.7226e+01 - logprior: -1.4021e+00
Epoch 4/10
21/21 - 1s - loss: 88.0858 - loglik: -8.6788e+01 - logprior: -1.2973e+00
Epoch 5/10
21/21 - 1s - loss: 88.3068 - loglik: -8.7032e+01 - logprior: -1.2745e+00
Fitted a model with MAP estimate = -88.2033
Time for alignment: 39.7860
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 154.3732 - loglik: -1.5115e+02 - logprior: -3.2256e+00
Epoch 2/10
19/19 - 1s - loss: 121.6455 - loglik: -1.2017e+02 - logprior: -1.4750e+00
Epoch 3/10
19/19 - 1s - loss: 108.4860 - loglik: -1.0695e+02 - logprior: -1.5371e+00
Epoch 4/10
19/19 - 1s - loss: 104.9281 - loglik: -1.0332e+02 - logprior: -1.6125e+00
Epoch 5/10
19/19 - 1s - loss: 103.9587 - loglik: -1.0245e+02 - logprior: -1.5070e+00
Epoch 6/10
19/19 - 1s - loss: 103.2364 - loglik: -1.0171e+02 - logprior: -1.5228e+00
Epoch 7/10
19/19 - 1s - loss: 103.4022 - loglik: -1.0190e+02 - logprior: -1.5071e+00
Fitted a model with MAP estimate = -98.8433
expansions: [(3, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 105.1952 - loglik: -1.0102e+02 - logprior: -4.1715e+00
Epoch 2/2
19/19 - 1s - loss: 95.5092 - loglik: -9.3183e+01 - logprior: -2.3265e+00
Fitted a model with MAP estimate = -90.2291
expansions: [(3, 1)]
discards: [ 0 12 36 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 98.1889 - loglik: -9.4283e+01 - logprior: -3.9061e+00
Epoch 2/2
19/19 - 1s - loss: 94.0430 - loglik: -9.2635e+01 - logprior: -1.4078e+00
Fitted a model with MAP estimate = -89.8887
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 93.1814 - loglik: -8.9517e+01 - logprior: -3.6647e+00
Epoch 2/10
21/21 - 1s - loss: 89.1732 - loglik: -8.7353e+01 - logprior: -1.8202e+00
Epoch 3/10
21/21 - 1s - loss: 88.5717 - loglik: -8.7170e+01 - logprior: -1.4015e+00
Epoch 4/10
21/21 - 1s - loss: 88.4643 - loglik: -8.7171e+01 - logprior: -1.2931e+00
Epoch 5/10
21/21 - 1s - loss: 88.4997 - loglik: -8.7225e+01 - logprior: -1.2746e+00
Fitted a model with MAP estimate = -88.1467
Time for alignment: 39.6599
Computed alignments with likelihoods: ['-88.1832', '-88.2033', '-88.1467']
Best model has likelihood: -88.1467
SP score = 0.9478
Training of 3 independent models on file OTCace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb7ec1f24c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbaf0a88fa0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 122 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 448.6202 - loglik: -4.4084e+02 - logprior: -7.7810e+00
Epoch 2/10
13/13 - 3s - loss: 389.4292 - loglik: -3.8758e+02 - logprior: -1.8489e+00
Epoch 3/10
13/13 - 4s - loss: 342.1180 - loglik: -3.4034e+02 - logprior: -1.7783e+00
Epoch 4/10
13/13 - 4s - loss: 323.7972 - loglik: -3.2167e+02 - logprior: -2.1282e+00
Epoch 5/10
13/13 - 4s - loss: 317.7873 - loglik: -3.1576e+02 - logprior: -2.0318e+00
Epoch 6/10
13/13 - 4s - loss: 315.3018 - loglik: -3.1328e+02 - logprior: -2.0231e+00
Epoch 7/10
13/13 - 4s - loss: 314.7192 - loglik: -3.1273e+02 - logprior: -1.9902e+00
Epoch 8/10
13/13 - 4s - loss: 313.4526 - loglik: -3.1153e+02 - logprior: -1.9247e+00
Epoch 9/10
13/13 - 4s - loss: 314.0985 - loglik: -3.1222e+02 - logprior: -1.8775e+00
Fitted a model with MAP estimate = -313.4960
expansions: [(0, 2), (9, 2), (18, 1), (24, 2), (26, 1), (36, 2), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 1), (68, 1), (69, 1), (71, 1), (72, 1), (73, 1), (78, 1), (82, 2), (94, 2), (103, 1), (104, 2), (105, 2), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 161 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 10s - loss: 317.1367 - loglik: -3.0693e+02 - logprior: -1.0205e+01
Epoch 2/2
13/13 - 5s - loss: 294.0658 - loglik: -2.9148e+02 - logprior: -2.5866e+00
Fitted a model with MAP estimate = -290.2966
expansions: [(87, 1)]
discards: [  0  12  29  47  48 109 136 138]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 154 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 301.7894 - loglik: -2.9266e+02 - logprior: -9.1275e+00
Epoch 2/2
13/13 - 5s - loss: 294.4786 - loglik: -2.9066e+02 - logprior: -3.8139e+00
Fitted a model with MAP estimate = -291.9965
expansions: [(0, 1)]
discards: [ 0 85]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 297.4615 - loglik: -2.9042e+02 - logprior: -7.0426e+00
Epoch 2/10
13/13 - 5s - loss: 290.5292 - loglik: -2.8908e+02 - logprior: -1.4455e+00
Epoch 3/10
13/13 - 5s - loss: 289.3478 - loglik: -2.8845e+02 - logprior: -9.0086e-01
Epoch 4/10
13/13 - 5s - loss: 289.7223 - loglik: -2.8889e+02 - logprior: -8.3442e-01
Fitted a model with MAP estimate = -289.0335
Time for alignment: 107.6201
Fitting a model of length 122 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 448.6935 - loglik: -4.4092e+02 - logprior: -7.7737e+00
Epoch 2/10
13/13 - 4s - loss: 389.2017 - loglik: -3.8734e+02 - logprior: -1.8626e+00
Epoch 3/10
13/13 - 4s - loss: 342.9221 - loglik: -3.4112e+02 - logprior: -1.8039e+00
Epoch 4/10
13/13 - 4s - loss: 324.6888 - loglik: -3.2253e+02 - logprior: -2.1558e+00
Epoch 5/10
13/13 - 4s - loss: 320.0857 - loglik: -3.1805e+02 - logprior: -2.0356e+00
Epoch 6/10
13/13 - 4s - loss: 317.4118 - loglik: -3.1539e+02 - logprior: -2.0258e+00
Epoch 7/10
13/13 - 4s - loss: 315.5622 - loglik: -3.1350e+02 - logprior: -2.0593e+00
Epoch 8/10
13/13 - 4s - loss: 314.6056 - loglik: -3.1256e+02 - logprior: -2.0461e+00
Epoch 9/10
13/13 - 4s - loss: 312.6864 - loglik: -3.1063e+02 - logprior: -2.0587e+00
Epoch 10/10
13/13 - 4s - loss: 312.8130 - loglik: -3.1077e+02 - logprior: -2.0477e+00
Fitted a model with MAP estimate = -311.8900
expansions: [(0, 2), (9, 2), (18, 1), (25, 2), (26, 1), (36, 2), (37, 3), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (47, 1), (68, 1), (69, 2), (71, 1), (72, 1), (73, 1), (78, 1), (81, 1), (82, 2), (99, 1), (100, 1), (102, 1), (103, 2), (104, 2), (105, 1), (115, 1), (116, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 162 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 9s - loss: 315.7721 - loglik: -3.0547e+02 - logprior: -1.0304e+01
Epoch 2/2
13/13 - 6s - loss: 291.4791 - loglik: -2.8882e+02 - logprior: -2.6549e+00
Fitted a model with MAP estimate = -288.2783
expansions: [(43, 1)]
discards: [  0  12  30  47  48  49 110 137 139]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 154 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 10s - loss: 300.7010 - loglik: -2.9152e+02 - logprior: -9.1832e+00
Epoch 2/2
13/13 - 6s - loss: 291.9414 - loglik: -2.8808e+02 - logprior: -3.8650e+00
Fitted a model with MAP estimate = -290.3794
expansions: [(0, 1)]
discards: [ 0 44]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 9s - loss: 296.3050 - loglik: -2.8921e+02 - logprior: -7.0926e+00
Epoch 2/10
13/13 - 6s - loss: 289.3636 - loglik: -2.8791e+02 - logprior: -1.4508e+00
Epoch 3/10
13/13 - 6s - loss: 288.1494 - loglik: -2.8723e+02 - logprior: -9.1663e-01
Epoch 4/10
13/13 - 7s - loss: 287.1074 - loglik: -2.8625e+02 - logprior: -8.5325e-01
Epoch 5/10
13/13 - 7s - loss: 287.5809 - loglik: -2.8691e+02 - logprior: -6.6729e-01
Fitted a model with MAP estimate = -287.2209
Time for alignment: 131.4378
Fitting a model of length 122 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 449.0051 - loglik: -4.4123e+02 - logprior: -7.7735e+00
Epoch 2/10
13/13 - 4s - loss: 389.4419 - loglik: -3.8758e+02 - logprior: -1.8618e+00
Epoch 3/10
13/13 - 4s - loss: 341.0110 - loglik: -3.3921e+02 - logprior: -1.8046e+00
Epoch 4/10
13/13 - 5s - loss: 322.8691 - loglik: -3.2070e+02 - logprior: -2.1643e+00
Epoch 5/10
13/13 - 5s - loss: 316.1714 - loglik: -3.1409e+02 - logprior: -2.0781e+00
Epoch 6/10
13/13 - 5s - loss: 314.1587 - loglik: -3.1211e+02 - logprior: -2.0473e+00
Epoch 7/10
13/13 - 5s - loss: 312.5766 - loglik: -3.1055e+02 - logprior: -2.0312e+00
Epoch 8/10
13/13 - 5s - loss: 312.0335 - loglik: -3.1005e+02 - logprior: -1.9806e+00
Epoch 9/10
13/13 - 4s - loss: 311.1353 - loglik: -3.0915e+02 - logprior: -1.9874e+00
Epoch 10/10
13/13 - 5s - loss: 311.8296 - loglik: -3.0986e+02 - logprior: -1.9690e+00
Fitted a model with MAP estimate = -311.2077
expansions: [(0, 2), (9, 2), (18, 1), (25, 2), (26, 1), (35, 1), (36, 1), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 1), (67, 2), (68, 1), (69, 1), (71, 1), (72, 1), (73, 1), (78, 1), (81, 1), (94, 5), (100, 1), (104, 2), (105, 2), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 165 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 10s - loss: 315.3297 - loglik: -3.0511e+02 - logprior: -1.0224e+01
Epoch 2/2
13/13 - 7s - loss: 292.0535 - loglik: -2.8939e+02 - logprior: -2.6637e+00
Fitted a model with MAP estimate = -287.8685
expansions: []
discards: [  0  12  30  48  49  88 125 126 127 140 142]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 154 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 9s - loss: 299.7497 - loglik: -2.9061e+02 - logprior: -9.1389e+00
Epoch 2/2
13/13 - 6s - loss: 291.7834 - loglik: -2.8793e+02 - logprior: -3.8559e+00
Fitted a model with MAP estimate = -289.6167
expansions: [(0, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 154 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 9s - loss: 294.1000 - loglik: -2.8704e+02 - logprior: -7.0630e+00
Epoch 2/10
13/13 - 6s - loss: 287.8271 - loglik: -2.8642e+02 - logprior: -1.4058e+00
Epoch 3/10
13/13 - 7s - loss: 287.3648 - loglik: -2.8651e+02 - logprior: -8.5611e-01
Epoch 4/10
13/13 - 6s - loss: 286.6688 - loglik: -2.8587e+02 - logprior: -7.9444e-01
Epoch 5/10
13/13 - 6s - loss: 286.8046 - loglik: -2.8621e+02 - logprior: -5.9814e-01
Fitted a model with MAP estimate = -286.3632
Time for alignment: 139.9467
Computed alignments with likelihoods: ['-289.0335', '-287.2209', '-286.3632']
Best model has likelihood: -286.3632
SP score = 0.5502
Training of 3 independent models on file lyase_1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cc410190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbba4d582e0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 236 on 7632 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 27s - loss: 818.0048 - loglik: -8.1458e+02 - logprior: -3.4282e+00
Epoch 2/10
17/17 - 27s - loss: 708.4270 - loglik: -7.0761e+02 - logprior: -8.2214e-01
Epoch 3/10
17/17 - 28s - loss: 646.6167 - loglik: -6.4507e+02 - logprior: -1.5508e+00
Epoch 4/10
17/17 - 27s - loss: 628.5435 - loglik: -6.2683e+02 - logprior: -1.7113e+00
Epoch 5/10
17/17 - 29s - loss: 623.5667 - loglik: -6.2182e+02 - logprior: -1.7475e+00
Epoch 6/10
17/17 - 28s - loss: 620.7892 - loglik: -6.1908e+02 - logprior: -1.7072e+00
Epoch 7/10
17/17 - 26s - loss: 620.5946 - loglik: -6.1892e+02 - logprior: -1.6705e+00
Epoch 8/10
17/17 - 24s - loss: 620.1012 - loglik: -6.1844e+02 - logprior: -1.6600e+00
Epoch 9/10
17/17 - 22s - loss: 619.0137 - loglik: -6.1736e+02 - logprior: -1.6573e+00
Epoch 10/10
17/17 - 23s - loss: 619.0155 - loglik: -6.1735e+02 - logprior: -1.6653e+00
Fitted a model with MAP estimate = -618.7973
expansions: [(13, 2), (14, 1), (15, 3), (16, 1), (17, 5), (29, 2), (41, 1), (50, 2), (54, 4), (60, 1), (64, 1), (67, 1), (91, 1), (93, 1), (94, 1), (99, 1), (100, 2), (101, 1), (102, 1), (104, 1), (109, 1), (129, 1), (134, 1), (139, 1), (142, 1), (143, 3), (144, 1), (161, 1), (164, 2), (165, 2), (167, 1), (171, 1), (172, 1), (175, 1), (178, 1), (188, 1), (193, 1), (202, 2), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 296 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 40s - loss: 598.0530 - loglik: -5.9461e+02 - logprior: -3.4392e+00
Epoch 2/2
34/34 - 33s - loss: 579.4105 - loglik: -5.7797e+02 - logprior: -1.4363e+00
Fitted a model with MAP estimate = -576.0057
expansions: [(77, 1), (208, 1), (256, 1)]
discards: [18 65 71 72]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 295 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 33s - loss: 581.1261 - loglik: -5.7900e+02 - logprior: -2.1224e+00
Epoch 2/2
34/34 - 32s - loss: 576.5596 - loglik: -5.7615e+02 - logprior: -4.0992e-01
Fitted a model with MAP estimate = -575.9426
expansions: [(180, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 296 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 35s - loss: 577.9445 - loglik: -5.7604e+02 - logprior: -1.9013e+00
Epoch 2/10
34/34 - 29s - loss: 575.7603 - loglik: -5.7568e+02 - logprior: -7.7290e-02
Epoch 3/10
34/34 - 31s - loss: 575.8883 - loglik: -5.7603e+02 - logprior: 0.1413
Fitted a model with MAP estimate = -574.9494
Time for alignment: 635.1896
Fitting a model of length 236 on 7632 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 25s - loss: 819.3894 - loglik: -8.1596e+02 - logprior: -3.4311e+00
Epoch 2/10
17/17 - 26s - loss: 713.6745 - loglik: -7.1285e+02 - logprior: -8.2640e-01
Epoch 3/10
17/17 - 26s - loss: 650.6344 - loglik: -6.4907e+02 - logprior: -1.5612e+00
Epoch 4/10
17/17 - 27s - loss: 630.6617 - loglik: -6.2892e+02 - logprior: -1.7447e+00
Epoch 5/10
17/17 - 28s - loss: 623.4586 - loglik: -6.2163e+02 - logprior: -1.8331e+00
Epoch 6/10
17/17 - 28s - loss: 620.4899 - loglik: -6.1870e+02 - logprior: -1.7849e+00
Epoch 7/10
17/17 - 28s - loss: 619.5435 - loglik: -6.1780e+02 - logprior: -1.7404e+00
Epoch 8/10
17/17 - 27s - loss: 618.8759 - loglik: -6.1715e+02 - logprior: -1.7251e+00
Epoch 9/10
17/17 - 26s - loss: 619.7486 - loglik: -6.1804e+02 - logprior: -1.7123e+00
Fitted a model with MAP estimate = -618.3707
expansions: [(13, 1), (14, 1), (16, 3), (17, 6), (22, 1), (29, 2), (41, 1), (48, 1), (51, 2), (55, 3), (57, 1), (61, 1), (65, 1), (66, 2), (88, 2), (93, 1), (94, 1), (96, 2), (99, 1), (100, 1), (102, 1), (103, 1), (104, 1), (110, 1), (111, 1), (130, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (147, 1), (165, 1), (167, 1), (172, 2), (173, 1), (175, 1), (178, 1), (184, 1), (185, 3), (186, 2), (188, 1), (190, 1), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1), (230, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 304 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 44s - loss: 593.8288 - loglik: -5.9045e+02 - logprior: -3.3828e+00
Epoch 2/2
34/34 - 40s - loss: 573.5861 - loglik: -5.7213e+02 - logprior: -1.4543e+00
Fitted a model with MAP estimate = -567.1872
expansions: [(73, 1), (74, 1)]
discards: [ 17  66  77  89 113 114]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 300 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 43s - loss: 571.6940 - loglik: -5.6946e+02 - logprior: -2.2383e+00
Epoch 2/2
34/34 - 39s - loss: 566.8590 - loglik: -5.6636e+02 - logprior: -5.0112e-01
Fitted a model with MAP estimate = -565.9652
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 300 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 37s - loss: 568.3987 - loglik: -5.6649e+02 - logprior: -1.9130e+00
Epoch 2/10
34/34 - 34s - loss: 564.7150 - loglik: -5.6456e+02 - logprior: -1.5865e-01
Epoch 3/10
34/34 - 37s - loss: 565.9001 - loglik: -5.6596e+02 - logprior: 0.0645
Fitted a model with MAP estimate = -564.6923
Time for alignment: 680.6935
Fitting a model of length 236 on 7632 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 25s - loss: 818.4975 - loglik: -8.1507e+02 - logprior: -3.4296e+00
Epoch 2/10
17/17 - 21s - loss: 711.0573 - loglik: -7.1026e+02 - logprior: -7.9766e-01
Epoch 3/10
17/17 - 20s - loss: 650.1281 - loglik: -6.4863e+02 - logprior: -1.4992e+00
Epoch 4/10
17/17 - 22s - loss: 629.6096 - loglik: -6.2791e+02 - logprior: -1.6952e+00
Epoch 5/10
17/17 - 22s - loss: 624.7932 - loglik: -6.2303e+02 - logprior: -1.7606e+00
Epoch 6/10
17/17 - 23s - loss: 621.1810 - loglik: -6.1946e+02 - logprior: -1.7182e+00
Epoch 7/10
17/17 - 25s - loss: 619.7311 - loglik: -6.1807e+02 - logprior: -1.6611e+00
Epoch 8/10
17/17 - 25s - loss: 619.1558 - loglik: -6.1752e+02 - logprior: -1.6350e+00
Epoch 9/10
17/17 - 23s - loss: 619.3850 - loglik: -6.1775e+02 - logprior: -1.6356e+00
Fitted a model with MAP estimate = -618.4081
expansions: [(13, 2), (14, 1), (16, 2), (17, 5), (18, 2), (22, 1), (29, 3), (51, 1), (52, 1), (55, 1), (56, 5), (65, 1), (68, 1), (73, 2), (93, 1), (94, 1), (96, 2), (99, 1), (100, 2), (101, 1), (102, 1), (104, 1), (109, 1), (130, 1), (137, 1), (140, 1), (142, 1), (143, 3), (144, 1), (165, 1), (168, 1), (172, 1), (173, 2), (175, 1), (178, 2), (188, 1), (190, 1), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1), (230, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 298 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 32s - loss: 599.7165 - loglik: -5.9630e+02 - logprior: -3.4205e+00
Epoch 2/2
34/34 - 32s - loss: 583.1838 - loglik: -5.8172e+02 - logprior: -1.4612e+00
Fitted a model with MAP estimate = -579.3213
expansions: [(188, 1)]
discards: [ 43  75  76  77 230]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 294 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 41s - loss: 585.6157 - loglik: -5.8345e+02 - logprior: -2.1704e+00
Epoch 2/2
34/34 - 40s - loss: 582.8766 - loglik: -5.8237e+02 - logprior: -5.0977e-01
Fitted a model with MAP estimate = -581.2386
expansions: [(74, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 298 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 30s - loss: 581.8380 - loglik: -5.7991e+02 - logprior: -1.9299e+00
Epoch 2/10
34/34 - 26s - loss: 577.4161 - loglik: -5.7713e+02 - logprior: -2.8637e-01
Epoch 3/10
34/34 - 25s - loss: 575.6189 - loglik: -5.7552e+02 - logprior: -9.7568e-02
Epoch 4/10
34/34 - 25s - loss: 575.8416 - loglik: -5.7589e+02 - logprior: 0.0519
Fitted a model with MAP estimate = -575.4818
Time for alignment: 596.9490
Computed alignments with likelihoods: ['-574.9494', '-564.6923', '-575.4818']
Best model has likelihood: -564.6923
SP score = 0.7284
Training of 3 independent models on file toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbb94135f40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c283e040>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 271.2259 - loglik: -1.8299e+02 - logprior: -8.8233e+01
Epoch 2/10
10/10 - 1s - loss: 183.5547 - loglik: -1.5984e+02 - logprior: -2.3719e+01
Epoch 3/10
10/10 - 1s - loss: 152.8556 - loglik: -1.4167e+02 - logprior: -1.1190e+01
Epoch 4/10
10/10 - 1s - loss: 138.6717 - loglik: -1.3209e+02 - logprior: -6.5788e+00
Epoch 5/10
10/10 - 1s - loss: 132.2140 - loglik: -1.2806e+02 - logprior: -4.1563e+00
Epoch 6/10
10/10 - 1s - loss: 128.9168 - loglik: -1.2610e+02 - logprior: -2.8120e+00
Epoch 7/10
10/10 - 1s - loss: 127.2693 - loglik: -1.2536e+02 - logprior: -1.9134e+00
Epoch 8/10
10/10 - 1s - loss: 126.2892 - loglik: -1.2495e+02 - logprior: -1.3416e+00
Epoch 9/10
10/10 - 1s - loss: 125.6185 - loglik: -1.2463e+02 - logprior: -9.8880e-01
Epoch 10/10
10/10 - 1s - loss: 125.0766 - loglik: -1.2433e+02 - logprior: -7.4551e-01
Fitted a model with MAP estimate = -124.8122
expansions: [(7, 2), (8, 4), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 220.0727 - loglik: -1.2141e+02 - logprior: -9.8661e+01
Epoch 2/2
10/10 - 1s - loss: 156.2678 - loglik: -1.1597e+02 - logprior: -4.0294e+01
Fitted a model with MAP estimate = -145.2824
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 190.0992 - loglik: -1.1128e+02 - logprior: -7.8816e+01
Epoch 2/2
10/10 - 1s - loss: 130.3891 - loglik: -1.0963e+02 - logprior: -2.0761e+01
Fitted a model with MAP estimate = -121.7089
expansions: [(11, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 65 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 185.4240 - loglik: -1.0812e+02 - logprior: -7.7303e+01
Epoch 2/10
10/10 - 1s - loss: 128.5368 - loglik: -1.0856e+02 - logprior: -1.9976e+01
Epoch 3/10
10/10 - 1s - loss: 117.1517 - loglik: -1.0918e+02 - logprior: -7.9747e+00
Epoch 4/10
10/10 - 1s - loss: 112.6621 - loglik: -1.0979e+02 - logprior: -2.8756e+00
Epoch 5/10
10/10 - 1s - loss: 110.3562 - loglik: -1.1032e+02 - logprior: -3.5802e-02
Epoch 6/10
10/10 - 1s - loss: 109.0267 - loglik: -1.1063e+02 - logprior: 1.6076
Epoch 7/10
10/10 - 1s - loss: 108.1467 - loglik: -1.1074e+02 - logprior: 2.5952
Epoch 8/10
10/10 - 1s - loss: 107.4325 - loglik: -1.1065e+02 - logprior: 3.2151
Epoch 9/10
10/10 - 1s - loss: 106.7817 - loglik: -1.1050e+02 - logprior: 3.7150
Epoch 10/10
10/10 - 1s - loss: 106.4467 - loglik: -1.1055e+02 - logprior: 4.1037
Fitted a model with MAP estimate = -106.2675
Time for alignment: 29.7611
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 271.2259 - loglik: -1.8299e+02 - logprior: -8.8233e+01
Epoch 2/10
10/10 - 1s - loss: 183.5547 - loglik: -1.5984e+02 - logprior: -2.3719e+01
Epoch 3/10
10/10 - 1s - loss: 152.8556 - loglik: -1.4167e+02 - logprior: -1.1190e+01
Epoch 4/10
10/10 - 1s - loss: 138.6717 - loglik: -1.3209e+02 - logprior: -6.5788e+00
Epoch 5/10
10/10 - 1s - loss: 132.2140 - loglik: -1.2806e+02 - logprior: -4.1563e+00
Epoch 6/10
10/10 - 1s - loss: 128.9168 - loglik: -1.2610e+02 - logprior: -2.8120e+00
Epoch 7/10
10/10 - 1s - loss: 127.2693 - loglik: -1.2536e+02 - logprior: -1.9134e+00
Epoch 8/10
10/10 - 1s - loss: 126.2892 - loglik: -1.2495e+02 - logprior: -1.3416e+00
Epoch 9/10
10/10 - 1s - loss: 125.6185 - loglik: -1.2463e+02 - logprior: -9.8880e-01
Epoch 10/10
10/10 - 1s - loss: 125.0766 - loglik: -1.2433e+02 - logprior: -7.4551e-01
Fitted a model with MAP estimate = -124.8122
expansions: [(7, 2), (8, 4), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 220.0727 - loglik: -1.2141e+02 - logprior: -9.8661e+01
Epoch 2/2
10/10 - 1s - loss: 156.2678 - loglik: -1.1597e+02 - logprior: -4.0294e+01
Fitted a model with MAP estimate = -145.2824
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 190.0992 - loglik: -1.1128e+02 - logprior: -7.8816e+01
Epoch 2/2
10/10 - 1s - loss: 130.3891 - loglik: -1.0963e+02 - logprior: -2.0761e+01
Fitted a model with MAP estimate = -121.7089
expansions: [(11, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 65 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 185.4240 - loglik: -1.0812e+02 - logprior: -7.7303e+01
Epoch 2/10
10/10 - 1s - loss: 128.5368 - loglik: -1.0856e+02 - logprior: -1.9976e+01
Epoch 3/10
10/10 - 1s - loss: 117.1517 - loglik: -1.0918e+02 - logprior: -7.9747e+00
Epoch 4/10
10/10 - 1s - loss: 112.6621 - loglik: -1.0979e+02 - logprior: -2.8756e+00
Epoch 5/10
10/10 - 1s - loss: 110.3562 - loglik: -1.1032e+02 - logprior: -3.5802e-02
Epoch 6/10
10/10 - 1s - loss: 109.0267 - loglik: -1.1063e+02 - logprior: 1.6076
Epoch 7/10
10/10 - 1s - loss: 108.1467 - loglik: -1.1074e+02 - logprior: 2.5952
Epoch 8/10
10/10 - 1s - loss: 107.4325 - loglik: -1.1065e+02 - logprior: 3.2151
Epoch 9/10
10/10 - 1s - loss: 106.7817 - loglik: -1.1050e+02 - logprior: 3.7150
Epoch 10/10
10/10 - 1s - loss: 106.4467 - loglik: -1.1055e+02 - logprior: 4.1037
Fitted a model with MAP estimate = -106.2675
Time for alignment: 29.1516
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 271.2259 - loglik: -1.8299e+02 - logprior: -8.8233e+01
Epoch 2/10
10/10 - 1s - loss: 183.5547 - loglik: -1.5984e+02 - logprior: -2.3719e+01
Epoch 3/10
10/10 - 1s - loss: 152.8556 - loglik: -1.4167e+02 - logprior: -1.1190e+01
Epoch 4/10
10/10 - 1s - loss: 138.6717 - loglik: -1.3209e+02 - logprior: -6.5788e+00
Epoch 5/10
10/10 - 1s - loss: 132.2140 - loglik: -1.2806e+02 - logprior: -4.1563e+00
Epoch 6/10
10/10 - 1s - loss: 128.9168 - loglik: -1.2610e+02 - logprior: -2.8120e+00
Epoch 7/10
10/10 - 1s - loss: 127.2693 - loglik: -1.2536e+02 - logprior: -1.9134e+00
Epoch 8/10
10/10 - 1s - loss: 126.2892 - loglik: -1.2495e+02 - logprior: -1.3416e+00
Epoch 9/10
10/10 - 1s - loss: 125.6185 - loglik: -1.2463e+02 - logprior: -9.8880e-01
Epoch 10/10
10/10 - 1s - loss: 125.0766 - loglik: -1.2433e+02 - logprior: -7.4551e-01
Fitted a model with MAP estimate = -124.8122
expansions: [(7, 2), (8, 4), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 220.0727 - loglik: -1.2141e+02 - logprior: -9.8661e+01
Epoch 2/2
10/10 - 1s - loss: 156.2678 - loglik: -1.1597e+02 - logprior: -4.0294e+01
Fitted a model with MAP estimate = -145.2824
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 190.0992 - loglik: -1.1128e+02 - logprior: -7.8816e+01
Epoch 2/2
10/10 - 1s - loss: 130.3891 - loglik: -1.0963e+02 - logprior: -2.0761e+01
Fitted a model with MAP estimate = -121.7089
expansions: [(11, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 65 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 185.4240 - loglik: -1.0812e+02 - logprior: -7.7303e+01
Epoch 2/10
10/10 - 1s - loss: 128.5368 - loglik: -1.0856e+02 - logprior: -1.9976e+01
Epoch 3/10
10/10 - 1s - loss: 117.1517 - loglik: -1.0918e+02 - logprior: -7.9747e+00
Epoch 4/10
10/10 - 1s - loss: 112.6621 - loglik: -1.0979e+02 - logprior: -2.8756e+00
Epoch 5/10
10/10 - 1s - loss: 110.3562 - loglik: -1.1032e+02 - logprior: -3.5802e-02
Epoch 6/10
10/10 - 1s - loss: 109.0267 - loglik: -1.1063e+02 - logprior: 1.6076
Epoch 7/10
10/10 - 1s - loss: 108.1467 - loglik: -1.1074e+02 - logprior: 2.5952
Epoch 8/10
10/10 - 1s - loss: 107.4325 - loglik: -1.1065e+02 - logprior: 3.2151
Epoch 9/10
10/10 - 1s - loss: 106.7817 - loglik: -1.1050e+02 - logprior: 3.7150
Epoch 10/10
10/10 - 1s - loss: 106.4467 - loglik: -1.1055e+02 - logprior: 4.1037
Fitted a model with MAP estimate = -106.2675
Time for alignment: 27.1574
Computed alignments with likelihoods: ['-106.2675', '-106.2675', '-106.2675']
Best model has likelihood: -106.2675
SP score = 0.8590
Training of 3 independent models on file msb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cd344310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9eecb5880>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 316.5611 - loglik: -3.0887e+02 - logprior: -7.6937e+00
Epoch 2/10
13/13 - 1s - loss: 285.6192 - loglik: -2.8375e+02 - logprior: -1.8646e+00
Epoch 3/10
13/13 - 1s - loss: 263.2858 - loglik: -2.6166e+02 - logprior: -1.6263e+00
Epoch 4/10
13/13 - 2s - loss: 252.5312 - loglik: -2.5063e+02 - logprior: -1.8991e+00
Epoch 5/10
13/13 - 1s - loss: 249.6717 - loglik: -2.4785e+02 - logprior: -1.8245e+00
Epoch 6/10
13/13 - 1s - loss: 248.9255 - loglik: -2.4718e+02 - logprior: -1.7495e+00
Epoch 7/10
13/13 - 1s - loss: 248.9312 - loglik: -2.4718e+02 - logprior: -1.7538e+00
Fitted a model with MAP estimate = -248.5422
expansions: [(9, 1), (10, 1), (13, 1), (18, 1), (19, 1), (27, 1), (28, 1), (29, 1), (30, 3), (31, 1), (40, 1), (43, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 256.4958 - loglik: -2.4751e+02 - logprior: -8.9860e+00
Epoch 2/2
13/13 - 2s - loss: 246.1517 - loglik: -2.4212e+02 - logprior: -4.0306e+00
Fitted a model with MAP estimate = -245.1114
expansions: [(0, 2)]
discards: [ 0 65 94]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 248.2219 - loglik: -2.4142e+02 - logprior: -6.8048e+00
Epoch 2/2
13/13 - 2s - loss: 242.3585 - loglik: -2.4054e+02 - logprior: -1.8144e+00
Fitted a model with MAP estimate = -241.8511
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 250.6810 - loglik: -2.4217e+02 - logprior: -8.5105e+00
Epoch 2/10
13/13 - 2s - loss: 244.0379 - loglik: -2.4144e+02 - logprior: -2.6003e+00
Epoch 3/10
13/13 - 2s - loss: 242.6603 - loglik: -2.4142e+02 - logprior: -1.2390e+00
Epoch 4/10
13/13 - 2s - loss: 242.4948 - loglik: -2.4153e+02 - logprior: -9.6759e-01
Epoch 5/10
13/13 - 2s - loss: 241.8700 - loglik: -2.4105e+02 - logprior: -8.1976e-01
Epoch 6/10
13/13 - 2s - loss: 241.9550 - loglik: -2.4118e+02 - logprior: -7.7952e-01
Fitted a model with MAP estimate = -241.8146
Time for alignment: 51.6350
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 316.4166 - loglik: -3.0872e+02 - logprior: -7.6973e+00
Epoch 2/10
13/13 - 1s - loss: 285.3242 - loglik: -2.8345e+02 - logprior: -1.8714e+00
Epoch 3/10
13/13 - 1s - loss: 262.5883 - loglik: -2.6092e+02 - logprior: -1.6648e+00
Epoch 4/10
13/13 - 2s - loss: 251.8098 - loglik: -2.4988e+02 - logprior: -1.9286e+00
Epoch 5/10
13/13 - 1s - loss: 249.9794 - loglik: -2.4813e+02 - logprior: -1.8500e+00
Epoch 6/10
13/13 - 1s - loss: 248.9788 - loglik: -2.4721e+02 - logprior: -1.7700e+00
Epoch 7/10
13/13 - 1s - loss: 248.4849 - loglik: -2.4671e+02 - logprior: -1.7767e+00
Epoch 8/10
13/13 - 2s - loss: 248.6840 - loglik: -2.4690e+02 - logprior: -1.7825e+00
Fitted a model with MAP estimate = -248.3572
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (19, 1), (27, 1), (28, 1), (29, 1), (30, 3), (31, 1), (40, 1), (41, 1), (43, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 257.0948 - loglik: -2.4809e+02 - logprior: -9.0016e+00
Epoch 2/2
13/13 - 2s - loss: 246.4243 - loglik: -2.4235e+02 - logprior: -4.0766e+00
Fitted a model with MAP estimate = -245.2518
expansions: [(0, 2)]
discards: [ 0 14 67 96]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 248.1978 - loglik: -2.4140e+02 - logprior: -6.7966e+00
Epoch 2/2
13/13 - 2s - loss: 242.3769 - loglik: -2.4057e+02 - logprior: -1.8073e+00
Fitted a model with MAP estimate = -241.7757
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 250.6501 - loglik: -2.4215e+02 - logprior: -8.5008e+00
Epoch 2/10
13/13 - 2s - loss: 244.1105 - loglik: -2.4152e+02 - logprior: -2.5925e+00
Epoch 3/10
13/13 - 2s - loss: 242.4046 - loglik: -2.4118e+02 - logprior: -1.2246e+00
Epoch 4/10
13/13 - 2s - loss: 242.1040 - loglik: -2.4114e+02 - logprior: -9.6632e-01
Epoch 5/10
13/13 - 2s - loss: 242.0463 - loglik: -2.4122e+02 - logprior: -8.2424e-01
Epoch 6/10
13/13 - 2s - loss: 242.0386 - loglik: -2.4127e+02 - logprior: -7.6969e-01
Epoch 7/10
13/13 - 2s - loss: 241.7705 - loglik: -2.4103e+02 - logprior: -7.4466e-01
Epoch 8/10
13/13 - 2s - loss: 242.1354 - loglik: -2.4142e+02 - logprior: -7.1770e-01
Fitted a model with MAP estimate = -241.6255
Time for alignment: 58.5663
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 316.4288 - loglik: -3.0873e+02 - logprior: -7.6992e+00
Epoch 2/10
13/13 - 1s - loss: 285.1435 - loglik: -2.8327e+02 - logprior: -1.8754e+00
Epoch 3/10
13/13 - 1s - loss: 262.5998 - loglik: -2.6095e+02 - logprior: -1.6502e+00
Epoch 4/10
13/13 - 1s - loss: 252.9712 - loglik: -2.5108e+02 - logprior: -1.8914e+00
Epoch 5/10
13/13 - 2s - loss: 249.7471 - loglik: -2.4793e+02 - logprior: -1.8177e+00
Epoch 6/10
13/13 - 1s - loss: 249.2628 - loglik: -2.4754e+02 - logprior: -1.7242e+00
Epoch 7/10
13/13 - 2s - loss: 248.5893 - loglik: -2.4686e+02 - logprior: -1.7312e+00
Epoch 8/10
13/13 - 2s - loss: 248.7626 - loglik: -2.4704e+02 - logprior: -1.7183e+00
Fitted a model with MAP estimate = -248.4965
expansions: [(9, 1), (10, 1), (13, 2), (15, 1), (25, 1), (28, 2), (29, 1), (30, 3), (31, 2), (40, 1), (43, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 256.7948 - loglik: -2.4781e+02 - logprior: -8.9862e+00
Epoch 2/2
13/13 - 2s - loss: 246.6610 - loglik: -2.4261e+02 - logprior: -4.0547e+00
Fitted a model with MAP estimate = -245.1997
expansions: [(0, 2)]
discards: [ 0 14 67 96]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 248.2861 - loglik: -2.4150e+02 - logprior: -6.7887e+00
Epoch 2/2
13/13 - 2s - loss: 242.4774 - loglik: -2.4069e+02 - logprior: -1.7825e+00
Fitted a model with MAP estimate = -241.8041
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 250.7865 - loglik: -2.4231e+02 - logprior: -8.4731e+00
Epoch 2/10
13/13 - 2s - loss: 243.6472 - loglik: -2.4109e+02 - logprior: -2.5599e+00
Epoch 3/10
13/13 - 2s - loss: 243.0074 - loglik: -2.4182e+02 - logprior: -1.1921e+00
Epoch 4/10
13/13 - 2s - loss: 241.9674 - loglik: -2.4103e+02 - logprior: -9.3329e-01
Epoch 5/10
13/13 - 2s - loss: 241.4966 - loglik: -2.4071e+02 - logprior: -7.8423e-01
Epoch 6/10
13/13 - 2s - loss: 242.0913 - loglik: -2.4134e+02 - logprior: -7.5079e-01
Fitted a model with MAP estimate = -241.7040
Time for alignment: 53.3562
Computed alignments with likelihoods: ['-241.8146', '-241.6255', '-241.7040']
Best model has likelihood: -241.6255
SP score = 0.9313
Training of 3 independent models on file rnasemam.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9eeb4d040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c0f47fd0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 432.4084 - loglik: -3.4684e+02 - logprior: -8.5566e+01
Epoch 2/10
10/10 - 1s - loss: 327.9050 - loglik: -3.0808e+02 - logprior: -1.9823e+01
Epoch 3/10
10/10 - 1s - loss: 276.7765 - loglik: -2.6879e+02 - logprior: -7.9904e+00
Epoch 4/10
10/10 - 1s - loss: 245.4774 - loglik: -2.4096e+02 - logprior: -4.5149e+00
Epoch 5/10
10/10 - 1s - loss: 231.7047 - loglik: -2.2928e+02 - logprior: -2.4224e+00
Epoch 6/10
10/10 - 1s - loss: 226.1646 - loglik: -2.2502e+02 - logprior: -1.1488e+00
Epoch 7/10
10/10 - 1s - loss: 223.2769 - loglik: -2.2305e+02 - logprior: -2.2660e-01
Epoch 8/10
10/10 - 1s - loss: 221.5715 - loglik: -2.2196e+02 - logprior: 0.3885
Epoch 9/10
10/10 - 1s - loss: 220.6325 - loglik: -2.2147e+02 - logprior: 0.8343
Epoch 10/10
10/10 - 1s - loss: 220.0365 - loglik: -2.2128e+02 - logprior: 1.2480
Fitted a model with MAP estimate = -219.7990
expansions: [(13, 3), (14, 2), (24, 1), (29, 2), (30, 1), (40, 1), (41, 1), (49, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 307.5757 - loglik: -2.1195e+02 - logprior: -9.5628e+01
Epoch 2/2
10/10 - 2s - loss: 234.5168 - loglik: -1.9829e+02 - logprior: -3.6223e+01
Fitted a model with MAP estimate = -222.2180
expansions: [(0, 2)]
discards: [  0  35 112 113]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 267.3150 - loglik: -1.9234e+02 - logprior: -7.4980e+01
Epoch 2/2
10/10 - 2s - loss: 206.7612 - loglik: -1.9091e+02 - logprior: -1.5849e+01
Fitted a model with MAP estimate = -197.9909
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 281.6087 - loglik: -1.9218e+02 - logprior: -8.9431e+01
Epoch 2/10
10/10 - 2s - loss: 214.6264 - loglik: -1.9260e+02 - logprior: -2.2028e+01
Epoch 3/10
10/10 - 2s - loss: 197.1045 - loglik: -1.9304e+02 - logprior: -4.0658e+00
Epoch 4/10
10/10 - 2s - loss: 190.9898 - loglik: -1.9337e+02 - logprior: 2.3778
Epoch 5/10
10/10 - 2s - loss: 187.9028 - loglik: -1.9349e+02 - logprior: 5.5864
Epoch 6/10
10/10 - 2s - loss: 186.1406 - loglik: -1.9358e+02 - logprior: 7.4397
Epoch 7/10
10/10 - 2s - loss: 185.0171 - loglik: -1.9365e+02 - logprior: 8.6305
Epoch 8/10
10/10 - 2s - loss: 184.2117 - loglik: -1.9371e+02 - logprior: 9.5023
Epoch 9/10
10/10 - 2s - loss: 183.5655 - loglik: -1.9380e+02 - logprior: 10.2391
Epoch 10/10
10/10 - 2s - loss: 182.9995 - loglik: -1.9391e+02 - logprior: 10.9113
Fitted a model with MAP estimate = -182.7072
Time for alignment: 51.2565
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 432.4084 - loglik: -3.4684e+02 - logprior: -8.5566e+01
Epoch 2/10
10/10 - 1s - loss: 327.9050 - loglik: -3.0808e+02 - logprior: -1.9823e+01
Epoch 3/10
10/10 - 1s - loss: 276.7765 - loglik: -2.6879e+02 - logprior: -7.9904e+00
Epoch 4/10
10/10 - 1s - loss: 245.4774 - loglik: -2.4096e+02 - logprior: -4.5149e+00
Epoch 5/10
10/10 - 1s - loss: 231.7047 - loglik: -2.2928e+02 - logprior: -2.4224e+00
Epoch 6/10
10/10 - 1s - loss: 226.1646 - loglik: -2.2502e+02 - logprior: -1.1488e+00
Epoch 7/10
10/10 - 1s - loss: 223.2769 - loglik: -2.2305e+02 - logprior: -2.2660e-01
Epoch 8/10
10/10 - 1s - loss: 221.5715 - loglik: -2.2196e+02 - logprior: 0.3885
Epoch 9/10
10/10 - 1s - loss: 220.6325 - loglik: -2.2147e+02 - logprior: 0.8343
Epoch 10/10
10/10 - 2s - loss: 220.0365 - loglik: -2.2128e+02 - logprior: 1.2480
Fitted a model with MAP estimate = -219.7990
expansions: [(13, 3), (14, 2), (24, 1), (29, 2), (30, 1), (40, 1), (41, 1), (49, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 307.5757 - loglik: -2.1195e+02 - logprior: -9.5628e+01
Epoch 2/2
10/10 - 2s - loss: 234.5168 - loglik: -1.9829e+02 - logprior: -3.6223e+01
Fitted a model with MAP estimate = -222.2180
expansions: [(0, 2)]
discards: [  0  35 112 113]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 267.3150 - loglik: -1.9234e+02 - logprior: -7.4980e+01
Epoch 2/2
10/10 - 2s - loss: 206.7612 - loglik: -1.9091e+02 - logprior: -1.5849e+01
Fitted a model with MAP estimate = -197.9909
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 281.6087 - loglik: -1.9218e+02 - logprior: -8.9431e+01
Epoch 2/10
10/10 - 2s - loss: 214.6264 - loglik: -1.9260e+02 - logprior: -2.2028e+01
Epoch 3/10
10/10 - 2s - loss: 197.1045 - loglik: -1.9304e+02 - logprior: -4.0658e+00
Epoch 4/10
10/10 - 2s - loss: 190.9898 - loglik: -1.9337e+02 - logprior: 2.3778
Epoch 5/10
10/10 - 2s - loss: 187.9028 - loglik: -1.9349e+02 - logprior: 5.5864
Epoch 6/10
10/10 - 2s - loss: 186.1406 - loglik: -1.9358e+02 - logprior: 7.4397
Epoch 7/10
10/10 - 2s - loss: 185.0171 - loglik: -1.9365e+02 - logprior: 8.6305
Epoch 8/10
10/10 - 2s - loss: 184.2117 - loglik: -1.9371e+02 - logprior: 9.5023
Epoch 9/10
10/10 - 2s - loss: 183.5655 - loglik: -1.9380e+02 - logprior: 10.2391
Epoch 10/10
10/10 - 2s - loss: 182.9995 - loglik: -1.9391e+02 - logprior: 10.9113
Fitted a model with MAP estimate = -182.7072
Time for alignment: 52.2703
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 432.4084 - loglik: -3.4684e+02 - logprior: -8.5566e+01
Epoch 2/10
10/10 - 1s - loss: 327.9050 - loglik: -3.0808e+02 - logprior: -1.9823e+01
Epoch 3/10
10/10 - 1s - loss: 276.7765 - loglik: -2.6879e+02 - logprior: -7.9904e+00
Epoch 4/10
10/10 - 1s - loss: 245.4774 - loglik: -2.4096e+02 - logprior: -4.5149e+00
Epoch 5/10
10/10 - 1s - loss: 231.7047 - loglik: -2.2928e+02 - logprior: -2.4224e+00
Epoch 6/10
10/10 - 1s - loss: 226.1646 - loglik: -2.2502e+02 - logprior: -1.1488e+00
Epoch 7/10
10/10 - 1s - loss: 223.2769 - loglik: -2.2305e+02 - logprior: -2.2660e-01
Epoch 8/10
10/10 - 1s - loss: 221.5715 - loglik: -2.2196e+02 - logprior: 0.3885
Epoch 9/10
10/10 - 1s - loss: 220.6325 - loglik: -2.2147e+02 - logprior: 0.8343
Epoch 10/10
10/10 - 1s - loss: 220.0365 - loglik: -2.2128e+02 - logprior: 1.2480
Fitted a model with MAP estimate = -219.7990
expansions: [(13, 3), (14, 2), (24, 1), (29, 2), (30, 1), (40, 1), (41, 1), (49, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 307.5757 - loglik: -2.1195e+02 - logprior: -9.5628e+01
Epoch 2/2
10/10 - 2s - loss: 234.5168 - loglik: -1.9829e+02 - logprior: -3.6223e+01
Fitted a model with MAP estimate = -222.2180
expansions: [(0, 2)]
discards: [  0  35 112 113]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 267.3150 - loglik: -1.9234e+02 - logprior: -7.4980e+01
Epoch 2/2
10/10 - 2s - loss: 206.7612 - loglik: -1.9091e+02 - logprior: -1.5849e+01
Fitted a model with MAP estimate = -197.9909
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 281.6087 - loglik: -1.9218e+02 - logprior: -8.9431e+01
Epoch 2/10
10/10 - 2s - loss: 214.6264 - loglik: -1.9260e+02 - logprior: -2.2028e+01
Epoch 3/10
10/10 - 2s - loss: 197.1045 - loglik: -1.9304e+02 - logprior: -4.0658e+00
Epoch 4/10
10/10 - 2s - loss: 190.9898 - loglik: -1.9337e+02 - logprior: 2.3778
Epoch 5/10
10/10 - 2s - loss: 187.9028 - loglik: -1.9349e+02 - logprior: 5.5864
Epoch 6/10
10/10 - 2s - loss: 186.1406 - loglik: -1.9358e+02 - logprior: 7.4397
Epoch 7/10
10/10 - 2s - loss: 185.0171 - loglik: -1.9365e+02 - logprior: 8.6305
Epoch 8/10
10/10 - 2s - loss: 184.2117 - loglik: -1.9371e+02 - logprior: 9.5023
Epoch 9/10
10/10 - 2s - loss: 183.5655 - loglik: -1.9380e+02 - logprior: 10.2391
Epoch 10/10
10/10 - 2s - loss: 182.9995 - loglik: -1.9391e+02 - logprior: 10.9113
Fitted a model with MAP estimate = -182.7072
Time for alignment: 51.3795
Computed alignments with likelihoods: ['-182.7072', '-182.7072', '-182.7072']
Best model has likelihood: -182.7072
SP score = 0.9152
Training of 3 independent models on file rub.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9f7d35730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9f7a88af0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 167.2384 - loglik: -1.3557e+02 - logprior: -3.1673e+01
Epoch 2/10
10/10 - 0s - loss: 124.1110 - loglik: -1.1516e+02 - logprior: -8.9498e+00
Epoch 3/10
10/10 - 0s - loss: 98.9787 - loglik: -9.4118e+01 - logprior: -4.8608e+00
Epoch 4/10
10/10 - 0s - loss: 83.9860 - loglik: -8.0227e+01 - logprior: -3.7590e+00
Epoch 5/10
10/10 - 0s - loss: 78.6886 - loglik: -7.5252e+01 - logprior: -3.4364e+00
Epoch 6/10
10/10 - 0s - loss: 77.0142 - loglik: -7.3851e+01 - logprior: -3.1630e+00
Epoch 7/10
10/10 - 0s - loss: 76.2593 - loglik: -7.3507e+01 - logprior: -2.7525e+00
Epoch 8/10
10/10 - 0s - loss: 75.8456 - loglik: -7.3388e+01 - logprior: -2.4577e+00
Epoch 9/10
10/10 - 0s - loss: 75.5117 - loglik: -7.3164e+01 - logprior: -2.3481e+00
Epoch 10/10
10/10 - 0s - loss: 75.3720 - loglik: -7.3078e+01 - logprior: -2.2941e+00
Fitted a model with MAP estimate = -75.3000
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 1), (24, 2), (26, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 112.7813 - loglik: -7.0469e+01 - logprior: -4.2313e+01
Epoch 2/2
10/10 - 0s - loss: 77.3774 - loglik: -6.3717e+01 - logprior: -1.3660e+01
Fitted a model with MAP estimate = -70.4471
expansions: []
discards: [32]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 90.1720 - loglik: -6.0331e+01 - logprior: -2.9841e+01
Epoch 2/2
10/10 - 0s - loss: 68.7528 - loglik: -6.0299e+01 - logprior: -8.4537e+00
Fitted a model with MAP estimate = -65.9312
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 87.7424 - loglik: -5.9507e+01 - logprior: -2.8235e+01
Epoch 2/10
10/10 - 0s - loss: 68.2747 - loglik: -6.0270e+01 - logprior: -8.0046e+00
Epoch 3/10
10/10 - 0s - loss: 64.7864 - loglik: -6.0805e+01 - logprior: -3.9814e+00
Epoch 4/10
10/10 - 0s - loss: 63.4886 - loglik: -6.1036e+01 - logprior: -2.4529e+00
Epoch 5/10
10/10 - 0s - loss: 62.9474 - loglik: -6.1226e+01 - logprior: -1.7212e+00
Epoch 6/10
10/10 - 0s - loss: 62.6213 - loglik: -6.1267e+01 - logprior: -1.3547e+00
Epoch 7/10
10/10 - 0s - loss: 62.3863 - loglik: -6.1273e+01 - logprior: -1.1137e+00
Epoch 8/10
10/10 - 0s - loss: 62.4066 - loglik: -6.1498e+01 - logprior: -9.0819e-01
Fitted a model with MAP estimate = -62.2750
Time for alignment: 25.6997
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 167.2522 - loglik: -1.3558e+02 - logprior: -3.1673e+01
Epoch 2/10
10/10 - 0s - loss: 124.0177 - loglik: -1.1506e+02 - logprior: -8.9546e+00
Epoch 3/10
10/10 - 0s - loss: 98.9274 - loglik: -9.4061e+01 - logprior: -4.8664e+00
Epoch 4/10
10/10 - 0s - loss: 83.9873 - loglik: -8.0227e+01 - logprior: -3.7603e+00
Epoch 5/10
10/10 - 0s - loss: 79.0138 - loglik: -7.5572e+01 - logprior: -3.4423e+00
Epoch 6/10
10/10 - 0s - loss: 77.1512 - loglik: -7.3987e+01 - logprior: -3.1640e+00
Epoch 7/10
10/10 - 0s - loss: 76.2308 - loglik: -7.3482e+01 - logprior: -2.7484e+00
Epoch 8/10
10/10 - 0s - loss: 75.8587 - loglik: -7.3402e+01 - logprior: -2.4570e+00
Epoch 9/10
10/10 - 0s - loss: 75.5706 - loglik: -7.3226e+01 - logprior: -2.3450e+00
Epoch 10/10
10/10 - 0s - loss: 75.3048 - loglik: -7.3009e+01 - logprior: -2.2959e+00
Fitted a model with MAP estimate = -75.3059
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 113.6551 - loglik: -7.1365e+01 - logprior: -4.2290e+01
Epoch 2/2
10/10 - 0s - loss: 77.7528 - loglik: -6.3953e+01 - logprior: -1.3800e+01
Fitted a model with MAP estimate = -70.7196
expansions: []
discards: [30 33]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 90.2902 - loglik: -6.0434e+01 - logprior: -2.9856e+01
Epoch 2/2
10/10 - 0s - loss: 68.7315 - loglik: -6.0275e+01 - logprior: -8.4561e+00
Fitted a model with MAP estimate = -65.9421
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 87.7045 - loglik: -5.9464e+01 - logprior: -2.8241e+01
Epoch 2/10
10/10 - 0s - loss: 68.3029 - loglik: -6.0307e+01 - logprior: -7.9957e+00
Epoch 3/10
10/10 - 0s - loss: 64.8134 - loglik: -6.0829e+01 - logprior: -3.9842e+00
Epoch 4/10
10/10 - 0s - loss: 63.5323 - loglik: -6.1078e+01 - logprior: -2.4545e+00
Epoch 5/10
10/10 - 0s - loss: 62.8805 - loglik: -6.1159e+01 - logprior: -1.7216e+00
Epoch 6/10
10/10 - 0s - loss: 62.5584 - loglik: -6.1204e+01 - logprior: -1.3546e+00
Epoch 7/10
10/10 - 0s - loss: 62.4533 - loglik: -6.1334e+01 - logprior: -1.1198e+00
Epoch 8/10
10/10 - 0s - loss: 62.2991 - loglik: -6.1388e+01 - logprior: -9.1104e-01
Epoch 9/10
10/10 - 0s - loss: 62.3071 - loglik: -6.1556e+01 - logprior: -7.5083e-01
Fitted a model with MAP estimate = -62.1964
Time for alignment: 25.9478
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 167.2324 - loglik: -1.3556e+02 - logprior: -3.1673e+01
Epoch 2/10
10/10 - 0s - loss: 123.8263 - loglik: -1.1487e+02 - logprior: -8.9516e+00
Epoch 3/10
10/10 - 0s - loss: 98.5669 - loglik: -9.3702e+01 - logprior: -4.8649e+00
Epoch 4/10
10/10 - 0s - loss: 83.7095 - loglik: -7.9941e+01 - logprior: -3.7682e+00
Epoch 5/10
10/10 - 0s - loss: 78.5359 - loglik: -7.5101e+01 - logprior: -3.4352e+00
Epoch 6/10
10/10 - 0s - loss: 76.9447 - loglik: -7.3790e+01 - logprior: -3.1546e+00
Epoch 7/10
10/10 - 0s - loss: 76.1455 - loglik: -7.3406e+01 - logprior: -2.7392e+00
Epoch 8/10
10/10 - 0s - loss: 75.7470 - loglik: -7.3297e+01 - logprior: -2.4502e+00
Epoch 9/10
10/10 - 0s - loss: 75.5678 - loglik: -7.3226e+01 - logprior: -2.3421e+00
Epoch 10/10
10/10 - 0s - loss: 75.3846 - loglik: -7.3090e+01 - logprior: -2.2943e+00
Fitted a model with MAP estimate = -75.2925
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 1), (24, 2), (26, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 112.8639 - loglik: -7.0556e+01 - logprior: -4.2308e+01
Epoch 2/2
10/10 - 0s - loss: 77.2918 - loglik: -6.3632e+01 - logprior: -1.3659e+01
Fitted a model with MAP estimate = -70.4425
expansions: []
discards: [32]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 90.0790 - loglik: -6.0239e+01 - logprior: -2.9840e+01
Epoch 2/2
10/10 - 0s - loss: 68.8263 - loglik: -6.0370e+01 - logprior: -8.4563e+00
Fitted a model with MAP estimate = -65.9245
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 87.7639 - loglik: -5.9524e+01 - logprior: -2.8240e+01
Epoch 2/10
10/10 - 0s - loss: 68.2997 - loglik: -6.0304e+01 - logprior: -7.9956e+00
Epoch 3/10
10/10 - 0s - loss: 64.7220 - loglik: -6.0738e+01 - logprior: -3.9840e+00
Epoch 4/10
10/10 - 0s - loss: 63.5912 - loglik: -6.1138e+01 - logprior: -2.4527e+00
Epoch 5/10
10/10 - 0s - loss: 62.7301 - loglik: -6.1008e+01 - logprior: -1.7224e+00
Epoch 6/10
10/10 - 0s - loss: 62.6629 - loglik: -6.1313e+01 - logprior: -1.3499e+00
Epoch 7/10
10/10 - 0s - loss: 62.4651 - loglik: -6.1347e+01 - logprior: -1.1182e+00
Epoch 8/10
10/10 - 0s - loss: 62.4267 - loglik: -6.1523e+01 - logprior: -9.0416e-01
Epoch 9/10
10/10 - 0s - loss: 62.2194 - loglik: -6.1467e+01 - logprior: -7.5281e-01
Epoch 10/10
10/10 - 0s - loss: 62.1346 - loglik: -6.1484e+01 - logprior: -6.5071e-01
Fitted a model with MAP estimate = -62.1222
Time for alignment: 25.4091
Computed alignments with likelihoods: ['-62.2750', '-62.1964', '-62.1222']
Best model has likelihood: -62.1222
SP score = 0.9918
Training of 3 independent models on file cyclo.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c27bc760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c029f190>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 445.2379 - loglik: -4.3991e+02 - logprior: -5.3260e+00
Epoch 2/10
15/15 - 4s - loss: 366.3235 - loglik: -3.6504e+02 - logprior: -1.2867e+00
Epoch 3/10
15/15 - 4s - loss: 319.9434 - loglik: -3.1840e+02 - logprior: -1.5453e+00
Epoch 4/10
15/15 - 4s - loss: 304.8101 - loglik: -3.0314e+02 - logprior: -1.6691e+00
Epoch 5/10
15/15 - 4s - loss: 302.9749 - loglik: -3.0144e+02 - logprior: -1.5344e+00
Epoch 6/10
15/15 - 5s - loss: 299.9658 - loglik: -2.9843e+02 - logprior: -1.5358e+00
Epoch 7/10
15/15 - 4s - loss: 299.5035 - loglik: -2.9800e+02 - logprior: -1.5060e+00
Epoch 8/10
15/15 - 4s - loss: 299.6420 - loglik: -2.9817e+02 - logprior: -1.4713e+00
Fitted a model with MAP estimate = -299.2917
expansions: [(8, 2), (11, 1), (12, 1), (16, 1), (24, 2), (25, 1), (49, 2), (55, 1), (59, 1), (66, 2), (69, 1), (91, 1), (92, 2), (106, 2), (112, 1), (114, 2), (115, 1), (116, 4), (119, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 157 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 300.6315 - loglik: -2.9413e+02 - logprior: -6.5008e+00
Epoch 2/2
15/15 - 6s - loss: 283.8889 - loglik: -2.8074e+02 - logprior: -3.1446e+00
Fitted a model with MAP estimate = -283.2053
expansions: [(0, 2)]
discards: [  0   7  57 107 123 135]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 284.7209 - loglik: -2.7983e+02 - logprior: -4.8906e+00
Epoch 2/2
15/15 - 5s - loss: 279.3983 - loglik: -2.7802e+02 - logprior: -1.3742e+00
Fitted a model with MAP estimate = -278.7291
expansions: [(77, 1)]
discards: [  0 136]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 286.0043 - loglik: -2.7971e+02 - logprior: -6.2991e+00
Epoch 2/10
15/15 - 5s - loss: 280.5233 - loglik: -2.7849e+02 - logprior: -2.0291e+00
Epoch 3/10
15/15 - 5s - loss: 278.6011 - loglik: -2.7762e+02 - logprior: -9.7894e-01
Epoch 4/10
15/15 - 5s - loss: 278.6256 - loglik: -2.7784e+02 - logprior: -7.8669e-01
Fitted a model with MAP estimate = -278.1844
Time for alignment: 115.3666
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 445.0709 - loglik: -4.3974e+02 - logprior: -5.3278e+00
Epoch 2/10
15/15 - 4s - loss: 365.5038 - loglik: -3.6422e+02 - logprior: -1.2839e+00
Epoch 3/10
15/15 - 4s - loss: 319.8569 - loglik: -3.1834e+02 - logprior: -1.5134e+00
Epoch 4/10
15/15 - 4s - loss: 306.3112 - loglik: -3.0464e+02 - logprior: -1.6666e+00
Epoch 5/10
15/15 - 4s - loss: 301.9559 - loglik: -3.0040e+02 - logprior: -1.5600e+00
Epoch 6/10
15/15 - 4s - loss: 300.5582 - loglik: -2.9900e+02 - logprior: -1.5612e+00
Epoch 7/10
15/15 - 4s - loss: 299.1355 - loglik: -2.9760e+02 - logprior: -1.5333e+00
Epoch 8/10
15/15 - 4s - loss: 299.3192 - loglik: -2.9782e+02 - logprior: -1.4982e+00
Fitted a model with MAP estimate = -298.7956
expansions: [(7, 3), (12, 1), (16, 1), (25, 1), (26, 2), (27, 1), (49, 2), (60, 1), (62, 1), (65, 2), (69, 1), (91, 1), (92, 2), (105, 2), (111, 1), (114, 3), (116, 3), (119, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 157 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 298.5998 - loglik: -2.9205e+02 - logprior: -6.5472e+00
Epoch 2/2
15/15 - 5s - loss: 284.1552 - loglik: -2.8089e+02 - logprior: -3.2619e+00
Fitted a model with MAP estimate = -281.5496
expansions: [(0, 2)]
discards: [  0   7  32  58 108 124]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 283.7049 - loglik: -2.7877e+02 - logprior: -4.9365e+00
Epoch 2/2
15/15 - 5s - loss: 278.3124 - loglik: -2.7691e+02 - logprior: -1.4013e+00
Fitted a model with MAP estimate = -278.2589
expansions: [(75, 1)]
discards: [ 0 81]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 285.3929 - loglik: -2.7905e+02 - logprior: -6.3388e+00
Epoch 2/10
15/15 - 5s - loss: 279.9540 - loglik: -2.7790e+02 - logprior: -2.0581e+00
Epoch 3/10
15/15 - 5s - loss: 280.3102 - loglik: -2.7930e+02 - logprior: -1.0059e+00
Fitted a model with MAP estimate = -278.5152
Time for alignment: 106.6065
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 444.5172 - loglik: -4.3919e+02 - logprior: -5.3282e+00
Epoch 2/10
15/15 - 4s - loss: 365.7870 - loglik: -3.6450e+02 - logprior: -1.2899e+00
Epoch 3/10
15/15 - 4s - loss: 322.8906 - loglik: -3.2137e+02 - logprior: -1.5160e+00
Epoch 4/10
15/15 - 4s - loss: 309.9723 - loglik: -3.0838e+02 - logprior: -1.5941e+00
Epoch 5/10
15/15 - 4s - loss: 306.2233 - loglik: -3.0475e+02 - logprior: -1.4702e+00
Epoch 6/10
15/15 - 4s - loss: 303.4904 - loglik: -3.0200e+02 - logprior: -1.4916e+00
Epoch 7/10
15/15 - 4s - loss: 303.1018 - loglik: -3.0161e+02 - logprior: -1.4881e+00
Epoch 8/10
15/15 - 4s - loss: 302.8355 - loglik: -3.0135e+02 - logprior: -1.4829e+00
Epoch 9/10
15/15 - 4s - loss: 301.5191 - loglik: -3.0004e+02 - logprior: -1.4800e+00
Epoch 10/10
15/15 - 5s - loss: 301.2049 - loglik: -2.9972e+02 - logprior: -1.4844e+00
Fitted a model with MAP estimate = -300.3258
expansions: [(7, 3), (10, 1), (16, 1), (24, 2), (25, 1), (49, 2), (60, 2), (66, 2), (67, 1), (91, 2), (92, 2), (108, 1), (112, 1), (114, 1), (115, 1), (117, 3), (118, 1), (119, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 298.8217 - loglik: -2.9235e+02 - logprior: -6.4692e+00
Epoch 2/2
15/15 - 6s - loss: 284.0170 - loglik: -2.8093e+02 - logprior: -3.0866e+00
Fitted a model with MAP estimate = -282.6976
expansions: [(0, 2)]
discards: [  0   7  57  78 108 140]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 284.5157 - loglik: -2.7965e+02 - logprior: -4.8658e+00
Epoch 2/2
15/15 - 6s - loss: 280.5397 - loglik: -2.7916e+02 - logprior: -1.3772e+00
Fitted a model with MAP estimate = -279.5473
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 286.4220 - loglik: -2.8014e+02 - logprior: -6.2776e+00
Epoch 2/10
15/15 - 6s - loss: 281.4195 - loglik: -2.7947e+02 - logprior: -1.9481e+00
Epoch 3/10
15/15 - 6s - loss: 280.1148 - loglik: -2.7917e+02 - logprior: -9.4951e-01
Epoch 4/10
15/15 - 6s - loss: 280.3171 - loglik: -2.7956e+02 - logprior: -7.5904e-01
Fitted a model with MAP estimate = -279.6395
Time for alignment: 126.6807
Computed alignments with likelihoods: ['-278.1844', '-278.2589', '-279.5473']
Best model has likelihood: -278.1844
SP score = 0.9235
Training of 3 independent models on file gpdh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9f7f90be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9d5d33400>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 119 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 344.5718 - loglik: -3.4044e+02 - logprior: -4.1284e+00
Epoch 2/10
17/17 - 5s - loss: 256.9912 - loglik: -2.5544e+02 - logprior: -1.5550e+00
Epoch 3/10
17/17 - 5s - loss: 215.8980 - loglik: -2.1410e+02 - logprior: -1.7997e+00
Epoch 4/10
17/17 - 5s - loss: 204.9084 - loglik: -2.0313e+02 - logprior: -1.7777e+00
Epoch 5/10
17/17 - 5s - loss: 201.9436 - loglik: -2.0017e+02 - logprior: -1.7728e+00
Epoch 6/10
17/17 - 5s - loss: 202.6228 - loglik: -2.0086e+02 - logprior: -1.7632e+00
Fitted a model with MAP estimate = -201.4859
expansions: [(0, 2), (15, 1), (16, 1), (17, 3), (27, 2), (28, 2), (29, 1), (42, 1), (43, 1), (44, 1), (48, 1), (49, 1), (55, 1), (56, 2), (57, 1), (64, 1), (65, 1), (77, 1), (78, 1), (85, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 10s - loss: 193.7804 - loglik: -1.8906e+02 - logprior: -4.7230e+00
Epoch 2/2
17/17 - 6s - loss: 178.4105 - loglik: -1.7701e+02 - logprior: -1.4051e+00
Fitted a model with MAP estimate = -176.9711
expansions: []
discards: [ 22  35 140]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 179.4331 - loglik: -1.7542e+02 - logprior: -4.0109e+00
Epoch 2/2
17/17 - 7s - loss: 176.8661 - loglik: -1.7546e+02 - logprior: -1.4087e+00
Fitted a model with MAP estimate = -176.0355
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 178.8784 - loglik: -1.7490e+02 - logprior: -3.9788e+00
Epoch 2/10
17/17 - 6s - loss: 175.5246 - loglik: -1.7416e+02 - logprior: -1.3632e+00
Epoch 3/10
17/17 - 6s - loss: 176.1187 - loglik: -1.7497e+02 - logprior: -1.1482e+00
Fitted a model with MAP estimate = -175.6596
Time for alignment: 112.1705
Fitting a model of length 119 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 344.4922 - loglik: -3.4036e+02 - logprior: -4.1300e+00
Epoch 2/10
17/17 - 5s - loss: 253.2464 - loglik: -2.5169e+02 - logprior: -1.5593e+00
Epoch 3/10
17/17 - 5s - loss: 211.2820 - loglik: -2.0946e+02 - logprior: -1.8182e+00
Epoch 4/10
17/17 - 5s - loss: 203.1266 - loglik: -2.0137e+02 - logprior: -1.7541e+00
Epoch 5/10
17/17 - 5s - loss: 202.1970 - loglik: -2.0047e+02 - logprior: -1.7319e+00
Epoch 6/10
17/17 - 5s - loss: 201.0461 - loglik: -1.9936e+02 - logprior: -1.6858e+00
Epoch 7/10
17/17 - 5s - loss: 200.9811 - loglik: -1.9930e+02 - logprior: -1.6840e+00
Epoch 8/10
17/17 - 5s - loss: 199.9692 - loglik: -1.9826e+02 - logprior: -1.7044e+00
Epoch 9/10
17/17 - 5s - loss: 200.1918 - loglik: -1.9842e+02 - logprior: -1.7693e+00
Fitted a model with MAP estimate = -199.8469
expansions: [(0, 2), (15, 1), (16, 4), (26, 2), (27, 2), (40, 1), (41, 1), (42, 1), (44, 1), (47, 1), (53, 1), (54, 1), (55, 1), (56, 1), (64, 1), (65, 1), (66, 1), (78, 1), (81, 1), (84, 1), (107, 1), (108, 1), (110, 1), (111, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 149 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 192.5825 - loglik: -1.8779e+02 - logprior: -4.7902e+00
Epoch 2/2
17/17 - 6s - loss: 177.4452 - loglik: -1.7590e+02 - logprior: -1.5433e+00
Fitted a model with MAP estimate = -176.7303
expansions: []
discards: [34]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 10s - loss: 179.4242 - loglik: -1.7558e+02 - logprior: -3.8460e+00
Epoch 2/2
17/17 - 6s - loss: 176.9475 - loglik: -1.7557e+02 - logprior: -1.3817e+00
Fitted a model with MAP estimate = -175.9310
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 178.9840 - loglik: -1.7502e+02 - logprior: -3.9615e+00
Epoch 2/10
17/17 - 7s - loss: 175.7939 - loglik: -1.7444e+02 - logprior: -1.3533e+00
Epoch 3/10
17/17 - 7s - loss: 175.8906 - loglik: -1.7475e+02 - logprior: -1.1432e+00
Fitted a model with MAP estimate = -175.6166
Time for alignment: 130.6568
Fitting a model of length 119 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 344.4137 - loglik: -3.4028e+02 - logprior: -4.1318e+00
Epoch 2/10
17/17 - 4s - loss: 254.2901 - loglik: -2.5274e+02 - logprior: -1.5513e+00
Epoch 3/10
17/17 - 4s - loss: 211.0588 - loglik: -2.0925e+02 - logprior: -1.8046e+00
Epoch 4/10
17/17 - 5s - loss: 202.8796 - loglik: -2.0108e+02 - logprior: -1.8008e+00
Epoch 5/10
17/17 - 5s - loss: 201.2628 - loglik: -1.9947e+02 - logprior: -1.7929e+00
Epoch 6/10
17/17 - 5s - loss: 200.7616 - loglik: -1.9900e+02 - logprior: -1.7594e+00
Epoch 7/10
17/17 - 5s - loss: 199.9994 - loglik: -1.9825e+02 - logprior: -1.7490e+00
Epoch 8/10
17/17 - 5s - loss: 199.6056 - loglik: -1.9785e+02 - logprior: -1.7547e+00
Epoch 9/10
17/17 - 5s - loss: 200.0676 - loglik: -1.9823e+02 - logprior: -1.8379e+00
Fitted a model with MAP estimate = -199.4073
expansions: [(0, 2), (15, 1), (16, 2), (17, 3), (26, 1), (27, 1), (28, 1), (40, 1), (41, 1), (44, 1), (45, 1), (47, 1), (53, 1), (55, 1), (56, 1), (57, 1), (64, 1), (65, 1), (66, 1), (78, 1), (81, 1), (84, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 150 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 193.3554 - loglik: -1.8853e+02 - logprior: -4.8242e+00
Epoch 2/2
17/17 - 7s - loss: 178.6935 - loglik: -1.7712e+02 - logprior: -1.5695e+00
Fitted a model with MAP estimate = -177.4610
expansions: []
discards: [ 19  20 139]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 147 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 10s - loss: 180.9218 - loglik: -1.7706e+02 - logprior: -3.8624e+00
Epoch 2/2
17/17 - 7s - loss: 177.0714 - loglik: -1.7566e+02 - logprior: -1.4080e+00
Fitted a model with MAP estimate = -176.4415
expansions: [(19, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 179.4136 - loglik: -1.7545e+02 - logprior: -3.9632e+00
Epoch 2/10
17/17 - 7s - loss: 175.9425 - loglik: -1.7458e+02 - logprior: -1.3625e+00
Epoch 3/10
17/17 - 7s - loss: 175.6688 - loglik: -1.7452e+02 - logprior: -1.1447e+00
Epoch 4/10
17/17 - 7s - loss: 176.0103 - loglik: -1.7494e+02 - logprior: -1.0726e+00
Fitted a model with MAP estimate = -175.5584
Time for alignment: 140.2094
Computed alignments with likelihoods: ['-175.6596', '-175.6166', '-175.5584']
Best model has likelihood: -175.5584
SP score = 0.6294
Training of 3 independent models on file sodcu.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb95c0eeee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9f79fc7c0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 419.6959 - loglik: -3.9899e+02 - logprior: -2.0705e+01
Epoch 2/10
10/10 - 2s - loss: 363.6591 - loglik: -3.5891e+02 - logprior: -4.7509e+00
Epoch 3/10
10/10 - 2s - loss: 319.7969 - loglik: -3.1722e+02 - logprior: -2.5764e+00
Epoch 4/10
10/10 - 2s - loss: 291.4315 - loglik: -2.8916e+02 - logprior: -2.2692e+00
Epoch 5/10
10/10 - 2s - loss: 279.5074 - loglik: -2.7718e+02 - logprior: -2.3237e+00
Epoch 6/10
10/10 - 2s - loss: 276.3421 - loglik: -2.7410e+02 - logprior: -2.2408e+00
Epoch 7/10
10/10 - 2s - loss: 273.4101 - loglik: -2.7130e+02 - logprior: -2.1106e+00
Epoch 8/10
10/10 - 2s - loss: 274.4551 - loglik: -2.7244e+02 - logprior: -2.0157e+00
Fitted a model with MAP estimate = -272.9227
expansions: [(8, 1), (9, 1), (10, 1), (13, 1), (17, 1), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (60, 1), (62, 1), (71, 1), (78, 1), (79, 1), (80, 1), (82, 1), (83, 1), (84, 1), (86, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 284.7529 - loglik: -2.6590e+02 - logprior: -1.8853e+01
Epoch 2/2
10/10 - 3s - loss: 263.1862 - loglik: -2.5863e+02 - logprior: -4.5548e+00
Fitted a model with MAP estimate = -259.9205
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 281.8343 - loglik: -2.5832e+02 - logprior: -2.3516e+01
Epoch 2/2
10/10 - 2s - loss: 267.2157 - loglik: -2.5792e+02 - logprior: -9.2960e+00
Fitted a model with MAP estimate = -264.6779
expansions: [(0, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 142 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 275.1049 - loglik: -2.5657e+02 - logprior: -1.8534e+01
Epoch 2/10
10/10 - 3s - loss: 259.9448 - loglik: -2.5573e+02 - logprior: -4.2167e+00
Epoch 3/10
10/10 - 3s - loss: 257.2964 - loglik: -2.5593e+02 - logprior: -1.3688e+00
Epoch 4/10
10/10 - 3s - loss: 256.4902 - loglik: -2.5618e+02 - logprior: -3.0864e-01
Epoch 5/10
10/10 - 3s - loss: 256.0558 - loglik: -2.5623e+02 - logprior: 0.1710
Epoch 6/10
10/10 - 3s - loss: 255.7111 - loglik: -2.5614e+02 - logprior: 0.4270
Epoch 7/10
10/10 - 3s - loss: 255.2414 - loglik: -2.5585e+02 - logprior: 0.6107
Epoch 8/10
10/10 - 3s - loss: 255.2639 - loglik: -2.5606e+02 - logprior: 0.7982
Fitted a model with MAP estimate = -255.1666
Time for alignment: 68.7859
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 419.5777 - loglik: -3.9887e+02 - logprior: -2.0705e+01
Epoch 2/10
10/10 - 2s - loss: 364.3114 - loglik: -3.5958e+02 - logprior: -4.7360e+00
Epoch 3/10
10/10 - 2s - loss: 319.8544 - loglik: -3.1738e+02 - logprior: -2.4711e+00
Epoch 4/10
10/10 - 2s - loss: 290.9465 - loglik: -2.8903e+02 - logprior: -1.9172e+00
Epoch 5/10
10/10 - 2s - loss: 281.8535 - loglik: -2.8010e+02 - logprior: -1.7525e+00
Epoch 6/10
10/10 - 2s - loss: 278.7949 - loglik: -2.7717e+02 - logprior: -1.6266e+00
Epoch 7/10
10/10 - 2s - loss: 276.8264 - loglik: -2.7528e+02 - logprior: -1.5435e+00
Epoch 8/10
10/10 - 2s - loss: 275.9963 - loglik: -2.7450e+02 - logprior: -1.4935e+00
Epoch 9/10
10/10 - 2s - loss: 275.6251 - loglik: -2.7418e+02 - logprior: -1.4431e+00
Epoch 10/10
10/10 - 2s - loss: 275.5998 - loglik: -2.7420e+02 - logprior: -1.4001e+00
Fitted a model with MAP estimate = -275.1659
expansions: [(8, 1), (9, 1), (14, 1), (17, 1), (23, 1), (24, 3), (26, 1), (53, 1), (55, 2), (62, 1), (79, 3), (80, 2), (81, 2), (84, 2), (86, 1), (98, 1), (101, 2), (102, 1), (103, 1), (104, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 284.6291 - loglik: -2.6581e+02 - logprior: -1.8819e+01
Epoch 2/2
10/10 - 3s - loss: 261.4847 - loglik: -2.5689e+02 - logprior: -4.5969e+00
Fitted a model with MAP estimate = -257.0605
expansions: []
discards: [ 65  96 104]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 142 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 271.8914 - loglik: -2.5347e+02 - logprior: -1.8421e+01
Epoch 2/2
10/10 - 3s - loss: 256.7095 - loglik: -2.5244e+02 - logprior: -4.2689e+00
Fitted a model with MAP estimate = -254.6495
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 142 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 269.6336 - loglik: -2.5135e+02 - logprior: -1.8282e+01
Epoch 2/10
10/10 - 3s - loss: 256.4033 - loglik: -2.5230e+02 - logprior: -4.1035e+00
Epoch 3/10
10/10 - 3s - loss: 253.4569 - loglik: -2.5221e+02 - logprior: -1.2419e+00
Epoch 4/10
10/10 - 3s - loss: 252.9144 - loglik: -2.5275e+02 - logprior: -1.6051e-01
Epoch 5/10
10/10 - 3s - loss: 252.8083 - loglik: -2.5314e+02 - logprior: 0.3318
Epoch 6/10
10/10 - 3s - loss: 251.6156 - loglik: -2.5220e+02 - logprior: 0.5893
Epoch 7/10
10/10 - 3s - loss: 251.1528 - loglik: -2.5196e+02 - logprior: 0.8030
Epoch 8/10
10/10 - 3s - loss: 252.6114 - loglik: -2.5361e+02 - logprior: 0.9985
Fitted a model with MAP estimate = -251.6403
Time for alignment: 74.5773
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 419.7455 - loglik: -3.9904e+02 - logprior: -2.0704e+01
Epoch 2/10
10/10 - 2s - loss: 364.3130 - loglik: -3.5957e+02 - logprior: -4.7391e+00
Epoch 3/10
10/10 - 2s - loss: 322.3462 - loglik: -3.1985e+02 - logprior: -2.4976e+00
Epoch 4/10
10/10 - 2s - loss: 293.8463 - loglik: -2.9180e+02 - logprior: -2.0464e+00
Epoch 5/10
10/10 - 2s - loss: 283.1452 - loglik: -2.8121e+02 - logprior: -1.9309e+00
Epoch 6/10
10/10 - 2s - loss: 277.7802 - loglik: -2.7601e+02 - logprior: -1.7690e+00
Epoch 7/10
10/10 - 2s - loss: 276.4668 - loglik: -2.7474e+02 - logprior: -1.7264e+00
Epoch 8/10
10/10 - 2s - loss: 276.0557 - loglik: -2.7437e+02 - logprior: -1.6883e+00
Epoch 9/10
10/10 - 2s - loss: 274.7200 - loglik: -2.7310e+02 - logprior: -1.6193e+00
Epoch 10/10
10/10 - 2s - loss: 275.1761 - loglik: -2.7357e+02 - logprior: -1.6058e+00
Fitted a model with MAP estimate = -274.5309
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (23, 1), (24, 4), (26, 1), (53, 1), (60, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (87, 1), (97, 4), (102, 1), (103, 1), (107, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 284.6342 - loglik: -2.6574e+02 - logprior: -1.8898e+01
Epoch 2/2
10/10 - 3s - loss: 261.3043 - loglik: -2.5666e+02 - logprior: -4.6438e+00
Fitted a model with MAP estimate = -256.8370
expansions: []
discards: [  0  31 119 120 121]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 282.4333 - loglik: -2.5897e+02 - logprior: -2.3467e+01
Epoch 2/2
10/10 - 2s - loss: 265.9556 - loglik: -2.5671e+02 - logprior: -9.2431e+00
Fitted a model with MAP estimate = -264.0658
expansions: [(0, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 142 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 274.0648 - loglik: -2.5559e+02 - logprior: -1.8471e+01
Epoch 2/10
10/10 - 2s - loss: 259.6594 - loglik: -2.5553e+02 - logprior: -4.1312e+00
Epoch 3/10
10/10 - 2s - loss: 256.0718 - loglik: -2.5478e+02 - logprior: -1.2879e+00
Epoch 4/10
10/10 - 2s - loss: 255.3658 - loglik: -2.5515e+02 - logprior: -2.1607e-01
Epoch 5/10
10/10 - 2s - loss: 254.9693 - loglik: -2.5524e+02 - logprior: 0.2746
Epoch 6/10
10/10 - 2s - loss: 254.2691 - loglik: -2.5479e+02 - logprior: 0.5172
Epoch 7/10
10/10 - 2s - loss: 254.0013 - loglik: -2.5471e+02 - logprior: 0.7116
Epoch 8/10
10/10 - 2s - loss: 254.4139 - loglik: -2.5531e+02 - logprior: 0.9006
Fitted a model with MAP estimate = -254.0589
Time for alignment: 69.5851
Computed alignments with likelihoods: ['-255.1666', '-251.6403', '-254.0589']
Best model has likelihood: -251.6403
SP score = 0.8620
Training of 3 independent models on file DEATH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cb43a250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbb82ea6cd0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 270.0549 - loglik: -2.3275e+02 - logprior: -3.7305e+01
Epoch 2/10
10/10 - 1s - loss: 230.9013 - loglik: -2.2136e+02 - logprior: -9.5372e+00
Epoch 3/10
10/10 - 1s - loss: 213.0909 - loglik: -2.0881e+02 - logprior: -4.2814e+00
Epoch 4/10
10/10 - 1s - loss: 202.3786 - loglik: -1.9993e+02 - logprior: -2.4508e+00
Epoch 5/10
10/10 - 1s - loss: 197.2489 - loglik: -1.9539e+02 - logprior: -1.8600e+00
Epoch 6/10
10/10 - 1s - loss: 194.3170 - loglik: -1.9257e+02 - logprior: -1.7442e+00
Epoch 7/10
10/10 - 1s - loss: 193.2223 - loglik: -1.9192e+02 - logprior: -1.3041e+00
Epoch 8/10
10/10 - 1s - loss: 192.3046 - loglik: -1.9153e+02 - logprior: -7.7564e-01
Epoch 9/10
10/10 - 1s - loss: 192.0702 - loglik: -1.9146e+02 - logprior: -6.1259e-01
Epoch 10/10
10/10 - 1s - loss: 191.6562 - loglik: -1.9110e+02 - logprior: -5.5827e-01
Fitted a model with MAP estimate = -191.6270
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (28, 1), (31, 2), (40, 2), (43, 1), (54, 1), (65, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 236.5124 - loglik: -1.8778e+02 - logprior: -4.8732e+01
Epoch 2/2
10/10 - 1s - loss: 199.0434 - loglik: -1.8480e+02 - logprior: -1.4240e+01
Fitted a model with MAP estimate = -192.2443
expansions: [(39, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 225.9732 - loglik: -1.8400e+02 - logprior: -4.1969e+01
Epoch 2/2
10/10 - 1s - loss: 199.9911 - loglik: -1.8398e+02 - logprior: -1.6015e+01
Fitted a model with MAP estimate = -196.0084
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 222.6770 - loglik: -1.8335e+02 - logprior: -3.9331e+01
Epoch 2/10
10/10 - 1s - loss: 194.4836 - loglik: -1.8338e+02 - logprior: -1.1105e+01
Epoch 3/10
10/10 - 1s - loss: 187.3555 - loglik: -1.8388e+02 - logprior: -3.4709e+00
Epoch 4/10
10/10 - 1s - loss: 184.8795 - loglik: -1.8394e+02 - logprior: -9.3470e-01
Epoch 5/10
10/10 - 1s - loss: 184.0111 - loglik: -1.8416e+02 - logprior: 0.1482
Epoch 6/10
10/10 - 1s - loss: 183.2772 - loglik: -1.8394e+02 - logprior: 0.6671
Epoch 7/10
10/10 - 1s - loss: 183.1602 - loglik: -1.8422e+02 - logprior: 1.0609
Epoch 8/10
10/10 - 1s - loss: 182.6117 - loglik: -1.8404e+02 - logprior: 1.4263
Epoch 9/10
10/10 - 1s - loss: 182.6533 - loglik: -1.8434e+02 - logprior: 1.6845
Fitted a model with MAP estimate = -182.5056
Time for alignment: 37.2011
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 270.0345 - loglik: -2.3273e+02 - logprior: -3.7305e+01
Epoch 2/10
10/10 - 1s - loss: 231.3491 - loglik: -2.2181e+02 - logprior: -9.5389e+00
Epoch 3/10
10/10 - 1s - loss: 214.9748 - loglik: -2.1065e+02 - logprior: -4.3260e+00
Epoch 4/10
10/10 - 1s - loss: 203.0312 - loglik: -2.0052e+02 - logprior: -2.5119e+00
Epoch 5/10
10/10 - 1s - loss: 197.1048 - loglik: -1.9519e+02 - logprior: -1.9164e+00
Epoch 6/10
10/10 - 1s - loss: 194.0678 - loglik: -1.9228e+02 - logprior: -1.7912e+00
Epoch 7/10
10/10 - 1s - loss: 192.9168 - loglik: -1.9163e+02 - logprior: -1.2869e+00
Epoch 8/10
10/10 - 1s - loss: 192.0345 - loglik: -1.9124e+02 - logprior: -7.9167e-01
Epoch 9/10
10/10 - 1s - loss: 191.8038 - loglik: -1.9116e+02 - logprior: -6.4519e-01
Epoch 10/10
10/10 - 1s - loss: 191.2966 - loglik: -1.9072e+02 - logprior: -5.7357e-01
Fitted a model with MAP estimate = -191.3034
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (30, 3), (40, 2), (43, 1), (54, 1), (65, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 236.3812 - loglik: -1.8765e+02 - logprior: -4.8727e+01
Epoch 2/2
10/10 - 1s - loss: 198.6888 - loglik: -1.8446e+02 - logprior: -1.4232e+01
Fitted a model with MAP estimate = -192.1789
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 225.9454 - loglik: -1.8390e+02 - logprior: -4.2049e+01
Epoch 2/2
10/10 - 1s - loss: 200.3299 - loglik: -1.8430e+02 - logprior: -1.6033e+01
Fitted a model with MAP estimate = -196.3190
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 222.9019 - loglik: -1.8353e+02 - logprior: -3.9376e+01
Epoch 2/10
10/10 - 1s - loss: 194.9515 - loglik: -1.8380e+02 - logprior: -1.1149e+01
Epoch 3/10
10/10 - 1s - loss: 187.7504 - loglik: -1.8423e+02 - logprior: -3.5198e+00
Epoch 4/10
10/10 - 1s - loss: 185.3509 - loglik: -1.8437e+02 - logprior: -9.8442e-01
Epoch 5/10
10/10 - 1s - loss: 184.1926 - loglik: -1.8428e+02 - logprior: 0.0902
Epoch 6/10
10/10 - 1s - loss: 183.7673 - loglik: -1.8438e+02 - logprior: 0.6142
Epoch 7/10
10/10 - 1s - loss: 183.2613 - loglik: -1.8427e+02 - logprior: 1.0077
Epoch 8/10
10/10 - 1s - loss: 183.3983 - loglik: -1.8477e+02 - logprior: 1.3697
Fitted a model with MAP estimate = -183.0340
Time for alignment: 36.9179
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 270.1804 - loglik: -2.3288e+02 - logprior: -3.7304e+01
Epoch 2/10
10/10 - 1s - loss: 231.0815 - loglik: -2.2154e+02 - logprior: -9.5374e+00
Epoch 3/10
10/10 - 1s - loss: 213.9193 - loglik: -2.0963e+02 - logprior: -4.2853e+00
Epoch 4/10
10/10 - 1s - loss: 202.6420 - loglik: -2.0017e+02 - logprior: -2.4679e+00
Epoch 5/10
10/10 - 1s - loss: 197.5591 - loglik: -1.9571e+02 - logprior: -1.8472e+00
Epoch 6/10
10/10 - 1s - loss: 194.6413 - loglik: -1.9294e+02 - logprior: -1.7024e+00
Epoch 7/10
10/10 - 1s - loss: 193.1002 - loglik: -1.9174e+02 - logprior: -1.3579e+00
Epoch 8/10
10/10 - 1s - loss: 192.3623 - loglik: -1.9152e+02 - logprior: -8.4203e-01
Epoch 9/10
10/10 - 1s - loss: 192.1648 - loglik: -1.9151e+02 - logprior: -6.5277e-01
Epoch 10/10
10/10 - 1s - loss: 191.7641 - loglik: -1.9118e+02 - logprior: -5.8322e-01
Fitted a model with MAP estimate = -191.6571
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (28, 1), (31, 2), (40, 2), (43, 1), (54, 1), (65, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 236.4344 - loglik: -1.8770e+02 - logprior: -4.8731e+01
Epoch 2/2
10/10 - 1s - loss: 199.1017 - loglik: -1.8486e+02 - logprior: -1.4242e+01
Fitted a model with MAP estimate = -192.2987
expansions: [(39, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 225.9502 - loglik: -1.8398e+02 - logprior: -4.1973e+01
Epoch 2/2
10/10 - 1s - loss: 200.1002 - loglik: -1.8409e+02 - logprior: -1.6008e+01
Fitted a model with MAP estimate = -196.1361
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 221.2602 - loglik: -1.8363e+02 - logprior: -3.7631e+01
Epoch 2/10
10/10 - 1s - loss: 192.8983 - loglik: -1.8337e+02 - logprior: -9.5245e+00
Epoch 3/10
10/10 - 1s - loss: 187.2465 - loglik: -1.8409e+02 - logprior: -3.1516e+00
Epoch 4/10
10/10 - 1s - loss: 185.0549 - loglik: -1.8418e+02 - logprior: -8.7807e-01
Epoch 5/10
10/10 - 1s - loss: 184.0805 - loglik: -1.8428e+02 - logprior: 0.1979
Epoch 6/10
10/10 - 1s - loss: 183.6329 - loglik: -1.8437e+02 - logprior: 0.7323
Epoch 7/10
10/10 - 1s - loss: 183.1317 - loglik: -1.8421e+02 - logprior: 1.0785
Epoch 8/10
10/10 - 1s - loss: 182.9602 - loglik: -1.8437e+02 - logprior: 1.4079
Epoch 9/10
10/10 - 1s - loss: 182.8834 - loglik: -1.8457e+02 - logprior: 1.6832
Epoch 10/10
10/10 - 1s - loss: 182.5674 - loglik: -1.8444e+02 - logprior: 1.8705
Fitted a model with MAP estimate = -182.5651
Time for alignment: 37.2031
Computed alignments with likelihoods: ['-182.5056', '-183.0340', '-182.5651']
Best model has likelihood: -182.5056
SP score = 0.7786
Training of 3 independent models on file aat.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbb945f65b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fba11461c40>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 40s - loss: 952.0729 - loglik: -9.5080e+02 - logprior: -1.2755e+00
Epoch 2/10
43/43 - 42s - loss: 838.0777 - loglik: -8.3647e+02 - logprior: -1.6070e+00
Epoch 3/10
43/43 - 47s - loss: 826.9702 - loglik: -8.2536e+02 - logprior: -1.6103e+00
Epoch 4/10
43/43 - 51s - loss: 822.6854 - loglik: -8.2108e+02 - logprior: -1.6023e+00
Epoch 5/10
43/43 - 52s - loss: 819.6782 - loglik: -8.1803e+02 - logprior: -1.6506e+00
Epoch 6/10
43/43 - 52s - loss: 818.0198 - loglik: -8.1634e+02 - logprior: -1.6756e+00
Epoch 7/10
43/43 - 45s - loss: 817.9304 - loglik: -8.1629e+02 - logprior: -1.6417e+00
Epoch 8/10
43/43 - 41s - loss: 818.3374 - loglik: -8.1665e+02 - logprior: -1.6906e+00
Fitted a model with MAP estimate = -832.0073
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (24, 1), (25, 3), (26, 1), (30, 1), (40, 1), (42, 1), (43, 1), (46, 1), (47, 1), (51, 2), (60, 1), (61, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 2), (86, 2), (90, 1), (91, 1), (93, 1), (95, 1), (96, 1), (101, 1), (104, 2), (121, 1), (122, 1), (125, 1), (131, 1), (133, 2), (143, 1), (148, 1), (150, 1), (153, 2), (156, 2), (157, 2), (168, 1), (181, 1), (183, 1), (185, 2), (186, 2), (187, 1), (188, 1), (204, 1), (205, 3), (206, 2), (207, 1), (210, 1), (219, 1), (220, 1), (221, 1), (222, 1), (225, 1), (226, 2), (236, 1), (239, 1), (240, 3), (242, 1), (245, 1), (250, 1), (253, 1), (260, 2), (269, 3), (270, 2), (271, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 372 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 70s - loss: 797.9160 - loglik: -7.9595e+02 - logprior: -1.9633e+00
Epoch 2/2
43/43 - 63s - loss: 783.8203 - loglik: -7.8298e+02 - logprior: -8.3815e-01
Fitted a model with MAP estimate = -810.7340
expansions: []
discards: [  0   1  32  67 106 112 138 173 199 206 242 266 267 268 294 299 315 316
 317 359 362]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 351 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 60s - loss: 786.8000 - loglik: -7.8551e+02 - logprior: -1.2869e+00
Epoch 2/2
43/43 - 68s - loss: 784.9620 - loglik: -7.8458e+02 - logprior: -3.8261e-01
Fitted a model with MAP estimate = -818.4427
expansions: [(0, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 353 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 114s - loss: 790.6959 - loglik: -7.8998e+02 - logprior: -7.1456e-01
Epoch 2/10
61/61 - 115s - loss: 775.3108 - loglik: -7.7496e+02 - logprior: -3.4730e-01
Epoch 3/10
61/61 - 92s - loss: 769.3938 - loglik: -7.6916e+02 - logprior: -2.3590e-01
Epoch 4/10
61/61 - 102s - loss: 767.9537 - loglik: -7.6778e+02 - logprior: -1.7593e-01
Epoch 5/10
61/61 - 90s - loss: 764.6395 - loglik: -7.6453e+02 - logprior: -1.1336e-01
Epoch 6/10
61/61 - 87s - loss: 762.4180 - loglik: -7.6239e+02 - logprior: -2.8537e-02
Epoch 7/10
61/61 - 101s - loss: 763.8857 - loglik: -7.6391e+02 - logprior: 0.0252
Fitted a model with MAP estimate = -761.1797
Time for alignment: 1817.1183
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 41s - loss: 952.6386 - loglik: -9.5135e+02 - logprior: -1.2836e+00
Epoch 2/10
43/43 - 43s - loss: 838.5894 - loglik: -8.3689e+02 - logprior: -1.6975e+00
Epoch 3/10
43/43 - 48s - loss: 826.3141 - loglik: -8.2459e+02 - logprior: -1.7222e+00
Epoch 4/10
43/43 - 54s - loss: 820.8473 - loglik: -8.1912e+02 - logprior: -1.7235e+00
Epoch 5/10
43/43 - 56s - loss: 819.3197 - loglik: -8.1757e+02 - logprior: -1.7456e+00
Epoch 6/10
43/43 - 56s - loss: 817.9514 - loglik: -8.1617e+02 - logprior: -1.7831e+00
Epoch 7/10
43/43 - 56s - loss: 818.1279 - loglik: -8.1637e+02 - logprior: -1.7619e+00
Fitted a model with MAP estimate = -826.9083
expansions: [(8, 1), (13, 1), (16, 1), (21, 1), (22, 1), (24, 1), (25, 3), (26, 1), (30, 1), (40, 1), (42, 1), (43, 1), (46, 1), (47, 1), (49, 2), (57, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 2), (86, 2), (90, 1), (91, 1), (93, 1), (95, 1), (97, 1), (98, 1), (120, 1), (121, 1), (122, 1), (125, 1), (131, 1), (132, 2), (143, 1), (148, 1), (150, 1), (153, 2), (154, 1), (155, 3), (168, 1), (181, 1), (183, 2), (185, 2), (186, 2), (187, 1), (197, 1), (204, 1), (205, 3), (206, 2), (207, 1), (210, 1), (219, 1), (220, 1), (221, 1), (222, 1), (225, 1), (226, 2), (236, 1), (238, 3), (242, 1), (244, 1), (245, 1), (249, 1), (260, 2), (269, 3), (270, 2), (271, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 370 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 93s - loss: 801.8994 - loglik: -7.9961e+02 - logprior: -2.2896e+00
Epoch 2/2
43/43 - 85s - loss: 784.0852 - loglik: -7.8299e+02 - logprior: -1.0998e+00
Fitted a model with MAP estimate = -815.4129
expansions: [(0, 2)]
discards: [  0  31  64 105 111 170 198 199 235 241 265 266 267 293 298 315 355 357
 360]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 353 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 62s - loss: 785.4836 - loglik: -7.8436e+02 - logprior: -1.1258e+00
Epoch 2/2
43/43 - 62s - loss: 782.9354 - loglik: -7.8261e+02 - logprior: -3.2653e-01
Fitted a model with MAP estimate = -813.3518
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 352 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 88s - loss: 792.1292 - loglik: -7.9132e+02 - logprior: -8.0657e-01
Epoch 2/10
61/61 - 110s - loss: 774.9735 - loglik: -7.7457e+02 - logprior: -4.0268e-01
Epoch 3/10
61/61 - 111s - loss: 771.6954 - loglik: -7.7142e+02 - logprior: -2.7096e-01
Epoch 4/10
61/61 - 89s - loss: 767.8589 - loglik: -7.6763e+02 - logprior: -2.2979e-01
Epoch 5/10
61/61 - 84s - loss: 765.6398 - loglik: -7.6546e+02 - logprior: -1.7642e-01
Epoch 6/10
61/61 - 84s - loss: 764.4216 - loglik: -7.6430e+02 - logprior: -1.2334e-01
Epoch 7/10
61/61 - 76s - loss: 761.9779 - loglik: -7.6200e+02 - logprior: 0.0208
Epoch 8/10
61/61 - 73s - loss: 761.6700 - loglik: -7.6176e+02 - logprior: 0.0943
Epoch 9/10
61/61 - 89s - loss: 762.8997 - loglik: -7.6308e+02 - logprior: 0.1836
Fitted a model with MAP estimate = -762.3503
Time for alignment: 1951.1535
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 47s - loss: 951.9836 - loglik: -9.5071e+02 - logprior: -1.2716e+00
Epoch 2/10
43/43 - 45s - loss: 838.7747 - loglik: -8.3708e+02 - logprior: -1.6968e+00
Epoch 3/10
43/43 - 48s - loss: 826.6744 - loglik: -8.2497e+02 - logprior: -1.7090e+00
Epoch 4/10
43/43 - 43s - loss: 825.1388 - loglik: -8.2343e+02 - logprior: -1.7052e+00
Epoch 5/10
43/43 - 44s - loss: 821.6368 - loglik: -8.1986e+02 - logprior: -1.7779e+00
Epoch 6/10
43/43 - 46s - loss: 820.9775 - loglik: -8.1918e+02 - logprior: -1.7977e+00
Epoch 7/10
43/43 - 44s - loss: 818.5666 - loglik: -8.1679e+02 - logprior: -1.7797e+00
Epoch 8/10
43/43 - 47s - loss: 820.4077 - loglik: -8.1861e+02 - logprior: -1.7993e+00
Fitted a model with MAP estimate = -829.6281
expansions: [(7, 2), (16, 1), (20, 1), (21, 1), (22, 2), (23, 1), (24, 2), (30, 1), (40, 1), (42, 1), (43, 1), (46, 1), (47, 1), (57, 1), (60, 1), (61, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 1), (86, 2), (90, 1), (91, 1), (93, 1), (95, 1), (96, 1), (102, 1), (120, 1), (121, 1), (122, 1), (123, 1), (125, 1), (130, 1), (132, 2), (148, 2), (153, 1), (156, 2), (157, 2), (168, 1), (181, 1), (183, 1), (185, 2), (186, 1), (187, 1), (188, 1), (205, 1), (206, 4), (207, 2), (208, 1), (210, 1), (219, 1), (220, 1), (221, 1), (222, 1), (225, 1), (226, 2), (236, 1), (238, 2), (242, 1), (245, 1), (248, 1), (249, 1), (260, 2), (269, 3), (270, 2), (271, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 81s - loss: 798.3861 - loglik: -7.9618e+02 - logprior: -2.2048e+00
Epoch 2/2
43/43 - 83s - loss: 784.6265 - loglik: -7.8368e+02 - logprior: -9.4714e-01
Fitted a model with MAP estimate = -813.4031
expansions: [(0, 1), (263, 1)]
discards: [  0  27 109 169 201 235 265 294 310 350 352 355]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 355 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 68s - loss: 784.5685 - loglik: -7.8353e+02 - logprior: -1.0358e+00
Epoch 2/2
43/43 - 67s - loss: 782.8581 - loglik: -7.8246e+02 - logprior: -3.9571e-01
Fitted a model with MAP estimate = -812.6815
expansions: []
discards: [257 284]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 353 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 107s - loss: 789.5601 - loglik: -7.8888e+02 - logprior: -6.7584e-01
Epoch 2/10
61/61 - 101s - loss: 776.5268 - loglik: -7.7615e+02 - logprior: -3.7955e-01
Epoch 3/10
61/61 - 96s - loss: 771.2086 - loglik: -7.7093e+02 - logprior: -2.8134e-01
Epoch 4/10
61/61 - 94s - loss: 767.3294 - loglik: -7.6708e+02 - logprior: -2.5211e-01
Epoch 5/10
61/61 - 94s - loss: 767.0101 - loglik: -7.6687e+02 - logprior: -1.4072e-01
Epoch 6/10
61/61 - 107s - loss: 762.9485 - loglik: -7.6287e+02 - logprior: -7.6390e-02
Epoch 7/10
61/61 - 91s - loss: 763.7836 - loglik: -7.6378e+02 - logprior: 4.8747e-04
Fitted a model with MAP estimate = -761.3196
Time for alignment: 1841.4068
Computed alignments with likelihoods: ['-761.1797', '-762.3503', '-761.3196']
Best model has likelihood: -761.1797
SP score = 0.8299
Training of 3 independent models on file ins.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9f7149ee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cc7eb040>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 249.9208 - loglik: -1.9326e+02 - logprior: -5.6663e+01
Epoch 2/10
10/10 - 1s - loss: 188.1821 - loglik: -1.7260e+02 - logprior: -1.5580e+01
Epoch 3/10
10/10 - 1s - loss: 162.7602 - loglik: -1.5516e+02 - logprior: -7.5999e+00
Epoch 4/10
10/10 - 1s - loss: 153.7489 - loglik: -1.4924e+02 - logprior: -4.5052e+00
Epoch 5/10
10/10 - 1s - loss: 150.6480 - loglik: -1.4769e+02 - logprior: -2.9622e+00
Epoch 6/10
10/10 - 1s - loss: 148.9420 - loglik: -1.4694e+02 - logprior: -2.0017e+00
Epoch 7/10
10/10 - 1s - loss: 148.8336 - loglik: -1.4740e+02 - logprior: -1.4307e+00
Epoch 8/10
10/10 - 1s - loss: 148.6923 - loglik: -1.4754e+02 - logprior: -1.1531e+00
Epoch 9/10
10/10 - 1s - loss: 147.9456 - loglik: -1.4694e+02 - logprior: -1.0013e+00
Epoch 10/10
10/10 - 1s - loss: 148.1516 - loglik: -1.4729e+02 - logprior: -8.5905e-01
Fitted a model with MAP estimate = -147.9339
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 209.5435 - loglik: -1.4534e+02 - logprior: -6.4202e+01
Epoch 2/2
10/10 - 1s - loss: 172.7260 - loglik: -1.4565e+02 - logprior: -2.7079e+01
Fitted a model with MAP estimate = -167.6901
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 196.2312 - loglik: -1.4383e+02 - logprior: -5.2404e+01
Epoch 2/2
10/10 - 1s - loss: 159.6002 - loglik: -1.4506e+02 - logprior: -1.4542e+01
Fitted a model with MAP estimate = -154.1551
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 207.2666 - loglik: -1.4513e+02 - logprior: -6.2136e+01
Epoch 2/10
10/10 - 1s - loss: 165.7787 - loglik: -1.4504e+02 - logprior: -2.0734e+01
Epoch 3/10
10/10 - 1s - loss: 154.3971 - loglik: -1.4623e+02 - logprior: -8.1702e+00
Epoch 4/10
10/10 - 1s - loss: 150.7290 - loglik: -1.4691e+02 - logprior: -3.8202e+00
Epoch 5/10
10/10 - 1s - loss: 149.1069 - loglik: -1.4710e+02 - logprior: -2.0075e+00
Epoch 6/10
10/10 - 1s - loss: 148.5417 - loglik: -1.4741e+02 - logprior: -1.1357e+00
Epoch 7/10
10/10 - 1s - loss: 148.1899 - loglik: -1.4751e+02 - logprior: -6.7650e-01
Epoch 8/10
10/10 - 1s - loss: 148.0496 - loglik: -1.4769e+02 - logprior: -3.5549e-01
Epoch 9/10
10/10 - 1s - loss: 146.9018 - loglik: -1.4682e+02 - logprior: -8.1749e-02
Epoch 10/10
10/10 - 1s - loss: 147.2627 - loglik: -1.4738e+02 - logprior: 0.1202
Fitted a model with MAP estimate = -147.1164
Time for alignment: 42.8146
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 250.6015 - loglik: -1.9394e+02 - logprior: -5.6664e+01
Epoch 2/10
10/10 - 1s - loss: 187.4854 - loglik: -1.7190e+02 - logprior: -1.5589e+01
Epoch 3/10
10/10 - 1s - loss: 162.5114 - loglik: -1.5492e+02 - logprior: -7.5897e+00
Epoch 4/10
10/10 - 1s - loss: 153.1552 - loglik: -1.4866e+02 - logprior: -4.4981e+00
Epoch 5/10
10/10 - 1s - loss: 150.7643 - loglik: -1.4777e+02 - logprior: -2.9942e+00
Epoch 6/10
10/10 - 1s - loss: 148.9099 - loglik: -1.4683e+02 - logprior: -2.0826e+00
Epoch 7/10
10/10 - 1s - loss: 149.0965 - loglik: -1.4758e+02 - logprior: -1.5198e+00
Fitted a model with MAP estimate = -148.5467
expansions: [(10, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 208.2795 - loglik: -1.4424e+02 - logprior: -6.4035e+01
Epoch 2/2
10/10 - 1s - loss: 170.6167 - loglik: -1.4365e+02 - logprior: -2.6970e+01
Fitted a model with MAP estimate = -165.0740
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 193.4147 - loglik: -1.4128e+02 - logprior: -5.2135e+01
Epoch 2/2
10/10 - 1s - loss: 156.8725 - loglik: -1.4237e+02 - logprior: -1.4504e+01
Fitted a model with MAP estimate = -151.4207
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 204.7842 - loglik: -1.4218e+02 - logprior: -6.2601e+01
Epoch 2/10
10/10 - 1s - loss: 164.9956 - loglik: -1.4283e+02 - logprior: -2.2161e+01
Epoch 3/10
10/10 - 1s - loss: 151.6973 - loglik: -1.4280e+02 - logprior: -8.8942e+00
Epoch 4/10
10/10 - 1s - loss: 148.5935 - loglik: -1.4467e+02 - logprior: -3.9207e+00
Epoch 5/10
10/10 - 1s - loss: 146.4102 - loglik: -1.4439e+02 - logprior: -2.0207e+00
Epoch 6/10
10/10 - 1s - loss: 145.2995 - loglik: -1.4416e+02 - logprior: -1.1368e+00
Epoch 7/10
10/10 - 1s - loss: 145.3670 - loglik: -1.4470e+02 - logprior: -6.6996e-01
Fitted a model with MAP estimate = -144.9406
Time for alignment: 36.6629
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 250.2385 - loglik: -1.9357e+02 - logprior: -5.6665e+01
Epoch 2/10
10/10 - 1s - loss: 187.2100 - loglik: -1.7162e+02 - logprior: -1.5593e+01
Epoch 3/10
10/10 - 1s - loss: 161.8275 - loglik: -1.5424e+02 - logprior: -7.5879e+00
Epoch 4/10
10/10 - 1s - loss: 152.4388 - loglik: -1.4785e+02 - logprior: -4.5900e+00
Epoch 5/10
10/10 - 1s - loss: 149.9600 - loglik: -1.4672e+02 - logprior: -3.2426e+00
Epoch 6/10
10/10 - 1s - loss: 147.8327 - loglik: -1.4549e+02 - logprior: -2.3405e+00
Epoch 7/10
10/10 - 1s - loss: 148.0886 - loglik: -1.4633e+02 - logprior: -1.7576e+00
Fitted a model with MAP estimate = -147.4755
expansions: [(11, 1), (12, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 206.5924 - loglik: -1.4256e+02 - logprior: -6.4028e+01
Epoch 2/2
10/10 - 1s - loss: 168.7613 - loglik: -1.4184e+02 - logprior: -2.6917e+01
Fitted a model with MAP estimate = -162.8673
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 191.7079 - loglik: -1.3974e+02 - logprior: -5.1973e+01
Epoch 2/2
10/10 - 1s - loss: 154.1641 - loglik: -1.3974e+02 - logprior: -1.4424e+01
Fitted a model with MAP estimate = -149.3143
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 201.9698 - loglik: -1.3959e+02 - logprior: -6.2381e+01
Epoch 2/10
10/10 - 1s - loss: 162.6782 - loglik: -1.4116e+02 - logprior: -2.1522e+01
Epoch 3/10
10/10 - 1s - loss: 149.7225 - loglik: -1.4125e+02 - logprior: -8.4774e+00
Epoch 4/10
10/10 - 1s - loss: 146.0328 - loglik: -1.4226e+02 - logprior: -3.7769e+00
Epoch 5/10
10/10 - 1s - loss: 144.8352 - loglik: -1.4292e+02 - logprior: -1.9179e+00
Epoch 6/10
10/10 - 1s - loss: 143.5453 - loglik: -1.4252e+02 - logprior: -1.0290e+00
Epoch 7/10
10/10 - 1s - loss: 142.0867 - loglik: -1.4153e+02 - logprior: -5.6164e-01
Epoch 8/10
10/10 - 1s - loss: 143.3730 - loglik: -1.4315e+02 - logprior: -2.1893e-01
Fitted a model with MAP estimate = -142.5643
Time for alignment: 35.8091
Computed alignments with likelihoods: ['-147.1164', '-144.9406', '-142.5643']
Best model has likelihood: -142.5643
SP score = 0.9209
Training of 3 independent models on file sti.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbba495ff70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbbad488220>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 538.4971 - loglik: -4.7175e+02 - logprior: -6.6744e+01
Epoch 2/10
10/10 - 2s - loss: 438.1954 - loglik: -4.2494e+02 - logprior: -1.3252e+01
Epoch 3/10
10/10 - 2s - loss: 383.5283 - loglik: -3.7928e+02 - logprior: -4.2521e+00
Epoch 4/10
10/10 - 2s - loss: 353.8291 - loglik: -3.5229e+02 - logprior: -1.5371e+00
Epoch 5/10
10/10 - 2s - loss: 341.1633 - loglik: -3.4105e+02 - logprior: -1.1686e-01
Epoch 6/10
10/10 - 2s - loss: 335.6781 - loglik: -3.3649e+02 - logprior: 0.8108
Epoch 7/10
10/10 - 3s - loss: 331.0578 - loglik: -3.3246e+02 - logprior: 1.4063
Epoch 8/10
10/10 - 3s - loss: 329.0117 - loglik: -3.3083e+02 - logprior: 1.8152
Epoch 9/10
10/10 - 2s - loss: 327.9543 - loglik: -3.3003e+02 - logprior: 2.0718
Epoch 10/10
10/10 - 3s - loss: 326.8568 - loglik: -3.2917e+02 - logprior: 2.3174
Fitted a model with MAP estimate = -326.6725
expansions: [(11, 3), (13, 1), (19, 2), (26, 1), (27, 1), (28, 1), (38, 1), (40, 2), (51, 1), (61, 1), (65, 1), (68, 1), (79, 4), (80, 2), (89, 1), (90, 1), (103, 1), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 172 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 396.8922 - loglik: -3.2172e+02 - logprior: -7.5171e+01
Epoch 2/2
10/10 - 4s - loss: 335.0815 - loglik: -3.0766e+02 - logprior: -2.7418e+01
Fitted a model with MAP estimate = -324.7133
expansions: [(0, 1)]
discards: [  0  11  23 162]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 361.4044 - loglik: -3.0284e+02 - logprior: -5.8561e+01
Epoch 2/2
10/10 - 4s - loss: 311.4907 - loglik: -3.0083e+02 - logprior: -1.0657e+01
Fitted a model with MAP estimate = -304.5834
expansions: []
discards: [138]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 357.6935 - loglik: -3.0031e+02 - logprior: -5.7379e+01
Epoch 2/10
10/10 - 3s - loss: 310.2986 - loglik: -3.0054e+02 - logprior: -9.7580e+00
Epoch 3/10
10/10 - 3s - loss: 299.7052 - loglik: -3.0011e+02 - logprior: 0.4048
Epoch 4/10
10/10 - 3s - loss: 295.7129 - loglik: -3.0054e+02 - logprior: 4.8280
Epoch 5/10
10/10 - 3s - loss: 292.4408 - loglik: -2.9983e+02 - logprior: 7.3880
Epoch 6/10
10/10 - 3s - loss: 291.2133 - loglik: -3.0018e+02 - logprior: 8.9674
Epoch 7/10
10/10 - 3s - loss: 290.1526 - loglik: -3.0018e+02 - logprior: 10.0275
Epoch 8/10
10/10 - 3s - loss: 289.0658 - loglik: -2.9988e+02 - logprior: 10.8121
Epoch 9/10
10/10 - 3s - loss: 288.8381 - loglik: -3.0024e+02 - logprior: 11.4046
Epoch 10/10
10/10 - 4s - loss: 288.3787 - loglik: -3.0027e+02 - logprior: 11.8871
Fitted a model with MAP estimate = -287.6993
Time for alignment: 90.1713
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 538.6638 - loglik: -4.7192e+02 - logprior: -6.6741e+01
Epoch 2/10
10/10 - 3s - loss: 439.1552 - loglik: -4.2589e+02 - logprior: -1.3266e+01
Epoch 3/10
10/10 - 2s - loss: 388.0885 - loglik: -3.8375e+02 - logprior: -4.3421e+00
Epoch 4/10
10/10 - 3s - loss: 358.2680 - loglik: -3.5675e+02 - logprior: -1.5188e+00
Epoch 5/10
10/10 - 3s - loss: 344.6680 - loglik: -3.4455e+02 - logprior: -1.2121e-01
Epoch 6/10
10/10 - 3s - loss: 335.7454 - loglik: -3.3640e+02 - logprior: 0.6546
Epoch 7/10
10/10 - 3s - loss: 331.6650 - loglik: -3.3287e+02 - logprior: 1.2011
Epoch 8/10
10/10 - 3s - loss: 329.2005 - loglik: -3.3085e+02 - logprior: 1.6494
Epoch 9/10
10/10 - 3s - loss: 327.3827 - loglik: -3.2931e+02 - logprior: 1.9261
Epoch 10/10
10/10 - 3s - loss: 327.0120 - loglik: -3.2917e+02 - logprior: 2.1604
Fitted a model with MAP estimate = -326.5242
expansions: [(11, 3), (12, 2), (16, 2), (19, 1), (26, 1), (27, 1), (28, 1), (38, 1), (40, 2), (51, 1), (61, 1), (65, 1), (68, 1), (71, 2), (77, 2), (79, 2), (80, 2), (89, 1), (90, 2), (103, 2), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (127, 1), (128, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 397.9257 - loglik: -3.2269e+02 - logprior: -7.5232e+01
Epoch 2/2
10/10 - 4s - loss: 333.6571 - loglik: -3.0570e+02 - logprior: -2.7955e+01
Fitted a model with MAP estimate = -322.6860
expansions: [(0, 1), (51, 1)]
discards: [  0  11  48  88 117 131]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 174 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 359.3224 - loglik: -3.0045e+02 - logprior: -5.8870e+01
Epoch 2/2
10/10 - 3s - loss: 307.6343 - loglik: -2.9665e+02 - logprior: -1.0987e+01
Fitted a model with MAP estimate = -300.5409
expansions: [(20, 1)]
discards: [142]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 174 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 353.2210 - loglik: -2.9584e+02 - logprior: -5.7381e+01
Epoch 2/10
10/10 - 3s - loss: 304.9168 - loglik: -2.9520e+02 - logprior: -9.7137e+00
Epoch 3/10
10/10 - 4s - loss: 295.2368 - loglik: -2.9566e+02 - logprior: 0.4278
Epoch 4/10
10/10 - 4s - loss: 290.3236 - loglik: -2.9522e+02 - logprior: 4.8976
Epoch 5/10
10/10 - 4s - loss: 288.1830 - loglik: -2.9565e+02 - logprior: 7.4663
Epoch 6/10
10/10 - 4s - loss: 286.7266 - loglik: -2.9577e+02 - logprior: 9.0479
Epoch 7/10
10/10 - 4s - loss: 285.3702 - loglik: -2.9549e+02 - logprior: 10.1233
Epoch 8/10
10/10 - 4s - loss: 285.0711 - loglik: -2.9599e+02 - logprior: 10.9189
Epoch 9/10
10/10 - 4s - loss: 283.9247 - loglik: -2.9545e+02 - logprior: 11.5205
Epoch 10/10
10/10 - 4s - loss: 283.2949 - loglik: -2.9531e+02 - logprior: 12.0183
Fitted a model with MAP estimate = -283.1481
Time for alignment: 94.6758
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 538.7246 - loglik: -4.7198e+02 - logprior: -6.6743e+01
Epoch 2/10
10/10 - 3s - loss: 439.4008 - loglik: -4.2613e+02 - logprior: -1.3268e+01
Epoch 3/10
10/10 - 3s - loss: 386.9394 - loglik: -3.8264e+02 - logprior: -4.3004e+00
Epoch 4/10
10/10 - 3s - loss: 358.5807 - loglik: -3.5726e+02 - logprior: -1.3178e+00
Epoch 5/10
10/10 - 3s - loss: 345.7503 - loglik: -3.4565e+02 - logprior: -1.0502e-01
Epoch 6/10
10/10 - 3s - loss: 339.3926 - loglik: -3.4014e+02 - logprior: 0.7498
Epoch 7/10
10/10 - 3s - loss: 335.9910 - loglik: -3.3744e+02 - logprior: 1.4449
Epoch 8/10
10/10 - 3s - loss: 334.2648 - loglik: -3.3611e+02 - logprior: 1.8422
Epoch 9/10
10/10 - 3s - loss: 332.8906 - loglik: -3.3503e+02 - logprior: 2.1416
Epoch 10/10
10/10 - 3s - loss: 332.1983 - loglik: -3.3465e+02 - logprior: 2.4484
Fitted a model with MAP estimate = -331.9579
expansions: [(11, 3), (13, 1), (19, 2), (26, 1), (27, 1), (28, 1), (36, 2), (38, 2), (39, 2), (72, 1), (77, 3), (79, 2), (89, 1), (90, 3), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 172 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 405.6592 - loglik: -3.3030e+02 - logprior: -7.5363e+01
Epoch 2/2
10/10 - 4s - loss: 342.9689 - loglik: -3.1513e+02 - logprior: -2.7834e+01
Fitted a model with MAP estimate = -331.7850
expansions: [(0, 1), (127, 1)]
discards: [  0  11  23  44  49  51 112 113 162]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 165 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 369.6525 - loglik: -3.1074e+02 - logprior: -5.8908e+01
Epoch 2/2
10/10 - 3s - loss: 318.3669 - loglik: -3.0722e+02 - logprior: -1.1144e+01
Fitted a model with MAP estimate = -310.9706
expansions: [(119, 1)]
discards: [134]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 165 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 364.0193 - loglik: -3.0630e+02 - logprior: -5.7716e+01
Epoch 2/10
10/10 - 3s - loss: 315.5232 - loglik: -3.0537e+02 - logprior: -1.0154e+01
Epoch 3/10
10/10 - 3s - loss: 306.4837 - loglik: -3.0644e+02 - logprior: -3.9891e-02
Epoch 4/10
10/10 - 3s - loss: 302.1862 - loglik: -3.0654e+02 - logprior: 4.3573
Epoch 5/10
10/10 - 3s - loss: 299.9921 - loglik: -3.0690e+02 - logprior: 6.9127
Epoch 6/10
10/10 - 3s - loss: 298.4283 - loglik: -3.0692e+02 - logprior: 8.4881
Epoch 7/10
10/10 - 3s - loss: 297.8263 - loglik: -3.0734e+02 - logprior: 9.5128
Epoch 8/10
10/10 - 4s - loss: 297.2825 - loglik: -3.0757e+02 - logprior: 10.2882
Epoch 9/10
10/10 - 4s - loss: 296.4283 - loglik: -3.0730e+02 - logprior: 10.8668
Epoch 10/10
10/10 - 4s - loss: 296.0360 - loglik: -3.0735e+02 - logprior: 11.3187
Fitted a model with MAP estimate = -295.5551
Time for alignment: 91.1617
Computed alignments with likelihoods: ['-287.6993', '-283.1481', '-295.5551']
Best model has likelihood: -283.1481
SP score = 0.7855
Training of 3 independent models on file glob.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c238e4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c1a5ab50>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 292.6338 - loglik: -2.8272e+02 - logprior: -9.9186e+00
Epoch 2/10
12/12 - 1s - loss: 253.9295 - loglik: -2.5153e+02 - logprior: -2.4033e+00
Epoch 3/10
12/12 - 1s - loss: 227.2991 - loglik: -2.2561e+02 - logprior: -1.6930e+00
Epoch 4/10
12/12 - 1s - loss: 215.9708 - loglik: -2.1427e+02 - logprior: -1.7053e+00
Epoch 5/10
12/12 - 1s - loss: 212.1890 - loglik: -2.1047e+02 - logprior: -1.7204e+00
Epoch 6/10
12/12 - 1s - loss: 209.1711 - loglik: -2.0751e+02 - logprior: -1.6593e+00
Epoch 7/10
12/12 - 1s - loss: 208.4721 - loglik: -2.0686e+02 - logprior: -1.6167e+00
Epoch 8/10
12/12 - 1s - loss: 208.4354 - loglik: -2.0680e+02 - logprior: -1.6331e+00
Epoch 9/10
12/12 - 1s - loss: 206.6937 - loglik: -2.0505e+02 - logprior: -1.6418e+00
Epoch 10/10
12/12 - 1s - loss: 207.1774 - loglik: -2.0555e+02 - logprior: -1.6272e+00
Fitted a model with MAP estimate = -207.0288
expansions: [(6, 3), (10, 3), (13, 1), (21, 1), (29, 1), (36, 4), (49, 1), (50, 3), (52, 1), (58, 3), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 212.9956 - loglik: -2.0154e+02 - logprior: -1.1454e+01
Epoch 2/2
12/12 - 2s - loss: 192.8954 - loglik: -1.8791e+02 - logprior: -4.9866e+00
Fitted a model with MAP estimate = -190.5681
expansions: [(0, 2), (46, 1)]
discards: [ 0 79]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 193.3034 - loglik: -1.8421e+02 - logprior: -9.0894e+00
Epoch 2/2
12/12 - 2s - loss: 184.6334 - loglik: -1.8213e+02 - logprior: -2.5047e+00
Fitted a model with MAP estimate = -183.5051
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 195.7839 - loglik: -1.8447e+02 - logprior: -1.1316e+01
Epoch 2/10
12/12 - 2s - loss: 186.9520 - loglik: -1.8321e+02 - logprior: -3.7449e+00
Epoch 3/10
12/12 - 2s - loss: 184.0188 - loglik: -1.8201e+02 - logprior: -2.0131e+00
Epoch 4/10
12/12 - 2s - loss: 183.8445 - loglik: -1.8253e+02 - logprior: -1.3127e+00
Epoch 5/10
12/12 - 2s - loss: 183.1527 - loglik: -1.8200e+02 - logprior: -1.1533e+00
Epoch 6/10
12/12 - 2s - loss: 183.0111 - loglik: -1.8200e+02 - logprior: -1.0161e+00
Epoch 7/10
12/12 - 2s - loss: 183.6315 - loglik: -1.8266e+02 - logprior: -9.6816e-01
Fitted a model with MAP estimate = -183.0983
Time for alignment: 54.8149
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 292.1893 - loglik: -2.8227e+02 - logprior: -9.9174e+00
Epoch 2/10
12/12 - 1s - loss: 254.9771 - loglik: -2.5257e+02 - logprior: -2.4119e+00
Epoch 3/10
12/12 - 1s - loss: 226.7630 - loglik: -2.2504e+02 - logprior: -1.7211e+00
Epoch 4/10
12/12 - 1s - loss: 216.4061 - loglik: -2.1469e+02 - logprior: -1.7124e+00
Epoch 5/10
12/12 - 1s - loss: 211.7691 - loglik: -2.1007e+02 - logprior: -1.6988e+00
Epoch 6/10
12/12 - 1s - loss: 209.5606 - loglik: -2.0792e+02 - logprior: -1.6432e+00
Epoch 7/10
12/12 - 1s - loss: 208.7379 - loglik: -2.0714e+02 - logprior: -1.6009e+00
Epoch 8/10
12/12 - 1s - loss: 207.5229 - loglik: -2.0592e+02 - logprior: -1.6070e+00
Epoch 9/10
12/12 - 1s - loss: 207.5175 - loglik: -2.0591e+02 - logprior: -1.6069e+00
Epoch 10/10
12/12 - 1s - loss: 206.7919 - loglik: -2.0520e+02 - logprior: -1.5953e+00
Fitted a model with MAP estimate = -207.0650
expansions: [(8, 1), (10, 5), (11, 1), (20, 1), (29, 1), (36, 4), (49, 1), (50, 3), (52, 1), (58, 3), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 213.1096 - loglik: -2.0164e+02 - logprior: -1.1473e+01
Epoch 2/2
12/12 - 2s - loss: 192.8953 - loglik: -1.8775e+02 - logprior: -5.1503e+00
Fitted a model with MAP estimate = -189.7638
expansions: [(0, 2), (46, 1)]
discards: [ 0 79]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 193.1193 - loglik: -1.8403e+02 - logprior: -9.0883e+00
Epoch 2/2
12/12 - 2s - loss: 183.6169 - loglik: -1.8110e+02 - logprior: -2.5162e+00
Fitted a model with MAP estimate = -183.1363
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 195.1074 - loglik: -1.8380e+02 - logprior: -1.1310e+01
Epoch 2/10
12/12 - 2s - loss: 186.9040 - loglik: -1.8315e+02 - logprior: -3.7573e+00
Epoch 3/10
12/12 - 2s - loss: 184.3157 - loglik: -1.8234e+02 - logprior: -1.9776e+00
Epoch 4/10
12/12 - 2s - loss: 182.4445 - loglik: -1.8115e+02 - logprior: -1.2914e+00
Epoch 5/10
12/12 - 2s - loss: 183.3965 - loglik: -1.8224e+02 - logprior: -1.1614e+00
Fitted a model with MAP estimate = -182.9844
Time for alignment: 51.2137
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 292.4662 - loglik: -2.8255e+02 - logprior: -9.9195e+00
Epoch 2/10
12/12 - 1s - loss: 252.9447 - loglik: -2.5054e+02 - logprior: -2.4053e+00
Epoch 3/10
12/12 - 1s - loss: 224.9702 - loglik: -2.2328e+02 - logprior: -1.6907e+00
Epoch 4/10
12/12 - 1s - loss: 215.7684 - loglik: -2.1407e+02 - logprior: -1.6964e+00
Epoch 5/10
12/12 - 1s - loss: 211.2438 - loglik: -2.0955e+02 - logprior: -1.6941e+00
Epoch 6/10
12/12 - 1s - loss: 209.5295 - loglik: -2.0790e+02 - logprior: -1.6251e+00
Epoch 7/10
12/12 - 1s - loss: 207.4227 - loglik: -2.0583e+02 - logprior: -1.5960e+00
Epoch 8/10
12/12 - 1s - loss: 207.3723 - loglik: -2.0575e+02 - logprior: -1.6201e+00
Epoch 9/10
12/12 - 1s - loss: 206.6936 - loglik: -2.0506e+02 - logprior: -1.6367e+00
Epoch 10/10
12/12 - 1s - loss: 205.8134 - loglik: -2.0418e+02 - logprior: -1.6310e+00
Fitted a model with MAP estimate = -206.1807
expansions: [(8, 1), (9, 1), (10, 5), (11, 2), (21, 1), (29, 1), (36, 3), (49, 4), (58, 2), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 213.2447 - loglik: -2.0177e+02 - logprior: -1.1478e+01
Epoch 2/2
12/12 - 2s - loss: 194.0199 - loglik: -1.8883e+02 - logprior: -5.1927e+00
Fitted a model with MAP estimate = -190.5998
expansions: [(0, 2), (65, 1)]
discards: [ 0 12 13 48 76 79]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 195.9520 - loglik: -1.8701e+02 - logprior: -8.9440e+00
Epoch 2/2
12/12 - 2s - loss: 186.7747 - loglik: -1.8443e+02 - logprior: -2.3448e+00
Fitted a model with MAP estimate = -185.9350
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 197.7471 - loglik: -1.8662e+02 - logprior: -1.1126e+01
Epoch 2/10
12/12 - 2s - loss: 189.3044 - loglik: -1.8575e+02 - logprior: -3.5497e+00
Epoch 3/10
12/12 - 2s - loss: 186.6228 - loglik: -1.8482e+02 - logprior: -1.8041e+00
Epoch 4/10
12/12 - 2s - loss: 185.8232 - loglik: -1.8469e+02 - logprior: -1.1361e+00
Epoch 5/10
12/12 - 2s - loss: 185.8311 - loglik: -1.8485e+02 - logprior: -9.8571e-01
Fitted a model with MAP estimate = -185.4970
Time for alignment: 48.6791
Computed alignments with likelihoods: ['-183.0983', '-182.9844', '-185.4970']
Best model has likelihood: -182.9844
SP score = 0.8711
Training of 3 independent models on file az.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c278c1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9f7b31790>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 329.2031 - loglik: -2.8897e+02 - logprior: -4.0234e+01
Epoch 2/10
10/10 - 1s - loss: 277.0018 - loglik: -2.6692e+02 - logprior: -1.0078e+01
Epoch 3/10
10/10 - 1s - loss: 250.9950 - loglik: -2.4678e+02 - logprior: -4.2114e+00
Epoch 4/10
10/10 - 1s - loss: 238.1104 - loglik: -2.3602e+02 - logprior: -2.0952e+00
Epoch 5/10
10/10 - 1s - loss: 232.8237 - loglik: -2.3169e+02 - logprior: -1.1382e+00
Epoch 6/10
10/10 - 1s - loss: 229.6797 - loglik: -2.2897e+02 - logprior: -7.0955e-01
Epoch 7/10
10/10 - 1s - loss: 228.3505 - loglik: -2.2785e+02 - logprior: -4.9616e-01
Epoch 8/10
10/10 - 1s - loss: 226.9832 - loglik: -2.2661e+02 - logprior: -3.6923e-01
Epoch 9/10
10/10 - 1s - loss: 226.5583 - loglik: -2.2623e+02 - logprior: -3.2811e-01
Epoch 10/10
10/10 - 1s - loss: 225.6817 - loglik: -2.2538e+02 - logprior: -2.9974e-01
Fitted a model with MAP estimate = -225.5711
expansions: [(0, 3), (6, 2), (7, 1), (8, 1), (36, 1), (37, 2), (43, 3), (49, 1), (53, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 273.4825 - loglik: -2.2115e+02 - logprior: -5.2328e+01
Epoch 2/2
10/10 - 1s - loss: 232.8020 - loglik: -2.1744e+02 - logprior: -1.5361e+01
Fitted a model with MAP estimate = -225.2894
expansions: []
discards: [ 0  1  9 68]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 89 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 263.0224 - loglik: -2.1694e+02 - logprior: -4.6083e+01
Epoch 2/2
10/10 - 1s - loss: 234.3869 - loglik: -2.1673e+02 - logprior: -1.7653e+01
Fitted a model with MAP estimate = -230.2716
expansions: [(0, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 256.7235 - loglik: -2.1600e+02 - logprior: -4.0719e+01
Epoch 2/10
10/10 - 1s - loss: 225.3543 - loglik: -2.1519e+02 - logprior: -1.0164e+01
Epoch 3/10
10/10 - 1s - loss: 219.3524 - loglik: -2.1597e+02 - logprior: -3.3830e+00
Epoch 4/10
10/10 - 1s - loss: 217.2834 - loglik: -2.1643e+02 - logprior: -8.5687e-01
Epoch 5/10
10/10 - 1s - loss: 216.1678 - loglik: -2.1661e+02 - logprior: 0.4379
Epoch 6/10
10/10 - 1s - loss: 215.7688 - loglik: -2.1691e+02 - logprior: 1.1452
Epoch 7/10
10/10 - 1s - loss: 215.3725 - loglik: -2.1693e+02 - logprior: 1.5577
Epoch 8/10
10/10 - 1s - loss: 215.2418 - loglik: -2.1706e+02 - logprior: 1.8182
Epoch 9/10
10/10 - 1s - loss: 214.7881 - loglik: -2.1680e+02 - logprior: 2.0088
Epoch 10/10
10/10 - 1s - loss: 214.8152 - loglik: -2.1699e+02 - logprior: 2.1719
Fitted a model with MAP estimate = -214.6373
Time for alignment: 45.4747
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 328.8386 - loglik: -2.8861e+02 - logprior: -4.0232e+01
Epoch 2/10
10/10 - 1s - loss: 277.0634 - loglik: -2.6700e+02 - logprior: -1.0068e+01
Epoch 3/10
10/10 - 1s - loss: 250.6791 - loglik: -2.4648e+02 - logprior: -4.1955e+00
Epoch 4/10
10/10 - 1s - loss: 237.1189 - loglik: -2.3503e+02 - logprior: -2.0874e+00
Epoch 5/10
10/10 - 1s - loss: 231.4049 - loglik: -2.3032e+02 - logprior: -1.0832e+00
Epoch 6/10
10/10 - 1s - loss: 228.9042 - loglik: -2.2832e+02 - logprior: -5.8617e-01
Epoch 7/10
10/10 - 1s - loss: 227.0311 - loglik: -2.2664e+02 - logprior: -3.8771e-01
Epoch 8/10
10/10 - 1s - loss: 226.0932 - loglik: -2.2580e+02 - logprior: -2.9564e-01
Epoch 9/10
10/10 - 1s - loss: 225.4093 - loglik: -2.2522e+02 - logprior: -1.9018e-01
Epoch 10/10
10/10 - 1s - loss: 225.2455 - loglik: -2.2520e+02 - logprior: -4.8404e-02
Fitted a model with MAP estimate = -224.9140
expansions: [(0, 3), (5, 1), (8, 1), (36, 1), (37, 2), (43, 5), (53, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 273.3337 - loglik: -2.2095e+02 - logprior: -5.2384e+01
Epoch 2/2
10/10 - 1s - loss: 233.1669 - loglik: -2.1775e+02 - logprior: -1.5417e+01
Fitted a model with MAP estimate = -225.8399
expansions: []
discards: [ 0  1 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 89 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 262.6789 - loglik: -2.1657e+02 - logprior: -4.6108e+01
Epoch 2/2
10/10 - 1s - loss: 235.3522 - loglik: -2.1765e+02 - logprior: -1.7701e+01
Fitted a model with MAP estimate = -230.3723
expansions: [(0, 3), (5, 1)]
discards: [ 0 52 53]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 256.9450 - loglik: -2.1639e+02 - logprior: -4.0554e+01
Epoch 2/10
10/10 - 1s - loss: 225.8267 - loglik: -2.1577e+02 - logprior: -1.0059e+01
Epoch 3/10
10/10 - 1s - loss: 219.6309 - loglik: -2.1630e+02 - logprior: -3.3304e+00
Epoch 4/10
10/10 - 1s - loss: 217.4122 - loglik: -2.1659e+02 - logprior: -8.1849e-01
Epoch 5/10
10/10 - 1s - loss: 216.5296 - loglik: -2.1698e+02 - logprior: 0.4545
Epoch 6/10
10/10 - 1s - loss: 215.9232 - loglik: -2.1708e+02 - logprior: 1.1547
Epoch 7/10
10/10 - 1s - loss: 215.1555 - loglik: -2.1671e+02 - logprior: 1.5549
Epoch 8/10
10/10 - 1s - loss: 215.1350 - loglik: -2.1694e+02 - logprior: 1.8033
Epoch 9/10
10/10 - 1s - loss: 214.7863 - loglik: -2.1677e+02 - logprior: 1.9865
Epoch 10/10
10/10 - 1s - loss: 214.9792 - loglik: -2.1713e+02 - logprior: 2.1503
Fitted a model with MAP estimate = -214.6895
Time for alignment: 44.5501
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 329.2299 - loglik: -2.8899e+02 - logprior: -4.0235e+01
Epoch 2/10
10/10 - 1s - loss: 276.1420 - loglik: -2.6607e+02 - logprior: -1.0070e+01
Epoch 3/10
10/10 - 1s - loss: 249.4465 - loglik: -2.4528e+02 - logprior: -4.1705e+00
Epoch 4/10
10/10 - 1s - loss: 237.6839 - loglik: -2.3565e+02 - logprior: -2.0304e+00
Epoch 5/10
10/10 - 1s - loss: 231.5556 - loglik: -2.3051e+02 - logprior: -1.0410e+00
Epoch 6/10
10/10 - 1s - loss: 229.0348 - loglik: -2.2847e+02 - logprior: -5.6597e-01
Epoch 7/10
10/10 - 1s - loss: 228.0281 - loglik: -2.2761e+02 - logprior: -4.1857e-01
Epoch 8/10
10/10 - 1s - loss: 226.7898 - loglik: -2.2648e+02 - logprior: -3.0708e-01
Epoch 9/10
10/10 - 1s - loss: 226.2569 - loglik: -2.2602e+02 - logprior: -2.3257e-01
Epoch 10/10
10/10 - 1s - loss: 225.2373 - loglik: -2.2512e+02 - logprior: -1.1822e-01
Fitted a model with MAP estimate = -225.3076
expansions: [(0, 3), (5, 2), (7, 1), (8, 1), (36, 1), (37, 2), (43, 5), (53, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 94 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 272.8928 - loglik: -2.2071e+02 - logprior: -5.2185e+01
Epoch 2/2
10/10 - 1s - loss: 232.5389 - loglik: -2.1720e+02 - logprior: -1.5339e+01
Fitted a model with MAP estimate = -225.1223
expansions: []
discards: [ 0  1 53 69]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 262.6000 - loglik: -2.1668e+02 - logprior: -4.5922e+01
Epoch 2/2
10/10 - 1s - loss: 235.0275 - loglik: -2.1741e+02 - logprior: -1.7613e+01
Fitted a model with MAP estimate = -230.2334
expansions: [(0, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 256.2003 - loglik: -2.1548e+02 - logprior: -4.0721e+01
Epoch 2/10
10/10 - 1s - loss: 226.0912 - loglik: -2.1592e+02 - logprior: -1.0169e+01
Epoch 3/10
10/10 - 1s - loss: 219.3044 - loglik: -2.1597e+02 - logprior: -3.3363e+00
Epoch 4/10
10/10 - 1s - loss: 217.2018 - loglik: -2.1636e+02 - logprior: -8.4015e-01
Epoch 5/10
10/10 - 1s - loss: 216.2091 - loglik: -2.1667e+02 - logprior: 0.4651
Epoch 6/10
10/10 - 1s - loss: 215.9471 - loglik: -2.1711e+02 - logprior: 1.1601
Epoch 7/10
10/10 - 1s - loss: 215.0742 - loglik: -2.1662e+02 - logprior: 1.5462
Epoch 8/10
10/10 - 1s - loss: 214.5860 - loglik: -2.1638e+02 - logprior: 1.7919
Epoch 9/10
10/10 - 1s - loss: 214.9640 - loglik: -2.1696e+02 - logprior: 1.9962
Fitted a model with MAP estimate = -214.5849
Time for alignment: 43.4271
Computed alignments with likelihoods: ['-214.6373', '-214.6895', '-214.5849']
Best model has likelihood: -214.5849
SP score = 0.7434
Training of 3 independent models on file ghf11.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fba10e62b20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbaf0ab4be0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 551.5311 - loglik: -4.7376e+02 - logprior: -7.7774e+01
Epoch 2/10
10/10 - 3s - loss: 418.9968 - loglik: -4.0377e+02 - logprior: -1.5223e+01
Epoch 3/10
10/10 - 3s - loss: 341.6475 - loglik: -3.3629e+02 - logprior: -5.3577e+00
Epoch 4/10
10/10 - 3s - loss: 295.0729 - loglik: -2.9170e+02 - logprior: -3.3765e+00
Epoch 5/10
10/10 - 3s - loss: 276.5818 - loglik: -2.7388e+02 - logprior: -2.7033e+00
Epoch 6/10
10/10 - 3s - loss: 269.1339 - loglik: -2.6754e+02 - logprior: -1.5957e+00
Epoch 7/10
10/10 - 4s - loss: 265.6832 - loglik: -2.6500e+02 - logprior: -6.8064e-01
Epoch 8/10
10/10 - 4s - loss: 263.9988 - loglik: -2.6384e+02 - logprior: -1.5908e-01
Epoch 9/10
10/10 - 3s - loss: 262.7328 - loglik: -2.6300e+02 - logprior: 0.2658
Epoch 10/10
10/10 - 4s - loss: 262.1931 - loglik: -2.6268e+02 - logprior: 0.4833
Fitted a model with MAP estimate = -261.2534
expansions: [(18, 1), (20, 1), (22, 1), (26, 1), (39, 2), (40, 3), (41, 2), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (122, 1), (124, 1), (125, 1), (127, 1), (129, 3), (133, 1), (141, 1), (142, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 180 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 341.7250 - loglik: -2.5339e+02 - logprior: -8.8336e+01
Epoch 2/2
10/10 - 5s - loss: 267.2645 - loglik: -2.3537e+02 - logprior: -3.1899e+01
Fitted a model with MAP estimate = -255.8207
expansions: [(0, 3), (15, 4), (89, 1)]
discards: [  0  43  45  46  49  56 121]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 296.8344 - loglik: -2.2762e+02 - logprior: -6.9219e+01
Epoch 2/2
10/10 - 5s - loss: 234.2147 - loglik: -2.2246e+02 - logprior: -1.1757e+01
Fitted a model with MAP estimate = -224.5991
expansions: [(16, 1), (22, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 306.2723 - loglik: -2.2145e+02 - logprior: -8.4827e+01
Epoch 2/10
10/10 - 5s - loss: 246.8078 - loglik: -2.2113e+02 - logprior: -2.5675e+01
Epoch 3/10
10/10 - 5s - loss: 224.2960 - loglik: -2.1962e+02 - logprior: -4.6802e+00
Epoch 4/10
10/10 - 5s - loss: 213.8849 - loglik: -2.1999e+02 - logprior: 6.1009
Epoch 5/10
10/10 - 5s - loss: 210.3565 - loglik: -2.2030e+02 - logprior: 9.9446
Epoch 6/10
10/10 - 5s - loss: 208.1076 - loglik: -2.2003e+02 - logprior: 11.9257
Epoch 7/10
10/10 - 5s - loss: 207.2350 - loglik: -2.2044e+02 - logprior: 13.2012
Epoch 8/10
10/10 - 6s - loss: 206.1477 - loglik: -2.2027e+02 - logprior: 14.1261
Epoch 9/10
10/10 - 5s - loss: 205.7017 - loglik: -2.2061e+02 - logprior: 14.9054
Epoch 10/10
10/10 - 5s - loss: 205.0698 - loglik: -2.2068e+02 - logprior: 15.6136
Fitted a model with MAP estimate = -204.6677
Time for alignment: 121.8480
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 551.3324 - loglik: -4.7356e+02 - logprior: -7.7770e+01
Epoch 2/10
10/10 - 4s - loss: 419.3026 - loglik: -4.0408e+02 - logprior: -1.5221e+01
Epoch 3/10
10/10 - 3s - loss: 341.7466 - loglik: -3.3648e+02 - logprior: -5.2711e+00
Epoch 4/10
10/10 - 3s - loss: 294.0139 - loglik: -2.9072e+02 - logprior: -3.2914e+00
Epoch 5/10
10/10 - 3s - loss: 276.8475 - loglik: -2.7432e+02 - logprior: -2.5247e+00
Epoch 6/10
10/10 - 3s - loss: 269.6290 - loglik: -2.6827e+02 - logprior: -1.3543e+00
Epoch 7/10
10/10 - 4s - loss: 266.1151 - loglik: -2.6559e+02 - logprior: -5.2350e-01
Epoch 8/10
10/10 - 4s - loss: 264.3810 - loglik: -2.6433e+02 - logprior: -5.5052e-02
Epoch 9/10
10/10 - 4s - loss: 262.7531 - loglik: -2.6310e+02 - logprior: 0.3440
Epoch 10/10
10/10 - 4s - loss: 262.6500 - loglik: -2.6343e+02 - logprior: 0.7758
Fitted a model with MAP estimate = -262.1303
expansions: [(18, 1), (20, 1), (22, 1), (26, 1), (39, 4), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (128, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 177 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 341.3831 - loglik: -2.5297e+02 - logprior: -8.8416e+01
Epoch 2/2
10/10 - 5s - loss: 266.7952 - loglik: -2.3516e+02 - logprior: -3.1633e+01
Fitted a model with MAP estimate = -255.6638
expansions: [(0, 3), (15, 4), (86, 1)]
discards: [  0  43  53 118]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 296.9763 - loglik: -2.2770e+02 - logprior: -6.9279e+01
Epoch 2/2
10/10 - 5s - loss: 233.9394 - loglik: -2.2217e+02 - logprior: -1.1771e+01
Fitted a model with MAP estimate = -224.4002
expansions: [(16, 1), (22, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 306.3957 - loglik: -2.2158e+02 - logprior: -8.4816e+01
Epoch 2/10
10/10 - 5s - loss: 245.6321 - loglik: -2.2009e+02 - logprior: -2.5544e+01
Epoch 3/10
10/10 - 5s - loss: 224.2243 - loglik: -2.1972e+02 - logprior: -4.5069e+00
Epoch 4/10
10/10 - 5s - loss: 213.9778 - loglik: -2.2015e+02 - logprior: 6.1700
Epoch 5/10
10/10 - 5s - loss: 210.0317 - loglik: -2.2003e+02 - logprior: 9.9977
Epoch 6/10
10/10 - 5s - loss: 208.0885 - loglik: -2.2007e+02 - logprior: 11.9844
Epoch 7/10
10/10 - 5s - loss: 207.2923 - loglik: -2.2055e+02 - logprior: 13.2555
Epoch 8/10
10/10 - 5s - loss: 205.9534 - loglik: -2.2014e+02 - logprior: 14.1879
Epoch 9/10
10/10 - 5s - loss: 205.4982 - loglik: -2.2047e+02 - logprior: 14.9717
Epoch 10/10
10/10 - 5s - loss: 204.7083 - loglik: -2.2039e+02 - logprior: 15.6857
Fitted a model with MAP estimate = -204.4752
Time for alignment: 120.4934
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 551.4033 - loglik: -4.7363e+02 - logprior: -7.7773e+01
Epoch 2/10
10/10 - 4s - loss: 418.9869 - loglik: -4.0377e+02 - logprior: -1.5221e+01
Epoch 3/10
10/10 - 4s - loss: 341.1703 - loglik: -3.3580e+02 - logprior: -5.3720e+00
Epoch 4/10
10/10 - 4s - loss: 294.7945 - loglik: -2.9149e+02 - logprior: -3.2998e+00
Epoch 5/10
10/10 - 3s - loss: 276.4684 - loglik: -2.7379e+02 - logprior: -2.6751e+00
Epoch 6/10
10/10 - 4s - loss: 268.4788 - loglik: -2.6649e+02 - logprior: -1.9906e+00
Epoch 7/10
10/10 - 4s - loss: 265.2704 - loglik: -2.6428e+02 - logprior: -9.8953e-01
Epoch 8/10
10/10 - 3s - loss: 263.2733 - loglik: -2.6290e+02 - logprior: -3.6928e-01
Epoch 9/10
10/10 - 4s - loss: 261.9220 - loglik: -2.6174e+02 - logprior: -1.8684e-01
Epoch 10/10
10/10 - 4s - loss: 261.6415 - loglik: -2.6182e+02 - logprior: 0.1785
Fitted a model with MAP estimate = -261.1506
expansions: [(18, 1), (20, 1), (22, 1), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 2), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (122, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 341.4561 - loglik: -2.5295e+02 - logprior: -8.8506e+01
Epoch 2/2
10/10 - 5s - loss: 267.4503 - loglik: -2.3559e+02 - logprior: -3.1856e+01
Fitted a model with MAP estimate = -256.3690
expansions: [(0, 3), (15, 4), (86, 1)]
discards: [  0  43  53 106 119]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 297.2724 - loglik: -2.2792e+02 - logprior: -6.9349e+01
Epoch 2/2
10/10 - 5s - loss: 234.4135 - loglik: -2.2264e+02 - logprior: -1.1770e+01
Fitted a model with MAP estimate = -224.5782
expansions: [(16, 1), (22, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 306.4773 - loglik: -2.2166e+02 - logprior: -8.4815e+01
Epoch 2/10
10/10 - 5s - loss: 245.8858 - loglik: -2.2027e+02 - logprior: -2.5614e+01
Epoch 3/10
10/10 - 5s - loss: 224.3525 - loglik: -2.1978e+02 - logprior: -4.5764e+00
Epoch 4/10
10/10 - 5s - loss: 214.0235 - loglik: -2.2015e+02 - logprior: 6.1298
Epoch 5/10
10/10 - 5s - loss: 209.4894 - loglik: -2.1945e+02 - logprior: 9.9616
Epoch 6/10
10/10 - 5s - loss: 208.4138 - loglik: -2.2035e+02 - logprior: 11.9384
Epoch 7/10
10/10 - 5s - loss: 207.0409 - loglik: -2.2025e+02 - logprior: 13.2103
Epoch 8/10
10/10 - 5s - loss: 206.0053 - loglik: -2.2014e+02 - logprior: 14.1368
Epoch 9/10
10/10 - 5s - loss: 206.0896 - loglik: -2.2100e+02 - logprior: 14.9119
Fitted a model with MAP estimate = -205.1437
Time for alignment: 117.2172
Computed alignments with likelihoods: ['-204.6677', '-204.4752', '-205.1437']
Best model has likelihood: -204.4752
SP score = 0.9114
Training of 3 independent models on file subt.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fba10d3d070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb80c1fdee0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 36s - loss: 813.1340 - loglik: -8.1113e+02 - logprior: -2.0056e+00
Epoch 2/10
33/33 - 34s - loss: 720.9346 - loglik: -7.2028e+02 - logprior: -6.5688e-01
Epoch 3/10
33/33 - 35s - loss: 709.9199 - loglik: -7.0932e+02 - logprior: -5.9974e-01
Epoch 4/10
33/33 - 34s - loss: 702.8517 - loglik: -7.0230e+02 - logprior: -5.5248e-01
Epoch 5/10
33/33 - 34s - loss: 712.7047 - loglik: -7.1217e+02 - logprior: -5.3467e-01
Fitted a model with MAP estimate = -705.9237
expansions: [(0, 5), (9, 1), (10, 1), (34, 4), (35, 1), (44, 2), (63, 1), (65, 1), (71, 1), (73, 1), (78, 1), (83, 1), (113, 1), (116, 1), (118, 1), (132, 2), (154, 4), (162, 4), (176, 1), (203, 1), (220, 2), (221, 4), (230, 3)]
discards: [225 226 227 228]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 270 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 47s - loss: 703.6694 - loglik: -7.0077e+02 - logprior: -2.8990e+00
Epoch 2/2
33/33 - 42s - loss: 696.3601 - loglik: -6.9585e+02 - logprior: -5.1146e-01
Fitted a model with MAP estimate = -694.4985
expansions: []
discards: [  1  42  56 180 191 192 193 194 256 265 266 267 268]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 257 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 40s - loss: 698.6526 - loglik: -6.9662e+02 - logprior: -2.0280e+00
Epoch 2/2
33/33 - 39s - loss: 699.6260 - loglik: -6.9946e+02 - logprior: -1.6993e-01
Fitted a model with MAP estimate = -696.1329
expansions: [(187, 4), (257, 4)]
discards: [  5 253 254]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 262 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 41s - loss: 697.0854 - loglik: -6.9523e+02 - logprior: -1.8512e+00
Epoch 2/10
33/33 - 42s - loss: 696.3546 - loglik: -6.9639e+02 - logprior: 0.0354
Epoch 3/10
33/33 - 44s - loss: 695.4072 - loglik: -6.9574e+02 - logprior: 0.3294
Epoch 4/10
33/33 - 42s - loss: 697.5737 - loglik: -6.9804e+02 - logprior: 0.4681
Fitted a model with MAP estimate = -694.1158
Time for alignment: 642.7039
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 37s - loss: 812.8724 - loglik: -8.1086e+02 - logprior: -2.0124e+00
Epoch 2/10
33/33 - 35s - loss: 719.2935 - loglik: -7.1867e+02 - logprior: -6.2052e-01
Epoch 3/10
33/33 - 35s - loss: 711.8029 - loglik: -7.1127e+02 - logprior: -5.3131e-01
Epoch 4/10
33/33 - 34s - loss: 711.1444 - loglik: -7.1064e+02 - logprior: -5.0369e-01
Epoch 5/10
33/33 - 32s - loss: 707.1943 - loglik: -7.0671e+02 - logprior: -4.8441e-01
Epoch 6/10
33/33 - 32s - loss: 709.8257 - loglik: -7.0934e+02 - logprior: -4.8084e-01
Fitted a model with MAP estimate = -707.3894
expansions: [(0, 5), (7, 1), (9, 2), (10, 1), (34, 4), (42, 1), (44, 2), (61, 1), (62, 2), (73, 3), (77, 1), (110, 1), (112, 1), (115, 1), (117, 1), (133, 2), (154, 3), (162, 5), (180, 2), (203, 1), (220, 2), (221, 3), (230, 3)]
discards: [  3   4 225 226 227 228]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 272 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 43s - loss: 704.6744 - loglik: -7.0176e+02 - logprior: -2.9182e+00
Epoch 2/2
33/33 - 42s - loss: 695.5314 - loglik: -6.9508e+02 - logprior: -4.5404e-01
Fitted a model with MAP estimate = -694.5208
expansions: [(272, 4)]
discards: [  1   2   3   4  42  56  75 182 192 193 194 195 196 216 259 262 267 268
 269 270 271]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 255 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 41s - loss: 700.7520 - loglik: -6.9873e+02 - logprior: -2.0202e+00
Epoch 2/2
33/33 - 42s - loss: 696.9033 - loglik: -6.9688e+02 - logprior: -2.2291e-02
Fitted a model with MAP estimate = -695.9752
expansions: [(0, 4), (1, 1), (249, 2), (255, 3)]
discards: [251 252]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 263 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 45s - loss: 700.0231 - loglik: -6.9709e+02 - logprior: -2.9367e+00
Epoch 2/10
33/33 - 43s - loss: 697.0435 - loglik: -6.9707e+02 - logprior: 0.0300
Epoch 3/10
33/33 - 41s - loss: 693.1160 - loglik: -6.9334e+02 - logprior: 0.2288
Epoch 4/10
33/33 - 40s - loss: 696.2804 - loglik: -6.9663e+02 - logprior: 0.3525
Fitted a model with MAP estimate = -694.6698
Time for alignment: 680.6945
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 33s - loss: 812.9268 - loglik: -8.1091e+02 - logprior: -2.0196e+00
Epoch 2/10
33/33 - 32s - loss: 720.2034 - loglik: -7.1939e+02 - logprior: -8.1236e-01
Epoch 3/10
33/33 - 34s - loss: 713.8812 - loglik: -7.1311e+02 - logprior: -7.7391e-01
Epoch 4/10
33/33 - 34s - loss: 705.8192 - loglik: -7.0510e+02 - logprior: -7.2017e-01
Epoch 5/10
33/33 - 34s - loss: 706.6404 - loglik: -7.0595e+02 - logprior: -6.9299e-01
Fitted a model with MAP estimate = -706.2526
expansions: [(0, 5), (7, 1), (9, 2), (10, 1), (34, 4), (64, 1), (71, 1), (72, 1), (73, 1), (79, 2), (88, 1), (113, 1), (116, 1), (118, 1), (134, 2), (155, 3), (163, 4), (183, 1), (210, 1), (216, 1), (221, 3), (230, 4)]
discards: [  2   3  62 225 226 227 228]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 265 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 43s - loss: 703.1339 - loglik: -7.0024e+02 - logprior: -2.8950e+00
Epoch 2/2
33/33 - 40s - loss: 695.3535 - loglik: -6.9483e+02 - logprior: -5.2283e-01
Fitted a model with MAP estimate = -695.3178
expansions: []
discards: [  1  42 178 189 190 191 259 260 261 262 263 264]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 253 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 41s - loss: 701.4075 - loglik: -6.9943e+02 - logprior: -1.9742e+00
Epoch 2/2
33/33 - 39s - loss: 695.4318 - loglik: -6.9531e+02 - logprior: -1.2197e-01
Fitted a model with MAP estimate = -696.8476
expansions: [(249, 2), (253, 4)]
discards: [  3   4   5   6 250]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 254 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 40s - loss: 700.2315 - loglik: -6.9842e+02 - logprior: -1.8079e+00
Epoch 2/10
33/33 - 37s - loss: 698.0637 - loglik: -6.9819e+02 - logprior: 0.1247
Epoch 3/10
33/33 - 38s - loss: 700.5422 - loglik: -7.0092e+02 - logprior: 0.3740
Fitted a model with MAP estimate = -696.6074
Time for alignment: 574.7143
Computed alignments with likelihoods: ['-694.1158', '-694.5208', '-695.3178']
Best model has likelihood: -694.1158
SP score = 0.7715
Training of 3 independent models on file profilin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cbaec280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9eeba9d30>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 410.2409 - loglik: -3.4833e+02 - logprior: -6.1916e+01
Epoch 2/10
10/10 - 2s - loss: 321.5502 - loglik: -3.0725e+02 - logprior: -1.4300e+01
Epoch 3/10
10/10 - 2s - loss: 273.9267 - loglik: -2.6775e+02 - logprior: -6.1795e+00
Epoch 4/10
10/10 - 2s - loss: 247.8571 - loglik: -2.4405e+02 - logprior: -3.8062e+00
Epoch 5/10
10/10 - 2s - loss: 238.9220 - loglik: -2.3634e+02 - logprior: -2.5805e+00
Epoch 6/10
10/10 - 2s - loss: 235.2009 - loglik: -2.3337e+02 - logprior: -1.8296e+00
Epoch 7/10
10/10 - 2s - loss: 233.2133 - loglik: -2.3197e+02 - logprior: -1.2444e+00
Epoch 8/10
10/10 - 2s - loss: 232.1952 - loglik: -2.3133e+02 - logprior: -8.6270e-01
Epoch 9/10
10/10 - 2s - loss: 230.9483 - loglik: -2.3030e+02 - logprior: -6.5030e-01
Epoch 10/10
10/10 - 2s - loss: 230.5027 - loglik: -2.3006e+02 - logprior: -4.4152e-01
Fitted a model with MAP estimate = -230.5713
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 1), (69, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 3), (88, 1), (91, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 291.3082 - loglik: -2.2186e+02 - logprior: -6.9444e+01
Epoch 2/2
10/10 - 2s - loss: 234.4156 - loglik: -2.0806e+02 - logprior: -2.6359e+01
Fitted a model with MAP estimate = -224.5470
expansions: [(0, 2)]
discards: [  0 108]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 254.7065 - loglik: -2.0030e+02 - logprior: -5.4403e+01
Epoch 2/2
10/10 - 2s - loss: 211.3031 - loglik: -1.9967e+02 - logprior: -1.1633e+01
Fitted a model with MAP estimate = -204.6154
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 265.9385 - loglik: -2.0070e+02 - logprior: -6.5240e+01
Epoch 2/10
10/10 - 2s - loss: 217.4246 - loglik: -2.0095e+02 - logprior: -1.6474e+01
Epoch 3/10
10/10 - 2s - loss: 203.6030 - loglik: -2.0020e+02 - logprior: -3.4001e+00
Epoch 4/10
10/10 - 2s - loss: 200.2050 - loglik: -2.0143e+02 - logprior: 1.2263
Epoch 5/10
10/10 - 2s - loss: 198.1869 - loglik: -2.0163e+02 - logprior: 3.4442
Epoch 6/10
10/10 - 2s - loss: 196.6590 - loglik: -2.0132e+02 - logprior: 4.6635
Epoch 7/10
10/10 - 2s - loss: 195.8248 - loglik: -2.0127e+02 - logprior: 5.4430
Epoch 8/10
10/10 - 2s - loss: 195.1078 - loglik: -2.0118e+02 - logprior: 6.0735
Epoch 9/10
10/10 - 2s - loss: 195.0309 - loglik: -2.0166e+02 - logprior: 6.6300
Epoch 10/10
10/10 - 2s - loss: 194.6175 - loglik: -2.0171e+02 - logprior: 7.0970
Fitted a model with MAP estimate = -194.4245
Time for alignment: 61.1652
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 410.7515 - loglik: -3.4884e+02 - logprior: -6.1916e+01
Epoch 2/10
10/10 - 2s - loss: 321.4488 - loglik: -3.0715e+02 - logprior: -1.4294e+01
Epoch 3/10
10/10 - 1s - loss: 274.6094 - loglik: -2.6839e+02 - logprior: -6.2212e+00
Epoch 4/10
10/10 - 1s - loss: 248.1562 - loglik: -2.4409e+02 - logprior: -4.0677e+00
Epoch 5/10
10/10 - 2s - loss: 237.1754 - loglik: -2.3414e+02 - logprior: -3.0312e+00
Epoch 6/10
10/10 - 2s - loss: 234.4967 - loglik: -2.3245e+02 - logprior: -2.0470e+00
Epoch 7/10
10/10 - 1s - loss: 232.4174 - loglik: -2.3106e+02 - logprior: -1.3527e+00
Epoch 8/10
10/10 - 2s - loss: 231.1276 - loglik: -2.3014e+02 - logprior: -9.8657e-01
Epoch 9/10
10/10 - 1s - loss: 230.9204 - loglik: -2.3017e+02 - logprior: -7.5510e-01
Epoch 10/10
10/10 - 1s - loss: 229.9837 - loglik: -2.2945e+02 - logprior: -5.3124e-01
Fitted a model with MAP estimate = -230.0095
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (54, 1), (62, 1), (63, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 3), (88, 1), (91, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 290.5762 - loglik: -2.2112e+02 - logprior: -6.9451e+01
Epoch 2/2
10/10 - 2s - loss: 234.8906 - loglik: -2.0851e+02 - logprior: -2.6377e+01
Fitted a model with MAP estimate = -224.5084
expansions: [(0, 2)]
discards: [  0 108]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 255.2907 - loglik: -2.0087e+02 - logprior: -5.4423e+01
Epoch 2/2
10/10 - 2s - loss: 210.6364 - loglik: -1.9901e+02 - logprior: -1.1626e+01
Fitted a model with MAP estimate = -204.6683
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 265.9572 - loglik: -2.0073e+02 - logprior: -6.5225e+01
Epoch 2/10
10/10 - 2s - loss: 217.6056 - loglik: -2.0114e+02 - logprior: -1.6467e+01
Epoch 3/10
10/10 - 2s - loss: 203.8198 - loglik: -2.0043e+02 - logprior: -3.3876e+00
Epoch 4/10
10/10 - 2s - loss: 199.9447 - loglik: -2.0117e+02 - logprior: 1.2294
Epoch 5/10
10/10 - 2s - loss: 198.0386 - loglik: -2.0150e+02 - logprior: 3.4608
Epoch 6/10
10/10 - 2s - loss: 196.8396 - loglik: -2.0152e+02 - logprior: 4.6789
Epoch 7/10
10/10 - 2s - loss: 195.5654 - loglik: -2.0102e+02 - logprior: 5.4580
Epoch 8/10
10/10 - 2s - loss: 196.0539 - loglik: -2.0214e+02 - logprior: 6.0864
Fitted a model with MAP estimate = -195.2336
Time for alignment: 52.6948
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 410.4679 - loglik: -3.4855e+02 - logprior: -6.1919e+01
Epoch 2/10
10/10 - 1s - loss: 321.4179 - loglik: -3.0711e+02 - logprior: -1.4305e+01
Epoch 3/10
10/10 - 2s - loss: 273.9701 - loglik: -2.6782e+02 - logprior: -6.1485e+00
Epoch 4/10
10/10 - 1s - loss: 248.9738 - loglik: -2.4535e+02 - logprior: -3.6263e+00
Epoch 5/10
10/10 - 1s - loss: 239.1748 - loglik: -2.3675e+02 - logprior: -2.4232e+00
Epoch 6/10
10/10 - 1s - loss: 235.6985 - loglik: -2.3410e+02 - logprior: -1.5959e+00
Epoch 7/10
10/10 - 1s - loss: 233.3353 - loglik: -2.3235e+02 - logprior: -9.8724e-01
Epoch 8/10
10/10 - 1s - loss: 232.4906 - loglik: -2.3187e+02 - logprior: -6.2115e-01
Epoch 9/10
10/10 - 1s - loss: 232.3254 - loglik: -2.3197e+02 - logprior: -3.5713e-01
Epoch 10/10
10/10 - 1s - loss: 231.4331 - loglik: -2.3129e+02 - logprior: -1.3961e-01
Fitted a model with MAP estimate = -231.0119
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (55, 1), (63, 1), (64, 1), (69, 1), (70, 1), (78, 1), (79, 1), (85, 1), (87, 4), (88, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 291.6529 - loglik: -2.2222e+02 - logprior: -6.9437e+01
Epoch 2/2
10/10 - 2s - loss: 234.6917 - loglik: -2.0831e+02 - logprior: -2.6377e+01
Fitted a model with MAP estimate = -224.7861
expansions: [(0, 2)]
discards: [  0  12 107]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 256.5034 - loglik: -2.0218e+02 - logprior: -5.4326e+01
Epoch 2/2
10/10 - 2s - loss: 211.3720 - loglik: -1.9982e+02 - logprior: -1.1549e+01
Fitted a model with MAP estimate = -205.4918
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 266.9229 - loglik: -2.0183e+02 - logprior: -6.5094e+01
Epoch 2/10
10/10 - 2s - loss: 217.8370 - loglik: -2.0152e+02 - logprior: -1.6312e+01
Epoch 3/10
10/10 - 2s - loss: 204.7891 - loglik: -2.0152e+02 - logprior: -3.2737e+00
Epoch 4/10
10/10 - 2s - loss: 200.9780 - loglik: -2.0231e+02 - logprior: 1.3286
Epoch 5/10
10/10 - 2s - loss: 198.9877 - loglik: -2.0254e+02 - logprior: 3.5540
Epoch 6/10
10/10 - 2s - loss: 196.7708 - loglik: -2.0154e+02 - logprior: 4.7668
Epoch 7/10
10/10 - 2s - loss: 197.5663 - loglik: -2.0312e+02 - logprior: 5.5518
Fitted a model with MAP estimate = -196.4991
Time for alignment: 51.2833
Computed alignments with likelihoods: ['-194.4245', '-195.2336', '-196.4991']
Best model has likelihood: -194.4245
SP score = 0.9168
Training of 3 independent models on file rrm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbba4a6af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c2d9d820>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 195.5690 - loglik: -1.9337e+02 - logprior: -2.1985e+00
Epoch 2/10
22/22 - 1s - loss: 162.9575 - loglik: -1.6163e+02 - logprior: -1.3251e+00
Epoch 3/10
22/22 - 1s - loss: 156.0528 - loglik: -1.5464e+02 - logprior: -1.4178e+00
Epoch 4/10
22/22 - 1s - loss: 154.2659 - loglik: -1.5295e+02 - logprior: -1.3180e+00
Epoch 5/10
22/22 - 1s - loss: 153.6719 - loglik: -1.5235e+02 - logprior: -1.3183e+00
Epoch 6/10
22/22 - 1s - loss: 153.3269 - loglik: -1.5203e+02 - logprior: -1.2935e+00
Epoch 7/10
22/22 - 1s - loss: 153.2737 - loglik: -1.5199e+02 - logprior: -1.2861e+00
Epoch 8/10
22/22 - 1s - loss: 153.0110 - loglik: -1.5173e+02 - logprior: -1.2778e+00
Epoch 9/10
22/22 - 1s - loss: 153.1525 - loglik: -1.5188e+02 - logprior: -1.2753e+00
Fitted a model with MAP estimate = -153.4065
expansions: [(8, 1), (9, 2), (12, 1), (14, 2), (17, 2), (21, 1), (22, 2), (25, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 154.7943 - loglik: -1.5188e+02 - logprior: -2.9164e+00
Epoch 2/2
22/22 - 1s - loss: 146.7070 - loglik: -1.4515e+02 - logprior: -1.5557e+00
Fitted a model with MAP estimate = -147.3443
expansions: [(0, 2)]
discards: [ 0  9 17 22 30 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 145.8263 - loglik: -1.4371e+02 - logprior: -2.1165e+00
Epoch 2/2
22/22 - 1s - loss: 143.0121 - loglik: -1.4199e+02 - logprior: -1.0203e+00
Fitted a model with MAP estimate = -146.2715
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 145.4285 - loglik: -1.4425e+02 - logprior: -1.1800e+00
Epoch 2/10
32/32 - 2s - loss: 142.9200 - loglik: -1.4206e+02 - logprior: -8.6474e-01
Epoch 3/10
32/32 - 2s - loss: 142.4796 - loglik: -1.4163e+02 - logprior: -8.5120e-01
Epoch 4/10
32/32 - 2s - loss: 141.9404 - loglik: -1.4110e+02 - logprior: -8.4388e-01
Epoch 5/10
32/32 - 2s - loss: 141.9729 - loglik: -1.4113e+02 - logprior: -8.3798e-01
Fitted a model with MAP estimate = -141.5388
Time for alignment: 58.1957
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 195.6793 - loglik: -1.9348e+02 - logprior: -2.1953e+00
Epoch 2/10
22/22 - 1s - loss: 164.0426 - loglik: -1.6272e+02 - logprior: -1.3216e+00
Epoch 3/10
22/22 - 1s - loss: 155.7820 - loglik: -1.5438e+02 - logprior: -1.4066e+00
Epoch 4/10
22/22 - 1s - loss: 154.5143 - loglik: -1.5321e+02 - logprior: -1.3038e+00
Epoch 5/10
22/22 - 1s - loss: 153.6489 - loglik: -1.5234e+02 - logprior: -1.3049e+00
Epoch 6/10
22/22 - 1s - loss: 153.3719 - loglik: -1.5209e+02 - logprior: -1.2830e+00
Epoch 7/10
22/22 - 1s - loss: 153.0778 - loglik: -1.5181e+02 - logprior: -1.2693e+00
Epoch 8/10
22/22 - 1s - loss: 153.4289 - loglik: -1.5217e+02 - logprior: -1.2613e+00
Fitted a model with MAP estimate = -153.2046
expansions: [(8, 1), (9, 2), (12, 1), (14, 2), (17, 2), (21, 1), (22, 1), (25, 1), (27, 1), (41, 1), (44, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 154.5030 - loglik: -1.5160e+02 - logprior: -2.9020e+00
Epoch 2/2
22/22 - 1s - loss: 146.2957 - loglik: -1.4478e+02 - logprior: -1.5205e+00
Fitted a model with MAP estimate = -147.1574
expansions: [(0, 2)]
discards: [ 0  9 17 22 66]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 145.6722 - loglik: -1.4356e+02 - logprior: -2.1147e+00
Epoch 2/2
22/22 - 1s - loss: 143.2760 - loglik: -1.4226e+02 - logprior: -1.0209e+00
Fitted a model with MAP estimate = -146.2518
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 145.5722 - loglik: -1.4439e+02 - logprior: -1.1810e+00
Epoch 2/10
32/32 - 2s - loss: 142.8675 - loglik: -1.4200e+02 - logprior: -8.6633e-01
Epoch 3/10
32/32 - 2s - loss: 142.3452 - loglik: -1.4150e+02 - logprior: -8.4617e-01
Epoch 4/10
32/32 - 2s - loss: 142.1369 - loglik: -1.4129e+02 - logprior: -8.5089e-01
Epoch 5/10
32/32 - 2s - loss: 141.7000 - loglik: -1.4086e+02 - logprior: -8.3588e-01
Epoch 6/10
32/32 - 2s - loss: 141.4988 - loglik: -1.4066e+02 - logprior: -8.3747e-01
Epoch 7/10
32/32 - 2s - loss: 141.3975 - loglik: -1.4056e+02 - logprior: -8.3424e-01
Epoch 8/10
32/32 - 2s - loss: 141.1539 - loglik: -1.4032e+02 - logprior: -8.3152e-01
Epoch 9/10
32/32 - 2s - loss: 140.9982 - loglik: -1.4017e+02 - logprior: -8.2878e-01
Epoch 10/10
32/32 - 2s - loss: 141.2946 - loglik: -1.4047e+02 - logprior: -8.2025e-01
Fitted a model with MAP estimate = -141.0952
Time for alignment: 66.0074
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 195.4669 - loglik: -1.9327e+02 - logprior: -2.1965e+00
Epoch 2/10
22/22 - 1s - loss: 162.9308 - loglik: -1.6161e+02 - logprior: -1.3246e+00
Epoch 3/10
22/22 - 1s - loss: 155.8957 - loglik: -1.5450e+02 - logprior: -1.3989e+00
Epoch 4/10
22/22 - 1s - loss: 154.3446 - loglik: -1.5304e+02 - logprior: -1.3088e+00
Epoch 5/10
22/22 - 1s - loss: 153.5416 - loglik: -1.5222e+02 - logprior: -1.3179e+00
Epoch 6/10
22/22 - 1s - loss: 152.9167 - loglik: -1.5162e+02 - logprior: -1.2945e+00
Epoch 7/10
22/22 - 1s - loss: 152.7970 - loglik: -1.5151e+02 - logprior: -1.2880e+00
Epoch 8/10
22/22 - 1s - loss: 153.0225 - loglik: -1.5175e+02 - logprior: -1.2774e+00
Fitted a model with MAP estimate = -152.8673
expansions: [(8, 1), (9, 2), (12, 1), (13, 2), (20, 2), (22, 2), (23, 1), (25, 1), (40, 1), (41, 1), (44, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 154.5354 - loglik: -1.5162e+02 - logprior: -2.9134e+00
Epoch 2/2
22/22 - 1s - loss: 146.6847 - loglik: -1.4513e+02 - logprior: -1.5509e+00
Fitted a model with MAP estimate = -147.2278
expansions: [(0, 2)]
discards: [ 0  9 16 25 29 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 145.6408 - loglik: -1.4352e+02 - logprior: -2.1165e+00
Epoch 2/2
22/22 - 1s - loss: 143.4196 - loglik: -1.4240e+02 - logprior: -1.0187e+00
Fitted a model with MAP estimate = -146.2408
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 145.5951 - loglik: -1.4442e+02 - logprior: -1.1794e+00
Epoch 2/10
32/32 - 2s - loss: 142.7351 - loglik: -1.4187e+02 - logprior: -8.6610e-01
Epoch 3/10
32/32 - 2s - loss: 142.2481 - loglik: -1.4140e+02 - logprior: -8.5247e-01
Epoch 4/10
32/32 - 2s - loss: 142.0324 - loglik: -1.4119e+02 - logprior: -8.4116e-01
Epoch 5/10
32/32 - 2s - loss: 141.9476 - loglik: -1.4111e+02 - logprior: -8.3591e-01
Epoch 6/10
32/32 - 2s - loss: 141.4144 - loglik: -1.4058e+02 - logprior: -8.3707e-01
Epoch 7/10
32/32 - 2s - loss: 141.5242 - loglik: -1.4069e+02 - logprior: -8.2945e-01
Fitted a model with MAP estimate = -141.1525
Time for alignment: 59.2493
Computed alignments with likelihoods: ['-141.5388', '-141.0952', '-141.1525']
Best model has likelihood: -141.0952
SP score = 0.8404
Training of 3 independent models on file KAS.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9de4852b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9ee9c8940>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 162 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 500.7441 - loglik: -4.8162e+02 - logprior: -1.9123e+01
Epoch 2/10
10/10 - 4s - loss: 396.6619 - loglik: -3.9326e+02 - logprior: -3.3972e+00
Epoch 3/10
10/10 - 5s - loss: 319.5395 - loglik: -3.1847e+02 - logprior: -1.0686e+00
Epoch 4/10
10/10 - 4s - loss: 281.5128 - loglik: -2.8084e+02 - logprior: -6.7028e-01
Epoch 5/10
10/10 - 5s - loss: 267.3762 - loglik: -2.6682e+02 - logprior: -5.5795e-01
Epoch 6/10
10/10 - 5s - loss: 260.6429 - loglik: -2.6027e+02 - logprior: -3.7064e-01
Epoch 7/10
10/10 - 5s - loss: 259.4564 - loglik: -2.5918e+02 - logprior: -2.8005e-01
Epoch 8/10
10/10 - 5s - loss: 258.1787 - loglik: -2.5796e+02 - logprior: -2.2244e-01
Epoch 9/10
10/10 - 5s - loss: 257.6111 - loglik: -2.5744e+02 - logprior: -1.7566e-01
Epoch 10/10
10/10 - 5s - loss: 258.9262 - loglik: -2.5876e+02 - logprior: -1.6479e-01
Fitted a model with MAP estimate = -257.3889
expansions: [(0, 34), (21, 1), (24, 2), (25, 1), (50, 1), (61, 1), (83, 1), (93, 1), (117, 1), (138, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 206 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 272.6446 - loglik: -2.4950e+02 - logprior: -2.3143e+01
Epoch 2/2
10/10 - 6s - loss: 234.8086 - loglik: -2.2873e+02 - logprior: -6.0763e+00
Fitted a model with MAP estimate = -227.6176
expansions: [(0, 12), (35, 2)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 205 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 246.9531 - loglik: -2.2668e+02 - logprior: -2.0269e+01
Epoch 2/2
10/10 - 6s - loss: 222.9482 - loglik: -2.1865e+02 - logprior: -4.2954e+00
Fitted a model with MAP estimate = -217.6511
expansions: [(0, 15)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 191 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 256.8840 - loglik: -2.3841e+02 - logprior: -1.8474e+01
Epoch 2/10
10/10 - 6s - loss: 232.9337 - loglik: -2.2948e+02 - logprior: -3.4517e+00
Epoch 3/10
10/10 - 5s - loss: 226.4333 - loglik: -2.2593e+02 - logprior: -5.0565e-01
Epoch 4/10
10/10 - 6s - loss: 224.4247 - loglik: -2.2494e+02 - logprior: 0.5202
Epoch 5/10
10/10 - 5s - loss: 222.1687 - loglik: -2.2324e+02 - logprior: 1.0738
Epoch 6/10
10/10 - 5s - loss: 223.2856 - loglik: -2.2482e+02 - logprior: 1.5330
Fitted a model with MAP estimate = -222.3416
Time for alignment: 131.2668
Fitting a model of length 162 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 499.7605 - loglik: -4.8064e+02 - logprior: -1.9122e+01
Epoch 2/10
10/10 - 4s - loss: 394.6115 - loglik: -3.9110e+02 - logprior: -3.5108e+00
Epoch 3/10
10/10 - 4s - loss: 314.1980 - loglik: -3.1264e+02 - logprior: -1.5562e+00
Epoch 4/10
10/10 - 4s - loss: 274.7104 - loglik: -2.7330e+02 - logprior: -1.4127e+00
Epoch 5/10
10/10 - 4s - loss: 258.3865 - loglik: -2.5700e+02 - logprior: -1.3906e+00
Epoch 6/10
10/10 - 4s - loss: 251.7375 - loglik: -2.5059e+02 - logprior: -1.1515e+00
Epoch 7/10
10/10 - 5s - loss: 247.7747 - loglik: -2.4660e+02 - logprior: -1.1758e+00
Epoch 8/10
10/10 - 4s - loss: 246.8721 - loglik: -2.4574e+02 - logprior: -1.1340e+00
Epoch 9/10
10/10 - 5s - loss: 245.7648 - loglik: -2.4465e+02 - logprior: -1.1148e+00
Epoch 10/10
10/10 - 4s - loss: 246.2986 - loglik: -2.4514e+02 - logprior: -1.1612e+00
Fitted a model with MAP estimate = -245.3494
expansions: [(50, 1), (60, 1), (80, 1), (89, 1), (108, 1), (136, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 167 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 267.0977 - loglik: -2.4387e+02 - logprior: -2.3224e+01
Epoch 2/2
10/10 - 5s - loss: 247.9939 - loglik: -2.3879e+02 - logprior: -9.2071e+00
Fitted a model with MAP estimate = -245.1084
expansions: [(0, 12)]
discards: [  0 138]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 177 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 257.3998 - loglik: -2.3855e+02 - logprior: -1.8852e+01
Epoch 2/2
10/10 - 5s - loss: 237.6832 - loglik: -2.3308e+02 - logprior: -4.6053e+00
Fitted a model with MAP estimate = -234.4439
expansions: [(149, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 10]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 253.7170 - loglik: -2.3543e+02 - logprior: -1.8286e+01
Epoch 2/10
10/10 - 5s - loss: 241.2590 - loglik: -2.3689e+02 - logprior: -4.3640e+00
Epoch 3/10
10/10 - 5s - loss: 235.3631 - loglik: -2.3384e+02 - logprior: -1.5221e+00
Epoch 4/10
10/10 - 5s - loss: 236.9822 - loglik: -2.3651e+02 - logprior: -4.6762e-01
Fitted a model with MAP estimate = -235.2365
Time for alignment: 107.5234
Fitting a model of length 162 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 499.0685 - loglik: -4.7995e+02 - logprior: -1.9117e+01
Epoch 2/10
10/10 - 4s - loss: 397.4422 - loglik: -3.9404e+02 - logprior: -3.4017e+00
Epoch 3/10
10/10 - 4s - loss: 321.8668 - loglik: -3.2071e+02 - logprior: -1.1605e+00
Epoch 4/10
10/10 - 4s - loss: 278.9354 - loglik: -2.7808e+02 - logprior: -8.5217e-01
Epoch 5/10
10/10 - 4s - loss: 262.2281 - loglik: -2.6152e+02 - logprior: -7.1249e-01
Epoch 6/10
10/10 - 4s - loss: 259.7507 - loglik: -2.5929e+02 - logprior: -4.6532e-01
Epoch 7/10
10/10 - 4s - loss: 257.2939 - loglik: -2.5693e+02 - logprior: -3.6303e-01
Epoch 8/10
10/10 - 4s - loss: 253.8060 - loglik: -2.5351e+02 - logprior: -2.9484e-01
Epoch 9/10
10/10 - 5s - loss: 256.8615 - loglik: -2.5661e+02 - logprior: -2.5539e-01
Fitted a model with MAP estimate = -254.6852
expansions: [(0, 36), (7, 1), (25, 2), (26, 1), (28, 1), (50, 1), (61, 1), (83, 1), (93, 1), (117, 1), (137, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 209 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 266.4727 - loglik: -2.4369e+02 - logprior: -2.2784e+01
Epoch 2/2
10/10 - 6s - loss: 230.5676 - loglik: -2.2461e+02 - logprior: -5.9581e+00
Fitted a model with MAP estimate = -219.3395
expansions: [(0, 12)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 204 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 242.1346 - loglik: -2.2192e+02 - logprior: -2.0218e+01
Epoch 2/2
10/10 - 6s - loss: 217.0438 - loglik: -2.1280e+02 - logprior: -4.2482e+00
Fitted a model with MAP estimate = -213.6924
expansions: [(0, 16)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 193 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 250.8540 - loglik: -2.3240e+02 - logprior: -1.8451e+01
Epoch 2/10
10/10 - 5s - loss: 227.7170 - loglik: -2.2424e+02 - logprior: -3.4740e+00
Epoch 3/10
10/10 - 5s - loss: 220.6276 - loglik: -2.2011e+02 - logprior: -5.1681e-01
Epoch 4/10
10/10 - 6s - loss: 217.8827 - loglik: -2.1840e+02 - logprior: 0.5220
Epoch 5/10
10/10 - 5s - loss: 216.1286 - loglik: -2.1721e+02 - logprior: 1.0769
Epoch 6/10
10/10 - 6s - loss: 216.9554 - loglik: -2.1849e+02 - logprior: 1.5346
Fitted a model with MAP estimate = -216.0006
Time for alignment: 123.0021
Computed alignments with likelihoods: ['-217.6511', '-234.4439', '-213.6924']
Best model has likelihood: -213.6924
SP score = 0.4175
Training of 3 independent models on file GEL.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb80c1bd820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c27919d0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 245.8341 - loglik: -2.2557e+02 - logprior: -2.0261e+01
Epoch 2/10
10/10 - 1s - loss: 216.5045 - loglik: -2.1109e+02 - logprior: -5.4185e+00
Epoch 3/10
10/10 - 1s - loss: 198.3891 - loglik: -1.9554e+02 - logprior: -2.8507e+00
Epoch 4/10
10/10 - 1s - loss: 190.1452 - loglik: -1.8795e+02 - logprior: -2.1930e+00
Epoch 5/10
10/10 - 1s - loss: 187.1378 - loglik: -1.8513e+02 - logprior: -2.0124e+00
Epoch 6/10
10/10 - 1s - loss: 185.2299 - loglik: -1.8335e+02 - logprior: -1.8781e+00
Epoch 7/10
10/10 - 1s - loss: 184.5767 - loglik: -1.8297e+02 - logprior: -1.6080e+00
Epoch 8/10
10/10 - 1s - loss: 184.1321 - loglik: -1.8268e+02 - logprior: -1.4535e+00
Epoch 9/10
10/10 - 1s - loss: 183.7194 - loglik: -1.8227e+02 - logprior: -1.4457e+00
Epoch 10/10
10/10 - 1s - loss: 183.6715 - loglik: -1.8225e+02 - logprior: -1.4241e+00
Fitted a model with MAP estimate = -183.5774
expansions: [(0, 2), (7, 2), (8, 2), (34, 1), (41, 2), (42, 2), (43, 1), (44, 1), (45, 1), (46, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 79 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 207.2948 - loglik: -1.8065e+02 - logprior: -2.6640e+01
Epoch 2/2
10/10 - 1s - loss: 185.4870 - loglik: -1.7735e+02 - logprior: -8.1416e+00
Fitted a model with MAP estimate = -181.8271
expansions: []
discards: [ 0 10 12]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 200.4916 - loglik: -1.7737e+02 - logprior: -2.3118e+01
Epoch 2/2
10/10 - 1s - loss: 186.1515 - loglik: -1.7708e+02 - logprior: -9.0686e+00
Fitted a model with MAP estimate = -183.7493
expansions: [(0, 2)]
discards: [ 0 48]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 196.0638 - loglik: -1.7586e+02 - logprior: -2.0201e+01
Epoch 2/10
10/10 - 1s - loss: 181.4332 - loglik: -1.7610e+02 - logprior: -5.3360e+00
Epoch 3/10
10/10 - 1s - loss: 178.3005 - loglik: -1.7604e+02 - logprior: -2.2626e+00
Epoch 4/10
10/10 - 1s - loss: 177.4591 - loglik: -1.7621e+02 - logprior: -1.2444e+00
Epoch 5/10
10/10 - 1s - loss: 177.0828 - loglik: -1.7625e+02 - logprior: -8.3489e-01
Epoch 6/10
10/10 - 1s - loss: 176.2935 - loglik: -1.7561e+02 - logprior: -6.8177e-01
Epoch 7/10
10/10 - 1s - loss: 176.3855 - loglik: -1.7584e+02 - logprior: -5.4619e-01
Fitted a model with MAP estimate = -176.2112
Time for alignment: 35.1935
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 246.0945 - loglik: -2.2583e+02 - logprior: -2.0261e+01
Epoch 2/10
10/10 - 1s - loss: 216.4732 - loglik: -2.1105e+02 - logprior: -5.4229e+00
Epoch 3/10
10/10 - 1s - loss: 199.3895 - loglik: -1.9651e+02 - logprior: -2.8827e+00
Epoch 4/10
10/10 - 1s - loss: 190.3915 - loglik: -1.8818e+02 - logprior: -2.2082e+00
Epoch 5/10
10/10 - 1s - loss: 186.6670 - loglik: -1.8467e+02 - logprior: -1.9967e+00
Epoch 6/10
10/10 - 1s - loss: 185.3364 - loglik: -1.8347e+02 - logprior: -1.8633e+00
Epoch 7/10
10/10 - 1s - loss: 184.4294 - loglik: -1.8281e+02 - logprior: -1.6149e+00
Epoch 8/10
10/10 - 1s - loss: 183.6514 - loglik: -1.8216e+02 - logprior: -1.4931e+00
Epoch 9/10
10/10 - 1s - loss: 183.6081 - loglik: -1.8211e+02 - logprior: -1.4937e+00
Epoch 10/10
10/10 - 1s - loss: 183.2454 - loglik: -1.8177e+02 - logprior: -1.4799e+00
Fitted a model with MAP estimate = -183.2698
expansions: [(0, 2), (7, 2), (8, 2), (23, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 2), (46, 1), (48, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 80 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 206.5201 - loglik: -1.7994e+02 - logprior: -2.6584e+01
Epoch 2/2
10/10 - 1s - loss: 185.3580 - loglik: -1.7718e+02 - logprior: -8.1731e+00
Fitted a model with MAP estimate = -181.2132
expansions: []
discards: [ 0 10 12 53 55 63]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 200.7534 - loglik: -1.7752e+02 - logprior: -2.3235e+01
Epoch 2/2
10/10 - 1s - loss: 186.6391 - loglik: -1.7756e+02 - logprior: -9.0829e+00
Fitted a model with MAP estimate = -183.9981
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 196.1682 - loglik: -1.7594e+02 - logprior: -2.0225e+01
Epoch 2/10
10/10 - 1s - loss: 181.3929 - loglik: -1.7605e+02 - logprior: -5.3407e+00
Epoch 3/10
10/10 - 1s - loss: 178.5725 - loglik: -1.7631e+02 - logprior: -2.2605e+00
Epoch 4/10
10/10 - 1s - loss: 177.8987 - loglik: -1.7666e+02 - logprior: -1.2435e+00
Epoch 5/10
10/10 - 1s - loss: 176.7678 - loglik: -1.7593e+02 - logprior: -8.4011e-01
Epoch 6/10
10/10 - 1s - loss: 177.0649 - loglik: -1.7639e+02 - logprior: -6.7629e-01
Fitted a model with MAP estimate = -176.7254
Time for alignment: 32.7446
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 245.8362 - loglik: -2.2557e+02 - logprior: -2.0262e+01
Epoch 2/10
10/10 - 1s - loss: 216.1915 - loglik: -2.1077e+02 - logprior: -5.4215e+00
Epoch 3/10
10/10 - 1s - loss: 198.4664 - loglik: -1.9560e+02 - logprior: -2.8708e+00
Epoch 4/10
10/10 - 1s - loss: 190.6009 - loglik: -1.8840e+02 - logprior: -2.2028e+00
Epoch 5/10
10/10 - 1s - loss: 187.3013 - loglik: -1.8529e+02 - logprior: -2.0080e+00
Epoch 6/10
10/10 - 1s - loss: 185.4803 - loglik: -1.8363e+02 - logprior: -1.8460e+00
Epoch 7/10
10/10 - 1s - loss: 184.7287 - loglik: -1.8315e+02 - logprior: -1.5765e+00
Epoch 8/10
10/10 - 1s - loss: 184.3204 - loglik: -1.8288e+02 - logprior: -1.4372e+00
Epoch 9/10
10/10 - 1s - loss: 183.7854 - loglik: -1.8236e+02 - logprior: -1.4284e+00
Epoch 10/10
10/10 - 1s - loss: 183.6321 - loglik: -1.8221e+02 - logprior: -1.4176e+00
Fitted a model with MAP estimate = -183.6834
expansions: [(0, 2), (7, 2), (8, 2), (34, 1), (41, 2), (42, 2), (43, 1), (44, 2), (46, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 79 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 207.1236 - loglik: -1.8048e+02 - logprior: -2.6640e+01
Epoch 2/2
10/10 - 1s - loss: 185.7912 - loglik: -1.7766e+02 - logprior: -8.1305e+00
Fitted a model with MAP estimate = -182.0098
expansions: []
discards: [ 0 10 12 52 56]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 200.7479 - loglik: -1.7753e+02 - logprior: -2.3214e+01
Epoch 2/2
10/10 - 1s - loss: 186.5864 - loglik: -1.7754e+02 - logprior: -9.0453e+00
Fitted a model with MAP estimate = -183.9497
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 196.0180 - loglik: -1.7581e+02 - logprior: -2.0212e+01
Epoch 2/10
10/10 - 1s - loss: 181.2823 - loglik: -1.7595e+02 - logprior: -5.3367e+00
Epoch 3/10
10/10 - 1s - loss: 178.5580 - loglik: -1.7630e+02 - logprior: -2.2594e+00
Epoch 4/10
10/10 - 1s - loss: 177.6360 - loglik: -1.7639e+02 - logprior: -1.2412e+00
Epoch 5/10
10/10 - 1s - loss: 177.1845 - loglik: -1.7634e+02 - logprior: -8.4508e-01
Epoch 6/10
10/10 - 1s - loss: 176.7964 - loglik: -1.7611e+02 - logprior: -6.8904e-01
Epoch 7/10
10/10 - 1s - loss: 176.7130 - loglik: -1.7616e+02 - logprior: -5.5221e-01
Epoch 8/10
10/10 - 1s - loss: 176.6801 - loglik: -1.7630e+02 - logprior: -3.8007e-01
Epoch 9/10
10/10 - 1s - loss: 176.6680 - loglik: -1.7643e+02 - logprior: -2.4093e-01
Epoch 10/10
10/10 - 1s - loss: 176.4615 - loglik: -1.7628e+02 - logprior: -1.7820e-01
Fitted a model with MAP estimate = -176.4305
Time for alignment: 36.9843
Computed alignments with likelihoods: ['-176.2112', '-176.7254', '-176.4305']
Best model has likelihood: -176.2112
SP score = 0.6726
Training of 3 independent models on file hr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cb535820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb7cc357be0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 199.0128 - loglik: -1.8753e+02 - logprior: -1.1485e+01
Epoch 2/10
11/11 - 1s - loss: 159.1277 - loglik: -1.5593e+02 - logprior: -3.1949e+00
Epoch 3/10
11/11 - 1s - loss: 126.4500 - loglik: -1.2415e+02 - logprior: -2.3003e+00
Epoch 4/10
11/11 - 1s - loss: 111.0124 - loglik: -1.0880e+02 - logprior: -2.2172e+00
Epoch 5/10
11/11 - 1s - loss: 106.2742 - loglik: -1.0435e+02 - logprior: -1.9237e+00
Epoch 6/10
11/11 - 1s - loss: 104.2196 - loglik: -1.0234e+02 - logprior: -1.8770e+00
Epoch 7/10
11/11 - 1s - loss: 103.6341 - loglik: -1.0177e+02 - logprior: -1.8658e+00
Epoch 8/10
11/11 - 1s - loss: 103.2103 - loglik: -1.0141e+02 - logprior: -1.7991e+00
Epoch 9/10
11/11 - 1s - loss: 103.0856 - loglik: -1.0128e+02 - logprior: -1.8044e+00
Epoch 10/10
11/11 - 1s - loss: 102.8211 - loglik: -1.0103e+02 - logprior: -1.7914e+00
Fitted a model with MAP estimate = -102.8419
expansions: [(0, 3), (15, 1), (26, 1), (27, 2), (28, 3), (29, 1), (30, 1), (31, 1), (38, 2), (40, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 109.5406 - loglik: -9.5859e+01 - logprior: -1.3681e+01
Epoch 2/2
11/11 - 1s - loss: 94.3853 - loglik: -9.0247e+01 - logprior: -4.1380e+00
Fitted a model with MAP estimate = -92.0676
expansions: []
discards: [ 0 33]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 103.6231 - loglik: -9.0515e+01 - logprior: -1.3108e+01
Epoch 2/2
11/11 - 1s - loss: 95.4366 - loglik: -9.0078e+01 - logprior: -5.3583e+00
Fitted a model with MAP estimate = -93.2850
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 101.9198 - loglik: -9.0074e+01 - logprior: -1.1845e+01
Epoch 2/10
11/11 - 1s - loss: 93.0008 - loglik: -8.9596e+01 - logprior: -3.4048e+00
Epoch 3/10
11/11 - 1s - loss: 91.3379 - loglik: -8.9192e+01 - logprior: -2.1461e+00
Epoch 4/10
11/11 - 1s - loss: 91.3844 - loglik: -8.9867e+01 - logprior: -1.5178e+00
Fitted a model with MAP estimate = -91.0035
Time for alignment: 29.3674
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 199.0703 - loglik: -1.8759e+02 - logprior: -1.1485e+01
Epoch 2/10
11/11 - 1s - loss: 158.4318 - loglik: -1.5522e+02 - logprior: -3.2158e+00
Epoch 3/10
11/11 - 1s - loss: 127.6749 - loglik: -1.2528e+02 - logprior: -2.3984e+00
Epoch 4/10
11/11 - 1s - loss: 110.7461 - loglik: -1.0845e+02 - logprior: -2.2949e+00
Epoch 5/10
11/11 - 1s - loss: 105.0814 - loglik: -1.0307e+02 - logprior: -2.0125e+00
Epoch 6/10
11/11 - 1s - loss: 103.6518 - loglik: -1.0165e+02 - logprior: -2.0017e+00
Epoch 7/10
11/11 - 1s - loss: 103.0889 - loglik: -1.0109e+02 - logprior: -1.9999e+00
Epoch 8/10
11/11 - 1s - loss: 102.1329 - loglik: -1.0018e+02 - logprior: -1.9484e+00
Epoch 9/10
11/11 - 1s - loss: 102.0982 - loglik: -1.0015e+02 - logprior: -1.9505e+00
Epoch 10/10
11/11 - 1s - loss: 101.8858 - loglik: -9.9954e+01 - logprior: -1.9318e+00
Fitted a model with MAP estimate = -101.8033
expansions: [(0, 3), (15, 1), (26, 1), (27, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (33, 1), (38, 2), (51, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 111.3606 - loglik: -9.7630e+01 - logprior: -1.3731e+01
Epoch 2/2
11/11 - 1s - loss: 94.8398 - loglik: -9.0591e+01 - logprior: -4.2488e+00
Fitted a model with MAP estimate = -92.1090
expansions: []
discards: [ 0 36 39 44]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 104.0455 - loglik: -9.0921e+01 - logprior: -1.3125e+01
Epoch 2/2
11/11 - 1s - loss: 94.9539 - loglik: -8.9555e+01 - logprior: -5.3992e+00
Fitted a model with MAP estimate = -93.3160
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 101.9340 - loglik: -9.0070e+01 - logprior: -1.1864e+01
Epoch 2/10
11/11 - 1s - loss: 93.0729 - loglik: -8.9646e+01 - logprior: -3.4265e+00
Epoch 3/10
11/11 - 1s - loss: 91.3148 - loglik: -8.9154e+01 - logprior: -2.1604e+00
Epoch 4/10
11/11 - 1s - loss: 91.2720 - loglik: -8.9744e+01 - logprior: -1.5283e+00
Epoch 5/10
11/11 - 1s - loss: 90.9238 - loglik: -8.9655e+01 - logprior: -1.2687e+00
Epoch 6/10
11/11 - 1s - loss: 90.9810 - loglik: -8.9783e+01 - logprior: -1.1976e+00
Fitted a model with MAP estimate = -90.7841
Time for alignment: 30.7849
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 198.8952 - loglik: -1.8741e+02 - logprior: -1.1485e+01
Epoch 2/10
11/11 - 1s - loss: 157.9604 - loglik: -1.5476e+02 - logprior: -3.2051e+00
Epoch 3/10
11/11 - 1s - loss: 123.7694 - loglik: -1.2140e+02 - logprior: -2.3701e+00
Epoch 4/10
11/11 - 1s - loss: 108.7646 - loglik: -1.0647e+02 - logprior: -2.2947e+00
Epoch 5/10
11/11 - 1s - loss: 104.2127 - loglik: -1.0219e+02 - logprior: -2.0201e+00
Epoch 6/10
11/11 - 1s - loss: 103.1891 - loglik: -1.0119e+02 - logprior: -1.9968e+00
Epoch 7/10
11/11 - 1s - loss: 102.3619 - loglik: -1.0037e+02 - logprior: -1.9923e+00
Epoch 8/10
11/11 - 1s - loss: 102.3107 - loglik: -1.0038e+02 - logprior: -1.9321e+00
Epoch 9/10
11/11 - 1s - loss: 101.9262 - loglik: -9.9990e+01 - logprior: -1.9365e+00
Epoch 10/10
11/11 - 1s - loss: 102.0353 - loglik: -1.0011e+02 - logprior: -1.9214e+00
Fitted a model with MAP estimate = -101.8268
expansions: [(0, 3), (15, 1), (26, 1), (27, 1), (28, 1), (29, 2), (30, 2), (31, 3), (34, 1), (37, 1), (43, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 73 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 110.2024 - loglik: -9.6513e+01 - logprior: -1.3690e+01
Epoch 2/2
11/11 - 1s - loss: 94.5134 - loglik: -9.0334e+01 - logprior: -4.1796e+00
Fitted a model with MAP estimate = -91.8431
expansions: []
discards: [ 0 36 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 103.6598 - loglik: -9.0543e+01 - logprior: -1.3116e+01
Epoch 2/2
11/11 - 1s - loss: 95.1621 - loglik: -8.9783e+01 - logprior: -5.3795e+00
Fitted a model with MAP estimate = -93.2952
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 101.8260 - loglik: -8.9955e+01 - logprior: -1.1871e+01
Epoch 2/10
11/11 - 1s - loss: 93.1266 - loglik: -8.9703e+01 - logprior: -3.4231e+00
Epoch 3/10
11/11 - 1s - loss: 91.5136 - loglik: -8.9352e+01 - logprior: -2.1616e+00
Epoch 4/10
11/11 - 1s - loss: 91.2364 - loglik: -8.9700e+01 - logprior: -1.5366e+00
Epoch 5/10
11/11 - 1s - loss: 90.7054 - loglik: -8.9435e+01 - logprior: -1.2703e+00
Epoch 6/10
11/11 - 1s - loss: 90.9611 - loglik: -8.9759e+01 - logprior: -1.2018e+00
Fitted a model with MAP estimate = -90.7924
Time for alignment: 29.5500
Computed alignments with likelihoods: ['-91.0035', '-90.7841', '-90.7924']
Best model has likelihood: -90.7841
SP score = 1.0000
Training of 3 independent models on file rvp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbbad2dfbe0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c0cff8b0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 172.2079 - loglik: -1.7148e+02 - logprior: -7.3162e-01
Epoch 2/10
42/42 - 3s - loss: 83.0544 - loglik: -8.2423e+01 - logprior: -6.3162e-01
Epoch 3/10
42/42 - 3s - loss: 79.8714 - loglik: -7.9253e+01 - logprior: -6.1885e-01
Epoch 4/10
42/42 - 3s - loss: 79.2355 - loglik: -7.8634e+01 - logprior: -6.0128e-01
Epoch 5/10
42/42 - 3s - loss: 78.5246 - loglik: -7.7935e+01 - logprior: -5.8962e-01
Epoch 6/10
42/42 - 3s - loss: 78.1862 - loglik: -7.7598e+01 - logprior: -5.8862e-01
Epoch 7/10
42/42 - 3s - loss: 77.9811 - loglik: -7.7394e+01 - logprior: -5.8675e-01
Epoch 8/10
42/42 - 3s - loss: 77.6068 - loglik: -7.6991e+01 - logprior: -6.1555e-01
Epoch 9/10
42/42 - 3s - loss: 77.4764 - loglik: -7.6789e+01 - logprior: -6.8708e-01
Epoch 10/10
42/42 - 3s - loss: 77.3122 - loglik: -7.6628e+01 - logprior: -6.8381e-01
Fitted a model with MAP estimate = -77.2027
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 45.7611 - loglik: -4.4849e+01 - logprior: -9.1210e-01
Epoch 2/2
42/42 - 4s - loss: 32.8351 - loglik: -3.2217e+01 - logprior: -6.1793e-01
Fitted a model with MAP estimate = -33.2494
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 7s - loss: 34.5529 - loglik: -3.3431e+01 - logprior: -1.1221e+00
Epoch 2/2
42/42 - 4s - loss: 32.4167 - loglik: -3.1590e+01 - logprior: -8.2663e-01
Fitted a model with MAP estimate = -33.2969
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 33.0951 - loglik: -3.2435e+01 - logprior: -6.5988e-01
Epoch 2/10
59/59 - 5s - loss: 32.6402 - loglik: -3.2075e+01 - logprior: -5.6557e-01
Epoch 3/10
59/59 - 5s - loss: 32.9553 - loglik: -3.2401e+01 - logprior: -5.5466e-01
Fitted a model with MAP estimate = -31.6991
Time for alignment: 148.4195
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 171.7830 - loglik: -1.7105e+02 - logprior: -7.3066e-01
Epoch 2/10
42/42 - 3s - loss: 82.3603 - loglik: -8.1729e+01 - logprior: -6.3147e-01
Epoch 3/10
42/42 - 3s - loss: 79.6338 - loglik: -7.9017e+01 - logprior: -6.1690e-01
Epoch 4/10
42/42 - 3s - loss: 79.0715 - loglik: -7.8466e+01 - logprior: -6.0507e-01
Epoch 5/10
42/42 - 3s - loss: 78.6274 - loglik: -7.8038e+01 - logprior: -5.8918e-01
Epoch 6/10
42/42 - 3s - loss: 78.3996 - loglik: -7.7816e+01 - logprior: -5.8325e-01
Epoch 7/10
42/42 - 3s - loss: 78.1351 - loglik: -7.7553e+01 - logprior: -5.8177e-01
Epoch 8/10
42/42 - 3s - loss: 77.5196 - loglik: -7.6919e+01 - logprior: -6.0098e-01
Epoch 9/10
42/42 - 3s - loss: 77.5205 - loglik: -7.6840e+01 - logprior: -6.8086e-01
Fitted a model with MAP estimate = -77.2253
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 6s - loss: 45.9232 - loglik: -4.5025e+01 - logprior: -8.9842e-01
Epoch 2/2
42/42 - 4s - loss: 32.6586 - loglik: -3.2040e+01 - logprior: -6.1894e-01
Fitted a model with MAP estimate = -33.2562
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 34.4246 - loglik: -3.3300e+01 - logprior: -1.1251e+00
Epoch 2/2
42/42 - 4s - loss: 32.8388 - loglik: -3.2015e+01 - logprior: -8.2405e-01
Fitted a model with MAP estimate = -33.3685
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 33.3551 - loglik: -3.2694e+01 - logprior: -6.6136e-01
Epoch 2/10
59/59 - 5s - loss: 32.7984 - loglik: -3.2232e+01 - logprior: -5.6645e-01
Epoch 3/10
59/59 - 5s - loss: 32.5982 - loglik: -3.2044e+01 - logprior: -5.5386e-01
Epoch 4/10
59/59 - 5s - loss: 31.8047 - loglik: -3.1261e+01 - logprior: -5.4360e-01
Epoch 5/10
59/59 - 5s - loss: 31.7715 - loglik: -3.1235e+01 - logprior: -5.3618e-01
Epoch 6/10
59/59 - 5s - loss: 31.2428 - loglik: -3.0717e+01 - logprior: -5.2569e-01
Epoch 7/10
59/59 - 5s - loss: 31.1432 - loglik: -3.0624e+01 - logprior: -5.1950e-01
Epoch 8/10
59/59 - 5s - loss: 30.9941 - loglik: -3.0484e+01 - logprior: -5.0981e-01
Epoch 9/10
59/59 - 5s - loss: 31.2350 - loglik: -3.0732e+01 - logprior: -5.0261e-01
Fitted a model with MAP estimate = -30.9351
Time for alignment: 177.0532
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 171.9469 - loglik: -1.7122e+02 - logprior: -7.2996e-01
Epoch 2/10
42/42 - 3s - loss: 82.6499 - loglik: -8.2017e+01 - logprior: -6.3293e-01
Epoch 3/10
42/42 - 3s - loss: 79.4875 - loglik: -7.8865e+01 - logprior: -6.2285e-01
Epoch 4/10
42/42 - 3s - loss: 79.4650 - loglik: -7.8867e+01 - logprior: -5.9830e-01
Epoch 5/10
42/42 - 3s - loss: 78.2923 - loglik: -7.7702e+01 - logprior: -5.9029e-01
Epoch 6/10
42/42 - 3s - loss: 78.1173 - loglik: -7.7530e+01 - logprior: -5.8767e-01
Epoch 7/10
42/42 - 3s - loss: 78.1064 - loglik: -7.7522e+01 - logprior: -5.8394e-01
Epoch 8/10
42/42 - 3s - loss: 78.1202 - loglik: -7.7531e+01 - logprior: -5.8956e-01
Fitted a model with MAP estimate = -77.4623
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 7s - loss: 45.6214 - loglik: -4.4746e+01 - logprior: -8.7533e-01
Epoch 2/2
42/42 - 4s - loss: 33.1755 - loglik: -3.2557e+01 - logprior: -6.1822e-01
Fitted a model with MAP estimate = -33.2911
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 6s - loss: 35.0950 - loglik: -3.3965e+01 - logprior: -1.1300e+00
Epoch 2/2
42/42 - 4s - loss: 32.4603 - loglik: -3.1635e+01 - logprior: -8.2508e-01
Fitted a model with MAP estimate = -33.3726
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 9s - loss: 33.2566 - loglik: -3.2598e+01 - logprior: -6.5882e-01
Epoch 2/10
59/59 - 5s - loss: 32.6733 - loglik: -3.2107e+01 - logprior: -5.6605e-01
Epoch 3/10
59/59 - 5s - loss: 32.7913 - loglik: -3.2239e+01 - logprior: -5.5236e-01
Fitted a model with MAP estimate = -31.7361
Time for alignment: 142.1093
Computed alignments with likelihoods: ['-31.6991', '-30.9351', '-31.7361']
Best model has likelihood: -30.9351
SP score = 0.6583
Training of 3 independent models on file mmp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c0531640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cb247310>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 124 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 486.4955 - loglik: -4.5732e+02 - logprior: -2.9180e+01
Epoch 2/10
10/10 - 3s - loss: 430.6400 - loglik: -4.2436e+02 - logprior: -6.2758e+00
Epoch 3/10
10/10 - 3s - loss: 394.2242 - loglik: -3.9173e+02 - logprior: -2.4985e+00
Epoch 4/10
10/10 - 3s - loss: 369.9706 - loglik: -3.6835e+02 - logprior: -1.6182e+00
Epoch 5/10
10/10 - 3s - loss: 361.6281 - loglik: -3.6025e+02 - logprior: -1.3799e+00
Epoch 6/10
10/10 - 3s - loss: 357.3314 - loglik: -3.5611e+02 - logprior: -1.2179e+00
Epoch 7/10
10/10 - 3s - loss: 355.3133 - loglik: -3.5426e+02 - logprior: -1.0577e+00
Epoch 8/10
10/10 - 3s - loss: 353.8778 - loglik: -3.5288e+02 - logprior: -1.0014e+00
Epoch 9/10
10/10 - 3s - loss: 354.6679 - loglik: -3.5377e+02 - logprior: -8.9360e-01
Fitted a model with MAP estimate = -353.9382
expansions: [(12, 1), (16, 6), (17, 1), (36, 2), (38, 2), (43, 1), (44, 2), (66, 1), (67, 1), (70, 1), (76, 1), (78, 2), (79, 4), (100, 1), (102, 1), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [ 0 45 46 47]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 154 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 385.9779 - loglik: -3.5305e+02 - logprior: -3.2929e+01
Epoch 2/2
10/10 - 4s - loss: 357.3182 - loglik: -3.4487e+02 - logprior: -1.2445e+01
Fitted a model with MAP estimate = -353.3326
expansions: [(0, 18), (57, 2), (59, 1)]
discards: [ 0 47 94]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 172 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 370.0219 - loglik: -3.4386e+02 - logprior: -2.6157e+01
Epoch 2/2
10/10 - 5s - loss: 345.9798 - loglik: -3.4033e+02 - logprior: -5.6513e+00
Fitted a model with MAP estimate = -342.2248
expansions: [(36, 1), (73, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 29]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 156 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 368.6257 - loglik: -3.3884e+02 - logprior: -2.9783e+01
Epoch 2/10
10/10 - 4s - loss: 344.8271 - loglik: -3.3837e+02 - logprior: -6.4579e+00
Epoch 3/10
10/10 - 4s - loss: 339.8943 - loglik: -3.3876e+02 - logprior: -1.1323e+00
Epoch 4/10
10/10 - 4s - loss: 337.9248 - loglik: -3.3866e+02 - logprior: 0.7345
Epoch 5/10
10/10 - 4s - loss: 337.3962 - loglik: -3.3896e+02 - logprior: 1.5672
Epoch 6/10
10/10 - 4s - loss: 336.3580 - loglik: -3.3843e+02 - logprior: 2.0724
Epoch 7/10
10/10 - 4s - loss: 336.7263 - loglik: -3.3920e+02 - logprior: 2.4767
Fitted a model with MAP estimate = -336.0450
Time for alignment: 94.6839
Fitting a model of length 124 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 485.4590 - loglik: -4.5628e+02 - logprior: -2.9182e+01
Epoch 2/10
10/10 - 3s - loss: 432.2659 - loglik: -4.2600e+02 - logprior: -6.2618e+00
Epoch 3/10
10/10 - 3s - loss: 394.4513 - loglik: -3.9203e+02 - logprior: -2.4188e+00
Epoch 4/10
10/10 - 3s - loss: 372.0819 - loglik: -3.7062e+02 - logprior: -1.4634e+00
Epoch 5/10
10/10 - 3s - loss: 361.8039 - loglik: -3.6068e+02 - logprior: -1.1243e+00
Epoch 6/10
10/10 - 3s - loss: 358.8443 - loglik: -3.5795e+02 - logprior: -8.9164e-01
Epoch 7/10
10/10 - 3s - loss: 355.9147 - loglik: -3.5513e+02 - logprior: -7.8811e-01
Epoch 8/10
10/10 - 3s - loss: 355.7173 - loglik: -3.5496e+02 - logprior: -7.5734e-01
Epoch 9/10
10/10 - 3s - loss: 355.6466 - loglik: -3.5494e+02 - logprior: -7.0298e-01
Epoch 10/10
10/10 - 3s - loss: 355.2768 - loglik: -3.5463e+02 - logprior: -6.4750e-01
Fitted a model with MAP estimate = -354.9030
expansions: [(3, 1), (5, 1), (16, 5), (17, 1), (18, 1), (36, 1), (42, 1), (43, 1), (44, 1), (66, 1), (67, 1), (70, 1), (79, 6), (100, 1), (102, 1), (111, 2), (113, 1)]
discards: [ 0 45 46 47]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 147 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 389.7048 - loglik: -3.5645e+02 - logprior: -3.3253e+01
Epoch 2/2
10/10 - 4s - loss: 360.7487 - loglik: -3.4790e+02 - logprior: -1.2847e+01
Fitted a model with MAP estimate = -356.4305
expansions: [(0, 19), (16, 1), (55, 3), (57, 1), (131, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 172 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 371.4610 - loglik: -3.4483e+02 - logprior: -2.6630e+01
Epoch 2/2
10/10 - 5s - loss: 346.8259 - loglik: -3.4100e+02 - logprior: -5.8279e+00
Fitted a model with MAP estimate = -342.6993
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 154 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 369.0155 - loglik: -3.3860e+02 - logprior: -3.0414e+01
Epoch 2/10
10/10 - 4s - loss: 344.8091 - loglik: -3.3782e+02 - logprior: -6.9854e+00
Epoch 3/10
10/10 - 4s - loss: 340.1278 - loglik: -3.3858e+02 - logprior: -1.5466e+00
Epoch 4/10
10/10 - 4s - loss: 338.1171 - loglik: -3.3845e+02 - logprior: 0.3371
Epoch 5/10
10/10 - 4s - loss: 337.3882 - loglik: -3.3856e+02 - logprior: 1.1688
Epoch 6/10
10/10 - 4s - loss: 337.4731 - loglik: -3.3916e+02 - logprior: 1.6892
Fitted a model with MAP estimate = -336.6674
Time for alignment: 92.9596
Fitting a model of length 124 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 485.5661 - loglik: -4.5638e+02 - logprior: -2.9185e+01
Epoch 2/10
10/10 - 3s - loss: 432.1344 - loglik: -4.2587e+02 - logprior: -6.2605e+00
Epoch 3/10
10/10 - 3s - loss: 391.2343 - loglik: -3.8879e+02 - logprior: -2.4453e+00
Epoch 4/10
10/10 - 3s - loss: 367.5420 - loglik: -3.6597e+02 - logprior: -1.5747e+00
Epoch 5/10
10/10 - 3s - loss: 359.6650 - loglik: -3.5834e+02 - logprior: -1.3274e+00
Epoch 6/10
10/10 - 3s - loss: 356.1667 - loglik: -3.5507e+02 - logprior: -1.0925e+00
Epoch 7/10
10/10 - 3s - loss: 355.1958 - loglik: -3.5423e+02 - logprior: -9.6291e-01
Epoch 8/10
10/10 - 3s - loss: 353.0393 - loglik: -3.5219e+02 - logprior: -8.4971e-01
Epoch 9/10
10/10 - 3s - loss: 355.1761 - loglik: -3.5443e+02 - logprior: -7.4770e-01
Fitted a model with MAP estimate = -353.6413
expansions: [(10, 1), (11, 1), (16, 5), (17, 1), (36, 3), (38, 1), (41, 1), (44, 1), (45, 1), (66, 2), (76, 1), (78, 2), (79, 3), (100, 1), (102, 1), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 155 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 382.6852 - loglik: -3.4965e+02 - logprior: -3.3031e+01
Epoch 2/2
10/10 - 4s - loss: 355.1290 - loglik: -3.4256e+02 - logprior: -1.2566e+01
Fitted a model with MAP estimate = -351.3653
expansions: [(0, 20), (16, 1), (57, 1)]
discards: [ 0 96]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 175 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 367.1072 - loglik: -3.4104e+02 - logprior: -2.6070e+01
Epoch 2/2
10/10 - 5s - loss: 345.5697 - loglik: -3.4013e+02 - logprior: -5.4432e+00
Fitted a model with MAP estimate = -341.0153
expansions: [(103, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 64 78 79 80 81]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 371.3712 - loglik: -3.4186e+02 - logprior: -2.9510e+01
Epoch 2/10
10/10 - 4s - loss: 347.6850 - loglik: -3.4140e+02 - logprior: -6.2871e+00
Epoch 3/10
10/10 - 4s - loss: 342.7132 - loglik: -3.4165e+02 - logprior: -1.0613e+00
Epoch 4/10
10/10 - 4s - loss: 341.0417 - loglik: -3.4186e+02 - logprior: 0.8199
Epoch 5/10
10/10 - 4s - loss: 339.8329 - loglik: -3.4150e+02 - logprior: 1.6670
Epoch 6/10
10/10 - 4s - loss: 338.9687 - loglik: -3.4114e+02 - logprior: 2.1692
Epoch 7/10
10/10 - 4s - loss: 340.8106 - loglik: -3.4337e+02 - logprior: 2.5630
Fitted a model with MAP estimate = -338.8797
Time for alignment: 92.4542
Computed alignments with likelihoods: ['-336.0450', '-336.6674', '-338.8797']
Best model has likelihood: -336.0450
SP score = 0.9765
Training of 3 independent models on file icd.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cc635c10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9ef00a640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 22s - loss: 857.1371 - loglik: -8.5443e+02 - logprior: -2.7093e+00
Epoch 2/10
29/29 - 20s - loss: 672.5172 - loglik: -6.7082e+02 - logprior: -1.7019e+00
Epoch 3/10
29/29 - 20s - loss: 642.2280 - loglik: -6.4026e+02 - logprior: -1.9690e+00
Epoch 4/10
29/29 - 20s - loss: 638.8585 - loglik: -6.3685e+02 - logprior: -2.0130e+00
Epoch 5/10
29/29 - 20s - loss: 633.8098 - loglik: -6.3176e+02 - logprior: -2.0524e+00
Epoch 6/10
29/29 - 20s - loss: 633.9392 - loglik: -6.3186e+02 - logprior: -2.0814e+00
Fitted a model with MAP estimate = -632.9247
expansions: [(16, 1), (17, 1), (26, 1), (30, 1), (31, 1), (37, 1), (39, 1), (42, 1), (48, 2), (49, 1), (50, 1), (73, 2), (87, 2), (88, 2), (89, 1), (94, 1), (117, 8), (121, 2), (124, 1), (125, 1), (142, 1), (145, 1), (152, 1), (154, 1), (155, 1), (156, 1), (163, 1), (166, 2), (173, 1), (185, 2), (186, 1), (191, 1), (192, 1), (205, 1), (218, 2), (219, 2), (220, 1), (233, 1), (249, 1), (250, 1), (252, 2), (256, 1), (258, 1), (260, 1), (262, 1), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 349 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 32s - loss: 612.4705 - loglik: -6.0735e+02 - logprior: -5.1200e+00
Epoch 2/2
29/29 - 28s - loss: 590.2502 - loglik: -5.8793e+02 - logprior: -2.3239e+00
Fitted a model with MAP estimate = -585.1027
expansions: [(0, 2), (138, 5)]
discards: [  0  60  84  85 103 140 141 142 143 144 145 204 268 307 334]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 341 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 30s - loss: 592.9525 - loglik: -5.9014e+02 - logprior: -2.8143e+00
Epoch 2/2
29/29 - 27s - loss: 587.8671 - loglik: -5.8760e+02 - logprior: -2.6936e-01
Fitted a model with MAP estimate = -586.3358
expansions: []
discards: [  0 135 136 137 138 139 330]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 334 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 30s - loss: 596.2825 - loglik: -5.9193e+02 - logprior: -4.3545e+00
Epoch 2/10
29/29 - 26s - loss: 590.8391 - loglik: -5.9041e+02 - logprior: -4.2775e-01
Epoch 3/10
29/29 - 26s - loss: 589.0074 - loglik: -5.8941e+02 - logprior: 0.4022
Epoch 4/10
29/29 - 26s - loss: 587.0555 - loglik: -5.8781e+02 - logprior: 0.7497
Epoch 5/10
29/29 - 26s - loss: 590.2019 - loglik: -5.9100e+02 - logprior: 0.7986
Fitted a model with MAP estimate = -587.5096
Time for alignment: 477.5473
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 23s - loss: 856.2040 - loglik: -8.5349e+02 - logprior: -2.7130e+00
Epoch 2/10
29/29 - 20s - loss: 673.0203 - loglik: -6.7122e+02 - logprior: -1.7969e+00
Epoch 3/10
29/29 - 20s - loss: 646.7329 - loglik: -6.4469e+02 - logprior: -2.0437e+00
Epoch 4/10
29/29 - 20s - loss: 638.6707 - loglik: -6.3666e+02 - logprior: -2.0097e+00
Epoch 5/10
29/29 - 20s - loss: 637.9961 - loglik: -6.3599e+02 - logprior: -2.0096e+00
Epoch 6/10
29/29 - 20s - loss: 635.9406 - loglik: -6.3391e+02 - logprior: -2.0323e+00
Epoch 7/10
29/29 - 20s - loss: 637.0363 - loglik: -6.3497e+02 - logprior: -2.0646e+00
Fitted a model with MAP estimate = -634.4749
expansions: [(16, 1), (22, 1), (24, 1), (28, 1), (29, 1), (30, 1), (31, 1), (36, 2), (46, 1), (48, 2), (49, 1), (72, 1), (76, 1), (87, 1), (88, 1), (89, 1), (120, 2), (121, 2), (123, 1), (124, 1), (126, 1), (127, 1), (141, 1), (147, 1), (151, 1), (152, 1), (154, 1), (162, 1), (173, 1), (184, 1), (185, 1), (186, 2), (190, 1), (191, 1), (205, 1), (217, 2), (218, 2), (219, 1), (240, 1), (248, 1), (249, 1), (251, 2), (258, 1), (260, 1), (261, 1), (263, 1), (264, 1), (270, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 337 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 29s - loss: 615.2150 - loglik: -6.1018e+02 - logprior: -5.0367e+00
Epoch 2/2
29/29 - 26s - loss: 594.3687 - loglik: -5.9223e+02 - logprior: -2.1348e+00
Fitted a model with MAP estimate = -591.6473
expansions: [(0, 2), (323, 3)]
discards: [  0  36 140 259 298]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 337 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 30s - loss: 594.2448 - loglik: -5.9133e+02 - logprior: -2.9107e+00
Epoch 2/2
29/29 - 27s - loss: 587.5342 - loglik: -5.8728e+02 - logprior: -2.5597e-01
Fitted a model with MAP estimate = -587.6202
expansions: [(138, 1)]
discards: [  0 322]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 29s - loss: 595.0225 - loglik: -5.9078e+02 - logprior: -4.2466e+00
Epoch 2/10
29/29 - 26s - loss: 590.4036 - loglik: -5.8993e+02 - logprior: -4.7486e-01
Epoch 3/10
29/29 - 26s - loss: 588.1398 - loglik: -5.8867e+02 - logprior: 0.5265
Epoch 4/10
29/29 - 26s - loss: 586.6909 - loglik: -5.8718e+02 - logprior: 0.4869
Epoch 5/10
29/29 - 26s - loss: 587.6884 - loglik: -5.8865e+02 - logprior: 0.9660
Fitted a model with MAP estimate = -586.7370
Time for alignment: 490.4167
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 22s - loss: 855.6562 - loglik: -8.5296e+02 - logprior: -2.7009e+00
Epoch 2/10
29/29 - 20s - loss: 672.5604 - loglik: -6.7085e+02 - logprior: -1.7115e+00
Epoch 3/10
29/29 - 20s - loss: 644.4584 - loglik: -6.4242e+02 - logprior: -2.0395e+00
Epoch 4/10
29/29 - 20s - loss: 638.7361 - loglik: -6.3672e+02 - logprior: -2.0136e+00
Epoch 5/10
29/29 - 20s - loss: 635.3772 - loglik: -6.3335e+02 - logprior: -2.0303e+00
Epoch 6/10
29/29 - 20s - loss: 635.3022 - loglik: -6.3328e+02 - logprior: -2.0225e+00
Epoch 7/10
29/29 - 20s - loss: 634.5211 - loglik: -6.3250e+02 - logprior: -2.0176e+00
Epoch 8/10
29/29 - 20s - loss: 632.3280 - loglik: -6.3028e+02 - logprior: -2.0498e+00
Epoch 9/10
29/29 - 20s - loss: 631.2820 - loglik: -6.2921e+02 - logprior: -2.0705e+00
Epoch 10/10
29/29 - 20s - loss: 631.6745 - loglik: -6.2961e+02 - logprior: -2.0614e+00
Fitted a model with MAP estimate = -630.9229
expansions: [(16, 1), (17, 1), (24, 1), (28, 1), (29, 1), (30, 1), (31, 1), (36, 1), (42, 1), (48, 3), (64, 2), (73, 1), (76, 1), (87, 1), (88, 2), (89, 3), (93, 1), (119, 1), (120, 2), (122, 1), (124, 1), (127, 1), (141, 1), (147, 1), (151, 1), (154, 2), (155, 1), (162, 1), (173, 1), (184, 1), (185, 1), (186, 2), (190, 1), (191, 1), (204, 1), (217, 2), (218, 2), (219, 1), (232, 1), (248, 1), (249, 1), (251, 1), (258, 1), (260, 1), (261, 1), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 343 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 30s - loss: 610.0690 - loglik: -6.0519e+02 - logprior: -4.8781e+00
Epoch 2/2
29/29 - 27s - loss: 588.9899 - loglik: -5.8687e+02 - logprior: -2.1247e+00
Fitted a model with MAP estimate = -586.0461
expansions: [(0, 2), (26, 1)]
discards: [  0  56  57 105 108 185 263 327 328]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 337 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 29s - loss: 589.9659 - loglik: -5.8715e+02 - logprior: -2.8130e+00
Epoch 2/2
29/29 - 26s - loss: 585.4176 - loglik: -5.8535e+02 - logprior: -6.8933e-02
Fitted a model with MAP estimate = -585.0928
expansions: [(323, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 338 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 30s - loss: 590.3934 - loglik: -5.8607e+02 - logprior: -4.3194e+00
Epoch 2/10
29/29 - 27s - loss: 585.8608 - loglik: -5.8552e+02 - logprior: -3.4322e-01
Epoch 3/10
29/29 - 27s - loss: 584.8054 - loglik: -5.8526e+02 - logprior: 0.4506
Epoch 4/10
29/29 - 27s - loss: 584.1982 - loglik: -5.8503e+02 - logprior: 0.8352
Epoch 5/10
29/29 - 26s - loss: 582.6447 - loglik: -5.8361e+02 - logprior: 0.9674
Epoch 6/10
29/29 - 27s - loss: 583.9882 - loglik: -5.8516e+02 - logprior: 1.1700
Fitted a model with MAP estimate = -582.4003
Time for alignment: 578.3366
Computed alignments with likelihoods: ['-585.1027', '-586.7370', '-582.4003']
Best model has likelihood: -582.4003
SP score = 0.8744
Training of 3 independent models on file seatoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cd31e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c2616730>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 617.0758 - loglik: -1.2832e+02 - logprior: -4.8876e+02
Epoch 2/10
10/10 - 0s - loss: 241.6613 - loglik: -1.0669e+02 - logprior: -1.3497e+02
Epoch 3/10
10/10 - 0s - loss: 148.9666 - loglik: -8.6695e+01 - logprior: -6.2271e+01
Epoch 4/10
10/10 - 0s - loss: 108.2721 - loglik: -7.3521e+01 - logprior: -3.4752e+01
Epoch 5/10
10/10 - 0s - loss: 89.8147 - loglik: -6.9750e+01 - logprior: -2.0065e+01
Epoch 6/10
10/10 - 0s - loss: 79.9434 - loglik: -6.9569e+01 - logprior: -1.0374e+01
Epoch 7/10
10/10 - 0s - loss: 74.1510 - loglik: -7.0105e+01 - logprior: -4.0464e+00
Epoch 8/10
10/10 - 0s - loss: 70.6278 - loglik: -7.0406e+01 - logprior: -2.2161e-01
Epoch 9/10
10/10 - 0s - loss: 68.2908 - loglik: -7.0583e+01 - logprior: 2.2924
Epoch 10/10
10/10 - 0s - loss: 66.6239 - loglik: -7.0783e+01 - logprior: 4.1589
Fitted a model with MAP estimate = -65.8751
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 713.2330 - loglik: -6.1200e+01 - logprior: -6.5203e+02
Epoch 2/2
10/10 - 0s - loss: 257.3744 - loglik: -5.4126e+01 - logprior: -2.0325e+02
Fitted a model with MAP estimate = -171.0233
expansions: []
discards: [23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 504.1058 - loglik: -4.8847e+01 - logprior: -4.5526e+02
Epoch 2/2
10/10 - 0s - loss: 170.0142 - loglik: -4.9134e+01 - logprior: -1.2088e+02
Fitted a model with MAP estimate = -120.2042
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 475.2232 - loglik: -4.7821e+01 - logprior: -4.2740e+02
Epoch 2/10
10/10 - 0s - loss: 162.2583 - loglik: -4.9043e+01 - logprior: -1.1322e+02
Epoch 3/10
10/10 - 0s - loss: 96.5090 - loglik: -5.0063e+01 - logprior: -4.6446e+01
Epoch 4/10
10/10 - 0s - loss: 68.6041 - loglik: -5.0818e+01 - logprior: -1.7786e+01
Epoch 5/10
10/10 - 0s - loss: 53.1979 - loglik: -5.1414e+01 - logprior: -1.7835e+00
Epoch 6/10
10/10 - 0s - loss: 44.1455 - loglik: -5.1874e+01 - logprior: 7.7286
Epoch 7/10
10/10 - 0s - loss: 38.4722 - loglik: -5.2231e+01 - logprior: 13.7585
Epoch 8/10
10/10 - 0s - loss: 34.5876 - loglik: -5.2511e+01 - logprior: 17.9231
Epoch 9/10
10/10 - 0s - loss: 31.6800 - loglik: -5.2735e+01 - logprior: 21.0552
Epoch 10/10
10/10 - 0s - loss: 29.3362 - loglik: -5.2921e+01 - logprior: 23.5844
Fitted a model with MAP estimate = -28.1932
Time for alignment: 22.8760
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 617.0758 - loglik: -1.2832e+02 - logprior: -4.8876e+02
Epoch 2/10
10/10 - 0s - loss: 241.6613 - loglik: -1.0669e+02 - logprior: -1.3497e+02
Epoch 3/10
10/10 - 0s - loss: 148.9666 - loglik: -8.6695e+01 - logprior: -6.2271e+01
Epoch 4/10
10/10 - 0s - loss: 108.2721 - loglik: -7.3521e+01 - logprior: -3.4752e+01
Epoch 5/10
10/10 - 0s - loss: 89.8147 - loglik: -6.9750e+01 - logprior: -2.0065e+01
Epoch 6/10
10/10 - 0s - loss: 79.9434 - loglik: -6.9569e+01 - logprior: -1.0374e+01
Epoch 7/10
10/10 - 0s - loss: 74.1510 - loglik: -7.0105e+01 - logprior: -4.0464e+00
Epoch 8/10
10/10 - 0s - loss: 70.6278 - loglik: -7.0406e+01 - logprior: -2.2161e-01
Epoch 9/10
10/10 - 0s - loss: 68.2908 - loglik: -7.0583e+01 - logprior: 2.2924
Epoch 10/10
10/10 - 0s - loss: 66.6239 - loglik: -7.0783e+01 - logprior: 4.1589
Fitted a model with MAP estimate = -65.8751
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 713.2330 - loglik: -6.1200e+01 - logprior: -6.5203e+02
Epoch 2/2
10/10 - 0s - loss: 257.3744 - loglik: -5.4126e+01 - logprior: -2.0325e+02
Fitted a model with MAP estimate = -171.0233
expansions: []
discards: [23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 504.1058 - loglik: -4.8847e+01 - logprior: -4.5526e+02
Epoch 2/2
10/10 - 0s - loss: 170.0142 - loglik: -4.9134e+01 - logprior: -1.2088e+02
Fitted a model with MAP estimate = -120.2042
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 475.2232 - loglik: -4.7821e+01 - logprior: -4.2740e+02
Epoch 2/10
10/10 - 0s - loss: 162.2583 - loglik: -4.9043e+01 - logprior: -1.1322e+02
Epoch 3/10
10/10 - 0s - loss: 96.5090 - loglik: -5.0063e+01 - logprior: -4.6446e+01
Epoch 4/10
10/10 - 0s - loss: 68.6041 - loglik: -5.0818e+01 - logprior: -1.7786e+01
Epoch 5/10
10/10 - 0s - loss: 53.1979 - loglik: -5.1414e+01 - logprior: -1.7835e+00
Epoch 6/10
10/10 - 0s - loss: 44.1455 - loglik: -5.1874e+01 - logprior: 7.7286
Epoch 7/10
10/10 - 0s - loss: 38.4722 - loglik: -5.2231e+01 - logprior: 13.7585
Epoch 8/10
10/10 - 0s - loss: 34.5876 - loglik: -5.2511e+01 - logprior: 17.9231
Epoch 9/10
10/10 - 0s - loss: 31.6800 - loglik: -5.2735e+01 - logprior: 21.0552
Epoch 10/10
10/10 - 0s - loss: 29.3362 - loglik: -5.2921e+01 - logprior: 23.5844
Fitted a model with MAP estimate = -28.1932
Time for alignment: 22.6113
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 617.0758 - loglik: -1.2832e+02 - logprior: -4.8876e+02
Epoch 2/10
10/10 - 0s - loss: 241.6613 - loglik: -1.0669e+02 - logprior: -1.3497e+02
Epoch 3/10
10/10 - 0s - loss: 148.9666 - loglik: -8.6695e+01 - logprior: -6.2271e+01
Epoch 4/10
10/10 - 0s - loss: 108.2721 - loglik: -7.3521e+01 - logprior: -3.4752e+01
Epoch 5/10
10/10 - 0s - loss: 89.8147 - loglik: -6.9750e+01 - logprior: -2.0065e+01
Epoch 6/10
10/10 - 0s - loss: 79.9434 - loglik: -6.9569e+01 - logprior: -1.0374e+01
Epoch 7/10
10/10 - 0s - loss: 74.1510 - loglik: -7.0105e+01 - logprior: -4.0464e+00
Epoch 8/10
10/10 - 0s - loss: 70.6278 - loglik: -7.0406e+01 - logprior: -2.2161e-01
Epoch 9/10
10/10 - 0s - loss: 68.2908 - loglik: -7.0583e+01 - logprior: 2.2924
Epoch 10/10
10/10 - 0s - loss: 66.6239 - loglik: -7.0783e+01 - logprior: 4.1589
Fitted a model with MAP estimate = -65.8751
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 713.2330 - loglik: -6.1200e+01 - logprior: -6.5203e+02
Epoch 2/2
10/10 - 0s - loss: 257.3744 - loglik: -5.4126e+01 - logprior: -2.0325e+02
Fitted a model with MAP estimate = -171.0233
expansions: []
discards: [23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 504.1058 - loglik: -4.8847e+01 - logprior: -4.5526e+02
Epoch 2/2
10/10 - 0s - loss: 170.0142 - loglik: -4.9134e+01 - logprior: -1.2088e+02
Fitted a model with MAP estimate = -120.2042
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 475.2232 - loglik: -4.7821e+01 - logprior: -4.2740e+02
Epoch 2/10
10/10 - 0s - loss: 162.2583 - loglik: -4.9043e+01 - logprior: -1.1322e+02
Epoch 3/10
10/10 - 0s - loss: 96.5090 - loglik: -5.0063e+01 - logprior: -4.6446e+01
Epoch 4/10
10/10 - 0s - loss: 68.6041 - loglik: -5.0818e+01 - logprior: -1.7786e+01
Epoch 5/10
10/10 - 0s - loss: 53.1979 - loglik: -5.1414e+01 - logprior: -1.7835e+00
Epoch 6/10
10/10 - 0s - loss: 44.1455 - loglik: -5.1874e+01 - logprior: 7.7286
Epoch 7/10
10/10 - 0s - loss: 38.4722 - loglik: -5.2231e+01 - logprior: 13.7585
Epoch 8/10
10/10 - 0s - loss: 34.5876 - loglik: -5.2511e+01 - logprior: 17.9231
Epoch 9/10
10/10 - 0s - loss: 31.6800 - loglik: -5.2735e+01 - logprior: 21.0552
Epoch 10/10
10/10 - 0s - loss: 29.3362 - loglik: -5.2921e+01 - logprior: 23.5844
Fitted a model with MAP estimate = -28.1932
Time for alignment: 24.0777
Computed alignments with likelihoods: ['-28.1932', '-28.1932', '-28.1932']
Best model has likelihood: -28.1932
SP score = 0.7083
Training of 3 independent models on file int.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cc088070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c03b6430>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 477.0226 - loglik: -4.7284e+02 - logprior: -4.1835e+00
Epoch 2/10
16/16 - 4s - loss: 439.0352 - loglik: -4.3801e+02 - logprior: -1.0209e+00
Epoch 3/10
16/16 - 4s - loss: 415.9433 - loglik: -4.1461e+02 - logprior: -1.3316e+00
Epoch 4/10
16/16 - 4s - loss: 406.9988 - loglik: -4.0564e+02 - logprior: -1.3573e+00
Epoch 5/10
16/16 - 4s - loss: 405.4025 - loglik: -4.0413e+02 - logprior: -1.2676e+00
Epoch 6/10
16/16 - 4s - loss: 403.0033 - loglik: -4.0168e+02 - logprior: -1.3227e+00
Epoch 7/10
16/16 - 4s - loss: 402.8595 - loglik: -4.0154e+02 - logprior: -1.3157e+00
Epoch 8/10
16/16 - 4s - loss: 402.1458 - loglik: -4.0082e+02 - logprior: -1.3219e+00
Epoch 9/10
16/16 - 4s - loss: 400.5591 - loglik: -3.9923e+02 - logprior: -1.3296e+00
Epoch 10/10
16/16 - 4s - loss: 402.2823 - loglik: -4.0093e+02 - logprior: -1.3509e+00
Fitted a model with MAP estimate = -401.2100
expansions: [(15, 3), (17, 1), (23, 1), (28, 2), (30, 1), (43, 1), (45, 1), (49, 2), (50, 2), (51, 1), (57, 1), (58, 2), (72, 1), (73, 2), (74, 2), (75, 2), (94, 4), (95, 2), (99, 1), (102, 1), (116, 3), (119, 1), (122, 1), (125, 1), (129, 1), (139, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 9s - loss: 403.4766 - loglik: -3.9804e+02 - logprior: -5.4359e+00
Epoch 2/2
16/16 - 6s - loss: 394.5322 - loglik: -3.9193e+02 - logprior: -2.6070e+00
Fitted a model with MAP estimate = -392.6261
expansions: [(0, 1)]
discards: [  0  15  33  61  73 123 149 178 179 180]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 172 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 395.1206 - loglik: -3.9110e+02 - logprior: -4.0251e+00
Epoch 2/2
16/16 - 6s - loss: 391.2525 - loglik: -3.8997e+02 - logprior: -1.2786e+00
Fitted a model with MAP estimate = -389.6866
expansions: [(113, 1), (172, 2)]
discards: [31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 174 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 10s - loss: 394.0811 - loglik: -3.9003e+02 - logprior: -4.0531e+00
Epoch 2/10
16/16 - 6s - loss: 390.2142 - loglik: -3.8901e+02 - logprior: -1.2012e+00
Epoch 3/10
16/16 - 6s - loss: 388.9453 - loglik: -3.8816e+02 - logprior: -7.8204e-01
Epoch 4/10
16/16 - 6s - loss: 390.0962 - loglik: -3.8948e+02 - logprior: -6.1900e-01
Fitted a model with MAP estimate = -389.0728
Time for alignment: 130.1529
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 477.2619 - loglik: -4.7307e+02 - logprior: -4.1955e+00
Epoch 2/10
16/16 - 4s - loss: 439.7084 - loglik: -4.3868e+02 - logprior: -1.0305e+00
Epoch 3/10
16/16 - 4s - loss: 418.7524 - loglik: -4.1744e+02 - logprior: -1.3082e+00
Epoch 4/10
16/16 - 4s - loss: 409.6939 - loglik: -4.0841e+02 - logprior: -1.2797e+00
Epoch 5/10
16/16 - 4s - loss: 406.8224 - loglik: -4.0565e+02 - logprior: -1.1728e+00
Epoch 6/10
16/16 - 4s - loss: 404.6469 - loglik: -4.0341e+02 - logprior: -1.2376e+00
Epoch 7/10
16/16 - 4s - loss: 403.6237 - loglik: -4.0237e+02 - logprior: -1.2541e+00
Epoch 8/10
16/16 - 4s - loss: 401.9318 - loglik: -4.0064e+02 - logprior: -1.2913e+00
Epoch 9/10
16/16 - 4s - loss: 402.3447 - loglik: -4.0105e+02 - logprior: -1.2962e+00
Fitted a model with MAP estimate = -401.8892
expansions: [(13, 1), (14, 1), (23, 3), (28, 3), (29, 1), (42, 1), (43, 1), (48, 2), (51, 1), (53, 1), (56, 1), (57, 1), (69, 1), (73, 2), (74, 3), (90, 1), (94, 5), (102, 1), (116, 3), (119, 1), (122, 1), (125, 1), (129, 1), (139, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 9s - loss: 403.0590 - loglik: -3.9766e+02 - logprior: -5.4016e+00
Epoch 2/2
16/16 - 6s - loss: 392.5751 - loglik: -3.9001e+02 - logprior: -2.5656e+00
Fitted a model with MAP estimate = -391.2116
expansions: [(0, 1)]
discards: [  0  25  33  34 146 175 176 177]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 394.5060 - loglik: -3.9050e+02 - logprior: -4.0103e+00
Epoch 2/2
16/16 - 6s - loss: 390.0782 - loglik: -3.8883e+02 - logprior: -1.2452e+00
Fitted a model with MAP estimate = -389.2898
expansions: [(32, 2), (171, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 175 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 392.0082 - loglik: -3.8797e+02 - logprior: -4.0392e+00
Epoch 2/10
16/16 - 6s - loss: 389.2150 - loglik: -3.8804e+02 - logprior: -1.1752e+00
Epoch 3/10
16/16 - 6s - loss: 387.2067 - loglik: -3.8643e+02 - logprior: -7.7619e-01
Epoch 4/10
16/16 - 6s - loss: 387.5758 - loglik: -3.8697e+02 - logprior: -6.0861e-01
Fitted a model with MAP estimate = -387.3776
Time for alignment: 125.8366
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 477.5148 - loglik: -4.7331e+02 - logprior: -4.2035e+00
Epoch 2/10
16/16 - 4s - loss: 439.9542 - loglik: -4.3893e+02 - logprior: -1.0274e+00
Epoch 3/10
16/16 - 4s - loss: 416.6848 - loglik: -4.1535e+02 - logprior: -1.3365e+00
Epoch 4/10
16/16 - 4s - loss: 408.2211 - loglik: -4.0688e+02 - logprior: -1.3400e+00
Epoch 5/10
16/16 - 4s - loss: 405.6527 - loglik: -4.0438e+02 - logprior: -1.2691e+00
Epoch 6/10
16/16 - 4s - loss: 402.1035 - loglik: -4.0078e+02 - logprior: -1.3253e+00
Epoch 7/10
16/16 - 4s - loss: 401.5918 - loglik: -4.0028e+02 - logprior: -1.3078e+00
Epoch 8/10
16/16 - 4s - loss: 401.3489 - loglik: -4.0001e+02 - logprior: -1.3377e+00
Epoch 9/10
16/16 - 4s - loss: 401.0134 - loglik: -3.9968e+02 - logprior: -1.3350e+00
Epoch 10/10
16/16 - 4s - loss: 400.5920 - loglik: -3.9925e+02 - logprior: -1.3439e+00
Fitted a model with MAP estimate = -400.5686
expansions: [(13, 1), (14, 1), (23, 1), (27, 1), (28, 3), (29, 1), (42, 1), (44, 1), (48, 2), (50, 1), (53, 1), (56, 1), (57, 1), (69, 1), (73, 2), (74, 3), (94, 2), (95, 4), (98, 1), (116, 1), (117, 1), (119, 1), (122, 1), (125, 1), (129, 1), (139, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 176 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 9s - loss: 402.2858 - loglik: -3.9690e+02 - logprior: -5.3869e+00
Epoch 2/2
16/16 - 6s - loss: 392.6351 - loglik: -3.9012e+02 - logprior: -2.5141e+00
Fitted a model with MAP estimate = -391.3588
expansions: [(0, 1)]
discards: [  0  31  32 173 174 175]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 394.4524 - loglik: -3.9042e+02 - logprior: -4.0335e+00
Epoch 2/2
16/16 - 5s - loss: 390.3238 - loglik: -3.8906e+02 - logprior: -1.2668e+00
Fitted a model with MAP estimate = -389.1915
expansions: [(31, 2), (171, 2)]
discards: [115]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 174 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 9s - loss: 392.8664 - loglik: -3.8880e+02 - logprior: -4.0678e+00
Epoch 2/10
16/16 - 6s - loss: 388.5601 - loglik: -3.8734e+02 - logprior: -1.2164e+00
Epoch 3/10
16/16 - 6s - loss: 387.7996 - loglik: -3.8700e+02 - logprior: -7.9936e-01
Epoch 4/10
16/16 - 6s - loss: 388.1012 - loglik: -3.8748e+02 - logprior: -6.2408e-01
Fitted a model with MAP estimate = -387.4979
Time for alignment: 130.4482
Computed alignments with likelihoods: ['-389.0728', '-387.3776', '-387.4979']
Best model has likelihood: -387.3776
SP score = 0.8112
Training of 3 independent models on file phc.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbb941eeca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cd430df0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 291.3210 - loglik: -2.7639e+02 - logprior: -1.4929e+01
Epoch 2/10
10/10 - 1s - loss: 254.6213 - loglik: -2.5074e+02 - logprior: -3.8850e+00
Epoch 3/10
10/10 - 1s - loss: 227.5220 - loglik: -2.2534e+02 - logprior: -2.1854e+00
Epoch 4/10
10/10 - 1s - loss: 215.3397 - loglik: -2.1349e+02 - logprior: -1.8460e+00
Epoch 5/10
10/10 - 1s - loss: 207.1717 - loglik: -2.0557e+02 - logprior: -1.6045e+00
Epoch 6/10
10/10 - 1s - loss: 205.6663 - loglik: -2.0420e+02 - logprior: -1.4669e+00
Epoch 7/10
10/10 - 1s - loss: 204.4628 - loglik: -2.0306e+02 - logprior: -1.4044e+00
Epoch 8/10
10/10 - 1s - loss: 203.8472 - loglik: -2.0245e+02 - logprior: -1.3992e+00
Epoch 9/10
10/10 - 1s - loss: 200.9890 - loglik: -1.9958e+02 - logprior: -1.4077e+00
Epoch 10/10
10/10 - 1s - loss: 204.3265 - loglik: -2.0290e+02 - logprior: -1.4231e+00
Fitted a model with MAP estimate = -202.5580
expansions: [(10, 2), (18, 1), (19, 1), (20, 1), (33, 1), (36, 3), (44, 1), (48, 1), (58, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 79 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 220.7408 - loglik: -2.0343e+02 - logprior: -1.7307e+01
Epoch 2/2
10/10 - 1s - loss: 202.2980 - loglik: -1.9475e+02 - logprior: -7.5499e+00
Fitted a model with MAP estimate = -198.1004
expansions: [(0, 9), (4, 1)]
discards: [ 0  9 10 19 23 42]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 209.9725 - loglik: -1.9562e+02 - logprior: -1.4351e+01
Epoch 2/2
10/10 - 1s - loss: 194.9776 - loglik: -1.9051e+02 - logprior: -4.4709e+00
Fitted a model with MAP estimate = -190.1056
expansions: [(58, 1)]
discards: [ 0  1  2  3  4  5  6  7 12]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 208.3119 - loglik: -1.9260e+02 - logprior: -1.5709e+01
Epoch 2/10
10/10 - 1s - loss: 196.4071 - loglik: -1.9165e+02 - logprior: -4.7579e+00
Epoch 3/10
10/10 - 1s - loss: 192.4822 - loglik: -1.8995e+02 - logprior: -2.5304e+00
Epoch 4/10
10/10 - 1s - loss: 190.9279 - loglik: -1.8903e+02 - logprior: -1.8986e+00
Epoch 5/10
10/10 - 1s - loss: 190.6971 - loglik: -1.8913e+02 - logprior: -1.5627e+00
Epoch 6/10
10/10 - 1s - loss: 191.4379 - loglik: -1.9021e+02 - logprior: -1.2317e+00
Fitted a model with MAP estimate = -190.5455
Time for alignment: 41.6772
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 291.3745 - loglik: -2.7644e+02 - logprior: -1.4935e+01
Epoch 2/10
10/10 - 1s - loss: 253.9688 - loglik: -2.5008e+02 - logprior: -3.8919e+00
Epoch 3/10
10/10 - 1s - loss: 225.9322 - loglik: -2.2379e+02 - logprior: -2.1424e+00
Epoch 4/10
10/10 - 1s - loss: 214.9679 - loglik: -2.1331e+02 - logprior: -1.6596e+00
Epoch 5/10
10/10 - 1s - loss: 209.8675 - loglik: -2.0844e+02 - logprior: -1.4277e+00
Epoch 6/10
10/10 - 1s - loss: 204.3423 - loglik: -2.0296e+02 - logprior: -1.3793e+00
Epoch 7/10
10/10 - 1s - loss: 204.3863 - loglik: -2.0309e+02 - logprior: -1.2966e+00
Fitted a model with MAP estimate = -203.8137
expansions: [(27, 1), (35, 2), (37, 2), (41, 2), (43, 2), (57, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 77 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 221.0468 - loglik: -2.0374e+02 - logprior: -1.7303e+01
Epoch 2/2
10/10 - 1s - loss: 205.0186 - loglik: -1.9745e+02 - logprior: -7.5665e+00
Fitted a model with MAP estimate = -201.3027
expansions: [(0, 6), (36, 1)]
discards: [ 0 40 49]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 208.4344 - loglik: -1.9438e+02 - logprior: -1.4054e+01
Epoch 2/2
10/10 - 1s - loss: 194.6131 - loglik: -1.9042e+02 - logprior: -4.1883e+00
Fitted a model with MAP estimate = -190.7436
expansions: []
discards: [1 2 3 4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 77 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 202.9588 - loglik: -1.8918e+02 - logprior: -1.3776e+01
Epoch 2/10
10/10 - 1s - loss: 193.0795 - loglik: -1.8900e+02 - logprior: -4.0758e+00
Epoch 3/10
10/10 - 1s - loss: 191.2427 - loglik: -1.8901e+02 - logprior: -2.2332e+00
Epoch 4/10
10/10 - 1s - loss: 188.8640 - loglik: -1.8726e+02 - logprior: -1.6049e+00
Epoch 5/10
10/10 - 1s - loss: 187.9266 - loglik: -1.8653e+02 - logprior: -1.3951e+00
Epoch 6/10
10/10 - 1s - loss: 186.6973 - loglik: -1.8545e+02 - logprior: -1.2434e+00
Epoch 7/10
10/10 - 1s - loss: 188.4908 - loglik: -1.8742e+02 - logprior: -1.0696e+00
Fitted a model with MAP estimate = -187.1503
Time for alignment: 39.9068
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 290.0762 - loglik: -2.7514e+02 - logprior: -1.4931e+01
Epoch 2/10
10/10 - 1s - loss: 255.7269 - loglik: -2.5183e+02 - logprior: -3.8998e+00
Epoch 3/10
10/10 - 1s - loss: 225.8730 - loglik: -2.2359e+02 - logprior: -2.2874e+00
Epoch 4/10
10/10 - 1s - loss: 210.9165 - loglik: -2.0887e+02 - logprior: -2.0436e+00
Epoch 5/10
10/10 - 1s - loss: 203.4480 - loglik: -2.0156e+02 - logprior: -1.8924e+00
Epoch 6/10
10/10 - 1s - loss: 202.4245 - loglik: -2.0064e+02 - logprior: -1.7851e+00
Epoch 7/10
10/10 - 1s - loss: 200.9446 - loglik: -1.9924e+02 - logprior: -1.7022e+00
Epoch 8/10
10/10 - 1s - loss: 199.0191 - loglik: -1.9733e+02 - logprior: -1.6873e+00
Epoch 9/10
10/10 - 1s - loss: 199.0028 - loglik: -1.9729e+02 - logprior: -1.7102e+00
Epoch 10/10
10/10 - 1s - loss: 198.8314 - loglik: -1.9711e+02 - logprior: -1.7172e+00
Fitted a model with MAP estimate = -199.1114
expansions: [(10, 1), (20, 1), (21, 1), (34, 1), (37, 1), (39, 1), (44, 1), (48, 2), (56, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 77 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 216.3915 - loglik: -1.9900e+02 - logprior: -1.7393e+01
Epoch 2/2
10/10 - 1s - loss: 198.8315 - loglik: -1.9126e+02 - logprior: -7.5681e+00
Fitted a model with MAP estimate = -196.0291
expansions: [(0, 6), (28, 1), (42, 1), (51, 1)]
discards: [ 0 10 20]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 205.1471 - loglik: -1.9105e+02 - logprior: -1.4097e+01
Epoch 2/2
10/10 - 1s - loss: 191.4220 - loglik: -1.8712e+02 - logprior: -4.3020e+00
Fitted a model with MAP estimate = -187.5347
expansions: []
discards: [0 1 2 3 4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 204.2069 - loglik: -1.8787e+02 - logprior: -1.6333e+01
Epoch 2/10
10/10 - 1s - loss: 193.9782 - loglik: -1.8887e+02 - logprior: -5.1104e+00
Epoch 3/10
10/10 - 1s - loss: 188.7867 - loglik: -1.8622e+02 - logprior: -2.5624e+00
Epoch 4/10
10/10 - 1s - loss: 188.3945 - loglik: -1.8650e+02 - logprior: -1.8900e+00
Epoch 5/10
10/10 - 1s - loss: 188.7942 - loglik: -1.8736e+02 - logprior: -1.4354e+00
Fitted a model with MAP estimate = -188.4724
Time for alignment: 39.9998
Computed alignments with likelihoods: ['-190.1056', '-187.1503', '-187.5347']
Best model has likelihood: -187.1503
SP score = 0.7533
Training of 3 independent models on file ricin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbaf0ec5c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb7cc22c070>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 167 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 613.8289 - loglik: -5.6102e+02 - logprior: -5.2810e+01
Epoch 2/10
10/10 - 4s - loss: 514.4066 - loglik: -5.0560e+02 - logprior: -8.8056e+00
Epoch 3/10
10/10 - 4s - loss: 452.7662 - loglik: -4.5104e+02 - logprior: -1.7238e+00
Epoch 4/10
10/10 - 4s - loss: 419.7239 - loglik: -4.1996e+02 - logprior: 0.2388
Epoch 5/10
10/10 - 4s - loss: 406.0780 - loglik: -4.0716e+02 - logprior: 1.0864
Epoch 6/10
10/10 - 4s - loss: 401.6428 - loglik: -4.0357e+02 - logprior: 1.9272
Epoch 7/10
10/10 - 4s - loss: 400.8719 - loglik: -4.0320e+02 - logprior: 2.3273
Epoch 8/10
10/10 - 4s - loss: 398.7027 - loglik: -4.0133e+02 - logprior: 2.6274
Epoch 9/10
10/10 - 4s - loss: 398.7172 - loglik: -4.0162e+02 - logprior: 2.8988
Fitted a model with MAP estimate = -398.4245
expansions: [(0, 3), (19, 2), (22, 2), (23, 1), (24, 1), (29, 3), (43, 1), (44, 2), (46, 1), (52, 1), (74, 1), (81, 2), (97, 1), (104, 1), (107, 2), (113, 1), (116, 4), (145, 6), (146, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 203 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 460.3558 - loglik: -3.9217e+02 - logprior: -6.8190e+01
Epoch 2/2
10/10 - 5s - loss: 394.7266 - loglik: -3.7867e+02 - logprior: -1.6053e+01
Fitted a model with MAP estimate = -382.2598
expansions: [(93, 2)]
discards: [  0   1   2  23  27  99 142]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 198 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 426.2569 - loglik: -3.7693e+02 - logprior: -4.9330e+01
Epoch 2/2
10/10 - 5s - loss: 382.3420 - loglik: -3.7430e+02 - logprior: -8.0409e+00
Fitted a model with MAP estimate = -376.5112
expansions: [(33, 2), (171, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 202 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 419.2396 - loglik: -3.7369e+02 - logprior: -4.5548e+01
Epoch 2/10
10/10 - 5s - loss: 378.7832 - loglik: -3.7239e+02 - logprior: -6.3895e+00
Epoch 3/10
10/10 - 5s - loss: 370.2584 - loglik: -3.7221e+02 - logprior: 1.9504
Epoch 4/10
10/10 - 5s - loss: 366.7585 - loglik: -3.7241e+02 - logprior: 5.6525
Epoch 5/10
10/10 - 5s - loss: 365.7874 - loglik: -3.7356e+02 - logprior: 7.7740
Epoch 6/10
10/10 - 5s - loss: 364.1570 - loglik: -3.7327e+02 - logprior: 9.1130
Epoch 7/10
10/10 - 5s - loss: 363.8951 - loglik: -3.7389e+02 - logprior: 9.9986
Epoch 8/10
10/10 - 5s - loss: 362.8734 - loglik: -3.7353e+02 - logprior: 10.6570
Epoch 9/10
10/10 - 5s - loss: 362.1576 - loglik: -3.7337e+02 - logprior: 11.2139
Epoch 10/10
10/10 - 5s - loss: 361.9791 - loglik: -3.7374e+02 - logprior: 11.7649
Fitted a model with MAP estimate = -361.4489
Time for alignment: 116.7084
Fitting a model of length 167 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 613.2723 - loglik: -5.6046e+02 - logprior: -5.2812e+01
Epoch 2/10
10/10 - 4s - loss: 516.1833 - loglik: -5.0737e+02 - logprior: -8.8146e+00
Epoch 3/10
10/10 - 4s - loss: 453.0757 - loglik: -4.5129e+02 - logprior: -1.7812e+00
Epoch 4/10
10/10 - 4s - loss: 419.8156 - loglik: -4.1989e+02 - logprior: 0.0792
Epoch 5/10
10/10 - 4s - loss: 407.3867 - loglik: -4.0816e+02 - logprior: 0.7770
Epoch 6/10
10/10 - 4s - loss: 403.9631 - loglik: -4.0538e+02 - logprior: 1.4190
Epoch 7/10
10/10 - 4s - loss: 402.0993 - loglik: -4.0397e+02 - logprior: 1.8739
Epoch 8/10
10/10 - 4s - loss: 401.2169 - loglik: -4.0341e+02 - logprior: 2.1967
Epoch 9/10
10/10 - 4s - loss: 400.7903 - loglik: -4.0326e+02 - logprior: 2.4715
Epoch 10/10
10/10 - 4s - loss: 400.2601 - loglik: -4.0295e+02 - logprior: 2.6946
Fitted a model with MAP estimate = -400.2383
expansions: [(9, 1), (10, 1), (16, 1), (19, 1), (20, 1), (22, 3), (23, 1), (29, 2), (42, 1), (44, 1), (45, 2), (46, 1), (52, 2), (74, 1), (81, 2), (97, 1), (104, 1), (107, 2), (114, 1), (116, 1), (117, 2), (118, 1), (145, 4), (146, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 201 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 457.2941 - loglik: -3.9696e+02 - logprior: -6.0330e+01
Epoch 2/2
10/10 - 5s - loss: 404.0555 - loglik: -3.8281e+02 - logprior: -2.1242e+01
Fitted a model with MAP estimate = -394.6234
expansions: [(0, 3)]
discards: [ 0 27 28 58 67 99]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 198 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 426.1517 - loglik: -3.7943e+02 - logprior: -4.6723e+01
Epoch 2/2
10/10 - 5s - loss: 385.0845 - loglik: -3.7752e+02 - logprior: -7.5596e+00
Fitted a model with MAP estimate = -378.8463
expansions: [(173, 3)]
discards: [  0   2  52 141]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 197 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 437.4397 - loglik: -3.7894e+02 - logprior: -5.8502e+01
Epoch 2/10
10/10 - 4s - loss: 393.9789 - loglik: -3.7713e+02 - logprior: -1.6850e+01
Epoch 3/10
10/10 - 4s - loss: 379.7132 - loglik: -3.7756e+02 - logprior: -2.1544e+00
Epoch 4/10
10/10 - 5s - loss: 372.9125 - loglik: -3.7763e+02 - logprior: 4.7132
Epoch 5/10
10/10 - 4s - loss: 369.6373 - loglik: -3.7691e+02 - logprior: 7.2754
Epoch 6/10
10/10 - 4s - loss: 368.6952 - loglik: -3.7729e+02 - logprior: 8.5916
Epoch 7/10
10/10 - 5s - loss: 368.0799 - loglik: -3.7750e+02 - logprior: 9.4172
Epoch 8/10
10/10 - 5s - loss: 366.9938 - loglik: -3.7712e+02 - logprior: 10.1312
Epoch 9/10
10/10 - 4s - loss: 366.4073 - loglik: -3.7718e+02 - logprior: 10.7747
Epoch 10/10
10/10 - 5s - loss: 366.1332 - loglik: -3.7748e+02 - logprior: 11.3427
Fitted a model with MAP estimate = -365.6857
Time for alignment: 119.1828
Fitting a model of length 167 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 613.1597 - loglik: -5.6035e+02 - logprior: -5.2807e+01
Epoch 2/10
10/10 - 4s - loss: 516.4434 - loglik: -5.0764e+02 - logprior: -8.8036e+00
Epoch 3/10
10/10 - 4s - loss: 453.4561 - loglik: -4.5175e+02 - logprior: -1.7077e+00
Epoch 4/10
10/10 - 4s - loss: 419.9063 - loglik: -4.2039e+02 - logprior: 0.4802
Epoch 5/10
10/10 - 4s - loss: 408.3717 - loglik: -4.0986e+02 - logprior: 1.4886
Epoch 6/10
10/10 - 4s - loss: 402.9694 - loglik: -4.0495e+02 - logprior: 1.9834
Epoch 7/10
10/10 - 4s - loss: 401.8686 - loglik: -4.0426e+02 - logprior: 2.3899
Epoch 8/10
10/10 - 4s - loss: 400.8778 - loglik: -4.0357e+02 - logprior: 2.6913
Epoch 9/10
10/10 - 4s - loss: 399.7493 - loglik: -4.0273e+02 - logprior: 2.9846
Epoch 10/10
10/10 - 4s - loss: 399.8165 - loglik: -4.0305e+02 - logprior: 3.2313
Fitted a model with MAP estimate = -399.4761
expansions: [(7, 1), (8, 2), (19, 2), (22, 1), (23, 3), (25, 1), (30, 2), (44, 3), (46, 1), (52, 2), (75, 2), (81, 2), (103, 1), (105, 2), (106, 1), (117, 2), (119, 1), (150, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 199 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 456.8558 - loglik: -3.9615e+02 - logprior: -6.0706e+01
Epoch 2/2
10/10 - 5s - loss: 404.2230 - loglik: -3.8238e+02 - logprior: -2.1838e+01
Fitted a model with MAP estimate = -394.5520
expansions: [(0, 3), (130, 1), (173, 3)]
discards: [  0  22  33  67 100 179 180]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 199 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 427.3823 - loglik: -3.8008e+02 - logprior: -4.7299e+01
Epoch 2/2
10/10 - 5s - loss: 383.2520 - loglik: -3.7495e+02 - logprior: -8.3049e+00
Fitted a model with MAP estimate = -377.1553
expansions: [(40, 1), (88, 1), (172, 1)]
discards: [ 0  2 34 60]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 198 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 436.3903 - loglik: -3.7734e+02 - logprior: -5.9054e+01
Epoch 2/10
10/10 - 4s - loss: 392.5239 - loglik: -3.7508e+02 - logprior: -1.7439e+01
Epoch 3/10
10/10 - 5s - loss: 378.0046 - loglik: -3.7536e+02 - logprior: -2.6458e+00
Epoch 4/10
10/10 - 5s - loss: 371.9561 - loglik: -3.7609e+02 - logprior: 4.1368
Epoch 5/10
10/10 - 4s - loss: 368.4452 - loglik: -3.7510e+02 - logprior: 6.6566
Epoch 6/10
10/10 - 5s - loss: 367.0419 - loglik: -3.7503e+02 - logprior: 7.9925
Epoch 7/10
10/10 - 5s - loss: 366.8405 - loglik: -3.7564e+02 - logprior: 8.8003
Epoch 8/10
10/10 - 5s - loss: 365.8724 - loglik: -3.7539e+02 - logprior: 9.5141
Epoch 9/10
10/10 - 5s - loss: 365.1272 - loglik: -3.7531e+02 - logprior: 10.1838
Epoch 10/10
10/10 - 5s - loss: 365.4948 - loglik: -3.7625e+02 - logprior: 10.7509
Fitted a model with MAP estimate = -364.6243
Time for alignment: 117.5498
Computed alignments with likelihoods: ['-361.4489', '-365.6857', '-364.6243']
Best model has likelihood: -361.4489
SP score = 0.7183
Training of 3 independent models on file PDZ.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c0182220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9eec2d7f0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 238.0912 - loglik: -2.3497e+02 - logprior: -3.1195e+00
Epoch 2/10
19/19 - 2s - loss: 206.6284 - loglik: -2.0546e+02 - logprior: -1.1699e+00
Epoch 3/10
19/19 - 2s - loss: 197.2329 - loglik: -1.9597e+02 - logprior: -1.2594e+00
Epoch 4/10
19/19 - 2s - loss: 194.5827 - loglik: -1.9338e+02 - logprior: -1.2072e+00
Epoch 5/10
19/19 - 2s - loss: 193.7074 - loglik: -1.9252e+02 - logprior: -1.1900e+00
Epoch 6/10
19/19 - 2s - loss: 193.1536 - loglik: -1.9198e+02 - logprior: -1.1710e+00
Epoch 7/10
19/19 - 2s - loss: 193.2012 - loglik: -1.9204e+02 - logprior: -1.1599e+00
Fitted a model with MAP estimate = -185.6829
expansions: [(0, 2), (3, 1), (4, 1), (6, 2), (16, 1), (17, 3), (18, 1), (22, 1), (24, 1), (48, 3), (49, 1), (52, 1), (53, 1), (55, 2), (58, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 192.7160 - loglik: -1.8865e+02 - logprior: -4.0674e+00
Epoch 2/2
19/19 - 2s - loss: 186.1279 - loglik: -1.8470e+02 - logprior: -1.4254e+00
Fitted a model with MAP estimate = -179.7893
expansions: []
discards: [ 0  1 12 25]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 188.6737 - loglik: -1.8580e+02 - logprior: -2.8757e+00
Epoch 2/2
19/19 - 2s - loss: 186.0557 - loglik: -1.8499e+02 - logprior: -1.0611e+00
Fitted a model with MAP estimate = -180.5839
expansions: [(0, 2), (24, 1), (25, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 180.7319 - loglik: -1.7846e+02 - logprior: -2.2722e+00
Epoch 2/10
23/23 - 2s - loss: 177.9836 - loglik: -1.7694e+02 - logprior: -1.0465e+00
Epoch 3/10
23/23 - 2s - loss: 177.0880 - loglik: -1.7609e+02 - logprior: -1.0003e+00
Epoch 4/10
23/23 - 2s - loss: 176.5550 - loglik: -1.7560e+02 - logprior: -9.5297e-01
Epoch 5/10
23/23 - 2s - loss: 176.6001 - loglik: -1.7567e+02 - logprior: -9.3010e-01
Fitted a model with MAP estimate = -176.3095
Time for alignment: 54.8093
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 238.7659 - loglik: -2.3565e+02 - logprior: -3.1185e+00
Epoch 2/10
19/19 - 2s - loss: 208.6337 - loglik: -2.0746e+02 - logprior: -1.1781e+00
Epoch 3/10
19/19 - 2s - loss: 196.9159 - loglik: -1.9562e+02 - logprior: -1.2921e+00
Epoch 4/10
19/19 - 2s - loss: 194.5590 - loglik: -1.9330e+02 - logprior: -1.2560e+00
Epoch 5/10
19/19 - 2s - loss: 193.8866 - loglik: -1.9266e+02 - logprior: -1.2305e+00
Epoch 6/10
19/19 - 2s - loss: 193.0419 - loglik: -1.9184e+02 - logprior: -1.2026e+00
Epoch 7/10
19/19 - 2s - loss: 193.4296 - loglik: -1.9224e+02 - logprior: -1.1882e+00
Fitted a model with MAP estimate = -186.0311
expansions: [(0, 2), (3, 1), (4, 1), (16, 1), (17, 3), (23, 1), (42, 1), (46, 2), (47, 1), (48, 1), (49, 2), (52, 1), (55, 2), (56, 1), (58, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 193.9355 - loglik: -1.8983e+02 - logprior: -4.1088e+00
Epoch 2/2
19/19 - 2s - loss: 187.0852 - loglik: -1.8565e+02 - logprior: -1.4349e+00
Fitted a model with MAP estimate = -180.7952
expansions: [(19, 4)]
discards: [ 0 25 57]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 190.2058 - loglik: -1.8635e+02 - logprior: -3.8607e+00
Epoch 2/2
19/19 - 2s - loss: 186.0511 - loglik: -1.8459e+02 - logprior: -1.4599e+00
Fitted a model with MAP estimate = -180.3054
expansions: []
discards: [ 0  1 20 21]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 181.7809 - loglik: -1.7958e+02 - logprior: -2.2005e+00
Epoch 2/10
23/23 - 2s - loss: 179.0096 - loglik: -1.7815e+02 - logprior: -8.5814e-01
Epoch 3/10
23/23 - 2s - loss: 177.9524 - loglik: -1.7715e+02 - logprior: -8.0147e-01
Epoch 4/10
23/23 - 2s - loss: 177.8133 - loglik: -1.7708e+02 - logprior: -7.3505e-01
Epoch 5/10
23/23 - 2s - loss: 177.6786 - loglik: -1.7696e+02 - logprior: -7.2040e-01
Epoch 6/10
23/23 - 2s - loss: 177.4324 - loglik: -1.7673e+02 - logprior: -7.0470e-01
Epoch 7/10
23/23 - 2s - loss: 177.4049 - loglik: -1.7672e+02 - logprior: -6.8854e-01
Epoch 8/10
23/23 - 2s - loss: 176.8452 - loglik: -1.7617e+02 - logprior: -6.7945e-01
Epoch 9/10
23/23 - 2s - loss: 177.2965 - loglik: -1.7664e+02 - logprior: -6.5911e-01
Fitted a model with MAP estimate = -177.0687
Time for alignment: 62.2195
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 238.1927 - loglik: -2.3507e+02 - logprior: -3.1223e+00
Epoch 2/10
19/19 - 2s - loss: 206.0313 - loglik: -2.0486e+02 - logprior: -1.1741e+00
Epoch 3/10
19/19 - 2s - loss: 196.3785 - loglik: -1.9512e+02 - logprior: -1.2598e+00
Epoch 4/10
19/19 - 2s - loss: 194.0966 - loglik: -1.9290e+02 - logprior: -1.1981e+00
Epoch 5/10
19/19 - 2s - loss: 193.3016 - loglik: -1.9213e+02 - logprior: -1.1680e+00
Epoch 6/10
19/19 - 2s - loss: 193.2040 - loglik: -1.9205e+02 - logprior: -1.1573e+00
Epoch 7/10
19/19 - 2s - loss: 193.1064 - loglik: -1.9195e+02 - logprior: -1.1574e+00
Epoch 8/10
19/19 - 2s - loss: 192.9219 - loglik: -1.9178e+02 - logprior: -1.1424e+00
Epoch 9/10
19/19 - 2s - loss: 192.9380 - loglik: -1.9180e+02 - logprior: -1.1405e+00
Fitted a model with MAP estimate = -186.0445
expansions: [(0, 2), (3, 1), (4, 1), (5, 1), (6, 1), (15, 2), (16, 5), (17, 2), (22, 1), (46, 2), (47, 1), (48, 2), (49, 1), (52, 2), (55, 2), (58, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 193.8251 - loglik: -1.8967e+02 - logprior: -4.1575e+00
Epoch 2/2
19/19 - 2s - loss: 186.1131 - loglik: -1.8460e+02 - logprior: -1.5106e+00
Fitted a model with MAP estimate = -180.0815
expansions: [(22, 1), (23, 1)]
discards: [ 0  1  2 24 25 26 27 28 62 74]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 191.2621 - loglik: -1.8747e+02 - logprior: -3.7936e+00
Epoch 2/2
19/19 - 2s - loss: 187.4214 - loglik: -1.8573e+02 - logprior: -1.6951e+00
Fitted a model with MAP estimate = -181.3230
expansions: [(0, 3), (23, 4), (26, 2)]
discards: [ 0 19 21]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 89 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 181.1985 - loglik: -1.7938e+02 - logprior: -1.8146e+00
Epoch 2/10
23/23 - 2s - loss: 177.9074 - loglik: -1.7694e+02 - logprior: -9.6666e-01
Epoch 3/10
23/23 - 2s - loss: 177.2358 - loglik: -1.7626e+02 - logprior: -9.7853e-01
Epoch 4/10
23/23 - 2s - loss: 176.3671 - loglik: -1.7543e+02 - logprior: -9.3897e-01
Epoch 5/10
23/23 - 2s - loss: 176.3941 - loglik: -1.7547e+02 - logprior: -9.2460e-01
Fitted a model with MAP estimate = -176.1371
Time for alignment: 57.4142
Computed alignments with likelihoods: ['-176.3095', '-177.0687', '-176.1371']
Best model has likelihood: -176.1371
SP score = 0.8456
Training of 3 independent models on file asp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbaf0a2e310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbba4de3580>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 18s - loss: 796.1801 - loglik: -7.9046e+02 - logprior: -5.7194e+00
Epoch 2/10
22/22 - 13s - loss: 684.9174 - loglik: -6.8430e+02 - logprior: -6.1658e-01
Epoch 3/10
22/22 - 13s - loss: 646.2786 - loglik: -6.4470e+02 - logprior: -1.5779e+00
Epoch 4/10
22/22 - 13s - loss: 640.7651 - loglik: -6.3939e+02 - logprior: -1.3798e+00
Epoch 5/10
22/22 - 13s - loss: 638.0067 - loglik: -6.3662e+02 - logprior: -1.3902e+00
Epoch 6/10
22/22 - 13s - loss: 638.8837 - loglik: -6.3746e+02 - logprior: -1.4267e+00
Fitted a model with MAP estimate = -637.3710
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 2), (35, 1), (45, 2), (47, 2), (48, 1), (49, 1), (69, 1), (70, 1), (72, 1), (76, 2), (79, 1), (82, 1), (98, 1), (100, 1), (106, 1), (108, 1), (110, 1), (119, 1), (121, 2), (137, 1), (143, 2), (144, 3), (148, 1), (152, 1), (156, 1), (157, 3), (158, 1), (179, 1), (181, 6), (184, 1), (185, 2), (193, 2), (206, 1), (207, 1), (208, 1), (209, 1), (214, 2), (224, 1), (225, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 317 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 633.3653 - loglik: -6.2501e+02 - logprior: -8.3561e+00
Epoch 2/2
22/22 - 18s - loss: 616.2004 - loglik: -6.1377e+02 - logprior: -2.4299e+00
Fitted a model with MAP estimate = -614.3297
expansions: [(0, 3), (129, 1)]
discards: [  0 175 176 195 196 197 227 228]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 313 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 617.2441 - loglik: -6.1234e+02 - logprior: -4.9051e+00
Epoch 2/2
22/22 - 18s - loss: 614.1898 - loglik: -6.1528e+02 - logprior: 1.0912
Fitted a model with MAP estimate = -610.2771
expansions: [(196, 2)]
discards: [ 0  1  2 36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 311 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 21s - loss: 623.1331 - loglik: -6.1582e+02 - logprior: -7.3099e+00
Epoch 2/10
22/22 - 17s - loss: 615.9001 - loglik: -6.1451e+02 - logprior: -1.3892e+00
Epoch 3/10
22/22 - 17s - loss: 613.1519 - loglik: -6.1368e+02 - logprior: 0.5282
Epoch 4/10
22/22 - 17s - loss: 609.4510 - loglik: -6.1225e+02 - logprior: 2.7993
Epoch 5/10
22/22 - 17s - loss: 611.6841 - loglik: -6.1474e+02 - logprior: 3.0543
Fitted a model with MAP estimate = -608.8423
Time for alignment: 298.4892
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 16s - loss: 798.2027 - loglik: -7.9248e+02 - logprior: -5.7230e+00
Epoch 2/10
22/22 - 13s - loss: 685.6079 - loglik: -6.8512e+02 - logprior: -4.8384e-01
Epoch 3/10
22/22 - 13s - loss: 649.6169 - loglik: -6.4812e+02 - logprior: -1.4968e+00
Epoch 4/10
22/22 - 13s - loss: 637.9828 - loglik: -6.3646e+02 - logprior: -1.5185e+00
Epoch 5/10
22/22 - 13s - loss: 637.7672 - loglik: -6.3626e+02 - logprior: -1.5048e+00
Epoch 6/10
22/22 - 13s - loss: 636.7964 - loglik: -6.3527e+02 - logprior: -1.5313e+00
Epoch 7/10
22/22 - 13s - loss: 635.5314 - loglik: -6.3400e+02 - logprior: -1.5265e+00
Epoch 8/10
22/22 - 13s - loss: 635.7538 - loglik: -6.3424e+02 - logprior: -1.5133e+00
Fitted a model with MAP estimate = -635.8732
expansions: [(14, 1), (15, 1), (32, 1), (33, 2), (34, 3), (36, 1), (45, 3), (48, 1), (49, 1), (66, 1), (70, 1), (71, 1), (76, 1), (77, 2), (78, 1), (79, 1), (81, 1), (83, 1), (100, 1), (104, 1), (105, 2), (107, 1), (109, 1), (120, 2), (143, 2), (144, 3), (148, 1), (149, 1), (156, 1), (157, 3), (158, 1), (177, 1), (178, 3), (179, 1), (180, 1), (183, 2), (184, 1), (185, 1), (193, 4), (205, 1), (206, 1), (209, 1), (212, 1), (213, 2), (214, 1), (225, 1), (236, 1), (238, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 319 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 22s - loss: 632.2133 - loglik: -6.2384e+02 - logprior: -8.3720e+00
Epoch 2/2
22/22 - 18s - loss: 616.1319 - loglik: -6.1358e+02 - logprior: -2.5504e+00
Fitted a model with MAP estimate = -613.3563
expansions: [(0, 3)]
discards: [  0  39  54 176 177 196 197 198 223 249]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 618.6038 - loglik: -6.1376e+02 - logprior: -4.8425e+00
Epoch 2/2
22/22 - 17s - loss: 612.6506 - loglik: -6.1382e+02 - logprior: 1.1690
Fitted a model with MAP estimate = -610.4055
expansions: [(194, 2)]
discards: [  0   1   2 268]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 20s - loss: 622.2581 - loglik: -6.1500e+02 - logprior: -7.2571e+00
Epoch 2/10
22/22 - 17s - loss: 618.7991 - loglik: -6.1748e+02 - logprior: -1.3221e+00
Epoch 3/10
22/22 - 17s - loss: 610.6393 - loglik: -6.1103e+02 - logprior: 0.3946
Epoch 4/10
22/22 - 17s - loss: 611.7505 - loglik: -6.1457e+02 - logprior: 2.8186
Fitted a model with MAP estimate = -609.3770
Time for alignment: 303.2362
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 17s - loss: 797.1505 - loglik: -7.9141e+02 - logprior: -5.7383e+00
Epoch 2/10
22/22 - 13s - loss: 684.3210 - loglik: -6.8377e+02 - logprior: -5.5318e-01
Epoch 3/10
22/22 - 13s - loss: 643.2558 - loglik: -6.4179e+02 - logprior: -1.4623e+00
Epoch 4/10
22/22 - 13s - loss: 641.0471 - loglik: -6.3976e+02 - logprior: -1.2897e+00
Epoch 5/10
22/22 - 13s - loss: 639.4578 - loglik: -6.3817e+02 - logprior: -1.2835e+00
Epoch 6/10
22/22 - 13s - loss: 637.5136 - loglik: -6.3619e+02 - logprior: -1.3211e+00
Epoch 7/10
22/22 - 13s - loss: 635.7478 - loglik: -6.3440e+02 - logprior: -1.3434e+00
Epoch 8/10
22/22 - 13s - loss: 634.8954 - loglik: -6.3357e+02 - logprior: -1.3268e+00
Epoch 9/10
22/22 - 13s - loss: 639.6563 - loglik: -6.3834e+02 - logprior: -1.3187e+00
Fitted a model with MAP estimate = -636.5273
expansions: [(12, 1), (13, 1), (32, 1), (33, 2), (34, 3), (36, 1), (46, 1), (49, 1), (50, 1), (51, 1), (70, 3), (71, 1), (76, 1), (77, 2), (81, 1), (82, 1), (99, 2), (104, 1), (105, 1), (107, 1), (109, 1), (118, 1), (120, 2), (143, 2), (144, 1), (149, 1), (150, 1), (152, 1), (156, 1), (157, 3), (158, 1), (179, 1), (181, 5), (184, 1), (185, 2), (194, 2), (195, 1), (206, 1), (209, 1), (210, 1), (213, 1), (214, 2), (215, 1), (224, 1), (225, 1), (227, 1), (236, 1), (238, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 316 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 635.1643 - loglik: -6.2691e+02 - logprior: -8.2561e+00
Epoch 2/2
22/22 - 18s - loss: 617.5123 - loglik: -6.1501e+02 - logprior: -2.5066e+00
Fitted a model with MAP estimate = -615.5476
expansions: [(0, 3), (2, 1), (102, 1), (118, 1)]
discards: [  0  38  82  99 172 194 195 226]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 314 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 621.5598 - loglik: -6.1681e+02 - logprior: -4.7530e+00
Epoch 2/2
22/22 - 18s - loss: 608.9962 - loglik: -6.1016e+02 - logprior: 1.1657
Fitted a model with MAP estimate = -610.4097
expansions: []
discards: [  0   1   2 269]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 21s - loss: 624.7241 - loglik: -6.1733e+02 - logprior: -7.3904e+00
Epoch 2/10
22/22 - 17s - loss: 614.1240 - loglik: -6.1267e+02 - logprior: -1.4528e+00
Epoch 3/10
22/22 - 17s - loss: 614.0543 - loglik: -6.1433e+02 - logprior: 0.2774
Epoch 4/10
22/22 - 17s - loss: 611.1326 - loglik: -6.1386e+02 - logprior: 2.7314
Epoch 5/10
22/22 - 17s - loss: 608.7774 - loglik: -6.1184e+02 - logprior: 3.0606
Epoch 6/10
22/22 - 17s - loss: 611.6601 - loglik: -6.1494e+02 - logprior: 3.2750
Fitted a model with MAP estimate = -609.5566
Time for alignment: 351.9552
Computed alignments with likelihoods: ['-608.8423', '-609.3770', '-609.5566']
Best model has likelihood: -608.8423
SP score = 0.9048
Training of 3 independent models on file bowman.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fba10b9d550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb7ad42cd90>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 172.2732 - loglik: -7.9883e+01 - logprior: -9.2390e+01
Epoch 2/10
10/10 - 1s - loss: 95.0732 - loglik: -6.8485e+01 - logprior: -2.6588e+01
Epoch 3/10
10/10 - 1s - loss: 73.1524 - loglik: -5.9855e+01 - logprior: -1.3297e+01
Epoch 4/10
10/10 - 1s - loss: 63.9535 - loglik: -5.5772e+01 - logprior: -8.1818e+00
Epoch 5/10
10/10 - 1s - loss: 59.2842 - loglik: -5.3726e+01 - logprior: -5.5577e+00
Epoch 6/10
10/10 - 1s - loss: 57.2416 - loglik: -5.3154e+01 - logprior: -4.0875e+00
Epoch 7/10
10/10 - 1s - loss: 56.1688 - loglik: -5.2942e+01 - logprior: -3.2268e+00
Epoch 8/10
10/10 - 1s - loss: 55.5262 - loglik: -5.2839e+01 - logprior: -2.6874e+00
Epoch 9/10
10/10 - 1s - loss: 55.2001 - loglik: -5.2841e+01 - logprior: -2.3595e+00
Epoch 10/10
10/10 - 1s - loss: 54.9850 - loglik: -5.2855e+01 - logprior: -2.1302e+00
Fitted a model with MAP estimate = -54.8960
expansions: [(0, 3), (13, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 173.2194 - loglik: -4.8881e+01 - logprior: -1.2434e+02
Epoch 2/2
10/10 - 1s - loss: 87.2756 - loglik: -4.7030e+01 - logprior: -4.0245e+01
Fitted a model with MAP estimate = -70.5434
expansions: [(0, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 147.3962 - loglik: -4.4375e+01 - logprior: -1.0302e+02
Epoch 2/2
10/10 - 1s - loss: 80.6388 - loglik: -4.4175e+01 - logprior: -3.6464e+01
Fitted a model with MAP estimate = -66.3779
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 128.1678 - loglik: -4.2412e+01 - logprior: -8.5756e+01
Epoch 2/10
10/10 - 1s - loss: 68.0494 - loglik: -4.3396e+01 - logprior: -2.4653e+01
Epoch 3/10
10/10 - 1s - loss: 56.1623 - loglik: -4.4133e+01 - logprior: -1.2029e+01
Epoch 4/10
10/10 - 1s - loss: 51.6225 - loglik: -4.4737e+01 - logprior: -6.8859e+00
Epoch 5/10
10/10 - 1s - loss: 49.1420 - loglik: -4.5058e+01 - logprior: -4.0838e+00
Epoch 6/10
10/10 - 1s - loss: 47.0478 - loglik: -4.4533e+01 - logprior: -2.5151e+00
Epoch 7/10
10/10 - 1s - loss: 46.0155 - loglik: -4.4401e+01 - logprior: -1.6144e+00
Epoch 8/10
10/10 - 1s - loss: 45.3482 - loglik: -4.4318e+01 - logprior: -1.0305e+00
Epoch 9/10
10/10 - 1s - loss: 44.9331 - loglik: -4.4226e+01 - logprior: -7.0718e-01
Epoch 10/10
10/10 - 1s - loss: 44.6568 - loglik: -4.4189e+01 - logprior: -4.6738e-01
Fitted a model with MAP estimate = -44.5327
Time for alignment: 26.7613
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 172.2732 - loglik: -7.9883e+01 - logprior: -9.2390e+01
Epoch 2/10
10/10 - 1s - loss: 95.0732 - loglik: -6.8485e+01 - logprior: -2.6588e+01
Epoch 3/10
10/10 - 1s - loss: 73.1524 - loglik: -5.9855e+01 - logprior: -1.3297e+01
Epoch 4/10
10/10 - 1s - loss: 63.9535 - loglik: -5.5772e+01 - logprior: -8.1818e+00
Epoch 5/10
10/10 - 1s - loss: 59.2842 - loglik: -5.3726e+01 - logprior: -5.5577e+00
Epoch 6/10
10/10 - 1s - loss: 57.2416 - loglik: -5.3154e+01 - logprior: -4.0875e+00
Epoch 7/10
10/10 - 1s - loss: 56.1688 - loglik: -5.2942e+01 - logprior: -3.2268e+00
Epoch 8/10
10/10 - 1s - loss: 55.5262 - loglik: -5.2839e+01 - logprior: -2.6874e+00
Epoch 9/10
10/10 - 1s - loss: 55.2001 - loglik: -5.2841e+01 - logprior: -2.3595e+00
Epoch 10/10
10/10 - 1s - loss: 54.9850 - loglik: -5.2855e+01 - logprior: -2.1302e+00
Fitted a model with MAP estimate = -54.8960
expansions: [(0, 3), (13, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 173.2194 - loglik: -4.8881e+01 - logprior: -1.2434e+02
Epoch 2/2
10/10 - 1s - loss: 87.2756 - loglik: -4.7030e+01 - logprior: -4.0245e+01
Fitted a model with MAP estimate = -70.5434
expansions: [(0, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 147.3962 - loglik: -4.4375e+01 - logprior: -1.0302e+02
Epoch 2/2
10/10 - 1s - loss: 80.6388 - loglik: -4.4175e+01 - logprior: -3.6464e+01
Fitted a model with MAP estimate = -66.3779
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 128.1678 - loglik: -4.2412e+01 - logprior: -8.5756e+01
Epoch 2/10
10/10 - 1s - loss: 68.0494 - loglik: -4.3396e+01 - logprior: -2.4653e+01
Epoch 3/10
10/10 - 1s - loss: 56.1623 - loglik: -4.4133e+01 - logprior: -1.2029e+01
Epoch 4/10
10/10 - 1s - loss: 51.6225 - loglik: -4.4737e+01 - logprior: -6.8859e+00
Epoch 5/10
10/10 - 1s - loss: 49.1420 - loglik: -4.5058e+01 - logprior: -4.0838e+00
Epoch 6/10
10/10 - 1s - loss: 47.0478 - loglik: -4.4533e+01 - logprior: -2.5151e+00
Epoch 7/10
10/10 - 1s - loss: 46.0155 - loglik: -4.4401e+01 - logprior: -1.6144e+00
Epoch 8/10
10/10 - 1s - loss: 45.3482 - loglik: -4.4318e+01 - logprior: -1.0305e+00
Epoch 9/10
10/10 - 1s - loss: 44.9331 - loglik: -4.4226e+01 - logprior: -7.0718e-01
Epoch 10/10
10/10 - 1s - loss: 44.6568 - loglik: -4.4189e+01 - logprior: -4.6738e-01
Fitted a model with MAP estimate = -44.5327
Time for alignment: 26.3352
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 172.2732 - loglik: -7.9883e+01 - logprior: -9.2390e+01
Epoch 2/10
10/10 - 1s - loss: 95.0732 - loglik: -6.8485e+01 - logprior: -2.6588e+01
Epoch 3/10
10/10 - 1s - loss: 73.1524 - loglik: -5.9855e+01 - logprior: -1.3297e+01
Epoch 4/10
10/10 - 1s - loss: 63.9535 - loglik: -5.5772e+01 - logprior: -8.1818e+00
Epoch 5/10
10/10 - 1s - loss: 59.2842 - loglik: -5.3726e+01 - logprior: -5.5577e+00
Epoch 6/10
10/10 - 1s - loss: 57.2416 - loglik: -5.3154e+01 - logprior: -4.0875e+00
Epoch 7/10
10/10 - 1s - loss: 56.1688 - loglik: -5.2942e+01 - logprior: -3.2268e+00
Epoch 8/10
10/10 - 1s - loss: 55.5262 - loglik: -5.2839e+01 - logprior: -2.6874e+00
Epoch 9/10
10/10 - 0s - loss: 55.2001 - loglik: -5.2841e+01 - logprior: -2.3595e+00
Epoch 10/10
10/10 - 1s - loss: 54.9850 - loglik: -5.2855e+01 - logprior: -2.1302e+00
Fitted a model with MAP estimate = -54.8960
expansions: [(0, 3), (13, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 173.2194 - loglik: -4.8881e+01 - logprior: -1.2434e+02
Epoch 2/2
10/10 - 1s - loss: 87.2756 - loglik: -4.7030e+01 - logprior: -4.0245e+01
Fitted a model with MAP estimate = -70.5434
expansions: [(0, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 147.3962 - loglik: -4.4375e+01 - logprior: -1.0302e+02
Epoch 2/2
10/10 - 1s - loss: 80.6388 - loglik: -4.4175e+01 - logprior: -3.6464e+01
Fitted a model with MAP estimate = -66.3779
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 128.1678 - loglik: -4.2412e+01 - logprior: -8.5756e+01
Epoch 2/10
10/10 - 1s - loss: 68.0494 - loglik: -4.3396e+01 - logprior: -2.4653e+01
Epoch 3/10
10/10 - 1s - loss: 56.1623 - loglik: -4.4133e+01 - logprior: -1.2029e+01
Epoch 4/10
10/10 - 1s - loss: 51.6225 - loglik: -4.4737e+01 - logprior: -6.8859e+00
Epoch 5/10
10/10 - 1s - loss: 49.1420 - loglik: -4.5058e+01 - logprior: -4.0838e+00
Epoch 6/10
10/10 - 1s - loss: 47.0478 - loglik: -4.4533e+01 - logprior: -2.5151e+00
Epoch 7/10
10/10 - 1s - loss: 46.0155 - loglik: -4.4401e+01 - logprior: -1.6144e+00
Epoch 8/10
10/10 - 1s - loss: 45.3482 - loglik: -4.4318e+01 - logprior: -1.0305e+00
Epoch 9/10
10/10 - 1s - loss: 44.9331 - loglik: -4.4226e+01 - logprior: -7.0718e-01
Epoch 10/10
10/10 - 1s - loss: 44.6568 - loglik: -4.4189e+01 - logprior: -4.6738e-01
Fitted a model with MAP estimate = -44.5327
Time for alignment: 27.3742
Computed alignments with likelihoods: ['-44.5327', '-44.5327', '-44.5327']
Best model has likelihood: -44.5327
SP score = 0.9419
Training of 3 independent models on file oxidored_q6.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbba496e6d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb7cc50b880>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 362.7664 - loglik: -3.5046e+02 - logprior: -1.2309e+01
Epoch 2/10
11/11 - 2s - loss: 309.5242 - loglik: -3.0657e+02 - logprior: -2.9580e+00
Epoch 3/10
11/11 - 2s - loss: 268.4225 - loglik: -2.6642e+02 - logprior: -2.0009e+00
Epoch 4/10
11/11 - 2s - loss: 249.1995 - loglik: -2.4705e+02 - logprior: -2.1539e+00
Epoch 5/10
11/11 - 2s - loss: 243.8509 - loglik: -2.4160e+02 - logprior: -2.2493e+00
Epoch 6/10
11/11 - 2s - loss: 239.7185 - loglik: -2.3740e+02 - logprior: -2.3137e+00
Epoch 7/10
11/11 - 2s - loss: 239.0061 - loglik: -2.3673e+02 - logprior: -2.2772e+00
Epoch 8/10
11/11 - 2s - loss: 237.0442 - loglik: -2.3482e+02 - logprior: -2.2200e+00
Epoch 9/10
11/11 - 2s - loss: 238.3941 - loglik: -2.3617e+02 - logprior: -2.2214e+00
Fitted a model with MAP estimate = -237.4245
expansions: [(8, 2), (9, 3), (10, 2), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (63, 2), (65, 1), (66, 1), (67, 2), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 116 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 248.3923 - loglik: -2.3416e+02 - logprior: -1.4233e+01
Epoch 2/2
11/11 - 2s - loss: 225.9026 - loglik: -2.1981e+02 - logprior: -6.0907e+00
Fitted a model with MAP estimate = -221.8011
expansions: [(0, 11)]
discards: [ 0  8 14 77 79 87]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 228.1664 - loglik: -2.1657e+02 - logprior: -1.1595e+01
Epoch 2/2
11/11 - 2s - loss: 213.7722 - loglik: -2.1055e+02 - logprior: -3.2267e+00
Fitted a model with MAP estimate = -212.1385
expansions: []
discards: [1 2 3 4 5 6 7 8 9]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 112 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 223.2283 - loglik: -2.1221e+02 - logprior: -1.1014e+01
Epoch 2/10
11/11 - 2s - loss: 213.3141 - loglik: -2.1060e+02 - logprior: -2.7141e+00
Epoch 3/10
11/11 - 2s - loss: 214.1022 - loglik: -2.1272e+02 - logprior: -1.3787e+00
Fitted a model with MAP estimate = -212.4859
Time for alignment: 55.4233
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 362.6322 - loglik: -3.5032e+02 - logprior: -1.2311e+01
Epoch 2/10
11/11 - 2s - loss: 309.6887 - loglik: -3.0673e+02 - logprior: -2.9555e+00
Epoch 3/10
11/11 - 2s - loss: 268.0243 - loglik: -2.6605e+02 - logprior: -1.9760e+00
Epoch 4/10
11/11 - 2s - loss: 248.0509 - loglik: -2.4591e+02 - logprior: -2.1360e+00
Epoch 5/10
11/11 - 2s - loss: 243.4063 - loglik: -2.4121e+02 - logprior: -2.1926e+00
Epoch 6/10
11/11 - 2s - loss: 240.4125 - loglik: -2.3820e+02 - logprior: -2.2119e+00
Epoch 7/10
11/11 - 2s - loss: 238.5939 - loglik: -2.3641e+02 - logprior: -2.1844e+00
Epoch 8/10
11/11 - 2s - loss: 238.8050 - loglik: -2.3668e+02 - logprior: -2.1235e+00
Fitted a model with MAP estimate = -237.6079
expansions: [(8, 2), (9, 3), (10, 1), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 5), (64, 1), (65, 1), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 247.2694 - loglik: -2.3311e+02 - logprior: -1.4156e+01
Epoch 2/2
11/11 - 2s - loss: 223.3629 - loglik: -2.1744e+02 - logprior: -5.9191e+00
Fitted a model with MAP estimate = -220.1515
expansions: [(4, 1)]
discards: [ 0  8 76 77]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 230.0534 - loglik: -2.1619e+02 - logprior: -1.3858e+01
Epoch 2/2
11/11 - 2s - loss: 218.5786 - loglik: -2.1414e+02 - logprior: -4.4365e+00
Fitted a model with MAP estimate = -215.5414
expansions: [(0, 9)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 119 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 224.3604 - loglik: -2.1305e+02 - logprior: -1.1313e+01
Epoch 2/10
11/11 - 2s - loss: 214.5114 - loglik: -2.1152e+02 - logprior: -2.9884e+00
Epoch 3/10
11/11 - 2s - loss: 209.6171 - loglik: -2.0815e+02 - logprior: -1.4689e+00
Epoch 4/10
11/11 - 3s - loss: 209.2779 - loglik: -2.0818e+02 - logprior: -1.0974e+00
Epoch 5/10
11/11 - 2s - loss: 209.0705 - loglik: -2.0816e+02 - logprior: -9.1341e-01
Epoch 6/10
11/11 - 3s - loss: 208.8861 - loglik: -2.0817e+02 - logprior: -7.2025e-01
Epoch 7/10
11/11 - 2s - loss: 208.3609 - loglik: -2.0775e+02 - logprior: -6.1058e-01
Epoch 8/10
11/11 - 3s - loss: 209.2886 - loglik: -2.0872e+02 - logprior: -5.7043e-01
Fitted a model with MAP estimate = -208.5983
Time for alignment: 67.0702
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 362.4283 - loglik: -3.5012e+02 - logprior: -1.2310e+01
Epoch 2/10
11/11 - 2s - loss: 309.2451 - loglik: -3.0630e+02 - logprior: -2.9479e+00
Epoch 3/10
11/11 - 2s - loss: 270.1957 - loglik: -2.6824e+02 - logprior: -1.9569e+00
Epoch 4/10
11/11 - 2s - loss: 250.5603 - loglik: -2.4846e+02 - logprior: -2.0962e+00
Epoch 5/10
11/11 - 2s - loss: 244.1255 - loglik: -2.4195e+02 - logprior: -2.1748e+00
Epoch 6/10
11/11 - 2s - loss: 241.2391 - loglik: -2.3904e+02 - logprior: -2.1963e+00
Epoch 7/10
11/11 - 2s - loss: 240.0029 - loglik: -2.3788e+02 - logprior: -2.1187e+00
Epoch 8/10
11/11 - 2s - loss: 239.2073 - loglik: -2.3715e+02 - logprior: -2.0568e+00
Epoch 9/10
11/11 - 2s - loss: 237.2395 - loglik: -2.3519e+02 - logprior: -2.0527e+00
Epoch 10/10
11/11 - 2s - loss: 238.5413 - loglik: -2.3649e+02 - logprior: -2.0503e+00
Fitted a model with MAP estimate = -238.1098
expansions: [(8, 2), (9, 3), (10, 1), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (63, 2), (64, 2), (65, 2), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 116 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 248.9118 - loglik: -2.3464e+02 - logprior: -1.4274e+01
Epoch 2/2
11/11 - 2s - loss: 226.2737 - loglik: -2.2014e+02 - logprior: -6.1341e+00
Fitted a model with MAP estimate = -221.4432
expansions: [(0, 10), (4, 1)]
discards: [  0   8  76  78  84 109]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 228.5337 - loglik: -2.1695e+02 - logprior: -1.1584e+01
Epoch 2/2
11/11 - 3s - loss: 214.1987 - loglik: -2.1100e+02 - logprior: -3.2009e+00
Fitted a model with MAP estimate = -211.6342
expansions: []
discards: [0 1 2 3 4 5 6 7 8 9]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 227.1717 - loglik: -2.1437e+02 - logprior: -1.2800e+01
Epoch 2/10
11/11 - 2s - loss: 216.9092 - loglik: -2.1375e+02 - logprior: -3.1636e+00
Epoch 3/10
11/11 - 2s - loss: 213.0728 - loglik: -2.1170e+02 - logprior: -1.3777e+00
Epoch 4/10
11/11 - 2s - loss: 213.7897 - loglik: -2.1288e+02 - logprior: -9.1420e-01
Fitted a model with MAP estimate = -212.8584
Time for alignment: 62.9035
Computed alignments with likelihoods: ['-212.1385', '-208.5983', '-211.6342']
Best model has likelihood: -208.5983
SP score = 0.7078
Training of 3 independent models on file aldosered.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb80c6f1e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb7cc50b880>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 224 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 19s - loss: 810.5276 - loglik: -8.0804e+02 - logprior: -2.4899e+00
Epoch 2/10
19/19 - 16s - loss: 705.3101 - loglik: -7.0439e+02 - logprior: -9.1732e-01
Epoch 3/10
19/19 - 16s - loss: 642.2616 - loglik: -6.4072e+02 - logprior: -1.5377e+00
Epoch 4/10
19/19 - 16s - loss: 630.1330 - loglik: -6.2848e+02 - logprior: -1.6561e+00
Epoch 5/10
19/19 - 15s - loss: 626.1875 - loglik: -6.2450e+02 - logprior: -1.6887e+00
Epoch 6/10
19/19 - 15s - loss: 625.3663 - loglik: -6.2370e+02 - logprior: -1.6687e+00
Epoch 7/10
19/19 - 16s - loss: 624.5002 - loglik: -6.2284e+02 - logprior: -1.6626e+00
Epoch 8/10
19/19 - 16s - loss: 623.9034 - loglik: -6.2225e+02 - logprior: -1.6530e+00
Epoch 9/10
19/19 - 15s - loss: 623.5119 - loglik: -6.2187e+02 - logprior: -1.6458e+00
Epoch 10/10
19/19 - 16s - loss: 623.6595 - loglik: -6.2202e+02 - logprior: -1.6444e+00
Fitted a model with MAP estimate = -581.4570
expansions: [(12, 3), (14, 1), (16, 1), (20, 1), (36, 1), (37, 1), (39, 1), (46, 3), (47, 2), (59, 2), (60, 1), (62, 4), (67, 2), (68, 2), (69, 2), (70, 1), (120, 1), (125, 2), (126, 5), (128, 1), (129, 1), (138, 3), (139, 1), (141, 1), (142, 1), (143, 1), (144, 1), (159, 1), (160, 5), (161, 1), (164, 1), (165, 2), (166, 1), (167, 2), (168, 3), (169, 2), (170, 2), (179, 1), (180, 2), (181, 1), (193, 1), (194, 1), (206, 1), (209, 1), (211, 2), (212, 2), (217, 1), (218, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 303 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 609.2615 - loglik: -6.0641e+02 - logprior: -2.8470e+00
Epoch 2/2
39/39 - 26s - loss: 592.8542 - loglik: -5.9161e+02 - logprior: -1.2419e+00
Fitted a model with MAP estimate = -552.4072
expansions: [(0, 2), (154, 2), (209, 1)]
discards: [  0  11  56  58  78  87  93 157 158 159 176 219 246 285 287]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 293 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 593.4847 - loglik: -5.9190e+02 - logprior: -1.5820e+00
Epoch 2/2
39/39 - 24s - loss: 591.1453 - loglik: -5.9051e+02 - logprior: -6.3208e-01
Fitted a model with MAP estimate = -552.8058
expansions: []
discards: [  0 150]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 291 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 31s - loss: 550.0315 - loglik: -5.4837e+02 - logprior: -1.6606e+00
Epoch 2/10
45/45 - 28s - loss: 548.9331 - loglik: -5.4842e+02 - logprior: -5.1029e-01
Epoch 3/10
45/45 - 28s - loss: 544.9798 - loglik: -5.4451e+02 - logprior: -4.6864e-01
Epoch 4/10
45/45 - 28s - loss: 544.8176 - loglik: -5.4446e+02 - logprior: -3.5605e-01
Epoch 5/10
45/45 - 27s - loss: 545.4684 - loglik: -5.4520e+02 - logprior: -2.6818e-01
Fitted a model with MAP estimate = -544.7406
Time for alignment: 541.1165
Fitting a model of length 224 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 18s - loss: 811.0644 - loglik: -8.0857e+02 - logprior: -2.4906e+00
Epoch 2/10
19/19 - 16s - loss: 703.4119 - loglik: -7.0248e+02 - logprior: -9.3215e-01
Epoch 3/10
19/19 - 16s - loss: 643.9295 - loglik: -6.4237e+02 - logprior: -1.5568e+00
Epoch 4/10
19/19 - 15s - loss: 629.1965 - loglik: -6.2745e+02 - logprior: -1.7481e+00
Epoch 5/10
19/19 - 15s - loss: 626.2227 - loglik: -6.2440e+02 - logprior: -1.8265e+00
Epoch 6/10
19/19 - 16s - loss: 624.3101 - loglik: -6.2252e+02 - logprior: -1.7854e+00
Epoch 7/10
19/19 - 15s - loss: 623.4656 - loglik: -6.2168e+02 - logprior: -1.7830e+00
Epoch 8/10
19/19 - 16s - loss: 623.2576 - loglik: -6.2148e+02 - logprior: -1.7734e+00
Epoch 9/10
19/19 - 15s - loss: 623.3934 - loglik: -6.2161e+02 - logprior: -1.7812e+00
Fitted a model with MAP estimate = -580.2745
expansions: [(12, 4), (14, 1), (16, 1), (20, 1), (36, 1), (37, 1), (39, 1), (42, 1), (45, 2), (46, 1), (59, 3), (62, 3), (65, 1), (66, 2), (67, 2), (70, 1), (90, 1), (97, 2), (118, 1), (120, 1), (122, 3), (123, 5), (124, 2), (125, 2), (127, 1), (133, 1), (135, 2), (137, 1), (138, 1), (141, 1), (145, 1), (151, 1), (159, 2), (165, 3), (167, 1), (169, 3), (170, 3), (181, 2), (182, 1), (191, 1), (193, 1), (203, 1), (210, 1), (212, 2), (213, 2), (215, 1), (217, 1), (218, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 301 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 607.4251 - loglik: -6.0451e+02 - logprior: -2.9156e+00
Epoch 2/2
39/39 - 26s - loss: 593.4575 - loglik: -5.9222e+02 - logprior: -1.2374e+00
Fitted a model with MAP estimate = -553.5730
expansions: [(0, 2), (209, 1), (227, 1), (228, 1)]
discards: [  0  12  58  77  86 123 157 158 162 179 212 213 214 244 283 285]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 290 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 594.9414 - loglik: -5.9327e+02 - logprior: -1.6728e+00
Epoch 2/2
39/39 - 24s - loss: 592.2982 - loglik: -5.9169e+02 - logprior: -6.1287e-01
Fitted a model with MAP estimate = -554.1392
expansions: [(73, 1), (204, 2), (205, 1), (206, 1)]
discards: [  0  13 199 200 201]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 290 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 30s - loss: 553.0211 - loglik: -5.5134e+02 - logprior: -1.6856e+00
Epoch 2/10
45/45 - 27s - loss: 548.7724 - loglik: -5.4822e+02 - logprior: -5.5117e-01
Epoch 3/10
45/45 - 28s - loss: 547.0765 - loglik: -5.4663e+02 - logprior: -4.4254e-01
Epoch 4/10
45/45 - 27s - loss: 545.8862 - loglik: -5.4548e+02 - logprior: -4.0624e-01
Epoch 5/10
45/45 - 27s - loss: 548.1063 - loglik: -5.4781e+02 - logprior: -2.9917e-01
Fitted a model with MAP estimate = -545.8093
Time for alignment: 519.3654
Fitting a model of length 224 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 19s - loss: 809.7951 - loglik: -8.0730e+02 - logprior: -2.4920e+00
Epoch 2/10
19/19 - 16s - loss: 703.7350 - loglik: -7.0281e+02 - logprior: -9.2706e-01
Epoch 3/10
19/19 - 15s - loss: 641.4240 - loglik: -6.3984e+02 - logprior: -1.5822e+00
Epoch 4/10
19/19 - 16s - loss: 627.6826 - loglik: -6.2595e+02 - logprior: -1.7350e+00
Epoch 5/10
19/19 - 16s - loss: 623.9769 - loglik: -6.2223e+02 - logprior: -1.7519e+00
Epoch 6/10
19/19 - 16s - loss: 623.3682 - loglik: -6.2168e+02 - logprior: -1.6916e+00
Epoch 7/10
19/19 - 16s - loss: 622.2726 - loglik: -6.2060e+02 - logprior: -1.6710e+00
Epoch 8/10
19/19 - 15s - loss: 622.2781 - loglik: -6.2061e+02 - logprior: -1.6647e+00
Fitted a model with MAP estimate = -579.4531
expansions: [(12, 4), (14, 1), (16, 1), (20, 1), (36, 1), (37, 1), (43, 1), (46, 2), (47, 1), (59, 2), (60, 2), (62, 4), (67, 2), (68, 2), (69, 2), (91, 1), (98, 2), (119, 1), (124, 2), (125, 1), (126, 4), (127, 1), (138, 2), (140, 1), (142, 1), (143, 1), (144, 1), (145, 1), (147, 2), (148, 1), (160, 1), (162, 2), (163, 1), (164, 1), (165, 2), (166, 2), (167, 1), (168, 2), (169, 8), (180, 2), (181, 1), (193, 1), (194, 1), (206, 1), (209, 1), (211, 2), (212, 2), (217, 1), (218, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 305 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 610.6525 - loglik: -6.0778e+02 - logprior: -2.8765e+00
Epoch 2/2
39/39 - 26s - loss: 594.1045 - loglik: -5.9283e+02 - logprior: -1.2722e+00
Fitted a model with MAP estimate = -554.3541
expansions: [(0, 2), (155, 1), (210, 1), (232, 1)]
discards: [  0  11  12  58  78  88  93 125 160 177 192 220 221 228 248 287 288]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 293 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 593.8813 - loglik: -5.9218e+02 - logprior: -1.7034e+00
Epoch 2/2
39/39 - 25s - loss: 591.2651 - loglik: -5.9064e+02 - logprior: -6.2336e-01
Fitted a model with MAP estimate = -553.0816
expansions: [(221, 1)]
discards: [  0  71 151 152 153 200 201 202]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 286 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 29s - loss: 554.1644 - loglik: -5.5248e+02 - logprior: -1.6837e+00
Epoch 2/10
45/45 - 27s - loss: 548.6162 - loglik: -5.4808e+02 - logprior: -5.4074e-01
Epoch 3/10
45/45 - 27s - loss: 550.0444 - loglik: -5.4957e+02 - logprior: -4.7520e-01
Fitted a model with MAP estimate = -548.0105
Time for alignment: 454.3337
Computed alignments with likelihoods: ['-544.7406', '-545.8093', '-548.0105']
Best model has likelihood: -544.7406
SP score = 0.9001
Training of 3 independent models on file sdr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9f801fd60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c2b0a4c0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 9s - loss: 445.5394 - loglik: -4.4449e+02 - logprior: -1.0482e+00
Epoch 2/10
30/30 - 7s - loss: 374.1504 - loglik: -3.7303e+02 - logprior: -1.1202e+00
Epoch 3/10
30/30 - 7s - loss: 362.5346 - loglik: -3.6142e+02 - logprior: -1.1123e+00
Epoch 4/10
30/30 - 7s - loss: 360.0502 - loglik: -3.5893e+02 - logprior: -1.1179e+00
Epoch 5/10
30/30 - 7s - loss: 359.8952 - loglik: -3.5880e+02 - logprior: -1.0983e+00
Epoch 6/10
30/30 - 7s - loss: 359.1252 - loglik: -3.5804e+02 - logprior: -1.0897e+00
Epoch 7/10
30/30 - 7s - loss: 358.6610 - loglik: -3.5758e+02 - logprior: -1.0834e+00
Epoch 8/10
30/30 - 7s - loss: 358.5080 - loglik: -3.5743e+02 - logprior: -1.0811e+00
Epoch 9/10
30/30 - 7s - loss: 359.0591 - loglik: -3.5798e+02 - logprior: -1.0793e+00
Fitted a model with MAP estimate = -348.6939
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 2), (48, 1), (51, 1), (52, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (92, 1), (93, 1), (94, 1), (97, 2), (99, 1), (111, 1), (113, 2), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 14s - loss: 354.0449 - loglik: -3.5256e+02 - logprior: -1.4887e+00
Epoch 2/2
30/30 - 10s - loss: 344.4992 - loglik: -3.4352e+02 - logprior: -9.7527e-01
Fitted a model with MAP estimate = -336.5057
expansions: []
discards: [ 25  48  51  57  93  95 127 148 155]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 12s - loss: 344.6315 - loglik: -3.4334e+02 - logprior: -1.2888e+00
Epoch 2/2
30/30 - 9s - loss: 342.2822 - loglik: -3.4136e+02 - logprior: -9.1839e-01
Fitted a model with MAP estimate = -336.9091
expansions: []
discards: [143]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 50157 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 15s - loss: 333.9455 - loglik: -3.3313e+02 - logprior: -8.1873e-01
Epoch 2/10
43/43 - 12s - loss: 332.1440 - loglik: -3.3150e+02 - logprior: -6.4602e-01
Epoch 3/10
43/43 - 13s - loss: 331.2006 - loglik: -3.3058e+02 - logprior: -6.2220e-01
Epoch 4/10
43/43 - 13s - loss: 330.8876 - loglik: -3.3028e+02 - logprior: -6.0947e-01
Epoch 5/10
43/43 - 12s - loss: 330.5612 - loglik: -3.2996e+02 - logprior: -6.0043e-01
Epoch 6/10
43/43 - 12s - loss: 330.4231 - loglik: -3.2983e+02 - logprior: -5.9697e-01
Epoch 7/10
43/43 - 13s - loss: 328.7062 - loglik: -3.2812e+02 - logprior: -5.8562e-01
Epoch 8/10
43/43 - 13s - loss: 329.4113 - loglik: -3.2883e+02 - logprior: -5.7809e-01
Fitted a model with MAP estimate = -328.9831
Time for alignment: 325.9164
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 11s - loss: 446.2925 - loglik: -4.4524e+02 - logprior: -1.0564e+00
Epoch 2/10
30/30 - 7s - loss: 374.0702 - loglik: -3.7294e+02 - logprior: -1.1280e+00
Epoch 3/10
30/30 - 7s - loss: 363.1458 - loglik: -3.6204e+02 - logprior: -1.1016e+00
Epoch 4/10
30/30 - 7s - loss: 360.6599 - loglik: -3.5955e+02 - logprior: -1.1081e+00
Epoch 5/10
30/30 - 7s - loss: 359.9326 - loglik: -3.5885e+02 - logprior: -1.0828e+00
Epoch 6/10
30/30 - 7s - loss: 359.3365 - loglik: -3.5826e+02 - logprior: -1.0721e+00
Epoch 7/10
30/30 - 7s - loss: 358.4134 - loglik: -3.5735e+02 - logprior: -1.0634e+00
Epoch 8/10
30/30 - 7s - loss: 358.8369 - loglik: -3.5778e+02 - logprior: -1.0604e+00
Fitted a model with MAP estimate = -348.5220
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (54, 1), (56, 1), (72, 2), (73, 2), (75, 1), (78, 2), (82, 1), (88, 1), (93, 1), (94, 1), (97, 2), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 179 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 12s - loss: 353.5446 - loglik: -3.5206e+02 - logprior: -1.4835e+00
Epoch 2/2
30/30 - 10s - loss: 345.0067 - loglik: -3.4403e+02 - logprior: -9.7265e-01
Fitted a model with MAP estimate = -336.5759
expansions: []
discards: [ 25  48  51  92  94 102 127 134 148 152 155]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 12s - loss: 344.4510 - loglik: -3.4318e+02 - logprior: -1.2751e+00
Epoch 2/2
30/30 - 9s - loss: 342.5197 - loglik: -3.4160e+02 - logprior: -9.1942e-01
Fitted a model with MAP estimate = -336.7392
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 50157 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 16s - loss: 333.7018 - loglik: -3.3288e+02 - logprior: -8.2151e-01
Epoch 2/10
43/43 - 12s - loss: 331.9353 - loglik: -3.3129e+02 - logprior: -6.4074e-01
Epoch 3/10
43/43 - 12s - loss: 331.1817 - loglik: -3.3056e+02 - logprior: -6.2364e-01
Epoch 4/10
43/43 - 13s - loss: 331.3091 - loglik: -3.3070e+02 - logprior: -6.1305e-01
Fitted a model with MAP estimate = -330.1420
Time for alignment: 268.4955
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 10s - loss: 446.3405 - loglik: -4.4530e+02 - logprior: -1.0417e+00
Epoch 2/10
30/30 - 7s - loss: 375.7622 - loglik: -3.7466e+02 - logprior: -1.0983e+00
Epoch 3/10
30/30 - 7s - loss: 364.4504 - loglik: -3.6337e+02 - logprior: -1.0766e+00
Epoch 4/10
30/30 - 7s - loss: 362.1475 - loglik: -3.6107e+02 - logprior: -1.0814e+00
Epoch 5/10
30/30 - 7s - loss: 361.8346 - loglik: -3.6077e+02 - logprior: -1.0611e+00
Epoch 6/10
30/30 - 7s - loss: 360.7735 - loglik: -3.5972e+02 - logprior: -1.0499e+00
Epoch 7/10
30/30 - 7s - loss: 360.7426 - loglik: -3.5970e+02 - logprior: -1.0433e+00
Epoch 8/10
30/30 - 7s - loss: 360.6610 - loglik: -3.5962e+02 - logprior: -1.0380e+00
Epoch 9/10
30/30 - 7s - loss: 360.2167 - loglik: -3.5918e+02 - logprior: -1.0381e+00
Epoch 10/10
30/30 - 7s - loss: 360.4473 - loglik: -3.5941e+02 - logprior: -1.0340e+00
Fitted a model with MAP estimate = -350.5377
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 2), (48, 1), (51, 1), (52, 1), (56, 1), (71, 2), (72, 2), (75, 1), (76, 2), (82, 1), (88, 1), (93, 1), (94, 1), (97, 2), (102, 2), (113, 3), (114, 2), (116, 2), (126, 1), (127, 1), (128, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 180 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 12s - loss: 354.1240 - loglik: -3.5263e+02 - logprior: -1.4917e+00
Epoch 2/2
30/30 - 10s - loss: 344.6002 - loglik: -3.4361e+02 - logprior: -9.9319e-01
Fitted a model with MAP estimate = -336.3246
expansions: []
discards: [ 25  48  51  57 101 128 135 150 156]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 12s - loss: 344.2041 - loglik: -3.4291e+02 - logprior: -1.2933e+00
Epoch 2/2
30/30 - 9s - loss: 342.7045 - loglik: -3.4176e+02 - logprior: -9.4018e-01
Fitted a model with MAP estimate = -336.6634
expansions: []
discards: [88 90]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 50157 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 15s - loss: 333.2228 - loglik: -3.3239e+02 - logprior: -8.3069e-01
Epoch 2/10
43/43 - 13s - loss: 332.5488 - loglik: -3.3190e+02 - logprior: -6.4661e-01
Epoch 3/10
43/43 - 13s - loss: 331.6006 - loglik: -3.3097e+02 - logprior: -6.2639e-01
Epoch 4/10
43/43 - 13s - loss: 331.0394 - loglik: -3.3042e+02 - logprior: -6.1800e-01
Epoch 5/10
43/43 - 13s - loss: 330.2329 - loglik: -3.2963e+02 - logprior: -6.0522e-01
Epoch 6/10
43/43 - 13s - loss: 329.5390 - loglik: -3.2894e+02 - logprior: -5.9832e-01
Epoch 7/10
43/43 - 12s - loss: 329.6662 - loglik: -3.2908e+02 - logprior: -5.9055e-01
Fitted a model with MAP estimate = -329.0719
Time for alignment: 320.7696
Computed alignments with likelihoods: ['-328.9831', '-330.1420', '-329.0719']
Best model has likelihood: -328.9831
SP score = 0.6777
Training of 3 independent models on file p450.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c015d790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cc2ad220>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 44s - loss: 1243.6143 - loglik: -1.2426e+03 - logprior: -9.8418e-01
Epoch 2/10
40/40 - 41s - loss: 1144.9164 - loglik: -1.1451e+03 - logprior: 0.2115
Epoch 3/10
40/40 - 41s - loss: 1137.0947 - loglik: -1.1374e+03 - logprior: 0.3100
Epoch 4/10
40/40 - 41s - loss: 1131.3546 - loglik: -1.1317e+03 - logprior: 0.3100
Epoch 5/10
40/40 - 41s - loss: 1125.0728 - loglik: -1.1253e+03 - logprior: 0.2599
Epoch 6/10
40/40 - 41s - loss: 1122.3092 - loglik: -1.1225e+03 - logprior: 0.1778
Epoch 7/10
40/40 - 41s - loss: 1120.5663 - loglik: -1.1208e+03 - logprior: 0.1923
Epoch 8/10
40/40 - 41s - loss: 1119.5042 - loglik: -1.1196e+03 - logprior: 0.1393
Epoch 9/10
40/40 - 41s - loss: 1119.1228 - loglik: -1.1193e+03 - logprior: 0.1289
Epoch 10/10
40/40 - 41s - loss: 1118.2200 - loglik: -1.1183e+03 - logprior: 0.1061
Fitted a model with MAP estimate = -852.1984
expansions: [(25, 1), (123, 1), (124, 1), (201, 1), (297, 2), (299, 1), (327, 45), (330, 74)]
discards: [  1 240]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 454 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 72s - loss: 1060.6808 - loglik: -1.0590e+03 - logprior: -1.6497e+00
Epoch 2/2
40/40 - 69s - loss: 1020.9382 - loglik: -1.0204e+03 - logprior: -5.1620e-01
Fitted a model with MAP estimate = -796.8790
expansions: [(442, 1), (454, 2)]
discards: [299 351 352 353 354]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 452 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 72s - loss: 1018.7588 - loglik: -1.0172e+03 - logprior: -1.5336e+00
Epoch 2/2
40/40 - 69s - loss: 1017.8176 - loglik: -1.0177e+03 - logprior: -7.0979e-02
Fitted a model with MAP estimate = -797.6324
expansions: []
discards: [450 451]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 450 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 97s - loss: 780.0936 - loglik: -7.7976e+02 - logprior: -3.3838e-01
Epoch 2/10
56/56 - 94s - loss: 762.3300 - loglik: -7.6244e+02 - logprior: 0.1068
Epoch 3/10
56/56 - 95s - loss: 765.2529 - loglik: -7.6535e+02 - logprior: 0.0995
Fitted a model with MAP estimate = -756.4345
Time for alignment: 1406.9433
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 44s - loss: 1246.0649 - loglik: -1.2450e+03 - logprior: -1.0595e+00
Epoch 2/10
40/40 - 41s - loss: 1148.0079 - loglik: -1.1481e+03 - logprior: 0.1243
Epoch 3/10
40/40 - 41s - loss: 1140.2853 - loglik: -1.1405e+03 - logprior: 0.1869
Epoch 4/10
40/40 - 41s - loss: 1134.2966 - loglik: -1.1345e+03 - logprior: 0.1922
Epoch 5/10
40/40 - 41s - loss: 1128.6145 - loglik: -1.1288e+03 - logprior: 0.1533
Epoch 6/10
40/40 - 41s - loss: 1124.8218 - loglik: -1.1249e+03 - logprior: 0.1063
Epoch 7/10
40/40 - 41s - loss: 1123.1949 - loglik: -1.1233e+03 - logprior: 0.0848
Epoch 8/10
40/40 - 41s - loss: 1122.1803 - loglik: -1.1222e+03 - logprior: 0.0368
Epoch 9/10
40/40 - 41s - loss: 1121.6879 - loglik: -1.1217e+03 - logprior: 0.0150
Epoch 10/10
40/40 - 41s - loss: 1120.9268 - loglik: -1.1209e+03 - logprior: -5.7930e-03
Fitted a model with MAP estimate = -854.3380
expansions: [(124, 1), (125, 1), (169, 1), (304, 1), (306, 1), (307, 1), (308, 2), (328, 2), (329, 1), (330, 119)]
discards: [  1 224 225 280 281 282]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 454 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 72s - loss: 1074.9436 - loglik: -1.0730e+03 - logprior: -1.9321e+00
Epoch 2/2
40/40 - 69s - loss: 1028.6206 - loglik: -1.0274e+03 - logprior: -1.1836e+00
Fitted a model with MAP estimate = -803.0537
expansions: [(337, 1), (345, 2), (350, 2), (354, 1), (362, 1), (377, 2), (378, 5), (398, 1), (410, 1), (418, 2)]
discards: [308 425 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451
 452]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 453 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 72s - loss: 1022.1755 - loglik: -1.0205e+03 - logprior: -1.6564e+00
Epoch 2/2
40/40 - 68s - loss: 1018.6010 - loglik: -1.0185e+03 - logprior: -7.0804e-02
Fitted a model with MAP estimate = -800.0321
expansions: [(452, 1), (453, 2)]
discards: [241 332 345 352 388 389]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 450 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 98s - loss: 781.0283 - loglik: -7.8061e+02 - logprior: -4.1755e-01
Epoch 2/10
56/56 - 94s - loss: 769.4584 - loglik: -7.6936e+02 - logprior: -1.0242e-01
Epoch 3/10
56/56 - 95s - loss: 760.2823 - loglik: -7.6041e+02 - logprior: 0.1285
Epoch 4/10
56/56 - 94s - loss: 755.9245 - loglik: -7.5609e+02 - logprior: 0.1610
Epoch 5/10
56/56 - 95s - loss: 757.9315 - loglik: -7.5807e+02 - logprior: 0.1377
Fitted a model with MAP estimate = -751.7353
Time for alignment: 1597.9562
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 44s - loss: 1246.0337 - loglik: -1.2450e+03 - logprior: -1.0263e+00
Epoch 2/10
40/40 - 41s - loss: 1142.0631 - loglik: -1.1421e+03 - logprior: 0.0264
Epoch 3/10
40/40 - 41s - loss: 1133.6326 - loglik: -1.1337e+03 - logprior: 0.1097
Epoch 4/10
40/40 - 41s - loss: 1127.5507 - loglik: -1.1277e+03 - logprior: 0.1161
Epoch 5/10
40/40 - 41s - loss: 1122.5840 - loglik: -1.1227e+03 - logprior: 0.0853
Epoch 6/10
40/40 - 41s - loss: 1119.6271 - loglik: -1.1197e+03 - logprior: 0.0472
Epoch 7/10
40/40 - 41s - loss: 1117.9152 - loglik: -1.1179e+03 - logprior: 0.0050
Epoch 8/10
40/40 - 41s - loss: 1118.5577 - loglik: -1.1185e+03 - logprior: -2.1983e-02
Fitted a model with MAP estimate = -846.8625
expansions: [(118, 2), (123, 1), (124, 1), (201, 1), (277, 1), (290, 2), (291, 2), (292, 2), (293, 6), (294, 2), (295, 3), (299, 1), (317, 2), (318, 2), (319, 6), (329, 10), (330, 88)]
discards: [  1 224]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 460 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 75s - loss: 1065.4249 - loglik: -1.0638e+03 - logprior: -1.6436e+00
Epoch 2/2
40/40 - 71s - loss: 1021.5468 - loglik: -1.0210e+03 - logprior: -5.6760e-01
Fitted a model with MAP estimate = -794.2800
expansions: [(456, 1), (460, 2)]
discards: [117 297 298 304 305 310 311 340 345 346 394]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 452 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 71s - loss: 1018.7087 - loglik: -1.0173e+03 - logprior: -1.4180e+00
Epoch 2/2
40/40 - 69s - loss: 1018.9406 - loglik: -1.0187e+03 - logprior: -2.4371e-01
Fitted a model with MAP estimate = -796.0433
expansions: []
discards: [384 450 451]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 449 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 97s - loss: 778.7521 - loglik: -7.7840e+02 - logprior: -3.5625e-01
Epoch 2/10
56/56 - 94s - loss: 765.0516 - loglik: -7.6508e+02 - logprior: 0.0248
Epoch 3/10
56/56 - 94s - loss: 760.8419 - loglik: -7.6108e+02 - logprior: 0.2375
Epoch 4/10
56/56 - 94s - loss: 756.4248 - loglik: -7.5664e+02 - logprior: 0.2128
Epoch 5/10
56/56 - 94s - loss: 754.3107 - loglik: -7.5466e+02 - logprior: 0.3510
Epoch 6/10
56/56 - 94s - loss: 751.7753 - loglik: -7.5205e+02 - logprior: 0.2768
Epoch 7/10
56/56 - 94s - loss: 750.2205 - loglik: -7.5087e+02 - logprior: 0.6536
Epoch 8/10
56/56 - 94s - loss: 749.8674 - loglik: -7.5068e+02 - logprior: 0.8133
Epoch 9/10
56/56 - 94s - loss: 750.8655 - loglik: -7.5149e+02 - logprior: 0.6292
Fitted a model with MAP estimate = -749.0999
Time for alignment: 1893.2785
Computed alignments with likelihoods: ['-756.4345', '-751.7353', '-749.0999']
Best model has likelihood: -749.0999
SP score = 0.7387
Training of 3 independent models on file hla.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c0b96790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c2afbe20>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 449.1759 - loglik: -4.4628e+02 - logprior: -2.8949e+00
Epoch 2/10
19/19 - 4s - loss: 279.9004 - loglik: -2.7843e+02 - logprior: -1.4734e+00
Epoch 3/10
19/19 - 4s - loss: 210.9042 - loglik: -2.0913e+02 - logprior: -1.7711e+00
Epoch 4/10
19/19 - 4s - loss: 201.5043 - loglik: -1.9958e+02 - logprior: -1.9252e+00
Epoch 5/10
19/19 - 4s - loss: 199.8766 - loglik: -1.9802e+02 - logprior: -1.8524e+00
Epoch 6/10
19/19 - 4s - loss: 195.9457 - loglik: -1.9402e+02 - logprior: -1.9271e+00
Epoch 7/10
19/19 - 4s - loss: 194.9154 - loglik: -1.9280e+02 - logprior: -2.1175e+00
Epoch 8/10
19/19 - 4s - loss: 194.1914 - loglik: -1.9201e+02 - logprior: -2.1852e+00
Epoch 9/10
19/19 - 4s - loss: 192.8642 - loglik: -1.9068e+02 - logprior: -2.1838e+00
Epoch 10/10
19/19 - 4s - loss: 192.9745 - loglik: -1.9079e+02 - logprior: -2.1848e+00
Fitted a model with MAP estimate = -187.9298
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 2), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 2), (102, 2), (111, 2), (114, 1), (121, 2), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 187 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 170.6644 - loglik: -1.6747e+02 - logprior: -3.1927e+00
Epoch 2/2
19/19 - 6s - loss: 134.8955 - loglik: -1.3337e+02 - logprior: -1.5213e+00
Fitted a model with MAP estimate = -140.0626
expansions: []
discards: [ 50  70  77 127 130 141 155 157 162]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 134.0320 - loglik: -1.3097e+02 - logprior: -3.0633e+00
Epoch 2/2
19/19 - 5s - loss: 130.4331 - loglik: -1.2920e+02 - logprior: -1.2334e+00
Fitted a model with MAP estimate = -139.5095
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 139.8279 - loglik: -1.3791e+02 - logprior: -1.9172e+00
Epoch 2/10
22/22 - 6s - loss: 133.0661 - loglik: -1.3220e+02 - logprior: -8.6543e-01
Epoch 3/10
22/22 - 6s - loss: 133.2989 - loglik: -1.3233e+02 - logprior: -9.6785e-01
Fitted a model with MAP estimate = -131.8731
Time for alignment: 135.2405
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 449.2303 - loglik: -4.4635e+02 - logprior: -2.8840e+00
Epoch 2/10
19/19 - 4s - loss: 280.1765 - loglik: -2.7876e+02 - logprior: -1.4164e+00
Epoch 3/10
19/19 - 4s - loss: 212.6456 - loglik: -2.1092e+02 - logprior: -1.7266e+00
Epoch 4/10
19/19 - 4s - loss: 203.1739 - loglik: -2.0129e+02 - logprior: -1.8843e+00
Epoch 5/10
19/19 - 4s - loss: 198.7587 - loglik: -1.9694e+02 - logprior: -1.8197e+00
Epoch 6/10
19/19 - 4s - loss: 196.4704 - loglik: -1.9464e+02 - logprior: -1.8295e+00
Epoch 7/10
19/19 - 4s - loss: 195.1530 - loglik: -1.9323e+02 - logprior: -1.9213e+00
Epoch 8/10
19/19 - 4s - loss: 194.6489 - loglik: -1.9266e+02 - logprior: -1.9857e+00
Epoch 9/10
19/19 - 4s - loss: 193.0973 - loglik: -1.9111e+02 - logprior: -1.9859e+00
Epoch 10/10
19/19 - 4s - loss: 194.3366 - loglik: -1.9235e+02 - logprior: -1.9905e+00
Fitted a model with MAP estimate = -187.9681
expansions: [(0, 2), (6, 1), (7, 1), (13, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 2), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 2), (102, 2), (111, 2), (114, 1), (121, 2), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 188 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 172.0822 - loglik: -1.6782e+02 - logprior: -4.2645e+00
Epoch 2/2
19/19 - 6s - loss: 133.7890 - loglik: -1.3225e+02 - logprior: -1.5382e+00
Fitted a model with MAP estimate = -138.9131
expansions: []
discards: [  0  51  71  78 128 131 142 156 158 163]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 136.5410 - loglik: -1.3246e+02 - logprior: -4.0775e+00
Epoch 2/2
19/19 - 5s - loss: 131.8666 - loglik: -1.3031e+02 - logprior: -1.5583e+00
Fitted a model with MAP estimate = -138.9274
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 139.3721 - loglik: -1.3740e+02 - logprior: -1.9737e+00
Epoch 2/10
22/22 - 6s - loss: 134.1280 - loglik: -1.3321e+02 - logprior: -9.1374e-01
Epoch 3/10
22/22 - 6s - loss: 132.5991 - loglik: -1.3165e+02 - logprior: -9.5374e-01
Epoch 4/10
22/22 - 6s - loss: 131.6375 - loglik: -1.3067e+02 - logprior: -9.6953e-01
Epoch 5/10
22/22 - 6s - loss: 130.9576 - loglik: -1.3000e+02 - logprior: -9.5668e-01
Epoch 6/10
22/22 - 6s - loss: 130.3455 - loglik: -1.2942e+02 - logprior: -9.2462e-01
Epoch 7/10
22/22 - 6s - loss: 130.8761 - loglik: -1.2998e+02 - logprior: -8.9225e-01
Fitted a model with MAP estimate = -129.9280
Time for alignment: 158.1276
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 449.4675 - loglik: -4.4658e+02 - logprior: -2.8916e+00
Epoch 2/10
19/19 - 4s - loss: 283.2211 - loglik: -2.8175e+02 - logprior: -1.4729e+00
Epoch 3/10
19/19 - 4s - loss: 213.4807 - loglik: -2.1171e+02 - logprior: -1.7744e+00
Epoch 4/10
19/19 - 4s - loss: 203.6359 - loglik: -2.0167e+02 - logprior: -1.9611e+00
Epoch 5/10
19/19 - 4s - loss: 199.9303 - loglik: -1.9808e+02 - logprior: -1.8494e+00
Epoch 6/10
19/19 - 4s - loss: 198.3627 - loglik: -1.9642e+02 - logprior: -1.9465e+00
Epoch 7/10
19/19 - 4s - loss: 196.2658 - loglik: -1.9414e+02 - logprior: -2.1238e+00
Epoch 8/10
19/19 - 4s - loss: 196.0412 - loglik: -1.9384e+02 - logprior: -2.1984e+00
Epoch 9/10
19/19 - 4s - loss: 195.0339 - loglik: -1.9284e+02 - logprior: -2.1896e+00
Epoch 10/10
19/19 - 4s - loss: 193.7393 - loglik: -1.9155e+02 - logprior: -2.1918e+00
Fitted a model with MAP estimate = -189.0731
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 2), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (83, 1), (91, 1), (97, 1), (99, 1), (101, 2), (104, 1), (114, 1), (120, 2), (121, 1), (122, 2), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 169.9230 - loglik: -1.6675e+02 - logprior: -3.1680e+00
Epoch 2/2
19/19 - 6s - loss: 134.2454 - loglik: -1.3281e+02 - logprior: -1.4329e+00
Fitted a model with MAP estimate = -140.0720
expansions: []
discards: [ 50  70  77 128 151 156]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 133.2981 - loglik: -1.3024e+02 - logprior: -3.0562e+00
Epoch 2/2
19/19 - 5s - loss: 131.0677 - loglik: -1.2984e+02 - logprior: -1.2276e+00
Fitted a model with MAP estimate = -139.4643
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 10s - loss: 139.4062 - loglik: -1.3749e+02 - logprior: -1.9133e+00
Epoch 2/10
22/22 - 6s - loss: 134.6128 - loglik: -1.3376e+02 - logprior: -8.5130e-01
Epoch 3/10
22/22 - 6s - loss: 132.3596 - loglik: -1.3141e+02 - logprior: -9.5456e-01
Epoch 4/10
22/22 - 6s - loss: 131.9384 - loglik: -1.3095e+02 - logprior: -9.8613e-01
Epoch 5/10
22/22 - 6s - loss: 130.9348 - loglik: -1.2999e+02 - logprior: -9.4699e-01
Epoch 6/10
22/22 - 6s - loss: 130.5497 - loglik: -1.2963e+02 - logprior: -9.2118e-01
Epoch 7/10
22/22 - 6s - loss: 129.9149 - loglik: -1.2902e+02 - logprior: -8.9308e-01
Epoch 8/10
22/22 - 6s - loss: 130.1640 - loglik: -1.2931e+02 - logprior: -8.5443e-01
Fitted a model with MAP estimate = -129.6494
Time for alignment: 165.1429
Computed alignments with likelihoods: ['-131.8731', '-129.9280', '-129.6494']
Best model has likelihood: -129.6494
SP score = 1.0000
Training of 3 independent models on file ltn.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c1c3dd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c1c52fa0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 184 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 615.8246 - loglik: -5.7942e+02 - logprior: -3.6406e+01
Epoch 2/10
10/10 - 4s - loss: 540.9058 - loglik: -5.3527e+02 - logprior: -5.6373e+00
Epoch 3/10
10/10 - 4s - loss: 483.8360 - loglik: -4.8275e+02 - logprior: -1.0853e+00
Epoch 4/10
10/10 - 4s - loss: 447.6661 - loglik: -4.4739e+02 - logprior: -2.8060e-01
Epoch 5/10
10/10 - 4s - loss: 432.3039 - loglik: -4.3248e+02 - logprior: 0.1776
Epoch 6/10
10/10 - 4s - loss: 429.0118 - loglik: -4.2952e+02 - logprior: 0.5049
Epoch 7/10
10/10 - 4s - loss: 425.3793 - loglik: -4.2621e+02 - logprior: 0.8259
Epoch 8/10
10/10 - 4s - loss: 425.0065 - loglik: -4.2600e+02 - logprior: 0.9984
Epoch 9/10
10/10 - 4s - loss: 423.7580 - loglik: -4.2487e+02 - logprior: 1.1084
Epoch 10/10
10/10 - 4s - loss: 423.3180 - loglik: -4.2450e+02 - logprior: 1.1855
Fitted a model with MAP estimate = -422.9594
expansions: [(11, 3), (19, 1), (31, 4), (42, 1), (45, 1), (61, 1), (62, 1), (63, 1), (64, 1), (75, 2), (77, 2), (78, 2), (87, 1), (88, 1), (90, 1), (91, 1), (93, 1), (101, 1), (102, 1), (119, 1), (127, 3), (129, 2), (131, 4), (139, 1), (146, 3), (149, 2), (153, 1), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 232 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 459.4312 - loglik: -4.1844e+02 - logprior: -4.0996e+01
Epoch 2/2
10/10 - 6s - loss: 421.5772 - loglik: -4.0784e+02 - logprior: -1.3734e+01
Fitted a model with MAP estimate = -414.7176
expansions: [(0, 3), (174, 1)]
discards: [  0  36  93 129 155 160 164 165 166 189 218]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 225 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 436.8856 - loglik: -4.0549e+02 - logprior: -3.1394e+01
Epoch 2/2
10/10 - 5s - loss: 405.1707 - loglik: -4.0152e+02 - logprior: -3.6463e+00
Fitted a model with MAP estimate = -400.9591
expansions: []
discards: [1 3]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 223 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 430.4291 - loglik: -4.0001e+02 - logprior: -3.0419e+01
Epoch 2/10
10/10 - 5s - loss: 402.9744 - loglik: -3.9986e+02 - logprior: -3.1127e+00
Epoch 3/10
10/10 - 5s - loss: 397.9892 - loglik: -4.0063e+02 - logprior: 2.6427
Epoch 4/10
10/10 - 5s - loss: 394.8304 - loglik: -3.9994e+02 - logprior: 5.1110
Epoch 5/10
10/10 - 5s - loss: 394.0394 - loglik: -4.0057e+02 - logprior: 6.5293
Epoch 6/10
10/10 - 5s - loss: 392.8531 - loglik: -4.0022e+02 - logprior: 7.3690
Epoch 7/10
10/10 - 5s - loss: 393.2295 - loglik: -4.0116e+02 - logprior: 7.9271
Fitted a model with MAP estimate = -392.5769
Time for alignment: 124.1887
Fitting a model of length 184 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 615.8184 - loglik: -5.7941e+02 - logprior: -3.6409e+01
Epoch 2/10
10/10 - 4s - loss: 539.7385 - loglik: -5.3410e+02 - logprior: -5.6430e+00
Epoch 3/10
10/10 - 4s - loss: 486.0732 - loglik: -4.8494e+02 - logprior: -1.1300e+00
Epoch 4/10
10/10 - 4s - loss: 446.5081 - loglik: -4.4606e+02 - logprior: -4.4499e-01
Epoch 5/10
10/10 - 4s - loss: 434.5575 - loglik: -4.3465e+02 - logprior: 0.0954
Epoch 6/10
10/10 - 4s - loss: 430.5117 - loglik: -4.3116e+02 - logprior: 0.6488
Epoch 7/10
10/10 - 4s - loss: 427.4468 - loglik: -4.2840e+02 - logprior: 0.9523
Epoch 8/10
10/10 - 4s - loss: 427.1135 - loglik: -4.2816e+02 - logprior: 1.0446
Epoch 9/10
10/10 - 4s - loss: 424.1251 - loglik: -4.2520e+02 - logprior: 1.0784
Epoch 10/10
10/10 - 4s - loss: 423.5446 - loglik: -4.2469e+02 - logprior: 1.1409
Fitted a model with MAP estimate = -422.7734
expansions: [(11, 3), (21, 1), (30, 2), (31, 3), (32, 1), (45, 1), (61, 2), (62, 2), (63, 2), (75, 1), (76, 2), (77, 2), (78, 2), (86, 1), (90, 1), (91, 1), (92, 1), (101, 1), (102, 1), (118, 1), (126, 5), (127, 1), (130, 1), (131, 1), (146, 1), (147, 2), (149, 2), (154, 2), (159, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 234 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 457.5871 - loglik: -4.1672e+02 - logprior: -4.0865e+01
Epoch 2/2
10/10 - 6s - loss: 420.0777 - loglik: -4.0666e+02 - logprior: -1.3415e+01
Fitted a model with MAP estimate = -412.5622
expansions: [(0, 3), (176, 1)]
discards: [  0  35  36  72  96 190 220]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 231 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 432.2370 - loglik: -4.0111e+02 - logprior: -3.1127e+01
Epoch 2/2
10/10 - 6s - loss: 401.9859 - loglik: -3.9858e+02 - logprior: -3.4091e+00
Fitted a model with MAP estimate = -397.6689
expansions: []
discards: [  1   3 154 155 156]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 226 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 429.7858 - loglik: -3.9959e+02 - logprior: -3.0195e+01
Epoch 2/10
10/10 - 5s - loss: 402.7599 - loglik: -3.9977e+02 - logprior: -2.9911e+00
Epoch 3/10
10/10 - 5s - loss: 396.1132 - loglik: -3.9888e+02 - logprior: 2.7690
Epoch 4/10
10/10 - 6s - loss: 394.4969 - loglik: -3.9974e+02 - logprior: 5.2429
Epoch 5/10
10/10 - 5s - loss: 393.2626 - loglik: -3.9990e+02 - logprior: 6.6383
Epoch 6/10
10/10 - 6s - loss: 392.5543 - loglik: -4.0003e+02 - logprior: 7.4776
Epoch 7/10
10/10 - 6s - loss: 392.0137 - loglik: -4.0005e+02 - logprior: 8.0373
Epoch 8/10
10/10 - 6s - loss: 392.3097 - loglik: -4.0076e+02 - logprior: 8.4465
Fitted a model with MAP estimate = -391.6036
Time for alignment: 130.4471
Fitting a model of length 184 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 615.8029 - loglik: -5.7940e+02 - logprior: -3.6404e+01
Epoch 2/10
10/10 - 4s - loss: 540.0761 - loglik: -5.3444e+02 - logprior: -5.6337e+00
Epoch 3/10
10/10 - 4s - loss: 484.8577 - loglik: -4.8378e+02 - logprior: -1.0739e+00
Epoch 4/10
10/10 - 4s - loss: 445.7205 - loglik: -4.4540e+02 - logprior: -3.2116e-01
Epoch 5/10
10/10 - 4s - loss: 433.4076 - loglik: -4.3366e+02 - logprior: 0.2516
Epoch 6/10
10/10 - 4s - loss: 429.2842 - loglik: -4.3012e+02 - logprior: 0.8408
Epoch 7/10
10/10 - 4s - loss: 425.8246 - loglik: -4.2701e+02 - logprior: 1.1890
Epoch 8/10
10/10 - 4s - loss: 425.4245 - loglik: -4.2662e+02 - logprior: 1.1956
Epoch 9/10
10/10 - 4s - loss: 424.7111 - loglik: -4.2597e+02 - logprior: 1.2602
Epoch 10/10
10/10 - 4s - loss: 423.2612 - loglik: -4.2463e+02 - logprior: 1.3730
Fitted a model with MAP estimate = -423.2678
expansions: [(11, 3), (19, 1), (30, 2), (31, 3), (32, 1), (41, 1), (48, 1), (60, 1), (61, 1), (62, 1), (63, 1), (75, 2), (76, 2), (77, 2), (86, 1), (87, 1), (90, 1), (92, 1), (101, 1), (102, 1), (128, 1), (130, 3), (132, 4), (139, 2), (146, 3), (151, 1), (153, 2), (173, 2), (174, 1), (175, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 231 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 458.3447 - loglik: -4.1744e+02 - logprior: -4.0900e+01
Epoch 2/2
10/10 - 6s - loss: 420.7079 - loglik: -4.0719e+02 - logprior: -1.3513e+01
Fitted a model with MAP estimate = -414.0623
expansions: [(0, 3)]
discards: [  0  35  36 163 164 165 174 217]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 226 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 436.1365 - loglik: -4.0492e+02 - logprior: -3.1212e+01
Epoch 2/2
10/10 - 5s - loss: 406.2178 - loglik: -4.0273e+02 - logprior: -3.4878e+00
Fitted a model with MAP estimate = -401.5407
expansions: []
discards: [ 1  3 90 94]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 222 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 431.6879 - loglik: -4.0135e+02 - logprior: -3.0336e+01
Epoch 2/10
10/10 - 5s - loss: 402.8667 - loglik: -3.9976e+02 - logprior: -3.1065e+00
Epoch 3/10
10/10 - 5s - loss: 397.6471 - loglik: -4.0029e+02 - logprior: 2.6465
Epoch 4/10
10/10 - 5s - loss: 395.1176 - loglik: -4.0025e+02 - logprior: 5.1327
Epoch 5/10
10/10 - 5s - loss: 394.9210 - loglik: -4.0144e+02 - logprior: 6.5166
Epoch 6/10
10/10 - 5s - loss: 392.9456 - loglik: -4.0033e+02 - logprior: 7.3795
Epoch 7/10
10/10 - 5s - loss: 393.5847 - loglik: -4.0151e+02 - logprior: 7.9252
Fitted a model with MAP estimate = -392.7520
Time for alignment: 124.6224
Computed alignments with likelihoods: ['-392.5769', '-391.6036', '-392.7520']
Best model has likelihood: -391.6036
SP score = 0.9436
Training of 3 independent models on file aadh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9f7b6a610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9f7a30eb0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 187 on 3127 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 644.7382 - loglik: -6.3196e+02 - logprior: -1.2778e+01
Epoch 2/10
10/10 - 6s - loss: 571.9576 - loglik: -5.6958e+02 - logprior: -2.3742e+00
Epoch 3/10
10/10 - 6s - loss: 502.5539 - loglik: -5.0108e+02 - logprior: -1.4714e+00
Epoch 4/10
10/10 - 6s - loss: 458.0174 - loglik: -4.5599e+02 - logprior: -2.0233e+00
Epoch 5/10
10/10 - 7s - loss: 442.5798 - loglik: -4.4021e+02 - logprior: -2.3692e+00
Epoch 6/10
10/10 - 6s - loss: 435.3227 - loglik: -4.3292e+02 - logprior: -2.4023e+00
Epoch 7/10
10/10 - 6s - loss: 431.7314 - loglik: -4.2951e+02 - logprior: -2.2180e+00
Epoch 8/10
10/10 - 7s - loss: 432.6879 - loglik: -4.3058e+02 - logprior: -2.1090e+00
Fitted a model with MAP estimate = -431.1219
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (50, 1), (64, 2), (66, 1), (75, 8), (97, 1), (98, 1), (100, 1), (119, 2), (121, 1), (122, 1), (144, 1), (146, 2), (147, 1), (154, 1), (160, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 11s - loss: 424.8038 - loglik: -4.1473e+02 - logprior: -1.0072e+01
Epoch 2/2
21/21 - 9s - loss: 402.0407 - loglik: -3.9877e+02 - logprior: -3.2740e+00
Fitted a model with MAP estimate = -397.1925
expansions: [(81, 1), (185, 1), (226, 2)]
discards: [ 20  28 149 214]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 399.8039 - loglik: -3.9297e+02 - logprior: -6.8334e+00
Epoch 2/2
21/21 - 9s - loss: 390.1192 - loglik: -3.8986e+02 - logprior: -2.6215e-01
Fitted a model with MAP estimate = -388.9614
expansions: [(95, 1)]
discards: [83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 396.1274 - loglik: -3.8998e+02 - logprior: -6.1446e+00
Epoch 2/10
21/21 - 9s - loss: 388.5789 - loglik: -3.8869e+02 - logprior: 0.1160
Epoch 3/10
21/21 - 9s - loss: 387.3647 - loglik: -3.8844e+02 - logprior: 1.0731
Epoch 4/10
21/21 - 9s - loss: 386.9072 - loglik: -3.8835e+02 - logprior: 1.4477
Epoch 5/10
21/21 - 9s - loss: 386.2515 - loglik: -3.8795e+02 - logprior: 1.6946
Epoch 6/10
21/21 - 9s - loss: 386.4236 - loglik: -3.8833e+02 - logprior: 1.9105
Fitted a model with MAP estimate = -386.2402
Time for alignment: 180.9189
Fitting a model of length 187 on 3127 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 643.5215 - loglik: -6.3076e+02 - logprior: -1.2758e+01
Epoch 2/10
10/10 - 6s - loss: 572.1744 - loglik: -5.6980e+02 - logprior: -2.3760e+00
Epoch 3/10
10/10 - 6s - loss: 507.7420 - loglik: -5.0622e+02 - logprior: -1.5260e+00
Epoch 4/10
10/10 - 6s - loss: 466.0012 - loglik: -4.6388e+02 - logprior: -2.1203e+00
Epoch 5/10
10/10 - 6s - loss: 447.6532 - loglik: -4.4510e+02 - logprior: -2.5563e+00
Epoch 6/10
10/10 - 7s - loss: 440.7853 - loglik: -4.3813e+02 - logprior: -2.6506e+00
Epoch 7/10
10/10 - 6s - loss: 436.5233 - loglik: -4.3405e+02 - logprior: -2.4693e+00
Epoch 8/10
10/10 - 6s - loss: 436.8537 - loglik: -4.3455e+02 - logprior: -2.3009e+00
Fitted a model with MAP estimate = -434.8963
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (23, 1), (25, 1), (36, 1), (39, 2), (40, 1), (46, 1), (62, 1), (63, 3), (66, 1), (76, 7), (78, 1), (81, 2), (96, 1), (99, 2), (119, 2), (121, 1), (122, 1), (124, 2), (146, 1), (147, 2), (158, 1), (160, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (178, 1), (179, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 244 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 428.2458 - loglik: -4.1821e+02 - logprior: -1.0034e+01
Epoch 2/2
21/21 - 10s - loss: 401.9349 - loglik: -3.9864e+02 - logprior: -3.2968e+00
Fitted a model with MAP estimate = -396.9068
expansions: [(0, 2), (99, 1)]
discards: [  0  20  28  52  83  87 111 134 154 164 220]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 404.6370 - loglik: -3.9821e+02 - logprior: -6.4307e+00
Epoch 2/2
21/21 - 9s - loss: 393.2245 - loglik: -3.9303e+02 - logprior: -1.9751e-01
Fitted a model with MAP estimate = -393.2363
expansions: [(146, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 13s - loss: 406.2975 - loglik: -3.9727e+02 - logprior: -9.0308e+00
Epoch 2/10
21/21 - 9s - loss: 394.2003 - loglik: -3.9368e+02 - logprior: -5.1756e-01
Epoch 3/10
21/21 - 9s - loss: 393.6116 - loglik: -3.9471e+02 - logprior: 1.0999
Epoch 4/10
21/21 - 9s - loss: 391.6457 - loglik: -3.9314e+02 - logprior: 1.4935
Epoch 5/10
21/21 - 9s - loss: 391.2518 - loglik: -3.9296e+02 - logprior: 1.7040
Epoch 6/10
21/21 - 10s - loss: 391.7406 - loglik: -3.9366e+02 - logprior: 1.9189
Fitted a model with MAP estimate = -391.1948
Time for alignment: 184.0036
Fitting a model of length 187 on 3127 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 644.2734 - loglik: -6.3151e+02 - logprior: -1.2761e+01
Epoch 2/10
10/10 - 6s - loss: 571.5709 - loglik: -5.6921e+02 - logprior: -2.3591e+00
Epoch 3/10
10/10 - 7s - loss: 508.7970 - loglik: -5.0728e+02 - logprior: -1.5181e+00
Epoch 4/10
10/10 - 6s - loss: 466.4498 - loglik: -4.6423e+02 - logprior: -2.2177e+00
Epoch 5/10
10/10 - 6s - loss: 445.9999 - loglik: -4.4324e+02 - logprior: -2.7568e+00
Epoch 6/10
10/10 - 6s - loss: 440.5818 - loglik: -4.3768e+02 - logprior: -2.8993e+00
Epoch 7/10
10/10 - 6s - loss: 436.1498 - loglik: -4.3340e+02 - logprior: -2.7459e+00
Epoch 8/10
10/10 - 7s - loss: 433.8980 - loglik: -4.3132e+02 - logprior: -2.5765e+00
Epoch 9/10
10/10 - 6s - loss: 434.3455 - loglik: -4.3179e+02 - logprior: -2.5531e+00
Fitted a model with MAP estimate = -432.6758
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (36, 1), (37, 1), (39, 2), (40, 1), (50, 1), (60, 1), (63, 3), (65, 1), (75, 5), (76, 2), (78, 1), (82, 1), (96, 1), (99, 1), (118, 2), (119, 1), (120, 1), (121, 1), (145, 1), (146, 1), (147, 1), (154, 1), (160, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (178, 1), (181, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 240 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 426.6216 - loglik: -4.1659e+02 - logprior: -1.0035e+01
Epoch 2/2
21/21 - 10s - loss: 403.3376 - loglik: -4.0014e+02 - logprior: -3.1974e+00
Fitted a model with MAP estimate = -398.2808
expansions: [(0, 2)]
discards: [  0  18  28 150 216]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 403.5237 - loglik: -3.9710e+02 - logprior: -6.4202e+00
Epoch 2/2
21/21 - 9s - loss: 393.5165 - loglik: -3.9335e+02 - logprior: -1.6872e-01
Fitted a model with MAP estimate = -393.5332
expansions: []
discards: [ 0 84]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 405.3806 - loglik: -3.9651e+02 - logprior: -8.8716e+00
Epoch 2/10
21/21 - 9s - loss: 397.0300 - loglik: -3.9673e+02 - logprior: -2.9680e-01
Epoch 3/10
21/21 - 9s - loss: 393.7506 - loglik: -3.9486e+02 - logprior: 1.1053
Epoch 4/10
21/21 - 9s - loss: 393.5345 - loglik: -3.9501e+02 - logprior: 1.4741
Epoch 5/10
21/21 - 9s - loss: 392.4293 - loglik: -3.9411e+02 - logprior: 1.6826
Epoch 6/10
21/21 - 10s - loss: 392.6809 - loglik: -3.9460e+02 - logprior: 1.9194
Fitted a model with MAP estimate = -392.4621
Time for alignment: 188.5145
Computed alignments with likelihoods: ['-386.2402', '-391.1948', '-392.4621']
Best model has likelihood: -386.2402
SP score = 0.6003
Training of 3 independent models on file gluts.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fba11444a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c2c19040>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 283.9209 - loglik: -2.8091e+02 - logprior: -3.0133e+00
Epoch 2/10
19/19 - 2s - loss: 253.7568 - loglik: -2.5287e+02 - logprior: -8.8764e-01
Epoch 3/10
19/19 - 2s - loss: 245.2381 - loglik: -2.4431e+02 - logprior: -9.2435e-01
Epoch 4/10
19/19 - 2s - loss: 241.9458 - loglik: -2.4111e+02 - logprior: -8.3745e-01
Epoch 5/10
19/19 - 2s - loss: 241.4233 - loglik: -2.4062e+02 - logprior: -8.0260e-01
Epoch 6/10
19/19 - 2s - loss: 239.5011 - loglik: -2.3871e+02 - logprior: -7.9039e-01
Epoch 7/10
19/19 - 2s - loss: 239.5544 - loglik: -2.3879e+02 - logprior: -7.6371e-01
Fitted a model with MAP estimate = -238.1707
expansions: [(0, 10), (16, 2), (20, 1), (30, 2), (57, 2), (58, 2), (59, 3), (61, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 240.9940 - loglik: -2.3728e+02 - logprior: -3.7138e+00
Epoch 2/2
19/19 - 3s - loss: 234.6514 - loglik: -2.3351e+02 - logprior: -1.1365e+00
Fitted a model with MAP estimate = -232.9109
expansions: [(0, 6), (22, 1)]
discards: [ 1  2  3  4  5 44 75 76 78 79 80]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 97 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 238.7322 - loglik: -2.3496e+02 - logprior: -3.7703e+00
Epoch 2/2
19/19 - 3s - loss: 234.7086 - loglik: -2.3346e+02 - logprior: -1.2482e+00
Fitted a model with MAP estimate = -233.0909
expansions: [(0, 5)]
discards: [0 1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 235.4722 - loglik: -2.3225e+02 - logprior: -3.2185e+00
Epoch 2/10
19/19 - 3s - loss: 233.0821 - loglik: -2.3197e+02 - logprior: -1.1111e+00
Epoch 3/10
19/19 - 3s - loss: 231.6899 - loglik: -2.3076e+02 - logprior: -9.3223e-01
Epoch 4/10
19/19 - 3s - loss: 231.5025 - loglik: -2.3067e+02 - logprior: -8.3547e-01
Epoch 5/10
19/19 - 3s - loss: 231.4523 - loglik: -2.3066e+02 - logprior: -7.8737e-01
Epoch 6/10
19/19 - 3s - loss: 231.3512 - loglik: -2.3058e+02 - logprior: -7.6744e-01
Epoch 7/10
19/19 - 3s - loss: 230.6280 - loglik: -2.2988e+02 - logprior: -7.4491e-01
Epoch 8/10
19/19 - 3s - loss: 231.2412 - loglik: -2.3052e+02 - logprior: -7.1994e-01
Fitted a model with MAP estimate = -230.9362
Time for alignment: 79.4263
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 283.7515 - loglik: -2.8074e+02 - logprior: -3.0070e+00
Epoch 2/10
19/19 - 2s - loss: 255.0218 - loglik: -2.5413e+02 - logprior: -8.8782e-01
Epoch 3/10
19/19 - 2s - loss: 245.2076 - loglik: -2.4430e+02 - logprior: -9.1087e-01
Epoch 4/10
19/19 - 2s - loss: 241.7117 - loglik: -2.4089e+02 - logprior: -8.2519e-01
Epoch 5/10
19/19 - 2s - loss: 240.9731 - loglik: -2.4017e+02 - logprior: -8.0453e-01
Epoch 6/10
19/19 - 2s - loss: 240.5519 - loglik: -2.3976e+02 - logprior: -7.9660e-01
Epoch 7/10
19/19 - 2s - loss: 238.8677 - loglik: -2.3808e+02 - logprior: -7.8868e-01
Epoch 8/10
19/19 - 2s - loss: 238.8275 - loglik: -2.3805e+02 - logprior: -7.7835e-01
Epoch 9/10
19/19 - 2s - loss: 239.4419 - loglik: -2.3866e+02 - logprior: -7.7757e-01
Fitted a model with MAP estimate = -237.8747
expansions: [(0, 10), (17, 1), (21, 3), (23, 1), (35, 1), (54, 1), (57, 2), (59, 4), (61, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 240.1092 - loglik: -2.3634e+02 - logprior: -3.7707e+00
Epoch 2/2
19/19 - 3s - loss: 234.1852 - loglik: -2.3297e+02 - logprior: -1.2137e+00
Fitted a model with MAP estimate = -231.9869
expansions: [(0, 6), (34, 1), (35, 1)]
discards: [ 1  2  3  4  5  6 75 80 81 84]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 237.3002 - loglik: -2.3351e+02 - logprior: -3.7915e+00
Epoch 2/2
19/19 - 3s - loss: 233.1523 - loglik: -2.3192e+02 - logprior: -1.2321e+00
Fitted a model with MAP estimate = -231.5183
expansions: [(0, 5)]
discards: [0 1 6]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 101 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 233.7755 - loglik: -2.3063e+02 - logprior: -3.1495e+00
Epoch 2/10
19/19 - 3s - loss: 231.2354 - loglik: -2.3018e+02 - logprior: -1.0559e+00
Epoch 3/10
19/19 - 3s - loss: 230.3773 - loglik: -2.2940e+02 - logprior: -9.7433e-01
Epoch 4/10
19/19 - 3s - loss: 230.2050 - loglik: -2.2933e+02 - logprior: -8.7470e-01
Epoch 5/10
19/19 - 3s - loss: 230.3107 - loglik: -2.2951e+02 - logprior: -7.9770e-01
Fitted a model with MAP estimate = -229.9250
Time for alignment: 77.0263
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 283.6897 - loglik: -2.8068e+02 - logprior: -3.0085e+00
Epoch 2/10
19/19 - 2s - loss: 254.1531 - loglik: -2.5327e+02 - logprior: -8.8225e-01
Epoch 3/10
19/19 - 2s - loss: 244.2611 - loglik: -2.4336e+02 - logprior: -8.9851e-01
Epoch 4/10
19/19 - 2s - loss: 242.0470 - loglik: -2.4123e+02 - logprior: -8.1679e-01
Epoch 5/10
19/19 - 2s - loss: 241.5134 - loglik: -2.4072e+02 - logprior: -7.9734e-01
Epoch 6/10
19/19 - 2s - loss: 239.9576 - loglik: -2.3917e+02 - logprior: -7.8954e-01
Epoch 7/10
19/19 - 2s - loss: 239.1887 - loglik: -2.3841e+02 - logprior: -7.7623e-01
Epoch 8/10
19/19 - 2s - loss: 239.1573 - loglik: -2.3838e+02 - logprior: -7.7767e-01
Epoch 9/10
19/19 - 2s - loss: 238.6262 - loglik: -2.3786e+02 - logprior: -7.6792e-01
Epoch 10/10
19/19 - 2s - loss: 238.6771 - loglik: -2.3790e+02 - logprior: -7.7206e-01
Fitted a model with MAP estimate = -237.5028
expansions: [(0, 10), (12, 1), (16, 2), (19, 2), (22, 1), (54, 1), (57, 2), (59, 3), (61, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 239.9534 - loglik: -2.3614e+02 - logprior: -3.8123e+00
Epoch 2/2
19/19 - 3s - loss: 234.5526 - loglik: -2.3338e+02 - logprior: -1.1763e+00
Fitted a model with MAP estimate = -232.4454
expansions: [(0, 6)]
discards: [ 1  2  3  4  5 27 75 78 79 80]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 97 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 238.0287 - loglik: -2.3425e+02 - logprior: -3.7752e+00
Epoch 2/2
19/19 - 3s - loss: 234.2053 - loglik: -2.3292e+02 - logprior: -1.2876e+00
Fitted a model with MAP estimate = -232.7230
expansions: [(0, 5)]
discards: [ 0  1  2  3 34]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 97 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 235.8232 - loglik: -2.3259e+02 - logprior: -3.2334e+00
Epoch 2/10
19/19 - 3s - loss: 233.1455 - loglik: -2.3209e+02 - logprior: -1.0594e+00
Epoch 3/10
19/19 - 3s - loss: 232.3565 - loglik: -2.3143e+02 - logprior: -9.2380e-01
Epoch 4/10
19/19 - 3s - loss: 232.2938 - loglik: -2.3145e+02 - logprior: -8.3967e-01
Epoch 5/10
19/19 - 3s - loss: 231.7300 - loglik: -2.3095e+02 - logprior: -7.8296e-01
Epoch 6/10
19/19 - 3s - loss: 231.9686 - loglik: -2.3122e+02 - logprior: -7.4982e-01
Fitted a model with MAP estimate = -231.7497
Time for alignment: 79.4058
Computed alignments with likelihoods: ['-230.9362', '-229.9250', '-231.7497']
Best model has likelihood: -229.9250
SP score = 0.3621
Training of 3 independent models on file tms.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbaf0e7f040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbb940c23d0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 219 on 2118 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 770.2855 - loglik: -7.5246e+02 - logprior: -1.7827e+01
Epoch 2/10
10/10 - 8s - loss: 660.3041 - loglik: -6.5752e+02 - logprior: -2.7846e+00
Epoch 3/10
10/10 - 8s - loss: 549.8102 - loglik: -5.4831e+02 - logprior: -1.4955e+00
Epoch 4/10
10/10 - 8s - loss: 475.3829 - loglik: -4.7307e+02 - logprior: -2.3175e+00
Epoch 5/10
10/10 - 8s - loss: 448.8036 - loglik: -4.4587e+02 - logprior: -2.9322e+00
Epoch 6/10
10/10 - 8s - loss: 438.6083 - loglik: -4.3541e+02 - logprior: -3.1953e+00
Epoch 7/10
10/10 - 8s - loss: 435.1022 - loglik: -4.3187e+02 - logprior: -3.2313e+00
Epoch 8/10
10/10 - 8s - loss: 431.3648 - loglik: -4.2831e+02 - logprior: -3.0513e+00
Epoch 9/10
10/10 - 8s - loss: 431.5689 - loglik: -4.2873e+02 - logprior: -2.8381e+00
Fitted a model with MAP estimate = -430.3807
expansions: [(8, 1), (9, 1), (12, 1), (14, 1), (15, 1), (24, 1), (33, 2), (46, 1), (55, 1), (57, 1), (58, 2), (60, 1), (61, 1), (63, 1), (86, 2), (95, 2), (97, 1), (100, 1), (114, 2), (115, 3), (129, 2), (131, 1), (140, 1), (141, 1), (156, 1), (160, 1), (162, 1), (163, 1), (176, 1), (178, 1), (180, 1), (184, 1), (188, 1), (192, 1), (195, 1), (196, 1), (197, 1), (200, 1), (209, 1), (212, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 421.3869 - loglik: -4.0562e+02 - logprior: -1.5764e+01
Epoch 2/2
17/17 - 9s - loss: 386.9195 - loglik: -3.8233e+02 - logprior: -4.5873e+00
Fitted a model with MAP estimate = -382.5787
expansions: [(0, 3)]
discards: [  0 135 155]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 14s - loss: 389.2931 - loglik: -3.7862e+02 - logprior: -1.0668e+01
Epoch 2/2
17/17 - 9s - loss: 374.7888 - loglik: -3.7524e+02 - logprior: 0.4479
Fitted a model with MAP estimate = -374.2772
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 12s - loss: 393.2746 - loglik: -3.7842e+02 - logprior: -1.4855e+01
Epoch 2/10
17/17 - 9s - loss: 382.4915 - loglik: -3.7921e+02 - logprior: -3.2834e+00
Epoch 3/10
17/17 - 9s - loss: 376.2553 - loglik: -3.7728e+02 - logprior: 1.0222
Epoch 4/10
17/17 - 9s - loss: 374.5717 - loglik: -3.7794e+02 - logprior: 3.3682
Epoch 5/10
17/17 - 9s - loss: 372.9216 - loglik: -3.7677e+02 - logprior: 3.8517
Epoch 6/10
17/17 - 9s - loss: 372.2670 - loglik: -3.7649e+02 - logprior: 4.2274
Epoch 7/10
17/17 - 9s - loss: 370.8783 - loglik: -3.7538e+02 - logprior: 4.5047
Epoch 8/10
17/17 - 9s - loss: 371.6740 - loglik: -3.7648e+02 - logprior: 4.8090
Fitted a model with MAP estimate = -371.5832
Time for alignment: 217.4957
Fitting a model of length 219 on 2118 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 12s - loss: 770.3610 - loglik: -7.5254e+02 - logprior: -1.7816e+01
Epoch 2/10
10/10 - 7s - loss: 658.6798 - loglik: -6.5587e+02 - logprior: -2.8139e+00
Epoch 3/10
10/10 - 8s - loss: 553.7315 - loglik: -5.5210e+02 - logprior: -1.6268e+00
Epoch 4/10
10/10 - 8s - loss: 476.9671 - loglik: -4.7421e+02 - logprior: -2.7563e+00
Epoch 5/10
10/10 - 8s - loss: 449.0551 - loglik: -4.4540e+02 - logprior: -3.6510e+00
Epoch 6/10
10/10 - 8s - loss: 436.9841 - loglik: -4.3303e+02 - logprior: -3.9531e+00
Epoch 7/10
10/10 - 8s - loss: 435.2629 - loglik: -4.3143e+02 - logprior: -3.8339e+00
Epoch 8/10
10/10 - 8s - loss: 431.6275 - loglik: -4.2807e+02 - logprior: -3.5527e+00
Epoch 9/10
10/10 - 8s - loss: 429.5074 - loglik: -4.2619e+02 - logprior: -3.3125e+00
Epoch 10/10
10/10 - 8s - loss: 429.9164 - loglik: -4.2669e+02 - logprior: -3.2231e+00
Fitted a model with MAP estimate = -429.3760
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (26, 1), (32, 1), (43, 1), (45, 1), (54, 1), (55, 1), (57, 1), (58, 1), (59, 1), (60, 1), (62, 1), (85, 1), (86, 1), (94, 2), (97, 1), (109, 1), (113, 1), (114, 1), (115, 3), (128, 2), (130, 1), (141, 1), (150, 1), (160, 1), (161, 1), (163, 1), (179, 1), (180, 1), (181, 1), (184, 1), (191, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 422.2652 - loglik: -4.0653e+02 - logprior: -1.5738e+01
Epoch 2/2
17/17 - 9s - loss: 386.7655 - loglik: -3.8225e+02 - logprior: -4.5178e+00
Fitted a model with MAP estimate = -382.4012
expansions: [(0, 3)]
discards: [  0 140 155]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 388.9736 - loglik: -3.7853e+02 - logprior: -1.0444e+01
Epoch 2/2
17/17 - 9s - loss: 374.6537 - loglik: -3.7517e+02 - logprior: 0.5120
Fitted a model with MAP estimate = -374.0824
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 394.1459 - loglik: -3.7933e+02 - logprior: -1.4816e+01
Epoch 2/10
17/17 - 9s - loss: 379.8475 - loglik: -3.7658e+02 - logprior: -3.2663e+00
Epoch 3/10
17/17 - 9s - loss: 377.3942 - loglik: -3.7831e+02 - logprior: 0.9164
Epoch 4/10
17/17 - 9s - loss: 372.1287 - loglik: -3.7557e+02 - logprior: 3.4404
Epoch 5/10
17/17 - 9s - loss: 373.1535 - loglik: -3.7708e+02 - logprior: 3.9303
Fitted a model with MAP estimate = -372.4271
Time for alignment: 198.2305
Fitting a model of length 219 on 2118 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 769.9442 - loglik: -7.5211e+02 - logprior: -1.7830e+01
Epoch 2/10
10/10 - 8s - loss: 659.2231 - loglik: -6.5644e+02 - logprior: -2.7833e+00
Epoch 3/10
10/10 - 8s - loss: 552.6193 - loglik: -5.5107e+02 - logprior: -1.5483e+00
Epoch 4/10
10/10 - 8s - loss: 476.2355 - loglik: -4.7360e+02 - logprior: -2.6346e+00
Epoch 5/10
10/10 - 8s - loss: 448.9166 - loglik: -4.4549e+02 - logprior: -3.4263e+00
Epoch 6/10
10/10 - 7s - loss: 436.7582 - loglik: -4.3308e+02 - logprior: -3.6809e+00
Epoch 7/10
10/10 - 8s - loss: 434.6634 - loglik: -4.3109e+02 - logprior: -3.5778e+00
Epoch 8/10
10/10 - 8s - loss: 431.0283 - loglik: -4.2772e+02 - logprior: -3.3064e+00
Epoch 9/10
10/10 - 8s - loss: 430.8079 - loglik: -4.2774e+02 - logprior: -3.0662e+00
Epoch 10/10
10/10 - 8s - loss: 430.2447 - loglik: -4.2726e+02 - logprior: -2.9825e+00
Fitted a model with MAP estimate = -429.2253
expansions: [(8, 1), (9, 1), (11, 1), (13, 3), (22, 1), (26, 1), (32, 1), (43, 1), (45, 1), (57, 1), (58, 2), (59, 1), (60, 3), (62, 1), (85, 1), (86, 1), (94, 2), (97, 1), (109, 1), (113, 1), (114, 2), (115, 1), (122, 1), (128, 2), (130, 1), (141, 1), (150, 1), (159, 1), (160, 1), (162, 1), (179, 1), (180, 1), (181, 1), (184, 1), (191, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 268 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 422.5070 - loglik: -4.0670e+02 - logprior: -1.5803e+01
Epoch 2/2
17/17 - 9s - loss: 386.0869 - loglik: -3.8131e+02 - logprior: -4.7735e+00
Fitted a model with MAP estimate = -382.6890
expansions: [(0, 3)]
discards: [  0  16  71  72 139 157]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 265 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 390.3230 - loglik: -3.7986e+02 - logprior: -1.0459e+01
Epoch 2/2
17/17 - 9s - loss: 376.4548 - loglik: -3.7697e+02 - logprior: 0.5110
Fitted a model with MAP estimate = -375.3205
expansions: [(72, 1)]
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 12s - loss: 393.8037 - loglik: -3.7908e+02 - logprior: -1.4719e+01
Epoch 2/10
17/17 - 9s - loss: 381.5796 - loglik: -3.7842e+02 - logprior: -3.1632e+00
Epoch 3/10
17/17 - 9s - loss: 376.3289 - loglik: -3.7762e+02 - logprior: 1.2956
Epoch 4/10
17/17 - 9s - loss: 373.0523 - loglik: -3.7651e+02 - logprior: 3.4578
Epoch 5/10
17/17 - 9s - loss: 372.3518 - loglik: -3.7628e+02 - logprior: 3.9286
Epoch 6/10
17/17 - 9s - loss: 372.8541 - loglik: -3.7715e+02 - logprior: 4.2943
Fitted a model with MAP estimate = -372.0377
Time for alignment: 205.3060
Computed alignments with likelihoods: ['-371.5832', '-372.4271', '-372.0377']
Best model has likelihood: -371.5832
SP score = 0.9498
Training of 3 independent models on file kunitz.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb7af6b7b80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbaf0cf2ee0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 179.3865 - loglik: -1.5937e+02 - logprior: -2.0017e+01
Epoch 2/10
10/10 - 1s - loss: 148.2482 - loglik: -1.4254e+02 - logprior: -5.7097e+00
Epoch 3/10
10/10 - 1s - loss: 130.5316 - loglik: -1.2725e+02 - logprior: -3.2779e+00
Epoch 4/10
10/10 - 1s - loss: 119.2626 - loglik: -1.1647e+02 - logprior: -2.7976e+00
Epoch 5/10
10/10 - 1s - loss: 113.9980 - loglik: -1.1126e+02 - logprior: -2.7417e+00
Epoch 6/10
10/10 - 1s - loss: 112.2010 - loglik: -1.0956e+02 - logprior: -2.6449e+00
Epoch 7/10
10/10 - 1s - loss: 111.6598 - loglik: -1.0916e+02 - logprior: -2.4987e+00
Epoch 8/10
10/10 - 1s - loss: 111.0066 - loglik: -1.0866e+02 - logprior: -2.3481e+00
Epoch 9/10
10/10 - 1s - loss: 110.9830 - loglik: -1.0872e+02 - logprior: -2.2656e+00
Epoch 10/10
10/10 - 1s - loss: 110.7707 - loglik: -1.0853e+02 - logprior: -2.2454e+00
Fitted a model with MAP estimate = -110.7327
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (19, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 134.4628 - loglik: -1.1204e+02 - logprior: -2.2421e+01
Epoch 2/2
10/10 - 1s - loss: 115.3531 - loglik: -1.0573e+02 - logprior: -9.6182e+00
Fitted a model with MAP estimate = -112.2902
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 121.0269 - loglik: -1.0322e+02 - logprior: -1.7806e+01
Epoch 2/2
10/10 - 1s - loss: 106.9672 - loglik: -1.0208e+02 - logprior: -4.8890e+00
Fitted a model with MAP estimate = -105.5955
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 124.0706 - loglik: -1.0381e+02 - logprior: -2.0258e+01
Epoch 2/10
10/10 - 1s - loss: 109.3444 - loglik: -1.0354e+02 - logprior: -5.8043e+00
Epoch 3/10
10/10 - 1s - loss: 106.4514 - loglik: -1.0354e+02 - logprior: -2.9073e+00
Epoch 4/10
10/10 - 1s - loss: 105.3484 - loglik: -1.0330e+02 - logprior: -2.0455e+00
Epoch 5/10
10/10 - 1s - loss: 104.8857 - loglik: -1.0345e+02 - logprior: -1.4317e+00
Epoch 6/10
10/10 - 1s - loss: 104.8465 - loglik: -1.0368e+02 - logprior: -1.1667e+00
Epoch 7/10
10/10 - 1s - loss: 104.6256 - loglik: -1.0356e+02 - logprior: -1.0613e+00
Epoch 8/10
10/10 - 1s - loss: 104.6924 - loglik: -1.0378e+02 - logprior: -9.0948e-01
Fitted a model with MAP estimate = -104.4717
Time for alignment: 27.8280
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 179.3818 - loglik: -1.5937e+02 - logprior: -2.0015e+01
Epoch 2/10
10/10 - 1s - loss: 148.1819 - loglik: -1.4247e+02 - logprior: -5.7100e+00
Epoch 3/10
10/10 - 1s - loss: 130.9297 - loglik: -1.2766e+02 - logprior: -3.2741e+00
Epoch 4/10
10/10 - 1s - loss: 118.7226 - loglik: -1.1593e+02 - logprior: -2.7922e+00
Epoch 5/10
10/10 - 1s - loss: 113.5554 - loglik: -1.1082e+02 - logprior: -2.7342e+00
Epoch 6/10
10/10 - 1s - loss: 112.1477 - loglik: -1.0951e+02 - logprior: -2.6418e+00
Epoch 7/10
10/10 - 1s - loss: 111.0051 - loglik: -1.0848e+02 - logprior: -2.5202e+00
Epoch 8/10
10/10 - 1s - loss: 110.7792 - loglik: -1.0840e+02 - logprior: -2.3824e+00
Epoch 9/10
10/10 - 1s - loss: 110.6348 - loglik: -1.0834e+02 - logprior: -2.2910e+00
Epoch 10/10
10/10 - 1s - loss: 110.5042 - loglik: -1.0825e+02 - logprior: -2.2571e+00
Fitted a model with MAP estimate = -110.4121
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (20, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 134.1652 - loglik: -1.1174e+02 - logprior: -2.2424e+01
Epoch 2/2
10/10 - 1s - loss: 115.4185 - loglik: -1.0580e+02 - logprior: -9.6171e+00
Fitted a model with MAP estimate = -112.2925
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 120.7855 - loglik: -1.0298e+02 - logprior: -1.7804e+01
Epoch 2/2
10/10 - 1s - loss: 107.3768 - loglik: -1.0249e+02 - logprior: -4.8853e+00
Fitted a model with MAP estimate = -105.5999
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 123.8230 - loglik: -1.0357e+02 - logprior: -2.0249e+01
Epoch 2/10
10/10 - 1s - loss: 109.7160 - loglik: -1.0392e+02 - logprior: -5.7978e+00
Epoch 3/10
10/10 - 1s - loss: 106.2634 - loglik: -1.0336e+02 - logprior: -2.9033e+00
Epoch 4/10
10/10 - 1s - loss: 105.4504 - loglik: -1.0340e+02 - logprior: -2.0512e+00
Epoch 5/10
10/10 - 1s - loss: 104.9536 - loglik: -1.0351e+02 - logprior: -1.4397e+00
Epoch 6/10
10/10 - 1s - loss: 104.8700 - loglik: -1.0371e+02 - logprior: -1.1607e+00
Epoch 7/10
10/10 - 1s - loss: 104.5776 - loglik: -1.0353e+02 - logprior: -1.0515e+00
Epoch 8/10
10/10 - 1s - loss: 104.6078 - loglik: -1.0371e+02 - logprior: -9.0162e-01
Fitted a model with MAP estimate = -104.4832
Time for alignment: 27.6722
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 179.5313 - loglik: -1.5952e+02 - logprior: -2.0016e+01
Epoch 2/10
10/10 - 1s - loss: 148.2179 - loglik: -1.4251e+02 - logprior: -5.7119e+00
Epoch 3/10
10/10 - 1s - loss: 130.5200 - loglik: -1.2722e+02 - logprior: -3.2980e+00
Epoch 4/10
10/10 - 1s - loss: 120.0117 - loglik: -1.1719e+02 - logprior: -2.8178e+00
Epoch 5/10
10/10 - 1s - loss: 114.6162 - loglik: -1.1188e+02 - logprior: -2.7409e+00
Epoch 6/10
10/10 - 1s - loss: 112.3592 - loglik: -1.0970e+02 - logprior: -2.6601e+00
Epoch 7/10
10/10 - 1s - loss: 111.6088 - loglik: -1.0909e+02 - logprior: -2.5157e+00
Epoch 8/10
10/10 - 1s - loss: 111.0001 - loglik: -1.0863e+02 - logprior: -2.3658e+00
Epoch 9/10
10/10 - 1s - loss: 110.9808 - loglik: -1.0870e+02 - logprior: -2.2839e+00
Epoch 10/10
10/10 - 1s - loss: 110.9115 - loglik: -1.0864e+02 - logprior: -2.2668e+00
Fitted a model with MAP estimate = -110.7461
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (19, 1), (22, 2), (23, 1), (34, 1), (35, 1), (36, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 134.5231 - loglik: -1.1210e+02 - logprior: -2.2424e+01
Epoch 2/2
10/10 - 1s - loss: 115.2717 - loglik: -1.0565e+02 - logprior: -9.6217e+00
Fitted a model with MAP estimate = -112.3473
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 32]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 120.9033 - loglik: -1.0310e+02 - logprior: -1.7806e+01
Epoch 2/2
10/10 - 1s - loss: 107.0699 - loglik: -1.0218e+02 - logprior: -4.8889e+00
Fitted a model with MAP estimate = -105.5717
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 123.8726 - loglik: -1.0364e+02 - logprior: -2.0236e+01
Epoch 2/10
10/10 - 1s - loss: 109.6043 - loglik: -1.0381e+02 - logprior: -5.7904e+00
Epoch 3/10
10/10 - 1s - loss: 106.2746 - loglik: -1.0336e+02 - logprior: -2.9108e+00
Epoch 4/10
10/10 - 1s - loss: 105.3891 - loglik: -1.0330e+02 - logprior: -2.0900e+00
Epoch 5/10
10/10 - 1s - loss: 105.0166 - loglik: -1.0353e+02 - logprior: -1.4824e+00
Epoch 6/10
10/10 - 1s - loss: 104.6958 - loglik: -1.0354e+02 - logprior: -1.1556e+00
Epoch 7/10
10/10 - 1s - loss: 104.5446 - loglik: -1.0352e+02 - logprior: -1.0228e+00
Epoch 8/10
10/10 - 1s - loss: 104.5874 - loglik: -1.0368e+02 - logprior: -9.0336e-01
Fitted a model with MAP estimate = -104.4544
Time for alignment: 29.4544
Computed alignments with likelihoods: ['-104.4717', '-104.4832', '-104.4544']
Best model has likelihood: -104.4544
SP score = 0.9750
Training of 3 independent models on file rhv.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fba114d3040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb7ade3f130>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 468.3954 - loglik: -4.6663e+02 - logprior: -1.7644e+00
Epoch 2/10
39/39 - 8s - loss: 387.9421 - loglik: -3.8702e+02 - logprior: -9.1903e-01
Epoch 3/10
39/39 - 7s - loss: 379.8044 - loglik: -3.7890e+02 - logprior: -9.0367e-01
Epoch 4/10
39/39 - 7s - loss: 377.4141 - loglik: -3.7651e+02 - logprior: -9.0394e-01
Epoch 5/10
39/39 - 8s - loss: 376.4520 - loglik: -3.7556e+02 - logprior: -8.9668e-01
Epoch 6/10
39/39 - 8s - loss: 375.4990 - loglik: -3.7461e+02 - logprior: -8.8637e-01
Epoch 7/10
39/39 - 8s - loss: 374.9420 - loglik: -3.7406e+02 - logprior: -8.8634e-01
Epoch 8/10
39/39 - 8s - loss: 374.2994 - loglik: -3.7343e+02 - logprior: -8.7409e-01
Epoch 9/10
39/39 - 8s - loss: 374.1340 - loglik: -3.7326e+02 - logprior: -8.7288e-01
Epoch 10/10
39/39 - 7s - loss: 374.1300 - loglik: -3.7326e+02 - logprior: -8.6773e-01
Fitted a model with MAP estimate = -309.5284
expansions: [(0, 13), (10, 1), (11, 1), (18, 1), (28, 1), (29, 3), (33, 3), (37, 1), (43, 1), (44, 2), (56, 1), (71, 1), (88, 1), (89, 1), (90, 1), (91, 1), (103, 1), (104, 1), (113, 1), (127, 7), (134, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 355.2893 - loglik: -3.5252e+02 - logprior: -2.7742e+00
Epoch 2/2
39/39 - 10s - loss: 341.6959 - loglik: -3.4046e+02 - logprior: -1.2406e+00
Fitted a model with MAP estimate = -285.2763
expansions: []
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 69]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 347.6134 - loglik: -3.4565e+02 - logprior: -1.9644e+00
Epoch 2/2
39/39 - 9s - loss: 345.0718 - loglik: -3.4439e+02 - logprior: -6.8499e-01
Fitted a model with MAP estimate = -290.2283
expansions: [(0, 11)]
discards: [159]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 15s - loss: 284.0327 - loglik: -2.8260e+02 - logprior: -1.4316e+00
Epoch 2/10
52/52 - 13s - loss: 281.4483 - loglik: -2.8062e+02 - logprior: -8.3263e-01
Epoch 3/10
52/52 - 12s - loss: 278.7301 - loglik: -2.7795e+02 - logprior: -7.8315e-01
Epoch 4/10
52/52 - 12s - loss: 276.8350 - loglik: -2.7612e+02 - logprior: -7.1815e-01
Epoch 5/10
52/52 - 12s - loss: 277.8362 - loglik: -2.7718e+02 - logprior: -6.5919e-01
Fitted a model with MAP estimate = -276.5916
Time for alignment: 251.3334
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 467.6703 - loglik: -4.6589e+02 - logprior: -1.7755e+00
Epoch 2/10
39/39 - 8s - loss: 388.7129 - loglik: -3.8758e+02 - logprior: -1.1288e+00
Epoch 3/10
39/39 - 8s - loss: 381.2743 - loglik: -3.8017e+02 - logprior: -1.1026e+00
Epoch 4/10
39/39 - 8s - loss: 379.4208 - loglik: -3.7835e+02 - logprior: -1.0690e+00
Epoch 5/10
39/39 - 8s - loss: 378.3134 - loglik: -3.7725e+02 - logprior: -1.0673e+00
Epoch 6/10
39/39 - 8s - loss: 377.1962 - loglik: -3.7612e+02 - logprior: -1.0794e+00
Epoch 7/10
39/39 - 8s - loss: 376.8521 - loglik: -3.7575e+02 - logprior: -1.1018e+00
Epoch 8/10
39/39 - 8s - loss: 376.3363 - loglik: -3.7524e+02 - logprior: -1.0920e+00
Epoch 9/10
39/39 - 7s - loss: 375.9245 - loglik: -3.7483e+02 - logprior: -1.0970e+00
Epoch 10/10
39/39 - 8s - loss: 375.6021 - loglik: -3.7449e+02 - logprior: -1.1083e+00
Fitted a model with MAP estimate = -309.4649
expansions: [(0, 14), (10, 1), (18, 1), (19, 1), (28, 1), (30, 1), (31, 4), (33, 1), (36, 1), (37, 2), (43, 1), (44, 2), (71, 1), (88, 1), (89, 1), (90, 1), (91, 1), (103, 1), (108, 2), (112, 1), (126, 10), (130, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 351.1066 - loglik: -3.4835e+02 - logprior: -2.7537e+00
Epoch 2/2
39/39 - 11s - loss: 336.7345 - loglik: -3.3509e+02 - logprior: -1.6493e+00
Fitted a model with MAP estimate = -282.2436
expansions: [(52, 1), (169, 1), (170, 2)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 72]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 179 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 340.0842 - loglik: -3.3738e+02 - logprior: -2.7002e+00
Epoch 2/2
39/39 - 10s - loss: 336.7615 - loglik: -3.3584e+02 - logprior: -9.1991e-01
Fitted a model with MAP estimate = -284.1594
expansions: [(0, 7)]
discards: [160]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 15s - loss: 278.4688 - loglik: -2.7706e+02 - logprior: -1.4069e+00
Epoch 2/10
52/52 - 13s - loss: 273.4410 - loglik: -2.7237e+02 - logprior: -1.0733e+00
Epoch 3/10
52/52 - 13s - loss: 274.9392 - loglik: -2.7393e+02 - logprior: -1.0108e+00
Fitted a model with MAP estimate = -272.3961
Time for alignment: 233.1257
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 467.9488 - loglik: -4.6618e+02 - logprior: -1.7689e+00
Epoch 2/10
39/39 - 8s - loss: 385.6133 - loglik: -3.8459e+02 - logprior: -1.0199e+00
Epoch 3/10
39/39 - 8s - loss: 378.7865 - loglik: -3.7781e+02 - logprior: -9.7147e-01
Epoch 4/10
39/39 - 7s - loss: 376.5381 - loglik: -3.7560e+02 - logprior: -9.4210e-01
Epoch 5/10
39/39 - 8s - loss: 374.8699 - loglik: -3.7391e+02 - logprior: -9.5810e-01
Epoch 6/10
39/39 - 8s - loss: 374.2392 - loglik: -3.7328e+02 - logprior: -9.6357e-01
Epoch 7/10
39/39 - 8s - loss: 373.5472 - loglik: -3.7259e+02 - logprior: -9.5237e-01
Epoch 8/10
39/39 - 8s - loss: 373.5068 - loglik: -3.7256e+02 - logprior: -9.5080e-01
Epoch 9/10
39/39 - 8s - loss: 373.6022 - loglik: -3.7265e+02 - logprior: -9.5269e-01
Fitted a model with MAP estimate = -309.2421
expansions: [(0, 11), (10, 1), (11, 1), (18, 1), (28, 2), (29, 2), (34, 2), (38, 1), (43, 1), (44, 1), (45, 1), (56, 1), (71, 1), (87, 1), (88, 3), (89, 1), (90, 1), (102, 2), (107, 2), (124, 6), (126, 1), (130, 3), (134, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 352.7455 - loglik: -3.4992e+02 - logprior: -2.8254e+00
Epoch 2/2
39/39 - 10s - loss: 338.1951 - loglik: -3.3676e+02 - logprior: -1.4346e+00
Fitted a model with MAP estimate = -283.0669
expansions: [(46, 1), (162, 1), (175, 1)]
discards: [  1   2   3   4   5   6   7   8   9  42 134 141 180 181 182 183]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 343.2188 - loglik: -3.4126e+02 - logprior: -1.9569e+00
Epoch 2/2
39/39 - 10s - loss: 339.5326 - loglik: -3.3879e+02 - logprior: -7.4136e-01
Fitted a model with MAP estimate = -286.1121
expansions: [(0, 10), (1, 1), (145, 1), (153, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 186 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 16s - loss: 278.7260 - loglik: -2.7721e+02 - logprior: -1.5145e+00
Epoch 2/10
52/52 - 13s - loss: 274.9008 - loglik: -2.7398e+02 - logprior: -9.2424e-01
Epoch 3/10
52/52 - 13s - loss: 273.8324 - loglik: -2.7297e+02 - logprior: -8.6631e-01
Epoch 4/10
52/52 - 12s - loss: 271.9179 - loglik: -2.7112e+02 - logprior: -7.9295e-01
Epoch 5/10
52/52 - 14s - loss: 270.3705 - loglik: -2.6964e+02 - logprior: -7.3035e-01
Epoch 6/10
52/52 - 12s - loss: 271.5837 - loglik: -2.7092e+02 - logprior: -6.5967e-01
Fitted a model with MAP estimate = -270.9196
Time for alignment: 260.9505
Computed alignments with likelihoods: ['-276.5916', '-272.3961', '-270.9196']
Best model has likelihood: -270.9196
SP score = 0.2047
Training of 3 independent models on file blm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbb82da2700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9eee1c910>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 818.8280 - loglik: -8.1729e+02 - logprior: -1.5391e+00
Epoch 2/10
37/37 - 21s - loss: 738.8871 - loglik: -7.3835e+02 - logprior: -5.4007e-01
Epoch 3/10
37/37 - 21s - loss: 722.5369 - loglik: -7.2194e+02 - logprior: -5.9377e-01
Epoch 4/10
37/37 - 21s - loss: 717.3295 - loglik: -7.1675e+02 - logprior: -5.7575e-01
Epoch 5/10
37/37 - 21s - loss: 714.9278 - loglik: -7.1430e+02 - logprior: -6.2283e-01
Epoch 6/10
37/37 - 21s - loss: 713.6300 - loglik: -7.1305e+02 - logprior: -5.8367e-01
Epoch 7/10
37/37 - 21s - loss: 713.7501 - loglik: -7.1305e+02 - logprior: -6.9924e-01
Fitted a model with MAP estimate = -712.6266
expansions: [(0, 4), (34, 2), (35, 1), (39, 1), (40, 1), (76, 2), (93, 1), (134, 1), (206, 3), (239, 1), (240, 1)]
discards: [103 105 106 124 162 163 164]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 265 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 27s - loss: 713.1912 - loglik: -7.1070e+02 - logprior: -2.4948e+00
Epoch 2/2
37/37 - 22s - loss: 708.4717 - loglik: -7.0784e+02 - logprior: -6.3340e-01
Fitted a model with MAP estimate = -706.6772
expansions: [(0, 2), (37, 1), (38, 1), (39, 1), (40, 1), (101, 17), (204, 11)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 298 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 30s - loss: 704.2158 - loglik: -7.0227e+02 - logprior: -1.9483e+00
Epoch 2/2
37/37 - 27s - loss: 697.8901 - loglik: -6.9728e+02 - logprior: -6.1074e-01
Fitted a model with MAP estimate = -696.1075
expansions: [(0, 2), (91, 1), (92, 1), (112, 1), (113, 7), (234, 3), (247, 1)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 313 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 31s - loss: 697.7787 - loglik: -6.9557e+02 - logprior: -2.2098e+00
Epoch 2/10
37/37 - 29s - loss: 694.7677 - loglik: -6.9444e+02 - logprior: -3.2990e-01
Epoch 3/10
37/37 - 29s - loss: 691.8826 - loglik: -6.9165e+02 - logprior: -2.3411e-01
Epoch 4/10
37/37 - 29s - loss: 695.1514 - loglik: -6.9515e+02 - logprior: -2.7346e-03
Fitted a model with MAP estimate = -693.0526
Time for alignment: 490.2238
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 25s - loss: 817.0540 - loglik: -8.1553e+02 - logprior: -1.5258e+00
Epoch 2/10
37/37 - 21s - loss: 732.2565 - loglik: -7.3170e+02 - logprior: -5.5792e-01
Epoch 3/10
37/37 - 21s - loss: 720.6217 - loglik: -7.2007e+02 - logprior: -5.5530e-01
Epoch 4/10
37/37 - 21s - loss: 715.0345 - loglik: -7.1443e+02 - logprior: -6.0681e-01
Epoch 5/10
37/37 - 21s - loss: 713.0480 - loglik: -7.1248e+02 - logprior: -5.7113e-01
Epoch 6/10
37/37 - 21s - loss: 711.9094 - loglik: -7.1125e+02 - logprior: -6.6252e-01
Epoch 7/10
37/37 - 21s - loss: 712.4037 - loglik: -7.1181e+02 - logprior: -5.9632e-01
Fitted a model with MAP estimate = -711.1822
expansions: [(0, 4), (34, 3), (35, 2), (39, 1), (67, 2), (98, 1), (100, 1), (133, 1), (206, 3), (239, 1), (240, 1)]
discards: [ 91 122 162 184]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 270 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 26s - loss: 710.4026 - loglik: -7.0782e+02 - logprior: -2.5809e+00
Epoch 2/2
37/37 - 23s - loss: 706.0284 - loglik: -7.0531e+02 - logprior: -7.1860e-01
Fitted a model with MAP estimate = -703.7592
expansions: [(0, 2), (39, 1), (44, 1), (209, 11)]
discards: [  0  78 117]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 282 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 27s - loss: 705.0571 - loglik: -7.0307e+02 - logprior: -1.9898e+00
Epoch 2/2
37/37 - 24s - loss: 701.0744 - loglik: -7.0058e+02 - logprior: -4.9640e-01
Fitted a model with MAP estimate = -700.5559
expansions: [(218, 4), (231, 1)]
discards: [  1 117]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 285 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 29s - loss: 703.5067 - loglik: -7.0193e+02 - logprior: -1.5792e+00
Epoch 2/10
37/37 - 25s - loss: 700.7074 - loglik: -7.0047e+02 - logprior: -2.4181e-01
Epoch 3/10
37/37 - 25s - loss: 700.0000 - loglik: -6.9989e+02 - logprior: -1.0952e-01
Epoch 4/10
37/37 - 25s - loss: 700.4319 - loglik: -7.0056e+02 - logprior: 0.1257
Fitted a model with MAP estimate = -699.8648
Time for alignment: 470.1819
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 24s - loss: 816.6025 - loglik: -8.1506e+02 - logprior: -1.5438e+00
Epoch 2/10
37/37 - 21s - loss: 729.6608 - loglik: -7.2909e+02 - logprior: -5.6654e-01
Epoch 3/10
37/37 - 21s - loss: 719.5549 - loglik: -7.1904e+02 - logprior: -5.1772e-01
Epoch 4/10
37/37 - 21s - loss: 716.0026 - loglik: -7.1547e+02 - logprior: -5.3722e-01
Epoch 5/10
37/37 - 21s - loss: 712.2195 - loglik: -7.1165e+02 - logprior: -5.6832e-01
Epoch 6/10
37/37 - 21s - loss: 712.6259 - loglik: -7.1187e+02 - logprior: -7.5120e-01
Fitted a model with MAP estimate = -711.4664
expansions: [(0, 4), (30, 1), (32, 4), (35, 1), (65, 1), (72, 2), (91, 3), (92, 12), (93, 4), (99, 1), (123, 1), (197, 12), (239, 1), (240, 1)]
discards: [102 103 105 188 189]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 297 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 29s - loss: 707.9405 - loglik: -7.0547e+02 - logprior: -2.4732e+00
Epoch 2/2
37/37 - 26s - loss: 697.1572 - loglik: -6.9627e+02 - logprior: -8.8992e-01
Fitted a model with MAP estimate = -695.6100
expansions: [(0, 2), (30, 1), (38, 2), (39, 1), (40, 1), (86, 1), (110, 4), (111, 1), (217, 1), (239, 3)]
discards: [  0 106]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 312 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 32s - loss: 696.0849 - loglik: -6.9400e+02 - logprior: -2.0845e+00
Epoch 2/2
37/37 - 28s - loss: 691.9219 - loglik: -6.9132e+02 - logprior: -5.9780e-01
Fitted a model with MAP estimate = -690.9752
expansions: [(0, 2), (90, 1)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 314 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 31s - loss: 692.3220 - loglik: -6.8997e+02 - logprior: -2.3526e+00
Epoch 2/10
37/37 - 29s - loss: 691.4834 - loglik: -6.9102e+02 - logprior: -4.5897e-01
Epoch 3/10
37/37 - 29s - loss: 690.6039 - loglik: -6.9041e+02 - logprior: -1.8988e-01
Epoch 4/10
37/37 - 29s - loss: 690.0509 - loglik: -6.9002e+02 - logprior: -2.7926e-02
Epoch 5/10
37/37 - 29s - loss: 689.5275 - loglik: -6.8962e+02 - logprior: 0.0903
Epoch 6/10
37/37 - 29s - loss: 689.0656 - loglik: -6.8936e+02 - logprior: 0.2928
Epoch 7/10
37/37 - 29s - loss: 690.5246 - loglik: -6.9088e+02 - logprior: 0.3548
Fitted a model with MAP estimate = -689.3875
Time for alignment: 575.1889
Computed alignments with likelihoods: ['-693.0526', '-699.8648', '-689.3875']
Best model has likelihood: -689.3875
SP score = 0.8811
Training of 3 independent models on file cyt3.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb7aee5d820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9f7a2c550>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 386.0511 - loglik: -2.7290e+02 - logprior: -1.1315e+02
Epoch 2/10
10/10 - 1s - loss: 289.1699 - loglik: -2.6187e+02 - logprior: -2.7299e+01
Epoch 3/10
10/10 - 1s - loss: 260.1383 - loglik: -2.5032e+02 - logprior: -9.8201e+00
Epoch 4/10
10/10 - 1s - loss: 245.6292 - loglik: -2.4228e+02 - logprior: -3.3535e+00
Epoch 5/10
10/10 - 1s - loss: 238.1184 - loglik: -2.3794e+02 - logprior: -1.8104e-01
Epoch 6/10
10/10 - 1s - loss: 233.0570 - loglik: -2.3469e+02 - logprior: 1.6333
Epoch 7/10
10/10 - 1s - loss: 229.9471 - loglik: -2.3255e+02 - logprior: 2.6020
Epoch 8/10
10/10 - 1s - loss: 228.1648 - loglik: -2.3142e+02 - logprior: 3.2535
Epoch 9/10
10/10 - 1s - loss: 227.0800 - loglik: -2.3084e+02 - logprior: 3.7621
Epoch 10/10
10/10 - 1s - loss: 226.3574 - loglik: -2.3052e+02 - logprior: 4.1671
Fitted a model with MAP estimate = -226.0598
expansions: [(0, 6), (37, 4), (51, 4), (64, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 376.5074 - loglik: -2.2775e+02 - logprior: -1.4876e+02
Epoch 2/2
10/10 - 1s - loss: 267.6614 - loglik: -2.2594e+02 - logprior: -4.1724e+01
Fitted a model with MAP estimate = -247.8278
expansions: [(0, 4)]
discards: [0 1 2 3 4 5]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 342.4958 - loglik: -2.2444e+02 - logprior: -1.1805e+02
Epoch 2/2
10/10 - 1s - loss: 254.7679 - loglik: -2.2459e+02 - logprior: -3.0180e+01
Fitted a model with MAP estimate = -239.4022
expansions: [(0, 4), (59, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 348.8969 - loglik: -2.2318e+02 - logprior: -1.2572e+02
Epoch 2/10
10/10 - 1s - loss: 259.0055 - loglik: -2.2348e+02 - logprior: -3.5521e+01
Epoch 3/10
10/10 - 1s - loss: 234.9481 - loglik: -2.2437e+02 - logprior: -1.0575e+01
Epoch 4/10
10/10 - 1s - loss: 224.7232 - loglik: -2.2497e+02 - logprior: 0.2437
Epoch 5/10
10/10 - 1s - loss: 220.1529 - loglik: -2.2524e+02 - logprior: 5.0898
Epoch 6/10
10/10 - 1s - loss: 217.7228 - loglik: -2.2542e+02 - logprior: 7.6974
Epoch 7/10
10/10 - 1s - loss: 216.2085 - loglik: -2.2548e+02 - logprior: 9.2671
Epoch 8/10
10/10 - 1s - loss: 215.1390 - loglik: -2.2545e+02 - logprior: 10.3155
Epoch 9/10
10/10 - 1s - loss: 214.2986 - loglik: -2.2539e+02 - logprior: 11.0908
Epoch 10/10
10/10 - 1s - loss: 213.5936 - loglik: -2.2533e+02 - logprior: 11.7378
Fitted a model with MAP estimate = -213.2422
Time for alignment: 36.6317
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 386.0511 - loglik: -2.7290e+02 - logprior: -1.1315e+02
Epoch 2/10
10/10 - 1s - loss: 289.1699 - loglik: -2.6187e+02 - logprior: -2.7299e+01
Epoch 3/10
10/10 - 1s - loss: 260.1383 - loglik: -2.5032e+02 - logprior: -9.8201e+00
Epoch 4/10
10/10 - 1s - loss: 245.6292 - loglik: -2.4228e+02 - logprior: -3.3535e+00
Epoch 5/10
10/10 - 1s - loss: 238.1184 - loglik: -2.3794e+02 - logprior: -1.8104e-01
Epoch 6/10
10/10 - 1s - loss: 233.0570 - loglik: -2.3469e+02 - logprior: 1.6333
Epoch 7/10
10/10 - 1s - loss: 229.9471 - loglik: -2.3255e+02 - logprior: 2.6020
Epoch 8/10
10/10 - 1s - loss: 228.1648 - loglik: -2.3142e+02 - logprior: 3.2535
Epoch 9/10
10/10 - 1s - loss: 227.0800 - loglik: -2.3084e+02 - logprior: 3.7621
Epoch 10/10
10/10 - 1s - loss: 226.3574 - loglik: -2.3052e+02 - logprior: 4.1671
Fitted a model with MAP estimate = -226.0598
expansions: [(0, 6), (37, 4), (51, 4), (64, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 376.5074 - loglik: -2.2775e+02 - logprior: -1.4876e+02
Epoch 2/2
10/10 - 1s - loss: 267.6614 - loglik: -2.2594e+02 - logprior: -4.1724e+01
Fitted a model with MAP estimate = -247.8278
expansions: [(0, 4)]
discards: [0 1 2 3 4 5]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 342.4958 - loglik: -2.2444e+02 - logprior: -1.1805e+02
Epoch 2/2
10/10 - 1s - loss: 254.7679 - loglik: -2.2459e+02 - logprior: -3.0180e+01
Fitted a model with MAP estimate = -239.4022
expansions: [(0, 4), (59, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 348.8969 - loglik: -2.2318e+02 - logprior: -1.2572e+02
Epoch 2/10
10/10 - 1s - loss: 259.0055 - loglik: -2.2348e+02 - logprior: -3.5521e+01
Epoch 3/10
10/10 - 1s - loss: 234.9481 - loglik: -2.2437e+02 - logprior: -1.0575e+01
Epoch 4/10
10/10 - 1s - loss: 224.7232 - loglik: -2.2497e+02 - logprior: 0.2437
Epoch 5/10
10/10 - 1s - loss: 220.1529 - loglik: -2.2524e+02 - logprior: 5.0898
Epoch 6/10
10/10 - 1s - loss: 217.7228 - loglik: -2.2542e+02 - logprior: 7.6974
Epoch 7/10
10/10 - 1s - loss: 216.2085 - loglik: -2.2548e+02 - logprior: 9.2671
Epoch 8/10
10/10 - 1s - loss: 215.1390 - loglik: -2.2545e+02 - logprior: 10.3155
Epoch 9/10
10/10 - 1s - loss: 214.2986 - loglik: -2.2539e+02 - logprior: 11.0908
Epoch 10/10
10/10 - 1s - loss: 213.5936 - loglik: -2.2533e+02 - logprior: 11.7378
Fitted a model with MAP estimate = -213.2422
Time for alignment: 36.9452
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 386.0511 - loglik: -2.7290e+02 - logprior: -1.1315e+02
Epoch 2/10
10/10 - 1s - loss: 289.1699 - loglik: -2.6187e+02 - logprior: -2.7299e+01
Epoch 3/10
10/10 - 1s - loss: 260.1383 - loglik: -2.5032e+02 - logprior: -9.8201e+00
Epoch 4/10
10/10 - 1s - loss: 245.6292 - loglik: -2.4228e+02 - logprior: -3.3535e+00
Epoch 5/10
10/10 - 1s - loss: 238.1184 - loglik: -2.3794e+02 - logprior: -1.8104e-01
Epoch 6/10
10/10 - 1s - loss: 233.0570 - loglik: -2.3469e+02 - logprior: 1.6333
Epoch 7/10
10/10 - 1s - loss: 229.9471 - loglik: -2.3255e+02 - logprior: 2.6020
Epoch 8/10
10/10 - 1s - loss: 228.1648 - loglik: -2.3142e+02 - logprior: 3.2535
Epoch 9/10
10/10 - 1s - loss: 227.0800 - loglik: -2.3084e+02 - logprior: 3.7621
Epoch 10/10
10/10 - 1s - loss: 226.3574 - loglik: -2.3052e+02 - logprior: 4.1671
Fitted a model with MAP estimate = -226.0598
expansions: [(0, 6), (37, 4), (51, 4), (64, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 376.5074 - loglik: -2.2775e+02 - logprior: -1.4876e+02
Epoch 2/2
10/10 - 1s - loss: 267.6614 - loglik: -2.2594e+02 - logprior: -4.1724e+01
Fitted a model with MAP estimate = -247.8278
expansions: [(0, 4)]
discards: [0 1 2 3 4 5]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 342.4958 - loglik: -2.2444e+02 - logprior: -1.1805e+02
Epoch 2/2
10/10 - 1s - loss: 254.7679 - loglik: -2.2459e+02 - logprior: -3.0180e+01
Fitted a model with MAP estimate = -239.4022
expansions: [(0, 4), (59, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 348.8969 - loglik: -2.2318e+02 - logprior: -1.2572e+02
Epoch 2/10
10/10 - 1s - loss: 259.0055 - loglik: -2.2348e+02 - logprior: -3.5521e+01
Epoch 3/10
10/10 - 1s - loss: 234.9481 - loglik: -2.2437e+02 - logprior: -1.0575e+01
Epoch 4/10
10/10 - 1s - loss: 224.7232 - loglik: -2.2497e+02 - logprior: 0.2437
Epoch 5/10
10/10 - 1s - loss: 220.1529 - loglik: -2.2524e+02 - logprior: 5.0898
Epoch 6/10
10/10 - 1s - loss: 217.7228 - loglik: -2.2542e+02 - logprior: 7.6974
Epoch 7/10
10/10 - 1s - loss: 216.2085 - loglik: -2.2548e+02 - logprior: 9.2671
Epoch 8/10
10/10 - 1s - loss: 215.1390 - loglik: -2.2545e+02 - logprior: 10.3155
Epoch 9/10
10/10 - 1s - loss: 214.2986 - loglik: -2.2539e+02 - logprior: 11.0908
Epoch 10/10
10/10 - 1s - loss: 213.5936 - loglik: -2.2533e+02 - logprior: 11.7378
Fitted a model with MAP estimate = -213.2422
Time for alignment: 36.2057
Computed alignments with likelihoods: ['-213.2422', '-213.2422', '-213.2422']
Best model has likelihood: -213.2422
SP score = 0.7846
Training of 3 independent models on file mofe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fba111b5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c0adce20>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 999.9001 - loglik: -9.9242e+02 - logprior: -7.4824e+00
Epoch 2/10
19/19 - 18s - loss: 876.1175 - loglik: -8.7732e+02 - logprior: 1.2074
Epoch 3/10
19/19 - 18s - loss: 821.4675 - loglik: -8.2183e+02 - logprior: 0.3631
Epoch 4/10
19/19 - 18s - loss: 805.2048 - loglik: -8.0529e+02 - logprior: 0.0839
Epoch 5/10
19/19 - 18s - loss: 798.3970 - loglik: -7.9847e+02 - logprior: 0.0720
Epoch 6/10
19/19 - 18s - loss: 796.0158 - loglik: -7.9603e+02 - logprior: 0.0143
Epoch 7/10
19/19 - 18s - loss: 799.5944 - loglik: -7.9952e+02 - logprior: -7.2265e-02
Fitted a model with MAP estimate = -794.3759
expansions: [(14, 1), (32, 1), (65, 1), (101, 2), (103, 1), (108, 1), (114, 1), (115, 2), (116, 1), (117, 1), (118, 1), (119, 1), (120, 6), (121, 3), (123, 3), (164, 1), (166, 1), (167, 1), (168, 1), (169, 1), (172, 1), (175, 2), (176, 1), (177, 1), (189, 2), (190, 2), (202, 1), (205, 1), (212, 2), (221, 2), (222, 1), (224, 1), (226, 1), (240, 1), (242, 1), (260, 1), (263, 1), (265, 7), (279, 3), (301, 1), (302, 3), (311, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 389 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 27s - loss: 792.7204 - loglik: -7.8152e+02 - logprior: -1.1199e+01
Epoch 2/2
19/19 - 24s - loss: 762.9235 - loglik: -7.5991e+02 - logprior: -3.0121e+00
Fitted a model with MAP estimate = -758.4008
expansions: [(0, 2), (383, 1)]
discards: [  0 104 138 139 146 147 225 318 338]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 383 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 29s - loss: 762.8972 - loglik: -7.5682e+02 - logprior: -6.0735e+00
Epoch 2/2
19/19 - 24s - loss: 750.1260 - loglik: -7.5255e+02 - logprior: 2.4239
Fitted a model with MAP estimate = -749.9397
expansions: []
discards: [  0 356]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 381 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 27s - loss: 767.0732 - loglik: -7.5787e+02 - logprior: -9.2068e+00
Epoch 2/10
19/19 - 24s - loss: 755.8425 - loglik: -7.5645e+02 - logprior: 0.6085
Epoch 3/10
19/19 - 24s - loss: 747.3464 - loglik: -7.5153e+02 - logprior: 4.1811
Epoch 4/10
19/19 - 24s - loss: 749.6671 - loglik: -7.5468e+02 - logprior: 5.0107
Fitted a model with MAP estimate = -748.3669
Time for alignment: 394.6946
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 21s - loss: 997.2954 - loglik: -9.8979e+02 - logprior: -7.5017e+00
Epoch 2/10
19/19 - 18s - loss: 869.5013 - loglik: -8.7056e+02 - logprior: 1.0550
Epoch 3/10
19/19 - 18s - loss: 816.6097 - loglik: -8.1678e+02 - logprior: 0.1706
Epoch 4/10
19/19 - 18s - loss: 801.0160 - loglik: -8.0116e+02 - logprior: 0.1451
Epoch 5/10
19/19 - 18s - loss: 791.9345 - loglik: -7.9206e+02 - logprior: 0.1299
Epoch 6/10
19/19 - 18s - loss: 791.0252 - loglik: -7.9102e+02 - logprior: -9.4575e-03
Epoch 7/10
19/19 - 18s - loss: 790.0242 - loglik: -7.8986e+02 - logprior: -1.6354e-01
Epoch 8/10
19/19 - 18s - loss: 791.1602 - loglik: -7.9096e+02 - logprior: -2.0006e-01
Fitted a model with MAP estimate = -789.0448
expansions: [(30, 1), (57, 1), (67, 1), (109, 1), (114, 1), (115, 3), (118, 1), (119, 1), (120, 1), (121, 10), (122, 1), (142, 1), (164, 1), (166, 1), (167, 1), (168, 2), (170, 1), (177, 1), (178, 2), (179, 1), (190, 2), (191, 2), (196, 1), (205, 1), (209, 1), (212, 2), (223, 2), (224, 2), (237, 1), (239, 1), (241, 2), (259, 1), (262, 1), (263, 4), (281, 2), (289, 1), (295, 1), (302, 2), (303, 2), (304, 2), (309, 1), (310, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 389 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 28s - loss: 786.9530 - loglik: -7.7563e+02 - logprior: -1.1327e+01
Epoch 2/2
19/19 - 25s - loss: 758.7527 - loglik: -7.5553e+02 - logprior: -3.2193e+00
Fitted a model with MAP estimate = -753.6914
expansions: [(0, 2), (265, 1), (315, 2), (316, 2)]
discards: [  0 117 143 144 193 207 225 336 365 366]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 386 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 27s - loss: 758.8752 - loglik: -7.5261e+02 - logprior: -6.2640e+00
Epoch 2/2
19/19 - 24s - loss: 747.7479 - loglik: -7.5013e+02 - logprior: 2.3782
Fitted a model with MAP estimate = -745.2215
expansions: [(142, 3)]
discards: [  0 314]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 387 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 27s - loss: 762.7244 - loglik: -7.5337e+02 - logprior: -9.3577e+00
Epoch 2/10
19/19 - 24s - loss: 746.4655 - loglik: -7.4698e+02 - logprior: 0.5156
Epoch 3/10
19/19 - 24s - loss: 744.9033 - loglik: -7.4901e+02 - logprior: 4.1057
Epoch 4/10
19/19 - 25s - loss: 743.1144 - loglik: -7.4799e+02 - logprior: 4.8754
Epoch 5/10
19/19 - 24s - loss: 744.3328 - loglik: -7.4967e+02 - logprior: 5.3403
Fitted a model with MAP estimate = -742.0466
Time for alignment: 440.9481
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 21s - loss: 998.7906 - loglik: -9.9129e+02 - logprior: -7.4989e+00
Epoch 2/10
19/19 - 18s - loss: 875.7114 - loglik: -8.7684e+02 - logprior: 1.1322
Epoch 3/10
19/19 - 18s - loss: 816.2260 - loglik: -8.1666e+02 - logprior: 0.4304
Epoch 4/10
19/19 - 18s - loss: 800.2574 - loglik: -8.0050e+02 - logprior: 0.2441
Epoch 5/10
19/19 - 18s - loss: 793.8557 - loglik: -7.9411e+02 - logprior: 0.2585
Epoch 6/10
19/19 - 18s - loss: 793.8509 - loglik: -7.9402e+02 - logprior: 0.1675
Epoch 7/10
19/19 - 18s - loss: 789.4531 - loglik: -7.8933e+02 - logprior: -1.2060e-01
Epoch 8/10
19/19 - 18s - loss: 790.4379 - loglik: -7.9051e+02 - logprior: 0.0737
Fitted a model with MAP estimate = -789.5411
expansions: [(14, 1), (40, 1), (98, 1), (99, 2), (107, 1), (112, 1), (113, 3), (116, 1), (117, 1), (118, 1), (121, 8), (143, 1), (162, 1), (163, 1), (165, 1), (166, 1), (169, 1), (172, 2), (175, 2), (176, 3), (177, 2), (197, 1), (204, 1), (220, 2), (222, 2), (223, 2), (236, 2), (237, 3), (256, 1), (257, 1), (262, 4), (278, 1), (279, 1), (302, 3), (304, 1), (311, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 383 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 27s - loss: 785.7390 - loglik: -7.7467e+02 - logprior: -1.1069e+01
Epoch 2/2
19/19 - 24s - loss: 757.5527 - loglik: -7.5499e+02 - logprior: -2.5649e+00
Fitted a model with MAP estimate = -754.5317
expansions: [(0, 2), (224, 1), (262, 1), (313, 1), (314, 1), (375, 1)]
discards: [  0 101 199 211 257 279]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 384 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 27s - loss: 757.6030 - loglik: -7.5155e+02 - logprior: -6.0486e+00
Epoch 2/2
19/19 - 24s - loss: 741.7281 - loglik: -7.4416e+02 - logprior: 2.4344
Fitted a model with MAP estimate = -742.7210
expansions: [(259, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 384 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 27s - loss: 758.4498 - loglik: -7.4932e+02 - logprior: -9.1315e+00
Epoch 2/10
19/19 - 24s - loss: 749.1888 - loglik: -7.4998e+02 - logprior: 0.7949
Epoch 3/10
19/19 - 24s - loss: 739.3958 - loglik: -7.4361e+02 - logprior: 4.2136
Epoch 4/10
19/19 - 24s - loss: 743.1003 - loglik: -7.4813e+02 - logprior: 5.0257
Fitted a model with MAP estimate = -740.4398
Time for alignment: 412.3520
Computed alignments with likelihoods: ['-748.3669', '-742.0466', '-740.4398']
Best model has likelihood: -740.4398
SP score = 0.7063
Training of 3 independent models on file ghf22.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cb6ba220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9ee9cb370>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 368.4236 - loglik: -3.1217e+02 - logprior: -5.6256e+01
Epoch 2/10
10/10 - 1s - loss: 291.0267 - loglik: -2.7766e+02 - logprior: -1.3366e+01
Epoch 3/10
10/10 - 1s - loss: 242.4973 - loglik: -2.3647e+02 - logprior: -6.0230e+00
Epoch 4/10
10/10 - 1s - loss: 214.9828 - loglik: -2.1116e+02 - logprior: -3.8252e+00
Epoch 5/10
10/10 - 1s - loss: 204.2803 - loglik: -2.0170e+02 - logprior: -2.5833e+00
Epoch 6/10
10/10 - 1s - loss: 198.4012 - loglik: -1.9646e+02 - logprior: -1.9449e+00
Epoch 7/10
10/10 - 1s - loss: 196.1611 - loglik: -1.9458e+02 - logprior: -1.5790e+00
Epoch 8/10
10/10 - 1s - loss: 194.6285 - loglik: -1.9324e+02 - logprior: -1.3912e+00
Epoch 9/10
10/10 - 1s - loss: 193.6145 - loglik: -1.9238e+02 - logprior: -1.2326e+00
Epoch 10/10
10/10 - 1s - loss: 193.6021 - loglik: -1.9257e+02 - logprior: -1.0344e+00
Fitted a model with MAP estimate = -193.2262
expansions: [(13, 3), (17, 3), (19, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (62, 1), (65, 1), (78, 4), (81, 1), (86, 1), (87, 1), (90, 1), (91, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 248.4837 - loglik: -1.8514e+02 - logprior: -6.3340e+01
Epoch 2/2
10/10 - 1s - loss: 198.4467 - loglik: -1.7381e+02 - logprior: -2.4635e+01
Fitted a model with MAP estimate = -189.7234
expansions: [(0, 2), (69, 1)]
discards: [ 0 20 98]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 219.0312 - loglik: -1.6927e+02 - logprior: -4.9760e+01
Epoch 2/2
10/10 - 1s - loss: 178.0104 - loglik: -1.6715e+02 - logprior: -1.0865e+01
Fitted a model with MAP estimate = -172.2213
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 228.6063 - loglik: -1.6842e+02 - logprior: -6.0187e+01
Epoch 2/10
10/10 - 1s - loss: 185.2281 - loglik: -1.6849e+02 - logprior: -1.6743e+01
Epoch 3/10
10/10 - 1s - loss: 172.3741 - loglik: -1.6848e+02 - logprior: -3.8923e+00
Epoch 4/10
10/10 - 1s - loss: 168.1090 - loglik: -1.6868e+02 - logprior: 0.5756
Epoch 5/10
10/10 - 1s - loss: 165.9151 - loglik: -1.6845e+02 - logprior: 2.5374
Epoch 6/10
10/10 - 1s - loss: 164.8054 - loglik: -1.6839e+02 - logprior: 3.5800
Epoch 7/10
10/10 - 1s - loss: 164.4260 - loglik: -1.6870e+02 - logprior: 4.2698
Epoch 8/10
10/10 - 1s - loss: 164.1135 - loglik: -1.6897e+02 - logprior: 4.8535
Epoch 9/10
10/10 - 1s - loss: 163.6897 - loglik: -1.6904e+02 - logprior: 5.3479
Epoch 10/10
10/10 - 1s - loss: 163.1212 - loglik: -1.6886e+02 - logprior: 5.7352
Fitted a model with MAP estimate = -163.0962
Time for alignment: 43.4223
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 368.7397 - loglik: -3.1248e+02 - logprior: -5.6256e+01
Epoch 2/10
10/10 - 1s - loss: 291.1599 - loglik: -2.7779e+02 - logprior: -1.3368e+01
Epoch 3/10
10/10 - 1s - loss: 242.0607 - loglik: -2.3604e+02 - logprior: -6.0181e+00
Epoch 4/10
10/10 - 1s - loss: 214.7168 - loglik: -2.1081e+02 - logprior: -3.9030e+00
Epoch 5/10
10/10 - 1s - loss: 203.5246 - loglik: -2.0073e+02 - logprior: -2.7902e+00
Epoch 6/10
10/10 - 1s - loss: 198.9249 - loglik: -1.9667e+02 - logprior: -2.2511e+00
Epoch 7/10
10/10 - 1s - loss: 195.3787 - loglik: -1.9352e+02 - logprior: -1.8567e+00
Epoch 8/10
10/10 - 1s - loss: 194.3067 - loglik: -1.9262e+02 - logprior: -1.6882e+00
Epoch 9/10
10/10 - 1s - loss: 193.4194 - loglik: -1.9198e+02 - logprior: -1.4367e+00
Epoch 10/10
10/10 - 1s - loss: 193.2856 - loglik: -1.9203e+02 - logprior: -1.2570e+00
Fitted a model with MAP estimate = -192.9402
expansions: [(13, 3), (16, 1), (17, 3), (18, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (60, 1), (62, 1), (76, 1), (77, 3), (78, 2), (87, 1), (89, 1), (90, 1), (91, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 248.6366 - loglik: -1.8524e+02 - logprior: -6.3394e+01
Epoch 2/2
10/10 - 1s - loss: 198.0438 - loglik: -1.7338e+02 - logprior: -2.4660e+01
Fitted a model with MAP estimate = -189.1004
expansions: [(0, 2), (70, 1)]
discards: [ 0 20 21 98 99]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 219.0443 - loglik: -1.6920e+02 - logprior: -4.9848e+01
Epoch 2/2
10/10 - 1s - loss: 178.0500 - loglik: -1.6716e+02 - logprior: -1.0887e+01
Fitted a model with MAP estimate = -172.0790
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 228.5335 - loglik: -1.6823e+02 - logprior: -6.0301e+01
Epoch 2/10
10/10 - 1s - loss: 185.0939 - loglik: -1.6804e+02 - logprior: -1.7049e+01
Epoch 3/10
10/10 - 1s - loss: 172.7910 - loglik: -1.6877e+02 - logprior: -4.0193e+00
Epoch 4/10
10/10 - 1s - loss: 167.8833 - loglik: -1.6846e+02 - logprior: 0.5756
Epoch 5/10
10/10 - 1s - loss: 166.1491 - loglik: -1.6871e+02 - logprior: 2.5561
Epoch 6/10
10/10 - 1s - loss: 164.9354 - loglik: -1.6854e+02 - logprior: 3.6009
Epoch 7/10
10/10 - 1s - loss: 164.3521 - loglik: -1.6864e+02 - logprior: 4.2896
Epoch 8/10
10/10 - 1s - loss: 164.0366 - loglik: -1.6889e+02 - logprior: 4.8542
Epoch 9/10
10/10 - 1s - loss: 163.7153 - loglik: -1.6906e+02 - logprior: 5.3441
Epoch 10/10
10/10 - 1s - loss: 163.3179 - loglik: -1.6905e+02 - logprior: 5.7303
Fitted a model with MAP estimate = -163.1269
Time for alignment: 42.6430
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 368.7424 - loglik: -3.1249e+02 - logprior: -5.6255e+01
Epoch 2/10
10/10 - 1s - loss: 291.5311 - loglik: -2.7817e+02 - logprior: -1.3364e+01
Epoch 3/10
10/10 - 1s - loss: 243.7360 - loglik: -2.3770e+02 - logprior: -6.0362e+00
Epoch 4/10
10/10 - 1s - loss: 216.3177 - loglik: -2.1233e+02 - logprior: -3.9917e+00
Epoch 5/10
10/10 - 1s - loss: 204.2256 - loglik: -2.0128e+02 - logprior: -2.9438e+00
Epoch 6/10
10/10 - 1s - loss: 198.4324 - loglik: -1.9603e+02 - logprior: -2.4021e+00
Epoch 7/10
10/10 - 1s - loss: 195.5995 - loglik: -1.9358e+02 - logprior: -2.0244e+00
Epoch 8/10
10/10 - 1s - loss: 194.5342 - loglik: -1.9271e+02 - logprior: -1.8207e+00
Epoch 9/10
10/10 - 1s - loss: 193.9077 - loglik: -1.9224e+02 - logprior: -1.6670e+00
Epoch 10/10
10/10 - 1s - loss: 193.4769 - loglik: -1.9198e+02 - logprior: -1.4936e+00
Fitted a model with MAP estimate = -193.3377
expansions: [(13, 3), (17, 3), (19, 1), (32, 1), (33, 1), (34, 2), (35, 1), (52, 1), (54, 2), (56, 2), (57, 1), (59, 1), (62, 1), (76, 1), (77, 3), (78, 1), (81, 1), (86, 1), (87, 1), (90, 1), (91, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 248.7728 - loglik: -1.8548e+02 - logprior: -6.3291e+01
Epoch 2/2
10/10 - 1s - loss: 197.7180 - loglik: -1.7323e+02 - logprior: -2.4486e+01
Fitted a model with MAP estimate = -189.0894
expansions: [(0, 2)]
discards: [ 0 20 97 98]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 218.0840 - loglik: -1.6831e+02 - logprior: -4.9772e+01
Epoch 2/2
10/10 - 1s - loss: 177.9145 - loglik: -1.6705e+02 - logprior: -1.0863e+01
Fitted a model with MAP estimate = -171.9788
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 228.6698 - loglik: -1.6835e+02 - logprior: -6.0324e+01
Epoch 2/10
10/10 - 1s - loss: 185.6593 - loglik: -1.6868e+02 - logprior: -1.6984e+01
Epoch 3/10
10/10 - 1s - loss: 172.4477 - loglik: -1.6848e+02 - logprior: -3.9633e+00
Epoch 4/10
10/10 - 1s - loss: 168.3211 - loglik: -1.6893e+02 - logprior: 0.6058
Epoch 5/10
10/10 - 1s - loss: 166.3146 - loglik: -1.6891e+02 - logprior: 2.5914
Epoch 6/10
10/10 - 1s - loss: 164.9653 - loglik: -1.6861e+02 - logprior: 3.6433
Epoch 7/10
10/10 - 1s - loss: 164.9235 - loglik: -1.6926e+02 - logprior: 4.3333
Epoch 8/10
10/10 - 1s - loss: 164.2165 - loglik: -1.6913e+02 - logprior: 4.9131
Epoch 9/10
10/10 - 1s - loss: 163.7630 - loglik: -1.6916e+02 - logprior: 5.4017
Epoch 10/10
10/10 - 1s - loss: 163.5624 - loglik: -1.6936e+02 - logprior: 5.7951
Fitted a model with MAP estimate = -163.3533
Time for alignment: 43.4104
Computed alignments with likelihoods: ['-163.0962', '-163.1269', '-163.3533']
Best model has likelihood: -163.0962
SP score = 0.9351
Training of 3 independent models on file flav.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbb82e00790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb7aefe1430>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 396.6357 - loglik: -3.8855e+02 - logprior: -8.0845e+00
Epoch 2/10
13/13 - 2s - loss: 355.1665 - loglik: -3.5334e+02 - logprior: -1.8294e+00
Epoch 3/10
13/13 - 2s - loss: 325.8470 - loglik: -3.2434e+02 - logprior: -1.5104e+00
Epoch 4/10
13/13 - 2s - loss: 312.9070 - loglik: -3.1116e+02 - logprior: -1.7475e+00
Epoch 5/10
13/13 - 2s - loss: 308.1876 - loglik: -3.0655e+02 - logprior: -1.6350e+00
Epoch 6/10
13/13 - 2s - loss: 306.1351 - loglik: -3.0459e+02 - logprior: -1.5500e+00
Epoch 7/10
13/13 - 2s - loss: 304.8419 - loglik: -3.0322e+02 - logprior: -1.6173e+00
Epoch 8/10
13/13 - 2s - loss: 304.4284 - loglik: -3.0281e+02 - logprior: -1.6228e+00
Epoch 9/10
13/13 - 2s - loss: 304.1824 - loglik: -3.0258e+02 - logprior: -1.6042e+00
Epoch 10/10
13/13 - 2s - loss: 304.3886 - loglik: -3.0279e+02 - logprior: -1.5995e+00
Fitted a model with MAP estimate = -304.0801
expansions: [(6, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (33, 1), (51, 1), (52, 1), (55, 1), (58, 2), (64, 2), (79, 2), (80, 1), (81, 2), (82, 1), (99, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 312.2544 - loglik: -3.0282e+02 - logprior: -9.4336e+00
Epoch 2/2
13/13 - 3s - loss: 296.9998 - loglik: -2.9267e+02 - logprior: -4.3249e+00
Fitted a model with MAP estimate = -294.8430
expansions: [(0, 2)]
discards: [ 0 35 80 98]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 296.3523 - loglik: -2.8925e+02 - logprior: -7.1038e+00
Epoch 2/2
13/13 - 3s - loss: 290.5456 - loglik: -2.8880e+02 - logprior: -1.7458e+00
Fitted a model with MAP estimate = -288.8829
expansions: []
discards: [ 0 73]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 300.0156 - loglik: -2.9125e+02 - logprior: -8.7678e+00
Epoch 2/10
13/13 - 3s - loss: 291.7611 - loglik: -2.8926e+02 - logprior: -2.5016e+00
Epoch 3/10
13/13 - 3s - loss: 289.3196 - loglik: -2.8829e+02 - logprior: -1.0258e+00
Epoch 4/10
13/13 - 3s - loss: 289.6066 - loglik: -2.8896e+02 - logprior: -6.4900e-01
Fitted a model with MAP estimate = -288.8700
Time for alignment: 69.4656
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 396.5786 - loglik: -3.8850e+02 - logprior: -8.0752e+00
Epoch 2/10
13/13 - 2s - loss: 355.2730 - loglik: -3.5345e+02 - logprior: -1.8249e+00
Epoch 3/10
13/13 - 2s - loss: 327.2084 - loglik: -3.2571e+02 - logprior: -1.4992e+00
Epoch 4/10
13/13 - 2s - loss: 314.7463 - loglik: -3.1303e+02 - logprior: -1.7209e+00
Epoch 5/10
13/13 - 2s - loss: 308.9940 - loglik: -3.0734e+02 - logprior: -1.6503e+00
Epoch 6/10
13/13 - 2s - loss: 306.2285 - loglik: -3.0467e+02 - logprior: -1.5571e+00
Epoch 7/10
13/13 - 2s - loss: 306.0048 - loglik: -3.0442e+02 - logprior: -1.5872e+00
Epoch 8/10
13/13 - 2s - loss: 305.3656 - loglik: -3.0378e+02 - logprior: -1.5903e+00
Epoch 9/10
13/13 - 2s - loss: 304.7026 - loglik: -3.0313e+02 - logprior: -1.5700e+00
Epoch 10/10
13/13 - 2s - loss: 305.0538 - loglik: -3.0349e+02 - logprior: -1.5673e+00
Fitted a model with MAP estimate = -304.6109
expansions: [(6, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 2), (25, 2), (26, 1), (27, 1), (28, 1), (51, 1), (52, 1), (55, 1), (58, 3), (64, 2), (80, 1), (81, 4), (82, 1), (99, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 135 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 311.8392 - loglik: -3.0241e+02 - logprior: -9.4336e+00
Epoch 2/2
13/13 - 3s - loss: 295.5276 - loglik: -2.9116e+02 - logprior: -4.3676e+00
Fitted a model with MAP estimate = -293.9105
expansions: [(0, 2)]
discards: [  0  28  73  74  81 101]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 296.8233 - loglik: -2.8968e+02 - logprior: -7.1417e+00
Epoch 2/2
13/13 - 3s - loss: 290.9072 - loglik: -2.8910e+02 - logprior: -1.8091e+00
Fitted a model with MAP estimate = -289.3495
expansions: [(122, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 299.9324 - loglik: -2.9094e+02 - logprior: -8.9907e+00
Epoch 2/10
13/13 - 3s - loss: 292.4716 - loglik: -2.8974e+02 - logprior: -2.7313e+00
Epoch 3/10
13/13 - 3s - loss: 289.9304 - loglik: -2.8882e+02 - logprior: -1.1071e+00
Epoch 4/10
13/13 - 3s - loss: 288.9281 - loglik: -2.8819e+02 - logprior: -7.3321e-01
Epoch 5/10
13/13 - 3s - loss: 288.6143 - loglik: -2.8808e+02 - logprior: -5.3623e-01
Epoch 6/10
13/13 - 3s - loss: 288.2697 - loglik: -2.8785e+02 - logprior: -4.2367e-01
Epoch 7/10
13/13 - 3s - loss: 287.8470 - loglik: -2.8749e+02 - logprior: -3.6002e-01
Epoch 8/10
13/13 - 3s - loss: 287.1113 - loglik: -2.8680e+02 - logprior: -3.1084e-01
Epoch 9/10
13/13 - 3s - loss: 288.0603 - loglik: -2.8781e+02 - logprior: -2.5129e-01
Fitted a model with MAP estimate = -287.3494
Time for alignment: 80.3423
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 396.7513 - loglik: -3.8869e+02 - logprior: -8.0651e+00
Epoch 2/10
13/13 - 2s - loss: 355.8008 - loglik: -3.5397e+02 - logprior: -1.8269e+00
Epoch 3/10
13/13 - 2s - loss: 329.1481 - loglik: -3.2768e+02 - logprior: -1.4723e+00
Epoch 4/10
13/13 - 2s - loss: 314.1950 - loglik: -3.1254e+02 - logprior: -1.6525e+00
Epoch 5/10
13/13 - 2s - loss: 309.4168 - loglik: -3.0790e+02 - logprior: -1.5166e+00
Epoch 6/10
13/13 - 2s - loss: 306.8946 - loglik: -3.0545e+02 - logprior: -1.4411e+00
Epoch 7/10
13/13 - 2s - loss: 306.2950 - loglik: -3.0482e+02 - logprior: -1.4730e+00
Epoch 8/10
13/13 - 2s - loss: 305.2876 - loglik: -3.0383e+02 - logprior: -1.4604e+00
Epoch 9/10
13/13 - 2s - loss: 305.6304 - loglik: -3.0419e+02 - logprior: -1.4444e+00
Fitted a model with MAP estimate = -305.0960
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 2), (26, 2), (27, 2), (28, 2), (51, 1), (52, 1), (55, 1), (57, 1), (73, 2), (76, 1), (81, 4), (82, 1), (100, 2), (101, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 137 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 312.6079 - loglik: -3.0323e+02 - logprior: -9.3789e+00
Epoch 2/2
13/13 - 3s - loss: 298.1696 - loglik: -2.9385e+02 - logprior: -4.3147e+00
Fitted a model with MAP estimate = -295.0672
expansions: [(0, 2)]
discards: [  0  31  36  39  91 103 128]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 297.4532 - loglik: -2.9038e+02 - logprior: -7.0734e+00
Epoch 2/2
13/13 - 3s - loss: 288.5093 - loglik: -2.8674e+02 - logprior: -1.7681e+00
Fitted a model with MAP estimate = -287.9055
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 297.9828 - loglik: -2.8918e+02 - logprior: -8.8036e+00
Epoch 2/10
13/13 - 3s - loss: 290.6931 - loglik: -2.8815e+02 - logprior: -2.5443e+00
Epoch 3/10
13/13 - 3s - loss: 288.1637 - loglik: -2.8710e+02 - logprior: -1.0663e+00
Epoch 4/10
13/13 - 3s - loss: 287.2093 - loglik: -2.8653e+02 - logprior: -6.7476e-01
Epoch 5/10
13/13 - 3s - loss: 287.3373 - loglik: -2.8686e+02 - logprior: -4.8099e-01
Fitted a model with MAP estimate = -287.2031
Time for alignment: 67.5696
Computed alignments with likelihoods: ['-288.8700', '-287.3494', '-287.2031']
Best model has likelihood: -287.2031
SP score = 0.9141
Training of 3 independent models on file sodfe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb7ad2102b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbaf0a061c0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 222.4515 - loglik: -2.1386e+02 - logprior: -8.5913e+00
Epoch 2/10
13/13 - 1s - loss: 172.4883 - loglik: -1.7027e+02 - logprior: -2.2230e+00
Epoch 3/10
13/13 - 1s - loss: 143.7650 - loglik: -1.4196e+02 - logprior: -1.8091e+00
Epoch 4/10
13/13 - 1s - loss: 136.1804 - loglik: -1.3448e+02 - logprior: -1.7015e+00
Epoch 5/10
13/13 - 1s - loss: 134.0976 - loglik: -1.3252e+02 - logprior: -1.5768e+00
Epoch 6/10
13/13 - 1s - loss: 133.4556 - loglik: -1.3186e+02 - logprior: -1.5933e+00
Epoch 7/10
13/13 - 1s - loss: 132.9130 - loglik: -1.3138e+02 - logprior: -1.5326e+00
Epoch 8/10
13/13 - 1s - loss: 132.7554 - loglik: -1.3122e+02 - logprior: -1.5327e+00
Epoch 9/10
13/13 - 2s - loss: 132.5422 - loglik: -1.3105e+02 - logprior: -1.4905e+00
Epoch 10/10
13/13 - 2s - loss: 132.5160 - loglik: -1.3103e+02 - logprior: -1.4846e+00
Fitted a model with MAP estimate = -132.4150
expansions: [(0, 4), (13, 1), (16, 1), (35, 2), (36, 1), (37, 2), (38, 2), (43, 3), (44, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 136.9060 - loglik: -1.2672e+02 - logprior: -1.0182e+01
Epoch 2/2
13/13 - 1s - loss: 121.1887 - loglik: -1.1811e+02 - logprior: -3.0755e+00
Fitted a model with MAP estimate = -118.6050
expansions: [(0, 2)]
discards: [46 49]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 126.0110 - loglik: -1.1596e+02 - logprior: -1.0055e+01
Epoch 2/2
13/13 - 1s - loss: 117.2756 - loglik: -1.1407e+02 - logprior: -3.2050e+00
Fitted a model with MAP estimate = -115.6907
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 121.2076 - loglik: -1.1328e+02 - logprior: -7.9274e+00
Epoch 2/10
13/13 - 1s - loss: 115.2348 - loglik: -1.1298e+02 - logprior: -2.2542e+00
Epoch 3/10
13/13 - 1s - loss: 114.5093 - loglik: -1.1281e+02 - logprior: -1.6982e+00
Epoch 4/10
13/13 - 1s - loss: 114.7066 - loglik: -1.1337e+02 - logprior: -1.3405e+00
Fitted a model with MAP estimate = -114.3577
Time for alignment: 47.4163
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 223.0551 - loglik: -2.1447e+02 - logprior: -8.5853e+00
Epoch 2/10
13/13 - 1s - loss: 175.5148 - loglik: -1.7327e+02 - logprior: -2.2474e+00
Epoch 3/10
13/13 - 1s - loss: 147.1947 - loglik: -1.4524e+02 - logprior: -1.9513e+00
Epoch 4/10
13/13 - 1s - loss: 139.8389 - loglik: -1.3793e+02 - logprior: -1.9089e+00
Epoch 5/10
13/13 - 1s - loss: 137.5764 - loglik: -1.3585e+02 - logprior: -1.7251e+00
Epoch 6/10
13/13 - 1s - loss: 136.6018 - loglik: -1.3488e+02 - logprior: -1.7254e+00
Epoch 7/10
13/13 - 1s - loss: 136.3199 - loglik: -1.3463e+02 - logprior: -1.6929e+00
Epoch 8/10
13/13 - 1s - loss: 135.1884 - loglik: -1.3348e+02 - logprior: -1.7088e+00
Epoch 9/10
13/13 - 1s - loss: 135.4091 - loglik: -1.3370e+02 - logprior: -1.7043e+00
Fitted a model with MAP estimate = -135.2527
expansions: [(0, 3), (13, 1), (14, 2), (15, 1), (16, 1), (33, 1), (35, 2), (36, 1), (37, 2), (38, 2), (43, 2), (44, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 138.3600 - loglik: -1.2817e+02 - logprior: -1.0186e+01
Epoch 2/2
13/13 - 1s - loss: 120.1984 - loglik: -1.1713e+02 - logprior: -3.0700e+00
Fitted a model with MAP estimate = -117.7379
expansions: []
discards: [45 49 52 64]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 124.2792 - loglik: -1.1634e+02 - logprior: -7.9342e+00
Epoch 2/2
13/13 - 1s - loss: 117.1798 - loglik: -1.1484e+02 - logprior: -2.3437e+00
Fitted a model with MAP estimate = -116.7453
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 122.5660 - loglik: -1.1470e+02 - logprior: -7.8653e+00
Epoch 2/10
13/13 - 1s - loss: 117.0934 - loglik: -1.1472e+02 - logprior: -2.3721e+00
Epoch 3/10
13/13 - 1s - loss: 116.0545 - loglik: -1.1431e+02 - logprior: -1.7472e+00
Epoch 4/10
13/13 - 1s - loss: 116.4236 - loglik: -1.1501e+02 - logprior: -1.4103e+00
Fitted a model with MAP estimate = -115.9680
Time for alignment: 43.6195
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 222.6029 - loglik: -2.1401e+02 - logprior: -8.5880e+00
Epoch 2/10
13/13 - 1s - loss: 173.0399 - loglik: -1.7080e+02 - logprior: -2.2358e+00
Epoch 3/10
13/13 - 1s - loss: 144.8270 - loglik: -1.4298e+02 - logprior: -1.8423e+00
Epoch 4/10
13/13 - 1s - loss: 136.6846 - loglik: -1.3495e+02 - logprior: -1.7385e+00
Epoch 5/10
13/13 - 1s - loss: 134.4469 - loglik: -1.3283e+02 - logprior: -1.6180e+00
Epoch 6/10
13/13 - 1s - loss: 132.9068 - loglik: -1.3128e+02 - logprior: -1.6315e+00
Epoch 7/10
13/13 - 1s - loss: 132.5709 - loglik: -1.3099e+02 - logprior: -1.5828e+00
Epoch 8/10
13/13 - 1s - loss: 132.3787 - loglik: -1.3080e+02 - logprior: -1.5815e+00
Epoch 9/10
13/13 - 1s - loss: 132.3353 - loglik: -1.3078e+02 - logprior: -1.5539e+00
Epoch 10/10
13/13 - 1s - loss: 131.9857 - loglik: -1.3044e+02 - logprior: -1.5411e+00
Fitted a model with MAP estimate = -132.0090
expansions: [(0, 4), (13, 1), (16, 1), (33, 1), (35, 2), (36, 1), (37, 2), (38, 2), (43, 3), (44, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 136.9324 - loglik: -1.2670e+02 - logprior: -1.0235e+01
Epoch 2/2
13/13 - 1s - loss: 121.1186 - loglik: -1.1799e+02 - logprior: -3.1307e+00
Fitted a model with MAP estimate = -118.2972
expansions: [(0, 2)]
discards: [43 47 50]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 125.7352 - loglik: -1.1568e+02 - logprior: -1.0055e+01
Epoch 2/2
13/13 - 1s - loss: 117.3562 - loglik: -1.1413e+02 - logprior: -3.2233e+00
Fitted a model with MAP estimate = -115.7069
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 121.0990 - loglik: -1.1317e+02 - logprior: -7.9328e+00
Epoch 2/10
13/13 - 1s - loss: 115.4132 - loglik: -1.1315e+02 - logprior: -2.2610e+00
Epoch 3/10
13/13 - 1s - loss: 114.6253 - loglik: -1.1292e+02 - logprior: -1.7013e+00
Epoch 4/10
13/13 - 1s - loss: 114.5173 - loglik: -1.1318e+02 - logprior: -1.3398e+00
Epoch 5/10
13/13 - 2s - loss: 114.5324 - loglik: -1.1328e+02 - logprior: -1.2485e+00
Fitted a model with MAP estimate = -114.2642
Time for alignment: 46.4342
Computed alignments with likelihoods: ['-114.3577', '-115.9680', '-114.2642']
Best model has likelihood: -114.2642
SP score = 0.4987
Training of 3 independent models on file zf-CCHH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c1ad43a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb1e4272250>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 6s - loss: 60.0068 - loglik: -5.9147e+01 - logprior: -8.5975e-01
Epoch 2/10
41/41 - 1s - loss: 46.6545 - loglik: -4.5752e+01 - logprior: -9.0262e-01
Epoch 3/10
41/41 - 1s - loss: 45.7484 - loglik: -4.4869e+01 - logprior: -8.7986e-01
Epoch 4/10
41/41 - 1s - loss: 45.5845 - loglik: -4.4711e+01 - logprior: -8.7327e-01
Epoch 5/10
41/41 - 1s - loss: 45.4615 - loglik: -4.4589e+01 - logprior: -8.7266e-01
Epoch 6/10
41/41 - 1s - loss: 45.3703 - loglik: -4.4504e+01 - logprior: -8.6654e-01
Epoch 7/10
41/41 - 1s - loss: 45.3550 - loglik: -4.4490e+01 - logprior: -8.6471e-01
Epoch 8/10
41/41 - 1s - loss: 45.3708 - loglik: -4.4507e+01 - logprior: -8.6416e-01
Fitted a model with MAP estimate = -44.9729
expansions: [(4, 1), (9, 2), (10, 1), (11, 1), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.6679 - loglik: -4.3600e+01 - logprior: -1.0676e+00
Epoch 2/2
41/41 - 1s - loss: 43.2909 - loglik: -4.2443e+01 - logprior: -8.4809e-01
Fitted a model with MAP estimate = -43.0128
expansions: []
discards: [11]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 43.7641 - loglik: -4.2718e+01 - logprior: -1.0464e+00
Epoch 2/2
41/41 - 1s - loss: 43.2354 - loglik: -4.2404e+01 - logprior: -8.3111e-01
Fitted a model with MAP estimate = -43.0159
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 42.6814 - loglik: -4.2006e+01 - logprior: -6.7578e-01
Epoch 2/10
58/58 - 2s - loss: 42.2899 - loglik: -4.1712e+01 - logprior: -5.7829e-01
Epoch 3/10
58/58 - 2s - loss: 42.2502 - loglik: -4.1675e+01 - logprior: -5.7482e-01
Epoch 4/10
58/58 - 2s - loss: 42.0825 - loglik: -4.1514e+01 - logprior: -5.6832e-01
Epoch 5/10
58/58 - 2s - loss: 42.1076 - loglik: -4.1540e+01 - logprior: -5.6796e-01
Fitted a model with MAP estimate = -42.0478
Time for alignment: 61.4258
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.5809 - loglik: -5.8711e+01 - logprior: -8.6945e-01
Epoch 2/10
41/41 - 1s - loss: 46.3717 - loglik: -4.5472e+01 - logprior: -8.9997e-01
Epoch 3/10
41/41 - 1s - loss: 45.6863 - loglik: -4.4805e+01 - logprior: -8.8154e-01
Epoch 4/10
41/41 - 1s - loss: 45.6249 - loglik: -4.4750e+01 - logprior: -8.7489e-01
Epoch 5/10
41/41 - 1s - loss: 45.4624 - loglik: -4.4592e+01 - logprior: -8.7014e-01
Epoch 6/10
41/41 - 1s - loss: 45.3676 - loglik: -4.4500e+01 - logprior: -8.6720e-01
Epoch 7/10
41/41 - 1s - loss: 45.3780 - loglik: -4.4512e+01 - logprior: -8.6599e-01
Fitted a model with MAP estimate = -44.9836
expansions: [(4, 1), (9, 2), (10, 2), (11, 1), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.9862 - loglik: -4.3910e+01 - logprior: -1.0757e+00
Epoch 2/2
41/41 - 1s - loss: 43.2856 - loglik: -4.2428e+01 - logprior: -8.5718e-01
Fitted a model with MAP estimate = -43.0311
expansions: []
discards: [11 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 43.8717 - loglik: -4.2826e+01 - logprior: -1.0461e+00
Epoch 2/2
41/41 - 1s - loss: 43.1020 - loglik: -4.2270e+01 - logprior: -8.3213e-01
Fitted a model with MAP estimate = -43.0461
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 6s - loss: 42.5995 - loglik: -4.1922e+01 - logprior: -6.7797e-01
Epoch 2/10
58/58 - 2s - loss: 42.2740 - loglik: -4.1695e+01 - logprior: -5.7857e-01
Epoch 3/10
58/58 - 2s - loss: 42.3256 - loglik: -4.1753e+01 - logprior: -5.7231e-01
Fitted a model with MAP estimate = -42.1062
Time for alignment: 54.8909
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.6233 - loglik: -5.8750e+01 - logprior: -8.7336e-01
Epoch 2/10
41/41 - 1s - loss: 46.3874 - loglik: -4.5490e+01 - logprior: -8.9717e-01
Epoch 3/10
41/41 - 1s - loss: 45.7009 - loglik: -4.4821e+01 - logprior: -8.8003e-01
Epoch 4/10
41/41 - 1s - loss: 45.5973 - loglik: -4.4723e+01 - logprior: -8.7475e-01
Epoch 5/10
41/41 - 1s - loss: 45.4602 - loglik: -4.4590e+01 - logprior: -8.7023e-01
Epoch 6/10
41/41 - 1s - loss: 45.4188 - loglik: -4.4553e+01 - logprior: -8.6629e-01
Epoch 7/10
41/41 - 1s - loss: 45.3079 - loglik: -4.4440e+01 - logprior: -8.6748e-01
Epoch 8/10
41/41 - 1s - loss: 45.3340 - loglik: -4.4471e+01 - logprior: -8.6349e-01
Fitted a model with MAP estimate = -44.9322
expansions: [(4, 1), (9, 2), (10, 1), (11, 1), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.7869 - loglik: -4.3720e+01 - logprior: -1.0668e+00
Epoch 2/2
41/41 - 1s - loss: 43.1708 - loglik: -4.2323e+01 - logprior: -8.4802e-01
Fitted a model with MAP estimate = -42.9782
expansions: []
discards: [11]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 43.7456 - loglik: -4.2700e+01 - logprior: -1.0456e+00
Epoch 2/2
41/41 - 1s - loss: 43.2288 - loglik: -4.2396e+01 - logprior: -8.3314e-01
Fitted a model with MAP estimate = -43.0639
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 42.7153 - loglik: -4.2038e+01 - logprior: -6.7702e-01
Epoch 2/10
58/58 - 2s - loss: 42.2736 - loglik: -4.1694e+01 - logprior: -5.7929e-01
Epoch 3/10
58/58 - 2s - loss: 42.2221 - loglik: -4.1650e+01 - logprior: -5.7252e-01
Epoch 4/10
58/58 - 2s - loss: 42.1084 - loglik: -4.1538e+01 - logprior: -5.7017e-01
Epoch 5/10
58/58 - 2s - loss: 42.1194 - loglik: -4.1552e+01 - logprior: -5.6701e-01
Fitted a model with MAP estimate = -42.0453
Time for alignment: 59.5203
Computed alignments with likelihoods: ['-42.0478', '-42.1062', '-42.0453']
Best model has likelihood: -42.0453
SP score = 0.9665
Training of 3 independent models on file scorptoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb1f55fa1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9f80f4e20>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 281.0226 - loglik: -1.5685e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 1s - loss: 168.7329 - loglik: -1.3494e+02 - logprior: -3.3790e+01
Epoch 3/10
10/10 - 1s - loss: 133.8678 - loglik: -1.1783e+02 - logprior: -1.6039e+01
Epoch 4/10
10/10 - 1s - loss: 118.0599 - loglik: -1.0844e+02 - logprior: -9.6160e+00
Epoch 5/10
10/10 - 1s - loss: 110.1340 - loglik: -1.0407e+02 - logprior: -6.0646e+00
Epoch 6/10
10/10 - 1s - loss: 106.4388 - loglik: -1.0246e+02 - logprior: -3.9836e+00
Epoch 7/10
10/10 - 0s - loss: 104.6747 - loglik: -1.0204e+02 - logprior: -2.6382e+00
Epoch 8/10
10/10 - 1s - loss: 103.6186 - loglik: -1.0187e+02 - logprior: -1.7491e+00
Epoch 9/10
10/10 - 1s - loss: 102.7236 - loglik: -1.0159e+02 - logprior: -1.1301e+00
Epoch 10/10
10/10 - 1s - loss: 101.5410 - loglik: -1.0084e+02 - logprior: -6.9865e-01
Fitted a model with MAP estimate = -101.1207
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (23, 3), (31, 2), (32, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 238.3013 - loglik: -9.9272e+01 - logprior: -1.3903e+02
Epoch 2/2
10/10 - 1s - loss: 151.7441 - loglik: -9.4247e+01 - logprior: -5.7497e+01
Fitted a model with MAP estimate = -137.6148
expansions: [(0, 3)]
discards: [ 0  8 20 29 40]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 203.3727 - loglik: -9.1461e+01 - logprior: -1.1191e+02
Epoch 2/2
10/10 - 1s - loss: 120.2309 - loglik: -9.0066e+01 - logprior: -3.0165e+01
Fitted a model with MAP estimate = -108.0287
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 220.0949 - loglik: -9.0957e+01 - logprior: -1.2914e+02
Epoch 2/10
10/10 - 1s - loss: 127.8370 - loglik: -9.1088e+01 - logprior: -3.6749e+01
Epoch 3/10
10/10 - 1s - loss: 105.5550 - loglik: -9.1492e+01 - logprior: -1.4063e+01
Epoch 4/10
10/10 - 1s - loss: 97.7235 - loglik: -9.1873e+01 - logprior: -5.8506e+00
Epoch 5/10
10/10 - 1s - loss: 93.8962 - loglik: -9.2186e+01 - logprior: -1.7098e+00
Epoch 6/10
10/10 - 1s - loss: 91.7790 - loglik: -9.2414e+01 - logprior: 0.6353
Epoch 7/10
10/10 - 1s - loss: 90.5177 - loglik: -9.2565e+01 - logprior: 2.0477
Epoch 8/10
10/10 - 1s - loss: 89.6967 - loglik: -9.2672e+01 - logprior: 2.9753
Epoch 9/10
10/10 - 1s - loss: 89.1074 - loglik: -9.2770e+01 - logprior: 3.6625
Epoch 10/10
10/10 - 1s - loss: 88.6420 - loglik: -9.2868e+01 - logprior: 4.2258
Fitted a model with MAP estimate = -88.4155
Time for alignment: 26.9060
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 281.0226 - loglik: -1.5685e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 1s - loss: 168.7329 - loglik: -1.3494e+02 - logprior: -3.3790e+01
Epoch 3/10
10/10 - 1s - loss: 133.8678 - loglik: -1.1783e+02 - logprior: -1.6039e+01
Epoch 4/10
10/10 - 1s - loss: 118.0599 - loglik: -1.0844e+02 - logprior: -9.6160e+00
Epoch 5/10
10/10 - 1s - loss: 110.1340 - loglik: -1.0407e+02 - logprior: -6.0646e+00
Epoch 6/10
10/10 - 1s - loss: 106.4388 - loglik: -1.0246e+02 - logprior: -3.9836e+00
Epoch 7/10
10/10 - 1s - loss: 104.6747 - loglik: -1.0204e+02 - logprior: -2.6382e+00
Epoch 8/10
10/10 - 1s - loss: 103.6186 - loglik: -1.0187e+02 - logprior: -1.7491e+00
Epoch 9/10
10/10 - 1s - loss: 102.7236 - loglik: -1.0159e+02 - logprior: -1.1301e+00
Epoch 10/10
10/10 - 1s - loss: 101.5410 - loglik: -1.0084e+02 - logprior: -6.9865e-01
Fitted a model with MAP estimate = -101.1207
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (23, 3), (31, 2), (32, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 238.3013 - loglik: -9.9272e+01 - logprior: -1.3903e+02
Epoch 2/2
10/10 - 1s - loss: 151.7441 - loglik: -9.4247e+01 - logprior: -5.7497e+01
Fitted a model with MAP estimate = -137.6148
expansions: [(0, 3)]
discards: [ 0  8 20 29 40]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 203.3727 - loglik: -9.1461e+01 - logprior: -1.1191e+02
Epoch 2/2
10/10 - 1s - loss: 120.2309 - loglik: -9.0066e+01 - logprior: -3.0165e+01
Fitted a model with MAP estimate = -108.0287
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 220.0949 - loglik: -9.0957e+01 - logprior: -1.2914e+02
Epoch 2/10
10/10 - 1s - loss: 127.8370 - loglik: -9.1088e+01 - logprior: -3.6749e+01
Epoch 3/10
10/10 - 1s - loss: 105.5550 - loglik: -9.1492e+01 - logprior: -1.4063e+01
Epoch 4/10
10/10 - 1s - loss: 97.7235 - loglik: -9.1873e+01 - logprior: -5.8506e+00
Epoch 5/10
10/10 - 1s - loss: 93.8962 - loglik: -9.2186e+01 - logprior: -1.7098e+00
Epoch 6/10
10/10 - 1s - loss: 91.7790 - loglik: -9.2414e+01 - logprior: 0.6353
Epoch 7/10
10/10 - 1s - loss: 90.5177 - loglik: -9.2565e+01 - logprior: 2.0477
Epoch 8/10
10/10 - 1s - loss: 89.6967 - loglik: -9.2672e+01 - logprior: 2.9753
Epoch 9/10
10/10 - 1s - loss: 89.1074 - loglik: -9.2770e+01 - logprior: 3.6625
Epoch 10/10
10/10 - 1s - loss: 88.6420 - loglik: -9.2868e+01 - logprior: 4.2258
Fitted a model with MAP estimate = -88.4155
Time for alignment: 27.1189
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 281.0226 - loglik: -1.5685e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 1s - loss: 168.7329 - loglik: -1.3494e+02 - logprior: -3.3790e+01
Epoch 3/10
10/10 - 1s - loss: 133.8678 - loglik: -1.1783e+02 - logprior: -1.6039e+01
Epoch 4/10
10/10 - 0s - loss: 118.0599 - loglik: -1.0844e+02 - logprior: -9.6160e+00
Epoch 5/10
10/10 - 1s - loss: 110.1340 - loglik: -1.0407e+02 - logprior: -6.0646e+00
Epoch 6/10
10/10 - 1s - loss: 106.4388 - loglik: -1.0246e+02 - logprior: -3.9836e+00
Epoch 7/10
10/10 - 1s - loss: 104.6747 - loglik: -1.0204e+02 - logprior: -2.6382e+00
Epoch 8/10
10/10 - 1s - loss: 103.6186 - loglik: -1.0187e+02 - logprior: -1.7491e+00
Epoch 9/10
10/10 - 1s - loss: 102.7236 - loglik: -1.0159e+02 - logprior: -1.1301e+00
Epoch 10/10
10/10 - 1s - loss: 101.5410 - loglik: -1.0084e+02 - logprior: -6.9865e-01
Fitted a model with MAP estimate = -101.1207
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (23, 3), (31, 2), (32, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 238.3013 - loglik: -9.9272e+01 - logprior: -1.3903e+02
Epoch 2/2
10/10 - 1s - loss: 151.7441 - loglik: -9.4247e+01 - logprior: -5.7497e+01
Fitted a model with MAP estimate = -137.6148
expansions: [(0, 3)]
discards: [ 0  8 20 29 40]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 203.3727 - loglik: -9.1461e+01 - logprior: -1.1191e+02
Epoch 2/2
10/10 - 1s - loss: 120.2309 - loglik: -9.0066e+01 - logprior: -3.0165e+01
Fitted a model with MAP estimate = -108.0287
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 220.0949 - loglik: -9.0957e+01 - logprior: -1.2914e+02
Epoch 2/10
10/10 - 1s - loss: 127.8370 - loglik: -9.1088e+01 - logprior: -3.6749e+01
Epoch 3/10
10/10 - 1s - loss: 105.5550 - loglik: -9.1492e+01 - logprior: -1.4063e+01
Epoch 4/10
10/10 - 1s - loss: 97.7235 - loglik: -9.1873e+01 - logprior: -5.8506e+00
Epoch 5/10
10/10 - 1s - loss: 93.8962 - loglik: -9.2186e+01 - logprior: -1.7098e+00
Epoch 6/10
10/10 - 1s - loss: 91.7790 - loglik: -9.2414e+01 - logprior: 0.6353
Epoch 7/10
10/10 - 1s - loss: 90.5177 - loglik: -9.2565e+01 - logprior: 2.0477
Epoch 8/10
10/10 - 0s - loss: 89.6967 - loglik: -9.2672e+01 - logprior: 2.9753
Epoch 9/10
10/10 - 1s - loss: 89.1074 - loglik: -9.2770e+01 - logprior: 3.6625
Epoch 10/10
10/10 - 1s - loss: 88.6420 - loglik: -9.2868e+01 - logprior: 4.2258
Fitted a model with MAP estimate = -88.4154
Time for alignment: 26.6173
Computed alignments with likelihoods: ['-88.4155', '-88.4155', '-88.4154']
Best model has likelihood: -88.4154
SP score = 0.8783
Training of 3 independent models on file biotin_lipoyl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cc203eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cbc6c3a0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 195.4025 - loglik: -1.9228e+02 - logprior: -3.1202e+00
Epoch 2/10
19/19 - 1s - loss: 155.5567 - loglik: -1.5433e+02 - logprior: -1.2310e+00
Epoch 3/10
19/19 - 1s - loss: 141.4444 - loglik: -1.4013e+02 - logprior: -1.3116e+00
Epoch 4/10
19/19 - 1s - loss: 139.4010 - loglik: -1.3813e+02 - logprior: -1.2754e+00
Epoch 5/10
19/19 - 1s - loss: 138.8012 - loglik: -1.3758e+02 - logprior: -1.2224e+00
Epoch 6/10
19/19 - 1s - loss: 138.6663 - loglik: -1.3747e+02 - logprior: -1.1972e+00
Epoch 7/10
19/19 - 1s - loss: 138.5200 - loglik: -1.3734e+02 - logprior: -1.1756e+00
Epoch 8/10
19/19 - 1s - loss: 138.2606 - loglik: -1.3710e+02 - logprior: -1.1606e+00
Epoch 9/10
19/19 - 1s - loss: 138.1712 - loglik: -1.3702e+02 - logprior: -1.1544e+00
Epoch 10/10
19/19 - 1s - loss: 138.4528 - loglik: -1.3730e+02 - logprior: -1.1486e+00
Fitted a model with MAP estimate = -138.5722
expansions: [(0, 3), (13, 2), (14, 1), (19, 1), (20, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 140.7662 - loglik: -1.3642e+02 - logprior: -4.3434e+00
Epoch 2/2
19/19 - 1s - loss: 131.3960 - loglik: -1.2985e+02 - logprior: -1.5412e+00
Fitted a model with MAP estimate = -131.9892
expansions: [(0, 2)]
discards: [27 49 60 65]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 132.9944 - loglik: -1.2909e+02 - logprior: -3.9000e+00
Epoch 2/2
19/19 - 1s - loss: 129.0814 - loglik: -1.2765e+02 - logprior: -1.4341e+00
Fitted a model with MAP estimate = -130.2528
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 6s - loss: 133.2045 - loglik: -1.2986e+02 - logprior: -3.3472e+00
Epoch 2/10
21/21 - 1s - loss: 130.8221 - loglik: -1.2901e+02 - logprior: -1.8156e+00
Epoch 3/10
21/21 - 1s - loss: 129.1827 - loglik: -1.2792e+02 - logprior: -1.2582e+00
Epoch 4/10
21/21 - 1s - loss: 128.5021 - loglik: -1.2741e+02 - logprior: -1.0879e+00
Epoch 5/10
21/21 - 1s - loss: 128.2561 - loglik: -1.2717e+02 - logprior: -1.0863e+00
Epoch 6/10
21/21 - 1s - loss: 128.2755 - loglik: -1.2722e+02 - logprior: -1.0557e+00
Fitted a model with MAP estimate = -128.0948
Time for alignment: 50.1839
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 195.3929 - loglik: -1.9227e+02 - logprior: -3.1188e+00
Epoch 2/10
19/19 - 1s - loss: 155.8248 - loglik: -1.5460e+02 - logprior: -1.2281e+00
Epoch 3/10
19/19 - 1s - loss: 141.6181 - loglik: -1.4030e+02 - logprior: -1.3139e+00
Epoch 4/10
19/19 - 1s - loss: 139.3226 - loglik: -1.3804e+02 - logprior: -1.2835e+00
Epoch 5/10
19/19 - 1s - loss: 138.9945 - loglik: -1.3777e+02 - logprior: -1.2278e+00
Epoch 6/10
19/19 - 1s - loss: 138.5395 - loglik: -1.3733e+02 - logprior: -1.2065e+00
Epoch 7/10
19/19 - 1s - loss: 138.3339 - loglik: -1.3715e+02 - logprior: -1.1855e+00
Epoch 8/10
19/19 - 1s - loss: 138.0725 - loglik: -1.3689e+02 - logprior: -1.1804e+00
Epoch 9/10
19/19 - 1s - loss: 138.1517 - loglik: -1.3698e+02 - logprior: -1.1747e+00
Fitted a model with MAP estimate = -138.5525
expansions: [(0, 3), (12, 1), (13, 1), (14, 1), (19, 2), (20, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 141.1674 - loglik: -1.3686e+02 - logprior: -4.3102e+00
Epoch 2/2
19/19 - 1s - loss: 131.0945 - loglik: -1.2952e+02 - logprior: -1.5749e+00
Fitted a model with MAP estimate = -131.8967
expansions: [(0, 2)]
discards: [25 28 50 61 66]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 132.8980 - loglik: -1.2899e+02 - logprior: -3.9108e+00
Epoch 2/2
19/19 - 1s - loss: 128.7130 - loglik: -1.2727e+02 - logprior: -1.4402e+00
Fitted a model with MAP estimate = -129.7960
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 132.9067 - loglik: -1.2955e+02 - logprior: -3.3554e+00
Epoch 2/10
21/21 - 1s - loss: 130.3614 - loglik: -1.2854e+02 - logprior: -1.8252e+00
Epoch 3/10
21/21 - 1s - loss: 129.1471 - loglik: -1.2788e+02 - logprior: -1.2626e+00
Epoch 4/10
21/21 - 1s - loss: 128.5744 - loglik: -1.2749e+02 - logprior: -1.0834e+00
Epoch 5/10
21/21 - 1s - loss: 128.2857 - loglik: -1.2721e+02 - logprior: -1.0771e+00
Epoch 6/10
21/21 - 1s - loss: 128.3284 - loglik: -1.2727e+02 - logprior: -1.0548e+00
Fitted a model with MAP estimate = -128.1621
Time for alignment: 46.3438
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 195.4648 - loglik: -1.9235e+02 - logprior: -3.1194e+00
Epoch 2/10
19/19 - 1s - loss: 157.7648 - loglik: -1.5654e+02 - logprior: -1.2292e+00
Epoch 3/10
19/19 - 1s - loss: 142.2853 - loglik: -1.4097e+02 - logprior: -1.3152e+00
Epoch 4/10
19/19 - 1s - loss: 139.4065 - loglik: -1.3811e+02 - logprior: -1.2937e+00
Epoch 5/10
19/19 - 1s - loss: 138.6079 - loglik: -1.3736e+02 - logprior: -1.2456e+00
Epoch 6/10
19/19 - 1s - loss: 138.6197 - loglik: -1.3740e+02 - logprior: -1.2213e+00
Fitted a model with MAP estimate = -138.6750
expansions: [(0, 3), (12, 1), (13, 1), (14, 1), (19, 2), (20, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 141.1676 - loglik: -1.3700e+02 - logprior: -4.1653e+00
Epoch 2/2
19/19 - 1s - loss: 131.1450 - loglik: -1.2959e+02 - logprior: -1.5590e+00
Fitted a model with MAP estimate = -131.7728
expansions: [(0, 2)]
discards: [25 28 50 61 66]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 132.9077 - loglik: -1.2901e+02 - logprior: -3.9010e+00
Epoch 2/2
19/19 - 1s - loss: 128.8121 - loglik: -1.2740e+02 - logprior: -1.4111e+00
Fitted a model with MAP estimate = -130.0494
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 132.9377 - loglik: -1.2960e+02 - logprior: -3.3427e+00
Epoch 2/10
21/21 - 1s - loss: 130.4440 - loglik: -1.2866e+02 - logprior: -1.7817e+00
Epoch 3/10
21/21 - 1s - loss: 128.9132 - loglik: -1.2760e+02 - logprior: -1.3091e+00
Epoch 4/10
21/21 - 1s - loss: 128.8016 - loglik: -1.2766e+02 - logprior: -1.1449e+00
Epoch 5/10
21/21 - 1s - loss: 128.4061 - loglik: -1.2731e+02 - logprior: -1.0937e+00
Epoch 6/10
21/21 - 1s - loss: 128.1080 - loglik: -1.2704e+02 - logprior: -1.0636e+00
Epoch 7/10
21/21 - 1s - loss: 127.9460 - loglik: -1.2691e+02 - logprior: -1.0393e+00
Epoch 8/10
21/21 - 1s - loss: 128.0193 - loglik: -1.2700e+02 - logprior: -1.0206e+00
Fitted a model with MAP estimate = -127.9440
Time for alignment: 44.5612
Computed alignments with likelihoods: ['-128.0948', '-128.1621', '-127.9440']
Best model has likelihood: -127.9440
SP score = 0.9488
Training of 3 independent models on file uce.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cb174580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb7adfd8550>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 390.9598 - loglik: -3.8284e+02 - logprior: -8.1165e+00
Epoch 2/10
13/13 - 2s - loss: 339.5164 - loglik: -3.3765e+02 - logprior: -1.8685e+00
Epoch 3/10
13/13 - 2s - loss: 297.8974 - loglik: -2.9614e+02 - logprior: -1.7557e+00
Epoch 4/10
13/13 - 2s - loss: 282.0741 - loglik: -2.8005e+02 - logprior: -2.0206e+00
Epoch 5/10
13/13 - 2s - loss: 277.3692 - loglik: -2.7528e+02 - logprior: -2.0930e+00
Epoch 6/10
13/13 - 2s - loss: 276.2266 - loglik: -2.7422e+02 - logprior: -2.0059e+00
Epoch 7/10
13/13 - 2s - loss: 275.5574 - loglik: -2.7360e+02 - logprior: -1.9566e+00
Epoch 8/10
13/13 - 2s - loss: 275.1046 - loglik: -2.7315e+02 - logprior: -1.9515e+00
Epoch 9/10
13/13 - 2s - loss: 274.7742 - loglik: -2.7284e+02 - logprior: -1.9371e+00
Epoch 10/10
13/13 - 2s - loss: 274.3290 - loglik: -2.7240e+02 - logprior: -1.9268e+00
Fitted a model with MAP estimate = -274.6199
expansions: [(7, 2), (8, 3), (11, 1), (15, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (61, 2), (70, 3), (75, 1), (76, 1), (77, 2), (86, 1), (99, 2), (100, 3), (101, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 277.9710 - loglik: -2.6845e+02 - logprior: -9.5163e+00
Epoch 2/2
13/13 - 3s - loss: 261.0882 - loglik: -2.5688e+02 - logprior: -4.2096e+00
Fitted a model with MAP estimate = -259.2676
expansions: [(0, 2)]
discards: [  0   9  89  90 129]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 262.6064 - loglik: -2.5540e+02 - logprior: -7.2069e+00
Epoch 2/2
13/13 - 3s - loss: 254.9783 - loglik: -2.5326e+02 - logprior: -1.7205e+00
Fitted a model with MAP estimate = -254.0600
expansions: []
discards: [ 0 23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 265.1307 - loglik: -2.5594e+02 - logprior: -9.1863e+00
Epoch 2/10
13/13 - 3s - loss: 257.8063 - loglik: -2.5468e+02 - logprior: -3.1235e+00
Epoch 3/10
13/13 - 3s - loss: 254.6062 - loglik: -2.5342e+02 - logprior: -1.1876e+00
Epoch 4/10
13/13 - 3s - loss: 254.1766 - loglik: -2.5353e+02 - logprior: -6.4777e-01
Epoch 5/10
13/13 - 3s - loss: 254.1663 - loglik: -2.5371e+02 - logprior: -4.5843e-01
Epoch 6/10
13/13 - 3s - loss: 253.9761 - loglik: -2.5357e+02 - logprior: -4.0867e-01
Epoch 7/10
13/13 - 3s - loss: 253.7243 - loglik: -2.5338e+02 - logprior: -3.4101e-01
Epoch 8/10
13/13 - 3s - loss: 253.2204 - loglik: -2.5290e+02 - logprior: -3.2378e-01
Epoch 9/10
13/13 - 3s - loss: 253.5301 - loglik: -2.5326e+02 - logprior: -2.6706e-01
Fitted a model with MAP estimate = -253.4385
Time for alignment: 79.6739
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 391.6200 - loglik: -3.8351e+02 - logprior: -8.1105e+00
Epoch 2/10
13/13 - 2s - loss: 337.4429 - loglik: -3.3557e+02 - logprior: -1.8704e+00
Epoch 3/10
13/13 - 2s - loss: 296.8638 - loglik: -2.9515e+02 - logprior: -1.7129e+00
Epoch 4/10
13/13 - 2s - loss: 283.3208 - loglik: -2.8134e+02 - logprior: -1.9854e+00
Epoch 5/10
13/13 - 2s - loss: 279.3830 - loglik: -2.7735e+02 - logprior: -2.0358e+00
Epoch 6/10
13/13 - 2s - loss: 277.6926 - loglik: -2.7574e+02 - logprior: -1.9513e+00
Epoch 7/10
13/13 - 2s - loss: 277.9731 - loglik: -2.7604e+02 - logprior: -1.9363e+00
Fitted a model with MAP estimate = -277.0831
expansions: [(7, 2), (8, 2), (11, 1), (15, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 2), (53, 1), (70, 2), (75, 2), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 279.5651 - loglik: -2.7007e+02 - logprior: -9.4983e+00
Epoch 2/2
13/13 - 3s - loss: 262.3712 - loglik: -2.5819e+02 - logprior: -4.1769e+00
Fitted a model with MAP estimate = -259.8331
expansions: [(0, 2)]
discards: [ 0 66 88 94]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 262.5270 - loglik: -2.5529e+02 - logprior: -7.2381e+00
Epoch 2/2
13/13 - 3s - loss: 254.9784 - loglik: -2.5320e+02 - logprior: -1.7798e+00
Fitted a model with MAP estimate = -254.1229
expansions: []
discards: [ 0 23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 265.0566 - loglik: -2.5584e+02 - logprior: -9.2182e+00
Epoch 2/10
13/13 - 3s - loss: 257.9807 - loglik: -2.5481e+02 - logprior: -3.1728e+00
Epoch 3/10
13/13 - 3s - loss: 255.0644 - loglik: -2.5384e+02 - logprior: -1.2252e+00
Epoch 4/10
13/13 - 3s - loss: 254.2951 - loglik: -2.5361e+02 - logprior: -6.8620e-01
Epoch 5/10
13/13 - 3s - loss: 253.9361 - loglik: -2.5346e+02 - logprior: -4.8033e-01
Epoch 6/10
13/13 - 2s - loss: 253.2160 - loglik: -2.5279e+02 - logprior: -4.2871e-01
Epoch 7/10
13/13 - 3s - loss: 254.6137 - loglik: -2.5422e+02 - logprior: -3.9304e-01
Fitted a model with MAP estimate = -253.6570
Time for alignment: 66.7650
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 390.7711 - loglik: -3.8266e+02 - logprior: -8.1098e+00
Epoch 2/10
13/13 - 2s - loss: 340.0295 - loglik: -3.3816e+02 - logprior: -1.8682e+00
Epoch 3/10
13/13 - 2s - loss: 300.9026 - loglik: -2.9919e+02 - logprior: -1.7147e+00
Epoch 4/10
13/13 - 2s - loss: 285.6988 - loglik: -2.8373e+02 - logprior: -1.9731e+00
Epoch 5/10
13/13 - 2s - loss: 279.6391 - loglik: -2.7759e+02 - logprior: -2.0462e+00
Epoch 6/10
13/13 - 2s - loss: 278.0748 - loglik: -2.7609e+02 - logprior: -1.9892e+00
Epoch 7/10
13/13 - 2s - loss: 275.8839 - loglik: -2.7391e+02 - logprior: -1.9776e+00
Epoch 8/10
13/13 - 2s - loss: 275.7824 - loglik: -2.7382e+02 - logprior: -1.9620e+00
Epoch 9/10
13/13 - 2s - loss: 276.3054 - loglik: -2.7436e+02 - logprior: -1.9453e+00
Fitted a model with MAP estimate = -275.6854
expansions: [(7, 2), (8, 3), (11, 1), (13, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (61, 2), (70, 2), (75, 1), (76, 1), (77, 2), (79, 1), (99, 5), (100, 2), (101, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 143 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 278.6533 - loglik: -2.6915e+02 - logprior: -9.5053e+00
Epoch 2/2
13/13 - 3s - loss: 262.0692 - loglik: -2.5785e+02 - logprior: -4.2227e+00
Fitted a model with MAP estimate = -259.8029
expansions: [(0, 2)]
discards: [  0   9  78 132]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 262.3391 - loglik: -2.5511e+02 - logprior: -7.2275e+00
Epoch 2/2
13/13 - 3s - loss: 255.1506 - loglik: -2.5341e+02 - logprior: -1.7398e+00
Fitted a model with MAP estimate = -253.8587
expansions: []
discards: [ 0 23 88]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 265.4782 - loglik: -2.5629e+02 - logprior: -9.1913e+00
Epoch 2/10
13/13 - 2s - loss: 257.7643 - loglik: -2.5464e+02 - logprior: -3.1264e+00
Epoch 3/10
13/13 - 2s - loss: 255.4239 - loglik: -2.5423e+02 - logprior: -1.1967e+00
Epoch 4/10
13/13 - 3s - loss: 254.5359 - loglik: -2.5389e+02 - logprior: -6.4844e-01
Epoch 5/10
13/13 - 3s - loss: 253.7467 - loglik: -2.5329e+02 - logprior: -4.5243e-01
Epoch 6/10
13/13 - 3s - loss: 253.4354 - loglik: -2.5304e+02 - logprior: -3.9254e-01
Epoch 7/10
13/13 - 3s - loss: 254.6571 - loglik: -2.5429e+02 - logprior: -3.6213e-01
Fitted a model with MAP estimate = -253.7395
Time for alignment: 73.1419
Computed alignments with likelihoods: ['-253.4385', '-253.6570', '-253.7395']
Best model has likelihood: -253.4385
SP score = 0.9413
Training of 3 independent models on file ghf1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb1f4497e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbb9427d970>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 33s - loss: 1078.7435 - loglik: -1.0754e+03 - logprior: -3.3531e+00
Epoch 2/10
25/25 - 30s - loss: 815.9271 - loglik: -8.1494e+02 - logprior: -9.8482e-01
Epoch 3/10
25/25 - 30s - loss: 763.5848 - loglik: -7.6126e+02 - logprior: -2.3205e+00
Epoch 4/10
25/25 - 30s - loss: 755.8586 - loglik: -7.5338e+02 - logprior: -2.4824e+00
Epoch 5/10
25/25 - 30s - loss: 751.3835 - loglik: -7.4895e+02 - logprior: -2.4318e+00
Epoch 6/10
25/25 - 30s - loss: 750.9511 - loglik: -7.4850e+02 - logprior: -2.4485e+00
Epoch 7/10
25/25 - 30s - loss: 748.7751 - loglik: -7.4633e+02 - logprior: -2.4411e+00
Epoch 8/10
25/25 - 30s - loss: 749.2534 - loglik: -7.4679e+02 - logprior: -2.4671e+00
Fitted a model with MAP estimate = -749.0700
expansions: [(43, 1), (125, 1), (134, 1), (143, 1), (144, 1), (145, 1), (162, 1), (163, 2), (167, 1), (168, 1), (172, 4), (173, 2), (174, 2), (175, 1), (188, 1), (189, 1), (190, 1), (191, 1), (192, 2), (193, 1), (196, 1), (197, 1), (198, 1), (199, 1), (201, 1), (202, 1), (204, 1), (206, 2), (207, 1), (208, 1), (211, 1), (212, 1), (217, 1), (223, 1), (224, 1), (225, 2), (226, 1), (227, 1), (229, 1), (230, 1), (231, 1), (236, 1), (238, 1), (239, 1), (250, 1), (251, 1), (255, 3), (256, 2), (257, 1), (259, 1), (260, 1), (266, 1), (267, 1), (281, 1), (282, 1), (283, 1), (299, 1), (301, 2), (302, 2), (316, 1), (318, 2), (319, 2), (321, 2), (326, 1), (328, 2), (351, 2), (354, 3), (358, 1), (364, 1), (366, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 461 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 46s - loss: 729.0773 - loglik: -7.2476e+02 - logprior: -4.3127e+00
Epoch 2/2
25/25 - 43s - loss: 696.0554 - loglik: -6.9520e+02 - logprior: -8.5403e-01
Fitted a model with MAP estimate = -693.1315
expansions: [(248, 1)]
discards: [170 185 216 241 312 313 371 399 437 438]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 47s - loss: 700.3422 - loglik: -6.9723e+02 - logprior: -3.1130e+00
Epoch 2/2
25/25 - 42s - loss: 695.8795 - loglik: -6.9672e+02 - logprior: 0.8453
Fitted a model with MAP estimate = -693.7338
expansions: [(308, 3)]
discards: [185]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 454 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 45s - loss: 697.0965 - loglik: -6.9447e+02 - logprior: -2.6311e+00
Epoch 2/10
25/25 - 42s - loss: 695.7573 - loglik: -6.9716e+02 - logprior: 1.4002
Epoch 3/10
25/25 - 42s - loss: 693.0753 - loglik: -6.9473e+02 - logprior: 1.6525
Epoch 4/10
25/25 - 42s - loss: 693.6724 - loglik: -6.9606e+02 - logprior: 2.3834
Fitted a model with MAP estimate = -691.6682
Time for alignment: 740.6564
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 33s - loss: 1077.8413 - loglik: -1.0745e+03 - logprior: -3.3252e+00
Epoch 2/10
25/25 - 30s - loss: 818.0619 - loglik: -8.1720e+02 - logprior: -8.6044e-01
Epoch 3/10
25/25 - 30s - loss: 764.8517 - loglik: -7.6299e+02 - logprior: -1.8598e+00
Epoch 4/10
25/25 - 30s - loss: 752.4666 - loglik: -7.5033e+02 - logprior: -2.1367e+00
Epoch 5/10
25/25 - 30s - loss: 751.8736 - loglik: -7.4971e+02 - logprior: -2.1662e+00
Epoch 6/10
25/25 - 30s - loss: 748.7890 - loglik: -7.4656e+02 - logprior: -2.2286e+00
Epoch 7/10
25/25 - 30s - loss: 750.2832 - loglik: -7.4803e+02 - logprior: -2.2548e+00
Fitted a model with MAP estimate = -748.5887
expansions: [(46, 1), (125, 1), (131, 1), (134, 1), (163, 1), (164, 3), (169, 1), (173, 5), (174, 2), (175, 2), (176, 1), (189, 1), (190, 1), (191, 5), (192, 2), (195, 1), (196, 1), (197, 1), (198, 1), (200, 1), (201, 1), (203, 1), (205, 1), (207, 1), (208, 1), (209, 1), (210, 1), (211, 1), (216, 1), (222, 1), (223, 1), (224, 2), (225, 1), (226, 1), (227, 2), (228, 1), (229, 1), (236, 1), (237, 1), (249, 1), (250, 1), (252, 1), (254, 3), (255, 3), (257, 1), (258, 1), (266, 1), (280, 1), (282, 1), (299, 1), (301, 2), (302, 2), (304, 1), (315, 1), (317, 1), (318, 1), (319, 1), (321, 2), (325, 2), (327, 2), (354, 1), (355, 1), (360, 1), (363, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 458 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 47s - loss: 723.7597 - loglik: -7.1948e+02 - logprior: -4.2812e+00
Epoch 2/2
25/25 - 43s - loss: 700.3824 - loglik: -6.9969e+02 - logprior: -6.9352e-01
Fitted a model with MAP estimate = -696.1674
expansions: []
discards: [184 185 186 187 215 371 399]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 45s - loss: 702.3145 - loglik: -6.9941e+02 - logprior: -2.9089e+00
Epoch 2/2
25/25 - 42s - loss: 697.6607 - loglik: -6.9871e+02 - logprior: 1.0485
Fitted a model with MAP estimate = -695.5572
expansions: []
discards: [307]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 450 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 44s - loss: 700.4302 - loglik: -6.9780e+02 - logprior: -2.6262e+00
Epoch 2/10
25/25 - 41s - loss: 698.1805 - loglik: -6.9966e+02 - logprior: 1.4764
Epoch 3/10
25/25 - 41s - loss: 695.5676 - loglik: -6.9782e+02 - logprior: 2.2485
Epoch 4/10
25/25 - 41s - loss: 694.9139 - loglik: -6.9746e+02 - logprior: 2.5479
Epoch 5/10
25/25 - 41s - loss: 695.2049 - loglik: -6.9805e+02 - logprior: 2.8423
Fitted a model with MAP estimate = -694.2353
Time for alignment: 746.0584
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 35s - loss: 1079.3402 - loglik: -1.0759e+03 - logprior: -3.4502e+00
Epoch 2/10
25/25 - 30s - loss: 822.1273 - loglik: -8.2126e+02 - logprior: -8.6959e-01
Epoch 3/10
25/25 - 30s - loss: 773.5082 - loglik: -7.7160e+02 - logprior: -1.9116e+00
Epoch 4/10
25/25 - 30s - loss: 769.9132 - loglik: -7.6790e+02 - logprior: -2.0123e+00
Epoch 5/10
25/25 - 30s - loss: 758.3312 - loglik: -7.5627e+02 - logprior: -2.0568e+00
Epoch 6/10
25/25 - 30s - loss: 759.4586 - loglik: -7.5736e+02 - logprior: -2.0967e+00
Fitted a model with MAP estimate = -757.2684
expansions: [(43, 1), (122, 1), (131, 1), (143, 1), (144, 1), (145, 1), (161, 2), (162, 2), (173, 3), (174, 2), (175, 2), (176, 1), (189, 1), (190, 1), (191, 1), (192, 1), (193, 2), (194, 1), (197, 1), (198, 3), (199, 2), (201, 1), (203, 1), (207, 2), (208, 1), (209, 1), (211, 1), (212, 1), (213, 1), (215, 1), (217, 1), (223, 1), (224, 1), (225, 2), (226, 1), (227, 1), (229, 1), (230, 1), (231, 1), (234, 1), (235, 1), (237, 1), (238, 1), (249, 1), (254, 2), (255, 4), (256, 2), (258, 3), (281, 1), (282, 2), (299, 1), (300, 2), (301, 1), (304, 1), (318, 2), (319, 1), (322, 2), (325, 3), (327, 2), (356, 1), (358, 1), (360, 1), (363, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 459 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 46s - loss: 732.8702 - loglik: -7.2869e+02 - logprior: -4.1848e+00
Epoch 2/2
25/25 - 43s - loss: 705.9988 - loglik: -7.0542e+02 - logprior: -5.8007e-01
Fitted a model with MAP estimate = -701.4853
expansions: [(351, 1), (440, 1)]
discards: [168 185 187 215 228 241 372 400]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 453 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 45s - loss: 703.9451 - loglik: -7.0054e+02 - logprior: -3.4078e+00
Epoch 2/2
25/25 - 42s - loss: 699.6227 - loglik: -7.0051e+02 - logprior: 0.8829
Fitted a model with MAP estimate = -694.8001
expansions: []
discards: [307 431]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 46s - loss: 701.5414 - loglik: -6.9904e+02 - logprior: -2.4980e+00
Epoch 2/10
25/25 - 41s - loss: 696.0350 - loglik: -6.9757e+02 - logprior: 1.5370
Epoch 3/10
25/25 - 42s - loss: 694.4651 - loglik: -6.9661e+02 - logprior: 2.1483
Epoch 4/10
25/25 - 42s - loss: 694.7911 - loglik: -6.9730e+02 - logprior: 2.5123
Fitted a model with MAP estimate = -693.6510
Time for alignment: 680.2228
Computed alignments with likelihoods: ['-691.6682', '-694.2353', '-693.6510']
Best model has likelihood: -691.6682
SP score = 0.9086
Training of 3 independent models on file ldh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb7aee871c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fba10fc2730>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 326.8612 - loglik: -3.2236e+02 - logprior: -4.4965e+00
Epoch 2/10
16/16 - 4s - loss: 255.5773 - loglik: -2.5411e+02 - logprior: -1.4702e+00
Epoch 3/10
16/16 - 4s - loss: 226.4620 - loglik: -2.2466e+02 - logprior: -1.7981e+00
Epoch 4/10
16/16 - 4s - loss: 215.6295 - loglik: -2.1386e+02 - logprior: -1.7677e+00
Epoch 5/10
16/16 - 3s - loss: 209.0732 - loglik: -2.0741e+02 - logprior: -1.6634e+00
Epoch 6/10
16/16 - 3s - loss: 207.1243 - loglik: -2.0542e+02 - logprior: -1.7085e+00
Epoch 7/10
16/16 - 4s - loss: 201.0782 - loglik: -1.9939e+02 - logprior: -1.6904e+00
Epoch 8/10
16/16 - 4s - loss: 200.6741 - loglik: -1.9899e+02 - logprior: -1.6810e+00
Epoch 9/10
16/16 - 3s - loss: 200.0480 - loglik: -1.9835e+02 - logprior: -1.6938e+00
Epoch 10/10
16/16 - 4s - loss: 198.5909 - loglik: -1.9690e+02 - logprior: -1.6891e+00
Fitted a model with MAP estimate = -199.0653
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 2), (17, 1), (18, 1), (72, 1), (94, 1), (97, 1)]
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 6s - loss: 203.0621 - loglik: -1.9741e+02 - logprior: -5.6517e+00
Epoch 2/2
16/16 - 4s - loss: 195.0134 - loglik: -1.9225e+02 - logprior: -2.7600e+00
Fitted a model with MAP estimate = -194.4551
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 6s - loss: 195.0337 - loglik: -1.9062e+02 - logprior: -4.4160e+00
Epoch 2/2
16/16 - 4s - loss: 190.9462 - loglik: -1.8933e+02 - logprior: -1.6177e+00
Fitted a model with MAP estimate = -190.3202
expansions: [(20, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 192.6664 - loglik: -1.8823e+02 - logprior: -4.4412e+00
Epoch 2/10
16/16 - 4s - loss: 190.1220 - loglik: -1.8856e+02 - logprior: -1.5651e+00
Epoch 3/10
16/16 - 4s - loss: 189.4667 - loglik: -1.8815e+02 - logprior: -1.3142e+00
Epoch 4/10
16/16 - 4s - loss: 189.6099 - loglik: -1.8838e+02 - logprior: -1.2316e+00
Fitted a model with MAP estimate = -189.1687
Time for alignment: 98.2964
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 327.2584 - loglik: -3.2276e+02 - logprior: -4.4992e+00
Epoch 2/10
16/16 - 3s - loss: 256.1846 - loglik: -2.5471e+02 - logprior: -1.4713e+00
Epoch 3/10
16/16 - 4s - loss: 225.5852 - loglik: -2.2374e+02 - logprior: -1.8483e+00
Epoch 4/10
16/16 - 4s - loss: 213.0904 - loglik: -2.1130e+02 - logprior: -1.7881e+00
Epoch 5/10
16/16 - 3s - loss: 211.0935 - loglik: -2.0939e+02 - logprior: -1.7028e+00
Epoch 6/10
16/16 - 3s - loss: 205.1902 - loglik: -2.0344e+02 - logprior: -1.7493e+00
Epoch 7/10
16/16 - 4s - loss: 202.3183 - loglik: -2.0058e+02 - logprior: -1.7391e+00
Epoch 8/10
16/16 - 3s - loss: 202.8211 - loglik: -2.0109e+02 - logprior: -1.7333e+00
Fitted a model with MAP estimate = -202.4566
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 2), (17, 1), (18, 1), (58, 1), (72, 1), (96, 1), (97, 1)]
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 6s - loss: 206.0596 - loglik: -2.0042e+02 - logprior: -5.6418e+00
Epoch 2/2
16/16 - 4s - loss: 195.9134 - loglik: -1.9313e+02 - logprior: -2.7864e+00
Fitted a model with MAP estimate = -194.8150
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 195.3084 - loglik: -1.9088e+02 - logprior: -4.4242e+00
Epoch 2/2
16/16 - 4s - loss: 190.7830 - loglik: -1.8912e+02 - logprior: -1.6657e+00
Fitted a model with MAP estimate = -189.8913
expansions: [(20, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 193.2067 - loglik: -1.8871e+02 - logprior: -4.4973e+00
Epoch 2/10
16/16 - 4s - loss: 189.4304 - loglik: -1.8779e+02 - logprior: -1.6384e+00
Epoch 3/10
16/16 - 4s - loss: 188.5854 - loglik: -1.8720e+02 - logprior: -1.3835e+00
Epoch 4/10
16/16 - 4s - loss: 188.3125 - loglik: -1.8702e+02 - logprior: -1.2941e+00
Epoch 5/10
16/16 - 4s - loss: 190.1626 - loglik: -1.8894e+02 - logprior: -1.2186e+00
Fitted a model with MAP estimate = -188.6649
Time for alignment: 94.2254
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 326.1094 - loglik: -3.2162e+02 - logprior: -4.4853e+00
Epoch 2/10
16/16 - 4s - loss: 253.0667 - loglik: -2.5161e+02 - logprior: -1.4613e+00
Epoch 3/10
16/16 - 4s - loss: 224.7663 - loglik: -2.2294e+02 - logprior: -1.8299e+00
Epoch 4/10
16/16 - 3s - loss: 215.7572 - loglik: -2.1406e+02 - logprior: -1.7001e+00
Epoch 5/10
16/16 - 4s - loss: 212.2211 - loglik: -2.1061e+02 - logprior: -1.6066e+00
Epoch 6/10
16/16 - 4s - loss: 207.7489 - loglik: -2.0613e+02 - logprior: -1.6181e+00
Epoch 7/10
16/16 - 4s - loss: 205.3893 - loglik: -2.0377e+02 - logprior: -1.6185e+00
Epoch 8/10
16/16 - 3s - loss: 201.7118 - loglik: -2.0008e+02 - logprior: -1.6322e+00
Epoch 9/10
16/16 - 3s - loss: 201.7997 - loglik: -2.0018e+02 - logprior: -1.6234e+00
Fitted a model with MAP estimate = -200.6138
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 2), (17, 2), (18, 1), (69, 1), (94, 1), (97, 1)]
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 204.3035 - loglik: -1.9870e+02 - logprior: -5.6033e+00
Epoch 2/2
16/16 - 4s - loss: 195.8937 - loglik: -1.9317e+02 - logprior: -2.7260e+00
Fitted a model with MAP estimate = -195.1219
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 6s - loss: 195.4904 - loglik: -1.9113e+02 - logprior: -4.3613e+00
Epoch 2/2
16/16 - 4s - loss: 191.6443 - loglik: -1.9004e+02 - logprior: -1.6076e+00
Fitted a model with MAP estimate = -190.7174
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 194.5817 - loglik: -1.9015e+02 - logprior: -4.4320e+00
Epoch 2/10
16/16 - 4s - loss: 190.6852 - loglik: -1.8911e+02 - logprior: -1.5715e+00
Epoch 3/10
16/16 - 4s - loss: 190.1355 - loglik: -1.8882e+02 - logprior: -1.3106e+00
Epoch 4/10
16/16 - 4s - loss: 190.1963 - loglik: -1.8897e+02 - logprior: -1.2295e+00
Fitted a model with MAP estimate = -190.2552
Time for alignment: 94.1456
Computed alignments with likelihoods: ['-189.1687', '-188.6649', '-190.2552']
Best model has likelihood: -188.6649
SP score = 0.5314
Training of 3 independent models on file Rhodanese.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fba10a139a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c2f48d60>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 308.7806 - loglik: -3.0565e+02 - logprior: -3.1340e+00
Epoch 2/10
19/19 - 2s - loss: 278.1984 - loglik: -2.7704e+02 - logprior: -1.1574e+00
Epoch 3/10
19/19 - 2s - loss: 263.6922 - loglik: -2.6222e+02 - logprior: -1.4704e+00
Epoch 4/10
19/19 - 2s - loss: 259.0171 - loglik: -2.5758e+02 - logprior: -1.4363e+00
Epoch 5/10
19/19 - 2s - loss: 257.9041 - loglik: -2.5650e+02 - logprior: -1.3993e+00
Epoch 6/10
19/19 - 2s - loss: 256.6544 - loglik: -2.5528e+02 - logprior: -1.3756e+00
Epoch 7/10
19/19 - 2s - loss: 257.1300 - loglik: -2.5577e+02 - logprior: -1.3562e+00
Fitted a model with MAP estimate = -242.5430
expansions: [(6, 3), (7, 2), (10, 1), (22, 4), (33, 9), (38, 2), (39, 1), (43, 2), (55, 2), (58, 2), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [ 0 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 258.8351 - loglik: -2.5484e+02 - logprior: -3.9921e+00
Epoch 2/2
19/19 - 3s - loss: 248.3049 - loglik: -2.4613e+02 - logprior: -2.1791e+00
Fitted a model with MAP estimate = -236.2051
expansions: [(0, 2), (48, 2)]
discards: [ 0  9 28 29 30 56 77 82 91]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 248.2023 - loglik: -2.4525e+02 - logprior: -2.9567e+00
Epoch 2/2
19/19 - 2s - loss: 244.6964 - loglik: -2.4354e+02 - logprior: -1.1516e+00
Fitted a model with MAP estimate = -234.9316
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 235.4826 - loglik: -2.3288e+02 - logprior: -2.5988e+00
Epoch 2/10
23/23 - 3s - loss: 231.3811 - loglik: -2.3030e+02 - logprior: -1.0850e+00
Epoch 3/10
23/23 - 3s - loss: 230.0299 - loglik: -2.2899e+02 - logprior: -1.0417e+00
Epoch 4/10
23/23 - 3s - loss: 230.7744 - loglik: -2.2978e+02 - logprior: -9.9721e-01
Fitted a model with MAP estimate = -229.7204
Time for alignment: 67.9723
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 308.5840 - loglik: -3.0545e+02 - logprior: -3.1354e+00
Epoch 2/10
19/19 - 2s - loss: 276.5672 - loglik: -2.7539e+02 - logprior: -1.1790e+00
Epoch 3/10
19/19 - 2s - loss: 262.5251 - loglik: -2.6108e+02 - logprior: -1.4480e+00
Epoch 4/10
19/19 - 2s - loss: 258.4214 - loglik: -2.5705e+02 - logprior: -1.3670e+00
Epoch 5/10
19/19 - 2s - loss: 257.1898 - loglik: -2.5586e+02 - logprior: -1.3298e+00
Epoch 6/10
19/19 - 2s - loss: 256.7502 - loglik: -2.5545e+02 - logprior: -1.3001e+00
Epoch 7/10
19/19 - 2s - loss: 255.7175 - loglik: -2.5443e+02 - logprior: -1.2858e+00
Epoch 8/10
19/19 - 2s - loss: 256.4247 - loglik: -2.5515e+02 - logprior: -1.2725e+00
Fitted a model with MAP estimate = -241.8986
expansions: [(6, 3), (7, 2), (8, 1), (10, 2), (39, 7), (58, 2), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 258.2895 - loglik: -2.5433e+02 - logprior: -3.9584e+00
Epoch 2/2
19/19 - 2s - loss: 251.0536 - loglik: -2.4899e+02 - logprior: -2.0637e+00
Fitted a model with MAP estimate = -237.8290
expansions: [(0, 2), (41, 9)]
discards: [ 0  9 10 15 49 50 72]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 250.4239 - loglik: -2.4748e+02 - logprior: -2.9394e+00
Epoch 2/2
19/19 - 2s - loss: 245.6924 - loglik: -2.4456e+02 - logprior: -1.1371e+00
Fitted a model with MAP estimate = -235.1458
expansions: []
discards: [ 0 85]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 235.7691 - loglik: -2.3319e+02 - logprior: -2.5810e+00
Epoch 2/10
23/23 - 3s - loss: 231.4397 - loglik: -2.3036e+02 - logprior: -1.0776e+00
Epoch 3/10
23/23 - 3s - loss: 231.2187 - loglik: -2.3020e+02 - logprior: -1.0173e+00
Epoch 4/10
23/23 - 3s - loss: 229.9592 - loglik: -2.2898e+02 - logprior: -9.8332e-01
Epoch 5/10
23/23 - 3s - loss: 230.2122 - loglik: -2.2924e+02 - logprior: -9.7711e-01
Fitted a model with MAP estimate = -229.7611
Time for alignment: 73.3265
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 308.9163 - loglik: -3.0578e+02 - logprior: -3.1343e+00
Epoch 2/10
19/19 - 2s - loss: 277.3246 - loglik: -2.7616e+02 - logprior: -1.1597e+00
Epoch 3/10
19/19 - 2s - loss: 263.7586 - loglik: -2.6232e+02 - logprior: -1.4378e+00
Epoch 4/10
19/19 - 2s - loss: 259.5784 - loglik: -2.5819e+02 - logprior: -1.3858e+00
Epoch 5/10
19/19 - 2s - loss: 257.8506 - loglik: -2.5649e+02 - logprior: -1.3575e+00
Epoch 6/10
19/19 - 2s - loss: 257.0185 - loglik: -2.5568e+02 - logprior: -1.3423e+00
Epoch 7/10
19/19 - 2s - loss: 256.8085 - loglik: -2.5547e+02 - logprior: -1.3375e+00
Epoch 8/10
19/19 - 2s - loss: 256.2025 - loglik: -2.5487e+02 - logprior: -1.3275e+00
Epoch 9/10
19/19 - 2s - loss: 256.4890 - loglik: -2.5517e+02 - logprior: -1.3196e+00
Fitted a model with MAP estimate = -242.9530
expansions: [(6, 3), (7, 2), (8, 1), (10, 1), (34, 9), (39, 2), (40, 1), (43, 2), (58, 1), (60, 1), (61, 1), (63, 2), (66, 3), (67, 1), (73, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 257.3298 - loglik: -2.5335e+02 - logprior: -3.9848e+00
Epoch 2/2
19/19 - 3s - loss: 248.5356 - loglik: -2.4640e+02 - logprior: -2.1338e+00
Fitted a model with MAP estimate = -236.5695
expansions: [(0, 2), (46, 2)]
discards: [ 0  9 12 54 87 92]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 247.7202 - loglik: -2.4479e+02 - logprior: -2.9310e+00
Epoch 2/2
19/19 - 2s - loss: 244.6779 - loglik: -2.4353e+02 - logprior: -1.1471e+00
Fitted a model with MAP estimate = -235.0197
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 235.6055 - loglik: -2.3300e+02 - logprior: -2.6062e+00
Epoch 2/10
23/23 - 3s - loss: 231.2093 - loglik: -2.3012e+02 - logprior: -1.0922e+00
Epoch 3/10
23/23 - 3s - loss: 231.0488 - loglik: -2.3002e+02 - logprior: -1.0247e+00
Epoch 4/10
23/23 - 3s - loss: 229.7107 - loglik: -2.2872e+02 - logprior: -9.9010e-01
Epoch 5/10
23/23 - 3s - loss: 229.9790 - loglik: -2.2901e+02 - logprior: -9.7051e-01
Fitted a model with MAP estimate = -229.5694
Time for alignment: 74.3562
Computed alignments with likelihoods: ['-229.7204', '-229.7611', '-229.5694']
Best model has likelihood: -229.5694
SP score = 0.7017
Training of 3 independent models on file slectin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb7ec74ec70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb20460a8b0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 432.4487 - loglik: -3.8697e+02 - logprior: -4.5483e+01
Epoch 2/10
10/10 - 2s - loss: 370.1453 - loglik: -3.5984e+02 - logprior: -1.0304e+01
Epoch 3/10
10/10 - 2s - loss: 335.1464 - loglik: -3.3104e+02 - logprior: -4.1046e+00
Epoch 4/10
10/10 - 2s - loss: 312.8116 - loglik: -3.1064e+02 - logprior: -2.1721e+00
Epoch 5/10
10/10 - 2s - loss: 303.0995 - loglik: -3.0177e+02 - logprior: -1.3266e+00
Epoch 6/10
10/10 - 2s - loss: 299.6872 - loglik: -2.9904e+02 - logprior: -6.4711e-01
Epoch 7/10
10/10 - 2s - loss: 298.1541 - loglik: -2.9797e+02 - logprior: -1.8790e-01
Epoch 8/10
10/10 - 2s - loss: 296.3587 - loglik: -2.9632e+02 - logprior: -4.0729e-02
Epoch 9/10
10/10 - 2s - loss: 295.5765 - loglik: -2.9561e+02 - logprior: 0.0328
Epoch 10/10
10/10 - 2s - loss: 294.7977 - loglik: -2.9492e+02 - logprior: 0.1259
Fitted a model with MAP estimate = -294.7798
expansions: [(7, 2), (10, 1), (17, 1), (18, 1), (19, 1), (26, 1), (27, 3), (42, 1), (48, 2), (59, 1), (62, 2), (64, 1), (73, 1), (82, 1), (90, 2), (91, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 343.4829 - loglik: -2.9274e+02 - logprior: -5.0739e+01
Epoch 2/2
10/10 - 2s - loss: 303.1733 - loglik: -2.8398e+02 - logprior: -1.9192e+01
Fitted a model with MAP estimate = -297.1049
expansions: [(9, 3)]
discards: [  0  33  58  75 109]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 330.9544 - loglik: -2.8135e+02 - logprior: -4.9606e+01
Epoch 2/2
10/10 - 2s - loss: 296.6865 - loglik: -2.7995e+02 - logprior: -1.6740e+01
Fitted a model with MAP estimate = -289.2835
expansions: [(0, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 315.7048 - loglik: -2.7689e+02 - logprior: -3.8815e+01
Epoch 2/10
10/10 - 2s - loss: 282.8082 - loglik: -2.7537e+02 - logprior: -7.4355e+00
Epoch 3/10
10/10 - 2s - loss: 276.9050 - loglik: -2.7597e+02 - logprior: -9.3343e-01
Epoch 4/10
10/10 - 2s - loss: 274.2726 - loglik: -2.7604e+02 - logprior: 1.7695
Epoch 5/10
10/10 - 2s - loss: 273.6490 - loglik: -2.7692e+02 - logprior: 3.2721
Epoch 6/10
10/10 - 2s - loss: 272.3321 - loglik: -2.7647e+02 - logprior: 4.1413
Epoch 7/10
10/10 - 2s - loss: 272.1154 - loglik: -2.7679e+02 - logprior: 4.6787
Epoch 8/10
10/10 - 2s - loss: 272.0826 - loglik: -2.7713e+02 - logprior: 5.0454
Epoch 9/10
10/10 - 2s - loss: 271.9898 - loglik: -2.7733e+02 - logprior: 5.3365
Epoch 10/10
10/10 - 2s - loss: 270.8949 - loglik: -2.7650e+02 - logprior: 5.6015
Fitted a model with MAP estimate = -271.2479
Time for alignment: 66.7537
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 432.2377 - loglik: -3.8675e+02 - logprior: -4.5485e+01
Epoch 2/10
10/10 - 2s - loss: 370.7908 - loglik: -3.6048e+02 - logprior: -1.0306e+01
Epoch 3/10
10/10 - 2s - loss: 333.5867 - loglik: -3.2942e+02 - logprior: -4.1637e+00
Epoch 4/10
10/10 - 2s - loss: 310.9331 - loglik: -3.0876e+02 - logprior: -2.1716e+00
Epoch 5/10
10/10 - 2s - loss: 301.8453 - loglik: -3.0071e+02 - logprior: -1.1382e+00
Epoch 6/10
10/10 - 2s - loss: 298.4883 - loglik: -2.9795e+02 - logprior: -5.3834e-01
Epoch 7/10
10/10 - 2s - loss: 296.9218 - loglik: -2.9683e+02 - logprior: -9.3040e-02
Epoch 8/10
10/10 - 2s - loss: 296.3807 - loglik: -2.9661e+02 - logprior: 0.2252
Epoch 9/10
10/10 - 2s - loss: 295.7234 - loglik: -2.9605e+02 - logprior: 0.3249
Epoch 10/10
10/10 - 2s - loss: 295.3184 - loglik: -2.9574e+02 - logprior: 0.4178
Fitted a model with MAP estimate = -295.1806
expansions: [(10, 2), (17, 1), (18, 1), (29, 3), (42, 2), (49, 2), (59, 3), (62, 1), (65, 1), (73, 1), (85, 1), (90, 2), (91, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 343.8723 - loglik: -2.9308e+02 - logprior: -5.0788e+01
Epoch 2/2
10/10 - 2s - loss: 304.6294 - loglik: -2.8525e+02 - logprior: -1.9378e+01
Fitted a model with MAP estimate = -298.5977
expansions: [(6, 2)]
discards: [  0   9  57 108]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 128 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 333.7634 - loglik: -2.8387e+02 - logprior: -4.9897e+01
Epoch 2/2
10/10 - 2s - loss: 299.6841 - loglik: -2.8264e+02 - logprior: -1.7046e+01
Fitted a model with MAP estimate = -292.7066
expansions: [(0, 5)]
discards: [ 0 49]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 319.6328 - loglik: -2.8047e+02 - logprior: -3.9165e+01
Epoch 2/10
10/10 - 2s - loss: 287.2961 - loglik: -2.7955e+02 - logprior: -7.7456e+00
Epoch 3/10
10/10 - 2s - loss: 280.2690 - loglik: -2.7895e+02 - logprior: -1.3220e+00
Epoch 4/10
10/10 - 2s - loss: 276.2182 - loglik: -2.7743e+02 - logprior: 1.2128
Epoch 5/10
10/10 - 2s - loss: 275.1596 - loglik: -2.7786e+02 - logprior: 2.6968
Epoch 6/10
10/10 - 2s - loss: 274.2714 - loglik: -2.7790e+02 - logprior: 3.6284
Epoch 7/10
10/10 - 2s - loss: 273.5870 - loglik: -2.7775e+02 - logprior: 4.1668
Epoch 8/10
10/10 - 2s - loss: 273.6429 - loglik: -2.7819e+02 - logprior: 4.5437
Fitted a model with MAP estimate = -273.2954
Time for alignment: 61.7465
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 432.4299 - loglik: -3.8695e+02 - logprior: -4.5483e+01
Epoch 2/10
10/10 - 2s - loss: 370.4760 - loglik: -3.6018e+02 - logprior: -1.0297e+01
Epoch 3/10
10/10 - 2s - loss: 334.2791 - loglik: -3.3018e+02 - logprior: -4.0999e+00
Epoch 4/10
10/10 - 2s - loss: 311.2090 - loglik: -3.0908e+02 - logprior: -2.1272e+00
Epoch 5/10
10/10 - 2s - loss: 301.8712 - loglik: -3.0061e+02 - logprior: -1.2648e+00
Epoch 6/10
10/10 - 2s - loss: 297.6469 - loglik: -2.9700e+02 - logprior: -6.4941e-01
Epoch 7/10
10/10 - 2s - loss: 296.2480 - loglik: -2.9609e+02 - logprior: -1.5667e-01
Epoch 8/10
10/10 - 2s - loss: 295.4912 - loglik: -2.9559e+02 - logprior: 0.0974
Epoch 9/10
10/10 - 2s - loss: 294.9691 - loglik: -2.9519e+02 - logprior: 0.2208
Epoch 10/10
10/10 - 2s - loss: 294.7775 - loglik: -2.9511e+02 - logprior: 0.3296
Fitted a model with MAP estimate = -294.4882
expansions: [(7, 2), (10, 2), (17, 1), (18, 1), (27, 1), (28, 4), (49, 2), (59, 3), (62, 1), (64, 1), (73, 1), (85, 1), (90, 2), (91, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 342.7142 - loglik: -2.9208e+02 - logprior: -5.0634e+01
Epoch 2/2
10/10 - 2s - loss: 303.0402 - loglik: -2.8389e+02 - logprior: -1.9154e+01
Fitted a model with MAP estimate = -296.8826
expansions: [(9, 3)]
discards: [  0  11  59 110]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 330.6251 - loglik: -2.8108e+02 - logprior: -4.9548e+01
Epoch 2/2
10/10 - 2s - loss: 295.7303 - loglik: -2.7903e+02 - logprior: -1.6704e+01
Fitted a model with MAP estimate = -288.7867
expansions: [(0, 3)]
discards: [ 0 35]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 315.3279 - loglik: -2.7662e+02 - logprior: -3.8710e+01
Epoch 2/10
10/10 - 2s - loss: 282.9870 - loglik: -2.7562e+02 - logprior: -7.3666e+00
Epoch 3/10
10/10 - 2s - loss: 276.2590 - loglik: -2.7539e+02 - logprior: -8.6966e-01
Epoch 4/10
10/10 - 2s - loss: 273.9124 - loglik: -2.7574e+02 - logprior: 1.8236
Epoch 5/10
10/10 - 2s - loss: 273.0909 - loglik: -2.7643e+02 - logprior: 3.3432
Epoch 6/10
10/10 - 2s - loss: 272.1445 - loglik: -2.7637e+02 - logprior: 4.2210
Epoch 7/10
10/10 - 2s - loss: 271.4400 - loglik: -2.7620e+02 - logprior: 4.7617
Epoch 8/10
10/10 - 2s - loss: 271.3842 - loglik: -2.7652e+02 - logprior: 5.1350
Epoch 9/10
10/10 - 2s - loss: 270.9044 - loglik: -2.7634e+02 - logprior: 5.4330
Epoch 10/10
10/10 - 2s - loss: 271.2086 - loglik: -2.7691e+02 - logprior: 5.7026
Fitted a model with MAP estimate = -270.7561
Time for alignment: 66.6520
Computed alignments with likelihoods: ['-271.2479', '-273.2954', '-270.7561']
Best model has likelihood: -270.7561
SP score = 0.9211
Training of 3 independent models on file hip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cc3b58e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9d596a8b0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 453.0040 - loglik: -1.8610e+02 - logprior: -2.6691e+02
Epoch 2/10
10/10 - 1s - loss: 230.3125 - loglik: -1.6038e+02 - logprior: -6.9931e+01
Epoch 3/10
10/10 - 1s - loss: 170.5663 - loglik: -1.3982e+02 - logprior: -3.0746e+01
Epoch 4/10
10/10 - 1s - loss: 142.5886 - loglik: -1.2591e+02 - logprior: -1.6682e+01
Epoch 5/10
10/10 - 1s - loss: 129.0955 - loglik: -1.2010e+02 - logprior: -8.9998e+00
Epoch 6/10
10/10 - 1s - loss: 121.7514 - loglik: -1.1805e+02 - logprior: -3.6990e+00
Epoch 7/10
10/10 - 1s - loss: 116.9155 - loglik: -1.1660e+02 - logprior: -3.1664e-01
Epoch 8/10
10/10 - 1s - loss: 114.1795 - loglik: -1.1605e+02 - logprior: 1.8670
Epoch 9/10
10/10 - 1s - loss: 112.6274 - loglik: -1.1610e+02 - logprior: 3.4742
Epoch 10/10
10/10 - 1s - loss: 111.5695 - loglik: -1.1625e+02 - logprior: 4.6800
Fitted a model with MAP estimate = -111.1080
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 461.9096 - loglik: -1.0728e+02 - logprior: -3.5463e+02
Epoch 2/2
10/10 - 1s - loss: 202.6653 - loglik: -9.6358e+01 - logprior: -1.0631e+02
Fitted a model with MAP estimate = -154.7184
expansions: []
discards: [ 0 21 37 48 53]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 395.8212 - loglik: -9.5377e+01 - logprior: -3.0044e+02
Epoch 2/2
10/10 - 1s - loss: 209.4037 - loglik: -9.5313e+01 - logprior: -1.1409e+02
Fitted a model with MAP estimate = -179.8537
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 367.9797 - loglik: -9.3377e+01 - logprior: -2.7460e+02
Epoch 2/10
10/10 - 1s - loss: 166.3695 - loglik: -9.3627e+01 - logprior: -7.2742e+01
Epoch 3/10
10/10 - 1s - loss: 116.0894 - loglik: -9.4373e+01 - logprior: -2.1716e+01
Epoch 4/10
10/10 - 1s - loss: 98.0752 - loglik: -9.5035e+01 - logprior: -3.0398e+00
Epoch 5/10
10/10 - 1s - loss: 88.9478 - loglik: -9.5513e+01 - logprior: 6.5656
Epoch 6/10
10/10 - 1s - loss: 83.6692 - loglik: -9.5912e+01 - logprior: 12.2424
Epoch 7/10
10/10 - 1s - loss: 80.3149 - loglik: -9.6195e+01 - logprior: 15.8797
Epoch 8/10
10/10 - 1s - loss: 77.9531 - loglik: -9.6399e+01 - logprior: 18.4464
Epoch 9/10
10/10 - 1s - loss: 76.1222 - loglik: -9.6557e+01 - logprior: 20.4348
Epoch 10/10
10/10 - 1s - loss: 74.5929 - loglik: -9.6681e+01 - logprior: 22.0882
Fitted a model with MAP estimate = -73.8287
Time for alignment: 29.6136
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 453.0040 - loglik: -1.8610e+02 - logprior: -2.6691e+02
Epoch 2/10
10/10 - 1s - loss: 230.3125 - loglik: -1.6038e+02 - logprior: -6.9931e+01
Epoch 3/10
10/10 - 1s - loss: 170.5663 - loglik: -1.3982e+02 - logprior: -3.0746e+01
Epoch 4/10
10/10 - 1s - loss: 142.5886 - loglik: -1.2591e+02 - logprior: -1.6682e+01
Epoch 5/10
10/10 - 1s - loss: 129.0955 - loglik: -1.2010e+02 - logprior: -8.9998e+00
Epoch 6/10
10/10 - 1s - loss: 121.7514 - loglik: -1.1805e+02 - logprior: -3.6990e+00
Epoch 7/10
10/10 - 1s - loss: 116.9155 - loglik: -1.1660e+02 - logprior: -3.1664e-01
Epoch 8/10
10/10 - 1s - loss: 114.1795 - loglik: -1.1605e+02 - logprior: 1.8670
Epoch 9/10
10/10 - 1s - loss: 112.6274 - loglik: -1.1610e+02 - logprior: 3.4742
Epoch 10/10
10/10 - 1s - loss: 111.5695 - loglik: -1.1625e+02 - logprior: 4.6800
Fitted a model with MAP estimate = -111.1080
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 461.9096 - loglik: -1.0728e+02 - logprior: -3.5463e+02
Epoch 2/2
10/10 - 1s - loss: 202.6653 - loglik: -9.6358e+01 - logprior: -1.0631e+02
Fitted a model with MAP estimate = -154.7184
expansions: []
discards: [ 0 21 37 48 53]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 395.8212 - loglik: -9.5377e+01 - logprior: -3.0044e+02
Epoch 2/2
10/10 - 1s - loss: 209.4037 - loglik: -9.5313e+01 - logprior: -1.1409e+02
Fitted a model with MAP estimate = -179.8537
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 367.9797 - loglik: -9.3377e+01 - logprior: -2.7460e+02
Epoch 2/10
10/10 - 1s - loss: 166.3695 - loglik: -9.3627e+01 - logprior: -7.2742e+01
Epoch 3/10
10/10 - 1s - loss: 116.0894 - loglik: -9.4373e+01 - logprior: -2.1716e+01
Epoch 4/10
10/10 - 1s - loss: 98.0752 - loglik: -9.5035e+01 - logprior: -3.0398e+00
Epoch 5/10
10/10 - 1s - loss: 88.9478 - loglik: -9.5513e+01 - logprior: 6.5656
Epoch 6/10
10/10 - 1s - loss: 83.6692 - loglik: -9.5912e+01 - logprior: 12.2424
Epoch 7/10
10/10 - 1s - loss: 80.3149 - loglik: -9.6195e+01 - logprior: 15.8797
Epoch 8/10
10/10 - 1s - loss: 77.9531 - loglik: -9.6399e+01 - logprior: 18.4464
Epoch 9/10
10/10 - 1s - loss: 76.1222 - loglik: -9.6557e+01 - logprior: 20.4348
Epoch 10/10
10/10 - 1s - loss: 74.5929 - loglik: -9.6681e+01 - logprior: 22.0882
Fitted a model with MAP estimate = -73.8287
Time for alignment: 30.1152
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 453.0040 - loglik: -1.8610e+02 - logprior: -2.6691e+02
Epoch 2/10
10/10 - 1s - loss: 230.3125 - loglik: -1.6038e+02 - logprior: -6.9931e+01
Epoch 3/10
10/10 - 1s - loss: 170.5663 - loglik: -1.3982e+02 - logprior: -3.0746e+01
Epoch 4/10
10/10 - 1s - loss: 142.5886 - loglik: -1.2591e+02 - logprior: -1.6682e+01
Epoch 5/10
10/10 - 1s - loss: 129.0955 - loglik: -1.2010e+02 - logprior: -8.9998e+00
Epoch 6/10
10/10 - 1s - loss: 121.7514 - loglik: -1.1805e+02 - logprior: -3.6990e+00
Epoch 7/10
10/10 - 1s - loss: 116.9155 - loglik: -1.1660e+02 - logprior: -3.1664e-01
Epoch 8/10
10/10 - 1s - loss: 114.1795 - loglik: -1.1605e+02 - logprior: 1.8670
Epoch 9/10
10/10 - 1s - loss: 112.6274 - loglik: -1.1610e+02 - logprior: 3.4742
Epoch 10/10
10/10 - 1s - loss: 111.5695 - loglik: -1.1625e+02 - logprior: 4.6800
Fitted a model with MAP estimate = -111.1080
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 461.9096 - loglik: -1.0728e+02 - logprior: -3.5463e+02
Epoch 2/2
10/10 - 1s - loss: 202.6653 - loglik: -9.6358e+01 - logprior: -1.0631e+02
Fitted a model with MAP estimate = -154.7184
expansions: []
discards: [ 0 21 37 48 53]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 395.8212 - loglik: -9.5377e+01 - logprior: -3.0044e+02
Epoch 2/2
10/10 - 1s - loss: 209.4037 - loglik: -9.5313e+01 - logprior: -1.1409e+02
Fitted a model with MAP estimate = -179.8537
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 367.9797 - loglik: -9.3377e+01 - logprior: -2.7460e+02
Epoch 2/10
10/10 - 1s - loss: 166.3695 - loglik: -9.3627e+01 - logprior: -7.2742e+01
Epoch 3/10
10/10 - 1s - loss: 116.0894 - loglik: -9.4373e+01 - logprior: -2.1716e+01
Epoch 4/10
10/10 - 1s - loss: 98.0752 - loglik: -9.5035e+01 - logprior: -3.0398e+00
Epoch 5/10
10/10 - 1s - loss: 88.9478 - loglik: -9.5513e+01 - logprior: 6.5656
Epoch 6/10
10/10 - 1s - loss: 83.6692 - loglik: -9.5912e+01 - logprior: 12.2424
Epoch 7/10
10/10 - 1s - loss: 80.3149 - loglik: -9.6195e+01 - logprior: 15.8797
Epoch 8/10
10/10 - 1s - loss: 77.9531 - loglik: -9.6399e+01 - logprior: 18.4464
Epoch 9/10
10/10 - 1s - loss: 76.1222 - loglik: -9.6557e+01 - logprior: 20.4348
Epoch 10/10
10/10 - 1s - loss: 74.5929 - loglik: -9.6681e+01 - logprior: 22.0882
Fitted a model with MAP estimate = -73.8287
Time for alignment: 28.7752
Computed alignments with likelihoods: ['-73.8287', '-73.8287', '-73.8287']
Best model has likelihood: -73.8287
SP score = 0.8520
Training of 3 independent models on file peroxidase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8342fe5e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9eece19a0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 200 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 11s - loss: 732.7435 - loglik: -7.2572e+02 - logprior: -7.0241e+00
Epoch 2/10
13/13 - 8s - loss: 640.5984 - loglik: -6.3952e+02 - logprior: -1.0815e+00
Epoch 3/10
13/13 - 8s - loss: 590.5234 - loglik: -5.8923e+02 - logprior: -1.2963e+00
Epoch 4/10
13/13 - 8s - loss: 579.1047 - loglik: -5.7773e+02 - logprior: -1.3730e+00
Epoch 5/10
13/13 - 8s - loss: 574.1255 - loglik: -5.7277e+02 - logprior: -1.3537e+00
Epoch 6/10
13/13 - 8s - loss: 573.4044 - loglik: -5.7207e+02 - logprior: -1.3363e+00
Epoch 7/10
13/13 - 8s - loss: 571.3941 - loglik: -5.7002e+02 - logprior: -1.3709e+00
Epoch 8/10
13/13 - 8s - loss: 573.7968 - loglik: -5.7238e+02 - logprior: -1.4180e+00
Fitted a model with MAP estimate = -571.4900
expansions: [(11, 3), (173, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 204 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 11s - loss: 575.8663 - loglik: -5.6851e+02 - logprior: -7.3555e+00
Epoch 2/2
13/13 - 8s - loss: 568.9008 - loglik: -5.6687e+02 - logprior: -2.0258e+00
Fitted a model with MAP estimate = -568.4755
expansions: []
discards: [12]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 203 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 11s - loss: 574.0355 - loglik: -5.6671e+02 - logprior: -7.3294e+00
Epoch 2/2
13/13 - 8s - loss: 569.3903 - loglik: -5.6746e+02 - logprior: -1.9353e+00
Fitted a model with MAP estimate = -568.5296
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 203 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 11s - loss: 574.0178 - loglik: -5.6678e+02 - logprior: -7.2356e+00
Epoch 2/10
13/13 - 8s - loss: 568.9841 - loglik: -5.6717e+02 - logprior: -1.8135e+00
Epoch 3/10
13/13 - 8s - loss: 568.1226 - loglik: -5.6672e+02 - logprior: -1.4012e+00
Epoch 4/10
13/13 - 8s - loss: 568.4174 - loglik: -5.6737e+02 - logprior: -1.0504e+00
Fitted a model with MAP estimate = -567.8396
Time for alignment: 180.1069
Fitting a model of length 200 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 11s - loss: 734.0528 - loglik: -7.2702e+02 - logprior: -7.0289e+00
Epoch 2/10
13/13 - 8s - loss: 649.8444 - loglik: -6.4878e+02 - logprior: -1.0642e+00
Epoch 3/10
13/13 - 8s - loss: 596.0695 - loglik: -5.9500e+02 - logprior: -1.0656e+00
Epoch 4/10
13/13 - 8s - loss: 580.0242 - loglik: -5.7889e+02 - logprior: -1.1311e+00
Epoch 5/10
13/13 - 8s - loss: 575.1830 - loglik: -5.7408e+02 - logprior: -1.1017e+00
Epoch 6/10
13/13 - 8s - loss: 570.7344 - loglik: -5.6969e+02 - logprior: -1.0450e+00
Epoch 7/10
13/13 - 8s - loss: 572.0927 - loglik: -5.7103e+02 - logprior: -1.0648e+00
Fitted a model with MAP estimate = -569.9192
expansions: [(45, 1), (110, 1), (179, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 202 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 11s - loss: 579.9348 - loglik: -5.7057e+02 - logprior: -9.3661e+00
Epoch 2/2
13/13 - 8s - loss: 570.5151 - loglik: -5.6658e+02 - logprior: -3.9381e+00
Fitted a model with MAP estimate = -570.2124
expansions: [(0, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 205 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 12s - loss: 574.2156 - loglik: -5.6698e+02 - logprior: -7.2346e+00
Epoch 2/2
13/13 - 8s - loss: 567.1097 - loglik: -5.6553e+02 - logprior: -1.5774e+00
Fitted a model with MAP estimate = -566.2920
expansions: []
discards: [1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 203 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 11s - loss: 571.8097 - loglik: -5.6477e+02 - logprior: -7.0396e+00
Epoch 2/10
13/13 - 8s - loss: 567.3683 - loglik: -5.6592e+02 - logprior: -1.4521e+00
Epoch 3/10
13/13 - 8s - loss: 566.4818 - loglik: -5.6557e+02 - logprior: -9.1426e-01
Epoch 4/10
13/13 - 8s - loss: 565.4180 - loglik: -5.6480e+02 - logprior: -6.1341e-01
Epoch 5/10
13/13 - 9s - loss: 566.8370 - loglik: -5.6638e+02 - logprior: -4.6192e-01
Fitted a model with MAP estimate = -565.7263
Time for alignment: 180.2780
Fitting a model of length 200 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 11s - loss: 733.2543 - loglik: -7.2623e+02 - logprior: -7.0251e+00
Epoch 2/10
13/13 - 8s - loss: 645.8969 - loglik: -6.4489e+02 - logprior: -1.0104e+00
Epoch 3/10
13/13 - 8s - loss: 594.1866 - loglik: -5.9305e+02 - logprior: -1.1330e+00
Epoch 4/10
13/13 - 8s - loss: 582.3075 - loglik: -5.8088e+02 - logprior: -1.4282e+00
Epoch 5/10
13/13 - 8s - loss: 575.9567 - loglik: -5.7475e+02 - logprior: -1.2088e+00
Epoch 6/10
13/13 - 8s - loss: 572.8622 - loglik: -5.7168e+02 - logprior: -1.1841e+00
Epoch 7/10
13/13 - 8s - loss: 573.6884 - loglik: -5.7243e+02 - logprior: -1.2595e+00
Fitted a model with MAP estimate = -572.3101
expansions: [(163, 1), (173, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 202 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 12s - loss: 574.5383 - loglik: -5.6721e+02 - logprior: -7.3247e+00
Epoch 2/2
13/13 - 8s - loss: 573.4752 - loglik: -5.7146e+02 - logprior: -2.0138e+00
Fitted a model with MAP estimate = -570.0930
expansions: [(0, 3), (9, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 206 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 11s - loss: 576.7451 - loglik: -5.6657e+02 - logprior: -1.0174e+01
Epoch 2/2
13/13 - 8s - loss: 571.7344 - loglik: -5.6897e+02 - logprior: -2.7608e+00
Fitted a model with MAP estimate = -569.4409
expansions: []
discards: [0 1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 203 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 11s - loss: 573.2327 - loglik: -5.6573e+02 - logprior: -7.5029e+00
Epoch 2/10
13/13 - 8s - loss: 570.1148 - loglik: -5.6838e+02 - logprior: -1.7375e+00
Epoch 3/10
13/13 - 8s - loss: 568.2703 - loglik: -5.6711e+02 - logprior: -1.1587e+00
Epoch 4/10
13/13 - 8s - loss: 569.1683 - loglik: -5.6833e+02 - logprior: -8.3770e-01
Fitted a model with MAP estimate = -568.1360
Time for alignment: 172.2034
Computed alignments with likelihoods: ['-567.8396', '-565.7263', '-568.1360']
Best model has likelihood: -565.7263
SP score = 0.5161
Training of 3 independent models on file TNF.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9ccf705e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb1f49fe2e0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 421.9616 - loglik: -3.4522e+02 - logprior: -7.6739e+01
Epoch 2/10
10/10 - 1s - loss: 336.8337 - loglik: -3.1923e+02 - logprior: -1.7607e+01
Epoch 3/10
10/10 - 1s - loss: 298.2946 - loglik: -2.9188e+02 - logprior: -6.4170e+00
Epoch 4/10
10/10 - 1s - loss: 278.1390 - loglik: -2.7554e+02 - logprior: -2.6025e+00
Epoch 5/10
10/10 - 1s - loss: 270.4059 - loglik: -2.6969e+02 - logprior: -7.1619e-01
Epoch 6/10
10/10 - 1s - loss: 266.4900 - loglik: -2.6690e+02 - logprior: 0.4125
Epoch 7/10
10/10 - 1s - loss: 264.3210 - loglik: -2.6541e+02 - logprior: 1.0874
Epoch 8/10
10/10 - 1s - loss: 262.7369 - loglik: -2.6425e+02 - logprior: 1.5085
Epoch 9/10
10/10 - 1s - loss: 261.2867 - loglik: -2.6314e+02 - logprior: 1.8511
Epoch 10/10
10/10 - 1s - loss: 260.9174 - loglik: -2.6306e+02 - logprior: 2.1439
Fitted a model with MAP estimate = -260.6901
expansions: [(5, 2), (6, 1), (7, 1), (10, 1), (12, 3), (14, 1), (18, 1), (36, 4), (45, 3), (63, 2), (64, 2), (68, 1), (83, 4), (86, 1), (88, 3), (89, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 342.3086 - loglik: -2.5693e+02 - logprior: -8.5382e+01
Epoch 2/2
10/10 - 2s - loss: 279.4717 - loglik: -2.4695e+02 - logprior: -3.2520e+01
Fitted a model with MAP estimate = -268.6148
expansions: [(0, 2), (113, 2)]
discards: [  0   5  82 106 107]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 307.8264 - loglik: -2.4045e+02 - logprior: -6.7378e+01
Epoch 2/2
10/10 - 2s - loss: 250.9270 - loglik: -2.3673e+02 - logprior: -1.4202e+01
Fitted a model with MAP estimate = -242.5919
expansions: [(16, 1), (79, 1), (110, 1)]
discards: [ 0 45]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 319.5677 - loglik: -2.3683e+02 - logprior: -8.2738e+01
Epoch 2/10
10/10 - 2s - loss: 262.2045 - loglik: -2.3613e+02 - logprior: -2.6070e+01
Epoch 3/10
10/10 - 2s - loss: 242.5612 - loglik: -2.3613e+02 - logprior: -6.4286e+00
Epoch 4/10
10/10 - 2s - loss: 234.1787 - loglik: -2.3617e+02 - logprior: 1.9897
Epoch 5/10
10/10 - 2s - loss: 231.1076 - loglik: -2.3631e+02 - logprior: 5.2007
Epoch 6/10
10/10 - 2s - loss: 229.5539 - loglik: -2.3642e+02 - logprior: 6.8634
Epoch 7/10
10/10 - 2s - loss: 228.4786 - loglik: -2.3636e+02 - logprior: 7.8785
Epoch 8/10
10/10 - 2s - loss: 228.0216 - loglik: -2.3662e+02 - logprior: 8.6003
Epoch 9/10
10/10 - 2s - loss: 227.3989 - loglik: -2.3660e+02 - logprior: 9.1976
Epoch 10/10
10/10 - 2s - loss: 226.8184 - loglik: -2.3656e+02 - logprior: 9.7464
Fitted a model with MAP estimate = -226.6755
Time for alignment: 50.9668
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 421.8534 - loglik: -3.4512e+02 - logprior: -7.6736e+01
Epoch 2/10
10/10 - 1s - loss: 336.7350 - loglik: -3.1913e+02 - logprior: -1.7607e+01
Epoch 3/10
10/10 - 1s - loss: 298.3127 - loglik: -2.9191e+02 - logprior: -6.4069e+00
Epoch 4/10
10/10 - 1s - loss: 279.2898 - loglik: -2.7694e+02 - logprior: -2.3452e+00
Epoch 5/10
10/10 - 1s - loss: 271.1334 - loglik: -2.7070e+02 - logprior: -4.3623e-01
Epoch 6/10
10/10 - 1s - loss: 267.1499 - loglik: -2.6761e+02 - logprior: 0.4586
Epoch 7/10
10/10 - 1s - loss: 265.5923 - loglik: -2.6674e+02 - logprior: 1.1429
Epoch 8/10
10/10 - 1s - loss: 264.2914 - loglik: -2.6601e+02 - logprior: 1.7219
Epoch 9/10
10/10 - 1s - loss: 263.7918 - loglik: -2.6590e+02 - logprior: 2.1051
Epoch 10/10
10/10 - 1s - loss: 263.2122 - loglik: -2.6553e+02 - logprior: 2.3158
Fitted a model with MAP estimate = -262.9188
expansions: [(5, 2), (6, 2), (11, 2), (12, 1), (18, 2), (19, 1), (36, 4), (45, 3), (63, 3), (78, 2), (83, 3), (86, 1), (88, 4), (89, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 344.4885 - loglik: -2.5916e+02 - logprior: -8.5330e+01
Epoch 2/2
10/10 - 2s - loss: 280.8964 - loglik: -2.4837e+02 - logprior: -3.2523e+01
Fitted a model with MAP estimate = -270.4394
expansions: [(0, 2)]
discards: [  0  27  45  46  97 105]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 310.2972 - loglik: -2.4323e+02 - logprior: -6.7064e+01
Epoch 2/2
10/10 - 2s - loss: 255.7461 - loglik: -2.4179e+02 - logprior: -1.3961e+01
Fitted a model with MAP estimate = -247.5013
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 322.8583 - loglik: -2.4224e+02 - logprior: -8.0614e+01
Epoch 2/10
10/10 - 2s - loss: 262.5961 - loglik: -2.4191e+02 - logprior: -2.0682e+01
Epoch 3/10
10/10 - 2s - loss: 246.1899 - loglik: -2.4231e+02 - logprior: -3.8780e+00
Epoch 4/10
10/10 - 2s - loss: 240.6143 - loglik: -2.4268e+02 - logprior: 2.0624
Epoch 5/10
10/10 - 2s - loss: 237.8091 - loglik: -2.4271e+02 - logprior: 4.9027
Epoch 6/10
10/10 - 2s - loss: 236.3652 - loglik: -2.4290e+02 - logprior: 6.5312
Epoch 7/10
10/10 - 2s - loss: 235.5763 - loglik: -2.4312e+02 - logprior: 7.5439
Epoch 8/10
10/10 - 2s - loss: 234.7580 - loglik: -2.4302e+02 - logprior: 8.2626
Epoch 9/10
10/10 - 2s - loss: 234.6021 - loglik: -2.4346e+02 - logprior: 8.8606
Epoch 10/10
10/10 - 2s - loss: 233.9726 - loglik: -2.4337e+02 - logprior: 9.3950
Fitted a model with MAP estimate = -233.6879
Time for alignment: 51.7355
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 421.9047 - loglik: -3.4517e+02 - logprior: -7.6736e+01
Epoch 2/10
10/10 - 1s - loss: 336.1046 - loglik: -3.1849e+02 - logprior: -1.7615e+01
Epoch 3/10
10/10 - 1s - loss: 296.3640 - loglik: -2.8992e+02 - logprior: -6.4461e+00
Epoch 4/10
10/10 - 1s - loss: 277.7968 - loglik: -2.7522e+02 - logprior: -2.5735e+00
Epoch 5/10
10/10 - 1s - loss: 270.0718 - loglik: -2.6933e+02 - logprior: -7.4564e-01
Epoch 6/10
10/10 - 1s - loss: 266.9809 - loglik: -2.6728e+02 - logprior: 0.3036
Epoch 7/10
10/10 - 1s - loss: 265.2845 - loglik: -2.6643e+02 - logprior: 1.1440
Epoch 8/10
10/10 - 1s - loss: 264.1618 - loglik: -2.6574e+02 - logprior: 1.5775
Epoch 9/10
10/10 - 1s - loss: 263.3860 - loglik: -2.6516e+02 - logprior: 1.7776
Epoch 10/10
10/10 - 1s - loss: 263.1809 - loglik: -2.6520e+02 - logprior: 2.0234
Fitted a model with MAP estimate = -262.9248
expansions: [(5, 1), (6, 1), (11, 3), (12, 1), (18, 1), (19, 1), (36, 4), (45, 1), (46, 1), (55, 2), (79, 2), (83, 3), (86, 1), (88, 4), (89, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 345.8056 - loglik: -2.6030e+02 - logprior: -8.5501e+01
Epoch 2/2
10/10 - 2s - loss: 283.1976 - loglik: -2.5039e+02 - logprior: -3.2810e+01
Fitted a model with MAP estimate = -272.4826
expansions: [(0, 2), (12, 1), (79, 2)]
discards: [  0  43  44  94 101]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 311.8867 - loglik: -2.4448e+02 - logprior: -6.7403e+01
Epoch 2/2
10/10 - 2s - loss: 256.1825 - loglik: -2.4195e+02 - logprior: -1.4232e+01
Fitted a model with MAP estimate = -247.9427
expansions: [(15, 1), (112, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 324.4208 - loglik: -2.4159e+02 - logprior: -8.2826e+01
Epoch 2/10
10/10 - 2s - loss: 266.5854 - loglik: -2.4063e+02 - logprior: -2.5953e+01
Epoch 3/10
10/10 - 2s - loss: 247.1488 - loglik: -2.4087e+02 - logprior: -6.2808e+00
Epoch 4/10
10/10 - 2s - loss: 238.6691 - loglik: -2.4062e+02 - logprior: 1.9534
Epoch 5/10
10/10 - 2s - loss: 235.7294 - loglik: -2.4088e+02 - logprior: 5.1457
Epoch 6/10
10/10 - 2s - loss: 234.3581 - loglik: -2.4116e+02 - logprior: 6.8024
Epoch 7/10
10/10 - 2s - loss: 233.2602 - loglik: -2.4108e+02 - logprior: 7.8161
Epoch 8/10
10/10 - 2s - loss: 232.8801 - loglik: -2.4141e+02 - logprior: 8.5344
Epoch 9/10
10/10 - 2s - loss: 232.1456 - loglik: -2.4126e+02 - logprior: 9.1120
Epoch 10/10
10/10 - 2s - loss: 231.4312 - loglik: -2.4108e+02 - logprior: 9.6505
Fitted a model with MAP estimate = -231.3928
Time for alignment: 50.2360
Computed alignments with likelihoods: ['-226.6755', '-233.6879', '-231.3928']
Best model has likelihood: -226.6755
SP score = 0.7489
Training of 3 independent models on file egf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cd406c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbba4d6fb50>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 97.0896 - loglik: -9.2665e+01 - logprior: -4.4247e+00
Epoch 2/10
17/17 - 1s - loss: 74.9583 - loglik: -7.3425e+01 - logprior: -1.5332e+00
Epoch 3/10
17/17 - 1s - loss: 65.1942 - loglik: -6.3543e+01 - logprior: -1.6512e+00
Epoch 4/10
17/17 - 1s - loss: 63.6636 - loglik: -6.2038e+01 - logprior: -1.6254e+00
Epoch 5/10
17/17 - 1s - loss: 63.1448 - loglik: -6.1614e+01 - logprior: -1.5304e+00
Epoch 6/10
17/17 - 1s - loss: 62.9843 - loglik: -6.1437e+01 - logprior: -1.5472e+00
Epoch 7/10
17/17 - 1s - loss: 63.0355 - loglik: -6.1513e+01 - logprior: -1.5222e+00
Fitted a model with MAP estimate = -62.9125
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 69.8500 - loglik: -6.4336e+01 - logprior: -5.5139e+00
Epoch 2/2
17/17 - 1s - loss: 61.5128 - loglik: -5.8999e+01 - logprior: -2.5140e+00
Fitted a model with MAP estimate = -59.3168
expansions: []
discards: [13 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 62.0333 - loglik: -5.7700e+01 - logprior: -4.3333e+00
Epoch 2/2
17/17 - 1s - loss: 58.4925 - loglik: -5.6832e+01 - logprior: -1.6608e+00
Fitted a model with MAP estimate = -58.1773
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 5s - loss: 61.3107 - loglik: -5.7077e+01 - logprior: -4.2335e+00
Epoch 2/10
17/17 - 1s - loss: 58.3476 - loglik: -5.6706e+01 - logprior: -1.6415e+00
Epoch 3/10
17/17 - 1s - loss: 58.1976 - loglik: -5.6783e+01 - logprior: -1.4148e+00
Epoch 4/10
17/17 - 1s - loss: 57.9399 - loglik: -5.6589e+01 - logprior: -1.3514e+00
Epoch 5/10
17/17 - 1s - loss: 57.8560 - loglik: -5.6534e+01 - logprior: -1.3222e+00
Epoch 6/10
17/17 - 1s - loss: 57.8954 - loglik: -5.6599e+01 - logprior: -1.2960e+00
Fitted a model with MAP estimate = -57.8445
Time for alignment: 29.7198
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 97.2460 - loglik: -9.2823e+01 - logprior: -4.4230e+00
Epoch 2/10
17/17 - 1s - loss: 75.0266 - loglik: -7.3486e+01 - logprior: -1.5407e+00
Epoch 3/10
17/17 - 1s - loss: 65.5241 - loglik: -6.3862e+01 - logprior: -1.6619e+00
Epoch 4/10
17/17 - 1s - loss: 63.7735 - loglik: -6.2147e+01 - logprior: -1.6261e+00
Epoch 5/10
17/17 - 1s - loss: 63.1128 - loglik: -6.1570e+01 - logprior: -1.5423e+00
Epoch 6/10
17/17 - 1s - loss: 63.1300 - loglik: -6.1575e+01 - logprior: -1.5547e+00
Fitted a model with MAP estimate = -62.9916
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 69.8276 - loglik: -6.4303e+01 - logprior: -5.5241e+00
Epoch 2/2
17/17 - 1s - loss: 61.4383 - loglik: -5.8959e+01 - logprior: -2.4797e+00
Fitted a model with MAP estimate = -59.2244
expansions: []
discards: [12 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 5s - loss: 61.8469 - loglik: -5.7526e+01 - logprior: -4.3212e+00
Epoch 2/2
17/17 - 1s - loss: 58.4444 - loglik: -5.6786e+01 - logprior: -1.6588e+00
Fitted a model with MAP estimate = -58.1738
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 61.2593 - loglik: -5.7032e+01 - logprior: -4.2274e+00
Epoch 2/10
17/17 - 1s - loss: 58.4469 - loglik: -5.6795e+01 - logprior: -1.6519e+00
Epoch 3/10
17/17 - 1s - loss: 57.9547 - loglik: -5.6537e+01 - logprior: -1.4175e+00
Epoch 4/10
17/17 - 1s - loss: 58.0183 - loglik: -5.6667e+01 - logprior: -1.3511e+00
Fitted a model with MAP estimate = -57.9241
Time for alignment: 26.6533
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 97.1447 - loglik: -9.2719e+01 - logprior: -4.4257e+00
Epoch 2/10
17/17 - 1s - loss: 75.6347 - loglik: -7.4080e+01 - logprior: -1.5546e+00
Epoch 3/10
17/17 - 1s - loss: 65.4669 - loglik: -6.3800e+01 - logprior: -1.6669e+00
Epoch 4/10
17/17 - 1s - loss: 62.9175 - loglik: -6.1270e+01 - logprior: -1.6473e+00
Epoch 5/10
17/17 - 1s - loss: 62.4233 - loglik: -6.0872e+01 - logprior: -1.5511e+00
Epoch 6/10
17/17 - 1s - loss: 62.3094 - loglik: -6.0742e+01 - logprior: -1.5669e+00
Epoch 7/10
17/17 - 1s - loss: 62.0856 - loglik: -6.0545e+01 - logprior: -1.5402e+00
Epoch 8/10
17/17 - 1s - loss: 62.1874 - loglik: -6.0666e+01 - logprior: -1.5212e+00
Fitted a model with MAP estimate = -62.0556
expansions: [(2, 1), (10, 1), (11, 2), (12, 3), (13, 1), (14, 1), (15, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 69.2105 - loglik: -6.3663e+01 - logprior: -5.5477e+00
Epoch 2/2
17/17 - 1s - loss: 61.3487 - loglik: -5.8600e+01 - logprior: -2.7491e+00
Fitted a model with MAP estimate = -59.5253
expansions: [(2, 1)]
discards: [ 0 13 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 5s - loss: 63.7979 - loglik: -5.8873e+01 - logprior: -4.9254e+00
Epoch 2/2
17/17 - 1s - loss: 58.6934 - loglik: -5.6979e+01 - logprior: -1.7144e+00
Fitted a model with MAP estimate = -58.2278
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 61.3116 - loglik: -5.7073e+01 - logprior: -4.2383e+00
Epoch 2/10
17/17 - 1s - loss: 58.3712 - loglik: -5.6722e+01 - logprior: -1.6493e+00
Epoch 3/10
17/17 - 1s - loss: 58.0590 - loglik: -5.6649e+01 - logprior: -1.4098e+00
Epoch 4/10
17/17 - 1s - loss: 57.9813 - loglik: -5.6629e+01 - logprior: -1.3526e+00
Epoch 5/10
17/17 - 1s - loss: 58.0465 - loglik: -5.6731e+01 - logprior: -1.3155e+00
Fitted a model with MAP estimate = -57.8887
Time for alignment: 28.7309
Computed alignments with likelihoods: ['-57.8445', '-57.9241', '-57.8887']
Best model has likelihood: -57.8445
SP score = 0.7611
Training of 3 independent models on file HMG_box.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbb941d3760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c0c8cf40>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 3s - loss: 193.7076 - loglik: -1.8562e+02 - logprior: -8.0917e+00
Epoch 2/10
13/13 - 1s - loss: 163.5904 - loglik: -1.6138e+02 - logprior: -2.2095e+00
Epoch 3/10
13/13 - 1s - loss: 147.8923 - loglik: -1.4599e+02 - logprior: -1.9023e+00
Epoch 4/10
13/13 - 1s - loss: 140.2550 - loglik: -1.3824e+02 - logprior: -2.0109e+00
Epoch 5/10
13/13 - 1s - loss: 137.2578 - loglik: -1.3537e+02 - logprior: -1.8890e+00
Epoch 6/10
13/13 - 1s - loss: 135.5165 - loglik: -1.3371e+02 - logprior: -1.8089e+00
Epoch 7/10
13/13 - 1s - loss: 134.7796 - loglik: -1.3295e+02 - logprior: -1.8308e+00
Epoch 8/10
13/13 - 1s - loss: 134.7006 - loglik: -1.3289e+02 - logprior: -1.8097e+00
Epoch 9/10
13/13 - 1s - loss: 134.5652 - loglik: -1.3277e+02 - logprior: -1.7970e+00
Epoch 10/10
13/13 - 1s - loss: 134.1966 - loglik: -1.3241e+02 - logprior: -1.7888e+00
Fitted a model with MAP estimate = -134.3012
expansions: [(12, 1), (17, 5), (18, 2), (19, 1), (34, 1), (41, 1), (44, 1), (45, 1), (46, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 3s - loss: 139.8742 - loglik: -1.3033e+02 - logprior: -9.5491e+00
Epoch 2/2
13/13 - 1s - loss: 126.9919 - loglik: -1.2245e+02 - logprior: -4.5382e+00
Fitted a model with MAP estimate = -125.5331
expansions: [(0, 2)]
discards: [ 0 23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 127.5008 - loglik: -1.2013e+02 - logprior: -7.3665e+00
Epoch 2/2
13/13 - 1s - loss: 121.8825 - loglik: -1.1963e+02 - logprior: -2.2531e+00
Fitted a model with MAP estimate = -120.8601
expansions: [(17, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 3s - loss: 130.8566 - loglik: -1.2154e+02 - logprior: -9.3207e+00
Epoch 2/10
13/13 - 1s - loss: 123.9785 - loglik: -1.2041e+02 - logprior: -3.5664e+00
Epoch 3/10
13/13 - 1s - loss: 121.2343 - loglik: -1.1947e+02 - logprior: -1.7655e+00
Epoch 4/10
13/13 - 1s - loss: 120.9451 - loglik: -1.1953e+02 - logprior: -1.4116e+00
Epoch 5/10
13/13 - 1s - loss: 120.2842 - loglik: -1.1903e+02 - logprior: -1.2575e+00
Epoch 6/10
13/13 - 1s - loss: 119.8377 - loglik: -1.1860e+02 - logprior: -1.2415e+00
Epoch 7/10
13/13 - 1s - loss: 119.7957 - loglik: -1.1855e+02 - logprior: -1.2424e+00
Epoch 8/10
13/13 - 1s - loss: 119.5965 - loglik: -1.1840e+02 - logprior: -1.2003e+00
Epoch 9/10
13/13 - 1s - loss: 119.4046 - loglik: -1.1824e+02 - logprior: -1.1689e+00
Epoch 10/10
13/13 - 1s - loss: 119.6192 - loglik: -1.1849e+02 - logprior: -1.1311e+00
Fitted a model with MAP estimate = -119.2986
Time for alignment: 36.8556
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 193.7021 - loglik: -1.8561e+02 - logprior: -8.0921e+00
Epoch 2/10
13/13 - 1s - loss: 163.9022 - loglik: -1.6169e+02 - logprior: -2.2133e+00
Epoch 3/10
13/13 - 1s - loss: 145.6961 - loglik: -1.4375e+02 - logprior: -1.9503e+00
Epoch 4/10
13/13 - 1s - loss: 139.6958 - loglik: -1.3763e+02 - logprior: -2.0695e+00
Epoch 5/10
13/13 - 1s - loss: 136.1656 - loglik: -1.3424e+02 - logprior: -1.9265e+00
Epoch 6/10
13/13 - 1s - loss: 134.9283 - loglik: -1.3308e+02 - logprior: -1.8521e+00
Epoch 7/10
13/13 - 1s - loss: 134.7888 - loglik: -1.3292e+02 - logprior: -1.8718e+00
Epoch 8/10
13/13 - 1s - loss: 133.8898 - loglik: -1.3204e+02 - logprior: -1.8485e+00
Epoch 9/10
13/13 - 1s - loss: 134.4027 - loglik: -1.3257e+02 - logprior: -1.8319e+00
Fitted a model with MAP estimate = -133.9876
expansions: [(12, 1), (17, 5), (18, 1), (29, 1), (32, 1), (38, 1), (42, 1), (45, 1), (46, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 139.3966 - loglik: -1.2985e+02 - logprior: -9.5486e+00
Epoch 2/2
13/13 - 1s - loss: 126.6880 - loglik: -1.2220e+02 - logprior: -4.4868e+00
Fitted a model with MAP estimate = -125.4809
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 3s - loss: 127.5141 - loglik: -1.2015e+02 - logprior: -7.3597e+00
Epoch 2/2
13/13 - 1s - loss: 121.7739 - loglik: -1.1952e+02 - logprior: -2.2569e+00
Fitted a model with MAP estimate = -120.9924
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 3s - loss: 130.5678 - loglik: -1.2122e+02 - logprior: -9.3501e+00
Epoch 2/10
13/13 - 1s - loss: 124.4588 - loglik: -1.2092e+02 - logprior: -3.5408e+00
Epoch 3/10
13/13 - 1s - loss: 122.0074 - loglik: -1.2022e+02 - logprior: -1.7868e+00
Epoch 4/10
13/13 - 1s - loss: 121.1508 - loglik: -1.1971e+02 - logprior: -1.4375e+00
Epoch 5/10
13/13 - 1s - loss: 121.1042 - loglik: -1.1981e+02 - logprior: -1.2913e+00
Epoch 6/10
13/13 - 1s - loss: 120.9544 - loglik: -1.1967e+02 - logprior: -1.2850e+00
Epoch 7/10
13/13 - 1s - loss: 121.2414 - loglik: -1.1996e+02 - logprior: -1.2794e+00
Fitted a model with MAP estimate = -120.9069
Time for alignment: 33.1856
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 193.7029 - loglik: -1.8561e+02 - logprior: -8.0921e+00
Epoch 2/10
13/13 - 1s - loss: 163.7428 - loglik: -1.6152e+02 - logprior: -2.2182e+00
Epoch 3/10
13/13 - 1s - loss: 146.2694 - loglik: -1.4433e+02 - logprior: -1.9415e+00
Epoch 4/10
13/13 - 1s - loss: 141.1753 - loglik: -1.3914e+02 - logprior: -2.0372e+00
Epoch 5/10
13/13 - 1s - loss: 137.4162 - loglik: -1.3554e+02 - logprior: -1.8760e+00
Epoch 6/10
13/13 - 1s - loss: 135.4701 - loglik: -1.3365e+02 - logprior: -1.8247e+00
Epoch 7/10
13/13 - 1s - loss: 134.5459 - loglik: -1.3268e+02 - logprior: -1.8693e+00
Epoch 8/10
13/13 - 1s - loss: 133.8813 - loglik: -1.3203e+02 - logprior: -1.8503e+00
Epoch 9/10
13/13 - 1s - loss: 134.1758 - loglik: -1.3235e+02 - logprior: -1.8304e+00
Fitted a model with MAP estimate = -133.8500
expansions: [(12, 1), (14, 1), (17, 5), (18, 2), (29, 1), (32, 1), (41, 1), (44, 1), (45, 1), (46, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 139.0899 - loglik: -1.2956e+02 - logprior: -9.5340e+00
Epoch 2/2
13/13 - 1s - loss: 125.9121 - loglik: -1.2142e+02 - logprior: -4.4883e+00
Fitted a model with MAP estimate = -123.8965
expansions: [(0, 1)]
discards: [ 0 24]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 3s - loss: 126.6742 - loglik: -1.1938e+02 - logprior: -7.2974e+00
Epoch 2/2
13/13 - 1s - loss: 121.3665 - loglik: -1.1934e+02 - logprior: -2.0312e+00
Fitted a model with MAP estimate = -120.5387
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 3s - loss: 125.8653 - loglik: -1.1862e+02 - logprior: -7.2489e+00
Epoch 2/10
13/13 - 1s - loss: 120.9791 - loglik: -1.1898e+02 - logprior: -1.9956e+00
Epoch 3/10
13/13 - 1s - loss: 120.2816 - loglik: -1.1866e+02 - logprior: -1.6197e+00
Epoch 4/10
13/13 - 1s - loss: 119.8772 - loglik: -1.1846e+02 - logprior: -1.4135e+00
Epoch 5/10
13/13 - 1s - loss: 119.9844 - loglik: -1.1876e+02 - logprior: -1.2239e+00
Fitted a model with MAP estimate = -119.8374
Time for alignment: 31.8873
Computed alignments with likelihoods: ['-119.2986', '-120.9069', '-119.8374']
Best model has likelihood: -119.2986
SP score = 0.9731
Training of 3 independent models on file hpr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cc5a5a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9c1574190>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 240.8699 - loglik: -2.2833e+02 - logprior: -1.2540e+01
Epoch 2/10
11/11 - 1s - loss: 208.4330 - loglik: -2.0519e+02 - logprior: -3.2451e+00
Epoch 3/10
11/11 - 1s - loss: 183.5022 - loglik: -1.8146e+02 - logprior: -2.0435e+00
Epoch 4/10
11/11 - 1s - loss: 167.3404 - loglik: -1.6555e+02 - logprior: -1.7895e+00
Epoch 5/10
11/11 - 1s - loss: 161.0873 - loglik: -1.5955e+02 - logprior: -1.5329e+00
Epoch 6/10
11/11 - 1s - loss: 158.8895 - loglik: -1.5746e+02 - logprior: -1.4336e+00
Epoch 7/10
11/11 - 1s - loss: 158.0320 - loglik: -1.5682e+02 - logprior: -1.2097e+00
Epoch 8/10
11/11 - 1s - loss: 157.5281 - loglik: -1.5647e+02 - logprior: -1.0620e+00
Epoch 9/10
11/11 - 1s - loss: 157.1306 - loglik: -1.5617e+02 - logprior: -9.6387e-01
Epoch 10/10
11/11 - 1s - loss: 156.9406 - loglik: -1.5602e+02 - logprior: -9.1778e-01
Fitted a model with MAP estimate = -156.7531
expansions: [(0, 6), (22, 1), (27, 1), (29, 2), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 167.3484 - loglik: -1.5167e+02 - logprior: -1.5683e+01
Epoch 2/2
11/11 - 1s - loss: 148.6584 - loglik: -1.4421e+02 - logprior: -4.4459e+00
Fitted a model with MAP estimate = -145.5723
expansions: []
discards: [ 0 37]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 159.4699 - loglik: -1.4532e+02 - logprior: -1.4153e+01
Epoch 2/2
11/11 - 1s - loss: 149.4492 - loglik: -1.4391e+02 - logprior: -5.5345e+00
Fitted a model with MAP estimate = -147.4371
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 155.5000 - loglik: -1.4317e+02 - logprior: -1.2326e+01
Epoch 2/10
11/11 - 1s - loss: 145.8075 - loglik: -1.4263e+02 - logprior: -3.1825e+00
Epoch 3/10
11/11 - 1s - loss: 144.4606 - loglik: -1.4259e+02 - logprior: -1.8711e+00
Epoch 4/10
11/11 - 1s - loss: 143.5889 - loglik: -1.4245e+02 - logprior: -1.1384e+00
Epoch 5/10
11/11 - 1s - loss: 143.7583 - loglik: -1.4292e+02 - logprior: -8.3411e-01
Fitted a model with MAP estimate = -143.3424
Time for alignment: 34.4851
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 240.8731 - loglik: -2.2833e+02 - logprior: -1.2540e+01
Epoch 2/10
11/11 - 1s - loss: 207.7037 - loglik: -2.0446e+02 - logprior: -3.2444e+00
Epoch 3/10
11/11 - 1s - loss: 181.5485 - loglik: -1.7948e+02 - logprior: -2.0732e+00
Epoch 4/10
11/11 - 1s - loss: 165.7488 - loglik: -1.6391e+02 - logprior: -1.8341e+00
Epoch 5/10
11/11 - 1s - loss: 160.8875 - loglik: -1.5932e+02 - logprior: -1.5632e+00
Epoch 6/10
11/11 - 1s - loss: 158.7350 - loglik: -1.5732e+02 - logprior: -1.4164e+00
Epoch 7/10
11/11 - 1s - loss: 157.6247 - loglik: -1.5646e+02 - logprior: -1.1693e+00
Epoch 8/10
11/11 - 1s - loss: 157.3499 - loglik: -1.5632e+02 - logprior: -1.0336e+00
Epoch 9/10
11/11 - 1s - loss: 156.9895 - loglik: -1.5604e+02 - logprior: -9.4792e-01
Epoch 10/10
11/11 - 1s - loss: 157.2045 - loglik: -1.5630e+02 - logprior: -9.0466e-01
Fitted a model with MAP estimate = -156.6604
expansions: [(0, 6), (22, 1), (27, 1), (29, 2), (32, 2), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 167.9929 - loglik: -1.5230e+02 - logprior: -1.5697e+01
Epoch 2/2
11/11 - 1s - loss: 148.4299 - loglik: -1.4393e+02 - logprior: -4.5037e+00
Fitted a model with MAP estimate = -145.7569
expansions: []
discards: [ 0 37 42]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 159.7079 - loglik: -1.4555e+02 - logprior: -1.4158e+01
Epoch 2/2
11/11 - 1s - loss: 149.5155 - loglik: -1.4399e+02 - logprior: -5.5269e+00
Fitted a model with MAP estimate = -147.4421
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 155.3462 - loglik: -1.4303e+02 - logprior: -1.2319e+01
Epoch 2/10
11/11 - 1s - loss: 146.4069 - loglik: -1.4322e+02 - logprior: -3.1829e+00
Epoch 3/10
11/11 - 1s - loss: 144.3172 - loglik: -1.4244e+02 - logprior: -1.8727e+00
Epoch 4/10
11/11 - 1s - loss: 143.7952 - loglik: -1.4265e+02 - logprior: -1.1410e+00
Epoch 5/10
11/11 - 1s - loss: 143.2488 - loglik: -1.4241e+02 - logprior: -8.3713e-01
Epoch 6/10
11/11 - 1s - loss: 143.2821 - loglik: -1.4256e+02 - logprior: -7.2410e-01
Fitted a model with MAP estimate = -143.2162
Time for alignment: 33.4318
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 241.0264 - loglik: -2.2848e+02 - logprior: -1.2543e+01
Epoch 2/10
11/11 - 1s - loss: 208.2524 - loglik: -2.0501e+02 - logprior: -3.2472e+00
Epoch 3/10
11/11 - 1s - loss: 183.2710 - loglik: -1.8122e+02 - logprior: -2.0518e+00
Epoch 4/10
11/11 - 1s - loss: 166.6115 - loglik: -1.6481e+02 - logprior: -1.8065e+00
Epoch 5/10
11/11 - 1s - loss: 161.2459 - loglik: -1.5970e+02 - logprior: -1.5469e+00
Epoch 6/10
11/11 - 1s - loss: 158.9939 - loglik: -1.5756e+02 - logprior: -1.4322e+00
Epoch 7/10
11/11 - 1s - loss: 157.6334 - loglik: -1.5644e+02 - logprior: -1.1925e+00
Epoch 8/10
11/11 - 1s - loss: 157.6896 - loglik: -1.5664e+02 - logprior: -1.0529e+00
Fitted a model with MAP estimate = -157.2216
expansions: [(0, 6), (22, 1), (27, 1), (29, 2), (32, 2), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 168.4370 - loglik: -1.5286e+02 - logprior: -1.5580e+01
Epoch 2/2
11/11 - 1s - loss: 148.7020 - loglik: -1.4426e+02 - logprior: -4.4445e+00
Fitted a model with MAP estimate = -145.8306
expansions: []
discards: [ 0 37 42]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 159.5090 - loglik: -1.4535e+02 - logprior: -1.4161e+01
Epoch 2/2
11/11 - 1s - loss: 149.8217 - loglik: -1.4428e+02 - logprior: -5.5423e+00
Fitted a model with MAP estimate = -147.4853
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 155.2479 - loglik: -1.4291e+02 - logprior: -1.2336e+01
Epoch 2/10
11/11 - 1s - loss: 146.2708 - loglik: -1.4308e+02 - logprior: -3.1907e+00
Epoch 3/10
11/11 - 1s - loss: 144.4605 - loglik: -1.4258e+02 - logprior: -1.8826e+00
Epoch 4/10
11/11 - 1s - loss: 143.7933 - loglik: -1.4266e+02 - logprior: -1.1365e+00
Epoch 5/10
11/11 - 1s - loss: 143.3432 - loglik: -1.4250e+02 - logprior: -8.4702e-01
Epoch 6/10
11/11 - 1s - loss: 143.2428 - loglik: -1.4252e+02 - logprior: -7.2092e-01
Epoch 7/10
11/11 - 1s - loss: 143.4081 - loglik: -1.4280e+02 - logprior: -6.0660e-01
Fitted a model with MAP estimate = -143.1657
Time for alignment: 31.5951
Computed alignments with likelihoods: ['-143.3424', '-143.2162', '-143.1657']
Best model has likelihood: -143.1657
SP score = 0.9930
Training of 3 independent models on file tim.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fbbad4a06a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fba112550d0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 192 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 573.4481 - loglik: -5.6444e+02 - logprior: -9.0099e+00
Epoch 2/10
12/12 - 5s - loss: 478.6583 - loglik: -4.7696e+02 - logprior: -1.6933e+00
Epoch 3/10
12/12 - 5s - loss: 395.6979 - loglik: -3.9368e+02 - logprior: -2.0184e+00
Epoch 4/10
12/12 - 5s - loss: 357.3274 - loglik: -3.5462e+02 - logprior: -2.7122e+00
Epoch 5/10
12/12 - 5s - loss: 349.2049 - loglik: -3.4636e+02 - logprior: -2.8473e+00
Epoch 6/10
12/12 - 5s - loss: 346.1160 - loglik: -3.4342e+02 - logprior: -2.6912e+00
Epoch 7/10
12/12 - 5s - loss: 343.7782 - loglik: -3.4115e+02 - logprior: -2.6247e+00
Epoch 8/10
12/12 - 5s - loss: 343.1151 - loglik: -3.4046e+02 - logprior: -2.6508e+00
Epoch 9/10
12/12 - 5s - loss: 342.5330 - loglik: -3.3989e+02 - logprior: -2.6404e+00
Epoch 10/10
12/12 - 5s - loss: 341.5224 - loglik: -3.3889e+02 - logprior: -2.6370e+00
Fitted a model with MAP estimate = -341.6959
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 2), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (62, 1), (66, 1), (76, 1), (78, 1), (85, 1), (88, 1), (90, 1), (93, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 3), (155, 1), (156, 1), (158, 1), (172, 1), (174, 3), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 240 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 342.3838 - loglik: -3.3160e+02 - logprior: -1.0788e+01
Epoch 2/2
12/12 - 7s - loss: 319.1284 - loglik: -3.1507e+02 - logprior: -4.0579e+00
Fitted a model with MAP estimate = -315.0204
expansions: [(0, 2), (191, 1), (215, 1)]
discards: [ 0 12]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 12s - loss: 318.6536 - loglik: -3.1054e+02 - logprior: -8.1134e+00
Epoch 2/2
12/12 - 7s - loss: 307.1164 - loglik: -3.0590e+02 - logprior: -1.2190e+00
Fitted a model with MAP estimate = -306.7140
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 313.2068 - loglik: -3.0530e+02 - logprior: -7.9078e+00
Epoch 2/10
12/12 - 7s - loss: 308.3792 - loglik: -3.0736e+02 - logprior: -1.0222e+00
Epoch 3/10
12/12 - 7s - loss: 304.5483 - loglik: -3.0445e+02 - logprior: -1.0240e-01
Epoch 4/10
12/12 - 7s - loss: 306.6904 - loglik: -3.0677e+02 - logprior: 0.0773
Fitted a model with MAP estimate = -304.9273
Time for alignment: 154.3761
Fitting a model of length 192 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 572.8491 - loglik: -5.6383e+02 - logprior: -9.0156e+00
Epoch 2/10
12/12 - 5s - loss: 479.2780 - loglik: -4.7759e+02 - logprior: -1.6854e+00
Epoch 3/10
12/12 - 5s - loss: 395.7061 - loglik: -3.9374e+02 - logprior: -1.9618e+00
Epoch 4/10
12/12 - 5s - loss: 358.8093 - loglik: -3.5625e+02 - logprior: -2.5572e+00
Epoch 5/10
12/12 - 5s - loss: 349.8966 - loglik: -3.4730e+02 - logprior: -2.5965e+00
Epoch 6/10
12/12 - 5s - loss: 346.4771 - loglik: -3.4406e+02 - logprior: -2.4137e+00
Epoch 7/10
12/12 - 5s - loss: 344.7327 - loglik: -3.4238e+02 - logprior: -2.3562e+00
Epoch 8/10
12/12 - 5s - loss: 343.1044 - loglik: -3.4071e+02 - logprior: -2.3969e+00
Epoch 9/10
12/12 - 5s - loss: 343.1394 - loglik: -3.4072e+02 - logprior: -2.4146e+00
Fitted a model with MAP estimate = -342.9734
expansions: [(11, 1), (12, 3), (16, 1), (17, 1), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (49, 1), (50, 1), (60, 1), (64, 1), (66, 1), (76, 1), (83, 1), (85, 1), (88, 1), (91, 1), (93, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (155, 1), (172, 1), (173, 3), (174, 1), (175, 1), (185, 1), (186, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 11s - loss: 343.9465 - loglik: -3.3312e+02 - logprior: -1.0825e+01
Epoch 2/2
12/12 - 7s - loss: 319.4278 - loglik: -3.1534e+02 - logprior: -4.0892e+00
Fitted a model with MAP estimate = -315.6692
expansions: [(0, 2), (12, 1), (13, 1), (187, 1), (189, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 241 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 316.9964 - loglik: -3.0887e+02 - logprior: -8.1286e+00
Epoch 2/2
12/12 - 7s - loss: 308.5622 - loglik: -3.0730e+02 - logprior: -1.2582e+00
Fitted a model with MAP estimate = -306.9586
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 241 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 313.7669 - loglik: -3.0582e+02 - logprior: -7.9454e+00
Epoch 2/10
12/12 - 7s - loss: 306.9394 - loglik: -3.0588e+02 - logprior: -1.0585e+00
Epoch 3/10
12/12 - 7s - loss: 307.0565 - loglik: -3.0690e+02 - logprior: -1.5655e-01
Fitted a model with MAP estimate = -305.5325
Time for alignment: 140.4421
Fitting a model of length 192 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 572.9584 - loglik: -5.6397e+02 - logprior: -8.9855e+00
Epoch 2/10
12/12 - 5s - loss: 478.3425 - loglik: -4.7671e+02 - logprior: -1.6327e+00
Epoch 3/10
12/12 - 5s - loss: 393.8290 - loglik: -3.9191e+02 - logprior: -1.9239e+00
Epoch 4/10
12/12 - 5s - loss: 359.3955 - loglik: -3.5686e+02 - logprior: -2.5375e+00
Epoch 5/10
12/12 - 5s - loss: 349.9731 - loglik: -3.4728e+02 - logprior: -2.6937e+00
Epoch 6/10
12/12 - 5s - loss: 345.8946 - loglik: -3.4334e+02 - logprior: -2.5530e+00
Epoch 7/10
12/12 - 5s - loss: 344.9052 - loglik: -3.4244e+02 - logprior: -2.4674e+00
Epoch 8/10
12/12 - 5s - loss: 343.6349 - loglik: -3.4115e+02 - logprior: -2.4826e+00
Epoch 9/10
12/12 - 5s - loss: 344.2400 - loglik: -3.4177e+02 - logprior: -2.4704e+00
Fitted a model with MAP estimate = -343.1995
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 1), (34, 2), (35, 1), (36, 1), (37, 1), (39, 2), (46, 1), (47, 1), (48, 1), (61, 1), (65, 1), (75, 1), (82, 1), (86, 1), (87, 1), (90, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 3), (122, 1), (149, 1), (152, 2), (153, 1), (154, 3), (155, 1), (157, 1), (171, 1), (172, 1), (173, 2), (174, 1), (187, 1), (189, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 343.5738 - loglik: -3.3278e+02 - logprior: -1.0797e+01
Epoch 2/2
12/12 - 7s - loss: 319.4912 - loglik: -3.1543e+02 - logprior: -4.0642e+00
Fitted a model with MAP estimate = -314.7768
expansions: [(0, 2)]
discards: [ 0 12 52]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 241 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 316.9691 - loglik: -3.0883e+02 - logprior: -8.1424e+00
Epoch 2/2
12/12 - 7s - loss: 307.7794 - loglik: -3.0650e+02 - logprior: -1.2840e+00
Fitted a model with MAP estimate = -306.7229
expansions: [(192, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 312.8924 - loglik: -3.0495e+02 - logprior: -7.9384e+00
Epoch 2/10
12/12 - 7s - loss: 307.5710 - loglik: -3.0653e+02 - logprior: -1.0435e+00
Epoch 3/10
12/12 - 7s - loss: 305.2233 - loglik: -3.0508e+02 - logprior: -1.4180e-01
Epoch 4/10
12/12 - 7s - loss: 304.3025 - loglik: -3.0437e+02 - logprior: 0.0704
Epoch 5/10
12/12 - 7s - loss: 304.9544 - loglik: -3.0530e+02 - logprior: 0.3462
Fitted a model with MAP estimate = -304.5813
Time for alignment: 157.8364
Computed alignments with likelihoods: ['-304.9273', '-305.5325', '-304.5813']
Best model has likelihood: -304.5813
SP score = 0.9682
Training of 3 independent models on file tgfb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cc2d7f70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9cd10ae50>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 303.8156 - loglik: -2.7675e+02 - logprior: -2.7068e+01
Epoch 2/10
10/10 - 1s - loss: 246.7464 - loglik: -2.3987e+02 - logprior: -6.8801e+00
Epoch 3/10
10/10 - 1s - loss: 208.5926 - loglik: -2.0501e+02 - logprior: -3.5855e+00
Epoch 4/10
10/10 - 1s - loss: 189.2092 - loglik: -1.8666e+02 - logprior: -2.5517e+00
Epoch 5/10
10/10 - 1s - loss: 180.8418 - loglik: -1.7861e+02 - logprior: -2.2338e+00
Epoch 6/10
10/10 - 1s - loss: 177.5512 - loglik: -1.7550e+02 - logprior: -2.0505e+00
Epoch 7/10
10/10 - 1s - loss: 176.3724 - loglik: -1.7459e+02 - logprior: -1.7818e+00
Epoch 8/10
10/10 - 1s - loss: 176.2235 - loglik: -1.7462e+02 - logprior: -1.6063e+00
Epoch 9/10
10/10 - 1s - loss: 175.6267 - loglik: -1.7409e+02 - logprior: -1.5398e+00
Epoch 10/10
10/10 - 1s - loss: 175.0467 - loglik: -1.7354e+02 - logprior: -1.5095e+00
Fitted a model with MAP estimate = -175.3000
expansions: [(0, 3), (11, 1), (12, 1), (34, 1), (36, 4), (48, 3), (49, 2), (60, 1), (69, 1), (70, 1), (72, 2), (73, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 200.8091 - loglik: -1.6664e+02 - logprior: -3.4167e+01
Epoch 2/2
10/10 - 1s - loss: 167.6682 - loglik: -1.5758e+02 - logprior: -1.0093e+01
Fitted a model with MAP estimate = -161.5559
expansions: [(47, 1)]
discards: [ 0 91]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 186.7345 - loglik: -1.5594e+02 - logprior: -3.0794e+01
Epoch 2/2
10/10 - 1s - loss: 167.1116 - loglik: -1.5499e+02 - logprior: -1.2125e+01
Fitted a model with MAP estimate = -163.3412
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 182.4285 - loglik: -1.5350e+02 - logprior: -2.8933e+01
Epoch 2/10
10/10 - 1s - loss: 160.5423 - loglik: -1.5240e+02 - logprior: -8.1468e+00
Epoch 3/10
10/10 - 1s - loss: 155.2340 - loglik: -1.5235e+02 - logprior: -2.8842e+00
Epoch 4/10
10/10 - 1s - loss: 153.3821 - loglik: -1.5225e+02 - logprior: -1.1359e+00
Epoch 5/10
10/10 - 1s - loss: 152.6838 - loglik: -1.5231e+02 - logprior: -3.7220e-01
Epoch 6/10
10/10 - 1s - loss: 152.2010 - loglik: -1.5219e+02 - logprior: -1.5193e-02
Epoch 7/10
10/10 - 1s - loss: 152.1462 - loglik: -1.5234e+02 - logprior: 0.1891
Epoch 8/10
10/10 - 1s - loss: 151.9567 - loglik: -1.5233e+02 - logprior: 0.3728
Epoch 9/10
10/10 - 1s - loss: 151.9288 - loglik: -1.5248e+02 - logprior: 0.5501
Epoch 10/10
10/10 - 1s - loss: 151.4095 - loglik: -1.5210e+02 - logprior: 0.6867
Fitted a model with MAP estimate = -151.5513
Time for alignment: 39.1811
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 303.5200 - loglik: -2.7645e+02 - logprior: -2.7067e+01
Epoch 2/10
10/10 - 1s - loss: 246.8615 - loglik: -2.3999e+02 - logprior: -6.8765e+00
Epoch 3/10
10/10 - 1s - loss: 210.6666 - loglik: -2.0708e+02 - logprior: -3.5860e+00
Epoch 4/10
10/10 - 1s - loss: 192.9125 - loglik: -1.9038e+02 - logprior: -2.5281e+00
Epoch 5/10
10/10 - 1s - loss: 184.2149 - loglik: -1.8203e+02 - logprior: -2.1801e+00
Epoch 6/10
10/10 - 1s - loss: 179.6826 - loglik: -1.7768e+02 - logprior: -2.0043e+00
Epoch 7/10
10/10 - 1s - loss: 177.3268 - loglik: -1.7557e+02 - logprior: -1.7520e+00
Epoch 8/10
10/10 - 1s - loss: 177.0094 - loglik: -1.7541e+02 - logprior: -1.5970e+00
Epoch 9/10
10/10 - 1s - loss: 176.3483 - loglik: -1.7480e+02 - logprior: -1.5498e+00
Epoch 10/10
10/10 - 1s - loss: 175.7734 - loglik: -1.7425e+02 - logprior: -1.5190e+00
Fitted a model with MAP estimate = -175.8610
expansions: [(0, 3), (11, 1), (12, 1), (34, 1), (35, 3), (36, 2), (50, 2), (60, 1), (69, 1), (70, 1), (72, 2), (73, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.0120 - loglik: -1.6880e+02 - logprior: -3.4216e+01
Epoch 2/2
10/10 - 1s - loss: 169.7216 - loglik: -1.5961e+02 - logprior: -1.0107e+01
Fitted a model with MAP estimate = -163.8178
expansions: [(91, 1)]
discards: [ 0 88]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 188.9873 - loglik: -1.5808e+02 - logprior: -3.0908e+01
Epoch 2/2
10/10 - 1s - loss: 168.6035 - loglik: -1.5643e+02 - logprior: -1.2174e+01
Fitted a model with MAP estimate = -165.0774
expansions: [(3, 1), (61, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 183.8438 - loglik: -1.5489e+02 - logprior: -2.8956e+01
Epoch 2/10
10/10 - 1s - loss: 161.6424 - loglik: -1.5344e+02 - logprior: -8.2039e+00
Epoch 3/10
10/10 - 1s - loss: 155.2597 - loglik: -1.5231e+02 - logprior: -2.9476e+00
Epoch 4/10
10/10 - 1s - loss: 154.0523 - loglik: -1.5287e+02 - logprior: -1.1778e+00
Epoch 5/10
10/10 - 1s - loss: 153.7455 - loglik: -1.5334e+02 - logprior: -4.0892e-01
Epoch 6/10
10/10 - 1s - loss: 152.8188 - loglik: -1.5277e+02 - logprior: -5.1435e-02
Epoch 7/10
10/10 - 1s - loss: 152.5550 - loglik: -1.5270e+02 - logprior: 0.1471
Epoch 8/10
10/10 - 1s - loss: 152.8596 - loglik: -1.5319e+02 - logprior: 0.3256
Fitted a model with MAP estimate = -152.3874
Time for alignment: 36.8301
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 303.8390 - loglik: -2.7677e+02 - logprior: -2.7069e+01
Epoch 2/10
10/10 - 1s - loss: 246.6833 - loglik: -2.3981e+02 - logprior: -6.8769e+00
Epoch 3/10
10/10 - 1s - loss: 206.9467 - loglik: -2.0342e+02 - logprior: -3.5235e+00
Epoch 4/10
10/10 - 1s - loss: 186.3306 - loglik: -1.8380e+02 - logprior: -2.5322e+00
Epoch 5/10
10/10 - 1s - loss: 179.0293 - loglik: -1.7681e+02 - logprior: -2.2242e+00
Epoch 6/10
10/10 - 1s - loss: 176.7830 - loglik: -1.7472e+02 - logprior: -2.0602e+00
Epoch 7/10
10/10 - 1s - loss: 175.3701 - loglik: -1.7358e+02 - logprior: -1.7865e+00
Epoch 8/10
10/10 - 1s - loss: 175.6615 - loglik: -1.7408e+02 - logprior: -1.5826e+00
Fitted a model with MAP estimate = -174.9014
expansions: [(0, 3), (11, 1), (12, 1), (33, 1), (36, 4), (49, 3), (50, 2), (51, 1), (69, 1), (70, 1), (72, 2), (73, 1), (74, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 200.1580 - loglik: -1.6648e+02 - logprior: -3.3676e+01
Epoch 2/2
10/10 - 1s - loss: 167.5426 - loglik: -1.5765e+02 - logprior: -9.8970e+00
Fitted a model with MAP estimate = -160.9899
expansions: []
discards: [ 0 90]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 186.4338 - loglik: -1.5559e+02 - logprior: -3.0840e+01
Epoch 2/2
10/10 - 1s - loss: 166.5187 - loglik: -1.5434e+02 - logprior: -1.2176e+01
Fitted a model with MAP estimate = -162.9960
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 182.4172 - loglik: -1.5345e+02 - logprior: -2.8966e+01
Epoch 2/10
10/10 - 1s - loss: 160.2204 - loglik: -1.5204e+02 - logprior: -8.1789e+00
Epoch 3/10
10/10 - 1s - loss: 155.5662 - loglik: -1.5265e+02 - logprior: -2.9187e+00
Epoch 4/10
10/10 - 1s - loss: 153.7061 - loglik: -1.5251e+02 - logprior: -1.1964e+00
Epoch 5/10
10/10 - 1s - loss: 152.9709 - loglik: -1.5254e+02 - logprior: -4.3000e-01
Epoch 6/10
10/10 - 1s - loss: 152.6231 - loglik: -1.5255e+02 - logprior: -6.9462e-02
Epoch 7/10
10/10 - 1s - loss: 152.2998 - loglik: -1.5243e+02 - logprior: 0.1280
Epoch 8/10
10/10 - 1s - loss: 152.1014 - loglik: -1.5242e+02 - logprior: 0.3199
Epoch 9/10
10/10 - 1s - loss: 152.4496 - loglik: -1.5294e+02 - logprior: 0.4913
Fitted a model with MAP estimate = -152.0533
Time for alignment: 35.3708
Computed alignments with likelihoods: ['-151.5513', '-152.3874', '-152.0533']
Best model has likelihood: -151.5513
SP score = 0.8169
Training of 3 independent models on file HLH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fba10a8b8b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9ccda7790>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 151.0231 - loglik: -1.4593e+02 - logprior: -5.0928e+00
Epoch 2/10
16/16 - 1s - loss: 126.4421 - loglik: -1.2487e+02 - logprior: -1.5674e+00
Epoch 3/10
16/16 - 1s - loss: 117.7172 - loglik: -1.1606e+02 - logprior: -1.6605e+00
Epoch 4/10
16/16 - 1s - loss: 113.8511 - loglik: -1.1219e+02 - logprior: -1.6655e+00
Epoch 5/10
16/16 - 1s - loss: 112.8239 - loglik: -1.1121e+02 - logprior: -1.6155e+00
Epoch 6/10
16/16 - 1s - loss: 111.7976 - loglik: -1.1022e+02 - logprior: -1.5797e+00
Epoch 7/10
16/16 - 1s - loss: 110.7517 - loglik: -1.0921e+02 - logprior: -1.5404e+00
Epoch 8/10
16/16 - 1s - loss: 109.3015 - loglik: -1.0778e+02 - logprior: -1.5189e+00
Epoch 9/10
16/16 - 1s - loss: 109.2853 - loglik: -1.0779e+02 - logprior: -1.4995e+00
Epoch 10/10
16/16 - 1s - loss: 109.0108 - loglik: -1.0752e+02 - logprior: -1.4916e+00
Fitted a model with MAP estimate = -108.8846
expansions: [(3, 1), (6, 1), (13, 1), (15, 2), (16, 1), (23, 7)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 111.8783 - loglik: -1.0563e+02 - logprior: -6.2472e+00
Epoch 2/2
16/16 - 1s - loss: 104.2513 - loglik: -1.0118e+02 - logprior: -3.0700e+00
Fitted a model with MAP estimate = -103.5620
expansions: [(0, 1)]
discards: [ 0 30]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 105.1957 - loglik: -1.0059e+02 - logprior: -4.6085e+00
Epoch 2/2
16/16 - 1s - loss: 101.8618 - loglik: -1.0022e+02 - logprior: -1.6410e+00
Fitted a model with MAP estimate = -101.4727
expansions: [(3, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 104.5421 - loglik: -9.9937e+01 - logprior: -4.6049e+00
Epoch 2/10
16/16 - 1s - loss: 101.4881 - loglik: -9.9913e+01 - logprior: -1.5752e+00
Epoch 3/10
16/16 - 1s - loss: 100.6088 - loglik: -9.9308e+01 - logprior: -1.3004e+00
Epoch 4/10
16/16 - 1s - loss: 100.8787 - loglik: -9.9655e+01 - logprior: -1.2237e+00
Fitted a model with MAP estimate = -100.6828
Time for alignment: 43.8777
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 151.0721 - loglik: -1.4598e+02 - logprior: -5.0942e+00
Epoch 2/10
16/16 - 1s - loss: 128.8605 - loglik: -1.2729e+02 - logprior: -1.5709e+00
Epoch 3/10
16/16 - 1s - loss: 118.3968 - loglik: -1.1673e+02 - logprior: -1.6637e+00
Epoch 4/10
16/16 - 1s - loss: 112.3633 - loglik: -1.1069e+02 - logprior: -1.6743e+00
Epoch 5/10
16/16 - 1s - loss: 109.5475 - loglik: -1.0791e+02 - logprior: -1.6405e+00
Epoch 6/10
16/16 - 1s - loss: 109.0928 - loglik: -1.0748e+02 - logprior: -1.6082e+00
Epoch 7/10
16/16 - 1s - loss: 108.2972 - loglik: -1.0673e+02 - logprior: -1.5721e+00
Epoch 8/10
16/16 - 1s - loss: 108.5611 - loglik: -1.0701e+02 - logprior: -1.5541e+00
Fitted a model with MAP estimate = -108.3854
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (18, 1), (24, 8), (33, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 112.4644 - loglik: -1.0622e+02 - logprior: -6.2435e+00
Epoch 2/2
16/16 - 1s - loss: 104.7892 - loglik: -1.0172e+02 - logprior: -3.0722e+00
Fitted a model with MAP estimate = -103.5751
expansions: [(0, 1)]
discards: [ 0 31 32]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 105.1714 - loglik: -1.0055e+02 - logprior: -4.6177e+00
Epoch 2/2
16/16 - 1s - loss: 102.2821 - loglik: -1.0064e+02 - logprior: -1.6409e+00
Fitted a model with MAP estimate = -101.5426
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 107.8265 - loglik: -1.0162e+02 - logprior: -6.2052e+00
Epoch 2/10
16/16 - 1s - loss: 103.4560 - loglik: -1.0066e+02 - logprior: -2.7983e+00
Epoch 3/10
16/16 - 1s - loss: 102.1434 - loglik: -1.0056e+02 - logprior: -1.5839e+00
Epoch 4/10
16/16 - 1s - loss: 101.8849 - loglik: -1.0057e+02 - logprior: -1.3193e+00
Epoch 5/10
16/16 - 1s - loss: 101.6102 - loglik: -1.0033e+02 - logprior: -1.2845e+00
Epoch 6/10
16/16 - 1s - loss: 101.3747 - loglik: -1.0010e+02 - logprior: -1.2778e+00
Epoch 7/10
16/16 - 1s - loss: 100.7846 - loglik: -9.9538e+01 - logprior: -1.2466e+00
Epoch 8/10
16/16 - 1s - loss: 101.3288 - loglik: -1.0011e+02 - logprior: -1.2184e+00
Fitted a model with MAP estimate = -101.0583
Time for alignment: 45.5753
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 151.1021 - loglik: -1.4600e+02 - logprior: -5.0977e+00
Epoch 2/10
16/16 - 1s - loss: 127.0146 - loglik: -1.2542e+02 - logprior: -1.5945e+00
Epoch 3/10
16/16 - 1s - loss: 115.6668 - loglik: -1.1395e+02 - logprior: -1.7142e+00
Epoch 4/10
16/16 - 1s - loss: 111.7373 - loglik: -1.0999e+02 - logprior: -1.7483e+00
Epoch 5/10
16/16 - 1s - loss: 109.9572 - loglik: -1.0827e+02 - logprior: -1.6892e+00
Epoch 6/10
16/16 - 1s - loss: 109.1657 - loglik: -1.0753e+02 - logprior: -1.6367e+00
Epoch 7/10
16/16 - 1s - loss: 109.6341 - loglik: -1.0804e+02 - logprior: -1.5930e+00
Fitted a model with MAP estimate = -109.2583
expansions: [(3, 1), (6, 1), (13, 1), (15, 2), (17, 1), (23, 3), (25, 2), (26, 2), (29, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 112.5942 - loglik: -1.0634e+02 - logprior: -6.2566e+00
Epoch 2/2
16/16 - 1s - loss: 104.9456 - loglik: -1.0188e+02 - logprior: -3.0650e+00
Fitted a model with MAP estimate = -103.5710
expansions: [(0, 1)]
discards: [ 0 33 36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 105.4706 - loglik: -1.0085e+02 - logprior: -4.6246e+00
Epoch 2/2
16/16 - 1s - loss: 101.6759 - loglik: -1.0003e+02 - logprior: -1.6421e+00
Fitted a model with MAP estimate = -101.5135
expansions: [(3, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 104.5054 - loglik: -9.9891e+01 - logprior: -4.6142e+00
Epoch 2/10
16/16 - 1s - loss: 101.1345 - loglik: -9.9545e+01 - logprior: -1.5890e+00
Epoch 3/10
16/16 - 1s - loss: 101.0539 - loglik: -9.9743e+01 - logprior: -1.3106e+00
Epoch 4/10
16/16 - 1s - loss: 100.7706 - loglik: -9.9534e+01 - logprior: -1.2369e+00
Epoch 5/10
16/16 - 1s - loss: 100.8237 - loglik: -9.9624e+01 - logprior: -1.1996e+00
Fitted a model with MAP estimate = -100.6584
Time for alignment: 40.5076
Computed alignments with likelihoods: ['-100.6828', '-101.0583', '-100.6584']
Best model has likelihood: -100.6584
SP score = 0.9257
Training of 3 independent models on file blmb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb1f5e7ccd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb1f5b5fe20>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 155 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 608.9654 - loglik: -6.0621e+02 - logprior: -2.7541e+00
Epoch 2/10
19/19 - 8s - loss: 567.7172 - loglik: -5.6705e+02 - logprior: -6.6979e-01
Epoch 3/10
19/19 - 8s - loss: 543.9899 - loglik: -5.4299e+02 - logprior: -9.9632e-01
Epoch 4/10
19/19 - 8s - loss: 538.0021 - loglik: -5.3700e+02 - logprior: -1.0002e+00
Epoch 5/10
19/19 - 8s - loss: 534.8664 - loglik: -5.3389e+02 - logprior: -9.8067e-01
Epoch 6/10
19/19 - 8s - loss: 533.2475 - loglik: -5.3228e+02 - logprior: -9.6586e-01
Epoch 7/10
19/19 - 8s - loss: 532.4958 - loglik: -5.3153e+02 - logprior: -9.6353e-01
Epoch 8/10
19/19 - 8s - loss: 531.5854 - loglik: -5.3062e+02 - logprior: -9.6751e-01
Epoch 9/10
19/19 - 8s - loss: 531.3413 - loglik: -5.3039e+02 - logprior: -9.5429e-01
Epoch 10/10
19/19 - 8s - loss: 531.6716 - loglik: -5.3071e+02 - logprior: -9.6515e-01
Fitted a model with MAP estimate = -496.6467
expansions: [(6, 1), (7, 1), (11, 1), (13, 1), (21, 2), (24, 3), (25, 1), (54, 1), (55, 1), (56, 2), (69, 1), (77, 5), (79, 1), (88, 2), (110, 5), (121, 1), (122, 2), (125, 4), (140, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 193 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 530.7150 - loglik: -5.2707e+02 - logprior: -3.6425e+00
Epoch 2/2
19/19 - 11s - loss: 521.9614 - loglik: -5.2015e+02 - logprior: -1.8131e+00
Fitted a model with MAP estimate = -492.7534
expansions: [(0, 1), (39, 2), (155, 1), (158, 1)]
discards: [  0 108 148]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 195 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 520.7802 - loglik: -5.1810e+02 - logprior: -2.6757e+00
Epoch 2/2
19/19 - 11s - loss: 517.7607 - loglik: -5.1675e+02 - logprior: -1.0082e+00
Fitted a model with MAP estimate = -492.6594
expansions: [(94, 1)]
discards: [68]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 195 on 17200 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 17s - loss: 483.9729 - loglik: -4.8245e+02 - logprior: -1.5180e+00
Epoch 2/10
25/25 - 14s - loss: 473.8264 - loglik: -4.7295e+02 - logprior: -8.7773e-01
Epoch 3/10
25/25 - 14s - loss: 468.4568 - loglik: -4.6767e+02 - logprior: -7.8995e-01
Epoch 4/10
25/25 - 14s - loss: 466.9188 - loglik: -4.6617e+02 - logprior: -7.4565e-01
Epoch 5/10
25/25 - 14s - loss: 465.3319 - loglik: -4.6462e+02 - logprior: -7.1131e-01
Epoch 6/10
25/25 - 14s - loss: 464.7706 - loglik: -4.6408e+02 - logprior: -6.8752e-01
Epoch 7/10
25/25 - 14s - loss: 462.7897 - loglik: -4.6212e+02 - logprior: -6.6866e-01
Epoch 8/10
25/25 - 14s - loss: 463.7216 - loglik: -4.6307e+02 - logprior: -6.4842e-01
Fitted a model with MAP estimate = -462.8370
Time for alignment: 312.4253
Fitting a model of length 155 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 608.6304 - loglik: -6.0586e+02 - logprior: -2.7686e+00
Epoch 2/10
19/19 - 8s - loss: 566.5281 - loglik: -5.6587e+02 - logprior: -6.5966e-01
Epoch 3/10
19/19 - 8s - loss: 542.3179 - loglik: -5.4136e+02 - logprior: -9.5912e-01
Epoch 4/10
19/19 - 8s - loss: 534.9292 - loglik: -5.3399e+02 - logprior: -9.4185e-01
Epoch 5/10
19/19 - 8s - loss: 531.3110 - loglik: -5.3037e+02 - logprior: -9.3761e-01
Epoch 6/10
19/19 - 8s - loss: 530.3799 - loglik: -5.2946e+02 - logprior: -9.2215e-01
Epoch 7/10
19/19 - 8s - loss: 528.9896 - loglik: -5.2806e+02 - logprior: -9.3243e-01
Epoch 8/10
19/19 - 8s - loss: 528.4214 - loglik: -5.2748e+02 - logprior: -9.3897e-01
Epoch 9/10
19/19 - 8s - loss: 527.5247 - loglik: -5.2658e+02 - logprior: -9.4664e-01
Epoch 10/10
19/19 - 8s - loss: 528.1378 - loglik: -5.2719e+02 - logprior: -9.4795e-01
Fitted a model with MAP estimate = -491.9721
expansions: [(6, 1), (7, 1), (11, 1), (13, 2), (20, 2), (24, 2), (25, 1), (26, 1), (55, 2), (69, 1), (70, 1), (81, 2), (87, 1), (92, 2), (111, 1), (122, 2), (124, 2), (125, 5), (126, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 528.8038 - loglik: -5.2515e+02 - logprior: -3.6504e+00
Epoch 2/2
19/19 - 10s - loss: 521.1354 - loglik: -5.1935e+02 - logprior: -1.7872e+00
Fitted a model with MAP estimate = -490.1917
expansions: [(0, 1), (36, 1), (92, 6), (152, 1)]
discards: [ 0 15 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 191 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 520.7855 - loglik: -5.1814e+02 - logprior: -2.6500e+00
Epoch 2/2
19/19 - 10s - loss: 517.1777 - loglik: -5.1617e+02 - logprior: -1.0060e+00
Fitted a model with MAP estimate = -490.3686
expansions: [(25, 1), (99, 1), (155, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 194 on 17200 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 17s - loss: 481.0384 - loglik: -4.7957e+02 - logprior: -1.4665e+00
Epoch 2/10
25/25 - 14s - loss: 471.0858 - loglik: -4.7033e+02 - logprior: -7.5571e-01
Epoch 3/10
25/25 - 13s - loss: 468.6826 - loglik: -4.6798e+02 - logprior: -7.0378e-01
Epoch 4/10
25/25 - 14s - loss: 465.7369 - loglik: -4.6506e+02 - logprior: -6.8158e-01
Epoch 5/10
25/25 - 14s - loss: 464.8717 - loglik: -4.6421e+02 - logprior: -6.6209e-01
Epoch 6/10
25/25 - 14s - loss: 463.1843 - loglik: -4.6254e+02 - logprior: -6.4387e-01
Epoch 7/10
25/25 - 14s - loss: 463.8244 - loglik: -4.6320e+02 - logprior: -6.2043e-01
Fitted a model with MAP estimate = -462.6701
Time for alignment: 291.7732
Fitting a model of length 155 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 609.0579 - loglik: -6.0629e+02 - logprior: -2.7656e+00
Epoch 2/10
19/19 - 8s - loss: 566.6057 - loglik: -5.6594e+02 - logprior: -6.6841e-01
Epoch 3/10
19/19 - 8s - loss: 540.8740 - loglik: -5.3985e+02 - logprior: -1.0275e+00
Epoch 4/10
19/19 - 8s - loss: 534.5587 - loglik: -5.3352e+02 - logprior: -1.0426e+00
Epoch 5/10
19/19 - 8s - loss: 531.7113 - loglik: -5.3065e+02 - logprior: -1.0566e+00
Epoch 6/10
19/19 - 8s - loss: 529.8086 - loglik: -5.2878e+02 - logprior: -1.0309e+00
Epoch 7/10
19/19 - 8s - loss: 529.2515 - loglik: -5.2823e+02 - logprior: -1.0207e+00
Epoch 8/10
19/19 - 8s - loss: 528.9774 - loglik: -5.2797e+02 - logprior: -1.0062e+00
Epoch 9/10
19/19 - 8s - loss: 528.1180 - loglik: -5.2712e+02 - logprior: -9.9963e-01
Epoch 10/10
19/19 - 8s - loss: 527.7659 - loglik: -5.2677e+02 - logprior: -9.9348e-01
Fitted a model with MAP estimate = -491.5370
expansions: [(6, 1), (7, 1), (11, 1), (13, 1), (20, 2), (24, 2), (25, 1), (54, 1), (55, 1), (70, 1), (72, 2), (73, 7), (76, 8), (81, 2), (105, 1), (122, 3), (123, 2), (124, 3), (125, 2), (138, 5)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 201 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 528.7104 - loglik: -5.2502e+02 - logprior: -3.6878e+00
Epoch 2/2
19/19 - 11s - loss: 520.4612 - loglik: -5.1864e+02 - logprior: -1.8252e+00
Fitted a model with MAP estimate = -488.3694
expansions: [(0, 1)]
discards: [  0  84  87  92 109 155 164 179]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 194 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 519.6835 - loglik: -5.1700e+02 - logprior: -2.6831e+00
Epoch 2/2
19/19 - 11s - loss: 516.8018 - loglik: -5.1577e+02 - logprior: -1.0309e+00
Fitted a model with MAP estimate = -489.2812
expansions: [(38, 2), (152, 1)]
discards: [ 85  86  87  88  89 153]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 191 on 17200 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 18s - loss: 480.3079 - loglik: -4.7881e+02 - logprior: -1.5020e+00
Epoch 2/10
25/25 - 13s - loss: 470.8291 - loglik: -4.7002e+02 - logprior: -8.0478e-01
Epoch 3/10
25/25 - 13s - loss: 468.2029 - loglik: -4.6746e+02 - logprior: -7.4240e-01
Epoch 4/10
25/25 - 14s - loss: 467.0589 - loglik: -4.6635e+02 - logprior: -7.0851e-01
Epoch 5/10
25/25 - 14s - loss: 465.5741 - loglik: -4.6490e+02 - logprior: -6.7427e-01
Epoch 6/10
25/25 - 14s - loss: 465.7368 - loglik: -4.6508e+02 - logprior: -6.5841e-01
Fitted a model with MAP estimate = -464.4698
Time for alignment: 285.1898
Computed alignments with likelihoods: ['-462.8370', '-462.6701', '-464.4698']
Best model has likelihood: -462.6701
SP score = 0.7040
Training of 3 independent models on file proteasome.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb8fc48c580>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb8fc48c280>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc48ceb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1b80>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b12b0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb8fc4b1bb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1a90>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b19d0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1790>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17f0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b17c0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1be0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1eb0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b1e80>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb8fc4b1580> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb8fc4b1670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb8fc4b15b0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb8fc4ae8b0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb9de2bd100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb7ad706c40>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fba7e0ae820> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fb95c070280>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb8fc4b1fa0> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7fbb939cf670>
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 492.1148 - loglik: -4.8617e+02 - logprior: -5.9444e+00
Epoch 2/10
14/14 - 4s - loss: 436.0412 - loglik: -4.3479e+02 - logprior: -1.2549e+00
Epoch 3/10
14/14 - 4s - loss: 401.7236 - loglik: -4.0044e+02 - logprior: -1.2798e+00
Epoch 4/10
14/14 - 4s - loss: 390.4760 - loglik: -3.8927e+02 - logprior: -1.2030e+00
Epoch 5/10
14/14 - 4s - loss: 384.8104 - loglik: -3.8363e+02 - logprior: -1.1773e+00
Epoch 6/10
14/14 - 4s - loss: 383.3843 - loglik: -3.8222e+02 - logprior: -1.1680e+00
Epoch 7/10
14/14 - 4s - loss: 382.4504 - loglik: -3.8131e+02 - logprior: -1.1417e+00
Epoch 8/10
14/14 - 4s - loss: 381.4339 - loglik: -3.8030e+02 - logprior: -1.1310e+00
Epoch 9/10
14/14 - 4s - loss: 380.9492 - loglik: -3.7981e+02 - logprior: -1.1397e+00
Epoch 10/10
14/14 - 4s - loss: 380.9470 - loglik: -3.7980e+02 - logprior: -1.1469e+00
Fitted a model with MAP estimate = -380.8639
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (57, 1), (65, 1), (70, 4), (74, 1), (81, 1), (103, 4), (117, 1), (120, 2), (128, 1), (131, 1), (133, 2), (134, 1), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 381.8659 - loglik: -3.7460e+02 - logprior: -7.2672e+00
Epoch 2/2
14/14 - 5s - loss: 364.5594 - loglik: -3.6278e+02 - logprior: -1.7826e+00
Fitted a model with MAP estimate = -362.2281
expansions: [(127, 1), (131, 1)]
discards: [ 41 149 166]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 366.6191 - loglik: -3.6121e+02 - logprior: -5.4136e+00
Epoch 2/2
14/14 - 5s - loss: 360.7074 - loglik: -3.5929e+02 - logprior: -1.4183e+00
Fitted a model with MAP estimate = -359.9524
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 10s - loss: 368.1389 - loglik: -3.6101e+02 - logprior: -7.1257e+00
Epoch 2/10
14/14 - 5s - loss: 363.9494 - loglik: -3.6109e+02 - logprior: -2.8555e+00
Epoch 3/10
14/14 - 5s - loss: 360.7473 - loglik: -3.5903e+02 - logprior: -1.7143e+00
Epoch 4/10
14/14 - 5s - loss: 360.3867 - loglik: -3.5979e+02 - logprior: -5.9284e-01
Epoch 5/10
14/14 - 5s - loss: 359.9864 - loglik: -3.5954e+02 - logprior: -4.4492e-01
Epoch 6/10
14/14 - 5s - loss: 359.6279 - loglik: -3.5921e+02 - logprior: -4.1889e-01
Epoch 7/10
14/14 - 5s - loss: 359.2371 - loglik: -3.5887e+02 - logprior: -3.6742e-01
Epoch 8/10
14/14 - 5s - loss: 359.9234 - loglik: -3.5963e+02 - logprior: -2.9463e-01
Fitted a model with MAP estimate = -359.3173
Time for alignment: 132.3871
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 491.2400 - loglik: -4.8530e+02 - logprior: -5.9393e+00
Epoch 2/10
14/14 - 4s - loss: 436.4199 - loglik: -4.3518e+02 - logprior: -1.2405e+00
Epoch 3/10
14/14 - 4s - loss: 401.5282 - loglik: -4.0027e+02 - logprior: -1.2616e+00
Epoch 4/10
14/14 - 4s - loss: 389.7933 - loglik: -3.8868e+02 - logprior: -1.1129e+00
Epoch 5/10
14/14 - 4s - loss: 385.0849 - loglik: -3.8401e+02 - logprior: -1.0730e+00
Epoch 6/10
14/14 - 4s - loss: 381.8081 - loglik: -3.8073e+02 - logprior: -1.0831e+00
Epoch 7/10
14/14 - 4s - loss: 381.9727 - loglik: -3.8090e+02 - logprior: -1.0687e+00
Fitted a model with MAP estimate = -381.0666
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (66, 1), (69, 1), (71, 4), (89, 1), (102, 4), (117, 1), (120, 2), (128, 1), (129, 2), (130, 2), (131, 1), (133, 2), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 379.5644 - loglik: -3.7240e+02 - logprior: -7.1676e+00
Epoch 2/2
14/14 - 5s - loss: 364.0708 - loglik: -3.6228e+02 - logprior: -1.7908e+00
Fitted a model with MAP estimate = -360.3280
expansions: [(127, 1), (128, 1), (130, 1)]
discards: [ 41  91  92 148 161]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 365.9642 - loglik: -3.6059e+02 - logprior: -5.3785e+00
Epoch 2/2
14/14 - 5s - loss: 359.9935 - loglik: -3.5857e+02 - logprior: -1.4268e+00
Fitted a model with MAP estimate = -358.8686
expansions: []
discards: [  0 167]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 366.9018 - loglik: -3.5975e+02 - logprior: -7.1503e+00
Epoch 2/10
14/14 - 5s - loss: 362.9012 - loglik: -3.6002e+02 - logprior: -2.8855e+00
Epoch 3/10
14/14 - 5s - loss: 359.7604 - loglik: -3.5796e+02 - logprior: -1.7960e+00
Epoch 4/10
14/14 - 5s - loss: 359.2628 - loglik: -3.5861e+02 - logprior: -6.5708e-01
Epoch 5/10
14/14 - 5s - loss: 358.2643 - loglik: -3.5778e+02 - logprior: -4.7955e-01
Epoch 6/10
14/14 - 5s - loss: 359.6236 - loglik: -3.5917e+02 - logprior: -4.5770e-01
Fitted a model with MAP estimate = -358.4599
Time for alignment: 112.4113
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 490.2574 - loglik: -4.8432e+02 - logprior: -5.9395e+00
Epoch 2/10
14/14 - 4s - loss: 437.0564 - loglik: -4.3582e+02 - logprior: -1.2361e+00
Epoch 3/10
14/14 - 4s - loss: 402.8186 - loglik: -4.0155e+02 - logprior: -1.2685e+00
Epoch 4/10
14/14 - 4s - loss: 391.2026 - loglik: -3.8998e+02 - logprior: -1.2262e+00
Epoch 5/10
14/14 - 4s - loss: 386.0922 - loglik: -3.8490e+02 - logprior: -1.1913e+00
Epoch 6/10
14/14 - 4s - loss: 384.6218 - loglik: -3.8345e+02 - logprior: -1.1709e+00
Epoch 7/10
14/14 - 4s - loss: 384.1853 - loglik: -3.8304e+02 - logprior: -1.1488e+00
Epoch 8/10
14/14 - 4s - loss: 382.4703 - loglik: -3.8133e+02 - logprior: -1.1428e+00
Epoch 9/10
14/14 - 4s - loss: 383.1395 - loglik: -3.8199e+02 - logprior: -1.1518e+00
Fitted a model with MAP estimate = -382.4134
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (58, 1), (61, 2), (69, 1), (71, 1), (74, 1), (82, 1), (102, 4), (120, 2), (124, 1), (128, 1), (129, 2), (130, 2), (131, 1), (133, 2), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 383.1809 - loglik: -3.7593e+02 - logprior: -7.2540e+00
Epoch 2/2
14/14 - 5s - loss: 365.6259 - loglik: -3.6385e+02 - logprior: -1.7759e+00
Fitted a model with MAP estimate = -363.2196
expansions: [(126, 1), (127, 1), (129, 1)]
discards: [ 41  77 147 160]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 367.5938 - loglik: -3.6229e+02 - logprior: -5.3010e+00
Epoch 2/2
14/14 - 5s - loss: 361.2088 - loglik: -3.5986e+02 - logprior: -1.3459e+00
Fitted a model with MAP estimate = -360.6003
expansions: []
discards: [  0 167]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 368.9579 - loglik: -3.6190e+02 - logprior: -7.0542e+00
Epoch 2/10
14/14 - 5s - loss: 363.9542 - loglik: -3.6115e+02 - logprior: -2.8061e+00
Epoch 3/10
14/14 - 5s - loss: 362.1631 - loglik: -3.6044e+02 - logprior: -1.7245e+00
Epoch 4/10
14/14 - 5s - loss: 361.0872 - loglik: -3.6051e+02 - logprior: -5.7843e-01
Epoch 5/10
14/14 - 5s - loss: 359.6996 - loglik: -3.5930e+02 - logprior: -4.0206e-01
Epoch 6/10
14/14 - 5s - loss: 361.4904 - loglik: -3.6112e+02 - logprior: -3.7127e-01
Fitted a model with MAP estimate = -360.1944
Time for alignment: 119.4461
Computed alignments with likelihoods: ['-359.3173', '-358.4599', '-360.1944']
Best model has likelihood: -358.4599
SP score = 0.8104
