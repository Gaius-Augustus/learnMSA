Training of 5 independent models on file PF00079.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff718e45040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff718e45310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff718e45f40>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 1632.2557 - loglik: -1.6307e+03 - logprior: -1.5570e+00
Epoch 2/10
39/39 - 32s - loss: 1507.8032 - loglik: -1.5059e+03 - logprior: -1.9083e+00
Epoch 3/10
39/39 - 32s - loss: 1497.3182 - loglik: -1.4955e+03 - logprior: -1.8617e+00
Epoch 4/10
39/39 - 33s - loss: 1494.0820 - loglik: -1.4923e+03 - logprior: -1.7589e+00
Epoch 5/10
39/39 - 34s - loss: 1491.1671 - loglik: -1.4894e+03 - logprior: -1.7766e+00
Epoch 6/10
39/39 - 35s - loss: 1487.9677 - loglik: -1.4861e+03 - logprior: -1.8136e+00
Epoch 7/10
39/39 - 36s - loss: 1479.9738 - loglik: -1.4781e+03 - logprior: -1.8544e+00
Epoch 8/10
39/39 - 37s - loss: 1468.7970 - loglik: -1.4668e+03 - logprior: -1.9338e+00
Epoch 9/10
39/39 - 37s - loss: 1443.7682 - loglik: -1.4416e+03 - logprior: -2.1254e+00
Epoch 10/10
39/39 - 38s - loss: 1306.9093 - loglik: -1.3010e+03 - logprior: -5.8114e+00
Fitted a model with MAP estimate = -1219.5774
expansions: []
discards: [  5   6   7   8  17  18  19  20  21  22  23  36  37  38  39  40  41  42
  82  83  84  85  93  94  95  96  97  98  99 100 101 102 103 104 105 106
 107 108 109 110 111 112 124 125 133 134 135 136 137 138 139 140 141 142
 143 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195
 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213
 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231
 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249
 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267
 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285
 286 287 288]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 1754.3781 - loglik: -1.7528e+03 - logprior: -1.5603e+00
Epoch 2/2
39/39 - 13s - loss: 1731.0909 - loglik: -1.7308e+03 - logprior: -3.0546e-01
Fitted a model with MAP estimate = -1718.4149
expansions: [(0, 41), (120, 6), (121, 164), (124, 66)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95
 96 97]
Re-initialized the encoder parameters.
Fitting a model of length 303 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 45s - loss: 1637.2369 - loglik: -1.6350e+03 - logprior: -2.2546e+00
Epoch 2/2
39/39 - 45s - loss: 1552.2137 - loglik: -1.5505e+03 - logprior: -1.6867e+00
Fitted a model with MAP estimate = -1546.2975
expansions: [(62, 84), (89, 1), (90, 1), (105, 1), (106, 1), (107, 1), (111, 1), (112, 1), (113, 3), (114, 1), (115, 1), (118, 1), (119, 2), (133, 1), (140, 1), (141, 1), (142, 1), (163, 1), (164, 1), (168, 1), (189, 1)]
discards: [  2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
  20  21 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271
 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289
 290 291 292 293 294 295 296 297 298 299 300 301 302]
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 59s - loss: 1513.4121 - loglik: -1.5120e+03 - logprior: -1.3865e+00
Epoch 2/10
39/39 - 57s - loss: 1480.5054 - loglik: -1.4800e+03 - logprior: -5.3959e-01
Epoch 3/10
39/39 - 57s - loss: 1475.9475 - loglik: -1.4755e+03 - logprior: -4.6450e-01
Epoch 4/10
39/39 - 56s - loss: 1473.1237 - loglik: -1.4727e+03 - logprior: -3.7185e-01
Epoch 5/10
39/39 - 56s - loss: 1469.2123 - loglik: -1.4689e+03 - logprior: -3.1751e-01
Epoch 6/10
39/39 - 56s - loss: 1467.2396 - loglik: -1.4669e+03 - logprior: -2.8423e-01
Epoch 7/10
39/39 - 56s - loss: 1457.8060 - loglik: -1.4575e+03 - logprior: -2.7033e-01
Epoch 8/10
39/39 - 54s - loss: 1444.3073 - loglik: -1.4439e+03 - logprior: -3.8478e-01
Epoch 9/10
39/39 - 53s - loss: 1378.5629 - loglik: -1.3775e+03 - logprior: -9.8420e-01
Epoch 10/10
39/39 - 52s - loss: 1229.5771 - loglik: -1.2261e+03 - logprior: -3.3833e+00
Fitted a model with MAP estimate = -1202.6430
Time for alignment: 1169.2217
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 1631.3041 - loglik: -1.6298e+03 - logprior: -1.4882e+00
Epoch 2/10
39/39 - 39s - loss: 1506.2100 - loglik: -1.5046e+03 - logprior: -1.6084e+00
Epoch 3/10
39/39 - 39s - loss: 1496.3676 - loglik: -1.4948e+03 - logprior: -1.5531e+00
Epoch 4/10
39/39 - 42s - loss: 1493.4449 - loglik: -1.4920e+03 - logprior: -1.4810e+00
Epoch 5/10
39/39 - 44s - loss: 1490.2306 - loglik: -1.4887e+03 - logprior: -1.4744e+00
Epoch 6/10
39/39 - 44s - loss: 1488.0793 - loglik: -1.4866e+03 - logprior: -1.4920e+00
Epoch 7/10
39/39 - 46s - loss: 1480.1439 - loglik: -1.4786e+03 - logprior: -1.5283e+00
Epoch 8/10
39/39 - 46s - loss: 1468.8866 - loglik: -1.4672e+03 - logprior: -1.6175e+00
Epoch 9/10
39/39 - 47s - loss: 1447.0446 - loglik: -1.4452e+03 - logprior: -1.8051e+00
Epoch 10/10
39/39 - 47s - loss: 1311.0779 - loglik: -1.3059e+03 - logprior: -5.0990e+00
Fitted a model with MAP estimate = -1220.5549
expansions: [(0, 2)]
discards: [ 16  17  18  19  20  21  22  35  36  37  38  39  40  49  50  65  66  67
  82  83  84  85  93  94  95 102 103 104 105 106 107 113 125 126 134 135
 136 137 138 139 140 141 142 143 144 164 165 166 167 168 169 170 171 172
 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190
 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208
 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226
 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244
 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262
 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280
 281 282 283 284 285 286 287 288]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 1751.9501 - loglik: -1.7488e+03 - logprior: -3.1957e+00
Epoch 2/2
39/39 - 14s - loss: 1731.2444 - loglik: -1.7309e+03 - logprior: -3.5082e-01
Fitted a model with MAP estimate = -1714.8321
expansions: [(0, 48), (121, 187)]
discards: [  2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37
  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55
  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73
  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91
  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109
 110 111 112 113 114 115 116 117 118 119 120]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 37s - loss: 1636.9038 - loglik: -1.6346e+03 - logprior: -2.3347e+00
Epoch 2/2
39/39 - 34s - loss: 1556.0072 - loglik: -1.5551e+03 - logprior: -9.3255e-01
Fitted a model with MAP estimate = -1551.6421
expansions: [(13, 1), (14, 1), (15, 1), (17, 1), (42, 2), (44, 2), (49, 79), (57, 1), (74, 1), (86, 1), (107, 1), (115, 1), (121, 1), (127, 1), (171, 1), (224, 1)]
discards: [  0 162]
Re-initialized the encoder parameters.
Fitting a model of length 331 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 62s - loss: 1513.0674 - loglik: -1.5109e+03 - logprior: -2.1964e+00
Epoch 2/10
39/39 - 63s - loss: 1485.0413 - loglik: -1.4847e+03 - logprior: -3.0948e-01
Epoch 3/10
39/39 - 61s - loss: 1481.4667 - loglik: -1.4813e+03 - logprior: -1.7445e-01
Epoch 4/10
39/39 - 62s - loss: 1478.8856 - loglik: -1.4787e+03 - logprior: -1.4390e-01
Epoch 5/10
39/39 - 66s - loss: 1477.5222 - loglik: -1.4774e+03 - logprior: -1.2393e-01
Epoch 6/10
39/39 - 66s - loss: 1471.9104 - loglik: -1.4718e+03 - logprior: -7.7724e-02
Epoch 7/10
39/39 - 60s - loss: 1467.0864 - loglik: -1.4670e+03 - logprior: -1.0231e-01
Epoch 8/10
39/39 - 58s - loss: 1451.6929 - loglik: -1.4514e+03 - logprior: -2.3516e-01
Epoch 9/10
39/39 - 50s - loss: 1386.9569 - loglik: -1.3857e+03 - logprior: -1.2594e+00
Epoch 10/10
39/39 - 47s - loss: 1232.6732 - loglik: -1.2286e+03 - logprior: -4.0310e+00
Fitted a model with MAP estimate = -1203.5390
Time for alignment: 1272.1512
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 40s - loss: 1630.8717 - loglik: -1.6293e+03 - logprior: -1.5396e+00
Epoch 2/10
39/39 - 37s - loss: 1508.2906 - loglik: -1.5065e+03 - logprior: -1.8123e+00
Epoch 3/10
39/39 - 37s - loss: 1497.6940 - loglik: -1.4960e+03 - logprior: -1.6927e+00
Epoch 4/10
39/39 - 39s - loss: 1495.1765 - loglik: -1.4936e+03 - logprior: -1.6217e+00
Epoch 5/10
39/39 - 41s - loss: 1492.5663 - loglik: -1.4910e+03 - logprior: -1.6064e+00
Epoch 6/10
39/39 - 44s - loss: 1489.5844 - loglik: -1.4880e+03 - logprior: -1.5860e+00
Epoch 7/10
39/39 - 45s - loss: 1483.4177 - loglik: -1.4818e+03 - logprior: -1.6155e+00
Epoch 8/10
39/39 - 45s - loss: 1472.2570 - loglik: -1.4705e+03 - logprior: -1.7047e+00
Epoch 9/10
39/39 - 46s - loss: 1449.4648 - loglik: -1.4475e+03 - logprior: -1.8815e+00
Epoch 10/10
39/39 - 48s - loss: 1336.2915 - loglik: -1.3293e+03 - logprior: -6.9787e+00
Fitted a model with MAP estimate = -1243.9848
expansions: []
discards: [ 15  16  17  18  19  20  21  22  35  36  37  38  39  40  50  51  52  63
  64  92  93  94  95  96  97 100 101 102 103 104 105 106 107 108 109 110
 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128
 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146
 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164
 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182
 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200
 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218
 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236
 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254
 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272
 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 1755.4103 - loglik: -1.7539e+03 - logprior: -1.5395e+00
Epoch 2/2
39/39 - 12s - loss: 1727.3167 - loglik: -1.7269e+03 - logprior: -4.3296e-01
Fitted a model with MAP estimate = -1716.2611
expansions: [(0, 41), (70, 1), (72, 1), (73, 195), (75, 40)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 51s - loss: 1617.6138 - loglik: -1.6154e+03 - logprior: -2.2576e+00
Epoch 2/2
39/39 - 50s - loss: 1535.4105 - loglik: -1.5340e+03 - logprior: -1.3801e+00
Fitted a model with MAP estimate = -1530.9806
expansions: [(0, 2), (123, 1), (226, 1), (247, 2), (257, 50), (258, 10), (270, 1), (271, 2), (272, 5), (273, 2), (274, 2), (275, 1), (286, 2), (288, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]
Re-initialized the encoder parameters.
Fitting a model of length 358 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 71s - loss: 1499.4606 - loglik: -1.4980e+03 - logprior: -1.4301e+00
Epoch 2/10
39/39 - 69s - loss: 1474.0410 - loglik: -1.4736e+03 - logprior: -4.2909e-01
Epoch 3/10
39/39 - 59s - loss: 1470.6758 - loglik: -1.4703e+03 - logprior: -3.7228e-01
Epoch 4/10
39/39 - 54s - loss: 1468.3396 - loglik: -1.4680e+03 - logprior: -3.1117e-01
Epoch 5/10
39/39 - 53s - loss: 1463.5747 - loglik: -1.4633e+03 - logprior: -2.6720e-01
Epoch 6/10
39/39 - 52s - loss: 1459.2455 - loglik: -1.4590e+03 - logprior: -2.6081e-01
Epoch 7/10
39/39 - 54s - loss: 1452.8324 - loglik: -1.4526e+03 - logprior: -2.4248e-01
Epoch 8/10
39/39 - 53s - loss: 1432.3584 - loglik: -1.4320e+03 - logprior: -3.3781e-01
Epoch 9/10
39/39 - 54s - loss: 1341.9409 - loglik: -1.3408e+03 - logprior: -1.0511e+00
Epoch 10/10
39/39 - 53s - loss: 1213.7871 - loglik: -1.2103e+03 - logprior: -3.4194e+00
Fitted a model with MAP estimate = -1199.7687
Time for alignment: 1279.7324
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 40s - loss: 1631.1875 - loglik: -1.6296e+03 - logprior: -1.5385e+00
Epoch 2/10
39/39 - 38s - loss: 1508.1921 - loglik: -1.5064e+03 - logprior: -1.8361e+00
Epoch 3/10
39/39 - 38s - loss: 1497.7028 - loglik: -1.4960e+03 - logprior: -1.7454e+00
Epoch 4/10
39/39 - 37s - loss: 1494.9446 - loglik: -1.4933e+03 - logprior: -1.6561e+00
Epoch 5/10
39/39 - 37s - loss: 1491.7205 - loglik: -1.4901e+03 - logprior: -1.6398e+00
Epoch 6/10
39/39 - 37s - loss: 1488.7937 - loglik: -1.4872e+03 - logprior: -1.6151e+00
Epoch 7/10
39/39 - 37s - loss: 1482.5282 - loglik: -1.4809e+03 - logprior: -1.6517e+00
Epoch 8/10
39/39 - 37s - loss: 1470.5649 - loglik: -1.4688e+03 - logprior: -1.7319e+00
Epoch 9/10
39/39 - 37s - loss: 1445.7491 - loglik: -1.4438e+03 - logprior: -1.9638e+00
Epoch 10/10
39/39 - 37s - loss: 1311.6638 - loglik: -1.3061e+03 - logprior: -5.5646e+00
Fitted a model with MAP estimate = -1219.5079
expansions: []
discards: [  5   6   7   8  17  18  19  20  21  22  23  36  37  38  39  40  41  49
  50  63  64  93  94  95  96  97  98  99 100 101 108 109 110 111 112 125
 126 134 135 136 137 138 139 140 141 142 143 144 145 162 163 164 172 180
 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198
 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216
 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234
 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252
 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270
 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 1755.6140 - loglik: -1.7536e+03 - logprior: -1.9791e+00
Epoch 2/2
39/39 - 13s - loss: 1718.8179 - loglik: -1.7184e+03 - logprior: -4.1943e-01
Fitted a model with MAP estimate = -1694.5451
expansions: [(0, 24), (72, 1), (78, 66), (84, 5), (97, 13), (107, 8), (119, 1), (121, 1), (127, 124)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 45s - loss: 1593.3927 - loglik: -1.5915e+03 - logprior: -1.8442e+00
Epoch 2/2
39/39 - 44s - loss: 1511.6901 - loglik: -1.5105e+03 - logprior: -1.1611e+00
Fitted a model with MAP estimate = -1504.4778
expansions: [(40, 1), (41, 2), (51, 1), (52, 1), (56, 5), (57, 2), (58, 2), (60, 1), (62, 3), (63, 2), (64, 1), (65, 2), (66, 1), (72, 1), (74, 3), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (85, 1), (86, 1), (89, 1), (116, 1), (129, 1), (130, 6), (158, 1), (159, 1), (160, 1), (181, 1), (203, 1), (224, 1), (229, 1), (242, 1), (250, 1), (253, 1), (261, 1), (277, 1), (279, 1), (283, 2), (285, 1), (297, 1), (299, 1), (302, 1)]
discards: [ 0 13 14 15 17]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 67s - loss: 1487.5450 - loglik: -1.4855e+03 - logprior: -2.0461e+00
Epoch 2/10
39/39 - 67s - loss: 1472.1929 - loglik: -1.4721e+03 - logprior: -9.4831e-02
Epoch 3/10
39/39 - 69s - loss: 1469.8004 - loglik: -1.4699e+03 - logprior: 0.1269
Epoch 4/10
39/39 - 69s - loss: 1465.3076 - loglik: -1.4655e+03 - logprior: 0.2235
Epoch 5/10
39/39 - 72s - loss: 1461.6377 - loglik: -1.4620e+03 - logprior: 0.3340
Epoch 6/10
39/39 - 68s - loss: 1458.1649 - loglik: -1.4586e+03 - logprior: 0.4078
Epoch 7/10
39/39 - 71s - loss: 1451.3826 - loglik: -1.4518e+03 - logprior: 0.3947
Epoch 8/10
39/39 - 67s - loss: 1433.1841 - loglik: -1.4334e+03 - logprior: 0.2580
Epoch 9/10
39/39 - 68s - loss: 1349.4661 - loglik: -1.3492e+03 - logprior: -2.0332e-01
Epoch 10/10
39/39 - 61s - loss: 1217.7218 - loglik: -1.2163e+03 - logprior: -1.3623e+00
Fitted a model with MAP estimate = -1196.4382
Time for alignment: 1319.6392
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 1632.3248 - loglik: -1.6308e+03 - logprior: -1.5427e+00
Epoch 2/10
39/39 - 39s - loss: 1508.4065 - loglik: -1.5065e+03 - logprior: -1.8612e+00
Epoch 3/10
39/39 - 39s - loss: 1497.7397 - loglik: -1.4960e+03 - logprior: -1.7403e+00
Epoch 4/10
39/39 - 39s - loss: 1495.3893 - loglik: -1.4937e+03 - logprior: -1.6921e+00
Epoch 5/10
39/39 - 39s - loss: 1491.9417 - loglik: -1.4902e+03 - logprior: -1.7014e+00
Epoch 6/10
39/39 - 39s - loss: 1487.5382 - loglik: -1.4858e+03 - logprior: -1.7138e+00
Epoch 7/10
39/39 - 38s - loss: 1481.5895 - loglik: -1.4799e+03 - logprior: -1.7209e+00
Epoch 8/10
39/39 - 39s - loss: 1469.9285 - loglik: -1.4681e+03 - logprior: -1.8182e+00
Epoch 9/10
39/39 - 40s - loss: 1444.3610 - loglik: -1.4424e+03 - logprior: -1.9679e+00
Epoch 10/10
39/39 - 39s - loss: 1285.7532 - loglik: -1.2799e+03 - logprior: -5.8376e+00
Fitted a model with MAP estimate = -1217.6561
expansions: []
discards: [  5   6   7   8  16  17  18  19  20  21  22  35  36  37  38  39  40  50
  51  64  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96
  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 124 125
 133 134 135 136 137 138 139 140 141 142 143 183 184 185 186 187 188 189
 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207
 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225
 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243
 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261
 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279
 280 281 282 283 284 285 286 287 288]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 1754.5566 - loglik: -1.7527e+03 - logprior: -1.8616e+00
Epoch 2/2
39/39 - 12s - loss: 1729.8253 - loglik: -1.7294e+03 - logprior: -4.4889e-01
Fitted a model with MAP estimate = -1713.8122
expansions: [(0, 48), (118, 182)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  72  73  74  75  76  77  78  79  80  81  82
  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100
 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 1634.6644 - loglik: -1.6324e+03 - logprior: -2.3115e+00
Epoch 2/2
39/39 - 33s - loss: 1559.8898 - loglik: -1.5577e+03 - logprior: -2.1635e+00
Fitted a model with MAP estimate = -1552.2648
expansions: [(186, 1), (194, 1), (223, 1), (247, 1), (249, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 181 182]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 1641.2303 - loglik: -1.6393e+03 - logprior: -1.9435e+00
Epoch 2/10
39/39 - 12s - loss: 1633.5691 - loglik: -1.6324e+03 - logprior: -1.1731e+00
Epoch 3/10
39/39 - 11s - loss: 1632.2341 - loglik: -1.6311e+03 - logprior: -1.1259e+00
Epoch 4/10
39/39 - 11s - loss: 1630.1038 - loglik: -1.6290e+03 - logprior: -1.0917e+00
Epoch 5/10
39/39 - 12s - loss: 1625.9932 - loglik: -1.6249e+03 - logprior: -1.0934e+00
Epoch 6/10
39/39 - 12s - loss: 1621.8260 - loglik: -1.6207e+03 - logprior: -1.0963e+00
Epoch 7/10
39/39 - 11s - loss: 1593.0199 - loglik: -1.5913e+03 - logprior: -1.7033e+00
Epoch 8/10
39/39 - 12s - loss: 1391.3229 - loglik: -1.3872e+03 - logprior: -4.0705e+00
Epoch 9/10
39/39 - 11s - loss: 1248.7103 - loglik: -1.2440e+03 - logprior: -4.6840e+00
Epoch 10/10
39/39 - 12s - loss: 1225.6808 - loglik: -1.2209e+03 - logprior: -4.6766e+00
Fitted a model with MAP estimate = -1221.3183
Time for alignment: 726.3321
Computed alignments with likelihoods: ['-1202.6430', '-1203.5390', '-1199.7687', '-1196.4382', '-1217.6561']
Best model has likelihood: -1196.4382  (prior= -1.4540 )
time for generating output: 0.2819
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00079.projection.fasta
SP score = 0.11383442265795207
Training of 5 independent models on file PF01381.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6d4cbc670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6d4d77f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6d4d77ca0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 300.5577 - loglik: -2.9735e+02 - logprior: -3.2070e+00
Epoch 2/10
19/19 - 1s - loss: 278.8512 - loglik: -2.7752e+02 - logprior: -1.3322e+00
Epoch 3/10
19/19 - 1s - loss: 270.7156 - loglik: -2.6923e+02 - logprior: -1.4859e+00
Epoch 4/10
19/19 - 1s - loss: 268.4447 - loglik: -2.6704e+02 - logprior: -1.3994e+00
Epoch 5/10
19/19 - 1s - loss: 267.2610 - loglik: -2.6588e+02 - logprior: -1.3793e+00
Epoch 6/10
19/19 - 1s - loss: 265.6787 - loglik: -2.6432e+02 - logprior: -1.3542e+00
Epoch 7/10
19/19 - 1s - loss: 264.2587 - loglik: -2.6291e+02 - logprior: -1.3384e+00
Epoch 8/10
19/19 - 1s - loss: 263.3228 - loglik: -2.6198e+02 - logprior: -1.3271e+00
Epoch 9/10
19/19 - 1s - loss: 262.5580 - loglik: -2.6122e+02 - logprior: -1.3217e+00
Epoch 10/10
19/19 - 1s - loss: 262.0309 - loglik: -2.6069e+02 - logprior: -1.3167e+00
Fitted a model with MAP estimate = -261.1919
expansions: [(6, 3), (15, 1), (19, 2), (20, 1), (27, 1), (28, 2), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 277.3870 - loglik: -2.7334e+02 - logprior: -4.0450e+00
Epoch 2/2
19/19 - 1s - loss: 265.3183 - loglik: -2.6334e+02 - logprior: -1.9769e+00
Fitted a model with MAP estimate = -263.2456
expansions: [(0, 2)]
discards: [ 0 23 34 41]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 264.8340 - loglik: -2.6183e+02 - logprior: -3.0019e+00
Epoch 2/2
19/19 - 1s - loss: 261.5580 - loglik: -2.6036e+02 - logprior: -1.2000e+00
Fitted a model with MAP estimate = -260.7740
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 266.1374 - loglik: -2.6238e+02 - logprior: -3.7529e+00
Epoch 2/10
19/19 - 1s - loss: 261.7756 - loglik: -2.6046e+02 - logprior: -1.3190e+00
Epoch 3/10
19/19 - 1s - loss: 260.8354 - loglik: -2.5969e+02 - logprior: -1.1451e+00
Epoch 4/10
19/19 - 1s - loss: 259.8289 - loglik: -2.5872e+02 - logprior: -1.1087e+00
Epoch 5/10
19/19 - 1s - loss: 258.8249 - loglik: -2.5773e+02 - logprior: -1.0895e+00
Epoch 6/10
19/19 - 1s - loss: 256.6480 - loglik: -2.5556e+02 - logprior: -1.0778e+00
Epoch 7/10
19/19 - 1s - loss: 255.2003 - loglik: -2.5413e+02 - logprior: -1.0643e+00
Epoch 8/10
19/19 - 1s - loss: 253.7566 - loglik: -2.5269e+02 - logprior: -1.0560e+00
Epoch 9/10
19/19 - 1s - loss: 253.1314 - loglik: -2.5207e+02 - logprior: -1.0450e+00
Epoch 10/10
19/19 - 1s - loss: 252.2037 - loglik: -2.5113e+02 - logprior: -1.0520e+00
Fitted a model with MAP estimate = -251.0196
Time for alignment: 42.7636
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 300.7082 - loglik: -2.9750e+02 - logprior: -3.2092e+00
Epoch 2/10
19/19 - 1s - loss: 278.2160 - loglik: -2.7684e+02 - logprior: -1.3779e+00
Epoch 3/10
19/19 - 1s - loss: 269.6374 - loglik: -2.6806e+02 - logprior: -1.5732e+00
Epoch 4/10
19/19 - 1s - loss: 268.0630 - loglik: -2.6660e+02 - logprior: -1.4654e+00
Epoch 5/10
19/19 - 1s - loss: 266.6998 - loglik: -2.6525e+02 - logprior: -1.4507e+00
Epoch 6/10
19/19 - 1s - loss: 265.3960 - loglik: -2.6397e+02 - logprior: -1.4163e+00
Epoch 7/10
19/19 - 1s - loss: 264.3851 - loglik: -2.6297e+02 - logprior: -1.4053e+00
Epoch 8/10
19/19 - 1s - loss: 263.0710 - loglik: -2.6167e+02 - logprior: -1.3881e+00
Epoch 9/10
19/19 - 1s - loss: 262.3300 - loglik: -2.6093e+02 - logprior: -1.3848e+00
Epoch 10/10
19/19 - 1s - loss: 261.9503 - loglik: -2.6055e+02 - logprior: -1.3811e+00
Fitted a model with MAP estimate = -261.1182
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (19, 2), (20, 1), (27, 2), (28, 2), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 277.2229 - loglik: -2.7316e+02 - logprior: -4.0660e+00
Epoch 2/2
19/19 - 1s - loss: 265.2364 - loglik: -2.6325e+02 - logprior: -1.9821e+00
Fitted a model with MAP estimate = -263.1500
expansions: [(0, 2)]
discards: [ 0 23 35 36 42]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 265.0748 - loglik: -2.6207e+02 - logprior: -3.0019e+00
Epoch 2/2
19/19 - 1s - loss: 261.3782 - loglik: -2.6018e+02 - logprior: -1.1992e+00
Fitted a model with MAP estimate = -260.7594
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 266.0769 - loglik: -2.6232e+02 - logprior: -3.7580e+00
Epoch 2/10
19/19 - 1s - loss: 261.7273 - loglik: -2.6040e+02 - logprior: -1.3287e+00
Epoch 3/10
19/19 - 1s - loss: 260.9317 - loglik: -2.5979e+02 - logprior: -1.1448e+00
Epoch 4/10
19/19 - 1s - loss: 259.9033 - loglik: -2.5880e+02 - logprior: -1.1052e+00
Epoch 5/10
19/19 - 1s - loss: 258.7693 - loglik: -2.5768e+02 - logprior: -1.0901e+00
Epoch 6/10
19/19 - 1s - loss: 256.7810 - loglik: -2.5570e+02 - logprior: -1.0779e+00
Epoch 7/10
19/19 - 1s - loss: 254.9858 - loglik: -2.5390e+02 - logprior: -1.0720e+00
Epoch 8/10
19/19 - 1s - loss: 254.0448 - loglik: -2.5297e+02 - logprior: -1.0563e+00
Epoch 9/10
19/19 - 1s - loss: 253.0176 - loglik: -2.5195e+02 - logprior: -1.0480e+00
Epoch 10/10
19/19 - 1s - loss: 252.0768 - loglik: -2.5101e+02 - logprior: -1.0448e+00
Fitted a model with MAP estimate = -251.1064
Time for alignment: 40.9010
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 300.6855 - loglik: -2.9748e+02 - logprior: -3.2072e+00
Epoch 2/10
19/19 - 1s - loss: 279.5020 - loglik: -2.7817e+02 - logprior: -1.3320e+00
Epoch 3/10
19/19 - 1s - loss: 270.4928 - loglik: -2.6893e+02 - logprior: -1.5628e+00
Epoch 4/10
19/19 - 1s - loss: 268.2888 - loglik: -2.6682e+02 - logprior: -1.4679e+00
Epoch 5/10
19/19 - 1s - loss: 267.1694 - loglik: -2.6573e+02 - logprior: -1.4320e+00
Epoch 6/10
19/19 - 1s - loss: 265.7654 - loglik: -2.6434e+02 - logprior: -1.4146e+00
Epoch 7/10
19/19 - 1s - loss: 264.6600 - loglik: -2.6325e+02 - logprior: -1.3992e+00
Epoch 8/10
19/19 - 1s - loss: 263.1926 - loglik: -2.6179e+02 - logprior: -1.3902e+00
Epoch 9/10
19/19 - 1s - loss: 262.7353 - loglik: -2.6133e+02 - logprior: -1.3840e+00
Epoch 10/10
19/19 - 1s - loss: 262.1347 - loglik: -2.6074e+02 - logprior: -1.3771e+00
Fitted a model with MAP estimate = -261.1589
expansions: [(6, 1), (7, 1), (8, 1), (10, 1), (18, 2), (19, 2), (27, 2), (28, 2), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 278.3310 - loglik: -2.7425e+02 - logprior: -4.0800e+00
Epoch 2/2
19/19 - 1s - loss: 265.6010 - loglik: -2.6359e+02 - logprior: -2.0135e+00
Fitted a model with MAP estimate = -263.2730
expansions: [(0, 2)]
discards: [ 0 22 25 36 37 43]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 264.9702 - loglik: -2.6196e+02 - logprior: -3.0075e+00
Epoch 2/2
19/19 - 1s - loss: 261.5570 - loglik: -2.6035e+02 - logprior: -1.2027e+00
Fitted a model with MAP estimate = -260.7694
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 266.1133 - loglik: -2.6236e+02 - logprior: -3.7500e+00
Epoch 2/10
19/19 - 1s - loss: 261.6718 - loglik: -2.6035e+02 - logprior: -1.3256e+00
Epoch 3/10
19/19 - 1s - loss: 260.9203 - loglik: -2.5978e+02 - logprior: -1.1437e+00
Epoch 4/10
19/19 - 1s - loss: 260.0146 - loglik: -2.5891e+02 - logprior: -1.1059e+00
Epoch 5/10
19/19 - 1s - loss: 258.6498 - loglik: -2.5755e+02 - logprior: -1.0923e+00
Epoch 6/10
19/19 - 1s - loss: 256.8414 - loglik: -2.5575e+02 - logprior: -1.0818e+00
Epoch 7/10
19/19 - 1s - loss: 254.9762 - loglik: -2.5390e+02 - logprior: -1.0692e+00
Epoch 8/10
19/19 - 1s - loss: 253.8642 - loglik: -2.5280e+02 - logprior: -1.0548e+00
Epoch 9/10
19/19 - 1s - loss: 253.0332 - loglik: -2.5197e+02 - logprior: -1.0507e+00
Epoch 10/10
19/19 - 1s - loss: 252.3501 - loglik: -2.5128e+02 - logprior: -1.0518e+00
Fitted a model with MAP estimate = -251.0841
Time for alignment: 42.5547
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 300.5994 - loglik: -2.9739e+02 - logprior: -3.2079e+00
Epoch 2/10
19/19 - 1s - loss: 278.7254 - loglik: -2.7738e+02 - logprior: -1.3456e+00
Epoch 3/10
19/19 - 1s - loss: 270.2228 - loglik: -2.6868e+02 - logprior: -1.5454e+00
Epoch 4/10
19/19 - 1s - loss: 267.7354 - loglik: -2.6627e+02 - logprior: -1.4614e+00
Epoch 5/10
19/19 - 1s - loss: 266.9591 - loglik: -2.6552e+02 - logprior: -1.4352e+00
Epoch 6/10
19/19 - 1s - loss: 265.3044 - loglik: -2.6388e+02 - logprior: -1.4164e+00
Epoch 7/10
19/19 - 1s - loss: 264.2403 - loglik: -2.6283e+02 - logprior: -1.3995e+00
Epoch 8/10
19/19 - 1s - loss: 262.9635 - loglik: -2.6156e+02 - logprior: -1.3922e+00
Epoch 9/10
19/19 - 1s - loss: 262.6223 - loglik: -2.6122e+02 - logprior: -1.3809e+00
Epoch 10/10
19/19 - 1s - loss: 261.9010 - loglik: -2.6050e+02 - logprior: -1.3824e+00
Fitted a model with MAP estimate = -261.1288
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (19, 2), (20, 1), (27, 1), (28, 2), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 276.9059 - loglik: -2.7285e+02 - logprior: -4.0538e+00
Epoch 2/2
19/19 - 1s - loss: 265.2025 - loglik: -2.6323e+02 - logprior: -1.9681e+00
Fitted a model with MAP estimate = -263.1472
expansions: [(0, 2)]
discards: [ 0 23 34 41]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 264.7287 - loglik: -2.6172e+02 - logprior: -3.0040e+00
Epoch 2/2
19/19 - 1s - loss: 261.5334 - loglik: -2.6033e+02 - logprior: -1.2012e+00
Fitted a model with MAP estimate = -260.7955
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 266.1132 - loglik: -2.6235e+02 - logprior: -3.7591e+00
Epoch 2/10
19/19 - 1s - loss: 261.7600 - loglik: -2.6044e+02 - logprior: -1.3233e+00
Epoch 3/10
19/19 - 1s - loss: 260.8224 - loglik: -2.5967e+02 - logprior: -1.1482e+00
Epoch 4/10
19/19 - 1s - loss: 260.0317 - loglik: -2.5893e+02 - logprior: -1.1047e+00
Epoch 5/10
19/19 - 1s - loss: 258.6906 - loglik: -2.5759e+02 - logprior: -1.0926e+00
Epoch 6/10
19/19 - 1s - loss: 256.7360 - loglik: -2.5565e+02 - logprior: -1.0755e+00
Epoch 7/10
19/19 - 1s - loss: 255.1168 - loglik: -2.5404e+02 - logprior: -1.0708e+00
Epoch 8/10
19/19 - 1s - loss: 253.8849 - loglik: -2.5282e+02 - logprior: -1.0506e+00
Epoch 9/10
19/19 - 1s - loss: 252.9532 - loglik: -2.5188e+02 - logprior: -1.0522e+00
Epoch 10/10
19/19 - 1s - loss: 252.4691 - loglik: -2.5140e+02 - logprior: -1.0465e+00
Fitted a model with MAP estimate = -251.1357
Time for alignment: 41.6364
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 300.6869 - loglik: -2.9748e+02 - logprior: -3.2087e+00
Epoch 2/10
19/19 - 1s - loss: 279.3191 - loglik: -2.7796e+02 - logprior: -1.3620e+00
Epoch 3/10
19/19 - 1s - loss: 269.8842 - loglik: -2.6832e+02 - logprior: -1.5674e+00
Epoch 4/10
19/19 - 1s - loss: 268.0034 - loglik: -2.6653e+02 - logprior: -1.4681e+00
Epoch 5/10
19/19 - 1s - loss: 266.9006 - loglik: -2.6546e+02 - logprior: -1.4417e+00
Epoch 6/10
19/19 - 1s - loss: 265.3833 - loglik: -2.6396e+02 - logprior: -1.4163e+00
Epoch 7/10
19/19 - 1s - loss: 264.0956 - loglik: -2.6268e+02 - logprior: -1.4007e+00
Epoch 8/10
19/19 - 1s - loss: 263.0853 - loglik: -2.6168e+02 - logprior: -1.3935e+00
Epoch 9/10
19/19 - 1s - loss: 262.3792 - loglik: -2.6098e+02 - logprior: -1.3839e+00
Epoch 10/10
19/19 - 1s - loss: 261.9868 - loglik: -2.6059e+02 - logprior: -1.3786e+00
Fitted a model with MAP estimate = -261.1328
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (19, 2), (20, 1), (27, 2), (28, 2), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 277.3173 - loglik: -2.7325e+02 - logprior: -4.0625e+00
Epoch 2/2
19/19 - 1s - loss: 265.2762 - loglik: -2.6330e+02 - logprior: -1.9771e+00
Fitted a model with MAP estimate = -263.1663
expansions: [(0, 2)]
discards: [ 0 23 34 36 42]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 264.9092 - loglik: -2.6191e+02 - logprior: -2.9984e+00
Epoch 2/2
19/19 - 1s - loss: 261.6391 - loglik: -2.6044e+02 - logprior: -1.1979e+00
Fitted a model with MAP estimate = -260.7703
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 266.0641 - loglik: -2.6232e+02 - logprior: -3.7487e+00
Epoch 2/10
19/19 - 1s - loss: 261.7015 - loglik: -2.6037e+02 - logprior: -1.3272e+00
Epoch 3/10
19/19 - 1s - loss: 260.8965 - loglik: -2.5975e+02 - logprior: -1.1448e+00
Epoch 4/10
19/19 - 1s - loss: 259.9525 - loglik: -2.5884e+02 - logprior: -1.1089e+00
Epoch 5/10
19/19 - 1s - loss: 258.6222 - loglik: -2.5753e+02 - logprior: -1.0872e+00
Epoch 6/10
19/19 - 1s - loss: 256.9514 - loglik: -2.5587e+02 - logprior: -1.0769e+00
Epoch 7/10
19/19 - 1s - loss: 254.8913 - loglik: -2.5381e+02 - logprior: -1.0670e+00
Epoch 8/10
19/19 - 1s - loss: 254.0388 - loglik: -2.5297e+02 - logprior: -1.0533e+00
Epoch 9/10
19/19 - 1s - loss: 253.0418 - loglik: -2.5197e+02 - logprior: -1.0544e+00
Epoch 10/10
19/19 - 1s - loss: 252.2522 - loglik: -2.5119e+02 - logprior: -1.0460e+00
Fitted a model with MAP estimate = -251.1012
Time for alignment: 41.6138
Computed alignments with likelihoods: ['-251.0196', '-251.1064', '-251.0841', '-251.1357', '-251.1012']
Best model has likelihood: -251.0196  (prior= -1.0374 )
time for generating output: 0.0987
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01381.projection.fasta
SP score = 0.656480445324381
Training of 5 independent models on file PF02878.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff66e6a9460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6b2dc72b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6b2dc72e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 733.1636 - loglik: -7.3016e+02 - logprior: -2.9993e+00
Epoch 2/10
19/19 - 3s - loss: 676.3822 - loglik: -6.7526e+02 - logprior: -1.1197e+00
Epoch 3/10
19/19 - 3s - loss: 651.4627 - loglik: -6.5026e+02 - logprior: -1.2025e+00
Epoch 4/10
19/19 - 3s - loss: 643.3569 - loglik: -6.4215e+02 - logprior: -1.2046e+00
Epoch 5/10
19/19 - 3s - loss: 640.4744 - loglik: -6.3935e+02 - logprior: -1.1243e+00
Epoch 6/10
19/19 - 3s - loss: 638.9774 - loglik: -6.3790e+02 - logprior: -1.0753e+00
Epoch 7/10
19/19 - 3s - loss: 637.1566 - loglik: -6.3611e+02 - logprior: -1.0424e+00
Epoch 8/10
19/19 - 3s - loss: 635.5720 - loglik: -6.3453e+02 - logprior: -1.0316e+00
Epoch 9/10
19/19 - 3s - loss: 634.5831 - loglik: -6.3352e+02 - logprior: -1.0498e+00
Epoch 10/10
19/19 - 3s - loss: 630.6169 - loglik: -6.2954e+02 - logprior: -1.0587e+00
Fitted a model with MAP estimate = -628.7122
expansions: [(12, 1), (14, 4), (17, 1), (25, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (66, 1), (70, 1), (71, 1), (97, 1), (101, 1), (103, 1), (107, 1), (108, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 653.3195 - loglik: -6.4951e+02 - logprior: -3.8052e+00
Epoch 2/2
19/19 - 4s - loss: 634.2356 - loglik: -6.3239e+02 - logprior: -1.8451e+00
Fitted a model with MAP estimate = -630.9249
expansions: [(0, 3)]
discards: [  0  14  15  38  76 138]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 634.8522 - loglik: -6.3206e+02 - logprior: -2.7967e+00
Epoch 2/2
19/19 - 4s - loss: 630.2484 - loglik: -6.2928e+02 - logprior: -9.7158e-01
Fitted a model with MAP estimate = -629.4952
expansions: []
discards: [  0   1   2 137]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 636.8465 - loglik: -6.3315e+02 - logprior: -3.7001e+00
Epoch 2/10
19/19 - 4s - loss: 632.4694 - loglik: -6.3092e+02 - logprior: -1.5450e+00
Epoch 3/10
19/19 - 4s - loss: 631.0231 - loglik: -6.3030e+02 - logprior: -7.1839e-01
Epoch 4/10
19/19 - 4s - loss: 629.5528 - loglik: -6.2914e+02 - logprior: -4.0844e-01
Epoch 5/10
19/19 - 4s - loss: 627.3350 - loglik: -6.2703e+02 - logprior: -3.0189e-01
Epoch 6/10
19/19 - 4s - loss: 625.6736 - loglik: -6.2541e+02 - logprior: -2.6050e-01
Epoch 7/10
19/19 - 4s - loss: 625.4661 - loglik: -6.2526e+02 - logprior: -2.0153e-01
Epoch 8/10
19/19 - 4s - loss: 622.8268 - loglik: -6.2263e+02 - logprior: -1.9022e-01
Epoch 9/10
19/19 - 4s - loss: 621.4661 - loglik: -6.2126e+02 - logprior: -1.8760e-01
Epoch 10/10
19/19 - 4s - loss: 615.7615 - loglik: -6.1554e+02 - logprior: -2.0704e-01
Fitted a model with MAP estimate = -609.5450
Time for alignment: 124.8887
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 732.6114 - loglik: -7.2961e+02 - logprior: -2.9964e+00
Epoch 2/10
19/19 - 3s - loss: 675.7852 - loglik: -6.7469e+02 - logprior: -1.0942e+00
Epoch 3/10
19/19 - 3s - loss: 650.8722 - loglik: -6.4970e+02 - logprior: -1.1758e+00
Epoch 4/10
19/19 - 3s - loss: 645.1732 - loglik: -6.4400e+02 - logprior: -1.1692e+00
Epoch 5/10
19/19 - 3s - loss: 640.3721 - loglik: -6.3925e+02 - logprior: -1.1207e+00
Epoch 6/10
19/19 - 3s - loss: 638.7690 - loglik: -6.3769e+02 - logprior: -1.0703e+00
Epoch 7/10
19/19 - 3s - loss: 637.2820 - loglik: -6.3622e+02 - logprior: -1.0503e+00
Epoch 8/10
19/19 - 3s - loss: 635.2002 - loglik: -6.3416e+02 - logprior: -1.0277e+00
Epoch 9/10
19/19 - 3s - loss: 634.4210 - loglik: -6.3336e+02 - logprior: -1.0489e+00
Epoch 10/10
19/19 - 3s - loss: 631.6737 - loglik: -6.3058e+02 - logprior: -1.0733e+00
Fitted a model with MAP estimate = -627.5317
expansions: [(12, 1), (14, 3), (16, 1), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (66, 1), (70, 1), (71, 1), (93, 1), (97, 1), (103, 1), (107, 1), (108, 1), (109, 2), (110, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 652.9109 - loglik: -6.5013e+02 - logprior: -2.7769e+00
Epoch 2/2
19/19 - 4s - loss: 633.3521 - loglik: -6.3231e+02 - logprior: -1.0437e+00
Fitted a model with MAP estimate = -630.6951
expansions: []
discards: [  0  16  76 138]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 637.6620 - loglik: -6.3389e+02 - logprior: -3.7722e+00
Epoch 2/2
19/19 - 4s - loss: 632.0161 - loglik: -6.3027e+02 - logprior: -1.7489e+00
Fitted a model with MAP estimate = -631.2088
expansions: [(0, 3)]
discards: [  0  36 136]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 633.8362 - loglik: -6.3111e+02 - logprior: -2.7243e+00
Epoch 2/10
19/19 - 5s - loss: 630.9137 - loglik: -6.3003e+02 - logprior: -8.8242e-01
Epoch 3/10
19/19 - 4s - loss: 629.2956 - loglik: -6.2858e+02 - logprior: -7.1832e-01
Epoch 4/10
19/19 - 5s - loss: 628.8454 - loglik: -6.2819e+02 - logprior: -6.5633e-01
Epoch 5/10
19/19 - 4s - loss: 627.3425 - loglik: -6.2672e+02 - logprior: -6.2132e-01
Epoch 6/10
19/19 - 5s - loss: 625.4658 - loglik: -6.2486e+02 - logprior: -6.0109e-01
Epoch 7/10
19/19 - 4s - loss: 624.4984 - loglik: -6.2392e+02 - logprior: -5.7411e-01
Epoch 8/10
19/19 - 4s - loss: 623.9113 - loglik: -6.2335e+02 - logprior: -5.5120e-01
Epoch 9/10
19/19 - 5s - loss: 620.0970 - loglik: -6.1952e+02 - logprior: -5.6761e-01
Epoch 10/10
19/19 - 5s - loss: 613.3217 - loglik: -6.1270e+02 - logprior: -5.9736e-01
Fitted a model with MAP estimate = -606.4169
Time for alignment: 132.4150
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 732.8932 - loglik: -7.2990e+02 - logprior: -2.9922e+00
Epoch 2/10
19/19 - 3s - loss: 673.8586 - loglik: -6.7277e+02 - logprior: -1.0931e+00
Epoch 3/10
19/19 - 3s - loss: 650.9660 - loglik: -6.4979e+02 - logprior: -1.1728e+00
Epoch 4/10
19/19 - 4s - loss: 644.1955 - loglik: -6.4307e+02 - logprior: -1.1275e+00
Epoch 5/10
19/19 - 3s - loss: 641.0704 - loglik: -6.3999e+02 - logprior: -1.0764e+00
Epoch 6/10
19/19 - 3s - loss: 639.8332 - loglik: -6.3878e+02 - logprior: -1.0442e+00
Epoch 7/10
19/19 - 3s - loss: 637.0469 - loglik: -6.3601e+02 - logprior: -1.0291e+00
Epoch 8/10
19/19 - 3s - loss: 636.3527 - loglik: -6.3532e+02 - logprior: -1.0225e+00
Epoch 9/10
19/19 - 3s - loss: 632.1901 - loglik: -6.3113e+02 - logprior: -1.0398e+00
Epoch 10/10
19/19 - 3s - loss: 631.8133 - loglik: -6.3072e+02 - logprior: -1.0731e+00
Fitted a model with MAP estimate = -626.5656
expansions: [(12, 1), (14, 4), (25, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (56, 1), (60, 1), (70, 1), (71, 1), (97, 1), (99, 1), (103, 1), (107, 1), (108, 1), (109, 2), (110, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 653.4424 - loglik: -6.5068e+02 - logprior: -2.7659e+00
Epoch 2/2
19/19 - 5s - loss: 633.0653 - loglik: -6.3207e+02 - logprior: -9.9927e-01
Fitted a model with MAP estimate = -629.9978
expansions: []
discards: [  0  17  36 137]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 636.7938 - loglik: -6.3304e+02 - logprior: -3.7528e+00
Epoch 2/2
19/19 - 4s - loss: 631.6919 - loglik: -6.2993e+02 - logprior: -1.7583e+00
Fitted a model with MAP estimate = -630.6383
expansions: [(0, 3), (16, 2)]
discards: [  0 135]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 633.0578 - loglik: -6.3028e+02 - logprior: -2.7812e+00
Epoch 2/10
19/19 - 5s - loss: 629.2681 - loglik: -6.2831e+02 - logprior: -9.6083e-01
Epoch 3/10
19/19 - 5s - loss: 627.2095 - loglik: -6.2640e+02 - logprior: -8.0878e-01
Epoch 4/10
19/19 - 5s - loss: 626.9562 - loglik: -6.2619e+02 - logprior: -7.6281e-01
Epoch 5/10
19/19 - 5s - loss: 626.0845 - loglik: -6.2537e+02 - logprior: -7.1299e-01
Epoch 6/10
19/19 - 5s - loss: 624.3215 - loglik: -6.2361e+02 - logprior: -7.0459e-01
Epoch 7/10
19/19 - 5s - loss: 623.1340 - loglik: -6.2245e+02 - logprior: -6.7803e-01
Epoch 8/10
19/19 - 5s - loss: 620.9536 - loglik: -6.2028e+02 - logprior: -6.6218e-01
Epoch 9/10
19/19 - 5s - loss: 619.3444 - loglik: -6.1867e+02 - logprior: -6.5984e-01
Epoch 10/10
19/19 - 5s - loss: 613.9564 - loglik: -6.1324e+02 - logprior: -6.9905e-01
Fitted a model with MAP estimate = -607.4017
Time for alignment: 137.6928
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 733.4119 - loglik: -7.3042e+02 - logprior: -2.9951e+00
Epoch 2/10
19/19 - 4s - loss: 675.0501 - loglik: -6.7394e+02 - logprior: -1.1110e+00
Epoch 3/10
19/19 - 3s - loss: 649.3173 - loglik: -6.4805e+02 - logprior: -1.2692e+00
Epoch 4/10
19/19 - 3s - loss: 642.3513 - loglik: -6.4112e+02 - logprior: -1.2288e+00
Epoch 5/10
19/19 - 3s - loss: 640.6726 - loglik: -6.3953e+02 - logprior: -1.1376e+00
Epoch 6/10
19/19 - 4s - loss: 638.2230 - loglik: -6.3715e+02 - logprior: -1.0642e+00
Epoch 7/10
19/19 - 4s - loss: 637.0385 - loglik: -6.3598e+02 - logprior: -1.0546e+00
Epoch 8/10
19/19 - 3s - loss: 634.7479 - loglik: -6.3369e+02 - logprior: -1.0453e+00
Epoch 9/10
19/19 - 4s - loss: 634.9780 - loglik: -6.3391e+02 - logprior: -1.0513e+00
Fitted a model with MAP estimate = -632.7269
expansions: [(12, 1), (14, 3), (16, 1), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 2), (94, 1), (101, 1), (103, 1), (107, 1), (108, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 647.5623 - loglik: -6.4376e+02 - logprior: -3.8021e+00
Epoch 2/2
19/19 - 5s - loss: 633.6241 - loglik: -6.3185e+02 - logprior: -1.7755e+00
Fitted a model with MAP estimate = -630.8392
expansions: [(0, 3)]
discards: [  0  37  75 138]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 633.5776 - loglik: -6.3082e+02 - logprior: -2.7607e+00
Epoch 2/2
19/19 - 5s - loss: 629.7017 - loglik: -6.2876e+02 - logprior: -9.4061e-01
Fitted a model with MAP estimate = -628.6422
expansions: []
discards: [ 0  2 18 91]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 635.5747 - loglik: -6.3187e+02 - logprior: -3.7011e+00
Epoch 2/10
19/19 - 4s - loss: 631.1357 - loglik: -6.2980e+02 - logprior: -1.3357e+00
Epoch 3/10
19/19 - 4s - loss: 629.3582 - loglik: -6.2877e+02 - logprior: -5.8473e-01
Epoch 4/10
19/19 - 4s - loss: 628.0095 - loglik: -6.2752e+02 - logprior: -4.8511e-01
Epoch 5/10
19/19 - 5s - loss: 627.1046 - loglik: -6.2666e+02 - logprior: -4.4605e-01
Epoch 6/10
19/19 - 4s - loss: 624.6716 - loglik: -6.2426e+02 - logprior: -4.1115e-01
Epoch 7/10
19/19 - 4s - loss: 626.0962 - loglik: -6.2572e+02 - logprior: -3.6728e-01
Fitted a model with MAP estimate = -623.0575
Time for alignment: 121.9363
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 733.1636 - loglik: -7.3017e+02 - logprior: -2.9971e+00
Epoch 2/10
19/19 - 3s - loss: 674.2077 - loglik: -6.7309e+02 - logprior: -1.1166e+00
Epoch 3/10
19/19 - 4s - loss: 647.4303 - loglik: -6.4620e+02 - logprior: -1.2332e+00
Epoch 4/10
19/19 - 3s - loss: 643.0870 - loglik: -6.4189e+02 - logprior: -1.1920e+00
Epoch 5/10
19/19 - 3s - loss: 640.0906 - loglik: -6.3899e+02 - logprior: -1.0928e+00
Epoch 6/10
19/19 - 3s - loss: 638.2617 - loglik: -6.3721e+02 - logprior: -1.0452e+00
Epoch 7/10
19/19 - 3s - loss: 637.0613 - loglik: -6.3603e+02 - logprior: -1.0209e+00
Epoch 8/10
19/19 - 4s - loss: 636.5502 - loglik: -6.3552e+02 - logprior: -1.0171e+00
Epoch 9/10
19/19 - 3s - loss: 633.4793 - loglik: -6.3245e+02 - logprior: -1.0194e+00
Epoch 10/10
19/19 - 3s - loss: 631.9474 - loglik: -6.3087e+02 - logprior: -1.0589e+00
Fitted a model with MAP estimate = -628.5917
expansions: [(12, 1), (14, 3), (16, 1), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 2), (97, 1), (99, 1), (103, 1), (107, 1), (108, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 652.9564 - loglik: -6.4916e+02 - logprior: -3.8009e+00
Epoch 2/2
19/19 - 5s - loss: 633.8063 - loglik: -6.3200e+02 - logprior: -1.8066e+00
Fitted a model with MAP estimate = -630.9180
expansions: [(0, 3)]
discards: [  0  75  91 138 140]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 633.5620 - loglik: -6.3082e+02 - logprior: -2.7448e+00
Epoch 2/2
19/19 - 4s - loss: 630.0345 - loglik: -6.2913e+02 - logprior: -9.0257e-01
Fitted a model with MAP estimate = -628.7627
expansions: []
discards: [ 0  1  2 18 39]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 636.9902 - loglik: -6.3337e+02 - logprior: -3.6215e+00
Epoch 2/10
19/19 - 5s - loss: 632.2036 - loglik: -6.3073e+02 - logprior: -1.4711e+00
Epoch 3/10
19/19 - 4s - loss: 630.1808 - loglik: -6.2954e+02 - logprior: -6.3960e-01
Epoch 4/10
19/19 - 4s - loss: 628.9902 - loglik: -6.2862e+02 - logprior: -3.6448e-01
Epoch 5/10
19/19 - 5s - loss: 627.1522 - loglik: -6.2689e+02 - logprior: -2.5651e-01
Epoch 6/10
19/19 - 5s - loss: 626.3274 - loglik: -6.2614e+02 - logprior: -1.8596e-01
Epoch 7/10
19/19 - 5s - loss: 624.1618 - loglik: -6.2401e+02 - logprior: -1.4177e-01
Epoch 8/10
19/19 - 5s - loss: 623.3953 - loglik: -6.2327e+02 - logprior: -1.2006e-01
Epoch 9/10
19/19 - 5s - loss: 621.6202 - loglik: -6.2151e+02 - logprior: -9.7712e-02
Epoch 10/10
19/19 - 5s - loss: 615.4738 - loglik: -6.1534e+02 - logprior: -1.1695e-01
Fitted a model with MAP estimate = -609.9778
Time for alignment: 136.1364
Computed alignments with likelihoods: ['-609.5450', '-606.4169', '-607.4017', '-623.0575', '-609.9778']
Best model has likelihood: -606.4169  (prior= -0.6256 )
time for generating output: 0.1518
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02878.projection.fasta
SP score = 0.704635761589404
Training of 5 independent models on file PF00970.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6f742fa90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6e5a86670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6eee47730>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 554.1968 - loglik: -5.5114e+02 - logprior: -3.0616e+00
Epoch 2/10
19/19 - 2s - loss: 515.0106 - loglik: -5.1380e+02 - logprior: -1.2058e+00
Epoch 3/10
19/19 - 2s - loss: 500.0449 - loglik: -4.9876e+02 - logprior: -1.2838e+00
Epoch 4/10
19/19 - 2s - loss: 496.8672 - loglik: -4.9559e+02 - logprior: -1.2787e+00
Epoch 5/10
19/19 - 2s - loss: 495.2649 - loglik: -4.9403e+02 - logprior: -1.2300e+00
Epoch 6/10
19/19 - 2s - loss: 494.0455 - loglik: -4.9284e+02 - logprior: -1.1983e+00
Epoch 7/10
19/19 - 2s - loss: 493.1986 - loglik: -4.9201e+02 - logprior: -1.1856e+00
Epoch 8/10
19/19 - 2s - loss: 493.2776 - loglik: -4.9210e+02 - logprior: -1.1712e+00
Fitted a model with MAP estimate = -491.5133
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (16, 1), (17, 1), (18, 1), (27, 1), (28, 1), (29, 1), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 498.9435 - loglik: -4.9511e+02 - logprior: -3.8288e+00
Epoch 2/2
19/19 - 3s - loss: 487.4365 - loglik: -4.8623e+02 - logprior: -1.2050e+00
Fitted a model with MAP estimate = -485.0097
expansions: []
discards: [ 0 74 81 82 84 95]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 491.8348 - loglik: -4.8788e+02 - logprior: -3.9506e+00
Epoch 2/2
19/19 - 3s - loss: 487.1992 - loglik: -4.8578e+02 - logprior: -1.4231e+00
Fitted a model with MAP estimate = -485.4919
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 488.3542 - loglik: -4.8549e+02 - logprior: -2.8601e+00
Epoch 2/10
19/19 - 3s - loss: 485.5069 - loglik: -4.8444e+02 - logprior: -1.0711e+00
Epoch 3/10
19/19 - 3s - loss: 485.1534 - loglik: -4.8415e+02 - logprior: -1.0039e+00
Epoch 4/10
19/19 - 3s - loss: 483.7767 - loglik: -4.8283e+02 - logprior: -9.4683e-01
Epoch 5/10
19/19 - 3s - loss: 483.4195 - loglik: -4.8250e+02 - logprior: -9.1770e-01
Epoch 6/10
19/19 - 3s - loss: 482.0397 - loglik: -4.8114e+02 - logprior: -8.9421e-01
Epoch 7/10
19/19 - 3s - loss: 481.3362 - loglik: -4.8045e+02 - logprior: -8.8172e-01
Epoch 8/10
19/19 - 3s - loss: 481.5108 - loglik: -4.8064e+02 - logprior: -8.6642e-01
Fitted a model with MAP estimate = -480.3632
Time for alignment: 79.9162
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 553.9630 - loglik: -5.5091e+02 - logprior: -3.0574e+00
Epoch 2/10
19/19 - 2s - loss: 515.8128 - loglik: -5.1462e+02 - logprior: -1.1880e+00
Epoch 3/10
19/19 - 2s - loss: 500.2303 - loglik: -4.9895e+02 - logprior: -1.2750e+00
Epoch 4/10
19/19 - 2s - loss: 496.0263 - loglik: -4.9475e+02 - logprior: -1.2743e+00
Epoch 5/10
19/19 - 2s - loss: 493.8015 - loglik: -4.9257e+02 - logprior: -1.2268e+00
Epoch 6/10
19/19 - 2s - loss: 492.4818 - loglik: -4.9129e+02 - logprior: -1.1846e+00
Epoch 7/10
19/19 - 2s - loss: 491.9109 - loglik: -4.9075e+02 - logprior: -1.1540e+00
Epoch 8/10
19/19 - 2s - loss: 491.3951 - loglik: -4.9024e+02 - logprior: -1.1484e+00
Epoch 9/10
19/19 - 2s - loss: 491.0252 - loglik: -4.8988e+02 - logprior: -1.1312e+00
Epoch 10/10
19/19 - 2s - loss: 489.1620 - loglik: -4.8800e+02 - logprior: -1.1499e+00
Fitted a model with MAP estimate = -487.8893
expansions: [(0, 2), (4, 1), (6, 1), (8, 1), (10, 1), (17, 1), (18, 2), (21, 1), (30, 1), (33, 2), (45, 2), (46, 1), (59, 2), (61, 2), (63, 3), (70, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 500.7856 - loglik: -4.9688e+02 - logprior: -3.9038e+00
Epoch 2/2
19/19 - 3s - loss: 487.7673 - loglik: -4.8653e+02 - logprior: -1.2366e+00
Fitted a model with MAP estimate = -485.2780
expansions: [(25, 1)]
discards: [ 0 45 59 76 80 95]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 491.2878 - loglik: -4.8733e+02 - logprior: -3.9595e+00
Epoch 2/2
19/19 - 3s - loss: 486.7674 - loglik: -4.8532e+02 - logprior: -1.4475e+00
Fitted a model with MAP estimate = -484.9597
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 487.7867 - loglik: -4.8494e+02 - logprior: -2.8482e+00
Epoch 2/10
19/19 - 3s - loss: 484.7657 - loglik: -4.8370e+02 - logprior: -1.0624e+00
Epoch 3/10
19/19 - 3s - loss: 484.7860 - loglik: -4.8379e+02 - logprior: -9.9175e-01
Fitted a model with MAP estimate = -483.7771
Time for alignment: 70.9556
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 554.0665 - loglik: -5.5101e+02 - logprior: -3.0590e+00
Epoch 2/10
19/19 - 2s - loss: 514.7849 - loglik: -5.1359e+02 - logprior: -1.1933e+00
Epoch 3/10
19/19 - 2s - loss: 500.5888 - loglik: -4.9931e+02 - logprior: -1.2744e+00
Epoch 4/10
19/19 - 2s - loss: 496.9467 - loglik: -4.9568e+02 - logprior: -1.2648e+00
Epoch 5/10
19/19 - 2s - loss: 495.6399 - loglik: -4.9442e+02 - logprior: -1.2194e+00
Epoch 6/10
19/19 - 2s - loss: 493.9202 - loglik: -4.9270e+02 - logprior: -1.2174e+00
Epoch 7/10
19/19 - 2s - loss: 493.2737 - loglik: -4.9206e+02 - logprior: -1.2086e+00
Epoch 8/10
19/19 - 2s - loss: 492.3001 - loglik: -4.9110e+02 - logprior: -1.1858e+00
Epoch 9/10
19/19 - 2s - loss: 491.4560 - loglik: -4.9026e+02 - logprior: -1.1869e+00
Epoch 10/10
19/19 - 2s - loss: 491.1540 - loglik: -4.8995e+02 - logprior: -1.1842e+00
Fitted a model with MAP estimate = -489.0572
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (16, 1), (17, 1), (18, 1), (19, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 1), (61, 2), (62, 2), (63, 1), (64, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 499.8023 - loglik: -4.9591e+02 - logprior: -3.8881e+00
Epoch 2/2
19/19 - 3s - loss: 487.5119 - loglik: -4.8636e+02 - logprior: -1.1476e+00
Fitted a model with MAP estimate = -484.9134
expansions: []
discards: [ 0 46 81 94]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 490.9846 - loglik: -4.8704e+02 - logprior: -3.9468e+00
Epoch 2/2
19/19 - 3s - loss: 486.8298 - loglik: -4.8544e+02 - logprior: -1.3893e+00
Fitted a model with MAP estimate = -484.8756
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 487.8099 - loglik: -4.8496e+02 - logprior: -2.8530e+00
Epoch 2/10
19/19 - 3s - loss: 484.5376 - loglik: -4.8348e+02 - logprior: -1.0612e+00
Epoch 3/10
19/19 - 3s - loss: 484.7257 - loglik: -4.8372e+02 - logprior: -1.0017e+00
Fitted a model with MAP estimate = -483.7912
Time for alignment: 69.6926
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 554.2753 - loglik: -5.5121e+02 - logprior: -3.0678e+00
Epoch 2/10
19/19 - 2s - loss: 515.5519 - loglik: -5.1433e+02 - logprior: -1.2181e+00
Epoch 3/10
19/19 - 2s - loss: 501.4188 - loglik: -5.0015e+02 - logprior: -1.2663e+00
Epoch 4/10
19/19 - 2s - loss: 496.4474 - loglik: -4.9516e+02 - logprior: -1.2854e+00
Epoch 5/10
19/19 - 2s - loss: 494.9369 - loglik: -4.9368e+02 - logprior: -1.2534e+00
Epoch 6/10
19/19 - 2s - loss: 493.6180 - loglik: -4.9237e+02 - logprior: -1.2465e+00
Epoch 7/10
19/19 - 2s - loss: 492.5905 - loglik: -4.9136e+02 - logprior: -1.2211e+00
Epoch 8/10
19/19 - 2s - loss: 492.2842 - loglik: -4.9107e+02 - logprior: -1.2072e+00
Epoch 9/10
19/19 - 2s - loss: 491.3024 - loglik: -4.9008e+02 - logprior: -1.2064e+00
Epoch 10/10
19/19 - 2s - loss: 490.4757 - loglik: -4.8925e+02 - logprior: -1.2100e+00
Fitted a model with MAP estimate = -489.2289
expansions: [(0, 2), (4, 1), (6, 1), (7, 1), (10, 1), (14, 1), (16, 1), (17, 1), (18, 1), (27, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (74, 1), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 500.0361 - loglik: -4.9614e+02 - logprior: -3.8922e+00
Epoch 2/2
19/19 - 3s - loss: 487.7679 - loglik: -4.8656e+02 - logprior: -1.2037e+00
Fitted a model with MAP estimate = -485.2216
expansions: []
discards: [ 0 46 76 82 84]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 491.3590 - loglik: -4.8739e+02 - logprior: -3.9661e+00
Epoch 2/2
19/19 - 3s - loss: 487.2105 - loglik: -4.8576e+02 - logprior: -1.4504e+00
Fitted a model with MAP estimate = -485.2120
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 487.9337 - loglik: -4.8507e+02 - logprior: -2.8615e+00
Epoch 2/10
19/19 - 3s - loss: 485.0276 - loglik: -4.8395e+02 - logprior: -1.0740e+00
Epoch 3/10
19/19 - 3s - loss: 485.0355 - loglik: -4.8403e+02 - logprior: -1.0039e+00
Fitted a model with MAP estimate = -484.0815
Time for alignment: 70.2951
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 554.1555 - loglik: -5.5110e+02 - logprior: -3.0575e+00
Epoch 2/10
19/19 - 2s - loss: 515.4940 - loglik: -5.1428e+02 - logprior: -1.2130e+00
Epoch 3/10
19/19 - 2s - loss: 500.4331 - loglik: -4.9913e+02 - logprior: -1.3014e+00
Epoch 4/10
19/19 - 2s - loss: 495.6702 - loglik: -4.9434e+02 - logprior: -1.3278e+00
Epoch 5/10
19/19 - 2s - loss: 493.9361 - loglik: -4.9266e+02 - logprior: -1.2741e+00
Epoch 6/10
19/19 - 2s - loss: 493.2946 - loglik: -4.9206e+02 - logprior: -1.2276e+00
Epoch 7/10
19/19 - 2s - loss: 492.4073 - loglik: -4.9120e+02 - logprior: -1.2038e+00
Epoch 8/10
19/19 - 2s - loss: 491.4255 - loglik: -4.9022e+02 - logprior: -1.1933e+00
Epoch 9/10
19/19 - 2s - loss: 491.4865 - loglik: -4.9029e+02 - logprior: -1.1836e+00
Fitted a model with MAP estimate = -489.7431
expansions: [(0, 2), (4, 1), (6, 1), (8, 1), (10, 1), (14, 1), (17, 1), (19, 1), (28, 1), (29, 1), (35, 1), (46, 1), (59, 2), (61, 2), (63, 2), (64, 2), (68, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 499.3720 - loglik: -4.9551e+02 - logprior: -3.8650e+00
Epoch 2/2
19/19 - 3s - loss: 488.2933 - loglik: -4.8706e+02 - logprior: -1.2293e+00
Fitted a model with MAP estimate = -485.8993
expansions: []
discards: [ 0 72 77 80 81 93]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 493.2425 - loglik: -4.8927e+02 - logprior: -3.9731e+00
Epoch 2/2
19/19 - 3s - loss: 488.5077 - loglik: -4.8704e+02 - logprior: -1.4692e+00
Fitted a model with MAP estimate = -486.6639
expansions: [(0, 2), (22, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 488.9716 - loglik: -4.8611e+02 - logprior: -2.8568e+00
Epoch 2/10
19/19 - 3s - loss: 485.5919 - loglik: -4.8452e+02 - logprior: -1.0692e+00
Epoch 3/10
19/19 - 3s - loss: 485.4627 - loglik: -4.8447e+02 - logprior: -9.8707e-01
Epoch 4/10
19/19 - 3s - loss: 483.8521 - loglik: -4.8292e+02 - logprior: -9.3037e-01
Epoch 5/10
19/19 - 3s - loss: 482.9137 - loglik: -4.8201e+02 - logprior: -9.0082e-01
Epoch 6/10
19/19 - 3s - loss: 482.4956 - loglik: -4.8161e+02 - logprior: -8.8404e-01
Epoch 7/10
19/19 - 3s - loss: 481.7008 - loglik: -4.8082e+02 - logprior: -8.7637e-01
Epoch 8/10
19/19 - 3s - loss: 480.9485 - loglik: -4.8009e+02 - logprior: -8.5396e-01
Epoch 9/10
19/19 - 3s - loss: 480.2664 - loglik: -4.7940e+02 - logprior: -8.5158e-01
Epoch 10/10
19/19 - 3s - loss: 479.5730 - loglik: -4.7872e+02 - logprior: -8.4316e-01
Fitted a model with MAP estimate = -478.6952
Time for alignment: 86.5557
Computed alignments with likelihoods: ['-480.3632', '-483.7771', '-483.7912', '-484.0815', '-478.6952']
Best model has likelihood: -478.6952  (prior= -0.8341 )
time for generating output: 0.1916
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00970.projection.fasta
SP score = 0.635934819897084
Training of 5 independent models on file PF00313.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6b3734400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6ef074490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6dd40bf10>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 359.8375 - loglik: -3.5663e+02 - logprior: -3.2101e+00
Epoch 2/10
19/19 - 1s - loss: 317.1091 - loglik: -3.1579e+02 - logprior: -1.3174e+00
Epoch 3/10
19/19 - 1s - loss: 301.4835 - loglik: -3.0019e+02 - logprior: -1.2893e+00
Epoch 4/10
19/19 - 1s - loss: 298.0154 - loglik: -2.9671e+02 - logprior: -1.3064e+00
Epoch 5/10
19/19 - 1s - loss: 297.0647 - loglik: -2.9580e+02 - logprior: -1.2655e+00
Epoch 6/10
19/19 - 1s - loss: 296.6298 - loglik: -2.9539e+02 - logprior: -1.2307e+00
Epoch 7/10
19/19 - 1s - loss: 295.5682 - loglik: -2.9434e+02 - logprior: -1.2174e+00
Epoch 8/10
19/19 - 1s - loss: 295.2433 - loglik: -2.9402e+02 - logprior: -1.2154e+00
Epoch 9/10
19/19 - 1s - loss: 294.6270 - loglik: -2.9340e+02 - logprior: -1.2139e+00
Epoch 10/10
19/19 - 1s - loss: 294.0433 - loglik: -2.9282e+02 - logprior: -1.2154e+00
Fitted a model with MAP estimate = -293.5403
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 299.1089 - loglik: -2.9491e+02 - logprior: -4.1988e+00
Epoch 2/2
19/19 - 1s - loss: 288.2528 - loglik: -2.8704e+02 - logprior: -1.2113e+00
Fitted a model with MAP estimate = -287.1181
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 293.0590 - loglik: -2.8905e+02 - logprior: -4.0117e+00
Epoch 2/2
19/19 - 1s - loss: 288.1967 - loglik: -2.8681e+02 - logprior: -1.3895e+00
Fitted a model with MAP estimate = -287.3850
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 291.2240 - loglik: -2.8806e+02 - logprior: -3.1685e+00
Epoch 2/10
19/19 - 1s - loss: 287.6406 - loglik: -2.8632e+02 - logprior: -1.3192e+00
Epoch 3/10
19/19 - 1s - loss: 286.7846 - loglik: -2.8559e+02 - logprior: -1.1932e+00
Epoch 4/10
19/19 - 1s - loss: 286.6764 - loglik: -2.8552e+02 - logprior: -1.1525e+00
Epoch 5/10
19/19 - 1s - loss: 285.5505 - loglik: -2.8443e+02 - logprior: -1.1139e+00
Epoch 6/10
19/19 - 1s - loss: 285.3847 - loglik: -2.8429e+02 - logprior: -1.0926e+00
Epoch 7/10
19/19 - 1s - loss: 284.6664 - loglik: -2.8358e+02 - logprior: -1.0829e+00
Epoch 8/10
19/19 - 1s - loss: 284.4525 - loglik: -2.8338e+02 - logprior: -1.0690e+00
Epoch 9/10
19/19 - 1s - loss: 283.3626 - loglik: -2.8230e+02 - logprior: -1.0580e+00
Epoch 10/10
19/19 - 1s - loss: 282.6571 - loglik: -2.8159e+02 - logprior: -1.0600e+00
Fitted a model with MAP estimate = -282.2496
Time for alignment: 47.7446
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 360.0807 - loglik: -3.5687e+02 - logprior: -3.2081e+00
Epoch 2/10
19/19 - 1s - loss: 317.0858 - loglik: -3.1577e+02 - logprior: -1.3176e+00
Epoch 3/10
19/19 - 1s - loss: 300.9189 - loglik: -2.9963e+02 - logprior: -1.2904e+00
Epoch 4/10
19/19 - 1s - loss: 298.8501 - loglik: -2.9754e+02 - logprior: -1.3041e+00
Epoch 5/10
19/19 - 1s - loss: 297.7027 - loglik: -2.9643e+02 - logprior: -1.2743e+00
Epoch 6/10
19/19 - 1s - loss: 296.3941 - loglik: -2.9513e+02 - logprior: -1.2550e+00
Epoch 7/10
19/19 - 1s - loss: 295.8963 - loglik: -2.9466e+02 - logprior: -1.2339e+00
Epoch 8/10
19/19 - 1s - loss: 295.0640 - loglik: -2.9383e+02 - logprior: -1.2293e+00
Epoch 9/10
19/19 - 1s - loss: 294.6400 - loglik: -2.9341e+02 - logprior: -1.2236e+00
Epoch 10/10
19/19 - 1s - loss: 294.2369 - loglik: -2.9300e+02 - logprior: -1.2275e+00
Fitted a model with MAP estimate = -293.6051
expansions: [(0, 2), (8, 1), (15, 1), (19, 1), (27, 1), (28, 3), (29, 2), (30, 1), (31, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 298.7348 - loglik: -2.9455e+02 - logprior: -4.1823e+00
Epoch 2/2
19/19 - 1s - loss: 288.3241 - loglik: -2.8712e+02 - logprior: -1.2052e+00
Fitted a model with MAP estimate = -287.0857
expansions: []
discards: [ 2 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 291.8679 - loglik: -2.8884e+02 - logprior: -3.0234e+00
Epoch 2/2
19/19 - 1s - loss: 287.8967 - loglik: -2.8657e+02 - logprior: -1.3230e+00
Fitted a model with MAP estimate = -287.1695
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 290.9907 - loglik: -2.8781e+02 - logprior: -3.1824e+00
Epoch 2/10
19/19 - 1s - loss: 287.5525 - loglik: -2.8623e+02 - logprior: -1.3256e+00
Epoch 3/10
19/19 - 1s - loss: 287.0421 - loglik: -2.8584e+02 - logprior: -1.2026e+00
Epoch 4/10
19/19 - 1s - loss: 286.2006 - loglik: -2.8504e+02 - logprior: -1.1550e+00
Epoch 5/10
19/19 - 1s - loss: 285.8690 - loglik: -2.8476e+02 - logprior: -1.1078e+00
Epoch 6/10
19/19 - 1s - loss: 285.2044 - loglik: -2.8410e+02 - logprior: -1.1030e+00
Epoch 7/10
19/19 - 1s - loss: 284.8140 - loglik: -2.8373e+02 - logprior: -1.0769e+00
Epoch 8/10
19/19 - 1s - loss: 284.1271 - loglik: -2.8305e+02 - logprior: -1.0714e+00
Epoch 9/10
19/19 - 1s - loss: 283.5856 - loglik: -2.8252e+02 - logprior: -1.0557e+00
Epoch 10/10
19/19 - 1s - loss: 282.6198 - loglik: -2.8155e+02 - logprior: -1.0606e+00
Fitted a model with MAP estimate = -282.2785
Time for alignment: 47.1199
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 359.9050 - loglik: -3.5670e+02 - logprior: -3.2089e+00
Epoch 2/10
19/19 - 1s - loss: 315.9283 - loglik: -3.1460e+02 - logprior: -1.3314e+00
Epoch 3/10
19/19 - 1s - loss: 300.2527 - loglik: -2.9894e+02 - logprior: -1.3129e+00
Epoch 4/10
19/19 - 1s - loss: 297.7683 - loglik: -2.9644e+02 - logprior: -1.3236e+00
Epoch 5/10
19/19 - 1s - loss: 296.9862 - loglik: -2.9569e+02 - logprior: -1.2893e+00
Epoch 6/10
19/19 - 1s - loss: 296.3773 - loglik: -2.9510e+02 - logprior: -1.2734e+00
Epoch 7/10
19/19 - 1s - loss: 295.4905 - loglik: -2.9422e+02 - logprior: -1.2650e+00
Epoch 8/10
19/19 - 1s - loss: 294.9682 - loglik: -2.9370e+02 - logprior: -1.2585e+00
Epoch 9/10
19/19 - 1s - loss: 294.2000 - loglik: -2.9294e+02 - logprior: -1.2518e+00
Epoch 10/10
19/19 - 1s - loss: 293.4535 - loglik: -2.9218e+02 - logprior: -1.2572e+00
Fitted a model with MAP estimate = -293.1561
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (26, 1), (27, 1), (28, 1), (29, 2), (30, 1), (31, 1), (39, 1), (40, 2), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 299.6566 - loglik: -2.9545e+02 - logprior: -4.2043e+00
Epoch 2/2
19/19 - 1s - loss: 288.8021 - loglik: -2.8754e+02 - logprior: -1.2630e+00
Fitted a model with MAP estimate = -287.2934
expansions: []
discards: [ 0 38 54]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 293.3300 - loglik: -2.8932e+02 - logprior: -4.0110e+00
Epoch 2/2
19/19 - 1s - loss: 288.1908 - loglik: -2.8679e+02 - logprior: -1.4036e+00
Fitted a model with MAP estimate = -287.5030
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 291.3039 - loglik: -2.8831e+02 - logprior: -2.9892e+00
Epoch 2/10
19/19 - 1s - loss: 287.7560 - loglik: -2.8666e+02 - logprior: -1.1005e+00
Epoch 3/10
19/19 - 1s - loss: 287.0091 - loglik: -2.8588e+02 - logprior: -1.1247e+00
Epoch 4/10
19/19 - 1s - loss: 286.4802 - loglik: -2.8530e+02 - logprior: -1.1771e+00
Epoch 5/10
19/19 - 1s - loss: 285.8283 - loglik: -2.8475e+02 - logprior: -1.0754e+00
Epoch 6/10
19/19 - 1s - loss: 284.9516 - loglik: -2.8387e+02 - logprior: -1.0819e+00
Epoch 7/10
19/19 - 1s - loss: 284.8427 - loglik: -2.8378e+02 - logprior: -1.0551e+00
Epoch 8/10
19/19 - 1s - loss: 284.0021 - loglik: -2.8295e+02 - logprior: -1.0476e+00
Epoch 9/10
19/19 - 1s - loss: 283.5894 - loglik: -2.8255e+02 - logprior: -1.0350e+00
Epoch 10/10
19/19 - 1s - loss: 282.8157 - loglik: -2.8178e+02 - logprior: -1.0291e+00
Fitted a model with MAP estimate = -282.2519
Time for alignment: 47.4645
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 360.0602 - loglik: -3.5685e+02 - logprior: -3.2110e+00
Epoch 2/10
19/19 - 1s - loss: 315.2469 - loglik: -3.1393e+02 - logprior: -1.3167e+00
Epoch 3/10
19/19 - 1s - loss: 300.2703 - loglik: -2.9897e+02 - logprior: -1.3042e+00
Epoch 4/10
19/19 - 1s - loss: 297.8389 - loglik: -2.9652e+02 - logprior: -1.3169e+00
Epoch 5/10
19/19 - 1s - loss: 297.1762 - loglik: -2.9591e+02 - logprior: -1.2644e+00
Epoch 6/10
19/19 - 1s - loss: 296.3756 - loglik: -2.9513e+02 - logprior: -1.2414e+00
Epoch 7/10
19/19 - 1s - loss: 295.6704 - loglik: -2.9443e+02 - logprior: -1.2344e+00
Epoch 8/10
19/19 - 1s - loss: 295.2319 - loglik: -2.9400e+02 - logprior: -1.2255e+00
Epoch 9/10
19/19 - 1s - loss: 294.6057 - loglik: -2.9337e+02 - logprior: -1.2244e+00
Epoch 10/10
19/19 - 1s - loss: 293.8120 - loglik: -2.9258e+02 - logprior: -1.2248e+00
Fitted a model with MAP estimate = -293.4657
expansions: [(0, 2), (8, 1), (15, 1), (20, 1), (27, 1), (28, 3), (29, 2), (30, 1), (31, 1), (39, 1), (40, 2), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 299.3453 - loglik: -2.9513e+02 - logprior: -4.2119e+00
Epoch 2/2
19/19 - 1s - loss: 288.3242 - loglik: -2.8709e+02 - logprior: -1.2329e+00
Fitted a model with MAP estimate = -287.1286
expansions: []
discards: [ 0 38 54]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 293.2143 - loglik: -2.8920e+02 - logprior: -4.0176e+00
Epoch 2/2
19/19 - 1s - loss: 288.1082 - loglik: -2.8672e+02 - logprior: -1.3856e+00
Fitted a model with MAP estimate = -287.3230
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 291.1988 - loglik: -2.8812e+02 - logprior: -3.0828e+00
Epoch 2/10
19/19 - 1s - loss: 287.5393 - loglik: -2.8622e+02 - logprior: -1.3217e+00
Epoch 3/10
19/19 - 1s - loss: 287.1561 - loglik: -2.8595e+02 - logprior: -1.2071e+00
Epoch 4/10
19/19 - 1s - loss: 286.1134 - loglik: -2.8496e+02 - logprior: -1.1551e+00
Epoch 5/10
19/19 - 1s - loss: 286.0111 - loglik: -2.8489e+02 - logprior: -1.1189e+00
Epoch 6/10
19/19 - 1s - loss: 285.3495 - loglik: -2.8425e+02 - logprior: -1.1005e+00
Epoch 7/10
19/19 - 1s - loss: 284.6503 - loglik: -2.8357e+02 - logprior: -1.0792e+00
Epoch 8/10
19/19 - 1s - loss: 283.9685 - loglik: -2.8289e+02 - logprior: -1.0759e+00
Epoch 9/10
19/19 - 1s - loss: 283.5710 - loglik: -2.8251e+02 - logprior: -1.0548e+00
Epoch 10/10
19/19 - 1s - loss: 282.8477 - loglik: -2.8178e+02 - logprior: -1.0612e+00
Fitted a model with MAP estimate = -282.2848
Time for alignment: 47.3160
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 360.0550 - loglik: -3.5685e+02 - logprior: -3.2098e+00
Epoch 2/10
19/19 - 1s - loss: 316.1334 - loglik: -3.1483e+02 - logprior: -1.3035e+00
Epoch 3/10
19/19 - 1s - loss: 300.9349 - loglik: -2.9966e+02 - logprior: -1.2786e+00
Epoch 4/10
19/19 - 1s - loss: 298.4485 - loglik: -2.9715e+02 - logprior: -1.3018e+00
Epoch 5/10
19/19 - 1s - loss: 297.4958 - loglik: -2.9623e+02 - logprior: -1.2597e+00
Epoch 6/10
19/19 - 1s - loss: 296.7219 - loglik: -2.9549e+02 - logprior: -1.2318e+00
Epoch 7/10
19/19 - 1s - loss: 296.1049 - loglik: -2.9487e+02 - logprior: -1.2242e+00
Epoch 8/10
19/19 - 1s - loss: 295.4740 - loglik: -2.9426e+02 - logprior: -1.2092e+00
Epoch 9/10
19/19 - 1s - loss: 294.7479 - loglik: -2.9353e+02 - logprior: -1.2099e+00
Epoch 10/10
19/19 - 1s - loss: 294.0651 - loglik: -2.9284e+02 - logprior: -1.2101e+00
Fitted a model with MAP estimate = -293.7609
expansions: [(0, 2), (8, 1), (15, 1), (20, 1), (27, 1), (28, 3), (29, 2), (30, 1), (31, 1), (40, 3), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 299.0173 - loglik: -2.9481e+02 - logprior: -4.2067e+00
Epoch 2/2
19/19 - 1s - loss: 288.4112 - loglik: -2.8717e+02 - logprior: -1.2380e+00
Fitted a model with MAP estimate = -287.1039
expansions: []
discards: [ 0 38 54]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 293.1427 - loglik: -2.8912e+02 - logprior: -4.0177e+00
Epoch 2/2
19/19 - 1s - loss: 288.1431 - loglik: -2.8674e+02 - logprior: -1.4018e+00
Fitted a model with MAP estimate = -287.3528
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 291.2224 - loglik: -2.8815e+02 - logprior: -3.0726e+00
Epoch 2/10
19/19 - 1s - loss: 287.7970 - loglik: -2.8648e+02 - logprior: -1.3126e+00
Epoch 3/10
19/19 - 1s - loss: 286.7141 - loglik: -2.8550e+02 - logprior: -1.2173e+00
Epoch 4/10
19/19 - 1s - loss: 286.3369 - loglik: -2.8518e+02 - logprior: -1.1504e+00
Epoch 5/10
19/19 - 1s - loss: 286.0842 - loglik: -2.8496e+02 - logprior: -1.1177e+00
Epoch 6/10
19/19 - 1s - loss: 285.0141 - loglik: -2.8391e+02 - logprior: -1.1031e+00
Epoch 7/10
19/19 - 1s - loss: 284.8689 - loglik: -2.8378e+02 - logprior: -1.0875e+00
Epoch 8/10
19/19 - 1s - loss: 284.1081 - loglik: -2.8303e+02 - logprior: -1.0732e+00
Epoch 9/10
19/19 - 1s - loss: 283.8144 - loglik: -2.8274e+02 - logprior: -1.0657e+00
Epoch 10/10
19/19 - 1s - loss: 282.3742 - loglik: -2.8130e+02 - logprior: -1.0678e+00
Fitted a model with MAP estimate = -282.2739
Time for alignment: 48.0888
Computed alignments with likelihoods: ['-282.2496', '-282.2785', '-282.2519', '-282.2848', '-282.2739']
Best model has likelihood: -282.2496  (prior= -1.0707 )
time for generating output: 0.1045
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00313.projection.fasta
SP score = 0.8080495356037152
Training of 5 independent models on file PF00009.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6885d4430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6e5bd1a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff718dc59a0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 1206.5488 - loglik: -1.2047e+03 - logprior: -1.8772e+00
Epoch 2/10
39/39 - 15s - loss: 1123.1172 - loglik: -1.1220e+03 - logprior: -1.1514e+00
Epoch 3/10
39/39 - 15s - loss: 1116.9182 - loglik: -1.1159e+03 - logprior: -1.0336e+00
Epoch 4/10
39/39 - 16s - loss: 1113.5187 - loglik: -1.1125e+03 - logprior: -9.8710e-01
Epoch 5/10
39/39 - 16s - loss: 1112.1636 - loglik: -1.1112e+03 - logprior: -9.7909e-01
Epoch 6/10
39/39 - 17s - loss: 1109.0569 - loglik: -1.1081e+03 - logprior: -9.6525e-01
Epoch 7/10
39/39 - 17s - loss: 1106.4623 - loglik: -1.1055e+03 - logprior: -9.5146e-01
Epoch 8/10
39/39 - 17s - loss: 1103.7494 - loglik: -1.1028e+03 - logprior: -9.6437e-01
Epoch 9/10
39/39 - 16s - loss: 1099.4392 - loglik: -1.0984e+03 - logprior: -9.7047e-01
Epoch 10/10
39/39 - 16s - loss: 1097.8781 - loglik: -1.0969e+03 - logprior: -9.9073e-01
Fitted a model with MAP estimate = -1092.0106
expansions: [(29, 1), (30, 2), (35, 2), (56, 1), (75, 2), (78, 1), (81, 1), (101, 1), (103, 1), (106, 1), (138, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1136.5048 - loglik: -1.1346e+03 - logprior: -1.9145e+00
Epoch 2/2
39/39 - 17s - loss: 1110.0485 - loglik: -1.1092e+03 - logprior: -8.8178e-01
Fitted a model with MAP estimate = -1104.9278
expansions: [(37, 1)]
discards: [ 0 82]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1113.9088 - loglik: -1.1112e+03 - logprior: -2.7100e+00
Epoch 2/2
39/39 - 18s - loss: 1109.9266 - loglik: -1.1088e+03 - logprior: -1.1518e+00
Fitted a model with MAP estimate = -1104.9227
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1107.2037 - loglik: -1.1055e+03 - logprior: -1.6915e+00
Epoch 2/10
39/39 - 19s - loss: 1103.5487 - loglik: -1.1029e+03 - logprior: -6.1447e-01
Epoch 3/10
39/39 - 20s - loss: 1103.9509 - loglik: -1.1035e+03 - logprior: -4.9743e-01
Fitted a model with MAP estimate = -1102.0168
Time for alignment: 375.3374
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1209.7875 - loglik: -1.2079e+03 - logprior: -1.8589e+00
Epoch 2/10
39/39 - 18s - loss: 1127.4453 - loglik: -1.1263e+03 - logprior: -1.1901e+00
Epoch 3/10
39/39 - 18s - loss: 1118.8374 - loglik: -1.1177e+03 - logprior: -1.1104e+00
Epoch 4/10
39/39 - 19s - loss: 1116.3560 - loglik: -1.1153e+03 - logprior: -1.0483e+00
Epoch 5/10
39/39 - 19s - loss: 1114.0182 - loglik: -1.1130e+03 - logprior: -1.0284e+00
Epoch 6/10
39/39 - 19s - loss: 1111.6490 - loglik: -1.1106e+03 - logprior: -1.0082e+00
Epoch 7/10
39/39 - 18s - loss: 1110.6548 - loglik: -1.1096e+03 - logprior: -1.0022e+00
Epoch 8/10
39/39 - 18s - loss: 1105.4900 - loglik: -1.1045e+03 - logprior: -1.0050e+00
Epoch 9/10
39/39 - 18s - loss: 1103.2997 - loglik: -1.1023e+03 - logprior: -1.0133e+00
Epoch 10/10
39/39 - 18s - loss: 1101.4977 - loglik: -1.1004e+03 - logprior: -1.0443e+00
Fitted a model with MAP estimate = -1095.5542
expansions: [(29, 1), (30, 2), (55, 1), (56, 1), (75, 3), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 1137.9718 - loglik: -1.1352e+03 - logprior: -2.8115e+00
Epoch 2/2
39/39 - 20s - loss: 1112.3246 - loglik: -1.1113e+03 - logprior: -1.0106e+00
Fitted a model with MAP estimate = -1107.0813
expansions: [(0, 2), (59, 1)]
discards: [ 0 60 80]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 1112.8861 - loglik: -1.1110e+03 - logprior: -1.8458e+00
Epoch 2/2
39/39 - 18s - loss: 1110.3325 - loglik: -1.1096e+03 - logprior: -7.6178e-01
Fitted a model with MAP estimate = -1105.6859
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1110.6960 - loglik: -1.1084e+03 - logprior: -2.3079e+00
Epoch 2/10
39/39 - 18s - loss: 1107.8726 - loglik: -1.1073e+03 - logprior: -5.4672e-01
Epoch 3/10
39/39 - 18s - loss: 1105.2589 - loglik: -1.1048e+03 - logprior: -4.6875e-01
Epoch 4/10
39/39 - 18s - loss: 1104.0807 - loglik: -1.1037e+03 - logprior: -3.9985e-01
Epoch 5/10
39/39 - 17s - loss: 1102.0243 - loglik: -1.1017e+03 - logprior: -2.9405e-01
Epoch 6/10
39/39 - 18s - loss: 1099.3461 - loglik: -1.0991e+03 - logprior: -2.2867e-01
Epoch 7/10
39/39 - 18s - loss: 1099.6272 - loglik: -1.0995e+03 - logprior: -1.4821e-01
Fitted a model with MAP estimate = -1095.7021
Time for alignment: 470.9279
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1211.4707 - loglik: -1.2096e+03 - logprior: -1.8635e+00
Epoch 2/10
39/39 - 16s - loss: 1133.4513 - loglik: -1.1324e+03 - logprior: -1.0790e+00
Epoch 3/10
39/39 - 16s - loss: 1126.2981 - loglik: -1.1253e+03 - logprior: -9.5961e-01
Epoch 4/10
39/39 - 17s - loss: 1124.4510 - loglik: -1.1235e+03 - logprior: -9.2040e-01
Epoch 5/10
39/39 - 18s - loss: 1121.0359 - loglik: -1.1201e+03 - logprior: -9.0281e-01
Epoch 6/10
39/39 - 17s - loss: 1118.3540 - loglik: -1.1175e+03 - logprior: -8.8976e-01
Epoch 7/10
39/39 - 18s - loss: 1115.4219 - loglik: -1.1145e+03 - logprior: -8.9696e-01
Epoch 8/10
39/39 - 18s - loss: 1111.9484 - loglik: -1.1110e+03 - logprior: -9.0393e-01
Epoch 9/10
39/39 - 18s - loss: 1109.0410 - loglik: -1.1081e+03 - logprior: -9.3461e-01
Epoch 10/10
39/39 - 18s - loss: 1105.9923 - loglik: -1.1050e+03 - logprior: -9.5015e-01
Fitted a model with MAP estimate = -1100.7309
expansions: [(25, 1), (30, 5), (35, 1), (56, 1), (57, 1), (80, 1), (93, 1), (101, 1), (137, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 1149.4868 - loglik: -1.1466e+03 - logprior: -2.9016e+00
Epoch 2/2
39/39 - 18s - loss: 1122.8569 - loglik: -1.1218e+03 - logprior: -1.0944e+00
Fitted a model with MAP estimate = -1117.1955
expansions: [(0, 2), (65, 1)]
discards: [  0  32  33  63 150 151]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1120.1796 - loglik: -1.1183e+03 - logprior: -1.8725e+00
Epoch 2/2
39/39 - 18s - loss: 1116.7314 - loglik: -1.1159e+03 - logprior: -7.9675e-01
Fitted a model with MAP estimate = -1111.5339
expansions: [(109, 1), (110, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1115.3041 - loglik: -1.1128e+03 - logprior: -2.5210e+00
Epoch 2/10
39/39 - 18s - loss: 1111.5139 - loglik: -1.1109e+03 - logprior: -6.1150e-01
Epoch 3/10
39/39 - 19s - loss: 1109.3347 - loglik: -1.1088e+03 - logprior: -5.1953e-01
Epoch 4/10
39/39 - 19s - loss: 1106.7439 - loglik: -1.1063e+03 - logprior: -4.2870e-01
Epoch 5/10
39/39 - 19s - loss: 1106.1199 - loglik: -1.1058e+03 - logprior: -3.5206e-01
Epoch 6/10
39/39 - 20s - loss: 1102.3903 - loglik: -1.1021e+03 - logprior: -2.8419e-01
Epoch 7/10
39/39 - 20s - loss: 1102.1348 - loglik: -1.1019e+03 - logprior: -1.8813e-01
Epoch 8/10
39/39 - 19s - loss: 1098.2136 - loglik: -1.0981e+03 - logprior: -1.2776e-01
Epoch 9/10
39/39 - 20s - loss: 1095.9154 - loglik: -1.0958e+03 - logprior: -6.5239e-02
Epoch 10/10
39/39 - 20s - loss: 1089.6013 - loglik: -1.0895e+03 - logprior: -5.4585e-02
Fitted a model with MAP estimate = -1083.9816
Time for alignment: 525.2051
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1208.5361 - loglik: -1.2067e+03 - logprior: -1.8731e+00
Epoch 2/10
39/39 - 17s - loss: 1127.3134 - loglik: -1.1262e+03 - logprior: -1.1050e+00
Epoch 3/10
39/39 - 17s - loss: 1117.5421 - loglik: -1.1165e+03 - logprior: -1.0344e+00
Epoch 4/10
39/39 - 17s - loss: 1115.2858 - loglik: -1.1143e+03 - logprior: -9.8245e-01
Epoch 5/10
39/39 - 17s - loss: 1112.5782 - loglik: -1.1116e+03 - logprior: -9.5760e-01
Epoch 6/10
39/39 - 16s - loss: 1110.8546 - loglik: -1.1099e+03 - logprior: -9.4253e-01
Epoch 7/10
39/39 - 16s - loss: 1108.4740 - loglik: -1.1075e+03 - logprior: -9.4649e-01
Epoch 8/10
39/39 - 17s - loss: 1104.6082 - loglik: -1.1036e+03 - logprior: -9.5125e-01
Epoch 9/10
39/39 - 16s - loss: 1101.3036 - loglik: -1.1003e+03 - logprior: -9.6677e-01
Epoch 10/10
39/39 - 16s - loss: 1098.1376 - loglik: -1.0971e+03 - logprior: -9.9798e-01
Fitted a model with MAP estimate = -1093.0890
expansions: [(26, 1), (32, 1), (35, 2), (56, 1), (76, 1), (82, 1), (93, 3), (103, 1), (105, 1), (106, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1140.9949 - loglik: -1.1381e+03 - logprior: -2.8499e+00
Epoch 2/2
39/39 - 17s - loss: 1113.4846 - loglik: -1.1124e+03 - logprior: -1.0513e+00
Fitted a model with MAP estimate = -1107.3296
expansions: [(0, 2), (26, 1), (30, 4)]
discards: [  0  24 100]
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 1111.8842 - loglik: -1.1100e+03 - logprior: -1.8375e+00
Epoch 2/2
39/39 - 19s - loss: 1107.6313 - loglik: -1.1069e+03 - logprior: -7.6754e-01
Fitted a model with MAP estimate = -1103.4552
expansions: []
discards: [ 0 33 38 39]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1109.7040 - loglik: -1.1074e+03 - logprior: -2.3015e+00
Epoch 2/10
39/39 - 18s - loss: 1105.6660 - loglik: -1.1051e+03 - logprior: -5.2885e-01
Epoch 3/10
39/39 - 18s - loss: 1105.3821 - loglik: -1.1049e+03 - logprior: -4.5458e-01
Epoch 4/10
39/39 - 18s - loss: 1102.1619 - loglik: -1.1018e+03 - logprior: -3.8331e-01
Epoch 5/10
39/39 - 18s - loss: 1101.8669 - loglik: -1.1016e+03 - logprior: -2.9164e-01
Epoch 6/10
39/39 - 18s - loss: 1098.2006 - loglik: -1.0980e+03 - logprior: -2.2613e-01
Epoch 7/10
39/39 - 18s - loss: 1097.6587 - loglik: -1.0975e+03 - logprior: -1.3747e-01
Epoch 8/10
39/39 - 18s - loss: 1092.8333 - loglik: -1.0927e+03 - logprior: -6.8580e-02
Epoch 9/10
39/39 - 18s - loss: 1089.1953 - loglik: -1.0892e+03 - logprior: 0.0123
Epoch 10/10
39/39 - 19s - loss: 1084.5609 - loglik: -1.0846e+03 - logprior: 0.0944
Fitted a model with MAP estimate = -1083.8589
Time for alignment: 503.6473
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1210.1951 - loglik: -1.2084e+03 - logprior: -1.8004e+00
Epoch 2/10
39/39 - 16s - loss: 1131.8101 - loglik: -1.1308e+03 - logprior: -9.6531e-01
Epoch 3/10
39/39 - 17s - loss: 1122.6365 - loglik: -1.1217e+03 - logprior: -8.8521e-01
Epoch 4/10
39/39 - 17s - loss: 1120.5193 - loglik: -1.1196e+03 - logprior: -8.7200e-01
Epoch 5/10
39/39 - 17s - loss: 1118.1617 - loglik: -1.1173e+03 - logprior: -8.6423e-01
Epoch 6/10
39/39 - 17s - loss: 1114.4570 - loglik: -1.1136e+03 - logprior: -8.6208e-01
Epoch 7/10
39/39 - 17s - loss: 1112.4578 - loglik: -1.1116e+03 - logprior: -8.5286e-01
Epoch 8/10
39/39 - 18s - loss: 1107.6130 - loglik: -1.1067e+03 - logprior: -8.7401e-01
Epoch 9/10
39/39 - 18s - loss: 1105.8116 - loglik: -1.1049e+03 - logprior: -9.0628e-01
Epoch 10/10
39/39 - 18s - loss: 1103.9969 - loglik: -1.1030e+03 - logprior: -9.3316e-01
Fitted a model with MAP estimate = -1097.4590
expansions: [(29, 1), (30, 4), (55, 1), (57, 1), (58, 1), (62, 1), (80, 3), (137, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 1146.8522 - loglik: -1.1439e+03 - logprior: -2.9415e+00
Epoch 2/2
39/39 - 18s - loss: 1117.3909 - loglik: -1.1162e+03 - logprior: -1.1849e+00
Fitted a model with MAP estimate = -1111.3359
expansions: [(0, 2), (64, 1), (65, 1), (88, 1), (89, 1), (150, 1)]
discards: [  0 151]
Re-initialized the encoder parameters.
Fitting a model of length 190 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1115.7122 - loglik: -1.1138e+03 - logprior: -1.9558e+00
Epoch 2/2
39/39 - 17s - loss: 1111.4135 - loglik: -1.1105e+03 - logprior: -9.0165e-01
Fitted a model with MAP estimate = -1106.7027
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 189 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1111.9590 - loglik: -1.1095e+03 - logprior: -2.4795e+00
Epoch 2/10
39/39 - 17s - loss: 1108.1294 - loglik: -1.1074e+03 - logprior: -6.9240e-01
Epoch 3/10
39/39 - 17s - loss: 1107.4415 - loglik: -1.1068e+03 - logprior: -6.2987e-01
Epoch 4/10
39/39 - 18s - loss: 1103.9629 - loglik: -1.1034e+03 - logprior: -5.3731e-01
Epoch 5/10
39/39 - 18s - loss: 1104.1659 - loglik: -1.1037e+03 - logprior: -4.4711e-01
Fitted a model with MAP estimate = -1101.3010
Time for alignment: 418.4619
Computed alignments with likelihoods: ['-1092.0106', '-1095.5542', '-1083.9816', '-1083.8589', '-1097.4590']
Best model has likelihood: -1083.8589  (prior= 0.1348 )
time for generating output: 0.2373
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00009.projection.fasta
SP score = 0.7383136536880651
Training of 5 independent models on file PF14604.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6f724d430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff67fda0be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff699cdef10>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 290.4704 - loglik: -2.8725e+02 - logprior: -3.2187e+00
Epoch 2/10
19/19 - 1s - loss: 261.5254 - loglik: -2.6009e+02 - logprior: -1.4356e+00
Epoch 3/10
19/19 - 1s - loss: 251.3750 - loglik: -2.4979e+02 - logprior: -1.5835e+00
Epoch 4/10
19/19 - 1s - loss: 249.1385 - loglik: -2.4770e+02 - logprior: -1.4355e+00
Epoch 5/10
19/19 - 1s - loss: 248.3391 - loglik: -2.4691e+02 - logprior: -1.4260e+00
Epoch 6/10
19/19 - 1s - loss: 247.4862 - loglik: -2.4607e+02 - logprior: -1.4093e+00
Epoch 7/10
19/19 - 1s - loss: 246.9314 - loglik: -2.4553e+02 - logprior: -1.3933e+00
Epoch 8/10
19/19 - 1s - loss: 246.3477 - loglik: -2.4495e+02 - logprior: -1.3856e+00
Epoch 9/10
19/19 - 1s - loss: 245.5329 - loglik: -2.4414e+02 - logprior: -1.3798e+00
Epoch 10/10
19/19 - 1s - loss: 245.2325 - loglik: -2.4384e+02 - logprior: -1.3779e+00
Fitted a model with MAP estimate = -244.3610
expansions: [(6, 2), (7, 1), (8, 1), (13, 2), (18, 1), (20, 1), (21, 2), (26, 1), (28, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 257.7781 - loglik: -2.5366e+02 - logprior: -4.1133e+00
Epoch 2/2
19/19 - 1s - loss: 247.5231 - loglik: -2.4547e+02 - logprior: -2.0489e+00
Fitted a model with MAP estimate = -245.7636
expansions: [(0, 1)]
discards: [ 0  5 17 26 39 43]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 248.7177 - loglik: -2.4548e+02 - logprior: -3.2412e+00
Epoch 2/2
19/19 - 1s - loss: 244.6673 - loglik: -2.4328e+02 - logprior: -1.3883e+00
Fitted a model with MAP estimate = -244.2235
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 247.2639 - loglik: -2.4402e+02 - logprior: -3.2479e+00
Epoch 2/10
19/19 - 1s - loss: 244.5245 - loglik: -2.4314e+02 - logprior: -1.3794e+00
Epoch 3/10
19/19 - 1s - loss: 244.0698 - loglik: -2.4278e+02 - logprior: -1.2847e+00
Epoch 4/10
19/19 - 1s - loss: 243.6990 - loglik: -2.4246e+02 - logprior: -1.2334e+00
Epoch 5/10
19/19 - 1s - loss: 243.1833 - loglik: -2.4198e+02 - logprior: -1.1967e+00
Epoch 6/10
19/19 - 1s - loss: 242.4229 - loglik: -2.4124e+02 - logprior: -1.1801e+00
Epoch 7/10
19/19 - 1s - loss: 241.8957 - loglik: -2.4072e+02 - logprior: -1.1643e+00
Epoch 8/10
19/19 - 1s - loss: 241.0404 - loglik: -2.3987e+02 - logprior: -1.1569e+00
Epoch 9/10
19/19 - 1s - loss: 240.5507 - loglik: -2.3940e+02 - logprior: -1.1414e+00
Epoch 10/10
19/19 - 1s - loss: 238.7799 - loglik: -2.3763e+02 - logprior: -1.1398e+00
Fitted a model with MAP estimate = -237.4883
Time for alignment: 44.0848
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 290.4926 - loglik: -2.8728e+02 - logprior: -3.2173e+00
Epoch 2/10
19/19 - 1s - loss: 261.7023 - loglik: -2.6027e+02 - logprior: -1.4296e+00
Epoch 3/10
19/19 - 1s - loss: 252.7352 - loglik: -2.5117e+02 - logprior: -1.5630e+00
Epoch 4/10
19/19 - 1s - loss: 250.2488 - loglik: -2.4884e+02 - logprior: -1.4107e+00
Epoch 5/10
19/19 - 1s - loss: 249.1639 - loglik: -2.4777e+02 - logprior: -1.3941e+00
Epoch 6/10
19/19 - 1s - loss: 248.2721 - loglik: -2.4690e+02 - logprior: -1.3720e+00
Epoch 7/10
19/19 - 1s - loss: 247.4676 - loglik: -2.4611e+02 - logprior: -1.3545e+00
Epoch 8/10
19/19 - 1s - loss: 247.1391 - loglik: -2.4578e+02 - logprior: -1.3470e+00
Epoch 9/10
19/19 - 1s - loss: 246.6917 - loglik: -2.4534e+02 - logprior: -1.3390e+00
Epoch 10/10
19/19 - 1s - loss: 245.8268 - loglik: -2.4448e+02 - logprior: -1.3357e+00
Fitted a model with MAP estimate = -245.5312
expansions: [(6, 2), (7, 1), (8, 1), (13, 1), (20, 2), (21, 3), (28, 4), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 257.1080 - loglik: -2.5300e+02 - logprior: -4.1056e+00
Epoch 2/2
19/19 - 1s - loss: 247.3053 - loglik: -2.4527e+02 - logprior: -2.0348e+00
Fitted a model with MAP estimate = -245.5112
expansions: [(0, 1)]
discards: [ 0  5 25 28 39 40 44]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 248.8382 - loglik: -2.4560e+02 - logprior: -3.2350e+00
Epoch 2/2
19/19 - 1s - loss: 244.6371 - loglik: -2.4326e+02 - logprior: -1.3814e+00
Fitted a model with MAP estimate = -244.2478
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 247.3316 - loglik: -2.4410e+02 - logprior: -3.2364e+00
Epoch 2/10
19/19 - 1s - loss: 244.4526 - loglik: -2.4308e+02 - logprior: -1.3733e+00
Epoch 3/10
19/19 - 1s - loss: 244.0782 - loglik: -2.4281e+02 - logprior: -1.2717e+00
Epoch 4/10
19/19 - 1s - loss: 243.7352 - loglik: -2.4251e+02 - logprior: -1.2214e+00
Epoch 5/10
19/19 - 1s - loss: 243.0855 - loglik: -2.4190e+02 - logprior: -1.1873e+00
Epoch 6/10
19/19 - 1s - loss: 242.5660 - loglik: -2.4139e+02 - logprior: -1.1706e+00
Epoch 7/10
19/19 - 1s - loss: 241.9185 - loglik: -2.4076e+02 - logprior: -1.1533e+00
Epoch 8/10
19/19 - 1s - loss: 241.0302 - loglik: -2.3988e+02 - logprior: -1.1448e+00
Epoch 9/10
19/19 - 1s - loss: 240.5526 - loglik: -2.3941e+02 - logprior: -1.1347e+00
Epoch 10/10
19/19 - 1s - loss: 238.9747 - loglik: -2.3783e+02 - logprior: -1.1290e+00
Fitted a model with MAP estimate = -237.6307
Time for alignment: 43.5250
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 290.5963 - loglik: -2.8738e+02 - logprior: -3.2161e+00
Epoch 2/10
19/19 - 1s - loss: 261.4670 - loglik: -2.6004e+02 - logprior: -1.4276e+00
Epoch 3/10
19/19 - 1s - loss: 252.2158 - loglik: -2.5065e+02 - logprior: -1.5634e+00
Epoch 4/10
19/19 - 1s - loss: 249.9784 - loglik: -2.4857e+02 - logprior: -1.4079e+00
Epoch 5/10
19/19 - 1s - loss: 248.9191 - loglik: -2.4752e+02 - logprior: -1.3981e+00
Epoch 6/10
19/19 - 1s - loss: 248.2319 - loglik: -2.4685e+02 - logprior: -1.3767e+00
Epoch 7/10
19/19 - 1s - loss: 247.4742 - loglik: -2.4610e+02 - logprior: -1.3620e+00
Epoch 8/10
19/19 - 1s - loss: 246.9185 - loglik: -2.4555e+02 - logprior: -1.3551e+00
Epoch 9/10
19/19 - 1s - loss: 246.3010 - loglik: -2.4494e+02 - logprior: -1.3478e+00
Epoch 10/10
19/19 - 1s - loss: 245.8232 - loglik: -2.4446e+02 - logprior: -1.3456e+00
Fitted a model with MAP estimate = -245.0361
expansions: [(6, 2), (7, 1), (8, 1), (13, 2), (20, 2), (21, 3), (28, 2), (29, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 258.2253 - loglik: -2.5410e+02 - logprior: -4.1257e+00
Epoch 2/2
19/19 - 1s - loss: 247.6204 - loglik: -2.4553e+02 - logprior: -2.0872e+00
Fitted a model with MAP estimate = -245.7943
expansions: [(0, 1)]
discards: [ 0  5 17 26 29 40 41 45]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 248.7786 - loglik: -2.4554e+02 - logprior: -3.2377e+00
Epoch 2/2
19/19 - 1s - loss: 244.7996 - loglik: -2.4342e+02 - logprior: -1.3829e+00
Fitted a model with MAP estimate = -244.2367
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 247.3427 - loglik: -2.4410e+02 - logprior: -3.2443e+00
Epoch 2/10
19/19 - 1s - loss: 244.4513 - loglik: -2.4307e+02 - logprior: -1.3784e+00
Epoch 3/10
19/19 - 1s - loss: 243.9703 - loglik: -2.4269e+02 - logprior: -1.2825e+00
Epoch 4/10
19/19 - 1s - loss: 243.7346 - loglik: -2.4250e+02 - logprior: -1.2296e+00
Epoch 5/10
19/19 - 1s - loss: 243.2370 - loglik: -2.4204e+02 - logprior: -1.1955e+00
Epoch 6/10
19/19 - 1s - loss: 242.5647 - loglik: -2.4138e+02 - logprior: -1.1819e+00
Epoch 7/10
19/19 - 1s - loss: 241.6141 - loglik: -2.4044e+02 - logprior: -1.1651e+00
Epoch 8/10
19/19 - 1s - loss: 241.2056 - loglik: -2.4004e+02 - logprior: -1.1549e+00
Epoch 9/10
19/19 - 1s - loss: 240.3945 - loglik: -2.3924e+02 - logprior: -1.1449e+00
Epoch 10/10
19/19 - 1s - loss: 239.0824 - loglik: -2.3793e+02 - logprior: -1.1412e+00
Fitted a model with MAP estimate = -237.4431
Time for alignment: 44.7053
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 290.5450 - loglik: -2.8733e+02 - logprior: -3.2163e+00
Epoch 2/10
19/19 - 1s - loss: 261.8331 - loglik: -2.6040e+02 - logprior: -1.4322e+00
Epoch 3/10
19/19 - 1s - loss: 252.5798 - loglik: -2.5101e+02 - logprior: -1.5693e+00
Epoch 4/10
19/19 - 1s - loss: 249.7494 - loglik: -2.4833e+02 - logprior: -1.4134e+00
Epoch 5/10
19/19 - 1s - loss: 249.0367 - loglik: -2.4764e+02 - logprior: -1.3972e+00
Epoch 6/10
19/19 - 1s - loss: 248.0095 - loglik: -2.4663e+02 - logprior: -1.3759e+00
Epoch 7/10
19/19 - 1s - loss: 247.5601 - loglik: -2.4619e+02 - logprior: -1.3584e+00
Epoch 8/10
19/19 - 1s - loss: 247.0424 - loglik: -2.4569e+02 - logprior: -1.3474e+00
Epoch 9/10
19/19 - 1s - loss: 246.5064 - loglik: -2.4515e+02 - logprior: -1.3411e+00
Epoch 10/10
19/19 - 1s - loss: 245.8505 - loglik: -2.4450e+02 - logprior: -1.3386e+00
Fitted a model with MAP estimate = -245.2292
expansions: [(6, 2), (7, 1), (8, 1), (13, 2), (20, 2), (21, 3), (28, 3), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 257.9030 - loglik: -2.5379e+02 - logprior: -4.1175e+00
Epoch 2/2
19/19 - 1s - loss: 247.6872 - loglik: -2.4562e+02 - logprior: -2.0641e+00
Fitted a model with MAP estimate = -245.9183
expansions: [(0, 1)]
discards: [ 0  5 17 26 29 40 44]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 248.7817 - loglik: -2.4554e+02 - logprior: -3.2438e+00
Epoch 2/2
19/19 - 1s - loss: 244.7731 - loglik: -2.4338e+02 - logprior: -1.3897e+00
Fitted a model with MAP estimate = -244.2643
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 247.2475 - loglik: -2.4400e+02 - logprior: -3.2452e+00
Epoch 2/10
19/19 - 1s - loss: 244.5725 - loglik: -2.4320e+02 - logprior: -1.3771e+00
Epoch 3/10
19/19 - 1s - loss: 243.9932 - loglik: -2.4271e+02 - logprior: -1.2812e+00
Epoch 4/10
19/19 - 1s - loss: 243.7577 - loglik: -2.4253e+02 - logprior: -1.2270e+00
Epoch 5/10
19/19 - 1s - loss: 243.1556 - loglik: -2.4196e+02 - logprior: -1.1919e+00
Epoch 6/10
19/19 - 1s - loss: 242.5928 - loglik: -2.4141e+02 - logprior: -1.1778e+00
Epoch 7/10
19/19 - 1s - loss: 241.7124 - loglik: -2.4054e+02 - logprior: -1.1627e+00
Epoch 8/10
19/19 - 1s - loss: 241.3367 - loglik: -2.4017e+02 - logprior: -1.1527e+00
Epoch 9/10
19/19 - 1s - loss: 240.3626 - loglik: -2.3921e+02 - logprior: -1.1409e+00
Epoch 10/10
19/19 - 1s - loss: 238.9127 - loglik: -2.3776e+02 - logprior: -1.1338e+00
Fitted a model with MAP estimate = -237.5337
Time for alignment: 43.8239
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 290.5942 - loglik: -2.8737e+02 - logprior: -3.2211e+00
Epoch 2/10
19/19 - 1s - loss: 261.1059 - loglik: -2.5967e+02 - logprior: -1.4377e+00
Epoch 3/10
19/19 - 1s - loss: 251.5315 - loglik: -2.4994e+02 - logprior: -1.5909e+00
Epoch 4/10
19/19 - 1s - loss: 249.4347 - loglik: -2.4801e+02 - logprior: -1.4251e+00
Epoch 5/10
19/19 - 1s - loss: 248.6901 - loglik: -2.4729e+02 - logprior: -1.3994e+00
Epoch 6/10
19/19 - 1s - loss: 248.0144 - loglik: -2.4662e+02 - logprior: -1.3847e+00
Epoch 7/10
19/19 - 1s - loss: 247.2547 - loglik: -2.4588e+02 - logprior: -1.3711e+00
Epoch 8/10
19/19 - 1s - loss: 246.6780 - loglik: -2.4530e+02 - logprior: -1.3633e+00
Epoch 9/10
19/19 - 1s - loss: 246.1267 - loglik: -2.4476e+02 - logprior: -1.3572e+00
Epoch 10/10
19/19 - 1s - loss: 245.4027 - loglik: -2.4403e+02 - logprior: -1.3547e+00
Fitted a model with MAP estimate = -244.9748
expansions: [(6, 2), (7, 1), (8, 1), (13, 2), (20, 2), (21, 2), (28, 2), (29, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 257.8667 - loglik: -2.5375e+02 - logprior: -4.1212e+00
Epoch 2/2
19/19 - 1s - loss: 247.6898 - loglik: -2.4563e+02 - logprior: -2.0625e+00
Fitted a model with MAP estimate = -245.7583
expansions: [(0, 1)]
discards: [ 0  5 17 26 39 40 44]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 248.7200 - loglik: -2.4548e+02 - logprior: -3.2401e+00
Epoch 2/2
19/19 - 1s - loss: 244.6316 - loglik: -2.4325e+02 - logprior: -1.3836e+00
Fitted a model with MAP estimate = -244.2345
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 247.3327 - loglik: -2.4409e+02 - logprior: -3.2457e+00
Epoch 2/10
19/19 - 1s - loss: 244.4537 - loglik: -2.4307e+02 - logprior: -1.3824e+00
Epoch 3/10
19/19 - 1s - loss: 244.0704 - loglik: -2.4279e+02 - logprior: -1.2843e+00
Epoch 4/10
19/19 - 1s - loss: 243.7576 - loglik: -2.4252e+02 - logprior: -1.2325e+00
Epoch 5/10
19/19 - 1s - loss: 243.0553 - loglik: -2.4186e+02 - logprior: -1.1968e+00
Epoch 6/10
19/19 - 1s - loss: 242.5296 - loglik: -2.4135e+02 - logprior: -1.1784e+00
Epoch 7/10
19/19 - 1s - loss: 241.9269 - loglik: -2.4075e+02 - logprior: -1.1669e+00
Epoch 8/10
19/19 - 1s - loss: 241.2310 - loglik: -2.4007e+02 - logprior: -1.1533e+00
Epoch 9/10
19/19 - 1s - loss: 240.2301 - loglik: -2.3908e+02 - logprior: -1.1407e+00
Epoch 10/10
19/19 - 1s - loss: 239.2805 - loglik: -2.3813e+02 - logprior: -1.1406e+00
Fitted a model with MAP estimate = -237.7010
Time for alignment: 41.9266
Computed alignments with likelihoods: ['-237.4883', '-237.6307', '-237.4431', '-237.5337', '-237.7010']
Best model has likelihood: -237.4431  (prior= -1.1259 )
time for generating output: 0.1025
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF14604.projection.fasta
SP score = 0.775165319617928
Training of 5 independent models on file PF00625.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff67ff5f790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6b33e2d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6915bbe50>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 939.8367 - loglik: -9.3701e+02 - logprior: -2.8313e+00
Epoch 2/10
19/19 - 6s - loss: 861.1955 - loglik: -8.5989e+02 - logprior: -1.3078e+00
Epoch 3/10
19/19 - 6s - loss: 832.2069 - loglik: -8.3055e+02 - logprior: -1.6539e+00
Epoch 4/10
19/19 - 6s - loss: 823.5412 - loglik: -8.2182e+02 - logprior: -1.7180e+00
Epoch 5/10
19/19 - 6s - loss: 823.4211 - loglik: -8.2178e+02 - logprior: -1.6399e+00
Epoch 6/10
19/19 - 6s - loss: 819.9105 - loglik: -8.1832e+02 - logprior: -1.5840e+00
Epoch 7/10
19/19 - 6s - loss: 820.2316 - loglik: -8.1866e+02 - logprior: -1.5668e+00
Fitted a model with MAP estimate = -811.7752
expansions: [(0, 2), (11, 1), (13, 1), (15, 1), (16, 1), (17, 1), (18, 1), (21, 1), (22, 1), (30, 1), (38, 1), (44, 1), (48, 1), (50, 1), (52, 1), (57, 1), (66, 1), (67, 1), (69, 1), (70, 1), (80, 1), (87, 1), (88, 1), (89, 2), (99, 2), (113, 1), (114, 2), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 810.0230 - loglik: -8.0773e+02 - logprior: -2.2909e+00
Epoch 2/2
39/39 - 10s - loss: 800.6807 - loglik: -7.9981e+02 - logprior: -8.7509e-01
Fitted a model with MAP estimate = -792.8435
expansions: []
discards: [  0 113 126]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 805.8777 - loglik: -8.0310e+02 - logprior: -2.7802e+00
Epoch 2/2
39/39 - 9s - loss: 801.9182 - loglik: -8.0110e+02 - logprior: -8.1875e-01
Fitted a model with MAP estimate = -793.3670
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 796.4415 - loglik: -7.9472e+02 - logprior: -1.7214e+00
Epoch 2/10
39/39 - 9s - loss: 794.2516 - loglik: -7.9368e+02 - logprior: -5.6917e-01
Epoch 3/10
39/39 - 9s - loss: 792.2480 - loglik: -7.9176e+02 - logprior: -4.8332e-01
Epoch 4/10
39/39 - 9s - loss: 793.1251 - loglik: -7.9272e+02 - logprior: -4.0637e-01
Fitted a model with MAP estimate = -791.4530
Time for alignment: 174.6773
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 938.8787 - loglik: -9.3604e+02 - logprior: -2.8414e+00
Epoch 2/10
19/19 - 6s - loss: 861.1025 - loglik: -8.5981e+02 - logprior: -1.2925e+00
Epoch 3/10
19/19 - 6s - loss: 832.1796 - loglik: -8.3065e+02 - logprior: -1.5335e+00
Epoch 4/10
19/19 - 6s - loss: 827.2725 - loglik: -8.2564e+02 - logprior: -1.6348e+00
Epoch 5/10
19/19 - 6s - loss: 821.3279 - loglik: -8.1971e+02 - logprior: -1.6171e+00
Epoch 6/10
19/19 - 6s - loss: 822.6564 - loglik: -8.2106e+02 - logprior: -1.5880e+00
Fitted a model with MAP estimate = -813.0960
expansions: [(0, 2), (11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (20, 1), (22, 1), (30, 1), (43, 2), (44, 1), (48, 1), (50, 1), (52, 1), (59, 1), (66, 2), (67, 1), (69, 1), (70, 1), (80, 1), (88, 1), (89, 1), (90, 2), (100, 1), (101, 1), (113, 1), (114, 2), (116, 1), (118, 1), (123, 1), (136, 1), (137, 1), (138, 1), (139, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 808.9306 - loglik: -8.0664e+02 - logprior: -2.2883e+00
Epoch 2/2
39/39 - 9s - loss: 797.8884 - loglik: -7.9699e+02 - logprior: -8.9742e-01
Fitted a model with MAP estimate = -789.4415
expansions: []
discards: [  0  55  85 116]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 802.4437 - loglik: -7.9970e+02 - logprior: -2.7431e+00
Epoch 2/2
39/39 - 10s - loss: 798.1843 - loglik: -7.9739e+02 - logprior: -7.9047e-01
Fitted a model with MAP estimate = -789.8116
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 792.4046 - loglik: -7.9072e+02 - logprior: -1.6836e+00
Epoch 2/10
39/39 - 10s - loss: 789.1649 - loglik: -7.8855e+02 - logprior: -6.1090e-01
Epoch 3/10
39/39 - 10s - loss: 789.6436 - loglik: -7.8912e+02 - logprior: -5.2056e-01
Fitted a model with MAP estimate = -787.9963
Time for alignment: 163.0859
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 938.4926 - loglik: -9.3566e+02 - logprior: -2.8345e+00
Epoch 2/10
19/19 - 6s - loss: 862.0643 - loglik: -8.6078e+02 - logprior: -1.2891e+00
Epoch 3/10
19/19 - 7s - loss: 828.7244 - loglik: -8.2713e+02 - logprior: -1.5918e+00
Epoch 4/10
19/19 - 7s - loss: 821.8935 - loglik: -8.2023e+02 - logprior: -1.6590e+00
Epoch 5/10
19/19 - 7s - loss: 821.1636 - loglik: -8.1958e+02 - logprior: -1.5783e+00
Epoch 6/10
19/19 - 7s - loss: 819.3339 - loglik: -8.1778e+02 - logprior: -1.5459e+00
Epoch 7/10
19/19 - 7s - loss: 818.2257 - loglik: -8.1669e+02 - logprior: -1.5262e+00
Epoch 8/10
19/19 - 7s - loss: 818.5123 - loglik: -8.1698e+02 - logprior: -1.5251e+00
Fitted a model with MAP estimate = -809.1961
expansions: [(0, 2), (14, 1), (16, 1), (17, 1), (18, 2), (19, 2), (21, 1), (23, 1), (36, 1), (43, 1), (44, 1), (48, 1), (50, 1), (52, 1), (57, 1), (58, 1), (66, 2), (68, 1), (71, 2), (80, 1), (87, 1), (88, 1), (89, 2), (99, 2), (113, 1), (114, 2), (115, 1), (116, 1), (117, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 186 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 807.0161 - loglik: -8.0476e+02 - logprior: -2.2532e+00
Epoch 2/2
39/39 - 10s - loss: 796.6383 - loglik: -7.9577e+02 - logprior: -8.6996e-01
Fitted a model with MAP estimate = -788.3422
expansions: []
discards: [  0  26  93 116]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 800.9118 - loglik: -7.9823e+02 - logprior: -2.6794e+00
Epoch 2/2
39/39 - 10s - loss: 796.7292 - loglik: -7.9601e+02 - logprior: -7.2242e-01
Fitted a model with MAP estimate = -788.2926
expansions: [(141, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 791.0428 - loglik: -7.8942e+02 - logprior: -1.6246e+00
Epoch 2/10
39/39 - 11s - loss: 788.6562 - loglik: -7.8818e+02 - logprior: -4.7992e-01
Epoch 3/10
39/39 - 11s - loss: 787.6192 - loglik: -7.8724e+02 - logprior: -3.8098e-01
Epoch 4/10
39/39 - 11s - loss: 786.2215 - loglik: -7.8592e+02 - logprior: -2.9943e-01
Epoch 5/10
39/39 - 10s - loss: 785.3155 - loglik: -7.8507e+02 - logprior: -2.3902e-01
Epoch 6/10
39/39 - 10s - loss: 783.8821 - loglik: -7.8370e+02 - logprior: -1.7656e-01
Epoch 7/10
39/39 - 10s - loss: 782.2155 - loglik: -7.8208e+02 - logprior: -1.2381e-01
Epoch 8/10
39/39 - 10s - loss: 780.0333 - loglik: -7.7992e+02 - logprior: -1.0144e-01
Epoch 9/10
39/39 - 9s - loss: 778.9661 - loglik: -7.7889e+02 - logprior: -6.2934e-02
Epoch 10/10
39/39 - 9s - loss: 775.8373 - loglik: -7.7576e+02 - logprior: -5.3050e-02
Fitted a model with MAP estimate = -774.4632
Time for alignment: 255.9306
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 939.4769 - loglik: -9.3663e+02 - logprior: -2.8485e+00
Epoch 2/10
19/19 - 6s - loss: 860.7097 - loglik: -8.5941e+02 - logprior: -1.2950e+00
Epoch 3/10
19/19 - 6s - loss: 832.9370 - loglik: -8.3137e+02 - logprior: -1.5708e+00
Epoch 4/10
19/19 - 6s - loss: 824.7810 - loglik: -8.2316e+02 - logprior: -1.6193e+00
Epoch 5/10
19/19 - 6s - loss: 822.7696 - loglik: -8.2124e+02 - logprior: -1.5281e+00
Epoch 6/10
19/19 - 6s - loss: 821.0127 - loglik: -8.1951e+02 - logprior: -1.4992e+00
Epoch 7/10
19/19 - 6s - loss: 821.6082 - loglik: -8.2013e+02 - logprior: -1.4689e+00
Fitted a model with MAP estimate = -812.2743
expansions: [(0, 2), (14, 1), (16, 1), (17, 1), (18, 2), (19, 1), (21, 1), (22, 1), (36, 1), (43, 2), (44, 1), (48, 1), (50, 2), (51, 1), (58, 1), (66, 1), (69, 1), (70, 1), (80, 1), (87, 2), (88, 1), (89, 1), (99, 1), (113, 1), (114, 2), (115, 1), (116, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 809.0656 - loglik: -8.0679e+02 - logprior: -2.2790e+00
Epoch 2/2
39/39 - 9s - loss: 800.0343 - loglik: -7.9916e+02 - logprior: -8.7155e-01
Fitted a model with MAP estimate = -791.7526
expansions: []
discards: [  0  54 112]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 805.0838 - loglik: -8.0235e+02 - logprior: -2.7331e+00
Epoch 2/2
39/39 - 9s - loss: 800.8724 - loglik: -8.0009e+02 - logprior: -7.7989e-01
Fitted a model with MAP estimate = -792.4772
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 795.1132 - loglik: -7.9344e+02 - logprior: -1.6760e+00
Epoch 2/10
39/39 - 9s - loss: 792.5925 - loglik: -7.9206e+02 - logprior: -5.3581e-01
Epoch 3/10
39/39 - 10s - loss: 791.7365 - loglik: -7.9129e+02 - logprior: -4.4602e-01
Epoch 4/10
39/39 - 10s - loss: 791.8682 - loglik: -7.9150e+02 - logprior: -3.6372e-01
Fitted a model with MAP estimate = -790.3112
Time for alignment: 174.8819
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 939.0394 - loglik: -9.3619e+02 - logprior: -2.8480e+00
Epoch 2/10
19/19 - 7s - loss: 860.4728 - loglik: -8.5918e+02 - logprior: -1.2966e+00
Epoch 3/10
19/19 - 7s - loss: 829.7627 - loglik: -8.2821e+02 - logprior: -1.5533e+00
Epoch 4/10
19/19 - 7s - loss: 824.2816 - loglik: -8.2275e+02 - logprior: -1.5283e+00
Epoch 5/10
19/19 - 7s - loss: 822.9652 - loglik: -8.2143e+02 - logprior: -1.5363e+00
Epoch 6/10
19/19 - 8s - loss: 821.1871 - loglik: -8.1966e+02 - logprior: -1.5273e+00
Epoch 7/10
19/19 - 7s - loss: 817.9686 - loglik: -8.1647e+02 - logprior: -1.4916e+00
Epoch 8/10
19/19 - 7s - loss: 818.7647 - loglik: -8.1728e+02 - logprior: -1.4729e+00
Fitted a model with MAP estimate = -810.5140
expansions: [(0, 2), (14, 1), (15, 2), (16, 1), (17, 1), (18, 1), (21, 1), (22, 1), (36, 1), (43, 2), (44, 1), (48, 1), (49, 1), (50, 1), (51, 1), (58, 1), (65, 1), (66, 1), (68, 1), (71, 2), (80, 1), (87, 2), (88, 1), (89, 1), (99, 1), (100, 1), (112, 1), (113, 1), (114, 2), (115, 1), (116, 1), (117, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 806.4985 - loglik: -8.0426e+02 - logprior: -2.2407e+00
Epoch 2/2
39/39 - 10s - loss: 795.0490 - loglik: -7.9422e+02 - logprior: -8.3231e-01
Fitted a model with MAP estimate = -786.8835
expansions: []
discards: [  0  93 114]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 800.0928 - loglik: -7.9740e+02 - logprior: -2.6939e+00
Epoch 2/2
39/39 - 10s - loss: 796.2879 - loglik: -7.9555e+02 - logprior: -7.4101e-01
Fitted a model with MAP estimate = -787.8866
expansions: []
discards: [53]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 791.1921 - loglik: -7.8957e+02 - logprior: -1.6214e+00
Epoch 2/10
39/39 - 10s - loss: 788.4499 - loglik: -7.8796e+02 - logprior: -4.8887e-01
Epoch 3/10
39/39 - 10s - loss: 788.1970 - loglik: -7.8781e+02 - logprior: -3.8321e-01
Epoch 4/10
39/39 - 11s - loss: 786.9231 - loglik: -7.8661e+02 - logprior: -3.0572e-01
Epoch 5/10
39/39 - 11s - loss: 785.6452 - loglik: -7.8540e+02 - logprior: -2.4235e-01
Epoch 6/10
39/39 - 11s - loss: 784.2429 - loglik: -7.8406e+02 - logprior: -1.7878e-01
Epoch 7/10
39/39 - 11s - loss: 782.0839 - loglik: -7.8193e+02 - logprior: -1.3959e-01
Epoch 8/10
39/39 - 11s - loss: 780.5125 - loglik: -7.8039e+02 - logprior: -1.0551e-01
Epoch 9/10
39/39 - 11s - loss: 780.7102 - loglik: -7.8061e+02 - logprior: -7.8547e-02
Fitted a model with MAP estimate = -777.7795
Time for alignment: 256.2103
Computed alignments with likelihoods: ['-791.4530', '-787.9963', '-774.4632', '-790.3112', '-777.7795']
Best model has likelihood: -774.4632  (prior= -0.0369 )
time for generating output: 0.4090
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00625.projection.fasta
SP score = 0.39694782115302896
Training of 5 independent models on file PF00476.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff67692d340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff66d3efe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff66d41dc40>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 55s - loss: 1950.8085 - loglik: -1.9494e+03 - logprior: -1.3848e+00
Epoch 2/10
39/39 - 48s - loss: 1772.2106 - loglik: -1.7705e+03 - logprior: -1.7258e+00
Epoch 3/10
39/39 - 51s - loss: 1757.2668 - loglik: -1.7556e+03 - logprior: -1.6892e+00
Epoch 4/10
39/39 - 51s - loss: 1753.4869 - loglik: -1.7519e+03 - logprior: -1.6062e+00
Epoch 5/10
39/39 - 54s - loss: 1748.3169 - loglik: -1.7467e+03 - logprior: -1.5765e+00
Epoch 6/10
39/39 - 55s - loss: 1747.3716 - loglik: -1.7458e+03 - logprior: -1.5710e+00
Epoch 7/10
39/39 - 56s - loss: 1743.7816 - loglik: -1.7421e+03 - logprior: -1.6744e+00
Epoch 8/10
39/39 - 59s - loss: 1740.1478 - loglik: -1.7384e+03 - logprior: -1.7255e+00
Epoch 9/10
39/39 - 60s - loss: 1738.9926 - loglik: -1.7372e+03 - logprior: -1.7899e+00
Epoch 10/10
39/39 - 54s - loss: 1735.5785 - loglik: -1.7337e+03 - logprior: -1.8958e+00
Fitted a model with MAP estimate = -1734.3964
expansions: [(0, 4), (43, 1), (46, 1), (57, 1), (60, 1), (62, 1), (67, 2), (72, 1), (73, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (93, 1), (103, 1), (113, 1), (119, 1), (123, 2), (125, 3), (127, 1), (145, 1), (147, 1), (150, 1), (151, 1), (155, 1), (158, 1), (161, 1), (162, 1), (163, 1), (165, 1), (166, 1), (180, 1), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (227, 1), (228, 1), (231, 3), (232, 1), (256, 1), (257, 1), (258, 1), (259, 3), (260, 2), (261, 1), (279, 2), (280, 1), (281, 1), (284, 1), (285, 1), (287, 1), (289, 1), (296, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 383 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 77s - loss: 1735.4250 - loglik: -1.7333e+03 - logprior: -2.1434e+00
Epoch 2/2
39/39 - 84s - loss: 1709.0663 - loglik: -1.7084e+03 - logprior: -6.2439e-01
Fitted a model with MAP estimate = -1704.9799
expansions: []
discards: [  0   2 147 152 231 326 349]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 62s - loss: 1714.1428 - loglik: -1.7123e+03 - logprior: -1.8475e+00
Epoch 2/2
39/39 - 58s - loss: 1708.6934 - loglik: -1.7088e+03 - logprior: 0.0816
Fitted a model with MAP estimate = -1705.3425
expansions: [(1, 1)]
discards: [99]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 61s - loss: 1710.8832 - loglik: -1.7098e+03 - logprior: -1.0755e+00
Epoch 2/10
39/39 - 59s - loss: 1706.4194 - loglik: -1.7065e+03 - logprior: 0.1089
Epoch 3/10
39/39 - 58s - loss: 1704.1100 - loglik: -1.7044e+03 - logprior: 0.2678
Epoch 4/10
39/39 - 59s - loss: 1701.0026 - loglik: -1.7014e+03 - logprior: 0.3857
Epoch 5/10
39/39 - 60s - loss: 1698.1962 - loglik: -1.6987e+03 - logprior: 0.4681
Epoch 6/10
39/39 - 59s - loss: 1695.8672 - loglik: -1.6963e+03 - logprior: 0.4875
Epoch 7/10
39/39 - 59s - loss: 1688.9932 - loglik: -1.6897e+03 - logprior: 0.6951
Epoch 8/10
39/39 - 61s - loss: 1687.9954 - loglik: -1.6888e+03 - logprior: 0.8180
Epoch 9/10
39/39 - 71s - loss: 1684.4513 - loglik: -1.6854e+03 - logprior: 0.9452
Epoch 10/10
39/39 - 82s - loss: 1683.1755 - loglik: -1.6843e+03 - logprior: 1.1080
Fitted a model with MAP estimate = -1681.5900
Time for alignment: 1766.7847
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 61s - loss: 1950.1827 - loglik: -1.9488e+03 - logprior: -1.4173e+00
Epoch 2/10
39/39 - 62s - loss: 1773.6537 - loglik: -1.7719e+03 - logprior: -1.7932e+00
Epoch 3/10
39/39 - 61s - loss: 1758.1678 - loglik: -1.7566e+03 - logprior: -1.5980e+00
Epoch 4/10
39/39 - 52s - loss: 1753.3507 - loglik: -1.7518e+03 - logprior: -1.5349e+00
Epoch 5/10
39/39 - 51s - loss: 1749.3293 - loglik: -1.7478e+03 - logprior: -1.5472e+00
Epoch 6/10
39/39 - 56s - loss: 1747.2682 - loglik: -1.7457e+03 - logprior: -1.5384e+00
Epoch 7/10
39/39 - 53s - loss: 1743.0623 - loglik: -1.7415e+03 - logprior: -1.5513e+00
Epoch 8/10
39/39 - 55s - loss: 1739.2877 - loglik: -1.7376e+03 - logprior: -1.7113e+00
Epoch 9/10
39/39 - 61s - loss: 1737.8890 - loglik: -1.7360e+03 - logprior: -1.8601e+00
Epoch 10/10
39/39 - 64s - loss: 1736.8990 - loglik: -1.7350e+03 - logprior: -1.8543e+00
Fitted a model with MAP estimate = -1734.2282
expansions: [(0, 4), (36, 1), (46, 1), (55, 1), (60, 1), (62, 2), (67, 1), (73, 1), (80, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (103, 1), (113, 1), (114, 1), (119, 1), (123, 2), (125, 3), (127, 1), (142, 1), (144, 1), (149, 1), (151, 1), (155, 1), (158, 1), (160, 1), (162, 1), (163, 1), (165, 1), (166, 1), (173, 1), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (229, 1), (230, 1), (231, 3), (232, 1), (256, 1), (257, 1), (258, 1), (259, 2), (260, 2), (261, 3), (279, 2), (280, 1), (282, 1), (284, 1), (285, 1), (287, 1), (289, 1), (296, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 384 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 98s - loss: 1736.0806 - loglik: -1.7339e+03 - logprior: -2.1449e+00
Epoch 2/2
39/39 - 94s - loss: 1709.6649 - loglik: -1.7090e+03 - logprior: -6.3662e-01
Fitted a model with MAP estimate = -1705.3964
expansions: []
discards: [  0   2 147 152 231 322 324 349]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 81s - loss: 1714.1578 - loglik: -1.7122e+03 - logprior: -1.9956e+00
Epoch 2/2
39/39 - 80s - loss: 1709.4327 - loglik: -1.7094e+03 - logprior: 0.0165
Fitted a model with MAP estimate = -1705.5064
expansions: [(0, 23)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 399 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 106s - loss: 1711.1384 - loglik: -1.7095e+03 - logprior: -1.6662e+00
Epoch 2/10
39/39 - 103s - loss: 1707.4651 - loglik: -1.7070e+03 - logprior: -4.7120e-01
Epoch 3/10
39/39 - 101s - loss: 1704.9453 - loglik: -1.7045e+03 - logprior: -4.0983e-01
Epoch 4/10
39/39 - 92s - loss: 1701.5369 - loglik: -1.7014e+03 - logprior: -1.7019e-01
Epoch 5/10
39/39 - 93s - loss: 1697.0762 - loglik: -1.6971e+03 - logprior: -1.0011e-02
Epoch 6/10
39/39 - 89s - loss: 1696.1807 - loglik: -1.6964e+03 - logprior: 0.1878
Epoch 7/10
39/39 - 91s - loss: 1688.4307 - loglik: -1.6887e+03 - logprior: 0.2720
Epoch 8/10
39/39 - 88s - loss: 1686.7950 - loglik: -1.6872e+03 - logprior: 0.3978
Epoch 9/10
39/39 - 98s - loss: 1685.5896 - loglik: -1.6860e+03 - logprior: 0.4145
Epoch 10/10
39/39 - 105s - loss: 1681.6547 - loglik: -1.6824e+03 - logprior: 0.7671
Fitted a model with MAP estimate = -1675.4833
Time for alignment: 2314.5515
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 60s - loss: 1950.9596 - loglik: -1.9496e+03 - logprior: -1.3601e+00
Epoch 2/10
39/39 - 55s - loss: 1773.8363 - loglik: -1.7722e+03 - logprior: -1.6031e+00
Epoch 3/10
39/39 - 61s - loss: 1757.6252 - loglik: -1.7560e+03 - logprior: -1.5834e+00
Epoch 4/10
39/39 - 55s - loss: 1752.7041 - loglik: -1.7512e+03 - logprior: -1.4960e+00
Epoch 5/10
39/39 - 50s - loss: 1750.5813 - loglik: -1.7491e+03 - logprior: -1.4895e+00
Epoch 6/10
39/39 - 57s - loss: 1747.4313 - loglik: -1.7459e+03 - logprior: -1.4854e+00
Epoch 7/10
39/39 - 56s - loss: 1743.5291 - loglik: -1.7420e+03 - logprior: -1.4747e+00
Epoch 8/10
39/39 - 51s - loss: 1740.1627 - loglik: -1.7386e+03 - logprior: -1.5024e+00
Epoch 9/10
39/39 - 56s - loss: 1738.3607 - loglik: -1.7366e+03 - logprior: -1.7837e+00
Epoch 10/10
39/39 - 63s - loss: 1737.3184 - loglik: -1.7356e+03 - logprior: -1.6987e+00
Fitted a model with MAP estimate = -1734.8031
expansions: [(0, 4), (36, 1), (42, 1), (45, 1), (60, 1), (62, 2), (67, 1), (74, 1), (80, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (94, 1), (113, 1), (114, 1), (119, 1), (123, 2), (125, 3), (127, 1), (142, 1), (144, 1), (146, 1), (148, 1), (155, 1), (158, 1), (160, 2), (161, 1), (162, 1), (164, 1), (168, 1), (186, 1), (188, 1), (189, 1), (190, 1), (193, 1), (205, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (229, 1), (230, 1), (231, 3), (232, 1), (256, 1), (257, 3), (258, 2), (259, 2), (260, 1), (279, 2), (281, 1), (282, 1), (284, 1), (285, 1), (289, 3), (296, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 383 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 99s - loss: 1737.5773 - loglik: -1.7353e+03 - logprior: -2.2583e+00
Epoch 2/2
39/39 - 98s - loss: 1709.7474 - loglik: -1.7089e+03 - logprior: -8.2740e-01
Fitted a model with MAP estimate = -1705.4626
expansions: []
discards: [  0   2 147 152 321 348 363]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 92s - loss: 1713.8041 - loglik: -1.7121e+03 - logprior: -1.6618e+00
Epoch 2/2
39/39 - 93s - loss: 1708.7268 - loglik: -1.7087e+03 - logprior: -5.9216e-02
Fitted a model with MAP estimate = -1705.3834
expansions: [(1, 1)]
discards: [99]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 89s - loss: 1710.7640 - loglik: -1.7096e+03 - logprior: -1.1429e+00
Epoch 2/10
39/39 - 77s - loss: 1706.4551 - loglik: -1.7065e+03 - logprior: 0.0176
Epoch 3/10
39/39 - 83s - loss: 1704.8409 - loglik: -1.7051e+03 - logprior: 0.2334
Epoch 4/10
39/39 - 82s - loss: 1701.3909 - loglik: -1.7018e+03 - logprior: 0.3641
Epoch 5/10
39/39 - 87s - loss: 1698.4860 - loglik: -1.6991e+03 - logprior: 0.6202
Epoch 6/10
39/39 - 74s - loss: 1695.5300 - loglik: -1.6962e+03 - logprior: 0.6786
Epoch 7/10
39/39 - 68s - loss: 1690.6454 - loglik: -1.6914e+03 - logprior: 0.7332
Epoch 8/10
39/39 - 69s - loss: 1685.6226 - loglik: -1.6865e+03 - logprior: 0.8478
Epoch 9/10
39/39 - 71s - loss: 1683.4247 - loglik: -1.6844e+03 - logprior: 1.0056
Epoch 10/10
39/39 - 70s - loss: 1684.2935 - loglik: -1.6854e+03 - logprior: 1.1334
Fitted a model with MAP estimate = -1680.6700
Time for alignment: 2160.7207
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 1949.4844 - loglik: -1.9481e+03 - logprior: -1.3910e+00
Epoch 2/10
39/39 - 47s - loss: 1772.2494 - loglik: -1.7706e+03 - logprior: -1.6652e+00
Epoch 3/10
39/39 - 49s - loss: 1756.1931 - loglik: -1.7545e+03 - logprior: -1.6433e+00
Epoch 4/10
39/39 - 48s - loss: 1751.3341 - loglik: -1.7497e+03 - logprior: -1.6189e+00
Epoch 5/10
39/39 - 47s - loss: 1748.7949 - loglik: -1.7472e+03 - logprior: -1.6057e+00
Epoch 6/10
39/39 - 47s - loss: 1744.7632 - loglik: -1.7432e+03 - logprior: -1.5628e+00
Epoch 7/10
39/39 - 49s - loss: 1742.2588 - loglik: -1.7407e+03 - logprior: -1.5827e+00
Epoch 8/10
39/39 - 49s - loss: 1737.9940 - loglik: -1.7364e+03 - logprior: -1.6000e+00
Epoch 9/10
39/39 - 48s - loss: 1737.3496 - loglik: -1.7355e+03 - logprior: -1.8368e+00
Epoch 10/10
39/39 - 47s - loss: 1735.0059 - loglik: -1.7332e+03 - logprior: -1.7763e+00
Fitted a model with MAP estimate = -1732.6845
expansions: [(0, 4), (24, 1), (42, 1), (45, 1), (60, 1), (62, 2), (66, 1), (72, 1), (73, 1), (79, 1), (80, 1), (82, 2), (83, 1), (84, 1), (90, 1), (94, 1), (103, 1), (112, 1), (113, 1), (118, 1), (122, 2), (125, 2), (127, 1), (142, 1), (144, 1), (146, 1), (149, 1), (155, 1), (158, 1), (160, 1), (162, 1), (163, 1), (164, 1), (165, 1), (168, 1), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (220, 1), (229, 1), (230, 1), (231, 3), (233, 1), (256, 1), (257, 3), (258, 2), (259, 2), (260, 1), (279, 2), (280, 1), (281, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 385 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 75s - loss: 1737.0221 - loglik: -1.7347e+03 - logprior: -2.3128e+00
Epoch 2/2
39/39 - 72s - loss: 1708.8772 - loglik: -1.7081e+03 - logprior: -8.0139e-01
Fitted a model with MAP estimate = -1704.6726
expansions: []
discards: [  0   2 147 152 231 322 349 364 365]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 62s - loss: 1713.9706 - loglik: -1.7122e+03 - logprior: -1.7368e+00
Epoch 2/2
39/39 - 56s - loss: 1708.7123 - loglik: -1.7086e+03 - logprior: -6.5351e-02
Fitted a model with MAP estimate = -1705.1631
expansions: [(1, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 377 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 65s - loss: 1709.8293 - loglik: -1.7086e+03 - logprior: -1.2385e+00
Epoch 2/10
39/39 - 66s - loss: 1705.8853 - loglik: -1.7060e+03 - logprior: 0.0808
Epoch 3/10
39/39 - 64s - loss: 1704.0979 - loglik: -1.7044e+03 - logprior: 0.3200
Epoch 4/10
39/39 - 65s - loss: 1699.9578 - loglik: -1.7004e+03 - logprior: 0.4852
Epoch 5/10
39/39 - 65s - loss: 1697.9946 - loglik: -1.6986e+03 - logprior: 0.6466
Epoch 6/10
39/39 - 64s - loss: 1694.2560 - loglik: -1.6950e+03 - logprior: 0.7694
Epoch 7/10
39/39 - 64s - loss: 1689.1322 - loglik: -1.6899e+03 - logprior: 0.7710
Epoch 8/10
39/39 - 65s - loss: 1686.1665 - loglik: -1.6870e+03 - logprior: 0.8877
Epoch 9/10
39/39 - 67s - loss: 1684.4882 - loglik: -1.6855e+03 - logprior: 1.0371
Epoch 10/10
39/39 - 65s - loss: 1680.2495 - loglik: -1.6814e+03 - logprior: 1.1660
Fitted a model with MAP estimate = -1678.1605
Time for alignment: 1696.7360
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 1948.5765 - loglik: -1.9472e+03 - logprior: -1.3482e+00
Epoch 2/10
39/39 - 46s - loss: 1773.3812 - loglik: -1.7719e+03 - logprior: -1.5130e+00
Epoch 3/10
39/39 - 49s - loss: 1759.6134 - loglik: -1.7581e+03 - logprior: -1.5453e+00
Epoch 4/10
39/39 - 50s - loss: 1754.3531 - loglik: -1.7528e+03 - logprior: -1.5202e+00
Epoch 5/10
39/39 - 51s - loss: 1751.5259 - loglik: -1.7500e+03 - logprior: -1.5676e+00
Epoch 6/10
39/39 - 48s - loss: 1748.7090 - loglik: -1.7471e+03 - logprior: -1.6188e+00
Epoch 7/10
39/39 - 49s - loss: 1744.2603 - loglik: -1.7427e+03 - logprior: -1.5623e+00
Epoch 8/10
39/39 - 51s - loss: 1741.2068 - loglik: -1.7396e+03 - logprior: -1.5988e+00
Epoch 9/10
39/39 - 49s - loss: 1740.0953 - loglik: -1.7383e+03 - logprior: -1.7499e+00
Epoch 10/10
39/39 - 50s - loss: 1737.8630 - loglik: -1.7361e+03 - logprior: -1.6953e+00
Fitted a model with MAP estimate = -1736.1170
expansions: [(0, 4), (36, 1), (46, 1), (56, 1), (62, 2), (67, 1), (73, 1), (83, 1), (84, 3), (85, 1), (86, 1), (92, 1), (93, 1), (104, 1), (114, 1), (115, 1), (120, 1), (124, 2), (126, 3), (128, 1), (143, 1), (145, 1), (147, 1), (150, 1), (156, 1), (159, 1), (161, 1), (163, 1), (164, 1), (166, 2), (169, 1), (182, 1), (186, 1), (188, 1), (190, 1), (194, 1), (205, 1), (206, 1), (208, 1), (209, 2), (210, 2), (228, 1), (229, 1), (230, 1), (231, 4), (233, 1), (256, 1), (257, 1), (258, 1), (259, 3), (260, 2), (261, 1), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (287, 1), (289, 1), (296, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 383 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 78s - loss: 1736.0771 - loglik: -1.7339e+03 - logprior: -2.1332e+00
Epoch 2/2
39/39 - 79s - loss: 1708.9084 - loglik: -1.7082e+03 - logprior: -6.7801e-01
Fitted a model with MAP estimate = -1705.1183
expansions: []
discards: [  0 147 152 256 287 327]
Re-initialized the encoder parameters.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 72s - loss: 1713.3009 - loglik: -1.7116e+03 - logprior: -1.7258e+00
Epoch 2/2
39/39 - 60s - loss: 1708.0909 - loglik: -1.7081e+03 - logprior: 0.0012
Fitted a model with MAP estimate = -1704.5209
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 377 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 59s - loss: 1709.5916 - loglik: -1.7084e+03 - logprior: -1.2352e+00
Epoch 2/10
39/39 - 59s - loss: 1705.6819 - loglik: -1.7057e+03 - logprior: 0.0059
Epoch 3/10
39/39 - 63s - loss: 1702.6116 - loglik: -1.7028e+03 - logprior: 0.2382
Epoch 4/10
39/39 - 64s - loss: 1701.9718 - loglik: -1.7023e+03 - logprior: 0.3348
Epoch 5/10
39/39 - 63s - loss: 1697.9882 - loglik: -1.6985e+03 - logprior: 0.5159
Epoch 6/10
39/39 - 64s - loss: 1692.6250 - loglik: -1.6933e+03 - logprior: 0.6403
Epoch 7/10
39/39 - 65s - loss: 1690.3359 - loglik: -1.6911e+03 - logprior: 0.7759
Epoch 8/10
39/39 - 66s - loss: 1685.6591 - loglik: -1.6866e+03 - logprior: 0.9178
Epoch 9/10
39/39 - 65s - loss: 1684.0128 - loglik: -1.6849e+03 - logprior: 0.9525
Epoch 10/10
39/39 - 69s - loss: 1681.0259 - loglik: -1.6821e+03 - logprior: 1.1410
Fitted a model with MAP estimate = -1678.7126
Time for alignment: 1727.9827
Computed alignments with likelihoods: ['-1681.5900', '-1675.4833', '-1680.6700', '-1678.1605', '-1678.7126']
Best model has likelihood: -1675.4833  (prior= 0.7342 )
time for generating output: 0.3377
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00476.projection.fasta
SP score = 0.9408843125378559
Training of 5 independent models on file PF02836.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6f71f1550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6d4d812e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff67feae250>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 1353.2733 - loglik: -1.3514e+03 - logprior: -1.8499e+00
Epoch 2/10
39/39 - 20s - loss: 1262.7391 - loglik: -1.2613e+03 - logprior: -1.4350e+00
Epoch 3/10
39/39 - 20s - loss: 1254.0269 - loglik: -1.2525e+03 - logprior: -1.5213e+00
Epoch 4/10
39/39 - 20s - loss: 1252.6526 - loglik: -1.2511e+03 - logprior: -1.5130e+00
Epoch 5/10
39/39 - 20s - loss: 1250.5215 - loglik: -1.2490e+03 - logprior: -1.5242e+00
Epoch 6/10
39/39 - 20s - loss: 1249.4458 - loglik: -1.2479e+03 - logprior: -1.5335e+00
Epoch 7/10
39/39 - 20s - loss: 1247.0818 - loglik: -1.2455e+03 - logprior: -1.5725e+00
Epoch 8/10
39/39 - 19s - loss: 1243.1592 - loglik: -1.2415e+03 - logprior: -1.6049e+00
Epoch 9/10
39/39 - 19s - loss: 1235.9797 - loglik: -1.2343e+03 - logprior: -1.6851e+00
Epoch 10/10
39/39 - 19s - loss: 1203.7189 - loglik: -1.2018e+03 - logprior: -1.8404e+00
Fitted a model with MAP estimate = -1112.4893
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 121 122 131 147 149 150 151 152
 153 154 163 164 165 166 167 170 171 172 173 188 189]
Re-initialized the encoder parameters.
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 1451.9244 - loglik: -1.4480e+03 - logprior: -3.9460e+00
Epoch 2/2
19/19 - 4s - loss: 1434.0197 - loglik: -1.4323e+03 - logprior: -1.7409e+00
Fitted a model with MAP estimate = -1430.5754
expansions: [(0, 151), (73, 78)]
discards: [ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49
 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 1345.0209 - loglik: -1.3430e+03 - logprior: -2.0576e+00
Epoch 2/2
39/39 - 23s - loss: 1264.1174 - loglik: -1.2629e+03 - logprior: -1.2186e+00
Fitted a model with MAP estimate = -1257.9300
expansions: [(6, 1), (29, 1), (34, 1), (88, 1), (90, 1), (190, 9), (192, 1), (231, 7)]
discards: [  0  93  94  95  96  97  98  99 100 101 102 103 104 105 106 129 130 131
 132 183 184 185 186 187 188 199 200 201 202 203 204 205 206 207 208 209
 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227
 228 229 230]
Re-initialized the encoder parameters.
Fitting a model of length 196 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1277.4415 - loglik: -1.2749e+03 - logprior: -2.5510e+00
Epoch 2/10
39/39 - 18s - loss: 1268.0801 - loglik: -1.2674e+03 - logprior: -6.5842e-01
Epoch 3/10
39/39 - 18s - loss: 1265.5464 - loglik: -1.2651e+03 - logprior: -4.2564e-01
Epoch 4/10
39/39 - 19s - loss: 1264.6450 - loglik: -1.2642e+03 - logprior: -4.0178e-01
Epoch 5/10
39/39 - 19s - loss: 1263.1908 - loglik: -1.2628e+03 - logprior: -3.7408e-01
Epoch 6/10
39/39 - 20s - loss: 1262.4026 - loglik: -1.2621e+03 - logprior: -3.4087e-01
Epoch 7/10
39/39 - 20s - loss: 1259.2137 - loglik: -1.2589e+03 - logprior: -3.2145e-01
Epoch 8/10
39/39 - 19s - loss: 1254.1851 - loglik: -1.2538e+03 - logprior: -3.1938e-01
Epoch 9/10
39/39 - 18s - loss: 1236.6355 - loglik: -1.2358e+03 - logprior: -7.7355e-01
Epoch 10/10
39/39 - 18s - loss: 1113.0474 - loglik: -1.1085e+03 - logprior: -4.5198e+00
Fitted a model with MAP estimate = -1043.6374
Time for alignment: 527.7629
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 1353.9362 - loglik: -1.3521e+03 - logprior: -1.8268e+00
Epoch 2/10
39/39 - 20s - loss: 1265.3323 - loglik: -1.2639e+03 - logprior: -1.3975e+00
Epoch 3/10
39/39 - 20s - loss: 1256.7035 - loglik: -1.2552e+03 - logprior: -1.4559e+00
Epoch 4/10
39/39 - 19s - loss: 1254.8308 - loglik: -1.2534e+03 - logprior: -1.4565e+00
Epoch 5/10
39/39 - 20s - loss: 1252.8287 - loglik: -1.2513e+03 - logprior: -1.4884e+00
Epoch 6/10
39/39 - 20s - loss: 1251.4413 - loglik: -1.2499e+03 - logprior: -1.4920e+00
Epoch 7/10
39/39 - 20s - loss: 1250.1843 - loglik: -1.2487e+03 - logprior: -1.4939e+00
Epoch 8/10
39/39 - 20s - loss: 1247.0013 - loglik: -1.2455e+03 - logprior: -1.5279e+00
Epoch 9/10
39/39 - 21s - loss: 1243.1711 - loglik: -1.2416e+03 - logprior: -1.5690e+00
Epoch 10/10
39/39 - 22s - loss: 1231.3578 - loglik: -1.2296e+03 - logprior: -1.6794e+00
Fitted a model with MAP estimate = -1215.5322
expansions: [(4, 1), (31, 1), (40, 1), (83, 3), (84, 6), (85, 1), (86, 1), (90, 1), (91, 1), (93, 1), (117, 7), (119, 2), (120, 1), (121, 2), (128, 1), (129, 1), (131, 7), (134, 1), (138, 2), (141, 3), (142, 4), (147, 2), (154, 1), (157, 1), (161, 1), (170, 1), (172, 1), (173, 5), (174, 1), (184, 1), (185, 1), (187, 1), (189, 1), (193, 1), (200, 1), (201, 1)]
discards: [  1   2  77  78  79 162 163 164 165 166 167 168 207 208]
Re-initialized the encoder parameters.
Fitting a model of length 266 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 37s - loss: 1296.5736 - loglik: -1.2945e+03 - logprior: -2.0374e+00
Epoch 2/2
39/39 - 34s - loss: 1245.0073 - loglik: -1.2440e+03 - logprior: -1.0236e+00
Fitted a model with MAP estimate = -1240.9883
expansions: [(3, 2), (86, 4), (179, 2), (188, 1), (206, 8), (217, 1), (266, 7)]
discards: [  0  78  79  80  83  84 135 136 137 155 157 164 174 175 176 180 192 193
 194 195 196 197 198 208 209 210 211 212 213 214 215 223 225 226 227 228
 229 233 234 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253
 254 255 256 257 258 259 260 261 262 263 264 265]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 1267.3856 - loglik: -1.2648e+03 - logprior: -2.5518e+00
Epoch 2/2
39/39 - 24s - loss: 1256.7087 - loglik: -1.2553e+03 - logprior: -1.3737e+00
Fitted a model with MAP estimate = -1254.4123
expansions: [(4, 1), (170, 1), (171, 1), (172, 1), (194, 4), (205, 6), (209, 1), (225, 15)]
discards: [  0   1  79  80  81  82  83  84 195 196 197 198 199 200 201 202 203 210
 211 219 220 221 222 223 224]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 1263.5790 - loglik: -1.2614e+03 - logprior: -2.2202e+00
Epoch 2/10
39/39 - 23s - loss: 1252.3776 - loglik: -1.2520e+03 - logprior: -3.3951e-01
Epoch 3/10
39/39 - 22s - loss: 1249.6052 - loglik: -1.2494e+03 - logprior: -2.2731e-01
Epoch 4/10
39/39 - 22s - loss: 1248.9010 - loglik: -1.2487e+03 - logprior: -1.7587e-01
Epoch 5/10
39/39 - 23s - loss: 1248.3218 - loglik: -1.2482e+03 - logprior: -1.2210e-01
Epoch 6/10
39/39 - 23s - loss: 1244.5623 - loglik: -1.2445e+03 - logprior: -5.8497e-02
Epoch 7/10
39/39 - 22s - loss: 1243.8817 - loglik: -1.2439e+03 - logprior: -1.6138e-02
Epoch 8/10
39/39 - 23s - loss: 1241.5321 - loglik: -1.2415e+03 - logprior: 0.0330
Epoch 9/10
39/39 - 24s - loss: 1232.3170 - loglik: -1.2323e+03 - logprior: 0.0077
Epoch 10/10
39/39 - 25s - loss: 1188.0651 - loglik: -1.1872e+03 - logprior: -8.2551e-01
Fitted a model with MAP estimate = -1108.2076
Time for alignment: 678.0818
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 1351.9852 - loglik: -1.3502e+03 - logprior: -1.8304e+00
Epoch 2/10
39/39 - 22s - loss: 1260.9094 - loglik: -1.2595e+03 - logprior: -1.4137e+00
Epoch 3/10
39/39 - 22s - loss: 1253.7928 - loglik: -1.2523e+03 - logprior: -1.4526e+00
Epoch 4/10
39/39 - 23s - loss: 1252.1917 - loglik: -1.2508e+03 - logprior: -1.4104e+00
Epoch 5/10
39/39 - 22s - loss: 1249.9999 - loglik: -1.2486e+03 - logprior: -1.4253e+00
Epoch 6/10
39/39 - 22s - loss: 1247.9752 - loglik: -1.2465e+03 - logprior: -1.4516e+00
Epoch 7/10
39/39 - 23s - loss: 1247.0925 - loglik: -1.2456e+03 - logprior: -1.4503e+00
Epoch 8/10
39/39 - 22s - loss: 1243.2841 - loglik: -1.2418e+03 - logprior: -1.4740e+00
Epoch 9/10
39/39 - 22s - loss: 1238.8463 - loglik: -1.2373e+03 - logprior: -1.5271e+00
Epoch 10/10
39/39 - 22s - loss: 1221.8230 - loglik: -1.2201e+03 - logprior: -1.6397e+00
Fitted a model with MAP estimate = -1200.4049
expansions: [(33, 1), (89, 1), (90, 1), (91, 1), (94, 1), (97, 1), (117, 5), (119, 2), (120, 1), (121, 2), (129, 1), (136, 1), (138, 1), (142, 6), (143, 1), (147, 1), (157, 1), (158, 1), (161, 4), (170, 1), (172, 1), (174, 1), (184, 1), (185, 1), (187, 1), (189, 1), (193, 1), (200, 1), (201, 1)]
discards: [163 164 165 166 167 168 207]
Re-initialized the encoder parameters.
Fitting a model of length 248 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 1311.8558 - loglik: -1.3096e+03 - logprior: -2.2948e+00
Epoch 2/2
39/39 - 26s - loss: 1249.8811 - loglik: -1.2487e+03 - logprior: -1.1643e+00
Fitted a model with MAP estimate = -1245.7459
expansions: [(4, 1), (6, 1), (85, 5), (92, 1), (126, 1), (131, 1), (144, 1), (148, 6), (164, 2), (165, 7), (248, 23)]
discards: [  0  80  81  82  83  93 128 129 132 157 158 159 160 178 179 180 181 182
 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200
 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218
 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236
 237 238 239 240 241 242 243 244 245 246 247]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 1274.2698 - loglik: -1.2718e+03 - logprior: -2.4524e+00
Epoch 2/2
39/39 - 20s - loss: 1262.1786 - loglik: -1.2608e+03 - logprior: -1.3626e+00
Fitted a model with MAP estimate = -1259.1917
expansions: [(3, 1), (4, 1), (86, 5), (87, 2), (200, 1), (204, 5), (206, 2), (213, 2), (214, 19)]
discards: [  0   1  82  83  84 152 175 180 190 191 192 193 194 195 196]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 1260.8970 - loglik: -1.2584e+03 - logprior: -2.4649e+00
Epoch 2/10
39/39 - 24s - loss: 1249.8909 - loglik: -1.2489e+03 - logprior: -9.4239e-01
Epoch 3/10
39/39 - 25s - loss: 1247.0365 - loglik: -1.2468e+03 - logprior: -2.5307e-01
Epoch 4/10
39/39 - 25s - loss: 1245.9088 - loglik: -1.2457e+03 - logprior: -2.0870e-01
Epoch 5/10
39/39 - 26s - loss: 1244.3353 - loglik: -1.2442e+03 - logprior: -1.4927e-01
Epoch 6/10
39/39 - 26s - loss: 1241.9323 - loglik: -1.2418e+03 - logprior: -1.0452e-01
Epoch 7/10
39/39 - 26s - loss: 1241.4764 - loglik: -1.2414e+03 - logprior: -6.7104e-02
Epoch 8/10
39/39 - 25s - loss: 1238.0137 - loglik: -1.2380e+03 - logprior: -1.8116e-02
Epoch 9/10
39/39 - 25s - loss: 1232.5288 - loglik: -1.2325e+03 - logprior: -7.0467e-03
Epoch 10/10
39/39 - 25s - loss: 1206.4418 - loglik: -1.2062e+03 - logprior: -2.5291e-01
Fitted a model with MAP estimate = -1134.5477
Time for alignment: 683.5888
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 1356.4761 - loglik: -1.3546e+03 - logprior: -1.8595e+00
Epoch 2/10
39/39 - 19s - loss: 1266.2134 - loglik: -1.2648e+03 - logprior: -1.3788e+00
Epoch 3/10
39/39 - 20s - loss: 1257.7451 - loglik: -1.2564e+03 - logprior: -1.3832e+00
Epoch 4/10
39/39 - 20s - loss: 1255.6632 - loglik: -1.2543e+03 - logprior: -1.3869e+00
Epoch 5/10
39/39 - 20s - loss: 1253.6901 - loglik: -1.2523e+03 - logprior: -1.3530e+00
Epoch 6/10
39/39 - 20s - loss: 1253.4835 - loglik: -1.2521e+03 - logprior: -1.3570e+00
Epoch 7/10
39/39 - 21s - loss: 1250.5892 - loglik: -1.2492e+03 - logprior: -1.3718e+00
Epoch 8/10
39/39 - 21s - loss: 1246.3031 - loglik: -1.2449e+03 - logprior: -1.4083e+00
Epoch 9/10
39/39 - 21s - loss: 1237.3993 - loglik: -1.2359e+03 - logprior: -1.5191e+00
Epoch 10/10
39/39 - 19s - loss: 1200.2493 - loglik: -1.1982e+03 - logprior: -2.0000e+00
Fitted a model with MAP estimate = -1105.2079
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 118 119 131 132 133 134 144 147 148 149 150 151
 152 153 154 161 162 163 164 165 166 167 168 185 186 187 188 189]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 1449.9484 - loglik: -1.4465e+03 - logprior: -3.4278e+00
Epoch 2/2
19/19 - 4s - loss: 1434.4226 - loglik: -1.4335e+03 - logprior: -9.3475e-01
Fitted a model with MAP estimate = -1430.2199
expansions: [(0, 116), (1, 2), (70, 97)]
discards: [ 8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31
 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55
 56 57 58 59 60 61 62 63 64 65 66 67 68 69]
Re-initialized the encoder parameters.
Fitting a model of length 223 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1336.0243 - loglik: -1.3338e+03 - logprior: -2.2664e+00
Epoch 2/2
39/39 - 17s - loss: 1263.6221 - loglik: -1.2628e+03 - logprior: -7.7886e-01
Fitted a model with MAP estimate = -1257.9394
expansions: [(0, 2), (26, 1), (32, 1), (48, 1), (82, 2), (83, 3), (118, 2), (120, 13), (121, 4), (130, 19), (223, 3)]
discards: [ 76  77  78 157 158 159 160 173 195 196 197 198 199 200 201 202 203 204
 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1266.1681 - loglik: -1.2636e+03 - logprior: -2.5489e+00
Epoch 2/10
39/39 - 18s - loss: 1253.5532 - loglik: -1.2527e+03 - logprior: -8.7331e-01
Epoch 3/10
39/39 - 19s - loss: 1250.7899 - loglik: -1.2500e+03 - logprior: -7.9618e-01
Epoch 4/10
39/39 - 19s - loss: 1250.0795 - loglik: -1.2493e+03 - logprior: -7.6026e-01
Epoch 5/10
39/39 - 19s - loss: 1248.8450 - loglik: -1.2481e+03 - logprior: -7.2330e-01
Epoch 6/10
39/39 - 19s - loss: 1247.1001 - loglik: -1.2464e+03 - logprior: -6.9683e-01
Epoch 7/10
39/39 - 19s - loss: 1244.8538 - loglik: -1.2442e+03 - logprior: -6.6681e-01
Epoch 8/10
39/39 - 19s - loss: 1243.7804 - loglik: -1.2431e+03 - logprior: -6.4141e-01
Epoch 9/10
39/39 - 19s - loss: 1238.2804 - loglik: -1.2376e+03 - logprior: -6.3724e-01
Epoch 10/10
39/39 - 19s - loss: 1214.7433 - loglik: -1.2138e+03 - logprior: -8.7521e-01
Fitted a model with MAP estimate = -1150.9965
Time for alignment: 510.6686
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1355.5414 - loglik: -1.3537e+03 - logprior: -1.8321e+00
Epoch 2/10
39/39 - 16s - loss: 1264.5360 - loglik: -1.2630e+03 - logprior: -1.5025e+00
Epoch 3/10
39/39 - 16s - loss: 1255.7509 - loglik: -1.2542e+03 - logprior: -1.5655e+00
Epoch 4/10
39/39 - 16s - loss: 1254.1970 - loglik: -1.2526e+03 - logprior: -1.5439e+00
Epoch 5/10
39/39 - 16s - loss: 1251.3898 - loglik: -1.2498e+03 - logprior: -1.5600e+00
Epoch 6/10
39/39 - 16s - loss: 1251.1036 - loglik: -1.2495e+03 - logprior: -1.5511e+00
Epoch 7/10
39/39 - 16s - loss: 1249.6292 - loglik: -1.2481e+03 - logprior: -1.5594e+00
Epoch 8/10
39/39 - 16s - loss: 1246.0688 - loglik: -1.2445e+03 - logprior: -1.5871e+00
Epoch 9/10
39/39 - 17s - loss: 1240.7256 - loglik: -1.2390e+03 - logprior: -1.6687e+00
Epoch 10/10
39/39 - 17s - loss: 1218.1833 - loglik: -1.2164e+03 - logprior: -1.7692e+00
Fitted a model with MAP estimate = -1154.5701
expansions: [(44, 1), (89, 1), (94, 1), (161, 1), (194, 1), (199, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  52  77  78  79 121 122 123 124 125 135 136 137
 138 148 149 150 151 152 153 154 155 162 163 164 165 166 167 168 169 182
 183 188 189 190 191]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 1396.5944 - loglik: -1.3939e+03 - logprior: -2.6513e+00
Epoch 2/2
39/39 - 10s - loss: 1338.9368 - loglik: -1.3381e+03 - logprior: -8.8328e-01
Fitted a model with MAP estimate = -1329.1432
expansions: [(0, 40), (11, 1), (38, 8), (47, 1), (73, 1), (74, 45), (75, 3), (76, 3), (141, 19)]
discards: [ 78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95
  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113
 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131
 132 133 134 135 136 137 138 139 140]
Re-initialized the encoder parameters.
Fitting a model of length 199 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 1307.2528 - loglik: -1.3053e+03 - logprior: -1.9454e+00
Epoch 2/2
39/39 - 15s - loss: 1276.4550 - loglik: -1.2757e+03 - logprior: -7.3292e-01
Fitted a model with MAP estimate = -1273.2032
expansions: [(0, 2), (2, 1), (5, 1), (23, 1), (24, 1), (82, 3), (126, 3), (179, 1), (199, 3)]
discards: [ 76  77  78 123 144 145 146 147 148 149 150 172 180 181 182 183 184 187
 194 195]
Re-initialized the encoder parameters.
Fitting a model of length 195 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 1284.6166 - loglik: -1.2823e+03 - logprior: -2.3521e+00
Epoch 2/10
39/39 - 14s - loss: 1273.0791 - loglik: -1.2720e+03 - logprior: -1.0999e+00
Epoch 3/10
39/39 - 14s - loss: 1271.6398 - loglik: -1.2705e+03 - logprior: -1.1353e+00
Epoch 4/10
39/39 - 14s - loss: 1270.4324 - loglik: -1.2693e+03 - logprior: -1.1326e+00
Epoch 5/10
39/39 - 14s - loss: 1268.4165 - loglik: -1.2673e+03 - logprior: -1.0996e+00
Epoch 6/10
39/39 - 14s - loss: 1266.3959 - loglik: -1.2653e+03 - logprior: -1.0681e+00
Epoch 7/10
39/39 - 14s - loss: 1265.7924 - loglik: -1.2647e+03 - logprior: -1.0388e+00
Epoch 8/10
39/39 - 14s - loss: 1261.0332 - loglik: -1.2600e+03 - logprior: -1.0146e+00
Epoch 9/10
39/39 - 14s - loss: 1243.3677 - loglik: -1.2419e+03 - logprior: -1.4671e+00
Epoch 10/10
39/39 - 14s - loss: 1130.9117 - loglik: -1.1272e+03 - logprior: -3.7039e+00
Fitted a model with MAP estimate = -1052.4954
Time for alignment: 436.0801
Computed alignments with likelihoods: ['-1043.6374', '-1108.2076', '-1134.5477', '-1105.2079', '-1052.4954']
Best model has likelihood: -1043.6374  (prior= -4.1129 )
time for generating output: 0.2369
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02836.projection.fasta
SP score = 0.489216799091941
Training of 5 independent models on file PF02777.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6bb977070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6888a7550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff66d957190>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 558.7293 - loglik: -5.5562e+02 - logprior: -3.1138e+00
Epoch 2/10
19/19 - 2s - loss: 488.9306 - loglik: -4.8761e+02 - logprior: -1.3233e+00
Epoch 3/10
19/19 - 2s - loss: 466.1515 - loglik: -4.6449e+02 - logprior: -1.6657e+00
Epoch 4/10
19/19 - 2s - loss: 461.0658 - loglik: -4.5943e+02 - logprior: -1.6373e+00
Epoch 5/10
19/19 - 2s - loss: 459.4397 - loglik: -4.5786e+02 - logprior: -1.5734e+00
Epoch 6/10
19/19 - 2s - loss: 458.9492 - loglik: -4.5738e+02 - logprior: -1.5649e+00
Epoch 7/10
19/19 - 2s - loss: 458.1351 - loglik: -4.5660e+02 - logprior: -1.5273e+00
Epoch 8/10
19/19 - 2s - loss: 458.1531 - loglik: -4.5663e+02 - logprior: -1.5202e+00
Fitted a model with MAP estimate = -456.6422
expansions: [(4, 1), (6, 1), (8, 1), (9, 1), (14, 1), (16, 1), (19, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (44, 1), (46, 3), (47, 1), (69, 1), (70, 2), (71, 2), (72, 1), (77, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 455.7941 - loglik: -4.5286e+02 - logprior: -2.9355e+00
Epoch 2/2
19/19 - 3s - loss: 442.6853 - loglik: -4.4152e+02 - logprior: -1.1644e+00
Fitted a model with MAP estimate = -440.9970
expansions: []
discards: [42 60 91]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 445.7097 - loglik: -4.4281e+02 - logprior: -2.8949e+00
Epoch 2/2
19/19 - 3s - loss: 441.7436 - loglik: -4.4064e+02 - logprior: -1.1064e+00
Fitted a model with MAP estimate = -440.8831
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 448.4625 - loglik: -4.4455e+02 - logprior: -3.9076e+00
Epoch 2/10
19/19 - 3s - loss: 442.9132 - loglik: -4.4078e+02 - logprior: -2.1300e+00
Epoch 3/10
19/19 - 3s - loss: 442.6461 - loglik: -4.4066e+02 - logprior: -1.9814e+00
Epoch 4/10
19/19 - 3s - loss: 440.9484 - loglik: -4.3945e+02 - logprior: -1.5016e+00
Epoch 5/10
19/19 - 3s - loss: 440.4250 - loglik: -4.3947e+02 - logprior: -9.4897e-01
Epoch 6/10
19/19 - 3s - loss: 439.5097 - loglik: -4.3855e+02 - logprior: -9.5316e-01
Epoch 7/10
19/19 - 3s - loss: 438.7472 - loglik: -4.3782e+02 - logprior: -9.2577e-01
Epoch 8/10
19/19 - 3s - loss: 438.3185 - loglik: -4.3741e+02 - logprior: -9.0237e-01
Epoch 9/10
19/19 - 3s - loss: 437.5602 - loglik: -4.3666e+02 - logprior: -8.9534e-01
Epoch 10/10
19/19 - 3s - loss: 437.4561 - loglik: -4.3655e+02 - logprior: -8.9224e-01
Fitted a model with MAP estimate = -436.5970
Time for alignment: 82.3060
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 558.4521 - loglik: -5.5534e+02 - logprior: -3.1163e+00
Epoch 2/10
19/19 - 2s - loss: 488.4892 - loglik: -4.8716e+02 - logprior: -1.3316e+00
Epoch 3/10
19/19 - 2s - loss: 465.1753 - loglik: -4.6351e+02 - logprior: -1.6620e+00
Epoch 4/10
19/19 - 2s - loss: 461.7965 - loglik: -4.6020e+02 - logprior: -1.5929e+00
Epoch 5/10
19/19 - 2s - loss: 460.9655 - loglik: -4.5942e+02 - logprior: -1.5444e+00
Epoch 6/10
19/19 - 2s - loss: 459.3030 - loglik: -4.5776e+02 - logprior: -1.5374e+00
Epoch 7/10
19/19 - 2s - loss: 458.9965 - loglik: -4.5746e+02 - logprior: -1.5311e+00
Epoch 8/10
19/19 - 2s - loss: 457.6407 - loglik: -4.5611e+02 - logprior: -1.5291e+00
Epoch 9/10
19/19 - 2s - loss: 457.7436 - loglik: -4.5621e+02 - logprior: -1.5247e+00
Fitted a model with MAP estimate = -456.6810
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 458.5057 - loglik: -4.5459e+02 - logprior: -3.9193e+00
Epoch 2/2
19/19 - 3s - loss: 445.2710 - loglik: -4.4332e+02 - logprior: -1.9491e+00
Fitted a model with MAP estimate = -443.1357
expansions: [(0, 2)]
discards: [ 0 42 61 62]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 446.1736 - loglik: -4.4328e+02 - logprior: -2.8895e+00
Epoch 2/2
19/19 - 3s - loss: 441.1958 - loglik: -4.4011e+02 - logprior: -1.0839e+00
Fitted a model with MAP estimate = -440.3682
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 448.1890 - loglik: -4.4435e+02 - logprior: -3.8343e+00
Epoch 2/10
19/19 - 3s - loss: 442.4001 - loglik: -4.4097e+02 - logprior: -1.4289e+00
Epoch 3/10
19/19 - 3s - loss: 441.1098 - loglik: -4.4016e+02 - logprior: -9.5288e-01
Epoch 4/10
19/19 - 3s - loss: 439.9439 - loglik: -4.3902e+02 - logprior: -9.1800e-01
Epoch 5/10
19/19 - 3s - loss: 439.5620 - loglik: -4.3867e+02 - logprior: -8.8669e-01
Epoch 6/10
19/19 - 3s - loss: 439.1372 - loglik: -4.3827e+02 - logprior: -8.6798e-01
Epoch 7/10
19/19 - 3s - loss: 438.3541 - loglik: -4.3750e+02 - logprior: -8.4788e-01
Epoch 8/10
19/19 - 3s - loss: 437.3959 - loglik: -4.3654e+02 - logprior: -8.4480e-01
Epoch 9/10
19/19 - 3s - loss: 437.3500 - loglik: -4.3650e+02 - logprior: -8.3813e-01
Epoch 10/10
19/19 - 3s - loss: 435.9996 - loglik: -4.3516e+02 - logprior: -8.2648e-01
Fitted a model with MAP estimate = -435.9601
Time for alignment: 85.8704
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 558.8343 - loglik: -5.5571e+02 - logprior: -3.1210e+00
Epoch 2/10
19/19 - 2s - loss: 489.0347 - loglik: -4.8772e+02 - logprior: -1.3149e+00
Epoch 3/10
19/19 - 2s - loss: 466.0502 - loglik: -4.6440e+02 - logprior: -1.6486e+00
Epoch 4/10
19/19 - 2s - loss: 461.6452 - loglik: -4.6003e+02 - logprior: -1.6133e+00
Epoch 5/10
19/19 - 2s - loss: 460.1112 - loglik: -4.5854e+02 - logprior: -1.5692e+00
Epoch 6/10
19/19 - 2s - loss: 458.6366 - loglik: -4.5709e+02 - logprior: -1.5455e+00
Epoch 7/10
19/19 - 2s - loss: 457.8446 - loglik: -4.5631e+02 - logprior: -1.5265e+00
Epoch 8/10
19/19 - 2s - loss: 457.5063 - loglik: -4.5597e+02 - logprior: -1.5270e+00
Epoch 9/10
19/19 - 2s - loss: 456.4168 - loglik: -4.5487e+02 - logprior: -1.5374e+00
Epoch 10/10
19/19 - 2s - loss: 456.2893 - loglik: -4.5474e+02 - logprior: -1.5413e+00
Fitted a model with MAP estimate = -455.5030
expansions: [(4, 1), (6, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 3), (71, 1), (72, 1), (77, 1)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 460.7281 - loglik: -4.5687e+02 - logprior: -3.8566e+00
Epoch 2/2
19/19 - 3s - loss: 446.8139 - loglik: -4.4491e+02 - logprior: -1.9055e+00
Fitted a model with MAP estimate = -444.4675
expansions: [(0, 3)]
discards: [ 0 41 60 61 89]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 446.7457 - loglik: -4.4385e+02 - logprior: -2.8976e+00
Epoch 2/2
19/19 - 3s - loss: 441.1674 - loglik: -4.4008e+02 - logprior: -1.0905e+00
Fitted a model with MAP estimate = -440.4411
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 448.0440 - loglik: -4.4423e+02 - logprior: -3.8097e+00
Epoch 2/10
19/19 - 3s - loss: 442.8362 - loglik: -4.4140e+02 - logprior: -1.4390e+00
Epoch 3/10
19/19 - 3s - loss: 440.6825 - loglik: -4.3974e+02 - logprior: -9.4002e-01
Epoch 4/10
19/19 - 3s - loss: 440.4355 - loglik: -4.3952e+02 - logprior: -9.1465e-01
Epoch 5/10
19/19 - 3s - loss: 439.3650 - loglik: -4.3847e+02 - logprior: -8.9057e-01
Epoch 6/10
19/19 - 3s - loss: 438.8534 - loglik: -4.3799e+02 - logprior: -8.5828e-01
Epoch 7/10
19/19 - 3s - loss: 438.5335 - loglik: -4.3767e+02 - logprior: -8.5384e-01
Epoch 8/10
19/19 - 3s - loss: 437.9617 - loglik: -4.3712e+02 - logprior: -8.3559e-01
Epoch 9/10
19/19 - 3s - loss: 436.2338 - loglik: -4.3539e+02 - logprior: -8.3256e-01
Epoch 10/10
19/19 - 3s - loss: 436.8385 - loglik: -4.3600e+02 - logprior: -8.2440e-01
Fitted a model with MAP estimate = -435.9442
Time for alignment: 88.1837
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 558.6561 - loglik: -5.5554e+02 - logprior: -3.1180e+00
Epoch 2/10
19/19 - 2s - loss: 489.2725 - loglik: -4.8795e+02 - logprior: -1.3189e+00
Epoch 3/10
19/19 - 2s - loss: 466.2652 - loglik: -4.6461e+02 - logprior: -1.6578e+00
Epoch 4/10
19/19 - 2s - loss: 460.9983 - loglik: -4.5936e+02 - logprior: -1.6343e+00
Epoch 5/10
19/19 - 2s - loss: 459.2612 - loglik: -4.5768e+02 - logprior: -1.5783e+00
Epoch 6/10
19/19 - 2s - loss: 458.4673 - loglik: -4.5690e+02 - logprior: -1.5591e+00
Epoch 7/10
19/19 - 2s - loss: 457.7373 - loglik: -4.5620e+02 - logprior: -1.5302e+00
Epoch 8/10
19/19 - 2s - loss: 457.1140 - loglik: -4.5559e+02 - logprior: -1.5187e+00
Epoch 9/10
19/19 - 2s - loss: 456.4564 - loglik: -4.5492e+02 - logprior: -1.5265e+00
Epoch 10/10
19/19 - 2s - loss: 455.3362 - loglik: -4.5379e+02 - logprior: -1.5329e+00
Fitted a model with MAP estimate = -455.1347
expansions: [(4, 1), (6, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (44, 1), (46, 3), (47, 1), (69, 1), (70, 2), (71, 2), (72, 1), (77, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 455.9387 - loglik: -4.5299e+02 - logprior: -2.9463e+00
Epoch 2/2
19/19 - 3s - loss: 442.3432 - loglik: -4.4119e+02 - logprior: -1.1544e+00
Fitted a model with MAP estimate = -440.9583
expansions: []
discards: [42 60 91]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 445.5885 - loglik: -4.4270e+02 - logprior: -2.8848e+00
Epoch 2/2
19/19 - 3s - loss: 441.7169 - loglik: -4.4062e+02 - logprior: -1.0993e+00
Fitted a model with MAP estimate = -440.8723
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 448.4250 - loglik: -4.4451e+02 - logprior: -3.9147e+00
Epoch 2/10
19/19 - 3s - loss: 443.1765 - loglik: -4.4103e+02 - logprior: -2.1509e+00
Epoch 3/10
19/19 - 3s - loss: 442.2169 - loglik: -4.4023e+02 - logprior: -1.9820e+00
Epoch 4/10
19/19 - 3s - loss: 441.2076 - loglik: -4.3954e+02 - logprior: -1.6633e+00
Epoch 5/10
19/19 - 3s - loss: 440.2997 - loglik: -4.3931e+02 - logprior: -9.8445e-01
Epoch 6/10
19/19 - 3s - loss: 439.2416 - loglik: -4.3830e+02 - logprior: -9.3795e-01
Epoch 7/10
19/19 - 3s - loss: 439.7928 - loglik: -4.3888e+02 - logprior: -9.1130e-01
Fitted a model with MAP estimate = -438.5418
Time for alignment: 79.8747
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 558.5284 - loglik: -5.5541e+02 - logprior: -3.1143e+00
Epoch 2/10
19/19 - 2s - loss: 488.1859 - loglik: -4.8686e+02 - logprior: -1.3250e+00
Epoch 3/10
19/19 - 2s - loss: 465.5393 - loglik: -4.6387e+02 - logprior: -1.6730e+00
Epoch 4/10
19/19 - 2s - loss: 461.5140 - loglik: -4.5990e+02 - logprior: -1.6140e+00
Epoch 5/10
19/19 - 2s - loss: 461.3256 - loglik: -4.5978e+02 - logprior: -1.5389e+00
Epoch 6/10
19/19 - 2s - loss: 459.6436 - loglik: -4.5813e+02 - logprior: -1.5096e+00
Epoch 7/10
19/19 - 2s - loss: 458.8550 - loglik: -4.5736e+02 - logprior: -1.4886e+00
Epoch 8/10
19/19 - 2s - loss: 458.5593 - loglik: -4.5705e+02 - logprior: -1.4990e+00
Epoch 9/10
19/19 - 2s - loss: 457.6638 - loglik: -4.5615e+02 - logprior: -1.5071e+00
Epoch 10/10
19/19 - 2s - loss: 456.2061 - loglik: -4.5469e+02 - logprior: -1.5090e+00
Fitted a model with MAP estimate = -456.3436
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 4), (47, 1), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 459.3451 - loglik: -4.5542e+02 - logprior: -3.9214e+00
Epoch 2/2
19/19 - 3s - loss: 445.7318 - loglik: -4.4376e+02 - logprior: -1.9667e+00
Fitted a model with MAP estimate = -444.0239
expansions: [(0, 2)]
discards: [ 0 42 59]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 446.3501 - loglik: -4.4346e+02 - logprior: -2.8852e+00
Epoch 2/2
19/19 - 3s - loss: 441.8215 - loglik: -4.4073e+02 - logprior: -1.0923e+00
Fitted a model with MAP estimate = -440.7169
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 448.4394 - loglik: -4.4464e+02 - logprior: -3.8026e+00
Epoch 2/10
19/19 - 3s - loss: 442.7021 - loglik: -4.4131e+02 - logprior: -1.3901e+00
Epoch 3/10
19/19 - 3s - loss: 441.0452 - loglik: -4.4009e+02 - logprior: -9.5475e-01
Epoch 4/10
19/19 - 3s - loss: 440.5578 - loglik: -4.3965e+02 - logprior: -9.0287e-01
Epoch 5/10
19/19 - 3s - loss: 439.7217 - loglik: -4.3885e+02 - logprior: -8.7289e-01
Epoch 6/10
19/19 - 3s - loss: 439.2551 - loglik: -4.3839e+02 - logprior: -8.5977e-01
Epoch 7/10
19/19 - 3s - loss: 438.1557 - loglik: -4.3731e+02 - logprior: -8.4173e-01
Epoch 8/10
19/19 - 3s - loss: 437.9726 - loglik: -4.3714e+02 - logprior: -8.2759e-01
Epoch 9/10
19/19 - 3s - loss: 436.8752 - loglik: -4.3604e+02 - logprior: -8.3015e-01
Epoch 10/10
19/19 - 3s - loss: 436.3194 - loglik: -4.3548e+02 - logprior: -8.2907e-01
Fitted a model with MAP estimate = -435.9519
Time for alignment: 87.6803
Computed alignments with likelihoods: ['-436.5970', '-435.9601', '-435.9442', '-438.5418', '-435.9519']
Best model has likelihood: -435.9442  (prior= -0.8413 )
time for generating output: 0.1259
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02777.projection.fasta
SP score = 0.9268085106382978
Training of 5 independent models on file PF07654.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff663883ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6bb991610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff717cccd90>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 471.1640 - loglik: -4.6805e+02 - logprior: -3.1189e+00
Epoch 2/10
19/19 - 1s - loss: 434.1792 - loglik: -4.3291e+02 - logprior: -1.2717e+00
Epoch 3/10
19/19 - 1s - loss: 423.1257 - loglik: -4.2184e+02 - logprior: -1.2807e+00
Epoch 4/10
19/19 - 1s - loss: 420.8808 - loglik: -4.1961e+02 - logprior: -1.2697e+00
Epoch 5/10
19/19 - 1s - loss: 419.7433 - loglik: -4.1849e+02 - logprior: -1.2483e+00
Epoch 6/10
19/19 - 1s - loss: 419.0043 - loglik: -4.1776e+02 - logprior: -1.2370e+00
Epoch 7/10
19/19 - 1s - loss: 417.9825 - loglik: -4.1675e+02 - logprior: -1.2278e+00
Epoch 8/10
19/19 - 1s - loss: 416.2867 - loglik: -4.1505e+02 - logprior: -1.2298e+00
Epoch 9/10
19/19 - 1s - loss: 415.0710 - loglik: -4.1383e+02 - logprior: -1.2273e+00
Epoch 10/10
19/19 - 1s - loss: 412.4884 - loglik: -4.1124e+02 - logprior: -1.2287e+00
Fitted a model with MAP estimate = -409.8894
expansions: [(7, 2), (8, 3), (9, 2), (23, 1), (30, 2), (31, 1), (35, 2), (38, 1), (46, 1), (48, 1), (49, 1), (55, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 426.6552 - loglik: -4.2378e+02 - logprior: -2.8759e+00
Epoch 2/2
19/19 - 2s - loss: 414.2025 - loglik: -4.1304e+02 - logprior: -1.1628e+00
Fitted a model with MAP estimate = -413.0691
expansions: []
discards: [ 0 10 39 73]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 419.1854 - loglik: -4.1535e+02 - logprior: -3.8384e+00
Epoch 2/2
19/19 - 1s - loss: 414.5458 - loglik: -4.1257e+02 - logprior: -1.9804e+00
Fitted a model with MAP estimate = -413.8982
expansions: [(0, 2), (7, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 415.5659 - loglik: -4.1274e+02 - logprior: -2.8239e+00
Epoch 2/10
19/19 - 1s - loss: 412.0473 - loglik: -4.1097e+02 - logprior: -1.0798e+00
Epoch 3/10
19/19 - 1s - loss: 411.9126 - loglik: -4.1092e+02 - logprior: -9.8731e-01
Epoch 4/10
19/19 - 1s - loss: 411.2422 - loglik: -4.1029e+02 - logprior: -9.4809e-01
Epoch 5/10
19/19 - 1s - loss: 410.3222 - loglik: -4.0940e+02 - logprior: -9.2302e-01
Epoch 6/10
19/19 - 2s - loss: 410.2388 - loglik: -4.0933e+02 - logprior: -9.0536e-01
Epoch 7/10
19/19 - 1s - loss: 408.8954 - loglik: -4.0799e+02 - logprior: -8.9502e-01
Epoch 8/10
19/19 - 1s - loss: 407.7516 - loglik: -4.0685e+02 - logprior: -8.9076e-01
Epoch 9/10
19/19 - 1s - loss: 405.8843 - loglik: -4.0498e+02 - logprior: -8.8637e-01
Epoch 10/10
19/19 - 2s - loss: 402.0938 - loglik: -4.0117e+02 - logprior: -9.0382e-01
Fitted a model with MAP estimate = -398.3833
Time for alignment: 58.4824
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 471.2234 - loglik: -4.6810e+02 - logprior: -3.1187e+00
Epoch 2/10
19/19 - 1s - loss: 436.0951 - loglik: -4.3483e+02 - logprior: -1.2692e+00
Epoch 3/10
19/19 - 1s - loss: 423.8279 - loglik: -4.2254e+02 - logprior: -1.2917e+00
Epoch 4/10
19/19 - 1s - loss: 420.9084 - loglik: -4.1964e+02 - logprior: -1.2705e+00
Epoch 5/10
19/19 - 1s - loss: 419.8617 - loglik: -4.1861e+02 - logprior: -1.2532e+00
Epoch 6/10
19/19 - 1s - loss: 418.7454 - loglik: -4.1749e+02 - logprior: -1.2500e+00
Epoch 7/10
19/19 - 1s - loss: 418.0535 - loglik: -4.1680e+02 - logprior: -1.2501e+00
Epoch 8/10
19/19 - 1s - loss: 416.7481 - loglik: -4.1549e+02 - logprior: -1.2447e+00
Epoch 9/10
19/19 - 1s - loss: 415.4731 - loglik: -4.1422e+02 - logprior: -1.2408e+00
Epoch 10/10
19/19 - 1s - loss: 413.5358 - loglik: -4.1227e+02 - logprior: -1.2518e+00
Fitted a model with MAP estimate = -411.0921
expansions: [(7, 2), (8, 3), (9, 2), (21, 1), (22, 1), (31, 1), (35, 1), (36, 1), (38, 1), (47, 1), (48, 1), (49, 1), (55, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 424.6026 - loglik: -4.2173e+02 - logprior: -2.8696e+00
Epoch 2/2
19/19 - 1s - loss: 414.0144 - loglik: -4.1287e+02 - logprior: -1.1422e+00
Fitted a model with MAP estimate = -412.6412
expansions: []
discards: [ 0 11 12 72]
Re-initialized the encoder parameters.
Fitting a model of length 80 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 418.9078 - loglik: -4.1509e+02 - logprior: -3.8187e+00
Epoch 2/2
19/19 - 1s - loss: 414.9916 - loglik: -4.1302e+02 - logprior: -1.9706e+00
Fitted a model with MAP estimate = -414.2893
expansions: [(0, 2), (10, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 415.8849 - loglik: -4.1303e+02 - logprior: -2.8524e+00
Epoch 2/10
19/19 - 1s - loss: 412.3793 - loglik: -4.1127e+02 - logprior: -1.1062e+00
Epoch 3/10
19/19 - 1s - loss: 412.0455 - loglik: -4.1103e+02 - logprior: -1.0176e+00
Epoch 4/10
19/19 - 1s - loss: 411.5151 - loglik: -4.1054e+02 - logprior: -9.7607e-01
Epoch 5/10
19/19 - 1s - loss: 410.6825 - loglik: -4.0973e+02 - logprior: -9.5130e-01
Epoch 6/10
19/19 - 1s - loss: 410.6456 - loglik: -4.0971e+02 - logprior: -9.3076e-01
Epoch 7/10
19/19 - 1s - loss: 408.8283 - loglik: -4.0789e+02 - logprior: -9.2650e-01
Epoch 8/10
19/19 - 1s - loss: 408.1215 - loglik: -4.0720e+02 - logprior: -9.1245e-01
Epoch 9/10
19/19 - 1s - loss: 406.2168 - loglik: -4.0529e+02 - logprior: -9.0840e-01
Epoch 10/10
19/19 - 1s - loss: 401.8355 - loglik: -4.0089e+02 - logprior: -9.2268e-01
Fitted a model with MAP estimate = -398.2161
Time for alignment: 56.6815
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 471.0635 - loglik: -4.6794e+02 - logprior: -3.1243e+00
Epoch 2/10
19/19 - 1s - loss: 435.5691 - loglik: -4.3430e+02 - logprior: -1.2715e+00
Epoch 3/10
19/19 - 1s - loss: 425.3220 - loglik: -4.2406e+02 - logprior: -1.2630e+00
Epoch 4/10
19/19 - 1s - loss: 422.5788 - loglik: -4.2132e+02 - logprior: -1.2565e+00
Epoch 5/10
19/19 - 1s - loss: 420.9823 - loglik: -4.1975e+02 - logprior: -1.2277e+00
Epoch 6/10
19/19 - 1s - loss: 419.5061 - loglik: -4.1828e+02 - logprior: -1.2206e+00
Epoch 7/10
19/19 - 1s - loss: 418.9897 - loglik: -4.1777e+02 - logprior: -1.2167e+00
Epoch 8/10
19/19 - 1s - loss: 417.4886 - loglik: -4.1625e+02 - logprior: -1.2317e+00
Epoch 9/10
19/19 - 1s - loss: 416.4832 - loglik: -4.1523e+02 - logprior: -1.2360e+00
Epoch 10/10
19/19 - 1s - loss: 413.9198 - loglik: -4.1265e+02 - logprior: -1.2525e+00
Fitted a model with MAP estimate = -411.8190
expansions: [(7, 2), (8, 3), (9, 2), (23, 1), (32, 1), (33, 1), (35, 1), (36, 1), (46, 1), (47, 1), (48, 1), (49, 1), (55, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 425.2424 - loglik: -4.2237e+02 - logprior: -2.8686e+00
Epoch 2/2
19/19 - 1s - loss: 413.9778 - loglik: -4.1284e+02 - logprior: -1.1411e+00
Fitted a model with MAP estimate = -412.5751
expansions: []
discards: [ 0 11 12 72]
Re-initialized the encoder parameters.
Fitting a model of length 80 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 418.7936 - loglik: -4.1497e+02 - logprior: -3.8198e+00
Epoch 2/2
19/19 - 1s - loss: 414.9223 - loglik: -4.1294e+02 - logprior: -1.9791e+00
Fitted a model with MAP estimate = -414.1880
expansions: [(0, 2), (10, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 416.0630 - loglik: -4.1322e+02 - logprior: -2.8429e+00
Epoch 2/10
19/19 - 1s - loss: 412.4939 - loglik: -4.1138e+02 - logprior: -1.1105e+00
Epoch 3/10
19/19 - 1s - loss: 412.1818 - loglik: -4.1116e+02 - logprior: -1.0253e+00
Epoch 4/10
19/19 - 1s - loss: 411.6004 - loglik: -4.1062e+02 - logprior: -9.8294e-01
Epoch 5/10
19/19 - 1s - loss: 411.2505 - loglik: -4.1029e+02 - logprior: -9.6027e-01
Epoch 6/10
19/19 - 1s - loss: 410.2599 - loglik: -4.0931e+02 - logprior: -9.4532e-01
Epoch 7/10
19/19 - 1s - loss: 409.6756 - loglik: -4.0873e+02 - logprior: -9.3526e-01
Epoch 8/10
19/19 - 1s - loss: 408.3066 - loglik: -4.0737e+02 - logprior: -9.2282e-01
Epoch 9/10
19/19 - 1s - loss: 406.5720 - loglik: -4.0564e+02 - logprior: -9.1847e-01
Epoch 10/10
19/19 - 1s - loss: 402.5400 - loglik: -4.0158e+02 - logprior: -9.3673e-01
Fitted a model with MAP estimate = -399.0924
Time for alignment: 57.3196
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 470.8917 - loglik: -4.6777e+02 - logprior: -3.1217e+00
Epoch 2/10
19/19 - 1s - loss: 434.7199 - loglik: -4.3344e+02 - logprior: -1.2750e+00
Epoch 3/10
19/19 - 1s - loss: 425.3447 - loglik: -4.2411e+02 - logprior: -1.2308e+00
Epoch 4/10
19/19 - 1s - loss: 421.5089 - loglik: -4.2023e+02 - logprior: -1.2793e+00
Epoch 5/10
19/19 - 1s - loss: 419.2545 - loglik: -4.1800e+02 - logprior: -1.2533e+00
Epoch 6/10
19/19 - 1s - loss: 419.0956 - loglik: -4.1784e+02 - logprior: -1.2516e+00
Epoch 7/10
19/19 - 1s - loss: 418.3777 - loglik: -4.1712e+02 - logprior: -1.2454e+00
Epoch 8/10
19/19 - 1s - loss: 416.0569 - loglik: -4.1480e+02 - logprior: -1.2425e+00
Epoch 9/10
19/19 - 1s - loss: 415.5793 - loglik: -4.1432e+02 - logprior: -1.2413e+00
Epoch 10/10
19/19 - 1s - loss: 412.9223 - loglik: -4.1165e+02 - logprior: -1.2509e+00
Fitted a model with MAP estimate = -411.2624
expansions: [(7, 2), (8, 3), (9, 2), (23, 1), (32, 1), (33, 1), (36, 1), (37, 1), (41, 1), (47, 1), (48, 1), (49, 1), (55, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 424.8539 - loglik: -4.2198e+02 - logprior: -2.8751e+00
Epoch 2/2
19/19 - 1s - loss: 414.0463 - loglik: -4.1290e+02 - logprior: -1.1411e+00
Fitted a model with MAP estimate = -412.5993
expansions: []
discards: [ 0 72]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 418.2590 - loglik: -4.1442e+02 - logprior: -3.8423e+00
Epoch 2/2
19/19 - 1s - loss: 414.3082 - loglik: -4.1232e+02 - logprior: -1.9915e+00
Fitted a model with MAP estimate = -413.5622
expansions: [(0, 2)]
discards: [ 0 10 11]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 416.2633 - loglik: -4.1344e+02 - logprior: -2.8204e+00
Epoch 2/10
19/19 - 1s - loss: 412.8130 - loglik: -4.1175e+02 - logprior: -1.0612e+00
Epoch 3/10
19/19 - 1s - loss: 412.0658 - loglik: -4.1110e+02 - logprior: -9.6806e-01
Epoch 4/10
19/19 - 1s - loss: 411.4099 - loglik: -4.1049e+02 - logprior: -9.2274e-01
Epoch 5/10
19/19 - 1s - loss: 410.8745 - loglik: -4.0998e+02 - logprior: -8.8992e-01
Epoch 6/10
19/19 - 1s - loss: 410.6565 - loglik: -4.0977e+02 - logprior: -8.8369e-01
Epoch 7/10
19/19 - 1s - loss: 409.7841 - loglik: -4.0891e+02 - logprior: -8.7050e-01
Epoch 8/10
19/19 - 1s - loss: 408.1602 - loglik: -4.0729e+02 - logprior: -8.5731e-01
Epoch 9/10
19/19 - 1s - loss: 405.8351 - loglik: -4.0497e+02 - logprior: -8.5566e-01
Epoch 10/10
19/19 - 1s - loss: 402.8197 - loglik: -4.0193e+02 - logprior: -8.6882e-01
Fitted a model with MAP estimate = -398.5843
Time for alignment: 56.5128
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 471.2768 - loglik: -4.6816e+02 - logprior: -3.1186e+00
Epoch 2/10
19/19 - 1s - loss: 434.1329 - loglik: -4.3287e+02 - logprior: -1.2649e+00
Epoch 3/10
19/19 - 1s - loss: 423.3167 - loglik: -4.2208e+02 - logprior: -1.2395e+00
Epoch 4/10
19/19 - 1s - loss: 420.9115 - loglik: -4.1966e+02 - logprior: -1.2453e+00
Epoch 5/10
19/19 - 1s - loss: 419.5488 - loglik: -4.1833e+02 - logprior: -1.2176e+00
Epoch 6/10
19/19 - 1s - loss: 418.5902 - loglik: -4.1738e+02 - logprior: -1.2029e+00
Epoch 7/10
19/19 - 1s - loss: 417.3781 - loglik: -4.1616e+02 - logprior: -1.2063e+00
Epoch 8/10
19/19 - 1s - loss: 417.0647 - loglik: -4.1585e+02 - logprior: -1.2006e+00
Epoch 9/10
19/19 - 1s - loss: 414.9542 - loglik: -4.1373e+02 - logprior: -1.2049e+00
Epoch 10/10
19/19 - 1s - loss: 412.2538 - loglik: -4.1102e+02 - logprior: -1.2119e+00
Fitted a model with MAP estimate = -410.1123
expansions: [(6, 1), (7, 1), (8, 2), (9, 2), (23, 1), (30, 1), (31, 1), (34, 2), (35, 1), (36, 1), (46, 1), (48, 1), (49, 1), (55, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 426.0116 - loglik: -4.2312e+02 - logprior: -2.8951e+00
Epoch 2/2
19/19 - 1s - loss: 413.9650 - loglik: -4.1287e+02 - logprior: -1.0922e+00
Fitted a model with MAP estimate = -412.8043
expansions: []
discards: [ 0  7 44 72]
Re-initialized the encoder parameters.
Fitting a model of length 80 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 419.1467 - loglik: -4.1533e+02 - logprior: -3.8171e+00
Epoch 2/2
19/19 - 1s - loss: 415.0054 - loglik: -4.1305e+02 - logprior: -1.9524e+00
Fitted a model with MAP estimate = -414.3263
expansions: [(0, 2), (8, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 415.9213 - loglik: -4.1312e+02 - logprior: -2.8060e+00
Epoch 2/10
19/19 - 1s - loss: 412.4346 - loglik: -4.1136e+02 - logprior: -1.0749e+00
Epoch 3/10
19/19 - 1s - loss: 412.1418 - loglik: -4.1116e+02 - logprior: -9.8595e-01
Epoch 4/10
19/19 - 1s - loss: 411.2502 - loglik: -4.1031e+02 - logprior: -9.3620e-01
Epoch 5/10
19/19 - 1s - loss: 411.1646 - loglik: -4.1025e+02 - logprior: -9.1166e-01
Epoch 6/10
19/19 - 1s - loss: 410.6044 - loglik: -4.0970e+02 - logprior: -8.9748e-01
Epoch 7/10
19/19 - 1s - loss: 409.4983 - loglik: -4.0860e+02 - logprior: -8.8668e-01
Epoch 8/10
19/19 - 1s - loss: 407.8280 - loglik: -4.0694e+02 - logprior: -8.8165e-01
Epoch 9/10
19/19 - 1s - loss: 405.8929 - loglik: -4.0500e+02 - logprior: -8.8347e-01
Epoch 10/10
19/19 - 1s - loss: 402.0468 - loglik: -4.0113e+02 - logprior: -8.9932e-01
Fitted a model with MAP estimate = -398.0391
Time for alignment: 55.3861
Computed alignments with likelihoods: ['-398.3833', '-398.2161', '-399.0924', '-398.5843', '-398.0391']
Best model has likelihood: -398.0391  (prior= -0.9132 )
time for generating output: 0.1144
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07654.projection.fasta
SP score = 0.7551020408163265
Training of 5 independent models on file PF04082.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6bb9f1c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff7188c3250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff5d28fabb0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1317.8829 - loglik: -1.3161e+03 - logprior: -1.7464e+00
Epoch 2/10
39/39 - 16s - loss: 1268.5347 - loglik: -1.2675e+03 - logprior: -1.0466e+00
Epoch 3/10
39/39 - 16s - loss: 1263.0963 - loglik: -1.2620e+03 - logprior: -1.0751e+00
Epoch 4/10
39/39 - 16s - loss: 1257.3998 - loglik: -1.2564e+03 - logprior: -1.0424e+00
Epoch 5/10
39/39 - 16s - loss: 1248.7347 - loglik: -1.2477e+03 - logprior: -1.0190e+00
Epoch 6/10
39/39 - 16s - loss: 1229.4136 - loglik: -1.2284e+03 - logprior: -1.0173e+00
Epoch 7/10
39/39 - 16s - loss: 1143.7207 - loglik: -1.1422e+03 - logprior: -1.4810e+00
Epoch 8/10
39/39 - 16s - loss: 964.4034 - loglik: -9.6103e+02 - logprior: -3.3313e+00
Epoch 9/10
39/39 - 16s - loss: 933.4554 - loglik: -9.3030e+02 - logprior: -3.1002e+00
Epoch 10/10
39/39 - 16s - loss: 930.3596 - loglik: -9.2724e+02 - logprior: -3.0582e+00
Fitted a model with MAP estimate = -928.9452
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 95]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 1374.7507 - loglik: -1.3726e+03 - logprior: -2.1947e+00
Epoch 2/2
39/39 - 9s - loss: 1337.4452 - loglik: -1.3366e+03 - logprior: -8.5359e-01
Fitted a model with MAP estimate = -1320.3756
expansions: [(0, 39), (3, 2), (8, 37), (15, 9), (24, 2), (32, 2), (34, 3), (54, 1), (55, 2), (61, 1), (69, 2), (120, 62)]
discards: [ 11  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87
  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105
 106 107 108 109 110 111 112 113 114 115 116 117 118 119]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 1292.4919 - loglik: -1.2900e+03 - logprior: -2.4992e+00
Epoch 2/2
39/39 - 21s - loss: 1264.6560 - loglik: -1.2641e+03 - logprior: -5.8763e-01
Fitted a model with MAP estimate = -1259.5733
expansions: [(33, 1), (56, 2), (60, 1), (93, 1), (95, 1), (96, 2), (97, 2), (98, 2), (123, 2), (185, 1), (186, 2), (232, 11)]
discards: [  0  27  28  29  39 125 148 212 213 214 215 229 230 231]
Re-initialized the encoder parameters.
Fitting a model of length 246 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 1271.3337 - loglik: -1.2691e+03 - logprior: -2.2611e+00
Epoch 2/10
39/39 - 23s - loss: 1260.6096 - loglik: -1.2602e+03 - logprior: -3.6939e-01
Epoch 3/10
39/39 - 24s - loss: 1256.3239 - loglik: -1.2561e+03 - logprior: -2.4989e-01
Epoch 4/10
39/39 - 24s - loss: 1251.8993 - loglik: -1.2517e+03 - logprior: -1.6967e-01
Epoch 5/10
39/39 - 24s - loss: 1244.3401 - loglik: -1.2442e+03 - logprior: -1.0423e-01
Epoch 6/10
39/39 - 24s - loss: 1226.9835 - loglik: -1.2269e+03 - logprior: -6.6137e-02
Epoch 7/10
39/39 - 24s - loss: 1175.1064 - loglik: -1.1750e+03 - logprior: -1.1266e-01
Epoch 8/10
39/39 - 24s - loss: 1015.6427 - loglik: -1.0145e+03 - logprior: -1.0992e+00
Epoch 9/10
39/39 - 25s - loss: 932.9237 - loglik: -9.3105e+02 - logprior: -1.8173e+00
Epoch 10/10
39/39 - 24s - loss: 926.2527 - loglik: -9.2449e+02 - logprior: -1.7081e+00
Fitted a model with MAP estimate = -924.0708
Time for alignment: 546.4953
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1316.5037 - loglik: -1.3148e+03 - logprior: -1.7484e+00
Epoch 2/10
39/39 - 17s - loss: 1269.1782 - loglik: -1.2681e+03 - logprior: -1.0645e+00
Epoch 3/10
39/39 - 17s - loss: 1263.3435 - loglik: -1.2623e+03 - logprior: -1.0581e+00
Epoch 4/10
39/39 - 17s - loss: 1257.1973 - loglik: -1.2562e+03 - logprior: -1.0327e+00
Epoch 5/10
39/39 - 17s - loss: 1249.1626 - loglik: -1.2481e+03 - logprior: -1.0062e+00
Epoch 6/10
39/39 - 16s - loss: 1229.5608 - loglik: -1.2285e+03 - logprior: -1.0057e+00
Epoch 7/10
39/39 - 16s - loss: 1142.1262 - loglik: -1.1406e+03 - logprior: -1.4740e+00
Epoch 8/10
39/39 - 16s - loss: 962.2689 - loglik: -9.5895e+02 - logprior: -3.2738e+00
Epoch 9/10
39/39 - 16s - loss: 934.0079 - loglik: -9.3083e+02 - logprior: -3.1173e+00
Epoch 10/10
39/39 - 16s - loss: 929.4873 - loglik: -9.2637e+02 - logprior: -3.0566e+00
Fitted a model with MAP estimate = -929.0792
expansions: []
discards: [ 39  40  41  42  43  44  45  46  47  48  49  99 100 101 102 103 104 105
 106 107 108 109 110 111 112 113 114 115 116 117 124]
Re-initialized the encoder parameters.
Fitting a model of length 161 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 1374.3737 - loglik: -1.3722e+03 - logprior: -2.2095e+00
Epoch 2/2
39/39 - 12s - loss: 1349.8555 - loglik: -1.3489e+03 - logprior: -9.4247e-01
Fitted a model with MAP estimate = -1338.1178
expansions: [(0, 78), (68, 2), (69, 3), (85, 2), (86, 18), (87, 7), (100, 3), (101, 2), (102, 5), (161, 66)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  70  72  73  88 103 104 105 106 107 108 109 110 111 112 113 114 115
 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133
 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151
 152 153 154 155 156 157 158 159 160]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 1298.5303 - loglik: -1.2967e+03 - logprior: -1.8275e+00
Epoch 2/2
39/39 - 19s - loss: 1263.4127 - loglik: -1.2628e+03 - logprior: -6.4863e-01
Fitted a model with MAP estimate = -1258.8742
expansions: [(26, 3), (29, 2), (93, 1), (96, 1), (97, 1), (111, 1), (119, 2), (135, 1), (175, 1), (176, 1), (230, 11)]
discards: [  0  51 157 210 211 212 213 227 228 229]
Re-initialized the encoder parameters.
Fitting a model of length 245 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 1268.4573 - loglik: -1.2661e+03 - logprior: -2.3727e+00
Epoch 2/10
39/39 - 25s - loss: 1259.0105 - loglik: -1.2586e+03 - logprior: -4.2080e-01
Epoch 3/10
39/39 - 26s - loss: 1255.1639 - loglik: -1.2549e+03 - logprior: -2.7296e-01
Epoch 4/10
39/39 - 28s - loss: 1250.7885 - loglik: -1.2506e+03 - logprior: -1.9290e-01
Epoch 5/10
39/39 - 28s - loss: 1241.4386 - loglik: -1.2413e+03 - logprior: -1.2761e-01
Epoch 6/10
39/39 - 29s - loss: 1224.5812 - loglik: -1.2245e+03 - logprior: -1.1087e-01
Epoch 7/10
39/39 - 31s - loss: 1160.7023 - loglik: -1.1604e+03 - logprior: -2.5182e-01
Epoch 8/10
39/39 - 31s - loss: 993.0791 - loglik: -9.9165e+02 - logprior: -1.3880e+00
Epoch 9/10
39/39 - 30s - loss: 932.0620 - loglik: -9.3007e+02 - logprior: -1.9422e+00
Epoch 10/10
39/39 - 28s - loss: 925.5878 - loglik: -9.2371e+02 - logprior: -1.8194e+00
Fitted a model with MAP estimate = -924.0996
Time for alignment: 591.3570
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1317.5259 - loglik: -1.3158e+03 - logprior: -1.7428e+00
Epoch 2/10
39/39 - 18s - loss: 1269.4371 - loglik: -1.2684e+03 - logprior: -1.0477e+00
Epoch 3/10
39/39 - 18s - loss: 1264.1376 - loglik: -1.2631e+03 - logprior: -1.0541e+00
Epoch 4/10
39/39 - 18s - loss: 1258.3099 - loglik: -1.2573e+03 - logprior: -1.0204e+00
Epoch 5/10
39/39 - 18s - loss: 1249.3215 - loglik: -1.2483e+03 - logprior: -1.0058e+00
Epoch 6/10
39/39 - 18s - loss: 1230.6063 - loglik: -1.2296e+03 - logprior: -9.8494e-01
Epoch 7/10
39/39 - 18s - loss: 1145.1838 - loglik: -1.1437e+03 - logprior: -1.4504e+00
Epoch 8/10
39/39 - 18s - loss: 963.5225 - loglik: -9.6012e+02 - logprior: -3.3562e+00
Epoch 9/10
39/39 - 18s - loss: 933.8315 - loglik: -9.3061e+02 - logprior: -3.1665e+00
Epoch 10/10
39/39 - 19s - loss: 930.2311 - loglik: -9.2705e+02 - logprior: -3.1144e+00
Fitted a model with MAP estimate = -929.1623
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  97 121 124 125]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 1373.9673 - loglik: -1.3715e+03 - logprior: -2.4292e+00
Epoch 2/2
39/39 - 10s - loss: 1353.9193 - loglik: -1.3529e+03 - logprior: -1.0474e+00
Fitted a model with MAP estimate = -1345.7828
expansions: [(0, 72), (19, 2), (20, 2), (21, 4), (27, 3), (30, 6), (31, 11), (123, 106)]
discards: [ 15  16  17  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46
  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64
  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82
  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100
 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118
 119 120 121 122]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 1298.8328 - loglik: -1.2970e+03 - logprior: -1.8765e+00
Epoch 2/2
39/39 - 24s - loss: 1261.1715 - loglik: -1.2606e+03 - logprior: -6.1788e-01
Fitted a model with MAP estimate = -1257.6313
expansions: [(23, 1), (28, 1), (29, 1), (49, 3), (52, 1), (85, 1), (90, 1), (92, 1), (106, 1)]
discards: [  0  88 115 116 128 211 215 216 217 218 232 233 234]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 1266.7523 - loglik: -1.2645e+03 - logprior: -2.2445e+00
Epoch 2/10
39/39 - 28s - loss: 1259.8080 - loglik: -1.2596e+03 - logprior: -1.9242e-01
Epoch 3/10
39/39 - 29s - loss: 1256.4728 - loglik: -1.2564e+03 - logprior: -8.2331e-02
Epoch 4/10
39/39 - 29s - loss: 1251.9802 - loglik: -1.2520e+03 - logprior: -7.5099e-03
Epoch 5/10
39/39 - 30s - loss: 1241.4143 - loglik: -1.2414e+03 - logprior: 0.0228
Epoch 6/10
39/39 - 30s - loss: 1219.5460 - loglik: -1.2195e+03 - logprior: -2.5278e-02
Epoch 7/10
39/39 - 31s - loss: 1130.6134 - loglik: -1.1301e+03 - logprior: -5.3172e-01
Epoch 8/10
39/39 - 31s - loss: 965.4914 - loglik: -9.6335e+02 - logprior: -2.0955e+00
Epoch 9/10
39/39 - 30s - loss: 932.0710 - loglik: -9.2974e+02 - logprior: -2.2768e+00
Epoch 10/10
39/39 - 30s - loss: 926.9088 - loglik: -9.2461e+02 - logprior: -2.2429e+00
Fitted a model with MAP estimate = -925.7574
Time for alignment: 634.6944
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 1318.6671 - loglik: -1.3169e+03 - logprior: -1.7488e+00
Epoch 2/10
39/39 - 22s - loss: 1270.7573 - loglik: -1.2697e+03 - logprior: -1.0861e+00
Epoch 3/10
39/39 - 23s - loss: 1263.5247 - loglik: -1.2624e+03 - logprior: -1.1252e+00
Epoch 4/10
39/39 - 22s - loss: 1258.6224 - loglik: -1.2575e+03 - logprior: -1.1047e+00
Epoch 5/10
39/39 - 22s - loss: 1248.9869 - loglik: -1.2479e+03 - logprior: -1.0978e+00
Epoch 6/10
39/39 - 22s - loss: 1230.8945 - loglik: -1.2298e+03 - logprior: -1.0702e+00
Epoch 7/10
39/39 - 22s - loss: 1146.4049 - loglik: -1.1449e+03 - logprior: -1.4711e+00
Epoch 8/10
39/39 - 22s - loss: 964.4005 - loglik: -9.6105e+02 - logprior: -3.3010e+00
Epoch 9/10
39/39 - 22s - loss: 933.4195 - loglik: -9.3026e+02 - logprior: -3.1019e+00
Epoch 10/10
39/39 - 21s - loss: 930.0167 - loglik: -9.2693e+02 - logprior: -3.0233e+00
Fitted a model with MAP estimate = -928.9061
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  97  98  99 100 101 102
 103 104 105 106 107 108 109 110 111 112 113]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 1374.7799 - loglik: -1.3724e+03 - logprior: -2.3950e+00
Epoch 2/2
39/39 - 11s - loss: 1348.4135 - loglik: -1.3473e+03 - logprior: -1.0983e+00
Fitted a model with MAP estimate = -1332.1430
expansions: [(0, 73), (36, 4), (37, 2), (47, 2), (48, 19), (49, 2), (51, 2), (52, 2), (127, 70)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  32
  33  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88
  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106
 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124
 125 126]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 1298.3179 - loglik: -1.2966e+03 - logprior: -1.7063e+00
Epoch 2/2
39/39 - 27s - loss: 1265.3751 - loglik: -1.2648e+03 - logprior: -5.2950e-01
Fitted a model with MAP estimate = -1260.3126
expansions: [(22, 1), (23, 2), (24, 2), (26, 2), (48, 3), (49, 1), (87, 1), (88, 1), (176, 1), (181, 1), (182, 2), (229, 11)]
discards: [  0 130 135 209 210 211 212 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 247 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 1271.2534 - loglik: -1.2690e+03 - logprior: -2.2701e+00
Epoch 2/10
39/39 - 32s - loss: 1260.0107 - loglik: -1.2596e+03 - logprior: -3.8255e-01
Epoch 3/10
39/39 - 32s - loss: 1255.7268 - loglik: -1.2555e+03 - logprior: -2.7066e-01
Epoch 4/10
39/39 - 32s - loss: 1250.7966 - loglik: -1.2506e+03 - logprior: -2.0121e-01
Epoch 5/10
39/39 - 29s - loss: 1241.6117 - loglik: -1.2415e+03 - logprior: -1.2872e-01
Epoch 6/10
39/39 - 28s - loss: 1226.0348 - loglik: -1.2259e+03 - logprior: -9.3395e-02
Epoch 7/10
39/39 - 27s - loss: 1171.8376 - loglik: -1.1716e+03 - logprior: -1.6817e-01
Epoch 8/10
39/39 - 28s - loss: 1010.3642 - loglik: -1.0092e+03 - logprior: -1.1009e+00
Epoch 9/10
39/39 - 27s - loss: 932.4957 - loglik: -9.3068e+02 - logprior: -1.7670e+00
Epoch 10/10
39/39 - 26s - loss: 925.4153 - loglik: -9.2367e+02 - logprior: -1.6892e+00
Fitted a model with MAP estimate = -923.6305
Time for alignment: 682.7807
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1317.8630 - loglik: -1.3161e+03 - logprior: -1.7491e+00
Epoch 2/10
39/39 - 18s - loss: 1269.1910 - loglik: -1.2681e+03 - logprior: -1.0906e+00
Epoch 3/10
39/39 - 18s - loss: 1263.0012 - loglik: -1.2619e+03 - logprior: -1.1023e+00
Epoch 4/10
39/39 - 17s - loss: 1256.4410 - loglik: -1.2554e+03 - logprior: -1.0744e+00
Epoch 5/10
39/39 - 17s - loss: 1249.1488 - loglik: -1.2481e+03 - logprior: -1.0614e+00
Epoch 6/10
39/39 - 18s - loss: 1229.9347 - loglik: -1.2289e+03 - logprior: -1.0493e+00
Epoch 7/10
39/39 - 18s - loss: 1146.3964 - loglik: -1.1449e+03 - logprior: -1.5042e+00
Epoch 8/10
39/39 - 18s - loss: 963.7238 - loglik: -9.6040e+02 - logprior: -3.2754e+00
Epoch 9/10
39/39 - 18s - loss: 933.1520 - loglik: -9.2994e+02 - logprior: -3.1563e+00
Epoch 10/10
39/39 - 19s - loss: 930.7831 - loglik: -9.2754e+02 - logprior: -3.1778e+00
Fitted a model with MAP estimate = -928.9988
expansions: []
discards: [ 38  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114
 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132
 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 166 167 168
 169 170 171 172 173 174 175 176 177 178 179 180 181 182]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 1375.5518 - loglik: -1.3735e+03 - logprior: -2.0031e+00
Epoch 2/2
39/39 - 10s - loss: 1353.0554 - loglik: -1.3522e+03 - logprior: -8.2224e-01
Fitted a model with MAP estimate = -1338.5168
expansions: [(0, 81), (85, 2), (86, 11), (94, 2), (97, 13), (124, 81)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76 115 116 117]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 1304.5444 - loglik: -1.3027e+03 - logprior: -1.8587e+00
Epoch 2/2
39/39 - 26s - loss: 1267.8755 - loglik: -1.2671e+03 - logprior: -7.4744e-01
Fitted a model with MAP estimate = -1261.9537
expansions: [(26, 4), (95, 1), (97, 1), (119, 2), (120, 6), (121, 3), (122, 1), (123, 1), (147, 1), (150, 1), (188, 1), (234, 11)]
discards: [  0  90 129 130 131 132 133 134 135 136 137 138 139 215 216 217 231 232
 233]
Re-initialized the encoder parameters.
Fitting a model of length 248 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 32s - loss: 1269.8868 - loglik: -1.2675e+03 - logprior: -2.3822e+00
Epoch 2/10
39/39 - 31s - loss: 1259.2006 - loglik: -1.2588e+03 - logprior: -4.2256e-01
Epoch 3/10
39/39 - 32s - loss: 1255.4075 - loglik: -1.2551e+03 - logprior: -3.1585e-01
Epoch 4/10
39/39 - 32s - loss: 1250.1238 - loglik: -1.2499e+03 - logprior: -2.2656e-01
Epoch 5/10
39/39 - 32s - loss: 1242.6884 - loglik: -1.2425e+03 - logprior: -1.6330e-01
Epoch 6/10
39/39 - 32s - loss: 1226.2170 - loglik: -1.2261e+03 - logprior: -1.4113e-01
Epoch 7/10
39/39 - 33s - loss: 1164.6470 - loglik: -1.1644e+03 - logprior: -2.6239e-01
Epoch 8/10
39/39 - 32s - loss: 1003.9533 - loglik: -1.0026e+03 - logprior: -1.2632e+00
Epoch 9/10
39/39 - 30s - loss: 931.5161 - loglik: -9.2954e+02 - logprior: -1.9263e+00
Epoch 10/10
39/39 - 28s - loss: 925.5032 - loglik: -9.2361e+02 - logprior: -1.8301e+00
Fitted a model with MAP estimate = -923.7629
Time for alignment: 657.5650
Computed alignments with likelihoods: ['-924.0708', '-924.0996', '-925.7574', '-923.6305', '-923.7629']
Best model has likelihood: -923.6305  (prior= -1.6256 )
time for generating output: 0.2359
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF04082.projection.fasta
SP score = 0.07472324723247233
Training of 5 independent models on file PF00046.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6bbae7340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6b316df40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6b2e86190>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 312.4973 - loglik: -3.0926e+02 - logprior: -3.2344e+00
Epoch 2/10
19/19 - 1s - loss: 281.1040 - loglik: -2.7958e+02 - logprior: -1.5265e+00
Epoch 3/10
19/19 - 1s - loss: 268.8367 - loglik: -2.6732e+02 - logprior: -1.5158e+00
Epoch 4/10
19/19 - 1s - loss: 265.8129 - loglik: -2.6434e+02 - logprior: -1.4680e+00
Epoch 5/10
19/19 - 1s - loss: 264.9355 - loglik: -2.6349e+02 - logprior: -1.4382e+00
Epoch 6/10
19/19 - 1s - loss: 264.0755 - loglik: -2.6264e+02 - logprior: -1.4286e+00
Epoch 7/10
19/19 - 1s - loss: 263.0881 - loglik: -2.6167e+02 - logprior: -1.4157e+00
Epoch 8/10
19/19 - 1s - loss: 262.7549 - loglik: -2.6133e+02 - logprior: -1.4160e+00
Epoch 9/10
19/19 - 1s - loss: 262.3455 - loglik: -2.6092e+02 - logprior: -1.4125e+00
Epoch 10/10
19/19 - 1s - loss: 261.2279 - loglik: -2.5979e+02 - logprior: -1.4223e+00
Fitted a model with MAP estimate = -260.9801
expansions: [(5, 1), (7, 2), (9, 1), (16, 1), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 267.1519 - loglik: -2.6390e+02 - logprior: -3.2502e+00
Epoch 2/2
19/19 - 1s - loss: 258.0805 - loglik: -2.5674e+02 - logprior: -1.3370e+00
Fitted a model with MAP estimate = -256.9464
expansions: []
discards: [35 38]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 261.1116 - loglik: -2.5798e+02 - logprior: -3.1349e+00
Epoch 2/2
19/19 - 1s - loss: 257.1255 - loglik: -2.5588e+02 - logprior: -1.2496e+00
Fitted a model with MAP estimate = -256.5636
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 260.6358 - loglik: -2.5753e+02 - logprior: -3.1054e+00
Epoch 2/10
19/19 - 1s - loss: 257.1243 - loglik: -2.5588e+02 - logprior: -1.2443e+00
Epoch 3/10
19/19 - 1s - loss: 256.1873 - loglik: -2.5507e+02 - logprior: -1.1150e+00
Epoch 4/10
19/19 - 1s - loss: 256.1122 - loglik: -2.5505e+02 - logprior: -1.0624e+00
Epoch 5/10
19/19 - 1s - loss: 255.2808 - loglik: -2.5424e+02 - logprior: -1.0395e+00
Epoch 6/10
19/19 - 1s - loss: 255.0101 - loglik: -2.5399e+02 - logprior: -1.0191e+00
Epoch 7/10
19/19 - 1s - loss: 253.9780 - loglik: -2.5295e+02 - logprior: -1.0179e+00
Epoch 8/10
19/19 - 1s - loss: 253.2668 - loglik: -2.5225e+02 - logprior: -1.0046e+00
Epoch 9/10
19/19 - 1s - loss: 252.5170 - loglik: -2.5148e+02 - logprior: -1.0282e+00
Epoch 10/10
19/19 - 1s - loss: 251.6099 - loglik: -2.5056e+02 - logprior: -1.0375e+00
Fitted a model with MAP estimate = -250.9516
Time for alignment: 46.4718
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 312.5855 - loglik: -3.0935e+02 - logprior: -3.2367e+00
Epoch 2/10
19/19 - 1s - loss: 281.5216 - loglik: -2.7999e+02 - logprior: -1.5306e+00
Epoch 3/10
19/19 - 1s - loss: 268.8435 - loglik: -2.6733e+02 - logprior: -1.5106e+00
Epoch 4/10
19/19 - 1s - loss: 266.1377 - loglik: -2.6467e+02 - logprior: -1.4674e+00
Epoch 5/10
19/19 - 1s - loss: 264.7695 - loglik: -2.6333e+02 - logprior: -1.4400e+00
Epoch 6/10
19/19 - 1s - loss: 263.8227 - loglik: -2.6239e+02 - logprior: -1.4256e+00
Epoch 7/10
19/19 - 1s - loss: 263.5637 - loglik: -2.6214e+02 - logprior: -1.4160e+00
Epoch 8/10
19/19 - 1s - loss: 262.5702 - loglik: -2.6115e+02 - logprior: -1.4099e+00
Epoch 9/10
19/19 - 1s - loss: 262.2698 - loglik: -2.6085e+02 - logprior: -1.4137e+00
Epoch 10/10
19/19 - 1s - loss: 261.1548 - loglik: -2.5972e+02 - logprior: -1.4257e+00
Fitted a model with MAP estimate = -260.9577
expansions: [(5, 1), (7, 2), (9, 1), (16, 1), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 267.1770 - loglik: -2.6393e+02 - logprior: -3.2437e+00
Epoch 2/2
19/19 - 1s - loss: 257.8992 - loglik: -2.5656e+02 - logprior: -1.3423e+00
Fitted a model with MAP estimate = -256.9754
expansions: []
discards: [35 38]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 261.0726 - loglik: -2.5792e+02 - logprior: -3.1513e+00
Epoch 2/2
19/19 - 1s - loss: 257.1298 - loglik: -2.5588e+02 - logprior: -1.2478e+00
Fitted a model with MAP estimate = -256.6111
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 260.6754 - loglik: -2.5756e+02 - logprior: -3.1169e+00
Epoch 2/10
19/19 - 1s - loss: 257.0482 - loglik: -2.5581e+02 - logprior: -1.2331e+00
Epoch 3/10
19/19 - 1s - loss: 256.3471 - loglik: -2.5523e+02 - logprior: -1.1193e+00
Epoch 4/10
19/19 - 1s - loss: 255.9387 - loglik: -2.5487e+02 - logprior: -1.0632e+00
Epoch 5/10
19/19 - 1s - loss: 255.5437 - loglik: -2.5451e+02 - logprior: -1.0335e+00
Epoch 6/10
19/19 - 1s - loss: 254.7202 - loglik: -2.5370e+02 - logprior: -1.0209e+00
Epoch 7/10
19/19 - 1s - loss: 254.1534 - loglik: -2.5313e+02 - logprior: -1.0138e+00
Epoch 8/10
19/19 - 1s - loss: 253.3040 - loglik: -2.5228e+02 - logprior: -1.0115e+00
Epoch 9/10
19/19 - 1s - loss: 252.5067 - loglik: -2.5148e+02 - logprior: -1.0176e+00
Epoch 10/10
19/19 - 1s - loss: 251.4909 - loglik: -2.5044e+02 - logprior: -1.0361e+00
Fitted a model with MAP estimate = -250.9545
Time for alignment: 44.7184
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 312.5170 - loglik: -3.0928e+02 - logprior: -3.2329e+00
Epoch 2/10
19/19 - 1s - loss: 280.3173 - loglik: -2.7880e+02 - logprior: -1.5178e+00
Epoch 3/10
19/19 - 1s - loss: 268.4268 - loglik: -2.6691e+02 - logprior: -1.5208e+00
Epoch 4/10
19/19 - 1s - loss: 265.7489 - loglik: -2.6428e+02 - logprior: -1.4691e+00
Epoch 5/10
19/19 - 1s - loss: 264.8533 - loglik: -2.6341e+02 - logprior: -1.4402e+00
Epoch 6/10
19/19 - 1s - loss: 263.9382 - loglik: -2.6250e+02 - logprior: -1.4292e+00
Epoch 7/10
19/19 - 1s - loss: 263.2579 - loglik: -2.6184e+02 - logprior: -1.4161e+00
Epoch 8/10
19/19 - 1s - loss: 263.0482 - loglik: -2.6163e+02 - logprior: -1.4123e+00
Epoch 9/10
19/19 - 1s - loss: 261.8759 - loglik: -2.6045e+02 - logprior: -1.4180e+00
Epoch 10/10
19/19 - 1s - loss: 261.5363 - loglik: -2.6010e+02 - logprior: -1.4216e+00
Fitted a model with MAP estimate = -260.9896
expansions: [(5, 1), (7, 2), (9, 1), (16, 1), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 267.0110 - loglik: -2.6377e+02 - logprior: -3.2450e+00
Epoch 2/2
19/19 - 1s - loss: 257.8334 - loglik: -2.5650e+02 - logprior: -1.3380e+00
Fitted a model with MAP estimate = -256.9523
expansions: []
discards: [35 38]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 261.0038 - loglik: -2.5786e+02 - logprior: -3.1423e+00
Epoch 2/2
19/19 - 1s - loss: 257.2355 - loglik: -2.5599e+02 - logprior: -1.2485e+00
Fitted a model with MAP estimate = -256.5726
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 260.6363 - loglik: -2.5752e+02 - logprior: -3.1119e+00
Epoch 2/10
19/19 - 1s - loss: 257.0899 - loglik: -2.5585e+02 - logprior: -1.2370e+00
Epoch 3/10
19/19 - 1s - loss: 256.3181 - loglik: -2.5520e+02 - logprior: -1.1129e+00
Epoch 4/10
19/19 - 1s - loss: 256.3113 - loglik: -2.5524e+02 - logprior: -1.0668e+00
Epoch 5/10
19/19 - 1s - loss: 255.2565 - loglik: -2.5422e+02 - logprior: -1.0345e+00
Epoch 6/10
19/19 - 1s - loss: 254.9206 - loglik: -2.5389e+02 - logprior: -1.0229e+00
Epoch 7/10
19/19 - 1s - loss: 253.9164 - loglik: -2.5290e+02 - logprior: -1.0136e+00
Epoch 8/10
19/19 - 1s - loss: 252.8841 - loglik: -2.5187e+02 - logprior: -1.0073e+00
Epoch 9/10
19/19 - 1s - loss: 252.9514 - loglik: -2.5192e+02 - logprior: -1.0191e+00
Fitted a model with MAP estimate = -251.8638
Time for alignment: 43.0908
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 312.5992 - loglik: -3.0936e+02 - logprior: -3.2362e+00
Epoch 2/10
19/19 - 1s - loss: 281.1333 - loglik: -2.7961e+02 - logprior: -1.5275e+00
Epoch 3/10
19/19 - 1s - loss: 268.0159 - loglik: -2.6646e+02 - logprior: -1.5513e+00
Epoch 4/10
19/19 - 1s - loss: 265.4661 - loglik: -2.6396e+02 - logprior: -1.5060e+00
Epoch 5/10
19/19 - 1s - loss: 264.1930 - loglik: -2.6272e+02 - logprior: -1.4703e+00
Epoch 6/10
19/19 - 1s - loss: 263.2541 - loglik: -2.6180e+02 - logprior: -1.4511e+00
Epoch 7/10
19/19 - 1s - loss: 262.2815 - loglik: -2.6083e+02 - logprior: -1.4419e+00
Epoch 8/10
19/19 - 1s - loss: 261.5577 - loglik: -2.6011e+02 - logprior: -1.4411e+00
Epoch 9/10
19/19 - 1s - loss: 260.7470 - loglik: -2.5929e+02 - logprior: -1.4466e+00
Epoch 10/10
19/19 - 1s - loss: 260.5975 - loglik: -2.5912e+02 - logprior: -1.4610e+00
Fitted a model with MAP estimate = -259.8043
expansions: [(5, 1), (7, 1), (10, 1), (14, 1), (16, 1), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 267.0492 - loglik: -2.6380e+02 - logprior: -3.2489e+00
Epoch 2/2
19/19 - 1s - loss: 257.8733 - loglik: -2.5654e+02 - logprior: -1.3359e+00
Fitted a model with MAP estimate = -256.9381
expansions: []
discards: [35 38]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 261.0742 - loglik: -2.5793e+02 - logprior: -3.1462e+00
Epoch 2/2
19/19 - 1s - loss: 257.2508 - loglik: -2.5600e+02 - logprior: -1.2541e+00
Fitted a model with MAP estimate = -256.5771
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 260.6682 - loglik: -2.5756e+02 - logprior: -3.1129e+00
Epoch 2/10
19/19 - 1s - loss: 256.8846 - loglik: -2.5565e+02 - logprior: -1.2337e+00
Epoch 3/10
19/19 - 1s - loss: 256.6103 - loglik: -2.5548e+02 - logprior: -1.1255e+00
Epoch 4/10
19/19 - 1s - loss: 256.0080 - loglik: -2.5495e+02 - logprior: -1.0582e+00
Epoch 5/10
19/19 - 1s - loss: 255.2775 - loglik: -2.5424e+02 - logprior: -1.0372e+00
Epoch 6/10
19/19 - 1s - loss: 255.0131 - loglik: -2.5399e+02 - logprior: -1.0189e+00
Epoch 7/10
19/19 - 1s - loss: 253.9061 - loglik: -2.5288e+02 - logprior: -1.0205e+00
Epoch 8/10
19/19 - 1s - loss: 253.4014 - loglik: -2.5238e+02 - logprior: -1.0134e+00
Epoch 9/10
19/19 - 1s - loss: 252.6885 - loglik: -2.5166e+02 - logprior: -1.0210e+00
Epoch 10/10
19/19 - 1s - loss: 251.2572 - loglik: -2.5020e+02 - logprior: -1.0404e+00
Fitted a model with MAP estimate = -250.9176
Time for alignment: 44.9972
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 312.4984 - loglik: -3.0927e+02 - logprior: -3.2309e+00
Epoch 2/10
19/19 - 1s - loss: 280.9911 - loglik: -2.7947e+02 - logprior: -1.5165e+00
Epoch 3/10
19/19 - 1s - loss: 267.8346 - loglik: -2.6627e+02 - logprior: -1.5600e+00
Epoch 4/10
19/19 - 1s - loss: 264.6166 - loglik: -2.6310e+02 - logprior: -1.5139e+00
Epoch 5/10
19/19 - 1s - loss: 263.4512 - loglik: -2.6195e+02 - logprior: -1.4944e+00
Epoch 6/10
19/19 - 1s - loss: 262.8955 - loglik: -2.6142e+02 - logprior: -1.4730e+00
Epoch 7/10
19/19 - 1s - loss: 262.1024 - loglik: -2.6065e+02 - logprior: -1.4497e+00
Epoch 8/10
19/19 - 1s - loss: 261.6086 - loglik: -2.6016e+02 - logprior: -1.4436e+00
Epoch 9/10
19/19 - 1s - loss: 260.9901 - loglik: -2.5953e+02 - logprior: -1.4494e+00
Epoch 10/10
19/19 - 1s - loss: 260.4014 - loglik: -2.5893e+02 - logprior: -1.4559e+00
Fitted a model with MAP estimate = -259.8081
expansions: [(5, 1), (7, 1), (10, 1), (14, 1), (16, 1), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 266.9985 - loglik: -2.6375e+02 - logprior: -3.2460e+00
Epoch 2/2
19/19 - 1s - loss: 257.9170 - loglik: -2.5659e+02 - logprior: -1.3305e+00
Fitted a model with MAP estimate = -256.9543
expansions: []
discards: [35 38]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 260.9904 - loglik: -2.5784e+02 - logprior: -3.1491e+00
Epoch 2/2
19/19 - 1s - loss: 257.1461 - loglik: -2.5590e+02 - logprior: -1.2448e+00
Fitted a model with MAP estimate = -256.5736
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 260.5110 - loglik: -2.5740e+02 - logprior: -3.1145e+00
Epoch 2/10
19/19 - 1s - loss: 257.2159 - loglik: -2.5598e+02 - logprior: -1.2373e+00
Epoch 3/10
19/19 - 1s - loss: 256.4093 - loglik: -2.5529e+02 - logprior: -1.1198e+00
Epoch 4/10
19/19 - 1s - loss: 255.9714 - loglik: -2.5490e+02 - logprior: -1.0655e+00
Epoch 5/10
19/19 - 1s - loss: 255.1788 - loglik: -2.5414e+02 - logprior: -1.0348e+00
Epoch 6/10
19/19 - 1s - loss: 255.0092 - loglik: -2.5399e+02 - logprior: -1.0150e+00
Epoch 7/10
19/19 - 1s - loss: 254.0522 - loglik: -2.5303e+02 - logprior: -1.0143e+00
Epoch 8/10
19/19 - 1s - loss: 253.2169 - loglik: -2.5219e+02 - logprior: -1.0156e+00
Epoch 9/10
19/19 - 1s - loss: 252.2894 - loglik: -2.5126e+02 - logprior: -1.0183e+00
Epoch 10/10
19/19 - 1s - loss: 252.0427 - loglik: -2.5099e+02 - logprior: -1.0396e+00
Fitted a model with MAP estimate = -250.9666
Time for alignment: 44.1690
Computed alignments with likelihoods: ['-250.9516', '-250.9545', '-251.8638', '-250.9176', '-250.9666']
Best model has likelihood: -250.9176  (prior= -1.0371 )
time for generating output: 0.0929
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00046.projection.fasta
SP score = 0.9288194444444444
Training of 5 independent models on file PF01814.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6f728e460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff718e639a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff67f7227f0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 733.8840 - loglik: -7.3090e+02 - logprior: -2.9826e+00
Epoch 2/10
19/19 - 3s - loss: 705.3696 - loglik: -7.0435e+02 - logprior: -1.0163e+00
Epoch 3/10
19/19 - 3s - loss: 693.4150 - loglik: -6.9217e+02 - logprior: -1.2427e+00
Epoch 4/10
19/19 - 3s - loss: 688.8066 - loglik: -6.8765e+02 - logprior: -1.1575e+00
Epoch 5/10
19/19 - 3s - loss: 684.2211 - loglik: -6.8305e+02 - logprior: -1.1631e+00
Epoch 6/10
19/19 - 3s - loss: 681.0710 - loglik: -6.7994e+02 - logprior: -1.1245e+00
Epoch 7/10
19/19 - 3s - loss: 676.7922 - loglik: -6.7566e+02 - logprior: -1.1176e+00
Epoch 8/10
19/19 - 3s - loss: 671.6400 - loglik: -6.7050e+02 - logprior: -1.1193e+00
Epoch 9/10
19/19 - 3s - loss: 655.2502 - loglik: -6.5410e+02 - logprior: -1.1255e+00
Epoch 10/10
19/19 - 3s - loss: 631.7105 - loglik: -6.3055e+02 - logprior: -1.1236e+00
Fitted a model with MAP estimate = -619.5342
expansions: [(15, 1), (18, 1), (20, 1), (24, 3), (25, 1), (28, 1), (35, 1), (54, 2), (68, 1), (75, 1), (82, 1), (84, 2), (86, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 745.1013 - loglik: -7.4111e+02 - logprior: -3.9872e+00
Epoch 2/2
19/19 - 4s - loss: 710.9825 - loglik: -7.0887e+02 - logprior: -2.1084e+00
Fitted a model with MAP estimate = -700.1865
expansions: [(0, 5), (20, 7), (21, 2), (25, 3), (27, 1), (28, 2), (63, 2), (86, 5), (98, 1)]
discards: [ 0 22 23 29 31 32 33 35 36 37 38 64 65 87 88 89 90 95 99]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 697.6128 - loglik: -6.9470e+02 - logprior: -2.9097e+00
Epoch 2/2
19/19 - 4s - loss: 686.7566 - loglik: -6.8560e+02 - logprior: -1.1536e+00
Fitted a model with MAP estimate = -684.3517
expansions: [(94, 1), (103, 1)]
discards: [ 1  2  3  4 31 71]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 688.9814 - loglik: -6.8630e+02 - logprior: -2.6837e+00
Epoch 2/10
19/19 - 4s - loss: 683.8205 - loglik: -6.8294e+02 - logprior: -8.8344e-01
Epoch 3/10
19/19 - 4s - loss: 681.5944 - loglik: -6.8072e+02 - logprior: -8.7347e-01
Epoch 4/10
19/19 - 4s - loss: 680.6931 - loglik: -6.7979e+02 - logprior: -8.9855e-01
Epoch 5/10
19/19 - 4s - loss: 677.3915 - loglik: -6.7654e+02 - logprior: -8.4688e-01
Epoch 6/10
19/19 - 4s - loss: 673.7480 - loglik: -6.7293e+02 - logprior: -8.1194e-01
Epoch 7/10
19/19 - 4s - loss: 670.9793 - loglik: -6.7020e+02 - logprior: -7.7262e-01
Epoch 8/10
19/19 - 5s - loss: 667.1506 - loglik: -6.6638e+02 - logprior: -7.5414e-01
Epoch 9/10
19/19 - 4s - loss: 653.6828 - loglik: -6.5292e+02 - logprior: -7.4262e-01
Epoch 10/10
19/19 - 4s - loss: 620.9761 - loglik: -6.2019e+02 - logprior: -7.5617e-01
Fitted a model with MAP estimate = -604.1720
Time for alignment: 126.3499
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 733.8284 - loglik: -7.3085e+02 - logprior: -2.9813e+00
Epoch 2/10
19/19 - 3s - loss: 704.5629 - loglik: -7.0355e+02 - logprior: -1.0176e+00
Epoch 3/10
19/19 - 3s - loss: 692.7453 - loglik: -6.9151e+02 - logprior: -1.2363e+00
Epoch 4/10
19/19 - 4s - loss: 688.9711 - loglik: -6.8783e+02 - logprior: -1.1353e+00
Epoch 5/10
19/19 - 4s - loss: 685.2866 - loglik: -6.8416e+02 - logprior: -1.1219e+00
Epoch 6/10
19/19 - 4s - loss: 680.9168 - loglik: -6.7980e+02 - logprior: -1.1117e+00
Epoch 7/10
19/19 - 4s - loss: 677.3528 - loglik: -6.7623e+02 - logprior: -1.1084e+00
Epoch 8/10
19/19 - 4s - loss: 669.7877 - loglik: -6.6865e+02 - logprior: -1.1190e+00
Epoch 9/10
19/19 - 4s - loss: 653.6887 - loglik: -6.5253e+02 - logprior: -1.1303e+00
Epoch 10/10
19/19 - 4s - loss: 629.8781 - loglik: -6.2871e+02 - logprior: -1.1339e+00
Fitted a model with MAP estimate = -618.6421
expansions: [(15, 1), (18, 1), (20, 1), (25, 3), (27, 1), (29, 1), (32, 1), (54, 2), (68, 1), (76, 1), (82, 1), (84, 2), (86, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 745.0435 - loglik: -7.4107e+02 - logprior: -3.9743e+00
Epoch 2/2
19/19 - 4s - loss: 711.9832 - loglik: -7.0985e+02 - logprior: -2.1312e+00
Fitted a model with MAP estimate = -700.4213
expansions: [(0, 5), (20, 2), (27, 2), (28, 2), (29, 3), (32, 3), (63, 3), (86, 4), (87, 3), (94, 3), (97, 2), (98, 1)]
discards: [ 0 21 22 23 24 25 33 36 37 64 65 88 89 90 91 92 99]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 697.3733 - loglik: -6.9446e+02 - logprior: -2.9126e+00
Epoch 2/2
19/19 - 6s - loss: 686.4642 - loglik: -6.8532e+02 - logprior: -1.1413e+00
Fitted a model with MAP estimate = -683.4667
expansions: [(39, 1), (70, 1)]
discards: [  1   2   3   4  26  37  48  97  98 111 112 113]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 688.6284 - loglik: -6.8599e+02 - logprior: -2.6355e+00
Epoch 2/10
19/19 - 5s - loss: 683.3842 - loglik: -6.8256e+02 - logprior: -8.2150e-01
Epoch 3/10
19/19 - 5s - loss: 681.6082 - loglik: -6.8078e+02 - logprior: -8.2230e-01
Epoch 4/10
19/19 - 5s - loss: 679.6533 - loglik: -6.7889e+02 - logprior: -7.5723e-01
Epoch 5/10
19/19 - 5s - loss: 677.0264 - loglik: -6.7630e+02 - logprior: -7.2593e-01
Epoch 6/10
19/19 - 5s - loss: 673.5988 - loglik: -6.7291e+02 - logprior: -6.8383e-01
Epoch 7/10
19/19 - 5s - loss: 670.3830 - loglik: -6.6971e+02 - logprior: -6.5785e-01
Epoch 8/10
19/19 - 5s - loss: 666.5219 - loglik: -6.6587e+02 - logprior: -6.3721e-01
Epoch 9/10
19/19 - 5s - loss: 652.9190 - loglik: -6.5226e+02 - logprior: -6.3953e-01
Epoch 10/10
19/19 - 5s - loss: 620.3253 - loglik: -6.1961e+02 - logprior: -6.9155e-01
Fitted a model with MAP estimate = -602.3120
Time for alignment: 145.1296
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 734.3170 - loglik: -7.3133e+02 - logprior: -2.9825e+00
Epoch 2/10
19/19 - 4s - loss: 706.5141 - loglik: -7.0550e+02 - logprior: -1.0139e+00
Epoch 3/10
19/19 - 4s - loss: 692.9396 - loglik: -6.9169e+02 - logprior: -1.2471e+00
Epoch 4/10
19/19 - 4s - loss: 688.4964 - loglik: -6.8733e+02 - logprior: -1.1673e+00
Epoch 5/10
19/19 - 4s - loss: 683.9254 - loglik: -6.8274e+02 - logprior: -1.1782e+00
Epoch 6/10
19/19 - 4s - loss: 680.5589 - loglik: -6.7941e+02 - logprior: -1.1385e+00
Epoch 7/10
19/19 - 4s - loss: 677.1226 - loglik: -6.7599e+02 - logprior: -1.1181e+00
Epoch 8/10
19/19 - 4s - loss: 671.5867 - loglik: -6.7045e+02 - logprior: -1.1163e+00
Epoch 9/10
19/19 - 4s - loss: 656.0600 - loglik: -6.5491e+02 - logprior: -1.1294e+00
Epoch 10/10
19/19 - 4s - loss: 631.1001 - loglik: -6.2995e+02 - logprior: -1.1187e+00
Fitted a model with MAP estimate = -619.7621
expansions: [(15, 1), (18, 1), (20, 1), (24, 1), (26, 1), (29, 1), (31, 1), (35, 1), (53, 2), (82, 1), (84, 2), (86, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 745.4341 - loglik: -7.4144e+02 - logprior: -3.9906e+00
Epoch 2/2
19/19 - 5s - loss: 711.3306 - loglik: -7.0917e+02 - logprior: -2.1576e+00
Fitted a model with MAP estimate = -700.2216
expansions: [(0, 5), (20, 6), (21, 2), (28, 4), (29, 2), (33, 1), (61, 4), (89, 8)]
discards: [ 0 22 23 24 25 30 31 34 62 82 83 84 92 93 95 96]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 696.5734 - loglik: -6.9364e+02 - logprior: -2.9373e+00
Epoch 2/2
19/19 - 5s - loss: 685.8298 - loglik: -6.8475e+02 - logprior: -1.0766e+00
Fitted a model with MAP estimate = -683.5625
expansions: [(97, 2), (98, 5)]
discards: [  1   2   3   4  28  29  30 101 102 112]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 688.9279 - loglik: -6.8631e+02 - logprior: -2.6172e+00
Epoch 2/10
19/19 - 5s - loss: 683.3205 - loglik: -6.8248e+02 - logprior: -8.4104e-01
Epoch 3/10
19/19 - 5s - loss: 681.4465 - loglik: -6.8059e+02 - logprior: -8.5357e-01
Epoch 4/10
19/19 - 5s - loss: 678.9415 - loglik: -6.7811e+02 - logprior: -8.3017e-01
Epoch 5/10
19/19 - 5s - loss: 676.8235 - loglik: -6.7601e+02 - logprior: -8.0942e-01
Epoch 6/10
19/19 - 5s - loss: 672.6354 - loglik: -6.7184e+02 - logprior: -7.8332e-01
Epoch 7/10
19/19 - 5s - loss: 669.4133 - loglik: -6.6865e+02 - logprior: -7.5488e-01
Epoch 8/10
19/19 - 5s - loss: 665.2841 - loglik: -6.6453e+02 - logprior: -7.3802e-01
Epoch 9/10
19/19 - 5s - loss: 651.6860 - loglik: -6.5093e+02 - logprior: -7.3699e-01
Epoch 10/10
19/19 - 6s - loss: 619.4344 - loglik: -6.1864e+02 - logprior: -7.7020e-01
Fitted a model with MAP estimate = -601.4394
Time for alignment: 150.9992
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 733.5900 - loglik: -7.3061e+02 - logprior: -2.9841e+00
Epoch 2/10
19/19 - 4s - loss: 705.1494 - loglik: -7.0412e+02 - logprior: -1.0250e+00
Epoch 3/10
19/19 - 4s - loss: 693.4994 - loglik: -6.9225e+02 - logprior: -1.2481e+00
Epoch 4/10
19/19 - 4s - loss: 688.4317 - loglik: -6.8728e+02 - logprior: -1.1460e+00
Epoch 5/10
19/19 - 4s - loss: 684.9218 - loglik: -6.8376e+02 - logprior: -1.1525e+00
Epoch 6/10
19/19 - 4s - loss: 680.4595 - loglik: -6.7932e+02 - logprior: -1.1343e+00
Epoch 7/10
19/19 - 4s - loss: 676.8856 - loglik: -6.7576e+02 - logprior: -1.1180e+00
Epoch 8/10
19/19 - 4s - loss: 670.2349 - loglik: -6.6909e+02 - logprior: -1.1259e+00
Epoch 9/10
19/19 - 4s - loss: 654.6689 - loglik: -6.5352e+02 - logprior: -1.1291e+00
Epoch 10/10
19/19 - 4s - loss: 630.2941 - loglik: -6.2914e+02 - logprior: -1.1175e+00
Fitted a model with MAP estimate = -618.7532
expansions: [(15, 1), (18, 1), (20, 1), (24, 3), (25, 1), (28, 1), (35, 1), (54, 2), (68, 1), (75, 1), (82, 1), (84, 2), (86, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 744.9859 - loglik: -7.4100e+02 - logprior: -3.9855e+00
Epoch 2/2
19/19 - 5s - loss: 711.5747 - loglik: -7.0944e+02 - logprior: -2.1352e+00
Fitted a model with MAP estimate = -700.1188
expansions: [(0, 5), (20, 6), (21, 2), (25, 3), (27, 1), (28, 2), (63, 4), (86, 5), (92, 7)]
discards: [  0  22  23  29  31  32  33  35  36  37  38  64  65  87  88  89  90  95
  96  98  99 103]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 697.2435 - loglik: -6.9434e+02 - logprior: -2.9051e+00
Epoch 2/2
19/19 - 5s - loss: 686.5213 - loglik: -6.8542e+02 - logprior: -1.1008e+00
Fitted a model with MAP estimate = -683.6176
expansions: [(40, 1), (95, 1), (105, 2), (117, 1)]
discards: [  1   2   3   4  70 113]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 687.8171 - loglik: -6.8518e+02 - logprior: -2.6390e+00
Epoch 2/10
19/19 - 5s - loss: 682.0164 - loglik: -6.8117e+02 - logprior: -8.4656e-01
Epoch 3/10
19/19 - 5s - loss: 680.7872 - loglik: -6.7995e+02 - logprior: -8.3432e-01
Epoch 4/10
19/19 - 6s - loss: 678.2945 - loglik: -6.7748e+02 - logprior: -8.0959e-01
Epoch 5/10
19/19 - 6s - loss: 676.0666 - loglik: -6.7529e+02 - logprior: -7.6766e-01
Epoch 6/10
19/19 - 6s - loss: 672.3664 - loglik: -6.7163e+02 - logprior: -7.2974e-01
Epoch 7/10
19/19 - 6s - loss: 669.0059 - loglik: -6.6829e+02 - logprior: -7.0621e-01
Epoch 8/10
19/19 - 5s - loss: 664.8950 - loglik: -6.6419e+02 - logprior: -6.8628e-01
Epoch 9/10
19/19 - 6s - loss: 650.3351 - loglik: -6.4963e+02 - logprior: -6.8525e-01
Epoch 10/10
19/19 - 6s - loss: 616.6902 - loglik: -6.1594e+02 - logprior: -7.2394e-01
Fitted a model with MAP estimate = -598.8760
Time for alignment: 149.6228
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 733.7318 - loglik: -7.3075e+02 - logprior: -2.9850e+00
Epoch 2/10
19/19 - 4s - loss: 704.0556 - loglik: -7.0303e+02 - logprior: -1.0218e+00
Epoch 3/10
19/19 - 4s - loss: 693.4846 - loglik: -6.9225e+02 - logprior: -1.2320e+00
Epoch 4/10
19/19 - 4s - loss: 689.9846 - loglik: -6.8886e+02 - logprior: -1.1200e+00
Epoch 5/10
19/19 - 4s - loss: 684.8134 - loglik: -6.8367e+02 - logprior: -1.1347e+00
Epoch 6/10
19/19 - 4s - loss: 680.3664 - loglik: -6.7923e+02 - logprior: -1.1289e+00
Epoch 7/10
19/19 - 4s - loss: 677.0368 - loglik: -6.7591e+02 - logprior: -1.1160e+00
Epoch 8/10
19/19 - 4s - loss: 669.6223 - loglik: -6.6849e+02 - logprior: -1.1187e+00
Epoch 9/10
19/19 - 4s - loss: 654.5571 - loglik: -6.5341e+02 - logprior: -1.1208e+00
Epoch 10/10
19/19 - 4s - loss: 630.9973 - loglik: -6.2985e+02 - logprior: -1.1153e+00
Fitted a model with MAP estimate = -618.3622
expansions: [(15, 1), (18, 1), (20, 1), (24, 3), (25, 1), (28, 1), (31, 1), (54, 2), (68, 1), (82, 1), (84, 2), (86, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 745.0516 - loglik: -7.4108e+02 - logprior: -3.9720e+00
Epoch 2/2
19/19 - 4s - loss: 712.5767 - loglik: -7.1045e+02 - logprior: -2.1245e+00
Fitted a model with MAP estimate = -701.1498
expansions: [(0, 5), (20, 7), (21, 2), (25, 3), (27, 1), (60, 1), (61, 3), (91, 11)]
discards: [ 0 22 23 29 31 32 33 35 36 37 39 62 63 64 84 85 86 87 88 89 94 95 97 98]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 697.6901 - loglik: -6.9476e+02 - logprior: -2.9265e+00
Epoch 2/2
19/19 - 4s - loss: 687.0092 - loglik: -6.8592e+02 - logprior: -1.0923e+00
Fitted a model with MAP estimate = -684.1724
expansions: [(36, 2), (40, 3), (73, 1)]
discards: [  1   2   3   4  67 107]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 688.7971 - loglik: -6.8617e+02 - logprior: -2.6283e+00
Epoch 2/10
19/19 - 4s - loss: 683.2684 - loglik: -6.8245e+02 - logprior: -8.1625e-01
Epoch 3/10
19/19 - 5s - loss: 681.5041 - loglik: -6.8071e+02 - logprior: -7.9187e-01
Epoch 4/10
19/19 - 5s - loss: 678.9377 - loglik: -6.7815e+02 - logprior: -7.8610e-01
Epoch 5/10
19/19 - 5s - loss: 676.9889 - loglik: -6.7625e+02 - logprior: -7.3305e-01
Epoch 6/10
19/19 - 5s - loss: 672.6757 - loglik: -6.7196e+02 - logprior: -7.0573e-01
Epoch 7/10
19/19 - 5s - loss: 670.2014 - loglik: -6.6951e+02 - logprior: -6.8273e-01
Epoch 8/10
19/19 - 5s - loss: 666.7425 - loglik: -6.6607e+02 - logprior: -6.6266e-01
Epoch 9/10
19/19 - 5s - loss: 651.2768 - loglik: -6.5059e+02 - logprior: -6.6929e-01
Epoch 10/10
19/19 - 5s - loss: 619.7983 - loglik: -6.1904e+02 - logprior: -7.2696e-01
Fitted a model with MAP estimate = -601.8973
Time for alignment: 138.8644
Computed alignments with likelihoods: ['-604.1720', '-602.3120', '-601.4394', '-598.8760', '-601.8973']
Best model has likelihood: -598.8760  (prior= -0.7524 )
time for generating output: 0.1543
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01814.projection.fasta
SP score = 0.8344760039177277
Training of 5 independent models on file PF00155.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6c4339e20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6909acfd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff676e27cd0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 1903.5068 - loglik: -1.9021e+03 - logprior: -1.4477e+00
Epoch 2/10
39/39 - 46s - loss: 1809.2678 - loglik: -1.8080e+03 - logprior: -1.3109e+00
Epoch 3/10
39/39 - 47s - loss: 1795.4966 - loglik: -1.7941e+03 - logprior: -1.3716e+00
Epoch 4/10
39/39 - 47s - loss: 1786.7363 - loglik: -1.7854e+03 - logprior: -1.3414e+00
Epoch 5/10
39/39 - 43s - loss: 1779.7935 - loglik: -1.7785e+03 - logprior: -1.3241e+00
Epoch 6/10
39/39 - 40s - loss: 1769.1925 - loglik: -1.7679e+03 - logprior: -1.2720e+00
Epoch 7/10
39/39 - 40s - loss: 1744.8113 - loglik: -1.7435e+03 - logprior: -1.2752e+00
Epoch 8/10
39/39 - 38s - loss: 1696.9584 - loglik: -1.6955e+03 - logprior: -1.4730e+00
Epoch 9/10
39/39 - 39s - loss: 1523.6877 - loglik: -1.5208e+03 - logprior: -2.7992e+00
Epoch 10/10
39/39 - 40s - loss: 1379.2913 - loglik: -1.3737e+03 - logprior: -5.4780e+00
Fitted a model with MAP estimate = -1358.5167
expansions: [(0, 1)]
discards: [  1   2   3   4  43  67  68  69  70  71  72  73  74  75  76  77  78  79
  80  81  88  89  90  91  92  93  94 103 104 105 106 107 108 109 110 111
 112 113 114 115 116 123 124 125 126 129 130 131 132 133 134 135 136 137
 138 200 201 202 203 204 205 220 221 222 223 224 225 226 227 228 229 234
 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252
 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270
 271 272 273 274 275 276 277 278 279]
Re-initialized the encoder parameters.
Fitting a model of length 164 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 2009.6737 - loglik: -2.0071e+03 - logprior: -2.6034e+00
Epoch 2/2
39/39 - 17s - loss: 1979.8644 - loglik: -1.9798e+03 - logprior: -2.6888e-02
Fitted a model with MAP estimate = -1939.5537
expansions: [(0, 65), (146, 59), (158, 13), (160, 30), (161, 2), (164, 105)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115]
Re-initialized the encoder parameters.
Fitting a model of length 322 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 57s - loss: 1866.6677 - loglik: -1.8651e+03 - logprior: -1.5529e+00
Epoch 2/2
39/39 - 61s - loss: 1793.9551 - loglik: -1.7932e+03 - logprior: -7.5271e-01
Fitted a model with MAP estimate = -1768.9001
expansions: [(122, 1), (193, 1), (194, 1), (229, 1), (231, 1), (232, 1), (233, 1), (245, 1), (251, 1), (252, 1), (253, 2), (255, 1), (267, 2), (268, 1), (270, 2), (272, 1), (284, 2), (285, 2), (286, 1), (289, 1), (294, 1), (301, 1), (304, 2), (305, 1), (313, 2), (314, 1), (315, 1), (316, 1)]
discards: [  0   1 212]
Re-initialized the encoder parameters.
Fitting a model of length 354 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 74s - loss: 1773.5168 - loglik: -1.7714e+03 - logprior: -2.0942e+00
Epoch 2/10
39/39 - 72s - loss: 1762.4775 - loglik: -1.7622e+03 - logprior: -2.5714e-01
Epoch 3/10
39/39 - 71s - loss: 1756.3365 - loglik: -1.7563e+03 - logprior: -9.8822e-03
Epoch 4/10
39/39 - 69s - loss: 1746.5048 - loglik: -1.7466e+03 - logprior: 0.0803
Epoch 5/10
39/39 - 65s - loss: 1737.7532 - loglik: -1.7379e+03 - logprior: 0.1707
Epoch 6/10
39/39 - 67s - loss: 1723.1100 - loglik: -1.7233e+03 - logprior: 0.2180
Epoch 7/10
39/39 - 68s - loss: 1686.2313 - loglik: -1.6864e+03 - logprior: 0.1868
Epoch 8/10
39/39 - 69s - loss: 1537.5393 - loglik: -1.5373e+03 - logprior: -2.2686e-01
Epoch 9/10
39/39 - 62s - loss: 1381.9548 - loglik: -1.3810e+03 - logprior: -8.7243e-01
Epoch 10/10
39/39 - 58s - loss: 1343.6962 - loglik: -1.3427e+03 - logprior: -9.3014e-01
Fitted a model with MAP estimate = -1341.1949
Time for alignment: 1471.1003
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 1903.0265 - loglik: -1.9016e+03 - logprior: -1.4250e+00
Epoch 2/10
39/39 - 39s - loss: 1808.9075 - loglik: -1.8075e+03 - logprior: -1.3615e+00
Epoch 3/10
39/39 - 42s - loss: 1795.7928 - loglik: -1.7944e+03 - logprior: -1.3598e+00
Epoch 4/10
39/39 - 40s - loss: 1787.5178 - loglik: -1.7862e+03 - logprior: -1.3014e+00
Epoch 5/10
39/39 - 37s - loss: 1779.7340 - loglik: -1.7784e+03 - logprior: -1.2944e+00
Epoch 6/10
39/39 - 37s - loss: 1769.6484 - loglik: -1.7683e+03 - logprior: -1.2829e+00
Epoch 7/10
39/39 - 39s - loss: 1744.6196 - loglik: -1.7433e+03 - logprior: -1.2919e+00
Epoch 8/10
39/39 - 41s - loss: 1674.0316 - loglik: -1.6726e+03 - logprior: -1.3739e+00
Epoch 9/10
39/39 - 36s - loss: 1434.0345 - loglik: -1.4300e+03 - logprior: -4.0197e+00
Epoch 10/10
39/39 - 32s - loss: 1369.2235 - loglik: -1.3651e+03 - logprior: -4.0860e+00
Fitted a model with MAP estimate = -1355.0157
expansions: [(0, 2)]
discards: [  1   2   3   4  31  40  72  73  74  75  76  77  78  79  80  81  82  83
  84  85  86 122 123 124 125 128 129 130 131 132 133 134 135 136 137 138
 139 140 141 165 166 167 168 169 170 206 207 208 209 210 211 212 213 214
 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232
 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250
 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268
 269 270 271 272 273 274 275 276 277 278 279]
Re-initialized the encoder parameters.
Fitting a model of length 163 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 2008.7904 - loglik: -2.0061e+03 - logprior: -2.6526e+00
Epoch 2/2
39/39 - 14s - loss: 1982.3005 - loglik: -1.9823e+03 - logprior: -1.9996e-02
Fitted a model with MAP estimate = -1943.3728
expansions: [(0, 126), (128, 33), (161, 27), (163, 77)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80 105 106]
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 44s - loss: 1886.7910 - loglik: -1.8850e+03 - logprior: -1.7625e+00
Epoch 2/2
39/39 - 43s - loss: 1805.2432 - loglik: -1.8044e+03 - logprior: -8.6664e-01
Fitted a model with MAP estimate = -1777.7517
expansions: [(18, 1), (30, 2), (36, 1), (51, 1), (146, 2), (170, 1), (177, 1), (185, 1), (186, 3), (190, 1), (242, 1), (243, 1), (250, 1), (256, 2), (257, 2), (258, 1), (259, 4), (279, 2), (280, 1), (333, 1)]
discards: [  0 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219
 220]
Re-initialized the encoder parameters.
Fitting a model of length 354 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 54s - loss: 1780.6007 - loglik: -1.7786e+03 - logprior: -1.9707e+00
Epoch 2/10
39/39 - 53s - loss: 1766.2921 - loglik: -1.7661e+03 - logprior: -2.1251e-01
Epoch 3/10
39/39 - 52s - loss: 1761.3750 - loglik: -1.7613e+03 - logprior: -2.3519e-02
Epoch 4/10
39/39 - 54s - loss: 1753.4266 - loglik: -1.7536e+03 - logprior: 0.1594
Epoch 5/10
39/39 - 52s - loss: 1738.9541 - loglik: -1.7392e+03 - logprior: 0.2288
Epoch 6/10
39/39 - 49s - loss: 1726.2646 - loglik: -1.7265e+03 - logprior: 0.2781
Epoch 7/10
39/39 - 47s - loss: 1692.6591 - loglik: -1.6929e+03 - logprior: 0.3071
Epoch 8/10
39/39 - 46s - loss: 1548.5830 - loglik: -1.5485e+03 - logprior: -8.4275e-02
Epoch 9/10
39/39 - 46s - loss: 1384.1991 - loglik: -1.3835e+03 - logprior: -6.0508e-01
Epoch 10/10
39/39 - 46s - loss: 1344.6602 - loglik: -1.3438e+03 - logprior: -8.3684e-01
Fitted a model with MAP estimate = -1341.5339
Time for alignment: 1166.7601
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 1904.3866 - loglik: -1.9030e+03 - logprior: -1.4345e+00
Epoch 2/10
39/39 - 31s - loss: 1812.2969 - loglik: -1.8110e+03 - logprior: -1.3020e+00
Epoch 3/10
39/39 - 31s - loss: 1799.1382 - loglik: -1.7978e+03 - logprior: -1.3191e+00
Epoch 4/10
39/39 - 31s - loss: 1790.0547 - loglik: -1.7887e+03 - logprior: -1.3102e+00
Epoch 5/10
39/39 - 31s - loss: 1782.1700 - loglik: -1.7808e+03 - logprior: -1.3192e+00
Epoch 6/10
39/39 - 32s - loss: 1772.2290 - loglik: -1.7709e+03 - logprior: -1.2992e+00
Epoch 7/10
39/39 - 33s - loss: 1746.5729 - loglik: -1.7452e+03 - logprior: -1.3049e+00
Epoch 8/10
39/39 - 33s - loss: 1675.3206 - loglik: -1.6739e+03 - logprior: -1.3865e+00
Epoch 9/10
39/39 - 34s - loss: 1434.7866 - loglik: -1.4307e+03 - logprior: -4.0687e+00
Epoch 10/10
39/39 - 34s - loss: 1369.2678 - loglik: -1.3648e+03 - logprior: -4.3995e+00
Fitted a model with MAP estimate = -1355.5645
expansions: [(0, 1)]
discards: [  1   2   3   4  42  43 116 117 118 119 120 121 122 124 125 137 138 201
 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219
 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237
 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255
 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273
 274 275 276 277 278 279]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 2009.2122 - loglik: -2.0068e+03 - logprior: -2.3682e+00
Epoch 2/2
39/39 - 18s - loss: 1982.3790 - loglik: -1.9825e+03 - logprior: 0.1482
Fitted a model with MAP estimate = -1949.7781
expansions: [(0, 118), (185, 172)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 168 169 170
 171 172 173 174 175 176 177 178 179 180 181 182 183 184]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 41s - loss: 1894.9775 - loglik: -1.8930e+03 - logprior: -1.9389e+00
Epoch 2/2
39/39 - 39s - loss: 1805.9297 - loglik: -1.8050e+03 - logprior: -9.5643e-01
Fitted a model with MAP estimate = -1780.5896
expansions: [(0, 2), (23, 1), (27, 2), (28, 2), (34, 1), (44, 2), (45, 2), (46, 1), (49, 1), (61, 1), (64, 1), (87, 1), (88, 1), (89, 1), (93, 1), (95, 1), (102, 1), (110, 4), (111, 7), (112, 2), (123, 10), (155, 1), (157, 2), (191, 1), (194, 1), (212, 3), (215, 1), (218, 1), (254, 1), (256, 1), (277, 1), (288, 2), (289, 1)]
discards: [  0 118 119 120 121]
Re-initialized the encoder parameters.
Fitting a model of length 355 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 66s - loss: 1771.1788 - loglik: -1.7695e+03 - logprior: -1.7024e+00
Epoch 2/10
39/39 - 62s - loss: 1758.7469 - loglik: -1.7582e+03 - logprior: -5.4806e-01
Epoch 3/10
39/39 - 57s - loss: 1753.7009 - loglik: -1.7533e+03 - logprior: -3.6518e-01
Epoch 4/10
39/39 - 56s - loss: 1743.7322 - loglik: -1.7434e+03 - logprior: -2.8067e-01
Epoch 5/10
39/39 - 56s - loss: 1735.9277 - loglik: -1.7357e+03 - logprior: -2.1307e-01
Epoch 6/10
39/39 - 61s - loss: 1721.7502 - loglik: -1.7215e+03 - logprior: -1.9231e-01
Epoch 7/10
39/39 - 62s - loss: 1684.6569 - loglik: -1.6844e+03 - logprior: -2.4857e-01
Epoch 8/10
39/39 - 62s - loss: 1527.0918 - loglik: -1.5264e+03 - logprior: -6.9179e-01
Epoch 9/10
39/39 - 60s - loss: 1378.9232 - loglik: -1.3775e+03 - logprior: -1.3442e+00
Epoch 10/10
39/39 - 59s - loss: 1344.5214 - loglik: -1.3421e+03 - logprior: -2.3533e+00
Fitted a model with MAP estimate = -1343.0274
Time for alignment: 1217.7516
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 1902.8774 - loglik: -1.9014e+03 - logprior: -1.4382e+00
Epoch 2/10
39/39 - 40s - loss: 1809.1320 - loglik: -1.8078e+03 - logprior: -1.3505e+00
Epoch 3/10
39/39 - 42s - loss: 1795.5281 - loglik: -1.7941e+03 - logprior: -1.4034e+00
Epoch 4/10
39/39 - 42s - loss: 1787.6699 - loglik: -1.7863e+03 - logprior: -1.3266e+00
Epoch 5/10
39/39 - 42s - loss: 1780.1512 - loglik: -1.7788e+03 - logprior: -1.3002e+00
Epoch 6/10
39/39 - 45s - loss: 1769.7590 - loglik: -1.7684e+03 - logprior: -1.3216e+00
Epoch 7/10
39/39 - 45s - loss: 1745.3882 - loglik: -1.7440e+03 - logprior: -1.3231e+00
Epoch 8/10
39/39 - 44s - loss: 1680.3147 - loglik: -1.6788e+03 - logprior: -1.4422e+00
Epoch 9/10
39/39 - 40s - loss: 1445.3589 - loglik: -1.4413e+03 - logprior: -4.0120e+00
Epoch 10/10
39/39 - 38s - loss: 1372.3420 - loglik: -1.3669e+03 - logprior: -5.3530e+00
Fitted a model with MAP estimate = -1355.9269
expansions: [(0, 2)]
discards: [  1   2   3   4  31  72  73  74  75  76  77  78  79 123 124 125 126 129
 130 131 132 133 134 135 136 137 138 203 204 205 206 207 208 209 210 211
 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229
 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247
 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265
 266 267 268 269 270 271 272 273 274 275 276 277 278 279]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 2008.7191 - loglik: -2.0062e+03 - logprior: -2.4972e+00
Epoch 2/2
39/39 - 19s - loss: 1983.1838 - loglik: -1.9833e+03 - logprior: 0.1309
Fitted a model with MAP estimate = -1950.8781
expansions: [(0, 107), (178, 198)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159]
Re-initialized the encoder parameters.
Fitting a model of length 323 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 56s - loss: 1905.6310 - loglik: -1.9038e+03 - logprior: -1.8799e+00
Epoch 2/2
39/39 - 58s - loss: 1810.5818 - loglik: -1.8085e+03 - logprior: -2.0526e+00
Fitted a model with MAP estimate = -1781.8696
expansions: [(140, 1), (142, 1), (144, 1), (145, 1), (150, 1), (157, 1), (161, 1), (164, 1), (165, 3), (181, 1), (182, 1), (183, 1), (186, 1), (187, 1), (188, 2), (190, 1), (191, 2), (192, 1), (203, 1), (206, 2), (207, 1), (210, 2), (211, 1), (212, 2), (216, 1), (217, 1), (223, 1), (232, 1), (234, 3), (235, 4), (236, 3), (237, 3), (238, 2), (239, 1), (251, 1), (253, 3), (254, 2), (255, 2), (257, 1), (267, 1), (268, 2), (269, 1), (271, 2), (272, 2), (281, 1), (285, 2), (286, 1), (288, 1), (289, 1), (304, 2), (305, 1), (313, 2), (314, 1), (315, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45]
Re-initialized the encoder parameters.
Fitting a model of length 358 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 69s - loss: 1774.3121 - loglik: -1.7726e+03 - logprior: -1.7141e+00
Epoch 2/10
39/39 - 67s - loss: 1757.3804 - loglik: -1.7570e+03 - logprior: -4.1557e-01
Epoch 3/10
39/39 - 67s - loss: 1753.7710 - loglik: -1.7535e+03 - logprior: -2.8106e-01
Epoch 4/10
39/39 - 59s - loss: 1744.3831 - loglik: -1.7442e+03 - logprior: -2.0374e-01
Epoch 5/10
39/39 - 59s - loss: 1736.5599 - loglik: -1.7364e+03 - logprior: -1.3216e-01
Epoch 6/10
39/39 - 60s - loss: 1721.8022 - loglik: -1.7217e+03 - logprior: -1.0761e-01
Epoch 7/10
39/39 - 63s - loss: 1684.4941 - loglik: -1.6843e+03 - logprior: -1.9445e-01
Epoch 8/10
39/39 - 68s - loss: 1533.8464 - loglik: -1.5332e+03 - logprior: -6.1625e-01
Epoch 9/10
39/39 - 71s - loss: 1379.6198 - loglik: -1.3783e+03 - logprior: -1.3044e+00
Epoch 10/10
39/39 - 71s - loss: 1345.0125 - loglik: -1.3425e+03 - logprior: -2.4473e+00
Fitted a model with MAP estimate = -1341.4678
Time for alignment: 1441.6612
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 44s - loss: 1902.6456 - loglik: -1.9012e+03 - logprior: -1.4349e+00
Epoch 2/10
39/39 - 44s - loss: 1810.2179 - loglik: -1.8090e+03 - logprior: -1.2363e+00
Epoch 3/10
39/39 - 41s - loss: 1798.1656 - loglik: -1.7970e+03 - logprior: -1.1873e+00
Epoch 4/10
39/39 - 40s - loss: 1790.5944 - loglik: -1.7894e+03 - logprior: -1.1708e+00
Epoch 5/10
39/39 - 39s - loss: 1782.2466 - loglik: -1.7810e+03 - logprior: -1.1982e+00
Epoch 6/10
39/39 - 40s - loss: 1772.5809 - loglik: -1.7715e+03 - logprior: -1.0978e+00
Epoch 7/10
39/39 - 39s - loss: 1746.8938 - loglik: -1.7457e+03 - logprior: -1.1442e+00
Epoch 8/10
39/39 - 39s - loss: 1681.1088 - loglik: -1.6798e+03 - logprior: -1.2667e+00
Epoch 9/10
39/39 - 41s - loss: 1443.0154 - loglik: -1.4391e+03 - logprior: -3.9071e+00
Epoch 10/10
39/39 - 41s - loss: 1370.5365 - loglik: -1.3661e+03 - logprior: -4.4036e+00
Fitted a model with MAP estimate = -1355.4005
expansions: [(0, 1)]
discards: [  1   2   3   4  42  72  73  74  75  76 130 131 132 133 134 135 136 137
 138 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213
 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231
 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249
 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267
 268 269 270 271 272 273 274 275 276 277 278 279]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 2009.1254 - loglik: -2.0069e+03 - logprior: -2.2360e+00
Epoch 2/2
39/39 - 17s - loss: 1980.5701 - loglik: -1.9807e+03 - logprior: 0.1173
Fitted a model with MAP estimate = -1944.7278
expansions: [(0, 186), (173, 16), (176, 16), (179, 78)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138]
Re-initialized the encoder parameters.
Fitting a model of length 336 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 47s - loss: 1873.5165 - loglik: -1.8719e+03 - logprior: -1.6658e+00
Epoch 2/2
39/39 - 45s - loss: 1797.0857 - loglik: -1.7964e+03 - logprior: -6.4841e-01
Fitted a model with MAP estimate = -1769.5587
expansions: [(18, 1), (28, 1), (29, 2), (39, 1), (50, 1), (52, 1), (77, 1), (143, 1), (173, 1), (199, 1), (233, 1), (236, 1), (238, 2), (239, 3), (247, 1), (253, 1), (254, 1), (255, 1), (256, 3), (257, 1)]
discards: [  0 282 283 284 285 286 287 288 298 299 300]
Re-initialized the encoder parameters.
Fitting a model of length 351 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 1772.1012 - loglik: -1.7701e+03 - logprior: -1.9754e+00
Epoch 2/10
39/39 - 44s - loss: 1759.6987 - loglik: -1.7594e+03 - logprior: -2.7910e-01
Epoch 3/10
39/39 - 44s - loss: 1756.0381 - loglik: -1.7559e+03 - logprior: -1.0363e-01
Epoch 4/10
39/39 - 44s - loss: 1746.6532 - loglik: -1.7467e+03 - logprior: 0.0052
Epoch 5/10
39/39 - 44s - loss: 1738.9231 - loglik: -1.7390e+03 - logprior: 0.0451
Epoch 6/10
39/39 - 45s - loss: 1724.5388 - loglik: -1.7246e+03 - logprior: 0.0757
Epoch 7/10
39/39 - 46s - loss: 1689.7256 - loglik: -1.6898e+03 - logprior: 0.0484
Epoch 8/10
39/39 - 47s - loss: 1553.3096 - loglik: -1.5530e+03 - logprior: -2.8887e-01
Epoch 9/10
39/39 - 48s - loss: 1388.5516 - loglik: -1.3874e+03 - logprior: -1.0854e+00
Epoch 10/10
39/39 - 49s - loss: 1346.2515 - loglik: -1.3450e+03 - logprior: -1.2385e+00
Fitted a model with MAP estimate = -1342.3522
Time for alignment: 1172.5976
Computed alignments with likelihoods: ['-1341.1949', '-1341.5339', '-1343.0274', '-1341.4678', '-1342.3522']
Best model has likelihood: -1341.1949  (prior= -0.8902 )
time for generating output: 1.3041
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00155.projection.fasta
SP score = 0.07618269767068735
Training of 5 independent models on file PF00450.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff688255dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff66e65c970>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff66ce8c0d0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 1881.8680 - loglik: -1.8804e+03 - logprior: -1.4825e+00
Epoch 2/10
39/39 - 52s - loss: 1748.5266 - loglik: -1.7472e+03 - logprior: -1.3250e+00
Epoch 3/10
39/39 - 54s - loss: 1738.1953 - loglik: -1.7368e+03 - logprior: -1.4293e+00
Epoch 4/10
39/39 - 51s - loss: 1734.8027 - loglik: -1.7333e+03 - logprior: -1.4682e+00
Epoch 5/10
39/39 - 48s - loss: 1730.6925 - loglik: -1.7292e+03 - logprior: -1.4994e+00
Epoch 6/10
39/39 - 47s - loss: 1729.7247 - loglik: -1.7282e+03 - logprior: -1.4905e+00
Epoch 7/10
39/39 - 46s - loss: 1725.3718 - loglik: -1.7238e+03 - logprior: -1.5089e+00
Epoch 8/10
39/39 - 46s - loss: 1719.2882 - loglik: -1.7177e+03 - logprior: -1.5558e+00
Epoch 9/10
39/39 - 46s - loss: 1699.0857 - loglik: -1.6974e+03 - logprior: -1.6484e+00
Epoch 10/10
39/39 - 45s - loss: 1579.3469 - loglik: -1.5735e+03 - logprior: -5.7758e+00
Fitted a model with MAP estimate = -1452.0805
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 221 227 228 229 234 235 255 256
 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274
 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292
 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310
 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 2017.8685 - loglik: -2.0159e+03 - logprior: -1.9603e+00
Epoch 2/2
39/39 - 12s - loss: 2000.7212 - loglik: -2.0002e+03 - logprior: -5.1594e-01
Fitted a model with MAP estimate = -1994.5850
expansions: [(0, 74), (2, 2), (77, 255)]
discards: [ 8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31
 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55
 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76]
Re-initialized the encoder parameters.
Fitting a model of length 339 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 54s - loss: 1874.3752 - loglik: -1.8722e+03 - logprior: -2.1508e+00
Epoch 2/2
39/39 - 51s - loss: 1755.4587 - loglik: -1.7543e+03 - logprior: -1.1835e+00
Fitted a model with MAP estimate = -1746.6069
expansions: [(78, 1), (97, 1), (132, 1), (141, 1), (184, 2), (185, 2), (186, 1), (234, 3), (235, 1), (240, 1), (241, 3), (242, 3), (251, 2), (252, 13), (256, 6), (259, 2), (260, 7), (261, 3), (262, 2), (264, 4), (265, 3), (266, 2), (267, 2), (268, 2), (269, 3), (272, 1), (293, 1), (294, 7), (319, 1), (326, 1), (330, 1), (332, 1), (334, 1)]
discards: [ 0  1  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26
 27 28 29 30 31 35]
Re-initialized the encoder parameters.
Fitting a model of length 394 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 70s - loss: 1730.7031 - loglik: -1.7284e+03 - logprior: -2.2913e+00
Epoch 2/10
39/39 - 69s - loss: 1713.2748 - loglik: -1.7126e+03 - logprior: -6.7589e-01
Epoch 3/10
39/39 - 69s - loss: 1709.9935 - loglik: -1.7097e+03 - logprior: -3.1869e-01
Epoch 4/10
39/39 - 68s - loss: 1706.6428 - loglik: -1.7064e+03 - logprior: -2.2974e-01
Epoch 5/10
39/39 - 69s - loss: 1706.6687 - loglik: -1.7064e+03 - logprior: -2.7455e-01
Fitted a model with MAP estimate = -1703.0017
Time for alignment: 1136.8520
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 1881.8425 - loglik: -1.8803e+03 - logprior: -1.5004e+00
Epoch 2/10
39/39 - 51s - loss: 1746.5081 - loglik: -1.7451e+03 - logprior: -1.4320e+00
Epoch 3/10
39/39 - 51s - loss: 1735.0477 - loglik: -1.7335e+03 - logprior: -1.5374e+00
Epoch 4/10
39/39 - 52s - loss: 1730.5344 - loglik: -1.7291e+03 - logprior: -1.4748e+00
Epoch 5/10
39/39 - 51s - loss: 1727.5460 - loglik: -1.7261e+03 - logprior: -1.4870e+00
Epoch 6/10
39/39 - 52s - loss: 1727.3059 - loglik: -1.7258e+03 - logprior: -1.4989e+00
Epoch 7/10
39/39 - 57s - loss: 1723.0494 - loglik: -1.7215e+03 - logprior: -1.5000e+00
Epoch 8/10
39/39 - 64s - loss: 1720.0903 - loglik: -1.7185e+03 - logprior: -1.5529e+00
Epoch 9/10
39/39 - 64s - loss: 1705.2140 - loglik: -1.7035e+03 - logprior: -1.6552e+00
Epoch 10/10
39/39 - 63s - loss: 1603.2686 - loglik: -1.5984e+03 - logprior: -4.8240e+00
Fitted a model with MAP estimate = -1453.3829
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 216 228 229
 230 231 232 233 234 235 252 253 254 255 256 257 262 263 264 265 266 288
 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306
 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324
 325]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 2015.6000 - loglik: -2.0138e+03 - logprior: -1.8389e+00
Epoch 2/2
39/39 - 12s - loss: 1998.8013 - loglik: -1.9982e+03 - logprior: -6.0592e-01
Fitted a model with MAP estimate = -1992.5916
expansions: [(0, 56), (109, 278)]
discards: [  8   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25
  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43
  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61
  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79
  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97
  98  99 100 101 102 103 104 105 106 107 108]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 71s - loss: 1871.2426 - loglik: -1.8695e+03 - logprior: -1.7886e+00
Epoch 2/2
39/39 - 75s - loss: 1749.3003 - loglik: -1.7479e+03 - logprior: -1.3771e+00
Fitted a model with MAP estimate = -1736.8477
expansions: [(81, 1), (115, 3), (117, 2), (123, 1), (156, 1), (159, 1), (161, 3), (162, 2), (178, 1), (180, 2), (181, 1), (182, 1), (196, 1), (197, 4), (202, 1), (206, 1), (208, 1), (209, 1), (210, 1), (211, 2), (212, 2), (213, 1), (232, 2), (233, 2), (239, 1), (240, 2), (241, 1), (255, 1), (262, 2), (264, 2), (265, 2), (271, 1), (272, 1), (274, 1), (277, 1), (278, 1), (286, 1), (297, 3), (298, 2), (299, 1), (300, 1), (301, 3), (307, 4), (312, 1), (330, 1), (333, 1), (334, 1), (335, 1), (337, 1)]
discards: [  1   5  16  17  20  22  23  38  39  41  42  43 214 215 216 217 218 219
 220]
Re-initialized the encoder parameters.
Fitting a model of length 398 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 94s - loss: 1723.5096 - loglik: -1.7218e+03 - logprior: -1.6654e+00
Epoch 2/10
39/39 - 89s - loss: 1708.4884 - loglik: -1.7081e+03 - logprior: -4.2312e-01
Epoch 3/10
39/39 - 91s - loss: 1705.3920 - loglik: -1.7051e+03 - logprior: -3.0853e-01
Epoch 4/10
39/39 - 95s - loss: 1702.7603 - loglik: -1.7025e+03 - logprior: -2.6495e-01
Epoch 5/10
39/39 - 99s - loss: 1700.5809 - loglik: -1.7003e+03 - logprior: -2.8943e-01
Epoch 6/10
39/39 - 92s - loss: 1698.4563 - loglik: -1.6982e+03 - logprior: -2.0729e-01
Epoch 7/10
39/39 - 90s - loss: 1693.5731 - loglik: -1.6934e+03 - logprior: -1.7455e-01
Epoch 8/10
39/39 - 95s - loss: 1689.2684 - loglik: -1.6890e+03 - logprior: -2.0349e-01
Epoch 9/10
39/39 - 100s - loss: 1656.3994 - loglik: -1.6558e+03 - logprior: -5.8610e-01
Epoch 10/10
39/39 - 96s - loss: 1497.3752 - loglik: -1.4947e+03 - logprior: -2.6572e+00
Fitted a model with MAP estimate = -1404.0117
Time for alignment: 1914.1420
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 65s - loss: 1883.2213 - loglik: -1.8817e+03 - logprior: -1.4728e+00
Epoch 2/10
39/39 - 62s - loss: 1747.8101 - loglik: -1.7464e+03 - logprior: -1.3801e+00
Epoch 3/10
39/39 - 63s - loss: 1736.9872 - loglik: -1.7355e+03 - logprior: -1.5052e+00
Epoch 4/10
39/39 - 67s - loss: 1734.6528 - loglik: -1.7332e+03 - logprior: -1.4634e+00
Epoch 5/10
39/39 - 72s - loss: 1731.7413 - loglik: -1.7303e+03 - logprior: -1.4453e+00
Epoch 6/10
39/39 - 69s - loss: 1728.5972 - loglik: -1.7271e+03 - logprior: -1.4947e+00
Epoch 7/10
39/39 - 71s - loss: 1725.3865 - loglik: -1.7239e+03 - logprior: -1.4972e+00
Epoch 8/10
39/39 - 67s - loss: 1721.0549 - loglik: -1.7195e+03 - logprior: -1.5469e+00
Epoch 9/10
39/39 - 65s - loss: 1700.3756 - loglik: -1.6987e+03 - logprior: -1.6510e+00
Epoch 10/10
39/39 - 63s - loss: 1563.2241 - loglik: -1.5569e+03 - logprior: -6.3277e+00
Fitted a model with MAP estimate = -1437.7609
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 216 222 228 229 230 235 236 258 259
 260 265 266 267 268 278 279 280 281 282 283 296 297 298 299 300 301 302
 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320
 321 322 323 324 325]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 2025.7460 - loglik: -2.0237e+03 - logprior: -2.0616e+00
Epoch 2/2
39/39 - 12s - loss: 1994.1779 - loglik: -1.9937e+03 - logprior: -5.1795e-01
Fitted a model with MAP estimate = -1981.0720
expansions: [(0, 38), (84, 2), (105, 282)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82]
Re-initialized the encoder parameters.
Fitting a model of length 344 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 59s - loss: 1869.8578 - loglik: -1.8681e+03 - logprior: -1.7144e+00
Epoch 2/2
39/39 - 51s - loss: 1747.0979 - loglik: -1.7458e+03 - logprior: -1.2747e+00
Fitted a model with MAP estimate = -1734.9318
expansions: [(82, 1), (117, 1), (120, 2), (121, 1), (122, 1), (123, 1), (159, 1), (160, 1), (162, 2), (163, 2), (179, 1), (182, 1), (183, 1), (196, 1), (198, 1), (199, 4), (204, 1), (205, 1), (208, 1), (210, 1), (211, 2), (212, 1), (213, 2), (214, 1), (225, 2), (232, 3), (240, 1), (241, 2), (242, 1), (250, 1), (255, 1), (263, 1), (265, 2), (269, 1), (273, 1), (274, 2), (277, 1), (298, 1), (299, 3), (302, 2), (303, 1), (304, 1), (308, 3), (316, 1), (333, 1), (336, 1), (337, 1), (338, 1), (339, 1)]
discards: [  0   1   5  15  16  17  18  19  20  21  22  23  24 215 216 217 218 219]
Re-initialized the encoder parameters.
Fitting a model of length 394 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 70s - loss: 1724.7125 - loglik: -1.7225e+03 - logprior: -2.1913e+00
Epoch 2/10
39/39 - 75s - loss: 1712.9484 - loglik: -1.7124e+03 - logprior: -5.1233e-01
Epoch 3/10
39/39 - 77s - loss: 1709.1599 - loglik: -1.7090e+03 - logprior: -1.9178e-01
Epoch 4/10
39/39 - 77s - loss: 1706.0701 - loglik: -1.7060e+03 - logprior: -1.0368e-01
Epoch 5/10
39/39 - 79s - loss: 1703.7059 - loglik: -1.7036e+03 - logprior: -5.9089e-02
Epoch 6/10
39/39 - 78s - loss: 1701.1046 - loglik: -1.7010e+03 - logprior: -5.8267e-02
Epoch 7/10
39/39 - 81s - loss: 1699.4229 - loglik: -1.6994e+03 - logprior: -4.4638e-02
Epoch 8/10
39/39 - 79s - loss: 1689.7446 - loglik: -1.6897e+03 - logprior: -4.0953e-02
Epoch 9/10
39/39 - 82s - loss: 1665.3281 - loglik: -1.6651e+03 - logprior: -1.9417e-01
Epoch 10/10
39/39 - 79s - loss: 1504.7051 - loglik: -1.5020e+03 - logprior: -2.6823e+00
Fitted a model with MAP estimate = -1413.1710
Time for alignment: 1780.4131
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 54s - loss: 1885.0768 - loglik: -1.8836e+03 - logprior: -1.5014e+00
Epoch 2/10
39/39 - 49s - loss: 1747.6284 - loglik: -1.7462e+03 - logprior: -1.4486e+00
Epoch 3/10
39/39 - 48s - loss: 1734.9479 - loglik: -1.7334e+03 - logprior: -1.5852e+00
Epoch 4/10
39/39 - 47s - loss: 1733.3529 - loglik: -1.7318e+03 - logprior: -1.5619e+00
Epoch 5/10
39/39 - 48s - loss: 1729.9729 - loglik: -1.7283e+03 - logprior: -1.6478e+00
Epoch 6/10
39/39 - 50s - loss: 1727.9707 - loglik: -1.7263e+03 - logprior: -1.6311e+00
Epoch 7/10
39/39 - 52s - loss: 1723.7390 - loglik: -1.7221e+03 - logprior: -1.6244e+00
Epoch 8/10
39/39 - 53s - loss: 1718.8131 - loglik: -1.7171e+03 - logprior: -1.6768e+00
Epoch 9/10
39/39 - 53s - loss: 1705.1783 - loglik: -1.7034e+03 - logprior: -1.7602e+00
Epoch 10/10
39/39 - 54s - loss: 1595.9059 - loglik: -1.5901e+03 - logprior: -5.8128e+00
Fitted a model with MAP estimate = -1446.7483
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 149 150 151 152 155 156 157 158 159 160 161 162 163 164 165
 166 167 168 169 217 224 230 231 232 233 234 235 236 247 257 258 259 263
 264 265 266 267 277 278 279 280 281 282 294 295 296 297 298 299 300 301
 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319
 320 321 322 323 324 325]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 2017.8356 - loglik: -2.0160e+03 - logprior: -1.8288e+00
Epoch 2/2
39/39 - 11s - loss: 1993.2648 - loglik: -1.9929e+03 - logprior: -3.3895e-01
Fitted a model with MAP estimate = -1981.6002
expansions: [(0, 43), (104, 283)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89]
Re-initialized the encoder parameters.
Fitting a model of length 340 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 53s - loss: 1864.5332 - loglik: -1.8628e+03 - logprior: -1.6998e+00
Epoch 2/2
39/39 - 51s - loss: 1741.5850 - loglik: -1.7403e+03 - logprior: -1.2513e+00
Fitted a model with MAP estimate = -1730.4765
expansions: [(77, 1), (108, 1), (111, 1), (114, 2), (116, 1), (117, 1), (118, 1), (125, 1), (150, 1), (152, 1), (153, 1), (155, 3), (156, 2), (171, 1), (173, 2), (174, 1), (179, 1), (189, 1), (190, 5), (195, 1), (199, 1), (201, 1), (202, 1), (203, 1), (204, 4), (205, 2), (225, 1), (231, 1), (232, 1), (233, 1), (234, 1), (248, 1), (249, 2), (255, 1), (257, 1), (258, 2), (260, 1), (266, 1), (269, 2), (290, 1), (291, 1), (292, 1), (296, 2), (297, 2), (302, 1), (305, 1), (325, 1), (331, 1), (332, 1), (333, 1), (335, 1)]
discards: [  1   5  19  20  21  30  31 213 214 215 216 217 218 219 220 221 222]
Re-initialized the encoder parameters.
Fitting a model of length 392 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 69s - loss: 1719.9128 - loglik: -1.7184e+03 - logprior: -1.4967e+00
Epoch 2/10
39/39 - 69s - loss: 1707.3838 - loglik: -1.7071e+03 - logprior: -2.9224e-01
Epoch 3/10
39/39 - 80s - loss: 1705.2576 - loglik: -1.7050e+03 - logprior: -2.2031e-01
Epoch 4/10
39/39 - 79s - loss: 1705.2151 - loglik: -1.7051e+03 - logprior: -1.4874e-01
Epoch 5/10
39/39 - 77s - loss: 1701.0559 - loglik: -1.7010e+03 - logprior: -8.2538e-02
Epoch 6/10
39/39 - 85s - loss: 1699.6469 - loglik: -1.6996e+03 - logprior: -3.0392e-02
Epoch 7/10
39/39 - 80s - loss: 1696.3191 - loglik: -1.6963e+03 - logprior: -3.4654e-02
Epoch 8/10
39/39 - 81s - loss: 1688.8534 - loglik: -1.6888e+03 - logprior: -3.9262e-02
Epoch 9/10
39/39 - 85s - loss: 1658.4889 - loglik: -1.6582e+03 - logprior: -3.0549e-01
Epoch 10/10
39/39 - 85s - loss: 1495.3365 - loglik: -1.4924e+03 - logprior: -2.8520e+00
Fitted a model with MAP estimate = -1410.1076
Time for alignment: 1616.5187
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 68s - loss: 1882.2618 - loglik: -1.8808e+03 - logprior: -1.5040e+00
Epoch 2/10
39/39 - 69s - loss: 1745.1975 - loglik: -1.7437e+03 - logprior: -1.5079e+00
Epoch 3/10
39/39 - 68s - loss: 1735.4047 - loglik: -1.7338e+03 - logprior: -1.6030e+00
Epoch 4/10
39/39 - 65s - loss: 1730.0510 - loglik: -1.7285e+03 - logprior: -1.5879e+00
Epoch 5/10
39/39 - 60s - loss: 1728.3975 - loglik: -1.7268e+03 - logprior: -1.5828e+00
Epoch 6/10
39/39 - 62s - loss: 1727.1919 - loglik: -1.7256e+03 - logprior: -1.6066e+00
Epoch 7/10
39/39 - 65s - loss: 1723.4847 - loglik: -1.7219e+03 - logprior: -1.6204e+00
Epoch 8/10
39/39 - 67s - loss: 1716.1923 - loglik: -1.7145e+03 - logprior: -1.6519e+00
Epoch 9/10
39/39 - 61s - loss: 1699.9707 - loglik: -1.6982e+03 - logprior: -1.7421e+00
Epoch 10/10
39/39 - 61s - loss: 1572.8619 - loglik: -1.5670e+03 - logprior: -5.8666e+00
Fitted a model with MAP estimate = -1439.0250
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 216 228 229 230 231 232 233 234 255
 256 257 262 263 264 265 275 276 277 278 279 280 281 292 293 308 309 310
 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 2019.4569 - loglik: -2.0176e+03 - logprior: -1.8921e+00
Epoch 2/2
39/39 - 12s - loss: 1997.5875 - loglik: -1.9971e+03 - logprior: -4.8531e-01
Fitted a model with MAP estimate = -1986.2014
expansions: [(0, 148), (113, 169)]
discards: [  2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37
  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55
  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73
  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91
  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109
 110 111 112]
Re-initialized the encoder parameters.
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 47s - loss: 1879.4153 - loglik: -1.8776e+03 - logprior: -1.7903e+00
Epoch 2/2
39/39 - 45s - loss: 1750.5187 - loglik: -1.7491e+03 - logprior: -1.3991e+00
Fitted a model with MAP estimate = -1740.4789
expansions: [(9, 1), (19, 2), (20, 1), (30, 1), (32, 1), (53, 1), (58, 1), (59, 2), (61, 1), (63, 1), (71, 1), (91, 1), (93, 1), (96, 2), (97, 1), (98, 1), (99, 1), (112, 1), (131, 1), (134, 1), (136, 3), (137, 1), (177, 1), (178, 4), (183, 1), (184, 1), (186, 2), (188, 1), (189, 3), (190, 4), (209, 3), (217, 2), (218, 1), (219, 1), (224, 1), (225, 2), (226, 1), (227, 1), (228, 2), (231, 1), (233, 2), (235, 2), (236, 2), (243, 2), (245, 1), (248, 1), (265, 1), (268, 1), (269, 4), (272, 2), (273, 2), (299, 1), (306, 1), (309, 1), (310, 1), (311, 1)]
discards: [  0 191 192 193 194 195 196 197 198]
Re-initialized the encoder parameters.
Fitting a model of length 394 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 70s - loss: 1723.1560 - loglik: -1.7209e+03 - logprior: -2.2798e+00
Epoch 2/10
39/39 - 71s - loss: 1707.9241 - loglik: -1.7075e+03 - logprior: -4.6891e-01
Epoch 3/10
39/39 - 74s - loss: 1703.7340 - loglik: -1.7035e+03 - logprior: -2.7670e-01
Epoch 4/10
39/39 - 85s - loss: 1703.5951 - loglik: -1.7034e+03 - logprior: -2.1614e-01
Epoch 5/10
39/39 - 82s - loss: 1700.7435 - loglik: -1.7005e+03 - logprior: -2.2622e-01
Epoch 6/10
39/39 - 78s - loss: 1693.7157 - loglik: -1.6936e+03 - logprior: -1.4850e-01
Epoch 7/10
39/39 - 83s - loss: 1696.0120 - loglik: -1.6959e+03 - logprior: -1.4786e-01
Fitted a model with MAP estimate = -1691.3178
Time for alignment: 1487.8553
Computed alignments with likelihoods: ['-1452.0805', '-1404.0117', '-1413.1710', '-1410.1076', '-1439.0250']
Best model has likelihood: -1404.0117  (prior= -4.3261 )
time for generating output: 0.4378
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00450.projection.fasta
SP score = 0.10838351088466883
Training of 5 independent models on file PF07679.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff699ec6bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6e6034c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff67f4aeb50>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 509.4239 - loglik: -5.0636e+02 - logprior: -3.0651e+00
Epoch 2/10
19/19 - 2s - loss: 476.8833 - loglik: -4.7561e+02 - logprior: -1.2713e+00
Epoch 3/10
19/19 - 2s - loss: 464.1133 - loglik: -4.6264e+02 - logprior: -1.4756e+00
Epoch 4/10
19/19 - 2s - loss: 461.1466 - loglik: -4.5978e+02 - logprior: -1.3694e+00
Epoch 5/10
19/19 - 2s - loss: 460.3379 - loglik: -4.5897e+02 - logprior: -1.3643e+00
Epoch 6/10
19/19 - 2s - loss: 459.0412 - loglik: -4.5771e+02 - logprior: -1.3296e+00
Epoch 7/10
19/19 - 2s - loss: 458.1139 - loglik: -4.5678e+02 - logprior: -1.3197e+00
Epoch 8/10
19/19 - 2s - loss: 457.1096 - loglik: -4.5579e+02 - logprior: -1.3061e+00
Epoch 9/10
19/19 - 2s - loss: 456.9717 - loglik: -4.5565e+02 - logprior: -1.3059e+00
Epoch 10/10
19/19 - 2s - loss: 456.0538 - loglik: -4.5474e+02 - logprior: -1.2978e+00
Fitted a model with MAP estimate = -455.5884
expansions: [(8, 1), (9, 2), (10, 3), (12, 1), (15, 1), (20, 1), (34, 1), (39, 1), (40, 1), (43, 2), (48, 1), (49, 2), (50, 2), (62, 1), (64, 2), (65, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 468.7331 - loglik: -4.6486e+02 - logprior: -3.8682e+00
Epoch 2/2
19/19 - 2s - loss: 458.0906 - loglik: -4.5608e+02 - logprior: -2.0069e+00
Fitted a model with MAP estimate = -456.1385
expansions: [(0, 2)]
discards: [ 0 12 64 66 84]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 457.7870 - loglik: -4.5492e+02 - logprior: -2.8688e+00
Epoch 2/2
19/19 - 2s - loss: 454.5198 - loglik: -4.5343e+02 - logprior: -1.0903e+00
Fitted a model with MAP estimate = -453.8349
expansions: [(54, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 458.9203 - loglik: -4.5521e+02 - logprior: -3.7117e+00
Epoch 2/10
19/19 - 2s - loss: 454.3779 - loglik: -4.5319e+02 - logprior: -1.1871e+00
Epoch 3/10
19/19 - 2s - loss: 453.7357 - loglik: -4.5281e+02 - logprior: -9.2742e-01
Epoch 4/10
19/19 - 2s - loss: 453.2728 - loglik: -4.5239e+02 - logprior: -8.7737e-01
Epoch 5/10
19/19 - 2s - loss: 451.7344 - loglik: -4.5087e+02 - logprior: -8.5603e-01
Epoch 6/10
19/19 - 2s - loss: 450.8178 - loglik: -4.4996e+02 - logprior: -8.4835e-01
Epoch 7/10
19/19 - 2s - loss: 449.6898 - loglik: -4.4884e+02 - logprior: -8.3775e-01
Epoch 8/10
19/19 - 2s - loss: 448.4147 - loglik: -4.4758e+02 - logprior: -8.2644e-01
Epoch 9/10
19/19 - 2s - loss: 447.6496 - loglik: -4.4682e+02 - logprior: -8.1874e-01
Epoch 10/10
19/19 - 2s - loss: 447.0228 - loglik: -4.4620e+02 - logprior: -8.1072e-01
Fitted a model with MAP estimate = -446.1763
Time for alignment: 70.1571
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 509.0374 - loglik: -5.0597e+02 - logprior: -3.0639e+00
Epoch 2/10
19/19 - 2s - loss: 477.6390 - loglik: -4.7640e+02 - logprior: -1.2409e+00
Epoch 3/10
19/19 - 2s - loss: 464.0716 - loglik: -4.6270e+02 - logprior: -1.3740e+00
Epoch 4/10
19/19 - 2s - loss: 462.4039 - loglik: -4.6113e+02 - logprior: -1.2726e+00
Epoch 5/10
19/19 - 2s - loss: 460.7410 - loglik: -4.5946e+02 - logprior: -1.2810e+00
Epoch 6/10
19/19 - 2s - loss: 459.7456 - loglik: -4.5849e+02 - logprior: -1.2503e+00
Epoch 7/10
19/19 - 2s - loss: 458.7967 - loglik: -4.5754e+02 - logprior: -1.2463e+00
Epoch 8/10
19/19 - 2s - loss: 457.8951 - loglik: -4.5665e+02 - logprior: -1.2320e+00
Epoch 9/10
19/19 - 2s - loss: 457.5267 - loglik: -4.5628e+02 - logprior: -1.2358e+00
Epoch 10/10
19/19 - 2s - loss: 457.5548 - loglik: -4.5631e+02 - logprior: -1.2272e+00
Fitted a model with MAP estimate = -456.3990
expansions: [(8, 1), (9, 2), (10, 3), (12, 1), (13, 1), (35, 1), (37, 2), (39, 2), (40, 3), (42, 4), (48, 2), (49, 1), (60, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 468.0337 - loglik: -4.6417e+02 - logprior: -3.8631e+00
Epoch 2/2
19/19 - 3s - loss: 457.8004 - loglik: -4.5581e+02 - logprior: -1.9910e+00
Fitted a model with MAP estimate = -456.0330
expansions: [(0, 2)]
discards: [ 0 12 46 50 53 68]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 457.7678 - loglik: -4.5490e+02 - logprior: -2.8645e+00
Epoch 2/2
19/19 - 2s - loss: 454.1987 - loglik: -4.5312e+02 - logprior: -1.0826e+00
Fitted a model with MAP estimate = -453.6328
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 458.8029 - loglik: -4.5510e+02 - logprior: -3.6993e+00
Epoch 2/10
19/19 - 2s - loss: 454.6544 - loglik: -4.5347e+02 - logprior: -1.1863e+00
Epoch 3/10
19/19 - 2s - loss: 453.3893 - loglik: -4.5248e+02 - logprior: -9.1347e-01
Epoch 4/10
19/19 - 2s - loss: 452.9070 - loglik: -4.5202e+02 - logprior: -8.8108e-01
Epoch 5/10
19/19 - 2s - loss: 452.0042 - loglik: -4.5114e+02 - logprior: -8.6108e-01
Epoch 6/10
19/19 - 2s - loss: 450.7714 - loglik: -4.4992e+02 - logprior: -8.4425e-01
Epoch 7/10
19/19 - 2s - loss: 449.2392 - loglik: -4.4840e+02 - logprior: -8.3449e-01
Epoch 8/10
19/19 - 2s - loss: 448.8646 - loglik: -4.4802e+02 - logprior: -8.3016e-01
Epoch 9/10
19/19 - 2s - loss: 447.8409 - loglik: -4.4701e+02 - logprior: -8.1656e-01
Epoch 10/10
19/19 - 2s - loss: 446.9638 - loglik: -4.4613e+02 - logprior: -8.1824e-01
Fitted a model with MAP estimate = -446.1766
Time for alignment: 71.3251
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 509.1936 - loglik: -5.0612e+02 - logprior: -3.0697e+00
Epoch 2/10
19/19 - 2s - loss: 476.5245 - loglik: -4.7525e+02 - logprior: -1.2742e+00
Epoch 3/10
19/19 - 2s - loss: 464.2454 - loglik: -4.6278e+02 - logprior: -1.4670e+00
Epoch 4/10
19/19 - 2s - loss: 461.0968 - loglik: -4.5970e+02 - logprior: -1.3968e+00
Epoch 5/10
19/19 - 2s - loss: 460.1592 - loglik: -4.5874e+02 - logprior: -1.4185e+00
Epoch 6/10
19/19 - 2s - loss: 458.8755 - loglik: -4.5749e+02 - logprior: -1.3809e+00
Epoch 7/10
19/19 - 2s - loss: 457.5158 - loglik: -4.5613e+02 - logprior: -1.3734e+00
Epoch 8/10
19/19 - 2s - loss: 457.2606 - loglik: -4.5589e+02 - logprior: -1.3625e+00
Epoch 9/10
19/19 - 2s - loss: 456.7715 - loglik: -4.5540e+02 - logprior: -1.3552e+00
Epoch 10/10
19/19 - 2s - loss: 456.0642 - loglik: -4.5470e+02 - logprior: -1.3476e+00
Fitted a model with MAP estimate = -455.5596
expansions: [(8, 1), (9, 2), (10, 2), (12, 1), (14, 1), (19, 1), (20, 1), (40, 2), (41, 3), (43, 2), (48, 1), (49, 2), (50, 2), (60, 1), (62, 1), (64, 2), (65, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 468.5488 - loglik: -4.6467e+02 - logprior: -3.8786e+00
Epoch 2/2
19/19 - 2s - loss: 457.8945 - loglik: -4.5587e+02 - logprior: -2.0285e+00
Fitted a model with MAP estimate = -456.1296
expansions: [(0, 2)]
discards: [ 0 12 49 52 66 68 87]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 457.7571 - loglik: -4.5490e+02 - logprior: -2.8619e+00
Epoch 2/2
19/19 - 2s - loss: 454.4138 - loglik: -4.5333e+02 - logprior: -1.0856e+00
Fitted a model with MAP estimate = -453.6075
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 458.8461 - loglik: -4.5514e+02 - logprior: -3.7081e+00
Epoch 2/10
19/19 - 2s - loss: 454.3715 - loglik: -4.5320e+02 - logprior: -1.1760e+00
Epoch 3/10
19/19 - 2s - loss: 453.8477 - loglik: -4.5292e+02 - logprior: -9.2618e-01
Epoch 4/10
19/19 - 2s - loss: 452.7239 - loglik: -4.5184e+02 - logprior: -8.8497e-01
Epoch 5/10
19/19 - 2s - loss: 452.2302 - loglik: -4.5137e+02 - logprior: -8.6044e-01
Epoch 6/10
19/19 - 2s - loss: 450.3446 - loglik: -4.4949e+02 - logprior: -8.4964e-01
Epoch 7/10
19/19 - 2s - loss: 449.8410 - loglik: -4.4899e+02 - logprior: -8.4315e-01
Epoch 8/10
19/19 - 2s - loss: 448.6367 - loglik: -4.4779e+02 - logprior: -8.3217e-01
Epoch 9/10
19/19 - 2s - loss: 447.7491 - loglik: -4.4691e+02 - logprior: -8.2772e-01
Epoch 10/10
19/19 - 2s - loss: 446.9334 - loglik: -4.4610e+02 - logprior: -8.1372e-01
Fitted a model with MAP estimate = -446.1736
Time for alignment: 70.1996
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 509.2590 - loglik: -5.0619e+02 - logprior: -3.0649e+00
Epoch 2/10
19/19 - 2s - loss: 476.7668 - loglik: -4.7554e+02 - logprior: -1.2236e+00
Epoch 3/10
19/19 - 2s - loss: 465.2888 - loglik: -4.6392e+02 - logprior: -1.3655e+00
Epoch 4/10
19/19 - 2s - loss: 462.1492 - loglik: -4.6085e+02 - logprior: -1.3011e+00
Epoch 5/10
19/19 - 2s - loss: 460.8244 - loglik: -4.5952e+02 - logprior: -1.3051e+00
Epoch 6/10
19/19 - 2s - loss: 459.4262 - loglik: -4.5815e+02 - logprior: -1.2675e+00
Epoch 7/10
19/19 - 2s - loss: 458.3568 - loglik: -4.5709e+02 - logprior: -1.2592e+00
Epoch 8/10
19/19 - 2s - loss: 457.9352 - loglik: -4.5667e+02 - logprior: -1.2506e+00
Epoch 9/10
19/19 - 2s - loss: 456.7916 - loglik: -4.5553e+02 - logprior: -1.2454e+00
Epoch 10/10
19/19 - 2s - loss: 456.7942 - loglik: -4.5554e+02 - logprior: -1.2409e+00
Fitted a model with MAP estimate = -455.7964
expansions: [(8, 4), (9, 3), (23, 1), (24, 1), (37, 2), (39, 2), (41, 2), (43, 2), (48, 1), (49, 2), (60, 1), (62, 1), (64, 2), (65, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 469.0121 - loglik: -4.6514e+02 - logprior: -3.8747e+00
Epoch 2/2
19/19 - 2s - loss: 457.9391 - loglik: -4.5593e+02 - logprior: -2.0134e+00
Fitted a model with MAP estimate = -456.0312
expansions: [(0, 2)]
discards: [ 0 13 46 50 53 67 86]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 457.9573 - loglik: -4.5509e+02 - logprior: -2.8658e+00
Epoch 2/2
19/19 - 2s - loss: 454.5614 - loglik: -4.5347e+02 - logprior: -1.0877e+00
Fitted a model with MAP estimate = -453.8480
expansions: [(54, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 458.7870 - loglik: -4.5507e+02 - logprior: -3.7134e+00
Epoch 2/10
19/19 - 2s - loss: 454.9456 - loglik: -4.5377e+02 - logprior: -1.1787e+00
Epoch 3/10
19/19 - 2s - loss: 453.4875 - loglik: -4.5257e+02 - logprior: -9.2054e-01
Epoch 4/10
19/19 - 2s - loss: 452.7891 - loglik: -4.5191e+02 - logprior: -8.7514e-01
Epoch 5/10
19/19 - 2s - loss: 452.0849 - loglik: -4.5122e+02 - logprior: -8.5763e-01
Epoch 6/10
19/19 - 2s - loss: 450.6943 - loglik: -4.4984e+02 - logprior: -8.4752e-01
Epoch 7/10
19/19 - 2s - loss: 449.6913 - loglik: -4.4885e+02 - logprior: -8.3143e-01
Epoch 8/10
19/19 - 2s - loss: 448.5490 - loglik: -4.4771e+02 - logprior: -8.2812e-01
Epoch 9/10
19/19 - 2s - loss: 447.9849 - loglik: -4.4715e+02 - logprior: -8.1665e-01
Epoch 10/10
19/19 - 2s - loss: 446.5304 - loglik: -4.4570e+02 - logprior: -8.1147e-01
Fitted a model with MAP estimate = -446.2108
Time for alignment: 68.8109
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 509.0814 - loglik: -5.0601e+02 - logprior: -3.0669e+00
Epoch 2/10
19/19 - 2s - loss: 476.1290 - loglik: -4.7487e+02 - logprior: -1.2559e+00
Epoch 3/10
19/19 - 2s - loss: 463.6155 - loglik: -4.6218e+02 - logprior: -1.4312e+00
Epoch 4/10
19/19 - 2s - loss: 461.3305 - loglik: -4.5999e+02 - logprior: -1.3414e+00
Epoch 5/10
19/19 - 2s - loss: 459.8968 - loglik: -4.5854e+02 - logprior: -1.3533e+00
Epoch 6/10
19/19 - 2s - loss: 459.2997 - loglik: -4.5798e+02 - logprior: -1.3145e+00
Epoch 7/10
19/19 - 2s - loss: 457.7000 - loglik: -4.5639e+02 - logprior: -1.3049e+00
Epoch 8/10
19/19 - 2s - loss: 457.2361 - loglik: -4.5593e+02 - logprior: -1.2936e+00
Epoch 9/10
19/19 - 2s - loss: 456.6931 - loglik: -4.5539e+02 - logprior: -1.2871e+00
Epoch 10/10
19/19 - 2s - loss: 456.1267 - loglik: -4.5483e+02 - logprior: -1.2785e+00
Fitted a model with MAP estimate = -455.5400
expansions: [(8, 1), (9, 2), (10, 3), (12, 1), (17, 1), (18, 1), (34, 1), (39, 1), (40, 2), (43, 2), (48, 1), (49, 2), (60, 1), (62, 1), (64, 2), (65, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 467.9671 - loglik: -4.6412e+02 - logprior: -3.8512e+00
Epoch 2/2
19/19 - 2s - loss: 457.9998 - loglik: -4.5601e+02 - logprior: -1.9927e+00
Fitted a model with MAP estimate = -456.0620
expansions: [(0, 2)]
discards: [ 0 12 50 65 84]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 457.8383 - loglik: -4.5497e+02 - logprior: -2.8685e+00
Epoch 2/2
19/19 - 2s - loss: 454.4369 - loglik: -4.5336e+02 - logprior: -1.0808e+00
Fitted a model with MAP estimate = -453.8487
expansions: [(54, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 458.8957 - loglik: -4.5518e+02 - logprior: -3.7159e+00
Epoch 2/10
19/19 - 2s - loss: 454.5875 - loglik: -4.5339e+02 - logprior: -1.1996e+00
Epoch 3/10
19/19 - 2s - loss: 453.8190 - loglik: -4.5290e+02 - logprior: -9.1775e-01
Epoch 4/10
19/19 - 2s - loss: 452.9069 - loglik: -4.5203e+02 - logprior: -8.7603e-01
Epoch 5/10
19/19 - 2s - loss: 451.7856 - loglik: -4.5092e+02 - logprior: -8.6078e-01
Epoch 6/10
19/19 - 2s - loss: 450.9166 - loglik: -4.5006e+02 - logprior: -8.4640e-01
Epoch 7/10
19/19 - 2s - loss: 449.4836 - loglik: -4.4864e+02 - logprior: -8.3247e-01
Epoch 8/10
19/19 - 2s - loss: 448.5561 - loglik: -4.4772e+02 - logprior: -8.2045e-01
Epoch 9/10
19/19 - 2s - loss: 447.9454 - loglik: -4.4711e+02 - logprior: -8.2114e-01
Epoch 10/10
19/19 - 2s - loss: 446.9485 - loglik: -4.4612e+02 - logprior: -8.1345e-01
Fitted a model with MAP estimate = -446.1951
Time for alignment: 68.8462
Computed alignments with likelihoods: ['-446.1763', '-446.1766', '-446.1736', '-446.2108', '-446.1951']
Best model has likelihood: -446.1736  (prior= -0.8186 )
time for generating output: 0.1162
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07679.projection.fasta
SP score = 0.7727021040974529
Training of 5 independent models on file PF07686.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff718defb80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff66ca5e760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff690b5a940>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 606.1361 - loglik: -6.0311e+02 - logprior: -3.0260e+00
Epoch 2/10
19/19 - 2s - loss: 574.3284 - loglik: -5.7313e+02 - logprior: -1.2031e+00
Epoch 3/10
19/19 - 2s - loss: 562.6757 - loglik: -5.6158e+02 - logprior: -1.0937e+00
Epoch 4/10
19/19 - 2s - loss: 560.9864 - loglik: -5.5997e+02 - logprior: -1.0145e+00
Epoch 5/10
19/19 - 2s - loss: 558.3101 - loglik: -5.5734e+02 - logprior: -9.6184e-01
Epoch 6/10
19/19 - 2s - loss: 556.7091 - loglik: -5.5575e+02 - logprior: -9.5737e-01
Epoch 7/10
19/19 - 2s - loss: 556.2507 - loglik: -5.5529e+02 - logprior: -9.5008e-01
Epoch 8/10
19/19 - 2s - loss: 554.2385 - loglik: -5.5330e+02 - logprior: -9.2703e-01
Epoch 9/10
19/19 - 2s - loss: 548.5077 - loglik: -5.4756e+02 - logprior: -9.3102e-01
Epoch 10/10
19/19 - 2s - loss: 534.4739 - loglik: -5.3353e+02 - logprior: -9.1901e-01
Fitted a model with MAP estimate = -523.1933
expansions: [(0, 2), (7, 1), (19, 1), (20, 2), (21, 1), (22, 2), (32, 1), (36, 2), (38, 1), (41, 2), (42, 1), (52, 1), (53, 2), (74, 1), (75, 1), (76, 2), (77, 2), (78, 1), (79, 2), (80, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 113 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 590.4857 - loglik: -5.8622e+02 - logprior: -4.2640e+00
Epoch 2/2
19/19 - 3s - loss: 564.1013 - loglik: -5.6273e+02 - logprior: -1.3681e+00
Fitted a model with MAP estimate = -558.7107
expansions: [(25, 1), (47, 5), (55, 3), (71, 1), (101, 2)]
discards: [  0  28  51  56  72  96 102 104]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 562.8314 - loglik: -5.5877e+02 - logprior: -4.0565e+00
Epoch 2/2
19/19 - 3s - loss: 556.4233 - loglik: -5.5501e+02 - logprior: -1.4102e+00
Fitted a model with MAP estimate = -554.2017
expansions: [(0, 1), (54, 1)]
discards: [ 0 21 35 36 37 38 39 40 41 42 43 44]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 560.1411 - loglik: -5.5727e+02 - logprior: -2.8735e+00
Epoch 2/10
19/19 - 2s - loss: 556.4814 - loglik: -5.5548e+02 - logprior: -1.0010e+00
Epoch 3/10
19/19 - 2s - loss: 555.4261 - loglik: -5.5458e+02 - logprior: -8.4483e-01
Epoch 4/10
19/19 - 2s - loss: 554.4600 - loglik: -5.5368e+02 - logprior: -7.7740e-01
Epoch 5/10
19/19 - 2s - loss: 552.7622 - loglik: -5.5203e+02 - logprior: -7.3216e-01
Epoch 6/10
19/19 - 2s - loss: 551.3814 - loglik: -5.5067e+02 - logprior: -7.0257e-01
Epoch 7/10
19/19 - 2s - loss: 550.2169 - loglik: -5.4952e+02 - logprior: -6.8401e-01
Epoch 8/10
19/19 - 2s - loss: 548.0193 - loglik: -5.4733e+02 - logprior: -6.7624e-01
Epoch 9/10
19/19 - 3s - loss: 543.2921 - loglik: -5.4260e+02 - logprior: -6.7714e-01
Epoch 10/10
19/19 - 2s - loss: 524.9221 - loglik: -5.2417e+02 - logprior: -7.2984e-01
Fitted a model with MAP estimate = -509.5560
Time for alignment: 87.0110
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 606.6074 - loglik: -6.0359e+02 - logprior: -3.0196e+00
Epoch 2/10
19/19 - 2s - loss: 578.1011 - loglik: -5.7693e+02 - logprior: -1.1724e+00
Epoch 3/10
19/19 - 2s - loss: 563.8973 - loglik: -5.6280e+02 - logprior: -1.0978e+00
Epoch 4/10
19/19 - 2s - loss: 560.4328 - loglik: -5.5946e+02 - logprior: -9.6726e-01
Epoch 5/10
19/19 - 2s - loss: 559.2122 - loglik: -5.5827e+02 - logprior: -9.3572e-01
Epoch 6/10
19/19 - 2s - loss: 557.2314 - loglik: -5.5629e+02 - logprior: -9.3162e-01
Epoch 7/10
19/19 - 2s - loss: 556.1008 - loglik: -5.5518e+02 - logprior: -9.1159e-01
Epoch 8/10
19/19 - 2s - loss: 554.1287 - loglik: -5.5320e+02 - logprior: -9.1737e-01
Epoch 9/10
19/19 - 2s - loss: 549.0936 - loglik: -5.4817e+02 - logprior: -9.0909e-01
Epoch 10/10
19/19 - 2s - loss: 533.4484 - loglik: -5.3253e+02 - logprior: -8.9145e-01
Fitted a model with MAP estimate = -522.1635
expansions: [(0, 2), (20, 1), (21, 2), (22, 1), (23, 2), (24, 1), (37, 3), (39, 1), (42, 2), (44, 1), (51, 2), (52, 2), (75, 1), (76, 3), (77, 2), (78, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 593.9500 - loglik: -5.8969e+02 - logprior: -4.2647e+00
Epoch 2/2
19/19 - 3s - loss: 565.1012 - loglik: -5.6377e+02 - logprior: -1.3295e+00
Fitted a model with MAP estimate = -559.7820
expansions: [(25, 1), (56, 3), (71, 2), (102, 1)]
discards: [  0  23  28  57  67  69  72  96 100 103]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 563.5086 - loglik: -5.5946e+02 - logprior: -4.0450e+00
Epoch 2/2
19/19 - 3s - loss: 557.3726 - loglik: -5.5602e+02 - logprior: -1.3558e+00
Fitted a model with MAP estimate = -555.0383
expansions: [(0, 1), (45, 2), (46, 3), (94, 1), (97, 1)]
discards: [ 0 34 35 36 37 38 39 40 41 42 43]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 561.1998 - loglik: -5.5834e+02 - logprior: -2.8568e+00
Epoch 2/10
19/19 - 2s - loss: 557.3754 - loglik: -5.5635e+02 - logprior: -1.0210e+00
Epoch 3/10
19/19 - 2s - loss: 556.1004 - loglik: -5.5522e+02 - logprior: -8.7756e-01
Epoch 4/10
19/19 - 2s - loss: 554.9796 - loglik: -5.5415e+02 - logprior: -8.3133e-01
Epoch 5/10
19/19 - 2s - loss: 554.0109 - loglik: -5.5322e+02 - logprior: -7.8250e-01
Epoch 6/10
19/19 - 2s - loss: 552.2772 - loglik: -5.5152e+02 - logprior: -7.4927e-01
Epoch 7/10
19/19 - 2s - loss: 550.5359 - loglik: -5.4979e+02 - logprior: -7.3827e-01
Epoch 8/10
19/19 - 2s - loss: 549.1780 - loglik: -5.4845e+02 - logprior: -7.1393e-01
Epoch 9/10
19/19 - 3s - loss: 543.6768 - loglik: -5.4294e+02 - logprior: -7.2428e-01
Epoch 10/10
19/19 - 3s - loss: 526.5286 - loglik: -5.2574e+02 - logprior: -7.6988e-01
Fitted a model with MAP estimate = -510.8858
Time for alignment: 85.0812
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 606.0221 - loglik: -6.0298e+02 - logprior: -3.0372e+00
Epoch 2/10
19/19 - 2s - loss: 577.5355 - loglik: -5.7634e+02 - logprior: -1.1963e+00
Epoch 3/10
19/19 - 2s - loss: 564.2119 - loglik: -5.6312e+02 - logprior: -1.0945e+00
Epoch 4/10
19/19 - 2s - loss: 561.9363 - loglik: -5.6098e+02 - logprior: -9.5468e-01
Epoch 5/10
19/19 - 2s - loss: 559.6082 - loglik: -5.5869e+02 - logprior: -9.1886e-01
Epoch 6/10
19/19 - 2s - loss: 558.7083 - loglik: -5.5781e+02 - logprior: -8.9357e-01
Epoch 7/10
19/19 - 2s - loss: 557.2424 - loglik: -5.5635e+02 - logprior: -8.8772e-01
Epoch 8/10
19/19 - 2s - loss: 555.6755 - loglik: -5.5479e+02 - logprior: -8.7610e-01
Epoch 9/10
19/19 - 2s - loss: 549.5536 - loglik: -5.4866e+02 - logprior: -8.7427e-01
Epoch 10/10
19/19 - 2s - loss: 535.0760 - loglik: -5.3419e+02 - logprior: -8.6396e-01
Fitted a model with MAP estimate = -521.6558
expansions: [(0, 2), (17, 1), (19, 1), (20, 2), (21, 1), (22, 2), (39, 2), (42, 1), (44, 1), (52, 4), (75, 1), (76, 3), (77, 2), (78, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 596.1052 - loglik: -5.9181e+02 - logprior: -4.2946e+00
Epoch 2/2
19/19 - 3s - loss: 566.8954 - loglik: -5.6553e+02 - logprior: -1.3643e+00
Fitted a model with MAP estimate = -561.6095
expansions: [(25, 1), (67, 2), (92, 1), (96, 2), (99, 1)]
discards: [  0  23  28  36  68  93  97 100 103 104]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 566.1579 - loglik: -5.6214e+02 - logprior: -4.0152e+00
Epoch 2/2
19/19 - 3s - loss: 559.8989 - loglik: -5.5866e+02 - logprior: -1.2436e+00
Fitted a model with MAP estimate = -556.9915
expansions: [(0, 1), (34, 1), (67, 1), (68, 1), (95, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 559.5865 - loglik: -5.5673e+02 - logprior: -2.8588e+00
Epoch 2/10
19/19 - 3s - loss: 555.7272 - loglik: -5.5471e+02 - logprior: -1.0121e+00
Epoch 3/10
19/19 - 3s - loss: 555.0339 - loglik: -5.5421e+02 - logprior: -8.2581e-01
Epoch 4/10
19/19 - 3s - loss: 554.5347 - loglik: -5.5376e+02 - logprior: -7.6814e-01
Epoch 5/10
19/19 - 3s - loss: 552.4965 - loglik: -5.5177e+02 - logprior: -7.2003e-01
Epoch 6/10
19/19 - 3s - loss: 550.9519 - loglik: -5.5027e+02 - logprior: -6.7827e-01
Epoch 7/10
19/19 - 3s - loss: 549.6826 - loglik: -5.4900e+02 - logprior: -6.6882e-01
Epoch 8/10
19/19 - 3s - loss: 548.2486 - loglik: -5.4759e+02 - logprior: -6.4989e-01
Epoch 9/10
19/19 - 3s - loss: 542.2385 - loglik: -5.4157e+02 - logprior: -6.5454e-01
Epoch 10/10
19/19 - 3s - loss: 525.1339 - loglik: -5.2443e+02 - logprior: -6.8515e-01
Fitted a model with MAP estimate = -508.5877
Time for alignment: 87.8323
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 606.6080 - loglik: -6.0358e+02 - logprior: -3.0272e+00
Epoch 2/10
19/19 - 2s - loss: 577.7287 - loglik: -5.7654e+02 - logprior: -1.1899e+00
Epoch 3/10
19/19 - 2s - loss: 564.9160 - loglik: -5.6381e+02 - logprior: -1.1012e+00
Epoch 4/10
19/19 - 2s - loss: 560.8430 - loglik: -5.5986e+02 - logprior: -9.8476e-01
Epoch 5/10
19/19 - 2s - loss: 559.8695 - loglik: -5.5892e+02 - logprior: -9.4675e-01
Epoch 6/10
19/19 - 2s - loss: 557.3152 - loglik: -5.5637e+02 - logprior: -9.4111e-01
Epoch 7/10
19/19 - 2s - loss: 556.4609 - loglik: -5.5552e+02 - logprior: -9.3356e-01
Epoch 8/10
19/19 - 2s - loss: 554.5023 - loglik: -5.5356e+02 - logprior: -9.2897e-01
Epoch 9/10
19/19 - 2s - loss: 549.8390 - loglik: -5.4889e+02 - logprior: -9.3316e-01
Epoch 10/10
19/19 - 2s - loss: 535.5808 - loglik: -5.3463e+02 - logprior: -9.2232e-01
Fitted a model with MAP estimate = -524.0034
expansions: [(0, 2), (17, 1), (19, 1), (20, 2), (21, 1), (22, 2), (23, 1), (36, 2), (38, 1), (42, 1), (51, 1), (52, 2), (53, 1), (75, 1), (76, 3), (77, 2), (78, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 591.2032 - loglik: -5.8694e+02 - logprior: -4.2635e+00
Epoch 2/2
19/19 - 3s - loss: 565.3412 - loglik: -5.6401e+02 - logprior: -1.3294e+00
Fitted a model with MAP estimate = -559.3869
expansions: [(25, 1), (50, 3), (65, 1), (68, 1), (93, 1), (100, 1)]
discards: [  0  23  28  51  66  94  98 101]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 563.3350 - loglik: -5.5926e+02 - logprior: -4.0738e+00
Epoch 2/2
19/19 - 3s - loss: 556.7133 - loglik: -5.5534e+02 - logprior: -1.3689e+00
Fitted a model with MAP estimate = -554.7915
expansions: [(0, 1), (46, 3), (53, 1), (54, 2)]
discards: [ 0 34 35 36 37 38 39 40 41 42 43 69]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 561.4454 - loglik: -5.5858e+02 - logprior: -2.8614e+00
Epoch 2/10
19/19 - 3s - loss: 557.4764 - loglik: -5.5647e+02 - logprior: -1.0041e+00
Epoch 3/10
19/19 - 3s - loss: 556.1107 - loglik: -5.5525e+02 - logprior: -8.5729e-01
Epoch 4/10
19/19 - 3s - loss: 555.5654 - loglik: -5.5479e+02 - logprior: -7.7328e-01
Epoch 5/10
19/19 - 3s - loss: 553.9996 - loglik: -5.5326e+02 - logprior: -7.3131e-01
Epoch 6/10
19/19 - 3s - loss: 552.4144 - loglik: -5.5169e+02 - logprior: -7.1886e-01
Epoch 7/10
19/19 - 3s - loss: 551.2513 - loglik: -5.5054e+02 - logprior: -7.0030e-01
Epoch 8/10
19/19 - 3s - loss: 549.2043 - loglik: -5.4849e+02 - logprior: -6.9849e-01
Epoch 9/10
19/19 - 3s - loss: 543.3326 - loglik: -5.4261e+02 - logprior: -7.0655e-01
Epoch 10/10
19/19 - 3s - loss: 525.1407 - loglik: -5.2436e+02 - logprior: -7.5984e-01
Fitted a model with MAP estimate = -509.7700
Time for alignment: 92.1173
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 606.2236 - loglik: -6.0320e+02 - logprior: -3.0264e+00
Epoch 2/10
19/19 - 2s - loss: 576.1535 - loglik: -5.7496e+02 - logprior: -1.1910e+00
Epoch 3/10
19/19 - 2s - loss: 564.3209 - loglik: -5.6324e+02 - logprior: -1.0773e+00
Epoch 4/10
19/19 - 2s - loss: 561.7865 - loglik: -5.6082e+02 - logprior: -9.6254e-01
Epoch 5/10
19/19 - 2s - loss: 559.9178 - loglik: -5.5900e+02 - logprior: -9.0975e-01
Epoch 6/10
19/19 - 2s - loss: 558.9681 - loglik: -5.5805e+02 - logprior: -9.1002e-01
Epoch 7/10
19/19 - 2s - loss: 556.8458 - loglik: -5.5593e+02 - logprior: -9.0973e-01
Epoch 8/10
19/19 - 2s - loss: 555.0457 - loglik: -5.5413e+02 - logprior: -8.9841e-01
Epoch 9/10
19/19 - 2s - loss: 550.3998 - loglik: -5.4947e+02 - logprior: -9.1620e-01
Epoch 10/10
19/19 - 2s - loss: 534.3466 - loglik: -5.3339e+02 - logprior: -9.2934e-01
Fitted a model with MAP estimate = -522.3777
expansions: [(0, 2), (20, 1), (21, 2), (22, 2), (23, 2), (24, 1), (39, 2), (42, 2), (44, 1), (74, 1), (75, 1), (76, 2), (77, 3), (78, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 594.6985 - loglik: -5.9044e+02 - logprior: -4.2584e+00
Epoch 2/2
19/19 - 3s - loss: 566.4525 - loglik: -5.6517e+02 - logprior: -1.2835e+00
Fitted a model with MAP estimate = -561.2081
expansions: [(25, 1), (55, 5), (68, 3), (69, 2), (97, 1), (98, 1)]
discards: [ 1 23 29 56 57 58 59 60 61 92 95 99]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 563.0184 - loglik: -5.6007e+02 - logprior: -2.9494e+00
Epoch 2/2
19/19 - 3s - loss: 557.4642 - loglik: -5.5636e+02 - logprior: -1.1053e+00
Fitted a model with MAP estimate = -555.5176
expansions: [(70, 1), (102, 1)]
discards: [26]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 558.6591 - loglik: -5.5585e+02 - logprior: -2.8121e+00
Epoch 2/10
19/19 - 3s - loss: 555.4584 - loglik: -5.5443e+02 - logprior: -1.0258e+00
Epoch 3/10
19/19 - 3s - loss: 555.2677 - loglik: -5.5441e+02 - logprior: -8.5358e-01
Epoch 4/10
19/19 - 3s - loss: 553.7588 - loglik: -5.5296e+02 - logprior: -7.9531e-01
Epoch 5/10
19/19 - 3s - loss: 552.4982 - loglik: -5.5174e+02 - logprior: -7.5838e-01
Epoch 6/10
19/19 - 3s - loss: 551.0118 - loglik: -5.5030e+02 - logprior: -7.0585e-01
Epoch 7/10
19/19 - 3s - loss: 549.5748 - loglik: -5.4887e+02 - logprior: -6.9363e-01
Epoch 8/10
19/19 - 3s - loss: 547.8253 - loglik: -5.4718e+02 - logprior: -6.3464e-01
Epoch 9/10
19/19 - 3s - loss: 542.5539 - loglik: -5.4190e+02 - logprior: -6.4253e-01
Epoch 10/10
19/19 - 3s - loss: 523.7141 - loglik: -5.2300e+02 - logprior: -6.9052e-01
Fitted a model with MAP estimate = -507.3211
Time for alignment: 92.0555
Computed alignments with likelihoods: ['-509.5560', '-510.8858', '-508.5877', '-509.7700', '-507.3211']
Best model has likelihood: -507.3211  (prior= -0.7094 )
time for generating output: 0.1457
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07686.projection.fasta
SP score = 0.866063212237315
Training of 5 independent models on file PF00505.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6b32d6d30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff66ce8cfd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6f7414ca0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 378.7148 - loglik: -3.7551e+02 - logprior: -3.2049e+00
Epoch 2/10
19/19 - 1s - loss: 344.6523 - loglik: -3.4307e+02 - logprior: -1.5861e+00
Epoch 3/10
19/19 - 1s - loss: 332.2854 - loglik: -3.3077e+02 - logprior: -1.5130e+00
Epoch 4/10
19/19 - 1s - loss: 330.0522 - loglik: -3.2854e+02 - logprior: -1.5125e+00
Epoch 5/10
19/19 - 1s - loss: 328.4644 - loglik: -3.2697e+02 - logprior: -1.4926e+00
Epoch 6/10
19/19 - 1s - loss: 328.0512 - loglik: -3.2654e+02 - logprior: -1.5058e+00
Epoch 7/10
19/19 - 1s - loss: 327.2523 - loglik: -3.2574e+02 - logprior: -1.5013e+00
Epoch 8/10
19/19 - 1s - loss: 326.8452 - loglik: -3.2532e+02 - logprior: -1.5139e+00
Epoch 9/10
19/19 - 1s - loss: 325.5696 - loglik: -3.2403e+02 - logprior: -1.5234e+00
Epoch 10/10
19/19 - 1s - loss: 324.6578 - loglik: -3.2310e+02 - logprior: -1.5407e+00
Fitted a model with MAP estimate = -323.9668
expansions: [(12, 1), (16, 1), (17, 1), (18, 1), (21, 1), (22, 2), (23, 1), (33, 1), (35, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 329.8725 - loglik: -3.2659e+02 - logprior: -3.2835e+00
Epoch 2/2
19/19 - 1s - loss: 320.4545 - loglik: -3.1916e+02 - logprior: -1.2915e+00
Fitted a model with MAP estimate = -319.5388
expansions: []
discards: [27]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 323.3216 - loglik: -3.2016e+02 - logprior: -3.1612e+00
Epoch 2/2
19/19 - 1s - loss: 320.2755 - loglik: -3.1905e+02 - logprior: -1.2298e+00
Fitted a model with MAP estimate = -319.2989
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 323.0393 - loglik: -3.1990e+02 - logprior: -3.1433e+00
Epoch 2/10
19/19 - 1s - loss: 319.6358 - loglik: -3.1842e+02 - logprior: -1.2203e+00
Epoch 3/10
19/19 - 1s - loss: 319.2078 - loglik: -3.1813e+02 - logprior: -1.0723e+00
Epoch 4/10
19/19 - 1s - loss: 318.7455 - loglik: -3.1771e+02 - logprior: -1.0295e+00
Epoch 5/10
19/19 - 1s - loss: 318.1798 - loglik: -3.1717e+02 - logprior: -1.0064e+00
Epoch 6/10
19/19 - 1s - loss: 317.4944 - loglik: -3.1649e+02 - logprior: -1.0002e+00
Epoch 7/10
19/19 - 1s - loss: 317.0432 - loglik: -3.1603e+02 - logprior: -1.0047e+00
Epoch 8/10
19/19 - 1s - loss: 315.5573 - loglik: -3.1454e+02 - logprior: -1.0115e+00
Epoch 9/10
19/19 - 1s - loss: 314.6849 - loglik: -3.1365e+02 - logprior: -1.0242e+00
Epoch 10/10
19/19 - 1s - loss: 312.8053 - loglik: -3.1175e+02 - logprior: -1.0433e+00
Fitted a model with MAP estimate = -311.4728
Time for alignment: 48.8064
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 378.6776 - loglik: -3.7548e+02 - logprior: -3.2018e+00
Epoch 2/10
19/19 - 1s - loss: 345.8490 - loglik: -3.4441e+02 - logprior: -1.4360e+00
Epoch 3/10
19/19 - 1s - loss: 334.7856 - loglik: -3.3317e+02 - logprior: -1.6110e+00
Epoch 4/10
19/19 - 1s - loss: 331.3705 - loglik: -3.2984e+02 - logprior: -1.5263e+00
Epoch 5/10
19/19 - 1s - loss: 329.7730 - loglik: -3.2819e+02 - logprior: -1.5764e+00
Epoch 6/10
19/19 - 1s - loss: 328.8618 - loglik: -3.2731e+02 - logprior: -1.5481e+00
Epoch 7/10
19/19 - 1s - loss: 328.4566 - loglik: -3.2691e+02 - logprior: -1.5403e+00
Epoch 8/10
19/19 - 1s - loss: 327.1997 - loglik: -3.2565e+02 - logprior: -1.5378e+00
Epoch 9/10
19/19 - 1s - loss: 326.1457 - loglik: -3.2460e+02 - logprior: -1.5373e+00
Epoch 10/10
19/19 - 1s - loss: 324.8354 - loglik: -3.2328e+02 - logprior: -1.5417e+00
Fitted a model with MAP estimate = -323.5969
expansions: [(12, 1), (16, 1), (17, 1), (18, 1), (21, 2), (22, 1), (23, 1), (24, 1), (26, 1), (35, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 335.4324 - loglik: -3.3139e+02 - logprior: -4.0473e+00
Epoch 2/2
19/19 - 1s - loss: 323.3604 - loglik: -3.2141e+02 - logprior: -1.9506e+00
Fitted a model with MAP estimate = -321.5389
expansions: [(0, 2)]
discards: [ 0 25]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 323.5845 - loglik: -3.2060e+02 - logprior: -2.9821e+00
Epoch 2/2
19/19 - 1s - loss: 319.6819 - loglik: -3.1851e+02 - logprior: -1.1666e+00
Fitted a model with MAP estimate = -318.9864
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 325.4784 - loglik: -3.2168e+02 - logprior: -3.8031e+00
Epoch 2/10
19/19 - 1s - loss: 319.9066 - loglik: -3.1863e+02 - logprior: -1.2810e+00
Epoch 3/10
19/19 - 1s - loss: 319.4313 - loglik: -3.1838e+02 - logprior: -1.0555e+00
Epoch 4/10
19/19 - 1s - loss: 318.6891 - loglik: -3.1767e+02 - logprior: -1.0155e+00
Epoch 5/10
19/19 - 1s - loss: 318.2710 - loglik: -3.1726e+02 - logprior: -1.0089e+00
Epoch 6/10
19/19 - 1s - loss: 317.4728 - loglik: -3.1647e+02 - logprior: -9.9486e-01
Epoch 7/10
19/19 - 1s - loss: 316.5021 - loglik: -3.1548e+02 - logprior: -1.0140e+00
Epoch 8/10
19/19 - 1s - loss: 315.8930 - loglik: -3.1487e+02 - logprior: -1.0098e+00
Epoch 9/10
19/19 - 1s - loss: 314.7169 - loglik: -3.1368e+02 - logprior: -1.0243e+00
Epoch 10/10
19/19 - 1s - loss: 312.6906 - loglik: -3.1164e+02 - logprior: -1.0393e+00
Fitted a model with MAP estimate = -311.4375
Time for alignment: 50.4452
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 378.9139 - loglik: -3.7571e+02 - logprior: -3.2013e+00
Epoch 2/10
19/19 - 1s - loss: 346.4747 - loglik: -3.4501e+02 - logprior: -1.4644e+00
Epoch 3/10
19/19 - 1s - loss: 333.6793 - loglik: -3.3211e+02 - logprior: -1.5656e+00
Epoch 4/10
19/19 - 1s - loss: 329.3505 - loglik: -3.2783e+02 - logprior: -1.5152e+00
Epoch 5/10
19/19 - 1s - loss: 328.1716 - loglik: -3.2666e+02 - logprior: -1.5063e+00
Epoch 6/10
19/19 - 1s - loss: 327.4055 - loglik: -3.2589e+02 - logprior: -1.5082e+00
Epoch 7/10
19/19 - 1s - loss: 327.0241 - loglik: -3.2551e+02 - logprior: -1.5026e+00
Epoch 8/10
19/19 - 1s - loss: 326.0713 - loglik: -3.2455e+02 - logprior: -1.5103e+00
Epoch 9/10
19/19 - 1s - loss: 325.5892 - loglik: -3.2405e+02 - logprior: -1.5245e+00
Epoch 10/10
19/19 - 1s - loss: 324.3673 - loglik: -3.2282e+02 - logprior: -1.5329e+00
Fitted a model with MAP estimate = -323.4759
expansions: [(14, 1), (16, 1), (17, 1), (18, 1), (21, 1), (22, 2), (23, 1), (33, 1), (35, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 329.9862 - loglik: -3.2671e+02 - logprior: -3.2752e+00
Epoch 2/2
19/19 - 1s - loss: 320.7137 - loglik: -3.1944e+02 - logprior: -1.2767e+00
Fitted a model with MAP estimate = -319.5463
expansions: []
discards: [27]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 323.4220 - loglik: -3.2027e+02 - logprior: -3.1564e+00
Epoch 2/2
19/19 - 1s - loss: 320.1056 - loglik: -3.1887e+02 - logprior: -1.2307e+00
Fitted a model with MAP estimate = -319.2637
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 323.0250 - loglik: -3.1988e+02 - logprior: -3.1424e+00
Epoch 2/10
19/19 - 1s - loss: 319.7883 - loglik: -3.1857e+02 - logprior: -1.2196e+00
Epoch 3/10
19/19 - 1s - loss: 319.1464 - loglik: -3.1808e+02 - logprior: -1.0657e+00
Epoch 4/10
19/19 - 1s - loss: 318.8048 - loglik: -3.1777e+02 - logprior: -1.0294e+00
Epoch 5/10
19/19 - 1s - loss: 318.0456 - loglik: -3.1704e+02 - logprior: -1.0076e+00
Epoch 6/10
19/19 - 1s - loss: 317.4682 - loglik: -3.1647e+02 - logprior: -9.9766e-01
Epoch 7/10
19/19 - 1s - loss: 317.1287 - loglik: -3.1612e+02 - logprior: -1.0018e+00
Epoch 8/10
19/19 - 1s - loss: 315.6503 - loglik: -3.1464e+02 - logprior: -1.0006e+00
Epoch 9/10
19/19 - 1s - loss: 314.1959 - loglik: -3.1316e+02 - logprior: -1.0288e+00
Epoch 10/10
19/19 - 1s - loss: 312.9851 - loglik: -3.1194e+02 - logprior: -1.0318e+00
Fitted a model with MAP estimate = -311.4719
Time for alignment: 49.7219
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 378.7706 - loglik: -3.7557e+02 - logprior: -3.2005e+00
Epoch 2/10
19/19 - 1s - loss: 345.9280 - loglik: -3.4449e+02 - logprior: -1.4340e+00
Epoch 3/10
19/19 - 1s - loss: 335.1940 - loglik: -3.3360e+02 - logprior: -1.5952e+00
Epoch 4/10
19/19 - 1s - loss: 331.9407 - loglik: -3.3043e+02 - logprior: -1.5062e+00
Epoch 5/10
19/19 - 1s - loss: 330.6332 - loglik: -3.2908e+02 - logprior: -1.5478e+00
Epoch 6/10
19/19 - 1s - loss: 329.5907 - loglik: -3.2806e+02 - logprior: -1.5279e+00
Epoch 7/10
19/19 - 1s - loss: 328.5535 - loglik: -3.2703e+02 - logprior: -1.5170e+00
Epoch 8/10
19/19 - 1s - loss: 328.4066 - loglik: -3.2689e+02 - logprior: -1.5056e+00
Epoch 9/10
19/19 - 1s - loss: 326.2860 - loglik: -3.2476e+02 - logprior: -1.5089e+00
Epoch 10/10
19/19 - 1s - loss: 326.1399 - loglik: -3.2461e+02 - logprior: -1.5155e+00
Fitted a model with MAP estimate = -324.4789
expansions: [(14, 1), (16, 1), (17, 1), (18, 1), (22, 2), (23, 2), (24, 1), (29, 1), (35, 1), (41, 1), (42, 1), (43, 1), (44, 1), (46, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 335.3068 - loglik: -3.3126e+02 - logprior: -4.0429e+00
Epoch 2/2
19/19 - 1s - loss: 323.2429 - loglik: -3.2129e+02 - logprior: -1.9552e+00
Fitted a model with MAP estimate = -321.5535
expansions: [(0, 2)]
discards: [ 0 28]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 323.4936 - loglik: -3.2050e+02 - logprior: -2.9912e+00
Epoch 2/2
19/19 - 1s - loss: 319.5417 - loglik: -3.1837e+02 - logprior: -1.1721e+00
Fitted a model with MAP estimate = -318.9936
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 325.5251 - loglik: -3.2172e+02 - logprior: -3.8064e+00
Epoch 2/10
19/19 - 1s - loss: 320.1941 - loglik: -3.1891e+02 - logprior: -1.2829e+00
Epoch 3/10
19/19 - 1s - loss: 319.2540 - loglik: -3.1822e+02 - logprior: -1.0353e+00
Epoch 4/10
19/19 - 1s - loss: 318.7568 - loglik: -3.1774e+02 - logprior: -1.0153e+00
Epoch 5/10
19/19 - 1s - loss: 318.3062 - loglik: -3.1730e+02 - logprior: -1.0048e+00
Epoch 6/10
19/19 - 1s - loss: 317.5838 - loglik: -3.1658e+02 - logprior: -9.9486e-01
Epoch 7/10
19/19 - 1s - loss: 316.9176 - loglik: -3.1591e+02 - logprior: -1.0045e+00
Epoch 8/10
19/19 - 1s - loss: 315.3788 - loglik: -3.1436e+02 - logprior: -1.0063e+00
Epoch 9/10
19/19 - 1s - loss: 314.4788 - loglik: -3.1345e+02 - logprior: -1.0213e+00
Epoch 10/10
19/19 - 1s - loss: 312.9727 - loglik: -3.1192e+02 - logprior: -1.0367e+00
Fitted a model with MAP estimate = -311.4002
Time for alignment: 48.8078
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 378.8139 - loglik: -3.7561e+02 - logprior: -3.2006e+00
Epoch 2/10
19/19 - 1s - loss: 347.1731 - loglik: -3.4562e+02 - logprior: -1.5504e+00
Epoch 3/10
19/19 - 1s - loss: 335.9881 - loglik: -3.3457e+02 - logprior: -1.4197e+00
Epoch 4/10
19/19 - 1s - loss: 332.0950 - loglik: -3.3065e+02 - logprior: -1.4410e+00
Epoch 5/10
19/19 - 1s - loss: 330.7273 - loglik: -3.2926e+02 - logprior: -1.4615e+00
Epoch 6/10
19/19 - 1s - loss: 329.5160 - loglik: -3.2806e+02 - logprior: -1.4551e+00
Epoch 7/10
19/19 - 1s - loss: 328.7591 - loglik: -3.2730e+02 - logprior: -1.4519e+00
Epoch 8/10
19/19 - 1s - loss: 327.6515 - loglik: -3.2618e+02 - logprior: -1.4589e+00
Epoch 9/10
19/19 - 1s - loss: 326.4229 - loglik: -3.2493e+02 - logprior: -1.4779e+00
Epoch 10/10
19/19 - 1s - loss: 324.6537 - loglik: -3.2314e+02 - logprior: -1.4924e+00
Fitted a model with MAP estimate = -323.3615
expansions: [(16, 1), (17, 2), (18, 1), (22, 2), (23, 3), (24, 1), (35, 1), (38, 1), (41, 1), (43, 1), (44, 1), (46, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 332.9213 - loglik: -3.2964e+02 - logprior: -3.2824e+00
Epoch 2/2
19/19 - 1s - loss: 321.1419 - loglik: -3.1982e+02 - logprior: -1.3195e+00
Fitted a model with MAP estimate = -319.9161
expansions: []
discards: [18 27]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 323.9940 - loglik: -3.2082e+02 - logprior: -3.1696e+00
Epoch 2/2
19/19 - 1s - loss: 320.5061 - loglik: -3.1925e+02 - logprior: -1.2538e+00
Fitted a model with MAP estimate = -319.6888
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 323.5970 - loglik: -3.2044e+02 - logprior: -3.1528e+00
Epoch 2/10
19/19 - 1s - loss: 320.0251 - loglik: -3.1880e+02 - logprior: -1.2235e+00
Epoch 3/10
19/19 - 1s - loss: 319.5707 - loglik: -3.1850e+02 - logprior: -1.0689e+00
Epoch 4/10
19/19 - 1s - loss: 319.0256 - loglik: -3.1800e+02 - logprior: -1.0268e+00
Epoch 5/10
19/19 - 1s - loss: 318.4668 - loglik: -3.1745e+02 - logprior: -1.0153e+00
Epoch 6/10
19/19 - 1s - loss: 318.1545 - loglik: -3.1715e+02 - logprior: -9.9858e-01
Epoch 7/10
19/19 - 1s - loss: 316.8053 - loglik: -3.1579e+02 - logprior: -1.0099e+00
Epoch 8/10
19/19 - 1s - loss: 316.4838 - loglik: -3.1546e+02 - logprior: -1.0141e+00
Epoch 9/10
19/19 - 1s - loss: 314.7775 - loglik: -3.1373e+02 - logprior: -1.0307e+00
Epoch 10/10
19/19 - 1s - loss: 312.8105 - loglik: -3.1175e+02 - logprior: -1.0465e+00
Fitted a model with MAP estimate = -311.6961
Time for alignment: 47.8098
Computed alignments with likelihoods: ['-311.4728', '-311.4375', '-311.4719', '-311.4002', '-311.6961']
Best model has likelihood: -311.4002  (prior= -1.0674 )
time for generating output: 0.1069
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00505.projection.fasta
SP score = 0.9522158950235207
Training of 5 independent models on file PF13393.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff5d111dcd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff676f239d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff66e6c26a0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 1546.2655 - loglik: -1.5446e+03 - logprior: -1.6501e+00
Epoch 2/10
39/39 - 24s - loss: 1429.1003 - loglik: -1.4275e+03 - logprior: -1.5684e+00
Epoch 3/10
39/39 - 25s - loss: 1416.2273 - loglik: -1.4146e+03 - logprior: -1.6458e+00
Epoch 4/10
39/39 - 26s - loss: 1411.8324 - loglik: -1.4102e+03 - logprior: -1.5939e+00
Epoch 5/10
39/39 - 28s - loss: 1408.6742 - loglik: -1.4071e+03 - logprior: -1.5695e+00
Epoch 6/10
39/39 - 29s - loss: 1403.6017 - loglik: -1.4020e+03 - logprior: -1.5789e+00
Epoch 7/10
39/39 - 30s - loss: 1396.1398 - loglik: -1.3945e+03 - logprior: -1.6072e+00
Epoch 8/10
39/39 - 31s - loss: 1371.4839 - loglik: -1.3697e+03 - logprior: -1.7345e+00
Epoch 9/10
39/39 - 32s - loss: 1333.6671 - loglik: -1.3313e+03 - logprior: -2.3341e+00
Epoch 10/10
39/39 - 32s - loss: 1209.6035 - loglik: -1.2018e+03 - logprior: -7.7967e+00
Fitted a model with MAP estimate = -1160.7812
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 163 194 195 196 197 198 199 200 201
 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219
 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237
 238 239 240 241 242]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 1680.2765 - loglik: -1.6770e+03 - logprior: -3.2954e+00
Epoch 2/2
19/19 - 6s - loss: 1655.8000 - loglik: -1.6555e+03 - logprior: -2.8331e-01
Fitted a model with MAP estimate = -1650.4150
expansions: [(0, 187), (76, 94)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48
 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72
 73 74 75]
Re-initialized the encoder parameters.
Fitting a model of length 282 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 39s - loss: 1523.6195 - loglik: -1.5210e+03 - logprior: -2.6063e+00
Epoch 2/2
39/39 - 38s - loss: 1419.3705 - loglik: -1.4186e+03 - logprior: -8.0294e-01
Fitted a model with MAP estimate = -1409.6272
expansions: [(18, 1), (21, 1), (28, 1), (60, 2), (82, 1), (140, 4), (161, 1), (169, 1), (170, 1), (171, 1), (230, 1), (250, 1), (251, 2), (253, 1), (265, 1), (266, 1), (269, 1), (270, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 304 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 45s - loss: 1406.5463 - loglik: -1.4043e+03 - logprior: -2.2329e+00
Epoch 2/10
39/39 - 37s - loss: 1395.3069 - loglik: -1.3949e+03 - logprior: -4.4161e-01
Epoch 3/10
39/39 - 36s - loss: 1392.6892 - loglik: -1.3923e+03 - logprior: -3.4109e-01
Epoch 4/10
39/39 - 37s - loss: 1387.7374 - loglik: -1.3875e+03 - logprior: -2.6666e-01
Epoch 5/10
39/39 - 38s - loss: 1383.8931 - loglik: -1.3837e+03 - logprior: -2.2071e-01
Epoch 6/10
39/39 - 40s - loss: 1376.5435 - loglik: -1.3763e+03 - logprior: -1.9620e-01
Epoch 7/10
39/39 - 44s - loss: 1362.8586 - loglik: -1.3626e+03 - logprior: -2.3752e-01
Epoch 8/10
39/39 - 44s - loss: 1310.7404 - loglik: -1.3101e+03 - logprior: -5.5849e-01
Epoch 9/10
39/39 - 43s - loss: 1213.0638 - loglik: -1.2115e+03 - logprior: -1.5436e+00
Epoch 10/10
39/39 - 43s - loss: 1142.1616 - loglik: -1.1387e+03 - logprior: -3.3751e+00
Fitted a model with MAP estimate = -1132.8805
Time for alignment: 907.4142
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 33s - loss: 1545.6487 - loglik: -1.5440e+03 - logprior: -1.6547e+00
Epoch 2/10
39/39 - 30s - loss: 1431.6565 - loglik: -1.4300e+03 - logprior: -1.6070e+00
Epoch 3/10
39/39 - 28s - loss: 1417.5002 - loglik: -1.4159e+03 - logprior: -1.6300e+00
Epoch 4/10
39/39 - 26s - loss: 1413.0792 - loglik: -1.4115e+03 - logprior: -1.5883e+00
Epoch 5/10
39/39 - 25s - loss: 1409.0476 - loglik: -1.4075e+03 - logprior: -1.5714e+00
Epoch 6/10
39/39 - 25s - loss: 1405.1281 - loglik: -1.4035e+03 - logprior: -1.5760e+00
Epoch 7/10
39/39 - 26s - loss: 1395.6139 - loglik: -1.3940e+03 - logprior: -1.6101e+00
Epoch 8/10
39/39 - 26s - loss: 1375.9098 - loglik: -1.3742e+03 - logprior: -1.7106e+00
Epoch 9/10
39/39 - 26s - loss: 1339.0432 - loglik: -1.3367e+03 - logprior: -2.3209e+00
Epoch 10/10
39/39 - 28s - loss: 1211.5743 - loglik: -1.2038e+03 - logprior: -7.7341e+00
Fitted a model with MAP estimate = -1161.4576
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 115 116 185 194 195 196 197 198 199 200 201 202 203 204 205
 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223
 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241
 242]
Re-initialized the encoder parameters.
Fitting a model of length 80 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 1686.4545 - loglik: -1.6832e+03 - logprior: -3.2673e+00
Epoch 2/2
19/19 - 6s - loss: 1655.2677 - loglik: -1.6547e+03 - logprior: -5.7020e-01
Fitted a model with MAP estimate = -1648.9176
expansions: [(0, 194), (80, 81)]
discards: [ 4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27
 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51
 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75
 76 77 78 79]
Re-initialized the encoder parameters.
Fitting a model of length 279 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 41s - loss: 1516.3215 - loglik: -1.5134e+03 - logprior: -2.9522e+00
Epoch 2/2
39/39 - 40s - loss: 1409.3293 - loglik: -1.4085e+03 - logprior: -8.7855e-01
Fitted a model with MAP estimate = -1400.2622
expansions: [(22, 1), (146, 2), (147, 1), (148, 1), (203, 6), (204, 3), (206, 4), (207, 1), (208, 2), (209, 2), (224, 1), (262, 1), (266, 1)]
discards: [ 0 59 60]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 45s - loss: 1401.3202 - loglik: -1.3991e+03 - logprior: -2.2676e+00
Epoch 2/10
39/39 - 42s - loss: 1390.7479 - loglik: -1.3902e+03 - logprior: -5.6213e-01
Epoch 3/10
39/39 - 38s - loss: 1387.0862 - loglik: -1.3866e+03 - logprior: -5.0995e-01
Epoch 4/10
39/39 - 36s - loss: 1383.8802 - loglik: -1.3834e+03 - logprior: -4.3142e-01
Epoch 5/10
39/39 - 35s - loss: 1380.1643 - loglik: -1.3798e+03 - logprior: -3.4198e-01
Epoch 6/10
39/39 - 33s - loss: 1374.7559 - loglik: -1.3745e+03 - logprior: -2.7665e-01
Epoch 7/10
39/39 - 34s - loss: 1358.6320 - loglik: -1.3583e+03 - logprior: -2.9382e-01
Epoch 8/10
39/39 - 35s - loss: 1307.6957 - loglik: -1.3071e+03 - logprior: -5.5197e-01
Epoch 9/10
39/39 - 38s - loss: 1207.2291 - loglik: -1.2053e+03 - logprior: -1.9032e+00
Epoch 10/10
39/39 - 39s - loss: 1141.4410 - loglik: -1.1379e+03 - logprior: -3.5151e+00
Fitted a model with MAP estimate = -1133.0559
Time for alignment: 873.2345
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 1542.7141 - loglik: -1.5411e+03 - logprior: -1.6604e+00
Epoch 2/10
39/39 - 26s - loss: 1431.7360 - loglik: -1.4301e+03 - logprior: -1.6119e+00
Epoch 3/10
39/39 - 26s - loss: 1418.4014 - loglik: -1.4168e+03 - logprior: -1.6450e+00
Epoch 4/10
39/39 - 25s - loss: 1414.4869 - loglik: -1.4129e+03 - logprior: -1.5824e+00
Epoch 5/10
39/39 - 24s - loss: 1409.6992 - loglik: -1.4081e+03 - logprior: -1.5504e+00
Epoch 6/10
39/39 - 24s - loss: 1406.2034 - loglik: -1.4047e+03 - logprior: -1.5375e+00
Epoch 7/10
39/39 - 24s - loss: 1396.8290 - loglik: -1.3952e+03 - logprior: -1.5758e+00
Epoch 8/10
39/39 - 24s - loss: 1370.8169 - loglik: -1.3691e+03 - logprior: -1.6726e+00
Epoch 9/10
39/39 - 24s - loss: 1332.7788 - loglik: -1.3304e+03 - logprior: -2.3835e+00
Epoch 10/10
39/39 - 25s - loss: 1207.1571 - loglik: -1.1991e+03 - logprior: -8.0215e+00
Fitted a model with MAP estimate = -1160.9213
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 115 173 193 194 195 196 197 198 199 200 201 202 203 204 205
 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223
 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241
 242]
Re-initialized the encoder parameters.
Fitting a model of length 80 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 1680.0591 - loglik: -1.6766e+03 - logprior: -3.4793e+00
Epoch 2/2
19/19 - 5s - loss: 1655.2532 - loglik: -1.6542e+03 - logprior: -1.0720e+00
Fitted a model with MAP estimate = -1649.6084
expansions: [(0, 225)]
discards: [10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33
 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57
 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 1547.7910 - loglik: -1.5457e+03 - logprior: -2.0632e+00
Epoch 2/2
39/39 - 25s - loss: 1464.5311 - loglik: -1.4637e+03 - logprior: -8.6643e-01
Fitted a model with MAP estimate = -1455.9401
expansions: [(18, 1), (29, 1), (65, 1), (147, 2), (161, 1), (171, 1), (195, 1), (197, 1), (213, 6), (219, 13), (220, 6), (227, 28), (228, 1), (230, 7)]
discards: [  0  58  59 188 189]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 1422.4984 - loglik: -1.4202e+03 - logprior: -2.3179e+00
Epoch 2/10
39/39 - 35s - loss: 1390.4668 - loglik: -1.3899e+03 - logprior: -5.8404e-01
Epoch 3/10
39/39 - 34s - loss: 1386.3800 - loglik: -1.3858e+03 - logprior: -5.3095e-01
Epoch 4/10
39/39 - 34s - loss: 1383.1923 - loglik: -1.3827e+03 - logprior: -4.6595e-01
Epoch 5/10
39/39 - 33s - loss: 1378.7843 - loglik: -1.3784e+03 - logprior: -4.2132e-01
Epoch 6/10
39/39 - 34s - loss: 1373.7081 - loglik: -1.3733e+03 - logprior: -3.9053e-01
Epoch 7/10
39/39 - 35s - loss: 1362.3750 - loglik: -1.3620e+03 - logprior: -3.6378e-01
Epoch 8/10
39/39 - 36s - loss: 1321.0919 - loglik: -1.3205e+03 - logprior: -5.7315e-01
Epoch 9/10
39/39 - 37s - loss: 1240.6244 - loglik: -1.2391e+03 - logprior: -1.4998e+00
Epoch 10/10
39/39 - 36s - loss: 1151.2137 - loglik: -1.1465e+03 - logprior: -4.6341e+00
Fitted a model with MAP estimate = -1136.6221
Time for alignment: 766.3793
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 1546.4395 - loglik: -1.5448e+03 - logprior: -1.6274e+00
Epoch 2/10
39/39 - 24s - loss: 1434.4362 - loglik: -1.4330e+03 - logprior: -1.4114e+00
Epoch 3/10
39/39 - 24s - loss: 1422.6130 - loglik: -1.4212e+03 - logprior: -1.3632e+00
Epoch 4/10
39/39 - 24s - loss: 1417.3677 - loglik: -1.4161e+03 - logprior: -1.2806e+00
Epoch 5/10
39/39 - 24s - loss: 1413.2006 - loglik: -1.4119e+03 - logprior: -1.2631e+00
Epoch 6/10
39/39 - 25s - loss: 1406.0499 - loglik: -1.4048e+03 - logprior: -1.2808e+00
Epoch 7/10
39/39 - 26s - loss: 1397.3937 - loglik: -1.3961e+03 - logprior: -1.3072e+00
Epoch 8/10
39/39 - 27s - loss: 1373.6365 - loglik: -1.3722e+03 - logprior: -1.4304e+00
Epoch 9/10
39/39 - 27s - loss: 1328.5785 - loglik: -1.3260e+03 - logprior: -2.5475e+00
Epoch 10/10
39/39 - 26s - loss: 1196.8275 - loglik: -1.1883e+03 - logprior: -8.4629e+00
Fitted a model with MAP estimate = -1160.0249
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 165 174 190 194 195 196
 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 218
 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236
 237 238 239 240 241 242]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 1675.2682 - loglik: -1.6718e+03 - logprior: -3.4421e+00
Epoch 2/2
19/19 - 5s - loss: 1655.7941 - loglik: -1.6552e+03 - logprior: -5.6644e-01
Fitted a model with MAP estimate = -1650.0169
expansions: [(0, 215), (75, 64)]
discards: [ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29
 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53
 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 1518.6217 - loglik: -1.5162e+03 - logprior: -2.4692e+00
Epoch 2/2
39/39 - 32s - loss: 1406.8809 - loglik: -1.4059e+03 - logprior: -9.3834e-01
Fitted a model with MAP estimate = -1398.5503
expansions: [(18, 1), (20, 1), (21, 1), (22, 1), (84, 1), (140, 3), (166, 3), (190, 1), (192, 1), (193, 2), (198, 1), (220, 1), (231, 1), (232, 1), (255, 3), (269, 1), (272, 1), (273, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 1397.0881 - loglik: -1.3947e+03 - logprior: -2.3666e+00
Epoch 2/10
39/39 - 36s - loss: 1385.8888 - loglik: -1.3853e+03 - logprior: -6.0358e-01
Epoch 3/10
39/39 - 38s - loss: 1383.0052 - loglik: -1.3825e+03 - logprior: -4.9214e-01
Epoch 4/10
39/39 - 40s - loss: 1378.2324 - loglik: -1.3778e+03 - logprior: -4.0606e-01
Epoch 5/10
39/39 - 40s - loss: 1373.1844 - loglik: -1.3728e+03 - logprior: -3.3448e-01
Epoch 6/10
39/39 - 41s - loss: 1370.2023 - loglik: -1.3699e+03 - logprior: -3.0836e-01
Epoch 7/10
39/39 - 45s - loss: 1356.0242 - loglik: -1.3557e+03 - logprior: -3.4926e-01
Epoch 8/10
39/39 - 47s - loss: 1314.2295 - loglik: -1.3136e+03 - logprior: -5.8169e-01
Epoch 9/10
39/39 - 48s - loss: 1227.4177 - loglik: -1.2260e+03 - logprior: -1.3832e+00
Epoch 10/10
39/39 - 49s - loss: 1151.6819 - loglik: -1.1475e+03 - logprior: -4.1392e+00
Fitted a model with MAP estimate = -1138.5530
Time for alignment: 865.0539
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 1544.8137 - loglik: -1.5432e+03 - logprior: -1.6449e+00
Epoch 2/10
39/39 - 32s - loss: 1430.3759 - loglik: -1.4289e+03 - logprior: -1.5174e+00
Epoch 3/10
39/39 - 33s - loss: 1417.5181 - loglik: -1.4159e+03 - logprior: -1.5788e+00
Epoch 4/10
39/39 - 32s - loss: 1413.0675 - loglik: -1.4115e+03 - logprior: -1.5837e+00
Epoch 5/10
39/39 - 30s - loss: 1408.8461 - loglik: -1.4073e+03 - logprior: -1.5486e+00
Epoch 6/10
39/39 - 28s - loss: 1405.1766 - loglik: -1.4036e+03 - logprior: -1.5491e+00
Epoch 7/10
39/39 - 26s - loss: 1396.0076 - loglik: -1.3944e+03 - logprior: -1.5627e+00
Epoch 8/10
39/39 - 25s - loss: 1373.8021 - loglik: -1.3721e+03 - logprior: -1.6720e+00
Epoch 9/10
39/39 - 26s - loss: 1333.9757 - loglik: -1.3314e+03 - logprior: -2.5206e+00
Epoch 10/10
39/39 - 26s - loss: 1208.5957 - loglik: -1.2003e+03 - logprior: -8.2130e+00
Fitted a model with MAP estimate = -1162.5219
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 115 116 131 132 133 185 192 193 194 195 196 197 198 199 200 201
 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219
 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237
 238 239 240 241 242]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 1678.5243 - loglik: -1.6752e+03 - logprior: -3.3226e+00
Epoch 2/2
19/19 - 5s - loss: 1656.0787 - loglik: -1.6550e+03 - logprior: -1.0621e+00
Fitted a model with MAP estimate = -1650.9145
expansions: [(0, 202), (76, 77)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48
 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72
 73 74 75]
Re-initialized the encoder parameters.
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 1520.9264 - loglik: -1.5192e+03 - logprior: -1.7149e+00
Epoch 2/2
39/39 - 36s - loss: 1416.6741 - loglik: -1.4158e+03 - logprior: -8.3078e-01
Fitted a model with MAP estimate = -1407.8851
expansions: [(18, 1), (20, 1), (21, 1), (23, 1), (81, 1), (131, 1), (138, 4), (169, 1), (194, 3), (205, 5), (248, 1), (250, 3), (264, 1), (267, 1), (268, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 1406.7255 - loglik: -1.4044e+03 - logprior: -2.3263e+00
Epoch 2/10
39/39 - 35s - loss: 1395.3755 - loglik: -1.3949e+03 - logprior: -5.1017e-01
Epoch 3/10
39/39 - 35s - loss: 1390.7831 - loglik: -1.3904e+03 - logprior: -4.0317e-01
Epoch 4/10
39/39 - 35s - loss: 1385.1824 - loglik: -1.3848e+03 - logprior: -3.3936e-01
Epoch 5/10
39/39 - 39s - loss: 1381.3464 - loglik: -1.3810e+03 - logprior: -2.9735e-01
Epoch 6/10
39/39 - 43s - loss: 1374.8970 - loglik: -1.3746e+03 - logprior: -2.5747e-01
Epoch 7/10
39/39 - 45s - loss: 1363.9796 - loglik: -1.3636e+03 - logprior: -3.4063e-01
Epoch 8/10
39/39 - 47s - loss: 1318.6202 - loglik: -1.3180e+03 - logprior: -5.4528e-01
Epoch 9/10
39/39 - 48s - loss: 1229.3340 - loglik: -1.2280e+03 - logprior: -1.2721e+00
Epoch 10/10
39/39 - 48s - loss: 1140.6771 - loglik: -1.1383e+03 - logprior: -2.3441e+00
Fitted a model with MAP estimate = -1126.9721
Time for alignment: 911.1555
Computed alignments with likelihoods: ['-1132.8805', '-1133.0559', '-1136.6221', '-1138.5530', '-1126.9721']
Best model has likelihood: -1126.9721  (prior= -1.6840 )
time for generating output: 0.2961
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13393.projection.fasta
SP score = 0.17858569716340392
Training of 5 independent models on file PF00202.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6e5c4a610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff67fec7250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6b340c8e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 60s - loss: 2069.1836 - loglik: -2.0679e+03 - logprior: -1.3081e+00
Epoch 2/10
39/39 - 60s - loss: 1911.4994 - loglik: -1.9106e+03 - logprior: -8.8606e-01
Epoch 3/10
39/39 - 63s - loss: 1897.9760 - loglik: -1.8970e+03 - logprior: -9.9820e-01
Epoch 4/10
39/39 - 64s - loss: 1891.4769 - loglik: -1.8905e+03 - logprior: -9.9763e-01
Epoch 5/10
39/39 - 65s - loss: 1885.1013 - loglik: -1.8841e+03 - logprior: -9.4190e-01
Epoch 6/10
39/39 - 65s - loss: 1878.3223 - loglik: -1.8773e+03 - logprior: -9.7595e-01
Epoch 7/10
39/39 - 62s - loss: 1861.7123 - loglik: -1.8607e+03 - logprior: -1.0178e+00
Epoch 8/10
39/39 - 53s - loss: 1817.2524 - loglik: -1.8159e+03 - logprior: -1.3127e+00
Epoch 9/10
39/39 - 52s - loss: 1687.4465 - loglik: -1.6815e+03 - logprior: -5.8600e+00
Epoch 10/10
39/39 - 52s - loss: 1561.6631 - loglik: -1.5533e+03 - logprior: -8.3326e+00
Fitted a model with MAP estimate = -1536.6078
expansions: [(0, 2)]
discards: [ 21  22  23  24  25  26  31  32  33  34  35  36  37  38  39  40  41  42
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90 107 108 109 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 2249.8262 - loglik: -2.2475e+03 - logprior: -2.3502e+00
Epoch 2/2
39/39 - 12s - loss: 2222.8757 - loglik: -2.2226e+03 - logprior: -2.7771e-01
Fitted a model with MAP estimate = -2170.9084
expansions: [(0, 249), (110, 59)]
discards: [  2   3  44  60  61  62  72  73  74  75  76  77  78  79  80  81  82  83
  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101
 102 103 104 105 106 107 108 109]
Re-initialized the encoder parameters.
Fitting a model of length 374 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 66s - loss: 2034.7959 - loglik: -2.0326e+03 - logprior: -2.2207e+00
Epoch 2/2
39/39 - 61s - loss: 1887.3546 - loglik: -1.8867e+03 - logprior: -6.6857e-01
Fitted a model with MAP estimate = -1857.8027
expansions: [(0, 2), (6, 1), (45, 1), (55, 1), (59, 1), (74, 1), (131, 1), (156, 2), (157, 1), (158, 1), (173, 1), (180, 1), (188, 1), (205, 2), (210, 1), (234, 1), (239, 2), (240, 3), (291, 3)]
discards: [306 307 308 309 310 311 312 313 314 315 322 323 324 325]
Re-initialized the encoder parameters.
Fitting a model of length 387 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 91s - loss: 1859.4535 - loglik: -1.8572e+03 - logprior: -2.2331e+00
Epoch 2/10
39/39 - 91s - loss: 1846.4445 - loglik: -1.8462e+03 - logprior: -2.0915e-01
Epoch 3/10
39/39 - 82s - loss: 1843.8165 - loglik: -1.8439e+03 - logprior: 0.0551
Epoch 4/10
39/39 - 89s - loss: 1838.1509 - loglik: -1.8383e+03 - logprior: 0.1826
Epoch 5/10
39/39 - 92s - loss: 1829.6666 - loglik: -1.8299e+03 - logprior: 0.2490
Epoch 6/10
39/39 - 92s - loss: 1822.8518 - loglik: -1.8231e+03 - logprior: 0.2926
Epoch 7/10
39/39 - 90s - loss: 1797.7369 - loglik: -1.7980e+03 - logprior: 0.3129
Epoch 8/10
39/39 - 73s - loss: 1719.6844 - loglik: -1.7196e+03 - logprior: -2.2366e-03
Epoch 9/10
39/39 - 73s - loss: 1592.6239 - loglik: -1.5909e+03 - logprior: -1.6743e+00
Epoch 10/10
39/39 - 74s - loss: 1512.5175 - loglik: -1.5095e+03 - logprior: -2.9090e+00
Fitted a model with MAP estimate = -1503.4996
Time for alignment: 1854.3578
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 60s - loss: 2071.2908 - loglik: -2.0700e+03 - logprior: -1.3364e+00
Epoch 2/10
39/39 - 51s - loss: 1911.2460 - loglik: -1.9102e+03 - logprior: -1.0163e+00
Epoch 3/10
39/39 - 46s - loss: 1898.9612 - loglik: -1.8979e+03 - logprior: -1.0397e+00
Epoch 4/10
39/39 - 47s - loss: 1891.8369 - loglik: -1.8907e+03 - logprior: -1.0818e+00
Epoch 5/10
39/39 - 54s - loss: 1885.0593 - loglik: -1.8840e+03 - logprior: -1.0502e+00
Epoch 6/10
39/39 - 60s - loss: 1878.4998 - loglik: -1.8774e+03 - logprior: -1.0906e+00
Epoch 7/10
39/39 - 65s - loss: 1863.3119 - loglik: -1.8621e+03 - logprior: -1.1955e+00
Epoch 8/10
39/39 - 66s - loss: 1821.6990 - loglik: -1.8203e+03 - logprior: -1.3513e+00
Epoch 9/10
39/39 - 61s - loss: 1699.7091 - loglik: -1.6944e+03 - logprior: -5.2504e+00
Epoch 10/10
39/39 - 59s - loss: 1565.5129 - loglik: -1.5574e+03 - logprior: -8.0248e+00
Fitted a model with MAP estimate = -1536.4327
expansions: []
discards: [  8   9  10  11  12  13  14  15  16  22  23  24  25  26  37  38  39  40
  41  42  43  72  73  74  75  76  77  78  87  88  89  90 108 109 110 148
 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166
 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184
 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202
 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220
 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238
 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256
 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274
 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292
 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310
 311 312 313 314 315 316 317 318]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 2256.0117 - loglik: -2.2539e+03 - logprior: -2.0702e+00
Epoch 2/2
39/39 - 13s - loss: 2216.3901 - loglik: -2.2155e+03 - logprior: -9.3908e-01
Fitted a model with MAP estimate = -2172.0203
expansions: [(0, 176), (2, 1), (6, 46), (7, 4), (27, 1), (29, 1), (30, 17), (113, 63)]
discards: [ 39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56
  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74
  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92
  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110
 111 112]
Re-initialized the encoder parameters.
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 78s - loss: 2046.2988 - loglik: -2.0441e+03 - logprior: -2.1955e+00
Epoch 2/2
39/39 - 76s - loss: 1918.0619 - loglik: -1.9174e+03 - logprior: -6.7083e-01
Fitted a model with MAP estimate = -1887.3995
expansions: [(0, 2), (35, 1), (59, 1), (61, 1), (150, 1), (188, 1), (196, 2), (197, 3), (206, 1), (213, 2), (215, 2), (216, 3), (218, 1), (223, 1), (233, 20), (235, 1), (263, 1), (264, 1), (265, 3), (266, 5), (267, 3)]
discards: [127 128 129 130 131 132 133]
Re-initialized the encoder parameters.
Fitting a model of length 397 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 79s - loss: 1869.6635 - loglik: -1.8673e+03 - logprior: -2.3703e+00
Epoch 2/10
39/39 - 78s - loss: 1845.1759 - loglik: -1.8448e+03 - logprior: -3.9750e-01
Epoch 3/10
39/39 - 80s - loss: 1843.0735 - loglik: -1.8430e+03 - logprior: -8.2303e-02
Epoch 4/10
39/39 - 74s - loss: 1836.8959 - loglik: -1.8369e+03 - logprior: 0.0205
Epoch 5/10
39/39 - 78s - loss: 1826.2150 - loglik: -1.8263e+03 - logprior: 0.1008
Epoch 6/10
39/39 - 89s - loss: 1819.5551 - loglik: -1.8197e+03 - logprior: 0.1541
Epoch 7/10
39/39 - 96s - loss: 1796.2865 - loglik: -1.7963e+03 - logprior: 0.0793
Epoch 8/10
39/39 - 97s - loss: 1709.8346 - loglik: -1.7095e+03 - logprior: -2.5862e-01
Epoch 9/10
39/39 - 92s - loss: 1589.2539 - loglik: -1.5874e+03 - logprior: -1.7736e+00
Epoch 10/10
39/39 - 75s - loss: 1511.0627 - loglik: -1.5087e+03 - logprior: -2.2610e+00
Fitted a model with MAP estimate = -1501.5702
Time for alignment: 1853.6898
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 2070.6831 - loglik: -2.0694e+03 - logprior: -1.3066e+00
Epoch 2/10
39/39 - 52s - loss: 1910.6007 - loglik: -1.9097e+03 - logprior: -9.2122e-01
Epoch 3/10
39/39 - 58s - loss: 1898.6975 - loglik: -1.8977e+03 - logprior: -9.7887e-01
Epoch 4/10
39/39 - 53s - loss: 1892.5693 - loglik: -1.8916e+03 - logprior: -9.7532e-01
Epoch 5/10
39/39 - 50s - loss: 1885.6951 - loglik: -1.8847e+03 - logprior: -9.5777e-01
Epoch 6/10
39/39 - 55s - loss: 1879.1328 - loglik: -1.8782e+03 - logprior: -9.5951e-01
Epoch 7/10
39/39 - 62s - loss: 1863.2572 - loglik: -1.8622e+03 - logprior: -1.0610e+00
Epoch 8/10
39/39 - 64s - loss: 1823.1866 - loglik: -1.8219e+03 - logprior: -1.2151e+00
Epoch 9/10
39/39 - 64s - loss: 1712.2344 - loglik: -1.7071e+03 - logprior: -5.1017e+00
Epoch 10/10
39/39 - 56s - loss: 1568.8490 - loglik: -1.5607e+03 - logprior: -8.0426e+00
Fitted a model with MAP estimate = -1537.7555
expansions: [(0, 1)]
discards: [ 21  22  23  24  25  26  27  36  37  38  39  40  41  42  43  71  72  73
  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89 106 107
 108 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163
 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181
 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199
 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217
 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235
 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253
 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271
 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289
 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307
 308 309 310 311 312 313 314 315 316 317 318]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 2250.9507 - loglik: -2.2487e+03 - logprior: -2.2808e+00
Epoch 2/2
39/39 - 11s - loss: 2222.1042 - loglik: -2.2217e+03 - logprior: -3.9952e-01
Fitted a model with MAP estimate = -2175.4894
expansions: [(0, 233), (57, 5), (111, 98)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  73  74  75  76  77  78  79  80  81  82  83  84  85  86
  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104
 105 106 107 108 109 110]
Re-initialized the encoder parameters.
Fitting a model of length 387 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 70s - loss: 2051.0525 - loglik: -2.0489e+03 - logprior: -2.1812e+00
Epoch 2/2
39/39 - 72s - loss: 1910.8628 - loglik: -1.9095e+03 - logprior: -1.3581e+00
Fitted a model with MAP estimate = -1880.8200
expansions: [(0, 2), (6, 1), (15, 1), (19, 1), (51, 1), (53, 1), (57, 1), (62, 1), (69, 1), (70, 1), (137, 1), (156, 2), (157, 1), (176, 1), (190, 1), (208, 2), (236, 30), (271, 1), (303, 1), (306, 1), (310, 1)]
discards: [139 140 141 142 143 289 292 293 294 295 296 297 298 345 346 347 348 349
 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367
 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385
 386]
Re-initialized the encoder parameters.
Fitting a model of length 385 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 71s - loss: 1866.2241 - loglik: -1.8641e+03 - logprior: -2.1051e+00
Epoch 2/10
39/39 - 70s - loss: 1846.5956 - loglik: -1.8464e+03 - logprior: -1.8168e-01
Epoch 3/10
39/39 - 69s - loss: 1843.5371 - loglik: -1.8436e+03 - logprior: 0.0400
Epoch 4/10
39/39 - 69s - loss: 1837.1052 - loglik: -1.8373e+03 - logprior: 0.1845
Epoch 5/10
39/39 - 69s - loss: 1830.6755 - loglik: -1.8309e+03 - logprior: 0.2414
Epoch 6/10
39/39 - 70s - loss: 1823.9497 - loglik: -1.8243e+03 - logprior: 0.3229
Epoch 7/10
39/39 - 69s - loss: 1794.1685 - loglik: -1.7944e+03 - logprior: 0.2193
Epoch 8/10
39/39 - 67s - loss: 1712.9930 - loglik: -1.7127e+03 - logprior: -2.1294e-01
Epoch 9/10
39/39 - 67s - loss: 1591.7875 - loglik: -1.5890e+03 - logprior: -2.7878e+00
Epoch 10/10
39/39 - 70s - loss: 1517.0226 - loglik: -1.5143e+03 - logprior: -2.6421e+00
Fitted a model with MAP estimate = -1504.2676
Time for alignment: 1676.7919
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 51s - loss: 2072.1189 - loglik: -2.0708e+03 - logprior: -1.3150e+00
Epoch 2/10
39/39 - 49s - loss: 1912.1456 - loglik: -1.9112e+03 - logprior: -9.9323e-01
Epoch 3/10
39/39 - 51s - loss: 1897.6394 - loglik: -1.8967e+03 - logprior: -9.7253e-01
Epoch 4/10
39/39 - 52s - loss: 1892.3462 - loglik: -1.8914e+03 - logprior: -9.5426e-01
Epoch 5/10
39/39 - 51s - loss: 1885.3313 - loglik: -1.8844e+03 - logprior: -9.5572e-01
Epoch 6/10
39/39 - 54s - loss: 1879.1298 - loglik: -1.8781e+03 - logprior: -9.8052e-01
Epoch 7/10
39/39 - 54s - loss: 1861.6418 - loglik: -1.8606e+03 - logprior: -1.0569e+00
Epoch 8/10
39/39 - 53s - loss: 1818.3612 - loglik: -1.8170e+03 - logprior: -1.3089e+00
Epoch 9/10
39/39 - 51s - loss: 1684.2141 - loglik: -1.6786e+03 - logprior: -5.5232e+00
Epoch 10/10
39/39 - 51s - loss: 1551.8047 - loglik: -1.5449e+03 - logprior: -6.8203e+00
Fitted a model with MAP estimate = -1526.2228
expansions: [(0, 2)]
discards: [ 22  23  24  25  26  43 107 108 109 148 149 150 151 152 153 154 155 156
 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174
 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192
 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210
 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228
 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246
 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264
 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282
 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300
 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 2249.1042 - loglik: -2.2468e+03 - logprior: -2.3123e+00
Epoch 2/2
39/39 - 15s - loss: 2225.7126 - loglik: -2.2256e+03 - logprior: -6.9498e-02
Fitted a model with MAP estimate = -2189.5143
expansions: [(0, 85), (1, 13), (2, 1), (141, 265)]
discards: [ 17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34
  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52
  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70
  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88
  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106
 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124
 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140]
Re-initialized the encoder parameters.
Fitting a model of length 381 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 71s - loss: 2026.2615 - loglik: -2.0241e+03 - logprior: -2.1285e+00
Epoch 2/2
39/39 - 69s - loss: 1881.4774 - loglik: -1.8808e+03 - logprior: -6.6721e-01
Fitted a model with MAP estimate = -1852.1722
expansions: [(239, 1), (271, 1), (273, 3), (296, 1), (303, 1), (306, 1), (307, 1), (309, 1), (311, 1), (314, 1), (315, 1), (339, 1), (344, 1), (368, 2), (372, 1)]
discards: [  1  44  85  86  87  88 168 169 170]
Re-initialized the encoder parameters.
Fitting a model of length 390 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 67s - loss: 1852.1981 - loglik: -1.8508e+03 - logprior: -1.4452e+00
Epoch 2/10
39/39 - 63s - loss: 1842.3728 - loglik: -1.8422e+03 - logprior: -1.3060e-01
Epoch 3/10
39/39 - 63s - loss: 1838.5603 - loglik: -1.8385e+03 - logprior: -1.8114e-02
Epoch 4/10
39/39 - 63s - loss: 1834.8802 - loglik: -1.8349e+03 - logprior: 0.0263
Epoch 5/10
39/39 - 63s - loss: 1828.7506 - loglik: -1.8289e+03 - logprior: 0.1109
Epoch 6/10
39/39 - 64s - loss: 1816.6576 - loglik: -1.8167e+03 - logprior: 0.0736
Epoch 7/10
39/39 - 71s - loss: 1792.9741 - loglik: -1.7929e+03 - logprior: -3.0669e-02
Epoch 8/10
39/39 - 70s - loss: 1696.5835 - loglik: -1.6962e+03 - logprior: -3.6570e-01
Epoch 9/10
39/39 - 75s - loss: 1585.4546 - loglik: -1.5836e+03 - logprior: -1.8092e+00
Epoch 10/10
39/39 - 75s - loss: 1513.0979 - loglik: -1.5099e+03 - logprior: -3.0925e+00
Fitted a model with MAP estimate = -1503.7806
Time for alignment: 1621.6459
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 55s - loss: 2072.8042 - loglik: -2.0715e+03 - logprior: -1.3255e+00
Epoch 2/10
39/39 - 53s - loss: 1909.9882 - loglik: -1.9089e+03 - logprior: -1.0914e+00
Epoch 3/10
39/39 - 53s - loss: 1896.5100 - loglik: -1.8953e+03 - logprior: -1.1857e+00
Epoch 4/10
39/39 - 53s - loss: 1890.7782 - loglik: -1.8896e+03 - logprior: -1.1622e+00
Epoch 5/10
39/39 - 53s - loss: 1884.9778 - loglik: -1.8838e+03 - logprior: -1.1269e+00
Epoch 6/10
39/39 - 54s - loss: 1879.0881 - loglik: -1.8779e+03 - logprior: -1.1559e+00
Epoch 7/10
39/39 - 55s - loss: 1864.1909 - loglik: -1.8630e+03 - logprior: -1.2051e+00
Epoch 8/10
39/39 - 55s - loss: 1827.4087 - loglik: -1.8260e+03 - logprior: -1.3271e+00
Epoch 9/10
39/39 - 54s - loss: 1755.2228 - loglik: -1.7524e+03 - logprior: -2.7458e+00
Epoch 10/10
39/39 - 48s - loss: 1590.8108 - loglik: -1.5814e+03 - logprior: -9.3487e+00
Fitted a model with MAP estimate = -1543.6673
expansions: []
discards: [ 22  23  24  25  26  31  32  33  34  35  36  37  38  39  40  41  42  68
  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86
  87  88  98  99 100 106 107 108 147 148 149 150 151 152 153 154 155 156
 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174
 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192
 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210
 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228
 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246
 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264
 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282
 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300
 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 2255.0117 - loglik: -2.2529e+03 - logprior: -2.1174e+00
Epoch 2/2
39/39 - 10s - loss: 2218.2808 - loglik: -2.2171e+03 - logprior: -1.2205e+00
Fitted a model with MAP estimate = -2165.2341
expansions: [(0, 78), (57, 1), (58, 61), (59, 16), (60, 8), (61, 1), (62, 17), (63, 6), (100, 1), (103, 91)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 69]
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 2029.0231 - loglik: -2.0271e+03 - logprior: -1.9586e+00
Epoch 2/2
39/39 - 50s - loss: 1907.4507 - loglik: -1.9067e+03 - logprior: -7.0879e-01
Fitted a model with MAP estimate = -1877.5230
expansions: [(0, 2), (43, 1), (53, 1), (59, 1), (66, 1), (129, 2), (130, 3), (138, 1), (139, 4), (148, 1), (150, 3), (151, 1), (152, 1), (171, 1), (174, 1), (175, 1), (183, 1), (186, 1), (197, 2), (201, 1), (202, 1), (223, 1), (232, 14), (274, 1), (302, 1), (303, 2), (335, 1)]
discards: [216 217 218 219 294 296 297 298]
Re-initialized the encoder parameters.
Fitting a model of length 392 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 69s - loss: 1865.7306 - loglik: -1.8638e+03 - logprior: -1.9500e+00
Epoch 2/10
39/39 - 68s - loss: 1847.1031 - loglik: -1.8470e+03 - logprior: -8.8002e-02
Epoch 3/10
39/39 - 68s - loss: 1843.6554 - loglik: -1.8437e+03 - logprior: 0.0791
Epoch 4/10
39/39 - 67s - loss: 1837.5806 - loglik: -1.8378e+03 - logprior: 0.1852
Epoch 5/10
39/39 - 69s - loss: 1829.7534 - loglik: -1.8300e+03 - logprior: 0.2678
Epoch 6/10
39/39 - 69s - loss: 1822.5406 - loglik: -1.8229e+03 - logprior: 0.3265
Epoch 7/10
39/39 - 68s - loss: 1800.0863 - loglik: -1.8003e+03 - logprior: 0.2788
Epoch 8/10
39/39 - 70s - loss: 1720.3870 - loglik: -1.7204e+03 - logprior: 0.0351
Epoch 9/10
39/39 - 70s - loss: 1605.0957 - loglik: -1.6040e+03 - logprior: -1.0267e+00
Epoch 10/10
39/39 - 69s - loss: 1512.7721 - loglik: -1.5102e+03 - logprior: -2.5102e+00
Fitted a model with MAP estimate = -1502.5058
Time for alignment: 1541.9266
Computed alignments with likelihoods: ['-1503.4996', '-1501.5702', '-1504.2676', '-1503.7806', '-1502.5058']
Best model has likelihood: -1501.5702  (prior= -1.8669 )
time for generating output: 1.4575
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00202.projection.fasta
SP score = 0.12496078861237861
Training of 5 independent models on file PF00018.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff66ca11be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff67f476670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff5b8065f70>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 270.8808 - loglik: -2.6765e+02 - logprior: -3.2261e+00
Epoch 2/10
19/19 - 1s - loss: 246.6264 - loglik: -2.4523e+02 - logprior: -1.4003e+00
Epoch 3/10
19/19 - 1s - loss: 239.3259 - loglik: -2.3779e+02 - logprior: -1.5309e+00
Epoch 4/10
19/19 - 1s - loss: 236.2359 - loglik: -2.3482e+02 - logprior: -1.4154e+00
Epoch 5/10
19/19 - 1s - loss: 235.1444 - loglik: -2.3375e+02 - logprior: -1.3913e+00
Epoch 6/10
19/19 - 1s - loss: 234.6494 - loglik: -2.3328e+02 - logprior: -1.3696e+00
Epoch 7/10
19/19 - 1s - loss: 234.0066 - loglik: -2.3265e+02 - logprior: -1.3500e+00
Epoch 8/10
19/19 - 1s - loss: 233.8971 - loglik: -2.3255e+02 - logprior: -1.3408e+00
Epoch 9/10
19/19 - 1s - loss: 233.9716 - loglik: -2.3263e+02 - logprior: -1.3323e+00
Fitted a model with MAP estimate = -233.4676
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (23, 2), (28, 2), (29, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 240.6980 - loglik: -2.3655e+02 - logprior: -4.1485e+00
Epoch 2/2
19/19 - 1s - loss: 232.5253 - loglik: -2.3047e+02 - logprior: -2.0559e+00
Fitted a model with MAP estimate = -230.9239
expansions: [(0, 2)]
discards: [ 0  9 12 29 32 40 42]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 233.2396 - loglik: -2.3017e+02 - logprior: -3.0701e+00
Epoch 2/2
19/19 - 1s - loss: 229.8842 - loglik: -2.2862e+02 - logprior: -1.2659e+00
Fitted a model with MAP estimate = -229.4145
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 234.2049 - loglik: -2.3061e+02 - logprior: -3.5972e+00
Epoch 2/10
19/19 - 1s - loss: 230.3271 - loglik: -2.2894e+02 - logprior: -1.3913e+00
Epoch 3/10
19/19 - 1s - loss: 229.7595 - loglik: -2.2846e+02 - logprior: -1.2993e+00
Epoch 4/10
19/19 - 1s - loss: 229.1016 - loglik: -2.2785e+02 - logprior: -1.2487e+00
Epoch 5/10
19/19 - 1s - loss: 228.8622 - loglik: -2.2764e+02 - logprior: -1.2205e+00
Epoch 6/10
19/19 - 1s - loss: 228.0998 - loglik: -2.2689e+02 - logprior: -1.2007e+00
Epoch 7/10
19/19 - 1s - loss: 227.8511 - loglik: -2.2665e+02 - logprior: -1.1899e+00
Epoch 8/10
19/19 - 1s - loss: 227.4294 - loglik: -2.2624e+02 - logprior: -1.1790e+00
Epoch 9/10
19/19 - 1s - loss: 227.1608 - loglik: -2.2598e+02 - logprior: -1.1700e+00
Epoch 10/10
19/19 - 1s - loss: 227.1492 - loglik: -2.2598e+02 - logprior: -1.1619e+00
Fitted a model with MAP estimate = -226.6408
Time for alignment: 39.5611
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 270.8874 - loglik: -2.6766e+02 - logprior: -3.2285e+00
Epoch 2/10
19/19 - 1s - loss: 246.2847 - loglik: -2.4487e+02 - logprior: -1.4133e+00
Epoch 3/10
19/19 - 1s - loss: 238.3741 - loglik: -2.3704e+02 - logprior: -1.3299e+00
Epoch 4/10
19/19 - 1s - loss: 235.9058 - loglik: -2.3462e+02 - logprior: -1.2850e+00
Epoch 5/10
19/19 - 1s - loss: 234.8199 - loglik: -2.3356e+02 - logprior: -1.2557e+00
Epoch 6/10
19/19 - 1s - loss: 234.1176 - loglik: -2.3288e+02 - logprior: -1.2361e+00
Epoch 7/10
19/19 - 1s - loss: 233.9561 - loglik: -2.3273e+02 - logprior: -1.2176e+00
Epoch 8/10
19/19 - 1s - loss: 233.7963 - loglik: -2.3258e+02 - logprior: -1.2084e+00
Epoch 9/10
19/19 - 1s - loss: 233.4271 - loglik: -2.3222e+02 - logprior: -1.2006e+00
Epoch 10/10
19/19 - 1s - loss: 233.5206 - loglik: -2.3231e+02 - logprior: -1.1982e+00
Fitted a model with MAP estimate = -232.8983
expansions: [(0, 2), (8, 1), (12, 1), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 238.8708 - loglik: -2.3452e+02 - logprior: -4.3523e+00
Epoch 2/2
19/19 - 1s - loss: 230.5753 - loglik: -2.2924e+02 - logprior: -1.3379e+00
Fitted a model with MAP estimate = -229.3576
expansions: []
discards: [ 0 28 40]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 235.0363 - loglik: -2.3095e+02 - logprior: -4.0910e+00
Epoch 2/2
19/19 - 1s - loss: 230.6168 - loglik: -2.2907e+02 - logprior: -1.5417e+00
Fitted a model with MAP estimate = -229.8507
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 232.9647 - loglik: -2.2969e+02 - logprior: -3.2766e+00
Epoch 2/10
19/19 - 1s - loss: 230.2932 - loglik: -2.2889e+02 - logprior: -1.4055e+00
Epoch 3/10
19/19 - 1s - loss: 229.5982 - loglik: -2.2830e+02 - logprior: -1.3000e+00
Epoch 4/10
19/19 - 1s - loss: 229.3584 - loglik: -2.2811e+02 - logprior: -1.2437e+00
Epoch 5/10
19/19 - 1s - loss: 228.4788 - loglik: -2.2726e+02 - logprior: -1.2134e+00
Epoch 6/10
19/19 - 1s - loss: 227.9035 - loglik: -2.2671e+02 - logprior: -1.1915e+00
Epoch 7/10
19/19 - 1s - loss: 227.7996 - loglik: -2.2661e+02 - logprior: -1.1834e+00
Epoch 8/10
19/19 - 1s - loss: 227.6191 - loglik: -2.2644e+02 - logprior: -1.1668e+00
Epoch 9/10
19/19 - 1s - loss: 227.1223 - loglik: -2.2595e+02 - logprior: -1.1621e+00
Epoch 10/10
19/19 - 1s - loss: 226.9856 - loglik: -2.2582e+02 - logprior: -1.1499e+00
Fitted a model with MAP estimate = -226.6413
Time for alignment: 40.6026
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 270.9305 - loglik: -2.6770e+02 - logprior: -3.2270e+00
Epoch 2/10
19/19 - 1s - loss: 246.1087 - loglik: -2.4471e+02 - logprior: -1.4001e+00
Epoch 3/10
19/19 - 1s - loss: 237.9227 - loglik: -2.3640e+02 - logprior: -1.5204e+00
Epoch 4/10
19/19 - 1s - loss: 235.6984 - loglik: -2.3427e+02 - logprior: -1.4289e+00
Epoch 5/10
19/19 - 1s - loss: 235.0390 - loglik: -2.3364e+02 - logprior: -1.4005e+00
Epoch 6/10
19/19 - 1s - loss: 234.4939 - loglik: -2.3311e+02 - logprior: -1.3824e+00
Epoch 7/10
19/19 - 1s - loss: 234.1019 - loglik: -2.3273e+02 - logprior: -1.3634e+00
Epoch 8/10
19/19 - 1s - loss: 233.9482 - loglik: -2.3258e+02 - logprior: -1.3559e+00
Epoch 9/10
19/19 - 1s - loss: 233.6028 - loglik: -2.3224e+02 - logprior: -1.3514e+00
Epoch 10/10
19/19 - 1s - loss: 233.9658 - loglik: -2.3261e+02 - logprior: -1.3457e+00
Fitted a model with MAP estimate = -233.2347
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 240.5886 - loglik: -2.3644e+02 - logprior: -4.1477e+00
Epoch 2/2
19/19 - 1s - loss: 232.5417 - loglik: -2.3051e+02 - logprior: -2.0310e+00
Fitted a model with MAP estimate = -230.9134
expansions: [(0, 2)]
discards: [ 0  9 12 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 233.1470 - loglik: -2.3008e+02 - logprior: -3.0667e+00
Epoch 2/2
19/19 - 1s - loss: 230.0367 - loglik: -2.2877e+02 - logprior: -1.2641e+00
Fitted a model with MAP estimate = -229.4075
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 234.2401 - loglik: -2.3066e+02 - logprior: -3.5815e+00
Epoch 2/10
19/19 - 1s - loss: 230.3379 - loglik: -2.2895e+02 - logprior: -1.3872e+00
Epoch 3/10
19/19 - 1s - loss: 229.7191 - loglik: -2.2842e+02 - logprior: -1.2952e+00
Epoch 4/10
19/19 - 1s - loss: 229.1860 - loglik: -2.2794e+02 - logprior: -1.2459e+00
Epoch 5/10
19/19 - 1s - loss: 228.6897 - loglik: -2.2747e+02 - logprior: -1.2171e+00
Epoch 6/10
19/19 - 1s - loss: 228.1590 - loglik: -2.2695e+02 - logprior: -1.2006e+00
Epoch 7/10
19/19 - 1s - loss: 227.9625 - loglik: -2.2677e+02 - logprior: -1.1866e+00
Epoch 8/10
19/19 - 1s - loss: 227.1949 - loglik: -2.2601e+02 - logprior: -1.1769e+00
Epoch 9/10
19/19 - 1s - loss: 227.1273 - loglik: -2.2595e+02 - logprior: -1.1711e+00
Epoch 10/10
19/19 - 1s - loss: 227.1528 - loglik: -2.2599e+02 - logprior: -1.1556e+00
Fitted a model with MAP estimate = -226.6592
Time for alignment: 40.8764
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 270.9128 - loglik: -2.6769e+02 - logprior: -3.2258e+00
Epoch 2/10
19/19 - 1s - loss: 245.8652 - loglik: -2.4447e+02 - logprior: -1.3988e+00
Epoch 3/10
19/19 - 1s - loss: 237.6900 - loglik: -2.3634e+02 - logprior: -1.3473e+00
Epoch 4/10
19/19 - 1s - loss: 235.4516 - loglik: -2.3418e+02 - logprior: -1.2721e+00
Epoch 5/10
19/19 - 1s - loss: 234.7609 - loglik: -2.3352e+02 - logprior: -1.2395e+00
Epoch 6/10
19/19 - 1s - loss: 234.2881 - loglik: -2.3306e+02 - logprior: -1.2246e+00
Epoch 7/10
19/19 - 1s - loss: 233.5181 - loglik: -2.3230e+02 - logprior: -1.2080e+00
Epoch 8/10
19/19 - 1s - loss: 233.5434 - loglik: -2.3234e+02 - logprior: -1.1980e+00
Fitted a model with MAP estimate = -233.2571
expansions: [(0, 2), (7, 2), (8, 1), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 238.8171 - loglik: -2.3453e+02 - logprior: -4.2879e+00
Epoch 2/2
19/19 - 1s - loss: 230.6293 - loglik: -2.2926e+02 - logprior: -1.3713e+00
Fitted a model with MAP estimate = -229.4144
expansions: []
discards: [ 0 11 29 41]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 235.0834 - loglik: -2.3098e+02 - logprior: -4.0985e+00
Epoch 2/2
19/19 - 1s - loss: 230.7014 - loglik: -2.2916e+02 - logprior: -1.5425e+00
Fitted a model with MAP estimate = -229.8531
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 233.0845 - loglik: -2.2981e+02 - logprior: -3.2744e+00
Epoch 2/10
19/19 - 1s - loss: 230.0523 - loglik: -2.2865e+02 - logprior: -1.4036e+00
Epoch 3/10
19/19 - 1s - loss: 229.6590 - loglik: -2.2836e+02 - logprior: -1.3015e+00
Epoch 4/10
19/19 - 1s - loss: 229.2159 - loglik: -2.2797e+02 - logprior: -1.2407e+00
Epoch 5/10
19/19 - 1s - loss: 228.7315 - loglik: -2.2751e+02 - logprior: -1.2155e+00
Epoch 6/10
19/19 - 1s - loss: 228.0807 - loglik: -2.2688e+02 - logprior: -1.1940e+00
Epoch 7/10
19/19 - 1s - loss: 227.8493 - loglik: -2.2666e+02 - logprior: -1.1818e+00
Epoch 8/10
19/19 - 1s - loss: 227.2393 - loglik: -2.2606e+02 - logprior: -1.1689e+00
Epoch 9/10
19/19 - 1s - loss: 227.2954 - loglik: -2.2613e+02 - logprior: -1.1593e+00
Fitted a model with MAP estimate = -227.0790
Time for alignment: 38.0459
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 271.0152 - loglik: -2.6779e+02 - logprior: -3.2268e+00
Epoch 2/10
19/19 - 1s - loss: 245.8501 - loglik: -2.4444e+02 - logprior: -1.4136e+00
Epoch 3/10
19/19 - 1s - loss: 238.1773 - loglik: -2.3685e+02 - logprior: -1.3232e+00
Epoch 4/10
19/19 - 1s - loss: 235.8255 - loglik: -2.3457e+02 - logprior: -1.2529e+00
Epoch 5/10
19/19 - 1s - loss: 234.6263 - loglik: -2.3340e+02 - logprior: -1.2231e+00
Epoch 6/10
19/19 - 1s - loss: 234.2028 - loglik: -2.3299e+02 - logprior: -1.2051e+00
Epoch 7/10
19/19 - 1s - loss: 233.8326 - loglik: -2.3264e+02 - logprior: -1.1897e+00
Epoch 8/10
19/19 - 1s - loss: 233.7593 - loglik: -2.3257e+02 - logprior: -1.1796e+00
Epoch 9/10
19/19 - 1s - loss: 233.3821 - loglik: -2.3220e+02 - logprior: -1.1742e+00
Epoch 10/10
19/19 - 1s - loss: 233.3685 - loglik: -2.3219e+02 - logprior: -1.1686e+00
Fitted a model with MAP estimate = -232.8739
expansions: [(0, 2), (7, 2), (8, 1), (13, 1), (21, 1), (22, 2), (23, 2), (28, 2), (29, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 239.2944 - loglik: -2.3492e+02 - logprior: -4.3788e+00
Epoch 2/2
19/19 - 1s - loss: 230.7970 - loglik: -2.2940e+02 - logprior: -1.3925e+00
Fitted a model with MAP estimate = -229.4490
expansions: []
discards: [ 0 11 29 32 40 42]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 235.2943 - loglik: -2.3119e+02 - logprior: -4.1010e+00
Epoch 2/2
19/19 - 1s - loss: 230.6600 - loglik: -2.2913e+02 - logprior: -1.5347e+00
Fitted a model with MAP estimate = -229.9128
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 232.9409 - loglik: -2.2966e+02 - logprior: -3.2772e+00
Epoch 2/10
19/19 - 1s - loss: 230.0593 - loglik: -2.2865e+02 - logprior: -1.4061e+00
Epoch 3/10
19/19 - 1s - loss: 229.8353 - loglik: -2.2853e+02 - logprior: -1.3023e+00
Epoch 4/10
19/19 - 1s - loss: 229.1410 - loglik: -2.2789e+02 - logprior: -1.2457e+00
Epoch 5/10
19/19 - 1s - loss: 228.6484 - loglik: -2.2744e+02 - logprior: -1.2099e+00
Epoch 6/10
19/19 - 1s - loss: 228.0724 - loglik: -2.2687e+02 - logprior: -1.1972e+00
Epoch 7/10
19/19 - 1s - loss: 227.7358 - loglik: -2.2655e+02 - logprior: -1.1819e+00
Epoch 8/10
19/19 - 1s - loss: 227.6347 - loglik: -2.2646e+02 - logprior: -1.1706e+00
Epoch 9/10
19/19 - 1s - loss: 227.3249 - loglik: -2.2615e+02 - logprior: -1.1632e+00
Epoch 10/10
19/19 - 1s - loss: 226.9589 - loglik: -2.2580e+02 - logprior: -1.1525e+00
Fitted a model with MAP estimate = -226.6789
Time for alignment: 39.2511
Computed alignments with likelihoods: ['-226.6408', '-226.6413', '-226.6592', '-227.0790', '-226.6789']
Best model has likelihood: -226.6408  (prior= -1.1354 )
time for generating output: 0.0855
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00018.projection.fasta
SP score = 0.8082068239891779
Training of 5 independent models on file PF00687.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6e5ca1cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff66d6498e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6d4bb7790>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 1023.8720 - loglik: -1.0221e+03 - logprior: -1.8049e+00
Epoch 2/10
39/39 - 9s - loss: 926.3675 - loglik: -9.2471e+02 - logprior: -1.6612e+00
Epoch 3/10
39/39 - 9s - loss: 918.7557 - loglik: -9.1711e+02 - logprior: -1.6451e+00
Epoch 4/10
39/39 - 9s - loss: 916.9166 - loglik: -9.1533e+02 - logprior: -1.5871e+00
Epoch 5/10
39/39 - 9s - loss: 914.8497 - loglik: -9.1327e+02 - logprior: -1.5723e+00
Epoch 6/10
39/39 - 9s - loss: 913.2894 - loglik: -9.1171e+02 - logprior: -1.5755e+00
Epoch 7/10
39/39 - 9s - loss: 910.2169 - loglik: -9.0862e+02 - logprior: -1.5833e+00
Epoch 8/10
39/39 - 9s - loss: 908.5132 - loglik: -9.0692e+02 - logprior: -1.5754e+00
Epoch 9/10
39/39 - 10s - loss: 905.7633 - loglik: -9.0417e+02 - logprior: -1.5712e+00
Epoch 10/10
39/39 - 10s - loss: 904.0200 - loglik: -9.0249e+02 - logprior: -1.5079e+00
Fitted a model with MAP estimate = -902.7594
expansions: [(3, 3), (4, 2), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (39, 1), (40, 2), (41, 1), (42, 2), (58, 1), (59, 1), (61, 2), (63, 1), (66, 1), (75, 1), (98, 2), (99, 1), (100, 1), (102, 1), (103, 1), (108, 1), (110, 1), (112, 1), (116, 3), (127, 1), (129, 2), (130, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 198 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 899.4086 - loglik: -8.9757e+02 - logprior: -1.8358e+00
Epoch 2/2
39/39 - 14s - loss: 883.3448 - loglik: -8.8250e+02 - logprior: -8.4365e-01
Fitted a model with MAP estimate = -881.6345
expansions: []
discards: [  4  52  57  80 122 150]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 886.9183 - loglik: -8.8533e+02 - logprior: -1.5851e+00
Epoch 2/2
39/39 - 14s - loss: 883.2911 - loglik: -8.8279e+02 - logprior: -4.9858e-01
Fitted a model with MAP estimate = -882.2275
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 886.1450 - loglik: -8.8481e+02 - logprior: -1.3369e+00
Epoch 2/10
39/39 - 14s - loss: 883.0059 - loglik: -8.8264e+02 - logprior: -3.6704e-01
Epoch 3/10
39/39 - 14s - loss: 881.8908 - loglik: -8.8167e+02 - logprior: -2.1601e-01
Epoch 4/10
39/39 - 13s - loss: 880.7582 - loglik: -8.8064e+02 - logprior: -1.1722e-01
Epoch 5/10
39/39 - 13s - loss: 879.4427 - loglik: -8.7942e+02 - logprior: -1.5495e-02
Epoch 6/10
39/39 - 13s - loss: 877.1401 - loglik: -8.7719e+02 - logprior: 0.0620
Epoch 7/10
39/39 - 13s - loss: 873.0878 - loglik: -8.7323e+02 - logprior: 0.1503
Epoch 8/10
39/39 - 13s - loss: 870.0820 - loglik: -8.7034e+02 - logprior: 0.2754
Epoch 9/10
39/39 - 13s - loss: 866.3509 - loglik: -8.6670e+02 - logprior: 0.3719
Epoch 10/10
39/39 - 13s - loss: 865.3104 - loglik: -8.6577e+02 - logprior: 0.4807
Fitted a model with MAP estimate = -863.9872
Time for alignment: 361.6796
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 1025.9688 - loglik: -1.0242e+03 - logprior: -1.8177e+00
Epoch 2/10
39/39 - 9s - loss: 928.4335 - loglik: -9.2671e+02 - logprior: -1.7198e+00
Epoch 3/10
39/39 - 9s - loss: 920.8814 - loglik: -9.1919e+02 - logprior: -1.6887e+00
Epoch 4/10
39/39 - 10s - loss: 918.5981 - loglik: -9.1697e+02 - logprior: -1.6223e+00
Epoch 5/10
39/39 - 10s - loss: 916.4375 - loglik: -9.1483e+02 - logprior: -1.6058e+00
Epoch 6/10
39/39 - 10s - loss: 914.5928 - loglik: -9.1296e+02 - logprior: -1.6276e+00
Epoch 7/10
39/39 - 10s - loss: 912.2989 - loglik: -9.1065e+02 - logprior: -1.6352e+00
Epoch 8/10
39/39 - 10s - loss: 909.2332 - loglik: -9.0758e+02 - logprior: -1.6417e+00
Epoch 9/10
39/39 - 11s - loss: 906.5788 - loglik: -9.0492e+02 - logprior: -1.6404e+00
Epoch 10/10
39/39 - 10s - loss: 905.1695 - loglik: -9.0355e+02 - logprior: -1.5946e+00
Fitted a model with MAP estimate = -903.2169
expansions: [(4, 2), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (39, 1), (40, 2), (41, 1), (42, 2), (58, 1), (59, 1), (60, 1), (63, 2), (66, 2), (74, 2), (80, 1), (97, 2), (99, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 1), (120, 1), (127, 1), (129, 2), (130, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 199 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 899.4878 - loglik: -8.9764e+02 - logprior: -1.8445e+00
Epoch 2/2
39/39 - 16s - loss: 883.1447 - loglik: -8.8231e+02 - logprior: -8.3859e-01
Fitted a model with MAP estimate = -881.5702
expansions: []
discards: [  5  52  57  83  89  98 124]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 887.5220 - loglik: -8.8604e+02 - logprior: -1.4838e+00
Epoch 2/2
39/39 - 16s - loss: 883.9921 - loglik: -8.8346e+02 - logprior: -5.3539e-01
Fitted a model with MAP estimate = -882.6817
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 886.7222 - loglik: -8.8538e+02 - logprior: -1.3389e+00
Epoch 2/10
39/39 - 15s - loss: 882.7938 - loglik: -8.8243e+02 - logprior: -3.6014e-01
Epoch 3/10
39/39 - 15s - loss: 882.7759 - loglik: -8.8253e+02 - logprior: -2.4314e-01
Epoch 4/10
39/39 - 15s - loss: 880.9586 - loglik: -8.8083e+02 - logprior: -1.2687e-01
Epoch 5/10
39/39 - 16s - loss: 879.7681 - loglik: -8.7973e+02 - logprior: -3.1703e-02
Epoch 6/10
39/39 - 15s - loss: 877.2858 - loglik: -8.7733e+02 - logprior: 0.0508
Epoch 7/10
39/39 - 15s - loss: 873.1986 - loglik: -8.7333e+02 - logprior: 0.1428
Epoch 8/10
39/39 - 15s - loss: 869.8039 - loglik: -8.7001e+02 - logprior: 0.2250
Epoch 9/10
39/39 - 16s - loss: 866.7311 - loglik: -8.6703e+02 - logprior: 0.3203
Epoch 10/10
39/39 - 15s - loss: 864.7178 - loglik: -8.6510e+02 - logprior: 0.4039
Fitted a model with MAP estimate = -864.1583
Time for alignment: 395.3055
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 1026.1515 - loglik: -1.0243e+03 - logprior: -1.8121e+00
Epoch 2/10
39/39 - 11s - loss: 927.2195 - loglik: -9.2551e+02 - logprior: -1.7076e+00
Epoch 3/10
39/39 - 11s - loss: 919.7120 - loglik: -9.1802e+02 - logprior: -1.6926e+00
Epoch 4/10
39/39 - 11s - loss: 917.9827 - loglik: -9.1634e+02 - logprior: -1.6388e+00
Epoch 5/10
39/39 - 11s - loss: 915.9047 - loglik: -9.1427e+02 - logprior: -1.6305e+00
Epoch 6/10
39/39 - 11s - loss: 913.4422 - loglik: -9.1181e+02 - logprior: -1.6204e+00
Epoch 7/10
39/39 - 11s - loss: 910.1769 - loglik: -9.0854e+02 - logprior: -1.6228e+00
Epoch 8/10
39/39 - 11s - loss: 908.2537 - loglik: -9.0661e+02 - logprior: -1.6254e+00
Epoch 9/10
39/39 - 11s - loss: 905.0192 - loglik: -9.0340e+02 - logprior: -1.6054e+00
Epoch 10/10
39/39 - 11s - loss: 903.1097 - loglik: -9.0151e+02 - logprior: -1.5772e+00
Fitted a model with MAP estimate = -901.3595
expansions: [(4, 2), (5, 3), (9, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (39, 1), (40, 2), (41, 1), (42, 2), (58, 1), (59, 1), (61, 2), (63, 1), (66, 1), (74, 2), (93, 1), (97, 2), (99, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 1), (120, 1), (127, 1), (129, 2), (130, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 199 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 900.4030 - loglik: -8.9852e+02 - logprior: -1.8790e+00
Epoch 2/2
39/39 - 14s - loss: 883.3275 - loglik: -8.8248e+02 - logprior: -8.4713e-01
Fitted a model with MAP estimate = -881.5188
expansions: []
discards: [  5   7  53  58  81  98 124]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 887.2307 - loglik: -8.8567e+02 - logprior: -1.5572e+00
Epoch 2/2
39/39 - 13s - loss: 883.5417 - loglik: -8.8305e+02 - logprior: -4.8912e-01
Fitted a model with MAP estimate = -882.5550
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 886.4889 - loglik: -8.8517e+02 - logprior: -1.3231e+00
Epoch 2/10
39/39 - 13s - loss: 882.4892 - loglik: -8.8213e+02 - logprior: -3.5438e-01
Epoch 3/10
39/39 - 13s - loss: 882.5516 - loglik: -8.8234e+02 - logprior: -2.0940e-01
Fitted a model with MAP estimate = -881.2195
Time for alignment: 278.0206
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 1024.9854 - loglik: -1.0232e+03 - logprior: -1.8064e+00
Epoch 2/10
39/39 - 10s - loss: 927.3936 - loglik: -9.2568e+02 - logprior: -1.7132e+00
Epoch 3/10
39/39 - 10s - loss: 917.9659 - loglik: -9.1624e+02 - logprior: -1.7274e+00
Epoch 4/10
39/39 - 10s - loss: 915.2587 - loglik: -9.1358e+02 - logprior: -1.6779e+00
Epoch 5/10
39/39 - 11s - loss: 913.7045 - loglik: -9.1205e+02 - logprior: -1.6506e+00
Epoch 6/10
39/39 - 11s - loss: 911.5509 - loglik: -9.0989e+02 - logprior: -1.6481e+00
Epoch 7/10
39/39 - 11s - loss: 908.9270 - loglik: -9.0725e+02 - logprior: -1.6636e+00
Epoch 8/10
39/39 - 10s - loss: 906.8727 - loglik: -9.0520e+02 - logprior: -1.6584e+00
Epoch 9/10
39/39 - 11s - loss: 903.2109 - loglik: -9.0154e+02 - logprior: -1.6531e+00
Epoch 10/10
39/39 - 11s - loss: 901.7864 - loglik: -9.0019e+02 - logprior: -1.5806e+00
Fitted a model with MAP estimate = -900.8486
expansions: [(3, 3), (4, 2), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (39, 1), (40, 2), (41, 1), (42, 2), (58, 1), (59, 1), (61, 2), (63, 2), (66, 2), (75, 1), (98, 2), (99, 1), (100, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (117, 1), (123, 1), (127, 1), (130, 1), (131, 1), (132, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 200 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 899.6810 - loglik: -8.9787e+02 - logprior: -1.8079e+00
Epoch 2/2
39/39 - 16s - loss: 882.6016 - loglik: -8.8174e+02 - logprior: -8.5631e-01
Fitted a model with MAP estimate = -881.2183
expansions: []
discards: [  4  52  57  80  84  89 124 176]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 887.5669 - loglik: -8.8612e+02 - logprior: -1.4427e+00
Epoch 2/2
39/39 - 16s - loss: 883.4672 - loglik: -8.8298e+02 - logprior: -4.8912e-01
Fitted a model with MAP estimate = -882.2518
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 885.9427 - loglik: -8.8468e+02 - logprior: -1.2626e+00
Epoch 2/10
39/39 - 15s - loss: 883.1270 - loglik: -8.8290e+02 - logprior: -2.2225e-01
Epoch 3/10
39/39 - 16s - loss: 881.8911 - loglik: -8.8170e+02 - logprior: -1.8978e-01
Epoch 4/10
39/39 - 15s - loss: 880.9056 - loglik: -8.8082e+02 - logprior: -8.6938e-02
Epoch 5/10
39/39 - 15s - loss: 878.8478 - loglik: -8.7885e+02 - logprior: 0.0060
Epoch 6/10
39/39 - 16s - loss: 877.2964 - loglik: -8.7736e+02 - logprior: 0.0681
Epoch 7/10
39/39 - 16s - loss: 873.8447 - loglik: -8.7397e+02 - logprior: 0.1399
Epoch 8/10
39/39 - 16s - loss: 870.3970 - loglik: -8.7060e+02 - logprior: 0.2146
Epoch 9/10
39/39 - 16s - loss: 866.3953 - loglik: -8.6670e+02 - logprior: 0.3272
Epoch 10/10
39/39 - 16s - loss: 864.9998 - loglik: -8.6538e+02 - logprior: 0.4042
Fitted a model with MAP estimate = -864.1297
Time for alignment: 405.8315
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 1023.6588 - loglik: -1.0219e+03 - logprior: -1.7960e+00
Epoch 2/10
39/39 - 11s - loss: 923.9292 - loglik: -9.2217e+02 - logprior: -1.7561e+00
Epoch 3/10
39/39 - 11s - loss: 916.6542 - loglik: -9.1490e+02 - logprior: -1.7544e+00
Epoch 4/10
39/39 - 11s - loss: 915.7869 - loglik: -9.1411e+02 - logprior: -1.6753e+00
Epoch 5/10
39/39 - 10s - loss: 912.9672 - loglik: -9.1131e+02 - logprior: -1.6531e+00
Epoch 6/10
39/39 - 10s - loss: 911.8345 - loglik: -9.1017e+02 - logprior: -1.6526e+00
Epoch 7/10
39/39 - 10s - loss: 909.0443 - loglik: -9.0737e+02 - logprior: -1.6600e+00
Epoch 8/10
39/39 - 10s - loss: 906.6800 - loglik: -9.0500e+02 - logprior: -1.6627e+00
Epoch 9/10
39/39 - 10s - loss: 903.5876 - loglik: -9.0191e+02 - logprior: -1.6572e+00
Epoch 10/10
39/39 - 10s - loss: 901.7139 - loglik: -9.0007e+02 - logprior: -1.6272e+00
Fitted a model with MAP estimate = -900.7243
expansions: [(4, 2), (5, 2), (8, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (41, 2), (42, 1), (43, 2), (52, 1), (58, 1), (59, 1), (61, 2), (64, 1), (70, 1), (75, 1), (98, 1), (100, 3), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (117, 1), (123, 1), (127, 1), (130, 1), (131, 1), (132, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 198 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 899.7981 - loglik: -8.9797e+02 - logprior: -1.8278e+00
Epoch 2/2
39/39 - 14s - loss: 883.8030 - loglik: -8.8300e+02 - logprior: -8.0246e-01
Fitted a model with MAP estimate = -882.1330
expansions: []
discards: [  5  52  57  80 126 174]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 886.3622 - loglik: -8.8494e+02 - logprior: -1.4256e+00
Epoch 2/2
39/39 - 13s - loss: 882.9869 - loglik: -8.8253e+02 - logprior: -4.5317e-01
Fitted a model with MAP estimate = -881.7625
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 885.4287 - loglik: -8.8417e+02 - logprior: -1.2608e+00
Epoch 2/10
39/39 - 14s - loss: 881.9438 - loglik: -8.8174e+02 - logprior: -2.0187e-01
Epoch 3/10
39/39 - 14s - loss: 881.6439 - loglik: -8.8148e+02 - logprior: -1.5755e-01
Epoch 4/10
39/39 - 15s - loss: 879.9356 - loglik: -8.7985e+02 - logprior: -8.4335e-02
Epoch 5/10
39/39 - 15s - loss: 878.8386 - loglik: -8.7884e+02 - logprior: 0.0027
Epoch 6/10
39/39 - 15s - loss: 876.4884 - loglik: -8.7655e+02 - logprior: 0.0730
Epoch 7/10
39/39 - 16s - loss: 873.0275 - loglik: -8.7318e+02 - logprior: 0.1607
Epoch 8/10
39/39 - 16s - loss: 869.9324 - loglik: -8.7019e+02 - logprior: 0.2743
Epoch 9/10
39/39 - 17s - loss: 865.2084 - loglik: -8.6556e+02 - logprior: 0.3654
Epoch 10/10
39/39 - 16s - loss: 864.1315 - loglik: -8.6452e+02 - logprior: 0.4128
Fitted a model with MAP estimate = -863.1796
Time for alignment: 382.9574
Computed alignments with likelihoods: ['-863.9872', '-864.1583', '-881.2195', '-864.1297', '-863.1796']
Best model has likelihood: -863.1796  (prior= 0.4460 )
time for generating output: 0.1694
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00687.projection.fasta
SP score = 0.7515950659293917
Training of 5 independent models on file PF03129.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff67f838c10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff595aa8070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6884c0160>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 520.0430 - loglik: -5.1695e+02 - logprior: -3.0962e+00
Epoch 2/10
19/19 - 2s - loss: 488.2879 - loglik: -4.8702e+02 - logprior: -1.2680e+00
Epoch 3/10
19/19 - 2s - loss: 475.8350 - loglik: -4.7455e+02 - logprior: -1.2856e+00
Epoch 4/10
19/19 - 2s - loss: 471.6326 - loglik: -4.7050e+02 - logprior: -1.1312e+00
Epoch 5/10
19/19 - 2s - loss: 468.8339 - loglik: -4.6773e+02 - logprior: -1.0953e+00
Epoch 6/10
19/19 - 2s - loss: 467.4361 - loglik: -4.6637e+02 - logprior: -1.0553e+00
Epoch 7/10
19/19 - 2s - loss: 466.0609 - loglik: -4.6500e+02 - logprior: -1.0474e+00
Epoch 8/10
19/19 - 2s - loss: 464.4153 - loglik: -4.6336e+02 - logprior: -1.0398e+00
Epoch 9/10
19/19 - 2s - loss: 462.7316 - loglik: -4.6168e+02 - logprior: -1.0326e+00
Epoch 10/10
19/19 - 2s - loss: 461.5531 - loglik: -4.6050e+02 - logprior: -1.0309e+00
Fitted a model with MAP estimate = -459.3560
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (24, 1), (31, 1), (37, 2), (38, 2), (50, 2), (55, 1), (59, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 482.2630 - loglik: -4.7836e+02 - logprior: -3.9023e+00
Epoch 2/2
19/19 - 2s - loss: 463.7997 - loglik: -4.6193e+02 - logprior: -1.8714e+00
Fitted a model with MAP estimate = -460.0691
expansions: [(0, 2)]
discards: [ 0  7 46 62]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 462.2046 - loglik: -4.5936e+02 - logprior: -2.8407e+00
Epoch 2/2
19/19 - 2s - loss: 456.8404 - loglik: -4.5581e+02 - logprior: -1.0334e+00
Fitted a model with MAP estimate = -454.7644
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 460.3017 - loglik: -4.5689e+02 - logprior: -3.4142e+00
Epoch 2/10
19/19 - 2s - loss: 455.7813 - loglik: -4.5468e+02 - logprior: -1.1036e+00
Epoch 3/10
19/19 - 2s - loss: 454.7336 - loglik: -4.5378e+02 - logprior: -9.5656e-01
Epoch 4/10
19/19 - 2s - loss: 453.6908 - loglik: -4.5279e+02 - logprior: -8.9765e-01
Epoch 5/10
19/19 - 2s - loss: 452.6160 - loglik: -4.5175e+02 - logprior: -8.6541e-01
Epoch 6/10
19/19 - 2s - loss: 451.2874 - loglik: -4.5043e+02 - logprior: -8.4902e-01
Epoch 7/10
19/19 - 2s - loss: 449.4871 - loglik: -4.4864e+02 - logprior: -8.3598e-01
Epoch 8/10
19/19 - 2s - loss: 448.3820 - loglik: -4.4756e+02 - logprior: -8.1336e-01
Epoch 9/10
19/19 - 2s - loss: 446.7169 - loglik: -4.4590e+02 - logprior: -7.9847e-01
Epoch 10/10
19/19 - 2s - loss: 444.1623 - loglik: -4.4334e+02 - logprior: -7.9848e-01
Fitted a model with MAP estimate = -441.2276
Time for alignment: 77.0478
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 519.9884 - loglik: -5.1689e+02 - logprior: -3.0991e+00
Epoch 2/10
19/19 - 2s - loss: 488.8651 - loglik: -4.8759e+02 - logprior: -1.2703e+00
Epoch 3/10
19/19 - 2s - loss: 476.4812 - loglik: -4.7517e+02 - logprior: -1.3123e+00
Epoch 4/10
19/19 - 2s - loss: 469.1051 - loglik: -4.6780e+02 - logprior: -1.3065e+00
Epoch 5/10
19/19 - 2s - loss: 467.3719 - loglik: -4.6596e+02 - logprior: -1.4113e+00
Epoch 6/10
19/19 - 2s - loss: 465.1428 - loglik: -4.6376e+02 - logprior: -1.3809e+00
Epoch 7/10
19/19 - 2s - loss: 463.8416 - loglik: -4.6246e+02 - logprior: -1.3704e+00
Epoch 8/10
19/19 - 2s - loss: 461.9872 - loglik: -4.6062e+02 - logprior: -1.3573e+00
Epoch 9/10
19/19 - 2s - loss: 460.8621 - loglik: -4.5949e+02 - logprior: -1.3549e+00
Epoch 10/10
19/19 - 2s - loss: 458.4498 - loglik: -4.5707e+02 - logprior: -1.3610e+00
Fitted a model with MAP estimate = -455.6619
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (24, 1), (30, 1), (36, 1), (37, 2), (38, 1), (50, 1), (55, 1), (57, 1), (64, 1), (68, 1), (69, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 479.3578 - loglik: -4.7546e+02 - logprior: -3.8979e+00
Epoch 2/2
19/19 - 2s - loss: 460.5019 - loglik: -4.5869e+02 - logprior: -1.8091e+00
Fitted a model with MAP estimate = -457.0222
expansions: [(0, 2)]
discards: [ 0  7 46]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 459.9824 - loglik: -4.5715e+02 - logprior: -2.8288e+00
Epoch 2/2
19/19 - 2s - loss: 455.9786 - loglik: -4.5496e+02 - logprior: -1.0161e+00
Fitted a model with MAP estimate = -454.6130
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 460.2181 - loglik: -4.5680e+02 - logprior: -3.4211e+00
Epoch 2/10
19/19 - 2s - loss: 455.7084 - loglik: -4.5461e+02 - logprior: -1.1025e+00
Epoch 3/10
19/19 - 2s - loss: 454.8177 - loglik: -4.5387e+02 - logprior: -9.4894e-01
Epoch 4/10
19/19 - 2s - loss: 453.7999 - loglik: -4.5289e+02 - logprior: -9.0412e-01
Epoch 5/10
19/19 - 2s - loss: 452.6601 - loglik: -4.5179e+02 - logprior: -8.6430e-01
Epoch 6/10
19/19 - 2s - loss: 451.2968 - loglik: -4.5045e+02 - logprior: -8.4223e-01
Epoch 7/10
19/19 - 2s - loss: 449.1921 - loglik: -4.4836e+02 - logprior: -8.2383e-01
Epoch 8/10
19/19 - 2s - loss: 448.6514 - loglik: -4.4783e+02 - logprior: -8.0945e-01
Epoch 9/10
19/19 - 2s - loss: 446.8313 - loglik: -4.4602e+02 - logprior: -7.9819e-01
Epoch 10/10
19/19 - 2s - loss: 444.3033 - loglik: -4.4349e+02 - logprior: -7.8977e-01
Fitted a model with MAP estimate = -441.3309
Time for alignment: 73.9218
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 520.1682 - loglik: -5.1707e+02 - logprior: -3.1008e+00
Epoch 2/10
19/19 - 2s - loss: 486.9825 - loglik: -4.8571e+02 - logprior: -1.2696e+00
Epoch 3/10
19/19 - 2s - loss: 473.3786 - loglik: -4.7203e+02 - logprior: -1.3454e+00
Epoch 4/10
19/19 - 2s - loss: 468.6171 - loglik: -4.6730e+02 - logprior: -1.3125e+00
Epoch 5/10
19/19 - 2s - loss: 466.9528 - loglik: -4.6555e+02 - logprior: -1.3958e+00
Epoch 6/10
19/19 - 2s - loss: 465.6306 - loglik: -4.6427e+02 - logprior: -1.3543e+00
Epoch 7/10
19/19 - 2s - loss: 464.3145 - loglik: -4.6296e+02 - logprior: -1.3481e+00
Epoch 8/10
19/19 - 2s - loss: 462.9385 - loglik: -4.6160e+02 - logprior: -1.3276e+00
Epoch 9/10
19/19 - 2s - loss: 461.2679 - loglik: -4.5992e+02 - logprior: -1.3308e+00
Epoch 10/10
19/19 - 2s - loss: 459.1495 - loglik: -4.5780e+02 - logprior: -1.3326e+00
Fitted a model with MAP estimate = -456.6399
expansions: [(7, 1), (8, 2), (9, 4), (12, 1), (15, 1), (27, 1), (30, 1), (36, 1), (37, 2), (38, 1), (50, 1), (55, 1), (57, 1), (64, 1), (68, 1), (69, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 478.5232 - loglik: -4.7464e+02 - logprior: -3.8855e+00
Epoch 2/2
19/19 - 2s - loss: 459.8034 - loglik: -4.5800e+02 - logprior: -1.8052e+00
Fitted a model with MAP estimate = -456.6251
expansions: [(0, 2)]
discards: [ 0  8  9 12 48]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 459.8739 - loglik: -4.5705e+02 - logprior: -2.8245e+00
Epoch 2/2
19/19 - 2s - loss: 456.1255 - loglik: -4.5511e+02 - logprior: -1.0159e+00
Fitted a model with MAP estimate = -454.5718
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 460.1371 - loglik: -4.5673e+02 - logprior: -3.4089e+00
Epoch 2/10
19/19 - 2s - loss: 455.8141 - loglik: -4.5471e+02 - logprior: -1.1003e+00
Epoch 3/10
19/19 - 2s - loss: 454.6839 - loglik: -4.5374e+02 - logprior: -9.4810e-01
Epoch 4/10
19/19 - 2s - loss: 454.0955 - loglik: -4.5320e+02 - logprior: -8.9493e-01
Epoch 5/10
19/19 - 2s - loss: 452.2126 - loglik: -4.5134e+02 - logprior: -8.6740e-01
Epoch 6/10
19/19 - 2s - loss: 451.3012 - loglik: -4.5045e+02 - logprior: -8.4299e-01
Epoch 7/10
19/19 - 2s - loss: 449.0844 - loglik: -4.4825e+02 - logprior: -8.2556e-01
Epoch 8/10
19/19 - 2s - loss: 448.7459 - loglik: -4.4793e+02 - logprior: -8.0495e-01
Epoch 9/10
19/19 - 2s - loss: 446.8618 - loglik: -4.4605e+02 - logprior: -7.9723e-01
Epoch 10/10
19/19 - 2s - loss: 444.5238 - loglik: -4.4371e+02 - logprior: -7.9351e-01
Fitted a model with MAP estimate = -441.2913
Time for alignment: 76.3522
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 520.1027 - loglik: -5.1700e+02 - logprior: -3.1001e+00
Epoch 2/10
19/19 - 2s - loss: 486.8935 - loglik: -4.8561e+02 - logprior: -1.2847e+00
Epoch 3/10
19/19 - 2s - loss: 472.0373 - loglik: -4.7069e+02 - logprior: -1.3514e+00
Epoch 4/10
19/19 - 2s - loss: 468.5383 - loglik: -4.6729e+02 - logprior: -1.2468e+00
Epoch 5/10
19/19 - 2s - loss: 465.7229 - loglik: -4.6451e+02 - logprior: -1.2080e+00
Epoch 6/10
19/19 - 2s - loss: 463.4312 - loglik: -4.6231e+02 - logprior: -1.1142e+00
Epoch 7/10
19/19 - 2s - loss: 462.4781 - loglik: -4.6139e+02 - logprior: -1.0804e+00
Epoch 8/10
19/19 - 2s - loss: 460.4182 - loglik: -4.5934e+02 - logprior: -1.0686e+00
Epoch 9/10
19/19 - 2s - loss: 459.3313 - loglik: -4.5825e+02 - logprior: -1.0602e+00
Epoch 10/10
19/19 - 2s - loss: 456.9213 - loglik: -4.5584e+02 - logprior: -1.0639e+00
Fitted a model with MAP estimate = -454.3887
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (22, 1), (29, 1), (30, 2), (37, 2), (38, 1), (50, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 478.9423 - loglik: -4.7505e+02 - logprior: -3.8944e+00
Epoch 2/2
19/19 - 2s - loss: 460.1020 - loglik: -4.5822e+02 - logprior: -1.8779e+00
Fitted a model with MAP estimate = -456.9463
expansions: [(0, 2)]
discards: [ 0  7 38 48]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 459.6444 - loglik: -4.5681e+02 - logprior: -2.8336e+00
Epoch 2/2
19/19 - 2s - loss: 455.8448 - loglik: -4.5482e+02 - logprior: -1.0284e+00
Fitted a model with MAP estimate = -454.5657
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 460.1247 - loglik: -4.5671e+02 - logprior: -3.4187e+00
Epoch 2/10
19/19 - 2s - loss: 455.9287 - loglik: -4.5482e+02 - logprior: -1.1087e+00
Epoch 3/10
19/19 - 2s - loss: 454.6667 - loglik: -4.5371e+02 - logprior: -9.5898e-01
Epoch 4/10
19/19 - 2s - loss: 453.7878 - loglik: -4.5288e+02 - logprior: -9.0302e-01
Epoch 5/10
19/19 - 2s - loss: 452.2504 - loglik: -4.5138e+02 - logprior: -8.6998e-01
Epoch 6/10
19/19 - 2s - loss: 451.4276 - loglik: -4.5057e+02 - logprior: -8.5211e-01
Epoch 7/10
19/19 - 2s - loss: 449.5795 - loglik: -4.4874e+02 - logprior: -8.2994e-01
Epoch 8/10
19/19 - 2s - loss: 448.3685 - loglik: -4.4754e+02 - logprior: -8.1193e-01
Epoch 9/10
19/19 - 2s - loss: 446.9688 - loglik: -4.4615e+02 - logprior: -8.0523e-01
Epoch 10/10
19/19 - 2s - loss: 443.8640 - loglik: -4.4304e+02 - logprior: -7.9978e-01
Fitted a model with MAP estimate = -441.2281
Time for alignment: 76.1803
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 520.0460 - loglik: -5.1695e+02 - logprior: -3.0983e+00
Epoch 2/10
19/19 - 2s - loss: 487.5521 - loglik: -4.8631e+02 - logprior: -1.2384e+00
Epoch 3/10
19/19 - 2s - loss: 472.7071 - loglik: -4.7143e+02 - logprior: -1.2768e+00
Epoch 4/10
19/19 - 2s - loss: 467.7911 - loglik: -4.6665e+02 - logprior: -1.1388e+00
Epoch 5/10
19/19 - 2s - loss: 466.0720 - loglik: -4.6495e+02 - logprior: -1.1217e+00
Epoch 6/10
19/19 - 2s - loss: 464.4359 - loglik: -4.6335e+02 - logprior: -1.0784e+00
Epoch 7/10
19/19 - 2s - loss: 463.1590 - loglik: -4.6209e+02 - logprior: -1.0606e+00
Epoch 8/10
19/19 - 2s - loss: 461.4695 - loglik: -4.6041e+02 - logprior: -1.0485e+00
Epoch 9/10
19/19 - 2s - loss: 460.7796 - loglik: -4.5972e+02 - logprior: -1.0455e+00
Epoch 10/10
19/19 - 2s - loss: 458.2236 - loglik: -4.5716e+02 - logprior: -1.0468e+00
Fitted a model with MAP estimate = -455.3872
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (24, 2), (36, 1), (37, 2), (38, 1), (50, 1), (53, 1), (54, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 477.3010 - loglik: -4.7341e+02 - logprior: -3.8875e+00
Epoch 2/2
19/19 - 2s - loss: 460.0997 - loglik: -4.5827e+02 - logprior: -1.8332e+00
Fitted a model with MAP estimate = -456.9506
expansions: [(0, 2)]
discards: [ 0  7 46]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 459.9749 - loglik: -4.5714e+02 - logprior: -2.8305e+00
Epoch 2/2
19/19 - 2s - loss: 456.0228 - loglik: -4.5501e+02 - logprior: -1.0134e+00
Fitted a model with MAP estimate = -454.6842
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 460.2502 - loglik: -4.5684e+02 - logprior: -3.4103e+00
Epoch 2/10
19/19 - 2s - loss: 455.5454 - loglik: -4.5445e+02 - logprior: -1.0964e+00
Epoch 3/10
19/19 - 2s - loss: 454.9205 - loglik: -4.5397e+02 - logprior: -9.5039e-01
Epoch 4/10
19/19 - 2s - loss: 453.9071 - loglik: -4.5301e+02 - logprior: -8.9427e-01
Epoch 5/10
19/19 - 2s - loss: 452.5390 - loglik: -4.5166e+02 - logprior: -8.7531e-01
Epoch 6/10
19/19 - 2s - loss: 451.1599 - loglik: -4.5031e+02 - logprior: -8.4106e-01
Epoch 7/10
19/19 - 2s - loss: 449.4145 - loglik: -4.4858e+02 - logprior: -8.2701e-01
Epoch 8/10
19/19 - 2s - loss: 448.7931 - loglik: -4.4798e+02 - logprior: -8.0348e-01
Epoch 9/10
19/19 - 2s - loss: 446.3817 - loglik: -4.4557e+02 - logprior: -7.9459e-01
Epoch 10/10
19/19 - 2s - loss: 444.1222 - loglik: -4.4331e+02 - logprior: -7.9659e-01
Fitted a model with MAP estimate = -441.0057
Time for alignment: 71.1602
Computed alignments with likelihoods: ['-441.2276', '-441.3309', '-441.2913', '-441.2281', '-441.0057']
Best model has likelihood: -441.0057  (prior= -0.8417 )
time for generating output: 0.1344
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF03129.projection.fasta
SP score = 0.978899554205127
Training of 5 independent models on file PF13378.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff5b7b8f820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff5b7b83340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6f73d35e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 1185.2445 - loglik: -1.1835e+03 - logprior: -1.7317e+00
Epoch 2/10
39/39 - 11s - loss: 1108.5397 - loglik: -1.1074e+03 - logprior: -1.1699e+00
Epoch 3/10
39/39 - 11s - loss: 1098.4532 - loglik: -1.0973e+03 - logprior: -1.1453e+00
Epoch 4/10
39/39 - 11s - loss: 1094.8351 - loglik: -1.0937e+03 - logprior: -1.1007e+00
Epoch 5/10
39/39 - 11s - loss: 1088.9884 - loglik: -1.0879e+03 - logprior: -1.0845e+00
Epoch 6/10
39/39 - 11s - loss: 1077.5109 - loglik: -1.0764e+03 - logprior: -1.0868e+00
Epoch 7/10
39/39 - 11s - loss: 1057.9500 - loglik: -1.0568e+03 - logprior: -1.0808e+00
Epoch 8/10
39/39 - 11s - loss: 1027.2224 - loglik: -1.0260e+03 - logprior: -1.2175e+00
Epoch 9/10
39/39 - 11s - loss: 924.6490 - loglik: -9.2094e+02 - logprior: -3.6609e+00
Epoch 10/10
39/39 - 11s - loss: 866.9683 - loglik: -8.6250e+02 - logprior: -4.4022e+00
Fitted a model with MAP estimate = -859.9346
expansions: [(0, 2)]
discards: [ 13  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52
  53  54  55  56  57  58  59  60  61  64  65  66  67  68  69  70  71  72
  73  74  75  76  77  78  79  80  81  82  83  84 104 105 106 107 108 150
 151 152 153 154 155 156 157 162 163]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 1272.7664 - loglik: -1.2690e+03 - logprior: -3.7470e+00
Epoch 2/2
19/19 - 6s - loss: 1255.3430 - loglik: -1.2548e+03 - logprior: -5.5560e-01
Fitted a model with MAP estimate = -1247.4123
expansions: [(0, 114), (112, 98)]
discards: [  2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37
  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55
  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73
  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91
  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109
 110 111]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 1150.1381 - loglik: -1.1478e+03 - logprior: -2.3483e+00
Epoch 2/2
39/39 - 16s - loss: 1081.3256 - loglik: -1.0805e+03 - logprior: -7.7989e-01
Fitted a model with MAP estimate = -1072.4301
expansions: [(91, 1), (158, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1076.8829 - loglik: -1.0746e+03 - logprior: -2.2971e+00
Epoch 2/10
39/39 - 19s - loss: 1071.2019 - loglik: -1.0706e+03 - logprior: -5.5451e-01
Epoch 3/10
39/39 - 19s - loss: 1068.9395 - loglik: -1.0685e+03 - logprior: -4.4146e-01
Epoch 4/10
39/39 - 19s - loss: 1066.0120 - loglik: -1.0656e+03 - logprior: -3.7814e-01
Epoch 5/10
39/39 - 18s - loss: 1059.7461 - loglik: -1.0594e+03 - logprior: -3.5448e-01
Epoch 6/10
39/39 - 18s - loss: 1044.1393 - loglik: -1.0437e+03 - logprior: -3.8262e-01
Epoch 7/10
39/39 - 18s - loss: 1008.1466 - loglik: -1.0076e+03 - logprior: -5.0506e-01
Epoch 8/10
39/39 - 18s - loss: 940.6633 - loglik: -9.3983e+02 - logprior: -7.9420e-01
Epoch 9/10
39/39 - 18s - loss: 868.7736 - loglik: -8.6747e+02 - logprior: -1.2518e+00
Epoch 10/10
39/39 - 18s - loss: 852.8123 - loglik: -8.5137e+02 - logprior: -1.3840e+00
Fitted a model with MAP estimate = -849.8062
Time for alignment: 410.6580
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 1184.2883 - loglik: -1.1826e+03 - logprior: -1.7359e+00
Epoch 2/10
39/39 - 12s - loss: 1106.0295 - loglik: -1.1048e+03 - logprior: -1.1977e+00
Epoch 3/10
39/39 - 13s - loss: 1097.3928 - loglik: -1.0963e+03 - logprior: -1.1378e+00
Epoch 4/10
39/39 - 13s - loss: 1093.3547 - loglik: -1.0923e+03 - logprior: -1.0938e+00
Epoch 5/10
39/39 - 13s - loss: 1087.7921 - loglik: -1.0867e+03 - logprior: -1.0913e+00
Epoch 6/10
39/39 - 13s - loss: 1076.4958 - loglik: -1.0754e+03 - logprior: -1.1007e+00
Epoch 7/10
39/39 - 13s - loss: 1058.0868 - loglik: -1.0570e+03 - logprior: -1.0922e+00
Epoch 8/10
39/39 - 13s - loss: 1029.0455 - loglik: -1.0278e+03 - logprior: -1.2157e+00
Epoch 9/10
39/39 - 12s - loss: 928.3430 - loglik: -9.2474e+02 - logprior: -3.5522e+00
Epoch 10/10
39/39 - 12s - loss: 867.0262 - loglik: -8.6245e+02 - logprior: -4.5105e+00
Fitted a model with MAP estimate = -859.4126
expansions: [(0, 2)]
discards: [  9  10  11  12  13  35  36  37  38  39  40  41  42  43  44  45  46  47
  48  49  50  51  52  59  60  61  64  65  66  67  68  69  70  72  75  76
  77  78  79  82  83  84  85  86  87  88 104 105 106 107 150 151 152 153
 154 155 156 157 162 163 168]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 1271.6737 - loglik: -1.2679e+03 - logprior: -3.8016e+00
Epoch 2/2
19/19 - 5s - loss: 1255.5312 - loglik: -1.2548e+03 - logprior: -6.8833e-01
Fitted a model with MAP estimate = -1245.5870
expansions: [(0, 85), (114, 112)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18
  19  20  21  22  23  24  25  26  27  32  33  34  35  36  37  38  39  40
  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58
  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76
  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94
  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112
 113]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 1167.6387 - loglik: -1.1654e+03 - logprior: -2.2332e+00
Epoch 2/2
39/39 - 13s - loss: 1095.4186 - loglik: -1.0945e+03 - logprior: -9.1861e-01
Fitted a model with MAP estimate = -1082.8739
expansions: [(83, 2), (84, 1), (103, 5), (104, 4), (105, 1), (106, 2), (107, 1), (132, 1), (143, 1), (165, 2)]
discards: [ 0 86 87]
Re-initialized the encoder parameters.
Fitting a model of length 219 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 1080.3368 - loglik: -1.0779e+03 - logprior: -2.4045e+00
Epoch 2/10
39/39 - 15s - loss: 1071.2704 - loglik: -1.0706e+03 - logprior: -6.6858e-01
Epoch 3/10
39/39 - 15s - loss: 1069.3984 - loglik: -1.0688e+03 - logprior: -5.5087e-01
Epoch 4/10
39/39 - 15s - loss: 1065.5490 - loglik: -1.0651e+03 - logprior: -4.9287e-01
Epoch 5/10
39/39 - 16s - loss: 1059.5128 - loglik: -1.0590e+03 - logprior: -4.5993e-01
Epoch 6/10
39/39 - 16s - loss: 1045.3217 - loglik: -1.0448e+03 - logprior: -4.9899e-01
Epoch 7/10
39/39 - 17s - loss: 1009.0581 - loglik: -1.0084e+03 - logprior: -6.2006e-01
Epoch 8/10
39/39 - 18s - loss: 943.0383 - loglik: -9.4214e+02 - logprior: -8.5860e-01
Epoch 9/10
39/39 - 18s - loss: 868.5416 - loglik: -8.6724e+02 - logprior: -1.2456e+00
Epoch 10/10
39/39 - 18s - loss: 851.4262 - loglik: -8.5002e+02 - logprior: -1.3406e+00
Fitted a model with MAP estimate = -848.4131
Time for alignment: 400.9531
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 1184.7355 - loglik: -1.1830e+03 - logprior: -1.7356e+00
Epoch 2/10
39/39 - 13s - loss: 1107.4384 - loglik: -1.1062e+03 - logprior: -1.2503e+00
Epoch 3/10
39/39 - 13s - loss: 1099.5410 - loglik: -1.0984e+03 - logprior: -1.1642e+00
Epoch 4/10
39/39 - 13s - loss: 1095.6364 - loglik: -1.0945e+03 - logprior: -1.1018e+00
Epoch 5/10
39/39 - 13s - loss: 1089.6189 - loglik: -1.0885e+03 - logprior: -1.0824e+00
Epoch 6/10
39/39 - 13s - loss: 1077.6547 - loglik: -1.0766e+03 - logprior: -1.0739e+00
Epoch 7/10
39/39 - 13s - loss: 1055.7902 - loglik: -1.0547e+03 - logprior: -1.1082e+00
Epoch 8/10
39/39 - 13s - loss: 1017.3105 - loglik: -1.0159e+03 - logprior: -1.3265e+00
Epoch 9/10
39/39 - 12s - loss: 899.7277 - loglik: -8.9544e+02 - logprior: -4.2325e+00
Epoch 10/10
39/39 - 11s - loss: 865.3221 - loglik: -8.6096e+02 - logprior: -4.2924e+00
Fitted a model with MAP estimate = -859.7351
expansions: [(0, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  33  34  36  37  38  39  40  41  42
  43  44  45  56  59  60  61  62  65  66  67  72  73  74  75  76  77  78
  79  81  82  83  84  85  86  87  88 105 106 107 108 113 114 115 162 163
 164]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 1277.5011 - loglik: -1.2744e+03 - logprior: -3.1185e+00
Epoch 2/2
19/19 - 4s - loss: 1254.1040 - loglik: -1.2533e+03 - logprior: -8.1495e-01
Fitted a model with MAP estimate = -1244.4971
expansions: [(0, 75), (8, 16), (102, 108)]
discards: [  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26
  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44
  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62
  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80
  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98
  99 100 101]
Re-initialized the encoder parameters.
Fitting a model of length 208 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 1168.3861 - loglik: -1.1662e+03 - logprior: -2.2359e+00
Epoch 2/2
39/39 - 12s - loss: 1091.1549 - loglik: -1.0901e+03 - logprior: -1.0650e+00
Fitted a model with MAP estimate = -1079.5897
expansions: [(15, 1), (24, 2), (25, 1), (40, 1), (52, 1), (73, 1), (74, 1), (75, 2), (76, 2), (77, 1), (80, 1), (81, 2), (119, 1)]
discards: [  0  93 138 139 141 142 151]
Re-initialized the encoder parameters.
Fitting a model of length 218 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 1081.7416 - loglik: -1.0793e+03 - logprior: -2.4026e+00
Epoch 2/10
39/39 - 13s - loss: 1072.7500 - loglik: -1.0720e+03 - logprior: -7.1233e-01
Epoch 3/10
39/39 - 13s - loss: 1070.2896 - loglik: -1.0697e+03 - logprior: -5.5004e-01
Epoch 4/10
39/39 - 13s - loss: 1066.4796 - loglik: -1.0660e+03 - logprior: -4.9888e-01
Epoch 5/10
39/39 - 13s - loss: 1061.5887 - loglik: -1.0611e+03 - logprior: -4.8267e-01
Epoch 6/10
39/39 - 13s - loss: 1048.1676 - loglik: -1.0476e+03 - logprior: -5.0724e-01
Epoch 7/10
39/39 - 13s - loss: 1013.2886 - loglik: -1.0126e+03 - logprior: -6.1827e-01
Epoch 8/10
39/39 - 13s - loss: 947.8516 - loglik: -9.4699e+02 - logprior: -8.2449e-01
Epoch 9/10
39/39 - 14s - loss: 870.9538 - loglik: -8.6967e+02 - logprior: -1.2351e+00
Epoch 10/10
39/39 - 14s - loss: 852.8861 - loglik: -8.5145e+02 - logprior: -1.3708e+00
Fitted a model with MAP estimate = -848.9170
Time for alignment: 362.0429
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 1184.4097 - loglik: -1.1827e+03 - logprior: -1.7202e+00
Epoch 2/10
39/39 - 11s - loss: 1107.5178 - loglik: -1.1063e+03 - logprior: -1.1939e+00
Epoch 3/10
39/39 - 11s - loss: 1098.6405 - loglik: -1.0975e+03 - logprior: -1.1148e+00
Epoch 4/10
39/39 - 11s - loss: 1093.7510 - loglik: -1.0926e+03 - logprior: -1.1595e+00
Epoch 5/10
39/39 - 11s - loss: 1086.9321 - loglik: -1.0857e+03 - logprior: -1.1783e+00
Epoch 6/10
39/39 - 12s - loss: 1075.9465 - loglik: -1.0747e+03 - logprior: -1.1976e+00
Epoch 7/10
39/39 - 11s - loss: 1057.7341 - loglik: -1.0565e+03 - logprior: -1.2363e+00
Epoch 8/10
39/39 - 11s - loss: 1027.0078 - loglik: -1.0257e+03 - logprior: -1.3091e+00
Epoch 9/10
39/39 - 11s - loss: 926.8307 - loglik: -9.2340e+02 - logprior: -3.3788e+00
Epoch 10/10
39/39 - 11s - loss: 866.7039 - loglik: -8.6226e+02 - logprior: -4.3769e+00
Fitted a model with MAP estimate = -859.8381
expansions: [(0, 2)]
discards: [ 13  35  36  37  38  39  40  41  42  43  59  60  61  62  64  65  66  67
  68  69  70  72  73  74  75  76  77  78  79  82  83  84  85  86  87  88
 105 106 107 108 162 163]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 1274.1700 - loglik: -1.2708e+03 - logprior: -3.3637e+00
Epoch 2/2
19/19 - 7s - loss: 1255.2130 - loglik: -1.2545e+03 - logprior: -7.4981e-01
Fitted a model with MAP estimate = -1246.6834
expansions: [(0, 99), (133, 111)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18
  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36
  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54
  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90
  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108
 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126
 127 128 129 130 131 132]
Re-initialized the encoder parameters.
Fitting a model of length 211 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 1150.3551 - loglik: -1.1482e+03 - logprior: -2.1873e+00
Epoch 2/2
39/39 - 15s - loss: 1082.5083 - loglik: -1.0817e+03 - logprior: -7.7836e-01
Fitted a model with MAP estimate = -1072.9579
expansions: [(133, 1), (137, 1), (164, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 1076.9550 - loglik: -1.0746e+03 - logprior: -2.3345e+00
Epoch 2/10
39/39 - 15s - loss: 1072.0557 - loglik: -1.0715e+03 - logprior: -5.6236e-01
Epoch 3/10
39/39 - 16s - loss: 1069.5287 - loglik: -1.0691e+03 - logprior: -4.5835e-01
Epoch 4/10
39/39 - 16s - loss: 1066.4908 - loglik: -1.0661e+03 - logprior: -4.0518e-01
Epoch 5/10
39/39 - 16s - loss: 1059.5535 - loglik: -1.0592e+03 - logprior: -3.8977e-01
Epoch 6/10
39/39 - 17s - loss: 1043.9281 - loglik: -1.0435e+03 - logprior: -4.1265e-01
Epoch 7/10
39/39 - 18s - loss: 1006.4864 - loglik: -1.0059e+03 - logprior: -5.2470e-01
Epoch 8/10
39/39 - 18s - loss: 939.1520 - loglik: -9.3829e+02 - logprior: -8.1884e-01
Epoch 9/10
39/39 - 18s - loss: 868.0225 - loglik: -8.6662e+02 - logprior: -1.3505e+00
Epoch 10/10
39/39 - 17s - loss: 854.5765 - loglik: -8.5310e+02 - logprior: -1.4119e+00
Fitted a model with MAP estimate = -851.1103
Time for alignment: 396.9329
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 1185.5303 - loglik: -1.1838e+03 - logprior: -1.7266e+00
Epoch 2/10
39/39 - 12s - loss: 1107.6700 - loglik: -1.1064e+03 - logprior: -1.2257e+00
Epoch 3/10
39/39 - 13s - loss: 1099.2006 - loglik: -1.0981e+03 - logprior: -1.1192e+00
Epoch 4/10
39/39 - 12s - loss: 1094.7411 - loglik: -1.0937e+03 - logprior: -1.0820e+00
Epoch 5/10
39/39 - 13s - loss: 1088.8278 - loglik: -1.0878e+03 - logprior: -1.0453e+00
Epoch 6/10
39/39 - 13s - loss: 1076.8293 - loglik: -1.0758e+03 - logprior: -1.0322e+00
Epoch 7/10
39/39 - 13s - loss: 1055.9408 - loglik: -1.0549e+03 - logprior: -1.0216e+00
Epoch 8/10
39/39 - 13s - loss: 1019.0127 - loglik: -1.0177e+03 - logprior: -1.2489e+00
Epoch 9/10
39/39 - 13s - loss: 899.0673 - loglik: -8.9468e+02 - logprior: -4.3318e+00
Epoch 10/10
39/39 - 13s - loss: 864.9780 - loglik: -8.6084e+02 - logprior: -4.0743e+00
Fitted a model with MAP estimate = -859.2446
expansions: [(0, 2)]
discards: [ 11  12  13  14  15  16  17  18  19  20  21  35  36  37  38  39  40  41
  42  43  44  45  46  47  48  49  50  51  52  53  54  56  57  58  59  60
  72  73  74  75  76  77  78  79  81  82  83  84  85  86  87  88 105 106
 107 108 109 110 111 112 113 114 115 161 162 163]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 1280.1875 - loglik: -1.2759e+03 - logprior: -4.2412e+00
Epoch 2/2
19/19 - 5s - loss: 1255.5366 - loglik: -1.2547e+03 - logprior: -8.2127e-01
Fitted a model with MAP estimate = -1246.5891
expansions: [(0, 121), (109, 91)]
discards: [  2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37
  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55
  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73
  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91
  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 1151.2177 - loglik: -1.1490e+03 - logprior: -2.2488e+00
Epoch 2/2
39/39 - 15s - loss: 1084.6620 - loglik: -1.0838e+03 - logprior: -8.4572e-01
Fitted a model with MAP estimate = -1074.8415
expansions: [(26, 1), (59, 1), (80, 1), (88, 1), (99, 1), (100, 1), (102, 1)]
discards: [  0 153]
Re-initialized the encoder parameters.
Fitting a model of length 219 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1076.4099 - loglik: -1.0740e+03 - logprior: -2.3612e+00
Epoch 2/10
39/39 - 17s - loss: 1070.4734 - loglik: -1.0699e+03 - logprior: -5.8463e-01
Epoch 3/10
39/39 - 16s - loss: 1068.9713 - loglik: -1.0685e+03 - logprior: -4.8172e-01
Epoch 4/10
39/39 - 17s - loss: 1064.6111 - loglik: -1.0642e+03 - logprior: -4.1702e-01
Epoch 5/10
39/39 - 16s - loss: 1058.4652 - loglik: -1.0580e+03 - logprior: -4.0995e-01
Epoch 6/10
39/39 - 16s - loss: 1043.2054 - loglik: -1.0427e+03 - logprior: -4.4276e-01
Epoch 7/10
39/39 - 18s - loss: 1005.7691 - loglik: -1.0052e+03 - logprior: -5.3051e-01
Epoch 8/10
39/39 - 17s - loss: 938.5850 - loglik: -9.3776e+02 - logprior: -7.8282e-01
Epoch 9/10
39/39 - 18s - loss: 866.4476 - loglik: -8.6517e+02 - logprior: -1.2209e+00
Epoch 10/10
39/39 - 18s - loss: 851.7100 - loglik: -8.5032e+02 - logprior: -1.3329e+00
Fitted a model with MAP estimate = -848.1209
Time for alignment: 412.6553
Computed alignments with likelihoods: ['-849.8062', '-848.4131', '-848.9170', '-851.1103', '-848.1209']
Best model has likelihood: -848.1209  (prior= -1.3783 )
time for generating output: 0.3200
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13378.projection.fasta
SP score = 0.5232986531124965
Training of 5 independent models on file PF00084.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff699b27d30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6eef23ee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff66d04ba30>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 331.4320 - loglik: -3.2824e+02 - logprior: -3.1925e+00
Epoch 2/10
19/19 - 1s - loss: 301.0093 - loglik: -2.9962e+02 - logprior: -1.3940e+00
Epoch 3/10
19/19 - 1s - loss: 293.3181 - loglik: -2.9185e+02 - logprior: -1.4637e+00
Epoch 4/10
19/19 - 1s - loss: 291.6896 - loglik: -2.9038e+02 - logprior: -1.3048e+00
Epoch 5/10
19/19 - 1s - loss: 291.2138 - loglik: -2.8991e+02 - logprior: -1.3024e+00
Epoch 6/10
19/19 - 1s - loss: 290.5787 - loglik: -2.8930e+02 - logprior: -1.2756e+00
Epoch 7/10
19/19 - 1s - loss: 289.9071 - loglik: -2.8863e+02 - logprior: -1.2690e+00
Epoch 8/10
19/19 - 1s - loss: 289.4100 - loglik: -2.8814e+02 - logprior: -1.2617e+00
Epoch 9/10
19/19 - 1s - loss: 288.8910 - loglik: -2.8762e+02 - logprior: -1.2559e+00
Epoch 10/10
19/19 - 1s - loss: 286.8139 - loglik: -2.8554e+02 - logprior: -1.2597e+00
Fitted a model with MAP estimate = -285.4260
expansions: [(11, 2), (12, 3), (13, 3), (14, 2), (28, 2), (30, 2), (34, 3), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 303.8812 - loglik: -2.9984e+02 - logprior: -4.0404e+00
Epoch 2/2
19/19 - 1s - loss: 291.9258 - loglik: -2.8974e+02 - logprior: -2.1835e+00
Fitted a model with MAP estimate = -289.1142
expansions: []
discards: [13 37 42 52]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 292.7296 - loglik: -2.8932e+02 - logprior: -3.4059e+00
Epoch 2/2
19/19 - 1s - loss: 287.5581 - loglik: -2.8615e+02 - logprior: -1.4059e+00
Fitted a model with MAP estimate = -286.8964
expansions: []
discards: [15 45]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 292.0873 - loglik: -2.8888e+02 - logprior: -3.2039e+00
Epoch 2/10
19/19 - 1s - loss: 287.6052 - loglik: -2.8626e+02 - logprior: -1.3448e+00
Epoch 3/10
19/19 - 1s - loss: 286.9431 - loglik: -2.8569e+02 - logprior: -1.2479e+00
Epoch 4/10
19/19 - 1s - loss: 286.6044 - loglik: -2.8541e+02 - logprior: -1.1891e+00
Epoch 5/10
19/19 - 1s - loss: 286.2005 - loglik: -2.8504e+02 - logprior: -1.1559e+00
Epoch 6/10
19/19 - 1s - loss: 285.3513 - loglik: -2.8420e+02 - logprior: -1.1425e+00
Epoch 7/10
19/19 - 1s - loss: 285.2871 - loglik: -2.8415e+02 - logprior: -1.1281e+00
Epoch 8/10
19/19 - 1s - loss: 284.2319 - loglik: -2.8311e+02 - logprior: -1.1149e+00
Epoch 9/10
19/19 - 1s - loss: 283.7753 - loglik: -2.8265e+02 - logprior: -1.1102e+00
Epoch 10/10
19/19 - 1s - loss: 280.5310 - loglik: -2.7940e+02 - logprior: -1.1127e+00
Fitted a model with MAP estimate = -277.6301
Time for alignment: 46.9623
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 331.3813 - loglik: -3.2819e+02 - logprior: -3.1910e+00
Epoch 2/10
19/19 - 1s - loss: 301.1685 - loglik: -2.9976e+02 - logprior: -1.4054e+00
Epoch 3/10
19/19 - 1s - loss: 292.7417 - loglik: -2.9125e+02 - logprior: -1.4903e+00
Epoch 4/10
19/19 - 1s - loss: 291.5665 - loglik: -2.9023e+02 - logprior: -1.3318e+00
Epoch 5/10
19/19 - 1s - loss: 290.8125 - loglik: -2.8948e+02 - logprior: -1.3312e+00
Epoch 6/10
19/19 - 1s - loss: 290.1487 - loglik: -2.8884e+02 - logprior: -1.3037e+00
Epoch 7/10
19/19 - 1s - loss: 290.0367 - loglik: -2.8874e+02 - logprior: -1.2941e+00
Epoch 8/10
19/19 - 1s - loss: 289.2614 - loglik: -2.8797e+02 - logprior: -1.2862e+00
Epoch 9/10
19/19 - 1s - loss: 289.0353 - loglik: -2.8774e+02 - logprior: -1.2816e+00
Epoch 10/10
19/19 - 1s - loss: 287.8232 - loglik: -2.8653e+02 - logprior: -1.2817e+00
Fitted a model with MAP estimate = -286.7502
expansions: [(11, 1), (12, 3), (13, 3), (14, 2), (18, 1), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 301.8004 - loglik: -2.9777e+02 - logprior: -4.0352e+00
Epoch 2/2
19/19 - 1s - loss: 291.9171 - loglik: -2.8976e+02 - logprior: -2.1612e+00
Fitted a model with MAP estimate = -289.0070
expansions: []
discards: [12 13 16 40 41 50]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 293.2504 - loglik: -2.8991e+02 - logprior: -3.3359e+00
Epoch 2/2
19/19 - 1s - loss: 287.8939 - loglik: -2.8654e+02 - logprior: -1.3507e+00
Fitted a model with MAP estimate = -287.4410
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 292.0826 - loglik: -2.8889e+02 - logprior: -3.1964e+00
Epoch 2/10
19/19 - 1s - loss: 287.8043 - loglik: -2.8646e+02 - logprior: -1.3404e+00
Epoch 3/10
19/19 - 1s - loss: 287.2683 - loglik: -2.8603e+02 - logprior: -1.2395e+00
Epoch 4/10
19/19 - 1s - loss: 286.9417 - loglik: -2.8576e+02 - logprior: -1.1801e+00
Epoch 5/10
19/19 - 1s - loss: 286.4880 - loglik: -2.8533e+02 - logprior: -1.1514e+00
Epoch 6/10
19/19 - 1s - loss: 285.7741 - loglik: -2.8463e+02 - logprior: -1.1350e+00
Epoch 7/10
19/19 - 1s - loss: 285.5934 - loglik: -2.8447e+02 - logprior: -1.1133e+00
Epoch 8/10
19/19 - 1s - loss: 284.8530 - loglik: -2.8374e+02 - logprior: -1.1068e+00
Epoch 9/10
19/19 - 1s - loss: 283.9828 - loglik: -2.8287e+02 - logprior: -1.1043e+00
Epoch 10/10
19/19 - 1s - loss: 280.8931 - loglik: -2.7978e+02 - logprior: -1.0956e+00
Fitted a model with MAP estimate = -278.2945
Time for alignment: 46.6527
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 331.3481 - loglik: -3.2815e+02 - logprior: -3.1931e+00
Epoch 2/10
19/19 - 1s - loss: 300.9611 - loglik: -2.9956e+02 - logprior: -1.4056e+00
Epoch 3/10
19/19 - 1s - loss: 293.4620 - loglik: -2.9198e+02 - logprior: -1.4780e+00
Epoch 4/10
19/19 - 1s - loss: 291.5520 - loglik: -2.9023e+02 - logprior: -1.3221e+00
Epoch 5/10
19/19 - 1s - loss: 290.9321 - loglik: -2.8960e+02 - logprior: -1.3250e+00
Epoch 6/10
19/19 - 1s - loss: 290.2652 - loglik: -2.8896e+02 - logprior: -1.3039e+00
Epoch 7/10
19/19 - 1s - loss: 289.8744 - loglik: -2.8858e+02 - logprior: -1.2908e+00
Epoch 8/10
19/19 - 1s - loss: 289.1140 - loglik: -2.8782e+02 - logprior: -1.2878e+00
Epoch 9/10
19/19 - 1s - loss: 287.9999 - loglik: -2.8671e+02 - logprior: -1.2791e+00
Epoch 10/10
19/19 - 1s - loss: 286.7694 - loglik: -2.8547e+02 - logprior: -1.2842e+00
Fitted a model with MAP estimate = -285.1273
expansions: [(11, 2), (12, 3), (13, 3), (14, 2), (26, 2), (30, 2), (32, 1), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 303.6378 - loglik: -2.9960e+02 - logprior: -4.0327e+00
Epoch 2/2
19/19 - 1s - loss: 292.1522 - loglik: -2.8998e+02 - logprior: -2.1717e+00
Fitted a model with MAP estimate = -289.0616
expansions: []
discards: [13 36 42 51]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 292.6746 - loglik: -2.8930e+02 - logprior: -3.3795e+00
Epoch 2/2
19/19 - 1s - loss: 287.5851 - loglik: -2.8620e+02 - logprior: -1.3805e+00
Fitted a model with MAP estimate = -286.9208
expansions: []
discards: [15]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 291.9521 - loglik: -2.8875e+02 - logprior: -3.2025e+00
Epoch 2/10
19/19 - 1s - loss: 287.6063 - loglik: -2.8626e+02 - logprior: -1.3446e+00
Epoch 3/10
19/19 - 1s - loss: 287.0575 - loglik: -2.8581e+02 - logprior: -1.2444e+00
Epoch 4/10
19/19 - 1s - loss: 286.5572 - loglik: -2.8537e+02 - logprior: -1.1863e+00
Epoch 5/10
19/19 - 1s - loss: 286.1425 - loglik: -2.8498e+02 - logprior: -1.1570e+00
Epoch 6/10
19/19 - 1s - loss: 285.4532 - loglik: -2.8431e+02 - logprior: -1.1408e+00
Epoch 7/10
19/19 - 1s - loss: 285.2669 - loglik: -2.8414e+02 - logprior: -1.1229e+00
Epoch 8/10
19/19 - 1s - loss: 284.2637 - loglik: -2.8314e+02 - logprior: -1.1149e+00
Epoch 9/10
19/19 - 1s - loss: 283.7821 - loglik: -2.8266e+02 - logprior: -1.1068e+00
Epoch 10/10
19/19 - 1s - loss: 280.8490 - loglik: -2.7973e+02 - logprior: -1.1057e+00
Fitted a model with MAP estimate = -277.7764
Time for alignment: 47.4514
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 331.3737 - loglik: -3.2818e+02 - logprior: -3.1918e+00
Epoch 2/10
19/19 - 1s - loss: 300.9400 - loglik: -2.9954e+02 - logprior: -1.3979e+00
Epoch 3/10
19/19 - 1s - loss: 293.4149 - loglik: -2.9195e+02 - logprior: -1.4627e+00
Epoch 4/10
19/19 - 1s - loss: 291.4474 - loglik: -2.9014e+02 - logprior: -1.3061e+00
Epoch 5/10
19/19 - 1s - loss: 290.7914 - loglik: -2.8947e+02 - logprior: -1.3148e+00
Epoch 6/10
19/19 - 1s - loss: 290.0968 - loglik: -2.8880e+02 - logprior: -1.2920e+00
Epoch 7/10
19/19 - 1s - loss: 289.6978 - loglik: -2.8841e+02 - logprior: -1.2822e+00
Epoch 8/10
19/19 - 1s - loss: 289.2275 - loglik: -2.8794e+02 - logprior: -1.2788e+00
Epoch 9/10
19/19 - 1s - loss: 288.4464 - loglik: -2.8716e+02 - logprior: -1.2724e+00
Epoch 10/10
19/19 - 1s - loss: 287.5283 - loglik: -2.8624e+02 - logprior: -1.2742e+00
Fitted a model with MAP estimate = -286.0802
expansions: [(11, 1), (12, 3), (13, 3), (14, 2), (26, 2), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 302.6856 - loglik: -2.9864e+02 - logprior: -4.0455e+00
Epoch 2/2
19/19 - 1s - loss: 292.0396 - loglik: -2.8985e+02 - logprior: -2.1925e+00
Fitted a model with MAP estimate = -289.0208
expansions: []
discards: [15 35 41 42 51]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 293.4022 - loglik: -2.9004e+02 - logprior: -3.3615e+00
Epoch 2/2
19/19 - 1s - loss: 287.6534 - loglik: -2.8630e+02 - logprior: -1.3540e+00
Fitted a model with MAP estimate = -287.1163
expansions: []
discards: [13 14]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 292.1726 - loglik: -2.8897e+02 - logprior: -3.1979e+00
Epoch 2/10
19/19 - 1s - loss: 287.8719 - loglik: -2.8653e+02 - logprior: -1.3387e+00
Epoch 3/10
19/19 - 1s - loss: 287.0096 - loglik: -2.8577e+02 - logprior: -1.2355e+00
Epoch 4/10
19/19 - 1s - loss: 286.9831 - loglik: -2.8580e+02 - logprior: -1.1822e+00
Epoch 5/10
19/19 - 1s - loss: 286.3176 - loglik: -2.8517e+02 - logprior: -1.1447e+00
Epoch 6/10
19/19 - 1s - loss: 285.8280 - loglik: -2.8469e+02 - logprior: -1.1292e+00
Epoch 7/10
19/19 - 1s - loss: 285.0163 - loglik: -2.8389e+02 - logprior: -1.1221e+00
Epoch 8/10
19/19 - 1s - loss: 284.8984 - loglik: -2.8379e+02 - logprior: -1.1022e+00
Epoch 9/10
19/19 - 1s - loss: 283.8115 - loglik: -2.8269e+02 - logprior: -1.1055e+00
Epoch 10/10
19/19 - 1s - loss: 281.2153 - loglik: -2.8010e+02 - logprior: -1.0970e+00
Fitted a model with MAP estimate = -278.5393
Time for alignment: 45.4008
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 331.1874 - loglik: -3.2800e+02 - logprior: -3.1915e+00
Epoch 2/10
19/19 - 1s - loss: 300.9046 - loglik: -2.9951e+02 - logprior: -1.3961e+00
Epoch 3/10
19/19 - 1s - loss: 293.0154 - loglik: -2.9154e+02 - logprior: -1.4745e+00
Epoch 4/10
19/19 - 1s - loss: 291.6394 - loglik: -2.9032e+02 - logprior: -1.3210e+00
Epoch 5/10
19/19 - 1s - loss: 291.1410 - loglik: -2.8981e+02 - logprior: -1.3258e+00
Epoch 6/10
19/19 - 1s - loss: 290.2844 - loglik: -2.8898e+02 - logprior: -1.2985e+00
Epoch 7/10
19/19 - 1s - loss: 289.8708 - loglik: -2.8857e+02 - logprior: -1.2908e+00
Epoch 8/10
19/19 - 1s - loss: 289.6183 - loglik: -2.8833e+02 - logprior: -1.2834e+00
Epoch 9/10
19/19 - 1s - loss: 288.7375 - loglik: -2.8745e+02 - logprior: -1.2773e+00
Epoch 10/10
19/19 - 1s - loss: 287.7997 - loglik: -2.8651e+02 - logprior: -1.2756e+00
Fitted a model with MAP estimate = -286.8271
expansions: [(11, 2), (12, 3), (13, 3), (14, 2), (16, 1), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 302.0843 - loglik: -2.9805e+02 - logprior: -4.0324e+00
Epoch 2/2
19/19 - 1s - loss: 291.7140 - loglik: -2.8954e+02 - logprior: -2.1733e+00
Fitted a model with MAP estimate = -289.0114
expansions: []
discards: [13 14 17 41 42 51]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 293.3946 - loglik: -2.9004e+02 - logprior: -3.3570e+00
Epoch 2/2
19/19 - 1s - loss: 288.0089 - loglik: -2.8664e+02 - logprior: -1.3692e+00
Fitted a model with MAP estimate = -287.3400
expansions: []
discards: [13]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 292.1689 - loglik: -2.8897e+02 - logprior: -3.1987e+00
Epoch 2/10
19/19 - 1s - loss: 287.8680 - loglik: -2.8653e+02 - logprior: -1.3380e+00
Epoch 3/10
19/19 - 1s - loss: 287.2883 - loglik: -2.8605e+02 - logprior: -1.2397e+00
Epoch 4/10
19/19 - 1s - loss: 286.8529 - loglik: -2.8567e+02 - logprior: -1.1831e+00
Epoch 5/10
19/19 - 1s - loss: 286.5365 - loglik: -2.8539e+02 - logprior: -1.1422e+00
Epoch 6/10
19/19 - 1s - loss: 285.9228 - loglik: -2.8478e+02 - logprior: -1.1368e+00
Epoch 7/10
19/19 - 1s - loss: 285.3623 - loglik: -2.8424e+02 - logprior: -1.1173e+00
Epoch 8/10
19/19 - 1s - loss: 284.8939 - loglik: -2.8378e+02 - logprior: -1.1092e+00
Epoch 9/10
19/19 - 1s - loss: 283.7029 - loglik: -2.8259e+02 - logprior: -1.0981e+00
Epoch 10/10
19/19 - 1s - loss: 281.0988 - loglik: -2.7998e+02 - logprior: -1.0996e+00
Fitted a model with MAP estimate = -278.2636
Time for alignment: 46.4267
Computed alignments with likelihoods: ['-277.6301', '-278.2945', '-277.7764', '-278.5393', '-278.2636']
Best model has likelihood: -277.6301  (prior= -1.1007 )
time for generating output: 0.1004
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00084.projection.fasta
SP score = 0.8373493975903614
Training of 5 independent models on file PF00232.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6eeef5b20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff7186278b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff57d03e130>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 67s - loss: 1946.5991 - loglik: -1.9451e+03 - logprior: -1.4706e+00
Epoch 2/10
39/39 - 74s - loss: 1767.0563 - loglik: -1.7657e+03 - logprior: -1.3390e+00
Epoch 3/10
39/39 - 83s - loss: 1752.5953 - loglik: -1.7511e+03 - logprior: -1.5088e+00
Epoch 4/10
39/39 - 80s - loss: 1748.7734 - loglik: -1.7473e+03 - logprior: -1.4872e+00
Epoch 5/10
39/39 - 81s - loss: 1744.0510 - loglik: -1.7426e+03 - logprior: -1.4711e+00
Epoch 6/10
39/39 - 74s - loss: 1743.1410 - loglik: -1.7416e+03 - logprior: -1.4841e+00
Epoch 7/10
39/39 - 71s - loss: 1741.7782 - loglik: -1.7403e+03 - logprior: -1.4972e+00
Epoch 8/10
39/39 - 77s - loss: 1739.7384 - loglik: -1.7382e+03 - logprior: -1.5303e+00
Epoch 9/10
39/39 - 79s - loss: 1739.3925 - loglik: -1.7374e+03 - logprior: -1.9300e+00
Epoch 10/10
39/39 - 82s - loss: 1739.5087 - loglik: -1.7378e+03 - logprior: -1.6682e+00
Fitted a model with MAP estimate = -1736.3052
expansions: [(0, 3), (38, 1), (42, 1), (131, 1), (143, 1), (144, 1), (145, 1), (161, 1), (162, 1), (163, 1), (173, 8), (174, 1), (175, 1), (176, 1), (184, 1), (186, 1), (187, 1), (188, 6), (189, 1), (192, 1), (193, 1), (194, 1), (195, 1), (197, 1), (198, 2), (199, 2), (201, 1), (205, 1), (208, 1), (212, 1), (215, 1), (220, 2), (221, 5), (222, 1), (225, 1), (226, 3), (227, 1), (228, 1), (229, 2), (230, 1), (241, 1), (243, 1), (244, 1), (245, 3), (246, 2), (247, 2), (248, 7), (249, 1), (250, 1), (252, 1), (265, 1), (267, 1), (268, 1), (284, 1), (286, 1), (287, 2), (289, 1), (290, 1), (302, 1), (303, 1), (304, 1), (316, 1), (326, 1), (329, 1), (341, 1), (348, 1), (350, 1), (351, 1), (352, 1)]
discards: [  2 127]
Re-initialized the encoder parameters.
Fitting a model of length 457 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 121s - loss: 1734.9631 - loglik: -1.7330e+03 - logprior: -1.9658e+00
Epoch 2/2
39/39 - 101s - loss: 1708.5787 - loglik: -1.7081e+03 - logprior: -4.9724e-01
Fitted a model with MAP estimate = -1704.8270
expansions: [(215, 1), (454, 1)]
discards: [  2 186 187 237 267 289 315 316 317 318 319 320 327 328]
Re-initialized the encoder parameters.
Fitting a model of length 445 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 91s - loss: 1713.3021 - loglik: -1.7120e+03 - logprior: -1.3072e+00
Epoch 2/2
39/39 - 91s - loss: 1708.8257 - loglik: -1.7085e+03 - logprior: -3.4288e-01
Fitted a model with MAP estimate = -1705.4664
expansions: [(307, 2), (308, 2), (309, 1), (311, 3), (312, 1)]
discards: [  2 212]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 88s - loss: 1711.1675 - loglik: -1.7101e+03 - logprior: -1.0280e+00
Epoch 2/10
39/39 - 80s - loss: 1707.3818 - loglik: -1.7074e+03 - logprior: 0.0099
Epoch 3/10
39/39 - 79s - loss: 1703.4757 - loglik: -1.7040e+03 - logprior: 0.5303
Epoch 4/10
39/39 - 77s - loss: 1702.8159 - loglik: -1.7035e+03 - logprior: 0.6954
Epoch 5/10
39/39 - 74s - loss: 1699.1356 - loglik: -1.6997e+03 - logprior: 0.5669
Epoch 6/10
39/39 - 73s - loss: 1697.5765 - loglik: -1.6982e+03 - logprior: 0.5830
Epoch 7/10
39/39 - 73s - loss: 1695.9285 - loglik: -1.6968e+03 - logprior: 0.9093
Epoch 8/10
39/39 - 73s - loss: 1688.9781 - loglik: -1.6898e+03 - logprior: 0.7933
Epoch 9/10
39/39 - 73s - loss: 1690.8201 - loglik: -1.6917e+03 - logprior: 0.9483
Fitted a model with MAP estimate = -1684.3615
Time for alignment: 2298.4612
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 54s - loss: 1949.2803 - loglik: -1.9479e+03 - logprior: -1.4295e+00
Epoch 2/10
39/39 - 51s - loss: 1769.7240 - loglik: -1.7686e+03 - logprior: -1.1478e+00
Epoch 3/10
39/39 - 51s - loss: 1755.3802 - loglik: -1.7541e+03 - logprior: -1.3006e+00
Epoch 4/10
39/39 - 52s - loss: 1750.1219 - loglik: -1.7489e+03 - logprior: -1.2118e+00
Epoch 5/10
39/39 - 52s - loss: 1746.8618 - loglik: -1.7456e+03 - logprior: -1.2164e+00
Epoch 6/10
39/39 - 54s - loss: 1744.7928 - loglik: -1.7436e+03 - logprior: -1.2112e+00
Epoch 7/10
39/39 - 55s - loss: 1746.5209 - loglik: -1.7451e+03 - logprior: -1.3927e+00
Fitted a model with MAP estimate = -1742.2163
expansions: [(0, 3), (43, 1), (134, 1), (147, 1), (163, 1), (164, 1), (165, 1), (168, 1), (169, 1), (173, 7), (174, 2), (175, 1), (176, 1), (184, 1), (186, 1), (187, 1), (188, 4), (189, 1), (190, 1), (195, 1), (196, 1), (197, 1), (199, 1), (200, 2), (201, 2), (203, 1), (207, 1), (210, 1), (217, 3), (221, 2), (222, 5), (223, 2), (226, 1), (227, 2), (228, 5), (229, 1), (239, 1), (243, 1), (244, 2), (245, 3), (247, 2), (248, 5), (250, 1), (252, 1), (253, 1), (255, 1), (268, 1), (270, 1), (284, 1), (285, 1), (287, 2), (288, 1), (289, 3), (291, 1), (304, 1), (312, 1), (326, 1), (329, 1), (341, 1), (351, 2), (352, 2)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 455 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 92s - loss: 1732.0901 - loglik: -1.7302e+03 - logprior: -1.9176e+00
Epoch 2/2
39/39 - 102s - loss: 1708.3341 - loglik: -1.7078e+03 - logprior: -5.6366e-01
Fitted a model with MAP estimate = -1705.4650
expansions: [(186, 5), (213, 1), (283, 1), (285, 1), (374, 1), (452, 1)]
discards: [  2   3 187 188 189 214 237 267 274 277 278 279 313 314 315 316]
Re-initialized the encoder parameters.
Fitting a model of length 449 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 107s - loss: 1715.5641 - loglik: -1.7142e+03 - logprior: -1.3377e+00
Epoch 2/2
39/39 - 105s - loss: 1709.3446 - loglik: -1.7092e+03 - logprior: -1.3966e-01
Fitted a model with MAP estimate = -1706.3020
expansions: [(186, 4), (215, 1), (274, 4), (306, 2)]
discards: [188 189 190 191 192]
Re-initialized the encoder parameters.
Fitting a model of length 455 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 132s - loss: 1710.9227 - loglik: -1.7098e+03 - logprior: -1.1656e+00
Epoch 2/10
39/39 - 130s - loss: 1704.6237 - loglik: -1.7045e+03 - logprior: -1.0186e-01
Epoch 3/10
39/39 - 115s - loss: 1701.8600 - loglik: -1.7019e+03 - logprior: 0.0409
Epoch 4/10
39/39 - 108s - loss: 1700.4666 - loglik: -1.7007e+03 - logprior: 0.2103
Epoch 5/10
39/39 - 106s - loss: 1695.5681 - loglik: -1.6960e+03 - logprior: 0.4367
Epoch 6/10
39/39 - 130s - loss: 1694.2850 - loglik: -1.6947e+03 - logprior: 0.4668
Epoch 7/10
39/39 - 134s - loss: 1693.0677 - loglik: -1.6935e+03 - logprior: 0.4267
Epoch 8/10
39/39 - 134s - loss: 1690.6346 - loglik: -1.6914e+03 - logprior: 0.7859
Epoch 9/10
39/39 - 116s - loss: 1687.7452 - loglik: -1.6885e+03 - logprior: 0.8188
Epoch 10/10
39/39 - 107s - loss: 1679.6318 - loglik: -1.6804e+03 - logprior: 0.7569
Fitted a model with MAP estimate = -1668.4373
Time for alignment: 2486.1817
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 69s - loss: 1944.9609 - loglik: -1.9435e+03 - logprior: -1.4826e+00
Epoch 2/10
39/39 - 71s - loss: 1768.3931 - loglik: -1.7671e+03 - logprior: -1.2656e+00
Epoch 3/10
39/39 - 82s - loss: 1755.7698 - loglik: -1.7544e+03 - logprior: -1.3924e+00
Epoch 4/10
39/39 - 85s - loss: 1751.0052 - loglik: -1.7497e+03 - logprior: -1.3271e+00
Epoch 5/10
39/39 - 85s - loss: 1748.6125 - loglik: -1.7473e+03 - logprior: -1.3230e+00
Epoch 6/10
39/39 - 86s - loss: 1743.9607 - loglik: -1.7426e+03 - logprior: -1.3380e+00
Epoch 7/10
39/39 - 76s - loss: 1746.2966 - loglik: -1.7448e+03 - logprior: -1.4716e+00
Fitted a model with MAP estimate = -1742.1797
expansions: [(0, 3), (38, 1), (106, 1), (134, 1), (144, 1), (164, 1), (165, 1), (169, 1), (175, 8), (176, 1), (177, 1), (178, 1), (186, 1), (188, 2), (189, 5), (190, 1), (191, 1), (194, 1), (195, 3), (196, 2), (197, 1), (198, 1), (200, 1), (201, 1), (204, 1), (206, 1), (208, 1), (212, 1), (215, 1), (220, 2), (221, 5), (222, 3), (224, 1), (226, 2), (227, 2), (228, 3), (229, 1), (239, 1), (241, 1), (242, 2), (243, 3), (244, 2), (246, 2), (247, 6), (249, 1), (251, 1), (252, 1), (265, 1), (267, 1), (268, 1), (269, 1), (284, 1), (286, 1), (287, 2), (289, 1), (300, 1), (302, 1), (303, 1), (304, 1), (306, 1), (325, 1), (328, 2), (340, 1), (350, 1), (351, 1), (352, 2)]
discards: [  2 127]
Re-initialized the encoder parameters.
Fitting a model of length 460 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 114s - loss: 1730.4165 - loglik: -1.7286e+03 - logprior: -1.7697e+00
Epoch 2/2
39/39 - 112s - loss: 1706.9066 - loglik: -1.7064e+03 - logprior: -4.7717e-01
Fitted a model with MAP estimate = -1704.1002
expansions: [(216, 1), (286, 1)]
discards: [  2 186 187 212 213 266 287 307 308 316 317 318 319 320]
Re-initialized the encoder parameters.
Fitting a model of length 448 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 120s - loss: 1713.9557 - loglik: -1.7124e+03 - logprior: -1.5123e+00
Epoch 2/2
39/39 - 109s - loss: 1709.1096 - loglik: -1.7089e+03 - logprior: -2.3354e-01
Fitted a model with MAP estimate = -1705.8994
expansions: [(305, 2), (306, 1)]
discards: [269 270 271 272]
Re-initialized the encoder parameters.
Fitting a model of length 447 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 123s - loss: 1713.8932 - loglik: -1.7127e+03 - logprior: -1.1845e+00
Epoch 2/10
39/39 - 100s - loss: 1709.3718 - loglik: -1.7093e+03 - logprior: -1.0549e-01
Epoch 3/10
39/39 - 89s - loss: 1708.8440 - loglik: -1.7088e+03 - logprior: 7.9583e-04
Epoch 4/10
39/39 - 86s - loss: 1703.5056 - loglik: -1.7037e+03 - logprior: 0.2409
Epoch 5/10
39/39 - 85s - loss: 1701.2488 - loglik: -1.7015e+03 - logprior: 0.2200
Epoch 6/10
39/39 - 85s - loss: 1699.8463 - loglik: -1.7002e+03 - logprior: 0.3451
Epoch 7/10
39/39 - 85s - loss: 1696.0981 - loglik: -1.6967e+03 - logprior: 0.6336
Epoch 8/10
39/39 - 86s - loss: 1694.5104 - loglik: -1.6952e+03 - logprior: 0.6739
Epoch 9/10
39/39 - 85s - loss: 1691.8201 - loglik: -1.6926e+03 - logprior: 0.7902
Epoch 10/10
39/39 - 84s - loss: 1684.5601 - loglik: -1.6854e+03 - logprior: 0.8395
Fitted a model with MAP estimate = -1670.7159
Time for alignment: 2462.9432
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 1952.2316 - loglik: -1.9508e+03 - logprior: -1.4251e+00
Epoch 2/10
39/39 - 56s - loss: 1768.4288 - loglik: -1.7673e+03 - logprior: -1.0846e+00
Epoch 3/10
39/39 - 58s - loss: 1754.2062 - loglik: -1.7531e+03 - logprior: -1.0705e+00
Epoch 4/10
39/39 - 58s - loss: 1751.1785 - loglik: -1.7502e+03 - logprior: -9.3233e-01
Epoch 5/10
39/39 - 59s - loss: 1746.6785 - loglik: -1.7458e+03 - logprior: -9.0514e-01
Epoch 6/10
39/39 - 59s - loss: 1744.9026 - loglik: -1.7439e+03 - logprior: -9.5863e-01
Epoch 7/10
39/39 - 60s - loss: 1743.9913 - loglik: -1.7429e+03 - logprior: -1.0361e+00
Epoch 8/10
39/39 - 56s - loss: 1741.6162 - loglik: -1.7405e+03 - logprior: -1.0684e+00
Epoch 9/10
39/39 - 52s - loss: 1738.8468 - loglik: -1.7377e+03 - logprior: -1.0944e+00
Epoch 10/10
39/39 - 50s - loss: 1740.1907 - loglik: -1.7391e+03 - logprior: -1.1026e+00
Fitted a model with MAP estimate = -1737.2955
expansions: [(0, 3), (43, 1), (123, 1), (134, 1), (147, 1), (164, 1), (165, 1), (177, 1), (178, 8), (179, 1), (189, 1), (191, 2), (192, 7), (193, 1), (196, 1), (197, 3), (198, 1), (200, 1), (201, 2), (202, 2), (203, 1), (206, 1), (208, 1), (210, 1), (214, 1), (217, 1), (222, 2), (223, 5), (224, 3), (226, 1), (228, 1), (229, 2), (230, 3), (242, 1), (243, 1), (245, 1), (246, 4), (247, 2), (248, 2), (249, 6), (250, 1), (251, 1), (253, 1), (269, 1), (271, 1), (287, 2), (288, 1), (289, 2), (291, 1), (292, 1), (304, 2), (305, 2), (307, 2), (312, 1), (326, 1), (329, 1), (341, 1), (352, 1), (354, 2), (355, 3)]
discards: [  2 127]
Re-initialized the encoder parameters.
Fitting a model of length 461 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 94s - loss: 1740.2771 - loglik: -1.7382e+03 - logprior: -2.0324e+00
Epoch 2/2
39/39 - 100s - loss: 1708.7899 - loglik: -1.7082e+03 - logprior: -5.7748e-01
Fitted a model with MAP estimate = -1705.3456
expansions: [(217, 1), (287, 1)]
discards: [  2   3 187 188 189 190 213 238 268 276 277 278 279 280 288 315 316 317
 318 402 459]
Re-initialized the encoder parameters.
Fitting a model of length 442 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 98s - loss: 1719.1204 - loglik: -1.7177e+03 - logprior: -1.3803e+00
Epoch 2/2
39/39 - 98s - loss: 1712.6019 - loglik: -1.7125e+03 - logprior: -9.4969e-02
Fitted a model with MAP estimate = -1709.5235
expansions: [(267, 4), (268, 1), (271, 1), (299, 2), (300, 2), (302, 1)]
discards: [207]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 104s - loss: 1712.3268 - loglik: -1.7112e+03 - logprior: -1.0806e+00
Epoch 2/10
39/39 - 104s - loss: 1707.7981 - loglik: -1.7080e+03 - logprior: 0.1733
Epoch 3/10
39/39 - 94s - loss: 1706.1993 - loglik: -1.7065e+03 - logprior: 0.3017
Epoch 4/10
39/39 - 89s - loss: 1702.7291 - loglik: -1.7031e+03 - logprior: 0.3969
Epoch 5/10
39/39 - 86s - loss: 1700.8987 - loglik: -1.7014e+03 - logprior: 0.4823
Epoch 6/10
39/39 - 78s - loss: 1697.5553 - loglik: -1.6983e+03 - logprior: 0.7123
Epoch 7/10
39/39 - 75s - loss: 1695.1368 - loglik: -1.6960e+03 - logprior: 0.8932
Epoch 8/10
39/39 - 75s - loss: 1694.5837 - loglik: -1.6955e+03 - logprior: 0.9315
Epoch 9/10
39/39 - 75s - loss: 1688.1002 - loglik: -1.6890e+03 - logprior: 0.9408
Epoch 10/10
39/39 - 77s - loss: 1678.0609 - loglik: -1.6790e+03 - logprior: 0.9441
Fitted a model with MAP estimate = -1659.9679
Time for alignment: 2254.0456
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 55s - loss: 1946.5391 - loglik: -1.9451e+03 - logprior: -1.4392e+00
Epoch 2/10
39/39 - 52s - loss: 1767.6498 - loglik: -1.7664e+03 - logprior: -1.2175e+00
Epoch 3/10
39/39 - 54s - loss: 1754.9341 - loglik: -1.7536e+03 - logprior: -1.3005e+00
Epoch 4/10
39/39 - 58s - loss: 1751.3824 - loglik: -1.7501e+03 - logprior: -1.3233e+00
Epoch 5/10
39/39 - 61s - loss: 1747.2682 - loglik: -1.7457e+03 - logprior: -1.5237e+00
Epoch 6/10
39/39 - 66s - loss: 1748.6075 - loglik: -1.7472e+03 - logprior: -1.4060e+00
Fitted a model with MAP estimate = -1744.6816
expansions: [(0, 3), (38, 1), (135, 1), (144, 1), (165, 1), (166, 1), (177, 1), (178, 9), (179, 1), (180, 1), (191, 2), (192, 5), (193, 1), (194, 1), (196, 2), (197, 2), (198, 1), (199, 2), (200, 1), (201, 2), (202, 2), (204, 1), (206, 1), (208, 1), (210, 1), (214, 1), (217, 1), (222, 2), (223, 7), (226, 1), (227, 2), (228, 3), (229, 3), (239, 1), (240, 1), (242, 1), (243, 2), (244, 2), (245, 2), (247, 1), (248, 6), (250, 1), (252, 1), (253, 1), (266, 1), (268, 1), (270, 1), (283, 1), (285, 1), (286, 3), (288, 2), (291, 1), (300, 1), (303, 1), (315, 1), (325, 1), (328, 1), (348, 1), (351, 1), (352, 2)]
discards: [  2 128]
Re-initialized the encoder parameters.
Fitting a model of length 457 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 108s - loss: 1730.6006 - loglik: -1.7286e+03 - logprior: -1.9588e+00
Epoch 2/2
39/39 - 105s - loss: 1708.3812 - loglik: -1.7076e+03 - logprior: -7.4563e-01
Fitted a model with MAP estimate = -1705.2571
expansions: [(216, 1), (283, 1), (324, 2), (454, 1)]
discards: [  2 187 188 189 190 213 214 227 239 269 277 278 279 280 290 310 317 318
 319 320 321 325 326 327]
Re-initialized the encoder parameters.
Fitting a model of length 438 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 98s - loss: 1718.8138 - loglik: -1.7174e+03 - logprior: -1.4396e+00
Epoch 2/2
39/39 - 84s - loss: 1713.2438 - loglik: -1.7130e+03 - logprior: -2.2811e-01
Fitted a model with MAP estimate = -1710.5745
expansions: [(267, 4), (268, 1), (300, 2), (355, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 445 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 79s - loss: 1713.5228 - loglik: -1.7123e+03 - logprior: -1.2095e+00
Epoch 2/10
39/39 - 74s - loss: 1709.5391 - loglik: -1.7097e+03 - logprior: 0.1548
Epoch 3/10
39/39 - 74s - loss: 1707.0497 - loglik: -1.7074e+03 - logprior: 0.3610
Epoch 4/10
39/39 - 83s - loss: 1703.2899 - loglik: -1.7036e+03 - logprior: 0.3024
Epoch 5/10
39/39 - 87s - loss: 1704.5128 - loglik: -1.7050e+03 - logprior: 0.5394
Fitted a model with MAP estimate = -1699.9499
Time for alignment: 1566.4310
Computed alignments with likelihoods: ['-1684.3615', '-1668.4373', '-1670.7159', '-1659.9679', '-1699.9499']
Best model has likelihood: -1659.9679  (prior= 0.6138 )
time for generating output: 0.4023
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00232.projection.fasta
SP score = 0.8623414199866459
Training of 5 independent models on file PF13522.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6d4914b80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff57ccc02b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff58d7d9f10>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 840.4743 - loglik: -8.3757e+02 - logprior: -2.9037e+00
Epoch 2/10
19/19 - 4s - loss: 756.9775 - loglik: -7.5605e+02 - logprior: -9.2586e-01
Epoch 3/10
19/19 - 4s - loss: 730.7844 - loglik: -7.2975e+02 - logprior: -1.0293e+00
Epoch 4/10
19/19 - 4s - loss: 725.6793 - loglik: -7.2467e+02 - logprior: -1.0046e+00
Epoch 5/10
19/19 - 4s - loss: 723.0751 - loglik: -7.2210e+02 - logprior: -9.7644e-01
Epoch 6/10
19/19 - 4s - loss: 721.3152 - loglik: -7.2036e+02 - logprior: -9.4506e-01
Epoch 7/10
19/19 - 4s - loss: 720.9094 - loglik: -7.1997e+02 - logprior: -9.3337e-01
Epoch 8/10
19/19 - 4s - loss: 718.6550 - loglik: -7.1773e+02 - logprior: -9.1969e-01
Epoch 9/10
19/19 - 4s - loss: 718.9800 - loglik: -7.1805e+02 - logprior: -9.2361e-01
Fitted a model with MAP estimate = -717.3579
expansions: [(0, 7), (7, 3), (8, 2), (10, 2), (34, 2), (35, 1), (37, 1), (49, 1), (53, 1), (55, 1), (58, 1), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (96, 2), (97, 1), (99, 1), (105, 1), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 726.6141 - loglik: -7.2271e+02 - logprior: -3.9067e+00
Epoch 2/2
19/19 - 6s - loss: 711.6169 - loglik: -7.1056e+02 - logprior: -1.0518e+00
Fitted a model with MAP estimate = -708.8137
expansions: [(0, 4)]
discards: [  1   2   3   4   6  22  49 100 101]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 717.3844 - loglik: -7.1353e+02 - logprior: -3.8546e+00
Epoch 2/2
19/19 - 5s - loss: 711.5540 - loglik: -7.1039e+02 - logprior: -1.1600e+00
Fitted a model with MAP estimate = -709.7184
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 714.3666 - loglik: -7.1125e+02 - logprior: -3.1136e+00
Epoch 2/10
19/19 - 5s - loss: 710.0927 - loglik: -7.0921e+02 - logprior: -8.8131e-01
Epoch 3/10
19/19 - 5s - loss: 708.3699 - loglik: -7.0769e+02 - logprior: -6.8177e-01
Epoch 4/10
19/19 - 6s - loss: 707.3621 - loglik: -7.0676e+02 - logprior: -5.9701e-01
Epoch 5/10
19/19 - 5s - loss: 706.0419 - loglik: -7.0551e+02 - logprior: -5.2697e-01
Epoch 6/10
19/19 - 6s - loss: 705.6660 - loglik: -7.0518e+02 - logprior: -4.7716e-01
Epoch 7/10
19/19 - 5s - loss: 703.6265 - loglik: -7.0317e+02 - logprior: -4.4946e-01
Epoch 8/10
19/19 - 5s - loss: 702.9479 - loglik: -7.0251e+02 - logprior: -4.2459e-01
Epoch 9/10
19/19 - 6s - loss: 701.3804 - loglik: -7.0098e+02 - logprior: -3.9115e-01
Epoch 10/10
19/19 - 6s - loss: 700.6924 - loglik: -7.0029e+02 - logprior: -3.8710e-01
Fitted a model with MAP estimate = -699.5870
Time for alignment: 156.5836
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 840.6669 - loglik: -8.3777e+02 - logprior: -2.8965e+00
Epoch 2/10
19/19 - 4s - loss: 756.0295 - loglik: -7.5509e+02 - logprior: -9.3703e-01
Epoch 3/10
19/19 - 4s - loss: 729.7856 - loglik: -7.2874e+02 - logprior: -1.0476e+00
Epoch 4/10
19/19 - 4s - loss: 724.4680 - loglik: -7.2348e+02 - logprior: -9.8696e-01
Epoch 5/10
19/19 - 4s - loss: 722.6825 - loglik: -7.2171e+02 - logprior: -9.6947e-01
Epoch 6/10
19/19 - 4s - loss: 720.8929 - loglik: -7.1995e+02 - logprior: -9.3322e-01
Epoch 7/10
19/19 - 4s - loss: 719.4913 - loglik: -7.1857e+02 - logprior: -9.1182e-01
Epoch 8/10
19/19 - 4s - loss: 718.1650 - loglik: -7.1725e+02 - logprior: -9.0514e-01
Epoch 9/10
19/19 - 4s - loss: 717.1111 - loglik: -7.1621e+02 - logprior: -8.8793e-01
Epoch 10/10
19/19 - 4s - loss: 716.9059 - loglik: -7.1601e+02 - logprior: -8.8587e-01
Fitted a model with MAP estimate = -715.3480
expansions: [(0, 7), (7, 2), (8, 2), (9, 2), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (53, 1), (55, 1), (58, 1), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (96, 2), (97, 2), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 159 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 727.2661 - loglik: -7.2339e+02 - logprior: -3.8770e+00
Epoch 2/2
19/19 - 6s - loss: 711.0339 - loglik: -7.0998e+02 - logprior: -1.0580e+00
Fitted a model with MAP estimate = -707.6981
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  19  23 100 101 129 148]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 715.9449 - loglik: -7.1220e+02 - logprior: -3.7495e+00
Epoch 2/2
19/19 - 5s - loss: 709.6703 - loglik: -7.0866e+02 - logprior: -1.0051e+00
Fitted a model with MAP estimate = -707.9091
expansions: [(0, 5)]
discards: [0 1 2 4]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 712.8917 - loglik: -7.0989e+02 - logprior: -2.9994e+00
Epoch 2/10
19/19 - 5s - loss: 708.2056 - loglik: -7.0737e+02 - logprior: -8.3771e-01
Epoch 3/10
19/19 - 6s - loss: 707.1071 - loglik: -7.0644e+02 - logprior: -6.7098e-01
Epoch 4/10
19/19 - 6s - loss: 705.6852 - loglik: -7.0514e+02 - logprior: -5.4831e-01
Epoch 5/10
19/19 - 6s - loss: 705.2426 - loglik: -7.0479e+02 - logprior: -4.5339e-01
Epoch 6/10
19/19 - 6s - loss: 703.6187 - loglik: -7.0322e+02 - logprior: -3.9163e-01
Epoch 7/10
19/19 - 6s - loss: 701.5280 - loglik: -7.0116e+02 - logprior: -3.5889e-01
Epoch 8/10
19/19 - 6s - loss: 701.3098 - loglik: -7.0097e+02 - logprior: -3.3435e-01
Epoch 9/10
19/19 - 6s - loss: 700.2546 - loglik: -6.9993e+02 - logprior: -3.1154e-01
Epoch 10/10
19/19 - 6s - loss: 698.7863 - loglik: -6.9848e+02 - logprior: -2.9089e-01
Fitted a model with MAP estimate = -697.9393
Time for alignment: 165.0983
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 840.5330 - loglik: -8.3763e+02 - logprior: -2.9032e+00
Epoch 2/10
19/19 - 4s - loss: 757.3655 - loglik: -7.5643e+02 - logprior: -9.3671e-01
Epoch 3/10
19/19 - 4s - loss: 728.0387 - loglik: -7.2700e+02 - logprior: -1.0363e+00
Epoch 4/10
19/19 - 4s - loss: 722.7550 - loglik: -7.2177e+02 - logprior: -9.8762e-01
Epoch 5/10
19/19 - 4s - loss: 720.1937 - loglik: -7.1922e+02 - logprior: -9.6683e-01
Epoch 6/10
19/19 - 4s - loss: 718.0620 - loglik: -7.1711e+02 - logprior: -9.5145e-01
Epoch 7/10
19/19 - 4s - loss: 717.3284 - loglik: -7.1639e+02 - logprior: -9.3095e-01
Epoch 8/10
19/19 - 4s - loss: 716.2812 - loglik: -7.1536e+02 - logprior: -9.1650e-01
Epoch 9/10
19/19 - 4s - loss: 715.6450 - loglik: -7.1472e+02 - logprior: -9.1042e-01
Epoch 10/10
19/19 - 5s - loss: 714.5933 - loglik: -7.1367e+02 - logprior: -9.1391e-01
Fitted a model with MAP estimate = -713.2837
expansions: [(0, 7), (7, 3), (8, 2), (10, 2), (28, 1), (35, 1), (37, 1), (49, 1), (52, 1), (54, 2), (58, 1), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (97, 1), (98, 1), (100, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 158 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 726.2069 - loglik: -7.2232e+02 - logprior: -3.8832e+00
Epoch 2/2
19/19 - 6s - loss: 709.7028 - loglik: -7.0869e+02 - logprior: -1.0082e+00
Fitted a model with MAP estimate = -706.9620
expansions: [(0, 4)]
discards: [  1   2   3   4   6  22  73 100 101 147]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 715.4823 - loglik: -7.1168e+02 - logprior: -3.7997e+00
Epoch 2/2
19/19 - 6s - loss: 709.7870 - loglik: -7.0869e+02 - logprior: -1.0938e+00
Fitted a model with MAP estimate = -707.7829
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 712.6047 - loglik: -7.0956e+02 - logprior: -3.0403e+00
Epoch 2/10
19/19 - 5s - loss: 707.4731 - loglik: -7.0668e+02 - logprior: -7.8974e-01
Epoch 3/10
19/19 - 6s - loss: 706.7126 - loglik: -7.0612e+02 - logprior: -5.8907e-01
Epoch 4/10
19/19 - 6s - loss: 705.8409 - loglik: -7.0533e+02 - logprior: -5.1231e-01
Epoch 5/10
19/19 - 6s - loss: 704.1484 - loglik: -7.0370e+02 - logprior: -4.4447e-01
Epoch 6/10
19/19 - 6s - loss: 702.7756 - loglik: -7.0238e+02 - logprior: -3.9436e-01
Epoch 7/10
19/19 - 6s - loss: 702.5195 - loglik: -7.0216e+02 - logprior: -3.5773e-01
Epoch 8/10
19/19 - 6s - loss: 700.1447 - loglik: -6.9979e+02 - logprior: -3.4687e-01
Epoch 9/10
19/19 - 6s - loss: 700.0983 - loglik: -6.9978e+02 - logprior: -3.1267e-01
Epoch 10/10
19/19 - 6s - loss: 698.2225 - loglik: -6.9791e+02 - logprior: -2.9788e-01
Fitted a model with MAP estimate = -697.5443
Time for alignment: 171.2609
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 839.8104 - loglik: -8.3690e+02 - logprior: -2.9075e+00
Epoch 2/10
19/19 - 4s - loss: 756.5176 - loglik: -7.5558e+02 - logprior: -9.3842e-01
Epoch 3/10
19/19 - 4s - loss: 728.3421 - loglik: -7.2731e+02 - logprior: -1.0303e+00
Epoch 4/10
19/19 - 4s - loss: 722.7951 - loglik: -7.2179e+02 - logprior: -1.0013e+00
Epoch 5/10
19/19 - 4s - loss: 719.4084 - loglik: -7.1843e+02 - logprior: -9.7163e-01
Epoch 6/10
19/19 - 4s - loss: 718.6289 - loglik: -7.1768e+02 - logprior: -9.4169e-01
Epoch 7/10
19/19 - 4s - loss: 717.5184 - loglik: -7.1659e+02 - logprior: -9.2161e-01
Epoch 8/10
19/19 - 4s - loss: 715.8604 - loglik: -7.1495e+02 - logprior: -9.0150e-01
Epoch 9/10
19/19 - 4s - loss: 716.5906 - loglik: -7.1568e+02 - logprior: -8.9799e-01
Fitted a model with MAP estimate = -714.2169
expansions: [(0, 7), (7, 3), (8, 2), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (55, 2), (56, 2), (60, 1), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (97, 1), (98, 1), (100, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 159 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 725.4720 - loglik: -7.2162e+02 - logprior: -3.8478e+00
Epoch 2/2
19/19 - 6s - loss: 710.2196 - loglik: -7.0919e+02 - logprior: -1.0287e+00
Fitted a model with MAP estimate = -706.9255
expansions: [(0, 4)]
discards: [  1   2   3   4   6  22  74  76 101 102 148]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 715.2022 - loglik: -7.1142e+02 - logprior: -3.7823e+00
Epoch 2/2
19/19 - 5s - loss: 709.5765 - loglik: -7.0850e+02 - logprior: -1.0813e+00
Fitted a model with MAP estimate = -707.6346
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 712.4507 - loglik: -7.0941e+02 - logprior: -3.0368e+00
Epoch 2/10
19/19 - 5s - loss: 707.2639 - loglik: -7.0646e+02 - logprior: -8.0242e-01
Epoch 3/10
19/19 - 5s - loss: 706.8004 - loglik: -7.0620e+02 - logprior: -5.9806e-01
Epoch 4/10
19/19 - 5s - loss: 705.9334 - loglik: -7.0540e+02 - logprior: -5.2902e-01
Epoch 5/10
19/19 - 6s - loss: 703.6144 - loglik: -7.0315e+02 - logprior: -4.6112e-01
Epoch 6/10
19/19 - 6s - loss: 702.5656 - loglik: -7.0215e+02 - logprior: -4.1144e-01
Epoch 7/10
19/19 - 5s - loss: 702.7880 - loglik: -7.0241e+02 - logprior: -3.6732e-01
Fitted a model with MAP estimate = -700.8879
Time for alignment: 140.6486
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 840.3442 - loglik: -8.3745e+02 - logprior: -2.8964e+00
Epoch 2/10
19/19 - 4s - loss: 757.6297 - loglik: -7.5670e+02 - logprior: -9.3035e-01
Epoch 3/10
19/19 - 4s - loss: 730.4219 - loglik: -7.2940e+02 - logprior: -1.0206e+00
Epoch 4/10
19/19 - 4s - loss: 725.1063 - loglik: -7.2413e+02 - logprior: -9.7253e-01
Epoch 5/10
19/19 - 4s - loss: 721.9110 - loglik: -7.2095e+02 - logprior: -9.5277e-01
Epoch 6/10
19/19 - 4s - loss: 720.0295 - loglik: -7.1909e+02 - logprior: -9.3172e-01
Epoch 7/10
19/19 - 4s - loss: 719.6310 - loglik: -7.1873e+02 - logprior: -8.8961e-01
Epoch 8/10
19/19 - 4s - loss: 718.0729 - loglik: -7.1719e+02 - logprior: -8.7745e-01
Epoch 9/10
19/19 - 4s - loss: 717.3292 - loglik: -7.1645e+02 - logprior: -8.6999e-01
Epoch 10/10
19/19 - 5s - loss: 717.3340 - loglik: -7.1646e+02 - logprior: -8.5953e-01
Fitted a model with MAP estimate = -715.2384
expansions: [(0, 7), (7, 3), (8, 2), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (53, 1), (55, 1), (58, 1), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (96, 2), (97, 2), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 158 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 727.6087 - loglik: -7.2373e+02 - logprior: -3.8808e+00
Epoch 2/2
19/19 - 6s - loss: 710.8036 - loglik: -7.0976e+02 - logprior: -1.0394e+00
Fitted a model with MAP estimate = -707.8566
expansions: [(0, 4)]
discards: [  1   2   3   4   6  22  99 100 128 147]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 716.0134 - loglik: -7.1223e+02 - logprior: -3.7809e+00
Epoch 2/2
19/19 - 5s - loss: 710.1440 - loglik: -7.0906e+02 - logprior: -1.0827e+00
Fitted a model with MAP estimate = -708.2239
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 712.9838 - loglik: -7.0995e+02 - logprior: -3.0304e+00
Epoch 2/10
19/19 - 5s - loss: 708.0767 - loglik: -7.0728e+02 - logprior: -7.9372e-01
Epoch 3/10
19/19 - 5s - loss: 707.1735 - loglik: -7.0659e+02 - logprior: -5.8637e-01
Epoch 4/10
19/19 - 5s - loss: 706.2286 - loglik: -7.0571e+02 - logprior: -5.1958e-01
Epoch 5/10
19/19 - 5s - loss: 704.5452 - loglik: -7.0410e+02 - logprior: -4.4208e-01
Epoch 6/10
19/19 - 5s - loss: 703.9034 - loglik: -7.0350e+02 - logprior: -3.9541e-01
Epoch 7/10
19/19 - 5s - loss: 701.9967 - loglik: -7.0163e+02 - logprior: -3.6468e-01
Epoch 8/10
19/19 - 5s - loss: 701.4333 - loglik: -7.0109e+02 - logprior: -3.3106e-01
Epoch 9/10
19/19 - 6s - loss: 700.9786 - loglik: -7.0065e+02 - logprior: -3.1914e-01
Epoch 10/10
19/19 - 6s - loss: 698.5994 - loglik: -6.9829e+02 - logprior: -3.0162e-01
Fitted a model with MAP estimate = -698.0821
Time for alignment: 164.0789
Computed alignments with likelihoods: ['-699.5870', '-697.9393', '-697.5443', '-700.8879', '-698.0821']
Best model has likelihood: -697.5443  (prior= -0.2830 )
time for generating output: 0.1636
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13522.projection.fasta
SP score = 0.6058806527051528
Training of 5 independent models on file PF00037.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff58d44bfd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6dd79a3d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff718aff400>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 136.2150 - loglik: -1.3284e+02 - logprior: -3.3781e+00
Epoch 2/10
19/19 - 0s - loss: 115.6979 - loglik: -1.1432e+02 - logprior: -1.3747e+00
Epoch 3/10
19/19 - 0s - loss: 109.8865 - loglik: -1.0842e+02 - logprior: -1.4697e+00
Epoch 4/10
19/19 - 0s - loss: 108.3090 - loglik: -1.0686e+02 - logprior: -1.4502e+00
Epoch 5/10
19/19 - 0s - loss: 107.9919 - loglik: -1.0656e+02 - logprior: -1.4256e+00
Epoch 6/10
19/19 - 0s - loss: 107.4924 - loglik: -1.0607e+02 - logprior: -1.4177e+00
Epoch 7/10
19/19 - 0s - loss: 107.2901 - loglik: -1.0587e+02 - logprior: -1.4096e+00
Epoch 8/10
19/19 - 0s - loss: 106.7994 - loglik: -1.0539e+02 - logprior: -1.4036e+00
Epoch 9/10
19/19 - 0s - loss: 106.5897 - loglik: -1.0518e+02 - logprior: -1.4017e+00
Epoch 10/10
19/19 - 0s - loss: 106.2986 - loglik: -1.0488e+02 - logprior: -1.4055e+00
Fitted a model with MAP estimate = -106.0449
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 115.0078 - loglik: -1.1023e+02 - logprior: -4.7776e+00
Epoch 2/2
19/19 - 0s - loss: 106.6534 - loglik: -1.0511e+02 - logprior: -1.5429e+00
Fitted a model with MAP estimate = -105.8355
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 111.0476 - loglik: -1.0801e+02 - logprior: -3.0327e+00
Epoch 2/10
19/19 - 0s - loss: 105.7350 - loglik: -1.0435e+02 - logprior: -1.3825e+00
Epoch 3/10
19/19 - 0s - loss: 105.2631 - loglik: -1.0397e+02 - logprior: -1.2937e+00
Epoch 4/10
19/19 - 0s - loss: 105.0209 - loglik: -1.0375e+02 - logprior: -1.2692e+00
Epoch 5/10
19/19 - 0s - loss: 104.6003 - loglik: -1.0337e+02 - logprior: -1.2288e+00
Epoch 6/10
19/19 - 0s - loss: 104.2238 - loglik: -1.0300e+02 - logprior: -1.2214e+00
Epoch 7/10
19/19 - 0s - loss: 103.7181 - loglik: -1.0250e+02 - logprior: -1.2103e+00
Epoch 8/10
19/19 - 0s - loss: 103.2768 - loglik: -1.0207e+02 - logprior: -1.2023e+00
Epoch 9/10
19/19 - 0s - loss: 102.6776 - loglik: -1.0147e+02 - logprior: -1.1945e+00
Epoch 10/10
19/19 - 0s - loss: 102.2204 - loglik: -1.0102e+02 - logprior: -1.1902e+00
Fitted a model with MAP estimate = -101.8448
Time for alignment: 21.3146
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 136.0303 - loglik: -1.3266e+02 - logprior: -3.3705e+00
Epoch 2/10
19/19 - 0s - loss: 115.5644 - loglik: -1.1420e+02 - logprior: -1.3623e+00
Epoch 3/10
19/19 - 0s - loss: 109.5603 - loglik: -1.0809e+02 - logprior: -1.4660e+00
Epoch 4/10
19/19 - 0s - loss: 108.3854 - loglik: -1.0693e+02 - logprior: -1.4518e+00
Epoch 5/10
19/19 - 0s - loss: 107.9515 - loglik: -1.0653e+02 - logprior: -1.4229e+00
Epoch 6/10
19/19 - 0s - loss: 107.5764 - loglik: -1.0615e+02 - logprior: -1.4187e+00
Epoch 7/10
19/19 - 0s - loss: 107.1293 - loglik: -1.0572e+02 - logprior: -1.4083e+00
Epoch 8/10
19/19 - 0s - loss: 106.9959 - loglik: -1.0558e+02 - logprior: -1.4060e+00
Epoch 9/10
19/19 - 0s - loss: 106.4771 - loglik: -1.0507e+02 - logprior: -1.4009e+00
Epoch 10/10
19/19 - 0s - loss: 106.2970 - loglik: -1.0488e+02 - logprior: -1.4068e+00
Fitted a model with MAP estimate = -106.0348
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 115.0781 - loglik: -1.1030e+02 - logprior: -4.7828e+00
Epoch 2/2
19/19 - 0s - loss: 106.6183 - loglik: -1.0508e+02 - logprior: -1.5366e+00
Fitted a model with MAP estimate = -105.8348
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 111.0178 - loglik: -1.0797e+02 - logprior: -3.0445e+00
Epoch 2/10
19/19 - 0s - loss: 105.7209 - loglik: -1.0434e+02 - logprior: -1.3793e+00
Epoch 3/10
19/19 - 0s - loss: 105.2755 - loglik: -1.0398e+02 - logprior: -1.2954e+00
Epoch 4/10
19/19 - 0s - loss: 105.0364 - loglik: -1.0377e+02 - logprior: -1.2686e+00
Epoch 5/10
19/19 - 0s - loss: 104.5139 - loglik: -1.0328e+02 - logprior: -1.2291e+00
Epoch 6/10
19/19 - 0s - loss: 104.2497 - loglik: -1.0302e+02 - logprior: -1.2213e+00
Epoch 7/10
19/19 - 0s - loss: 103.7128 - loglik: -1.0250e+02 - logprior: -1.2103e+00
Epoch 8/10
19/19 - 0s - loss: 103.2494 - loglik: -1.0204e+02 - logprior: -1.2018e+00
Epoch 9/10
19/19 - 0s - loss: 102.7257 - loglik: -1.0152e+02 - logprior: -1.1957e+00
Epoch 10/10
19/19 - 0s - loss: 102.2929 - loglik: -1.0109e+02 - logprior: -1.1899e+00
Fitted a model with MAP estimate = -101.8534
Time for alignment: 25.6219
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 136.0892 - loglik: -1.3273e+02 - logprior: -3.3639e+00
Epoch 2/10
19/19 - 0s - loss: 115.4413 - loglik: -1.1409e+02 - logprior: -1.3547e+00
Epoch 3/10
19/19 - 0s - loss: 109.7022 - loglik: -1.0826e+02 - logprior: -1.4433e+00
Epoch 4/10
19/19 - 0s - loss: 108.3908 - loglik: -1.0694e+02 - logprior: -1.4448e+00
Epoch 5/10
19/19 - 0s - loss: 107.9289 - loglik: -1.0650e+02 - logprior: -1.4267e+00
Epoch 6/10
19/19 - 0s - loss: 107.6034 - loglik: -1.0618e+02 - logprior: -1.4146e+00
Epoch 7/10
19/19 - 0s - loss: 107.2066 - loglik: -1.0579e+02 - logprior: -1.4105e+00
Epoch 8/10
19/19 - 0s - loss: 106.8369 - loglik: -1.0542e+02 - logprior: -1.4059e+00
Epoch 9/10
19/19 - 0s - loss: 106.5638 - loglik: -1.0515e+02 - logprior: -1.4003e+00
Epoch 10/10
19/19 - 0s - loss: 106.2066 - loglik: -1.0479e+02 - logprior: -1.4034e+00
Fitted a model with MAP estimate = -106.0249
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 115.0824 - loglik: -1.1030e+02 - logprior: -4.7833e+00
Epoch 2/2
19/19 - 0s - loss: 106.5919 - loglik: -1.0503e+02 - logprior: -1.5596e+00
Fitted a model with MAP estimate = -105.8192
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 111.0733 - loglik: -1.0802e+02 - logprior: -3.0512e+00
Epoch 2/10
19/19 - 0s - loss: 105.7000 - loglik: -1.0435e+02 - logprior: -1.3535e+00
Epoch 3/10
19/19 - 0s - loss: 105.2948 - loglik: -1.0398e+02 - logprior: -1.3117e+00
Epoch 4/10
19/19 - 0s - loss: 105.0224 - loglik: -1.0376e+02 - logprior: -1.2638e+00
Epoch 5/10
19/19 - 0s - loss: 104.5658 - loglik: -1.0333e+02 - logprior: -1.2300e+00
Epoch 6/10
19/19 - 0s - loss: 104.2128 - loglik: -1.0299e+02 - logprior: -1.2217e+00
Epoch 7/10
19/19 - 0s - loss: 103.7272 - loglik: -1.0251e+02 - logprior: -1.2101e+00
Epoch 8/10
19/19 - 0s - loss: 103.2156 - loglik: -1.0201e+02 - logprior: -1.2004e+00
Epoch 9/10
19/19 - 0s - loss: 102.8534 - loglik: -1.0165e+02 - logprior: -1.1962e+00
Epoch 10/10
19/19 - 0s - loss: 102.2956 - loglik: -1.0109e+02 - logprior: -1.1892e+00
Fitted a model with MAP estimate = -101.9168
Time for alignment: 23.1946
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 136.0499 - loglik: -1.3269e+02 - logprior: -3.3550e+00
Epoch 2/10
19/19 - 0s - loss: 115.6480 - loglik: -1.1430e+02 - logprior: -1.3494e+00
Epoch 3/10
19/19 - 0s - loss: 109.9225 - loglik: -1.0850e+02 - logprior: -1.4245e+00
Epoch 4/10
19/19 - 0s - loss: 108.4023 - loglik: -1.0696e+02 - logprior: -1.4416e+00
Epoch 5/10
19/19 - 0s - loss: 107.9446 - loglik: -1.0651e+02 - logprior: -1.4297e+00
Epoch 6/10
19/19 - 0s - loss: 107.5841 - loglik: -1.0616e+02 - logprior: -1.4214e+00
Epoch 7/10
19/19 - 0s - loss: 107.1437 - loglik: -1.0573e+02 - logprior: -1.4065e+00
Epoch 8/10
19/19 - 0s - loss: 106.8499 - loglik: -1.0544e+02 - logprior: -1.4061e+00
Epoch 9/10
19/19 - 0s - loss: 106.6077 - loglik: -1.0520e+02 - logprior: -1.4018e+00
Epoch 10/10
19/19 - 0s - loss: 106.3639 - loglik: -1.0495e+02 - logprior: -1.4049e+00
Fitted a model with MAP estimate = -106.0440
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 115.0110 - loglik: -1.1022e+02 - logprior: -4.7891e+00
Epoch 2/2
19/19 - 0s - loss: 106.6640 - loglik: -1.0514e+02 - logprior: -1.5226e+00
Fitted a model with MAP estimate = -105.8498
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 111.0541 - loglik: -1.0802e+02 - logprior: -3.0302e+00
Epoch 2/10
19/19 - 0s - loss: 105.6736 - loglik: -1.0427e+02 - logprior: -1.4009e+00
Epoch 3/10
19/19 - 0s - loss: 105.2770 - loglik: -1.0399e+02 - logprior: -1.2830e+00
Epoch 4/10
19/19 - 0s - loss: 105.0376 - loglik: -1.0377e+02 - logprior: -1.2707e+00
Epoch 5/10
19/19 - 0s - loss: 104.5656 - loglik: -1.0333e+02 - logprior: -1.2299e+00
Epoch 6/10
19/19 - 0s - loss: 104.1939 - loglik: -1.0297e+02 - logprior: -1.2197e+00
Epoch 7/10
19/19 - 0s - loss: 103.7418 - loglik: -1.0253e+02 - logprior: -1.2100e+00
Epoch 8/10
19/19 - 0s - loss: 103.1885 - loglik: -1.0198e+02 - logprior: -1.2013e+00
Epoch 9/10
19/19 - 0s - loss: 102.9162 - loglik: -1.0171e+02 - logprior: -1.1933e+00
Epoch 10/10
19/19 - 0s - loss: 102.2249 - loglik: -1.0102e+02 - logprior: -1.1898e+00
Fitted a model with MAP estimate = -101.9402
Time for alignment: 23.0485
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 135.9411 - loglik: -1.3257e+02 - logprior: -3.3661e+00
Epoch 2/10
19/19 - 0s - loss: 115.2978 - loglik: -1.1396e+02 - logprior: -1.3407e+00
Epoch 3/10
19/19 - 0s - loss: 109.9143 - loglik: -1.0850e+02 - logprior: -1.4179e+00
Epoch 4/10
19/19 - 0s - loss: 108.4498 - loglik: -1.0701e+02 - logprior: -1.4378e+00
Epoch 5/10
19/19 - 0s - loss: 107.8963 - loglik: -1.0647e+02 - logprior: -1.4246e+00
Epoch 6/10
19/19 - 0s - loss: 107.5212 - loglik: -1.0610e+02 - logprior: -1.4156e+00
Epoch 7/10
19/19 - 0s - loss: 107.2267 - loglik: -1.0581e+02 - logprior: -1.4107e+00
Epoch 8/10
19/19 - 0s - loss: 106.8570 - loglik: -1.0545e+02 - logprior: -1.4033e+00
Epoch 9/10
19/19 - 0s - loss: 106.5327 - loglik: -1.0512e+02 - logprior: -1.4038e+00
Epoch 10/10
19/19 - 0s - loss: 106.3360 - loglik: -1.0492e+02 - logprior: -1.4017e+00
Fitted a model with MAP estimate = -106.0203
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 115.0995 - loglik: -1.1030e+02 - logprior: -4.7947e+00
Epoch 2/2
19/19 - 0s - loss: 106.6063 - loglik: -1.0509e+02 - logprior: -1.5194e+00
Fitted a model with MAP estimate = -105.8490
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 111.0445 - loglik: -1.0801e+02 - logprior: -3.0339e+00
Epoch 2/10
19/19 - 0s - loss: 105.6663 - loglik: -1.0426e+02 - logprior: -1.4065e+00
Epoch 3/10
19/19 - 0s - loss: 105.2840 - loglik: -1.0400e+02 - logprior: -1.2828e+00
Epoch 4/10
19/19 - 0s - loss: 105.0404 - loglik: -1.0377e+02 - logprior: -1.2716e+00
Epoch 5/10
19/19 - 0s - loss: 104.5457 - loglik: -1.0331e+02 - logprior: -1.2304e+00
Epoch 6/10
19/19 - 0s - loss: 104.1543 - loglik: -1.0293e+02 - logprior: -1.2203e+00
Epoch 7/10
19/19 - 0s - loss: 103.7980 - loglik: -1.0258e+02 - logprior: -1.2103e+00
Epoch 8/10
19/19 - 0s - loss: 103.2765 - loglik: -1.0206e+02 - logprior: -1.2029e+00
Epoch 9/10
19/19 - 0s - loss: 102.6829 - loglik: -1.0148e+02 - logprior: -1.1956e+00
Epoch 10/10
19/19 - 0s - loss: 102.3687 - loglik: -1.0116e+02 - logprior: -1.1911e+00
Fitted a model with MAP estimate = -101.9113
Time for alignment: 23.2027
Computed alignments with likelihoods: ['-101.8448', '-101.8534', '-101.9168', '-101.9402', '-101.9113']
Best model has likelihood: -101.8448  (prior= -1.1882 )
time for generating output: 0.0825
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00037.projection.fasta
SP score = 0.8643659711075441
Training of 5 independent models on file PF00224.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff66c0fea60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff718a49040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6e5b578e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 1632.8544 - loglik: -1.6314e+03 - logprior: -1.4776e+00
Epoch 2/10
39/39 - 29s - loss: 1429.3018 - loglik: -1.4280e+03 - logprior: -1.3434e+00
Epoch 3/10
39/39 - 31s - loss: 1418.8123 - loglik: -1.4175e+03 - logprior: -1.2689e+00
Epoch 4/10
39/39 - 32s - loss: 1416.1830 - loglik: -1.4150e+03 - logprior: -1.1747e+00
Epoch 5/10
39/39 - 31s - loss: 1415.1376 - loglik: -1.4140e+03 - logprior: -1.1592e+00
Epoch 6/10
39/39 - 30s - loss: 1412.8184 - loglik: -1.4116e+03 - logprior: -1.1674e+00
Epoch 7/10
39/39 - 30s - loss: 1411.2250 - loglik: -1.4100e+03 - logprior: -1.2007e+00
Epoch 8/10
39/39 - 30s - loss: 1411.0375 - loglik: -1.4098e+03 - logprior: -1.2157e+00
Epoch 9/10
39/39 - 31s - loss: 1408.8510 - loglik: -1.4076e+03 - logprior: -1.2474e+00
Epoch 10/10
39/39 - 33s - loss: 1408.6342 - loglik: -1.4074e+03 - logprior: -1.2470e+00
Fitted a model with MAP estimate = -1407.3928
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (44, 1), (46, 2), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (90, 1), (93, 1), (105, 1), (109, 1), (110, 1), (111, 1), (113, 1), (133, 1), (134, 1), (142, 1), (145, 2), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (183, 1), (184, 1), (188, 1), (189, 1), (206, 1), (212, 1), (214, 2), (216, 2), (227, 1), (229, 3), (230, 1), (243, 1), (265, 1), (267, 1), (269, 1), (270, 1), (272, 3), (273, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 347 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 55s - loss: 1386.9662 - loglik: -1.3851e+03 - logprior: -1.8311e+00
Epoch 2/2
39/39 - 47s - loss: 1367.0540 - loglik: -1.3665e+03 - logprior: -5.5988e-01
Fitted a model with MAP estimate = -1364.3411
expansions: []
discards: [  2  57  98 179 214]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 47s - loss: 1372.2358 - loglik: -1.3710e+03 - logprior: -1.2224e+00
Epoch 2/2
39/39 - 47s - loss: 1366.9604 - loglik: -1.3671e+03 - logprior: 0.1105
Fitted a model with MAP estimate = -1364.6723
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 342 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 1369.1705 - loglik: -1.3679e+03 - logprior: -1.2391e+00
Epoch 2/10
39/39 - 48s - loss: 1365.1906 - loglik: -1.3654e+03 - logprior: 0.2452
Epoch 3/10
39/39 - 45s - loss: 1364.4136 - loglik: -1.3649e+03 - logprior: 0.4484
Epoch 4/10
39/39 - 45s - loss: 1363.1053 - loglik: -1.3633e+03 - logprior: 0.2382
Epoch 5/10
39/39 - 48s - loss: 1359.8800 - loglik: -1.3606e+03 - logprior: 0.7147
Epoch 6/10
39/39 - 47s - loss: 1360.0907 - loglik: -1.3608e+03 - logprior: 0.7224
Fitted a model with MAP estimate = -1357.8619
Time for alignment: 1018.9486
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 33s - loss: 1634.7988 - loglik: -1.6333e+03 - logprior: -1.4882e+00
Epoch 2/10
39/39 - 31s - loss: 1438.1136 - loglik: -1.4367e+03 - logprior: -1.4271e+00
Epoch 3/10
39/39 - 28s - loss: 1425.2330 - loglik: -1.4238e+03 - logprior: -1.3852e+00
Epoch 4/10
39/39 - 25s - loss: 1422.5878 - loglik: -1.4213e+03 - logprior: -1.2713e+00
Epoch 5/10
39/39 - 24s - loss: 1420.3905 - loglik: -1.4191e+03 - logprior: -1.2698e+00
Epoch 6/10
39/39 - 24s - loss: 1417.9038 - loglik: -1.4166e+03 - logprior: -1.3041e+00
Epoch 7/10
39/39 - 23s - loss: 1416.4359 - loglik: -1.4151e+03 - logprior: -1.3397e+00
Epoch 8/10
39/39 - 23s - loss: 1415.3335 - loglik: -1.4139e+03 - logprior: -1.3985e+00
Epoch 9/10
39/39 - 23s - loss: 1414.5031 - loglik: -1.4131e+03 - logprior: -1.4038e+00
Epoch 10/10
39/39 - 23s - loss: 1414.6354 - loglik: -1.4132e+03 - logprior: -1.4161e+00
Fitted a model with MAP estimate = -1412.6800
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (18, 2), (37, 1), (39, 1), (43, 1), (44, 1), (45, 2), (47, 1), (69, 1), (70, 1), (73, 1), (77, 1), (79, 2), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (96, 1), (108, 1), (110, 1), (111, 1), (113, 1), (132, 3), (133, 1), (142, 1), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (181, 1), (182, 1), (188, 1), (189, 1), (204, 1), (208, 1), (211, 1), (213, 1), (216, 1), (217, 1), (227, 1), (229, 3), (230, 1), (243, 1), (258, 1), (264, 1), (267, 1), (269, 1), (270, 1), (271, 1), (272, 2), (273, 4)]
discards: [51]
Re-initialized the encoder parameters.
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 1392.5673 - loglik: -1.3907e+03 - logprior: -1.8642e+00
Epoch 2/2
39/39 - 32s - loss: 1372.8529 - loglik: -1.3724e+03 - logprior: -4.9016e-01
Fitted a model with MAP estimate = -1370.1218
expansions: []
discards: [  0  57  97 163 213]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 1378.4318 - loglik: -1.3762e+03 - logprior: -2.2688e+00
Epoch 2/2
39/39 - 31s - loss: 1373.5513 - loglik: -1.3734e+03 - logprior: -1.9156e-01
Fitted a model with MAP estimate = -1370.1179
expansions: [(0, 2)]
discards: [  0 340]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 35s - loss: 1375.1552 - loglik: -1.3740e+03 - logprior: -1.2003e+00
Epoch 2/10
39/39 - 31s - loss: 1371.1632 - loglik: -1.3715e+03 - logprior: 0.2903
Epoch 3/10
39/39 - 31s - loss: 1370.2363 - loglik: -1.3708e+03 - logprior: 0.5710
Epoch 4/10
39/39 - 31s - loss: 1368.1558 - loglik: -1.3687e+03 - logprior: 0.5378
Epoch 5/10
39/39 - 31s - loss: 1365.7036 - loglik: -1.3665e+03 - logprior: 0.7952
Epoch 6/10
39/39 - 31s - loss: 1365.5903 - loglik: -1.3664e+03 - logprior: 0.8369
Epoch 7/10
39/39 - 31s - loss: 1362.4812 - loglik: -1.3634e+03 - logprior: 0.8821
Epoch 8/10
39/39 - 31s - loss: 1360.7411 - loglik: -1.3617e+03 - logprior: 0.9851
Epoch 9/10
39/39 - 31s - loss: 1360.5051 - loglik: -1.3616e+03 - logprior: 1.0816
Epoch 10/10
39/39 - 31s - loss: 1360.9071 - loglik: -1.3621e+03 - logprior: 1.2037
Fitted a model with MAP estimate = -1359.4083
Time for alignment: 867.1182
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 1634.2500 - loglik: -1.6328e+03 - logprior: -1.4964e+00
Epoch 2/10
39/39 - 22s - loss: 1435.4304 - loglik: -1.4340e+03 - logprior: -1.3960e+00
Epoch 3/10
39/39 - 22s - loss: 1425.3351 - loglik: -1.4240e+03 - logprior: -1.2942e+00
Epoch 4/10
39/39 - 22s - loss: 1422.6052 - loglik: -1.4214e+03 - logprior: -1.2122e+00
Epoch 5/10
39/39 - 22s - loss: 1420.2307 - loglik: -1.4190e+03 - logprior: -1.2128e+00
Epoch 6/10
39/39 - 22s - loss: 1418.4277 - loglik: -1.4172e+03 - logprior: -1.2279e+00
Epoch 7/10
39/39 - 22s - loss: 1416.0194 - loglik: -1.4147e+03 - logprior: -1.2728e+00
Epoch 8/10
39/39 - 22s - loss: 1416.1598 - loglik: -1.4148e+03 - logprior: -1.3032e+00
Fitted a model with MAP estimate = -1413.1308
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (18, 1), (37, 1), (38, 1), (39, 1), (43, 1), (44, 1), (45, 2), (47, 1), (64, 1), (68, 1), (70, 1), (72, 1), (79, 2), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (105, 1), (109, 1), (110, 1), (111, 1), (113, 1), (120, 1), (132, 1), (133, 1), (142, 1), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (179, 1), (182, 1), (183, 1), (187, 1), (206, 1), (208, 1), (214, 2), (216, 1), (217, 1), (227, 1), (229, 3), (230, 1), (243, 1), (265, 1), (267, 1), (269, 3), (271, 1), (272, 2), (273, 4)]
discards: [51]
Re-initialized the encoder parameters.
Fitting a model of length 345 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 1391.2007 - loglik: -1.3894e+03 - logprior: -1.8127e+00
Epoch 2/2
39/39 - 31s - loss: 1372.4025 - loglik: -1.3720e+03 - logprior: -4.0829e-01
Fitted a model with MAP estimate = -1369.6731
expansions: []
discards: [  0   1  57  97 212]
Re-initialized the encoder parameters.
Fitting a model of length 340 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 1378.1519 - loglik: -1.3769e+03 - logprior: -1.2698e+00
Epoch 2/2
39/39 - 31s - loss: 1373.0861 - loglik: -1.3731e+03 - logprior: 0.0365
Fitted a model with MAP estimate = -1370.5531
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 342 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 1374.3462 - loglik: -1.3731e+03 - logprior: -1.2194e+00
Epoch 2/10
39/39 - 31s - loss: 1370.4413 - loglik: -1.3705e+03 - logprior: 0.0990
Epoch 3/10
39/39 - 31s - loss: 1369.5269 - loglik: -1.3695e+03 - logprior: 0.0125
Epoch 4/10
39/39 - 31s - loss: 1367.4521 - loglik: -1.3679e+03 - logprior: 0.4259
Epoch 5/10
39/39 - 31s - loss: 1365.8745 - loglik: -1.3662e+03 - logprior: 0.3789
Epoch 6/10
39/39 - 31s - loss: 1364.2164 - loglik: -1.3649e+03 - logprior: 0.6866
Epoch 7/10
39/39 - 31s - loss: 1362.9135 - loglik: -1.3634e+03 - logprior: 0.4833
Epoch 8/10
39/39 - 31s - loss: 1360.3241 - loglik: -1.3611e+03 - logprior: 0.8171
Epoch 9/10
39/39 - 31s - loss: 1360.9546 - loglik: -1.3616e+03 - logprior: 0.6121
Fitted a model with MAP estimate = -1359.5845
Time for alignment: 752.6880
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 1632.1156 - loglik: -1.6306e+03 - logprior: -1.4685e+00
Epoch 2/10
39/39 - 22s - loss: 1429.5615 - loglik: -1.4282e+03 - logprior: -1.3312e+00
Epoch 3/10
39/39 - 22s - loss: 1418.9976 - loglik: -1.4178e+03 - logprior: -1.2317e+00
Epoch 4/10
39/39 - 22s - loss: 1417.6383 - loglik: -1.4165e+03 - logprior: -1.1139e+00
Epoch 5/10
39/39 - 22s - loss: 1415.3905 - loglik: -1.4143e+03 - logprior: -1.0778e+00
Epoch 6/10
39/39 - 22s - loss: 1414.2509 - loglik: -1.4132e+03 - logprior: -1.0856e+00
Epoch 7/10
39/39 - 22s - loss: 1412.6102 - loglik: -1.4115e+03 - logprior: -1.0997e+00
Epoch 8/10
39/39 - 22s - loss: 1410.8010 - loglik: -1.4097e+03 - logprior: -1.1377e+00
Epoch 9/10
39/39 - 22s - loss: 1411.1183 - loglik: -1.4099e+03 - logprior: -1.1692e+00
Fitted a model with MAP estimate = -1408.6893
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (38, 1), (40, 1), (44, 1), (45, 1), (46, 2), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (77, 1), (79, 2), (82, 1), (83, 1), (84, 1), (90, 1), (93, 1), (105, 1), (109, 1), (110, 1), (111, 1), (113, 1), (133, 1), (134, 1), (142, 1), (145, 2), (148, 2), (149, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (183, 1), (184, 1), (188, 1), (207, 1), (209, 1), (215, 2), (217, 2), (228, 1), (230, 3), (231, 1), (237, 1), (243, 1), (265, 1), (267, 1), (270, 2), (272, 3), (273, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 347 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 1385.8560 - loglik: -1.3840e+03 - logprior: -1.8466e+00
Epoch 2/2
39/39 - 32s - loss: 1367.2081 - loglik: -1.3668e+03 - logprior: -4.4705e-01
Fitted a model with MAP estimate = -1364.4873
expansions: []
discards: [  0  57  98 179 214]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 1373.0084 - loglik: -1.3707e+03 - logprior: -2.2725e+00
Epoch 2/2
39/39 - 31s - loss: 1367.8572 - loglik: -1.3677e+03 - logprior: -1.6913e-01
Fitted a model with MAP estimate = -1364.5224
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 1368.7559 - loglik: -1.3676e+03 - logprior: -1.1282e+00
Epoch 2/10
39/39 - 31s - loss: 1364.7410 - loglik: -1.3649e+03 - logprior: 0.1462
Epoch 3/10
39/39 - 31s - loss: 1363.7758 - loglik: -1.3639e+03 - logprior: 0.1439
Epoch 4/10
39/39 - 31s - loss: 1362.4932 - loglik: -1.3629e+03 - logprior: 0.3871
Epoch 5/10
39/39 - 31s - loss: 1361.2881 - loglik: -1.3617e+03 - logprior: 0.4402
Epoch 6/10
39/39 - 31s - loss: 1357.9773 - loglik: -1.3585e+03 - logprior: 0.5048
Epoch 7/10
39/39 - 31s - loss: 1355.5792 - loglik: -1.3563e+03 - logprior: 0.7717
Epoch 8/10
39/39 - 31s - loss: 1355.9738 - loglik: -1.3567e+03 - logprior: 0.7207
Fitted a model with MAP estimate = -1354.2510
Time for alignment: 749.1157
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 1634.6019 - loglik: -1.6331e+03 - logprior: -1.4820e+00
Epoch 2/10
39/39 - 22s - loss: 1435.4309 - loglik: -1.4340e+03 - logprior: -1.3986e+00
Epoch 3/10
39/39 - 22s - loss: 1424.4006 - loglik: -1.4231e+03 - logprior: -1.3396e+00
Epoch 4/10
39/39 - 22s - loss: 1422.1371 - loglik: -1.4209e+03 - logprior: -1.2472e+00
Epoch 5/10
39/39 - 22s - loss: 1419.5405 - loglik: -1.4183e+03 - logprior: -1.2564e+00
Epoch 6/10
39/39 - 22s - loss: 1417.9703 - loglik: -1.4167e+03 - logprior: -1.2913e+00
Epoch 7/10
39/39 - 22s - loss: 1415.5756 - loglik: -1.4143e+03 - logprior: -1.3151e+00
Epoch 8/10
39/39 - 22s - loss: 1414.8954 - loglik: -1.4135e+03 - logprior: -1.3436e+00
Epoch 9/10
39/39 - 22s - loss: 1414.7252 - loglik: -1.4134e+03 - logprior: -1.3598e+00
Epoch 10/10
39/39 - 22s - loss: 1413.6027 - loglik: -1.4122e+03 - logprior: -1.3630e+00
Fitted a model with MAP estimate = -1412.1765
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (38, 1), (39, 1), (40, 1), (44, 2), (45, 3), (47, 1), (64, 1), (68, 1), (70, 1), (77, 1), (79, 2), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (105, 1), (109, 1), (110, 2), (112, 2), (113, 1), (133, 1), (134, 1), (136, 1), (145, 2), (148, 1), (149, 1), (159, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (183, 1), (184, 1), (188, 1), (205, 1), (209, 1), (212, 1), (214, 2), (216, 1), (217, 1), (230, 3), (231, 1), (243, 1), (244, 1), (258, 1), (264, 1), (267, 1), (269, 1), (270, 1), (271, 1), (272, 2), (273, 4)]
discards: [51]
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 1392.2323 - loglik: -1.3902e+03 - logprior: -1.9860e+00
Epoch 2/2
39/39 - 32s - loss: 1372.3940 - loglik: -1.3718e+03 - logprior: -5.8063e-01
Fitted a model with MAP estimate = -1369.7307
expansions: []
discards: [  0   1  57  58  98 138 142 181 216]
Re-initialized the encoder parameters.
Fitting a model of length 340 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 1377.7396 - loglik: -1.3764e+03 - logprior: -1.3800e+00
Epoch 2/2
39/39 - 31s - loss: 1373.0840 - loglik: -1.3733e+03 - logprior: 0.1725
Fitted a model with MAP estimate = -1370.7355
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 342 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 1374.8613 - loglik: -1.3736e+03 - logprior: -1.2696e+00
Epoch 2/10
39/39 - 31s - loss: 1370.0112 - loglik: -1.3702e+03 - logprior: 0.1562
Epoch 3/10
39/39 - 31s - loss: 1368.6582 - loglik: -1.3689e+03 - logprior: 0.2178
Epoch 4/10
39/39 - 31s - loss: 1367.7068 - loglik: -1.3682e+03 - logprior: 0.4534
Epoch 5/10
39/39 - 31s - loss: 1365.2339 - loglik: -1.3656e+03 - logprior: 0.3859
Epoch 6/10
39/39 - 31s - loss: 1364.5370 - loglik: -1.3652e+03 - logprior: 0.6279
Epoch 7/10
39/39 - 31s - loss: 1362.2737 - loglik: -1.3629e+03 - logprior: 0.6225
Epoch 8/10
39/39 - 31s - loss: 1360.3807 - loglik: -1.3611e+03 - logprior: 0.7544
Epoch 9/10
39/39 - 31s - loss: 1359.8246 - loglik: -1.3607e+03 - logprior: 0.8696
Epoch 10/10
39/39 - 31s - loss: 1359.0198 - loglik: -1.3600e+03 - logprior: 0.9618
Fitted a model with MAP estimate = -1358.7580
Time for alignment: 829.9165
Computed alignments with likelihoods: ['-1357.8619', '-1359.4083', '-1359.5845', '-1354.2510', '-1358.7580']
Best model has likelihood: -1354.2510  (prior= 0.9265 )
time for generating output: 0.2539
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00224.projection.fasta
SP score = 0.9135408353285448
Training of 5 independent models on file PF13365.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff5a77e13d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff57cec5460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff5a788ef70>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 808.2693 - loglik: -8.0526e+02 - logprior: -3.0101e+00
Epoch 2/10
19/19 - 4s - loss: 737.2345 - loglik: -7.3600e+02 - logprior: -1.2354e+00
Epoch 3/10
19/19 - 4s - loss: 713.7775 - loglik: -7.1211e+02 - logprior: -1.6706e+00
Epoch 4/10
19/19 - 4s - loss: 710.6133 - loglik: -7.0912e+02 - logprior: -1.4936e+00
Epoch 5/10
19/19 - 4s - loss: 708.9968 - loglik: -7.0755e+02 - logprior: -1.4479e+00
Epoch 6/10
19/19 - 4s - loss: 706.2123 - loglik: -7.0480e+02 - logprior: -1.4098e+00
Epoch 7/10
19/19 - 4s - loss: 705.1064 - loglik: -7.0371e+02 - logprior: -1.3898e+00
Epoch 8/10
19/19 - 4s - loss: 703.7227 - loglik: -7.0235e+02 - logprior: -1.3683e+00
Epoch 9/10
19/19 - 4s - loss: 702.1891 - loglik: -7.0083e+02 - logprior: -1.3447e+00
Epoch 10/10
19/19 - 4s - loss: 701.1525 - loglik: -6.9983e+02 - logprior: -1.3124e+00
Fitted a model with MAP estimate = -697.2891
expansions: [(7, 2), (21, 3), (22, 1), (25, 2), (30, 2), (35, 1), (37, 2), (45, 2), (46, 3), (49, 1), (50, 1), (55, 1), (56, 1), (71, 2), (84, 5), (85, 3), (86, 1), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 147 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 713.4844 - loglik: -7.1038e+02 - logprior: -3.1047e+00
Epoch 2/2
39/39 - 6s - loss: 701.9687 - loglik: -7.0054e+02 - logprior: -1.4293e+00
Fitted a model with MAP estimate = -696.9726
expansions: []
discards: [ 23  30  37  48  59  60  92 111 139]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 704.5254 - loglik: -7.0229e+02 - logprior: -2.2337e+00
Epoch 2/2
39/39 - 6s - loss: 700.6526 - loglik: -6.9945e+02 - logprior: -1.1976e+00
Fitted a model with MAP estimate = -696.6681
expansions: [(105, 1)]
discards: [55]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 9s - loss: 701.6155 - loglik: -6.9941e+02 - logprior: -2.2035e+00
Epoch 2/10
39/39 - 6s - loss: 698.3856 - loglik: -6.9724e+02 - logprior: -1.1425e+00
Epoch 3/10
39/39 - 6s - loss: 696.2448 - loglik: -6.9523e+02 - logprior: -1.0123e+00
Epoch 4/10
39/39 - 6s - loss: 693.8643 - loglik: -6.9296e+02 - logprior: -8.9667e-01
Epoch 5/10
39/39 - 6s - loss: 692.1605 - loglik: -6.9134e+02 - logprior: -8.1700e-01
Epoch 6/10
39/39 - 6s - loss: 690.6111 - loglik: -6.8984e+02 - logprior: -7.5772e-01
Epoch 7/10
39/39 - 6s - loss: 688.6859 - loglik: -6.8797e+02 - logprior: -7.0742e-01
Epoch 8/10
39/39 - 6s - loss: 685.8986 - loglik: -6.8526e+02 - logprior: -6.2022e-01
Epoch 9/10
39/39 - 6s - loss: 681.5665 - loglik: -6.8100e+02 - logprior: -5.5006e-01
Epoch 10/10
39/39 - 6s - loss: 669.9368 - loglik: -6.6928e+02 - logprior: -6.2599e-01
Fitted a model with MAP estimate = -659.6737
Time for alignment: 168.2766
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 808.0336 - loglik: -8.0502e+02 - logprior: -3.0172e+00
Epoch 2/10
19/19 - 4s - loss: 738.1043 - loglik: -7.3687e+02 - logprior: -1.2354e+00
Epoch 3/10
19/19 - 4s - loss: 715.3377 - loglik: -7.1370e+02 - logprior: -1.6344e+00
Epoch 4/10
19/19 - 4s - loss: 709.8547 - loglik: -7.0838e+02 - logprior: -1.4775e+00
Epoch 5/10
19/19 - 4s - loss: 707.2296 - loglik: -7.0581e+02 - logprior: -1.4153e+00
Epoch 6/10
19/19 - 4s - loss: 707.1020 - loglik: -7.0571e+02 - logprior: -1.3832e+00
Epoch 7/10
19/19 - 4s - loss: 704.4947 - loglik: -7.0313e+02 - logprior: -1.3602e+00
Epoch 8/10
19/19 - 4s - loss: 703.2019 - loglik: -7.0185e+02 - logprior: -1.3467e+00
Epoch 9/10
19/19 - 4s - loss: 701.9384 - loglik: -7.0060e+02 - logprior: -1.3269e+00
Epoch 10/10
19/19 - 4s - loss: 700.1570 - loglik: -6.9883e+02 - logprior: -1.3112e+00
Fitted a model with MAP estimate = -696.5537
expansions: [(7, 2), (22, 1), (23, 2), (24, 1), (25, 2), (28, 1), (29, 2), (37, 2), (45, 2), (46, 3), (49, 1), (50, 2), (55, 1), (71, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 147 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 713.5428 - loglik: -7.1037e+02 - logprior: -3.1707e+00
Epoch 2/2
39/39 - 6s - loss: 701.7690 - loglik: -7.0022e+02 - logprior: -1.5498e+00
Fitted a model with MAP estimate = -696.7503
expansions: []
discards: [ 25  30  37  48  59  60  61  68 107 139]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 704.5745 - loglik: -7.0228e+02 - logprior: -2.2949e+00
Epoch 2/2
39/39 - 6s - loss: 700.6617 - loglik: -6.9946e+02 - logprior: -1.2014e+00
Fitted a model with MAP estimate = -696.7350
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 9s - loss: 702.0741 - loglik: -6.9985e+02 - logprior: -2.2291e+00
Epoch 2/10
39/39 - 6s - loss: 698.1255 - loglik: -6.9697e+02 - logprior: -1.1509e+00
Epoch 3/10
39/39 - 6s - loss: 696.3671 - loglik: -6.9534e+02 - logprior: -1.0282e+00
Epoch 4/10
39/39 - 6s - loss: 693.7769 - loglik: -6.9284e+02 - logprior: -9.2930e-01
Epoch 5/10
39/39 - 6s - loss: 692.9382 - loglik: -6.9209e+02 - logprior: -8.4577e-01
Epoch 6/10
39/39 - 6s - loss: 690.9700 - loglik: -6.9019e+02 - logprior: -7.7035e-01
Epoch 7/10
39/39 - 6s - loss: 688.3624 - loglik: -6.8765e+02 - logprior: -7.0252e-01
Epoch 8/10
39/39 - 6s - loss: 685.4454 - loglik: -6.8481e+02 - logprior: -6.2089e-01
Epoch 9/10
39/39 - 6s - loss: 681.8269 - loglik: -6.8126e+02 - logprior: -5.4810e-01
Epoch 10/10
39/39 - 6s - loss: 668.5524 - loglik: -6.6783e+02 - logprior: -6.9493e-01
Fitted a model with MAP estimate = -657.1139
Time for alignment: 167.0308
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 807.9227 - loglik: -8.0491e+02 - logprior: -3.0095e+00
Epoch 2/10
19/19 - 4s - loss: 736.2578 - loglik: -7.3503e+02 - logprior: -1.2260e+00
Epoch 3/10
19/19 - 4s - loss: 714.4037 - loglik: -7.1276e+02 - logprior: -1.6451e+00
Epoch 4/10
19/19 - 4s - loss: 711.2151 - loglik: -7.0975e+02 - logprior: -1.4611e+00
Epoch 5/10
19/19 - 4s - loss: 708.3575 - loglik: -7.0694e+02 - logprior: -1.4117e+00
Epoch 6/10
19/19 - 4s - loss: 705.9951 - loglik: -7.0461e+02 - logprior: -1.3785e+00
Epoch 7/10
19/19 - 4s - loss: 707.3139 - loglik: -7.0595e+02 - logprior: -1.3559e+00
Fitted a model with MAP estimate = -701.9712
expansions: [(7, 2), (21, 3), (22, 1), (25, 1), (30, 2), (35, 1), (37, 1), (46, 3), (49, 1), (50, 2), (57, 1), (71, 1), (84, 2), (85, 4), (86, 6), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 145 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 711.9226 - loglik: -7.0884e+02 - logprior: -3.0796e+00
Epoch 2/2
39/39 - 6s - loss: 701.5564 - loglik: -7.0013e+02 - logprior: -1.4223e+00
Fitted a model with MAP estimate = -696.7436
expansions: []
discards: [ 23  36  57  64 137]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 704.4574 - loglik: -7.0216e+02 - logprior: -2.2972e+00
Epoch 2/2
39/39 - 6s - loss: 700.5192 - loglik: -6.9930e+02 - logprior: -1.2216e+00
Fitted a model with MAP estimate = -696.6380
expansions: []
discards: [101]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 701.9125 - loglik: -6.9965e+02 - logprior: -2.2582e+00
Epoch 2/10
39/39 - 6s - loss: 697.9736 - loglik: -6.9681e+02 - logprior: -1.1617e+00
Epoch 3/10
39/39 - 6s - loss: 695.9374 - loglik: -6.9490e+02 - logprior: -1.0345e+00
Epoch 4/10
39/39 - 6s - loss: 694.3514 - loglik: -6.9341e+02 - logprior: -9.4236e-01
Epoch 5/10
39/39 - 6s - loss: 692.4210 - loglik: -6.9156e+02 - logprior: -8.5031e-01
Epoch 6/10
39/39 - 6s - loss: 689.7184 - loglik: -6.8892e+02 - logprior: -7.9160e-01
Epoch 7/10
39/39 - 6s - loss: 688.4293 - loglik: -6.8769e+02 - logprior: -7.2570e-01
Epoch 8/10
39/39 - 6s - loss: 685.2377 - loglik: -6.8456e+02 - logprior: -6.5926e-01
Epoch 9/10
39/39 - 6s - loss: 680.2800 - loglik: -6.7967e+02 - logprior: -5.8612e-01
Epoch 10/10
39/39 - 6s - loss: 667.0520 - loglik: -6.6626e+02 - logprior: -7.6340e-01
Fitted a model with MAP estimate = -656.0477
Time for alignment: 157.4246
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 808.4041 - loglik: -8.0540e+02 - logprior: -3.0077e+00
Epoch 2/10
19/19 - 4s - loss: 735.5841 - loglik: -7.3434e+02 - logprior: -1.2398e+00
Epoch 3/10
19/19 - 4s - loss: 713.7169 - loglik: -7.1203e+02 - logprior: -1.6817e+00
Epoch 4/10
19/19 - 4s - loss: 710.5721 - loglik: -7.0907e+02 - logprior: -1.4982e+00
Epoch 5/10
19/19 - 4s - loss: 709.2888 - loglik: -7.0785e+02 - logprior: -1.4396e+00
Epoch 6/10
19/19 - 4s - loss: 707.0327 - loglik: -7.0563e+02 - logprior: -1.4025e+00
Epoch 7/10
19/19 - 4s - loss: 703.8701 - loglik: -7.0248e+02 - logprior: -1.3836e+00
Epoch 8/10
19/19 - 4s - loss: 703.4788 - loglik: -7.0210e+02 - logprior: -1.3683e+00
Epoch 9/10
19/19 - 4s - loss: 701.5804 - loglik: -7.0022e+02 - logprior: -1.3527e+00
Epoch 10/10
19/19 - 4s - loss: 701.3565 - loglik: -7.0001e+02 - logprior: -1.3329e+00
Fitted a model with MAP estimate = -697.0129
expansions: [(7, 2), (21, 1), (23, 1), (24, 1), (25, 1), (28, 1), (29, 2), (36, 1), (37, 2), (46, 2), (49, 1), (50, 1), (55, 1), (71, 1), (84, 2), (85, 3), (86, 4), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 713.3268 - loglik: -7.1023e+02 - logprior: -3.1009e+00
Epoch 2/2
39/39 - 6s - loss: 701.8552 - loglik: -7.0041e+02 - logprior: -1.4402e+00
Fitted a model with MAP estimate = -697.1904
expansions: []
discards: [ 35  46 102 133]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 704.7173 - loglik: -7.0242e+02 - logprior: -2.2927e+00
Epoch 2/2
39/39 - 6s - loss: 700.3506 - loglik: -6.9915e+02 - logprior: -1.2028e+00
Fitted a model with MAP estimate = -696.8011
expansions: []
discards: [55]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 9s - loss: 702.1441 - loglik: -6.9992e+02 - logprior: -2.2218e+00
Epoch 2/10
39/39 - 6s - loss: 698.1157 - loglik: -6.9698e+02 - logprior: -1.1363e+00
Epoch 3/10
39/39 - 6s - loss: 696.3666 - loglik: -6.9536e+02 - logprior: -1.0054e+00
Epoch 4/10
39/39 - 6s - loss: 694.9701 - loglik: -6.9405e+02 - logprior: -9.1818e-01
Epoch 5/10
39/39 - 6s - loss: 692.3936 - loglik: -6.9155e+02 - logprior: -8.3419e-01
Epoch 6/10
39/39 - 6s - loss: 690.5669 - loglik: -6.8978e+02 - logprior: -7.7870e-01
Epoch 7/10
39/39 - 6s - loss: 688.2520 - loglik: -6.8753e+02 - logprior: -7.0736e-01
Epoch 8/10
39/39 - 6s - loss: 687.0269 - loglik: -6.8637e+02 - logprior: -6.3670e-01
Epoch 9/10
39/39 - 6s - loss: 681.0532 - loglik: -6.8047e+02 - logprior: -5.5993e-01
Epoch 10/10
39/39 - 6s - loss: 669.6808 - loglik: -6.6898e+02 - logprior: -6.7557e-01
Fitted a model with MAP estimate = -659.0170
Time for alignment: 165.1793
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 808.0664 - loglik: -8.0505e+02 - logprior: -3.0171e+00
Epoch 2/10
19/19 - 4s - loss: 737.4339 - loglik: -7.3620e+02 - logprior: -1.2316e+00
Epoch 3/10
19/19 - 4s - loss: 715.7421 - loglik: -7.1410e+02 - logprior: -1.6390e+00
Epoch 4/10
19/19 - 4s - loss: 710.8519 - loglik: -7.0938e+02 - logprior: -1.4702e+00
Epoch 5/10
19/19 - 4s - loss: 707.8837 - loglik: -7.0647e+02 - logprior: -1.4111e+00
Epoch 6/10
19/19 - 4s - loss: 706.9905 - loglik: -7.0561e+02 - logprior: -1.3757e+00
Epoch 7/10
19/19 - 4s - loss: 705.7454 - loglik: -7.0439e+02 - logprior: -1.3486e+00
Epoch 8/10
19/19 - 4s - loss: 704.8508 - loglik: -7.0351e+02 - logprior: -1.3325e+00
Epoch 9/10
19/19 - 4s - loss: 701.8193 - loglik: -7.0049e+02 - logprior: -1.3161e+00
Epoch 10/10
19/19 - 4s - loss: 699.6860 - loglik: -6.9838e+02 - logprior: -1.2928e+00
Fitted a model with MAP estimate = -697.2531
expansions: [(7, 2), (21, 3), (22, 1), (25, 1), (29, 2), (35, 1), (37, 2), (45, 1), (46, 3), (49, 1), (50, 2), (55, 1), (56, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 145 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 713.6317 - loglik: -7.1047e+02 - logprior: -3.1586e+00
Epoch 2/2
39/39 - 6s - loss: 701.8384 - loglik: -7.0032e+02 - logprior: -1.5197e+00
Fitted a model with MAP estimate = -696.9029
expansions: []
discards: [ 23  35  47  57  59  66 105 137]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 704.6752 - loglik: -7.0238e+02 - logprior: -2.2932e+00
Epoch 2/2
39/39 - 6s - loss: 700.5936 - loglik: -6.9939e+02 - logprior: -1.2068e+00
Fitted a model with MAP estimate = -696.7341
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 9s - loss: 701.7407 - loglik: -6.9950e+02 - logprior: -2.2382e+00
Epoch 2/10
39/39 - 6s - loss: 698.5609 - loglik: -6.9739e+02 - logprior: -1.1700e+00
Epoch 3/10
39/39 - 6s - loss: 696.5018 - loglik: -6.9546e+02 - logprior: -1.0367e+00
Epoch 4/10
39/39 - 6s - loss: 694.0239 - loglik: -6.9305e+02 - logprior: -9.7099e-01
Epoch 5/10
39/39 - 6s - loss: 692.6278 - loglik: -6.9174e+02 - logprior: -8.8096e-01
Epoch 6/10
39/39 - 6s - loss: 689.9700 - loglik: -6.8917e+02 - logprior: -7.9516e-01
Epoch 7/10
39/39 - 6s - loss: 688.2801 - loglik: -6.8755e+02 - logprior: -7.1790e-01
Epoch 8/10
39/39 - 6s - loss: 686.7987 - loglik: -6.8616e+02 - logprior: -6.2553e-01
Epoch 9/10
39/39 - 6s - loss: 681.7439 - loglik: -6.8117e+02 - logprior: -5.5602e-01
Epoch 10/10
39/39 - 6s - loss: 667.5653 - loglik: -6.6678e+02 - logprior: -7.5109e-01
Fitted a model with MAP estimate = -656.5338
Time for alignment: 165.9877
Computed alignments with likelihoods: ['-659.6737', '-657.1139', '-656.0477', '-659.0170', '-656.5338']
Best model has likelihood: -656.0477  (prior= -1.0147 )
time for generating output: 0.2138
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13365.projection.fasta
SP score = 0.11895003306271429
Training of 5 independent models on file PF00078.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6f7261e80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff676d0ae20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff66e3b99d0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 960.2417 - loglik: -9.5828e+02 - logprior: -1.9571e+00
Epoch 2/10
39/39 - 7s - loss: 919.9678 - loglik: -9.1867e+02 - logprior: -1.2939e+00
Epoch 3/10
39/39 - 7s - loss: 915.9630 - loglik: -9.1464e+02 - logprior: -1.3162e+00
Epoch 4/10
39/39 - 7s - loss: 913.1908 - loglik: -9.1189e+02 - logprior: -1.2908e+00
Epoch 5/10
39/39 - 7s - loss: 909.7911 - loglik: -9.0850e+02 - logprior: -1.2834e+00
Epoch 6/10
39/39 - 7s - loss: 905.0317 - loglik: -9.0375e+02 - logprior: -1.2626e+00
Epoch 7/10
39/39 - 7s - loss: 893.0125 - loglik: -8.9168e+02 - logprior: -1.3077e+00
Epoch 8/10
39/39 - 7s - loss: 770.0967 - loglik: -7.6670e+02 - logprior: -3.3576e+00
Epoch 9/10
39/39 - 7s - loss: 705.3564 - loglik: -7.0184e+02 - logprior: -3.4603e+00
Epoch 10/10
39/39 - 7s - loss: 695.3892 - loglik: -6.9198e+02 - logprior: -3.3465e+00
Fitted a model with MAP estimate = -691.4588
expansions: []
discards: [  3  32  37  38  56  57  58  59  60  61  62  63  64  65  66  67  68  69
  70  71  72  73  74  75  76  77  78  79  80  81  82  85  86  87  88 112
 124]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 1009.0284 - loglik: -1.0063e+03 - logprior: -2.7519e+00
Epoch 2/2
19/19 - 4s - loss: 993.3755 - loglik: -9.9299e+02 - logprior: -3.8320e-01
Fitted a model with MAP estimate = -992.2784
expansions: [(0, 82), (91, 85)]
discards: [ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49
 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73
 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 942.9105 - loglik: -9.4049e+02 - logprior: -2.4180e+00
Epoch 2/2
39/39 - 9s - loss: 903.1511 - loglik: -9.0215e+02 - logprior: -1.0038e+00
Fitted a model with MAP estimate = -899.6569
expansions: [(39, 1)]
discards: [  0  89  90  91  92  93  94 105 106 107 108 113 128 129 130]
Re-initialized the encoder parameters.
Fitting a model of length 155 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 908.9468 - loglik: -9.0652e+02 - logprior: -2.4307e+00
Epoch 2/10
39/39 - 8s - loss: 904.1589 - loglik: -9.0354e+02 - logprior: -6.1660e-01
Epoch 3/10
39/39 - 8s - loss: 902.0798 - loglik: -9.0151e+02 - logprior: -5.7146e-01
Epoch 4/10
39/39 - 8s - loss: 899.2083 - loglik: -8.9866e+02 - logprior: -5.3900e-01
Epoch 5/10
39/39 - 8s - loss: 896.3508 - loglik: -8.9582e+02 - logprior: -5.2219e-01
Epoch 6/10
39/39 - 8s - loss: 891.0975 - loglik: -8.9058e+02 - logprior: -5.0485e-01
Epoch 7/10
39/39 - 8s - loss: 870.9120 - loglik: -8.7013e+02 - logprior: -7.5762e-01
Epoch 8/10
39/39 - 8s - loss: 748.1188 - loglik: -7.4578e+02 - logprior: -2.2962e+00
Epoch 9/10
39/39 - 8s - loss: 700.9899 - loglik: -6.9869e+02 - logprior: -2.2487e+00
Epoch 10/10
39/39 - 8s - loss: 689.8746 - loglik: -6.8759e+02 - logprior: -2.2267e+00
Fitted a model with MAP estimate = -686.5137
Time for alignment: 222.5092
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 959.7245 - loglik: -9.5778e+02 - logprior: -1.9406e+00
Epoch 2/10
39/39 - 7s - loss: 922.0947 - loglik: -9.2090e+02 - logprior: -1.1930e+00
Epoch 3/10
39/39 - 7s - loss: 917.2148 - loglik: -9.1601e+02 - logprior: -1.2053e+00
Epoch 4/10
39/39 - 7s - loss: 914.4439 - loglik: -9.1324e+02 - logprior: -1.1962e+00
Epoch 5/10
39/39 - 7s - loss: 911.9547 - loglik: -9.1076e+02 - logprior: -1.1838e+00
Epoch 6/10
39/39 - 7s - loss: 906.6375 - loglik: -9.0545e+02 - logprior: -1.1745e+00
Epoch 7/10
39/39 - 7s - loss: 893.7573 - loglik: -8.9253e+02 - logprior: -1.2079e+00
Epoch 8/10
39/39 - 7s - loss: 767.0126 - loglik: -7.6390e+02 - logprior: -3.0701e+00
Epoch 9/10
39/39 - 7s - loss: 702.4675 - loglik: -6.9909e+02 - logprior: -3.3220e+00
Epoch 10/10
39/39 - 7s - loss: 694.1361 - loglik: -6.9076e+02 - logprior: -3.3220e+00
Fitted a model with MAP estimate = -690.8790
expansions: []
discards: [  3  36  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54
  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
  73  74  75  84  85  86  87 112 124]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 1013.3091 - loglik: -1.0106e+03 - logprior: -2.7115e+00
Epoch 2/2
19/19 - 3s - loss: 995.0975 - loglik: -9.9468e+02 - logprior: -4.1750e-01
Fitted a model with MAP estimate = -992.0061
expansions: [(0, 82), (83, 85)]
discards: [ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49
 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73
 74 75 76 77 78 79 80 81 82]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 942.9070 - loglik: -9.4075e+02 - logprior: -2.1522e+00
Epoch 2/2
39/39 - 9s - loss: 904.0035 - loglik: -9.0298e+02 - logprior: -1.0203e+00
Fitted a model with MAP estimate = -900.9045
expansions: [(59, 1)]
discards: [  0  89  90  91  92  93  94 105 106 107 108 109 110 111 112 125 127]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 911.2021 - loglik: -9.0873e+02 - logprior: -2.4720e+00
Epoch 2/10
39/39 - 8s - loss: 905.9705 - loglik: -9.0530e+02 - logprior: -6.7041e-01
Epoch 3/10
39/39 - 8s - loss: 904.1472 - loglik: -9.0356e+02 - logprior: -5.8109e-01
Epoch 4/10
39/39 - 8s - loss: 901.3964 - loglik: -9.0084e+02 - logprior: -5.5465e-01
Epoch 5/10
39/39 - 8s - loss: 898.1730 - loglik: -8.9766e+02 - logprior: -5.0595e-01
Epoch 6/10
39/39 - 8s - loss: 893.0881 - loglik: -8.9257e+02 - logprior: -5.0079e-01
Epoch 7/10
39/39 - 8s - loss: 870.9989 - loglik: -8.7019e+02 - logprior: -7.8448e-01
Epoch 8/10
39/39 - 8s - loss: 745.7916 - loglik: -7.4332e+02 - logprior: -2.4286e+00
Epoch 9/10
39/39 - 8s - loss: 699.3619 - loglik: -6.9691e+02 - logprior: -2.4001e+00
Epoch 10/10
39/39 - 8s - loss: 690.0135 - loglik: -6.8747e+02 - logprior: -2.4885e+00
Fitted a model with MAP estimate = -686.7710
Time for alignment: 220.6660
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 961.1241 - loglik: -9.5916e+02 - logprior: -1.9654e+00
Epoch 2/10
39/39 - 7s - loss: 921.0353 - loglik: -9.1976e+02 - logprior: -1.2763e+00
Epoch 3/10
39/39 - 7s - loss: 916.9532 - loglik: -9.1568e+02 - logprior: -1.2717e+00
Epoch 4/10
39/39 - 7s - loss: 913.7269 - loglik: -9.1247e+02 - logprior: -1.2550e+00
Epoch 5/10
39/39 - 7s - loss: 910.4195 - loglik: -9.0917e+02 - logprior: -1.2390e+00
Epoch 6/10
39/39 - 7s - loss: 905.6175 - loglik: -9.0439e+02 - logprior: -1.2111e+00
Epoch 7/10
39/39 - 7s - loss: 894.4893 - loglik: -8.9323e+02 - logprior: -1.2398e+00
Epoch 8/10
39/39 - 7s - loss: 771.8877 - loglik: -7.6870e+02 - logprior: -3.1530e+00
Epoch 9/10
39/39 - 7s - loss: 704.2551 - loglik: -7.0088e+02 - logprior: -3.3201e+00
Epoch 10/10
39/39 - 7s - loss: 695.3561 - loglik: -6.9202e+02 - logprior: -3.2811e+00
Fitted a model with MAP estimate = -691.5912
expansions: []
discards: [  3  38  43  60  84  85  86  87  88  89  90  91  92  93  94  95  96  97
 112 124]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 1012.9442 - loglik: -1.0095e+03 - logprior: -3.4585e+00
Epoch 2/2
19/19 - 4s - loss: 994.2089 - loglik: -9.9262e+02 - logprior: -1.5937e+00
Fitted a model with MAP estimate = -991.9463
expansions: [(0, 89), (108, 73)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  75
  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93
  94  95  96  97  98  99 100 101 102 103 104 105 106 107]
Re-initialized the encoder parameters.
Fitting a model of length 166 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 952.9411 - loglik: -9.5046e+02 - logprior: -2.4784e+00
Epoch 2/2
39/39 - 9s - loss: 915.2443 - loglik: -9.1384e+02 - logprior: -1.4025e+00
Fitted a model with MAP estimate = -911.7668
expansions: [(20, 1), (22, 1), (23, 2), (33, 2), (34, 1), (53, 1), (55, 2), (56, 3), (57, 7), (60, 1), (70, 2), (71, 1), (72, 1), (87, 14)]
discards: [  0 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145
 146 147 148 149 150 151 152 153 154 155]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 909.0050 - loglik: -9.0637e+02 - logprior: -2.6351e+00
Epoch 2/10
39/39 - 9s - loss: 899.3612 - loglik: -8.9844e+02 - logprior: -9.2368e-01
Epoch 3/10
39/39 - 9s - loss: 897.1841 - loglik: -8.9627e+02 - logprior: -9.1269e-01
Epoch 4/10
39/39 - 9s - loss: 893.4271 - loglik: -8.9251e+02 - logprior: -9.1323e-01
Epoch 5/10
39/39 - 9s - loss: 890.7620 - loglik: -8.8986e+02 - logprior: -8.8855e-01
Epoch 6/10
39/39 - 9s - loss: 885.5833 - loglik: -8.8468e+02 - logprior: -8.9232e-01
Epoch 7/10
39/39 - 9s - loss: 868.8600 - loglik: -8.6796e+02 - logprior: -8.8162e-01
Epoch 8/10
39/39 - 10s - loss: 754.3318 - loglik: -7.5210e+02 - logprior: -2.1990e+00
Epoch 9/10
39/39 - 10s - loss: 698.1721 - loglik: -6.9575e+02 - logprior: -2.3737e+00
Epoch 10/10
39/39 - 10s - loss: 687.5311 - loglik: -6.8524e+02 - logprior: -2.2292e+00
Fitted a model with MAP estimate = -683.2287
Time for alignment: 237.2433
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 959.6158 - loglik: -9.5767e+02 - logprior: -1.9472e+00
Epoch 2/10
39/39 - 7s - loss: 920.4689 - loglik: -9.1930e+02 - logprior: -1.1688e+00
Epoch 3/10
39/39 - 7s - loss: 917.5097 - loglik: -9.1636e+02 - logprior: -1.1517e+00
Epoch 4/10
39/39 - 7s - loss: 914.2228 - loglik: -9.1308e+02 - logprior: -1.1394e+00
Epoch 5/10
39/39 - 7s - loss: 911.2677 - loglik: -9.1012e+02 - logprior: -1.1337e+00
Epoch 6/10
39/39 - 7s - loss: 905.9536 - loglik: -9.0482e+02 - logprior: -1.1214e+00
Epoch 7/10
39/39 - 7s - loss: 892.0261 - loglik: -8.9081e+02 - logprior: -1.1904e+00
Epoch 8/10
39/39 - 7s - loss: 766.4512 - loglik: -7.6329e+02 - logprior: -3.1261e+00
Epoch 9/10
39/39 - 7s - loss: 703.2158 - loglik: -6.9985e+02 - logprior: -3.3170e+00
Epoch 10/10
39/39 - 7s - loss: 693.9349 - loglik: -6.9062e+02 - logprior: -3.2533e+00
Fitted a model with MAP estimate = -690.5399
expansions: []
discards: [  3  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  84  85  86
  87 112 124]
Re-initialized the encoder parameters.
Fitting a model of length 89 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 1012.2230 - loglik: -1.0095e+03 - logprior: -2.7052e+00
Epoch 2/2
19/19 - 4s - loss: 993.4100 - loglik: -9.9285e+02 - logprior: -5.6396e-01
Fitted a model with MAP estimate = -992.0023
expansions: [(0, 73), (89, 95)]
discards: [ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49
 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73
 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 949.8500 - loglik: -9.4597e+02 - logprior: -3.8841e+00
Epoch 2/2
39/39 - 9s - loss: 908.5421 - loglik: -9.0726e+02 - logprior: -1.2841e+00
Fitted a model with MAP estimate = -903.0597
expansions: [(130, 1)]
discards: [ 0  1  2 19 20 25 36 39 40 43 46 47 48 49 53 68 69 70 71 72 73 74 75 76
 77 78 79 80 81 82 83 84 85 86]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 921.5491 - loglik: -9.1899e+02 - logprior: -2.5591e+00
Epoch 2/10
39/39 - 7s - loss: 914.7734 - loglik: -9.1397e+02 - logprior: -8.0375e-01
Epoch 3/10
39/39 - 7s - loss: 912.7753 - loglik: -9.1205e+02 - logprior: -7.1918e-01
Epoch 4/10
39/39 - 7s - loss: 909.2496 - loglik: -9.0855e+02 - logprior: -6.9532e-01
Epoch 5/10
39/39 - 7s - loss: 906.2980 - loglik: -9.0562e+02 - logprior: -6.6666e-01
Epoch 6/10
39/39 - 7s - loss: 902.9170 - loglik: -9.0224e+02 - logprior: -6.6291e-01
Epoch 7/10
39/39 - 7s - loss: 884.8983 - loglik: -8.8403e+02 - logprior: -8.4792e-01
Epoch 8/10
39/39 - 7s - loss: 755.8394 - loglik: -7.5291e+02 - logprior: -2.8939e+00
Epoch 9/10
39/39 - 7s - loss: 703.4682 - loglik: -7.0049e+02 - logprior: -2.9258e+00
Epoch 10/10
39/39 - 7s - loss: 693.4378 - loglik: -6.9048e+02 - logprior: -2.9032e+00
Fitted a model with MAP estimate = -689.2371
Time for alignment: 212.4152
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 9s - loss: 959.3458 - loglik: -9.5741e+02 - logprior: -1.9399e+00
Epoch 2/10
39/39 - 7s - loss: 918.6100 - loglik: -9.1738e+02 - logprior: -1.2271e+00
Epoch 3/10
39/39 - 7s - loss: 914.0256 - loglik: -9.1280e+02 - logprior: -1.2196e+00
Epoch 4/10
39/39 - 7s - loss: 912.2213 - loglik: -9.1102e+02 - logprior: -1.1950e+00
Epoch 5/10
39/39 - 7s - loss: 907.6426 - loglik: -9.0645e+02 - logprior: -1.1807e+00
Epoch 6/10
39/39 - 7s - loss: 904.6234 - loglik: -9.0345e+02 - logprior: -1.1573e+00
Epoch 7/10
39/39 - 7s - loss: 892.8301 - loglik: -8.9165e+02 - logprior: -1.1582e+00
Epoch 8/10
39/39 - 7s - loss: 781.9775 - loglik: -7.7899e+02 - logprior: -2.9537e+00
Epoch 9/10
39/39 - 7s - loss: 702.2969 - loglik: -6.9872e+02 - logprior: -3.5286e+00
Epoch 10/10
39/39 - 7s - loss: 693.2391 - loglik: -6.8971e+02 - logprior: -3.4687e+00
Fitted a model with MAP estimate = -690.6641
expansions: []
discards: [  3  36  39  40  41  62  63  64  65  66  67  68  69  70  71  72  73  74
  75  83  84  85  86 112 113 114 115 116 117 118 119 120 121 124]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 1012.7073 - loglik: -1.0096e+03 - logprior: -3.1451e+00
Epoch 2/2
19/19 - 4s - loss: 993.7605 - loglik: -9.9320e+02 - logprior: -5.5612e-01
Fitted a model with MAP estimate = -991.9990
expansions: [(0, 84), (94, 84)]
discards: [ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49
 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73
 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 942.7464 - loglik: -9.4039e+02 - logprior: -2.3595e+00
Epoch 2/2
39/39 - 9s - loss: 903.4980 - loglik: -9.0248e+02 - logprior: -1.0168e+00
Fitted a model with MAP estimate = -899.9147
expansions: []
discards: [  0  27  89  90  91  92  93  94 108 109 110 114 123 130 140]
Re-initialized the encoder parameters.
Fitting a model of length 155 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 910.1642 - loglik: -9.0776e+02 - logprior: -2.4088e+00
Epoch 2/10
39/39 - 8s - loss: 905.2950 - loglik: -9.0470e+02 - logprior: -5.9267e-01
Epoch 3/10
39/39 - 8s - loss: 904.0775 - loglik: -9.0355e+02 - logprior: -5.2631e-01
Epoch 4/10
39/39 - 8s - loss: 902.2525 - loglik: -9.0175e+02 - logprior: -5.0072e-01
Epoch 5/10
39/39 - 8s - loss: 896.7794 - loglik: -8.9630e+02 - logprior: -4.7367e-01
Epoch 6/10
39/39 - 8s - loss: 893.8049 - loglik: -8.9331e+02 - logprior: -4.7511e-01
Epoch 7/10
39/39 - 8s - loss: 869.6071 - loglik: -8.6879e+02 - logprior: -7.9172e-01
Epoch 8/10
39/39 - 8s - loss: 743.2534 - loglik: -7.4092e+02 - logprior: -2.2981e+00
Epoch 9/10
39/39 - 8s - loss: 697.6755 - loglik: -6.9543e+02 - logprior: -2.1969e+00
Epoch 10/10
39/39 - 8s - loss: 689.0618 - loglik: -6.8677e+02 - logprior: -2.2366e+00
Fitted a model with MAP estimate = -686.0580
Time for alignment: 220.8399
Computed alignments with likelihoods: ['-686.5137', '-686.7710', '-683.2287', '-689.2371', '-686.0580']
Best model has likelihood: -683.2287  (prior= -2.1806 )
time for generating output: 0.1996
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00078.projection.fasta
SP score = 0.3734555599840574
Training of 5 independent models on file PF00150.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6637ec490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6b2237130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff66d2a3970>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 1618.9867 - loglik: -1.6176e+03 - logprior: -1.4046e+00
Epoch 2/10
39/39 - 18s - loss: 1550.2142 - loglik: -1.5496e+03 - logprior: -5.9488e-01
Epoch 3/10
39/39 - 18s - loss: 1541.7090 - loglik: -1.5410e+03 - logprior: -6.8665e-01
Epoch 4/10
39/39 - 18s - loss: 1536.3220 - loglik: -1.5357e+03 - logprior: -5.9834e-01
Epoch 5/10
39/39 - 18s - loss: 1532.8259 - loglik: -1.5323e+03 - logprior: -5.6022e-01
Epoch 6/10
39/39 - 18s - loss: 1528.2614 - loglik: -1.5277e+03 - logprior: -5.4535e-01
Epoch 7/10
39/39 - 18s - loss: 1512.4440 - loglik: -1.5119e+03 - logprior: -5.3083e-01
Epoch 8/10
39/39 - 18s - loss: 1380.1660 - loglik: -1.3781e+03 - logprior: -2.0559e+00
Epoch 9/10
39/39 - 19s - loss: 1185.3766 - loglik: -1.1811e+03 - logprior: -4.1816e+00
Epoch 10/10
39/39 - 18s - loss: 1163.5120 - loglik: -1.1597e+03 - logprior: -3.7718e+00
Fitted a model with MAP estimate = -1159.0159
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  95  96 106 107 108 109 110 111 112 113 114 115 116 117 118
 164 165 166]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 1682.3422 - loglik: -1.6802e+03 - logprior: -2.1512e+00
Epoch 2/2
39/39 - 10s - loss: 1667.9526 - loglik: -1.6672e+03 - logprior: -7.0738e-01
Fitted a model with MAP estimate = -1662.1879
expansions: [(0, 55), (18, 168), (19, 3), (136, 46)]
discards: [  1   2   7   8   9  10  20  21  22  23  24  25  26  27  28  29  30  31
  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49
  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67
  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85
  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103
 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121
 122 123 124 125 126 127 128 129 130 131 132 133 134 135]
Re-initialized the encoder parameters.
Fitting a model of length 286 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 1612.3959 - loglik: -1.6101e+03 - logprior: -2.2899e+00
Epoch 2/2
39/39 - 25s - loss: 1541.8792 - loglik: -1.5407e+03 - logprior: -1.2270e+00
Fitted a model with MAP estimate = -1533.1239
expansions: [(0, 5), (90, 2), (136, 4), (137, 2), (237, 2), (238, 1), (239, 8), (242, 1), (243, 3), (244, 2), (258, 1), (262, 2), (271, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  55  56  57 173 174 175 176 177]
Re-initialized the encoder parameters.
Fitting a model of length 271 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 1540.0594 - loglik: -1.5380e+03 - logprior: -2.0417e+00
Epoch 2/10
39/39 - 24s - loss: 1530.9797 - loglik: -1.5307e+03 - logprior: -2.7868e-01
Epoch 3/10
39/39 - 24s - loss: 1528.9238 - loglik: -1.5287e+03 - logprior: -2.1960e-01
Epoch 4/10
39/39 - 24s - loss: 1524.8463 - loglik: -1.5247e+03 - logprior: -1.7261e-01
Epoch 5/10
39/39 - 24s - loss: 1520.7417 - loglik: -1.5206e+03 - logprior: -1.2508e-01
Epoch 6/10
39/39 - 23s - loss: 1515.4182 - loglik: -1.5153e+03 - logprior: -6.0195e-02
Epoch 7/10
39/39 - 23s - loss: 1493.4490 - loglik: -1.4934e+03 - logprior: -3.9387e-02
Epoch 8/10
39/39 - 23s - loss: 1346.5275 - loglik: -1.3448e+03 - logprior: -1.6847e+00
Epoch 9/10
39/39 - 23s - loss: 1193.4027 - loglik: -1.1901e+03 - logprior: -3.2094e+00
Epoch 10/10
39/39 - 23s - loss: 1165.6384 - loglik: -1.1627e+03 - logprior: -2.8950e+00
Fitted a model with MAP estimate = -1159.1483
Time for alignment: 597.3841
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 1620.1813 - loglik: -1.6188e+03 - logprior: -1.4077e+00
Epoch 2/10
39/39 - 18s - loss: 1551.7589 - loglik: -1.5512e+03 - logprior: -5.9478e-01
Epoch 3/10
39/39 - 18s - loss: 1543.2047 - loglik: -1.5426e+03 - logprior: -6.4220e-01
Epoch 4/10
39/39 - 18s - loss: 1538.1262 - loglik: -1.5375e+03 - logprior: -5.9171e-01
Epoch 5/10
39/39 - 18s - loss: 1533.2612 - loglik: -1.5326e+03 - logprior: -6.0899e-01
Epoch 6/10
39/39 - 18s - loss: 1529.2301 - loglik: -1.5286e+03 - logprior: -5.8994e-01
Epoch 7/10
39/39 - 18s - loss: 1510.7493 - loglik: -1.5102e+03 - logprior: -5.4947e-01
Epoch 8/10
39/39 - 18s - loss: 1371.0823 - loglik: -1.3690e+03 - logprior: -2.0675e+00
Epoch 9/10
39/39 - 18s - loss: 1184.4708 - loglik: -1.1803e+03 - logprior: -4.1539e+00
Epoch 10/10
39/39 - 18s - loss: 1163.4335 - loglik: -1.1596e+03 - logprior: -3.7253e+00
Fitted a model with MAP estimate = -1159.2824
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75 108 109 110 111 112 113 114 165 166 167 190 191]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 1689.8057 - loglik: -1.6874e+03 - logprior: -2.3957e+00
Epoch 2/2
39/39 - 10s - loss: 1665.8881 - loglik: -1.6649e+03 - logprior: -9.5131e-01
Fitted a model with MAP estimate = -1658.2979
expansions: [(0, 77), (141, 143)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  32  33  34  35  36
  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54
  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90
  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108
 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126
 127 128 129 130 131 132 133 134 135 136 137 138 139 140]
Re-initialized the encoder parameters.
Fitting a model of length 221 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1618.5143 - loglik: -1.6170e+03 - logprior: -1.4895e+00
Epoch 2/2
39/39 - 17s - loss: 1552.7821 - loglik: -1.5522e+03 - logprior: -5.8505e-01
Fitted a model with MAP estimate = -1544.7600
expansions: [(0, 4), (40, 1), (43, 1), (51, 2), (87, 2), (88, 1), (89, 1), (93, 15), (94, 3), (96, 1), (100, 2), (115, 2), (116, 2), (117, 1), (119, 1), (134, 2), (150, 3), (170, 2), (171, 1), (174, 5), (177, 4), (200, 2), (204, 2), (205, 1), (207, 1), (208, 1)]
discards: [77 78 79 80]
Re-initialized the encoder parameters.
Fitting a model of length 280 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 1542.5586 - loglik: -1.5405e+03 - logprior: -2.0374e+00
Epoch 2/10
39/39 - 24s - loss: 1529.6846 - loglik: -1.5292e+03 - logprior: -5.1143e-01
Epoch 3/10
39/39 - 25s - loss: 1527.3356 - loglik: -1.5269e+03 - logprior: -4.8090e-01
Epoch 4/10
39/39 - 25s - loss: 1522.6935 - loglik: -1.5223e+03 - logprior: -4.3246e-01
Epoch 5/10
39/39 - 24s - loss: 1519.0308 - loglik: -1.5186e+03 - logprior: -4.0526e-01
Epoch 6/10
39/39 - 25s - loss: 1514.3258 - loglik: -1.5139e+03 - logprior: -3.8338e-01
Epoch 7/10
39/39 - 25s - loss: 1495.3988 - loglik: -1.4950e+03 - logprior: -4.1980e-01
Epoch 8/10
39/39 - 24s - loss: 1360.5269 - loglik: -1.3591e+03 - logprior: -1.4200e+00
Epoch 9/10
39/39 - 25s - loss: 1186.9635 - loglik: -1.1838e+03 - logprior: -3.1338e+00
Epoch 10/10
39/39 - 24s - loss: 1160.7419 - loglik: -1.1578e+03 - logprior: -2.8345e+00
Fitted a model with MAP estimate = -1155.6721
Time for alignment: 580.8983
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1620.1331 - loglik: -1.6187e+03 - logprior: -1.4227e+00
Epoch 2/10
39/39 - 18s - loss: 1552.1630 - loglik: -1.5515e+03 - logprior: -6.7961e-01
Epoch 3/10
39/39 - 18s - loss: 1542.6989 - loglik: -1.5419e+03 - logprior: -7.9679e-01
Epoch 4/10
39/39 - 18s - loss: 1538.2681 - loglik: -1.5375e+03 - logprior: -7.6935e-01
Epoch 5/10
39/39 - 18s - loss: 1534.3533 - loglik: -1.5336e+03 - logprior: -7.4239e-01
Epoch 6/10
39/39 - 18s - loss: 1530.0789 - loglik: -1.5293e+03 - logprior: -7.2428e-01
Epoch 7/10
39/39 - 18s - loss: 1510.6954 - loglik: -1.5099e+03 - logprior: -7.2723e-01
Epoch 8/10
39/39 - 18s - loss: 1364.1188 - loglik: -1.3616e+03 - logprior: -2.4746e+00
Epoch 9/10
39/39 - 18s - loss: 1180.9354 - loglik: -1.1767e+03 - logprior: -4.2159e+00
Epoch 10/10
39/39 - 18s - loss: 1162.5959 - loglik: -1.1588e+03 - logprior: -3.7442e+00
Fitted a model with MAP estimate = -1159.1570
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69 107 108
 109 110 111 112 113 114 115 116 165 166 167]
Re-initialized the encoder parameters.
Fitting a model of length 146 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 1685.7815 - loglik: -1.6841e+03 - logprior: -1.6596e+00
Epoch 2/2
39/39 - 10s - loss: 1669.1063 - loglik: -1.6691e+03 - logprior: -1.8540e-02
Fitted a model with MAP estimate = -1664.5379
expansions: [(0, 130), (146, 143)]
discards: [  4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21
  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39
  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57
  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75
  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93
  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129
 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145]
Re-initialized the encoder parameters.
Fitting a model of length 277 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 1610.2771 - loglik: -1.6083e+03 - logprior: -1.9865e+00
Epoch 2/2
39/39 - 24s - loss: 1539.6129 - loglik: -1.5391e+03 - logprior: -5.5825e-01
Fitted a model with MAP estimate = -1531.8063
expansions: [(0, 3), (103, 4), (104, 2), (182, 2), (264, 1)]
discards: [ 38  39  40  41 130 131 132 133 184 186 198 199]
Re-initialized the encoder parameters.
Fitting a model of length 277 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 1537.1699 - loglik: -1.5346e+03 - logprior: -2.6096e+00
Epoch 2/10
39/39 - 24s - loss: 1529.8628 - loglik: -1.5293e+03 - logprior: -5.9210e-01
Epoch 3/10
39/39 - 24s - loss: 1527.2268 - loglik: -1.5268e+03 - logprior: -4.2546e-01
Epoch 4/10
39/39 - 24s - loss: 1523.7976 - loglik: -1.5234e+03 - logprior: -3.7487e-01
Epoch 5/10
39/39 - 24s - loss: 1519.9224 - loglik: -1.5196e+03 - logprior: -3.2235e-01
Epoch 6/10
39/39 - 24s - loss: 1515.8174 - loglik: -1.5155e+03 - logprior: -2.7648e-01
Epoch 7/10
39/39 - 24s - loss: 1494.7275 - loglik: -1.4944e+03 - logprior: -2.6395e-01
Epoch 8/10
39/39 - 24s - loss: 1363.7924 - loglik: -1.3624e+03 - logprior: -1.3990e+00
Epoch 9/10
39/39 - 24s - loss: 1189.7241 - loglik: -1.1866e+03 - logprior: -3.0579e+00
Epoch 10/10
39/39 - 24s - loss: 1162.4735 - loglik: -1.1596e+03 - logprior: -2.8289e+00
Fitted a model with MAP estimate = -1157.1434
Time for alignment: 600.6918
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1620.2090 - loglik: -1.6188e+03 - logprior: -1.4254e+00
Epoch 2/10
39/39 - 18s - loss: 1551.5502 - loglik: -1.5509e+03 - logprior: -6.6275e-01
Epoch 3/10
39/39 - 18s - loss: 1542.7419 - loglik: -1.5420e+03 - logprior: -7.7139e-01
Epoch 4/10
39/39 - 18s - loss: 1536.9138 - loglik: -1.5361e+03 - logprior: -7.7924e-01
Epoch 5/10
39/39 - 18s - loss: 1533.2841 - loglik: -1.5325e+03 - logprior: -7.6928e-01
Epoch 6/10
39/39 - 18s - loss: 1529.1816 - loglik: -1.5284e+03 - logprior: -7.6062e-01
Epoch 7/10
39/39 - 18s - loss: 1510.4895 - loglik: -1.5097e+03 - logprior: -7.3329e-01
Epoch 8/10
39/39 - 18s - loss: 1369.2723 - loglik: -1.3669e+03 - logprior: -2.2915e+00
Epoch 9/10
39/39 - 18s - loss: 1183.9855 - loglik: -1.1798e+03 - logprior: -4.1616e+00
Epoch 10/10
39/39 - 18s - loss: 1163.3557 - loglik: -1.1596e+03 - logprior: -3.7181e+00
Fitted a model with MAP estimate = -1159.1893
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70 106
 107 108 109 110 111 112 113 114 115 140 164 165 166 167 190 191 192 193
 194 195 196 197 198 199 200 201 202 203]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 1683.0947 - loglik: -1.6810e+03 - logprior: -2.0941e+00
Epoch 2/2
39/39 - 9s - loss: 1667.5934 - loglik: -1.6671e+03 - logprior: -4.6374e-01
Fitted a model with MAP estimate = -1659.9518
expansions: [(0, 53), (18, 12), (19, 132), (129, 69)]
discards: [ 24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41
  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59
  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77
  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95
  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113
 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 1605.9691 - loglik: -1.6036e+03 - logprior: -2.3701e+00
Epoch 2/2
39/39 - 26s - loss: 1539.9130 - loglik: -1.5388e+03 - logprior: -1.1096e+00
Fitted a model with MAP estimate = -1532.3632
expansions: [(79, 3), (80, 2), (168, 1), (169, 2), (171, 2), (175, 1), (176, 1), (180, 1), (194, 1), (278, 1)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18
  19  20  21  22  23  24  25  27  28  29  30  31  44  45  46  47  48  49
  50  51  52  53  54  55  64  65  66  67  68  69 108 109 110 111 112 216
 217 218 219 220 221 222 223 224 225 226 227 228 229]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 1548.1455 - loglik: -1.5467e+03 - logprior: -1.4206e+00
Epoch 2/10
39/39 - 19s - loss: 1538.7849 - loglik: -1.5385e+03 - logprior: -2.7087e-01
Epoch 3/10
39/39 - 19s - loss: 1536.0917 - loglik: -1.5359e+03 - logprior: -1.7021e-01
Epoch 4/10
39/39 - 19s - loss: 1532.9670 - loglik: -1.5329e+03 - logprior: -1.1230e-01
Epoch 5/10
39/39 - 19s - loss: 1528.8666 - loglik: -1.5288e+03 - logprior: -7.9379e-02
Epoch 6/10
39/39 - 20s - loss: 1524.1885 - loglik: -1.5241e+03 - logprior: -3.9561e-02
Epoch 7/10
39/39 - 19s - loss: 1507.3455 - loglik: -1.5073e+03 - logprior: -3.4287e-02
Epoch 8/10
39/39 - 19s - loss: 1363.5822 - loglik: -1.3614e+03 - logprior: -2.1662e+00
Epoch 9/10
39/39 - 19s - loss: 1191.3812 - loglik: -1.1870e+03 - logprior: -4.3328e+00
Epoch 10/10
39/39 - 19s - loss: 1165.4729 - loglik: -1.1617e+03 - logprior: -3.7267e+00
Fitted a model with MAP estimate = -1160.4890
Time for alignment: 553.8942
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1621.4268 - loglik: -1.6200e+03 - logprior: -1.4190e+00
Epoch 2/10
39/39 - 18s - loss: 1552.6437 - loglik: -1.5519e+03 - logprior: -7.0685e-01
Epoch 3/10
39/39 - 18s - loss: 1544.5388 - loglik: -1.5437e+03 - logprior: -8.6265e-01
Epoch 4/10
39/39 - 18s - loss: 1539.8754 - loglik: -1.5390e+03 - logprior: -8.2399e-01
Epoch 5/10
39/39 - 18s - loss: 1534.6880 - loglik: -1.5339e+03 - logprior: -8.2258e-01
Epoch 6/10
39/39 - 18s - loss: 1530.7118 - loglik: -1.5299e+03 - logprior: -8.1347e-01
Epoch 7/10
39/39 - 18s - loss: 1511.6202 - loglik: -1.5108e+03 - logprior: -7.8357e-01
Epoch 8/10
39/39 - 18s - loss: 1371.8575 - loglik: -1.3695e+03 - logprior: -2.3517e+00
Epoch 9/10
39/39 - 18s - loss: 1183.2024 - loglik: -1.1789e+03 - logprior: -4.2853e+00
Epoch 10/10
39/39 - 18s - loss: 1162.7430 - loglik: -1.1589e+03 - logprior: -3.7702e+00
Fitted a model with MAP estimate = -1158.7944
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70 106
 107 108 109 110 111 112 159 160 161 166 167 168]
Re-initialized the encoder parameters.
Fitting a model of length 145 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 1685.4325 - loglik: -1.6836e+03 - logprior: -1.7918e+00
Epoch 2/2
39/39 - 10s - loss: 1667.3076 - loglik: -1.6673e+03 - logprior: 0.0402
Fitted a model with MAP estimate = -1663.2427
expansions: [(0, 123), (145, 150)]
discards: [  4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21
  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39
  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57
  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75
  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93
  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129
 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]
Re-initialized the encoder parameters.
Fitting a model of length 277 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 1613.3801 - loglik: -1.6104e+03 - logprior: -3.0007e+00
Epoch 2/2
39/39 - 24s - loss: 1540.1145 - loglik: -1.5393e+03 - logprior: -8.2124e-01
Fitted a model with MAP estimate = -1531.9751
expansions: [(0, 3), (17, 4), (100, 4), (101, 7), (144, 1), (153, 1), (157, 1), (183, 2), (262, 1)]
discards: [  9  10  11  12  13  14  15  86 116 117 118 119 120 121 122 123 124 125
 187 190 191 192 193]
Re-initialized the encoder parameters.
Fitting a model of length 278 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 1536.5249 - loglik: -1.5336e+03 - logprior: -2.8756e+00
Epoch 2/10
39/39 - 25s - loss: 1527.8583 - loglik: -1.5272e+03 - logprior: -7.0329e-01
Epoch 3/10
39/39 - 24s - loss: 1525.3040 - loglik: -1.5250e+03 - logprior: -3.1371e-01
Epoch 4/10
39/39 - 25s - loss: 1522.1543 - loglik: -1.5219e+03 - logprior: -2.5831e-01
Epoch 5/10
39/39 - 24s - loss: 1517.5814 - loglik: -1.5173e+03 - logprior: -2.2634e-01
Epoch 6/10
39/39 - 24s - loss: 1514.3643 - loglik: -1.5142e+03 - logprior: -1.8104e-01
Epoch 7/10
39/39 - 24s - loss: 1494.2982 - loglik: -1.4941e+03 - logprior: -1.6334e-01
Epoch 8/10
39/39 - 24s - loss: 1366.8127 - loglik: -1.3656e+03 - logprior: -1.1905e+00
Epoch 9/10
39/39 - 24s - loss: 1190.0125 - loglik: -1.1870e+03 - logprior: -2.9401e+00
Epoch 10/10
39/39 - 24s - loss: 1160.9132 - loglik: -1.1582e+03 - logprior: -2.6973e+00
Fitted a model with MAP estimate = -1155.7897
Time for alignment: 600.3348
Computed alignments with likelihoods: ['-1159.0159', '-1155.6721', '-1157.1434', '-1159.1893', '-1155.7897']
Best model has likelihood: -1155.6721  (prior= -2.7735 )
time for generating output: 0.3493
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00150.projection.fasta
SP score = 0.017728119180633148
Training of 5 independent models on file PF13561.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6f762edc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6632322b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6e5bb7910>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 1220.7404 - loglik: -1.2189e+03 - logprior: -1.8069e+00
Epoch 2/10
39/39 - 9s - loss: 1127.0981 - loglik: -1.1255e+03 - logprior: -1.5765e+00
Epoch 3/10
39/39 - 9s - loss: 1118.0244 - loglik: -1.1164e+03 - logprior: -1.5746e+00
Epoch 4/10
39/39 - 9s - loss: 1113.7479 - loglik: -1.1123e+03 - logprior: -1.4868e+00
Epoch 5/10
39/39 - 9s - loss: 1108.1412 - loglik: -1.1067e+03 - logprior: -1.4806e+00
Epoch 6/10
39/39 - 9s - loss: 1099.0197 - loglik: -1.0975e+03 - logprior: -1.4797e+00
Epoch 7/10
39/39 - 10s - loss: 1077.1666 - loglik: -1.0756e+03 - logprior: -1.5266e+00
Epoch 8/10
39/39 - 10s - loss: 1034.8414 - loglik: -1.0332e+03 - logprior: -1.5602e+00
Epoch 9/10
39/39 - 10s - loss: 938.4583 - loglik: -9.3469e+02 - logprior: -3.7162e+00
Epoch 10/10
39/39 - 10s - loss: 894.4150 - loglik: -8.9014e+02 - logprior: -4.2137e+00
Fitted a model with MAP estimate = -887.7308
expansions: []
discards: [118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135
 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153
 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 1339.3531 - loglik: -1.3367e+03 - logprior: -2.6892e+00
Epoch 2/2
19/19 - 4s - loss: 1316.4139 - loglik: -1.3164e+03 - logprior: -5.6068e-02
Fitted a model with MAP estimate = -1300.8007
expansions: [(118, 199)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 1210.3431 - loglik: -1.2079e+03 - logprior: -2.4284e+00
Epoch 2/2
39/39 - 10s - loss: 1116.3029 - loglik: -1.1150e+03 - logprior: -1.3412e+00
Fitted a model with MAP estimate = -1103.8369
expansions: [(12, 1), (19, 1), (26, 1), (29, 1), (35, 1), (36, 1), (38, 1), (39, 1), (41, 1), (49, 1), (50, 1), (53, 1), (72, 2), (75, 1), (83, 1), (93, 1), (96, 1), (101, 1), (116, 1), (118, 2), (120, 2), (138, 1), (159, 2), (160, 1), (164, 1), (166, 1), (167, 2), (168, 2), (181, 1), (195, 1), (197, 1), (202, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 1102.7568 - loglik: -1.1003e+03 - logprior: -2.4304e+00
Epoch 2/10
39/39 - 13s - loss: 1093.1642 - loglik: -1.0925e+03 - logprior: -6.5954e-01
Epoch 3/10
39/39 - 13s - loss: 1089.8704 - loglik: -1.0894e+03 - logprior: -4.7459e-01
Epoch 4/10
39/39 - 13s - loss: 1085.1848 - loglik: -1.0848e+03 - logprior: -4.1628e-01
Epoch 5/10
39/39 - 13s - loss: 1077.7042 - loglik: -1.0773e+03 - logprior: -3.8083e-01
Epoch 6/10
39/39 - 13s - loss: 1062.0439 - loglik: -1.0617e+03 - logprior: -3.7359e-01
Epoch 7/10
39/39 - 13s - loss: 1017.3182 - loglik: -1.0168e+03 - logprior: -5.2279e-01
Epoch 8/10
39/39 - 13s - loss: 941.4182 - loglik: -9.4052e+02 - logprior: -8.5650e-01
Epoch 9/10
39/39 - 13s - loss: 888.2845 - loglik: -8.8732e+02 - logprior: -9.1271e-01
Epoch 10/10
39/39 - 13s - loss: 877.6200 - loglik: -8.7675e+02 - logprior: -8.1081e-01
Fitted a model with MAP estimate = -874.7193
Time for alignment: 322.6000
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 1219.6979 - loglik: -1.2179e+03 - logprior: -1.8150e+00
Epoch 2/10
39/39 - 9s - loss: 1126.0120 - loglik: -1.1244e+03 - logprior: -1.5940e+00
Epoch 3/10
39/39 - 10s - loss: 1117.1566 - loglik: -1.1156e+03 - logprior: -1.6029e+00
Epoch 4/10
39/39 - 9s - loss: 1112.1602 - loglik: -1.1106e+03 - logprior: -1.5280e+00
Epoch 5/10
39/39 - 9s - loss: 1106.8695 - loglik: -1.1054e+03 - logprior: -1.5054e+00
Epoch 6/10
39/39 - 9s - loss: 1098.2573 - loglik: -1.0967e+03 - logprior: -1.5038e+00
Epoch 7/10
39/39 - 10s - loss: 1076.2454 - loglik: -1.0747e+03 - logprior: -1.5522e+00
Epoch 8/10
39/39 - 10s - loss: 1037.0488 - loglik: -1.0354e+03 - logprior: -1.5650e+00
Epoch 9/10
39/39 - 10s - loss: 939.9230 - loglik: -9.3631e+02 - logprior: -3.5574e+00
Epoch 10/10
39/39 - 10s - loss: 894.9117 - loglik: -8.9039e+02 - logprior: -4.4631e+00
Fitted a model with MAP estimate = -887.7801
expansions: []
discards: [118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135
 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153
 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 1343.0469 - loglik: -1.3404e+03 - logprior: -2.6558e+00
Epoch 2/2
19/19 - 4s - loss: 1317.1696 - loglik: -1.3171e+03 - logprior: -5.7673e-02
Fitted a model with MAP estimate = -1299.2363
expansions: [(118, 192)]
discards: [  0  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33
  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51
  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69
  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87
  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105
 106 107 108 109 110 111 112 113 114 115 116 117]
Re-initialized the encoder parameters.
Fitting a model of length 208 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 1201.8335 - loglik: -1.1991e+03 - logprior: -2.7169e+00
Epoch 2/2
39/39 - 11s - loss: 1115.2629 - loglik: -1.1132e+03 - logprior: -2.0713e+00
Fitted a model with MAP estimate = -1102.7190
expansions: [(3, 1), (38, 1), (39, 1), (41, 1), (42, 1), (44, 1), (54, 1), (59, 1), (75, 1), (76, 2), (87, 1), (97, 1), (100, 1), (105, 1), (120, 1), (122, 2), (125, 1), (147, 1), (166, 2), (169, 1), (171, 1), (172, 1), (173, 1), (186, 1), (196, 1), (201, 1), (202, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 1101.6838 - loglik: -1.0992e+03 - logprior: -2.4611e+00
Epoch 2/10
39/39 - 13s - loss: 1094.0740 - loglik: -1.0929e+03 - logprior: -1.1655e+00
Epoch 3/10
39/39 - 13s - loss: 1089.2483 - loglik: -1.0888e+03 - logprior: -4.4419e-01
Epoch 4/10
39/39 - 13s - loss: 1084.6498 - loglik: -1.0843e+03 - logprior: -3.9073e-01
Epoch 5/10
39/39 - 13s - loss: 1077.5663 - loglik: -1.0772e+03 - logprior: -3.4858e-01
Epoch 6/10
39/39 - 13s - loss: 1062.5061 - loglik: -1.0622e+03 - logprior: -3.2929e-01
Epoch 7/10
39/39 - 13s - loss: 1019.9619 - loglik: -1.0195e+03 - logprior: -4.8046e-01
Epoch 8/10
39/39 - 13s - loss: 942.1550 - loglik: -9.4130e+02 - logprior: -8.1438e-01
Epoch 9/10
39/39 - 13s - loss: 889.5769 - loglik: -8.8868e+02 - logprior: -8.4430e-01
Epoch 10/10
39/39 - 13s - loss: 879.4865 - loglik: -8.7872e+02 - logprior: -7.0367e-01
Fitted a model with MAP estimate = -875.9981
Time for alignment: 322.5452
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 1220.0264 - loglik: -1.2182e+03 - logprior: -1.8272e+00
Epoch 2/10
39/39 - 10s - loss: 1126.8154 - loglik: -1.1252e+03 - logprior: -1.5883e+00
Epoch 3/10
39/39 - 9s - loss: 1118.0256 - loglik: -1.1164e+03 - logprior: -1.6116e+00
Epoch 4/10
39/39 - 9s - loss: 1112.9963 - loglik: -1.1114e+03 - logprior: -1.5623e+00
Epoch 5/10
39/39 - 10s - loss: 1107.8634 - loglik: -1.1063e+03 - logprior: -1.5320e+00
Epoch 6/10
39/39 - 9s - loss: 1098.8340 - loglik: -1.0973e+03 - logprior: -1.5196e+00
Epoch 7/10
39/39 - 10s - loss: 1077.1334 - loglik: -1.0755e+03 - logprior: -1.5711e+00
Epoch 8/10
39/39 - 10s - loss: 1035.7098 - loglik: -1.0341e+03 - logprior: -1.5875e+00
Epoch 9/10
39/39 - 10s - loss: 940.2997 - loglik: -9.3677e+02 - logprior: -3.4775e+00
Epoch 10/10
39/39 - 10s - loss: 895.0233 - loglik: -8.9078e+02 - logprior: -4.1792e+00
Fitted a model with MAP estimate = -887.6399
expansions: []
discards: [109 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134
 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152
 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170
 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 1343.8877 - loglik: -1.3412e+03 - logprior: -2.7325e+00
Epoch 2/2
19/19 - 4s - loss: 1314.8153 - loglik: -1.3146e+03 - logprior: -2.0652e-01
Fitted a model with MAP estimate = -1300.2675
expansions: [(117, 185)]
discards: [  0  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26
  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44
  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62
  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80
  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98
  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116]
Re-initialized the encoder parameters.
Fitting a model of length 194 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 1213.0360 - loglik: -1.2102e+03 - logprior: -2.8095e+00
Epoch 2/2
39/39 - 10s - loss: 1122.2588 - loglik: -1.1200e+03 - logprior: -2.2359e+00
Fitted a model with MAP estimate = -1110.1544
expansions: [(3, 1), (27, 1), (31, 1), (34, 1), (35, 1), (36, 1), (38, 1), (39, 1), (41, 1), (50, 1), (51, 1), (53, 1), (70, 1), (71, 2), (74, 1), (82, 1), (92, 1), (93, 1), (94, 1), (99, 1), (102, 1), (114, 1), (115, 2), (117, 2), (127, 1), (131, 1), (138, 1), (155, 1), (156, 5), (159, 2), (160, 2), (161, 2), (162, 3), (168, 1), (182, 1), (188, 2), (189, 1), (194, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 244 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 1104.5636 - loglik: -1.1021e+03 - logprior: -2.5129e+00
Epoch 2/10
39/39 - 13s - loss: 1093.5975 - loglik: -1.0923e+03 - logprior: -1.3087e+00
Epoch 3/10
39/39 - 13s - loss: 1089.3273 - loglik: -1.0888e+03 - logprior: -5.1104e-01
Epoch 4/10
39/39 - 13s - loss: 1084.5205 - loglik: -1.0841e+03 - logprior: -4.4546e-01
Epoch 5/10
39/39 - 13s - loss: 1077.5261 - loglik: -1.0771e+03 - logprior: -4.2068e-01
Epoch 6/10
39/39 - 13s - loss: 1061.9880 - loglik: -1.0616e+03 - logprior: -4.0376e-01
Epoch 7/10
39/39 - 13s - loss: 1017.9983 - loglik: -1.0174e+03 - logprior: -5.3326e-01
Epoch 8/10
39/39 - 13s - loss: 937.4736 - loglik: -9.3658e+02 - logprior: -8.5398e-01
Epoch 9/10
39/39 - 13s - loss: 887.9749 - loglik: -8.8698e+02 - logprior: -9.4519e-01
Epoch 10/10
39/39 - 13s - loss: 875.4202 - loglik: -8.7449e+02 - logprior: -8.7321e-01
Fitted a model with MAP estimate = -872.8553
Time for alignment: 321.9748
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 1220.7698 - loglik: -1.2189e+03 - logprior: -1.8219e+00
Epoch 2/10
39/39 - 9s - loss: 1127.0421 - loglik: -1.1255e+03 - logprior: -1.5419e+00
Epoch 3/10
39/39 - 9s - loss: 1118.2861 - loglik: -1.1167e+03 - logprior: -1.5719e+00
Epoch 4/10
39/39 - 10s - loss: 1112.8685 - loglik: -1.1113e+03 - logprior: -1.5493e+00
Epoch 5/10
39/39 - 9s - loss: 1107.3578 - loglik: -1.1058e+03 - logprior: -1.5149e+00
Epoch 6/10
39/39 - 10s - loss: 1099.3676 - loglik: -1.0979e+03 - logprior: -1.4951e+00
Epoch 7/10
39/39 - 9s - loss: 1077.4475 - loglik: -1.0759e+03 - logprior: -1.5357e+00
Epoch 8/10
39/39 - 10s - loss: 1037.7296 - loglik: -1.0361e+03 - logprior: -1.5589e+00
Epoch 9/10
39/39 - 10s - loss: 952.3372 - loglik: -9.4872e+02 - logprior: -3.5640e+00
Epoch 10/10
39/39 - 10s - loss: 895.2928 - loglik: -8.9071e+02 - logprior: -4.5210e+00
Fitted a model with MAP estimate = -887.8964
expansions: []
discards: [118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135
 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153
 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 1345.7738 - loglik: -1.3430e+03 - logprior: -2.7721e+00
Epoch 2/2
19/19 - 4s - loss: 1315.7386 - loglik: -1.3156e+03 - logprior: -1.5673e-01
Fitted a model with MAP estimate = -1298.0088
expansions: [(118, 172)]
discards: [  0  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27
  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45
  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63
  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81
  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99
 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 1222.4211 - loglik: -1.2196e+03 - logprior: -2.8342e+00
Epoch 2/2
39/39 - 9s - loss: 1130.9091 - loglik: -1.1285e+03 - logprior: -2.4436e+00
Fitted a model with MAP estimate = -1116.5028
expansions: [(4, 1), (23, 1), (26, 1), (34, 1), (35, 3), (37, 1), (38, 1), (40, 1), (45, 1), (48, 1), (49, 1), (51, 1), (68, 1), (69, 2), (72, 1), (73, 1), (79, 1), (85, 1), (88, 1), (89, 1), (90, 1), (93, 1), (96, 1), (97, 1), (109, 1), (110, 2), (112, 2), (113, 1), (122, 1), (125, 1), (127, 1), (131, 1), (147, 2), (148, 5), (150, 2), (151, 3), (152, 3), (157, 1), (158, 1), (163, 1), (165, 1), (170, 1), (173, 1), (176, 1), (177, 1), (182, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 1105.1978 - loglik: -1.1027e+03 - logprior: -2.5272e+00
Epoch 2/10
39/39 - 13s - loss: 1094.3820 - loglik: -1.0930e+03 - logprior: -1.3958e+00
Epoch 3/10
39/39 - 13s - loss: 1089.1045 - loglik: -1.0886e+03 - logprior: -5.2575e-01
Epoch 4/10
39/39 - 13s - loss: 1084.9250 - loglik: -1.0845e+03 - logprior: -4.3616e-01
Epoch 5/10
39/39 - 13s - loss: 1077.0131 - loglik: -1.0766e+03 - logprior: -3.9780e-01
Epoch 6/10
39/39 - 13s - loss: 1062.7196 - loglik: -1.0623e+03 - logprior: -3.9455e-01
Epoch 7/10
39/39 - 13s - loss: 1015.9005 - loglik: -1.0153e+03 - logprior: -5.3671e-01
Epoch 8/10
39/39 - 13s - loss: 939.2570 - loglik: -9.3833e+02 - logprior: -8.8094e-01
Epoch 9/10
39/39 - 13s - loss: 888.2217 - loglik: -8.8723e+02 - logprior: -9.3634e-01
Epoch 10/10
39/39 - 13s - loss: 875.6104 - loglik: -8.7472e+02 - logprior: -8.3086e-01
Fitted a model with MAP estimate = -873.3442
Time for alignment: 319.1454
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 1221.0679 - loglik: -1.2193e+03 - logprior: -1.8152e+00
Epoch 2/10
39/39 - 9s - loss: 1124.7479 - loglik: -1.1231e+03 - logprior: -1.6140e+00
Epoch 3/10
39/39 - 10s - loss: 1115.9048 - loglik: -1.1142e+03 - logprior: -1.6756e+00
Epoch 4/10
39/39 - 9s - loss: 1110.7507 - loglik: -1.1092e+03 - logprior: -1.5657e+00
Epoch 5/10
39/39 - 9s - loss: 1105.9506 - loglik: -1.1044e+03 - logprior: -1.5227e+00
Epoch 6/10
39/39 - 10s - loss: 1097.0470 - loglik: -1.0955e+03 - logprior: -1.5185e+00
Epoch 7/10
39/39 - 10s - loss: 1073.8568 - loglik: -1.0723e+03 - logprior: -1.5630e+00
Epoch 8/10
39/39 - 10s - loss: 1034.3185 - loglik: -1.0327e+03 - logprior: -1.5817e+00
Epoch 9/10
39/39 - 10s - loss: 946.1893 - loglik: -9.4283e+02 - logprior: -3.3101e+00
Epoch 10/10
39/39 - 10s - loss: 894.9939 - loglik: -8.9034e+02 - logprior: -4.5890e+00
Fitted a model with MAP estimate = -888.0113
expansions: []
discards: [  5   6   7   8   9  10  11  12  13 110 111 117 118 119 120 134 135 136
 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154
 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172
 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 1344.2197 - loglik: -1.3413e+03 - logprior: -2.8904e+00
Epoch 2/2
19/19 - 4s - loss: 1315.3411 - loglik: -1.3151e+03 - logprior: -2.2238e-01
Fitted a model with MAP estimate = -1300.2251
expansions: [(119, 129)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98 106]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 1228.5551 - loglik: -1.2263e+03 - logprior: -2.2969e+00
Epoch 2/2
39/39 - 7s - loss: 1169.8940 - loglik: -1.1690e+03 - logprior: -9.2197e-01
Fitted a model with MAP estimate = -1159.9063
expansions: [(13, 2), (14, 1), (15, 2), (17, 79), (18, 1), (31, 1), (34, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 1122.2257 - loglik: -1.1198e+03 - logprior: -2.3874e+00
Epoch 2/10
39/39 - 12s - loss: 1094.9520 - loglik: -1.0942e+03 - logprior: -7.2245e-01
Epoch 3/10
39/39 - 13s - loss: 1090.7789 - loglik: -1.0902e+03 - logprior: -5.3251e-01
Epoch 4/10
39/39 - 13s - loss: 1086.8320 - loglik: -1.0864e+03 - logprior: -4.7535e-01
Epoch 5/10
39/39 - 13s - loss: 1079.6549 - loglik: -1.0792e+03 - logprior: -4.4110e-01
Epoch 6/10
39/39 - 13s - loss: 1065.7014 - loglik: -1.0653e+03 - logprior: -4.1679e-01
Epoch 7/10
39/39 - 13s - loss: 1024.8937 - loglik: -1.0243e+03 - logprior: -5.3930e-01
Epoch 8/10
39/39 - 13s - loss: 951.5738 - loglik: -9.5066e+02 - logprior: -8.7166e-01
Epoch 9/10
39/39 - 13s - loss: 897.6344 - loglik: -8.9658e+02 - logprior: -1.0041e+00
Epoch 10/10
39/39 - 13s - loss: 882.1723 - loglik: -8.8116e+02 - logprior: -9.4808e-01
Fitted a model with MAP estimate = -879.5882
Time for alignment: 303.7222
Computed alignments with likelihoods: ['-874.7193', '-875.9981', '-872.8553', '-873.3442', '-879.5882']
Best model has likelihood: -872.8553  (prior= -0.8179 )
time for generating output: 0.3473
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13561.projection.fasta
SP score = 0.19344075252993054
Training of 5 independent models on file PF05746.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff4c01aa640>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff4c01aa400>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aab80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aafa0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aadf0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff4c01aae20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa220>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa3d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa2b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa940>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aabe0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa1c0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa280>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa160>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aa370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff4c01aa910> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4c01aa7f0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff4c01aaa90> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff4c021d3a0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6883c46a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff67fc1a7c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff67fe033d0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7ff5e1224790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7ff5d5b6aca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4c01f22e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 651.9774 - loglik: -6.4891e+02 - logprior: -3.0719e+00
Epoch 2/10
19/19 - 2s - loss: 608.9185 - loglik: -6.0757e+02 - logprior: -1.3444e+00
Epoch 3/10
19/19 - 2s - loss: 586.1167 - loglik: -5.8446e+02 - logprior: -1.6537e+00
Epoch 4/10
19/19 - 2s - loss: 578.5164 - loglik: -5.7693e+02 - logprior: -1.5800e+00
Epoch 5/10
19/19 - 2s - loss: 575.4869 - loglik: -5.7389e+02 - logprior: -1.5966e+00
Epoch 6/10
19/19 - 2s - loss: 574.4542 - loglik: -5.7289e+02 - logprior: -1.5541e+00
Epoch 7/10
19/19 - 2s - loss: 572.2115 - loglik: -5.7069e+02 - logprior: -1.5175e+00
Epoch 8/10
19/19 - 2s - loss: 569.7372 - loglik: -5.6822e+02 - logprior: -1.5044e+00
Epoch 9/10
19/19 - 2s - loss: 564.9314 - loglik: -5.6339e+02 - logprior: -1.5218e+00
Epoch 10/10
19/19 - 2s - loss: 554.8963 - loglik: -5.5334e+02 - logprior: -1.5296e+00
Fitted a model with MAP estimate = -548.4126
expansions: [(17, 1), (18, 4), (22, 1), (23, 1), (24, 1), (31, 2), (32, 1), (34, 1), (44, 1), (48, 1), (50, 1), (56, 1), (63, 1), (64, 1), (66, 2), (68, 2), (71, 1), (76, 2), (77, 1), (79, 2), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 604.3786 - loglik: -6.0041e+02 - logprior: -3.9666e+00
Epoch 2/2
19/19 - 3s - loss: 573.4494 - loglik: -5.7132e+02 - logprior: -2.1248e+00
Fitted a model with MAP estimate = -569.4231
expansions: [(0, 2), (20, 2)]
discards: [  0  22  40  84  88 101 104]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 570.7908 - loglik: -5.6788e+02 - logprior: -2.9063e+00
Epoch 2/2
19/19 - 3s - loss: 565.7751 - loglik: -5.6472e+02 - logprior: -1.0585e+00
Fitted a model with MAP estimate = -564.5936
expansions: []
discards: [ 0 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 572.7165 - loglik: -5.6911e+02 - logprior: -3.6092e+00
Epoch 2/10
19/19 - 3s - loss: 567.5264 - loglik: -5.6648e+02 - logprior: -1.0457e+00
Epoch 3/10
19/19 - 3s - loss: 565.6873 - loglik: -5.6485e+02 - logprior: -8.3838e-01
Epoch 4/10
19/19 - 3s - loss: 564.1812 - loglik: -5.6338e+02 - logprior: -7.9681e-01
Epoch 5/10
19/19 - 3s - loss: 563.7918 - loglik: -5.6303e+02 - logprior: -7.5833e-01
Epoch 6/10
19/19 - 3s - loss: 561.7541 - loglik: -5.6102e+02 - logprior: -7.2896e-01
Epoch 7/10
19/19 - 3s - loss: 560.6744 - loglik: -5.5995e+02 - logprior: -7.1166e-01
Epoch 8/10
19/19 - 3s - loss: 557.8495 - loglik: -5.5710e+02 - logprior: -7.3815e-01
Epoch 9/10
19/19 - 3s - loss: 549.5272 - loglik: -5.4874e+02 - logprior: -7.6646e-01
Epoch 10/10
19/19 - 3s - loss: 536.2440 - loglik: -5.3539e+02 - logprior: -8.2808e-01
Fitted a model with MAP estimate = -525.9978
Time for alignment: 93.4089
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 652.1911 - loglik: -6.4912e+02 - logprior: -3.0741e+00
Epoch 2/10
19/19 - 2s - loss: 607.0598 - loglik: -6.0572e+02 - logprior: -1.3441e+00
Epoch 3/10
19/19 - 2s - loss: 583.8422 - loglik: -5.8216e+02 - logprior: -1.6780e+00
Epoch 4/10
19/19 - 2s - loss: 576.9655 - loglik: -5.7537e+02 - logprior: -1.5940e+00
Epoch 5/10
19/19 - 2s - loss: 574.4914 - loglik: -5.7291e+02 - logprior: -1.5744e+00
Epoch 6/10
19/19 - 2s - loss: 573.0656 - loglik: -5.7152e+02 - logprior: -1.5361e+00
Epoch 7/10
19/19 - 2s - loss: 571.6706 - loglik: -5.7015e+02 - logprior: -1.5116e+00
Epoch 8/10
19/19 - 2s - loss: 569.3055 - loglik: -5.6779e+02 - logprior: -1.5044e+00
Epoch 9/10
19/19 - 2s - loss: 564.4182 - loglik: -5.6289e+02 - logprior: -1.5098e+00
Epoch 10/10
19/19 - 2s - loss: 556.2003 - loglik: -5.5463e+02 - logprior: -1.5483e+00
Fitted a model with MAP estimate = -549.4890
expansions: [(17, 1), (18, 4), (22, 1), (23, 1), (24, 1), (28, 2), (32, 1), (34, 1), (40, 1), (44, 1), (48, 1), (49, 1), (55, 1), (62, 1), (66, 2), (68, 2), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 600.3920 - loglik: -5.9652e+02 - logprior: -3.8741e+00
Epoch 2/2
19/19 - 3s - loss: 572.5669 - loglik: -5.7063e+02 - logprior: -1.9320e+00
Fitted a model with MAP estimate = -568.9417
expansions: [(0, 2), (20, 1)]
discards: [ 0 22 36 84 88]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 570.3350 - loglik: -5.6743e+02 - logprior: -2.9062e+00
Epoch 2/2
19/19 - 3s - loss: 565.5918 - loglik: -5.6455e+02 - logprior: -1.0458e+00
Fitted a model with MAP estimate = -564.7353
expansions: [(25, 1)]
discards: [ 0 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 572.9182 - loglik: -5.6931e+02 - logprior: -3.6053e+00
Epoch 2/10
19/19 - 3s - loss: 567.9474 - loglik: -5.6697e+02 - logprior: -9.7880e-01
Epoch 3/10
19/19 - 3s - loss: 565.7853 - loglik: -5.6503e+02 - logprior: -7.5343e-01
Epoch 4/10
19/19 - 3s - loss: 564.7533 - loglik: -5.6398e+02 - logprior: -7.7623e-01
Epoch 5/10
19/19 - 3s - loss: 562.3463 - loglik: -5.6158e+02 - logprior: -7.6180e-01
Epoch 6/10
19/19 - 3s - loss: 562.2163 - loglik: -5.6148e+02 - logprior: -7.3539e-01
Epoch 7/10
19/19 - 3s - loss: 560.1985 - loglik: -5.5946e+02 - logprior: -7.2848e-01
Epoch 8/10
19/19 - 3s - loss: 556.1463 - loglik: -5.5537e+02 - logprior: -7.6509e-01
Epoch 9/10
19/19 - 3s - loss: 548.7136 - loglik: -5.4792e+02 - logprior: -7.8118e-01
Epoch 10/10
19/19 - 3s - loss: 534.2339 - loglik: -5.3337e+02 - logprior: -8.4197e-01
Fitted a model with MAP estimate = -524.0539
Time for alignment: 94.2771
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 651.9297 - loglik: -6.4885e+02 - logprior: -3.0752e+00
Epoch 2/10
19/19 - 2s - loss: 607.8732 - loglik: -6.0652e+02 - logprior: -1.3513e+00
Epoch 3/10
19/19 - 2s - loss: 582.3685 - loglik: -5.8070e+02 - logprior: -1.6711e+00
Epoch 4/10
19/19 - 2s - loss: 575.5504 - loglik: -5.7394e+02 - logprior: -1.6047e+00
Epoch 5/10
19/19 - 2s - loss: 573.4910 - loglik: -5.7192e+02 - logprior: -1.5669e+00
Epoch 6/10
19/19 - 2s - loss: 571.7336 - loglik: -5.7021e+02 - logprior: -1.5178e+00
Epoch 7/10
19/19 - 2s - loss: 569.9444 - loglik: -5.6844e+02 - logprior: -1.4987e+00
Epoch 8/10
19/19 - 2s - loss: 568.0496 - loglik: -5.6655e+02 - logprior: -1.4867e+00
Epoch 9/10
19/19 - 2s - loss: 563.5814 - loglik: -5.6206e+02 - logprior: -1.5016e+00
Epoch 10/10
19/19 - 2s - loss: 556.1323 - loglik: -5.5458e+02 - logprior: -1.5254e+00
Fitted a model with MAP estimate = -549.6194
expansions: [(17, 2), (18, 5), (19, 1), (22, 1), (24, 1), (28, 2), (32, 1), (34, 1), (40, 1), (43, 1), (47, 1), (49, 1), (56, 1), (62, 1), (63, 1), (69, 2), (71, 1), (74, 1), (76, 1), (79, 2), (85, 1), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 599.1760 - loglik: -5.9530e+02 - logprior: -3.8797e+00
Epoch 2/2
19/19 - 3s - loss: 571.8884 - loglik: -5.7000e+02 - logprior: -1.8919e+00
Fitted a model with MAP estimate = -568.7162
expansions: [(0, 2)]
discards: [  0  38  90 105]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 569.9588 - loglik: -5.6708e+02 - logprior: -2.8747e+00
Epoch 2/2
19/19 - 3s - loss: 565.4478 - loglik: -5.6441e+02 - logprior: -1.0327e+00
Fitted a model with MAP estimate = -564.2800
expansions: []
discards: [ 0 17 18 19]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 572.7920 - loglik: -5.6918e+02 - logprior: -3.6124e+00
Epoch 2/10
19/19 - 3s - loss: 567.8491 - loglik: -5.6681e+02 - logprior: -1.0382e+00
Epoch 3/10
19/19 - 3s - loss: 565.4344 - loglik: -5.6462e+02 - logprior: -8.1111e-01
Epoch 4/10
19/19 - 3s - loss: 564.4975 - loglik: -5.6371e+02 - logprior: -7.8719e-01
Epoch 5/10
19/19 - 3s - loss: 563.3445 - loglik: -5.6260e+02 - logprior: -7.4571e-01
Epoch 6/10
19/19 - 3s - loss: 561.6479 - loglik: -5.6091e+02 - logprior: -7.2795e-01
Epoch 7/10
19/19 - 3s - loss: 560.0135 - loglik: -5.5930e+02 - logprior: -7.0798e-01
Epoch 8/10
19/19 - 3s - loss: 556.4443 - loglik: -5.5567e+02 - logprior: -7.6513e-01
Epoch 9/10
19/19 - 3s - loss: 548.0444 - loglik: -5.4722e+02 - logprior: -8.0540e-01
Epoch 10/10
19/19 - 3s - loss: 533.8478 - loglik: -5.3297e+02 - logprior: -8.5661e-01
Fitted a model with MAP estimate = -523.0316
Time for alignment: 93.3936
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 651.9821 - loglik: -6.4891e+02 - logprior: -3.0727e+00
Epoch 2/10
19/19 - 2s - loss: 609.7333 - loglik: -6.0840e+02 - logprior: -1.3366e+00
Epoch 3/10
19/19 - 2s - loss: 585.6265 - loglik: -5.8398e+02 - logprior: -1.6430e+00
Epoch 4/10
19/19 - 2s - loss: 576.7189 - loglik: -5.7512e+02 - logprior: -1.5967e+00
Epoch 5/10
19/19 - 2s - loss: 574.1754 - loglik: -5.7259e+02 - logprior: -1.5849e+00
Epoch 6/10
19/19 - 2s - loss: 573.0501 - loglik: -5.7150e+02 - logprior: -1.5479e+00
Epoch 7/10
19/19 - 2s - loss: 571.8148 - loglik: -5.7028e+02 - logprior: -1.5266e+00
Epoch 8/10
19/19 - 2s - loss: 568.4886 - loglik: -5.6696e+02 - logprior: -1.5174e+00
Epoch 9/10
19/19 - 2s - loss: 564.3707 - loglik: -5.6282e+02 - logprior: -1.5296e+00
Epoch 10/10
19/19 - 2s - loss: 555.6802 - loglik: -5.5409e+02 - logprior: -1.5679e+00
Fitted a model with MAP estimate = -549.7691
expansions: [(17, 1), (18, 4), (22, 1), (23, 1), (29, 1), (33, 1), (35, 1), (38, 1), (40, 1), (42, 1), (43, 1), (49, 1), (55, 1), (62, 1), (63, 1), (69, 2), (71, 1), (74, 1), (76, 1), (79, 2), (85, 1), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 600.6074 - loglik: -5.9672e+02 - logprior: -3.8849e+00
Epoch 2/2
19/19 - 3s - loss: 572.1180 - loglik: -5.7026e+02 - logprior: -1.8601e+00
Fitted a model with MAP estimate = -568.9270
expansions: [(0, 2), (20, 1)]
discards: [  0  22  87 102]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 569.8403 - loglik: -5.6696e+02 - logprior: -2.8830e+00
Epoch 2/2
19/19 - 3s - loss: 565.5890 - loglik: -5.6455e+02 - logprior: -1.0377e+00
Fitted a model with MAP estimate = -564.6514
expansions: [(23, 1), (25, 1)]
discards: [ 0 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 572.8826 - loglik: -5.6929e+02 - logprior: -3.5929e+00
Epoch 2/10
19/19 - 3s - loss: 567.7763 - loglik: -5.6679e+02 - logprior: -9.8243e-01
Epoch 3/10
19/19 - 3s - loss: 565.6257 - loglik: -5.6489e+02 - logprior: -7.3687e-01
Epoch 4/10
19/19 - 3s - loss: 564.3064 - loglik: -5.6355e+02 - logprior: -7.5518e-01
Epoch 5/10
19/19 - 3s - loss: 563.0861 - loglik: -5.6233e+02 - logprior: -7.4851e-01
Epoch 6/10
19/19 - 3s - loss: 561.9010 - loglik: -5.6118e+02 - logprior: -7.1356e-01
Epoch 7/10
19/19 - 3s - loss: 560.2640 - loglik: -5.5955e+02 - logprior: -7.0538e-01
Epoch 8/10
19/19 - 3s - loss: 556.8865 - loglik: -5.5614e+02 - logprior: -7.3748e-01
Epoch 9/10
19/19 - 3s - loss: 548.5371 - loglik: -5.4776e+02 - logprior: -7.5618e-01
Epoch 10/10
19/19 - 3s - loss: 534.3784 - loglik: -5.3353e+02 - logprior: -8.2224e-01
Fitted a model with MAP estimate = -523.0700
Time for alignment: 94.3108
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 651.9045 - loglik: -6.4883e+02 - logprior: -3.0725e+00
Epoch 2/10
19/19 - 2s - loss: 608.5645 - loglik: -6.0724e+02 - logprior: -1.3282e+00
Epoch 3/10
19/19 - 2s - loss: 585.6546 - loglik: -5.8404e+02 - logprior: -1.6136e+00
Epoch 4/10
19/19 - 2s - loss: 578.5527 - loglik: -5.7704e+02 - logprior: -1.5109e+00
Epoch 5/10
19/19 - 2s - loss: 575.6587 - loglik: -5.7419e+02 - logprior: -1.4613e+00
Epoch 6/10
19/19 - 2s - loss: 574.8668 - loglik: -5.7343e+02 - logprior: -1.4320e+00
Epoch 7/10
19/19 - 2s - loss: 572.9136 - loglik: -5.7149e+02 - logprior: -1.4167e+00
Epoch 8/10
19/19 - 2s - loss: 570.7432 - loglik: -5.6932e+02 - logprior: -1.4111e+00
Epoch 9/10
19/19 - 2s - loss: 565.9559 - loglik: -5.6450e+02 - logprior: -1.4402e+00
Epoch 10/10
19/19 - 2s - loss: 558.6659 - loglik: -5.5716e+02 - logprior: -1.4816e+00
Fitted a model with MAP estimate = -552.0023
expansions: [(17, 1), (18, 4), (22, 1), (23, 1), (24, 1), (28, 2), (32, 1), (34, 1), (40, 1), (48, 2), (49, 2), (63, 1), (64, 1), (66, 2), (68, 2), (71, 1), (77, 1), (79, 1), (82, 1), (85, 1), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 599.6713 - loglik: -5.9573e+02 - logprior: -3.9461e+00
Epoch 2/2
19/19 - 3s - loss: 572.3047 - loglik: -5.7040e+02 - logprior: -1.9006e+00
Fitted a model with MAP estimate = -568.8138
expansions: [(0, 2), (20, 1)]
discards: [ 0 22 36 60 89]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 569.7777 - loglik: -5.6688e+02 - logprior: -2.8939e+00
Epoch 2/2
19/19 - 3s - loss: 565.9913 - loglik: -5.6495e+02 - logprior: -1.0380e+00
Fitted a model with MAP estimate = -564.5397
expansions: [(25, 1)]
discards: [ 0 16 17 83]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 573.0397 - loglik: -5.6946e+02 - logprior: -3.5845e+00
Epoch 2/10
19/19 - 3s - loss: 567.3564 - loglik: -5.6639e+02 - logprior: -9.6779e-01
Epoch 3/10
19/19 - 3s - loss: 566.0808 - loglik: -5.6532e+02 - logprior: -7.5964e-01
Epoch 4/10
19/19 - 3s - loss: 564.2417 - loglik: -5.6348e+02 - logprior: -7.6327e-01
Epoch 5/10
19/19 - 3s - loss: 562.7690 - loglik: -5.6200e+02 - logprior: -7.6283e-01
Epoch 6/10
19/19 - 3s - loss: 561.9155 - loglik: -5.6119e+02 - logprior: -7.2010e-01
Epoch 7/10
19/19 - 3s - loss: 560.0075 - loglik: -5.5925e+02 - logprior: -7.4972e-01
Epoch 8/10
19/19 - 3s - loss: 556.3758 - loglik: -5.5561e+02 - logprior: -7.5788e-01
Epoch 9/10
19/19 - 3s - loss: 548.9126 - loglik: -5.4810e+02 - logprior: -7.9703e-01
Epoch 10/10
19/19 - 3s - loss: 533.7303 - loglik: -5.3287e+02 - logprior: -8.3652e-01
Fitted a model with MAP estimate = -523.7057
Time for alignment: 92.6420
Computed alignments with likelihoods: ['-525.9978', '-524.0539', '-523.0316', '-523.0700', '-523.7057']
Best model has likelihood: -523.0316  (prior= -0.8462 )
time for generating output: 0.1665
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF05746.projection.fasta
SP score = 0.8772261623325454
