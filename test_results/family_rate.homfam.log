Training of 3 independent models on file cys.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f36a9b1b6d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f367f25adc0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 172 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 574.8748 - loglik: -5.6646e+02 - logprior: -8.4106e+00
Epoch 2/10
12/12 - 5s - loss: 503.8678 - loglik: -5.0231e+02 - logprior: -1.5561e+00
Epoch 3/10
12/12 - 5s - loss: 440.7744 - loglik: -4.3937e+02 - logprior: -1.4077e+00
Epoch 4/10
12/12 - 5s - loss: 414.5530 - loglik: -4.1278e+02 - logprior: -1.7740e+00
Epoch 5/10
12/12 - 5s - loss: 410.3300 - loglik: -4.0849e+02 - logprior: -1.8368e+00
Epoch 6/10
12/12 - 5s - loss: 407.0987 - loglik: -4.0537e+02 - logprior: -1.7255e+00
Epoch 7/10
12/12 - 5s - loss: 405.6382 - loglik: -4.0395e+02 - logprior: -1.6913e+00
Epoch 8/10
12/12 - 5s - loss: 405.4907 - loglik: -4.0379e+02 - logprior: -1.7033e+00
Epoch 9/10
12/12 - 5s - loss: 402.4352 - loglik: -4.0075e+02 - logprior: -1.6859e+00
Epoch 10/10
12/12 - 5s - loss: 404.5013 - loglik: -4.0283e+02 - logprior: -1.6746e+00
Fitted a model with MAP estimate = -403.8309
expansions: [(9, 3), (10, 1), (11, 2), (31, 1), (32, 2), (33, 3), (34, 2), (47, 3), (58, 1), (60, 1), (61, 1), (62, 1), (75, 1), (80, 2), (81, 2), (82, 2), (89, 2), (91, 1), (98, 1), (101, 1), (112, 2), (114, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (149, 1), (159, 1), (167, 1), (168, 2)]
discards: [0]
Fitting a model of length 220 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 414.0341 - loglik: -4.0421e+02 - logprior: -9.8236e+00
Epoch 2/2
12/12 - 7s - loss: 392.8360 - loglik: -3.8888e+02 - logprior: -3.9539e+00
Fitted a model with MAP estimate = -389.9609
expansions: [(0, 3)]
discards: [  0   9  42  45 102 107 144 178]
Fitting a model of length 215 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 396.8622 - loglik: -3.8926e+02 - logprior: -7.6059e+00
Epoch 2/2
12/12 - 7s - loss: 388.0544 - loglik: -3.8668e+02 - logprior: -1.3738e+00
Fitted a model with MAP estimate = -385.0592
expansions: []
discards: [ 0  2 61 62]
Fitting a model of length 211 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 399.9149 - loglik: -3.9033e+02 - logprior: -9.5897e+00
Epoch 2/10
12/12 - 7s - loss: 389.4913 - loglik: -3.8616e+02 - logprior: -3.3358e+00
Epoch 3/10
12/12 - 7s - loss: 386.7333 - loglik: -3.8504e+02 - logprior: -1.6957e+00
Epoch 4/10
12/12 - 7s - loss: 385.7939 - loglik: -3.8567e+02 - logprior: -1.2601e-01
Epoch 5/10
12/12 - 7s - loss: 381.6350 - loglik: -3.8183e+02 - logprior: 0.1960
Epoch 6/10
12/12 - 7s - loss: 379.8638 - loglik: -3.8014e+02 - logprior: 0.2791
Epoch 7/10
12/12 - 7s - loss: 382.5373 - loglik: -3.8281e+02 - logprior: 0.2688
Fitted a model with MAP estimate = -380.3002
Time for alignment: 163.4829
Fitting a model of length 172 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 574.7042 - loglik: -5.6623e+02 - logprior: -8.4709e+00
Epoch 2/10
12/12 - 5s - loss: 505.1850 - loglik: -5.0357e+02 - logprior: -1.6167e+00
Epoch 3/10
12/12 - 5s - loss: 442.8036 - loglik: -4.4130e+02 - logprior: -1.5028e+00
Epoch 4/10
12/12 - 5s - loss: 415.7442 - loglik: -4.1378e+02 - logprior: -1.9649e+00
Epoch 5/10
12/12 - 5s - loss: 407.9873 - loglik: -4.0595e+02 - logprior: -2.0404e+00
Epoch 6/10
12/12 - 5s - loss: 405.4192 - loglik: -4.0348e+02 - logprior: -1.9431e+00
Epoch 7/10
12/12 - 5s - loss: 402.7285 - loglik: -4.0081e+02 - logprior: -1.9199e+00
Epoch 8/10
12/12 - 5s - loss: 403.4601 - loglik: -4.0155e+02 - logprior: -1.9088e+00
Fitted a model with MAP estimate = -402.2972
expansions: [(9, 3), (10, 1), (12, 1), (13, 1), (31, 1), (32, 2), (33, 1), (35, 2), (48, 3), (57, 1), (58, 1), (60, 1), (63, 1), (76, 1), (81, 1), (83, 1), (84, 2), (86, 1), (90, 1), (93, 1), (96, 1), (99, 1), (102, 1), (113, 2), (115, 1), (122, 2), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (150, 1), (159, 1), (168, 2), (169, 1)]
discards: [0]
Fitting a model of length 219 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 413.1559 - loglik: -4.0333e+02 - logprior: -9.8261e+00
Epoch 2/2
12/12 - 7s - loss: 392.3047 - loglik: -3.8835e+02 - logprior: -3.9539e+00
Fitted a model with MAP estimate = -389.2119
expansions: [(0, 3)]
discards: [  0   9  44  61 142 155 157 177]
Fitting a model of length 214 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 396.4854 - loglik: -3.8888e+02 - logprior: -7.6091e+00
Epoch 2/2
12/12 - 7s - loss: 386.8313 - loglik: -3.8543e+02 - logprior: -1.4019e+00
Fitted a model with MAP estimate = -385.1685
expansions: []
discards: [  0   2 105]
Fitting a model of length 211 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 9s - loss: 397.9596 - loglik: -3.8833e+02 - logprior: -9.6316e+00
Epoch 2/10
12/12 - 7s - loss: 390.6545 - loglik: -3.8727e+02 - logprior: -3.3843e+00
Epoch 3/10
12/12 - 7s - loss: 386.0537 - loglik: -3.8417e+02 - logprior: -1.8873e+00
Epoch 4/10
12/12 - 7s - loss: 382.6694 - loglik: -3.8247e+02 - logprior: -2.0299e-01
Epoch 5/10
12/12 - 7s - loss: 383.3081 - loglik: -3.8348e+02 - logprior: 0.1693
Fitted a model with MAP estimate = -381.2602
Time for alignment: 140.5364
Fitting a model of length 172 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 576.2851 - loglik: -5.6784e+02 - logprior: -8.4432e+00
Epoch 2/10
12/12 - 5s - loss: 504.8684 - loglik: -5.0324e+02 - logprior: -1.6280e+00
Epoch 3/10
12/12 - 5s - loss: 445.5226 - loglik: -4.4395e+02 - logprior: -1.5689e+00
Epoch 4/10
12/12 - 5s - loss: 418.4206 - loglik: -4.1640e+02 - logprior: -2.0242e+00
Epoch 5/10
12/12 - 5s - loss: 408.3834 - loglik: -4.0639e+02 - logprior: -1.9912e+00
Epoch 6/10
12/12 - 5s - loss: 404.6587 - loglik: -4.0285e+02 - logprior: -1.8050e+00
Epoch 7/10
12/12 - 5s - loss: 404.2218 - loglik: -4.0245e+02 - logprior: -1.7673e+00
Epoch 8/10
12/12 - 5s - loss: 403.7080 - loglik: -4.0194e+02 - logprior: -1.7686e+00
Epoch 9/10
12/12 - 5s - loss: 400.5596 - loglik: -3.9881e+02 - logprior: -1.7520e+00
Epoch 10/10
12/12 - 5s - loss: 402.8608 - loglik: -4.0111e+02 - logprior: -1.7520e+00
Fitted a model with MAP estimate = -401.9180
expansions: [(9, 3), (10, 1), (12, 1), (13, 1), (31, 1), (32, 2), (33, 1), (35, 2), (48, 3), (50, 1), (58, 1), (60, 1), (62, 2), (74, 1), (80, 1), (81, 1), (82, 1), (83, 2), (90, 2), (92, 1), (96, 1), (98, 1), (102, 1), (112, 2), (122, 1), (123, 2), (126, 2), (137, 1), (140, 2), (160, 1), (162, 2), (163, 1), (167, 2)]
discards: [0]
Fitting a model of length 219 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 411.5918 - loglik: -4.0181e+02 - logprior: -9.7859e+00
Epoch 2/2
12/12 - 7s - loss: 393.2299 - loglik: -3.8936e+02 - logprior: -3.8716e+00
Fitted a model with MAP estimate = -388.7200
expansions: [(0, 3)]
discards: [  0   9  44 143 205]
Fitting a model of length 217 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 396.4868 - loglik: -3.8888e+02 - logprior: -7.6020e+00
Epoch 2/2
12/12 - 7s - loss: 385.3296 - loglik: -3.8395e+02 - logprior: -1.3808e+00
Fitted a model with MAP estimate = -384.2090
expansions: []
discards: [ 0  2 61 62 81]
Fitting a model of length 212 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 397.7468 - loglik: -3.8815e+02 - logprior: -9.5952e+00
Epoch 2/10
12/12 - 7s - loss: 392.4856 - loglik: -3.8910e+02 - logprior: -3.3855e+00
Epoch 3/10
12/12 - 7s - loss: 385.9841 - loglik: -3.8413e+02 - logprior: -1.8546e+00
Epoch 4/10
12/12 - 7s - loss: 385.0399 - loglik: -3.8485e+02 - logprior: -1.9377e-01
Epoch 5/10
12/12 - 7s - loss: 381.5959 - loglik: -3.8179e+02 - logprior: 0.1956
Epoch 6/10
12/12 - 7s - loss: 380.0843 - loglik: -3.8038e+02 - logprior: 0.2912
Epoch 7/10
12/12 - 7s - loss: 381.1309 - loglik: -3.8139e+02 - logprior: 0.2628
Fitted a model with MAP estimate = -379.8215
Time for alignment: 165.4496
Computed alignments with likelihoods: ['-380.3002', '-381.2602', '-379.8215']
Best model has likelihood: -379.8215
SP score = 0.9330
Training of 3 independent models on file DMRL_synthase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f36883dd8b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f368859b5e0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 405.9859 - loglik: -3.8589e+02 - logprior: -2.0099e+01
Epoch 2/10
10/10 - 1s - loss: 340.9682 - loglik: -3.3638e+02 - logprior: -4.5905e+00
Epoch 3/10
10/10 - 1s - loss: 289.0357 - loglik: -2.8665e+02 - logprior: -2.3899e+00
Epoch 4/10
10/10 - 1s - loss: 257.0206 - loglik: -2.5497e+02 - logprior: -2.0527e+00
Epoch 5/10
10/10 - 1s - loss: 245.7618 - loglik: -2.4400e+02 - logprior: -1.7645e+00
Epoch 6/10
10/10 - 1s - loss: 241.3040 - loglik: -2.3993e+02 - logprior: -1.3755e+00
Epoch 7/10
10/10 - 1s - loss: 239.9229 - loglik: -2.3877e+02 - logprior: -1.1501e+00
Epoch 8/10
10/10 - 1s - loss: 239.1938 - loglik: -2.3811e+02 - logprior: -1.0828e+00
Epoch 9/10
10/10 - 1s - loss: 238.1516 - loglik: -2.3715e+02 - logprior: -1.0041e+00
Epoch 10/10
10/10 - 1s - loss: 237.3194 - loglik: -2.3638e+02 - logprior: -9.3551e-01
Fitted a model with MAP estimate = -237.7018
expansions: [(0, 4), (13, 3), (18, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (105, 1), (106, 5)]
discards: []
Fitting a model of length 145 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 259.4999 - loglik: -2.3304e+02 - logprior: -2.6455e+01
Epoch 2/2
10/10 - 2s - loss: 221.4233 - loglik: -2.1402e+02 - logprior: -7.4044e+00
Fitted a model with MAP estimate = -213.0967
expansions: [(133, 1)]
discards: [59 62]
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 228.8094 - loglik: -2.1034e+02 - logprior: -1.8469e+01
Epoch 2/2
10/10 - 2s - loss: 208.7995 - loglik: -2.0492e+02 - logprior: -3.8779e+00
Fitted a model with MAP estimate = -206.7805
expansions: []
discards: []
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 224.2493 - loglik: -2.0691e+02 - logprior: -1.7335e+01
Epoch 2/10
10/10 - 2s - loss: 208.3936 - loglik: -2.0488e+02 - logprior: -3.5147e+00
Epoch 3/10
10/10 - 2s - loss: 204.9448 - loglik: -2.0408e+02 - logprior: -8.6868e-01
Epoch 4/10
10/10 - 2s - loss: 203.8546 - loglik: -2.0390e+02 - logprior: 0.0500
Epoch 5/10
10/10 - 2s - loss: 202.0571 - loglik: -2.0255e+02 - logprior: 0.4886
Epoch 6/10
10/10 - 2s - loss: 200.7162 - loglik: -2.0152e+02 - logprior: 0.8016
Epoch 7/10
10/10 - 2s - loss: 201.5595 - loglik: -2.0263e+02 - logprior: 1.0715
Fitted a model with MAP estimate = -200.6836
Time for alignment: 53.5185
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 405.9672 - loglik: -3.8587e+02 - logprior: -2.0097e+01
Epoch 2/10
10/10 - 1s - loss: 340.4911 - loglik: -3.3590e+02 - logprior: -4.5911e+00
Epoch 3/10
10/10 - 1s - loss: 286.8026 - loglik: -2.8437e+02 - logprior: -2.4324e+00
Epoch 4/10
10/10 - 1s - loss: 253.1327 - loglik: -2.5095e+02 - logprior: -2.1820e+00
Epoch 5/10
10/10 - 1s - loss: 241.8849 - loglik: -2.3993e+02 - logprior: -1.9533e+00
Epoch 6/10
10/10 - 1s - loss: 236.6528 - loglik: -2.3509e+02 - logprior: -1.5624e+00
Epoch 7/10
10/10 - 1s - loss: 234.8687 - loglik: -2.3351e+02 - logprior: -1.3570e+00
Epoch 8/10
10/10 - 1s - loss: 233.3654 - loglik: -2.3213e+02 - logprior: -1.2370e+00
Epoch 9/10
10/10 - 1s - loss: 232.8947 - loglik: -2.3177e+02 - logprior: -1.1245e+00
Epoch 10/10
10/10 - 1s - loss: 232.6199 - loglik: -2.3154e+02 - logprior: -1.0764e+00
Fitted a model with MAP estimate = -232.1974
expansions: [(0, 4), (13, 1), (14, 1), (19, 1), (20, 1), (26, 1), (27, 1), (30, 1), (31, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (98, 2), (114, 3), (115, 4)]
discards: []
Fitting a model of length 148 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 257.6877 - loglik: -2.3129e+02 - logprior: -2.6397e+01
Epoch 2/2
10/10 - 2s - loss: 218.8612 - loglik: -2.1148e+02 - logprior: -7.3813e+00
Fitted a model with MAP estimate = -212.0146
expansions: []
discards: [ 59  62 122 142]
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 226.9517 - loglik: -2.0850e+02 - logprior: -1.8450e+01
Epoch 2/2
10/10 - 2s - loss: 209.7854 - loglik: -2.0589e+02 - logprior: -3.8906e+00
Fitted a model with MAP estimate = -206.7363
expansions: []
discards: []
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 223.5226 - loglik: -2.0619e+02 - logprior: -1.7332e+01
Epoch 2/10
10/10 - 2s - loss: 208.7520 - loglik: -2.0524e+02 - logprior: -3.5110e+00
Epoch 3/10
10/10 - 2s - loss: 205.8777 - loglik: -2.0501e+02 - logprior: -8.6280e-01
Epoch 4/10
10/10 - 2s - loss: 202.8952 - loglik: -2.0295e+02 - logprior: 0.0562
Epoch 5/10
10/10 - 2s - loss: 202.4063 - loglik: -2.0290e+02 - logprior: 0.4953
Epoch 6/10
10/10 - 2s - loss: 201.0797 - loglik: -2.0189e+02 - logprior: 0.8080
Epoch 7/10
10/10 - 2s - loss: 201.2409 - loglik: -2.0232e+02 - logprior: 1.0781
Fitted a model with MAP estimate = -200.6789
Time for alignment: 53.6942
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 406.2308 - loglik: -3.8613e+02 - logprior: -2.0100e+01
Epoch 2/10
10/10 - 1s - loss: 340.9630 - loglik: -3.3637e+02 - logprior: -4.5973e+00
Epoch 3/10
10/10 - 1s - loss: 288.7867 - loglik: -2.8633e+02 - logprior: -2.4555e+00
Epoch 4/10
10/10 - 1s - loss: 255.9689 - loglik: -2.5377e+02 - logprior: -2.1960e+00
Epoch 5/10
10/10 - 1s - loss: 242.9132 - loglik: -2.4086e+02 - logprior: -2.0560e+00
Epoch 6/10
10/10 - 1s - loss: 238.2268 - loglik: -2.3658e+02 - logprior: -1.6454e+00
Epoch 7/10
10/10 - 1s - loss: 235.0170 - loglik: -2.3368e+02 - logprior: -1.3348e+00
Epoch 8/10
10/10 - 1s - loss: 233.8325 - loglik: -2.3267e+02 - logprior: -1.1670e+00
Epoch 9/10
10/10 - 1s - loss: 233.8739 - loglik: -2.3285e+02 - logprior: -1.0281e+00
Fitted a model with MAP estimate = -233.1847
expansions: [(0, 3), (14, 3), (15, 1), (22, 1), (26, 1), (27, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (76, 1), (77, 1), (78, 3), (81, 1), (98, 2), (114, 3), (115, 4)]
discards: []
Fitting a model of length 148 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 256.9138 - loglik: -2.3069e+02 - logprior: -2.6223e+01
Epoch 2/2
10/10 - 2s - loss: 219.5785 - loglik: -2.1228e+02 - logprior: -7.2989e+00
Fitted a model with MAP estimate = -212.0354
expansions: []
discards: [ 59  62 122 142]
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 227.4269 - loglik: -2.0900e+02 - logprior: -1.8425e+01
Epoch 2/2
10/10 - 2s - loss: 209.6377 - loglik: -2.0575e+02 - logprior: -3.8854e+00
Fitted a model with MAP estimate = -206.7136
expansions: []
discards: []
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 223.9817 - loglik: -2.0665e+02 - logprior: -1.7334e+01
Epoch 2/10
10/10 - 2s - loss: 208.4025 - loglik: -2.0489e+02 - logprior: -3.5148e+00
Epoch 3/10
10/10 - 2s - loss: 205.1357 - loglik: -2.0427e+02 - logprior: -8.6128e-01
Epoch 4/10
10/10 - 2s - loss: 203.2221 - loglik: -2.0328e+02 - logprior: 0.0579
Epoch 5/10
10/10 - 2s - loss: 202.5300 - loglik: -2.0302e+02 - logprior: 0.4899
Epoch 6/10
10/10 - 2s - loss: 201.2960 - loglik: -2.0210e+02 - logprior: 0.8057
Epoch 7/10
10/10 - 2s - loss: 200.7775 - loglik: -2.0185e+02 - logprior: 1.0712
Epoch 8/10
10/10 - 2s - loss: 200.7568 - loglik: -2.0200e+02 - logprior: 1.2438
Epoch 9/10
10/10 - 2s - loss: 200.2268 - loglik: -2.0158e+02 - logprior: 1.3499
Epoch 10/10
10/10 - 2s - loss: 200.2434 - loglik: -2.0169e+02 - logprior: 1.4426
Fitted a model with MAP estimate = -199.7767
Time for alignment: 58.0084
Computed alignments with likelihoods: ['-200.6836', '-200.6789', '-199.7767']
Best model has likelihood: -199.7767
SP score = 0.9092
Training of 3 independent models on file Stap_Strp_toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34c9cde940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f36989d18b0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 316.8456 - loglik: -2.4838e+02 - logprior: -6.8467e+01
Epoch 2/10
10/10 - 1s - loss: 236.1687 - loglik: -2.1897e+02 - logprior: -1.7202e+01
Epoch 3/10
10/10 - 1s - loss: 203.7217 - loglik: -1.9637e+02 - logprior: -7.3482e+00
Epoch 4/10
10/10 - 2s - loss: 189.7057 - loglik: -1.8589e+02 - logprior: -3.8108e+00
Epoch 5/10
10/10 - 1s - loss: 183.3784 - loglik: -1.8148e+02 - logprior: -1.8940e+00
Epoch 6/10
10/10 - 1s - loss: 179.8658 - loglik: -1.7906e+02 - logprior: -8.0842e-01
Epoch 7/10
10/10 - 2s - loss: 177.6178 - loglik: -1.7747e+02 - logprior: -1.5121e-01
Epoch 8/10
10/10 - 1s - loss: 175.9559 - loglik: -1.7616e+02 - logprior: 0.2073
Epoch 9/10
10/10 - 1s - loss: 174.7784 - loglik: -1.7524e+02 - logprior: 0.4623
Epoch 10/10
10/10 - 1s - loss: 174.2013 - loglik: -1.7486e+02 - logprior: 0.6561
Fitted a model with MAP estimate = -174.0422
expansions: [(9, 3), (14, 1), (15, 1), (26, 1), (37, 2), (46, 1), (58, 6)]
discards: [0]
Fitting a model of length 85 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 252.9351 - loglik: -1.7612e+02 - logprior: -7.6820e+01
Epoch 2/2
10/10 - 2s - loss: 199.5535 - loglik: -1.6860e+02 - logprior: -3.0957e+01
Fitted a model with MAP estimate = -190.2651
expansions: [(32, 3)]
discards: [42]
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 239.9187 - loglik: -1.6597e+02 - logprior: -7.3954e+01
Epoch 2/2
10/10 - 2s - loss: 184.9291 - loglik: -1.6289e+02 - logprior: -2.2042e+01
Fitted a model with MAP estimate = -173.3432
expansions: []
discards: []
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 223.5933 - loglik: -1.6196e+02 - logprior: -6.1632e+01
Epoch 2/10
10/10 - 2s - loss: 176.2018 - loglik: -1.6124e+02 - logprior: -1.4957e+01
Epoch 3/10
10/10 - 2s - loss: 166.5556 - loglik: -1.6131e+02 - logprior: -5.2469e+00
Epoch 4/10
10/10 - 2s - loss: 162.6706 - loglik: -1.6146e+02 - logprior: -1.2078e+00
Epoch 5/10
10/10 - 2s - loss: 160.5547 - loglik: -1.6153e+02 - logprior: 0.9752
Epoch 6/10
10/10 - 2s - loss: 159.3259 - loglik: -1.6155e+02 - logprior: 2.2209
Epoch 7/10
10/10 - 2s - loss: 158.4955 - loglik: -1.6148e+02 - logprior: 2.9881
Epoch 8/10
10/10 - 2s - loss: 157.8241 - loglik: -1.6137e+02 - logprior: 3.5439
Epoch 9/10
10/10 - 2s - loss: 157.3477 - loglik: -1.6133e+02 - logprior: 3.9806
Epoch 10/10
10/10 - 2s - loss: 156.9171 - loglik: -1.6130e+02 - logprior: 4.3832
Fitted a model with MAP estimate = -156.6813
Time for alignment: 53.0288
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 316.8553 - loglik: -2.4839e+02 - logprior: -6.8469e+01
Epoch 2/10
10/10 - 1s - loss: 236.1619 - loglik: -2.1896e+02 - logprior: -1.7200e+01
Epoch 3/10
10/10 - 1s - loss: 203.4624 - loglik: -1.9612e+02 - logprior: -7.3385e+00
Epoch 4/10
10/10 - 1s - loss: 189.7158 - loglik: -1.8595e+02 - logprior: -3.7651e+00
Epoch 5/10
10/10 - 1s - loss: 182.6345 - loglik: -1.8079e+02 - logprior: -1.8425e+00
Epoch 6/10
10/10 - 1s - loss: 178.1861 - loglik: -1.7725e+02 - logprior: -9.3341e-01
Epoch 7/10
10/10 - 2s - loss: 175.5459 - loglik: -1.7517e+02 - logprior: -3.7715e-01
Epoch 8/10
10/10 - 2s - loss: 174.3313 - loglik: -1.7429e+02 - logprior: -3.9723e-02
Epoch 9/10
10/10 - 2s - loss: 173.7111 - loglik: -1.7394e+02 - logprior: 0.2266
Epoch 10/10
10/10 - 1s - loss: 173.3488 - loglik: -1.7378e+02 - logprior: 0.4271
Fitted a model with MAP estimate = -173.1876
expansions: [(14, 2), (15, 1), (26, 1), (37, 1), (52, 1), (58, 6)]
discards: [0]
Fitting a model of length 82 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 251.7917 - loglik: -1.7482e+02 - logprior: -7.6969e+01
Epoch 2/2
10/10 - 2s - loss: 199.3234 - loglik: -1.6839e+02 - logprior: -3.0929e+01
Fitted a model with MAP estimate = -190.5923
expansions: [(30, 2)]
discards: [ 0 13]
Fitting a model of length 82 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 244.7524 - loglik: -1.6878e+02 - logprior: -7.5969e+01
Epoch 2/2
10/10 - 2s - loss: 194.6361 - loglik: -1.6640e+02 - logprior: -2.8237e+01
Fitted a model with MAP estimate = -184.2190
expansions: [(0, 4), (29, 1)]
discards: [0]
Fitting a model of length 86 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 226.6259 - loglik: -1.6593e+02 - logprior: -6.0694e+01
Epoch 2/10
10/10 - 2s - loss: 177.8589 - loglik: -1.6308e+02 - logprior: -1.4777e+01
Epoch 3/10
10/10 - 2s - loss: 167.6406 - loglik: -1.6238e+02 - logprior: -5.2603e+00
Epoch 4/10
10/10 - 2s - loss: 163.5527 - loglik: -1.6224e+02 - logprior: -1.3137e+00
Epoch 5/10
10/10 - 2s - loss: 161.5085 - loglik: -1.6242e+02 - logprior: 0.9109
Epoch 6/10
10/10 - 2s - loss: 160.3746 - loglik: -1.6261e+02 - logprior: 2.2390
Epoch 7/10
10/10 - 2s - loss: 159.6720 - loglik: -1.6272e+02 - logprior: 3.0505
Epoch 8/10
10/10 - 2s - loss: 159.2058 - loglik: -1.6281e+02 - logprior: 3.6022
Epoch 9/10
10/10 - 2s - loss: 158.8566 - loglik: -1.6288e+02 - logprior: 4.0247
Epoch 10/10
10/10 - 2s - loss: 158.5668 - loglik: -1.6293e+02 - logprior: 4.3605
Fitted a model with MAP estimate = -158.3917
Time for alignment: 51.3102
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 316.8490 - loglik: -2.4838e+02 - logprior: -6.8467e+01
Epoch 2/10
10/10 - 2s - loss: 236.1744 - loglik: -2.1897e+02 - logprior: -1.7202e+01
Epoch 3/10
10/10 - 1s - loss: 203.2930 - loglik: -1.9595e+02 - logprior: -7.3414e+00
Epoch 4/10
10/10 - 1s - loss: 189.7514 - loglik: -1.8604e+02 - logprior: -3.7152e+00
Epoch 5/10
10/10 - 2s - loss: 184.7432 - loglik: -1.8304e+02 - logprior: -1.6994e+00
Epoch 6/10
10/10 - 2s - loss: 182.1473 - loglik: -1.8156e+02 - logprior: -5.8365e-01
Epoch 7/10
10/10 - 1s - loss: 180.2983 - loglik: -1.8032e+02 - logprior: 0.0246
Epoch 8/10
10/10 - 2s - loss: 178.7412 - loglik: -1.7913e+02 - logprior: 0.3855
Epoch 9/10
10/10 - 2s - loss: 177.4992 - loglik: -1.7814e+02 - logprior: 0.6414
Epoch 10/10
10/10 - 2s - loss: 176.5310 - loglik: -1.7737e+02 - logprior: 0.8378
Fitted a model with MAP estimate = -176.0373
expansions: [(14, 2), (26, 2), (37, 1), (46, 1), (58, 6)]
discards: [0]
Fitting a model of length 82 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 253.5292 - loglik: -1.7660e+02 - logprior: -7.6931e+01
Epoch 2/2
10/10 - 1s - loss: 200.5050 - loglik: -1.6967e+02 - logprior: -3.0838e+01
Fitted a model with MAP estimate = -191.1883
expansions: []
discards: [0]
Fitting a model of length 81 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 244.2461 - loglik: -1.6894e+02 - logprior: -7.5306e+01
Epoch 2/2
10/10 - 2s - loss: 192.3859 - loglik: -1.6699e+02 - logprior: -2.5391e+01
Fitted a model with MAP estimate = -180.8909
expansions: [(0, 3), (5, 2), (7, 2)]
discards: [0]
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 227.2758 - loglik: -1.6644e+02 - logprior: -6.0839e+01
Epoch 2/10
10/10 - 2s - loss: 177.7542 - loglik: -1.6299e+02 - logprior: -1.4765e+01
Epoch 3/10
10/10 - 2s - loss: 167.3451 - loglik: -1.6218e+02 - logprior: -5.1677e+00
Epoch 4/10
10/10 - 2s - loss: 163.2622 - loglik: -1.6205e+02 - logprior: -1.2076e+00
Epoch 5/10
10/10 - 2s - loss: 161.0870 - loglik: -1.6208e+02 - logprior: 0.9881
Epoch 6/10
10/10 - 2s - loss: 159.9256 - loglik: -1.6224e+02 - logprior: 2.3106
Epoch 7/10
10/10 - 2s - loss: 159.1957 - loglik: -1.6233e+02 - logprior: 3.1388
Epoch 8/10
10/10 - 2s - loss: 158.7034 - loglik: -1.6239e+02 - logprior: 3.6893
Epoch 9/10
10/10 - 2s - loss: 158.3377 - loglik: -1.6243e+02 - logprior: 4.0893
Epoch 10/10
10/10 - 2s - loss: 158.0444 - loglik: -1.6246e+02 - logprior: 4.4177
Fitted a model with MAP estimate = -157.8734
Time for alignment: 52.0317
Computed alignments with likelihoods: ['-156.6813', '-158.3917', '-157.8734']
Best model has likelihood: -156.6813
SP score = 0.3374
Training of 3 independent models on file ghf5.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f332848a3a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34f45176d0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 225 on 2717 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 12s - loss: 777.0500 - loglik: -7.6320e+02 - logprior: -1.3848e+01
Epoch 2/10
10/10 - 9s - loss: 735.5807 - loglik: -7.3369e+02 - logprior: -1.8955e+00
Epoch 3/10
10/10 - 9s - loss: 694.2218 - loglik: -6.9441e+02 - logprior: 0.1894
Epoch 4/10
10/10 - 9s - loss: 671.4024 - loglik: -6.7192e+02 - logprior: 0.5191
Epoch 5/10
10/10 - 9s - loss: 648.8735 - loglik: -6.4925e+02 - logprior: 0.3806
Epoch 6/10
10/10 - 9s - loss: 640.0603 - loglik: -6.4022e+02 - logprior: 0.1597
Epoch 7/10
10/10 - 9s - loss: 636.4553 - loglik: -6.3653e+02 - logprior: 0.0714
Epoch 8/10
10/10 - 9s - loss: 628.9352 - loglik: -6.2912e+02 - logprior: 0.1858
Epoch 9/10
10/10 - 9s - loss: 627.9276 - loglik: -6.2816e+02 - logprior: 0.2329
Epoch 10/10
10/10 - 9s - loss: 625.5795 - loglik: -6.2578e+02 - logprior: 0.1966
Fitted a model with MAP estimate = -624.8887
expansions: [(0, 3), (27, 1), (45, 5), (49, 2), (51, 1), (53, 3), (86, 11), (92, 1), (94, 1), (98, 1), (112, 2), (118, 1), (148, 1), (154, 2), (172, 10), (175, 2), (191, 1), (198, 4), (207, 1), (208, 1)]
discards: [ 20 217 218]
Fitting a model of length 276 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 15s - loss: 701.6692 - loglik: -6.8989e+02 - logprior: -1.1779e+01
Epoch 2/2
20/20 - 12s - loss: 646.7288 - loglik: -6.4585e+02 - logprior: -8.8156e-01
Fitted a model with MAP estimate = -637.4641
expansions: [(24, 2)]
discards: [  1   4   5   6  44  45  46  47  48  49  58  62  64  92  93  94  95 206
 207 208 209 210 249]
Fitting a model of length 255 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 656.2184 - loglik: -6.4905e+02 - logprior: -7.1702e+00
Epoch 2/2
20/20 - 12s - loss: 639.7499 - loglik: -6.4025e+02 - logprior: 0.5037
Fitted a model with MAP estimate = -635.3933
expansions: [(0, 3), (4, 2), (20, 2), (45, 3), (53, 1), (92, 1), (226, 1), (228, 1)]
discards: [ 78  81  82 172 191]
Fitting a model of length 264 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 15s - loss: 657.4152 - loglik: -6.4434e+02 - logprior: -1.3074e+01
Epoch 2/10
20/20 - 12s - loss: 638.5633 - loglik: -6.3778e+02 - logprior: -7.8396e-01
Epoch 3/10
20/20 - 12s - loss: 630.5050 - loglik: -6.3208e+02 - logprior: 1.5747
Epoch 4/10
20/20 - 12s - loss: 625.3667 - loglik: -6.2776e+02 - logprior: 2.3895
Epoch 5/10
20/20 - 12s - loss: 616.8137 - loglik: -6.1942e+02 - logprior: 2.6102
Epoch 6/10
20/20 - 12s - loss: 613.1300 - loglik: -6.1590e+02 - logprior: 2.7678
Epoch 7/10
20/20 - 12s - loss: 608.2881 - loglik: -6.1117e+02 - logprior: 2.8858
Epoch 8/10
20/20 - 12s - loss: 607.3080 - loglik: -6.1030e+02 - logprior: 2.9946
Epoch 9/10
20/20 - 12s - loss: 603.6688 - loglik: -6.0675e+02 - logprior: 3.0861
Epoch 10/10
20/20 - 12s - loss: 601.9613 - loglik: -6.0513e+02 - logprior: 3.1648
Fitted a model with MAP estimate = -601.6948
Time for alignment: 297.5589
Fitting a model of length 225 on 2717 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 777.4017 - loglik: -7.6355e+02 - logprior: -1.3852e+01
Epoch 2/10
10/10 - 9s - loss: 732.1816 - loglik: -7.3028e+02 - logprior: -1.8973e+00
Epoch 3/10
10/10 - 9s - loss: 697.0464 - loglik: -6.9720e+02 - logprior: 0.1569
Epoch 4/10
10/10 - 9s - loss: 668.8949 - loglik: -6.6935e+02 - logprior: 0.4517
Epoch 5/10
10/10 - 9s - loss: 648.2324 - loglik: -6.4850e+02 - logprior: 0.2722
Epoch 6/10
10/10 - 9s - loss: 643.8995 - loglik: -6.4405e+02 - logprior: 0.1496
Epoch 7/10
10/10 - 9s - loss: 633.2272 - loglik: -6.3341e+02 - logprior: 0.1792
Epoch 8/10
10/10 - 9s - loss: 632.2966 - loglik: -6.3260e+02 - logprior: 0.3050
Epoch 9/10
10/10 - 9s - loss: 626.6562 - loglik: -6.2700e+02 - logprior: 0.3410
Epoch 10/10
10/10 - 9s - loss: 627.0166 - loglik: -6.2734e+02 - logprior: 0.3221
Fitted a model with MAP estimate = -625.4867
expansions: [(0, 4), (20, 1), (51, 1), (53, 3), (55, 1), (85, 11), (91, 1), (93, 1), (99, 1), (118, 1), (124, 1), (170, 13), (200, 3), (201, 1), (207, 1), (208, 1)]
discards: [217 218]
Fitting a model of length 268 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 15s - loss: 704.0641 - loglik: -6.9212e+02 - logprior: -1.1946e+01
Epoch 2/2
20/20 - 12s - loss: 649.0420 - loglik: -6.4822e+02 - logprior: -8.2571e-01
Fitted a model with MAP estimate = -638.8658
expansions: [(0, 7), (49, 1), (241, 2)]
discards: [  3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  57  59  87
  88 198 199 200 201]
Fitting a model of length 255 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 661.6097 - loglik: -6.4966e+02 - logprior: -1.1950e+01
Epoch 2/2
20/20 - 11s - loss: 640.6241 - loglik: -6.4023e+02 - logprior: -3.9077e-01
Fitted a model with MAP estimate = -636.0203
expansions: [(0, 5), (1, 1), (2, 1), (11, 1), (50, 1), (78, 2), (91, 1)]
discards: [187 188]
Fitting a model of length 265 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 15s - loss: 655.0102 - loglik: -6.4386e+02 - logprior: -1.1153e+01
Epoch 2/10
20/20 - 12s - loss: 636.6985 - loglik: -6.3607e+02 - logprior: -6.2562e-01
Epoch 3/10
20/20 - 12s - loss: 631.7610 - loglik: -6.3347e+02 - logprior: 1.7088
Epoch 4/10
20/20 - 12s - loss: 623.6379 - loglik: -6.2600e+02 - logprior: 2.3651
Epoch 5/10
20/20 - 12s - loss: 616.1921 - loglik: -6.1884e+02 - logprior: 2.6465
Epoch 6/10
20/20 - 12s - loss: 615.3970 - loglik: -6.1822e+02 - logprior: 2.8235
Epoch 7/10
20/20 - 12s - loss: 606.4463 - loglik: -6.0940e+02 - logprior: 2.9563
Epoch 8/10
20/20 - 12s - loss: 606.7037 - loglik: -6.0971e+02 - logprior: 3.0081
Fitted a model with MAP estimate = -603.7057
Time for alignment: 271.9984
Fitting a model of length 225 on 2717 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 776.3475 - loglik: -7.6251e+02 - logprior: -1.3839e+01
Epoch 2/10
10/10 - 9s - loss: 734.5895 - loglik: -7.3270e+02 - logprior: -1.8856e+00
Epoch 3/10
10/10 - 9s - loss: 698.3273 - loglik: -6.9850e+02 - logprior: 0.1757
Epoch 4/10
10/10 - 9s - loss: 665.9304 - loglik: -6.6639e+02 - logprior: 0.4633
Epoch 5/10
10/10 - 9s - loss: 653.5372 - loglik: -6.5377e+02 - logprior: 0.2336
Epoch 6/10
10/10 - 9s - loss: 641.3546 - loglik: -6.4140e+02 - logprior: 0.0423
Epoch 7/10
10/10 - 9s - loss: 633.7217 - loglik: -6.3382e+02 - logprior: 0.1013
Epoch 8/10
10/10 - 9s - loss: 630.7980 - loglik: -6.3104e+02 - logprior: 0.2449
Epoch 9/10
10/10 - 9s - loss: 626.6710 - loglik: -6.2690e+02 - logprior: 0.2285
Epoch 10/10
10/10 - 9s - loss: 626.6910 - loglik: -6.2688e+02 - logprior: 0.1930
Fitted a model with MAP estimate = -624.7734
expansions: [(0, 3), (45, 1), (52, 1), (54, 1), (57, 1), (64, 1), (86, 12), (92, 3), (93, 1), (110, 3), (113, 1), (114, 1), (115, 1), (117, 1), (150, 1), (153, 2), (169, 11), (170, 1), (198, 3), (201, 1), (207, 1), (208, 1)]
discards: [217 218]
Fitting a model of length 275 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 16s - loss: 703.6421 - loglik: -6.9178e+02 - logprior: -1.1864e+01
Epoch 2/2
20/20 - 12s - loss: 645.6722 - loglik: -6.4501e+02 - logprior: -6.5848e-01
Fitted a model with MAP estimate = -637.1863
expansions: [(0, 4), (245, 1)]
discards: [  1   2   3   4   5  86  87  94 113 135 136 141 186 203 204 205 247 248]
Fitting a model of length 262 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 658.8817 - loglik: -6.4698e+02 - logprior: -1.1901e+01
Epoch 2/2
20/20 - 12s - loss: 637.1285 - loglik: -6.3685e+02 - logprior: -2.8314e-01
Fitted a model with MAP estimate = -633.6476
expansions: [(0, 4), (85, 2)]
discards: [  0 194]
Fitting a model of length 266 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 15s - loss: 649.7147 - loglik: -6.4158e+02 - logprior: -8.1373e+00
Epoch 2/10
20/20 - 12s - loss: 634.1866 - loglik: -6.3518e+02 - logprior: 0.9966
Epoch 3/10
20/20 - 12s - loss: 627.6700 - loglik: -6.2962e+02 - logprior: 1.9489
Epoch 4/10
20/20 - 12s - loss: 623.7520 - loglik: -6.2624e+02 - logprior: 2.4853
Epoch 5/10
20/20 - 12s - loss: 616.6500 - loglik: -6.1934e+02 - logprior: 2.6947
Epoch 6/10
20/20 - 12s - loss: 612.3007 - loglik: -6.1524e+02 - logprior: 2.9360
Epoch 7/10
20/20 - 12s - loss: 607.2803 - loglik: -6.1040e+02 - logprior: 3.1177
Epoch 8/10
20/20 - 12s - loss: 604.9504 - loglik: -6.0819e+02 - logprior: 3.2433
Epoch 9/10
20/20 - 12s - loss: 603.0231 - loglik: -6.0640e+02 - logprior: 3.3722
Epoch 10/10
20/20 - 12s - loss: 600.9839 - loglik: -6.0454e+02 - logprior: 3.5569
Fitted a model with MAP estimate = -599.7146
Time for alignment: 298.8081
Computed alignments with likelihoods: ['-601.6948', '-603.7057', '-599.7146']
Best model has likelihood: -599.7146
SP score = 0.6372
Training of 3 independent models on file myb_DNA-binding.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f36991336a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f368860e130>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.5870 - loglik: -1.3135e+02 - logprior: -3.2362e+00
Epoch 2/10
19/19 - 1s - loss: 113.1535 - loglik: -1.1178e+02 - logprior: -1.3737e+00
Epoch 3/10
19/19 - 1s - loss: 105.7767 - loglik: -1.0423e+02 - logprior: -1.5432e+00
Epoch 4/10
19/19 - 1s - loss: 103.4901 - loglik: -1.0204e+02 - logprior: -1.4527e+00
Epoch 5/10
19/19 - 1s - loss: 102.5251 - loglik: -1.0108e+02 - logprior: -1.4421e+00
Epoch 6/10
19/19 - 1s - loss: 101.5818 - loglik: -1.0014e+02 - logprior: -1.4460e+00
Epoch 7/10
19/19 - 1s - loss: 101.3419 - loglik: -9.9903e+01 - logprior: -1.4390e+00
Epoch 8/10
19/19 - 1s - loss: 101.2591 - loglik: -9.9821e+01 - logprior: -1.4378e+00
Epoch 9/10
19/19 - 1s - loss: 100.8742 - loglik: -9.9441e+01 - logprior: -1.4334e+00
Epoch 10/10
19/19 - 1s - loss: 101.0816 - loglik: -9.9648e+01 - logprior: -1.4332e+00
Fitted a model with MAP estimate = -99.4913
expansions: [(10, 2), (12, 2), (13, 2), (14, 1), (20, 1), (22, 2), (28, 1), (29, 1), (30, 2)]
discards: [0]
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 105.8704 - loglik: -1.0165e+02 - logprior: -4.2217e+00
Epoch 2/2
19/19 - 1s - loss: 96.3375 - loglik: -9.4207e+01 - logprior: -2.1301e+00
Fitted a model with MAP estimate = -93.0982
expansions: [(0, 1)]
discards: [ 0 13 16 30]
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 96.9042 - loglik: -9.3859e+01 - logprior: -3.0449e+00
Epoch 2/2
19/19 - 1s - loss: 93.4897 - loglik: -9.2072e+01 - logprior: -1.4182e+00
Fitted a model with MAP estimate = -91.6007
expansions: []
discards: []
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 94.4029 - loglik: -9.1293e+01 - logprior: -3.1103e+00
Epoch 2/10
19/19 - 1s - loss: 91.7413 - loglik: -9.0318e+01 - logprior: -1.4229e+00
Epoch 3/10
19/19 - 1s - loss: 91.0664 - loglik: -8.9749e+01 - logprior: -1.3175e+00
Epoch 4/10
19/19 - 1s - loss: 90.4033 - loglik: -8.9116e+01 - logprior: -1.2871e+00
Epoch 5/10
19/19 - 1s - loss: 90.3496 - loglik: -8.9087e+01 - logprior: -1.2628e+00
Epoch 6/10
19/19 - 1s - loss: 89.8174 - loglik: -8.8569e+01 - logprior: -1.2486e+00
Epoch 7/10
19/19 - 1s - loss: 89.4326 - loglik: -8.8194e+01 - logprior: -1.2389e+00
Epoch 8/10
19/19 - 1s - loss: 89.2629 - loglik: -8.8032e+01 - logprior: -1.2314e+00
Epoch 9/10
19/19 - 1s - loss: 89.4522 - loglik: -8.8231e+01 - logprior: -1.2207e+00
Fitted a model with MAP estimate = -89.2098
Time for alignment: 46.0141
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.6368 - loglik: -1.3140e+02 - logprior: -3.2366e+00
Epoch 2/10
19/19 - 1s - loss: 113.8366 - loglik: -1.1248e+02 - logprior: -1.3605e+00
Epoch 3/10
19/19 - 1s - loss: 106.6390 - loglik: -1.0514e+02 - logprior: -1.4991e+00
Epoch 4/10
19/19 - 1s - loss: 104.4889 - loglik: -1.0308e+02 - logprior: -1.4059e+00
Epoch 5/10
19/19 - 1s - loss: 103.4616 - loglik: -1.0209e+02 - logprior: -1.3765e+00
Epoch 6/10
19/19 - 1s - loss: 103.0891 - loglik: -1.0172e+02 - logprior: -1.3684e+00
Epoch 7/10
19/19 - 1s - loss: 102.7758 - loglik: -1.0142e+02 - logprior: -1.3587e+00
Epoch 8/10
19/19 - 1s - loss: 102.4695 - loglik: -1.0112e+02 - logprior: -1.3545e+00
Epoch 9/10
19/19 - 1s - loss: 102.5565 - loglik: -1.0121e+02 - logprior: -1.3515e+00
Fitted a model with MAP estimate = -100.9789
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (22, 2), (28, 3)]
discards: [0]
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 108.2627 - loglik: -1.0406e+02 - logprior: -4.2020e+00
Epoch 2/2
19/19 - 1s - loss: 97.4551 - loglik: -9.5334e+01 - logprior: -2.1210e+00
Fitted a model with MAP estimate = -94.0762
expansions: [(0, 1), (37, 1)]
discards: [ 0 10 29]
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 97.8112 - loglik: -9.4742e+01 - logprior: -3.0695e+00
Epoch 2/2
19/19 - 1s - loss: 93.9933 - loglik: -9.2562e+01 - logprior: -1.4311e+00
Fitted a model with MAP estimate = -91.8299
expansions: []
discards: []
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 94.6573 - loglik: -9.1532e+01 - logprior: -3.1257e+00
Epoch 2/10
19/19 - 1s - loss: 91.8225 - loglik: -9.0398e+01 - logprior: -1.4241e+00
Epoch 3/10
19/19 - 1s - loss: 91.4222 - loglik: -9.0107e+01 - logprior: -1.3157e+00
Epoch 4/10
19/19 - 1s - loss: 90.4636 - loglik: -8.9180e+01 - logprior: -1.2834e+00
Epoch 5/10
19/19 - 1s - loss: 90.4561 - loglik: -8.9195e+01 - logprior: -1.2615e+00
Epoch 6/10
19/19 - 1s - loss: 89.9824 - loglik: -8.8735e+01 - logprior: -1.2471e+00
Epoch 7/10
19/19 - 1s - loss: 89.5260 - loglik: -8.8290e+01 - logprior: -1.2359e+00
Epoch 8/10
19/19 - 1s - loss: 89.6246 - loglik: -8.8399e+01 - logprior: -1.2260e+00
Fitted a model with MAP estimate = -89.4519
Time for alignment: 43.8949
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.6364 - loglik: -1.3139e+02 - logprior: -3.2428e+00
Epoch 2/10
19/19 - 1s - loss: 113.6197 - loglik: -1.1225e+02 - logprior: -1.3682e+00
Epoch 3/10
19/19 - 1s - loss: 106.3862 - loglik: -1.0495e+02 - logprior: -1.4402e+00
Epoch 4/10
19/19 - 1s - loss: 103.9133 - loglik: -1.0254e+02 - logprior: -1.3777e+00
Epoch 5/10
19/19 - 1s - loss: 103.0956 - loglik: -1.0172e+02 - logprior: -1.3731e+00
Epoch 6/10
19/19 - 1s - loss: 102.3982 - loglik: -1.0101e+02 - logprior: -1.3864e+00
Epoch 7/10
19/19 - 1s - loss: 102.2264 - loglik: -1.0085e+02 - logprior: -1.3786e+00
Epoch 8/10
19/19 - 1s - loss: 101.9079 - loglik: -1.0053e+02 - logprior: -1.3793e+00
Epoch 9/10
19/19 - 1s - loss: 101.6364 - loglik: -1.0026e+02 - logprior: -1.3756e+00
Epoch 10/10
19/19 - 1s - loss: 101.6354 - loglik: -1.0026e+02 - logprior: -1.3729e+00
Fitted a model with MAP estimate = -100.1964
expansions: [(6, 1), (10, 1), (12, 1), (13, 1), (14, 1), (20, 1), (23, 2), (29, 1), (30, 2)]
discards: [0]
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 108.1260 - loglik: -1.0391e+02 - logprior: -4.2119e+00
Epoch 2/2
19/19 - 1s - loss: 98.9120 - loglik: -9.6928e+01 - logprior: -1.9841e+00
Fitted a model with MAP estimate = -95.8944
expansions: [(0, 1)]
discards: [ 0 29]
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 99.5248 - loglik: -9.6440e+01 - logprior: -3.0850e+00
Epoch 2/2
19/19 - 1s - loss: 95.2335 - loglik: -9.3773e+01 - logprior: -1.4603e+00
Fitted a model with MAP estimate = -93.0740
expansions: [(35, 1)]
discards: []
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 95.4888 - loglik: -9.2374e+01 - logprior: -3.1151e+00
Epoch 2/10
19/19 - 1s - loss: 92.1438 - loglik: -9.0722e+01 - logprior: -1.4215e+00
Epoch 3/10
19/19 - 1s - loss: 91.2821 - loglik: -8.9969e+01 - logprior: -1.3133e+00
Epoch 4/10
19/19 - 1s - loss: 90.4527 - loglik: -8.9169e+01 - logprior: -1.2841e+00
Epoch 5/10
19/19 - 1s - loss: 90.4107 - loglik: -8.9152e+01 - logprior: -1.2590e+00
Epoch 6/10
19/19 - 1s - loss: 89.8447 - loglik: -8.8596e+01 - logprior: -1.2491e+00
Epoch 7/10
19/19 - 1s - loss: 89.7987 - loglik: -8.8563e+01 - logprior: -1.2354e+00
Epoch 8/10
19/19 - 1s - loss: 89.5731 - loglik: -8.8345e+01 - logprior: -1.2281e+00
Epoch 9/10
19/19 - 1s - loss: 89.6351 - loglik: -8.8416e+01 - logprior: -1.2192e+00
Fitted a model with MAP estimate = -89.4116
Time for alignment: 46.0675
Computed alignments with likelihoods: ['-89.2098', '-89.4519', '-89.4116']
Best model has likelihood: -89.2098
SP score = 0.9359
Training of 3 independent models on file ghf10.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f367f1c0310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34a6e7f1f0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 225 on 1502 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 668.3670 - loglik: -6.4338e+02 - logprior: -2.4986e+01
Epoch 2/10
10/10 - 8s - loss: 605.7750 - loglik: -6.0220e+02 - logprior: -3.5799e+00
Epoch 3/10
10/10 - 8s - loss: 551.6603 - loglik: -5.5186e+02 - logprior: 0.1980
Epoch 4/10
10/10 - 8s - loss: 509.4730 - loglik: -5.0980e+02 - logprior: 0.3296
Epoch 5/10
10/10 - 8s - loss: 485.5605 - loglik: -4.8548e+02 - logprior: -8.4300e-02
Epoch 6/10
10/10 - 8s - loss: 473.8462 - loglik: -4.7341e+02 - logprior: -4.3958e-01
Epoch 7/10
10/10 - 8s - loss: 469.9908 - loglik: -4.6915e+02 - logprior: -8.3696e-01
Epoch 8/10
10/10 - 8s - loss: 466.6615 - loglik: -4.6601e+02 - logprior: -6.5154e-01
Epoch 9/10
10/10 - 8s - loss: 465.6427 - loglik: -4.6511e+02 - logprior: -5.3284e-01
Epoch 10/10
10/10 - 8s - loss: 464.7001 - loglik: -4.6430e+02 - logprior: -3.9890e-01
Fitted a model with MAP estimate = -464.3051
expansions: [(24, 2), (25, 3), (37, 2), (38, 1), (39, 1), (49, 2), (50, 1), (52, 2), (53, 1), (54, 1), (67, 1), (68, 4), (69, 2), (70, 1), (71, 2), (72, 1), (73, 1), (76, 1), (85, 1), (101, 1), (102, 1), (103, 4), (110, 1), (132, 2), (133, 1), (134, 2), (135, 2), (136, 2), (138, 2), (140, 1), (157, 1), (158, 5), (159, 2), (167, 1), (173, 7), (176, 3), (198, 1)]
discards: [  1   2 200 223]
Fitting a model of length 290 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 471.0079 - loglik: -4.5474e+02 - logprior: -1.6271e+01
Epoch 2/2
15/15 - 10s - loss: 439.3156 - loglik: -4.3998e+02 - logprior: 0.6617
Fitted a model with MAP estimate = -428.8701
expansions: [(21, 1)]
discards: [ 22  88 135 173 176 234 235 236 237]
Fitting a model of length 282 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 451.1832 - loglik: -4.3594e+02 - logprior: -1.5240e+01
Epoch 2/2
15/15 - 9s - loss: 429.3544 - loglik: -4.3073e+02 - logprior: 1.3782
Fitted a model with MAP estimate = -426.0181
expansions: [(0, 3), (203, 1), (233, 1)]
discards: [165]
Fitting a model of length 286 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 13s - loss: 455.4172 - loglik: -4.3194e+02 - logprior: -2.3477e+01
Epoch 2/10
15/15 - 9s - loss: 428.0416 - loglik: -4.2727e+02 - logprior: -7.7037e-01
Epoch 3/10
15/15 - 9s - loss: 424.9520 - loglik: -4.2929e+02 - logprior: 4.3403
Epoch 4/10
15/15 - 9s - loss: 420.8626 - loglik: -4.2690e+02 - logprior: 6.0337
Epoch 5/10
15/15 - 10s - loss: 419.3990 - loglik: -4.2603e+02 - logprior: 6.6355
Epoch 6/10
15/15 - 9s - loss: 415.7420 - loglik: -4.2282e+02 - logprior: 7.0807
Epoch 7/10
15/15 - 9s - loss: 419.1348 - loglik: -4.2656e+02 - logprior: 7.4298
Fitted a model with MAP estimate = -417.3502
Time for alignment: 215.1863
Fitting a model of length 225 on 1502 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 668.2326 - loglik: -6.4324e+02 - logprior: -2.4990e+01
Epoch 2/10
10/10 - 8s - loss: 607.9620 - loglik: -6.0440e+02 - logprior: -3.5583e+00
Epoch 3/10
10/10 - 8s - loss: 552.7922 - loglik: -5.5307e+02 - logprior: 0.2757
Epoch 4/10
10/10 - 8s - loss: 511.5658 - loglik: -5.1221e+02 - logprior: 0.6451
Epoch 5/10
10/10 - 8s - loss: 484.6044 - loglik: -4.8505e+02 - logprior: 0.4430
Epoch 6/10
10/10 - 8s - loss: 474.4168 - loglik: -4.7456e+02 - logprior: 0.1460
Epoch 7/10
10/10 - 8s - loss: 468.1818 - loglik: -4.6795e+02 - logprior: -2.3631e-01
Epoch 8/10
10/10 - 8s - loss: 467.1236 - loglik: -4.6701e+02 - logprior: -1.1286e-01
Epoch 9/10
10/10 - 8s - loss: 466.8309 - loglik: -4.6688e+02 - logprior: 0.0459
Epoch 10/10
10/10 - 8s - loss: 464.5915 - loglik: -4.6481e+02 - logprior: 0.2235
Fitted a model with MAP estimate = -464.8663
expansions: [(25, 2), (26, 1), (27, 1), (40, 1), (41, 1), (42, 1), (52, 1), (54, 1), (56, 2), (57, 3), (69, 1), (70, 4), (71, 2), (72, 1), (73, 2), (74, 1), (75, 1), (77, 2), (86, 1), (102, 4), (104, 2), (132, 1), (133, 2), (134, 2), (135, 7), (137, 1), (157, 1), (158, 6), (174, 9)]
discards: [  1   2 223]
Fitting a model of length 286 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 473.0157 - loglik: -4.5687e+02 - logprior: -1.6145e+01
Epoch 2/2
15/15 - 9s - loss: 435.9381 - loglik: -4.3664e+02 - logprior: 0.7037
Fitted a model with MAP estimate = -429.9836
expansions: [(68, 1), (207, 1)]
discards: [ 23  88 131 170]
Fitting a model of length 284 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 449.8634 - loglik: -4.3482e+02 - logprior: -1.5044e+01
Epoch 2/2
15/15 - 9s - loss: 428.8442 - loglik: -4.3043e+02 - logprior: 1.5856
Fitted a model with MAP estimate = -425.0436
expansions: [(0, 3), (203, 2), (232, 1)]
discards: []
Fitting a model of length 290 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 13s - loss: 453.7519 - loglik: -4.3046e+02 - logprior: -2.3287e+01
Epoch 2/10
15/15 - 10s - loss: 427.3624 - loglik: -4.2669e+02 - logprior: -6.6915e-01
Epoch 3/10
15/15 - 9s - loss: 421.5138 - loglik: -4.2601e+02 - logprior: 4.4913
Epoch 4/10
15/15 - 10s - loss: 419.5537 - loglik: -4.2571e+02 - logprior: 6.1580
Epoch 5/10
15/15 - 10s - loss: 416.2774 - loglik: -4.2303e+02 - logprior: 6.7574
Epoch 6/10
15/15 - 10s - loss: 418.3406 - loglik: -4.2554e+02 - logprior: 7.1960
Fitted a model with MAP estimate = -416.3313
Time for alignment: 206.3060
Fitting a model of length 225 on 1502 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 668.2499 - loglik: -6.4326e+02 - logprior: -2.4989e+01
Epoch 2/10
10/10 - 8s - loss: 606.1347 - loglik: -6.0257e+02 - logprior: -3.5663e+00
Epoch 3/10
10/10 - 8s - loss: 549.5424 - loglik: -5.4965e+02 - logprior: 0.1119
Epoch 4/10
10/10 - 8s - loss: 509.8846 - loglik: -5.1014e+02 - logprior: 0.2600
Epoch 5/10
10/10 - 8s - loss: 487.7468 - loglik: -4.8759e+02 - logprior: -1.5373e-01
Epoch 6/10
10/10 - 8s - loss: 473.4477 - loglik: -4.7308e+02 - logprior: -3.6699e-01
Epoch 7/10
10/10 - 8s - loss: 467.9189 - loglik: -4.6730e+02 - logprior: -6.2051e-01
Epoch 8/10
10/10 - 8s - loss: 466.6811 - loglik: -4.6621e+02 - logprior: -4.7458e-01
Epoch 9/10
10/10 - 8s - loss: 463.9265 - loglik: -4.6351e+02 - logprior: -4.1290e-01
Epoch 10/10
10/10 - 8s - loss: 462.9568 - loglik: -4.6270e+02 - logprior: -2.5401e-01
Fitted a model with MAP estimate = -463.1517
expansions: [(23, 1), (24, 2), (25, 2), (39, 2), (41, 1), (43, 1), (47, 1), (50, 1), (51, 1), (53, 1), (54, 1), (55, 2), (66, 1), (69, 4), (70, 2), (71, 1), (73, 1), (74, 1), (75, 1), (77, 1), (80, 1), (102, 1), (103, 5), (133, 2), (134, 2), (135, 4), (136, 2), (137, 1), (138, 1), (140, 1), (157, 1), (158, 6), (174, 10)]
discards: [  1   2 223]
Fitting a model of length 287 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 473.0149 - loglik: -4.5688e+02 - logprior: -1.6132e+01
Epoch 2/2
15/15 - 9s - loss: 434.3709 - loglik: -4.3515e+02 - logprior: 0.7805
Fitted a model with MAP estimate = -427.9211
expansions: [(0, 3), (123, 1), (207, 1)]
discards: [ 23  89 169 177]
Fitting a model of length 288 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 459.5008 - loglik: -4.3531e+02 - logprior: -2.4195e+01
Epoch 2/2
15/15 - 10s - loss: 428.5558 - loglik: -4.2690e+02 - logprior: -1.6570e+00
Fitted a model with MAP estimate = -424.7556
expansions: [(206, 2)]
discards: [  0   1   2 168]
Fitting a model of length 286 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 445.2552 - loglik: -4.2834e+02 - logprior: -1.6912e+01
Epoch 2/10
15/15 - 9s - loss: 427.2660 - loglik: -4.2958e+02 - logprior: 2.3091
Epoch 3/10
15/15 - 9s - loss: 423.7967 - loglik: -4.2906e+02 - logprior: 5.2655
Epoch 4/10
15/15 - 9s - loss: 418.8703 - loglik: -4.2512e+02 - logprior: 6.2489
Epoch 5/10
15/15 - 9s - loss: 419.1233 - loglik: -4.2589e+02 - logprior: 6.7632
Fitted a model with MAP estimate = -418.0717
Time for alignment: 195.9838
Computed alignments with likelihoods: ['-417.3502', '-416.3313', '-418.0717']
Best model has likelihood: -416.3313
SP score = 0.9201
Training of 3 independent models on file il8.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32a9f68910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33203f0460>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 226.7957 - loglik: -1.8513e+02 - logprior: -4.1662e+01
Epoch 2/10
10/10 - 1s - loss: 179.9505 - loglik: -1.6877e+02 - logprior: -1.1177e+01
Epoch 3/10
10/10 - 1s - loss: 160.3691 - loglik: -1.5489e+02 - logprior: -5.4837e+00
Epoch 4/10
10/10 - 1s - loss: 150.9632 - loglik: -1.4751e+02 - logprior: -3.4521e+00
Epoch 5/10
10/10 - 1s - loss: 147.1320 - loglik: -1.4473e+02 - logprior: -2.4003e+00
Epoch 6/10
10/10 - 1s - loss: 144.8604 - loglik: -1.4299e+02 - logprior: -1.8742e+00
Epoch 7/10
10/10 - 1s - loss: 143.6081 - loglik: -1.4194e+02 - logprior: -1.6653e+00
Epoch 8/10
10/10 - 1s - loss: 142.8066 - loglik: -1.4132e+02 - logprior: -1.4833e+00
Epoch 9/10
10/10 - 1s - loss: 142.3153 - loglik: -1.4104e+02 - logprior: -1.2752e+00
Epoch 10/10
10/10 - 1s - loss: 141.9555 - loglik: -1.4083e+02 - logprior: -1.1257e+00
Fitted a model with MAP estimate = -141.8986
expansions: [(0, 2), (17, 1), (18, 3), (20, 1), (21, 1), (28, 1), (38, 1), (43, 3)]
discards: []
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 195.4668 - loglik: -1.4055e+02 - logprior: -5.4918e+01
Epoch 2/2
10/10 - 1s - loss: 151.2748 - loglik: -1.3455e+02 - logprior: -1.6724e+01
Fitted a model with MAP estimate = -143.0304
expansions: [(3, 1)]
discards: [ 0 22]
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 182.0355 - loglik: -1.3462e+02 - logprior: -4.7413e+01
Epoch 2/2
10/10 - 1s - loss: 152.1144 - loglik: -1.3378e+02 - logprior: -1.8338e+01
Fitted a model with MAP estimate = -147.2160
expansions: [(18, 1)]
discards: [0]
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 176.4752 - loglik: -1.3401e+02 - logprior: -4.2460e+01
Epoch 2/10
10/10 - 1s - loss: 144.2321 - loglik: -1.3275e+02 - logprior: -1.1486e+01
Epoch 3/10
10/10 - 1s - loss: 137.1296 - loglik: -1.3272e+02 - logprior: -4.4115e+00
Epoch 4/10
10/10 - 1s - loss: 133.7057 - loglik: -1.3181e+02 - logprior: -1.8967e+00
Epoch 5/10
10/10 - 1s - loss: 132.0526 - loglik: -1.3136e+02 - logprior: -6.8932e-01
Epoch 6/10
10/10 - 1s - loss: 131.0848 - loglik: -1.3095e+02 - logprior: -1.3264e-01
Epoch 7/10
10/10 - 1s - loss: 130.7271 - loglik: -1.3086e+02 - logprior: 0.1333
Epoch 8/10
10/10 - 1s - loss: 130.1316 - loglik: -1.3051e+02 - logprior: 0.3832
Epoch 9/10
10/10 - 1s - loss: 129.9947 - loglik: -1.3066e+02 - logprior: 0.6677
Epoch 10/10
10/10 - 1s - loss: 129.8563 - loglik: -1.3074e+02 - logprior: 0.8810
Fitted a model with MAP estimate = -129.6882
Time for alignment: 29.0209
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 226.5697 - loglik: -1.8491e+02 - logprior: -4.1663e+01
Epoch 2/10
10/10 - 1s - loss: 179.9877 - loglik: -1.6881e+02 - logprior: -1.1182e+01
Epoch 3/10
10/10 - 1s - loss: 160.8559 - loglik: -1.5539e+02 - logprior: -5.4616e+00
Epoch 4/10
10/10 - 1s - loss: 151.6815 - loglik: -1.4827e+02 - logprior: -3.4150e+00
Epoch 5/10
10/10 - 1s - loss: 147.4199 - loglik: -1.4504e+02 - logprior: -2.3776e+00
Epoch 6/10
10/10 - 1s - loss: 145.4064 - loglik: -1.4347e+02 - logprior: -1.9315e+00
Epoch 7/10
10/10 - 1s - loss: 144.2811 - loglik: -1.4255e+02 - logprior: -1.7310e+00
Epoch 8/10
10/10 - 1s - loss: 143.7203 - loglik: -1.4217e+02 - logprior: -1.5463e+00
Epoch 9/10
10/10 - 1s - loss: 143.3127 - loglik: -1.4199e+02 - logprior: -1.3224e+00
Epoch 10/10
10/10 - 1s - loss: 143.1444 - loglik: -1.4197e+02 - logprior: -1.1727e+00
Fitted a model with MAP estimate = -143.0443
expansions: [(0, 2), (17, 1), (18, 3), (20, 1), (21, 1), (24, 1), (35, 1), (43, 3)]
discards: []
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 197.9048 - loglik: -1.4294e+02 - logprior: -5.4964e+01
Epoch 2/2
10/10 - 1s - loss: 153.8020 - loglik: -1.3708e+02 - logprior: -1.6719e+01
Fitted a model with MAP estimate = -145.7066
expansions: [(3, 1)]
discards: [ 0 23]
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 184.6508 - loglik: -1.3722e+02 - logprior: -4.7433e+01
Epoch 2/2
10/10 - 1s - loss: 154.6932 - loglik: -1.3635e+02 - logprior: -1.8347e+01
Fitted a model with MAP estimate = -149.3419
expansions: [(18, 1)]
discards: [0]
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 178.5009 - loglik: -1.3604e+02 - logprior: -4.2457e+01
Epoch 2/10
10/10 - 1s - loss: 146.5911 - loglik: -1.3509e+02 - logprior: -1.1499e+01
Epoch 3/10
10/10 - 1s - loss: 138.8681 - loglik: -1.3444e+02 - logprior: -4.4327e+00
Epoch 4/10
10/10 - 1s - loss: 135.8715 - loglik: -1.3395e+02 - logprior: -1.9167e+00
Epoch 5/10
10/10 - 1s - loss: 134.0623 - loglik: -1.3335e+02 - logprior: -7.0883e-01
Epoch 6/10
10/10 - 1s - loss: 132.9614 - loglik: -1.3281e+02 - logprior: -1.5193e-01
Epoch 7/10
10/10 - 1s - loss: 132.4018 - loglik: -1.3252e+02 - logprior: 0.1161
Epoch 8/10
10/10 - 1s - loss: 131.9178 - loglik: -1.3229e+02 - logprior: 0.3738
Epoch 9/10
10/10 - 1s - loss: 131.5708 - loglik: -1.3224e+02 - logprior: 0.6666
Epoch 10/10
10/10 - 1s - loss: 131.3745 - loglik: -1.3226e+02 - logprior: 0.8808
Fitted a model with MAP estimate = -131.3173
Time for alignment: 29.2479
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 226.7425 - loglik: -1.8508e+02 - logprior: -4.1661e+01
Epoch 2/10
10/10 - 1s - loss: 180.2018 - loglik: -1.6902e+02 - logprior: -1.1179e+01
Epoch 3/10
10/10 - 1s - loss: 160.5459 - loglik: -1.5507e+02 - logprior: -5.4769e+00
Epoch 4/10
10/10 - 1s - loss: 150.7571 - loglik: -1.4733e+02 - logprior: -3.4300e+00
Epoch 5/10
10/10 - 1s - loss: 146.2827 - loglik: -1.4382e+02 - logprior: -2.4647e+00
Epoch 6/10
10/10 - 1s - loss: 144.2010 - loglik: -1.4220e+02 - logprior: -1.9977e+00
Epoch 7/10
10/10 - 1s - loss: 143.2241 - loglik: -1.4148e+02 - logprior: -1.7394e+00
Epoch 8/10
10/10 - 1s - loss: 142.4377 - loglik: -1.4091e+02 - logprior: -1.5284e+00
Epoch 9/10
10/10 - 1s - loss: 142.2316 - loglik: -1.4091e+02 - logprior: -1.3176e+00
Epoch 10/10
10/10 - 1s - loss: 142.0208 - loglik: -1.4086e+02 - logprior: -1.1637e+00
Fitted a model with MAP estimate = -141.8807
expansions: [(0, 2), (17, 1), (18, 2), (22, 2), (24, 1), (28, 1), (38, 1), (43, 3)]
discards: []
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 196.0722 - loglik: -1.4116e+02 - logprior: -5.4914e+01
Epoch 2/2
10/10 - 1s - loss: 151.9355 - loglik: -1.3517e+02 - logprior: -1.6768e+01
Fitted a model with MAP estimate = -143.7537
expansions: [(3, 1), (18, 1)]
discards: [ 0 27]
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 182.6321 - loglik: -1.3524e+02 - logprior: -4.7394e+01
Epoch 2/2
10/10 - 1s - loss: 152.7157 - loglik: -1.3440e+02 - logprior: -1.8316e+01
Fitted a model with MAP estimate = -147.4067
expansions: []
discards: [0]
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 176.5277 - loglik: -1.3406e+02 - logprior: -4.2470e+01
Epoch 2/10
10/10 - 1s - loss: 144.5788 - loglik: -1.3311e+02 - logprior: -1.1474e+01
Epoch 3/10
10/10 - 1s - loss: 136.9922 - loglik: -1.3259e+02 - logprior: -4.3972e+00
Epoch 4/10
10/10 - 1s - loss: 134.0228 - loglik: -1.3215e+02 - logprior: -1.8776e+00
Epoch 5/10
10/10 - 1s - loss: 132.3341 - loglik: -1.3164e+02 - logprior: -6.9377e-01
Epoch 6/10
10/10 - 1s - loss: 131.3521 - loglik: -1.3121e+02 - logprior: -1.4645e-01
Epoch 7/10
10/10 - 1s - loss: 130.7092 - loglik: -1.3082e+02 - logprior: 0.1100
Epoch 8/10
10/10 - 1s - loss: 130.5978 - loglik: -1.3096e+02 - logprior: 0.3671
Epoch 9/10
10/10 - 1s - loss: 130.2749 - loglik: -1.3093e+02 - logprior: 0.6536
Epoch 10/10
10/10 - 1s - loss: 129.9701 - loglik: -1.3084e+02 - logprior: 0.8656
Fitted a model with MAP estimate = -129.9477
Time for alignment: 29.3204
Computed alignments with likelihoods: ['-129.6882', '-131.3173', '-129.9477']
Best model has likelihood: -129.6882
SP score = 0.9090
Training of 3 independent models on file cytb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34eaf91b80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34f4082eb0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 250.4932 - loglik: -2.3730e+02 - logprior: -1.3198e+01
Epoch 2/10
11/11 - 1s - loss: 221.4748 - loglik: -2.1804e+02 - logprior: -3.4334e+00
Epoch 3/10
11/11 - 1s - loss: 199.6445 - loglik: -1.9748e+02 - logprior: -2.1652e+00
Epoch 4/10
11/11 - 1s - loss: 189.5311 - loglik: -1.8746e+02 - logprior: -2.0699e+00
Epoch 5/10
11/11 - 1s - loss: 186.1325 - loglik: -1.8409e+02 - logprior: -2.0387e+00
Epoch 6/10
11/11 - 1s - loss: 184.0665 - loglik: -1.8217e+02 - logprior: -1.8978e+00
Epoch 7/10
11/11 - 1s - loss: 182.5510 - loglik: -1.8072e+02 - logprior: -1.8291e+00
Epoch 8/10
11/11 - 1s - loss: 181.6026 - loglik: -1.7978e+02 - logprior: -1.8268e+00
Epoch 9/10
11/11 - 1s - loss: 181.6761 - loglik: -1.7985e+02 - logprior: -1.8278e+00
Fitted a model with MAP estimate = -180.8176
expansions: [(9, 1), (10, 1), (11, 2), (12, 2), (15, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (45, 1), (48, 1), (49, 1), (50, 2), (53, 2)]
discards: [0]
Fitting a model of length 79 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 205.2481 - loglik: -1.9024e+02 - logprior: -1.5010e+01
Epoch 2/2
11/11 - 1s - loss: 185.6179 - loglik: -1.7922e+02 - logprior: -6.4013e+00
Fitted a model with MAP estimate = -181.7333
expansions: [(0, 2)]
discards: [ 0 15 16 45 71]
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 188.5757 - loglik: -1.7679e+02 - logprior: -1.1787e+01
Epoch 2/2
11/11 - 1s - loss: 177.6742 - loglik: -1.7461e+02 - logprior: -3.0641e+00
Fitted a model with MAP estimate = -176.1112
expansions: []
discards: [0]
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 190.4972 - loglik: -1.7628e+02 - logprior: -1.4218e+01
Epoch 2/10
11/11 - 1s - loss: 179.8669 - loglik: -1.7567e+02 - logprior: -4.1978e+00
Epoch 3/10
11/11 - 1s - loss: 176.4487 - loglik: -1.7434e+02 - logprior: -2.1132e+00
Epoch 4/10
11/11 - 1s - loss: 175.1135 - loglik: -1.7375e+02 - logprior: -1.3617e+00
Epoch 5/10
11/11 - 1s - loss: 173.6758 - loglik: -1.7271e+02 - logprior: -9.6374e-01
Epoch 6/10
11/11 - 1s - loss: 173.1970 - loglik: -1.7233e+02 - logprior: -8.7061e-01
Epoch 7/10
11/11 - 1s - loss: 173.2619 - loglik: -1.7250e+02 - logprior: -7.6033e-01
Fitted a model with MAP estimate = -172.3511
Time for alignment: 44.2991
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 250.7743 - loglik: -2.3758e+02 - logprior: -1.3197e+01
Epoch 2/10
11/11 - 1s - loss: 220.2150 - loglik: -2.1678e+02 - logprior: -3.4311e+00
Epoch 3/10
11/11 - 1s - loss: 199.4193 - loglik: -1.9727e+02 - logprior: -2.1482e+00
Epoch 4/10
11/11 - 1s - loss: 188.7658 - loglik: -1.8671e+02 - logprior: -2.0576e+00
Epoch 5/10
11/11 - 1s - loss: 184.9969 - loglik: -1.8296e+02 - logprior: -2.0409e+00
Epoch 6/10
11/11 - 1s - loss: 183.5021 - loglik: -1.8161e+02 - logprior: -1.8954e+00
Epoch 7/10
11/11 - 1s - loss: 183.2803 - loglik: -1.8151e+02 - logprior: -1.7653e+00
Epoch 8/10
11/11 - 1s - loss: 181.6569 - loglik: -1.7990e+02 - logprior: -1.7576e+00
Epoch 9/10
11/11 - 1s - loss: 181.8386 - loglik: -1.8006e+02 - logprior: -1.7835e+00
Fitted a model with MAP estimate = -181.2556
expansions: [(9, 1), (10, 1), (11, 2), (12, 2), (17, 1), (24, 1), (26, 2), (32, 1), (35, 2), (45, 1), (49, 1), (50, 2), (53, 2)]
discards: [0]
Fitting a model of length 78 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 202.4874 - loglik: -1.8749e+02 - logprior: -1.4999e+01
Epoch 2/2
11/11 - 1s - loss: 184.2031 - loglik: -1.7786e+02 - logprior: -6.3401e+00
Fitted a model with MAP estimate = -181.2176
expansions: [(0, 3)]
discards: [ 0 15 16 34 70]
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 189.0121 - loglik: -1.7721e+02 - logprior: -1.1804e+01
Epoch 2/2
11/11 - 1s - loss: 177.9947 - loglik: -1.7487e+02 - logprior: -3.1246e+00
Fitted a model with MAP estimate = -176.3356
expansions: []
discards: [0 2]
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 191.5370 - loglik: -1.7723e+02 - logprior: -1.4312e+01
Epoch 2/10
11/11 - 1s - loss: 179.6174 - loglik: -1.7528e+02 - logprior: -4.3357e+00
Epoch 3/10
11/11 - 1s - loss: 176.6544 - loglik: -1.7457e+02 - logprior: -2.0850e+00
Epoch 4/10
11/11 - 1s - loss: 174.9380 - loglik: -1.7361e+02 - logprior: -1.3254e+00
Epoch 5/10
11/11 - 1s - loss: 174.2207 - loglik: -1.7324e+02 - logprior: -9.8194e-01
Epoch 6/10
11/11 - 1s - loss: 172.6299 - loglik: -1.7178e+02 - logprior: -8.5304e-01
Epoch 7/10
11/11 - 1s - loss: 173.1105 - loglik: -1.7236e+02 - logprior: -7.4697e-01
Fitted a model with MAP estimate = -172.4400
Time for alignment: 43.5445
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 250.7611 - loglik: -2.3756e+02 - logprior: -1.3197e+01
Epoch 2/10
11/11 - 1s - loss: 220.5348 - loglik: -2.1710e+02 - logprior: -3.4344e+00
Epoch 3/10
11/11 - 1s - loss: 198.8579 - loglik: -1.9672e+02 - logprior: -2.1424e+00
Epoch 4/10
11/11 - 1s - loss: 188.8406 - loglik: -1.8681e+02 - logprior: -2.0291e+00
Epoch 5/10
11/11 - 1s - loss: 185.7498 - loglik: -1.8378e+02 - logprior: -1.9740e+00
Epoch 6/10
11/11 - 1s - loss: 183.2560 - loglik: -1.8144e+02 - logprior: -1.8116e+00
Epoch 7/10
11/11 - 1s - loss: 183.1628 - loglik: -1.8149e+02 - logprior: -1.6774e+00
Epoch 8/10
11/11 - 1s - loss: 182.2396 - loglik: -1.8061e+02 - logprior: -1.6334e+00
Epoch 9/10
11/11 - 1s - loss: 181.6067 - loglik: -1.7996e+02 - logprior: -1.6513e+00
Epoch 10/10
11/11 - 1s - loss: 182.2361 - loglik: -1.8057e+02 - logprior: -1.6703e+00
Fitted a model with MAP estimate = -181.3220
expansions: [(11, 4), (17, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (45, 1), (49, 1), (50, 3), (53, 1)]
discards: [0]
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 202.2695 - loglik: -1.8732e+02 - logprior: -1.4946e+01
Epoch 2/2
11/11 - 1s - loss: 184.7275 - loglik: -1.7847e+02 - logprior: -6.2598e+00
Fitted a model with MAP estimate = -181.7594
expansions: [(0, 2)]
discards: [ 0 43]
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 188.1288 - loglik: -1.7636e+02 - logprior: -1.1771e+01
Epoch 2/2
11/11 - 1s - loss: 177.2187 - loglik: -1.7414e+02 - logprior: -3.0820e+00
Fitted a model with MAP estimate = -176.1387
expansions: []
discards: [ 0 65]
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 191.0229 - loglik: -1.7686e+02 - logprior: -1.4166e+01
Epoch 2/10
11/11 - 1s - loss: 179.8454 - loglik: -1.7569e+02 - logprior: -4.1528e+00
Epoch 3/10
11/11 - 1s - loss: 176.0513 - loglik: -1.7398e+02 - logprior: -2.0731e+00
Epoch 4/10
11/11 - 1s - loss: 175.4145 - loglik: -1.7406e+02 - logprior: -1.3520e+00
Epoch 5/10
11/11 - 1s - loss: 173.7324 - loglik: -1.7279e+02 - logprior: -9.4584e-01
Epoch 6/10
11/11 - 1s - loss: 173.7376 - loglik: -1.7288e+02 - logprior: -8.5422e-01
Fitted a model with MAP estimate = -172.8463
Time for alignment: 43.2230
Computed alignments with likelihoods: ['-172.3511', '-172.4400', '-172.8463']
Best model has likelihood: -172.3511
SP score = 0.8243
Training of 3 independent models on file kringle.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33203f5b80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32e868ae20>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 264.4399 - loglik: -2.2380e+02 - logprior: -4.0644e+01
Epoch 2/10
10/10 - 1s - loss: 208.5297 - loglik: -1.9778e+02 - logprior: -1.0748e+01
Epoch 3/10
10/10 - 1s - loss: 177.7617 - loglik: -1.7236e+02 - logprior: -5.4051e+00
Epoch 4/10
10/10 - 1s - loss: 155.8662 - loglik: -1.5216e+02 - logprior: -3.7087e+00
Epoch 5/10
10/10 - 1s - loss: 147.3499 - loglik: -1.4437e+02 - logprior: -2.9847e+00
Epoch 6/10
10/10 - 1s - loss: 144.2464 - loglik: -1.4169e+02 - logprior: -2.5555e+00
Epoch 7/10
10/10 - 1s - loss: 142.7475 - loglik: -1.4050e+02 - logprior: -2.2448e+00
Epoch 8/10
10/10 - 1s - loss: 142.1725 - loglik: -1.4009e+02 - logprior: -2.0823e+00
Epoch 9/10
10/10 - 1s - loss: 141.7485 - loglik: -1.3981e+02 - logprior: -1.9414e+00
Epoch 10/10
10/10 - 1s - loss: 141.4140 - loglik: -1.3956e+02 - logprior: -1.8521e+00
Fitted a model with MAP estimate = -141.3352
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 1), (32, 2), (34, 2), (44, 2), (50, 1), (52, 3)]
discards: []
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 170.7196 - loglik: -1.3389e+02 - logprior: -3.6830e+01
Epoch 2/2
10/10 - 1s - loss: 136.7372 - loglik: -1.2718e+02 - logprior: -9.5574e+00
Fitted a model with MAP estimate = -131.7778
expansions: []
discards: [ 0 46 59 70]
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 176.8795 - loglik: -1.3167e+02 - logprior: -4.5212e+01
Epoch 2/2
10/10 - 1s - loss: 148.8954 - loglik: -1.3046e+02 - logprior: -1.8438e+01
Fitted a model with MAP estimate = -144.8646
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 164.6664 - loglik: -1.2829e+02 - logprior: -3.6380e+01
Epoch 2/10
10/10 - 1s - loss: 135.5619 - loglik: -1.2653e+02 - logprior: -9.0361e+00
Epoch 3/10
10/10 - 1s - loss: 129.7686 - loglik: -1.2628e+02 - logprior: -3.4891e+00
Epoch 4/10
10/10 - 1s - loss: 127.3843 - loglik: -1.2610e+02 - logprior: -1.2870e+00
Epoch 5/10
10/10 - 1s - loss: 125.9037 - loglik: -1.2575e+02 - logprior: -1.5120e-01
Epoch 6/10
10/10 - 1s - loss: 125.5023 - loglik: -1.2596e+02 - logprior: 0.4549
Epoch 7/10
10/10 - 1s - loss: 125.1089 - loglik: -1.2590e+02 - logprior: 0.7958
Epoch 8/10
10/10 - 1s - loss: 124.9392 - loglik: -1.2597e+02 - logprior: 1.0353
Epoch 9/10
10/10 - 1s - loss: 124.5838 - loglik: -1.2582e+02 - logprior: 1.2339
Epoch 10/10
10/10 - 1s - loss: 124.5158 - loglik: -1.2592e+02 - logprior: 1.4041
Fitted a model with MAP estimate = -124.4001
Time for alignment: 31.0000
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 264.4095 - loglik: -2.2377e+02 - logprior: -4.0644e+01
Epoch 2/10
10/10 - 1s - loss: 208.5104 - loglik: -1.9775e+02 - logprior: -1.0756e+01
Epoch 3/10
10/10 - 1s - loss: 178.4454 - loglik: -1.7300e+02 - logprior: -5.4474e+00
Epoch 4/10
10/10 - 1s - loss: 156.8657 - loglik: -1.5309e+02 - logprior: -3.7765e+00
Epoch 5/10
10/10 - 1s - loss: 147.4327 - loglik: -1.4440e+02 - logprior: -3.0285e+00
Epoch 6/10
10/10 - 1s - loss: 144.9199 - loglik: -1.4238e+02 - logprior: -2.5388e+00
Epoch 7/10
10/10 - 1s - loss: 143.1702 - loglik: -1.4091e+02 - logprior: -2.2583e+00
Epoch 8/10
10/10 - 1s - loss: 142.2579 - loglik: -1.4018e+02 - logprior: -2.0781e+00
Epoch 9/10
10/10 - 1s - loss: 142.0353 - loglik: -1.4009e+02 - logprior: -1.9460e+00
Epoch 10/10
10/10 - 1s - loss: 141.6933 - loglik: -1.3982e+02 - logprior: -1.8726e+00
Fitted a model with MAP estimate = -141.4423
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 3)]
discards: []
Fitting a model of length 83 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 171.2773 - loglik: -1.3440e+02 - logprior: -3.6874e+01
Epoch 2/2
10/10 - 1s - loss: 136.9525 - loglik: -1.2727e+02 - logprior: -9.6862e+00
Fitted a model with MAP estimate = -131.8684
expansions: []
discards: [ 0 46 59 66 71]
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 176.9995 - loglik: -1.3180e+02 - logprior: -4.5198e+01
Epoch 2/2
10/10 - 1s - loss: 148.6770 - loglik: -1.3026e+02 - logprior: -1.8417e+01
Fitted a model with MAP estimate = -144.8134
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 164.6165 - loglik: -1.2831e+02 - logprior: -3.6303e+01
Epoch 2/10
10/10 - 1s - loss: 135.5058 - loglik: -1.2650e+02 - logprior: -9.0051e+00
Epoch 3/10
10/10 - 1s - loss: 129.6099 - loglik: -1.2615e+02 - logprior: -3.4623e+00
Epoch 4/10
10/10 - 1s - loss: 127.2169 - loglik: -1.2596e+02 - logprior: -1.2569e+00
Epoch 5/10
10/10 - 1s - loss: 125.9902 - loglik: -1.2587e+02 - logprior: -1.1930e-01
Epoch 6/10
10/10 - 1s - loss: 125.4646 - loglik: -1.2595e+02 - logprior: 0.4871
Epoch 7/10
10/10 - 1s - loss: 125.1647 - loglik: -1.2600e+02 - logprior: 0.8339
Epoch 8/10
10/10 - 1s - loss: 124.5960 - loglik: -1.2567e+02 - logprior: 1.0771
Epoch 9/10
10/10 - 1s - loss: 124.8146 - loglik: -1.2609e+02 - logprior: 1.2798
Fitted a model with MAP estimate = -124.4802
Time for alignment: 31.0463
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 264.3138 - loglik: -2.2367e+02 - logprior: -4.0644e+01
Epoch 2/10
10/10 - 1s - loss: 208.7507 - loglik: -1.9800e+02 - logprior: -1.0752e+01
Epoch 3/10
10/10 - 1s - loss: 178.6143 - loglik: -1.7314e+02 - logprior: -5.4735e+00
Epoch 4/10
10/10 - 1s - loss: 155.9772 - loglik: -1.5202e+02 - logprior: -3.9604e+00
Epoch 5/10
10/10 - 1s - loss: 146.5423 - loglik: -1.4319e+02 - logprior: -3.3477e+00
Epoch 6/10
10/10 - 1s - loss: 143.1287 - loglik: -1.4024e+02 - logprior: -2.8874e+00
Epoch 7/10
10/10 - 1s - loss: 141.5886 - loglik: -1.3901e+02 - logprior: -2.5752e+00
Epoch 8/10
10/10 - 1s - loss: 140.8161 - loglik: -1.3845e+02 - logprior: -2.3655e+00
Epoch 9/10
10/10 - 1s - loss: 140.4688 - loglik: -1.3826e+02 - logprior: -2.2127e+00
Epoch 10/10
10/10 - 1s - loss: 140.4836 - loglik: -1.3836e+02 - logprior: -2.1192e+00
Fitted a model with MAP estimate = -140.1799
expansions: [(2, 1), (5, 1), (10, 1), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 3)]
discards: []
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 172.2523 - loglik: -1.3532e+02 - logprior: -3.6932e+01
Epoch 2/2
10/10 - 1s - loss: 138.1421 - loglik: -1.2827e+02 - logprior: -9.8750e+00
Fitted a model with MAP estimate = -132.8259
expansions: []
discards: [45 58 65 70]
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 163.0362 - loglik: -1.2714e+02 - logprior: -3.5898e+01
Epoch 2/2
10/10 - 1s - loss: 135.3026 - loglik: -1.2603e+02 - logprior: -9.2683e+00
Fitted a model with MAP estimate = -131.5358
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 175.7490 - loglik: -1.3082e+02 - logprior: -4.4931e+01
Epoch 2/10
10/10 - 1s - loss: 147.9263 - loglik: -1.2966e+02 - logprior: -1.8263e+01
Epoch 3/10
10/10 - 1s - loss: 141.6509 - loglik: -1.2888e+02 - logprior: -1.2775e+01
Epoch 4/10
10/10 - 1s - loss: 138.8747 - loglik: -1.2846e+02 - logprior: -1.0419e+01
Epoch 5/10
10/10 - 1s - loss: 135.6255 - loglik: -1.2812e+02 - logprior: -7.5080e+00
Epoch 6/10
10/10 - 1s - loss: 129.8913 - loglik: -1.2829e+02 - logprior: -1.5976e+00
Epoch 7/10
10/10 - 1s - loss: 127.8919 - loglik: -1.2844e+02 - logprior: 0.5474
Epoch 8/10
10/10 - 1s - loss: 127.4256 - loglik: -1.2834e+02 - logprior: 0.9131
Epoch 9/10
10/10 - 1s - loss: 127.4576 - loglik: -1.2857e+02 - logprior: 1.1171
Fitted a model with MAP estimate = -127.1568
Time for alignment: 30.8415
Computed alignments with likelihoods: ['-124.4001', '-124.4802', '-127.1568']
Best model has likelihood: -124.4001
SP score = 0.8483
Training of 3 independent models on file LIM.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32c842d2e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f36a9972700>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 172.2952 - loglik: -1.6668e+02 - logprior: -5.6180e+00
Epoch 2/10
15/15 - 1s - loss: 144.4570 - loglik: -1.4270e+02 - logprior: -1.7601e+00
Epoch 3/10
15/15 - 1s - loss: 130.7296 - loglik: -1.2895e+02 - logprior: -1.7818e+00
Epoch 4/10
15/15 - 1s - loss: 126.7600 - loglik: -1.2504e+02 - logprior: -1.7169e+00
Epoch 5/10
15/15 - 1s - loss: 125.6606 - loglik: -1.2403e+02 - logprior: -1.6314e+00
Epoch 6/10
15/15 - 1s - loss: 125.0302 - loglik: -1.2338e+02 - logprior: -1.6490e+00
Epoch 7/10
15/15 - 1s - loss: 124.7458 - loglik: -1.2312e+02 - logprior: -1.6263e+00
Epoch 8/10
15/15 - 1s - loss: 124.5412 - loglik: -1.2293e+02 - logprior: -1.6068e+00
Epoch 9/10
15/15 - 1s - loss: 124.2301 - loglik: -1.2263e+02 - logprior: -1.6021e+00
Epoch 10/10
15/15 - 1s - loss: 124.1249 - loglik: -1.2253e+02 - logprior: -1.5924e+00
Fitted a model with MAP estimate = -124.0080
expansions: [(9, 2), (11, 2), (12, 2), (13, 2), (24, 2), (28, 1), (29, 2), (30, 2), (32, 3), (34, 2)]
discards: [0]
Fitting a model of length 64 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 5s - loss: 138.1546 - loglik: -1.3122e+02 - logprior: -6.9345e+00
Epoch 2/2
15/15 - 1s - loss: 127.6824 - loglik: -1.2417e+02 - logprior: -3.5121e+00
Fitted a model with MAP estimate = -125.2637
expansions: [(0, 2)]
discards: [ 0 12 15 18 32 47 51]
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 127.7555 - loglik: -1.2263e+02 - logprior: -5.1207e+00
Epoch 2/2
15/15 - 1s - loss: 121.2241 - loglik: -1.1956e+02 - logprior: -1.6682e+00
Fitted a model with MAP estimate = -120.2575
expansions: []
discards: [0]
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 129.3338 - loglik: -1.2281e+02 - logprior: -6.5234e+00
Epoch 2/10
15/15 - 1s - loss: 122.1468 - loglik: -1.1996e+02 - logprior: -2.1914e+00
Epoch 3/10
15/15 - 1s - loss: 120.6020 - loglik: -1.1912e+02 - logprior: -1.4842e+00
Epoch 4/10
15/15 - 1s - loss: 120.0299 - loglik: -1.1871e+02 - logprior: -1.3172e+00
Epoch 5/10
15/15 - 1s - loss: 119.4928 - loglik: -1.1823e+02 - logprior: -1.2630e+00
Epoch 6/10
15/15 - 1s - loss: 119.0619 - loglik: -1.1782e+02 - logprior: -1.2459e+00
Epoch 7/10
15/15 - 1s - loss: 118.5842 - loglik: -1.1734e+02 - logprior: -1.2403e+00
Epoch 8/10
15/15 - 1s - loss: 118.7050 - loglik: -1.1748e+02 - logprior: -1.2279e+00
Fitted a model with MAP estimate = -118.1606
Time for alignment: 36.9624
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.3229 - loglik: -1.6670e+02 - logprior: -5.6190e+00
Epoch 2/10
15/15 - 1s - loss: 144.8597 - loglik: -1.4311e+02 - logprior: -1.7490e+00
Epoch 3/10
15/15 - 1s - loss: 131.1037 - loglik: -1.2933e+02 - logprior: -1.7772e+00
Epoch 4/10
15/15 - 1s - loss: 126.5944 - loglik: -1.2487e+02 - logprior: -1.7234e+00
Epoch 5/10
15/15 - 1s - loss: 125.4872 - loglik: -1.2386e+02 - logprior: -1.6227e+00
Epoch 6/10
15/15 - 1s - loss: 124.8081 - loglik: -1.2317e+02 - logprior: -1.6346e+00
Epoch 7/10
15/15 - 1s - loss: 124.1874 - loglik: -1.2257e+02 - logprior: -1.6156e+00
Epoch 8/10
15/15 - 1s - loss: 124.1316 - loglik: -1.2253e+02 - logprior: -1.5968e+00
Epoch 9/10
15/15 - 1s - loss: 123.8597 - loglik: -1.2227e+02 - logprior: -1.5933e+00
Epoch 10/10
15/15 - 1s - loss: 123.6171 - loglik: -1.2203e+02 - logprior: -1.5881e+00
Fitted a model with MAP estimate = -123.5947
expansions: [(9, 2), (10, 3), (11, 2), (15, 1), (27, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Fitting a model of length 61 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 136.7019 - loglik: -1.2980e+02 - logprior: -6.8974e+00
Epoch 2/2
15/15 - 1s - loss: 126.9733 - loglik: -1.2353e+02 - logprior: -3.4427e+00
Fitted a model with MAP estimate = -124.8739
expansions: []
discards: [11 15 45]
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 128.2228 - loglik: -1.2213e+02 - logprior: -6.0893e+00
Epoch 2/2
15/15 - 1s - loss: 121.7942 - loglik: -1.1976e+02 - logprior: -2.0352e+00
Fitted a model with MAP estimate = -120.8540
expansions: [(10, 1)]
discards: []
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 125.6353 - loglik: -1.2024e+02 - logprior: -5.3951e+00
Epoch 2/10
15/15 - 1s - loss: 121.2312 - loglik: -1.1931e+02 - logprior: -1.9163e+00
Epoch 3/10
15/15 - 1s - loss: 120.2326 - loglik: -1.1876e+02 - logprior: -1.4690e+00
Epoch 4/10
15/15 - 1s - loss: 119.7827 - loglik: -1.1846e+02 - logprior: -1.3269e+00
Epoch 5/10
15/15 - 1s - loss: 119.2241 - loglik: -1.1796e+02 - logprior: -1.2681e+00
Epoch 6/10
15/15 - 1s - loss: 118.7704 - loglik: -1.1751e+02 - logprior: -1.2566e+00
Epoch 7/10
15/15 - 1s - loss: 118.5160 - loglik: -1.1728e+02 - logprior: -1.2389e+00
Epoch 8/10
15/15 - 1s - loss: 118.1244 - loglik: -1.1690e+02 - logprior: -1.2281e+00
Epoch 9/10
15/15 - 1s - loss: 117.8929 - loglik: -1.1668e+02 - logprior: -1.2152e+00
Epoch 10/10
15/15 - 1s - loss: 117.6897 - loglik: -1.1649e+02 - logprior: -1.1968e+00
Fitted a model with MAP estimate = -117.5291
Time for alignment: 37.3056
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 172.3646 - loglik: -1.6675e+02 - logprior: -5.6182e+00
Epoch 2/10
15/15 - 1s - loss: 144.3674 - loglik: -1.4261e+02 - logprior: -1.7602e+00
Epoch 3/10
15/15 - 1s - loss: 130.1685 - loglik: -1.2838e+02 - logprior: -1.7884e+00
Epoch 4/10
15/15 - 1s - loss: 126.2231 - loglik: -1.2448e+02 - logprior: -1.7457e+00
Epoch 5/10
15/15 - 1s - loss: 125.0994 - loglik: -1.2343e+02 - logprior: -1.6738e+00
Epoch 6/10
15/15 - 1s - loss: 124.3752 - loglik: -1.2269e+02 - logprior: -1.6862e+00
Epoch 7/10
15/15 - 1s - loss: 123.9749 - loglik: -1.2231e+02 - logprior: -1.6638e+00
Epoch 8/10
15/15 - 1s - loss: 123.7968 - loglik: -1.2215e+02 - logprior: -1.6449e+00
Epoch 9/10
15/15 - 1s - loss: 123.5732 - loglik: -1.2193e+02 - logprior: -1.6414e+00
Epoch 10/10
15/15 - 1s - loss: 123.3216 - loglik: -1.2169e+02 - logprior: -1.6344e+00
Fitted a model with MAP estimate = -123.2753
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (25, 2), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 137.4338 - loglik: -1.3053e+02 - logprior: -6.9010e+00
Epoch 2/2
15/15 - 1s - loss: 126.9987 - loglik: -1.2354e+02 - logprior: -3.4601e+00
Fitted a model with MAP estimate = -124.8659
expansions: []
discards: [12 15 31 46]
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 128.4140 - loglik: -1.2234e+02 - logprior: -6.0770e+00
Epoch 2/2
15/15 - 1s - loss: 121.6692 - loglik: -1.1965e+02 - logprior: -2.0143e+00
Fitted a model with MAP estimate = -120.9195
expansions: []
discards: []
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 125.5796 - loglik: -1.2023e+02 - logprior: -5.3545e+00
Epoch 2/10
15/15 - 1s - loss: 121.5918 - loglik: -1.1967e+02 - logprior: -1.9207e+00
Epoch 3/10
15/15 - 1s - loss: 120.5614 - loglik: -1.1910e+02 - logprior: -1.4612e+00
Epoch 4/10
15/15 - 1s - loss: 119.8884 - loglik: -1.1856e+02 - logprior: -1.3234e+00
Epoch 5/10
15/15 - 1s - loss: 119.5934 - loglik: -1.1832e+02 - logprior: -1.2694e+00
Epoch 6/10
15/15 - 1s - loss: 119.1411 - loglik: -1.1789e+02 - logprior: -1.2519e+00
Epoch 7/10
15/15 - 1s - loss: 118.6876 - loglik: -1.1745e+02 - logprior: -1.2394e+00
Epoch 8/10
15/15 - 1s - loss: 118.2765 - loglik: -1.1706e+02 - logprior: -1.2201e+00
Epoch 9/10
15/15 - 1s - loss: 118.0564 - loglik: -1.1684e+02 - logprior: -1.2136e+00
Epoch 10/10
15/15 - 1s - loss: 117.9615 - loglik: -1.1677e+02 - logprior: -1.1915e+00
Fitted a model with MAP estimate = -117.7523
Time for alignment: 37.8972
Computed alignments with likelihoods: ['-118.1606', '-117.5291', '-117.7523']
Best model has likelihood: -117.5291
SP score = 0.9790
Training of 3 independent models on file annexin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34eb17a220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b8b1b370>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 197.4851 - loglik: -1.8316e+02 - logprior: -1.4328e+01
Epoch 2/10
10/10 - 1s - loss: 169.8823 - loglik: -1.6581e+02 - logprior: -4.0686e+00
Epoch 3/10
10/10 - 2s - loss: 151.9085 - loglik: -1.4949e+02 - logprior: -2.4151e+00
Epoch 4/10
10/10 - 2s - loss: 140.7645 - loglik: -1.3859e+02 - logprior: -2.1770e+00
Epoch 5/10
10/10 - 2s - loss: 135.6474 - loglik: -1.3341e+02 - logprior: -2.2341e+00
Epoch 6/10
10/10 - 2s - loss: 133.0841 - loglik: -1.3078e+02 - logprior: -2.3068e+00
Epoch 7/10
10/10 - 2s - loss: 131.1551 - loglik: -1.2888e+02 - logprior: -2.2749e+00
Epoch 8/10
10/10 - 2s - loss: 130.8434 - loglik: -1.2866e+02 - logprior: -2.1855e+00
Epoch 9/10
10/10 - 2s - loss: 130.3493 - loglik: -1.2822e+02 - logprior: -2.1261e+00
Epoch 10/10
10/10 - 2s - loss: 130.1371 - loglik: -1.2803e+02 - logprior: -2.1110e+00
Fitted a model with MAP estimate = -130.0649
expansions: [(4, 2), (5, 2), (6, 2), (8, 2), (24, 1), (30, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: [0]
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 148.4896 - loglik: -1.3249e+02 - logprior: -1.6002e+01
Epoch 2/2
10/10 - 2s - loss: 128.7609 - loglik: -1.2196e+02 - logprior: -6.8052e+00
Fitted a model with MAP estimate = -125.5159
expansions: [(0, 2)]
discards: [ 0 10 13]
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 132.6354 - loglik: -1.2000e+02 - logprior: -1.2635e+01
Epoch 2/2
10/10 - 1s - loss: 121.1932 - loglik: -1.1775e+02 - logprior: -3.4409e+00
Fitted a model with MAP estimate = -119.8154
expansions: []
discards: [0]
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 135.2143 - loglik: -1.2019e+02 - logprior: -1.5020e+01
Epoch 2/10
10/10 - 2s - loss: 124.0780 - loglik: -1.1940e+02 - logprior: -4.6736e+00
Epoch 3/10
10/10 - 1s - loss: 120.2906 - loglik: -1.1784e+02 - logprior: -2.4493e+00
Epoch 4/10
10/10 - 2s - loss: 118.9106 - loglik: -1.1721e+02 - logprior: -1.6972e+00
Epoch 5/10
10/10 - 2s - loss: 118.1995 - loglik: -1.1701e+02 - logprior: -1.1939e+00
Epoch 6/10
10/10 - 1s - loss: 117.1185 - loglik: -1.1604e+02 - logprior: -1.0747e+00
Epoch 7/10
10/10 - 2s - loss: 117.3097 - loglik: -1.1635e+02 - logprior: -9.6058e-01
Fitted a model with MAP estimate = -116.7660
Time for alignment: 51.0663
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 197.5981 - loglik: -1.8327e+02 - logprior: -1.4328e+01
Epoch 2/10
10/10 - 2s - loss: 170.2617 - loglik: -1.6619e+02 - logprior: -4.0673e+00
Epoch 3/10
10/10 - 1s - loss: 152.2726 - loglik: -1.4986e+02 - logprior: -2.4120e+00
Epoch 4/10
10/10 - 1s - loss: 141.2460 - loglik: -1.3907e+02 - logprior: -2.1758e+00
Epoch 5/10
10/10 - 2s - loss: 135.9516 - loglik: -1.3370e+02 - logprior: -2.2528e+00
Epoch 6/10
10/10 - 1s - loss: 133.8983 - loglik: -1.3156e+02 - logprior: -2.3366e+00
Epoch 7/10
10/10 - 1s - loss: 131.6397 - loglik: -1.2932e+02 - logprior: -2.3228e+00
Epoch 8/10
10/10 - 2s - loss: 131.0174 - loglik: -1.2875e+02 - logprior: -2.2651e+00
Epoch 9/10
10/10 - 2s - loss: 130.7530 - loglik: -1.2853e+02 - logprior: -2.2230e+00
Epoch 10/10
10/10 - 2s - loss: 130.2182 - loglik: -1.2801e+02 - logprior: -2.2105e+00
Fitted a model with MAP estimate = -130.1469
expansions: [(4, 2), (5, 1), (6, 1), (7, 2), (8, 2), (24, 1), (30, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: [0]
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 149.2082 - loglik: -1.3321e+02 - logprior: -1.5995e+01
Epoch 2/2
10/10 - 1s - loss: 128.5640 - loglik: -1.2177e+02 - logprior: -6.7942e+00
Fitted a model with MAP estimate = -125.5064
expansions: [(0, 2)]
discards: [ 0 10 13]
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 132.2129 - loglik: -1.1958e+02 - logprior: -1.2633e+01
Epoch 2/2
10/10 - 1s - loss: 121.2466 - loglik: -1.1781e+02 - logprior: -3.4398e+00
Fitted a model with MAP estimate = -119.8251
expansions: []
discards: [0]
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 135.3461 - loglik: -1.2028e+02 - logprior: -1.5062e+01
Epoch 2/10
10/10 - 1s - loss: 124.1124 - loglik: -1.1939e+02 - logprior: -4.7189e+00
Epoch 3/10
10/10 - 2s - loss: 120.2639 - loglik: -1.1781e+02 - logprior: -2.4545e+00
Epoch 4/10
10/10 - 1s - loss: 118.7935 - loglik: -1.1709e+02 - logprior: -1.6986e+00
Epoch 5/10
10/10 - 2s - loss: 118.3501 - loglik: -1.1716e+02 - logprior: -1.1947e+00
Epoch 6/10
10/10 - 1s - loss: 117.2558 - loglik: -1.1618e+02 - logprior: -1.0742e+00
Epoch 7/10
10/10 - 2s - loss: 116.9479 - loglik: -1.1598e+02 - logprior: -9.6824e-01
Epoch 8/10
10/10 - 2s - loss: 116.9290 - loglik: -1.1603e+02 - logprior: -8.9700e-01
Epoch 9/10
10/10 - 1s - loss: 116.2739 - loglik: -1.1538e+02 - logprior: -8.9153e-01
Epoch 10/10
10/10 - 1s - loss: 116.4781 - loglik: -1.1562e+02 - logprior: -8.5912e-01
Fitted a model with MAP estimate = -116.1595
Time for alignment: 54.5141
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 197.4920 - loglik: -1.8316e+02 - logprior: -1.4328e+01
Epoch 2/10
10/10 - 2s - loss: 170.3035 - loglik: -1.6623e+02 - logprior: -4.0719e+00
Epoch 3/10
10/10 - 2s - loss: 152.8564 - loglik: -1.5042e+02 - logprior: -2.4319e+00
Epoch 4/10
10/10 - 2s - loss: 142.1702 - loglik: -1.4003e+02 - logprior: -2.1399e+00
Epoch 5/10
10/10 - 2s - loss: 136.2351 - loglik: -1.3406e+02 - logprior: -2.1791e+00
Epoch 6/10
10/10 - 1s - loss: 132.5698 - loglik: -1.3030e+02 - logprior: -2.2664e+00
Epoch 7/10
10/10 - 2s - loss: 131.4283 - loglik: -1.2919e+02 - logprior: -2.2414e+00
Epoch 8/10
10/10 - 2s - loss: 130.1923 - loglik: -1.2803e+02 - logprior: -2.1644e+00
Epoch 9/10
10/10 - 2s - loss: 129.8165 - loglik: -1.2769e+02 - logprior: -2.1251e+00
Epoch 10/10
10/10 - 1s - loss: 129.4753 - loglik: -1.2736e+02 - logprior: -2.1155e+00
Fitted a model with MAP estimate = -129.5020
expansions: [(4, 2), (5, 1), (7, 2), (9, 2), (13, 1), (24, 1), (30, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 2)]
discards: [0]
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 148.0187 - loglik: -1.3203e+02 - logprior: -1.5992e+01
Epoch 2/2
10/10 - 1s - loss: 129.1136 - loglik: -1.2232e+02 - logprior: -6.7974e+00
Fitted a model with MAP estimate = -125.4789
expansions: [(0, 2)]
discards: [ 0  9 13]
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 132.5447 - loglik: -1.1991e+02 - logprior: -1.2636e+01
Epoch 2/2
10/10 - 2s - loss: 121.9627 - loglik: -1.1852e+02 - logprior: -3.4389e+00
Fitted a model with MAP estimate = -119.9311
expansions: []
discards: [0]
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 135.2246 - loglik: -1.2020e+02 - logprior: -1.5022e+01
Epoch 2/10
10/10 - 1s - loss: 124.0140 - loglik: -1.1934e+02 - logprior: -4.6702e+00
Epoch 3/10
10/10 - 2s - loss: 120.3397 - loglik: -1.1789e+02 - logprior: -2.4526e+00
Epoch 4/10
10/10 - 2s - loss: 119.3322 - loglik: -1.1764e+02 - logprior: -1.6935e+00
Epoch 5/10
10/10 - 1s - loss: 117.8649 - loglik: -1.1667e+02 - logprior: -1.1963e+00
Epoch 6/10
10/10 - 2s - loss: 117.3492 - loglik: -1.1627e+02 - logprior: -1.0781e+00
Epoch 7/10
10/10 - 1s - loss: 116.7666 - loglik: -1.1581e+02 - logprior: -9.6159e-01
Epoch 8/10
10/10 - 1s - loss: 116.9231 - loglik: -1.1602e+02 - logprior: -9.0291e-01
Fitted a model with MAP estimate = -116.5031
Time for alignment: 51.7224
Computed alignments with likelihoods: ['-116.7660', '-116.1595', '-116.5031']
Best model has likelihood: -116.1595
SP score = 0.9966
Training of 3 independent models on file hormone_rec.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34a70a6f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b8c87be0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 159 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 551.7209 - loglik: -5.4072e+02 - logprior: -1.0998e+01
Epoch 2/10
11/11 - 4s - loss: 504.8102 - loglik: -5.0293e+02 - logprior: -1.8771e+00
Epoch 3/10
11/11 - 4s - loss: 466.5868 - loglik: -4.6582e+02 - logprior: -7.6999e-01
Epoch 4/10
11/11 - 4s - loss: 442.7715 - loglik: -4.4211e+02 - logprior: -6.6564e-01
Epoch 5/10
11/11 - 4s - loss: 432.6732 - loglik: -4.3210e+02 - logprior: -5.6925e-01
Epoch 6/10
11/11 - 4s - loss: 427.4039 - loglik: -4.2697e+02 - logprior: -4.3315e-01
Epoch 7/10
11/11 - 4s - loss: 424.3186 - loglik: -4.2394e+02 - logprior: -3.8312e-01
Epoch 8/10
11/11 - 4s - loss: 423.2479 - loglik: -4.2290e+02 - logprior: -3.4556e-01
Epoch 9/10
11/11 - 4s - loss: 420.8134 - loglik: -4.2050e+02 - logprior: -3.1520e-01
Epoch 10/10
11/11 - 4s - loss: 420.8282 - loglik: -4.2047e+02 - logprior: -3.5463e-01
Fitted a model with MAP estimate = -420.1925
expansions: [(0, 8), (8, 3), (9, 2), (10, 1), (18, 1), (51, 1), (52, 1), (56, 1), (57, 2), (58, 1), (65, 1), (70, 1), (71, 1), (76, 1), (77, 2), (82, 1), (110, 1), (112, 1), (114, 1), (115, 1), (120, 1), (126, 1), (127, 1), (149, 2), (159, 5)]
discards: []
Fitting a model of length 201 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 9s - loss: 464.6753 - loglik: -4.5113e+02 - logprior: -1.3541e+01
Epoch 2/2
11/11 - 5s - loss: 432.0385 - loglik: -4.2885e+02 - logprior: -3.1845e+00
Fitted a model with MAP estimate = -424.4364
expansions: [(0, 21), (186, 1)]
discards: [  0   1   2   3   4   5   6   7   8  18  19  20  21  22 196 197 198 199
 200]
Fitting a model of length 204 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 8s - loss: 443.2217 - loglik: -4.3263e+02 - logprior: -1.0593e+01
Epoch 2/2
11/11 - 5s - loss: 424.1500 - loglik: -4.2219e+02 - logprior: -1.9591e+00
Fitted a model with MAP estimate = -421.5798
expansions: [(0, 18)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 21 22 23 24 25 26]
Fitting a model of length 199 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 8s - loss: 440.0052 - loglik: -4.2730e+02 - logprior: -1.2701e+01
Epoch 2/10
11/11 - 5s - loss: 423.4969 - loglik: -4.2097e+02 - logprior: -2.5315e+00
Epoch 3/10
11/11 - 5s - loss: 418.2000 - loglik: -4.1783e+02 - logprior: -3.7165e-01
Epoch 4/10
11/11 - 5s - loss: 413.9388 - loglik: -4.1417e+02 - logprior: 0.2307
Epoch 5/10
11/11 - 5s - loss: 410.1136 - loglik: -4.1068e+02 - logprior: 0.5639
Epoch 6/10
11/11 - 5s - loss: 408.4356 - loglik: -4.0933e+02 - logprior: 0.8987
Epoch 7/10
11/11 - 5s - loss: 404.9097 - loglik: -4.0592e+02 - logprior: 1.0140
Epoch 8/10
11/11 - 5s - loss: 403.6104 - loglik: -4.0470e+02 - logprior: 1.0878
Epoch 9/10
11/11 - 5s - loss: 403.0331 - loglik: -4.0418e+02 - logprior: 1.1468
Epoch 10/10
11/11 - 5s - loss: 399.4794 - loglik: -4.0066e+02 - logprior: 1.1825
Fitted a model with MAP estimate = -400.1461
Time for alignment: 144.4011
Fitting a model of length 159 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 551.0699 - loglik: -5.4005e+02 - logprior: -1.1023e+01
Epoch 2/10
11/11 - 4s - loss: 505.4529 - loglik: -5.0357e+02 - logprior: -1.8836e+00
Epoch 3/10
11/11 - 4s - loss: 464.1803 - loglik: -4.6345e+02 - logprior: -7.3038e-01
Epoch 4/10
11/11 - 4s - loss: 443.7203 - loglik: -4.4316e+02 - logprior: -5.5675e-01
Epoch 5/10
11/11 - 4s - loss: 431.9727 - loglik: -4.3155e+02 - logprior: -4.2019e-01
Epoch 6/10
11/11 - 4s - loss: 426.3510 - loglik: -4.2600e+02 - logprior: -3.4685e-01
Epoch 7/10
11/11 - 4s - loss: 424.0197 - loglik: -4.2367e+02 - logprior: -3.4973e-01
Epoch 8/10
11/11 - 4s - loss: 421.9457 - loglik: -4.2165e+02 - logprior: -2.9765e-01
Epoch 9/10
11/11 - 4s - loss: 420.8539 - loglik: -4.2061e+02 - logprior: -2.3895e-01
Epoch 10/10
11/11 - 4s - loss: 421.5338 - loglik: -4.2132e+02 - logprior: -2.1126e-01
Fitted a model with MAP estimate = -420.3046
expansions: [(0, 7), (8, 5), (34, 1), (42, 1), (56, 2), (57, 2), (58, 1), (70, 1), (71, 1), (72, 2), (79, 2), (80, 1), (83, 1), (107, 1), (114, 1), (119, 3), (120, 2), (127, 1), (150, 3), (159, 5)]
discards: []
Fitting a model of length 202 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 9s - loss: 464.5515 - loglik: -4.5097e+02 - logprior: -1.3582e+01
Epoch 2/2
11/11 - 5s - loss: 432.6606 - loglik: -4.2928e+02 - logprior: -3.3766e+00
Fitted a model with MAP estimate = -424.2711
expansions: [(0, 18)]
discards: [  0   1   2   3   4   5   6  71  93 104 151 197 198 199 200 201]
Fitting a model of length 204 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 8s - loss: 442.4170 - loglik: -4.3130e+02 - logprior: -1.1113e+01
Epoch 2/2
11/11 - 5s - loss: 423.0609 - loglik: -4.2102e+02 - logprior: -2.0386e+00
Fitted a model with MAP estimate = -420.4119
expansions: [(0, 18), (194, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]
Fitting a model of length 203 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 9s - loss: 438.0332 - loglik: -4.2533e+02 - logprior: -1.2705e+01
Epoch 2/10
11/11 - 5s - loss: 423.4662 - loglik: -4.2087e+02 - logprior: -2.5946e+00
Epoch 3/10
11/11 - 5s - loss: 417.9448 - loglik: -4.1752e+02 - logprior: -4.2420e-01
Epoch 4/10
11/11 - 5s - loss: 413.8812 - loglik: -4.1425e+02 - logprior: 0.3715
Epoch 5/10
11/11 - 5s - loss: 410.5309 - loglik: -4.1120e+02 - logprior: 0.6724
Epoch 6/10
11/11 - 5s - loss: 407.4844 - loglik: -4.0841e+02 - logprior: 0.9226
Epoch 7/10
11/11 - 5s - loss: 405.6299 - loglik: -4.0671e+02 - logprior: 1.0782
Epoch 8/10
11/11 - 5s - loss: 403.8806 - loglik: -4.0500e+02 - logprior: 1.1201
Epoch 9/10
11/11 - 5s - loss: 403.0259 - loglik: -4.0420e+02 - logprior: 1.1707
Epoch 10/10
11/11 - 5s - loss: 400.9658 - loglik: -4.0216e+02 - logprior: 1.1902
Fitted a model with MAP estimate = -400.5060
Time for alignment: 145.4384
Fitting a model of length 159 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 550.4534 - loglik: -5.3942e+02 - logprior: -1.1037e+01
Epoch 2/10
11/11 - 4s - loss: 505.7474 - loglik: -5.0385e+02 - logprior: -1.9006e+00
Epoch 3/10
11/11 - 4s - loss: 463.7533 - loglik: -4.6294e+02 - logprior: -8.1626e-01
Epoch 4/10
11/11 - 4s - loss: 442.5979 - loglik: -4.4188e+02 - logprior: -7.1910e-01
Epoch 5/10
11/11 - 4s - loss: 432.0096 - loglik: -4.3138e+02 - logprior: -6.3425e-01
Epoch 6/10
11/11 - 4s - loss: 426.2616 - loglik: -4.2576e+02 - logprior: -5.0453e-01
Epoch 7/10
11/11 - 4s - loss: 423.9469 - loglik: -4.2346e+02 - logprior: -4.8354e-01
Epoch 8/10
11/11 - 4s - loss: 422.8822 - loglik: -4.2245e+02 - logprior: -4.3265e-01
Epoch 9/10
11/11 - 4s - loss: 420.8223 - loglik: -4.2043e+02 - logprior: -3.9016e-01
Epoch 10/10
11/11 - 4s - loss: 420.2718 - loglik: -4.1988e+02 - logprior: -3.9475e-01
Fitted a model with MAP estimate = -420.1267
expansions: [(0, 8), (8, 3), (43, 1), (54, 1), (56, 2), (57, 1), (59, 2), (66, 2), (70, 1), (71, 3), (75, 2), (76, 1), (77, 1), (86, 1), (112, 1), (120, 1), (121, 3), (127, 1), (131, 1), (149, 1), (150, 2), (159, 5)]
discards: []
Fitting a model of length 203 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 8s - loss: 465.5843 - loglik: -4.5197e+02 - logprior: -1.3613e+01
Epoch 2/2
11/11 - 5s - loss: 431.9926 - loglik: -4.2867e+02 - logprior: -3.3241e+00
Fitted a model with MAP estimate = -425.5375
expansions: [(0, 21)]
discards: [  0   1   2   3   4   5   6   7  19  20  70  76  85 100 198 199 200 201
 202]
Fitting a model of length 205 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 9s - loss: 444.6241 - loglik: -4.3353e+02 - logprior: -1.1099e+01
Epoch 2/2
11/11 - 5s - loss: 425.9652 - loglik: -4.2391e+02 - logprior: -2.0528e+00
Fitted a model with MAP estimate = -422.5568
expansions: [(0, 18)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
 25 26 27 28]
Fitting a model of length 195 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 8s - loss: 442.2015 - loglik: -4.2941e+02 - logprior: -1.2789e+01
Epoch 2/10
11/11 - 5s - loss: 424.6822 - loglik: -4.2213e+02 - logprior: -2.5560e+00
Epoch 3/10
11/11 - 5s - loss: 421.2784 - loglik: -4.2088e+02 - logprior: -3.9624e-01
Epoch 4/10
11/11 - 5s - loss: 414.9223 - loglik: -4.1526e+02 - logprior: 0.3398
Epoch 5/10
11/11 - 5s - loss: 413.3659 - loglik: -4.1405e+02 - logprior: 0.6794
Epoch 6/10
11/11 - 5s - loss: 409.8351 - loglik: -4.1083e+02 - logprior: 0.9930
Epoch 7/10
11/11 - 5s - loss: 406.4530 - loglik: -4.0756e+02 - logprior: 1.1116
Epoch 8/10
11/11 - 5s - loss: 405.7440 - loglik: -4.0691e+02 - logprior: 1.1688
Epoch 9/10
11/11 - 5s - loss: 404.2213 - loglik: -4.0545e+02 - logprior: 1.2257
Epoch 10/10
11/11 - 5s - loss: 402.3245 - loglik: -4.0358e+02 - logprior: 1.2533
Fitted a model with MAP estimate = -402.1628
Time for alignment: 141.9556
Computed alignments with likelihoods: ['-400.1461', '-400.5060', '-402.1628']
Best model has likelihood: -400.1461
SP score = 0.7136
Training of 3 independent models on file Acetyltransf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32e8420be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34d2128fa0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 250.1440 - loglik: -2.4898e+02 - logprior: -1.1606e+00
Epoch 2/10
29/29 - 3s - loss: 223.4899 - loglik: -2.2265e+02 - logprior: -8.3488e-01
Epoch 3/10
29/29 - 3s - loss: 216.9057 - loglik: -2.1610e+02 - logprior: -8.0132e-01
Epoch 4/10
29/29 - 3s - loss: 215.0861 - loglik: -2.1429e+02 - logprior: -7.9508e-01
Epoch 5/10
29/29 - 3s - loss: 213.3958 - loglik: -2.1261e+02 - logprior: -7.8265e-01
Epoch 6/10
29/29 - 3s - loss: 211.8914 - loglik: -2.1111e+02 - logprior: -7.8208e-01
Epoch 7/10
29/29 - 3s - loss: 211.4987 - loglik: -2.1071e+02 - logprior: -7.9343e-01
Epoch 8/10
29/29 - 3s - loss: 210.5946 - loglik: -2.0979e+02 - logprior: -8.0238e-01
Epoch 9/10
29/29 - 3s - loss: 210.8079 - loglik: -2.1001e+02 - logprior: -8.0163e-01
Fitted a model with MAP estimate = -199.1276
expansions: [(1, 1), (2, 1), (14, 2), (15, 1), (22, 1), (27, 2), (38, 2), (41, 1), (43, 1), (44, 2), (46, 1), (48, 1), (49, 1), (50, 1), (51, 1), (53, 1)]
discards: []
Fitting a model of length 84 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 228.8379 - loglik: -2.2769e+02 - logprior: -1.1502e+00
Epoch 2/2
29/29 - 3s - loss: 215.4667 - loglik: -2.1470e+02 - logprior: -7.6802e-01
Fitted a model with MAP estimate = -195.4617
expansions: [(14, 4), (16, 1)]
discards: [18 19 20 21 35 47 57]
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 217.3730 - loglik: -2.1629e+02 - logprior: -1.0798e+00
Epoch 2/2
29/29 - 3s - loss: 213.8022 - loglik: -2.1312e+02 - logprior: -6.8191e-01
Fitted a model with MAP estimate = -194.6255
expansions: []
discards: [1]
Fitting a model of length 81 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 196.7754 - loglik: -1.9616e+02 - logprior: -6.1925e-01
Epoch 2/10
42/42 - 4s - loss: 194.4405 - loglik: -1.9395e+02 - logprior: -4.9387e-01
Epoch 3/10
42/42 - 4s - loss: 191.9913 - loglik: -1.9152e+02 - logprior: -4.7363e-01
Epoch 4/10
42/42 - 4s - loss: 191.0522 - loglik: -1.9059e+02 - logprior: -4.6361e-01
Epoch 5/10
42/42 - 4s - loss: 189.1535 - loglik: -1.8870e+02 - logprior: -4.5744e-01
Epoch 6/10
42/42 - 4s - loss: 187.4467 - loglik: -1.8699e+02 - logprior: -4.5736e-01
Epoch 7/10
42/42 - 4s - loss: 187.0053 - loglik: -1.8655e+02 - logprior: -4.5246e-01
Epoch 8/10
42/42 - 4s - loss: 185.5424 - loglik: -1.8509e+02 - logprior: -4.5405e-01
Epoch 9/10
42/42 - 4s - loss: 185.6871 - loglik: -1.8524e+02 - logprior: -4.4564e-01
Fitted a model with MAP estimate = -185.2064
Time for alignment: 120.1984
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 6s - loss: 249.4834 - loglik: -2.4833e+02 - logprior: -1.1582e+00
Epoch 2/10
29/29 - 3s - loss: 223.2745 - loglik: -2.2245e+02 - logprior: -8.2103e-01
Epoch 3/10
29/29 - 3s - loss: 217.5631 - loglik: -2.1676e+02 - logprior: -8.0262e-01
Epoch 4/10
29/29 - 3s - loss: 214.9518 - loglik: -2.1414e+02 - logprior: -8.1558e-01
Epoch 5/10
29/29 - 3s - loss: 213.5732 - loglik: -2.1276e+02 - logprior: -8.0964e-01
Epoch 6/10
29/29 - 3s - loss: 212.1068 - loglik: -2.1130e+02 - logprior: -8.0308e-01
Epoch 7/10
29/29 - 3s - loss: 211.9916 - loglik: -2.1119e+02 - logprior: -7.9795e-01
Epoch 8/10
29/29 - 3s - loss: 210.7984 - loglik: -2.1000e+02 - logprior: -8.0340e-01
Epoch 9/10
29/29 - 3s - loss: 211.1815 - loglik: -2.1038e+02 - logprior: -7.9902e-01
Fitted a model with MAP estimate = -199.1018
expansions: [(1, 1), (2, 1), (14, 2), (16, 3), (17, 2), (27, 2), (38, 2), (41, 1), (43, 2), (44, 2), (45, 2), (48, 1), (49, 1), (50, 1), (51, 1), (53, 1)]
discards: []
Fitting a model of length 89 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 227.9192 - loglik: -2.2676e+02 - logprior: -1.1603e+00
Epoch 2/2
29/29 - 3s - loss: 215.6444 - loglik: -2.1486e+02 - logprior: -7.8362e-01
Fitted a model with MAP estimate = -195.3384
expansions: [(17, 1)]
discards: [15 24 38 50 57 58 64]
Fitting a model of length 83 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 5s - loss: 217.1393 - loglik: -2.1605e+02 - logprior: -1.0869e+00
Epoch 2/2
29/29 - 3s - loss: 214.0268 - loglik: -2.1334e+02 - logprior: -6.8926e-01
Fitted a model with MAP estimate = -194.5590
expansions: [(21, 1)]
discards: [ 1 14]
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 197.2645 - loglik: -1.9655e+02 - logprior: -7.1518e-01
Epoch 2/10
42/42 - 4s - loss: 194.0348 - loglik: -1.9344e+02 - logprior: -5.9627e-01
Epoch 3/10
42/42 - 4s - loss: 192.2207 - loglik: -1.9163e+02 - logprior: -5.8855e-01
Epoch 4/10
42/42 - 4s - loss: 191.0426 - loglik: -1.9046e+02 - logprior: -5.8050e-01
Epoch 5/10
42/42 - 4s - loss: 189.0346 - loglik: -1.8846e+02 - logprior: -5.7769e-01
Epoch 6/10
42/42 - 4s - loss: 187.4360 - loglik: -1.8686e+02 - logprior: -5.7987e-01
Epoch 7/10
42/42 - 4s - loss: 186.9559 - loglik: -1.8637e+02 - logprior: -5.8847e-01
Epoch 8/10
42/42 - 4s - loss: 185.8185 - loglik: -1.8524e+02 - logprior: -5.8168e-01
Epoch 9/10
42/42 - 4s - loss: 185.0436 - loglik: -1.8446e+02 - logprior: -5.7894e-01
Epoch 10/10
42/42 - 4s - loss: 184.8004 - loglik: -1.8422e+02 - logprior: -5.8072e-01
Fitted a model with MAP estimate = -184.6306
Time for alignment: 128.0692
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 6s - loss: 249.7604 - loglik: -2.4860e+02 - logprior: -1.1618e+00
Epoch 2/10
29/29 - 3s - loss: 223.4644 - loglik: -2.2263e+02 - logprior: -8.3018e-01
Epoch 3/10
29/29 - 3s - loss: 217.1469 - loglik: -2.1635e+02 - logprior: -7.9867e-01
Epoch 4/10
29/29 - 3s - loss: 215.2067 - loglik: -2.1442e+02 - logprior: -7.8336e-01
Epoch 5/10
29/29 - 3s - loss: 213.8646 - loglik: -2.1309e+02 - logprior: -7.7676e-01
Epoch 6/10
29/29 - 3s - loss: 212.0246 - loglik: -2.1123e+02 - logprior: -7.9009e-01
Epoch 7/10
29/29 - 3s - loss: 211.1847 - loglik: -2.1039e+02 - logprior: -7.9021e-01
Epoch 8/10
29/29 - 3s - loss: 210.6364 - loglik: -2.0984e+02 - logprior: -7.9445e-01
Epoch 9/10
29/29 - 3s - loss: 210.2342 - loglik: -2.0944e+02 - logprior: -7.9256e-01
Epoch 10/10
29/29 - 3s - loss: 210.7137 - loglik: -2.0993e+02 - logprior: -7.8588e-01
Fitted a model with MAP estimate = -199.5667
expansions: [(1, 1), (2, 1), (12, 2), (14, 2), (27, 2), (38, 2), (41, 1), (43, 1), (44, 2), (46, 1), (48, 1), (49, 1), (50, 1), (51, 1), (53, 1)]
discards: []
Fitting a model of length 84 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 229.4571 - loglik: -2.2831e+02 - logprior: -1.1486e+00
Epoch 2/2
29/29 - 3s - loss: 215.7830 - loglik: -2.1502e+02 - logprior: -7.6417e-01
Fitted a model with MAP estimate = -195.4010
expansions: [(15, 2), (18, 2)]
discards: [20 35 47 57]
Fitting a model of length 84 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 217.2802 - loglik: -2.1620e+02 - logprior: -1.0808e+00
Epoch 2/2
29/29 - 3s - loss: 213.6283 - loglik: -2.1295e+02 - logprior: -6.8039e-01
Fitted a model with MAP estimate = -194.6840
expansions: []
discards: [1]
Fitting a model of length 83 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 197.0338 - loglik: -1.9640e+02 - logprior: -6.3537e-01
Epoch 2/10
42/42 - 4s - loss: 194.1877 - loglik: -1.9369e+02 - logprior: -4.9821e-01
Epoch 3/10
42/42 - 4s - loss: 192.0624 - loglik: -1.9158e+02 - logprior: -4.7895e-01
Epoch 4/10
42/42 - 4s - loss: 190.8955 - loglik: -1.9043e+02 - logprior: -4.6972e-01
Epoch 5/10
42/42 - 4s - loss: 188.9904 - loglik: -1.8853e+02 - logprior: -4.6327e-01
Epoch 6/10
42/42 - 4s - loss: 187.5204 - loglik: -1.8706e+02 - logprior: -4.6294e-01
Epoch 7/10
42/42 - 4s - loss: 186.8911 - loglik: -1.8643e+02 - logprior: -4.5789e-01
Epoch 8/10
42/42 - 4s - loss: 185.2498 - loglik: -1.8479e+02 - logprior: -4.6058e-01
Epoch 9/10
42/42 - 4s - loss: 185.5039 - loglik: -1.8505e+02 - logprior: -4.5510e-01
Fitted a model with MAP estimate = -185.0521
Time for alignment: 125.3207
Computed alignments with likelihoods: ['-185.2064', '-184.6306', '-185.0521']
Best model has likelihood: -184.6306
SP score = 0.4869
Training of 3 independent models on file phoslip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34f4527670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f369967e1f0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 375.1923 - loglik: -3.2994e+02 - logprior: -4.5255e+01
Epoch 2/10
10/10 - 1s - loss: 298.1615 - loglik: -2.8736e+02 - logprior: -1.0801e+01
Epoch 3/10
10/10 - 1s - loss: 249.9380 - loglik: -2.4505e+02 - logprior: -4.8842e+00
Epoch 4/10
10/10 - 1s - loss: 220.3621 - loglik: -2.1718e+02 - logprior: -3.1857e+00
Epoch 5/10
10/10 - 1s - loss: 209.2565 - loglik: -2.0685e+02 - logprior: -2.4100e+00
Epoch 6/10
10/10 - 1s - loss: 205.0060 - loglik: -2.0312e+02 - logprior: -1.8849e+00
Epoch 7/10
10/10 - 1s - loss: 202.7574 - loglik: -2.0121e+02 - logprior: -1.5453e+00
Epoch 8/10
10/10 - 1s - loss: 201.5942 - loglik: -2.0030e+02 - logprior: -1.2947e+00
Epoch 9/10
10/10 - 1s - loss: 201.1946 - loglik: -2.0006e+02 - logprior: -1.1348e+00
Epoch 10/10
10/10 - 1s - loss: 200.7661 - loglik: -1.9974e+02 - logprior: -1.0295e+00
Fitted a model with MAP estimate = -200.5207
expansions: [(10, 4), (12, 1), (13, 1), (16, 1), (30, 1), (31, 1), (44, 2), (45, 2), (51, 2), (52, 1), (63, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 249.4663 - loglik: -1.9830e+02 - logprior: -5.1170e+01
Epoch 2/2
10/10 - 1s - loss: 206.2243 - loglik: -1.8629e+02 - logprior: -1.9937e+01
Fitted a model with MAP estimate = -198.1979
expansions: [(0, 3)]
discards: [  0 100 101]
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 225.5025 - loglik: -1.8517e+02 - logprior: -4.0332e+01
Epoch 2/2
10/10 - 1s - loss: 191.4367 - loglik: -1.8212e+02 - logprior: -9.3169e+00
Fitted a model with MAP estimate = -185.9765
expansions: []
discards: [0 2]
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 234.1188 - loglik: -1.8457e+02 - logprior: -4.9547e+01
Epoch 2/10
10/10 - 1s - loss: 198.8646 - loglik: -1.8302e+02 - logprior: -1.5843e+01
Epoch 3/10
10/10 - 1s - loss: 186.5187 - loglik: -1.8178e+02 - logprior: -4.7411e+00
Epoch 4/10
10/10 - 1s - loss: 181.1466 - loglik: -1.8063e+02 - logprior: -5.1900e-01
Epoch 5/10
10/10 - 1s - loss: 178.4738 - loglik: -1.7945e+02 - logprior: 0.9745
Epoch 6/10
10/10 - 1s - loss: 177.2692 - loglik: -1.7902e+02 - logprior: 1.7466
Epoch 7/10
10/10 - 1s - loss: 176.8743 - loglik: -1.7926e+02 - logprior: 2.3868
Epoch 8/10
10/10 - 1s - loss: 176.2167 - loglik: -1.7912e+02 - logprior: 2.9058
Epoch 9/10
10/10 - 1s - loss: 175.5998 - loglik: -1.7886e+02 - logprior: 3.2557
Epoch 10/10
10/10 - 1s - loss: 175.1314 - loglik: -1.7865e+02 - logprior: 3.5166
Fitted a model with MAP estimate = -175.1158
Time for alignment: 44.9289
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 375.0770 - loglik: -3.2982e+02 - logprior: -4.5256e+01
Epoch 2/10
10/10 - 1s - loss: 299.3613 - loglik: -2.8856e+02 - logprior: -1.0806e+01
Epoch 3/10
10/10 - 1s - loss: 254.3572 - loglik: -2.4950e+02 - logprior: -4.8581e+00
Epoch 4/10
10/10 - 1s - loss: 224.3364 - loglik: -2.2146e+02 - logprior: -2.8749e+00
Epoch 5/10
10/10 - 1s - loss: 210.5733 - loglik: -2.0865e+02 - logprior: -1.9234e+00
Epoch 6/10
10/10 - 1s - loss: 205.7984 - loglik: -2.0438e+02 - logprior: -1.4186e+00
Epoch 7/10
10/10 - 1s - loss: 203.7231 - loglik: -2.0257e+02 - logprior: -1.1571e+00
Epoch 8/10
10/10 - 1s - loss: 202.3155 - loglik: -2.0133e+02 - logprior: -9.8125e-01
Epoch 9/10
10/10 - 1s - loss: 201.6511 - loglik: -2.0083e+02 - logprior: -8.1629e-01
Epoch 10/10
10/10 - 1s - loss: 201.3073 - loglik: -2.0060e+02 - logprior: -7.1204e-01
Fitted a model with MAP estimate = -200.9545
expansions: [(10, 4), (12, 1), (13, 2), (14, 1), (15, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (63, 1), (82, 4)]
discards: [0]
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 251.5417 - loglik: -2.0027e+02 - logprior: -5.1275e+01
Epoch 2/2
10/10 - 1s - loss: 208.7359 - loglik: -1.8857e+02 - logprior: -2.0168e+01
Fitted a model with MAP estimate = -200.7978
expansions: [(0, 3)]
discards: [  0  17 101]
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 226.4131 - loglik: -1.8610e+02 - logprior: -4.0315e+01
Epoch 2/2
10/10 - 1s - loss: 191.2242 - loglik: -1.8194e+02 - logprior: -9.2837e+00
Fitted a model with MAP estimate = -186.1631
expansions: []
discards: [ 0  2 54 55]
Fitting a model of length 113 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 235.8254 - loglik: -1.8638e+02 - logprior: -4.9449e+01
Epoch 2/10
10/10 - 1s - loss: 199.9661 - loglik: -1.8430e+02 - logprior: -1.5670e+01
Epoch 3/10
10/10 - 1s - loss: 187.9311 - loglik: -1.8332e+02 - logprior: -4.6072e+00
Epoch 4/10
10/10 - 1s - loss: 182.3412 - loglik: -1.8188e+02 - logprior: -4.5730e-01
Epoch 5/10
10/10 - 1s - loss: 180.2005 - loglik: -1.8120e+02 - logprior: 0.9947
Epoch 6/10
10/10 - 1s - loss: 178.8647 - loglik: -1.8062e+02 - logprior: 1.7546
Epoch 7/10
10/10 - 1s - loss: 177.9688 - loglik: -1.8037e+02 - logprior: 2.4038
Epoch 8/10
10/10 - 1s - loss: 177.4209 - loglik: -1.8035e+02 - logprior: 2.9300
Epoch 9/10
10/10 - 1s - loss: 177.2553 - loglik: -1.8053e+02 - logprior: 3.2739
Epoch 10/10
10/10 - 1s - loss: 176.7181 - loglik: -1.8025e+02 - logprior: 3.5348
Fitted a model with MAP estimate = -176.5229
Time for alignment: 43.0609
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 375.1221 - loglik: -3.2986e+02 - logprior: -4.5259e+01
Epoch 2/10
10/10 - 1s - loss: 298.8611 - loglik: -2.8805e+02 - logprior: -1.0811e+01
Epoch 3/10
10/10 - 1s - loss: 251.4160 - loglik: -2.4648e+02 - logprior: -4.9383e+00
Epoch 4/10
10/10 - 1s - loss: 221.3223 - loglik: -2.1817e+02 - logprior: -3.1509e+00
Epoch 5/10
10/10 - 1s - loss: 209.0535 - loglik: -2.0682e+02 - logprior: -2.2308e+00
Epoch 6/10
10/10 - 1s - loss: 204.6950 - loglik: -2.0296e+02 - logprior: -1.7375e+00
Epoch 7/10
10/10 - 1s - loss: 202.7658 - loglik: -2.0126e+02 - logprior: -1.5056e+00
Epoch 8/10
10/10 - 1s - loss: 201.5138 - loglik: -2.0018e+02 - logprior: -1.3380e+00
Epoch 9/10
10/10 - 1s - loss: 201.0493 - loglik: -1.9984e+02 - logprior: -1.2078e+00
Epoch 10/10
10/10 - 1s - loss: 200.2629 - loglik: -1.9915e+02 - logprior: -1.1089e+00
Fitted a model with MAP estimate = -200.3000
expansions: [(10, 2), (11, 2), (12, 2), (13, 1), (14, 2), (15, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (63, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 251.0709 - loglik: -1.9984e+02 - logprior: -5.1232e+01
Epoch 2/2
10/10 - 1s - loss: 207.1076 - loglik: -1.8690e+02 - logprior: -2.0212e+01
Fitted a model with MAP estimate = -199.1104
expansions: [(0, 3)]
discards: [  0  10  20 102 103]
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 226.3674 - loglik: -1.8599e+02 - logprior: -4.0382e+01
Epoch 2/2
10/10 - 1s - loss: 191.5806 - loglik: -1.8225e+02 - logprior: -9.3271e+00
Fitted a model with MAP estimate = -186.2950
expansions: []
discards: [0 2]
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 234.2445 - loglik: -1.8482e+02 - logprior: -4.9423e+01
Epoch 2/10
10/10 - 1s - loss: 198.4941 - loglik: -1.8314e+02 - logprior: -1.5355e+01
Epoch 3/10
10/10 - 1s - loss: 186.0458 - loglik: -1.8164e+02 - logprior: -4.4097e+00
Epoch 4/10
10/10 - 1s - loss: 180.6800 - loglik: -1.8023e+02 - logprior: -4.5205e-01
Epoch 5/10
10/10 - 1s - loss: 178.6282 - loglik: -1.7960e+02 - logprior: 0.9718
Epoch 6/10
10/10 - 1s - loss: 177.2253 - loglik: -1.7897e+02 - logprior: 1.7398
Epoch 7/10
10/10 - 1s - loss: 176.7181 - loglik: -1.7911e+02 - logprior: 2.3929
Epoch 8/10
10/10 - 1s - loss: 175.9830 - loglik: -1.7890e+02 - logprior: 2.9134
Epoch 9/10
10/10 - 1s - loss: 175.4861 - loglik: -1.7874e+02 - logprior: 3.2575
Epoch 10/10
10/10 - 1s - loss: 175.4149 - loglik: -1.7893e+02 - logprior: 3.5180
Fitted a model with MAP estimate = -175.0244
Time for alignment: 43.8846
Computed alignments with likelihoods: ['-175.1158', '-176.5229', '-175.0244']
Best model has likelihood: -175.0244
SP score = 0.9250
Training of 3 independent models on file cah.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34eb353d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34f4245e80>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 184 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 644.5245 - loglik: -6.1609e+02 - logprior: -2.8438e+01
Epoch 2/10
10/10 - 4s - loss: 573.3050 - loglik: -5.6851e+02 - logprior: -4.7935e+00
Epoch 3/10
10/10 - 4s - loss: 513.0160 - loglik: -5.1194e+02 - logprior: -1.0720e+00
Epoch 4/10
10/10 - 4s - loss: 474.2338 - loglik: -4.7366e+02 - logprior: -5.7026e-01
Epoch 5/10
10/10 - 4s - loss: 459.8131 - loglik: -4.5954e+02 - logprior: -2.6918e-01
Epoch 6/10
10/10 - 4s - loss: 455.7825 - loglik: -4.5569e+02 - logprior: -8.7943e-02
Epoch 7/10
10/10 - 4s - loss: 452.3174 - loglik: -4.5244e+02 - logprior: 0.1176
Epoch 8/10
10/10 - 4s - loss: 451.0132 - loglik: -4.5126e+02 - logprior: 0.2511
Epoch 9/10
10/10 - 4s - loss: 451.0607 - loglik: -4.5141e+02 - logprior: 0.3540
Fitted a model with MAP estimate = -450.2863
expansions: [(14, 1), (15, 1), (16, 2), (28, 1), (29, 1), (30, 2), (31, 2), (40, 1), (41, 2), (42, 1), (50, 1), (51, 1), (54, 4), (90, 1), (91, 7), (112, 1), (113, 2), (114, 1), (115, 4), (118, 1), (119, 1), (120, 3), (130, 2), (157, 1), (163, 1), (166, 2), (167, 8)]
discards: [0]
Fitting a model of length 238 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 487.8729 - loglik: -4.5606e+02 - logprior: -3.1814e+01
Epoch 2/2
10/10 - 6s - loss: 451.5448 - loglik: -4.4067e+02 - logprior: -1.0870e+01
Fitted a model with MAP estimate = -443.7996
expansions: [(0, 4), (70, 2), (147, 1)]
discards: [  0  17  73  74 142]
Fitting a model of length 240 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 464.5984 - loglik: -4.3996e+02 - logprior: -2.4635e+01
Epoch 2/2
10/10 - 6s - loss: 436.1900 - loglik: -4.3275e+02 - logprior: -3.4408e+00
Fitted a model with MAP estimate = -431.7029
expansions: [(119, 1)]
discards: [  1   2   3 113 114 115 116 213]
Fitting a model of length 233 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 461.1752 - loglik: -4.3725e+02 - logprior: -2.3928e+01
Epoch 2/10
10/10 - 6s - loss: 436.8963 - loglik: -4.3390e+02 - logprior: -2.9988e+00
Epoch 3/10
10/10 - 6s - loss: 431.4581 - loglik: -4.3282e+02 - logprior: 1.3575
Epoch 4/10
10/10 - 6s - loss: 428.8688 - loglik: -4.3194e+02 - logprior: 3.0737
Epoch 5/10
10/10 - 6s - loss: 426.4508 - loglik: -4.3036e+02 - logprior: 3.9112
Epoch 6/10
10/10 - 6s - loss: 426.6447 - loglik: -4.3111e+02 - logprior: 4.4647
Fitted a model with MAP estimate = -425.5517
Time for alignment: 125.1903
Fitting a model of length 184 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 643.5837 - loglik: -6.1514e+02 - logprior: -2.8446e+01
Epoch 2/10
10/10 - 4s - loss: 574.4621 - loglik: -5.6966e+02 - logprior: -4.8028e+00
Epoch 3/10
10/10 - 4s - loss: 512.3403 - loglik: -5.1125e+02 - logprior: -1.0939e+00
Epoch 4/10
10/10 - 4s - loss: 475.3161 - loglik: -4.7466e+02 - logprior: -6.5354e-01
Epoch 5/10
10/10 - 4s - loss: 462.7380 - loglik: -4.6254e+02 - logprior: -1.9978e-01
Epoch 6/10
10/10 - 4s - loss: 459.3390 - loglik: -4.5937e+02 - logprior: 0.0299
Epoch 7/10
10/10 - 4s - loss: 456.8412 - loglik: -4.5700e+02 - logprior: 0.1550
Epoch 8/10
10/10 - 4s - loss: 455.3617 - loglik: -4.5560e+02 - logprior: 0.2346
Epoch 9/10
10/10 - 4s - loss: 455.8467 - loglik: -4.5619e+02 - logprior: 0.3407
Fitted a model with MAP estimate = -454.5343
expansions: [(14, 1), (15, 1), (28, 2), (29, 2), (30, 2), (31, 2), (40, 1), (41, 2), (42, 1), (50, 1), (51, 1), (54, 3), (61, 1), (80, 1), (90, 1), (112, 1), (114, 2), (115, 2), (116, 3), (118, 2), (119, 2), (120, 3), (130, 2), (163, 1), (164, 1), (166, 2), (167, 8)]
discards: [0]
Fitting a model of length 234 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 491.1053 - loglik: -4.5929e+02 - logprior: -3.1818e+01
Epoch 2/2
10/10 - 6s - loss: 454.6605 - loglik: -4.4396e+02 - logprior: -1.0704e+01
Fitted a model with MAP estimate = -448.2887
expansions: [(0, 3), (81, 1), (138, 1)]
discards: [  0  29 143 150 153 167]
Fitting a model of length 233 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 467.4778 - loglik: -4.4322e+02 - logprior: -2.4259e+01
Epoch 2/2
10/10 - 6s - loss: 438.9989 - loglik: -4.3601e+02 - logprior: -2.9859e+00
Fitted a model with MAP estimate = -435.6456
expansions: [(71, 2)]
discards: [  0   1 206]
Fitting a model of length 232 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 469.7098 - loglik: -4.3918e+02 - logprior: -3.0531e+01
Epoch 2/10
10/10 - 6s - loss: 443.9494 - loglik: -4.3500e+02 - logprior: -8.9530e+00
Epoch 3/10
10/10 - 6s - loss: 436.7360 - loglik: -4.3505e+02 - logprior: -1.6881e+00
Epoch 4/10
10/10 - 6s - loss: 429.2368 - loglik: -4.3228e+02 - logprior: 3.0423
Epoch 5/10
10/10 - 6s - loss: 428.1318 - loglik: -4.3261e+02 - logprior: 4.4764
Epoch 6/10
10/10 - 6s - loss: 427.8948 - loglik: -4.3298e+02 - logprior: 5.0807
Epoch 7/10
10/10 - 6s - loss: 425.8558 - loglik: -4.3146e+02 - logprior: 5.6047
Epoch 8/10
10/10 - 6s - loss: 426.1655 - loglik: -4.3216e+02 - logprior: 5.9929
Fitted a model with MAP estimate = -425.2013
Time for alignment: 134.5753
Fitting a model of length 184 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 645.2177 - loglik: -6.1677e+02 - logprior: -2.8444e+01
Epoch 2/10
10/10 - 4s - loss: 571.4156 - loglik: -5.6663e+02 - logprior: -4.7863e+00
Epoch 3/10
10/10 - 4s - loss: 514.9462 - loglik: -5.1391e+02 - logprior: -1.0381e+00
Epoch 4/10
10/10 - 4s - loss: 477.5668 - loglik: -4.7706e+02 - logprior: -5.0699e-01
Epoch 5/10
10/10 - 4s - loss: 464.2740 - loglik: -4.6413e+02 - logprior: -1.4437e-01
Epoch 6/10
10/10 - 4s - loss: 459.4354 - loglik: -4.5947e+02 - logprior: 0.0355
Epoch 7/10
10/10 - 4s - loss: 456.8312 - loglik: -4.5707e+02 - logprior: 0.2388
Epoch 8/10
10/10 - 4s - loss: 455.1788 - loglik: -4.5555e+02 - logprior: 0.3750
Epoch 9/10
10/10 - 4s - loss: 455.6099 - loglik: -4.5609e+02 - logprior: 0.4752
Fitted a model with MAP estimate = -454.3640
expansions: [(14, 1), (15, 1), (16, 2), (30, 1), (31, 2), (32, 1), (40, 2), (41, 2), (42, 1), (50, 1), (51, 1), (52, 1), (53, 3), (80, 1), (90, 1), (113, 1), (114, 2), (115, 1), (116, 4), (118, 2), (119, 2), (120, 3), (154, 1), (164, 1), (166, 3), (167, 7)]
discards: [0]
Fitting a model of length 231 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 492.1131 - loglik: -4.6026e+02 - logprior: -3.1851e+01
Epoch 2/2
10/10 - 6s - loss: 456.7943 - loglik: -4.4603e+02 - logprior: -1.0767e+01
Fitted a model with MAP estimate = -448.0744
expansions: [(0, 4), (38, 1), (140, 1)]
discards: [  0  17  89 136 150]
Fitting a model of length 232 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 467.6370 - loglik: -4.4327e+02 - logprior: -2.4364e+01
Epoch 2/2
10/10 - 6s - loss: 440.0556 - loglik: -4.3703e+02 - logprior: -3.0221e+00
Fitted a model with MAP estimate = -435.3063
expansions: [(72, 2)]
discards: [  1   2   3 153 206]
Fitting a model of length 229 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 462.1179 - loglik: -4.3834e+02 - logprior: -2.3777e+01
Epoch 2/10
10/10 - 6s - loss: 436.5350 - loglik: -4.3373e+02 - logprior: -2.8034e+00
Epoch 3/10
10/10 - 6s - loss: 432.0681 - loglik: -4.3371e+02 - logprior: 1.6384
Epoch 4/10
10/10 - 6s - loss: 428.9702 - loglik: -4.3232e+02 - logprior: 3.3489
Epoch 5/10
10/10 - 6s - loss: 428.2819 - loglik: -4.3247e+02 - logprior: 4.1858
Epoch 6/10
10/10 - 6s - loss: 427.4342 - loglik: -4.3217e+02 - logprior: 4.7343
Epoch 7/10
10/10 - 6s - loss: 425.7372 - loglik: -4.3101e+02 - logprior: 5.2741
Epoch 8/10
10/10 - 6s - loss: 425.5356 - loglik: -4.3124e+02 - logprior: 5.7032
Epoch 9/10
10/10 - 6s - loss: 425.6851 - loglik: -4.3166e+02 - logprior: 5.9762
Fitted a model with MAP estimate = -424.5488
Time for alignment: 138.3671
Computed alignments with likelihoods: ['-425.5517', '-425.2013', '-424.5488']
Best model has likelihood: -424.5488
SP score = 0.8972
Training of 3 independent models on file trfl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34a6568520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f36b2435040>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 739.2977 - loglik: -6.9873e+02 - logprior: -4.0570e+01
Epoch 2/10
11/11 - 8s - loss: 627.3220 - loglik: -6.2450e+02 - logprior: -2.8255e+00
Epoch 3/10
11/11 - 9s - loss: 541.2430 - loglik: -5.4264e+02 - logprior: 1.3985
Epoch 4/10
11/11 - 9s - loss: 493.0146 - loglik: -4.9517e+02 - logprior: 2.1583
Epoch 5/10
11/11 - 9s - loss: 478.6379 - loglik: -4.8098e+02 - logprior: 2.3447
Epoch 6/10
11/11 - 8s - loss: 469.2717 - loglik: -4.7178e+02 - logprior: 2.5074
Epoch 7/10
11/11 - 8s - loss: 461.5557 - loglik: -4.6428e+02 - logprior: 2.7199
Epoch 8/10
11/11 - 9s - loss: 463.9318 - loglik: -4.6682e+02 - logprior: 2.8890
Fitted a model with MAP estimate = -461.9115
expansions: [(19, 5), (21, 1), (22, 1), (29, 1), (36, 1), (47, 1), (49, 1), (51, 2), (63, 1), (64, 3), (65, 1), (78, 1), (79, 2), (81, 1), (91, 1), (93, 1), (103, 5), (104, 1), (106, 4), (107, 1), (150, 1), (160, 2), (162, 5), (163, 1), (180, 1), (181, 3), (197, 3), (198, 1), (199, 2), (201, 2), (202, 5), (221, 1), (223, 1), (224, 1), (225, 2)]
discards: [  0 211]
Fitting a model of length 308 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 511.1296 - loglik: -4.6433e+02 - logprior: -4.6798e+01
Epoch 2/2
11/11 - 12s - loss: 448.2104 - loglik: -4.3590e+02 - logprior: -1.2312e+01
Fitted a model with MAP estimate = -440.8121
expansions: [(0, 2), (225, 1)]
discards: [  0  22  62  79  80  81  97 128 129 135 136 137]
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 15s - loss: 471.9387 - loglik: -4.3733e+02 - logprior: -3.4613e+01
Epoch 2/2
11/11 - 12s - loss: 430.6010 - loglik: -4.3063e+02 - logprior: 0.0272
Fitted a model with MAP estimate = -425.1059
expansions: []
discards: [  0  22 296]
Fitting a model of length 296 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 480.9805 - loglik: -4.3630e+02 - logprior: -4.4683e+01
Epoch 2/10
11/11 - 11s - loss: 443.3948 - loglik: -4.3394e+02 - logprior: -9.4528e+00
Epoch 3/10
11/11 - 11s - loss: 427.2825 - loglik: -4.3017e+02 - logprior: 2.8885
Epoch 4/10
11/11 - 12s - loss: 419.9611 - loglik: -4.3018e+02 - logprior: 10.2178
Epoch 5/10
11/11 - 11s - loss: 414.8689 - loglik: -4.2723e+02 - logprior: 12.3583
Epoch 6/10
11/11 - 12s - loss: 413.7765 - loglik: -4.2715e+02 - logprior: 13.3716
Epoch 7/10
11/11 - 11s - loss: 412.6654 - loglik: -4.2687e+02 - logprior: 14.2067
Epoch 8/10
11/11 - 12s - loss: 411.4327 - loglik: -4.2640e+02 - logprior: 14.9699
Epoch 9/10
11/11 - 12s - loss: 410.8289 - loglik: -4.2646e+02 - logprior: 15.6285
Epoch 10/10
11/11 - 11s - loss: 410.1275 - loglik: -4.2632e+02 - logprior: 16.1938
Fitted a model with MAP estimate = -409.8930
Time for alignment: 262.7234
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 737.3653 - loglik: -6.9680e+02 - logprior: -4.0562e+01
Epoch 2/10
11/11 - 9s - loss: 628.0504 - loglik: -6.2522e+02 - logprior: -2.8340e+00
Epoch 3/10
11/11 - 8s - loss: 543.2814 - loglik: -5.4473e+02 - logprior: 1.4437
Epoch 4/10
11/11 - 9s - loss: 494.5230 - loglik: -4.9667e+02 - logprior: 2.1455
Epoch 5/10
11/11 - 8s - loss: 476.3621 - loglik: -4.7866e+02 - logprior: 2.2976
Epoch 6/10
11/11 - 9s - loss: 469.7941 - loglik: -4.7239e+02 - logprior: 2.5983
Epoch 7/10
11/11 - 8s - loss: 466.4056 - loglik: -4.6937e+02 - logprior: 2.9664
Epoch 8/10
11/11 - 8s - loss: 462.8927 - loglik: -4.6607e+02 - logprior: 3.1745
Epoch 9/10
11/11 - 7s - loss: 465.0462 - loglik: -4.6840e+02 - logprior: 3.3525
Fitted a model with MAP estimate = -463.0446
expansions: [(20, 1), (22, 2), (25, 1), (37, 1), (48, 1), (50, 1), (62, 1), (64, 1), (66, 1), (79, 1), (80, 1), (81, 2), (91, 1), (93, 1), (103, 3), (104, 1), (105, 1), (107, 1), (108, 4), (121, 2), (150, 1), (160, 1), (161, 1), (168, 2), (180, 1), (181, 5), (197, 4), (200, 2), (201, 5), (220, 1), (222, 2), (224, 1), (225, 1)]
discards: [  0   1 210]
Fitting a model of length 296 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 15s - loss: 513.0153 - loglik: -4.6610e+02 - logprior: -4.6916e+01
Epoch 2/2
11/11 - 11s - loss: 456.8350 - loglik: -4.4434e+02 - logprior: -1.2492e+01
Fitted a model with MAP estimate = -446.3338
expansions: [(0, 3), (20, 2), (215, 1), (239, 2), (241, 1)]
discards: [  0 118 119 128 129 130 145 273]
Fitting a model of length 297 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 475.7899 - loglik: -4.4129e+02 - logprior: -3.4500e+01
Epoch 2/2
11/11 - 11s - loss: 436.2561 - loglik: -4.3676e+02 - logprior: 0.5047
Fitted a model with MAP estimate = -428.4442
expansions: []
discards: [ 0 22]
Fitting a model of length 295 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 481.1289 - loglik: -4.3669e+02 - logprior: -4.4443e+01
Epoch 2/10
11/11 - 12s - loss: 443.9912 - loglik: -4.3516e+02 - logprior: -8.8298e+00
Epoch 3/10
11/11 - 12s - loss: 427.4302 - loglik: -4.3136e+02 - logprior: 3.9289
Epoch 4/10
11/11 - 12s - loss: 418.3934 - loglik: -4.2916e+02 - logprior: 10.7664
Epoch 5/10
11/11 - 11s - loss: 417.6496 - loglik: -4.3045e+02 - logprior: 12.8007
Epoch 6/10
11/11 - 12s - loss: 414.6176 - loglik: -4.2835e+02 - logprior: 13.7361
Epoch 7/10
11/11 - 11s - loss: 414.0653 - loglik: -4.2858e+02 - logprior: 14.5160
Epoch 8/10
11/11 - 12s - loss: 411.4511 - loglik: -4.2682e+02 - logprior: 15.3680
Epoch 9/10
11/11 - 11s - loss: 410.0288 - loglik: -4.2604e+02 - logprior: 16.0112
Epoch 10/10
11/11 - 12s - loss: 414.8289 - loglik: -4.3139e+02 - logprior: 16.5592
Fitted a model with MAP estimate = -410.4246
Time for alignment: 263.9772
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 11s - loss: 737.2222 - loglik: -6.9667e+02 - logprior: -4.0556e+01
Epoch 2/10
11/11 - 9s - loss: 624.3427 - loglik: -6.2148e+02 - logprior: -2.8621e+00
Epoch 3/10
11/11 - 9s - loss: 541.6741 - loglik: -5.4313e+02 - logprior: 1.4522
Epoch 4/10
11/11 - 8s - loss: 487.1425 - loglik: -4.8928e+02 - logprior: 2.1328
Epoch 5/10
11/11 - 8s - loss: 473.8748 - loglik: -4.7625e+02 - logprior: 2.3717
Epoch 6/10
11/11 - 9s - loss: 465.5468 - loglik: -4.6814e+02 - logprior: 2.5980
Epoch 7/10
11/11 - 8s - loss: 459.9410 - loglik: -4.6284e+02 - logprior: 2.8943
Epoch 8/10
11/11 - 9s - loss: 463.2869 - loglik: -4.6638e+02 - logprior: 3.0921
Fitted a model with MAP estimate = -459.5379
expansions: [(19, 4), (21, 1), (22, 1), (26, 1), (35, 1), (42, 1), (52, 2), (64, 4), (65, 1), (78, 1), (79, 2), (81, 1), (91, 1), (93, 1), (103, 4), (104, 1), (105, 2), (106, 1), (129, 1), (136, 1), (161, 1), (163, 5), (164, 1), (182, 2), (183, 2), (197, 1), (198, 2), (199, 4), (201, 2), (202, 5), (220, 1), (222, 2), (224, 1), (225, 1)]
discards: [0]
Fitting a model of length 305 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 16s - loss: 509.6174 - loglik: -4.6278e+02 - logprior: -4.6841e+01
Epoch 2/2
11/11 - 12s - loss: 449.4633 - loglik: -4.3734e+02 - logprior: -1.2123e+01
Fitted a model with MAP estimate = -441.4592
expansions: [(0, 2), (250, 1)]
discards: [  0  21  61  77  95 126 127 128 299 302]
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 15s - loss: 475.9956 - loglik: -4.4160e+02 - logprior: -3.4394e+01
Epoch 2/2
11/11 - 11s - loss: 433.4566 - loglik: -4.3370e+02 - logprior: 0.2443
Fitted a model with MAP estimate = -426.8572
expansions: [(294, 1)]
discards: [  0  76 277]
Fitting a model of length 296 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 15s - loss: 481.6523 - loglik: -4.3739e+02 - logprior: -4.4265e+01
Epoch 2/10
11/11 - 12s - loss: 442.0252 - loglik: -4.3380e+02 - logprior: -8.2297e+00
Epoch 3/10
11/11 - 11s - loss: 426.3451 - loglik: -4.3114e+02 - logprior: 4.7934
Epoch 4/10
11/11 - 11s - loss: 418.2236 - loglik: -4.2895e+02 - logprior: 10.7231
Epoch 5/10
11/11 - 11s - loss: 414.8345 - loglik: -4.2745e+02 - logprior: 12.6159
Epoch 6/10
11/11 - 11s - loss: 414.1389 - loglik: -4.2770e+02 - logprior: 13.5635
Epoch 7/10
11/11 - 12s - loss: 410.2056 - loglik: -4.2467e+02 - logprior: 14.4620
Epoch 8/10
11/11 - 12s - loss: 412.1976 - loglik: -4.2742e+02 - logprior: 15.2222
Fitted a model with MAP estimate = -410.5528
Time for alignment: 238.4016
Computed alignments with likelihoods: ['-409.8930', '-410.4246', '-410.5528']
Best model has likelihood: -409.8930
SP score = 0.9290
Training of 3 independent models on file serpin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f367f3e3f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3676a20040>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 18s - loss: 813.4583 - loglik: -8.0753e+02 - logprior: -5.9305e+00
Epoch 2/10
21/21 - 16s - loss: 690.2717 - loglik: -6.8969e+02 - logprior: -5.8038e-01
Epoch 3/10
21/21 - 15s - loss: 642.1661 - loglik: -6.3989e+02 - logprior: -2.2801e+00
Epoch 4/10
21/21 - 16s - loss: 630.0651 - loglik: -6.2788e+02 - logprior: -2.1854e+00
Epoch 5/10
21/21 - 16s - loss: 630.9069 - loglik: -6.2876e+02 - logprior: -2.1500e+00
Fitted a model with MAP estimate = -627.1658
expansions: [(13, 1), (14, 2), (15, 1), (52, 4), (54, 1), (55, 1), (60, 2), (61, 2), (62, 1), (64, 1), (65, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (79, 1), (80, 1), (81, 1), (85, 1), (90, 1), (93, 1), (94, 1), (95, 1), (101, 1), (103, 1), (110, 1), (112, 1), (114, 1), (116, 1), (134, 1), (135, 1), (138, 1), (141, 1), (145, 1), (153, 1), (154, 1), (155, 1), (157, 1), (158, 1), (160, 1), (161, 1), (170, 1), (180, 1), (183, 2), (189, 1), (190, 1), (191, 1), (193, 3), (194, 1), (208, 1), (209, 1), (211, 1), (212, 1), (214, 1), (223, 1), (229, 2), (230, 2), (232, 1), (234, 1), (256, 1), (257, 3), (258, 2), (260, 2), (261, 1), (272, 2), (273, 1)]
discards: [0 1]
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 633.6915 - loglik: -6.2544e+02 - logprior: -8.2471e+00
Epoch 2/2
21/21 - 23s - loss: 608.1143 - loglik: -6.0647e+02 - logprior: -1.6464e+00
Fitted a model with MAP estimate = -602.7457
expansions: [(0, 3), (18, 1)]
discards: [  0 293 332]
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 27s - loss: 611.6769 - loglik: -6.0718e+02 - logprior: -4.4925e+00
Epoch 2/2
21/21 - 24s - loss: 600.9809 - loglik: -6.0308e+02 - logprior: 2.0993
Fitted a model with MAP estimate = -597.8130
expansions: [(77, 1)]
discards: [1]
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 27s - loss: 611.4655 - loglik: -6.0749e+02 - logprior: -3.9744e+00
Epoch 2/10
21/21 - 24s - loss: 600.7128 - loglik: -6.0331e+02 - logprior: 2.6011
Epoch 3/10
21/21 - 24s - loss: 594.8763 - loglik: -5.9820e+02 - logprior: 3.3279
Epoch 4/10
21/21 - 23s - loss: 593.1298 - loglik: -5.9694e+02 - logprior: 3.8073
Epoch 5/10
21/21 - 23s - loss: 592.9448 - loglik: -5.9695e+02 - logprior: 4.0008
Epoch 6/10
21/21 - 23s - loss: 587.9102 - loglik: -5.9215e+02 - logprior: 4.2372
Epoch 7/10
21/21 - 24s - loss: 588.2784 - loglik: -5.9278e+02 - logprior: 4.5028
Fitted a model with MAP estimate = -586.7578
Time for alignment: 412.2800
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 18s - loss: 812.2681 - loglik: -8.0633e+02 - logprior: -5.9371e+00
Epoch 2/10
21/21 - 16s - loss: 693.8786 - loglik: -6.9328e+02 - logprior: -5.9915e-01
Epoch 3/10
21/21 - 16s - loss: 641.4329 - loglik: -6.3900e+02 - logprior: -2.4358e+00
Epoch 4/10
21/21 - 16s - loss: 633.1548 - loglik: -6.3077e+02 - logprior: -2.3896e+00
Epoch 5/10
21/21 - 17s - loss: 629.2244 - loglik: -6.2692e+02 - logprior: -2.3039e+00
Epoch 6/10
21/21 - 16s - loss: 627.6871 - loglik: -6.2538e+02 - logprior: -2.3054e+00
Epoch 7/10
21/21 - 17s - loss: 627.5011 - loglik: -6.2518e+02 - logprior: -2.3226e+00
Epoch 8/10
21/21 - 17s - loss: 625.9816 - loglik: -6.2369e+02 - logprior: -2.2927e+00
Epoch 9/10
21/21 - 18s - loss: 626.8900 - loglik: -6.2465e+02 - logprior: -2.2401e+00
Fitted a model with MAP estimate = -625.8723
expansions: [(13, 1), (14, 2), (15, 1), (50, 1), (52, 2), (54, 3), (55, 2), (57, 1), (60, 1), (62, 2), (65, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 2), (84, 1), (86, 1), (87, 1), (90, 1), (93, 1), (94, 1), (95, 1), (102, 1), (107, 1), (113, 1), (115, 1), (117, 1), (130, 1), (136, 1), (138, 1), (139, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 2), (159, 1), (160, 1), (161, 1), (170, 1), (180, 1), (183, 1), (190, 1), (191, 1), (192, 2), (193, 1), (194, 2), (195, 1), (196, 1), (210, 1), (213, 1), (215, 1), (219, 1), (224, 1), (229, 2), (230, 3), (232, 1), (234, 1), (256, 1), (257, 1), (258, 1), (259, 2), (261, 2), (262, 1), (271, 1), (272, 2), (273, 1)]
discards: [1]
Fitting a model of length 368 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 32s - loss: 638.7911 - loglik: -6.3321e+02 - logprior: -5.5807e+00
Epoch 2/2
21/21 - 30s - loss: 606.5710 - loglik: -6.0755e+02 - logprior: 0.9785
Fitted a model with MAP estimate = -600.5063
expansions: []
discards: [ 57 205 252 296 336]
Fitting a model of length 363 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 33s - loss: 613.4164 - loglik: -6.0905e+02 - logprior: -4.3672e+00
Epoch 2/2
21/21 - 33s - loss: 600.8405 - loglik: -6.0300e+02 - logprior: 2.1552
Fitted a model with MAP estimate = -598.6335
expansions: [(77, 1)]
discards: []
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 37s - loss: 613.0007 - loglik: -6.0902e+02 - logprior: -3.9827e+00
Epoch 2/10
21/21 - 34s - loss: 599.3249 - loglik: -6.0203e+02 - logprior: 2.7066
Epoch 3/10
21/21 - 37s - loss: 597.4355 - loglik: -6.0084e+02 - logprior: 3.3997
Epoch 4/10
21/21 - 40s - loss: 593.7845 - loglik: -5.9764e+02 - logprior: 3.8581
Epoch 5/10
21/21 - 43s - loss: 594.0110 - loglik: -5.9815e+02 - logprior: 4.1344
Fitted a model with MAP estimate = -590.5141
Time for alignment: 548.6869
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 29s - loss: 815.1905 - loglik: -8.0926e+02 - logprior: -5.9315e+00
Epoch 2/10
21/21 - 26s - loss: 692.9453 - loglik: -6.9226e+02 - logprior: -6.8339e-01
Epoch 3/10
21/21 - 28s - loss: 638.9860 - loglik: -6.3645e+02 - logprior: -2.5359e+00
Epoch 4/10
21/21 - 29s - loss: 632.2631 - loglik: -6.2976e+02 - logprior: -2.5017e+00
Epoch 5/10
21/21 - 29s - loss: 628.1147 - loglik: -6.2573e+02 - logprior: -2.3881e+00
Epoch 6/10
21/21 - 28s - loss: 626.5789 - loglik: -6.2411e+02 - logprior: -2.4650e+00
Epoch 7/10
21/21 - 26s - loss: 626.4426 - loglik: -6.2392e+02 - logprior: -2.5210e+00
Epoch 8/10
21/21 - 27s - loss: 625.1930 - loglik: -6.2269e+02 - logprior: -2.5042e+00
Epoch 9/10
21/21 - 27s - loss: 623.9520 - loglik: -6.2147e+02 - logprior: -2.4786e+00
Epoch 10/10
21/21 - 26s - loss: 625.3054 - loglik: -6.2286e+02 - logprior: -2.4448e+00
Fitted a model with MAP estimate = -623.8450
expansions: [(13, 1), (14, 2), (15, 1), (53, 3), (56, 1), (57, 1), (62, 2), (63, 2), (65, 3), (76, 1), (77, 1), (78, 1), (79, 1), (81, 1), (82, 1), (83, 1), (88, 1), (89, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (102, 1), (104, 1), (111, 1), (130, 1), (132, 1), (137, 1), (138, 1), (139, 1), (147, 1), (155, 1), (156, 1), (157, 1), (159, 1), (160, 1), (162, 1), (163, 1), (169, 1), (181, 2), (184, 1), (187, 1), (190, 1), (191, 1), (192, 2), (193, 1), (194, 2), (195, 1), (196, 1), (208, 1), (209, 1), (212, 1), (215, 1), (219, 1), (223, 1), (229, 2), (230, 2), (232, 1), (234, 1), (256, 1), (257, 1), (258, 1), (259, 2), (261, 2), (262, 1), (271, 1), (272, 2), (273, 1)]
discards: [1]
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 43s - loss: 641.7275 - loglik: -6.3616e+02 - logprior: -5.5686e+00
Epoch 2/2
21/21 - 39s - loss: 603.2747 - loglik: -6.0427e+02 - logprior: 0.9996
Fitted a model with MAP estimate = -600.3546
expansions: [(145, 1)]
discards: [250 295 334]
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 45s - loss: 611.3403 - loglik: -6.0692e+02 - logprior: -4.4185e+00
Epoch 2/2
21/21 - 48s - loss: 601.6694 - loglik: -6.0386e+02 - logprior: 2.1893
Fitted a model with MAP estimate = -597.9076
expansions: []
discards: []
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 51s - loss: 610.3323 - loglik: -6.0649e+02 - logprior: -3.8455e+00
Epoch 2/10
21/21 - 45s - loss: 600.3017 - loglik: -6.0299e+02 - logprior: 2.6875
Epoch 3/10
21/21 - 47s - loss: 596.9984 - loglik: -6.0050e+02 - logprior: 3.5026
Epoch 4/10
21/21 - 50s - loss: 594.1444 - loglik: -5.9802e+02 - logprior: 3.8741
Epoch 5/10
21/21 - 47s - loss: 589.7001 - loglik: -5.9385e+02 - logprior: 4.1523
Epoch 6/10
21/21 - 44s - loss: 591.5436 - loglik: -5.9593e+02 - logprior: 4.3895
Fitted a model with MAP estimate = -588.2689
Time for alignment: 835.1238
Computed alignments with likelihoods: ['-586.7578', '-590.5141', '-588.2689']
Best model has likelihood: -586.7578
SP score = 0.9655
Training of 3 independent models on file ghf13.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34f40b9cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34f40b9be0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 836.2729 - loglik: -8.3457e+02 - logprior: -1.6982e+00
Epoch 2/10
39/39 - 34s - loss: 735.2247 - loglik: -7.3387e+02 - logprior: -1.3525e+00
Epoch 3/10
39/39 - 35s - loss: 719.9153 - loglik: -7.1850e+02 - logprior: -1.4187e+00
Epoch 4/10
39/39 - 35s - loss: 711.7510 - loglik: -7.1033e+02 - logprior: -1.4201e+00
Epoch 5/10
39/39 - 34s - loss: 707.1838 - loglik: -7.0578e+02 - logprior: -1.3992e+00
Epoch 6/10
39/39 - 36s - loss: 704.8619 - loglik: -7.0342e+02 - logprior: -1.4433e+00
Epoch 7/10
39/39 - 38s - loss: 704.2183 - loglik: -7.0275e+02 - logprior: -1.4646e+00
Epoch 8/10
39/39 - 39s - loss: 702.8317 - loglik: -7.0136e+02 - logprior: -1.4721e+00
Epoch 9/10
39/39 - 42s - loss: 702.8408 - loglik: -7.0136e+02 - logprior: -1.4791e+00
Fitted a model with MAP estimate = -606.5783
expansions: [(14, 1), (31, 1), (41, 1), (56, 1), (81, 4), (82, 1), (83, 1), (85, 1), (95, 4), (100, 2), (104, 1), (106, 3), (110, 1), (119, 1), (120, 4), (121, 3), (122, 2), (140, 1), (144, 1), (157, 1), (160, 5), (168, 1), (169, 2), (170, 6), (172, 3), (173, 2), (175, 1), (176, 1), (179, 1), (181, 2), (186, 1), (188, 3), (189, 1), (190, 1), (192, 2), (193, 1), (208, 1), (209, 1), (212, 2), (213, 4), (215, 1), (224, 1), (244, 2)]
discards: [0]
Fitting a model of length 323 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 66s - loss: 747.2875 - loglik: -7.4436e+02 - logprior: -2.9268e+00
Epoch 2/2
39/39 - 64s - loss: 697.4576 - loglik: -6.9550e+02 - logprior: -1.9539e+00
Fitted a model with MAP estimate = -597.4125
expansions: [(0, 2), (106, 3), (218, 1), (241, 3), (323, 2)]
discards: [  0  95  96 109 110 111 112 113 114 115 116 117 118 121 122 123 124 125
 126 127 128 129 130 146 153 178 196 207 210 237 254 321 322]
Fitting a model of length 301 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 61s - loss: 711.5838 - loglik: -7.0977e+02 - logprior: -1.8097e+00
Epoch 2/2
39/39 - 59s - loss: 700.2482 - loglik: -6.9966e+02 - logprior: -5.9071e-01
Fitted a model with MAP estimate = -600.1071
expansions: [(96, 1), (105, 2), (110, 4), (113, 8), (157, 1), (214, 1), (217, 1), (218, 1)]
discards: [  0 158 159 202 264 265 299 300]
Fitting a model of length 312 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 60s - loss: 607.3041 - loglik: -6.0531e+02 - logprior: -1.9981e+00
Epoch 2/10
43/43 - 59s - loss: 594.6947 - loglik: -5.9416e+02 - logprior: -5.2984e-01
Epoch 3/10
43/43 - 49s - loss: 589.8740 - loglik: -5.8959e+02 - logprior: -2.8453e-01
Epoch 4/10
43/43 - 52s - loss: 585.4097 - loglik: -5.8520e+02 - logprior: -2.0541e-01
Epoch 5/10
43/43 - 59s - loss: 581.2700 - loglik: -5.8117e+02 - logprior: -9.9826e-02
Epoch 6/10
43/43 - 52s - loss: 578.3788 - loglik: -5.7816e+02 - logprior: -2.1559e-01
Epoch 7/10
43/43 - 51s - loss: 575.8305 - loglik: -5.7574e+02 - logprior: -9.0451e-02
Epoch 8/10
43/43 - 58s - loss: 576.0670 - loglik: -5.7620e+02 - logprior: 0.1287
Fitted a model with MAP estimate = -574.1547
Time for alignment: 1280.5956
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 835.5622 - loglik: -8.3388e+02 - logprior: -1.6818e+00
Epoch 2/10
39/39 - 41s - loss: 734.7960 - loglik: -7.3353e+02 - logprior: -1.2633e+00
Epoch 3/10
39/39 - 42s - loss: 718.6368 - loglik: -7.1726e+02 - logprior: -1.3780e+00
Epoch 4/10
39/39 - 43s - loss: 711.7920 - loglik: -7.1037e+02 - logprior: -1.4201e+00
Epoch 5/10
39/39 - 42s - loss: 707.4843 - loglik: -7.0606e+02 - logprior: -1.4262e+00
Epoch 6/10
39/39 - 41s - loss: 705.0955 - loglik: -7.0364e+02 - logprior: -1.4522e+00
Epoch 7/10
39/39 - 39s - loss: 704.1371 - loglik: -7.0268e+02 - logprior: -1.4531e+00
Epoch 8/10
39/39 - 40s - loss: 703.3723 - loglik: -7.0192e+02 - logprior: -1.4527e+00
Epoch 9/10
39/39 - 40s - loss: 702.1760 - loglik: -7.0068e+02 - logprior: -1.4997e+00
Epoch 10/10
39/39 - 40s - loss: 701.8575 - loglik: -7.0037e+02 - logprior: -1.4849e+00
Fitted a model with MAP estimate = -606.5085
expansions: [(14, 1), (30, 1), (75, 1), (82, 6), (84, 1), (94, 2), (96, 3), (104, 1), (106, 4), (110, 1), (116, 1), (118, 1), (119, 1), (120, 1), (121, 2), (122, 1), (123, 1), (147, 1), (149, 5), (151, 1), (160, 1), (161, 2), (162, 2), (167, 2), (168, 4), (170, 1), (171, 2), (172, 2), (173, 1), (174, 1), (175, 1), (179, 3), (180, 2), (181, 1), (186, 1), (187, 1), (188, 2), (190, 3), (191, 1), (192, 1), (206, 1), (207, 1), (212, 4), (244, 2)]
discards: [0]
Fitting a model of length 321 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 63s - loss: 756.4397 - loglik: -7.5349e+02 - logprior: -2.9538e+00
Epoch 2/2
39/39 - 53s - loss: 701.0440 - loglik: -6.9920e+02 - logprior: -1.8455e+00
Fitted a model with MAP estimate = -599.4672
expansions: [(0, 3), (122, 1), (188, 1), (195, 1), (197, 2), (209, 2), (238, 1), (250, 1), (255, 1), (321, 3)]
discards: [  0 110 118 123 124 125 126 127 128 129 145 176 177 178 179 202 222 235
 236 252 253 257 286 287 319 320]
Fitting a model of length 311 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 47s - loss: 707.3442 - loglik: -7.0550e+02 - logprior: -1.8452e+00
Epoch 2/2
39/39 - 47s - loss: 695.1180 - loglik: -6.9443e+02 - logprior: -6.9298e-01
Fitted a model with MAP estimate = -596.6381
expansions: [(122, 4), (124, 1), (125, 3), (167, 1), (204, 1), (237, 3)]
discards: [  0   2 106 116 169 170 171 172 173 174 175 308 309 310]
Fitting a model of length 310 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 50s - loss: 602.9879 - loglik: -6.0107e+02 - logprior: -1.9174e+00
Epoch 2/10
43/43 - 52s - loss: 598.1693 - loglik: -5.9770e+02 - logprior: -4.7051e-01
Epoch 3/10
43/43 - 61s - loss: 585.9312 - loglik: -5.8566e+02 - logprior: -2.7180e-01
Epoch 4/10
43/43 - 67s - loss: 586.3029 - loglik: -5.8615e+02 - logprior: -1.5159e-01
Fitted a model with MAP estimate = -581.2394
Time for alignment: 1111.7215
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 841.0529 - loglik: -8.3936e+02 - logprior: -1.6973e+00
Epoch 2/10
39/39 - 41s - loss: 739.1281 - loglik: -7.3773e+02 - logprior: -1.3943e+00
Epoch 3/10
39/39 - 40s - loss: 720.8830 - loglik: -7.1935e+02 - logprior: -1.5322e+00
Epoch 4/10
39/39 - 39s - loss: 714.3696 - loglik: -7.1285e+02 - logprior: -1.5204e+00
Epoch 5/10
39/39 - 40s - loss: 710.1508 - loglik: -7.0863e+02 - logprior: -1.5242e+00
Epoch 6/10
39/39 - 40s - loss: 707.4523 - loglik: -7.0593e+02 - logprior: -1.5201e+00
Epoch 7/10
39/39 - 41s - loss: 706.4525 - loglik: -7.0493e+02 - logprior: -1.5259e+00
Epoch 8/10
39/39 - 40s - loss: 705.8723 - loglik: -7.0435e+02 - logprior: -1.5238e+00
Epoch 9/10
39/39 - 40s - loss: 705.1106 - loglik: -7.0359e+02 - logprior: -1.5190e+00
Epoch 10/10
39/39 - 41s - loss: 704.1601 - loglik: -7.0263e+02 - logprior: -1.5294e+00
Fitted a model with MAP estimate = -608.9028
expansions: [(14, 1), (27, 1), (40, 1), (44, 1), (55, 1), (56, 1), (79, 5), (80, 2), (81, 2), (91, 2), (93, 3), (97, 1), (98, 1), (101, 1), (104, 3), (108, 1), (117, 1), (118, 1), (119, 1), (120, 3), (121, 2), (149, 1), (150, 5), (152, 1), (167, 6), (169, 3), (170, 2), (171, 1), (172, 1), (177, 1), (178, 1), (179, 2), (184, 2), (185, 1), (186, 1), (187, 3), (190, 1), (206, 1), (207, 1), (213, 1), (239, 1), (244, 3)]
discards: [  0 160 161]
Fitting a model of length 315 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 53s - loss: 753.8975 - loglik: -7.5096e+02 - logprior: -2.9368e+00
Epoch 2/2
39/39 - 56s - loss: 702.9743 - loglik: -7.0110e+02 - logprior: -1.8779e+00
Fitted a model with MAP estimate = -601.3303
expansions: [(0, 2), (208, 1), (215, 1)]
discards: [  0  87  93 106 112 113 120 124 125 126 127 128 129 130 131 132 133 155
 187 200 201 202 203 204 205 245 246 250 251 277 312 313 314]
Fitting a model of length 286 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 713.9937 - loglik: -7.1222e+02 - logprior: -1.7750e+00
Epoch 2/2
39/39 - 45s - loss: 702.6862 - loglik: -7.0220e+02 - logprior: -4.8978e-01
Fitted a model with MAP estimate = -602.2727
expansions: [(110, 1), (119, 10), (217, 3), (226, 2), (286, 3)]
discards: [0]
Fitting a model of length 304 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 49s - loss: 605.6173 - loglik: -6.0361e+02 - logprior: -2.0045e+00
Epoch 2/10
43/43 - 52s - loss: 597.4734 - loglik: -5.9686e+02 - logprior: -6.1258e-01
Epoch 3/10
43/43 - 60s - loss: 591.4003 - loglik: -5.9089e+02 - logprior: -5.1149e-01
Epoch 4/10
43/43 - 64s - loss: 587.2554 - loglik: -5.8697e+02 - logprior: -2.8518e-01
Epoch 5/10
43/43 - 63s - loss: 585.4134 - loglik: -5.8528e+02 - logprior: -1.3389e-01
Epoch 6/10
43/43 - 46s - loss: 578.4314 - loglik: -5.7804e+02 - logprior: -3.9482e-01
Epoch 7/10
43/43 - 40s - loss: 579.2695 - loglik: -5.7913e+02 - logprior: -1.4416e-01
Fitted a model with MAP estimate = -577.8895
Time for alignment: 1201.2160
Computed alignments with likelihoods: ['-574.1547', '-581.2394', '-577.8895']
Best model has likelihood: -574.1547
SP score = 0.5310
Training of 3 independent models on file cryst.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34a6d54100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34f44b4ee0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 274.5876 - loglik: -2.3645e+02 - logprior: -3.8135e+01
Epoch 2/10
10/10 - 1s - loss: 222.6291 - loglik: -2.1257e+02 - logprior: -1.0056e+01
Epoch 3/10
10/10 - 1s - loss: 196.9362 - loglik: -1.9186e+02 - logprior: -5.0756e+00
Epoch 4/10
10/10 - 1s - loss: 179.7681 - loglik: -1.7633e+02 - logprior: -3.4392e+00
Epoch 5/10
10/10 - 1s - loss: 172.4620 - loglik: -1.6984e+02 - logprior: -2.6188e+00
Epoch 6/10
10/10 - 1s - loss: 168.8837 - loglik: -1.6665e+02 - logprior: -2.2360e+00
Epoch 7/10
10/10 - 1s - loss: 167.7111 - loglik: -1.6569e+02 - logprior: -2.0233e+00
Epoch 8/10
10/10 - 1s - loss: 166.7378 - loglik: -1.6485e+02 - logprior: -1.8896e+00
Epoch 9/10
10/10 - 1s - loss: 166.2867 - loglik: -1.6452e+02 - logprior: -1.7628e+00
Epoch 10/10
10/10 - 1s - loss: 165.5934 - loglik: -1.6392e+02 - logprior: -1.6727e+00
Fitted a model with MAP estimate = -165.4425
expansions: [(7, 2), (13, 1), (15, 2), (18, 1), (20, 1), (23, 2), (30, 1), (38, 1), (46, 1), (55, 2), (56, 1), (57, 4)]
discards: [0]
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 209.3054 - loglik: -1.6653e+02 - logprior: -4.2776e+01
Epoch 2/2
10/10 - 1s - loss: 174.4897 - loglik: -1.5714e+02 - logprior: -1.7350e+01
Fitted a model with MAP estimate = -168.1311
expansions: [(0, 2)]
discards: [ 0 18]
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 187.6235 - loglik: -1.5380e+02 - logprior: -3.3820e+01
Epoch 2/2
10/10 - 1s - loss: 159.7181 - loglik: -1.5116e+02 - logprior: -8.5611e+00
Fitted a model with MAP estimate = -155.8651
expansions: []
discards: [ 0 72]
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 193.5032 - loglik: -1.5422e+02 - logprior: -3.9281e+01
Epoch 2/10
10/10 - 1s - loss: 163.4676 - loglik: -1.5290e+02 - logprior: -1.0570e+01
Epoch 3/10
10/10 - 1s - loss: 156.1477 - loglik: -1.5224e+02 - logprior: -3.9115e+00
Epoch 4/10
10/10 - 1s - loss: 152.6028 - loglik: -1.5090e+02 - logprior: -1.7058e+00
Epoch 5/10
10/10 - 1s - loss: 151.2421 - loglik: -1.5047e+02 - logprior: -7.7336e-01
Epoch 6/10
10/10 - 1s - loss: 150.1893 - loglik: -1.4999e+02 - logprior: -1.9623e-01
Epoch 7/10
10/10 - 1s - loss: 149.9415 - loglik: -1.5025e+02 - logprior: 0.3131
Epoch 8/10
10/10 - 1s - loss: 149.2893 - loglik: -1.4996e+02 - logprior: 0.6657
Epoch 9/10
10/10 - 1s - loss: 149.1543 - loglik: -1.5002e+02 - logprior: 0.8666
Epoch 10/10
10/10 - 1s - loss: 149.1372 - loglik: -1.5014e+02 - logprior: 1.0000
Fitted a model with MAP estimate = -148.8906
Time for alignment: 46.3138
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 274.7242 - loglik: -2.3659e+02 - logprior: -3.8134e+01
Epoch 2/10
10/10 - 1s - loss: 222.9409 - loglik: -2.1289e+02 - logprior: -1.0050e+01
Epoch 3/10
10/10 - 1s - loss: 195.8809 - loglik: -1.9081e+02 - logprior: -5.0728e+00
Epoch 4/10
10/10 - 1s - loss: 179.4463 - loglik: -1.7608e+02 - logprior: -3.3667e+00
Epoch 5/10
10/10 - 1s - loss: 171.3330 - loglik: -1.6884e+02 - logprior: -2.4941e+00
Epoch 6/10
10/10 - 1s - loss: 168.1653 - loglik: -1.6595e+02 - logprior: -2.2154e+00
Epoch 7/10
10/10 - 1s - loss: 166.5119 - loglik: -1.6438e+02 - logprior: -2.1325e+00
Epoch 8/10
10/10 - 1s - loss: 166.0153 - loglik: -1.6417e+02 - logprior: -1.8408e+00
Epoch 9/10
10/10 - 1s - loss: 165.0916 - loglik: -1.6350e+02 - logprior: -1.5866e+00
Epoch 10/10
10/10 - 1s - loss: 164.2308 - loglik: -1.6267e+02 - logprior: -1.5622e+00
Fitted a model with MAP estimate = -163.9832
expansions: [(0, 2), (12, 1), (15, 2), (18, 1), (20, 1), (23, 2), (30, 1), (32, 1), (43, 1), (55, 3), (57, 4)]
discards: []
Fitting a model of length 84 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 214.0848 - loglik: -1.6398e+02 - logprior: -5.0105e+01
Epoch 2/2
10/10 - 1s - loss: 170.0026 - loglik: -1.5493e+02 - logprior: -1.5070e+01
Fitted a model with MAP estimate = -161.6816
expansions: []
discards: [ 0 18 68]
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 199.6824 - loglik: -1.5631e+02 - logprior: -4.3375e+01
Epoch 2/2
10/10 - 1s - loss: 171.1248 - loglik: -1.5428e+02 - logprior: -1.6841e+01
Fitted a model with MAP estimate = -166.3212
expansions: []
discards: []
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 192.9308 - loglik: -1.5334e+02 - logprior: -3.9589e+01
Epoch 2/10
10/10 - 1s - loss: 163.6222 - loglik: -1.5286e+02 - logprior: -1.0766e+01
Epoch 3/10
10/10 - 1s - loss: 156.0203 - loglik: -1.5209e+02 - logprior: -3.9336e+00
Epoch 4/10
10/10 - 1s - loss: 152.5081 - loglik: -1.5082e+02 - logprior: -1.6910e+00
Epoch 5/10
10/10 - 1s - loss: 150.8962 - loglik: -1.5013e+02 - logprior: -7.6439e-01
Epoch 6/10
10/10 - 1s - loss: 150.1921 - loglik: -1.5002e+02 - logprior: -1.7562e-01
Epoch 7/10
10/10 - 1s - loss: 149.6669 - loglik: -1.5000e+02 - logprior: 0.3361
Epoch 8/10
10/10 - 1s - loss: 149.5237 - loglik: -1.5021e+02 - logprior: 0.6869
Epoch 9/10
10/10 - 1s - loss: 148.9133 - loglik: -1.4979e+02 - logprior: 0.8737
Epoch 10/10
10/10 - 1s - loss: 149.2384 - loglik: -1.5025e+02 - logprior: 1.0131
Fitted a model with MAP estimate = -148.8351
Time for alignment: 45.5894
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 274.5072 - loglik: -2.3637e+02 - logprior: -3.8134e+01
Epoch 2/10
10/10 - 1s - loss: 222.6989 - loglik: -2.1265e+02 - logprior: -1.0053e+01
Epoch 3/10
10/10 - 1s - loss: 195.4479 - loglik: -1.9041e+02 - logprior: -5.0374e+00
Epoch 4/10
10/10 - 1s - loss: 179.8036 - loglik: -1.7651e+02 - logprior: -3.2904e+00
Epoch 5/10
10/10 - 1s - loss: 172.3880 - loglik: -1.6998e+02 - logprior: -2.4092e+00
Epoch 6/10
10/10 - 1s - loss: 169.1807 - loglik: -1.6705e+02 - logprior: -2.1345e+00
Epoch 7/10
10/10 - 1s - loss: 167.6554 - loglik: -1.6560e+02 - logprior: -2.0567e+00
Epoch 8/10
10/10 - 1s - loss: 166.7728 - loglik: -1.6500e+02 - logprior: -1.7722e+00
Epoch 9/10
10/10 - 1s - loss: 166.2806 - loglik: -1.6482e+02 - logprior: -1.4561e+00
Epoch 10/10
10/10 - 1s - loss: 165.6934 - loglik: -1.6435e+02 - logprior: -1.3420e+00
Fitted a model with MAP estimate = -165.3926
expansions: [(0, 2), (12, 1), (14, 2), (15, 2), (20, 2), (23, 3), (30, 1), (32, 1), (43, 1), (55, 3), (58, 3)]
discards: []
Fitting a model of length 86 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 216.7450 - loglik: -1.6674e+02 - logprior: -5.0003e+01
Epoch 2/2
10/10 - 1s - loss: 171.2458 - loglik: -1.5600e+02 - logprior: -1.5251e+01
Fitted a model with MAP estimate = -162.2793
expansions: [(76, 1)]
discards: [ 0 19 20 27 71]
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 199.3649 - loglik: -1.5610e+02 - logprior: -4.3268e+01
Epoch 2/2
10/10 - 1s - loss: 170.9467 - loglik: -1.5416e+02 - logprior: -1.6783e+01
Fitted a model with MAP estimate = -165.6912
expansions: []
discards: []
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 192.5063 - loglik: -1.5297e+02 - logprior: -3.9539e+01
Epoch 2/10
10/10 - 1s - loss: 162.6868 - loglik: -1.5197e+02 - logprior: -1.0720e+01
Epoch 3/10
10/10 - 1s - loss: 155.1449 - loglik: -1.5129e+02 - logprior: -3.8565e+00
Epoch 4/10
10/10 - 1s - loss: 151.9623 - loglik: -1.5035e+02 - logprior: -1.6173e+00
Epoch 5/10
10/10 - 1s - loss: 150.3694 - loglik: -1.4968e+02 - logprior: -6.9152e-01
Epoch 6/10
10/10 - 1s - loss: 149.3656 - loglik: -1.4925e+02 - logprior: -1.1096e-01
Epoch 7/10
10/10 - 1s - loss: 148.9456 - loglik: -1.4936e+02 - logprior: 0.4129
Epoch 8/10
10/10 - 1s - loss: 148.4581 - loglik: -1.4922e+02 - logprior: 0.7626
Epoch 9/10
10/10 - 1s - loss: 148.7250 - loglik: -1.4968e+02 - logprior: 0.9565
Fitted a model with MAP estimate = -148.2546
Time for alignment: 44.7139
Computed alignments with likelihoods: ['-148.8906', '-148.8351', '-148.2546']
Best model has likelihood: -148.2546
SP score = 0.9243
Training of 3 independent models on file Ald_Xan_dh_2.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34eae4f670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f35ed548f40>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 87 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 322.9113 - loglik: -3.0608e+02 - logprior: -1.6828e+01
Epoch 2/10
10/10 - 5s - loss: 277.9818 - loglik: -2.7370e+02 - logprior: -4.2821e+00
Epoch 3/10
10/10 - 5s - loss: 245.5344 - loglik: -2.4321e+02 - logprior: -2.3252e+00
Epoch 4/10
10/10 - 4s - loss: 226.1697 - loglik: -2.2408e+02 - logprior: -2.0861e+00
Epoch 5/10
10/10 - 5s - loss: 219.3972 - loglik: -2.1737e+02 - logprior: -2.0298e+00
Epoch 6/10
10/10 - 5s - loss: 216.5527 - loglik: -2.1465e+02 - logprior: -1.8978e+00
Epoch 7/10
10/10 - 4s - loss: 214.4364 - loglik: -2.1272e+02 - logprior: -1.7213e+00
Epoch 8/10
10/10 - 5s - loss: 215.4666 - loglik: -2.1394e+02 - logprior: -1.5302e+00
Fitted a model with MAP estimate = -214.6231
expansions: [(7, 1), (9, 3), (10, 2), (11, 2), (27, 1), (29, 1), (33, 1), (47, 3), (48, 3), (49, 1), (50, 1), (57, 1), (73, 4), (74, 2)]
discards: [0]
Fitting a model of length 112 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 234.1675 - loglik: -2.1542e+02 - logprior: -1.8743e+01
Epoch 2/2
10/10 - 5s - loss: 210.4825 - loglik: -2.0289e+02 - logprior: -7.5972e+00
Fitted a model with MAP estimate = -207.3984
expansions: [(0, 2)]
discards: [ 0 14 16]
Fitting a model of length 111 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 215.6717 - loglik: -2.0091e+02 - logprior: -1.4761e+01
Epoch 2/2
10/10 - 5s - loss: 201.7474 - loglik: -1.9820e+02 - logprior: -3.5429e+00
Fitted a model with MAP estimate = -199.4991
expansions: []
discards: [ 0 61]
Fitting a model of length 109 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 218.7324 - loglik: -2.0127e+02 - logprior: -1.7466e+01
Epoch 2/10
10/10 - 6s - loss: 203.8105 - loglik: -1.9904e+02 - logprior: -4.7678e+00
Epoch 3/10
10/10 - 5s - loss: 200.3572 - loglik: -1.9837e+02 - logprior: -1.9873e+00
Epoch 4/10
10/10 - 7s - loss: 197.9813 - loglik: -1.9679e+02 - logprior: -1.1878e+00
Epoch 5/10
10/10 - 7s - loss: 197.0353 - loglik: -1.9652e+02 - logprior: -5.1965e-01
Epoch 6/10
10/10 - 7s - loss: 196.8576 - loglik: -1.9664e+02 - logprior: -2.2050e-01
Epoch 7/10
10/10 - 7s - loss: 196.8667 - loglik: -1.9676e+02 - logprior: -1.1042e-01
Fitted a model with MAP estimate = -196.2646
Time for alignment: 131.2512
Fitting a model of length 87 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 323.0497 - loglik: -3.0622e+02 - logprior: -1.6831e+01
Epoch 2/10
10/10 - 5s - loss: 278.0789 - loglik: -2.7380e+02 - logprior: -4.2790e+00
Epoch 3/10
10/10 - 4s - loss: 246.3005 - loglik: -2.4404e+02 - logprior: -2.2600e+00
Epoch 4/10
10/10 - 5s - loss: 227.4743 - loglik: -2.2557e+02 - logprior: -1.9079e+00
Epoch 5/10
10/10 - 4s - loss: 220.9017 - loglik: -2.1908e+02 - logprior: -1.8214e+00
Epoch 6/10
10/10 - 5s - loss: 217.4093 - loglik: -2.1568e+02 - logprior: -1.7289e+00
Epoch 7/10
10/10 - 5s - loss: 215.6772 - loglik: -2.1408e+02 - logprior: -1.6013e+00
Epoch 8/10
10/10 - 5s - loss: 215.5428 - loglik: -2.1405e+02 - logprior: -1.4915e+00
Epoch 9/10
10/10 - 5s - loss: 215.1269 - loglik: -2.1368e+02 - logprior: -1.4477e+00
Epoch 10/10
10/10 - 5s - loss: 214.2839 - loglik: -2.1283e+02 - logprior: -1.4534e+00
Fitted a model with MAP estimate = -214.4153
expansions: [(10, 3), (11, 2), (12, 2), (15, 1), (26, 1), (27, 1), (43, 1), (48, 5), (49, 1), (57, 1), (72, 1), (73, 3), (74, 2)]
discards: [0]
Fitting a model of length 110 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 233.8680 - loglik: -2.1513e+02 - logprior: -1.8739e+01
Epoch 2/2
10/10 - 7s - loss: 211.6471 - loglik: -2.0400e+02 - logprior: -7.6472e+00
Fitted a model with MAP estimate = -207.5385
expansions: [(0, 2)]
discards: [ 0 14 16]
Fitting a model of length 109 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 215.8402 - loglik: -2.0106e+02 - logprior: -1.4776e+01
Epoch 2/2
10/10 - 5s - loss: 202.0780 - loglik: -1.9853e+02 - logprior: -3.5449e+00
Fitted a model with MAP estimate = -199.7603
expansions: []
discards: [0]
Fitting a model of length 108 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 218.7734 - loglik: -2.0138e+02 - logprior: -1.7394e+01
Epoch 2/10
10/10 - 6s - loss: 203.9675 - loglik: -1.9934e+02 - logprior: -4.6259e+00
Epoch 3/10
10/10 - 6s - loss: 199.9523 - loglik: -1.9813e+02 - logprior: -1.8195e+00
Epoch 4/10
10/10 - 6s - loss: 198.3928 - loglik: -1.9739e+02 - logprior: -1.0043e+00
Epoch 5/10
10/10 - 6s - loss: 197.1552 - loglik: -1.9677e+02 - logprior: -3.8145e-01
Epoch 6/10
10/10 - 6s - loss: 197.4209 - loglik: -1.9737e+02 - logprior: -5.4870e-02
Fitted a model with MAP estimate = -196.5253
Time for alignment: 136.8971
Fitting a model of length 87 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 323.2182 - loglik: -3.0639e+02 - logprior: -1.6828e+01
Epoch 2/10
10/10 - 5s - loss: 277.9826 - loglik: -2.7371e+02 - logprior: -4.2725e+00
Epoch 3/10
10/10 - 5s - loss: 247.1319 - loglik: -2.4485e+02 - logprior: -2.2803e+00
Epoch 4/10
10/10 - 5s - loss: 229.5309 - loglik: -2.2752e+02 - logprior: -2.0092e+00
Epoch 5/10
10/10 - 4s - loss: 221.1138 - loglik: -2.1912e+02 - logprior: -1.9893e+00
Epoch 6/10
10/10 - 5s - loss: 217.8981 - loglik: -2.1603e+02 - logprior: -1.8694e+00
Epoch 7/10
10/10 - 5s - loss: 215.7029 - loglik: -2.1403e+02 - logprior: -1.6770e+00
Epoch 8/10
10/10 - 5s - loss: 215.0383 - loglik: -2.1350e+02 - logprior: -1.5433e+00
Epoch 9/10
10/10 - 5s - loss: 214.8344 - loglik: -2.1335e+02 - logprior: -1.4879e+00
Epoch 10/10
10/10 - 5s - loss: 215.2732 - loglik: -2.1379e+02 - logprior: -1.4783e+00
Fitted a model with MAP estimate = -214.6426
expansions: [(10, 3), (11, 2), (12, 2), (21, 1), (27, 1), (29, 1), (43, 1), (45, 1), (47, 1), (48, 5), (49, 2), (50, 2), (72, 1), (73, 4), (74, 2)]
discards: [0]
Fitting a model of length 115 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 235.1534 - loglik: -2.1640e+02 - logprior: -1.8755e+01
Epoch 2/2
10/10 - 8s - loss: 212.6351 - loglik: -2.0487e+02 - logprior: -7.7669e+00
Fitted a model with MAP estimate = -207.9896
expansions: [(0, 2)]
discards: [ 0 14 16 62 63 64 69 94]
Fitting a model of length 109 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 216.7515 - loglik: -2.0193e+02 - logprior: -1.4823e+01
Epoch 2/2
10/10 - 7s - loss: 202.2449 - loglik: -1.9869e+02 - logprior: -3.5566e+00
Fitted a model with MAP estimate = -199.8387
expansions: []
discards: [0]
Fitting a model of length 108 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 218.4611 - loglik: -2.0098e+02 - logprior: -1.7477e+01
Epoch 2/10
10/10 - 5s - loss: 204.7340 - loglik: -1.9997e+02 - logprior: -4.7664e+00
Epoch 3/10
10/10 - 5s - loss: 200.0589 - loglik: -1.9805e+02 - logprior: -2.0074e+00
Epoch 4/10
10/10 - 7s - loss: 198.4192 - loglik: -1.9722e+02 - logprior: -1.2042e+00
Epoch 5/10
10/10 - 6s - loss: 197.5140 - loglik: -1.9697e+02 - logprior: -5.4112e-01
Epoch 6/10
10/10 - 6s - loss: 196.6721 - loglik: -1.9643e+02 - logprior: -2.4597e-01
Epoch 7/10
10/10 - 6s - loss: 196.4207 - loglik: -1.9628e+02 - logprior: -1.3631e-01
Epoch 8/10
10/10 - 7s - loss: 196.4973 - loglik: -1.9650e+02 - logprior: 0.0064
Fitted a model with MAP estimate = -196.1055
Time for alignment: 151.1548
Computed alignments with likelihoods: ['-196.2646', '-196.5253', '-196.1055']
Best model has likelihood: -196.1055
SP score = 0.1811
Training of 3 independent models on file ace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32aa534c10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34c9b1ea00>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 404 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 65s - loss: 1194.0270 - loglik: -1.1905e+03 - logprior: -3.4799e+00
Epoch 2/10
24/24 - 58s - loss: 1003.6519 - loglik: -1.0044e+03 - logprior: 0.7229
Epoch 3/10
24/24 - 57s - loss: 950.4929 - loglik: -9.5045e+02 - logprior: -4.5572e-02
Epoch 4/10
24/24 - 56s - loss: 945.0976 - loglik: -9.4497e+02 - logprior: -1.2620e-01
Epoch 5/10
24/24 - 66s - loss: 935.0771 - loglik: -9.3498e+02 - logprior: -1.0140e-01
Epoch 6/10
24/24 - 66s - loss: 937.7368 - loglik: -9.3751e+02 - logprior: -2.2332e-01
Fitted a model with MAP estimate = -935.3087
expansions: [(0, 3), (136, 1), (140, 1), (182, 1), (192, 1), (212, 1), (213, 1), (225, 1), (226, 1), (228, 4), (229, 3), (230, 3), (232, 2), (236, 2), (237, 3), (242, 1), (245, 1), (248, 1), (249, 3), (250, 5), (251, 1), (271, 2), (272, 1), (273, 1), (291, 2), (292, 6), (293, 3), (295, 3), (297, 4), (298, 4), (306, 2), (309, 3), (311, 1), (322, 1), (323, 1), (324, 1), (325, 1), (342, 1), (343, 2), (368, 1), (372, 3), (373, 1), (397, 4), (399, 7)]
discards: [ 1  2  3  4  5 34 35]
Fitting a model of length 492 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 116s - loss: 969.8560 - loglik: -9.6440e+02 - logprior: -5.4524e+00
Epoch 2/2
24/24 - 107s - loss: 927.0739 - loglik: -9.2804e+02 - logprior: 0.9613
Fitted a model with MAP estimate = -918.7668
expansions: [(0, 5), (235, 1), (347, 1), (416, 2), (419, 1), (438, 2), (475, 1), (477, 1), (483, 1), (484, 2), (485, 2)]
discards: [  2   3   4   5   6   7   8   9  10  11  12  32 240 329 373 478 487 488
 489 490 491]
Fitting a model of length 490 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 109s - loss: 934.2829 - loglik: -9.2902e+02 - logprior: -5.2636e+00
Epoch 2/2
24/24 - 99s - loss: 924.3818 - loglik: -9.2611e+02 - logprior: 1.7240
Fitted a model with MAP estimate = -916.0975
expansions: [(0, 6), (249, 1), (344, 2), (431, 1), (487, 1), (489, 3)]
discards: [  0   1   2   3   4   7   8   9 329 330 331 332 333 464]
Fitting a model of length 490 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 96s - loss: 935.4553 - loglik: -9.3199e+02 - logprior: -3.4686e+00
Epoch 2/10
24/24 - 120s - loss: 915.6677 - loglik: -9.1782e+02 - logprior: 2.1528
Epoch 3/10
24/24 - 120s - loss: 915.8287 - loglik: -9.1908e+02 - logprior: 3.2514
Fitted a model with MAP estimate = -908.5399
Time for alignment: 1444.7117
Fitting a model of length 404 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 76s - loss: 1193.3378 - loglik: -1.1900e+03 - logprior: -3.3858e+00
Epoch 2/10
24/24 - 84s - loss: 1000.7900 - loglik: -1.0014e+03 - logprior: 0.6333
Epoch 3/10
24/24 - 63s - loss: 947.3538 - loglik: -9.4735e+02 - logprior: -1.2052e-03
Epoch 4/10
24/24 - 68s - loss: 937.9391 - loglik: -9.3813e+02 - logprior: 0.1897
Epoch 5/10
24/24 - 82s - loss: 936.0662 - loglik: -9.3613e+02 - logprior: 0.0653
Epoch 6/10
24/24 - 67s - loss: 936.9894 - loglik: -9.3703e+02 - logprior: 0.0362
Fitted a model with MAP estimate = -933.1274
expansions: [(0, 3), (141, 1), (163, 2), (182, 1), (192, 1), (212, 1), (213, 1), (227, 1), (229, 4), (230, 3), (231, 3), (233, 3), (236, 3), (237, 3), (238, 1), (249, 1), (250, 3), (251, 4), (253, 1), (269, 1), (270, 1), (272, 1), (273, 1), (274, 1), (292, 1), (293, 7), (294, 3), (296, 3), (298, 5), (299, 3), (307, 1), (310, 3), (312, 1), (322, 1), (324, 2), (325, 2), (326, 1), (344, 2), (346, 1), (364, 1), (368, 1), (372, 1), (375, 1), (391, 2), (392, 3), (397, 2), (398, 2), (400, 8), (404, 4)]
discards: [ 1  2  3  4  5  6  7 34 35]
Fitting a model of length 501 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 133s - loss: 970.3914 - loglik: -9.6469e+02 - logprior: -5.6988e+00
Epoch 2/2
24/24 - 142s - loss: 926.2993 - loglik: -9.2684e+02 - logprior: 0.5444
Fitted a model with MAP estimate = -918.1920
expansions: [(0, 5), (234, 1), (347, 2), (367, 1), (416, 3), (419, 1), (484, 1), (486, 2), (487, 3)]
discards: [  2   3   4   5   6   7   8   9  10  11  12  30 158 239 281 372 390 491
 492 493 494 495 496 497 498 499 500]
Fitting a model of length 493 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 132s - loss: 937.1863 - loglik: -9.3169e+02 - logprior: -5.4982e+00
Epoch 2/2
24/24 - 135s - loss: 921.5547 - loglik: -9.2293e+02 - logprior: 1.3771
Fitted a model with MAP estimate = -915.4945
expansions: [(0, 6), (339, 2), (478, 1), (489, 1), (493, 4)]
discards: [  0   1   2   3   4 328 329 330 331 332 409 464]
Fitting a model of length 495 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 124s - loss: 930.6675 - loglik: -9.2751e+02 - logprior: -3.1595e+00
Epoch 2/10
24/24 - 92s - loss: 918.5836 - loglik: -9.2107e+02 - logprior: 2.4869
Epoch 3/10
24/24 - 115s - loss: 910.8171 - loglik: -9.1403e+02 - logprior: 3.2166
Epoch 4/10
24/24 - 94s - loss: 905.9097 - loglik: -9.0937e+02 - logprior: 3.4610
Epoch 5/10
24/24 - 112s - loss: 900.1535 - loglik: -9.0379e+02 - logprior: 3.6353
Epoch 6/10
24/24 - 134s - loss: 896.9665 - loglik: -9.0095e+02 - logprior: 3.9789
Epoch 7/10
24/24 - 137s - loss: 895.0381 - loglik: -8.9919e+02 - logprior: 4.1515
Epoch 8/10
24/24 - 128s - loss: 891.8791 - loglik: -8.9614e+02 - logprior: 4.2566
Epoch 9/10
24/24 - 130s - loss: 893.2510 - loglik: -8.9771e+02 - logprior: 4.4557
Fitted a model with MAP estimate = -890.2157
Time for alignment: 2411.7760
Fitting a model of length 404 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 90s - loss: 1197.6370 - loglik: -1.1942e+03 - logprior: -3.4257e+00
Epoch 2/10
24/24 - 74s - loss: 998.9330 - loglik: -9.9959e+02 - logprior: 0.6582
Epoch 3/10
24/24 - 78s - loss: 957.5887 - loglik: -9.5755e+02 - logprior: -3.8847e-02
Epoch 4/10
24/24 - 81s - loss: 940.7781 - loglik: -9.4090e+02 - logprior: 0.1171
Epoch 5/10
24/24 - 63s - loss: 941.0623 - loglik: -9.4116e+02 - logprior: 0.0979
Fitted a model with MAP estimate = -938.3631
expansions: [(0, 3), (131, 1), (132, 1), (140, 1), (162, 2), (181, 1), (191, 1), (211, 1), (212, 1), (229, 5), (230, 3), (231, 2), (234, 2), (238, 2), (239, 2), (240, 1), (245, 1), (248, 1), (251, 1), (252, 3), (253, 3), (254, 2), (255, 1), (271, 1), (272, 1), (273, 2), (274, 3), (275, 2), (291, 1), (292, 2), (293, 5), (294, 2), (297, 2), (299, 5), (300, 4), (308, 2), (310, 4), (311, 2), (312, 1), (320, 1), (322, 1), (323, 1), (324, 1), (329, 1), (342, 2), (344, 1), (366, 2), (368, 1), (369, 2), (370, 3), (371, 1), (395, 9)]
discards: [  1   2   3   4   5   6   7   8 107]
Fitting a model of length 501 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 114s - loss: 966.7709 - loglik: -9.6124e+02 - logprior: -5.5274e+00
Epoch 2/2
24/24 - 99s - loss: 924.6120 - loglik: -9.2538e+02 - logprior: 0.7662
Fitted a model with MAP estimate = -917.4535
expansions: [(0, 5), (245, 1), (346, 1), (351, 1), (421, 2), (487, 1), (489, 1), (491, 4)]
discards: [  2   3   4   5   6   7   8   9  10  11 159 241 242 282 314 342 378 495
 496 497 498 499 500]
Fitting a model of length 494 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 135s - loss: 935.8788 - loglik: -9.3034e+02 - logprior: -5.5391e+00
Epoch 2/2
24/24 - 136s - loss: 921.4478 - loglik: -9.2313e+02 - logprior: 1.6780
Fitted a model with MAP estimate = -914.8723
expansions: [(0, 6), (350, 1), (489, 1), (490, 1), (494, 5)]
discards: [ 0  1  2  3  4  7  8  9 25]
Fitting a model of length 499 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 102s - loss: 932.4299 - loglik: -9.2896e+02 - logprior: -3.4723e+00
Epoch 2/10
24/24 - 100s - loss: 915.2407 - loglik: -9.1749e+02 - logprior: 2.2536
Epoch 3/10
24/24 - 92s - loss: 907.7789 - loglik: -9.1078e+02 - logprior: 3.0043
Epoch 4/10
24/24 - 76s - loss: 908.6478 - loglik: -9.1198e+02 - logprior: 3.3356
Fitted a model with MAP estimate = -901.4270
Time for alignment: 1605.0088
Computed alignments with likelihoods: ['-908.5399', '-890.2157', '-901.4270']
Best model has likelihood: -890.2157
SP score = 0.8561
Training of 3 independent models on file tRNA-synt_2b.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3676e64310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f36884e5d60>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 136 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 512.9677 - loglik: -5.1013e+02 - logprior: -2.8401e+00
Epoch 2/10
19/19 - 9s - loss: 442.8645 - loglik: -4.4180e+02 - logprior: -1.0683e+00
Epoch 3/10
19/19 - 9s - loss: 410.1490 - loglik: -4.0861e+02 - logprior: -1.5409e+00
Epoch 4/10
19/19 - 9s - loss: 402.2086 - loglik: -4.0068e+02 - logprior: -1.5333e+00
Epoch 5/10
19/19 - 9s - loss: 398.7386 - loglik: -3.9719e+02 - logprior: -1.5494e+00
Epoch 6/10
19/19 - 10s - loss: 396.6407 - loglik: -3.9508e+02 - logprior: -1.5618e+00
Epoch 7/10
19/19 - 10s - loss: 394.9222 - loglik: -3.9334e+02 - logprior: -1.5790e+00
Epoch 8/10
19/19 - 10s - loss: 393.5840 - loglik: -3.9203e+02 - logprior: -1.5509e+00
Epoch 9/10
19/19 - 10s - loss: 393.6965 - loglik: -3.9216e+02 - logprior: -1.5407e+00
Fitted a model with MAP estimate = -393.9074
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 2), (26, 2), (39, 2), (40, 2), (45, 1), (55, 1), (57, 1), (58, 1), (59, 2), (69, 1), (71, 2), (89, 2), (92, 1), (93, 1), (95, 1), (102, 2), (103, 2), (104, 3), (106, 2), (118, 1), (122, 1), (125, 1), (126, 2), (133, 1)]
discards: [2]
Fitting a model of length 179 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 18s - loss: 411.3469 - loglik: -4.0854e+02 - logprior: -2.8100e+00
Epoch 2/2
19/19 - 16s - loss: 386.9957 - loglik: -3.8589e+02 - logprior: -1.1105e+00
Fitted a model with MAP estimate = -383.3307
expansions: []
discards: [ 13  30  35  50  77  93 113 131 135 165]
Fitting a model of length 169 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 388.8976 - loglik: -3.8615e+02 - logprior: -2.7485e+00
Epoch 2/2
19/19 - 13s - loss: 383.4117 - loglik: -3.8251e+02 - logprior: -9.0489e-01
Fitted a model with MAP estimate = -382.2327
expansions: []
discards: [  0 132]
Fitting a model of length 167 on 11293 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 17s - loss: 389.2045 - loglik: -3.8600e+02 - logprior: -3.2069e+00
Epoch 2/10
20/20 - 14s - loss: 383.7977 - loglik: -3.8235e+02 - logprior: -1.4438e+00
Epoch 3/10
20/20 - 15s - loss: 381.6977 - loglik: -3.8093e+02 - logprior: -7.6470e-01
Epoch 4/10
20/20 - 15s - loss: 378.7665 - loglik: -3.7835e+02 - logprior: -4.1150e-01
Epoch 5/10
20/20 - 14s - loss: 375.4630 - loglik: -3.7489e+02 - logprior: -5.7551e-01
Epoch 6/10
20/20 - 15s - loss: 375.3829 - loglik: -3.7485e+02 - logprior: -5.3735e-01
Epoch 7/10
20/20 - 14s - loss: 372.2154 - loglik: -3.7168e+02 - logprior: -5.3196e-01
Epoch 8/10
20/20 - 13s - loss: 370.6929 - loglik: -3.7017e+02 - logprior: -5.2661e-01
Epoch 9/10
20/20 - 13s - loss: 372.8174 - loglik: -3.7232e+02 - logprior: -5.0098e-01
Fitted a model with MAP estimate = -370.2948
Time for alignment: 340.7680
Fitting a model of length 136 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 512.8173 - loglik: -5.0997e+02 - logprior: -2.8492e+00
Epoch 2/10
19/19 - 9s - loss: 444.0195 - loglik: -4.4294e+02 - logprior: -1.0748e+00
Epoch 3/10
19/19 - 10s - loss: 413.3535 - loglik: -4.1183e+02 - logprior: -1.5233e+00
Epoch 4/10
19/19 - 10s - loss: 404.3250 - loglik: -4.0279e+02 - logprior: -1.5312e+00
Epoch 5/10
19/19 - 10s - loss: 401.3126 - loglik: -3.9977e+02 - logprior: -1.5389e+00
Epoch 6/10
19/19 - 10s - loss: 399.0735 - loglik: -3.9756e+02 - logprior: -1.5118e+00
Epoch 7/10
19/19 - 10s - loss: 396.9200 - loglik: -3.9538e+02 - logprior: -1.5428e+00
Epoch 8/10
19/19 - 10s - loss: 395.9507 - loglik: -3.9440e+02 - logprior: -1.5465e+00
Epoch 9/10
19/19 - 10s - loss: 395.8411 - loglik: -3.9431e+02 - logprior: -1.5270e+00
Epoch 10/10
19/19 - 9s - loss: 395.4363 - loglik: -3.9392e+02 - logprior: -1.5181e+00
Fitted a model with MAP estimate = -395.7234
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (14, 2), (23, 1), (27, 1), (39, 2), (40, 1), (45, 1), (55, 2), (57, 2), (58, 1), (68, 1), (69, 2), (71, 2), (89, 2), (92, 1), (93, 1), (95, 1), (99, 1), (103, 2), (104, 3), (106, 2), (118, 1), (119, 1), (122, 1), (126, 2), (133, 2)]
discards: [2]
Fitting a model of length 179 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 17s - loss: 416.9250 - loglik: -4.1411e+02 - logprior: -2.8170e+00
Epoch 2/2
19/19 - 15s - loss: 389.0522 - loglik: -3.8792e+02 - logprior: -1.1324e+00
Fitted a model with MAP estimate = -385.6254
expansions: []
discards: [ 19  69  72  88  93 113 134 135 166]
Fitting a model of length 170 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 17s - loss: 391.0855 - loglik: -3.8828e+02 - logprior: -2.8018e+00
Epoch 2/2
19/19 - 15s - loss: 385.8133 - loglik: -3.8488e+02 - logprior: -9.3648e-01
Fitted a model with MAP estimate = -383.9248
expansions: []
discards: [ 13 165]
Fitting a model of length 168 on 11293 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 19s - loss: 387.9444 - loglik: -3.8553e+02 - logprior: -2.4176e+00
Epoch 2/10
20/20 - 17s - loss: 383.6428 - loglik: -3.8285e+02 - logprior: -7.9201e-01
Epoch 3/10
20/20 - 17s - loss: 382.1197 - loglik: -3.8147e+02 - logprior: -6.4740e-01
Epoch 4/10
20/20 - 16s - loss: 379.8733 - loglik: -3.7928e+02 - logprior: -5.9390e-01
Epoch 5/10
20/20 - 16s - loss: 377.3025 - loglik: -3.7675e+02 - logprior: -5.5155e-01
Epoch 6/10
20/20 - 16s - loss: 375.7471 - loglik: -3.7520e+02 - logprior: -5.5023e-01
Epoch 7/10
20/20 - 16s - loss: 374.2517 - loglik: -3.7373e+02 - logprior: -5.1910e-01
Epoch 8/10
20/20 - 16s - loss: 371.6401 - loglik: -3.7112e+02 - logprior: -5.1981e-01
Epoch 9/10
20/20 - 16s - loss: 370.4501 - loglik: -3.6998e+02 - logprior: -4.7419e-01
Epoch 10/10
20/20 - 16s - loss: 369.5938 - loglik: -3.6913e+02 - logprior: -4.6254e-01
Fitted a model with MAP estimate = -369.4238
Time for alignment: 391.6809
Fitting a model of length 136 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 512.8445 - loglik: -5.0998e+02 - logprior: -2.8600e+00
Epoch 2/10
19/19 - 11s - loss: 444.4905 - loglik: -4.4339e+02 - logprior: -1.0970e+00
Epoch 3/10
19/19 - 11s - loss: 411.2930 - loglik: -4.0973e+02 - logprior: -1.5664e+00
Epoch 4/10
19/19 - 11s - loss: 401.2678 - loglik: -3.9968e+02 - logprior: -1.5830e+00
Epoch 5/10
19/19 - 11s - loss: 397.8847 - loglik: -3.9629e+02 - logprior: -1.5988e+00
Epoch 6/10
19/19 - 11s - loss: 396.2163 - loglik: -3.9466e+02 - logprior: -1.5610e+00
Epoch 7/10
19/19 - 11s - loss: 393.3159 - loglik: -3.9174e+02 - logprior: -1.5713e+00
Epoch 8/10
19/19 - 10s - loss: 393.4987 - loglik: -3.9192e+02 - logprior: -1.5800e+00
Fitted a model with MAP estimate = -393.4380
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 2), (26, 2), (32, 1), (39, 2), (44, 1), (45, 1), (55, 2), (57, 1), (58, 1), (59, 2), (68, 1), (71, 2), (89, 2), (92, 1), (93, 1), (95, 1), (99, 1), (103, 2), (104, 2), (106, 2), (118, 1), (119, 1), (122, 1), (126, 2), (133, 2)]
discards: [2]
Fitting a model of length 179 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 17s - loss: 410.4686 - loglik: -4.0768e+02 - logprior: -2.7931e+00
Epoch 2/2
19/19 - 15s - loss: 385.1229 - loglik: -3.8395e+02 - logprior: -1.1775e+00
Fitted a model with MAP estimate = -381.4986
expansions: []
discards: [ 13  30  35  71  78  94 114 135 174]
Fitting a model of length 170 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 18s - loss: 387.0618 - loglik: -3.8429e+02 - logprior: -2.7747e+00
Epoch 2/2
19/19 - 14s - loss: 380.8766 - loglik: -3.7996e+02 - logprior: -9.1229e-01
Fitted a model with MAP estimate = -380.0304
expansions: []
discards: []
Fitting a model of length 170 on 11293 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 17s - loss: 383.3200 - loglik: -3.8089e+02 - logprior: -2.4251e+00
Epoch 2/10
20/20 - 14s - loss: 378.8333 - loglik: -3.7803e+02 - logprior: -8.0232e-01
Epoch 3/10
20/20 - 14s - loss: 377.4680 - loglik: -3.7680e+02 - logprior: -6.6951e-01
Epoch 4/10
20/20 - 14s - loss: 374.7993 - loglik: -3.7420e+02 - logprior: -6.0171e-01
Epoch 5/10
20/20 - 14s - loss: 374.5765 - loglik: -3.7400e+02 - logprior: -5.7843e-01
Epoch 6/10
20/20 - 15s - loss: 370.1199 - loglik: -3.6956e+02 - logprior: -5.5663e-01
Epoch 7/10
20/20 - 15s - loss: 370.4954 - loglik: -3.6997e+02 - logprior: -5.2109e-01
Fitted a model with MAP estimate = -369.0105
Time for alignment: 318.1645
Computed alignments with likelihoods: ['-370.2948', '-369.4238', '-369.0105']
Best model has likelihood: -369.0105
SP score = 0.5037
Training of 3 independent models on file ChtBD.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34d2125ee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32c8640b20>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 170.4901 - loglik: -1.1159e+02 - logprior: -5.8900e+01
Epoch 2/10
10/10 - 0s - loss: 105.8244 - loglik: -8.9263e+01 - logprior: -1.6562e+01
Epoch 3/10
10/10 - 0s - loss: 81.8949 - loglik: -7.3655e+01 - logprior: -8.2396e+00
Epoch 4/10
10/10 - 0s - loss: 71.3590 - loglik: -6.6169e+01 - logprior: -5.1902e+00
Epoch 5/10
10/10 - 0s - loss: 65.8926 - loglik: -6.2274e+01 - logprior: -3.6189e+00
Epoch 6/10
10/10 - 0s - loss: 63.6024 - loglik: -6.0770e+01 - logprior: -2.8328e+00
Epoch 7/10
10/10 - 0s - loss: 62.8595 - loglik: -6.0464e+01 - logprior: -2.3953e+00
Epoch 8/10
10/10 - 0s - loss: 62.3752 - loglik: -6.0277e+01 - logprior: -2.0986e+00
Epoch 9/10
10/10 - 0s - loss: 62.0227 - loglik: -6.0120e+01 - logprior: -1.9024e+00
Epoch 10/10
10/10 - 0s - loss: 61.7388 - loglik: -5.9941e+01 - logprior: -1.7982e+00
Fitted a model with MAP estimate = -61.6041
expansions: [(0, 4), (14, 1), (21, 1), (25, 2)]
discards: []
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 137.2181 - loglik: -5.8100e+01 - logprior: -7.9118e+01
Epoch 2/2
10/10 - 0s - loss: 79.0944 - loglik: -5.3710e+01 - logprior: -2.5384e+01
Fitted a model with MAP estimate = -67.8918
expansions: [(0, 2)]
discards: [31]
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 117.2102 - loglik: -5.1550e+01 - logprior: -6.5660e+01
Epoch 2/2
10/10 - 0s - loss: 73.6263 - loglik: -5.0588e+01 - logprior: -2.3038e+01
Fitted a model with MAP estimate = -64.6746
expansions: []
discards: []
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 104.9987 - loglik: -5.0219e+01 - logprior: -5.4779e+01
Epoch 2/10
10/10 - 0s - loss: 66.1586 - loglik: -5.0825e+01 - logprior: -1.5334e+01
Epoch 3/10
10/10 - 0s - loss: 58.5012 - loglik: -5.1203e+01 - logprior: -7.2986e+00
Epoch 4/10
10/10 - 0s - loss: 55.5949 - loglik: -5.1451e+01 - logprior: -4.1437e+00
Epoch 5/10
10/10 - 0s - loss: 53.6831 - loglik: -5.1098e+01 - logprior: -2.5847e+00
Epoch 6/10
10/10 - 0s - loss: 52.7848 - loglik: -5.1053e+01 - logprior: -1.7316e+00
Epoch 7/10
10/10 - 0s - loss: 52.2961 - loglik: -5.1270e+01 - logprior: -1.0257e+00
Epoch 8/10
10/10 - 0s - loss: 51.7791 - loglik: -5.1127e+01 - logprior: -6.5212e-01
Epoch 9/10
10/10 - 0s - loss: 51.6896 - loglik: -5.1294e+01 - logprior: -3.9513e-01
Epoch 10/10
10/10 - 0s - loss: 51.4100 - loglik: -5.1193e+01 - logprior: -2.1694e-01
Fitted a model with MAP estimate = -51.3507
Time for alignment: 25.4921
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 170.5096 - loglik: -1.1161e+02 - logprior: -5.8900e+01
Epoch 2/10
10/10 - 0s - loss: 105.7957 - loglik: -8.9227e+01 - logprior: -1.6568e+01
Epoch 3/10
10/10 - 0s - loss: 80.9025 - loglik: -7.2663e+01 - logprior: -8.2392e+00
Epoch 4/10
10/10 - 0s - loss: 70.2748 - loglik: -6.5109e+01 - logprior: -5.1655e+00
Epoch 5/10
10/10 - 0s - loss: 65.5112 - loglik: -6.1903e+01 - logprior: -3.6083e+00
Epoch 6/10
10/10 - 0s - loss: 63.4664 - loglik: -6.0644e+01 - logprior: -2.8222e+00
Epoch 7/10
10/10 - 0s - loss: 62.8033 - loglik: -6.0425e+01 - logprior: -2.3786e+00
Epoch 8/10
10/10 - 0s - loss: 62.2018 - loglik: -6.0121e+01 - logprior: -2.0806e+00
Epoch 9/10
10/10 - 0s - loss: 62.1125 - loglik: -6.0239e+01 - logprior: -1.8739e+00
Epoch 10/10
10/10 - 0s - loss: 61.7373 - loglik: -5.9988e+01 - logprior: -1.7497e+00
Fitted a model with MAP estimate = -61.5798
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 137.9739 - loglik: -5.8927e+01 - logprior: -7.9047e+01
Epoch 2/2
10/10 - 0s - loss: 79.4632 - loglik: -5.3865e+01 - logprior: -2.5598e+01
Fitted a model with MAP estimate = -68.2958
expansions: [(0, 2)]
discards: [14 32]
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 117.3859 - loglik: -5.1687e+01 - logprior: -6.5699e+01
Epoch 2/2
10/10 - 0s - loss: 73.6339 - loglik: -5.0590e+01 - logprior: -2.3044e+01
Fitted a model with MAP estimate = -64.6669
expansions: []
discards: []
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 105.1035 - loglik: -5.0323e+01 - logprior: -5.4781e+01
Epoch 2/10
10/10 - 0s - loss: 66.0579 - loglik: -5.0724e+01 - logprior: -1.5334e+01
Epoch 3/10
10/10 - 0s - loss: 58.5318 - loglik: -5.1233e+01 - logprior: -7.2989e+00
Epoch 4/10
10/10 - 0s - loss: 55.5053 - loglik: -5.1354e+01 - logprior: -4.1516e+00
Epoch 5/10
10/10 - 0s - loss: 53.6259 - loglik: -5.0973e+01 - logprior: -2.6529e+00
Epoch 6/10
10/10 - 0s - loss: 52.8266 - loglik: -5.1172e+01 - logprior: -1.6550e+00
Epoch 7/10
10/10 - 0s - loss: 52.0730 - loglik: -5.1061e+01 - logprior: -1.0122e+00
Epoch 8/10
10/10 - 0s - loss: 51.8388 - loglik: -5.1196e+01 - logprior: -6.4253e-01
Epoch 9/10
10/10 - 0s - loss: 51.6190 - loglik: -5.1229e+01 - logprior: -3.8959e-01
Epoch 10/10
10/10 - 0s - loss: 51.4824 - loglik: -5.1270e+01 - logprior: -2.1234e-01
Fitted a model with MAP estimate = -51.3518
Time for alignment: 25.2801
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 170.5136 - loglik: -1.1161e+02 - logprior: -5.8900e+01
Epoch 2/10
10/10 - 0s - loss: 105.7057 - loglik: -8.9137e+01 - logprior: -1.6569e+01
Epoch 3/10
10/10 - 0s - loss: 81.0631 - loglik: -7.2826e+01 - logprior: -8.2372e+00
Epoch 4/10
10/10 - 0s - loss: 70.4003 - loglik: -6.5252e+01 - logprior: -5.1482e+00
Epoch 5/10
10/10 - 0s - loss: 65.9095 - loglik: -6.2344e+01 - logprior: -3.5659e+00
Epoch 6/10
10/10 - 0s - loss: 62.9284 - loglik: -6.0126e+01 - logprior: -2.8028e+00
Epoch 7/10
10/10 - 0s - loss: 61.7217 - loglik: -5.9347e+01 - logprior: -2.3743e+00
Epoch 8/10
10/10 - 0s - loss: 61.0760 - loglik: -5.8973e+01 - logprior: -2.1027e+00
Epoch 9/10
10/10 - 0s - loss: 60.5809 - loglik: -5.8706e+01 - logprior: -1.8745e+00
Epoch 10/10
10/10 - 0s - loss: 60.6881 - loglik: -5.8997e+01 - logprior: -1.6911e+00
Fitted a model with MAP estimate = -60.4598
expansions: [(0, 4), (10, 2), (21, 1)]
discards: []
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 137.0087 - loglik: -5.7706e+01 - logprior: -7.9303e+01
Epoch 2/2
10/10 - 0s - loss: 78.9299 - loglik: -5.3463e+01 - logprior: -2.5467e+01
Fitted a model with MAP estimate = -67.7717
expansions: [(0, 2)]
discards: [14]
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 117.0057 - loglik: -5.1196e+01 - logprior: -6.5810e+01
Epoch 2/2
10/10 - 0s - loss: 73.4330 - loglik: -5.0366e+01 - logprior: -2.3067e+01
Fitted a model with MAP estimate = -64.4449
expansions: []
discards: []
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 104.8733 - loglik: -5.0059e+01 - logprior: -5.4814e+01
Epoch 2/10
10/10 - 0s - loss: 65.8278 - loglik: -5.0422e+01 - logprior: -1.5406e+01
Epoch 3/10
10/10 - 0s - loss: 58.4448 - loglik: -5.1066e+01 - logprior: -7.3789e+00
Epoch 4/10
10/10 - 0s - loss: 55.4097 - loglik: -5.1191e+01 - logprior: -4.2187e+00
Epoch 5/10
10/10 - 0s - loss: 53.7609 - loglik: -5.1108e+01 - logprior: -2.6526e+00
Epoch 6/10
10/10 - 0s - loss: 52.7762 - loglik: -5.0958e+01 - logprior: -1.8182e+00
Epoch 7/10
10/10 - 0s - loss: 52.1033 - loglik: -5.0988e+01 - logprior: -1.1149e+00
Epoch 8/10
10/10 - 0s - loss: 51.7171 - loglik: -5.0971e+01 - logprior: -7.4644e-01
Epoch 9/10
10/10 - 0s - loss: 51.5176 - loglik: -5.1024e+01 - logprior: -4.9359e-01
Epoch 10/10
10/10 - 0s - loss: 51.3936 - loglik: -5.1078e+01 - logprior: -3.1545e-01
Fitted a model with MAP estimate = -51.2261
Time for alignment: 24.6008
Computed alignments with likelihoods: ['-51.3507', '-51.3518', '-51.2261']
Best model has likelihood: -51.2261
SP score = 0.9734
Training of 3 independent models on file adh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32a9c4af10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32e82261c0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 8s - loss: 372.6104 - loglik: -3.6980e+02 - logprior: -2.8110e+00
Epoch 2/10
20/20 - 4s - loss: 327.7237 - loglik: -3.2646e+02 - logprior: -1.2630e+00
Epoch 3/10
20/20 - 4s - loss: 307.4131 - loglik: -3.0595e+02 - logprior: -1.4653e+00
Epoch 4/10
20/20 - 4s - loss: 301.0693 - loglik: -2.9966e+02 - logprior: -1.4066e+00
Epoch 5/10
20/20 - 5s - loss: 298.2146 - loglik: -2.9680e+02 - logprior: -1.4181e+00
Epoch 6/10
20/20 - 4s - loss: 296.3783 - loglik: -2.9500e+02 - logprior: -1.3820e+00
Epoch 7/10
20/20 - 5s - loss: 295.8205 - loglik: -2.9444e+02 - logprior: -1.3776e+00
Epoch 8/10
20/20 - 4s - loss: 294.3644 - loglik: -2.9299e+02 - logprior: -1.3771e+00
Epoch 9/10
20/20 - 5s - loss: 294.2995 - loglik: -2.9291e+02 - logprior: -1.3845e+00
Epoch 10/10
20/20 - 5s - loss: 294.1856 - loglik: -2.9281e+02 - logprior: -1.3755e+00
Fitted a model with MAP estimate = -285.7774
expansions: [(5, 1), (8, 1), (11, 2), (13, 1), (19, 1), (21, 1), (23, 2), (31, 1), (37, 2), (39, 1), (40, 1), (47, 1), (51, 1), (57, 1), (58, 2), (59, 1), (60, 1), (61, 2), (62, 2), (76, 1), (78, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Fitting a model of length 138 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 10s - loss: 321.9449 - loglik: -3.1827e+02 - logprior: -3.6740e+00
Epoch 2/2
20/20 - 7s - loss: 295.8868 - loglik: -2.9413e+02 - logprior: -1.7534e+00
Fitted a model with MAP estimate = -278.1380
expansions: [(0, 1)]
discards: [  0  12  30  76  84  85 104]
Fitting a model of length 132 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 9s - loss: 296.6606 - loglik: -2.9392e+02 - logprior: -2.7376e+00
Epoch 2/2
20/20 - 7s - loss: 290.1080 - loglik: -2.8913e+02 - logprior: -9.7970e-01
Fitted a model with MAP estimate = -275.3037
expansions: []
discards: []
Fitting a model of length 132 on 21331 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
28/28 - 12s - loss: 274.8612 - loglik: -2.7345e+02 - logprior: -1.4123e+00
Epoch 2/10
28/28 - 8s - loss: 270.2337 - loglik: -2.6940e+02 - logprior: -8.3750e-01
Epoch 3/10
28/28 - 9s - loss: 267.3328 - loglik: -2.6651e+02 - logprior: -8.2500e-01
Epoch 4/10
28/28 - 10s - loss: 263.9069 - loglik: -2.6311e+02 - logprior: -7.9982e-01
Epoch 5/10
28/28 - 9s - loss: 262.7091 - loglik: -2.6192e+02 - logprior: -7.8878e-01
Epoch 6/10
28/28 - 10s - loss: 260.5453 - loglik: -2.5977e+02 - logprior: -7.7576e-01
Epoch 7/10
28/28 - 9s - loss: 259.6763 - loglik: -2.5891e+02 - logprior: -7.6741e-01
Epoch 8/10
28/28 - 9s - loss: 258.6949 - loglik: -2.5793e+02 - logprior: -7.6628e-01
Epoch 9/10
28/28 - 9s - loss: 258.7340 - loglik: -2.5797e+02 - logprior: -7.5962e-01
Fitted a model with MAP estimate = -258.1503
Time for alignment: 208.9708
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 8s - loss: 372.4997 - loglik: -3.6969e+02 - logprior: -2.8076e+00
Epoch 2/10
20/20 - 5s - loss: 325.4738 - loglik: -3.2422e+02 - logprior: -1.2499e+00
Epoch 3/10
20/20 - 5s - loss: 305.3730 - loglik: -3.0391e+02 - logprior: -1.4605e+00
Epoch 4/10
20/20 - 5s - loss: 299.5356 - loglik: -2.9816e+02 - logprior: -1.3766e+00
Epoch 5/10
20/20 - 5s - loss: 296.9088 - loglik: -2.9553e+02 - logprior: -1.3821e+00
Epoch 6/10
20/20 - 5s - loss: 295.8986 - loglik: -2.9455e+02 - logprior: -1.3510e+00
Epoch 7/10
20/20 - 5s - loss: 294.8207 - loglik: -2.9349e+02 - logprior: -1.3289e+00
Epoch 8/10
20/20 - 5s - loss: 294.8782 - loglik: -2.9355e+02 - logprior: -1.3232e+00
Fitted a model with MAP estimate = -284.2780
expansions: [(5, 1), (8, 2), (10, 2), (13, 2), (19, 1), (20, 1), (31, 1), (35, 1), (36, 2), (38, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 2), (62, 2), (76, 1), (78, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Fitting a model of length 138 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 10s - loss: 318.4785 - loglik: -3.1483e+02 - logprior: -3.6520e+00
Epoch 2/2
20/20 - 8s - loss: 295.0334 - loglik: -2.9331e+02 - logprior: -1.7240e+00
Fitted a model with MAP estimate = -277.8795
expansions: [(0, 2), (99, 1)]
discards: [  0   8  13  46  76  84  85 104]
Fitting a model of length 133 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 9s - loss: 295.3538 - loglik: -2.9280e+02 - logprior: -2.5523e+00
Epoch 2/2
20/20 - 8s - loss: 289.0248 - loglik: -2.8812e+02 - logprior: -9.0438e-01
Fitted a model with MAP estimate = -274.6149
expansions: []
discards: []
Fitting a model of length 133 on 21331 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
28/28 - 12s - loss: 274.1248 - loglik: -2.7288e+02 - logprior: -1.2480e+00
Epoch 2/10
28/28 - 9s - loss: 269.7520 - loglik: -2.6902e+02 - logprior: -7.3283e-01
Epoch 3/10
28/28 - 9s - loss: 266.5390 - loglik: -2.6582e+02 - logprior: -7.1872e-01
Epoch 4/10
28/28 - 9s - loss: 263.7390 - loglik: -2.6304e+02 - logprior: -6.9444e-01
Epoch 5/10
28/28 - 9s - loss: 261.9040 - loglik: -2.6121e+02 - logprior: -6.8900e-01
Epoch 6/10
28/28 - 9s - loss: 260.4748 - loglik: -2.5979e+02 - logprior: -6.8277e-01
Epoch 7/10
28/28 - 9s - loss: 259.3481 - loglik: -2.5868e+02 - logprior: -6.7094e-01
Epoch 8/10
28/28 - 8s - loss: 258.4396 - loglik: -2.5778e+02 - logprior: -6.6421e-01
Epoch 9/10
28/28 - 8s - loss: 258.3167 - loglik: -2.5766e+02 - logprior: -6.5766e-01
Epoch 10/10
28/28 - 7s - loss: 257.5302 - loglik: -2.5688e+02 - logprior: -6.4995e-01
Fitted a model with MAP estimate = -257.6091
Time for alignment: 211.7889
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 8s - loss: 372.5495 - loglik: -3.6974e+02 - logprior: -2.8071e+00
Epoch 2/10
20/20 - 4s - loss: 325.1067 - loglik: -3.2387e+02 - logprior: -1.2363e+00
Epoch 3/10
20/20 - 5s - loss: 305.3756 - loglik: -3.0393e+02 - logprior: -1.4407e+00
Epoch 4/10
20/20 - 5s - loss: 299.3429 - loglik: -2.9796e+02 - logprior: -1.3872e+00
Epoch 5/10
20/20 - 4s - loss: 296.1122 - loglik: -2.9470e+02 - logprior: -1.4074e+00
Epoch 6/10
20/20 - 4s - loss: 295.3776 - loglik: -2.9401e+02 - logprior: -1.3701e+00
Epoch 7/10
20/20 - 5s - loss: 294.4871 - loglik: -2.9313e+02 - logprior: -1.3524e+00
Epoch 8/10
20/20 - 4s - loss: 294.1966 - loglik: -2.9285e+02 - logprior: -1.3474e+00
Epoch 9/10
20/20 - 5s - loss: 293.9263 - loglik: -2.9258e+02 - logprior: -1.3414e+00
Epoch 10/10
20/20 - 5s - loss: 293.7712 - loglik: -2.9244e+02 - logprior: -1.3357e+00
Fitted a model with MAP estimate = -284.4870
expansions: [(5, 1), (8, 2), (10, 2), (13, 2), (19, 1), (22, 1), (31, 1), (37, 2), (39, 1), (40, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 2), (64, 2), (76, 1), (78, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Fitting a model of length 138 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 9s - loss: 321.1059 - loglik: -3.1744e+02 - logprior: -3.6644e+00
Epoch 2/2
20/20 - 7s - loss: 295.9992 - loglik: -2.9425e+02 - logprior: -1.7484e+00
Fitted a model with MAP estimate = -278.3039
expansions: [(0, 1), (99, 1)]
discards: [  0   8  13  76  81  87 104]
Fitting a model of length 133 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 9s - loss: 296.4434 - loglik: -2.9371e+02 - logprior: -2.7377e+00
Epoch 2/2
20/20 - 6s - loss: 289.5261 - loglik: -2.8856e+02 - logprior: -9.7072e-01
Fitted a model with MAP estimate = -275.1874
expansions: []
discards: [44]
Fitting a model of length 132 on 21331 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
28/28 - 9s - loss: 274.9834 - loglik: -2.7356e+02 - logprior: -1.4217e+00
Epoch 2/10
28/28 - 8s - loss: 270.0727 - loglik: -2.6922e+02 - logprior: -8.5418e-01
Epoch 3/10
28/28 - 8s - loss: 267.3815 - loglik: -2.6657e+02 - logprior: -8.1417e-01
Epoch 4/10
28/28 - 8s - loss: 263.8365 - loglik: -2.6304e+02 - logprior: -7.9744e-01
Epoch 5/10
28/28 - 7s - loss: 262.4638 - loglik: -2.6168e+02 - logprior: -7.8172e-01
Epoch 6/10
28/28 - 8s - loss: 260.8586 - loglik: -2.6008e+02 - logprior: -7.7675e-01
Epoch 7/10
28/28 - 8s - loss: 259.5584 - loglik: -2.5879e+02 - logprior: -7.6921e-01
Epoch 8/10
28/28 - 9s - loss: 259.0970 - loglik: -2.5833e+02 - logprior: -7.6840e-01
Epoch 9/10
28/28 - 8s - loss: 258.7625 - loglik: -2.5802e+02 - logprior: -7.4624e-01
Epoch 10/10
28/28 - 7s - loss: 257.7225 - loglik: -2.5697e+02 - logprior: -7.5018e-01
Fitted a model with MAP estimate = -258.0356
Time for alignment: 202.5550
Computed alignments with likelihoods: ['-258.1503', '-257.6091', '-258.0356']
Best model has likelihood: -257.6091
SP score = 0.8062
Training of 3 independent models on file Sulfotransfer.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f35ed5cb6d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34c9b722b0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 198 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 13s - loss: 704.8146 - loglik: -6.8919e+02 - logprior: -1.5621e+01
Epoch 2/10
10/10 - 9s - loss: 663.9552 - loglik: -6.6135e+02 - logprior: -2.6043e+00
Epoch 3/10
10/10 - 9s - loss: 625.7529 - loglik: -6.2529e+02 - logprior: -4.6066e-01
Epoch 4/10
10/10 - 10s - loss: 593.6792 - loglik: -5.9334e+02 - logprior: -3.3505e-01
Epoch 5/10
10/10 - 9s - loss: 578.4328 - loglik: -5.7803e+02 - logprior: -4.0203e-01
Epoch 6/10
10/10 - 10s - loss: 568.5543 - loglik: -5.6823e+02 - logprior: -3.2882e-01
Epoch 7/10
10/10 - 10s - loss: 563.3450 - loglik: -5.6314e+02 - logprior: -2.0786e-01
Epoch 8/10
10/10 - 10s - loss: 561.7068 - loglik: -5.6156e+02 - logprior: -1.4358e-01
Epoch 9/10
10/10 - 11s - loss: 558.7576 - loglik: -5.5868e+02 - logprior: -8.0999e-02
Epoch 10/10
10/10 - 10s - loss: 559.2957 - loglik: -5.5918e+02 - logprior: -1.1951e-01
Fitted a model with MAP estimate = -557.6946
expansions: [(18, 1), (25, 1), (29, 1), (49, 9), (92, 4), (93, 2), (104, 1), (105, 1), (121, 1), (137, 1), (153, 5), (164, 2), (166, 3), (167, 2), (182, 1), (183, 1), (184, 2)]
discards: [  2 172 173 174 175 176 177 178 179]
Fitting a model of length 227 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 16s - loss: 672.6473 - loglik: -6.5772e+02 - logprior: -1.4932e+01
Epoch 2/2
10/10 - 14s - loss: 617.1488 - loglik: -6.1253e+02 - logprior: -4.6212e+00
Fitted a model with MAP estimate = -600.4659
expansions: [(43, 3), (77, 3), (106, 4)]
discards: [ 30  31  32  33  34  35  36  37  38  51  52  53  96  97  98  99 100 101
 102 110 111 112 113 178 179 180 181 182 183 184 185 191 200 201 202 203
 204 213 214 223 225 226]
Fitting a model of length 195 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 14s - loss: 622.6664 - loglik: -6.0768e+02 - logprior: -1.4983e+01
Epoch 2/2
10/10 - 11s - loss: 597.2106 - loglik: -5.9410e+02 - logprior: -3.1094e+00
Fitted a model with MAP estimate = -591.9294
expansions: [(38, 3), (42, 2), (98, 4), (99, 2), (100, 2), (164, 6), (165, 2), (171, 9), (194, 2)]
discards: [175]
Fitting a model of length 226 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 17s - loss: 606.4359 - loglik: -5.9229e+02 - logprior: -1.4147e+01
Epoch 2/10
10/10 - 15s - loss: 586.0613 - loglik: -5.8330e+02 - logprior: -2.7590e+00
Epoch 3/10
10/10 - 15s - loss: 575.8849 - loglik: -5.7536e+02 - logprior: -5.2636e-01
Epoch 4/10
10/10 - 15s - loss: 572.4833 - loglik: -5.7281e+02 - logprior: 0.3273
Epoch 5/10
10/10 - 14s - loss: 565.5996 - loglik: -5.6631e+02 - logprior: 0.7074
Epoch 6/10
10/10 - 13s - loss: 557.5297 - loglik: -5.5838e+02 - logprior: 0.8484
Epoch 7/10
10/10 - 13s - loss: 553.6266 - loglik: -5.5471e+02 - logprior: 1.0809
Epoch 8/10
10/10 - 13s - loss: 548.7548 - loglik: -5.5001e+02 - logprior: 1.2507
Epoch 9/10
10/10 - 12s - loss: 548.7357 - loglik: -5.5006e+02 - logprior: 1.3219
Epoch 10/10
10/10 - 12s - loss: 545.9589 - loglik: -5.4734e+02 - logprior: 1.3767
Fitted a model with MAP estimate = -545.4288
Time for alignment: 326.1973
Fitting a model of length 198 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 13s - loss: 703.9305 - loglik: -6.8830e+02 - logprior: -1.5629e+01
Epoch 2/10
10/10 - 9s - loss: 665.1740 - loglik: -6.6257e+02 - logprior: -2.6061e+00
Epoch 3/10
10/10 - 9s - loss: 626.0608 - loglik: -6.2557e+02 - logprior: -4.9039e-01
Epoch 4/10
10/10 - 9s - loss: 594.7726 - loglik: -5.9421e+02 - logprior: -5.5872e-01
Epoch 5/10
10/10 - 9s - loss: 579.5072 - loglik: -5.7867e+02 - logprior: -8.3581e-01
Epoch 6/10
10/10 - 9s - loss: 569.5179 - loglik: -5.6864e+02 - logprior: -8.7550e-01
Epoch 7/10
10/10 - 9s - loss: 564.7007 - loglik: -5.6392e+02 - logprior: -7.7615e-01
Epoch 8/10
10/10 - 9s - loss: 561.8792 - loglik: -5.6125e+02 - logprior: -6.3380e-01
Epoch 9/10
10/10 - 9s - loss: 561.4074 - loglik: -5.6081e+02 - logprior: -5.9599e-01
Epoch 10/10
10/10 - 9s - loss: 558.3729 - loglik: -5.5777e+02 - logprior: -6.0468e-01
Fitted a model with MAP estimate = -559.0764
expansions: [(18, 1), (25, 1), (27, 1), (29, 2), (48, 11), (89, 3), (101, 1), (102, 2), (103, 1), (104, 1), (120, 1), (132, 1), (134, 1), (135, 1), (146, 2), (148, 2), (151, 1), (152, 1), (153, 1), (167, 3), (182, 1), (183, 1), (184, 2)]
discards: [  2 173 174 175 176 177 178]
Fitting a model of length 233 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 14s - loss: 669.0043 - loglik: -6.5419e+02 - logprior: -1.4811e+01
Epoch 2/2
10/10 - 11s - loss: 613.5856 - loglik: -6.0929e+02 - logprior: -4.2923e+00
Fitted a model with MAP estimate = -597.1251
expansions: [(31, 2), (80, 3), (119, 1)]
discards: [ 33  34  35  36  37  38  39  52 107 108 109 110 120 185 186 187 188 189
 190 191 192 193 194 195 196 204 205 206 207 208 209 219 220 221 222 223
 229 230 231 232]
Fitting a model of length 199 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 623.4960 - loglik: -6.0856e+02 - logprior: -1.4938e+01
Epoch 2/2
10/10 - 9s - loss: 596.3004 - loglik: -5.9327e+02 - logprior: -3.0276e+00
Fitted a model with MAP estimate = -591.6042
expansions: [(30, 2), (39, 4), (102, 3), (103, 2), (104, 1), (182, 2), (199, 11)]
discards: [194]
Fitting a model of length 223 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 13s - loss: 605.0539 - loglik: -5.9092e+02 - logprior: -1.4134e+01
Epoch 2/10
10/10 - 11s - loss: 586.7391 - loglik: -5.8413e+02 - logprior: -2.6121e+00
Epoch 3/10
10/10 - 11s - loss: 579.8615 - loglik: -5.7963e+02 - logprior: -2.2795e-01
Epoch 4/10
10/10 - 11s - loss: 572.1473 - loglik: -5.7285e+02 - logprior: 0.6995
Epoch 5/10
10/10 - 11s - loss: 567.2762 - loglik: -5.6829e+02 - logprior: 1.0158
Epoch 6/10
10/10 - 11s - loss: 559.3573 - loglik: -5.6056e+02 - logprior: 1.2066
Epoch 7/10
10/10 - 11s - loss: 554.7887 - loglik: -5.5624e+02 - logprior: 1.4538
Epoch 8/10
10/10 - 11s - loss: 552.7099 - loglik: -5.5428e+02 - logprior: 1.5691
Epoch 9/10
10/10 - 11s - loss: 548.4456 - loglik: -5.5003e+02 - logprior: 1.5884
Epoch 10/10
10/10 - 10s - loss: 548.8055 - loglik: -5.5030e+02 - logprior: 1.4897
Fitted a model with MAP estimate = -547.0825
Time for alignment: 274.2677
Fitting a model of length 198 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 704.5303 - loglik: -6.8892e+02 - logprior: -1.5613e+01
Epoch 2/10
10/10 - 9s - loss: 662.9694 - loglik: -6.6037e+02 - logprior: -2.5947e+00
Epoch 3/10
10/10 - 9s - loss: 624.6057 - loglik: -6.2412e+02 - logprior: -4.8936e-01
Epoch 4/10
10/10 - 9s - loss: 593.2232 - loglik: -5.9273e+02 - logprior: -4.9076e-01
Epoch 5/10
10/10 - 9s - loss: 578.8773 - loglik: -5.7828e+02 - logprior: -6.0214e-01
Epoch 6/10
10/10 - 9s - loss: 569.7327 - loglik: -5.6913e+02 - logprior: -6.0443e-01
Epoch 7/10
10/10 - 9s - loss: 562.4899 - loglik: -5.6193e+02 - logprior: -5.6047e-01
Epoch 8/10
10/10 - 9s - loss: 560.6021 - loglik: -5.6012e+02 - logprior: -4.8255e-01
Epoch 9/10
10/10 - 9s - loss: 558.9266 - loglik: -5.5851e+02 - logprior: -4.1738e-01
Epoch 10/10
10/10 - 9s - loss: 558.7938 - loglik: -5.5840e+02 - logprior: -3.9290e-01
Fitted a model with MAP estimate = -557.9894
expansions: [(18, 1), (25, 1), (27, 1), (30, 2), (44, 4), (64, 3), (65, 1), (91, 3), (95, 1), (103, 1), (118, 1), (119, 1), (120, 2), (135, 1), (148, 2), (150, 3), (152, 1), (162, 1), (166, 1), (181, 1), (183, 1), (184, 2), (185, 1)]
discards: [  2 171 172 173 174 175 176 177 178]
Fitting a model of length 225 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 14s - loss: 667.9992 - loglik: -6.5313e+02 - logprior: -1.4868e+01
Epoch 2/2
10/10 - 11s - loss: 612.2631 - loglik: -6.0784e+02 - logprior: -4.4241e+00
Fitted a model with MAP estimate = -597.8714
expansions: [(32, 2), (45, 4), (174, 1), (194, 10), (195, 1)]
discards: [ 34  35  36  37  38  39  40  50  51  52  53  54  55  74  75 120 136 171
 177 178 180 181 182 183 184 185 191 196 197 198 199 200]
Fitting a model of length 211 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 12s - loss: 618.6642 - loglik: -6.0384e+02 - logprior: -1.4824e+01
Epoch 2/2
10/10 - 10s - loss: 591.1678 - loglik: -5.8845e+02 - logprior: -2.7223e+00
Fitted a model with MAP estimate = -584.3205
expansions: [(30, 2), (46, 2), (93, 2), (111, 1), (164, 5), (172, 1)]
discards: [36]
Fitting a model of length 223 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 14s - loss: 598.8568 - loglik: -5.8482e+02 - logprior: -1.4033e+01
Epoch 2/10
10/10 - 11s - loss: 582.3790 - loglik: -5.7992e+02 - logprior: -2.4546e+00
Epoch 3/10
10/10 - 11s - loss: 575.8220 - loglik: -5.7571e+02 - logprior: -1.0915e-01
Epoch 4/10
10/10 - 11s - loss: 570.7238 - loglik: -5.7150e+02 - logprior: 0.7712
Epoch 5/10
10/10 - 12s - loss: 563.4457 - loglik: -5.6455e+02 - logprior: 1.1034
Epoch 6/10
10/10 - 11s - loss: 559.6529 - loglik: -5.6086e+02 - logprior: 1.2048
Epoch 7/10
10/10 - 11s - loss: 554.2325 - loglik: -5.5560e+02 - logprior: 1.3679
Epoch 8/10
10/10 - 11s - loss: 550.7923 - loglik: -5.5229e+02 - logprior: 1.4934
Epoch 9/10
10/10 - 11s - loss: 547.9538 - loglik: -5.4949e+02 - logprior: 1.5357
Epoch 10/10
10/10 - 11s - loss: 546.2302 - loglik: -5.4780e+02 - logprior: 1.5686
Fitted a model with MAP estimate = -545.7265
Time for alignment: 281.1130
Computed alignments with likelihoods: ['-545.4288', '-547.0825', '-545.7265']
Best model has likelihood: -545.4288
SP score = 0.7169
Training of 3 independent models on file hom.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b8c87400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f367f5abd30>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.1239 - loglik: -1.5089e+02 - logprior: -3.2298e+00
Epoch 2/10
19/19 - 1s - loss: 121.1386 - loglik: -1.1965e+02 - logprior: -1.4837e+00
Epoch 3/10
19/19 - 1s - loss: 108.3393 - loglik: -1.0683e+02 - logprior: -1.5120e+00
Epoch 4/10
19/19 - 1s - loss: 104.9252 - loglik: -1.0333e+02 - logprior: -1.5930e+00
Epoch 5/10
19/19 - 1s - loss: 103.7390 - loglik: -1.0225e+02 - logprior: -1.4884e+00
Epoch 6/10
19/19 - 1s - loss: 103.0072 - loglik: -1.0153e+02 - logprior: -1.4758e+00
Epoch 7/10
19/19 - 1s - loss: 102.5949 - loglik: -1.0112e+02 - logprior: -1.4720e+00
Epoch 8/10
19/19 - 1s - loss: 102.4892 - loglik: -1.0101e+02 - logprior: -1.4832e+00
Epoch 9/10
19/19 - 1s - loss: 102.0537 - loglik: -1.0056e+02 - logprior: -1.4930e+00
Epoch 10/10
19/19 - 1s - loss: 101.9706 - loglik: -1.0048e+02 - logprior: -1.4908e+00
Fitted a model with MAP estimate = -97.8115
expansions: [(3, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: [0]
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.3939 - loglik: -1.0325e+02 - logprior: -4.1445e+00
Epoch 2/2
19/19 - 1s - loss: 96.7415 - loglik: -9.4445e+01 - logprior: -2.2968e+00
Fitted a model with MAP estimate = -90.9068
expansions: [(3, 1)]
discards: [ 0 12 36 39]
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 99.9070 - loglik: -9.6004e+01 - logprior: -3.9028e+00
Epoch 2/2
19/19 - 1s - loss: 95.2672 - loglik: -9.3882e+01 - logprior: -1.3853e+00
Fitted a model with MAP estimate = -90.4950
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 94.4848 - loglik: -9.0887e+01 - logprior: -3.5974e+00
Epoch 2/10
21/21 - 1s - loss: 90.1934 - loglik: -8.8451e+01 - logprior: -1.7419e+00
Epoch 3/10
21/21 - 1s - loss: 89.2026 - loglik: -8.7900e+01 - logprior: -1.3027e+00
Epoch 4/10
21/21 - 1s - loss: 88.5287 - loglik: -8.7305e+01 - logprior: -1.2237e+00
Epoch 5/10
21/21 - 1s - loss: 87.5301 - loglik: -8.6323e+01 - logprior: -1.2067e+00
Epoch 6/10
21/21 - 1s - loss: 87.9096 - loglik: -8.6705e+01 - logprior: -1.2049e+00
Fitted a model with MAP estimate = -87.3687
Time for alignment: 43.5737
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.3431 - loglik: -1.5111e+02 - logprior: -3.2309e+00
Epoch 2/10
19/19 - 1s - loss: 120.2994 - loglik: -1.1880e+02 - logprior: -1.5010e+00
Epoch 3/10
19/19 - 1s - loss: 106.9835 - loglik: -1.0543e+02 - logprior: -1.5541e+00
Epoch 4/10
19/19 - 1s - loss: 103.6721 - loglik: -1.0206e+02 - logprior: -1.6082e+00
Epoch 5/10
19/19 - 1s - loss: 103.0364 - loglik: -1.0155e+02 - logprior: -1.4889e+00
Epoch 6/10
19/19 - 1s - loss: 102.4077 - loglik: -1.0091e+02 - logprior: -1.5014e+00
Epoch 7/10
19/19 - 1s - loss: 101.9840 - loglik: -1.0052e+02 - logprior: -1.4688e+00
Epoch 8/10
19/19 - 1s - loss: 101.9140 - loglik: -1.0045e+02 - logprior: -1.4630e+00
Epoch 9/10
19/19 - 1s - loss: 101.9895 - loglik: -1.0054e+02 - logprior: -1.4525e+00
Fitted a model with MAP estimate = -97.8438
expansions: [(3, 1), (5, 1), (7, 2), (16, 1), (17, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: [0]
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 106.7075 - loglik: -1.0257e+02 - logprior: -4.1367e+00
Epoch 2/2
19/19 - 1s - loss: 96.4886 - loglik: -9.4158e+01 - logprior: -2.3309e+00
Fitted a model with MAP estimate = -90.8197
expansions: [(3, 1)]
discards: [ 0 21 36 39]
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 99.6569 - loglik: -9.5787e+01 - logprior: -3.8695e+00
Epoch 2/2
19/19 - 1s - loss: 95.3278 - loglik: -9.3918e+01 - logprior: -1.4101e+00
Fitted a model with MAP estimate = -90.3227
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 94.3131 - loglik: -9.0682e+01 - logprior: -3.6314e+00
Epoch 2/10
21/21 - 1s - loss: 90.5140 - loglik: -8.8697e+01 - logprior: -1.8168e+00
Epoch 3/10
21/21 - 1s - loss: 89.2783 - loglik: -8.7938e+01 - logprior: -1.3402e+00
Epoch 4/10
21/21 - 1s - loss: 88.4245 - loglik: -8.7181e+01 - logprior: -1.2439e+00
Epoch 5/10
21/21 - 1s - loss: 87.8818 - loglik: -8.6672e+01 - logprior: -1.2095e+00
Epoch 6/10
21/21 - 1s - loss: 87.7251 - loglik: -8.6521e+01 - logprior: -1.2041e+00
Epoch 7/10
21/21 - 1s - loss: 87.1735 - loglik: -8.5974e+01 - logprior: -1.1990e+00
Epoch 8/10
21/21 - 1s - loss: 87.3075 - loglik: -8.6113e+01 - logprior: -1.1943e+00
Fitted a model with MAP estimate = -87.0745
Time for alignment: 45.8474
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.1107 - loglik: -1.5088e+02 - logprior: -3.2293e+00
Epoch 2/10
19/19 - 1s - loss: 120.5669 - loglik: -1.1908e+02 - logprior: -1.4840e+00
Epoch 3/10
19/19 - 1s - loss: 106.3750 - loglik: -1.0483e+02 - logprior: -1.5454e+00
Epoch 4/10
19/19 - 1s - loss: 103.2869 - loglik: -1.0168e+02 - logprior: -1.6037e+00
Epoch 5/10
19/19 - 1s - loss: 101.9815 - loglik: -1.0049e+02 - logprior: -1.4899e+00
Epoch 6/10
19/19 - 1s - loss: 101.7024 - loglik: -1.0020e+02 - logprior: -1.5009e+00
Epoch 7/10
19/19 - 1s - loss: 101.2916 - loglik: -9.9813e+01 - logprior: -1.4787e+00
Epoch 8/10
19/19 - 1s - loss: 101.3540 - loglik: -9.9884e+01 - logprior: -1.4703e+00
Fitted a model with MAP estimate = -97.0906
expansions: [(3, 1), (5, 1), (7, 2), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: [0]
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 106.4281 - loglik: -1.0229e+02 - logprior: -4.1341e+00
Epoch 2/2
19/19 - 1s - loss: 96.6995 - loglik: -9.4396e+01 - logprior: -2.3036e+00
Fitted a model with MAP estimate = -90.8729
expansions: [(3, 1)]
discards: [ 0 21 36 39]
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 99.8856 - loglik: -9.6014e+01 - logprior: -3.8716e+00
Epoch 2/2
19/19 - 1s - loss: 95.1687 - loglik: -9.3771e+01 - logprior: -1.3973e+00
Fitted a model with MAP estimate = -90.4613
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 94.3648 - loglik: -9.0747e+01 - logprior: -3.6174e+00
Epoch 2/10
21/21 - 1s - loss: 90.4254 - loglik: -8.8575e+01 - logprior: -1.8500e+00
Epoch 3/10
21/21 - 1s - loss: 89.3083 - loglik: -8.7968e+01 - logprior: -1.3401e+00
Epoch 4/10
21/21 - 1s - loss: 88.3961 - loglik: -8.7147e+01 - logprior: -1.2489e+00
Epoch 5/10
21/21 - 1s - loss: 87.8155 - loglik: -8.6578e+01 - logprior: -1.2380e+00
Epoch 6/10
21/21 - 1s - loss: 87.6260 - loglik: -8.6406e+01 - logprior: -1.2197e+00
Epoch 7/10
21/21 - 1s - loss: 87.4855 - loglik: -8.6272e+01 - logprior: -1.2136e+00
Epoch 8/10
21/21 - 1s - loss: 87.0595 - loglik: -8.5855e+01 - logprior: -1.2045e+00
Epoch 9/10
21/21 - 1s - loss: 87.2043 - loglik: -8.6014e+01 - logprior: -1.1906e+00
Fitted a model with MAP estimate = -86.9992
Time for alignment: 44.3232
Computed alignments with likelihoods: ['-87.3687', '-87.0745', '-86.9992']
Best model has likelihood: -86.9992
SP score = 0.9459
Training of 3 independent models on file OTCace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f35ed07afd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32c8522d90>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 122 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 448.2157 - loglik: -4.4046e+02 - logprior: -7.7594e+00
Epoch 2/10
13/13 - 4s - loss: 388.3063 - loglik: -3.8647e+02 - logprior: -1.8358e+00
Epoch 3/10
13/13 - 3s - loss: 341.0972 - loglik: -3.3937e+02 - logprior: -1.7290e+00
Epoch 4/10
13/13 - 3s - loss: 322.9848 - loglik: -3.2093e+02 - logprior: -2.0516e+00
Epoch 5/10
13/13 - 4s - loss: 315.9549 - loglik: -3.1403e+02 - logprior: -1.9236e+00
Epoch 6/10
13/13 - 4s - loss: 313.1748 - loglik: -3.1126e+02 - logprior: -1.9130e+00
Epoch 7/10
13/13 - 4s - loss: 312.0602 - loglik: -3.1017e+02 - logprior: -1.8946e+00
Epoch 8/10
13/13 - 4s - loss: 311.1971 - loglik: -3.0935e+02 - logprior: -1.8462e+00
Epoch 9/10
13/13 - 4s - loss: 309.8517 - loglik: -3.0800e+02 - logprior: -1.8516e+00
Epoch 10/10
13/13 - 4s - loss: 310.3579 - loglik: -3.0851e+02 - logprior: -1.8477e+00
Fitted a model with MAP estimate = -309.6153
expansions: [(0, 2), (9, 2), (21, 1), (25, 2), (26, 1), (36, 2), (37, 4), (38, 1), (39, 2), (43, 1), (45, 1), (47, 1), (67, 2), (68, 1), (69, 1), (71, 1), (72, 1), (73, 1), (78, 1), (81, 1), (94, 7), (102, 1), (105, 2), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Fitting a model of length 165 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 9s - loss: 324.6429 - loglik: -3.1449e+02 - logprior: -1.0155e+01
Epoch 2/2
13/13 - 6s - loss: 294.2549 - loglik: -2.9161e+02 - logprior: -2.6465e+00
Fitted a model with MAP estimate = -288.9880
expansions: []
discards: [  0  12  30  48  54  88 126 127 128 129 142]
Fitting a model of length 154 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 9s - loss: 304.0576 - loglik: -2.9496e+02 - logprior: -9.0934e+00
Epoch 2/2
13/13 - 6s - loss: 292.6047 - loglik: -2.8880e+02 - logprior: -3.8047e+00
Fitted a model with MAP estimate = -290.5545
expansions: [(0, 2)]
discards: [ 0 44]
Fitting a model of length 154 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 296.3874 - loglik: -2.8937e+02 - logprior: -7.0132e+00
Epoch 2/10
13/13 - 6s - loss: 289.1473 - loglik: -2.8754e+02 - logprior: -1.6051e+00
Epoch 3/10
13/13 - 6s - loss: 286.7654 - loglik: -2.8586e+02 - logprior: -9.0915e-01
Epoch 4/10
13/13 - 6s - loss: 286.2674 - loglik: -2.8565e+02 - logprior: -6.1874e-01
Epoch 5/10
13/13 - 6s - loss: 285.1930 - loglik: -2.8468e+02 - logprior: -5.1456e-01
Epoch 6/10
13/13 - 6s - loss: 285.0854 - loglik: -2.8464e+02 - logprior: -4.4708e-01
Epoch 7/10
13/13 - 6s - loss: 284.0962 - loglik: -2.8370e+02 - logprior: -3.9476e-01
Epoch 8/10
13/13 - 6s - loss: 285.1586 - loglik: -2.8481e+02 - logprior: -3.4638e-01
Fitted a model with MAP estimate = -284.1975
Time for alignment: 141.4221
Fitting a model of length 122 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 448.1541 - loglik: -4.4039e+02 - logprior: -7.7622e+00
Epoch 2/10
13/13 - 4s - loss: 387.1866 - loglik: -3.8533e+02 - logprior: -1.8586e+00
Epoch 3/10
13/13 - 4s - loss: 338.5565 - loglik: -3.3679e+02 - logprior: -1.7652e+00
Epoch 4/10
13/13 - 4s - loss: 321.9430 - loglik: -3.2000e+02 - logprior: -1.9478e+00
Epoch 5/10
13/13 - 5s - loss: 315.8897 - loglik: -3.1407e+02 - logprior: -1.8196e+00
Epoch 6/10
13/13 - 4s - loss: 312.9770 - loglik: -3.1109e+02 - logprior: -1.8843e+00
Epoch 7/10
13/13 - 4s - loss: 311.9586 - loglik: -3.1003e+02 - logprior: -1.9271e+00
Epoch 8/10
13/13 - 4s - loss: 311.1804 - loglik: -3.0929e+02 - logprior: -1.8889e+00
Epoch 9/10
13/13 - 4s - loss: 310.4991 - loglik: -3.0861e+02 - logprior: -1.8900e+00
Epoch 10/10
13/13 - 5s - loss: 309.5554 - loglik: -3.0770e+02 - logprior: -1.8559e+00
Fitted a model with MAP estimate = -309.6818
expansions: [(0, 2), (9, 2), (18, 1), (25, 2), (26, 1), (36, 2), (37, 4), (38, 1), (39, 2), (43, 1), (45, 1), (47, 1), (67, 2), (68, 1), (69, 1), (71, 1), (72, 1), (73, 1), (78, 1), (79, 1), (81, 2), (100, 1), (102, 1), (103, 1), (104, 2), (105, 1), (115, 1), (116, 1)]
discards: []
Fitting a model of length 161 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 9s - loss: 323.6345 - loglik: -3.1344e+02 - logprior: -1.0194e+01
Epoch 2/2
13/13 - 6s - loss: 296.4582 - loglik: -2.9382e+02 - logprior: -2.6385e+00
Fitted a model with MAP estimate = -291.1755
expansions: [(132, 1)]
discards: [  0  12  30  48  54  88 111 138]
Fitting a model of length 154 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 10s - loss: 304.6225 - loglik: -2.9552e+02 - logprior: -9.1042e+00
Epoch 2/2
13/13 - 5s - loss: 293.5419 - loglik: -2.8977e+02 - logprior: -3.7769e+00
Fitted a model with MAP estimate = -291.2427
expansions: [(0, 2)]
discards: [ 0 44]
Fitting a model of length 154 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 9s - loss: 297.4129 - loglik: -2.9040e+02 - logprior: -7.0095e+00
Epoch 2/10
13/13 - 5s - loss: 288.7597 - loglik: -2.8717e+02 - logprior: -1.5940e+00
Epoch 3/10
13/13 - 5s - loss: 287.2676 - loglik: -2.8634e+02 - logprior: -9.2333e-01
Epoch 4/10
13/13 - 5s - loss: 286.0347 - loglik: -2.8541e+02 - logprior: -6.2457e-01
Epoch 5/10
13/13 - 6s - loss: 285.8061 - loglik: -2.8528e+02 - logprior: -5.2507e-01
Epoch 6/10
13/13 - 6s - loss: 284.9477 - loglik: -2.8450e+02 - logprior: -4.4848e-01
Epoch 7/10
13/13 - 6s - loss: 285.2830 - loglik: -2.8487e+02 - logprior: -4.1105e-01
Fitted a model with MAP estimate = -284.5777
Time for alignment: 144.0584
Fitting a model of length 122 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 448.5290 - loglik: -4.4076e+02 - logprior: -7.7652e+00
Epoch 2/10
13/13 - 4s - loss: 387.6798 - loglik: -3.8583e+02 - logprior: -1.8474e+00
Epoch 3/10
13/13 - 4s - loss: 339.7421 - loglik: -3.3797e+02 - logprior: -1.7688e+00
Epoch 4/10
13/13 - 4s - loss: 322.7899 - loglik: -3.2078e+02 - logprior: -2.0096e+00
Epoch 5/10
13/13 - 4s - loss: 317.3428 - loglik: -3.1554e+02 - logprior: -1.8078e+00
Epoch 6/10
13/13 - 4s - loss: 314.9564 - loglik: -3.1319e+02 - logprior: -1.7662e+00
Epoch 7/10
13/13 - 4s - loss: 313.4248 - loglik: -3.1168e+02 - logprior: -1.7456e+00
Epoch 8/10
13/13 - 5s - loss: 312.8974 - loglik: -3.1120e+02 - logprior: -1.7019e+00
Epoch 9/10
13/13 - 4s - loss: 311.7242 - loglik: -3.1003e+02 - logprior: -1.6892e+00
Epoch 10/10
13/13 - 4s - loss: 312.0013 - loglik: -3.1033e+02 - logprior: -1.6711e+00
Fitted a model with MAP estimate = -311.2695
expansions: [(0, 2), (9, 1), (10, 1), (21, 1), (25, 2), (26, 1), (36, 2), (37, 4), (38, 1), (39, 2), (43, 2), (44, 1), (67, 2), (68, 1), (69, 1), (71, 2), (73, 1), (78, 1), (81, 1), (85, 1), (99, 1), (100, 1), (102, 1), (103, 2), (104, 2), (114, 1), (115, 1), (116, 1)]
discards: []
Fitting a model of length 162 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 9s - loss: 324.6694 - loglik: -3.1460e+02 - logprior: -1.0065e+01
Epoch 2/2
13/13 - 6s - loss: 295.7247 - loglik: -2.9321e+02 - logprior: -2.5136e+00
Fitted a model with MAP estimate = -289.9986
expansions: []
discards: [  0  30  48  54 137 139]
Fitting a model of length 156 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 302.3571 - loglik: -2.9333e+02 - logprior: -9.0230e+00
Epoch 2/2
13/13 - 6s - loss: 293.0471 - loglik: -2.8940e+02 - logprior: -3.6480e+00
Fitted a model with MAP estimate = -290.0860
expansions: [(0, 2)]
discards: [ 0 10 45 83 87]
Fitting a model of length 153 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 298.2251 - loglik: -2.9136e+02 - logprior: -6.8604e+00
Epoch 2/10
13/13 - 6s - loss: 290.5713 - loglik: -2.8905e+02 - logprior: -1.5229e+00
Epoch 3/10
13/13 - 5s - loss: 288.3941 - loglik: -2.8754e+02 - logprior: -8.4978e-01
Epoch 4/10
13/13 - 5s - loss: 286.7196 - loglik: -2.8618e+02 - logprior: -5.3982e-01
Epoch 5/10
13/13 - 5s - loss: 286.3718 - loglik: -2.8593e+02 - logprior: -4.3871e-01
Epoch 6/10
13/13 - 5s - loss: 285.6519 - loglik: -2.8529e+02 - logprior: -3.6283e-01
Epoch 7/10
13/13 - 5s - loss: 285.6673 - loglik: -2.8535e+02 - logprior: -3.2141e-01
Fitted a model with MAP estimate = -285.2316
Time for alignment: 140.2226
Computed alignments with likelihoods: ['-284.1975', '-284.5777', '-285.2316']
Best model has likelihood: -284.1975
SP score = 0.5568
Training of 3 independent models on file lyase_1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32e8056af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32c85c77f0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 236 on 7632 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 24s - loss: 817.3500 - loglik: -8.1392e+02 - logprior: -3.4292e+00
Epoch 2/10
17/17 - 23s - loss: 704.2809 - loglik: -7.0347e+02 - logprior: -8.1527e-01
Epoch 3/10
17/17 - 26s - loss: 639.4752 - loglik: -6.3793e+02 - logprior: -1.5416e+00
Epoch 4/10
17/17 - 26s - loss: 619.2116 - loglik: -6.1751e+02 - logprior: -1.6986e+00
Epoch 5/10
17/17 - 28s - loss: 613.0116 - loglik: -6.1124e+02 - logprior: -1.7753e+00
Epoch 6/10
17/17 - 27s - loss: 611.9872 - loglik: -6.1023e+02 - logprior: -1.7533e+00
Epoch 7/10
17/17 - 28s - loss: 610.6245 - loglik: -6.0891e+02 - logprior: -1.7105e+00
Epoch 8/10
17/17 - 29s - loss: 609.7184 - loglik: -6.0802e+02 - logprior: -1.6988e+00
Epoch 9/10
17/17 - 27s - loss: 609.3096 - loglik: -6.0759e+02 - logprior: -1.7170e+00
Epoch 10/10
17/17 - 26s - loss: 607.9185 - loglik: -6.0619e+02 - logprior: -1.7318e+00
Fitted a model with MAP estimate = -607.8058
expansions: [(13, 1), (14, 1), (16, 3), (17, 6), (29, 3), (41, 1), (51, 2), (55, 5), (65, 1), (66, 1), (67, 1), (94, 2), (95, 2), (96, 2), (99, 1), (100, 1), (102, 1), (103, 1), (104, 1), (107, 1), (111, 1), (131, 1), (137, 1), (140, 1), (142, 1), (143, 3), (144, 1), (165, 1), (167, 1), (171, 2), (173, 2), (175, 1), (178, 1), (188, 1), (193, 1), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1), (230, 1)]
discards: [0]
Fitting a model of length 297 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 41s - loss: 603.7623 - loglik: -6.0048e+02 - logprior: -3.2832e+00
Epoch 2/2
34/34 - 39s - loss: 578.8031 - loglik: -5.7738e+02 - logprior: -1.4187e+00
Fitted a model with MAP estimate = -573.1616
expansions: [(77, 2), (187, 1)]
discards: [ 21  22  23  41  65  71  72  73  74  75 119 121 124]
Fitting a model of length 287 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 36s - loss: 588.2088 - loglik: -5.8627e+02 - logprior: -1.9377e+00
Epoch 2/2
34/34 - 32s - loss: 580.4080 - loglik: -5.8026e+02 - logprior: -1.4336e-01
Fitted a model with MAP estimate = -577.7352
expansions: [(58, 1), (247, 2)]
discards: [67]
Fitting a model of length 289 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 36s - loss: 585.3826 - loglik: -5.8376e+02 - logprior: -1.6215e+00
Epoch 2/10
34/34 - 32s - loss: 579.5452 - loglik: -5.7964e+02 - logprior: 0.0956
Epoch 3/10
34/34 - 34s - loss: 575.4672 - loglik: -5.7573e+02 - logprior: 0.2600
Epoch 4/10
34/34 - 36s - loss: 573.6184 - loglik: -5.7405e+02 - logprior: 0.4349
Epoch 5/10
34/34 - 37s - loss: 573.1345 - loglik: -5.7370e+02 - logprior: 0.5609
Epoch 6/10
34/34 - 39s - loss: 572.6326 - loglik: -5.7338e+02 - logprior: 0.7444
Epoch 7/10
34/34 - 36s - loss: 572.5024 - loglik: -5.7340e+02 - logprior: 0.8941
Epoch 8/10
34/34 - 37s - loss: 571.6255 - loglik: -5.7271e+02 - logprior: 1.0796
Epoch 9/10
34/34 - 37s - loss: 571.7842 - loglik: -5.7291e+02 - logprior: 1.1246
Fitted a model with MAP estimate = -571.2290
Time for alignment: 881.2989
Fitting a model of length 236 on 7632 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 29s - loss: 818.1575 - loglik: -8.1473e+02 - logprior: -3.4278e+00
Epoch 2/10
17/17 - 25s - loss: 708.2507 - loglik: -7.0742e+02 - logprior: -8.3548e-01
Epoch 3/10
17/17 - 28s - loss: 643.2754 - loglik: -6.4171e+02 - logprior: -1.5697e+00
Epoch 4/10
17/17 - 26s - loss: 623.0709 - loglik: -6.2131e+02 - logprior: -1.7607e+00
Epoch 5/10
17/17 - 25s - loss: 616.5191 - loglik: -6.1469e+02 - logprior: -1.8267e+00
Epoch 6/10
17/17 - 24s - loss: 614.0943 - loglik: -6.1231e+02 - logprior: -1.7890e+00
Epoch 7/10
17/17 - 26s - loss: 612.3279 - loglik: -6.1060e+02 - logprior: -1.7269e+00
Epoch 8/10
17/17 - 24s - loss: 612.4649 - loglik: -6.1076e+02 - logprior: -1.7082e+00
Fitted a model with MAP estimate = -611.2912
expansions: [(9, 1), (12, 1), (14, 1), (15, 3), (16, 1), (17, 5), (29, 3), (51, 1), (52, 1), (55, 6), (65, 1), (66, 2), (92, 2), (95, 1), (96, 2), (99, 1), (100, 1), (102, 1), (103, 1), (104, 1), (110, 1), (111, 1), (131, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (165, 1), (167, 1), (171, 2), (173, 1), (175, 1), (178, 1), (188, 1), (193, 1), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1), (230, 1)]
discards: [0]
Fitting a model of length 296 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 37s - loss: 604.6299 - loglik: -6.0133e+02 - logprior: -3.2955e+00
Epoch 2/2
34/34 - 33s - loss: 579.4805 - loglik: -5.7816e+02 - logprior: -1.3204e+00
Fitted a model with MAP estimate = -574.9090
expansions: [(78, 2), (187, 1), (256, 2)]
discards: [ 19  20  23  24  42  74  75  76  89 118]
Fitting a model of length 291 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 39s - loss: 590.1476 - loglik: -5.8823e+02 - logprior: -1.9224e+00
Epoch 2/2
34/34 - 38s - loss: 578.8290 - loglik: -5.7860e+02 - logprior: -2.3267e-01
Fitted a model with MAP estimate = -577.5522
expansions: [(69, 3)]
discards: [ 70  71 181]
Fitting a model of length 291 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 37s - loss: 586.7339 - loglik: -5.8509e+02 - logprior: -1.6485e+00
Epoch 2/10
34/34 - 34s - loss: 576.7095 - loglik: -5.7666e+02 - logprior: -4.7621e-02
Epoch 3/10
34/34 - 33s - loss: 574.9812 - loglik: -5.7512e+02 - logprior: 0.1369
Epoch 4/10
34/34 - 33s - loss: 572.2330 - loglik: -5.7252e+02 - logprior: 0.2843
Epoch 5/10
34/34 - 33s - loss: 570.4325 - loglik: -5.7077e+02 - logprior: 0.3394
Epoch 6/10
34/34 - 34s - loss: 571.8755 - loglik: -5.7241e+02 - logprior: 0.5392
Fitted a model with MAP estimate = -570.6536
Time for alignment: 710.0462
Fitting a model of length 236 on 7632 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 28s - loss: 817.6141 - loglik: -8.1419e+02 - logprior: -3.4291e+00
Epoch 2/10
17/17 - 26s - loss: 711.8706 - loglik: -7.1104e+02 - logprior: -8.3335e-01
Epoch 3/10
17/17 - 28s - loss: 643.6557 - loglik: -6.4201e+02 - logprior: -1.6460e+00
Epoch 4/10
17/17 - 25s - loss: 622.4955 - loglik: -6.2063e+02 - logprior: -1.8661e+00
Epoch 5/10
17/17 - 25s - loss: 618.7397 - loglik: -6.1686e+02 - logprior: -1.8837e+00
Epoch 6/10
17/17 - 25s - loss: 615.0252 - loglik: -6.1318e+02 - logprior: -1.8458e+00
Epoch 7/10
17/17 - 25s - loss: 614.6279 - loglik: -6.1283e+02 - logprior: -1.7960e+00
Epoch 8/10
17/17 - 24s - loss: 613.6487 - loglik: -6.1187e+02 - logprior: -1.7746e+00
Epoch 9/10
17/17 - 24s - loss: 614.1609 - loglik: -6.1240e+02 - logprior: -1.7612e+00
Fitted a model with MAP estimate = -613.6055
expansions: [(13, 2), (14, 1), (15, 3), (16, 1), (17, 5), (29, 3), (38, 1), (50, 1), (51, 1), (54, 1), (55, 4), (56, 1), (65, 2), (66, 1), (67, 1), (91, 1), (93, 1), (94, 1), (99, 1), (100, 2), (101, 1), (102, 1), (104, 1), (109, 1), (110, 1), (134, 1), (136, 1), (138, 1), (141, 1), (143, 2), (144, 1), (147, 1), (164, 2), (165, 2), (167, 1), (171, 1), (172, 1), (178, 1), (186, 2), (188, 2), (193, 1), (203, 1), (208, 1), (211, 1), (225, 1), (229, 1), (230, 1)]
discards: [0]
Fitting a model of length 301 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 41s - loss: 603.2063 - loglik: -5.9990e+02 - logprior: -3.3062e+00
Epoch 2/2
34/34 - 37s - loss: 575.6389 - loglik: -5.7426e+02 - logprior: -1.3826e+00
Fitted a model with MAP estimate = -571.5313
expansions: [(73, 1), (187, 1), (238, 3)]
discards: [ 18  42  74  75  76 188 210 211]
Fitting a model of length 298 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 38s - loss: 584.6503 - loglik: -5.8274e+02 - logprior: -1.9140e+00
Epoch 2/2
34/34 - 35s - loss: 574.6511 - loglik: -5.7440e+02 - logprior: -2.4824e-01
Fitted a model with MAP estimate = -572.9251
expansions: [(206, 2), (232, 1)]
discards: []
Fitting a model of length 301 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 40s - loss: 579.7516 - loglik: -5.7803e+02 - logprior: -1.7252e+00
Epoch 2/10
34/34 - 35s - loss: 573.7248 - loglik: -5.7376e+02 - logprior: 0.0349
Epoch 3/10
34/34 - 32s - loss: 568.8934 - loglik: -5.6914e+02 - logprior: 0.2510
Epoch 4/10
34/34 - 30s - loss: 569.1993 - loglik: -5.6958e+02 - logprior: 0.3765
Fitted a model with MAP estimate = -567.1352
Time for alignment: 675.3903
Computed alignments with likelihoods: ['-571.2290', '-570.6536', '-567.1352']
Best model has likelihood: -567.1352
SP score = 0.6763
Training of 3 independent models on file toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34f3e982b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34eae4f100>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 271.1073 - loglik: -1.8287e+02 - logprior: -8.8232e+01
Epoch 2/10
10/10 - 1s - loss: 183.8881 - loglik: -1.6018e+02 - logprior: -2.3709e+01
Epoch 3/10
10/10 - 1s - loss: 152.9164 - loglik: -1.4176e+02 - logprior: -1.1161e+01
Epoch 4/10
10/10 - 1s - loss: 138.0595 - loglik: -1.3147e+02 - logprior: -6.5876e+00
Epoch 5/10
10/10 - 1s - loss: 130.7560 - loglik: -1.2652e+02 - logprior: -4.2338e+00
Epoch 6/10
10/10 - 1s - loss: 126.8943 - loglik: -1.2399e+02 - logprior: -2.9043e+00
Epoch 7/10
10/10 - 1s - loss: 125.1820 - loglik: -1.2318e+02 - logprior: -2.0034e+00
Epoch 8/10
10/10 - 1s - loss: 124.2811 - loglik: -1.2281e+02 - logprior: -1.4672e+00
Epoch 9/10
10/10 - 1s - loss: 123.7401 - loglik: -1.2259e+02 - logprior: -1.1473e+00
Epoch 10/10
10/10 - 1s - loss: 123.3897 - loglik: -1.2248e+02 - logprior: -9.1112e-01
Fitted a model with MAP estimate = -123.2420
expansions: [(7, 2), (8, 4), (16, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 221.6335 - loglik: -1.2300e+02 - logprior: -9.8637e+01
Epoch 2/2
10/10 - 1s - loss: 156.3401 - loglik: -1.1613e+02 - logprior: -4.0212e+01
Fitted a model with MAP estimate = -144.6751
expansions: [(0, 1), (48, 1)]
discards: [ 0 30 31]
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 191.2972 - loglik: -1.1260e+02 - logprior: -7.8701e+01
Epoch 2/2
10/10 - 1s - loss: 130.5166 - loglik: -1.0983e+02 - logprior: -2.0691e+01
Fitted a model with MAP estimate = -121.2729
expansions: [(11, 1)]
discards: []
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 186.7532 - loglik: -1.0957e+02 - logprior: -7.7179e+01
Epoch 2/10
10/10 - 1s - loss: 128.4862 - loglik: -1.0859e+02 - logprior: -1.9896e+01
Epoch 3/10
10/10 - 1s - loss: 115.8321 - loglik: -1.0777e+02 - logprior: -8.0602e+00
Epoch 4/10
10/10 - 1s - loss: 110.7712 - loglik: -1.0775e+02 - logprior: -3.0252e+00
Epoch 5/10
10/10 - 1s - loss: 108.2035 - loglik: -1.0796e+02 - logprior: -2.4567e-01
Epoch 6/10
10/10 - 1s - loss: 106.7971 - loglik: -1.0816e+02 - logprior: 1.3651
Epoch 7/10
10/10 - 1s - loss: 105.9436 - loglik: -1.0829e+02 - logprior: 2.3477
Epoch 8/10
10/10 - 1s - loss: 105.3726 - loglik: -1.0836e+02 - logprior: 2.9883
Epoch 9/10
10/10 - 1s - loss: 104.9442 - loglik: -1.0840e+02 - logprior: 3.4602
Epoch 10/10
10/10 - 1s - loss: 104.6006 - loglik: -1.0844e+02 - logprior: 3.8397
Fitted a model with MAP estimate = -104.4324
Time for alignment: 29.5269
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 271.1073 - loglik: -1.8287e+02 - logprior: -8.8232e+01
Epoch 2/10
10/10 - 1s - loss: 183.8881 - loglik: -1.6018e+02 - logprior: -2.3709e+01
Epoch 3/10
10/10 - 1s - loss: 152.9164 - loglik: -1.4176e+02 - logprior: -1.1161e+01
Epoch 4/10
10/10 - 1s - loss: 138.0595 - loglik: -1.3147e+02 - logprior: -6.5876e+00
Epoch 5/10
10/10 - 1s - loss: 130.7559 - loglik: -1.2652e+02 - logprior: -4.2338e+00
Epoch 6/10
10/10 - 1s - loss: 126.8943 - loglik: -1.2399e+02 - logprior: -2.9043e+00
Epoch 7/10
10/10 - 1s - loss: 125.1819 - loglik: -1.2318e+02 - logprior: -2.0034e+00
Epoch 8/10
10/10 - 1s - loss: 124.2812 - loglik: -1.2281e+02 - logprior: -1.4672e+00
Epoch 9/10
10/10 - 1s - loss: 123.7401 - loglik: -1.2259e+02 - logprior: -1.1473e+00
Epoch 10/10
10/10 - 1s - loss: 123.3897 - loglik: -1.2248e+02 - logprior: -9.1112e-01
Fitted a model with MAP estimate = -123.2419
expansions: [(7, 2), (8, 4), (16, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 221.6335 - loglik: -1.2300e+02 - logprior: -9.8637e+01
Epoch 2/2
10/10 - 1s - loss: 156.3401 - loglik: -1.1613e+02 - logprior: -4.0212e+01
Fitted a model with MAP estimate = -144.6751
expansions: [(0, 1), (48, 1)]
discards: [ 0 30 31]
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 191.2972 - loglik: -1.1260e+02 - logprior: -7.8701e+01
Epoch 2/2
10/10 - 1s - loss: 130.5166 - loglik: -1.0983e+02 - logprior: -2.0691e+01
Fitted a model with MAP estimate = -121.2729
expansions: [(11, 1)]
discards: []
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 186.7532 - loglik: -1.0957e+02 - logprior: -7.7179e+01
Epoch 2/10
10/10 - 1s - loss: 128.4862 - loglik: -1.0859e+02 - logprior: -1.9896e+01
Epoch 3/10
10/10 - 1s - loss: 115.8321 - loglik: -1.0777e+02 - logprior: -8.0602e+00
Epoch 4/10
10/10 - 1s - loss: 110.7712 - loglik: -1.0775e+02 - logprior: -3.0252e+00
Epoch 5/10
10/10 - 1s - loss: 108.2035 - loglik: -1.0796e+02 - logprior: -2.4568e-01
Epoch 6/10
10/10 - 1s - loss: 106.7971 - loglik: -1.0816e+02 - logprior: 1.3651
Epoch 7/10
10/10 - 1s - loss: 105.9436 - loglik: -1.0829e+02 - logprior: 2.3477
Epoch 8/10
10/10 - 1s - loss: 105.3726 - loglik: -1.0836e+02 - logprior: 2.9883
Epoch 9/10
10/10 - 1s - loss: 104.9441 - loglik: -1.0840e+02 - logprior: 3.4601
Epoch 10/10
10/10 - 1s - loss: 104.6007 - loglik: -1.0844e+02 - logprior: 3.8397
Fitted a model with MAP estimate = -104.4324
Time for alignment: 30.7968
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 271.1073 - loglik: -1.8287e+02 - logprior: -8.8232e+01
Epoch 2/10
10/10 - 1s - loss: 183.8881 - loglik: -1.6018e+02 - logprior: -2.3709e+01
Epoch 3/10
10/10 - 1s - loss: 152.9164 - loglik: -1.4176e+02 - logprior: -1.1161e+01
Epoch 4/10
10/10 - 1s - loss: 138.0595 - loglik: -1.3147e+02 - logprior: -6.5876e+00
Epoch 5/10
10/10 - 1s - loss: 130.7559 - loglik: -1.2652e+02 - logprior: -4.2338e+00
Epoch 6/10
10/10 - 1s - loss: 126.8943 - loglik: -1.2399e+02 - logprior: -2.9043e+00
Epoch 7/10
10/10 - 1s - loss: 125.1820 - loglik: -1.2318e+02 - logprior: -2.0034e+00
Epoch 8/10
10/10 - 1s - loss: 124.2811 - loglik: -1.2281e+02 - logprior: -1.4672e+00
Epoch 9/10
10/10 - 1s - loss: 123.7401 - loglik: -1.2259e+02 - logprior: -1.1473e+00
Epoch 10/10
10/10 - 1s - loss: 123.3897 - loglik: -1.2248e+02 - logprior: -9.1112e-01
Fitted a model with MAP estimate = -123.2418
expansions: [(7, 2), (8, 4), (16, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 221.6335 - loglik: -1.2300e+02 - logprior: -9.8637e+01
Epoch 2/2
10/10 - 1s - loss: 156.3401 - loglik: -1.1613e+02 - logprior: -4.0212e+01
Fitted a model with MAP estimate = -144.6751
expansions: [(0, 1), (48, 1)]
discards: [ 0 30 31]
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 191.2972 - loglik: -1.1260e+02 - logprior: -7.8701e+01
Epoch 2/2
10/10 - 1s - loss: 130.5166 - loglik: -1.0983e+02 - logprior: -2.0691e+01
Fitted a model with MAP estimate = -121.2729
expansions: [(11, 1)]
discards: []
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 186.7532 - loglik: -1.0957e+02 - logprior: -7.7179e+01
Epoch 2/10
10/10 - 1s - loss: 128.4863 - loglik: -1.0859e+02 - logprior: -1.9896e+01
Epoch 3/10
10/10 - 1s - loss: 115.8321 - loglik: -1.0777e+02 - logprior: -8.0602e+00
Epoch 4/10
10/10 - 1s - loss: 110.7712 - loglik: -1.0775e+02 - logprior: -3.0252e+00
Epoch 5/10
10/10 - 1s - loss: 108.2035 - loglik: -1.0796e+02 - logprior: -2.4569e-01
Epoch 6/10
10/10 - 1s - loss: 106.7971 - loglik: -1.0816e+02 - logprior: 1.3651
Epoch 7/10
10/10 - 1s - loss: 105.9436 - loglik: -1.0829e+02 - logprior: 2.3477
Epoch 8/10
10/10 - 1s - loss: 105.3726 - loglik: -1.0836e+02 - logprior: 2.9883
Epoch 9/10
10/10 - 1s - loss: 104.9442 - loglik: -1.0840e+02 - logprior: 3.4601
Epoch 10/10
10/10 - 1s - loss: 104.6006 - loglik: -1.0844e+02 - logprior: 3.8397
Fitted a model with MAP estimate = -104.4325
Time for alignment: 27.9312
Computed alignments with likelihoods: ['-104.4324', '-104.4324', '-104.4325']
Best model has likelihood: -104.4324
SP score = 0.9195
Training of 3 independent models on file msb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34a729b790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34a72dde50>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 316.1895 - loglik: -3.0850e+02 - logprior: -7.6902e+00
Epoch 2/10
13/13 - 1s - loss: 283.5247 - loglik: -2.8166e+02 - logprior: -1.8621e+00
Epoch 3/10
13/13 - 1s - loss: 260.0863 - loglik: -2.5846e+02 - logprior: -1.6260e+00
Epoch 4/10
13/13 - 2s - loss: 249.9931 - loglik: -2.4819e+02 - logprior: -1.8002e+00
Epoch 5/10
13/13 - 1s - loss: 247.9628 - loglik: -2.4624e+02 - logprior: -1.7247e+00
Epoch 6/10
13/13 - 1s - loss: 245.7250 - loglik: -2.4407e+02 - logprior: -1.6508e+00
Epoch 7/10
13/13 - 1s - loss: 245.2265 - loglik: -2.4357e+02 - logprior: -1.6590e+00
Epoch 8/10
13/13 - 1s - loss: 244.9525 - loglik: -2.4331e+02 - logprior: -1.6467e+00
Epoch 9/10
13/13 - 2s - loss: 244.4369 - loglik: -2.4282e+02 - logprior: -1.6159e+00
Epoch 10/10
13/13 - 1s - loss: 244.2560 - loglik: -2.4266e+02 - logprior: -1.5982e+00
Fitted a model with MAP estimate = -243.8945
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 1), (30, 3), (31, 2), (32, 3), (39, 1), (40, 1), (44, 1), (51, 2), (62, 2), (68, 1), (69, 4), (70, 2)]
discards: [0]
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 263.1987 - loglik: -2.5421e+02 - logprior: -8.9890e+00
Epoch 2/2
13/13 - 2s - loss: 248.3010 - loglik: -2.4423e+02 - logprior: -4.0759e+00
Fitted a model with MAP estimate = -246.0402
expansions: [(0, 2), (34, 1)]
discards: [ 0 14 35 67 95]
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 250.5867 - loglik: -2.4375e+02 - logprior: -6.8412e+00
Epoch 2/2
13/13 - 2s - loss: 242.7408 - loglik: -2.4093e+02 - logprior: -1.8130e+00
Fitted a model with MAP estimate = -242.1788
expansions: []
discards: [0]
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 252.1033 - loglik: -2.4357e+02 - logprior: -8.5339e+00
Epoch 2/10
13/13 - 2s - loss: 245.1634 - loglik: -2.4253e+02 - logprior: -2.6288e+00
Epoch 3/10
13/13 - 2s - loss: 242.4159 - loglik: -2.4117e+02 - logprior: -1.2432e+00
Epoch 4/10
13/13 - 2s - loss: 240.9685 - loglik: -2.3999e+02 - logprior: -9.7622e-01
Epoch 5/10
13/13 - 2s - loss: 241.1687 - loglik: -2.4033e+02 - logprior: -8.3599e-01
Fitted a model with MAP estimate = -240.0609
Time for alignment: 53.2696
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 316.4581 - loglik: -3.0876e+02 - logprior: -7.6947e+00
Epoch 2/10
13/13 - 1s - loss: 284.7612 - loglik: -2.8288e+02 - logprior: -1.8764e+00
Epoch 3/10
13/13 - 1s - loss: 260.8382 - loglik: -2.5918e+02 - logprior: -1.6534e+00
Epoch 4/10
13/13 - 1s - loss: 251.2981 - loglik: -2.4942e+02 - logprior: -1.8809e+00
Epoch 5/10
13/13 - 1s - loss: 247.4523 - loglik: -2.4565e+02 - logprior: -1.8071e+00
Epoch 6/10
13/13 - 1s - loss: 246.5319 - loglik: -2.4483e+02 - logprior: -1.7055e+00
Epoch 7/10
13/13 - 1s - loss: 245.6920 - loglik: -2.4399e+02 - logprior: -1.7037e+00
Epoch 8/10
13/13 - 1s - loss: 245.4292 - loglik: -2.4373e+02 - logprior: -1.6983e+00
Epoch 9/10
13/13 - 1s - loss: 244.4613 - loglik: -2.4280e+02 - logprior: -1.6578e+00
Epoch 10/10
13/13 - 1s - loss: 244.5524 - loglik: -2.4291e+02 - logprior: -1.6422e+00
Fitted a model with MAP estimate = -244.1112
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (21, 1), (27, 1), (28, 1), (29, 2), (30, 3), (31, 2), (39, 1), (40, 1), (44, 1), (51, 2), (62, 2), (68, 1), (69, 4), (70, 2)]
discards: [0]
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 263.4636 - loglik: -2.5449e+02 - logprior: -8.9732e+00
Epoch 2/2
13/13 - 2s - loss: 248.0209 - loglik: -2.4397e+02 - logprior: -4.0553e+00
Fitted a model with MAP estimate = -246.0808
expansions: [(0, 2)]
discards: [ 0 14 68 96]
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 250.6544 - loglik: -2.4383e+02 - logprior: -6.8249e+00
Epoch 2/2
13/13 - 2s - loss: 242.8084 - loglik: -2.4100e+02 - logprior: -1.8067e+00
Fitted a model with MAP estimate = -242.1067
expansions: []
discards: [ 0 35]
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 251.9183 - loglik: -2.4340e+02 - logprior: -8.5199e+00
Epoch 2/10
13/13 - 2s - loss: 244.7654 - loglik: -2.4216e+02 - logprior: -2.6003e+00
Epoch 3/10
13/13 - 2s - loss: 242.9880 - loglik: -2.4175e+02 - logprior: -1.2357e+00
Epoch 4/10
13/13 - 2s - loss: 241.1394 - loglik: -2.4019e+02 - logprior: -9.5289e-01
Epoch 5/10
13/13 - 2s - loss: 240.7332 - loglik: -2.3991e+02 - logprior: -8.2066e-01
Epoch 6/10
13/13 - 2s - loss: 239.4792 - loglik: -2.3870e+02 - logprior: -7.7607e-01
Epoch 7/10
13/13 - 2s - loss: 239.0468 - loglik: -2.3829e+02 - logprior: -7.5873e-01
Epoch 8/10
13/13 - 2s - loss: 238.0112 - loglik: -2.3727e+02 - logprior: -7.4010e-01
Epoch 9/10
13/13 - 2s - loss: 236.8797 - loglik: -2.3617e+02 - logprior: -7.0940e-01
Epoch 10/10
13/13 - 2s - loss: 235.3080 - loglik: -2.3462e+02 - logprior: -6.8900e-01
Fitted a model with MAP estimate = -234.9869
Time for alignment: 60.6531
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 316.5144 - loglik: -3.0882e+02 - logprior: -7.6951e+00
Epoch 2/10
13/13 - 1s - loss: 284.3371 - loglik: -2.8247e+02 - logprior: -1.8627e+00
Epoch 3/10
13/13 - 1s - loss: 260.4080 - loglik: -2.5880e+02 - logprior: -1.6081e+00
Epoch 4/10
13/13 - 1s - loss: 250.7262 - loglik: -2.4895e+02 - logprior: -1.7748e+00
Epoch 5/10
13/13 - 1s - loss: 247.9976 - loglik: -2.4634e+02 - logprior: -1.6616e+00
Epoch 6/10
13/13 - 1s - loss: 246.6144 - loglik: -2.4504e+02 - logprior: -1.5757e+00
Epoch 7/10
13/13 - 1s - loss: 245.7368 - loglik: -2.4415e+02 - logprior: -1.5912e+00
Epoch 8/10
13/13 - 1s - loss: 245.2434 - loglik: -2.4366e+02 - logprior: -1.5794e+00
Epoch 9/10
13/13 - 1s - loss: 245.1606 - loglik: -2.4361e+02 - logprior: -1.5535e+00
Epoch 10/10
13/13 - 1s - loss: 244.0241 - loglik: -2.4249e+02 - logprior: -1.5376e+00
Fitted a model with MAP estimate = -244.2163
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 1), (30, 3), (31, 2), (40, 3), (42, 1), (51, 2), (62, 2), (68, 1), (69, 4), (70, 2)]
discards: [0]
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 263.7898 - loglik: -2.5479e+02 - logprior: -8.9984e+00
Epoch 2/2
13/13 - 2s - loss: 248.1639 - loglik: -2.4407e+02 - logprior: -4.0900e+00
Fitted a model with MAP estimate = -246.1945
expansions: [(0, 2), (34, 1)]
discards: [ 0 14 65 93]
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 250.3463 - loglik: -2.4351e+02 - logprior: -6.8410e+00
Epoch 2/2
13/13 - 2s - loss: 243.1811 - loglik: -2.4137e+02 - logprior: -1.8147e+00
Fitted a model with MAP estimate = -242.1531
expansions: []
discards: [0]
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 252.1112 - loglik: -2.4356e+02 - logprior: -8.5504e+00
Epoch 2/10
13/13 - 2s - loss: 244.9864 - loglik: -2.4236e+02 - logprior: -2.6286e+00
Epoch 3/10
13/13 - 2s - loss: 242.5512 - loglik: -2.4130e+02 - logprior: -1.2478e+00
Epoch 4/10
13/13 - 2s - loss: 241.2253 - loglik: -2.4026e+02 - logprior: -9.6991e-01
Epoch 5/10
13/13 - 2s - loss: 241.0772 - loglik: -2.4024e+02 - logprior: -8.3231e-01
Epoch 6/10
13/13 - 2s - loss: 239.7236 - loglik: -2.3893e+02 - logprior: -7.9851e-01
Epoch 7/10
13/13 - 2s - loss: 239.0331 - loglik: -2.3826e+02 - logprior: -7.6931e-01
Epoch 8/10
13/13 - 2s - loss: 237.9653 - loglik: -2.3722e+02 - logprior: -7.4985e-01
Epoch 9/10
13/13 - 2s - loss: 237.3644 - loglik: -2.3664e+02 - logprior: -7.2122e-01
Epoch 10/10
13/13 - 2s - loss: 235.2182 - loglik: -2.3451e+02 - logprior: -7.0959e-01
Fitted a model with MAP estimate = -235.0486
Time for alignment: 58.7253
Computed alignments with likelihoods: ['-240.0609', '-234.9869', '-235.0486']
Best model has likelihood: -234.9869
SP score = 0.9272
Training of 3 independent models on file rnasemam.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f35ed261130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34d215dac0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 432.2513 - loglik: -3.4669e+02 - logprior: -8.5565e+01
Epoch 2/10
10/10 - 1s - loss: 327.9169 - loglik: -3.0811e+02 - logprior: -1.9804e+01
Epoch 3/10
10/10 - 1s - loss: 276.9611 - loglik: -2.6901e+02 - logprior: -7.9472e+00
Epoch 4/10
10/10 - 1s - loss: 245.8492 - loglik: -2.4144e+02 - logprior: -4.4098e+00
Epoch 5/10
10/10 - 1s - loss: 230.7927 - loglik: -2.2846e+02 - logprior: -2.3301e+00
Epoch 6/10
10/10 - 1s - loss: 224.5470 - loglik: -2.2349e+02 - logprior: -1.0527e+00
Epoch 7/10
10/10 - 1s - loss: 221.3983 - loglik: -2.2126e+02 - logprior: -1.3364e-01
Epoch 8/10
10/10 - 1s - loss: 219.5814 - loglik: -2.1995e+02 - logprior: 0.3729
Epoch 9/10
10/10 - 1s - loss: 218.6546 - loglik: -2.1945e+02 - logprior: 0.7940
Epoch 10/10
10/10 - 1s - loss: 218.0816 - loglik: -2.1927e+02 - logprior: 1.1919
Fitted a model with MAP estimate = -217.8345
expansions: [(12, 1), (13, 2), (14, 2), (24, 1), (29, 1), (30, 1), (40, 2), (41, 1), (55, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 313.6570 - loglik: -2.1812e+02 - logprior: -9.5537e+01
Epoch 2/2
10/10 - 1s - loss: 236.7612 - loglik: -2.0064e+02 - logprior: -3.6117e+01
Fitted a model with MAP estimate = -222.9305
expansions: [(0, 3)]
discards: [  0  47 112 113]
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 272.6508 - loglik: -1.9772e+02 - logprior: -7.4931e+01
Epoch 2/2
10/10 - 1s - loss: 208.7391 - loglik: -1.9292e+02 - logprior: -1.5823e+01
Fitted a model with MAP estimate = -198.7072
expansions: []
discards: [0 1]
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 285.8592 - loglik: -1.9619e+02 - logprior: -8.9666e+01
Epoch 2/10
10/10 - 1s - loss: 216.7504 - loglik: -1.9429e+02 - logprior: -2.2462e+01
Epoch 3/10
10/10 - 1s - loss: 196.8343 - loglik: -1.9271e+02 - logprior: -4.1278e+00
Epoch 4/10
10/10 - 1s - loss: 189.2690 - loglik: -1.9163e+02 - logprior: 2.3606
Epoch 5/10
10/10 - 1s - loss: 185.7158 - loglik: -1.9125e+02 - logprior: 5.5324
Epoch 6/10
10/10 - 1s - loss: 183.7902 - loglik: -1.9117e+02 - logprior: 7.3804
Epoch 7/10
10/10 - 1s - loss: 182.6046 - loglik: -1.9117e+02 - logprior: 8.5641
Epoch 8/10
10/10 - 1s - loss: 181.7640 - loglik: -1.9119e+02 - logprior: 9.4289
Epoch 9/10
10/10 - 1s - loss: 181.0893 - loglik: -1.9124e+02 - logprior: 10.1513
Epoch 10/10
10/10 - 1s - loss: 180.5065 - loglik: -1.9131e+02 - logprior: 10.8065
Fitted a model with MAP estimate = -180.2065
Time for alignment: 45.6718
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 432.2513 - loglik: -3.4669e+02 - logprior: -8.5565e+01
Epoch 2/10
10/10 - 1s - loss: 327.9169 - loglik: -3.0811e+02 - logprior: -1.9804e+01
Epoch 3/10
10/10 - 1s - loss: 276.9611 - loglik: -2.6901e+02 - logprior: -7.9472e+00
Epoch 4/10
10/10 - 1s - loss: 245.8493 - loglik: -2.4144e+02 - logprior: -4.4098e+00
Epoch 5/10
10/10 - 1s - loss: 230.7928 - loglik: -2.2846e+02 - logprior: -2.3301e+00
Epoch 6/10
10/10 - 1s - loss: 224.5469 - loglik: -2.2349e+02 - logprior: -1.0527e+00
Epoch 7/10
10/10 - 1s - loss: 221.3984 - loglik: -2.2126e+02 - logprior: -1.3364e-01
Epoch 8/10
10/10 - 1s - loss: 219.5815 - loglik: -2.1995e+02 - logprior: 0.3729
Epoch 9/10
10/10 - 1s - loss: 218.6547 - loglik: -2.1945e+02 - logprior: 0.7940
Epoch 10/10
10/10 - 1s - loss: 218.0816 - loglik: -2.1927e+02 - logprior: 1.1920
Fitted a model with MAP estimate = -217.8347
expansions: [(12, 1), (13, 2), (14, 2), (24, 1), (29, 1), (30, 1), (40, 2), (41, 1), (55, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 313.6570 - loglik: -2.1812e+02 - logprior: -9.5537e+01
Epoch 2/2
10/10 - 1s - loss: 236.7612 - loglik: -2.0064e+02 - logprior: -3.6117e+01
Fitted a model with MAP estimate = -222.9305
expansions: [(0, 3)]
discards: [  0  47 112 113]
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 272.6508 - loglik: -1.9772e+02 - logprior: -7.4931e+01
Epoch 2/2
10/10 - 1s - loss: 208.7390 - loglik: -1.9292e+02 - logprior: -1.5823e+01
Fitted a model with MAP estimate = -198.7072
expansions: []
discards: [0 1]
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 285.8592 - loglik: -1.9619e+02 - logprior: -8.9666e+01
Epoch 2/10
10/10 - 1s - loss: 216.7503 - loglik: -1.9429e+02 - logprior: -2.2462e+01
Epoch 3/10
10/10 - 1s - loss: 196.8343 - loglik: -1.9271e+02 - logprior: -4.1278e+00
Epoch 4/10
10/10 - 1s - loss: 189.2690 - loglik: -1.9163e+02 - logprior: 2.3606
Epoch 5/10
10/10 - 1s - loss: 185.7159 - loglik: -1.9125e+02 - logprior: 5.5324
Epoch 6/10
10/10 - 1s - loss: 183.7902 - loglik: -1.9117e+02 - logprior: 7.3804
Epoch 7/10
10/10 - 1s - loss: 182.6045 - loglik: -1.9117e+02 - logprior: 8.5641
Epoch 8/10
10/10 - 1s - loss: 181.7640 - loglik: -1.9119e+02 - logprior: 9.4289
Epoch 9/10
10/10 - 1s - loss: 181.0891 - loglik: -1.9124e+02 - logprior: 10.1513
Epoch 10/10
10/10 - 1s - loss: 180.5065 - loglik: -1.9131e+02 - logprior: 10.8065
Fitted a model with MAP estimate = -180.2070
Time for alignment: 45.8686
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 432.2513 - loglik: -3.4669e+02 - logprior: -8.5565e+01
Epoch 2/10
10/10 - 1s - loss: 327.9169 - loglik: -3.0811e+02 - logprior: -1.9804e+01
Epoch 3/10
10/10 - 1s - loss: 276.9611 - loglik: -2.6901e+02 - logprior: -7.9472e+00
Epoch 4/10
10/10 - 1s - loss: 245.8493 - loglik: -2.4144e+02 - logprior: -4.4098e+00
Epoch 5/10
10/10 - 1s - loss: 230.7928 - loglik: -2.2846e+02 - logprior: -2.3301e+00
Epoch 6/10
10/10 - 1s - loss: 224.5469 - loglik: -2.2349e+02 - logprior: -1.0527e+00
Epoch 7/10
10/10 - 1s - loss: 221.3984 - loglik: -2.2126e+02 - logprior: -1.3364e-01
Epoch 8/10
10/10 - 1s - loss: 219.5814 - loglik: -2.1995e+02 - logprior: 0.3729
Epoch 9/10
10/10 - 1s - loss: 218.6546 - loglik: -2.1945e+02 - logprior: 0.7940
Epoch 10/10
10/10 - 1s - loss: 218.0816 - loglik: -2.1927e+02 - logprior: 1.1920
Fitted a model with MAP estimate = -217.8346
expansions: [(12, 1), (13, 2), (14, 2), (24, 1), (29, 1), (30, 1), (40, 2), (41, 1), (55, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 313.6570 - loglik: -2.1812e+02 - logprior: -9.5537e+01
Epoch 2/2
10/10 - 1s - loss: 236.7612 - loglik: -2.0064e+02 - logprior: -3.6117e+01
Fitted a model with MAP estimate = -222.9305
expansions: [(0, 3)]
discards: [  0  47 112 113]
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 272.6508 - loglik: -1.9772e+02 - logprior: -7.4931e+01
Epoch 2/2
10/10 - 1s - loss: 208.7391 - loglik: -1.9292e+02 - logprior: -1.5823e+01
Fitted a model with MAP estimate = -198.7071
expansions: []
discards: [0 1]
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 285.8592 - loglik: -1.9619e+02 - logprior: -8.9666e+01
Epoch 2/10
10/10 - 1s - loss: 216.7503 - loglik: -1.9429e+02 - logprior: -2.2462e+01
Epoch 3/10
10/10 - 1s - loss: 196.8342 - loglik: -1.9271e+02 - logprior: -4.1279e+00
Epoch 4/10
10/10 - 1s - loss: 189.2689 - loglik: -1.9163e+02 - logprior: 2.3606
Epoch 5/10
10/10 - 1s - loss: 185.7159 - loglik: -1.9125e+02 - logprior: 5.5324
Epoch 6/10
10/10 - 1s - loss: 183.7900 - loglik: -1.9117e+02 - logprior: 7.3805
Epoch 7/10
10/10 - 1s - loss: 182.6044 - loglik: -1.9117e+02 - logprior: 8.5642
Epoch 8/10
10/10 - 1s - loss: 181.7639 - loglik: -1.9119e+02 - logprior: 9.4290
Epoch 9/10
10/10 - 1s - loss: 181.0891 - loglik: -1.9124e+02 - logprior: 10.1514
Epoch 10/10
10/10 - 1s - loss: 180.5063 - loglik: -1.9131e+02 - logprior: 10.8066
Fitted a model with MAP estimate = -180.2065
Time for alignment: 44.4223
Computed alignments with likelihoods: ['-180.2065', '-180.2070', '-180.2065']
Best model has likelihood: -180.2065
SP score = 0.9099
Training of 3 independent models on file rub.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f36881498e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34afbcf970>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 167.0926 - loglik: -1.3542e+02 - logprior: -3.1673e+01
Epoch 2/10
10/10 - 0s - loss: 124.2810 - loglik: -1.1533e+02 - logprior: -8.9501e+00
Epoch 3/10
10/10 - 0s - loss: 99.1949 - loglik: -9.4339e+01 - logprior: -4.8556e+00
Epoch 4/10
10/10 - 0s - loss: 84.2314 - loglik: -8.0499e+01 - logprior: -3.7328e+00
Epoch 5/10
10/10 - 0s - loss: 78.6295 - loglik: -7.5229e+01 - logprior: -3.4007e+00
Epoch 6/10
10/10 - 0s - loss: 76.5305 - loglik: -7.3377e+01 - logprior: -3.1534e+00
Epoch 7/10
10/10 - 0s - loss: 75.4721 - loglik: -7.2720e+01 - logprior: -2.7523e+00
Epoch 8/10
10/10 - 0s - loss: 74.8859 - loglik: -7.2423e+01 - logprior: -2.4625e+00
Epoch 9/10
10/10 - 0s - loss: 74.6152 - loglik: -7.2260e+01 - logprior: -2.3548e+00
Epoch 10/10
10/10 - 0s - loss: 74.4753 - loglik: -7.2153e+01 - logprior: -2.3223e+00
Fitted a model with MAP estimate = -74.3405
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 114.6307 - loglik: -7.2331e+01 - logprior: -4.2300e+01
Epoch 2/2
10/10 - 0s - loss: 77.9384 - loglik: -6.4164e+01 - logprior: -1.3775e+01
Fitted a model with MAP estimate = -70.7147
expansions: []
discards: [30 33]
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 90.9597 - loglik: -6.1122e+01 - logprior: -2.9838e+01
Epoch 2/2
10/10 - 0s - loss: 68.9543 - loglik: -6.0526e+01 - logprior: -8.4285e+00
Fitted a model with MAP estimate = -65.9688
expansions: []
discards: []
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 88.6907 - loglik: -6.0484e+01 - logprior: -2.8207e+01
Epoch 2/10
10/10 - 0s - loss: 68.3772 - loglik: -6.0392e+01 - logprior: -7.9854e+00
Epoch 3/10
10/10 - 0s - loss: 64.5805 - loglik: -6.0601e+01 - logprior: -3.9798e+00
Epoch 4/10
10/10 - 0s - loss: 63.0940 - loglik: -6.0618e+01 - logprior: -2.4756e+00
Epoch 5/10
10/10 - 0s - loss: 62.5264 - loglik: -6.0764e+01 - logprior: -1.7626e+00
Epoch 6/10
10/10 - 0s - loss: 61.9444 - loglik: -6.0531e+01 - logprior: -1.4131e+00
Epoch 7/10
10/10 - 0s - loss: 61.6626 - loglik: -6.0477e+01 - logprior: -1.1858e+00
Epoch 8/10
10/10 - 0s - loss: 61.5997 - loglik: -6.0616e+01 - logprior: -9.8366e-01
Epoch 9/10
10/10 - 0s - loss: 61.4696 - loglik: -6.0635e+01 - logprior: -8.3410e-01
Epoch 10/10
10/10 - 0s - loss: 61.4780 - loglik: -6.0742e+01 - logprior: -7.3593e-01
Fitted a model with MAP estimate = -61.3194
Time for alignment: 26.2253
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 167.1147 - loglik: -1.3544e+02 - logprior: -3.1674e+01
Epoch 2/10
10/10 - 0s - loss: 124.2953 - loglik: -1.1534e+02 - logprior: -8.9571e+00
Epoch 3/10
10/10 - 0s - loss: 99.6851 - loglik: -9.4819e+01 - logprior: -4.8665e+00
Epoch 4/10
10/10 - 0s - loss: 84.4350 - loglik: -8.0706e+01 - logprior: -3.7290e+00
Epoch 5/10
10/10 - 0s - loss: 78.9574 - loglik: -7.5558e+01 - logprior: -3.3993e+00
Epoch 6/10
10/10 - 0s - loss: 76.6403 - loglik: -7.3481e+01 - logprior: -3.1589e+00
Epoch 7/10
10/10 - 0s - loss: 75.5194 - loglik: -7.2759e+01 - logprior: -2.7604e+00
Epoch 8/10
10/10 - 0s - loss: 74.9286 - loglik: -7.2462e+01 - logprior: -2.4665e+00
Epoch 9/10
10/10 - 0s - loss: 74.6374 - loglik: -7.2280e+01 - logprior: -2.3572e+00
Epoch 10/10
10/10 - 0s - loss: 74.3785 - loglik: -7.2049e+01 - logprior: -2.3298e+00
Fitted a model with MAP estimate = -74.3485
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 114.5027 - loglik: -7.2213e+01 - logprior: -4.2290e+01
Epoch 2/2
10/10 - 0s - loss: 78.0125 - loglik: -6.4243e+01 - logprior: -1.3769e+01
Fitted a model with MAP estimate = -70.7284
expansions: []
discards: [30 33]
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 90.9194 - loglik: -6.1090e+01 - logprior: -2.9830e+01
Epoch 2/2
10/10 - 0s - loss: 69.0642 - loglik: -6.0633e+01 - logprior: -8.4311e+00
Fitted a model with MAP estimate = -65.9705
expansions: []
discards: []
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 88.5357 - loglik: -6.0323e+01 - logprior: -2.8212e+01
Epoch 2/10
10/10 - 0s - loss: 68.5101 - loglik: -6.0527e+01 - logprior: -7.9829e+00
Epoch 3/10
10/10 - 0s - loss: 64.5795 - loglik: -6.0598e+01 - logprior: -3.9817e+00
Epoch 4/10
10/10 - 0s - loss: 63.1765 - loglik: -6.0700e+01 - logprior: -2.4764e+00
Epoch 5/10
10/10 - 0s - loss: 62.3498 - loglik: -6.0585e+01 - logprior: -1.7646e+00
Epoch 6/10
10/10 - 0s - loss: 61.9920 - loglik: -6.0572e+01 - logprior: -1.4202e+00
Epoch 7/10
10/10 - 0s - loss: 61.7245 - loglik: -6.0541e+01 - logprior: -1.1833e+00
Epoch 8/10
10/10 - 0s - loss: 61.5056 - loglik: -6.0514e+01 - logprior: -9.9169e-01
Epoch 9/10
10/10 - 0s - loss: 61.5907 - loglik: -6.0755e+01 - logprior: -8.3531e-01
Fitted a model with MAP estimate = -61.4045
Time for alignment: 25.6100
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 167.2077 - loglik: -1.3554e+02 - logprior: -3.1672e+01
Epoch 2/10
10/10 - 0s - loss: 124.0702 - loglik: -1.1512e+02 - logprior: -8.9501e+00
Epoch 3/10
10/10 - 0s - loss: 98.5029 - loglik: -9.3628e+01 - logprior: -4.8751e+00
Epoch 4/10
10/10 - 0s - loss: 83.3313 - loglik: -7.9547e+01 - logprior: -3.7843e+00
Epoch 5/10
10/10 - 0s - loss: 78.2277 - loglik: -7.4792e+01 - logprior: -3.4361e+00
Epoch 6/10
10/10 - 0s - loss: 76.2528 - loglik: -7.3110e+01 - logprior: -3.1426e+00
Epoch 7/10
10/10 - 0s - loss: 75.3347 - loglik: -7.2606e+01 - logprior: -2.7287e+00
Epoch 8/10
10/10 - 0s - loss: 74.8119 - loglik: -7.2360e+01 - logprior: -2.4520e+00
Epoch 9/10
10/10 - 0s - loss: 74.5805 - loglik: -7.2219e+01 - logprior: -2.3615e+00
Epoch 10/10
10/10 - 0s - loss: 74.5048 - loglik: -7.2188e+01 - logprior: -2.3171e+00
Fitted a model with MAP estimate = -74.3246
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 114.5676 - loglik: -7.2271e+01 - logprior: -4.2296e+01
Epoch 2/2
10/10 - 0s - loss: 77.9146 - loglik: -6.4139e+01 - logprior: -1.3775e+01
Fitted a model with MAP estimate = -70.7204
expansions: []
discards: [30 33]
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 90.9615 - loglik: -6.1125e+01 - logprior: -2.9837e+01
Epoch 2/2
10/10 - 0s - loss: 69.0252 - loglik: -6.0596e+01 - logprior: -8.4291e+00
Fitted a model with MAP estimate = -65.9692
expansions: []
discards: []
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 88.4515 - loglik: -6.0241e+01 - logprior: -2.8211e+01
Epoch 2/10
10/10 - 0s - loss: 68.6082 - loglik: -6.0625e+01 - logprior: -7.9829e+00
Epoch 3/10
10/10 - 0s - loss: 64.5005 - loglik: -6.0518e+01 - logprior: -3.9820e+00
Epoch 4/10
10/10 - 0s - loss: 63.2693 - loglik: -6.0793e+01 - logprior: -2.4762e+00
Epoch 5/10
10/10 - 0s - loss: 62.4263 - loglik: -6.0661e+01 - logprior: -1.7652e+00
Epoch 6/10
10/10 - 0s - loss: 61.9836 - loglik: -6.0568e+01 - logprior: -1.4160e+00
Epoch 7/10
10/10 - 0s - loss: 61.6802 - loglik: -6.0500e+01 - logprior: -1.1804e+00
Epoch 8/10
10/10 - 0s - loss: 61.6429 - loglik: -6.0651e+01 - logprior: -9.9156e-01
Epoch 9/10
10/10 - 0s - loss: 61.3725 - loglik: -6.0541e+01 - logprior: -8.3173e-01
Epoch 10/10
10/10 - 0s - loss: 61.3887 - loglik: -6.0647e+01 - logprior: -7.4154e-01
Fitted a model with MAP estimate = -61.3206
Time for alignment: 24.6530
Computed alignments with likelihoods: ['-61.3194', '-61.4045', '-61.3206']
Best model has likelihood: -61.3194
SP score = 0.9918
Training of 3 independent models on file cyclo.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32a92fcc70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3698968e80>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 445.4022 - loglik: -4.4008e+02 - logprior: -5.3184e+00
Epoch 2/10
15/15 - 4s - loss: 367.5392 - loglik: -3.6626e+02 - logprior: -1.2768e+00
Epoch 3/10
15/15 - 4s - loss: 321.7429 - loglik: -3.2021e+02 - logprior: -1.5308e+00
Epoch 4/10
15/15 - 4s - loss: 307.8827 - loglik: -3.0622e+02 - logprior: -1.6651e+00
Epoch 5/10
15/15 - 4s - loss: 302.1784 - loglik: -3.0065e+02 - logprior: -1.5317e+00
Epoch 6/10
15/15 - 4s - loss: 300.6161 - loglik: -2.9909e+02 - logprior: -1.5279e+00
Epoch 7/10
15/15 - 4s - loss: 299.3830 - loglik: -2.9785e+02 - logprior: -1.5330e+00
Epoch 8/10
15/15 - 4s - loss: 297.7704 - loglik: -2.9622e+02 - logprior: -1.5551e+00
Epoch 9/10
15/15 - 4s - loss: 297.6068 - loglik: -2.9605e+02 - logprior: -1.5563e+00
Epoch 10/10
15/15 - 4s - loss: 297.1618 - loglik: -2.9560e+02 - logprior: -1.5572e+00
Fitted a model with MAP estimate = -296.8924
expansions: [(7, 3), (10, 1), (16, 1), (24, 3), (25, 1), (39, 1), (55, 1), (59, 1), (64, 2), (65, 2), (69, 1), (91, 1), (92, 2), (110, 1), (112, 1), (114, 2), (115, 1), (116, 4), (119, 2)]
discards: [0]
Fitting a model of length 158 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 302.9662 - loglik: -2.9645e+02 - logprior: -6.5173e+00
Epoch 2/2
15/15 - 5s - loss: 286.1332 - loglik: -2.8290e+02 - logprior: -3.2346e+00
Fitted a model with MAP estimate = -283.3451
expansions: [(0, 2)]
discards: [  0   7  29  77  84 109 136]
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 288.1829 - loglik: -2.8326e+02 - logprior: -4.9258e+00
Epoch 2/2
15/15 - 4s - loss: 281.1103 - loglik: -2.7972e+02 - logprior: -1.3949e+00
Fitted a model with MAP estimate = -280.1991
expansions: []
discards: [ 0 65 66]
Fitting a model of length 150 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 292.3149 - loglik: -2.8601e+02 - logprior: -6.3017e+00
Epoch 2/10
15/15 - 4s - loss: 286.2083 - loglik: -2.8412e+02 - logprior: -2.0843e+00
Epoch 3/10
15/15 - 4s - loss: 283.7458 - loglik: -2.8278e+02 - logprior: -9.6554e-01
Epoch 4/10
15/15 - 4s - loss: 282.0129 - loglik: -2.8120e+02 - logprior: -8.1345e-01
Epoch 5/10
15/15 - 4s - loss: 281.6410 - loglik: -2.8088e+02 - logprior: -7.5754e-01
Epoch 6/10
15/15 - 4s - loss: 281.3400 - loglik: -2.8062e+02 - logprior: -7.2007e-01
Epoch 7/10
15/15 - 4s - loss: 279.2521 - loglik: -2.7858e+02 - logprior: -6.7609e-01
Epoch 8/10
15/15 - 4s - loss: 280.3541 - loglik: -2.7973e+02 - logprior: -6.2749e-01
Fitted a model with MAP estimate = -279.6931
Time for alignment: 125.0679
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 444.6703 - loglik: -4.3935e+02 - logprior: -5.3249e+00
Epoch 2/10
15/15 - 4s - loss: 366.8266 - loglik: -3.6555e+02 - logprior: -1.2729e+00
Epoch 3/10
15/15 - 4s - loss: 317.2885 - loglik: -3.1573e+02 - logprior: -1.5592e+00
Epoch 4/10
15/15 - 4s - loss: 305.8705 - loglik: -3.0420e+02 - logprior: -1.6676e+00
Epoch 5/10
15/15 - 4s - loss: 300.7999 - loglik: -2.9926e+02 - logprior: -1.5393e+00
Epoch 6/10
15/15 - 4s - loss: 300.4120 - loglik: -2.9887e+02 - logprior: -1.5394e+00
Epoch 7/10
15/15 - 4s - loss: 299.3021 - loglik: -2.9778e+02 - logprior: -1.5175e+00
Epoch 8/10
15/15 - 4s - loss: 298.4053 - loglik: -2.9691e+02 - logprior: -1.4940e+00
Epoch 9/10
15/15 - 4s - loss: 298.8799 - loglik: -2.9739e+02 - logprior: -1.4937e+00
Fitted a model with MAP estimate = -298.2479
expansions: [(7, 3), (10, 1), (16, 1), (24, 3), (25, 1), (49, 2), (60, 1), (66, 3), (67, 1), (69, 1), (91, 1), (92, 2), (108, 1), (112, 1), (114, 3), (116, 3), (119, 2), (120, 1)]
discards: [0]
Fitting a model of length 158 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 304.4966 - loglik: -2.9800e+02 - logprior: -6.4920e+00
Epoch 2/2
15/15 - 5s - loss: 286.9476 - loglik: -2.8374e+02 - logprior: -3.2095e+00
Fitted a model with MAP estimate = -284.5400
expansions: [(0, 2)]
discards: [  0   7  29  58  79 109 149]
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 289.1723 - loglik: -2.8426e+02 - logprior: -4.9099e+00
Epoch 2/2
15/15 - 4s - loss: 282.3918 - loglik: -2.8098e+02 - logprior: -1.4144e+00
Fitted a model with MAP estimate = -281.5454
expansions: []
discards: [ 0 81]
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 291.0513 - loglik: -2.8471e+02 - logprior: -6.3458e+00
Epoch 2/10
15/15 - 4s - loss: 286.1839 - loglik: -2.8407e+02 - logprior: -2.1144e+00
Epoch 3/10
15/15 - 4s - loss: 281.8271 - loglik: -2.8082e+02 - logprior: -1.0074e+00
Epoch 4/10
15/15 - 4s - loss: 280.7339 - loglik: -2.7990e+02 - logprior: -8.2912e-01
Epoch 5/10
15/15 - 4s - loss: 280.3874 - loglik: -2.7962e+02 - logprior: -7.7085e-01
Epoch 6/10
15/15 - 4s - loss: 280.3097 - loglik: -2.7956e+02 - logprior: -7.5065e-01
Epoch 7/10
15/15 - 4s - loss: 278.7095 - loglik: -2.7800e+02 - logprior: -7.0512e-01
Epoch 8/10
15/15 - 4s - loss: 278.9039 - loglik: -2.7825e+02 - logprior: -6.5538e-01
Fitted a model with MAP estimate = -278.6902
Time for alignment: 121.2662
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 444.4719 - loglik: -4.3915e+02 - logprior: -5.3184e+00
Epoch 2/10
15/15 - 4s - loss: 366.2553 - loglik: -3.6498e+02 - logprior: -1.2750e+00
Epoch 3/10
15/15 - 4s - loss: 318.3703 - loglik: -3.1684e+02 - logprior: -1.5323e+00
Epoch 4/10
15/15 - 4s - loss: 305.2252 - loglik: -3.0359e+02 - logprior: -1.6381e+00
Epoch 5/10
15/15 - 4s - loss: 301.2622 - loglik: -2.9977e+02 - logprior: -1.4912e+00
Epoch 6/10
15/15 - 4s - loss: 299.4691 - loglik: -2.9797e+02 - logprior: -1.4981e+00
Epoch 7/10
15/15 - 4s - loss: 299.2657 - loglik: -2.9779e+02 - logprior: -1.4753e+00
Epoch 8/10
15/15 - 4s - loss: 297.9664 - loglik: -2.9652e+02 - logprior: -1.4478e+00
Epoch 9/10
15/15 - 4s - loss: 298.5322 - loglik: -2.9709e+02 - logprior: -1.4404e+00
Fitted a model with MAP estimate = -297.9761
expansions: [(5, 1), (7, 2), (10, 1), (14, 1), (24, 3), (25, 1), (49, 2), (60, 1), (65, 4), (69, 1), (91, 1), (92, 2), (112, 1), (114, 4), (116, 3), (119, 2), (120, 1)]
discards: [0]
Fitting a model of length 158 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 303.2460 - loglik: -2.9676e+02 - logprior: -6.4901e+00
Epoch 2/2
15/15 - 5s - loss: 284.6016 - loglik: -2.8137e+02 - logprior: -3.2312e+00
Fitted a model with MAP estimate = -282.8952
expansions: [(0, 2)]
discards: [  0   7  29  58  67  84 109 140 149]
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 290.0617 - loglik: -2.8515e+02 - logprior: -4.9149e+00
Epoch 2/2
15/15 - 4s - loss: 282.9188 - loglik: -2.8149e+02 - logprior: -1.4290e+00
Fitted a model with MAP estimate = -281.6777
expansions: [(65, 1)]
discards: [0]
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 290.6289 - loglik: -2.8429e+02 - logprior: -6.3375e+00
Epoch 2/10
15/15 - 4s - loss: 282.8934 - loglik: -2.8075e+02 - logprior: -2.1460e+00
Epoch 3/10
15/15 - 4s - loss: 282.1074 - loglik: -2.8112e+02 - logprior: -9.9232e-01
Epoch 4/10
15/15 - 4s - loss: 279.3953 - loglik: -2.7858e+02 - logprior: -8.1927e-01
Epoch 5/10
15/15 - 4s - loss: 278.9832 - loglik: -2.7822e+02 - logprior: -7.6017e-01
Epoch 6/10
15/15 - 4s - loss: 278.1801 - loglik: -2.7744e+02 - logprior: -7.4376e-01
Epoch 7/10
15/15 - 4s - loss: 277.8284 - loglik: -2.7713e+02 - logprior: -6.9791e-01
Epoch 8/10
15/15 - 4s - loss: 277.0626 - loglik: -2.7641e+02 - logprior: -6.4856e-01
Epoch 9/10
15/15 - 4s - loss: 277.7770 - loglik: -2.7718e+02 - logprior: -6.0168e-01
Fitted a model with MAP estimate = -277.1052
Time for alignment: 125.5868
Computed alignments with likelihoods: ['-279.6931', '-278.6902', '-277.1052']
Best model has likelihood: -277.1052
SP score = 0.9034
Training of 3 independent models on file gpdh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32c8371580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f35ed1139d0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 119 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 344.2308 - loglik: -3.4010e+02 - logprior: -4.1269e+00
Epoch 2/10
17/17 - 4s - loss: 254.6235 - loglik: -2.5310e+02 - logprior: -1.5280e+00
Epoch 3/10
17/17 - 4s - loss: 212.9789 - loglik: -2.1113e+02 - logprior: -1.8484e+00
Epoch 4/10
17/17 - 3s - loss: 203.0861 - loglik: -2.0129e+02 - logprior: -1.7941e+00
Epoch 5/10
17/17 - 4s - loss: 200.4673 - loglik: -1.9868e+02 - logprior: -1.7845e+00
Epoch 6/10
17/17 - 4s - loss: 200.6396 - loglik: -1.9889e+02 - logprior: -1.7533e+00
Fitted a model with MAP estimate = -199.5898
expansions: [(0, 2), (15, 1), (16, 2), (17, 3), (26, 1), (27, 1), (28, 1), (40, 1), (41, 1), (44, 1), (48, 1), (49, 1), (53, 1), (54, 1), (55, 1), (56, 1), (64, 1), (65, 1), (66, 1), (78, 1), (81, 1), (84, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Fitting a model of length 150 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 195.9300 - loglik: -1.9130e+02 - logprior: -4.6264e+00
Epoch 2/2
17/17 - 5s - loss: 179.8749 - loglik: -1.7847e+02 - logprior: -1.4065e+00
Fitted a model with MAP estimate = -177.7652
expansions: []
discards: [ 20 139]
Fitting a model of length 148 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 181.9435 - loglik: -1.7795e+02 - logprior: -3.9889e+00
Epoch 2/2
17/17 - 5s - loss: 177.6626 - loglik: -1.7628e+02 - logprior: -1.3841e+00
Fitted a model with MAP estimate = -176.7986
expansions: []
discards: []
Fitting a model of length 148 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 181.9225 - loglik: -1.7797e+02 - logprior: -3.9482e+00
Epoch 2/10
17/17 - 5s - loss: 176.7040 - loglik: -1.7538e+02 - logprior: -1.3213e+00
Epoch 3/10
17/17 - 6s - loss: 176.8834 - loglik: -1.7578e+02 - logprior: -1.1006e+00
Fitted a model with MAP estimate = -175.8822
Time for alignment: 92.8656
Fitting a model of length 119 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 344.7804 - loglik: -3.4065e+02 - logprior: -4.1329e+00
Epoch 2/10
17/17 - 4s - loss: 255.6737 - loglik: -2.5413e+02 - logprior: -1.5486e+00
Epoch 3/10
17/17 - 4s - loss: 216.4163 - loglik: -2.1460e+02 - logprior: -1.8150e+00
Epoch 4/10
17/17 - 4s - loss: 204.8180 - loglik: -2.0298e+02 - logprior: -1.8406e+00
Epoch 5/10
17/17 - 4s - loss: 200.9506 - loglik: -1.9909e+02 - logprior: -1.8630e+00
Epoch 6/10
17/17 - 4s - loss: 199.4637 - loglik: -1.9760e+02 - logprior: -1.8611e+00
Epoch 7/10
17/17 - 4s - loss: 199.9216 - loglik: -1.9809e+02 - logprior: -1.8360e+00
Fitted a model with MAP estimate = -199.3893
expansions: [(0, 2), (15, 1), (16, 1), (17, 3), (18, 1), (26, 2), (27, 1), (28, 1), (40, 1), (41, 1), (44, 1), (45, 1), (47, 1), (53, 1), (55, 1), (56, 1), (57, 1), (64, 1), (65, 1), (75, 1), (78, 1), (81, 1), (84, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Fitting a model of length 152 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 195.6089 - loglik: -1.9089e+02 - logprior: -4.7207e+00
Epoch 2/2
17/17 - 5s - loss: 178.2021 - loglik: -1.7684e+02 - logprior: -1.3584e+00
Fitted a model with MAP estimate = -176.6158
expansions: []
discards: [ 22  34 141]
Fitting a model of length 149 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 182.3558 - loglik: -1.7838e+02 - logprior: -3.9767e+00
Epoch 2/2
17/17 - 5s - loss: 176.5824 - loglik: -1.7521e+02 - logprior: -1.3741e+00
Fitted a model with MAP estimate = -176.3256
expansions: []
discards: []
Fitting a model of length 149 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 181.4716 - loglik: -1.7753e+02 - logprior: -3.9377e+00
Epoch 2/10
17/17 - 5s - loss: 177.4320 - loglik: -1.7613e+02 - logprior: -1.3038e+00
Epoch 3/10
17/17 - 4s - loss: 175.2583 - loglik: -1.7417e+02 - logprior: -1.0885e+00
Epoch 4/10
17/17 - 5s - loss: 175.1181 - loglik: -1.7411e+02 - logprior: -1.0075e+00
Epoch 5/10
17/17 - 5s - loss: 175.4497 - loglik: -1.7448e+02 - logprior: -9.6640e-01
Fitted a model with MAP estimate = -174.5576
Time for alignment: 108.8844
Fitting a model of length 119 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 345.1058 - loglik: -3.4097e+02 - logprior: -4.1349e+00
Epoch 2/10
17/17 - 4s - loss: 254.5412 - loglik: -2.5302e+02 - logprior: -1.5246e+00
Epoch 3/10
17/17 - 4s - loss: 211.1097 - loglik: -2.0925e+02 - logprior: -1.8552e+00
Epoch 4/10
17/17 - 4s - loss: 202.9005 - loglik: -2.0109e+02 - logprior: -1.8126e+00
Epoch 5/10
17/17 - 3s - loss: 201.6498 - loglik: -1.9984e+02 - logprior: -1.8088e+00
Epoch 6/10
17/17 - 4s - loss: 200.0106 - loglik: -1.9822e+02 - logprior: -1.7950e+00
Epoch 7/10
17/17 - 4s - loss: 198.8609 - loglik: -1.9706e+02 - logprior: -1.7981e+00
Epoch 8/10
17/17 - 4s - loss: 199.5735 - loglik: -1.9781e+02 - logprior: -1.7656e+00
Fitted a model with MAP estimate = -199.1984
expansions: [(0, 2), (15, 1), (16, 2), (17, 3), (26, 2), (27, 2), (40, 1), (41, 1), (44, 1), (45, 1), (47, 1), (53, 1), (54, 1), (55, 1), (56, 1), (60, 1), (65, 1), (66, 1), (78, 1), (81, 1), (84, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Fitting a model of length 152 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 194.9523 - loglik: -1.9026e+02 - logprior: -4.6967e+00
Epoch 2/2
17/17 - 5s - loss: 179.1743 - loglik: -1.7781e+02 - logprior: -1.3675e+00
Fitted a model with MAP estimate = -176.5355
expansions: []
discards: [ 20  35 141]
Fitting a model of length 149 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 181.6759 - loglik: -1.7770e+02 - logprior: -3.9718e+00
Epoch 2/2
17/17 - 5s - loss: 176.7759 - loglik: -1.7540e+02 - logprior: -1.3720e+00
Fitted a model with MAP estimate = -176.1648
expansions: []
discards: []
Fitting a model of length 149 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 180.4966 - loglik: -1.7657e+02 - logprior: -3.9227e+00
Epoch 2/10
17/17 - 5s - loss: 177.6461 - loglik: -1.7635e+02 - logprior: -1.3003e+00
Epoch 3/10
17/17 - 5s - loss: 175.8214 - loglik: -1.7474e+02 - logprior: -1.0842e+00
Epoch 4/10
17/17 - 5s - loss: 174.8190 - loglik: -1.7380e+02 - logprior: -1.0154e+00
Epoch 5/10
17/17 - 4s - loss: 174.9190 - loglik: -1.7396e+02 - logprior: -9.6204e-01
Fitted a model with MAP estimate = -174.3855
Time for alignment: 111.6185
Computed alignments with likelihoods: ['-175.8822', '-174.5576', '-174.3855']
Best model has likelihood: -174.3855
SP score = 0.6224
Training of 3 independent models on file sodcu.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f367696f310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34c9ad33d0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 419.7448 - loglik: -3.9904e+02 - logprior: -2.0708e+01
Epoch 2/10
10/10 - 2s - loss: 364.1583 - loglik: -3.5943e+02 - logprior: -4.7249e+00
Epoch 3/10
10/10 - 2s - loss: 323.6568 - loglik: -3.2121e+02 - logprior: -2.4426e+00
Epoch 4/10
10/10 - 2s - loss: 294.4939 - loglik: -2.9254e+02 - logprior: -1.9495e+00
Epoch 5/10
10/10 - 2s - loss: 283.1004 - loglik: -2.8130e+02 - logprior: -1.7995e+00
Epoch 6/10
10/10 - 2s - loss: 277.3963 - loglik: -2.7565e+02 - logprior: -1.7479e+00
Epoch 7/10
10/10 - 2s - loss: 274.7513 - loglik: -2.7301e+02 - logprior: -1.7430e+00
Epoch 8/10
10/10 - 2s - loss: 273.6597 - loglik: -2.7191e+02 - logprior: -1.7515e+00
Epoch 9/10
10/10 - 2s - loss: 271.8463 - loglik: -2.7013e+02 - logprior: -1.7155e+00
Epoch 10/10
10/10 - 2s - loss: 272.9673 - loglik: -2.7131e+02 - logprior: -1.6604e+00
Fitted a model with MAP estimate = -271.8309
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (23, 1), (24, 4), (37, 1), (53, 1), (60, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (98, 1), (99, 1), (101, 1), (102, 4), (103, 1), (104, 1)]
discards: []
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 288.2043 - loglik: -2.6933e+02 - logprior: -1.8875e+01
Epoch 2/2
10/10 - 2s - loss: 261.3727 - loglik: -2.5685e+02 - logprior: -4.5257e+00
Fitted a model with MAP estimate = -256.7687
expansions: []
discards: [  0  31 124 125]
Fitting a model of length 141 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 282.7419 - loglik: -2.5942e+02 - logprior: -2.3325e+01
Epoch 2/2
10/10 - 2s - loss: 265.6754 - loglik: -2.5656e+02 - logprior: -9.1131e+00
Fitted a model with MAP estimate = -262.6973
expansions: [(0, 5)]
discards: [0]
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 274.9494 - loglik: -2.5655e+02 - logprior: -1.8395e+01
Epoch 2/10
10/10 - 2s - loss: 258.4702 - loglik: -2.5448e+02 - logprior: -3.9929e+00
Epoch 3/10
10/10 - 2s - loss: 254.4391 - loglik: -2.5331e+02 - logprior: -1.1291e+00
Epoch 4/10
10/10 - 2s - loss: 252.9424 - loglik: -2.5286e+02 - logprior: -7.9221e-02
Epoch 5/10
10/10 - 2s - loss: 250.6503 - loglik: -2.5106e+02 - logprior: 0.4107
Epoch 6/10
10/10 - 2s - loss: 250.3051 - loglik: -2.5093e+02 - logprior: 0.6270
Epoch 7/10
10/10 - 2s - loss: 249.3422 - loglik: -2.5012e+02 - logprior: 0.7807
Epoch 8/10
10/10 - 2s - loss: 249.9489 - loglik: -2.5089e+02 - logprior: 0.9374
Fitted a model with MAP estimate = -249.1557
Time for alignment: 61.6974
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 419.4058 - loglik: -3.9870e+02 - logprior: -2.0707e+01
Epoch 2/10
10/10 - 2s - loss: 363.6977 - loglik: -3.5896e+02 - logprior: -4.7407e+00
Epoch 3/10
10/10 - 2s - loss: 318.6196 - loglik: -3.1610e+02 - logprior: -2.5234e+00
Epoch 4/10
10/10 - 2s - loss: 290.5956 - loglik: -2.8847e+02 - logprior: -2.1224e+00
Epoch 5/10
10/10 - 2s - loss: 279.3301 - loglik: -2.7723e+02 - logprior: -2.1045e+00
Epoch 6/10
10/10 - 2s - loss: 276.4465 - loglik: -2.7443e+02 - logprior: -2.0207e+00
Epoch 7/10
10/10 - 2s - loss: 274.0688 - loglik: -2.7216e+02 - logprior: -1.9128e+00
Epoch 8/10
10/10 - 2s - loss: 272.9821 - loglik: -2.7118e+02 - logprior: -1.8035e+00
Epoch 9/10
10/10 - 2s - loss: 273.1665 - loglik: -2.7147e+02 - logprior: -1.6991e+00
Fitted a model with MAP estimate = -272.5926
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (55, 2), (62, 1), (79, 2), (80, 2), (81, 2), (86, 1), (87, 1), (98, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Fitting a model of length 142 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 290.6484 - loglik: -2.7183e+02 - logprior: -1.8818e+01
Epoch 2/2
10/10 - 2s - loss: 265.9130 - loglik: -2.6144e+02 - logprior: -4.4752e+00
Fitted a model with MAP estimate = -261.6817
expansions: []
discards: [ 0 66]
Fitting a model of length 140 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 285.7025 - loglik: -2.6234e+02 - logprior: -2.3360e+01
Epoch 2/2
10/10 - 2s - loss: 269.2979 - loglik: -2.6013e+02 - logprior: -9.1654e+00
Fitted a model with MAP estimate = -266.2366
expansions: [(0, 5), (117, 3)]
discards: [  0 118]
Fitting a model of length 146 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 279.2415 - loglik: -2.6075e+02 - logprior: -1.8496e+01
Epoch 2/10
10/10 - 2s - loss: 261.1750 - loglik: -2.5702e+02 - logprior: -4.1558e+00
Epoch 3/10
10/10 - 2s - loss: 257.2551 - loglik: -2.5591e+02 - logprior: -1.3486e+00
Epoch 4/10
10/10 - 2s - loss: 253.9608 - loglik: -2.5367e+02 - logprior: -2.8694e-01
Epoch 5/10
10/10 - 2s - loss: 252.2868 - loglik: -2.5249e+02 - logprior: 0.1989
Epoch 6/10
10/10 - 2s - loss: 251.6954 - loglik: -2.5213e+02 - logprior: 0.4389
Epoch 7/10
10/10 - 2s - loss: 250.2384 - loglik: -2.5081e+02 - logprior: 0.5764
Epoch 8/10
10/10 - 2s - loss: 250.9849 - loglik: -2.5170e+02 - logprior: 0.7145
Fitted a model with MAP estimate = -250.2877
Time for alignment: 60.3921
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 419.8376 - loglik: -3.9913e+02 - logprior: -2.0705e+01
Epoch 2/10
10/10 - 2s - loss: 364.5900 - loglik: -3.5985e+02 - logprior: -4.7404e+00
Epoch 3/10
10/10 - 2s - loss: 324.6961 - loglik: -3.2220e+02 - logprior: -2.4981e+00
Epoch 4/10
10/10 - 2s - loss: 297.8891 - loglik: -2.9584e+02 - logprior: -2.0445e+00
Epoch 5/10
10/10 - 2s - loss: 284.4722 - loglik: -2.8249e+02 - logprior: -1.9810e+00
Epoch 6/10
10/10 - 2s - loss: 279.4737 - loglik: -2.7755e+02 - logprior: -1.9230e+00
Epoch 7/10
10/10 - 2s - loss: 276.5817 - loglik: -2.7469e+02 - logprior: -1.8931e+00
Epoch 8/10
10/10 - 2s - loss: 274.4465 - loglik: -2.7255e+02 - logprior: -1.8936e+00
Epoch 9/10
10/10 - 2s - loss: 274.0289 - loglik: -2.7219e+02 - logprior: -1.8397e+00
Epoch 10/10
10/10 - 2s - loss: 273.7422 - loglik: -2.7196e+02 - logprior: -1.7861e+00
Fitted a model with MAP estimate = -273.3209
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (23, 1), (24, 4), (26, 1), (53, 1), (55, 2), (62, 1), (71, 1), (80, 2), (81, 2), (82, 2), (85, 5), (86, 1), (94, 2), (102, 1), (103, 1), (104, 1)]
discards: []
Fitting a model of length 149 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 289.1586 - loglik: -2.7013e+02 - logprior: -1.9032e+01
Epoch 2/2
10/10 - 2s - loss: 261.5013 - loglik: -2.5674e+02 - logprior: -4.7591e+00
Fitted a model with MAP estimate = -256.5519
expansions: []
discards: [ 0 31 67 97 99]
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 281.0672 - loglik: -2.5759e+02 - logprior: -2.3479e+01
Epoch 2/2
10/10 - 2s - loss: 264.1690 - loglik: -2.5493e+02 - logprior: -9.2402e+00
Fitted a model with MAP estimate = -261.0371
expansions: [(0, 5)]
discards: [0]
Fitting a model of length 148 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 273.6386 - loglik: -2.5510e+02 - logprior: -1.8540e+01
Epoch 2/10
10/10 - 2s - loss: 257.0089 - loglik: -2.5285e+02 - logprior: -4.1633e+00
Epoch 3/10
10/10 - 2s - loss: 252.8797 - loglik: -2.5158e+02 - logprior: -1.2963e+00
Epoch 4/10
10/10 - 2s - loss: 251.4724 - loglik: -2.5123e+02 - logprior: -2.4094e-01
Epoch 5/10
10/10 - 2s - loss: 249.8415 - loglik: -2.5008e+02 - logprior: 0.2374
Epoch 6/10
10/10 - 2s - loss: 248.4924 - loglik: -2.4896e+02 - logprior: 0.4689
Epoch 7/10
10/10 - 2s - loss: 248.4497 - loglik: -2.4906e+02 - logprior: 0.6144
Epoch 8/10
10/10 - 2s - loss: 247.8531 - loglik: -2.4863e+02 - logprior: 0.7723
Epoch 9/10
10/10 - 2s - loss: 246.9992 - loglik: -2.4791e+02 - logprior: 0.9072
Epoch 10/10
10/10 - 2s - loss: 247.8929 - loglik: -2.4889e+02 - logprior: 0.9954
Fitted a model with MAP estimate = -247.3919
Time for alignment: 67.2250
Computed alignments with likelihoods: ['-249.1557', '-250.2877', '-247.3919']
Best model has likelihood: -247.3919
SP score = 0.8666
Training of 3 independent models on file DEATH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34d23ce130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33207dbb80>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 269.9553 - loglik: -2.3265e+02 - logprior: -3.7304e+01
Epoch 2/10
10/10 - 1s - loss: 230.4668 - loglik: -2.2094e+02 - logprior: -9.5266e+00
Epoch 3/10
10/10 - 1s - loss: 212.5530 - loglik: -2.0829e+02 - logprior: -4.2648e+00
Epoch 4/10
10/10 - 1s - loss: 199.8517 - loglik: -1.9745e+02 - logprior: -2.4028e+00
Epoch 5/10
10/10 - 1s - loss: 193.9137 - loglik: -1.9218e+02 - logprior: -1.7322e+00
Epoch 6/10
10/10 - 1s - loss: 191.0329 - loglik: -1.8947e+02 - logprior: -1.5661e+00
Epoch 7/10
10/10 - 1s - loss: 189.2266 - loglik: -1.8814e+02 - logprior: -1.0849e+00
Epoch 8/10
10/10 - 1s - loss: 188.1201 - loglik: -1.8752e+02 - logprior: -5.9549e-01
Epoch 9/10
10/10 - 1s - loss: 187.5355 - loglik: -1.8709e+02 - logprior: -4.4998e-01
Epoch 10/10
10/10 - 1s - loss: 186.8646 - loglik: -1.8649e+02 - logprior: -3.7088e-01
Fitted a model with MAP estimate = -186.5243
expansions: [(0, 3), (6, 1), (22, 1), (30, 3), (40, 2), (43, 1), (55, 1), (65, 4)]
discards: []
Fitting a model of length 81 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 252.2525 - loglik: -2.0328e+02 - logprior: -4.8971e+01
Epoch 2/2
10/10 - 1s - loss: 204.8944 - loglik: -1.9066e+02 - logprior: -1.4239e+01
Fitted a model with MAP estimate = -195.0002
expansions: [(33, 1), (34, 2)]
discards: [0]
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 233.2268 - loglik: -1.9130e+02 - logprior: -4.1924e+01
Epoch 2/2
10/10 - 1s - loss: 202.5273 - loglik: -1.8675e+02 - logprior: -1.5774e+01
Fitted a model with MAP estimate = -197.1158
expansions: [(0, 3)]
discards: [0 1]
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 223.3774 - loglik: -1.8626e+02 - logprior: -3.7114e+01
Epoch 2/10
10/10 - 1s - loss: 193.1970 - loglik: -1.8408e+02 - logprior: -9.1123e+00
Epoch 3/10
10/10 - 1s - loss: 186.0828 - loglik: -1.8317e+02 - logprior: -2.9172e+00
Epoch 4/10
10/10 - 1s - loss: 182.9253 - loglik: -1.8228e+02 - logprior: -6.4473e-01
Epoch 5/10
10/10 - 1s - loss: 181.2221 - loglik: -1.8170e+02 - logprior: 0.4794
Epoch 6/10
10/10 - 1s - loss: 179.8437 - loglik: -1.8090e+02 - logprior: 1.0592
Epoch 7/10
10/10 - 1s - loss: 178.9380 - loglik: -1.8029e+02 - logprior: 1.3545
Epoch 8/10
10/10 - 1s - loss: 177.9334 - loglik: -1.7945e+02 - logprior: 1.5163
Epoch 9/10
10/10 - 1s - loss: 177.4560 - loglik: -1.7908e+02 - logprior: 1.6287
Epoch 10/10
10/10 - 1s - loss: 176.7519 - loglik: -1.7851e+02 - logprior: 1.7605
Fitted a model with MAP estimate = -176.5856
Time for alignment: 36.9829
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 269.9437 - loglik: -2.3264e+02 - logprior: -3.7304e+01
Epoch 2/10
10/10 - 1s - loss: 230.2515 - loglik: -2.2072e+02 - logprior: -9.5274e+00
Epoch 3/10
10/10 - 1s - loss: 212.0845 - loglik: -2.0782e+02 - logprior: -4.2636e+00
Epoch 4/10
10/10 - 1s - loss: 199.6657 - loglik: -1.9728e+02 - logprior: -2.3882e+00
Epoch 5/10
10/10 - 1s - loss: 193.0210 - loglik: -1.9128e+02 - logprior: -1.7432e+00
Epoch 6/10
10/10 - 1s - loss: 190.2757 - loglik: -1.8873e+02 - logprior: -1.5413e+00
Epoch 7/10
10/10 - 1s - loss: 188.5209 - loglik: -1.8751e+02 - logprior: -1.0129e+00
Epoch 8/10
10/10 - 1s - loss: 187.6539 - loglik: -1.8711e+02 - logprior: -5.4768e-01
Epoch 9/10
10/10 - 1s - loss: 186.8966 - loglik: -1.8647e+02 - logprior: -4.2918e-01
Epoch 10/10
10/10 - 1s - loss: 186.4490 - loglik: -1.8609e+02 - logprior: -3.6069e-01
Fitted a model with MAP estimate = -186.1431
expansions: [(0, 3), (6, 1), (22, 1), (28, 2), (29, 1), (30, 3), (40, 2), (43, 1), (59, 1), (65, 4)]
discards: []
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 250.5488 - loglik: -2.0180e+02 - logprior: -4.8745e+01
Epoch 2/2
10/10 - 1s - loss: 203.7090 - loglik: -1.8960e+02 - logprior: -1.4113e+01
Fitted a model with MAP estimate = -194.1726
expansions: []
discards: [0]
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 232.0492 - loglik: -1.9024e+02 - logprior: -4.1807e+01
Epoch 2/2
10/10 - 1s - loss: 202.5147 - loglik: -1.8659e+02 - logprior: -1.5921e+01
Fitted a model with MAP estimate = -197.2947
expansions: [(0, 3)]
discards: [ 0  1 34]
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 223.7249 - loglik: -1.8644e+02 - logprior: -3.7284e+01
Epoch 2/10
10/10 - 1s - loss: 193.7674 - loglik: -1.8453e+02 - logprior: -9.2334e+00
Epoch 3/10
10/10 - 1s - loss: 186.3173 - loglik: -1.8333e+02 - logprior: -2.9847e+00
Epoch 4/10
10/10 - 1s - loss: 183.4766 - loglik: -1.8277e+02 - logprior: -7.0980e-01
Epoch 5/10
10/10 - 1s - loss: 181.4868 - loglik: -1.8189e+02 - logprior: 0.4038
Epoch 6/10
10/10 - 1s - loss: 180.2760 - loglik: -1.8127e+02 - logprior: 0.9943
Epoch 7/10
10/10 - 1s - loss: 179.3480 - loglik: -1.8064e+02 - logprior: 1.2912
Epoch 8/10
10/10 - 1s - loss: 178.3088 - loglik: -1.7976e+02 - logprior: 1.4503
Epoch 9/10
10/10 - 1s - loss: 177.7133 - loglik: -1.7928e+02 - logprior: 1.5671
Epoch 10/10
10/10 - 1s - loss: 177.1078 - loglik: -1.7881e+02 - logprior: 1.7045
Fitted a model with MAP estimate = -176.9440
Time for alignment: 37.6597
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 269.9550 - loglik: -2.3265e+02 - logprior: -3.7304e+01
Epoch 2/10
10/10 - 1s - loss: 230.2879 - loglik: -2.2076e+02 - logprior: -9.5306e+00
Epoch 3/10
10/10 - 1s - loss: 212.2663 - loglik: -2.0796e+02 - logprior: -4.3022e+00
Epoch 4/10
10/10 - 1s - loss: 201.3833 - loglik: -1.9897e+02 - logprior: -2.4087e+00
Epoch 5/10
10/10 - 1s - loss: 194.6196 - loglik: -1.9283e+02 - logprior: -1.7892e+00
Epoch 6/10
10/10 - 1s - loss: 191.1619 - loglik: -1.8943e+02 - logprior: -1.7299e+00
Epoch 7/10
10/10 - 1s - loss: 189.5194 - loglik: -1.8827e+02 - logprior: -1.2447e+00
Epoch 8/10
10/10 - 1s - loss: 188.2322 - loglik: -1.8748e+02 - logprior: -7.4990e-01
Epoch 9/10
10/10 - 1s - loss: 187.5241 - loglik: -1.8690e+02 - logprior: -6.2057e-01
Epoch 10/10
10/10 - 1s - loss: 187.1620 - loglik: -1.8661e+02 - logprior: -5.4790e-01
Fitted a model with MAP estimate = -186.9825
expansions: [(0, 3), (6, 1), (22, 1), (25, 2), (27, 1), (28, 1), (31, 2), (34, 1), (40, 2), (43, 1), (55, 1), (65, 4)]
discards: []
Fitting a model of length 85 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 252.3895 - loglik: -2.0363e+02 - logprior: -4.8756e+01
Epoch 2/2
10/10 - 1s - loss: 204.4499 - loglik: -1.9024e+02 - logprior: -1.4211e+01
Fitted a model with MAP estimate = -194.7302
expansions: []
discards: [ 0 30]
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 232.8082 - loglik: -1.9108e+02 - logprior: -4.1729e+01
Epoch 2/2
10/10 - 1s - loss: 202.8388 - loglik: -1.8703e+02 - logprior: -1.5812e+01
Fitted a model with MAP estimate = -197.6537
expansions: [(0, 3)]
discards: [0 1]
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 224.0846 - loglik: -1.8691e+02 - logprior: -3.7172e+01
Epoch 2/10
10/10 - 1s - loss: 194.0342 - loglik: -1.8493e+02 - logprior: -9.1066e+00
Epoch 3/10
10/10 - 1s - loss: 186.7219 - loglik: -1.8385e+02 - logprior: -2.8728e+00
Epoch 4/10
10/10 - 1s - loss: 183.4153 - loglik: -1.8281e+02 - logprior: -6.0316e-01
Epoch 5/10
10/10 - 1s - loss: 181.3582 - loglik: -1.8183e+02 - logprior: 0.4768
Epoch 6/10
10/10 - 1s - loss: 179.9975 - loglik: -1.8106e+02 - logprior: 1.0625
Epoch 7/10
10/10 - 1s - loss: 178.9596 - loglik: -1.8036e+02 - logprior: 1.4006
Epoch 8/10
10/10 - 1s - loss: 178.0804 - loglik: -1.7965e+02 - logprior: 1.5731
Epoch 9/10
10/10 - 1s - loss: 177.4391 - loglik: -1.7913e+02 - logprior: 1.6954
Epoch 10/10
10/10 - 1s - loss: 176.8724 - loglik: -1.7869e+02 - logprior: 1.8140
Fitted a model with MAP estimate = -176.6334
Time for alignment: 36.4210
Computed alignments with likelihoods: ['-176.5856', '-176.9440', '-176.6334']
Best model has likelihood: -176.5856
SP score = 0.7880
Training of 3 independent models on file aat.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33282cf370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f349ceb2fd0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 33s - loss: 950.6331 - loglik: -9.4936e+02 - logprior: -1.2764e+00
Epoch 2/10
43/43 - 29s - loss: 827.5102 - loglik: -8.2583e+02 - logprior: -1.6807e+00
Epoch 3/10
43/43 - 29s - loss: 814.1802 - loglik: -8.1250e+02 - logprior: -1.6835e+00
Epoch 4/10
43/43 - 28s - loss: 807.2649 - loglik: -8.0562e+02 - logprior: -1.6456e+00
Epoch 5/10
43/43 - 29s - loss: 805.7651 - loglik: -8.0410e+02 - logprior: -1.6628e+00
Epoch 6/10
43/43 - 29s - loss: 804.1808 - loglik: -8.0246e+02 - logprior: -1.7230e+00
Epoch 7/10
43/43 - 29s - loss: 801.9455 - loglik: -8.0025e+02 - logprior: -1.6996e+00
Epoch 8/10
43/43 - 29s - loss: 801.3380 - loglik: -7.9965e+02 - logprior: -1.6899e+00
Epoch 9/10
43/43 - 29s - loss: 800.3398 - loglik: -7.9866e+02 - logprior: -1.6838e+00
Epoch 10/10
43/43 - 29s - loss: 801.0166 - loglik: -7.9929e+02 - logprior: -1.7303e+00
Fitted a model with MAP estimate = -814.8267
expansions: [(8, 1), (13, 1), (16, 1), (21, 1), (22, 1), (23, 3), (24, 2), (29, 1), (32, 1), (39, 1), (41, 1), (42, 1), (45, 2), (46, 1), (48, 2), (56, 1), (59, 1), (61, 1), (77, 1), (79, 1), (80, 1), (81, 1), (90, 1), (91, 2), (92, 1), (95, 1), (97, 1), (98, 1), (103, 2), (119, 1), (120, 1), (121, 1), (124, 1), (130, 1), (131, 2), (142, 1), (147, 1), (149, 1), (152, 2), (155, 2), (156, 2), (167, 1), (180, 1), (182, 2), (184, 2), (185, 2), (187, 1), (200, 2), (203, 1), (204, 3), (205, 2), (206, 1), (209, 1), (218, 1), (219, 1), (220, 2), (224, 2), (225, 2), (238, 2), (242, 1), (244, 1), (245, 1), (249, 1), (260, 2), (269, 2), (270, 2), (271, 2)]
discards: [0]
Fitting a model of length 371 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 48s - loss: 841.9901 - loglik: -8.3990e+02 - logprior: -2.0938e+00
Epoch 2/2
43/43 - 45s - loss: 787.4367 - loglik: -7.8639e+02 - logprior: -1.0486e+00
Fitted a model with MAP estimate = -782.1117
expansions: [(0, 2)]
discards: [  0  26  32  60  65 117 136 171 205 236 241 261 269 293 299 300 317 359
 361]
Fitting a model of length 354 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 45s - loss: 797.6917 - loglik: -7.9668e+02 - logprior: -1.0093e+00
Epoch 2/2
43/43 - 42s - loss: 784.9238 - loglik: -7.8474e+02 - logprior: -1.8257e-01
Fitted a model with MAP estimate = -782.4378
expansions: []
discards: [0 1]
Fitting a model of length 352 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 60s - loss: 774.9879 - loglik: -7.7417e+02 - logprior: -8.1702e-01
Epoch 2/10
61/61 - 57s - loss: 761.3954 - loglik: -7.6115e+02 - logprior: -2.4266e-01
Epoch 3/10
61/61 - 57s - loss: 754.4219 - loglik: -7.5430e+02 - logprior: -1.2105e-01
Epoch 4/10
61/61 - 57s - loss: 746.5250 - loglik: -7.4648e+02 - logprior: -4.5766e-02
Epoch 5/10
61/61 - 57s - loss: 741.8681 - loglik: -7.4188e+02 - logprior: 0.0154
Epoch 6/10
61/61 - 58s - loss: 738.7324 - loglik: -7.3881e+02 - logprior: 0.0796
Epoch 7/10
61/61 - 57s - loss: 737.5872 - loglik: -7.3770e+02 - logprior: 0.1136
Epoch 8/10
61/61 - 57s - loss: 734.7094 - loglik: -7.3491e+02 - logprior: 0.2047
Epoch 9/10
61/61 - 58s - loss: 734.4561 - loglik: -7.3474e+02 - logprior: 0.2828
Epoch 10/10
61/61 - 57s - loss: 733.1978 - loglik: -7.3357e+02 - logprior: 0.3714
Fitted a model with MAP estimate = -732.4192
Time for alignment: 1347.0970
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 33s - loss: 949.7044 - loglik: -9.4844e+02 - logprior: -1.2641e+00
Epoch 2/10
43/43 - 29s - loss: 828.3597 - loglik: -8.2672e+02 - logprior: -1.6408e+00
Epoch 3/10
43/43 - 29s - loss: 813.7322 - loglik: -8.1208e+02 - logprior: -1.6504e+00
Epoch 4/10
43/43 - 29s - loss: 808.4143 - loglik: -8.0682e+02 - logprior: -1.5898e+00
Epoch 5/10
43/43 - 29s - loss: 804.5344 - loglik: -8.0295e+02 - logprior: -1.5885e+00
Epoch 6/10
43/43 - 29s - loss: 802.7795 - loglik: -8.0113e+02 - logprior: -1.6458e+00
Epoch 7/10
43/43 - 29s - loss: 801.8298 - loglik: -8.0014e+02 - logprior: -1.6916e+00
Epoch 8/10
43/43 - 29s - loss: 801.6588 - loglik: -8.0004e+02 - logprior: -1.6223e+00
Epoch 9/10
43/43 - 29s - loss: 800.7669 - loglik: -7.9912e+02 - logprior: -1.6461e+00
Epoch 10/10
43/43 - 29s - loss: 800.2723 - loglik: -7.9858e+02 - logprior: -1.6907e+00
Fitted a model with MAP estimate = -815.0480
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (24, 1), (25, 2), (29, 1), (30, 1), (40, 1), (42, 1), (43, 1), (46, 2), (47, 1), (57, 1), (60, 1), (61, 1), (62, 1), (65, 2), (80, 2), (81, 1), (82, 1), (86, 2), (90, 1), (91, 1), (93, 1), (95, 1), (97, 1), (98, 1), (104, 2), (121, 1), (122, 1), (125, 1), (128, 2), (130, 1), (133, 2), (148, 2), (149, 1), (152, 2), (155, 3), (157, 2), (168, 1), (180, 1), (182, 1), (184, 2), (186, 1), (187, 1), (188, 1), (204, 1), (205, 2), (206, 3), (207, 1), (208, 2), (209, 1), (218, 1), (219, 1), (220, 1), (224, 2), (236, 1), (238, 2), (242, 1), (245, 1), (248, 1), (249, 1), (260, 2), (269, 2), (270, 2), (271, 1)]
discards: []
Fitting a model of length 370 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 47s - loss: 841.0002 - loglik: -8.3915e+02 - logprior: -1.8502e+00
Epoch 2/2
43/43 - 45s - loss: 786.6709 - loglik: -7.8588e+02 - logprior: -7.9212e-01
Fitted a model with MAP estimate = -781.2846
expansions: []
discards: [  0  32  60  85 103 112 138 168 176 200 204 209 242 268 269 272 277 301
 358]
Fitting a model of length 351 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 44s - loss: 799.6130 - loglik: -7.9795e+02 - logprior: -1.6674e+00
Epoch 2/2
43/43 - 41s - loss: 786.9085 - loglik: -7.8672e+02 - logprior: -1.9165e-01
Fitted a model with MAP estimate = -783.4611
expansions: [(0, 1)]
discards: [0]
Fitting a model of length 351 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 60s - loss: 774.8919 - loglik: -7.7433e+02 - logprior: -5.5909e-01
Epoch 2/10
61/61 - 57s - loss: 761.3746 - loglik: -7.6107e+02 - logprior: -2.9972e-01
Epoch 3/10
61/61 - 57s - loss: 753.8164 - loglik: -7.5356e+02 - logprior: -2.5320e-01
Epoch 4/10
61/61 - 58s - loss: 747.9279 - loglik: -7.4780e+02 - logprior: -1.2936e-01
Epoch 5/10
61/61 - 57s - loss: 742.5871 - loglik: -7.4251e+02 - logprior: -7.4173e-02
Epoch 6/10
61/61 - 57s - loss: 738.1318 - loglik: -7.3812e+02 - logprior: -1.3679e-02
Epoch 7/10
61/61 - 58s - loss: 736.6393 - loglik: -7.3669e+02 - logprior: 0.0545
Epoch 8/10
61/61 - 57s - loss: 736.4416 - loglik: -7.3659e+02 - logprior: 0.1449
Epoch 9/10
61/61 - 58s - loss: 734.9883 - loglik: -7.3518e+02 - logprior: 0.1908
Epoch 10/10
61/61 - 57s - loss: 733.4987 - loglik: -7.3379e+02 - logprior: 0.2884
Fitted a model with MAP estimate = -732.7793
Time for alignment: 1342.3689
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 32s - loss: 948.7759 - loglik: -9.4749e+02 - logprior: -1.2855e+00
Epoch 2/10
43/43 - 29s - loss: 826.2610 - loglik: -8.2456e+02 - logprior: -1.6966e+00
Epoch 3/10
43/43 - 29s - loss: 812.7189 - loglik: -8.1099e+02 - logprior: -1.7253e+00
Epoch 4/10
43/43 - 29s - loss: 805.8298 - loglik: -8.0413e+02 - logprior: -1.6949e+00
Epoch 5/10
43/43 - 30s - loss: 803.2654 - loglik: -8.0157e+02 - logprior: -1.6953e+00
Epoch 6/10
43/43 - 30s - loss: 802.7705 - loglik: -8.0108e+02 - logprior: -1.6911e+00
Epoch 7/10
43/43 - 29s - loss: 799.8555 - loglik: -7.9815e+02 - logprior: -1.7017e+00
Epoch 8/10
43/43 - 29s - loss: 800.6642 - loglik: -7.9893e+02 - logprior: -1.7311e+00
Fitted a model with MAP estimate = -809.6999
expansions: [(8, 1), (13, 1), (16, 1), (21, 1), (22, 1), (24, 1), (25, 2), (26, 1), (30, 1), (40, 1), (42, 1), (43, 1), (46, 2), (47, 1), (57, 1), (60, 1), (61, 1), (62, 1), (65, 2), (80, 2), (81, 1), (82, 1), (86, 2), (88, 1), (91, 1), (96, 1), (98, 1), (99, 1), (104, 2), (120, 1), (121, 1), (122, 1), (125, 1), (131, 1), (132, 2), (143, 1), (148, 1), (150, 1), (153, 2), (156, 2), (157, 2), (168, 1), (181, 1), (183, 3), (184, 1), (186, 1), (187, 1), (188, 1), (204, 1), (205, 2), (206, 3), (207, 1), (208, 2), (209, 1), (219, 1), (221, 1), (225, 2), (226, 2), (236, 1), (238, 2), (242, 1), (245, 1), (248, 1), (249, 1), (260, 2), (269, 3), (270, 2), (271, 1)]
discards: [1]
Fitting a model of length 369 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 52s - loss: 836.0211 - loglik: -8.3456e+02 - logprior: -1.4633e+00
Epoch 2/2
43/43 - 55s - loss: 787.3733 - loglik: -7.8657e+02 - logprior: -7.9975e-01
Fitted a model with MAP estimate = -780.4571
expansions: [(293, 1)]
discards: [ 32  59  84 102 111 136 171 205 237 266 268 269 274 297 298 356]
Fitting a model of length 354 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 65s - loss: 796.7709 - loglik: -7.9578e+02 - logprior: -9.9391e-01
Epoch 2/2
43/43 - 66s - loss: 784.8660 - loglik: -7.8461e+02 - logprior: -2.5490e-01
Fitted a model with MAP estimate = -780.7349
expansions: []
discards: [281 340]
Fitting a model of length 352 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 95s - loss: 774.9565 - loglik: -7.7436e+02 - logprior: -5.9181e-01
Epoch 2/10
61/61 - 92s - loss: 760.6849 - loglik: -7.6037e+02 - logprior: -3.1565e-01
Epoch 3/10
61/61 - 108s - loss: 753.7936 - loglik: -7.5352e+02 - logprior: -2.7083e-01
Epoch 4/10
61/61 - 113s - loss: 745.6596 - loglik: -7.4549e+02 - logprior: -1.7037e-01
Epoch 5/10
61/61 - 115s - loss: 743.7420 - loglik: -7.4366e+02 - logprior: -8.2292e-02
Epoch 6/10
61/61 - 107s - loss: 737.9909 - loglik: -7.3793e+02 - logprior: -6.4580e-02
Epoch 7/10
61/61 - 108s - loss: 737.7562 - loglik: -7.3779e+02 - logprior: 0.0368
Epoch 8/10
61/61 - 108s - loss: 735.1338 - loglik: -7.3523e+02 - logprior: 0.1013
Epoch 9/10
61/61 - 100s - loss: 733.9976 - loglik: -7.3418e+02 - logprior: 0.1830
Epoch 10/10
61/61 - 101s - loss: 734.3231 - loglik: -7.3455e+02 - logprior: 0.2263
Fitted a model with MAP estimate = -732.7807
Time for alignment: 1922.5987
Computed alignments with likelihoods: ['-732.4192', '-732.7793', '-732.7807']
Best model has likelihood: -732.4192
SP score = 0.8113
Training of 3 independent models on file ins.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f332049d880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32a84d8a90>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 250.2644 - loglik: -1.9360e+02 - logprior: -5.6660e+01
Epoch 2/10
10/10 - 1s - loss: 187.8241 - loglik: -1.7226e+02 - logprior: -1.5567e+01
Epoch 3/10
10/10 - 1s - loss: 162.7469 - loglik: -1.5519e+02 - logprior: -7.5598e+00
Epoch 4/10
10/10 - 1s - loss: 151.5969 - loglik: -1.4706e+02 - logprior: -4.5381e+00
Epoch 5/10
10/10 - 1s - loss: 148.7463 - loglik: -1.4559e+02 - logprior: -3.1601e+00
Epoch 6/10
10/10 - 1s - loss: 145.9663 - loglik: -1.4368e+02 - logprior: -2.2883e+00
Epoch 7/10
10/10 - 1s - loss: 145.7670 - loglik: -1.4402e+02 - logprior: -1.7444e+00
Epoch 8/10
10/10 - 1s - loss: 144.8896 - loglik: -1.4338e+02 - logprior: -1.5072e+00
Epoch 9/10
10/10 - 1s - loss: 144.2173 - loglik: -1.4285e+02 - logprior: -1.3712e+00
Epoch 10/10
10/10 - 1s - loss: 144.7235 - loglik: -1.4350e+02 - logprior: -1.2227e+00
Fitted a model with MAP estimate = -144.2038
expansions: [(11, 1), (12, 2)]
discards: [0]
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 209.4571 - loglik: -1.4535e+02 - logprior: -6.4104e+01
Epoch 2/2
10/10 - 1s - loss: 170.0158 - loglik: -1.4317e+02 - logprior: -2.6844e+01
Fitted a model with MAP estimate = -163.2074
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 193.6888 - loglik: -1.4140e+02 - logprior: -5.2287e+01
Epoch 2/2
10/10 - 1s - loss: 155.2029 - loglik: -1.4084e+02 - logprior: -1.4367e+01
Fitted a model with MAP estimate = -149.3597
expansions: []
discards: [0]
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 203.4642 - loglik: -1.4136e+02 - logprior: -6.2103e+01
Epoch 2/10
10/10 - 1s - loss: 162.2979 - loglik: -1.4134e+02 - logprior: -2.0957e+01
Epoch 3/10
10/10 - 1s - loss: 149.6918 - loglik: -1.4153e+02 - logprior: -8.1624e+00
Epoch 4/10
10/10 - 1s - loss: 144.0385 - loglik: -1.4034e+02 - logprior: -3.6975e+00
Epoch 5/10
10/10 - 1s - loss: 143.3605 - loglik: -1.4145e+02 - logprior: -1.9070e+00
Epoch 6/10
10/10 - 1s - loss: 140.6893 - loglik: -1.3963e+02 - logprior: -1.0605e+00
Epoch 7/10
10/10 - 1s - loss: 140.8364 - loglik: -1.4023e+02 - logprior: -6.0421e-01
Fitted a model with MAP estimate = -140.5624
Time for alignment: 40.3462
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 250.3512 - loglik: -1.9368e+02 - logprior: -5.6667e+01
Epoch 2/10
10/10 - 1s - loss: 187.5909 - loglik: -1.7201e+02 - logprior: -1.5581e+01
Epoch 3/10
10/10 - 1s - loss: 162.2861 - loglik: -1.5476e+02 - logprior: -7.5258e+00
Epoch 4/10
10/10 - 1s - loss: 152.6711 - loglik: -1.4816e+02 - logprior: -4.5075e+00
Epoch 5/10
10/10 - 1s - loss: 149.0819 - loglik: -1.4598e+02 - logprior: -3.0987e+00
Epoch 6/10
10/10 - 1s - loss: 145.7959 - loglik: -1.4349e+02 - logprior: -2.3071e+00
Epoch 7/10
10/10 - 1s - loss: 145.8653 - loglik: -1.4405e+02 - logprior: -1.8200e+00
Fitted a model with MAP estimate = -145.0977
expansions: [(11, 1), (12, 2)]
discards: [0]
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 208.8317 - loglik: -1.4483e+02 - logprior: -6.4006e+01
Epoch 2/2
10/10 - 1s - loss: 169.5535 - loglik: -1.4272e+02 - logprior: -2.6833e+01
Fitted a model with MAP estimate = -162.9483
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 193.3345 - loglik: -1.4138e+02 - logprior: -5.1957e+01
Epoch 2/2
10/10 - 1s - loss: 154.8980 - loglik: -1.4055e+02 - logprior: -1.4347e+01
Fitted a model with MAP estimate = -149.3389
expansions: []
discards: [0]
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 203.7441 - loglik: -1.4145e+02 - logprior: -6.2298e+01
Epoch 2/10
10/10 - 1s - loss: 162.8031 - loglik: -1.4132e+02 - logprior: -2.1485e+01
Epoch 3/10
10/10 - 1s - loss: 149.3352 - loglik: -1.4088e+02 - logprior: -8.4565e+00
Epoch 4/10
10/10 - 1s - loss: 144.9371 - loglik: -1.4115e+02 - logprior: -3.7841e+00
Epoch 5/10
10/10 - 1s - loss: 142.6663 - loglik: -1.4070e+02 - logprior: -1.9647e+00
Epoch 6/10
10/10 - 1s - loss: 140.9214 - loglik: -1.3981e+02 - logprior: -1.1129e+00
Epoch 7/10
10/10 - 1s - loss: 141.4478 - loglik: -1.4079e+02 - logprior: -6.5675e-01
Fitted a model with MAP estimate = -140.6209
Time for alignment: 37.6912
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 249.8772 - loglik: -1.9322e+02 - logprior: -5.6661e+01
Epoch 2/10
10/10 - 1s - loss: 187.8511 - loglik: -1.7229e+02 - logprior: -1.5558e+01
Epoch 3/10
10/10 - 1s - loss: 161.6222 - loglik: -1.5406e+02 - logprior: -7.5628e+00
Epoch 4/10
10/10 - 1s - loss: 153.1329 - loglik: -1.4867e+02 - logprior: -4.4616e+00
Epoch 5/10
10/10 - 1s - loss: 148.8168 - loglik: -1.4590e+02 - logprior: -2.9155e+00
Epoch 6/10
10/10 - 1s - loss: 147.8373 - loglik: -1.4586e+02 - logprior: -1.9750e+00
Epoch 7/10
10/10 - 1s - loss: 146.5589 - loglik: -1.4513e+02 - logprior: -1.4313e+00
Epoch 8/10
10/10 - 1s - loss: 146.7870 - loglik: -1.4560e+02 - logprior: -1.1872e+00
Fitted a model with MAP estimate = -146.1550
expansions: []
discards: [0]
Fitting a model of length 46 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 211.8059 - loglik: -1.4767e+02 - logprior: -6.4131e+01
Epoch 2/2
10/10 - 1s - loss: 173.8810 - loglik: -1.4693e+02 - logprior: -2.6954e+01
Fitted a model with MAP estimate = -167.6142
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 197.4637 - loglik: -1.4523e+02 - logprior: -5.2230e+01
Epoch 2/2
10/10 - 1s - loss: 159.9090 - loglik: -1.4543e+02 - logprior: -1.4481e+01
Fitted a model with MAP estimate = -154.1057
expansions: []
discards: [0 1]
Fitting a model of length 45 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 210.3234 - loglik: -1.4703e+02 - logprior: -6.3292e+01
Epoch 2/10
10/10 - 1s - loss: 172.3761 - loglik: -1.4689e+02 - logprior: -2.5482e+01
Epoch 3/10
10/10 - 1s - loss: 159.3846 - loglik: -1.4652e+02 - logprior: -1.2868e+01
Epoch 4/10
10/10 - 1s - loss: 150.9178 - loglik: -1.4612e+02 - logprior: -4.7968e+00
Epoch 5/10
10/10 - 1s - loss: 148.4189 - loglik: -1.4619e+02 - logprior: -2.2309e+00
Epoch 6/10
10/10 - 1s - loss: 147.5858 - loglik: -1.4630e+02 - logprior: -1.2861e+00
Epoch 7/10
10/10 - 1s - loss: 146.4630 - loglik: -1.4563e+02 - logprior: -8.2893e-01
Epoch 8/10
10/10 - 1s - loss: 146.2177 - loglik: -1.4570e+02 - logprior: -5.2105e-01
Epoch 9/10
10/10 - 1s - loss: 146.2712 - loglik: -1.4599e+02 - logprior: -2.8292e-01
Fitted a model with MAP estimate = -145.7981
Time for alignment: 38.6833
Computed alignments with likelihoods: ['-140.5624', '-140.6209', '-145.7981']
Best model has likelihood: -140.5624
SP score = 0.9459
Training of 3 independent models on file sti.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b8bc1370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32a91fc280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 538.4155 - loglik: -4.7168e+02 - logprior: -6.6740e+01
Epoch 2/10
10/10 - 2s - loss: 439.0878 - loglik: -4.2584e+02 - logprior: -1.3252e+01
Epoch 3/10
10/10 - 2s - loss: 386.7245 - loglik: -3.8246e+02 - logprior: -4.2695e+00
Epoch 4/10
10/10 - 3s - loss: 356.6242 - loglik: -3.5511e+02 - logprior: -1.5171e+00
Epoch 5/10
10/10 - 3s - loss: 341.8529 - loglik: -3.4173e+02 - logprior: -1.2306e-01
Epoch 6/10
10/10 - 3s - loss: 334.9639 - loglik: -3.3567e+02 - logprior: 0.7080
Epoch 7/10
10/10 - 3s - loss: 331.3034 - loglik: -3.3246e+02 - logprior: 1.1561
Epoch 8/10
10/10 - 3s - loss: 329.0365 - loglik: -3.3058e+02 - logprior: 1.5455
Epoch 9/10
10/10 - 3s - loss: 328.1679 - loglik: -3.2999e+02 - logprior: 1.8227
Epoch 10/10
10/10 - 3s - loss: 327.0885 - loglik: -3.2912e+02 - logprior: 2.0315
Fitted a model with MAP estimate = -326.7975
expansions: [(11, 3), (12, 2), (16, 3), (24, 1), (26, 1), (27, 1), (28, 1), (36, 1), (38, 1), (40, 2), (51, 1), (55, 1), (66, 1), (70, 1), (77, 2), (79, 2), (80, 1), (89, 1), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (127, 1), (128, 2)]
discards: [0]
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 404.4964 - loglik: -3.2934e+02 - logprior: -7.5156e+01
Epoch 2/2
10/10 - 4s - loss: 338.6366 - loglik: -3.1150e+02 - logprior: -2.7134e+01
Fitted a model with MAP estimate = -326.3694
expansions: [(0, 2), (64, 1), (128, 1)]
discards: [ 0 11 47]
Fitting a model of length 174 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 364.3157 - loglik: -3.0610e+02 - logprior: -5.8220e+01
Epoch 2/2
10/10 - 4s - loss: 310.7017 - loglik: -3.0055e+02 - logprior: -1.0156e+01
Fitted a model with MAP estimate = -301.6359
expansions: [(86, 1)]
discards: [  0 142]
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 375.8255 - loglik: -3.0411e+02 - logprior: -7.1713e+01
Epoch 2/10
10/10 - 4s - loss: 319.7978 - loglik: -3.0126e+02 - logprior: -1.8541e+01
Epoch 3/10
10/10 - 4s - loss: 300.2107 - loglik: -2.9844e+02 - logprior: -1.7682e+00
Epoch 4/10
10/10 - 4s - loss: 292.8554 - loglik: -2.9721e+02 - logprior: 4.3516
Epoch 5/10
10/10 - 4s - loss: 289.6328 - loglik: -2.9662e+02 - logprior: 6.9863
Epoch 6/10
10/10 - 5s - loss: 287.8040 - loglik: -2.9625e+02 - logprior: 8.4472
Epoch 7/10
10/10 - 4s - loss: 286.7998 - loglik: -2.9623e+02 - logprior: 9.4317
Epoch 8/10
10/10 - 4s - loss: 285.8228 - loglik: -2.9606e+02 - logprior: 10.2405
Epoch 9/10
10/10 - 4s - loss: 285.4419 - loglik: -2.9640e+02 - logprior: 10.9604
Epoch 10/10
10/10 - 4s - loss: 284.8984 - loglik: -2.9649e+02 - logprior: 11.5930
Fitted a model with MAP estimate = -284.5081
Time for alignment: 97.5601
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 538.4891 - loglik: -4.7175e+02 - logprior: -6.6743e+01
Epoch 2/10
10/10 - 3s - loss: 438.1358 - loglik: -4.2489e+02 - logprior: -1.3248e+01
Epoch 3/10
10/10 - 3s - loss: 382.9890 - loglik: -3.7881e+02 - logprior: -4.1811e+00
Epoch 4/10
10/10 - 3s - loss: 354.4122 - loglik: -3.5303e+02 - logprior: -1.3786e+00
Epoch 5/10
10/10 - 3s - loss: 342.8449 - loglik: -3.4284e+02 - logprior: -3.5185e-04
Epoch 6/10
10/10 - 3s - loss: 337.2956 - loglik: -3.3809e+02 - logprior: 0.7938
Epoch 7/10
10/10 - 3s - loss: 334.5229 - loglik: -3.3590e+02 - logprior: 1.3746
Epoch 8/10
10/10 - 3s - loss: 332.1368 - loglik: -3.3393e+02 - logprior: 1.7957
Epoch 9/10
10/10 - 3s - loss: 331.4264 - loglik: -3.3349e+02 - logprior: 2.0623
Epoch 10/10
10/10 - 3s - loss: 330.5195 - loglik: -3.3275e+02 - logprior: 2.2271
Fitted a model with MAP estimate = -330.1281
expansions: [(11, 2), (13, 2), (16, 3), (26, 1), (27, 1), (28, 1), (36, 2), (38, 2), (39, 2), (61, 1), (62, 1), (69, 1), (79, 4), (80, 2), (89, 1), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 407.8071 - loglik: -3.3276e+02 - logprior: -7.5043e+01
Epoch 2/2
10/10 - 4s - loss: 341.0279 - loglik: -3.1371e+02 - logprior: -2.7314e+01
Fitted a model with MAP estimate = -328.4773
expansions: [(0, 1), (19, 1), (25, 1), (128, 1)]
discards: [ 0 14 45 50 52]
Fitting a model of length 172 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 366.2512 - loglik: -3.0811e+02 - logprior: -5.8146e+01
Epoch 2/2
10/10 - 4s - loss: 310.7103 - loglik: -3.0068e+02 - logprior: -1.0031e+01
Fitted a model with MAP estimate = -301.7675
expansions: []
discards: [140 161]
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 358.8596 - loglik: -3.0213e+02 - logprior: -5.6732e+01
Epoch 2/10
10/10 - 4s - loss: 307.8584 - loglik: -2.9870e+02 - logprior: -9.1573e+00
Epoch 3/10
10/10 - 4s - loss: 296.8440 - loglik: -2.9758e+02 - logprior: 0.7338
Epoch 4/10
10/10 - 4s - loss: 291.8127 - loglik: -2.9673e+02 - logprior: 4.9179
Epoch 5/10
10/10 - 5s - loss: 289.1062 - loglik: -2.9645e+02 - logprior: 7.3409
Epoch 6/10
10/10 - 5s - loss: 288.0581 - loglik: -2.9692e+02 - logprior: 8.8592
Epoch 7/10
10/10 - 4s - loss: 286.8072 - loglik: -2.9669e+02 - logprior: 9.8875
Epoch 8/10
10/10 - 5s - loss: 286.3140 - loglik: -2.9699e+02 - logprior: 10.6793
Epoch 9/10
10/10 - 5s - loss: 285.6656 - loglik: -2.9697e+02 - logprior: 11.3052
Epoch 10/10
10/10 - 5s - loss: 284.4616 - loglik: -2.9625e+02 - logprior: 11.7911
Fitted a model with MAP estimate = -284.5794
Time for alignment: 109.7596
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 538.3916 - loglik: -4.7165e+02 - logprior: -6.6742e+01
Epoch 2/10
10/10 - 3s - loss: 438.8026 - loglik: -4.2554e+02 - logprior: -1.3264e+01
Epoch 3/10
10/10 - 3s - loss: 385.7558 - loglik: -3.8144e+02 - logprior: -4.3139e+00
Epoch 4/10
10/10 - 3s - loss: 355.1340 - loglik: -3.5366e+02 - logprior: -1.4690e+00
Epoch 5/10
10/10 - 3s - loss: 342.1168 - loglik: -3.4210e+02 - logprior: -1.8540e-02
Epoch 6/10
10/10 - 3s - loss: 334.7879 - loglik: -3.3554e+02 - logprior: 0.7538
Epoch 7/10
10/10 - 3s - loss: 331.2669 - loglik: -3.3247e+02 - logprior: 1.2007
Epoch 8/10
10/10 - 3s - loss: 329.2623 - loglik: -3.3094e+02 - logprior: 1.6733
Epoch 9/10
10/10 - 3s - loss: 328.0876 - loglik: -3.3008e+02 - logprior: 1.9916
Epoch 10/10
10/10 - 4s - loss: 326.9770 - loglik: -3.2916e+02 - logprior: 2.1819
Fitted a model with MAP estimate = -326.5469
expansions: [(11, 2), (13, 2), (19, 2), (26, 1), (27, 1), (28, 1), (36, 2), (38, 2), (39, 2), (55, 1), (66, 1), (68, 1), (71, 2), (77, 2), (79, 2), (80, 1), (89, 1), (90, 2), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Fitting a model of length 175 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 404.9605 - loglik: -3.2992e+02 - logprior: -7.5041e+01
Epoch 2/2
10/10 - 5s - loss: 338.7462 - loglik: -3.1137e+02 - logprior: -2.7381e+01
Fitted a model with MAP estimate = -326.9691
expansions: [(0, 2), (130, 1)]
discards: [  0  14  44  49  50  88 116]
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 365.7933 - loglik: -3.0746e+02 - logprior: -5.8332e+01
Epoch 2/2
10/10 - 5s - loss: 312.1317 - loglik: -3.0187e+02 - logprior: -1.0265e+01
Fitted a model with MAP estimate = -303.4946
expansions: []
discards: [  0 139 161]
Fitting a model of length 168 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 377.5408 - loglik: -3.0628e+02 - logprior: -7.1259e+01
Epoch 2/10
10/10 - 4s - loss: 320.2328 - loglik: -3.0287e+02 - logprior: -1.7365e+01
Epoch 3/10
10/10 - 4s - loss: 302.6807 - loglik: -3.0109e+02 - logprior: -1.5940e+00
Epoch 4/10
10/10 - 5s - loss: 295.5525 - loglik: -2.9951e+02 - logprior: 3.9543
Epoch 5/10
10/10 - 5s - loss: 292.1793 - loglik: -2.9867e+02 - logprior: 6.4920
Epoch 6/10
10/10 - 5s - loss: 290.3126 - loglik: -2.9825e+02 - logprior: 7.9340
Epoch 7/10
10/10 - 5s - loss: 289.4527 - loglik: -2.9835e+02 - logprior: 8.8979
Epoch 8/10
10/10 - 5s - loss: 288.4857 - loglik: -2.9819e+02 - logprior: 9.7078
Epoch 9/10
10/10 - 5s - loss: 288.3189 - loglik: -2.9875e+02 - logprior: 10.4318
Epoch 10/10
10/10 - 5s - loss: 287.4037 - loglik: -2.9846e+02 - logprior: 11.0568
Fitted a model with MAP estimate = -287.1598
Time for alignment: 116.7497
Computed alignments with likelihoods: ['-284.5081', '-284.5794', '-287.1598']
Best model has likelihood: -284.5081
SP score = 0.7886
Training of 3 independent models on file glob.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33281f96d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34eaf9b700>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 292.4131 - loglik: -2.8249e+02 - logprior: -9.9184e+00
Epoch 2/10
12/12 - 2s - loss: 253.6634 - loglik: -2.5126e+02 - logprior: -2.4047e+00
Epoch 3/10
12/12 - 2s - loss: 225.3435 - loglik: -2.2365e+02 - logprior: -1.6900e+00
Epoch 4/10
12/12 - 2s - loss: 215.5037 - loglik: -2.1383e+02 - logprior: -1.6759e+00
Epoch 5/10
12/12 - 2s - loss: 210.6734 - loglik: -2.0898e+02 - logprior: -1.6945e+00
Epoch 6/10
12/12 - 2s - loss: 208.4379 - loglik: -2.0679e+02 - logprior: -1.6433e+00
Epoch 7/10
12/12 - 2s - loss: 207.2693 - loglik: -2.0565e+02 - logprior: -1.6150e+00
Epoch 8/10
12/12 - 2s - loss: 206.4507 - loglik: -2.0481e+02 - logprior: -1.6399e+00
Epoch 9/10
12/12 - 2s - loss: 206.2071 - loglik: -2.0456e+02 - logprior: -1.6482e+00
Epoch 10/10
12/12 - 2s - loss: 205.3544 - loglik: -2.0372e+02 - logprior: -1.6333e+00
Fitted a model with MAP estimate = -205.6038
expansions: [(6, 3), (10, 4), (13, 1), (21, 1), (29, 1), (36, 4), (49, 1), (50, 3), (52, 1), (58, 2), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 219.6745 - loglik: -2.0822e+02 - logprior: -1.1455e+01
Epoch 2/2
12/12 - 2s - loss: 201.2214 - loglik: -1.9644e+02 - logprior: -4.7840e+00
Fitted a model with MAP estimate = -197.2051
expansions: [(0, 6), (47, 1)]
discards: [ 0 12 77 80]
Fitting a model of length 110 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 204.0887 - loglik: -1.9487e+02 - logprior: -9.2206e+00
Epoch 2/2
12/12 - 2s - loss: 192.3973 - loglik: -1.9003e+02 - logprior: -2.3630e+00
Fitted a model with MAP estimate = -190.1352
expansions: []
discards: [1 2 3 4 5]
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 199.7442 - loglik: -1.9093e+02 - logprior: -8.8187e+00
Epoch 2/10
12/12 - 3s - loss: 191.4043 - loglik: -1.8914e+02 - logprior: -2.2687e+00
Epoch 3/10
12/12 - 2s - loss: 189.2234 - loglik: -1.8772e+02 - logprior: -1.4995e+00
Epoch 4/10
12/12 - 2s - loss: 186.4489 - loglik: -1.8529e+02 - logprior: -1.1628e+00
Epoch 5/10
12/12 - 2s - loss: 185.5435 - loglik: -1.8455e+02 - logprior: -9.8925e-01
Epoch 6/10
12/12 - 2s - loss: 184.1614 - loglik: -1.8323e+02 - logprior: -9.3199e-01
Epoch 7/10
12/12 - 2s - loss: 183.1149 - loglik: -1.8221e+02 - logprior: -9.0355e-01
Epoch 8/10
12/12 - 2s - loss: 182.3833 - loglik: -1.8149e+02 - logprior: -8.8948e-01
Epoch 9/10
12/12 - 2s - loss: 181.9588 - loglik: -1.8109e+02 - logprior: -8.7335e-01
Epoch 10/10
12/12 - 2s - loss: 181.2151 - loglik: -1.8039e+02 - logprior: -8.2541e-01
Fitted a model with MAP estimate = -181.5619
Time for alignment: 68.1566
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 292.4869 - loglik: -2.8257e+02 - logprior: -9.9179e+00
Epoch 2/10
12/12 - 1s - loss: 253.9652 - loglik: -2.5156e+02 - logprior: -2.4036e+00
Epoch 3/10
12/12 - 1s - loss: 225.8126 - loglik: -2.2411e+02 - logprior: -1.7063e+00
Epoch 4/10
12/12 - 2s - loss: 216.2164 - loglik: -2.1451e+02 - logprior: -1.7052e+00
Epoch 5/10
12/12 - 2s - loss: 211.3516 - loglik: -2.0967e+02 - logprior: -1.6831e+00
Epoch 6/10
12/12 - 2s - loss: 208.6768 - loglik: -2.0704e+02 - logprior: -1.6319e+00
Epoch 7/10
12/12 - 2s - loss: 206.7813 - loglik: -2.0519e+02 - logprior: -1.5935e+00
Epoch 8/10
12/12 - 2s - loss: 206.4783 - loglik: -2.0487e+02 - logprior: -1.6070e+00
Epoch 9/10
12/12 - 2s - loss: 206.1162 - loglik: -2.0449e+02 - logprior: -1.6256e+00
Epoch 10/10
12/12 - 2s - loss: 205.3565 - loglik: -2.0373e+02 - logprior: -1.6275e+00
Fitted a model with MAP estimate = -205.6301
expansions: [(8, 1), (10, 4), (11, 2), (20, 1), (29, 1), (36, 4), (49, 1), (50, 3), (52, 1), (58, 2), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 221.3215 - loglik: -2.0989e+02 - logprior: -1.1431e+01
Epoch 2/2
12/12 - 2s - loss: 201.2838 - loglik: -1.9651e+02 - logprior: -4.7780e+00
Fitted a model with MAP estimate = -197.2299
expansions: [(0, 5)]
discards: [ 0 76 79]
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 203.6672 - loglik: -1.9450e+02 - logprior: -9.1640e+00
Epoch 2/2
12/12 - 2s - loss: 192.2961 - loglik: -1.8997e+02 - logprior: -2.3253e+00
Fitted a model with MAP estimate = -190.3558
expansions: [(50, 1)]
discards: [1 2 3 4]
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 199.6214 - loglik: -1.9085e+02 - logprior: -8.7740e+00
Epoch 2/10
12/12 - 2s - loss: 191.9123 - loglik: -1.8970e+02 - logprior: -2.2080e+00
Epoch 3/10
12/12 - 2s - loss: 188.6074 - loglik: -1.8720e+02 - logprior: -1.4106e+00
Epoch 4/10
12/12 - 2s - loss: 187.9623 - loglik: -1.8685e+02 - logprior: -1.1105e+00
Epoch 5/10
12/12 - 2s - loss: 185.3092 - loglik: -1.8440e+02 - logprior: -9.1169e-01
Epoch 6/10
12/12 - 2s - loss: 184.2709 - loglik: -1.8341e+02 - logprior: -8.6438e-01
Epoch 7/10
12/12 - 2s - loss: 182.5639 - loglik: -1.8173e+02 - logprior: -8.3374e-01
Epoch 8/10
12/12 - 2s - loss: 182.5881 - loglik: -1.8177e+02 - logprior: -8.1786e-01
Fitted a model with MAP estimate = -182.1734
Time for alignment: 63.0938
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 292.6392 - loglik: -2.8272e+02 - logprior: -9.9177e+00
Epoch 2/10
12/12 - 2s - loss: 253.3658 - loglik: -2.5096e+02 - logprior: -2.4054e+00
Epoch 3/10
12/12 - 1s - loss: 225.7855 - loglik: -2.2408e+02 - logprior: -1.7022e+00
Epoch 4/10
12/12 - 1s - loss: 215.4032 - loglik: -2.1371e+02 - logprior: -1.6915e+00
Epoch 5/10
12/12 - 1s - loss: 210.9233 - loglik: -2.0925e+02 - logprior: -1.6726e+00
Epoch 6/10
12/12 - 1s - loss: 208.4521 - loglik: -2.0682e+02 - logprior: -1.6318e+00
Epoch 7/10
12/12 - 2s - loss: 207.2908 - loglik: -2.0569e+02 - logprior: -1.6029e+00
Epoch 8/10
12/12 - 2s - loss: 206.4172 - loglik: -2.0479e+02 - logprior: -1.6232e+00
Epoch 9/10
12/12 - 2s - loss: 206.5654 - loglik: -2.0494e+02 - logprior: -1.6214e+00
Fitted a model with MAP estimate = -205.9222
expansions: [(8, 1), (10, 5), (11, 1), (20, 1), (29, 1), (36, 4), (49, 2), (50, 3), (52, 1), (58, 2), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 221.4955 - loglik: -2.1005e+02 - logprior: -1.1450e+01
Epoch 2/2
12/12 - 2s - loss: 200.7965 - loglik: -1.9595e+02 - logprior: -4.8503e+00
Fitted a model with MAP estimate = -196.4312
expansions: [(0, 4), (46, 1)]
discards: [ 0 62 77 80]
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 202.2854 - loglik: -1.9323e+02 - logprior: -9.0588e+00
Epoch 2/2
12/12 - 2s - loss: 191.9950 - loglik: -1.8970e+02 - logprior: -2.2951e+00
Fitted a model with MAP estimate = -189.3464
expansions: []
discards: [1 2 3]
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 199.3371 - loglik: -1.9050e+02 - logprior: -8.8344e+00
Epoch 2/10
12/12 - 2s - loss: 191.6813 - loglik: -1.8940e+02 - logprior: -2.2851e+00
Epoch 3/10
12/12 - 2s - loss: 188.8872 - loglik: -1.8741e+02 - logprior: -1.4795e+00
Epoch 4/10
12/12 - 2s - loss: 186.6239 - loglik: -1.8551e+02 - logprior: -1.1137e+00
Epoch 5/10
12/12 - 2s - loss: 186.2394 - loglik: -1.8530e+02 - logprior: -9.3985e-01
Epoch 6/10
12/12 - 2s - loss: 183.9923 - loglik: -1.8310e+02 - logprior: -8.8944e-01
Epoch 7/10
12/12 - 2s - loss: 182.9565 - loglik: -1.8209e+02 - logprior: -8.7010e-01
Epoch 8/10
12/12 - 2s - loss: 182.1165 - loglik: -1.8123e+02 - logprior: -8.8394e-01
Epoch 9/10
12/12 - 2s - loss: 181.3380 - loglik: -1.8045e+02 - logprior: -8.8622e-01
Epoch 10/10
12/12 - 2s - loss: 180.8714 - loglik: -1.8000e+02 - logprior: -8.6827e-01
Fitted a model with MAP estimate = -181.2623
Time for alignment: 65.3136
Computed alignments with likelihoods: ['-181.5619', '-182.1734', '-181.2623']
Best model has likelihood: -181.2623
SP score = 0.9026
Training of 3 independent models on file az.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3698fc55b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34eb1c59d0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 329.1222 - loglik: -2.8889e+02 - logprior: -4.0234e+01
Epoch 2/10
10/10 - 1s - loss: 276.1935 - loglik: -2.6614e+02 - logprior: -1.0052e+01
Epoch 3/10
10/10 - 1s - loss: 250.5537 - loglik: -2.4641e+02 - logprior: -4.1476e+00
Epoch 4/10
10/10 - 1s - loss: 236.4902 - loglik: -2.3448e+02 - logprior: -2.0119e+00
Epoch 5/10
10/10 - 1s - loss: 231.1547 - loglik: -2.3014e+02 - logprior: -1.0120e+00
Epoch 6/10
10/10 - 1s - loss: 228.1543 - loglik: -2.2763e+02 - logprior: -5.2244e-01
Epoch 7/10
10/10 - 1s - loss: 226.4488 - loglik: -2.2609e+02 - logprior: -3.5489e-01
Epoch 8/10
10/10 - 1s - loss: 225.5894 - loglik: -2.2537e+02 - logprior: -2.2158e-01
Epoch 9/10
10/10 - 1s - loss: 224.8282 - loglik: -2.2470e+02 - logprior: -1.2750e-01
Epoch 10/10
10/10 - 1s - loss: 224.8884 - loglik: -2.2484e+02 - logprior: -4.5217e-02
Fitted a model with MAP estimate = -224.4738
expansions: [(0, 3), (5, 1), (8, 1), (36, 1), (37, 2), (43, 10), (53, 3)]
discards: []
Fitting a model of length 97 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 276.5897 - loglik: -2.2457e+02 - logprior: -5.2022e+01
Epoch 2/2
10/10 - 1s - loss: 233.2605 - loglik: -2.1788e+02 - logprior: -1.5381e+01
Fitted a model with MAP estimate = -225.1983
expansions: [(7, 1)]
discards: [ 0  1 72]
Fitting a model of length 95 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 263.1228 - loglik: -2.1748e+02 - logprior: -4.5646e+01
Epoch 2/2
10/10 - 1s - loss: 233.5769 - loglik: -2.1601e+02 - logprior: -1.7564e+01
Fitted a model with MAP estimate = -228.8100
expansions: [(0, 3)]
discards: [ 0 55]
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 257.1188 - loglik: -2.1643e+02 - logprior: -4.0691e+01
Epoch 2/10
10/10 - 1s - loss: 224.8532 - loglik: -2.1472e+02 - logprior: -1.0131e+01
Epoch 3/10
10/10 - 1s - loss: 217.4052 - loglik: -2.1406e+02 - logprior: -3.3445e+00
Epoch 4/10
10/10 - 1s - loss: 214.8030 - loglik: -2.1396e+02 - logprior: -8.4444e-01
Epoch 5/10
10/10 - 1s - loss: 213.2810 - loglik: -2.1370e+02 - logprior: 0.4207
Epoch 6/10
10/10 - 1s - loss: 212.4467 - loglik: -2.1357e+02 - logprior: 1.1248
Epoch 7/10
10/10 - 1s - loss: 212.0345 - loglik: -2.1355e+02 - logprior: 1.5131
Epoch 8/10
10/10 - 1s - loss: 211.4628 - loglik: -2.1322e+02 - logprior: 1.7591
Epoch 9/10
10/10 - 1s - loss: 211.6568 - loglik: -2.1360e+02 - logprior: 1.9464
Fitted a model with MAP estimate = -211.2729
Time for alignment: 46.6408
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 329.2159 - loglik: -2.8898e+02 - logprior: -4.0232e+01
Epoch 2/10
10/10 - 1s - loss: 276.3947 - loglik: -2.6635e+02 - logprior: -1.0049e+01
Epoch 3/10
10/10 - 1s - loss: 249.6035 - loglik: -2.4545e+02 - logprior: -4.1527e+00
Epoch 4/10
10/10 - 1s - loss: 236.4045 - loglik: -2.3436e+02 - logprior: -2.0432e+00
Epoch 5/10
10/10 - 1s - loss: 230.6851 - loglik: -2.2967e+02 - logprior: -1.0151e+00
Epoch 6/10
10/10 - 1s - loss: 227.9973 - loglik: -2.2744e+02 - logprior: -5.5954e-01
Epoch 7/10
10/10 - 1s - loss: 226.2206 - loglik: -2.2586e+02 - logprior: -3.6485e-01
Epoch 8/10
10/10 - 1s - loss: 225.0921 - loglik: -2.2486e+02 - logprior: -2.3437e-01
Epoch 9/10
10/10 - 1s - loss: 224.5796 - loglik: -2.2444e+02 - logprior: -1.4223e-01
Epoch 10/10
10/10 - 1s - loss: 223.9726 - loglik: -2.2390e+02 - logprior: -7.3002e-02
Fitted a model with MAP estimate = -223.8026
expansions: [(0, 3), (5, 1), (8, 1), (37, 2), (44, 12), (53, 3)]
discards: []
Fitting a model of length 98 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 276.0114 - loglik: -2.2403e+02 - logprior: -5.1977e+01
Epoch 2/2
10/10 - 1s - loss: 233.3027 - loglik: -2.1788e+02 - logprior: -1.5422e+01
Fitted a model with MAP estimate = -224.8887
expansions: [(7, 1)]
discards: [ 0  1 55 73]
Fitting a model of length 95 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 263.1848 - loglik: -2.1751e+02 - logprior: -4.5671e+01
Epoch 2/2
10/10 - 1s - loss: 233.9119 - loglik: -2.1630e+02 - logprior: -1.7608e+01
Fitted a model with MAP estimate = -228.7661
expansions: [(0, 4)]
discards: [0]
Fitting a model of length 98 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 256.2402 - loglik: -2.1548e+02 - logprior: -4.0756e+01
Epoch 2/10
10/10 - 2s - loss: 223.7479 - loglik: -2.1361e+02 - logprior: -1.0140e+01
Epoch 3/10
10/10 - 1s - loss: 216.6796 - loglik: -2.1340e+02 - logprior: -3.2811e+00
Epoch 4/10
10/10 - 1s - loss: 213.9563 - loglik: -2.1318e+02 - logprior: -7.7204e-01
Epoch 5/10
10/10 - 1s - loss: 212.8443 - loglik: -2.1333e+02 - logprior: 0.4898
Epoch 6/10
10/10 - 1s - loss: 211.7704 - loglik: -2.1295e+02 - logprior: 1.1819
Epoch 7/10
10/10 - 1s - loss: 211.7477 - loglik: -2.1332e+02 - logprior: 1.5722
Epoch 8/10
10/10 - 1s - loss: 211.4131 - loglik: -2.1324e+02 - logprior: 1.8295
Epoch 9/10
10/10 - 2s - loss: 210.9808 - loglik: -2.1300e+02 - logprior: 2.0185
Epoch 10/10
10/10 - 2s - loss: 210.7652 - loglik: -2.1295e+02 - logprior: 2.1867
Fitted a model with MAP estimate = -210.6423
Time for alignment: 48.6091
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 328.9738 - loglik: -2.8874e+02 - logprior: -4.0233e+01
Epoch 2/10
10/10 - 1s - loss: 276.0782 - loglik: -2.6603e+02 - logprior: -1.0051e+01
Epoch 3/10
10/10 - 1s - loss: 249.6650 - loglik: -2.4553e+02 - logprior: -4.1346e+00
Epoch 4/10
10/10 - 1s - loss: 236.0766 - loglik: -2.3409e+02 - logprior: -1.9855e+00
Epoch 5/10
10/10 - 1s - loss: 230.4790 - loglik: -2.2947e+02 - logprior: -1.0110e+00
Epoch 6/10
10/10 - 1s - loss: 227.6768 - loglik: -2.2712e+02 - logprior: -5.6027e-01
Epoch 7/10
10/10 - 1s - loss: 226.3425 - loglik: -2.2596e+02 - logprior: -3.8415e-01
Epoch 8/10
10/10 - 1s - loss: 225.0791 - loglik: -2.2479e+02 - logprior: -2.8736e-01
Epoch 9/10
10/10 - 1s - loss: 224.8396 - loglik: -2.2465e+02 - logprior: -1.8784e-01
Epoch 10/10
10/10 - 1s - loss: 224.1436 - loglik: -2.2405e+02 - logprior: -8.8875e-02
Fitted a model with MAP estimate = -223.9951
expansions: [(0, 3), (5, 1), (7, 1), (8, 1), (36, 1), (37, 2), (43, 11), (53, 3)]
discards: []
Fitting a model of length 99 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 276.2155 - loglik: -2.2436e+02 - logprior: -5.1854e+01
Epoch 2/2
10/10 - 1s - loss: 232.8516 - loglik: -2.1756e+02 - logprior: -1.5291e+01
Fitted a model with MAP estimate = -224.4328
expansions: []
discards: [ 0  1 57 74]
Fitting a model of length 95 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 263.2064 - loglik: -2.1761e+02 - logprior: -4.5593e+01
Epoch 2/2
10/10 - 1s - loss: 233.8042 - loglik: -2.1615e+02 - logprior: -1.7653e+01
Fitted a model with MAP estimate = -229.0023
expansions: [(0, 3)]
discards: [ 0 55]
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 257.3859 - loglik: -2.1658e+02 - logprior: -4.0811e+01
Epoch 2/10
10/10 - 2s - loss: 225.1221 - loglik: -2.1488e+02 - logprior: -1.0239e+01
Epoch 3/10
10/10 - 1s - loss: 218.5867 - loglik: -2.1516e+02 - logprior: -3.4248e+00
Epoch 4/10
10/10 - 1s - loss: 214.8595 - loglik: -2.1396e+02 - logprior: -8.9682e-01
Epoch 5/10
10/10 - 1s - loss: 213.8412 - loglik: -2.1424e+02 - logprior: 0.4012
Epoch 6/10
10/10 - 1s - loss: 213.0565 - loglik: -2.1415e+02 - logprior: 1.0941
Epoch 7/10
10/10 - 1s - loss: 212.3357 - loglik: -2.1382e+02 - logprior: 1.4883
Epoch 8/10
10/10 - 1s - loss: 211.9422 - loglik: -2.1367e+02 - logprior: 1.7311
Epoch 9/10
10/10 - 1s - loss: 212.2162 - loglik: -2.1413e+02 - logprior: 1.9149
Fitted a model with MAP estimate = -211.7032
Time for alignment: 46.4722
Computed alignments with likelihoods: ['-211.2729', '-210.6423', '-211.7032']
Best model has likelihood: -210.6423
SP score = 0.7224
Training of 3 independent models on file ghf11.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f350d412d30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34eae7aee0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 551.1523 - loglik: -4.7338e+02 - logprior: -7.7770e+01
Epoch 2/10
10/10 - 4s - loss: 419.5602 - loglik: -4.0436e+02 - logprior: -1.5205e+01
Epoch 3/10
10/10 - 4s - loss: 341.4651 - loglik: -3.3617e+02 - logprior: -5.2987e+00
Epoch 4/10
10/10 - 4s - loss: 293.0311 - loglik: -2.8965e+02 - logprior: -3.3772e+00
Epoch 5/10
10/10 - 4s - loss: 275.1224 - loglik: -2.7239e+02 - logprior: -2.7366e+00
Epoch 6/10
10/10 - 4s - loss: 268.2467 - loglik: -2.6692e+02 - logprior: -1.3271e+00
Epoch 7/10
10/10 - 4s - loss: 265.6418 - loglik: -2.6539e+02 - logprior: -2.5625e-01
Epoch 8/10
10/10 - 4s - loss: 263.4150 - loglik: -2.6352e+02 - logprior: 0.1011
Epoch 9/10
10/10 - 4s - loss: 262.3406 - loglik: -2.6256e+02 - logprior: 0.2194
Epoch 10/10
10/10 - 4s - loss: 261.8622 - loglik: -2.6241e+02 - logprior: 0.5456
Fitted a model with MAP estimate = -261.4553
expansions: [(21, 1), (23, 2), (26, 1), (39, 2), (40, 1), (41, 3), (44, 1), (45, 2), (58, 1), (77, 1), (78, 3), (79, 2), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (122, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Fitting a model of length 181 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 345.4826 - loglik: -2.5724e+02 - logprior: -8.8238e+01
Epoch 2/2
10/10 - 6s - loss: 268.5963 - loglik: -2.3645e+02 - logprior: -3.2143e+01
Fitted a model with MAP estimate = -255.4591
expansions: [(0, 3), (14, 1), (15, 3), (16, 2), (88, 1)]
discards: [  0  47  48  55  92  96 122]
Fitting a model of length 184 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 298.5499 - loglik: -2.2923e+02 - logprior: -6.9318e+01
Epoch 2/2
10/10 - 6s - loss: 233.6067 - loglik: -2.2188e+02 - logprior: -1.1730e+01
Fitted a model with MAP estimate = -222.9069
expansions: [(18, 1)]
discards: [ 0  1 22 51]
Fitting a model of length 181 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 308.1866 - loglik: -2.2349e+02 - logprior: -8.4700e+01
Epoch 2/10
10/10 - 5s - loss: 245.9022 - loglik: -2.2049e+02 - logprior: -2.5414e+01
Epoch 3/10
10/10 - 5s - loss: 224.4456 - loglik: -2.2007e+02 - logprior: -4.3800e+00
Epoch 4/10
10/10 - 5s - loss: 212.7449 - loglik: -2.1889e+02 - logprior: 6.1460
Epoch 5/10
10/10 - 5s - loss: 209.0102 - loglik: -2.1897e+02 - logprior: 9.9573
Epoch 6/10
10/10 - 5s - loss: 207.6343 - loglik: -2.1961e+02 - logprior: 11.9731
Epoch 7/10
10/10 - 5s - loss: 206.2399 - loglik: -2.1951e+02 - logprior: 13.2687
Epoch 8/10
10/10 - 5s - loss: 204.9351 - loglik: -2.1914e+02 - logprior: 14.2097
Epoch 9/10
10/10 - 6s - loss: 204.9715 - loglik: -2.1997e+02 - logprior: 14.9984
Fitted a model with MAP estimate = -204.0938
Time for alignment: 128.5842
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 550.9969 - loglik: -4.7323e+02 - logprior: -7.7769e+01
Epoch 2/10
10/10 - 4s - loss: 419.4579 - loglik: -4.0425e+02 - logprior: -1.5203e+01
Epoch 3/10
10/10 - 4s - loss: 340.6445 - loglik: -3.3533e+02 - logprior: -5.3181e+00
Epoch 4/10
10/10 - 4s - loss: 293.3172 - loglik: -2.8986e+02 - logprior: -3.4547e+00
Epoch 5/10
10/10 - 4s - loss: 274.8239 - loglik: -2.7179e+02 - logprior: -3.0333e+00
Epoch 6/10
10/10 - 4s - loss: 267.4482 - loglik: -2.6565e+02 - logprior: -1.7970e+00
Epoch 7/10
10/10 - 4s - loss: 264.7276 - loglik: -2.6406e+02 - logprior: -6.7230e-01
Epoch 8/10
10/10 - 4s - loss: 262.8149 - loglik: -2.6256e+02 - logprior: -2.5306e-01
Epoch 9/10
10/10 - 4s - loss: 262.5572 - loglik: -2.6253e+02 - logprior: -2.7262e-02
Epoch 10/10
10/10 - 4s - loss: 261.0713 - loglik: -2.6145e+02 - logprior: 0.3809
Fitted a model with MAP estimate = -261.0729
expansions: [(21, 1), (23, 2), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 3), (79, 2), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (122, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Fitting a model of length 179 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 344.7432 - loglik: -2.5637e+02 - logprior: -8.8373e+01
Epoch 2/2
10/10 - 5s - loss: 268.7567 - loglik: -2.3673e+02 - logprior: -3.2031e+01
Fitted a model with MAP estimate = -255.6280
expansions: [(0, 3), (14, 1), (15, 3), (16, 2), (86, 1)]
discards: [  0  43  53  90  94 120]
Fitting a model of length 183 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 299.3422 - loglik: -2.2998e+02 - logprior: -6.9366e+01
Epoch 2/2
10/10 - 5s - loss: 233.8983 - loglik: -2.2213e+02 - logprior: -1.1772e+01
Fitted a model with MAP estimate = -223.1772
expansions: [(18, 1)]
discards: [ 0  1 22]
Fitting a model of length 181 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 308.0429 - loglik: -2.2336e+02 - logprior: -8.4678e+01
Epoch 2/10
10/10 - 5s - loss: 245.9563 - loglik: -2.2068e+02 - logprior: -2.5275e+01
Epoch 3/10
10/10 - 5s - loss: 223.7884 - loglik: -2.1954e+02 - logprior: -4.2503e+00
Epoch 4/10
10/10 - 6s - loss: 213.1216 - loglik: -2.1929e+02 - logprior: 6.1688
Epoch 5/10
10/10 - 6s - loss: 209.2628 - loglik: -2.1922e+02 - logprior: 9.9547
Epoch 6/10
10/10 - 6s - loss: 207.2666 - loglik: -2.1924e+02 - logprior: 11.9689
Epoch 7/10
10/10 - 6s - loss: 205.9664 - loglik: -2.1923e+02 - logprior: 13.2675
Epoch 8/10
10/10 - 6s - loss: 205.5585 - loglik: -2.1976e+02 - logprior: 14.1999
Epoch 9/10
10/10 - 6s - loss: 204.0872 - loglik: -2.1909e+02 - logprior: 14.9981
Epoch 10/10
10/10 - 5s - loss: 204.4157 - loglik: -2.2013e+02 - logprior: 15.7170
Fitted a model with MAP estimate = -203.4503
Time for alignment: 133.4173
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 551.3694 - loglik: -4.7360e+02 - logprior: -7.7771e+01
Epoch 2/10
10/10 - 4s - loss: 418.6152 - loglik: -4.0341e+02 - logprior: -1.5208e+01
Epoch 3/10
10/10 - 4s - loss: 341.8301 - loglik: -3.3649e+02 - logprior: -5.3417e+00
Epoch 4/10
10/10 - 3s - loss: 293.7789 - loglik: -2.9030e+02 - logprior: -3.4762e+00
Epoch 5/10
10/10 - 4s - loss: 274.9386 - loglik: -2.7189e+02 - logprior: -3.0483e+00
Epoch 6/10
10/10 - 4s - loss: 268.0941 - loglik: -2.6601e+02 - logprior: -2.0840e+00
Epoch 7/10
10/10 - 4s - loss: 264.4381 - loglik: -2.6330e+02 - logprior: -1.1375e+00
Epoch 8/10
10/10 - 4s - loss: 262.8027 - loglik: -2.6225e+02 - logprior: -5.4950e-01
Epoch 9/10
10/10 - 4s - loss: 261.5807 - loglik: -2.6145e+02 - logprior: -1.3118e-01
Epoch 10/10
10/10 - 4s - loss: 261.3544 - loglik: -2.6158e+02 - logprior: 0.2211
Fitted a model with MAP estimate = -260.7819
expansions: [(18, 1), (20, 1), (22, 1), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 3), (79, 2), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (122, 1), (124, 1), (125, 1), (127, 1), (129, 3), (133, 1), (137, 1), (138, 1)]
discards: [0]
Fitting a model of length 179 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 345.5062 - loglik: -2.5714e+02 - logprior: -8.8369e+01
Epoch 2/2
10/10 - 5s - loss: 268.3965 - loglik: -2.3652e+02 - logprior: -3.1879e+01
Fitted a model with MAP estimate = -256.5987
expansions: [(0, 3), (15, 3), (16, 1), (86, 1)]
discards: [  0  43  53  90  94 120]
Fitting a model of length 181 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 300.0045 - loglik: -2.3082e+02 - logprior: -6.9183e+01
Epoch 2/2
10/10 - 5s - loss: 235.7253 - loglik: -2.2412e+02 - logprior: -1.1609e+01
Fitted a model with MAP estimate = -224.6615
expansions: [(16, 1), (17, 1)]
discards: [0 1]
Fitting a model of length 181 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 309.2267 - loglik: -2.2451e+02 - logprior: -8.4716e+01
Epoch 2/10
10/10 - 6s - loss: 246.1803 - loglik: -2.2063e+02 - logprior: -2.5554e+01
Epoch 3/10
10/10 - 5s - loss: 224.6556 - loglik: -2.2006e+02 - logprior: -4.5999e+00
Epoch 4/10
10/10 - 5s - loss: 212.5010 - loglik: -2.1860e+02 - logprior: 6.0959
Epoch 5/10
10/10 - 6s - loss: 209.7012 - loglik: -2.1963e+02 - logprior: 9.9330
Epoch 6/10
10/10 - 5s - loss: 207.7386 - loglik: -2.1967e+02 - logprior: 11.9278
Epoch 7/10
10/10 - 6s - loss: 206.3573 - loglik: -2.1957e+02 - logprior: 13.2115
Epoch 8/10
10/10 - 6s - loss: 205.1267 - loglik: -2.1927e+02 - logprior: 14.1456
Epoch 9/10
10/10 - 6s - loss: 204.4368 - loglik: -2.1937e+02 - logprior: 14.9353
Epoch 10/10
10/10 - 6s - loss: 204.0150 - loglik: -2.1967e+02 - logprior: 15.6501
Fitted a model with MAP estimate = -203.5338
Time for alignment: 127.8776
Computed alignments with likelihoods: ['-204.0938', '-203.4503', '-203.5338']
Best model has likelihood: -203.4503
SP score = 0.9142
Training of 3 independent models on file subt.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32aabd10a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34c9578640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 35s - loss: 813.7192 - loglik: -8.1170e+02 - logprior: -2.0160e+00
Epoch 2/10
33/33 - 30s - loss: 712.6418 - loglik: -7.1202e+02 - logprior: -6.2075e-01
Epoch 3/10
33/33 - 32s - loss: 705.5204 - loglik: -7.0499e+02 - logprior: -5.3212e-01
Epoch 4/10
33/33 - 34s - loss: 701.2638 - loglik: -7.0075e+02 - logprior: -5.0937e-01
Epoch 5/10
33/33 - 36s - loss: 694.8980 - loglik: -6.9439e+02 - logprior: -5.0818e-01
Epoch 6/10
33/33 - 37s - loss: 699.0397 - loglik: -6.9854e+02 - logprior: -5.0121e-01
Fitted a model with MAP estimate = -694.6252
expansions: [(0, 5), (5, 1), (8, 1), (9, 1), (34, 2), (60, 1), (63, 2), (72, 1), (73, 2), (78, 2), (113, 1), (116, 1), (118, 1), (135, 1), (155, 3), (184, 1), (204, 1), (217, 1), (222, 3), (223, 2)]
discards: []
Fitting a model of length 263 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 50s - loss: 718.3281 - loglik: -7.1551e+02 - logprior: -2.8142e+00
Epoch 2/2
33/33 - 49s - loss: 696.4597 - loglik: -6.9625e+02 - logprior: -2.1008e-01
Fitted a model with MAP estimate = -694.8460
expansions: []
discards: [  1   5   6   7 178 254 258 259 260 261 262]
Fitting a model of length 252 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 48s - loss: 706.5312 - loglik: -7.0464e+02 - logprior: -1.8887e+00
Epoch 2/2
33/33 - 45s - loss: 696.5081 - loglik: -6.9662e+02 - logprior: 0.1125
Fitted a model with MAP estimate = -695.9178
expansions: [(0, 5), (1, 1), (246, 5), (252, 4)]
discards: [ 68 249 250 251]
Fitting a model of length 263 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 38s - loss: 704.8483 - loglik: -7.0198e+02 - logprior: -2.8709e+00
Epoch 2/10
33/33 - 33s - loss: 697.2167 - loglik: -6.9732e+02 - logprior: 0.1024
Epoch 3/10
33/33 - 32s - loss: 694.1612 - loglik: -6.9447e+02 - logprior: 0.3085
Epoch 4/10
33/33 - 31s - loss: 691.5072 - loglik: -6.9193e+02 - logprior: 0.4222
Epoch 5/10
33/33 - 32s - loss: 686.7574 - loglik: -6.8728e+02 - logprior: 0.5276
Epoch 6/10
33/33 - 33s - loss: 686.3829 - loglik: -6.8702e+02 - logprior: 0.6370
Epoch 7/10
33/33 - 34s - loss: 681.7049 - loglik: -6.8241e+02 - logprior: 0.7091
Epoch 8/10
33/33 - 35s - loss: 681.9334 - loglik: -6.8271e+02 - logprior: 0.7723
Fitted a model with MAP estimate = -681.1613
Time for alignment: 813.6388
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 30s - loss: 812.2023 - loglik: -8.1020e+02 - logprior: -2.0051e+00
Epoch 2/10
33/33 - 29s - loss: 714.8069 - loglik: -7.1421e+02 - logprior: -5.9416e-01
Epoch 3/10
33/33 - 28s - loss: 703.8076 - loglik: -7.0331e+02 - logprior: -4.9389e-01
Epoch 4/10
33/33 - 28s - loss: 699.4548 - loglik: -6.9898e+02 - logprior: -4.7059e-01
Epoch 5/10
33/33 - 29s - loss: 700.5440 - loglik: -7.0008e+02 - logprior: -4.5996e-01
Fitted a model with MAP estimate = -696.1593
expansions: [(0, 5), (5, 1), (7, 1), (9, 1), (34, 2), (61, 1), (62, 2), (71, 1), (72, 2), (77, 1), (113, 1), (116, 1), (118, 1), (135, 1), (155, 3), (163, 4), (177, 1), (204, 1), (217, 1), (220, 2), (221, 3)]
discards: []
Fitting a model of length 266 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 37s - loss: 713.2914 - loglik: -7.1051e+02 - logprior: -2.7772e+00
Epoch 2/2
33/33 - 34s - loss: 694.8723 - loglik: -6.9459e+02 - logprior: -2.8085e-01
Fitted a model with MAP estimate = -693.1515
expansions: []
discards: [  1   5   6   7  72 177 188 189 190 191 261 262 263 264 265]
Fitting a model of length 251 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 33s - loss: 703.8093 - loglik: -7.0188e+02 - logprior: -1.9298e+00
Epoch 2/2
33/33 - 31s - loss: 700.6234 - loglik: -7.0066e+02 - logprior: 0.0350
Fitted a model with MAP estimate = -695.3653
expansions: [(0, 5), (1, 1), (182, 5), (245, 2), (251, 4)]
discards: [246 247 248 250]
Fitting a model of length 264 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 35s - loss: 705.6784 - loglik: -7.0288e+02 - logprior: -2.7943e+00
Epoch 2/10
33/33 - 31s - loss: 697.0167 - loglik: -6.9703e+02 - logprior: 0.0167
Epoch 3/10
33/33 - 31s - loss: 691.9216 - loglik: -6.9216e+02 - logprior: 0.2418
Epoch 4/10
33/33 - 31s - loss: 686.4210 - loglik: -6.8679e+02 - logprior: 0.3653
Epoch 5/10
33/33 - 33s - loss: 689.5153 - loglik: -6.9001e+02 - logprior: 0.4967
Fitted a model with MAP estimate = -684.4324
Time for alignment: 554.0154
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 30s - loss: 810.6870 - loglik: -8.0869e+02 - logprior: -1.9987e+00
Epoch 2/10
33/33 - 28s - loss: 716.0966 - loglik: -7.1535e+02 - logprior: -7.4835e-01
Epoch 3/10
33/33 - 28s - loss: 705.2196 - loglik: -7.0456e+02 - logprior: -6.5473e-01
Epoch 4/10
33/33 - 29s - loss: 700.6358 - loglik: -7.0004e+02 - logprior: -5.9298e-01
Epoch 5/10
33/33 - 30s - loss: 696.5518 - loglik: -6.9599e+02 - logprior: -5.6487e-01
Epoch 6/10
33/33 - 31s - loss: 697.8210 - loglik: -6.9726e+02 - logprior: -5.5687e-01
Fitted a model with MAP estimate = -694.4252
expansions: [(0, 4), (6, 1), (7, 1), (9, 1), (34, 4), (44, 1), (63, 1), (65, 1), (71, 1), (72, 1), (78, 1), (88, 1), (113, 1), (116, 1), (118, 1), (135, 1), (155, 3), (177, 1), (204, 1), (210, 1), (220, 2), (221, 3)]
discards: []
Fitting a model of length 263 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 45s - loss: 718.7598 - loglik: -7.1588e+02 - logprior: -2.8800e+00
Epoch 2/2
33/33 - 42s - loss: 695.8976 - loglik: -6.9563e+02 - logprior: -2.6608e-01
Fitted a model with MAP estimate = -695.1304
expansions: [(17, 1), (188, 5)]
discards: [  1   2   3   6   7  43 178 258 259 260 261 262]
Fitting a model of length 257 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 40s - loss: 705.9033 - loglik: -7.0397e+02 - logprior: -1.9359e+00
Epoch 2/2
33/33 - 37s - loss: 695.2752 - loglik: -6.9530e+02 - logprior: 0.0297
Fitted a model with MAP estimate = -693.6124
expansions: [(0, 5), (251, 2), (257, 4)]
discards: [183 184 185 186 187 252 253 254 255]
Fitting a model of length 259 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 42s - loss: 706.6223 - loglik: -7.0379e+02 - logprior: -2.8325e+00
Epoch 2/10
33/33 - 40s - loss: 698.5557 - loglik: -6.9864e+02 - logprior: 0.0802
Epoch 3/10
33/33 - 38s - loss: 693.0536 - loglik: -6.9338e+02 - logprior: 0.3302
Epoch 4/10
33/33 - 37s - loss: 688.5236 - loglik: -6.8898e+02 - logprior: 0.4611
Epoch 5/10
33/33 - 38s - loss: 690.6224 - loglik: -6.9120e+02 - logprior: 0.5778
Fitted a model with MAP estimate = -685.9776
Time for alignment: 665.0173
Computed alignments with likelihoods: ['-681.1613', '-684.4324', '-685.9776']
Best model has likelihood: -681.1613
SP score = 0.7873
Training of 3 independent models on file profilin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f36a9abc2e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34f4092970>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 410.3669 - loglik: -3.4845e+02 - logprior: -6.1918e+01
Epoch 2/10
10/10 - 2s - loss: 320.4492 - loglik: -3.0617e+02 - logprior: -1.4278e+01
Epoch 3/10
10/10 - 2s - loss: 273.8575 - loglik: -2.6774e+02 - logprior: -6.1165e+00
Epoch 4/10
10/10 - 2s - loss: 247.0454 - loglik: -2.4341e+02 - logprior: -3.6386e+00
Epoch 5/10
10/10 - 2s - loss: 235.7716 - loglik: -2.3325e+02 - logprior: -2.5231e+00
Epoch 6/10
10/10 - 2s - loss: 232.4965 - loglik: -2.3083e+02 - logprior: -1.6649e+00
Epoch 7/10
10/10 - 2s - loss: 230.6949 - loglik: -2.2975e+02 - logprior: -9.4525e-01
Epoch 8/10
10/10 - 2s - loss: 229.2802 - loglik: -2.2868e+02 - logprior: -6.0152e-01
Epoch 9/10
10/10 - 2s - loss: 228.6079 - loglik: -2.2821e+02 - logprior: -4.0208e-01
Epoch 10/10
10/10 - 2s - loss: 228.2594 - loglik: -2.2805e+02 - logprior: -2.0694e-01
Fitted a model with MAP estimate = -227.9654
expansions: [(11, 1), (12, 2), (13, 5), (37, 2), (38, 1), (39, 2), (45, 1), (51, 1), (54, 1), (62, 1), (63, 1), (70, 1), (78, 1), (79, 1), (88, 1), (91, 5)]
discards: [0]
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 303.1736 - loglik: -2.3390e+02 - logprior: -6.9274e+01
Epoch 2/2
10/10 - 2s - loss: 241.5926 - loglik: -2.1538e+02 - logprior: -2.6216e+01
Fitted a model with MAP estimate = -229.7428
expansions: [(0, 5)]
discards: [  0  13  44 114]
Fitting a model of length 127 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 268.0231 - loglik: -2.1368e+02 - logprior: -5.4348e+01
Epoch 2/2
10/10 - 2s - loss: 218.6460 - loglik: -2.0708e+02 - logprior: -1.1567e+01
Fitted a model with MAP estimate = -210.6805
expansions: []
discards: [1 2 3 4]
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 262.9207 - loglik: -2.0963e+02 - logprior: -5.3294e+01
Epoch 2/10
10/10 - 2s - loss: 217.4088 - loglik: -2.0642e+02 - logprior: -1.0991e+01
Epoch 3/10
10/10 - 2s - loss: 206.8304 - loglik: -2.0468e+02 - logprior: -2.1510e+00
Epoch 4/10
10/10 - 2s - loss: 201.1669 - loglik: -2.0273e+02 - logprior: 1.5596
Epoch 5/10
10/10 - 2s - loss: 197.7643 - loglik: -2.0137e+02 - logprior: 3.6011
Epoch 6/10
10/10 - 2s - loss: 195.7227 - loglik: -2.0051e+02 - logprior: 4.7884
Epoch 7/10
10/10 - 2s - loss: 194.5743 - loglik: -2.0004e+02 - logprior: 5.4618
Epoch 8/10
10/10 - 2s - loss: 194.3822 - loglik: -2.0028e+02 - logprior: 5.8939
Epoch 9/10
10/10 - 2s - loss: 193.6716 - loglik: -1.9995e+02 - logprior: 6.2820
Epoch 10/10
10/10 - 2s - loss: 192.9261 - loglik: -1.9966e+02 - logprior: 6.7307
Fitted a model with MAP estimate = -192.9153
Time for alignment: 61.9777
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 410.6647 - loglik: -3.4875e+02 - logprior: -6.1916e+01
Epoch 2/10
10/10 - 1s - loss: 320.4944 - loglik: -3.0621e+02 - logprior: -1.4286e+01
Epoch 3/10
10/10 - 1s - loss: 272.9865 - loglik: -2.6686e+02 - logprior: -6.1217e+00
Epoch 4/10
10/10 - 2s - loss: 247.8258 - loglik: -2.4423e+02 - logprior: -3.5978e+00
Epoch 5/10
10/10 - 2s - loss: 238.7945 - loglik: -2.3655e+02 - logprior: -2.2491e+00
Epoch 6/10
10/10 - 1s - loss: 232.7930 - loglik: -2.3115e+02 - logprior: -1.6432e+00
Epoch 7/10
10/10 - 1s - loss: 231.3084 - loglik: -2.3021e+02 - logprior: -1.0938e+00
Epoch 8/10
10/10 - 1s - loss: 230.1570 - loglik: -2.2950e+02 - logprior: -6.6118e-01
Epoch 9/10
10/10 - 2s - loss: 229.2344 - loglik: -2.2880e+02 - logprior: -4.3786e-01
Epoch 10/10
10/10 - 1s - loss: 229.0394 - loglik: -2.2877e+02 - logprior: -2.6465e-01
Fitted a model with MAP estimate = -228.5952
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 1), (71, 3), (78, 1), (79, 1), (85, 1), (87, 3), (88, 1), (91, 1)]
discards: [0]
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 302.8074 - loglik: -2.3351e+02 - logprior: -6.9302e+01
Epoch 2/2
10/10 - 2s - loss: 239.9375 - loglik: -2.1377e+02 - logprior: -2.6166e+01
Fitted a model with MAP estimate = -228.7519
expansions: [(0, 5)]
discards: [  0  13  87 109]
Fitting a model of length 127 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 266.6402 - loglik: -2.1233e+02 - logprior: -5.4312e+01
Epoch 2/2
10/10 - 2s - loss: 219.3925 - loglik: -2.0785e+02 - logprior: -1.1542e+01
Fitted a model with MAP estimate = -210.5955
expansions: []
discards: [1 2 3 4]
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 262.7823 - loglik: -2.0951e+02 - logprior: -5.3276e+01
Epoch 2/10
10/10 - 2s - loss: 217.3465 - loglik: -2.0639e+02 - logprior: -1.0960e+01
Epoch 3/10
10/10 - 2s - loss: 207.1297 - loglik: -2.0495e+02 - logprior: -2.1771e+00
Epoch 4/10
10/10 - 2s - loss: 201.2888 - loglik: -2.0283e+02 - logprior: 1.5443
Epoch 5/10
10/10 - 2s - loss: 197.8469 - loglik: -2.0145e+02 - logprior: 3.6030
Epoch 6/10
10/10 - 2s - loss: 196.3866 - loglik: -2.0121e+02 - logprior: 4.8279
Epoch 7/10
10/10 - 2s - loss: 195.0513 - loglik: -2.0057e+02 - logprior: 5.5203
Epoch 8/10
10/10 - 2s - loss: 193.3875 - loglik: -1.9934e+02 - logprior: 5.9506
Epoch 9/10
10/10 - 2s - loss: 194.2288 - loglik: -2.0057e+02 - logprior: 6.3380
Fitted a model with MAP estimate = -193.3883
Time for alignment: 55.8370
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 410.5768 - loglik: -3.4866e+02 - logprior: -6.1917e+01
Epoch 2/10
10/10 - 1s - loss: 320.4579 - loglik: -3.0618e+02 - logprior: -1.4281e+01
Epoch 3/10
10/10 - 1s - loss: 272.8230 - loglik: -2.6671e+02 - logprior: -6.1168e+00
Epoch 4/10
10/10 - 1s - loss: 246.9189 - loglik: -2.4322e+02 - logprior: -3.6995e+00
Epoch 5/10
10/10 - 1s - loss: 237.5083 - loglik: -2.3497e+02 - logprior: -2.5397e+00
Epoch 6/10
10/10 - 1s - loss: 233.3353 - loglik: -2.3159e+02 - logprior: -1.7413e+00
Epoch 7/10
10/10 - 1s - loss: 230.9704 - loglik: -2.2968e+02 - logprior: -1.2874e+00
Epoch 8/10
10/10 - 1s - loss: 229.2798 - loglik: -2.2834e+02 - logprior: -9.3552e-01
Epoch 9/10
10/10 - 1s - loss: 229.1381 - loglik: -2.2846e+02 - logprior: -6.8269e-01
Epoch 10/10
10/10 - 1s - loss: 228.4439 - loglik: -2.2797e+02 - logprior: -4.7551e-01
Fitted a model with MAP estimate = -228.0747
expansions: [(11, 1), (12, 2), (13, 5), (37, 2), (38, 1), (39, 2), (45, 1), (55, 1), (63, 1), (64, 1), (69, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 3), (88, 1), (91, 1)]
discards: [0]
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 302.4937 - loglik: -2.3324e+02 - logprior: -6.9250e+01
Epoch 2/2
10/10 - 2s - loss: 240.8025 - loglik: -2.1464e+02 - logprior: -2.6166e+01
Fitted a model with MAP estimate = -228.9522
expansions: [(0, 5)]
discards: [  0  44 109]
Fitting a model of length 128 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 265.6917 - loglik: -2.1131e+02 - logprior: -5.4380e+01
Epoch 2/2
10/10 - 2s - loss: 218.3169 - loglik: -2.0671e+02 - logprior: -1.1607e+01
Fitted a model with MAP estimate = -209.4109
expansions: []
discards: [ 1  2  3  4 17]
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 263.1107 - loglik: -2.0986e+02 - logprior: -5.3248e+01
Epoch 2/10
10/10 - 2s - loss: 217.1453 - loglik: -2.0618e+02 - logprior: -1.0966e+01
Epoch 3/10
10/10 - 2s - loss: 206.5235 - loglik: -2.0437e+02 - logprior: -2.1527e+00
Epoch 4/10
10/10 - 2s - loss: 200.8222 - loglik: -2.0237e+02 - logprior: 1.5452
Epoch 5/10
10/10 - 2s - loss: 197.2775 - loglik: -2.0089e+02 - logprior: 3.6167
Epoch 6/10
10/10 - 2s - loss: 196.3220 - loglik: -2.0111e+02 - logprior: 4.7910
Epoch 7/10
10/10 - 2s - loss: 194.5041 - loglik: -1.9998e+02 - logprior: 5.4748
Epoch 8/10
10/10 - 2s - loss: 193.8619 - loglik: -1.9977e+02 - logprior: 5.9063
Epoch 9/10
10/10 - 2s - loss: 193.4227 - loglik: -1.9973e+02 - logprior: 6.3118
Epoch 10/10
10/10 - 2s - loss: 193.0147 - loglik: -1.9978e+02 - logprior: 6.7639
Fitted a model with MAP estimate = -192.7507
Time for alignment: 53.6159
Computed alignments with likelihoods: ['-192.9153', '-193.3883', '-192.7507']
Best model has likelihood: -192.7507
SP score = 0.9410
Training of 3 independent models on file rrm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32c85e6b80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32a998ee20>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 195.3567 - loglik: -1.9316e+02 - logprior: -2.1960e+00
Epoch 2/10
22/22 - 1s - loss: 162.1221 - loglik: -1.6080e+02 - logprior: -1.3223e+00
Epoch 3/10
22/22 - 1s - loss: 153.9594 - loglik: -1.5255e+02 - logprior: -1.4110e+00
Epoch 4/10
22/22 - 1s - loss: 151.9155 - loglik: -1.5062e+02 - logprior: -1.2916e+00
Epoch 5/10
22/22 - 1s - loss: 151.6622 - loglik: -1.5037e+02 - logprior: -1.2883e+00
Epoch 6/10
22/22 - 1s - loss: 150.9825 - loglik: -1.4972e+02 - logprior: -1.2607e+00
Epoch 7/10
22/22 - 1s - loss: 150.3992 - loglik: -1.4915e+02 - logprior: -1.2500e+00
Epoch 8/10
22/22 - 1s - loss: 150.5158 - loglik: -1.4927e+02 - logprior: -1.2423e+00
Fitted a model with MAP estimate = -151.5718
expansions: [(8, 1), (9, 2), (11, 1), (14, 2), (20, 2), (21, 1), (22, 2), (28, 1), (40, 1), (41, 1), (45, 2), (47, 2), (48, 1), (49, 3)]
discards: [0]
Fitting a model of length 77 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 159.0070 - loglik: -1.5611e+02 - logprior: -2.9009e+00
Epoch 2/2
22/22 - 1s - loss: 147.3225 - loglik: -1.4578e+02 - logprior: -1.5458e+00
Fitted a model with MAP estimate = -145.0156
expansions: [(0, 2)]
discards: [ 0  9 18 25 31 58 68]
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 147.2769 - loglik: -1.4518e+02 - logprior: -2.0987e+00
Epoch 2/2
22/22 - 1s - loss: 143.8798 - loglik: -1.4289e+02 - logprior: -9.9322e-01
Fitted a model with MAP estimate = -143.5459
expansions: []
discards: [0]
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 144.8426 - loglik: -1.4364e+02 - logprior: -1.1979e+00
Epoch 2/10
32/32 - 2s - loss: 141.8192 - loglik: -1.4095e+02 - logprior: -8.7063e-01
Epoch 3/10
32/32 - 2s - loss: 140.8526 - loglik: -1.4001e+02 - logprior: -8.4676e-01
Epoch 4/10
32/32 - 2s - loss: 139.7210 - loglik: -1.3888e+02 - logprior: -8.4275e-01
Epoch 5/10
32/32 - 2s - loss: 139.5590 - loglik: -1.3873e+02 - logprior: -8.3335e-01
Epoch 6/10
32/32 - 2s - loss: 138.7873 - loglik: -1.3796e+02 - logprior: -8.2566e-01
Epoch 7/10
32/32 - 2s - loss: 138.4026 - loglik: -1.3758e+02 - logprior: -8.2240e-01
Epoch 8/10
32/32 - 2s - loss: 137.9933 - loglik: -1.3717e+02 - logprior: -8.2152e-01
Epoch 9/10
32/32 - 2s - loss: 137.9256 - loglik: -1.3710e+02 - logprior: -8.2137e-01
Epoch 10/10
32/32 - 2s - loss: 137.4527 - loglik: -1.3663e+02 - logprior: -8.2380e-01
Fitted a model with MAP estimate = -137.2419
Time for alignment: 67.6724
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 195.3326 - loglik: -1.9314e+02 - logprior: -2.1954e+00
Epoch 2/10
22/22 - 1s - loss: 162.1962 - loglik: -1.6088e+02 - logprior: -1.3211e+00
Epoch 3/10
22/22 - 1s - loss: 154.3404 - loglik: -1.5293e+02 - logprior: -1.4135e+00
Epoch 4/10
22/22 - 1s - loss: 152.0327 - loglik: -1.5072e+02 - logprior: -1.3146e+00
Epoch 5/10
22/22 - 2s - loss: 151.6107 - loglik: -1.5030e+02 - logprior: -1.3112e+00
Epoch 6/10
22/22 - 1s - loss: 150.9784 - loglik: -1.4970e+02 - logprior: -1.2793e+00
Epoch 7/10
22/22 - 1s - loss: 150.7053 - loglik: -1.4943e+02 - logprior: -1.2718e+00
Epoch 8/10
22/22 - 1s - loss: 150.2293 - loglik: -1.4896e+02 - logprior: -1.2651e+00
Epoch 9/10
22/22 - 1s - loss: 150.1810 - loglik: -1.4892e+02 - logprior: -1.2590e+00
Epoch 10/10
22/22 - 1s - loss: 149.9732 - loglik: -1.4871e+02 - logprior: -1.2586e+00
Fitted a model with MAP estimate = -151.7458
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (20, 2), (22, 2), (23, 1), (25, 1), (40, 1), (41, 1), (44, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 158.9669 - loglik: -1.5607e+02 - logprior: -2.8984e+00
Epoch 2/2
22/22 - 1s - loss: 147.6118 - loglik: -1.4609e+02 - logprior: -1.5228e+00
Fitted a model with MAP estimate = -145.0050
expansions: [(0, 2)]
discards: [ 0  9 17 25 30 67]
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 147.3411 - loglik: -1.4525e+02 - logprior: -2.0947e+00
Epoch 2/2
22/22 - 1s - loss: 143.8550 - loglik: -1.4287e+02 - logprior: -9.8857e-01
Fitted a model with MAP estimate = -143.5498
expansions: []
discards: [0]
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 145.0327 - loglik: -1.4384e+02 - logprior: -1.1922e+00
Epoch 2/10
32/32 - 2s - loss: 141.6886 - loglik: -1.4081e+02 - logprior: -8.7610e-01
Epoch 3/10
32/32 - 2s - loss: 140.7517 - loglik: -1.3990e+02 - logprior: -8.5171e-01
Epoch 4/10
32/32 - 2s - loss: 139.8065 - loglik: -1.3897e+02 - logprior: -8.4064e-01
Epoch 5/10
32/32 - 2s - loss: 139.5542 - loglik: -1.3872e+02 - logprior: -8.3565e-01
Epoch 6/10
32/32 - 2s - loss: 138.6681 - loglik: -1.3784e+02 - logprior: -8.2914e-01
Epoch 7/10
32/32 - 2s - loss: 138.5954 - loglik: -1.3777e+02 - logprior: -8.2568e-01
Epoch 8/10
32/32 - 2s - loss: 137.9497 - loglik: -1.3713e+02 - logprior: -8.1822e-01
Epoch 9/10
32/32 - 2s - loss: 137.7932 - loglik: -1.3697e+02 - logprior: -8.2623e-01
Epoch 10/10
32/32 - 2s - loss: 137.5925 - loglik: -1.3678e+02 - logprior: -8.1324e-01
Fitted a model with MAP estimate = -137.2302
Time for alignment: 70.6892
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 195.5346 - loglik: -1.9334e+02 - logprior: -2.1943e+00
Epoch 2/10
22/22 - 1s - loss: 162.8158 - loglik: -1.6150e+02 - logprior: -1.3195e+00
Epoch 3/10
22/22 - 1s - loss: 154.5700 - loglik: -1.5316e+02 - logprior: -1.4127e+00
Epoch 4/10
22/22 - 1s - loss: 152.2488 - loglik: -1.5093e+02 - logprior: -1.3162e+00
Epoch 5/10
22/22 - 1s - loss: 151.6367 - loglik: -1.5033e+02 - logprior: -1.3112e+00
Epoch 6/10
22/22 - 1s - loss: 151.0271 - loglik: -1.4975e+02 - logprior: -1.2800e+00
Epoch 7/10
22/22 - 1s - loss: 150.4367 - loglik: -1.4917e+02 - logprior: -1.2690e+00
Epoch 8/10
22/22 - 1s - loss: 150.3590 - loglik: -1.4910e+02 - logprior: -1.2639e+00
Epoch 9/10
22/22 - 1s - loss: 150.1092 - loglik: -1.4885e+02 - logprior: -1.2616e+00
Epoch 10/10
22/22 - 1s - loss: 150.1357 - loglik: -1.4888e+02 - logprior: -1.2571e+00
Fitted a model with MAP estimate = -151.8101
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (20, 2), (22, 2), (23, 1), (25, 1), (40, 1), (41, 1), (44, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 159.1126 - loglik: -1.5622e+02 - logprior: -2.8923e+00
Epoch 2/2
22/22 - 2s - loss: 147.6054 - loglik: -1.4608e+02 - logprior: -1.5209e+00
Fitted a model with MAP estimate = -145.0024
expansions: [(0, 2)]
discards: [ 0  9 17 25 30 67]
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 147.1861 - loglik: -1.4509e+02 - logprior: -2.0937e+00
Epoch 2/2
22/22 - 1s - loss: 143.8711 - loglik: -1.4288e+02 - logprior: -9.8777e-01
Fitted a model with MAP estimate = -143.6181
expansions: []
discards: [0]
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 144.8691 - loglik: -1.4367e+02 - logprior: -1.1950e+00
Epoch 2/10
32/32 - 2s - loss: 141.8651 - loglik: -1.4099e+02 - logprior: -8.7248e-01
Epoch 3/10
32/32 - 2s - loss: 140.7028 - loglik: -1.3986e+02 - logprior: -8.4717e-01
Epoch 4/10
32/32 - 2s - loss: 140.0012 - loglik: -1.3915e+02 - logprior: -8.4736e-01
Epoch 5/10
32/32 - 2s - loss: 139.4567 - loglik: -1.3863e+02 - logprior: -8.2935e-01
Epoch 6/10
32/32 - 2s - loss: 138.7966 - loglik: -1.3797e+02 - logprior: -8.3162e-01
Epoch 7/10
32/32 - 2s - loss: 138.4652 - loglik: -1.3764e+02 - logprior: -8.2381e-01
Epoch 8/10
32/32 - 2s - loss: 137.9586 - loglik: -1.3714e+02 - logprior: -8.1986e-01
Epoch 9/10
32/32 - 2s - loss: 137.9381 - loglik: -1.3711e+02 - logprior: -8.2403e-01
Epoch 10/10
32/32 - 2s - loss: 137.3747 - loglik: -1.3656e+02 - logprior: -8.1680e-01
Fitted a model with MAP estimate = -137.2589
Time for alignment: 70.5171
Computed alignments with likelihoods: ['-137.2419', '-137.2302', '-137.2589']
Best model has likelihood: -137.2302
SP score = 0.8788
Training of 3 independent models on file KAS.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f349cee28e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32a84c4af0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 162 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 499.3080 - loglik: -4.8020e+02 - logprior: -1.9113e+01
Epoch 2/10
10/10 - 5s - loss: 395.5770 - loglik: -3.9219e+02 - logprior: -3.3901e+00
Epoch 3/10
10/10 - 6s - loss: 318.6564 - loglik: -3.1754e+02 - logprior: -1.1154e+00
Epoch 4/10
10/10 - 6s - loss: 276.7552 - loglik: -2.7604e+02 - logprior: -7.1226e-01
Epoch 5/10
10/10 - 6s - loss: 262.4411 - loglik: -2.6197e+02 - logprior: -4.7446e-01
Epoch 6/10
10/10 - 6s - loss: 259.0434 - loglik: -2.5881e+02 - logprior: -2.3210e-01
Epoch 7/10
10/10 - 5s - loss: 253.5074 - loglik: -2.5333e+02 - logprior: -1.7322e-01
Epoch 8/10
10/10 - 6s - loss: 253.5231 - loglik: -2.5336e+02 - logprior: -1.6037e-01
Fitted a model with MAP estimate = -253.2179
expansions: [(0, 38), (24, 2), (25, 1), (50, 1), (59, 1), (80, 1), (89, 1), (108, 1), (137, 1)]
discards: []
Fitting a model of length 209 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 276.6314 - loglik: -2.5423e+02 - logprior: -2.2403e+01
Epoch 2/2
10/10 - 8s - loss: 237.9223 - loglik: -2.3245e+02 - logprior: -5.4730e+00
Fitted a model with MAP estimate = -223.8245
expansions: [(0, 17)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]
Fitting a model of length 192 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 12s - loss: 265.5341 - loglik: -2.4558e+02 - logprior: -1.9950e+01
Epoch 2/2
10/10 - 7s - loss: 235.1064 - loglik: -2.3117e+02 - logprior: -3.9324e+00
Fitted a model with MAP estimate = -228.8780
expansions: [(0, 29)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]
Fitting a model of length 204 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 259.2423 - loglik: -2.4125e+02 - logprior: -1.7991e+01
Epoch 2/10
10/10 - 8s - loss: 230.5483 - loglik: -2.2753e+02 - logprior: -3.0157e+00
Epoch 3/10
10/10 - 8s - loss: 217.1234 - loglik: -2.1686e+02 - logprior: -2.6044e-01
Epoch 4/10
10/10 - 9s - loss: 210.1039 - loglik: -2.1074e+02 - logprior: 0.6407
Epoch 5/10
10/10 - 8s - loss: 207.6624 - loglik: -2.0881e+02 - logprior: 1.1462
Epoch 6/10
10/10 - 8s - loss: 206.1205 - loglik: -2.0753e+02 - logprior: 1.4144
Epoch 7/10
10/10 - 9s - loss: 204.4353 - loglik: -2.0601e+02 - logprior: 1.5739
Epoch 8/10
10/10 - 8s - loss: 204.3949 - loglik: -2.0613e+02 - logprior: 1.7376
Epoch 9/10
10/10 - 9s - loss: 203.0439 - loglik: -2.0492e+02 - logprior: 1.8803
Epoch 10/10
10/10 - 9s - loss: 203.3344 - loglik: -2.0534e+02 - logprior: 2.0051
Fitted a model with MAP estimate = -203.0854
Time for alignment: 189.6893
Fitting a model of length 162 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 499.7896 - loglik: -4.8067e+02 - logprior: -1.9119e+01
Epoch 2/10
10/10 - 6s - loss: 393.0552 - loglik: -3.8959e+02 - logprior: -3.4687e+00
Epoch 3/10
10/10 - 6s - loss: 308.8539 - loglik: -3.0738e+02 - logprior: -1.4729e+00
Epoch 4/10
10/10 - 6s - loss: 268.2909 - loglik: -2.6681e+02 - logprior: -1.4847e+00
Epoch 5/10
10/10 - 6s - loss: 255.1666 - loglik: -2.5393e+02 - logprior: -1.2319e+00
Epoch 6/10
10/10 - 7s - loss: 250.1163 - loglik: -2.4900e+02 - logprior: -1.1198e+00
Epoch 7/10
10/10 - 6s - loss: 247.0067 - loglik: -2.4598e+02 - logprior: -1.0219e+00
Epoch 8/10
10/10 - 6s - loss: 247.4305 - loglik: -2.4637e+02 - logprior: -1.0628e+00
Fitted a model with MAP estimate = -246.1511
expansions: [(50, 1), (61, 1), (80, 1), (91, 1), (108, 1), (137, 1)]
discards: [0]
Fitting a model of length 167 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 280.9332 - loglik: -2.5802e+02 - logprior: -2.2915e+01
Epoch 2/2
10/10 - 6s - loss: 254.8495 - loglik: -2.4598e+02 - logprior: -8.8685e+00
Fitted a model with MAP estimate = -251.6612
expansions: [(0, 16)]
discards: [ 0 43 44 45]
Fitting a model of length 179 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 271.9351 - loglik: -2.5321e+02 - logprior: -1.8726e+01
Epoch 2/2
10/10 - 7s - loss: 247.2905 - loglik: -2.4274e+02 - logprior: -4.5521e+00
Fitted a model with MAP estimate = -241.6804
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 58]
Fitting a model of length 163 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 276.5742 - loglik: -2.5527e+02 - logprior: -2.1306e+01
Epoch 2/10
10/10 - 6s - loss: 255.1035 - loglik: -2.4969e+02 - logprior: -5.4138e+00
Epoch 3/10
10/10 - 6s - loss: 252.4467 - loglik: -2.5112e+02 - logprior: -1.3259e+00
Epoch 4/10
10/10 - 7s - loss: 247.0213 - loglik: -2.4686e+02 - logprior: -1.5788e-01
Epoch 5/10
10/10 - 7s - loss: 245.8733 - loglik: -2.4602e+02 - logprior: 0.1467
Epoch 6/10
10/10 - 7s - loss: 246.4077 - loglik: -2.4680e+02 - logprior: 0.3903
Fitted a model with MAP estimate = -244.9830
Time for alignment: 141.7602
Fitting a model of length 162 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 499.2599 - loglik: -4.8014e+02 - logprior: -1.9119e+01
Epoch 2/10
10/10 - 7s - loss: 396.5271 - loglik: -3.9314e+02 - logprior: -3.3860e+00
Epoch 3/10
10/10 - 6s - loss: 317.3503 - loglik: -3.1626e+02 - logprior: -1.0904e+00
Epoch 4/10
10/10 - 7s - loss: 277.8129 - loglik: -2.7700e+02 - logprior: -8.1452e-01
Epoch 5/10
10/10 - 7s - loss: 263.0075 - loglik: -2.6232e+02 - logprior: -6.8862e-01
Epoch 6/10
10/10 - 7s - loss: 255.7605 - loglik: -2.5529e+02 - logprior: -4.6667e-01
Epoch 7/10
10/10 - 8s - loss: 254.8775 - loglik: -2.5447e+02 - logprior: -4.0868e-01
Epoch 8/10
10/10 - 7s - loss: 252.7636 - loglik: -2.5239e+02 - logprior: -3.7135e-01
Epoch 9/10
10/10 - 7s - loss: 253.9532 - loglik: -2.5366e+02 - logprior: -2.9519e-01
Fitted a model with MAP estimate = -252.9038
expansions: [(0, 36), (1, 1), (25, 2), (26, 1), (27, 1), (50, 1), (59, 1), (80, 1), (91, 1), (96, 1), (137, 1)]
discards: []
Fitting a model of length 209 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 13s - loss: 277.9398 - loglik: -2.5536e+02 - logprior: -2.2579e+01
Epoch 2/2
10/10 - 10s - loss: 234.8634 - loglik: -2.2935e+02 - logprior: -5.5155e+00
Fitted a model with MAP estimate = -223.3054
expansions: [(0, 17)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]
Fitting a model of length 192 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 13s - loss: 264.2574 - loglik: -2.4425e+02 - logprior: -2.0009e+01
Epoch 2/2
10/10 - 8s - loss: 236.5483 - loglik: -2.3259e+02 - logprior: -3.9626e+00
Fitted a model with MAP estimate = -228.8789
expansions: [(0, 29)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]
Fitting a model of length 204 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 13s - loss: 260.5134 - loglik: -2.4250e+02 - logprior: -1.8009e+01
Epoch 2/10
10/10 - 11s - loss: 229.0088 - loglik: -2.2601e+02 - logprior: -2.9952e+00
Epoch 3/10
10/10 - 11s - loss: 218.1805 - loglik: -2.1795e+02 - logprior: -2.3217e-01
Epoch 4/10
10/10 - 11s - loss: 209.8293 - loglik: -2.1046e+02 - logprior: 0.6338
Epoch 5/10
10/10 - 11s - loss: 206.9861 - loglik: -2.0812e+02 - logprior: 1.1311
Epoch 6/10
10/10 - 12s - loss: 205.5282 - loglik: -2.0694e+02 - logprior: 1.4075
Epoch 7/10
10/10 - 11s - loss: 206.0996 - loglik: -2.0769e+02 - logprior: 1.5944
Fitted a model with MAP estimate = -204.1093
Time for alignment: 209.9175
Computed alignments with likelihoods: ['-203.0854', '-241.6804', '-204.1093']
Best model has likelihood: -203.0854
SP score = 0.4389
Training of 3 independent models on file GEL.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33280142b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b7e8ff40>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 245.8595 - loglik: -2.2559e+02 - logprior: -2.0265e+01
Epoch 2/10
10/10 - 1s - loss: 215.4131 - loglik: -2.0999e+02 - logprior: -5.4253e+00
Epoch 3/10
10/10 - 1s - loss: 197.9931 - loglik: -1.9514e+02 - logprior: -2.8566e+00
Epoch 4/10
10/10 - 1s - loss: 189.4024 - loglik: -1.8726e+02 - logprior: -2.1374e+00
Epoch 5/10
10/10 - 1s - loss: 186.2984 - loglik: -1.8441e+02 - logprior: -1.8898e+00
Epoch 6/10
10/10 - 1s - loss: 183.9975 - loglik: -1.8228e+02 - logprior: -1.7169e+00
Epoch 7/10
10/10 - 1s - loss: 183.3367 - loglik: -1.8188e+02 - logprior: -1.4560e+00
Epoch 8/10
10/10 - 1s - loss: 182.4109 - loglik: -1.8110e+02 - logprior: -1.3144e+00
Epoch 9/10
10/10 - 1s - loss: 181.8901 - loglik: -1.8058e+02 - logprior: -1.3133e+00
Epoch 10/10
10/10 - 1s - loss: 181.5169 - loglik: -1.8021e+02 - logprior: -1.3041e+00
Fitted a model with MAP estimate = -181.4113
expansions: [(0, 2), (7, 2), (8, 2), (23, 1), (41, 3), (42, 2), (43, 1), (44, 3), (51, 1), (53, 1)]
discards: []
Fitting a model of length 80 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 213.4278 - loglik: -1.8673e+02 - logprior: -2.6695e+01
Epoch 2/2
10/10 - 1s - loss: 188.8891 - loglik: -1.8065e+02 - logprior: -8.2372e+00
Fitted a model with MAP estimate = -183.6940
expansions: []
discards: [ 0 10 12 49 55 57]
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 204.4560 - loglik: -1.8121e+02 - logprior: -2.3243e+01
Epoch 2/2
10/10 - 1s - loss: 187.7206 - loglik: -1.7868e+02 - logprior: -9.0370e+00
Fitted a model with MAP estimate = -184.7231
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 198.0988 - loglik: -1.7785e+02 - logprior: -2.0253e+01
Epoch 2/10
10/10 - 1s - loss: 182.3319 - loglik: -1.7697e+02 - logprior: -5.3594e+00
Epoch 3/10
10/10 - 1s - loss: 178.8124 - loglik: -1.7655e+02 - logprior: -2.2648e+00
Epoch 4/10
10/10 - 1s - loss: 177.2379 - loglik: -1.7598e+02 - logprior: -1.2599e+00
Epoch 5/10
10/10 - 1s - loss: 176.0271 - loglik: -1.7515e+02 - logprior: -8.7594e-01
Epoch 6/10
10/10 - 1s - loss: 175.5376 - loglik: -1.7481e+02 - logprior: -7.2257e-01
Epoch 7/10
10/10 - 1s - loss: 175.1233 - loglik: -1.7457e+02 - logprior: -5.5353e-01
Epoch 8/10
10/10 - 1s - loss: 174.8811 - loglik: -1.7452e+02 - logprior: -3.5912e-01
Epoch 9/10
10/10 - 1s - loss: 174.7082 - loglik: -1.7445e+02 - logprior: -2.6019e-01
Epoch 10/10
10/10 - 1s - loss: 174.3990 - loglik: -1.7419e+02 - logprior: -2.0447e-01
Fitted a model with MAP estimate = -174.1998
Time for alignment: 39.1335
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 245.8796 - loglik: -2.2562e+02 - logprior: -2.0263e+01
Epoch 2/10
10/10 - 1s - loss: 215.9447 - loglik: -2.1052e+02 - logprior: -5.4224e+00
Epoch 3/10
10/10 - 1s - loss: 200.1475 - loglik: -1.9726e+02 - logprior: -2.8848e+00
Epoch 4/10
10/10 - 1s - loss: 191.1851 - loglik: -1.8894e+02 - logprior: -2.2480e+00
Epoch 5/10
10/10 - 1s - loss: 186.2406 - loglik: -1.8416e+02 - logprior: -2.0758e+00
Epoch 6/10
10/10 - 1s - loss: 184.2782 - loglik: -1.8238e+02 - logprior: -1.9010e+00
Epoch 7/10
10/10 - 1s - loss: 183.1311 - loglik: -1.8152e+02 - logprior: -1.6139e+00
Epoch 8/10
10/10 - 1s - loss: 182.4754 - loglik: -1.8101e+02 - logprior: -1.4688e+00
Epoch 9/10
10/10 - 1s - loss: 182.2204 - loglik: -1.8076e+02 - logprior: -1.4584e+00
Epoch 10/10
10/10 - 1s - loss: 181.8222 - loglik: -1.8039e+02 - logprior: -1.4318e+00
Fitted a model with MAP estimate = -181.7761
expansions: [(0, 2), (8, 1), (9, 1), (10, 1), (34, 1), (41, 2), (42, 2), (44, 2), (46, 1), (51, 1), (53, 1)]
discards: []
Fitting a model of length 77 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 214.0174 - loglik: -1.8729e+02 - logprior: -2.6729e+01
Epoch 2/2
10/10 - 1s - loss: 188.4701 - loglik: -1.8035e+02 - logprior: -8.1245e+00
Fitted a model with MAP estimate = -183.4964
expansions: []
discards: [ 0 56]
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.4911 - loglik: -1.8034e+02 - logprior: -2.3148e+01
Epoch 2/2
10/10 - 1s - loss: 188.2687 - loglik: -1.7919e+02 - logprior: -9.0808e+00
Fitted a model with MAP estimate = -184.9518
expansions: [(0, 2)]
discards: [ 0 11]
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 199.0831 - loglik: -1.7884e+02 - logprior: -2.0239e+01
Epoch 2/10
10/10 - 1s - loss: 182.9723 - loglik: -1.7760e+02 - logprior: -5.3726e+00
Epoch 3/10
10/10 - 1s - loss: 179.4824 - loglik: -1.7717e+02 - logprior: -2.3077e+00
Epoch 4/10
10/10 - 1s - loss: 177.9166 - loglik: -1.7664e+02 - logprior: -1.2785e+00
Epoch 5/10
10/10 - 1s - loss: 176.8121 - loglik: -1.7592e+02 - logprior: -8.9376e-01
Epoch 6/10
10/10 - 1s - loss: 176.0294 - loglik: -1.7530e+02 - logprior: -7.3422e-01
Epoch 7/10
10/10 - 1s - loss: 175.5169 - loglik: -1.7495e+02 - logprior: -5.6765e-01
Epoch 8/10
10/10 - 1s - loss: 175.2743 - loglik: -1.7492e+02 - logprior: -3.5020e-01
Epoch 9/10
10/10 - 1s - loss: 175.1866 - loglik: -1.7496e+02 - logprior: -2.2661e-01
Epoch 10/10
10/10 - 1s - loss: 174.7416 - loglik: -1.7458e+02 - logprior: -1.6476e-01
Fitted a model with MAP estimate = -174.5662
Time for alignment: 36.8869
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 245.9662 - loglik: -2.2571e+02 - logprior: -2.0261e+01
Epoch 2/10
10/10 - 1s - loss: 215.6362 - loglik: -2.1022e+02 - logprior: -5.4155e+00
Epoch 3/10
10/10 - 1s - loss: 197.9801 - loglik: -1.9510e+02 - logprior: -2.8814e+00
Epoch 4/10
10/10 - 1s - loss: 189.2494 - loglik: -1.8705e+02 - logprior: -2.2010e+00
Epoch 5/10
10/10 - 1s - loss: 185.3499 - loglik: -1.8336e+02 - logprior: -1.9854e+00
Epoch 6/10
10/10 - 1s - loss: 184.0023 - loglik: -1.8216e+02 - logprior: -1.8427e+00
Epoch 7/10
10/10 - 1s - loss: 182.8056 - loglik: -1.8122e+02 - logprior: -1.5849e+00
Epoch 8/10
10/10 - 1s - loss: 182.6585 - loglik: -1.8120e+02 - logprior: -1.4635e+00
Epoch 9/10
10/10 - 1s - loss: 181.9953 - loglik: -1.8053e+02 - logprior: -1.4658e+00
Epoch 10/10
10/10 - 1s - loss: 181.9891 - loglik: -1.8054e+02 - logprior: -1.4489e+00
Fitted a model with MAP estimate = -181.6808
expansions: [(0, 2), (8, 1), (9, 1), (23, 1), (41, 2), (42, 3), (44, 1), (48, 1), (51, 1), (53, 1)]
discards: []
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 211.7020 - loglik: -1.8497e+02 - logprior: -2.6733e+01
Epoch 2/2
10/10 - 1s - loss: 187.4309 - loglik: -1.7931e+02 - logprior: -8.1242e+00
Fitted a model with MAP estimate = -183.0431
expansions: []
discards: [0]
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.1129 - loglik: -1.7992e+02 - logprior: -2.3194e+01
Epoch 2/2
10/10 - 1s - loss: 187.9756 - loglik: -1.7892e+02 - logprior: -9.0583e+00
Fitted a model with MAP estimate = -185.0343
expansions: [(0, 2)]
discards: [ 0 48]
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 198.9631 - loglik: -1.7874e+02 - logprior: -2.0227e+01
Epoch 2/10
10/10 - 1s - loss: 182.9809 - loglik: -1.7763e+02 - logprior: -5.3501e+00
Epoch 3/10
10/10 - 1s - loss: 179.6848 - loglik: -1.7741e+02 - logprior: -2.2735e+00
Epoch 4/10
10/10 - 1s - loss: 177.9603 - loglik: -1.7670e+02 - logprior: -1.2601e+00
Epoch 5/10
10/10 - 1s - loss: 177.1203 - loglik: -1.7624e+02 - logprior: -8.7649e-01
Epoch 6/10
10/10 - 1s - loss: 176.3042 - loglik: -1.7558e+02 - logprior: -7.2107e-01
Epoch 7/10
10/10 - 1s - loss: 175.9810 - loglik: -1.7543e+02 - logprior: -5.5391e-01
Epoch 8/10
10/10 - 1s - loss: 175.5635 - loglik: -1.7520e+02 - logprior: -3.6083e-01
Epoch 9/10
10/10 - 1s - loss: 175.2400 - loglik: -1.7500e+02 - logprior: -2.4236e-01
Epoch 10/10
10/10 - 1s - loss: 174.9786 - loglik: -1.7479e+02 - logprior: -1.9104e-01
Fitted a model with MAP estimate = -174.8330
Time for alignment: 36.7064
Computed alignments with likelihoods: ['-174.1998', '-174.5662', '-174.8330']
Best model has likelihood: -174.1998
SP score = 0.7075
Training of 3 independent models on file hr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b7f66820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f36885c3340>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 198.7574 - loglik: -1.8727e+02 - logprior: -1.1484e+01
Epoch 2/10
11/11 - 1s - loss: 158.1817 - loglik: -1.5496e+02 - logprior: -3.2188e+00
Epoch 3/10
11/11 - 1s - loss: 125.2526 - loglik: -1.2286e+02 - logprior: -2.3934e+00
Epoch 4/10
11/11 - 1s - loss: 109.2772 - loglik: -1.0701e+02 - logprior: -2.2689e+00
Epoch 5/10
11/11 - 1s - loss: 104.4151 - loglik: -1.0243e+02 - logprior: -1.9835e+00
Epoch 6/10
11/11 - 1s - loss: 102.8191 - loglik: -1.0084e+02 - logprior: -1.9768e+00
Epoch 7/10
11/11 - 1s - loss: 101.0699 - loglik: -9.9085e+01 - logprior: -1.9852e+00
Epoch 8/10
11/11 - 1s - loss: 100.7350 - loglik: -9.8779e+01 - logprior: -1.9563e+00
Epoch 9/10
11/11 - 1s - loss: 100.2924 - loglik: -9.8299e+01 - logprior: -1.9930e+00
Epoch 10/10
11/11 - 1s - loss: 100.1485 - loglik: -9.8142e+01 - logprior: -2.0067e+00
Fitted a model with MAP estimate = -100.0097
expansions: [(0, 3), (15, 1), (26, 1), (27, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (33, 1), (34, 1), (38, 1), (51, 1)]
discards: []
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 112.5755 - loglik: -9.8869e+01 - logprior: -1.3707e+01
Epoch 2/2
11/11 - 1s - loss: 95.5461 - loglik: -9.1322e+01 - logprior: -4.2237e+00
Fitted a model with MAP estimate = -92.8292
expansions: []
discards: [ 0 36 39 44]
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 105.4282 - loglik: -9.2334e+01 - logprior: -1.3094e+01
Epoch 2/2
11/11 - 1s - loss: 96.5154 - loglik: -9.1146e+01 - logprior: -5.3694e+00
Fitted a model with MAP estimate = -94.2164
expansions: []
discards: [0]
Fitting a model of length 69 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 103.7181 - loglik: -9.1853e+01 - logprior: -1.1865e+01
Epoch 2/10
11/11 - 1s - loss: 94.3046 - loglik: -9.0892e+01 - logprior: -3.4126e+00
Epoch 3/10
11/11 - 1s - loss: 92.1603 - loglik: -9.0011e+01 - logprior: -2.1493e+00
Epoch 4/10
11/11 - 1s - loss: 91.5939 - loglik: -9.0064e+01 - logprior: -1.5301e+00
Epoch 5/10
11/11 - 1s - loss: 90.9912 - loglik: -8.9721e+01 - logprior: -1.2699e+00
Epoch 6/10
11/11 - 1s - loss: 90.1804 - loglik: -8.8968e+01 - logprior: -1.2123e+00
Epoch 7/10
11/11 - 1s - loss: 89.9076 - loglik: -8.8755e+01 - logprior: -1.1530e+00
Epoch 8/10
11/11 - 1s - loss: 90.0471 - loglik: -8.8889e+01 - logprior: -1.1585e+00
Fitted a model with MAP estimate = -89.6555
Time for alignment: 33.5759
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 199.2377 - loglik: -1.8776e+02 - logprior: -1.1482e+01
Epoch 2/10
11/11 - 1s - loss: 158.7382 - loglik: -1.5553e+02 - logprior: -3.2048e+00
Epoch 3/10
11/11 - 1s - loss: 126.3476 - loglik: -1.2398e+02 - logprior: -2.3653e+00
Epoch 4/10
11/11 - 1s - loss: 111.3345 - loglik: -1.0911e+02 - logprior: -2.2245e+00
Epoch 5/10
11/11 - 1s - loss: 106.9844 - loglik: -1.0508e+02 - logprior: -1.9049e+00
Epoch 6/10
11/11 - 1s - loss: 104.8427 - loglik: -1.0295e+02 - logprior: -1.8948e+00
Epoch 7/10
11/11 - 1s - loss: 103.6832 - loglik: -1.0179e+02 - logprior: -1.8941e+00
Epoch 8/10
11/11 - 1s - loss: 103.3940 - loglik: -1.0156e+02 - logprior: -1.8378e+00
Epoch 9/10
11/11 - 1s - loss: 103.1710 - loglik: -1.0132e+02 - logprior: -1.8467e+00
Epoch 10/10
11/11 - 1s - loss: 102.4425 - loglik: -1.0061e+02 - logprior: -1.8342e+00
Fitted a model with MAP estimate = -102.6557
expansions: [(0, 3), (14, 1), (15, 1), (27, 2), (28, 3), (29, 1), (30, 1), (31, 1), (38, 2), (43, 1)]
discards: []
Fitting a model of length 72 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 112.5191 - loglik: -9.8844e+01 - logprior: -1.3675e+01
Epoch 2/2
11/11 - 1s - loss: 95.9624 - loglik: -9.1841e+01 - logprior: -4.1211e+00
Fitted a model with MAP estimate = -93.0438
expansions: []
discards: [ 0 33]
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 105.6965 - loglik: -9.2618e+01 - logprior: -1.3079e+01
Epoch 2/2
11/11 - 1s - loss: 96.3689 - loglik: -9.1025e+01 - logprior: -5.3442e+00
Fitted a model with MAP estimate = -94.2397
expansions: [(0, 1)]
discards: [0]
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 101.9316 - loglik: -9.1162e+01 - logprior: -1.0769e+01
Epoch 2/10
11/11 - 1s - loss: 93.0183 - loglik: -9.0106e+01 - logprior: -2.9126e+00
Epoch 3/10
11/11 - 1s - loss: 91.2696 - loglik: -8.9529e+01 - logprior: -1.7403e+00
Epoch 4/10
11/11 - 1s - loss: 90.6264 - loglik: -8.9086e+01 - logprior: -1.5405e+00
Epoch 5/10
11/11 - 1s - loss: 90.3057 - loglik: -8.8839e+01 - logprior: -1.4663e+00
Epoch 6/10
11/11 - 1s - loss: 89.3625 - loglik: -8.8052e+01 - logprior: -1.3103e+00
Epoch 7/10
11/11 - 1s - loss: 89.0991 - loglik: -8.7882e+01 - logprior: -1.2167e+00
Epoch 8/10
11/11 - 1s - loss: 89.0242 - loglik: -8.7820e+01 - logprior: -1.2037e+00
Epoch 9/10
11/11 - 1s - loss: 88.9860 - loglik: -8.7780e+01 - logprior: -1.2065e+00
Epoch 10/10
11/11 - 1s - loss: 88.7878 - loglik: -8.7607e+01 - logprior: -1.1811e+00
Fitted a model with MAP estimate = -88.6824
Time for alignment: 34.2129
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 198.7633 - loglik: -1.8728e+02 - logprior: -1.1484e+01
Epoch 2/10
11/11 - 1s - loss: 157.7021 - loglik: -1.5449e+02 - logprior: -3.2113e+00
Epoch 3/10
11/11 - 1s - loss: 123.0965 - loglik: -1.2071e+02 - logprior: -2.3844e+00
Epoch 4/10
11/11 - 1s - loss: 109.3578 - loglik: -1.0710e+02 - logprior: -2.2534e+00
Epoch 5/10
11/11 - 1s - loss: 104.8378 - loglik: -1.0286e+02 - logprior: -1.9745e+00
Epoch 6/10
11/11 - 1s - loss: 103.6725 - loglik: -1.0171e+02 - logprior: -1.9659e+00
Epoch 7/10
11/11 - 1s - loss: 102.7419 - loglik: -1.0078e+02 - logprior: -1.9596e+00
Epoch 8/10
11/11 - 1s - loss: 101.8350 - loglik: -9.9920e+01 - logprior: -1.9154e+00
Epoch 9/10
11/11 - 1s - loss: 101.7017 - loglik: -9.9775e+01 - logprior: -1.9270e+00
Epoch 10/10
11/11 - 1s - loss: 101.4835 - loglik: -9.9572e+01 - logprior: -1.9119e+00
Fitted a model with MAP estimate = -101.4699
expansions: [(0, 3), (15, 1), (26, 1), (28, 1), (29, 3), (30, 2), (31, 1), (32, 2), (33, 1), (36, 1), (37, 1), (43, 1)]
discards: []
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 113.5342 - loglik: -9.9824e+01 - logprior: -1.3710e+01
Epoch 2/2
11/11 - 1s - loss: 95.4661 - loglik: -9.1231e+01 - logprior: -4.2354e+00
Fitted a model with MAP estimate = -92.9552
expansions: []
discards: [ 0 35 39 44]
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 105.6802 - loglik: -9.2571e+01 - logprior: -1.3109e+01
Epoch 2/2
11/11 - 1s - loss: 96.2519 - loglik: -9.0875e+01 - logprior: -5.3769e+00
Fitted a model with MAP estimate = -94.2415
expansions: []
discards: [0]
Fitting a model of length 69 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 103.4984 - loglik: -9.1628e+01 - logprior: -1.1870e+01
Epoch 2/10
11/11 - 1s - loss: 94.4321 - loglik: -9.1018e+01 - logprior: -3.4145e+00
Epoch 3/10
11/11 - 1s - loss: 92.3381 - loglik: -9.0184e+01 - logprior: -2.1537e+00
Epoch 4/10
11/11 - 1s - loss: 91.3650 - loglik: -8.9822e+01 - logprior: -1.5426e+00
Epoch 5/10
11/11 - 1s - loss: 91.1666 - loglik: -8.9891e+01 - logprior: -1.2752e+00
Epoch 6/10
11/11 - 1s - loss: 90.2900 - loglik: -8.9065e+01 - logprior: -1.2250e+00
Epoch 7/10
11/11 - 1s - loss: 90.1598 - loglik: -8.9001e+01 - logprior: -1.1592e+00
Epoch 8/10
11/11 - 1s - loss: 89.4439 - loglik: -8.8281e+01 - logprior: -1.1633e+00
Epoch 9/10
11/11 - 1s - loss: 89.7325 - loglik: -8.8597e+01 - logprior: -1.1359e+00
Fitted a model with MAP estimate = -89.5684
Time for alignment: 32.6681
Computed alignments with likelihoods: ['-89.6555', '-88.6824', '-89.5684']
Best model has likelihood: -88.6824
SP score = 0.9957
Training of 3 independent models on file rvp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34a6a7b490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32c86b6c10>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 172.1265 - loglik: -1.7139e+02 - logprior: -7.3628e-01
Epoch 2/10
42/42 - 4s - loss: 81.6728 - loglik: -8.1038e+01 - logprior: -6.3494e-01
Epoch 3/10
42/42 - 3s - loss: 78.8998 - loglik: -7.8282e+01 - logprior: -6.1811e-01
Epoch 4/10
42/42 - 4s - loss: 78.6429 - loglik: -7.8048e+01 - logprior: -5.9446e-01
Epoch 5/10
42/42 - 4s - loss: 78.1200 - loglik: -7.7530e+01 - logprior: -5.9008e-01
Epoch 6/10
42/42 - 3s - loss: 77.9324 - loglik: -7.7352e+01 - logprior: -5.8073e-01
Epoch 7/10
42/42 - 4s - loss: 77.5603 - loglik: -7.6987e+01 - logprior: -5.7301e-01
Epoch 8/10
42/42 - 4s - loss: 77.6237 - loglik: -7.7054e+01 - logprior: -5.6965e-01
Fitted a model with MAP estimate = -77.2218
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 7s - loss: 47.7779 - loglik: -4.6961e+01 - logprior: -8.1647e-01
Epoch 2/2
42/42 - 4s - loss: 34.2948 - loglik: -3.3692e+01 - logprior: -6.0318e-01
Fitted a model with MAP estimate = -33.3884
expansions: []
discards: [0]
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 38.6031 - loglik: -3.7665e+01 - logprior: -9.3797e-01
Epoch 2/2
42/42 - 4s - loss: 35.0032 - loglik: -3.4521e+01 - logprior: -4.8215e-01
Fitted a model with MAP estimate = -34.5584
expansions: []
discards: []
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 10s - loss: 34.4010 - loglik: -3.3809e+01 - logprior: -5.9201e-01
Epoch 2/10
59/59 - 6s - loss: 32.7900 - loglik: -3.2238e+01 - logprior: -5.5247e-01
Epoch 3/10
59/59 - 7s - loss: 33.1618 - loglik: -3.2622e+01 - logprior: -5.3961e-01
Fitted a model with MAP estimate = -32.1395
Time for alignment: 156.1909
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 171.9576 - loglik: -1.7123e+02 - logprior: -7.3017e-01
Epoch 2/10
42/42 - 4s - loss: 81.4649 - loglik: -8.0827e+01 - logprior: -6.3801e-01
Epoch 3/10
42/42 - 4s - loss: 78.7778 - loglik: -7.8160e+01 - logprior: -6.1818e-01
Epoch 4/10
42/42 - 4s - loss: 78.4752 - loglik: -7.7875e+01 - logprior: -6.0002e-01
Epoch 5/10
42/42 - 4s - loss: 78.4153 - loglik: -7.7832e+01 - logprior: -5.8339e-01
Epoch 6/10
42/42 - 4s - loss: 77.9224 - loglik: -7.7346e+01 - logprior: -5.7676e-01
Epoch 7/10
42/42 - 4s - loss: 77.6009 - loglik: -7.7028e+01 - logprior: -5.7309e-01
Epoch 8/10
42/42 - 4s - loss: 77.4331 - loglik: -7.6858e+01 - logprior: -5.7460e-01
Epoch 9/10
42/42 - 4s - loss: 77.4818 - loglik: -7.6910e+01 - logprior: -5.7200e-01
Fitted a model with MAP estimate = -77.1872
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 7s - loss: 47.7050 - loglik: -4.6876e+01 - logprior: -8.2872e-01
Epoch 2/2
42/42 - 4s - loss: 34.3520 - loglik: -3.3751e+01 - logprior: -6.0117e-01
Fitted a model with MAP estimate = -33.3891
expansions: []
discards: []
Fitting a model of length 96 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 9s - loss: 33.6857 - loglik: -3.3228e+01 - logprior: -4.5799e-01
Epoch 2/10
59/59 - 6s - loss: 32.5004 - loglik: -3.2106e+01 - logprior: -3.9467e-01
Epoch 3/10
59/59 - 6s - loss: 32.2182 - loglik: -3.1838e+01 - logprior: -3.8019e-01
Epoch 4/10
59/59 - 6s - loss: 31.5605 - loglik: -3.1190e+01 - logprior: -3.7045e-01
Epoch 5/10
59/59 - 7s - loss: 31.7049 - loglik: -3.1343e+01 - logprior: -3.6178e-01
Fitted a model with MAP estimate = -31.2326
Time for alignment: 136.9338
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 172.0667 - loglik: -1.7133e+02 - logprior: -7.3217e-01
Epoch 2/10
42/42 - 4s - loss: 81.7592 - loglik: -8.1128e+01 - logprior: -6.3121e-01
Epoch 3/10
42/42 - 4s - loss: 79.1141 - loglik: -7.8499e+01 - logprior: -6.1517e-01
Epoch 4/10
42/42 - 4s - loss: 78.4430 - loglik: -7.7844e+01 - logprior: -5.9930e-01
Epoch 5/10
42/42 - 4s - loss: 77.9831 - loglik: -7.7396e+01 - logprior: -5.8719e-01
Epoch 6/10
42/42 - 4s - loss: 77.8761 - loglik: -7.7297e+01 - logprior: -5.7896e-01
Epoch 7/10
42/42 - 4s - loss: 77.8143 - loglik: -7.7244e+01 - logprior: -5.6997e-01
Epoch 8/10
42/42 - 4s - loss: 77.2379 - loglik: -7.6660e+01 - logprior: -5.7778e-01
Epoch 9/10
42/42 - 4s - loss: 77.6103 - loglik: -7.7040e+01 - logprior: -5.7019e-01
Fitted a model with MAP estimate = -77.2025
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 47.9741 - loglik: -4.7153e+01 - logprior: -8.2142e-01
Epoch 2/2
42/42 - 5s - loss: 34.2961 - loglik: -3.3694e+01 - logprior: -6.0233e-01
Fitted a model with MAP estimate = -33.4033
expansions: []
discards: [0]
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 7s - loss: 38.4481 - loglik: -3.7516e+01 - logprior: -9.3212e-01
Epoch 2/2
42/42 - 5s - loss: 35.2923 - loglik: -3.4818e+01 - logprior: -4.7459e-01
Fitted a model with MAP estimate = -34.4079
expansions: []
discards: []
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 9s - loss: 34.3557 - loglik: -3.3737e+01 - logprior: -6.1836e-01
Epoch 2/10
59/59 - 7s - loss: 32.8897 - loglik: -3.2333e+01 - logprior: -5.5648e-01
Epoch 3/10
59/59 - 7s - loss: 33.0086 - loglik: -3.2467e+01 - logprior: -5.4194e-01
Fitted a model with MAP estimate = -32.1321
Time for alignment: 168.9673
Computed alignments with likelihoods: ['-32.1395', '-31.2326', '-32.1321']
Best model has likelihood: -31.2326
SP score = 0.2835
Training of 3 independent models on file mmp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3688474220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b8ab2190>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 124 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 486.1888 - loglik: -4.5700e+02 - logprior: -2.9187e+01
Epoch 2/10
10/10 - 5s - loss: 430.5566 - loglik: -4.2431e+02 - logprior: -6.2495e+00
Epoch 3/10
10/10 - 5s - loss: 392.6325 - loglik: -3.9030e+02 - logprior: -2.3340e+00
Epoch 4/10
10/10 - 5s - loss: 368.5266 - loglik: -3.6722e+02 - logprior: -1.3097e+00
Epoch 5/10
10/10 - 5s - loss: 359.7597 - loglik: -3.5879e+02 - logprior: -9.6913e-01
Epoch 6/10
10/10 - 5s - loss: 356.6666 - loglik: -3.5581e+02 - logprior: -8.5748e-01
Epoch 7/10
10/10 - 5s - loss: 354.5017 - loglik: -3.5380e+02 - logprior: -7.0359e-01
Epoch 8/10
10/10 - 5s - loss: 352.3741 - loglik: -3.5179e+02 - logprior: -5.7944e-01
Epoch 9/10
10/10 - 5s - loss: 352.6710 - loglik: -3.5214e+02 - logprior: -5.3471e-01
Fitted a model with MAP estimate = -352.5582
expansions: [(3, 1), (4, 1), (15, 1), (16, 4), (17, 1), (36, 2), (38, 2), (43, 1), (45, 2), (66, 1), (67, 1), (70, 1), (79, 6), (100, 1), (102, 1), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [0]
Fitting a model of length 156 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 391.1534 - loglik: -3.5812e+02 - logprior: -3.3035e+01
Epoch 2/2
10/10 - 7s - loss: 357.4929 - loglik: -3.4469e+02 - logprior: -1.2804e+01
Fitted a model with MAP estimate = -352.9629
expansions: [(0, 24)]
discards: [  0  47  59  60  87  99 139]
Fitting a model of length 173 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 378.0339 - loglik: -3.5144e+02 - logprior: -2.6597e+01
Epoch 2/2
10/10 - 8s - loss: 352.9500 - loglik: -3.4704e+02 - logprior: -5.9138e+00
Fitted a model with MAP estimate = -348.1311
expansions: [(79, 1), (80, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22 157]
Fitting a model of length 151 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 377.9964 - loglik: -3.4720e+02 - logprior: -3.0794e+01
Epoch 2/10
10/10 - 7s - loss: 349.8327 - loglik: -3.4251e+02 - logprior: -7.3257e+00
Epoch 3/10
10/10 - 7s - loss: 345.7907 - loglik: -3.4419e+02 - logprior: -1.6014e+00
Epoch 4/10
10/10 - 7s - loss: 341.5738 - loglik: -3.4197e+02 - logprior: 0.3998
Epoch 5/10
10/10 - 7s - loss: 339.8298 - loglik: -3.4113e+02 - logprior: 1.3010
Epoch 6/10
10/10 - 7s - loss: 340.4328 - loglik: -3.4214e+02 - logprior: 1.7067
Fitted a model with MAP estimate = -338.9987
Time for alignment: 137.8274
Fitting a model of length 124 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 486.3509 - loglik: -4.5717e+02 - logprior: -2.9186e+01
Epoch 2/10
10/10 - 5s - loss: 430.5040 - loglik: -4.2425e+02 - logprior: -6.2529e+00
Epoch 3/10
10/10 - 5s - loss: 389.5083 - loglik: -3.8709e+02 - logprior: -2.4194e+00
Epoch 4/10
10/10 - 5s - loss: 368.4697 - loglik: -3.6706e+02 - logprior: -1.4067e+00
Epoch 5/10
10/10 - 5s - loss: 358.6303 - loglik: -3.5771e+02 - logprior: -9.1666e-01
Epoch 6/10
10/10 - 5s - loss: 355.8438 - loglik: -3.5507e+02 - logprior: -7.7506e-01
Epoch 7/10
10/10 - 5s - loss: 354.4823 - loglik: -3.5379e+02 - logprior: -6.9270e-01
Epoch 8/10
10/10 - 5s - loss: 352.5631 - loglik: -3.5198e+02 - logprior: -5.7853e-01
Epoch 9/10
10/10 - 5s - loss: 353.2862 - loglik: -3.5277e+02 - logprior: -5.1458e-01
Fitted a model with MAP estimate = -352.4691
expansions: [(5, 1), (11, 1), (16, 5), (17, 1), (36, 2), (38, 2), (41, 1), (44, 2), (66, 2), (79, 6), (100, 1), (102, 1), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [ 0 45 46]
Fitting a model of length 153 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 392.7708 - loglik: -3.5978e+02 - logprior: -3.2989e+01
Epoch 2/2
10/10 - 7s - loss: 361.0640 - loglik: -3.4841e+02 - logprior: -1.2658e+01
Fitted a model with MAP estimate = -355.1880
expansions: [(0, 24), (16, 1), (57, 1)]
discards: [  0  47  96 136 137]
Fitting a model of length 174 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 375.2587 - loglik: -3.4889e+02 - logprior: -2.6368e+01
Epoch 2/2
10/10 - 8s - loss: 349.9326 - loglik: -3.4419e+02 - logprior: -5.7446e+00
Fitted a model with MAP estimate = -345.6478
expansions: [(104, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 81
 82]
Fitting a model of length 150 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 376.8362 - loglik: -3.4633e+02 - logprior: -3.0508e+01
Epoch 2/10
10/10 - 6s - loss: 350.2058 - loglik: -3.4295e+02 - logprior: -7.2561e+00
Epoch 3/10
10/10 - 6s - loss: 343.9836 - loglik: -3.4225e+02 - logprior: -1.7349e+00
Epoch 4/10
10/10 - 6s - loss: 341.1775 - loglik: -3.4127e+02 - logprior: 0.0930
Epoch 5/10
10/10 - 7s - loss: 338.4191 - loglik: -3.3929e+02 - logprior: 0.8663
Epoch 6/10
10/10 - 6s - loss: 338.3181 - loglik: -3.3963e+02 - logprior: 1.3146
Epoch 7/10
10/10 - 6s - loss: 337.0284 - loglik: -3.3866e+02 - logprior: 1.6292
Epoch 8/10
10/10 - 7s - loss: 336.5513 - loglik: -3.3845e+02 - logprior: 1.9030
Epoch 9/10
10/10 - 7s - loss: 336.0982 - loglik: -3.3821e+02 - logprior: 2.1077
Epoch 10/10
10/10 - 7s - loss: 336.3913 - loglik: -3.3866e+02 - logprior: 2.2736
Fitted a model with MAP estimate = -335.8272
Time for alignment: 159.0698
Fitting a model of length 124 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 485.3789 - loglik: -4.5619e+02 - logprior: -2.9191e+01
Epoch 2/10
10/10 - 5s - loss: 431.1073 - loglik: -4.2484e+02 - logprior: -6.2667e+00
Epoch 3/10
10/10 - 5s - loss: 394.5092 - loglik: -3.9205e+02 - logprior: -2.4617e+00
Epoch 4/10
10/10 - 5s - loss: 370.4961 - loglik: -3.6890e+02 - logprior: -1.6004e+00
Epoch 5/10
10/10 - 5s - loss: 361.7663 - loglik: -3.6046e+02 - logprior: -1.3093e+00
Epoch 6/10
10/10 - 5s - loss: 358.2950 - loglik: -3.5724e+02 - logprior: -1.0578e+00
Epoch 7/10
10/10 - 5s - loss: 354.9138 - loglik: -3.5407e+02 - logprior: -8.3964e-01
Epoch 8/10
10/10 - 5s - loss: 355.5593 - loglik: -3.5482e+02 - logprior: -7.4091e-01
Fitted a model with MAP estimate = -354.3853
expansions: [(10, 1), (11, 1), (15, 1), (16, 4), (17, 1), (36, 2), (42, 1), (58, 1), (66, 1), (67, 2), (79, 5), (100, 1), (108, 1), (111, 3), (112, 1), (113, 2)]
discards: [0]
Fitting a model of length 151 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 390.1554 - loglik: -3.5701e+02 - logprior: -3.3144e+01
Epoch 2/2
10/10 - 7s - loss: 359.6773 - loglik: -3.4680e+02 - logprior: -1.2882e+01
Fitted a model with MAP estimate = -354.3076
expansions: [(0, 22)]
discards: [ 0 55 56]
Fitting a model of length 170 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 376.1220 - loglik: -3.4953e+02 - logprior: -2.6592e+01
Epoch 2/2
10/10 - 8s - loss: 351.2252 - loglik: -3.4541e+02 - logprior: -5.8193e+00
Fitted a model with MAP estimate = -346.6918
expansions: [(76, 3)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]
Fitting a model of length 152 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 375.3248 - loglik: -3.4465e+02 - logprior: -3.0670e+01
Epoch 2/10
10/10 - 7s - loss: 348.4007 - loglik: -3.4115e+02 - logprior: -7.2493e+00
Epoch 3/10
10/10 - 6s - loss: 341.9902 - loglik: -3.4025e+02 - logprior: -1.7384e+00
Epoch 4/10
10/10 - 7s - loss: 340.5438 - loglik: -3.4076e+02 - logprior: 0.2142
Epoch 5/10
10/10 - 7s - loss: 336.8827 - loglik: -3.3793e+02 - logprior: 1.0510
Epoch 6/10
10/10 - 7s - loss: 336.5646 - loglik: -3.3800e+02 - logprior: 1.4312
Epoch 7/10
10/10 - 7s - loss: 335.9480 - loglik: -3.3765e+02 - logprior: 1.6996
Epoch 8/10
10/10 - 7s - loss: 335.6403 - loglik: -3.3763e+02 - logprior: 1.9924
Epoch 9/10
10/10 - 7s - loss: 335.2547 - loglik: -3.3752e+02 - logprior: 2.2641
Epoch 10/10
10/10 - 7s - loss: 334.5830 - loglik: -3.3702e+02 - logprior: 2.4374
Fitted a model with MAP estimate = -334.7667
Time for alignment: 157.0028
Computed alignments with likelihoods: ['-338.9987', '-335.8272', '-334.7667']
Best model has likelihood: -334.7667
SP score = 0.9580
Training of 3 independent models on file icd.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34a6c89be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32c86b6910>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 38s - loss: 853.9432 - loglik: -8.5125e+02 - logprior: -2.6888e+00
Epoch 2/10
29/29 - 36s - loss: 670.3547 - loglik: -6.6875e+02 - logprior: -1.6000e+00
Epoch 3/10
29/29 - 33s - loss: 639.3075 - loglik: -6.3741e+02 - logprior: -1.8962e+00
Epoch 4/10
29/29 - 32s - loss: 633.7701 - loglik: -6.3181e+02 - logprior: -1.9642e+00
Epoch 5/10
29/29 - 33s - loss: 630.3179 - loglik: -6.2835e+02 - logprior: -1.9647e+00
Epoch 6/10
29/29 - 35s - loss: 629.7424 - loglik: -6.2776e+02 - logprior: -1.9838e+00
Epoch 7/10
29/29 - 33s - loss: 628.3339 - loglik: -6.2629e+02 - logprior: -2.0419e+00
Epoch 8/10
29/29 - 31s - loss: 627.5892 - loglik: -6.2556e+02 - logprior: -2.0259e+00
Epoch 9/10
29/29 - 29s - loss: 628.5636 - loglik: -6.2658e+02 - logprior: -1.9883e+00
Fitted a model with MAP estimate = -626.8693
expansions: [(16, 1), (22, 1), (24, 1), (29, 1), (30, 1), (31, 1), (37, 1), (39, 1), (49, 2), (50, 2), (73, 1), (87, 1), (88, 1), (89, 1), (90, 4), (119, 2), (120, 1), (121, 2), (124, 1), (125, 1), (128, 1), (142, 1), (152, 1), (153, 1), (155, 1), (156, 1), (163, 1), (165, 1), (173, 1), (184, 1), (185, 1), (186, 2), (190, 1), (191, 1), (205, 1), (217, 2), (218, 2), (219, 1), (233, 1), (248, 1), (249, 1), (251, 1), (258, 1), (260, 2), (261, 1), (262, 1), (263, 2), (264, 1), (267, 2)]
discards: [0]
Fitting a model of length 340 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 51s - loss: 616.4226 - loglik: -6.1172e+02 - logprior: -4.7000e+00
Epoch 2/2
29/29 - 45s - loss: 590.5989 - loglik: -5.8872e+02 - logprior: -1.8740e+00
Fitted a model with MAP estimate = -585.2086
expansions: [(0, 2), (330, 2)]
discards: [  0  58  59 107 108 139 262 326 327]
Fitting a model of length 335 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 45s - loss: 594.5467 - loglik: -5.9189e+02 - logprior: -2.6519e+00
Epoch 2/2
29/29 - 48s - loss: 586.5672 - loglik: -5.8673e+02 - logprior: 0.1587
Fitted a model with MAP estimate = -584.9849
expansions: [(321, 2)]
discards: [0]
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 55s - loss: 595.2299 - loglik: -5.9124e+02 - logprior: -3.9883e+00
Epoch 2/10
29/29 - 53s - loss: 589.3029 - loglik: -5.8920e+02 - logprior: -9.9792e-02
Epoch 3/10
29/29 - 53s - loss: 582.3116 - loglik: -5.8330e+02 - logprior: 0.9852
Epoch 4/10
29/29 - 53s - loss: 582.5045 - loglik: -5.8331e+02 - logprior: 0.8036
Fitted a model with MAP estimate = -582.0370
Time for alignment: 863.7120
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 40s - loss: 855.0358 - loglik: -8.5232e+02 - logprior: -2.7165e+00
Epoch 2/10
29/29 - 37s - loss: 668.7564 - loglik: -6.6697e+02 - logprior: -1.7912e+00
Epoch 3/10
29/29 - 36s - loss: 643.6704 - loglik: -6.4164e+02 - logprior: -2.0301e+00
Epoch 4/10
29/29 - 35s - loss: 632.4844 - loglik: -6.3044e+02 - logprior: -2.0433e+00
Epoch 5/10
29/29 - 37s - loss: 628.4980 - loglik: -6.2643e+02 - logprior: -2.0678e+00
Epoch 6/10
29/29 - 37s - loss: 625.5352 - loglik: -6.2344e+02 - logprior: -2.0933e+00
Epoch 7/10
29/29 - 37s - loss: 626.1710 - loglik: -6.2404e+02 - logprior: -2.1291e+00
Fitted a model with MAP estimate = -624.5295
expansions: [(16, 1), (17, 1), (24, 2), (29, 1), (30, 1), (31, 1), (37, 1), (39, 1), (48, 1), (49, 1), (53, 1), (64, 1), (77, 1), (88, 1), (89, 1), (90, 2), (91, 2), (94, 1), (118, 1), (122, 2), (123, 1), (124, 1), (125, 1), (128, 1), (142, 1), (151, 1), (153, 1), (155, 1), (163, 1), (172, 1), (173, 1), (184, 1), (185, 1), (186, 1), (191, 1), (192, 1), (204, 1), (205, 1), (217, 2), (218, 3), (219, 1), (240, 1), (250, 1), (251, 3), (258, 1), (260, 1), (261, 1), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [ 0 97 98]
Fitting a model of length 341 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 54s - loss: 616.7944 - loglik: -6.1184e+02 - logprior: -4.9536e+00
Epoch 2/2
29/29 - 48s - loss: 589.3606 - loglik: -5.8714e+02 - logprior: -2.2183e+00
Fitted a model with MAP estimate = -584.4878
expansions: [(0, 2), (114, 1)]
discards: [  0  36 108 258 259 260 325 326]
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 52s - loss: 592.2727 - loglik: -5.8958e+02 - logprior: -2.6918e+00
Epoch 2/2
29/29 - 45s - loss: 585.4304 - loglik: -5.8536e+02 - logprior: -6.6398e-02
Fitted a model with MAP estimate = -582.9106
expansions: [(258, 2), (322, 2)]
discards: [  0 105 294]
Fitting a model of length 337 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 46s - loss: 593.9532 - loglik: -5.8974e+02 - logprior: -4.2108e+00
Epoch 2/10
29/29 - 48s - loss: 585.9317 - loglik: -5.8536e+02 - logprior: -5.7326e-01
Epoch 3/10
29/29 - 43s - loss: 581.5595 - loglik: -5.8234e+02 - logprior: 0.7795
Epoch 4/10
29/29 - 41s - loss: 579.0012 - loglik: -5.7968e+02 - logprior: 0.6794
Epoch 5/10
29/29 - 44s - loss: 579.4954 - loglik: -5.8054e+02 - logprior: 1.0417
Fitted a model with MAP estimate = -578.1456
Time for alignment: 842.0352
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 36s - loss: 855.2645 - loglik: -8.5256e+02 - logprior: -2.7067e+00
Epoch 2/10
29/29 - 37s - loss: 671.0860 - loglik: -6.6933e+02 - logprior: -1.7549e+00
Epoch 3/10
29/29 - 38s - loss: 644.6557 - loglik: -6.4267e+02 - logprior: -1.9880e+00
Epoch 4/10
29/29 - 38s - loss: 639.7717 - loglik: -6.3777e+02 - logprior: -2.0005e+00
Epoch 5/10
29/29 - 39s - loss: 636.0415 - loglik: -6.3399e+02 - logprior: -2.0487e+00
Epoch 6/10
29/29 - 39s - loss: 637.0294 - loglik: -6.3497e+02 - logprior: -2.0586e+00
Fitted a model with MAP estimate = -635.0399
expansions: [(16, 1), (22, 1), (24, 1), (29, 1), (30, 1), (37, 2), (39, 1), (48, 1), (49, 1), (50, 2), (73, 1), (77, 1), (88, 1), (89, 2), (90, 3), (94, 1), (119, 2), (120, 2), (121, 1), (123, 1), (124, 1), (125, 1), (126, 1), (142, 2), (152, 1), (153, 1), (154, 2), (155, 1), (166, 2), (173, 1), (184, 1), (185, 1), (186, 1), (191, 1), (192, 1), (204, 1), (205, 1), (217, 2), (218, 2), (219, 1), (234, 1), (248, 1), (249, 1), (251, 1), (258, 1), (260, 2), (261, 1), (262, 1), (263, 2), (264, 1), (267, 2)]
discards: [0]
Fitting a model of length 344 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 59s - loss: 622.6686 - loglik: -6.1790e+02 - logprior: -4.7663e+00
Epoch 2/2
29/29 - 55s - loss: 594.8403 - loglik: -5.9299e+02 - logprior: -1.8530e+00
Fitted a model with MAP estimate = -590.8401
expansions: [(0, 2), (35, 1)]
discards: [  0  41  60 104 107 171 187 202 266 330 331]
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 51s - loss: 599.7750 - loglik: -5.9731e+02 - logprior: -2.4614e+00
Epoch 2/2
29/29 - 46s - loss: 594.6479 - loglik: -5.9479e+02 - logprior: 0.1442
Fitted a model with MAP estimate = -590.7240
expansions: [(38, 1), (324, 2)]
discards: [0]
Fitting a model of length 338 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 45s - loss: 600.5226 - loglik: -5.9641e+02 - logprior: -4.1089e+00
Epoch 2/10
29/29 - 45s - loss: 593.7102 - loglik: -5.9345e+02 - logprior: -2.6129e-01
Epoch 3/10
29/29 - 49s - loss: 589.0714 - loglik: -5.8955e+02 - logprior: 0.4799
Epoch 4/10
29/29 - 49s - loss: 588.7098 - loglik: -5.8963e+02 - logprior: 0.9197
Epoch 5/10
29/29 - 50s - loss: 586.0969 - loglik: -5.8694e+02 - logprior: 0.8413
Epoch 6/10
29/29 - 51s - loss: 585.6887 - loglik: -5.8699e+02 - logprior: 1.3019
Epoch 7/10
29/29 - 47s - loss: 586.5975 - loglik: -5.8774e+02 - logprior: 1.1390
Fitted a model with MAP estimate = -585.2933
Time for alignment: 952.8632
Computed alignments with likelihoods: ['-582.0370', '-578.1456', '-585.2933']
Best model has likelihood: -578.1456
SP score = 0.8949
Training of 3 independent models on file seatoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f349ce129d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33205d92e0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 616.9520 - loglik: -1.2820e+02 - logprior: -4.8875e+02
Epoch 2/10
10/10 - 0s - loss: 241.7299 - loglik: -1.0679e+02 - logprior: -1.3494e+02
Epoch 3/10
10/10 - 0s - loss: 150.1408 - loglik: -8.8012e+01 - logprior: -6.2129e+01
Epoch 4/10
10/10 - 0s - loss: 108.6910 - loglik: -7.4124e+01 - logprior: -3.4567e+01
Epoch 5/10
10/10 - 0s - loss: 89.2905 - loglik: -6.9259e+01 - logprior: -2.0031e+01
Epoch 6/10
10/10 - 0s - loss: 79.2790 - loglik: -6.8743e+01 - logprior: -1.0536e+01
Epoch 7/10
10/10 - 0s - loss: 73.3685 - loglik: -6.9212e+01 - logprior: -4.1566e+00
Epoch 8/10
10/10 - 0s - loss: 69.7435 - loglik: -6.9530e+01 - logprior: -2.1319e-01
Epoch 9/10
10/10 - 0s - loss: 67.3306 - loglik: -6.9610e+01 - logprior: 2.2794
Epoch 10/10
10/10 - 0s - loss: 65.6118 - loglik: -6.9695e+01 - logprior: 4.0834
Fitted a model with MAP estimate = -64.8391
expansions: [(0, 3), (6, 1), (17, 3), (26, 3)]
discards: []
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 714.1811 - loglik: -6.2213e+01 - logprior: -6.5197e+02
Epoch 2/2
10/10 - 0s - loss: 257.8776 - loglik: -5.4557e+01 - logprior: -2.0332e+02
Fitted a model with MAP estimate = -171.2527
expansions: []
discards: [23]
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 505.4462 - loglik: -5.0047e+01 - logprior: -4.5540e+02
Epoch 2/2
10/10 - 0s - loss: 170.7562 - loglik: -4.9576e+01 - logprior: -1.2118e+02
Fitted a model with MAP estimate = -120.7631
expansions: []
discards: []
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 476.8197 - loglik: -4.9045e+01 - logprior: -4.2777e+02
Epoch 2/10
10/10 - 0s - loss: 163.2216 - loglik: -4.9472e+01 - logprior: -1.1375e+02
Epoch 3/10
10/10 - 0s - loss: 97.1671 - loglik: -4.9999e+01 - logprior: -4.7168e+01
Epoch 4/10
10/10 - 0s - loss: 69.0512 - loglik: -5.0443e+01 - logprior: -1.8608e+01
Epoch 5/10
10/10 - 0s - loss: 53.5429 - loglik: -5.0844e+01 - logprior: -2.6993e+00
Epoch 6/10
10/10 - 0s - loss: 44.4654 - loglik: -5.1204e+01 - logprior: 6.7386
Epoch 7/10
10/10 - 0s - loss: 38.7984 - loglik: -5.1497e+01 - logprior: 12.6983
Epoch 8/10
10/10 - 0s - loss: 34.9218 - loglik: -5.1713e+01 - logprior: 16.7911
Epoch 9/10
10/10 - 0s - loss: 32.0235 - loglik: -5.1877e+01 - logprior: 19.8540
Epoch 10/10
10/10 - 0s - loss: 29.6894 - loglik: -5.2011e+01 - logprior: 22.3212
Fitted a model with MAP estimate = -28.5569
Time for alignment: 24.0517
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 616.9520 - loglik: -1.2820e+02 - logprior: -4.8875e+02
Epoch 2/10
10/10 - 0s - loss: 241.7299 - loglik: -1.0679e+02 - logprior: -1.3494e+02
Epoch 3/10
10/10 - 0s - loss: 150.1408 - loglik: -8.8012e+01 - logprior: -6.2129e+01
Epoch 4/10
10/10 - 0s - loss: 108.6910 - loglik: -7.4124e+01 - logprior: -3.4567e+01
Epoch 5/10
10/10 - 0s - loss: 89.2905 - loglik: -6.9259e+01 - logprior: -2.0031e+01
Epoch 6/10
10/10 - 0s - loss: 79.2789 - loglik: -6.8743e+01 - logprior: -1.0536e+01
Epoch 7/10
10/10 - 0s - loss: 73.3684 - loglik: -6.9212e+01 - logprior: -4.1566e+00
Epoch 8/10
10/10 - 0s - loss: 69.7435 - loglik: -6.9530e+01 - logprior: -2.1319e-01
Epoch 9/10
10/10 - 0s - loss: 67.3306 - loglik: -6.9610e+01 - logprior: 2.2794
Epoch 10/10
10/10 - 0s - loss: 65.6118 - loglik: -6.9695e+01 - logprior: 4.0834
Fitted a model with MAP estimate = -64.8390
expansions: [(0, 3), (6, 1), (17, 3), (26, 3)]
discards: []
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 714.1811 - loglik: -6.2213e+01 - logprior: -6.5197e+02
Epoch 2/2
10/10 - 0s - loss: 257.8776 - loglik: -5.4557e+01 - logprior: -2.0332e+02
Fitted a model with MAP estimate = -171.2527
expansions: []
discards: [23]
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 505.4462 - loglik: -5.0047e+01 - logprior: -4.5540e+02
Epoch 2/2
10/10 - 0s - loss: 170.7562 - loglik: -4.9576e+01 - logprior: -1.2118e+02
Fitted a model with MAP estimate = -120.7631
expansions: []
discards: []
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 476.8196 - loglik: -4.9045e+01 - logprior: -4.2777e+02
Epoch 2/10
10/10 - 0s - loss: 163.2216 - loglik: -4.9472e+01 - logprior: -1.1375e+02
Epoch 3/10
10/10 - 0s - loss: 97.1671 - loglik: -4.9999e+01 - logprior: -4.7168e+01
Epoch 4/10
10/10 - 0s - loss: 69.0512 - loglik: -5.0443e+01 - logprior: -1.8608e+01
Epoch 5/10
10/10 - 0s - loss: 53.5430 - loglik: -5.0844e+01 - logprior: -2.6993e+00
Epoch 6/10
10/10 - 0s - loss: 44.4654 - loglik: -5.1204e+01 - logprior: 6.7386
Epoch 7/10
10/10 - 0s - loss: 38.7985 - loglik: -5.1497e+01 - logprior: 12.6982
Epoch 8/10
10/10 - 0s - loss: 34.9220 - loglik: -5.1713e+01 - logprior: 16.7910
Epoch 9/10
10/10 - 0s - loss: 32.0236 - loglik: -5.1878e+01 - logprior: 19.8539
Epoch 10/10
10/10 - 0s - loss: 29.6894 - loglik: -5.2010e+01 - logprior: 22.3211
Fitted a model with MAP estimate = -28.5572
Time for alignment: 22.7585
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 616.9520 - loglik: -1.2820e+02 - logprior: -4.8875e+02
Epoch 2/10
10/10 - 0s - loss: 241.7299 - loglik: -1.0679e+02 - logprior: -1.3494e+02
Epoch 3/10
10/10 - 0s - loss: 150.1408 - loglik: -8.8012e+01 - logprior: -6.2129e+01
Epoch 4/10
10/10 - 0s - loss: 108.6910 - loglik: -7.4124e+01 - logprior: -3.4567e+01
Epoch 5/10
10/10 - 0s - loss: 89.2905 - loglik: -6.9259e+01 - logprior: -2.0031e+01
Epoch 6/10
10/10 - 0s - loss: 79.2790 - loglik: -6.8743e+01 - logprior: -1.0536e+01
Epoch 7/10
10/10 - 0s - loss: 73.3685 - loglik: -6.9212e+01 - logprior: -4.1566e+00
Epoch 8/10
10/10 - 0s - loss: 69.7435 - loglik: -6.9530e+01 - logprior: -2.1319e-01
Epoch 9/10
10/10 - 0s - loss: 67.3306 - loglik: -6.9610e+01 - logprior: 2.2794
Epoch 10/10
10/10 - 0s - loss: 65.6117 - loglik: -6.9695e+01 - logprior: 4.0834
Fitted a model with MAP estimate = -64.8391
expansions: [(0, 3), (6, 1), (17, 3), (26, 3)]
discards: []
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 714.1811 - loglik: -6.2213e+01 - logprior: -6.5197e+02
Epoch 2/2
10/10 - 0s - loss: 257.8776 - loglik: -5.4557e+01 - logprior: -2.0332e+02
Fitted a model with MAP estimate = -171.2527
expansions: []
discards: [23]
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 505.4462 - loglik: -5.0047e+01 - logprior: -4.5540e+02
Epoch 2/2
10/10 - 0s - loss: 170.7563 - loglik: -4.9576e+01 - logprior: -1.2118e+02
Fitted a model with MAP estimate = -120.7631
expansions: []
discards: []
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 476.8197 - loglik: -4.9045e+01 - logprior: -4.2777e+02
Epoch 2/10
10/10 - 0s - loss: 163.2217 - loglik: -4.9472e+01 - logprior: -1.1375e+02
Epoch 3/10
10/10 - 0s - loss: 97.1673 - loglik: -4.9999e+01 - logprior: -4.7168e+01
Epoch 4/10
10/10 - 0s - loss: 69.0515 - loglik: -5.0443e+01 - logprior: -1.8608e+01
Epoch 5/10
10/10 - 0s - loss: 53.5432 - loglik: -5.0844e+01 - logprior: -2.6994e+00
Epoch 6/10
10/10 - 0s - loss: 44.4656 - loglik: -5.1204e+01 - logprior: 6.7384
Epoch 7/10
10/10 - 0s - loss: 38.7988 - loglik: -5.1497e+01 - logprior: 12.6980
Epoch 8/10
10/10 - 0s - loss: 34.9222 - loglik: -5.1713e+01 - logprior: 16.7908
Epoch 9/10
10/10 - 0s - loss: 32.0238 - loglik: -5.1878e+01 - logprior: 19.8537
Epoch 10/10
10/10 - 0s - loss: 29.6898 - loglik: -5.2011e+01 - logprior: 22.3209
Fitted a model with MAP estimate = -28.5575
Time for alignment: 24.9605
Computed alignments with likelihoods: ['-28.5569', '-28.5572', '-28.5575']
Best model has likelihood: -28.5569
SP score = 0.8171
Training of 3 independent models on file int.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32a9fba9d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34eaec0d00>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 477.0633 - loglik: -4.7288e+02 - logprior: -4.1858e+00
Epoch 2/10
16/16 - 5s - loss: 438.7865 - loglik: -4.3778e+02 - logprior: -1.0101e+00
Epoch 3/10
16/16 - 5s - loss: 414.1405 - loglik: -4.1285e+02 - logprior: -1.2945e+00
Epoch 4/10
16/16 - 5s - loss: 405.3487 - loglik: -4.0405e+02 - logprior: -1.3015e+00
Epoch 5/10
16/16 - 5s - loss: 403.2303 - loglik: -4.0202e+02 - logprior: -1.2125e+00
Epoch 6/10
16/16 - 5s - loss: 398.9509 - loglik: -3.9768e+02 - logprior: -1.2723e+00
Epoch 7/10
16/16 - 6s - loss: 398.4420 - loglik: -3.9717e+02 - logprior: -1.2713e+00
Epoch 8/10
16/16 - 6s - loss: 395.9592 - loglik: -3.9466e+02 - logprior: -1.3036e+00
Epoch 9/10
16/16 - 6s - loss: 396.4134 - loglik: -3.9507e+02 - logprior: -1.3423e+00
Fitted a model with MAP estimate = -395.2978
expansions: [(13, 1), (14, 1), (23, 3), (28, 2), (30, 1), (41, 1), (42, 1), (43, 1), (48, 2), (50, 1), (53, 1), (56, 2), (69, 1), (72, 1), (74, 2), (75, 2), (93, 1), (94, 4), (95, 1), (99, 1), (114, 1), (116, 1), (117, 2), (122, 2), (125, 1), (129, 1), (139, 2)]
discards: [0]
Fitting a model of length 178 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 11s - loss: 427.0357 - loglik: -4.2175e+02 - logprior: -5.2811e+00
Epoch 2/2
16/16 - 9s - loss: 402.3048 - loglik: -3.9978e+02 - logprior: -2.5217e+00
Fitted a model with MAP estimate = -397.2317
expansions: [(0, 1), (178, 3)]
discards: [  0  26  33  34  71  96 119 148 176 177]
Fitting a model of length 172 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 13s - loss: 404.7152 - loglik: -4.0088e+02 - logprior: -3.8320e+00
Epoch 2/2
16/16 - 9s - loss: 396.9146 - loglik: -3.9572e+02 - logprior: -1.1992e+00
Fitted a model with MAP estimate = -394.7628
expansions: [(32, 1)]
discards: [169 170 171]
Fitting a model of length 170 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 11s - loss: 401.7202 - loglik: -3.9779e+02 - logprior: -3.9284e+00
Epoch 2/10
16/16 - 9s - loss: 395.3140 - loglik: -3.9426e+02 - logprior: -1.0581e+00
Epoch 3/10
16/16 - 9s - loss: 393.5716 - loglik: -3.9295e+02 - logprior: -6.2171e-01
Epoch 4/10
16/16 - 10s - loss: 390.2910 - loglik: -3.8982e+02 - logprior: -4.6961e-01
Epoch 5/10
16/16 - 9s - loss: 389.6459 - loglik: -3.8924e+02 - logprior: -4.0171e-01
Epoch 6/10
16/16 - 10s - loss: 386.8728 - loglik: -3.8649e+02 - logprior: -3.7953e-01
Epoch 7/10
16/16 - 10s - loss: 384.8282 - loglik: -3.8446e+02 - logprior: -3.6702e-01
Epoch 8/10
16/16 - 10s - loss: 384.2591 - loglik: -3.8390e+02 - logprior: -3.5603e-01
Epoch 9/10
16/16 - 10s - loss: 381.9995 - loglik: -3.8163e+02 - logprior: -3.6509e-01
Epoch 10/10
16/16 - 10s - loss: 380.8239 - loglik: -3.8046e+02 - logprior: -3.6110e-01
Fitted a model with MAP estimate = -380.5874
Time for alignment: 227.1784
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 9s - loss: 477.0291 - loglik: -4.7285e+02 - logprior: -4.1839e+00
Epoch 2/10
16/16 - 7s - loss: 439.7943 - loglik: -4.3879e+02 - logprior: -1.0077e+00
Epoch 3/10
16/16 - 7s - loss: 415.3160 - loglik: -4.1403e+02 - logprior: -1.2892e+00
Epoch 4/10
16/16 - 7s - loss: 405.9089 - loglik: -4.0463e+02 - logprior: -1.2778e+00
Epoch 5/10
16/16 - 7s - loss: 400.5459 - loglik: -3.9932e+02 - logprior: -1.2291e+00
Epoch 6/10
16/16 - 6s - loss: 399.3410 - loglik: -3.9804e+02 - logprior: -1.3016e+00
Epoch 7/10
16/16 - 6s - loss: 396.8991 - loglik: -3.9561e+02 - logprior: -1.2873e+00
Epoch 8/10
16/16 - 7s - loss: 396.7151 - loglik: -3.9539e+02 - logprior: -1.3248e+00
Epoch 9/10
16/16 - 7s - loss: 394.4969 - loglik: -3.9312e+02 - logprior: -1.3742e+00
Epoch 10/10
16/16 - 7s - loss: 394.0251 - loglik: -3.9262e+02 - logprior: -1.4063e+00
Fitted a model with MAP estimate = -393.7315
expansions: [(13, 1), (14, 1), (23, 1), (27, 1), (28, 2), (30, 1), (41, 1), (45, 1), (47, 2), (48, 2), (50, 1), (51, 1), (52, 2), (72, 1), (73, 1), (74, 2), (75, 2), (94, 4), (95, 1), (99, 1), (113, 1), (116, 1), (120, 1), (122, 1), (125, 1), (129, 1), (139, 2)]
discards: [0]
Fitting a model of length 175 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 12s - loss: 426.8994 - loglik: -4.2164e+02 - logprior: -5.2643e+00
Epoch 2/2
16/16 - 8s - loss: 403.4802 - loglik: -4.0100e+02 - logprior: -2.4831e+00
Fitted a model with MAP estimate = -398.6122
expansions: [(0, 1), (154, 1), (175, 3)]
discards: [  0  32  33  58  66  96 173 174]
Fitting a model of length 172 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 11s - loss: 404.9741 - loglik: -4.0115e+02 - logprior: -3.8211e+00
Epoch 2/2
16/16 - 8s - loss: 397.4134 - loglik: -3.9624e+02 - logprior: -1.1731e+00
Fitted a model with MAP estimate = -395.0336
expansions: []
discards: [169 170 171]
Fitting a model of length 169 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 11s - loss: 402.2021 - loglik: -3.9828e+02 - logprior: -3.9201e+00
Epoch 2/10
16/16 - 8s - loss: 395.9096 - loglik: -3.9487e+02 - logprior: -1.0400e+00
Epoch 3/10
16/16 - 8s - loss: 393.4026 - loglik: -3.9279e+02 - logprior: -6.1433e-01
Epoch 4/10
16/16 - 8s - loss: 391.7066 - loglik: -3.9123e+02 - logprior: -4.7233e-01
Epoch 5/10
16/16 - 9s - loss: 388.5325 - loglik: -3.8814e+02 - logprior: -3.9436e-01
Epoch 6/10
16/16 - 9s - loss: 387.7461 - loglik: -3.8738e+02 - logprior: -3.6763e-01
Epoch 7/10
16/16 - 9s - loss: 385.3232 - loglik: -3.8498e+02 - logprior: -3.4498e-01
Epoch 8/10
16/16 - 9s - loss: 383.9821 - loglik: -3.8365e+02 - logprior: -3.3164e-01
Epoch 9/10
16/16 - 9s - loss: 382.4504 - loglik: -3.8212e+02 - logprior: -3.3395e-01
Epoch 10/10
16/16 - 9s - loss: 381.1144 - loglik: -3.8080e+02 - logprior: -3.1254e-01
Fitted a model with MAP estimate = -380.9072
Time for alignment: 233.4534
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 10s - loss: 476.3448 - loglik: -4.7216e+02 - logprior: -4.1854e+00
Epoch 2/10
16/16 - 6s - loss: 438.8807 - loglik: -4.3786e+02 - logprior: -1.0234e+00
Epoch 3/10
16/16 - 6s - loss: 412.4315 - loglik: -4.1108e+02 - logprior: -1.3486e+00
Epoch 4/10
16/16 - 7s - loss: 404.1614 - loglik: -4.0284e+02 - logprior: -1.3185e+00
Epoch 5/10
16/16 - 7s - loss: 399.9687 - loglik: -3.9869e+02 - logprior: -1.2758e+00
Epoch 6/10
16/16 - 7s - loss: 398.3203 - loglik: -3.9699e+02 - logprior: -1.3281e+00
Epoch 7/10
16/16 - 7s - loss: 396.1913 - loglik: -3.9487e+02 - logprior: -1.3179e+00
Epoch 8/10
16/16 - 7s - loss: 395.6017 - loglik: -3.9425e+02 - logprior: -1.3553e+00
Epoch 9/10
16/16 - 7s - loss: 394.8903 - loglik: -3.9351e+02 - logprior: -1.3785e+00
Epoch 10/10
16/16 - 6s - loss: 393.2964 - loglik: -3.9190e+02 - logprior: -1.4003e+00
Fitted a model with MAP estimate = -393.4472
expansions: [(13, 1), (14, 1), (17, 1), (23, 1), (28, 2), (30, 1), (43, 1), (45, 1), (48, 3), (50, 1), (57, 2), (58, 1), (69, 1), (72, 1), (74, 2), (75, 2), (93, 1), (94, 5), (95, 1), (99, 1), (102, 1), (116, 1), (117, 2), (119, 1), (122, 1), (125, 1), (129, 1), (139, 2)]
discards: [0]
Fitting a model of length 178 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 11s - loss: 427.2574 - loglik: -4.2198e+02 - logprior: -5.2730e+00
Epoch 2/2
16/16 - 8s - loss: 402.5749 - loglik: -4.0005e+02 - logprior: -2.5297e+00
Fitted a model with MAP estimate = -397.3765
expansions: [(0, 1)]
discards: [  0  32  33  70 148 176 177]
Fitting a model of length 172 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 10s - loss: 403.7487 - loglik: -3.9984e+02 - logprior: -3.9038e+00
Epoch 2/2
16/16 - 7s - loss: 397.3168 - loglik: -3.9616e+02 - logprior: -1.1585e+00
Fitted a model with MAP estimate = -394.7899
expansions: [(163, 1), (172, 2)]
discards: [ 92 115 116]
Fitting a model of length 172 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 10s - loss: 402.6170 - loglik: -3.9867e+02 - logprior: -3.9513e+00
Epoch 2/10
16/16 - 7s - loss: 396.4328 - loglik: -3.9531e+02 - logprior: -1.1211e+00
Epoch 3/10
16/16 - 7s - loss: 393.6195 - loglik: -3.9291e+02 - logprior: -7.1400e-01
Epoch 4/10
16/16 - 7s - loss: 391.2297 - loglik: -3.9065e+02 - logprior: -5.8126e-01
Epoch 5/10
16/16 - 7s - loss: 388.5701 - loglik: -3.8805e+02 - logprior: -5.1537e-01
Epoch 6/10
16/16 - 7s - loss: 386.7668 - loglik: -3.8628e+02 - logprior: -4.8392e-01
Epoch 7/10
16/16 - 7s - loss: 386.0916 - loglik: -3.8562e+02 - logprior: -4.7208e-01
Epoch 8/10
16/16 - 7s - loss: 384.0780 - loglik: -3.8363e+02 - logprior: -4.5172e-01
Epoch 9/10
16/16 - 7s - loss: 382.3399 - loglik: -3.8190e+02 - logprior: -4.4135e-01
Epoch 10/10
16/16 - 7s - loss: 380.5728 - loglik: -3.8014e+02 - logprior: -4.3195e-01
Fitted a model with MAP estimate = -380.7481
Time for alignment: 215.5084
Computed alignments with likelihoods: ['-380.5874', '-380.9072', '-380.7481']
Best model has likelihood: -380.5874
SP score = 0.7756
Training of 3 independent models on file phc.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32aa66ef40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32a8381640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 292.5083 - loglik: -2.7758e+02 - logprior: -1.4931e+01
Epoch 2/10
10/10 - 1s - loss: 251.0887 - loglik: -2.4719e+02 - logprior: -3.8958e+00
Epoch 3/10
10/10 - 1s - loss: 225.1358 - loglik: -2.2293e+02 - logprior: -2.2049e+00
Epoch 4/10
10/10 - 1s - loss: 212.8551 - loglik: -2.1094e+02 - logprior: -1.9155e+00
Epoch 5/10
10/10 - 1s - loss: 205.8360 - loglik: -2.0398e+02 - logprior: -1.8606e+00
Epoch 6/10
10/10 - 1s - loss: 201.8813 - loglik: -2.0000e+02 - logprior: -1.8769e+00
Epoch 7/10
10/10 - 1s - loss: 200.3928 - loglik: -1.9848e+02 - logprior: -1.9110e+00
Epoch 8/10
10/10 - 1s - loss: 200.5473 - loglik: -1.9861e+02 - logprior: -1.9399e+00
Fitted a model with MAP estimate = -199.3975
expansions: [(25, 1), (34, 1), (37, 3), (39, 1), (45, 2), (48, 1), (54, 1), (56, 1)]
discards: [0]
Fitting a model of length 78 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 223.4805 - loglik: -2.0616e+02 - logprior: -1.7318e+01
Epoch 2/2
10/10 - 1s - loss: 205.6415 - loglik: -1.9812e+02 - logprior: -7.5202e+00
Fitted a model with MAP estimate = -201.5944
expansions: [(0, 12)]
discards: [ 0 24 39 40]
Fitting a model of length 86 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 213.9431 - loglik: -1.9941e+02 - logprior: -1.4534e+01
Epoch 2/2
10/10 - 1s - loss: 197.9829 - loglik: -1.9344e+02 - logprior: -4.5414e+00
Fitted a model with MAP estimate = -192.7244
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 59]
Fitting a model of length 74 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 215.2640 - loglik: -1.9897e+02 - logprior: -1.6294e+01
Epoch 2/10
10/10 - 1s - loss: 201.7623 - loglik: -1.9679e+02 - logprior: -4.9699e+00
Epoch 3/10
10/10 - 1s - loss: 197.3877 - loglik: -1.9499e+02 - logprior: -2.4025e+00
Epoch 4/10
10/10 - 1s - loss: 195.6362 - loglik: -1.9403e+02 - logprior: -1.6110e+00
Epoch 5/10
10/10 - 1s - loss: 195.8716 - loglik: -1.9454e+02 - logprior: -1.3343e+00
Fitted a model with MAP estimate = -194.2329
Time for alignment: 39.6125
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 291.5845 - loglik: -2.7665e+02 - logprior: -1.4933e+01
Epoch 2/10
10/10 - 1s - loss: 252.6154 - loglik: -2.4872e+02 - logprior: -3.8961e+00
Epoch 3/10
10/10 - 1s - loss: 225.6350 - loglik: -2.2341e+02 - logprior: -2.2233e+00
Epoch 4/10
10/10 - 1s - loss: 210.3017 - loglik: -2.0835e+02 - logprior: -1.9561e+00
Epoch 5/10
10/10 - 1s - loss: 206.1064 - loglik: -2.0424e+02 - logprior: -1.8679e+00
Epoch 6/10
10/10 - 1s - loss: 203.5522 - loglik: -2.0166e+02 - logprior: -1.8883e+00
Epoch 7/10
10/10 - 1s - loss: 202.3242 - loglik: -2.0044e+02 - logprior: -1.8812e+00
Epoch 8/10
10/10 - 1s - loss: 201.3959 - loglik: -1.9948e+02 - logprior: -1.9168e+00
Epoch 9/10
10/10 - 1s - loss: 200.6740 - loglik: -1.9874e+02 - logprior: -1.9380e+00
Epoch 10/10
10/10 - 1s - loss: 200.3404 - loglik: -1.9837e+02 - logprior: -1.9664e+00
Fitted a model with MAP estimate = -200.3409
expansions: [(25, 1), (34, 1), (36, 1), (37, 1), (39, 1), (46, 2), (47, 1), (48, 1), (54, 1), (56, 1)]
discards: [0]
Fitting a model of length 78 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 225.6570 - loglik: -2.0832e+02 - logprior: -1.7342e+01
Epoch 2/2
10/10 - 1s - loss: 205.7787 - loglik: -1.9822e+02 - logprior: -7.5627e+00
Fitted a model with MAP estimate = -202.2031
expansions: [(0, 9), (50, 1)]
discards: [ 0 37 42]
Fitting a model of length 85 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 213.3147 - loglik: -1.9894e+02 - logprior: -1.4377e+01
Epoch 2/2
10/10 - 1s - loss: 194.4610 - loglik: -1.9006e+02 - logprior: -4.4043e+00
Fitted a model with MAP estimate = -191.8193
expansions: [(49, 1)]
discards: [ 0  1  2  3  4  5  6  7 57 58]
Fitting a model of length 76 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 215.0189 - loglik: -1.9918e+02 - logprior: -1.5841e+01
Epoch 2/10
10/10 - 1s - loss: 200.7894 - loglik: -1.9605e+02 - logprior: -4.7357e+00
Epoch 3/10
10/10 - 1s - loss: 197.7659 - loglik: -1.9533e+02 - logprior: -2.4354e+00
Epoch 4/10
10/10 - 1s - loss: 194.4242 - loglik: -1.9265e+02 - logprior: -1.7699e+00
Epoch 5/10
10/10 - 1s - loss: 194.8855 - loglik: -1.9345e+02 - logprior: -1.4373e+00
Fitted a model with MAP estimate = -193.7622
Time for alignment: 41.7048
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 290.8628 - loglik: -2.7594e+02 - logprior: -1.4926e+01
Epoch 2/10
10/10 - 1s - loss: 253.0494 - loglik: -2.4917e+02 - logprior: -3.8816e+00
Epoch 3/10
10/10 - 1s - loss: 223.8341 - loglik: -2.2163e+02 - logprior: -2.2054e+00
Epoch 4/10
10/10 - 1s - loss: 210.7245 - loglik: -2.0887e+02 - logprior: -1.8551e+00
Epoch 5/10
10/10 - 1s - loss: 205.7552 - loglik: -2.0401e+02 - logprior: -1.7451e+00
Epoch 6/10
10/10 - 1s - loss: 204.3710 - loglik: -2.0259e+02 - logprior: -1.7799e+00
Epoch 7/10
10/10 - 1s - loss: 199.7720 - loglik: -1.9804e+02 - logprior: -1.7322e+00
Epoch 8/10
10/10 - 1s - loss: 201.3807 - loglik: -1.9968e+02 - logprior: -1.7044e+00
Fitted a model with MAP estimate = -200.3198
expansions: [(10, 1), (29, 1), (35, 3), (36, 2), (44, 1), (45, 1), (48, 1), (56, 1)]
discards: [0]
Fitting a model of length 78 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 224.4458 - loglik: -2.0722e+02 - logprior: -1.7225e+01
Epoch 2/2
10/10 - 1s - loss: 204.9843 - loglik: -1.9764e+02 - logprior: -7.3426e+00
Fitted a model with MAP estimate = -200.2876
expansions: [(0, 7)]
discards: [ 0 37]
Fitting a model of length 83 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 208.4741 - loglik: -1.9437e+02 - logprior: -1.4108e+01
Epoch 2/2
10/10 - 1s - loss: 193.0992 - loglik: -1.8898e+02 - logprior: -4.1151e+00
Fitted a model with MAP estimate = -189.6687
expansions: []
discards: [1 2 3 4 5]
Fitting a model of length 78 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 205.7863 - loglik: -1.9210e+02 - logprior: -1.3686e+01
Epoch 2/10
10/10 - 1s - loss: 193.9156 - loglik: -1.8993e+02 - logprior: -3.9820e+00
Epoch 3/10
10/10 - 1s - loss: 190.5466 - loglik: -1.8843e+02 - logprior: -2.1180e+00
Epoch 4/10
10/10 - 1s - loss: 189.7155 - loglik: -1.8820e+02 - logprior: -1.5128e+00
Epoch 5/10
10/10 - 1s - loss: 188.5962 - loglik: -1.8728e+02 - logprior: -1.3160e+00
Epoch 6/10
10/10 - 1s - loss: 187.5137 - loglik: -1.8632e+02 - logprior: -1.1948e+00
Epoch 7/10
10/10 - 1s - loss: 187.6071 - loglik: -1.8656e+02 - logprior: -1.0492e+00
Fitted a model with MAP estimate = -186.8035
Time for alignment: 43.1670
Computed alignments with likelihoods: ['-192.7244', '-191.8193', '-186.8035']
Best model has likelihood: -186.8035
SP score = 0.6791
Training of 3 independent models on file ricin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2ceee55ee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2d006d9fd0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 167 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 613.5576 - loglik: -5.6075e+02 - logprior: -5.2808e+01
Epoch 2/10
10/10 - 4s - loss: 515.6715 - loglik: -5.0691e+02 - logprior: -8.7655e+00
Epoch 3/10
10/10 - 4s - loss: 453.4157 - loglik: -4.5173e+02 - logprior: -1.6855e+00
Epoch 4/10
10/10 - 4s - loss: 419.9464 - loglik: -4.2024e+02 - logprior: 0.2891
Epoch 5/10
10/10 - 4s - loss: 405.9228 - loglik: -4.0713e+02 - logprior: 1.2073
Epoch 6/10
10/10 - 4s - loss: 401.4803 - loglik: -4.0329e+02 - logprior: 1.8138
Epoch 7/10
10/10 - 4s - loss: 399.1723 - loglik: -4.0129e+02 - logprior: 2.1211
Epoch 8/10
10/10 - 4s - loss: 398.4288 - loglik: -4.0074e+02 - logprior: 2.3095
Epoch 9/10
10/10 - 4s - loss: 396.6301 - loglik: -3.9910e+02 - logprior: 2.4744
Epoch 10/10
10/10 - 4s - loss: 397.4387 - loglik: -4.0007e+02 - logprior: 2.6360
Fitted a model with MAP estimate = -396.5278
expansions: [(9, 2), (19, 2), (22, 1), (23, 3), (29, 3), (44, 1), (45, 2), (46, 1), (52, 2), (81, 1), (82, 1), (103, 1), (104, 1), (106, 1), (115, 1), (117, 3), (118, 1), (145, 3), (151, 4), (154, 1)]
discards: [0]
Fitting a model of length 201 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 477.9678 - loglik: -4.1743e+02 - logprior: -6.0541e+01
Epoch 2/2
10/10 - 5s - loss: 412.0922 - loglik: -3.9107e+02 - logprior: -2.1018e+01
Fitted a model with MAP estimate = -399.4556
expansions: [(0, 11)]
discards: [  0  21  28  29 171]
Fitting a model of length 207 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 440.3056 - loglik: -3.9349e+02 - logprior: -4.6818e+01
Epoch 2/2
10/10 - 6s - loss: 391.2385 - loglik: -3.8352e+02 - logprior: -7.7137e+00
Fitted a model with MAP estimate = -381.2928
expansions: [(19, 1)]
discards: [  1   2   3   4   5   6   7   8   9  10  43  63  73 187]
Fitting a model of length 194 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 435.8064 - loglik: -3.9017e+02 - logprior: -4.5633e+01
Epoch 2/10
10/10 - 5s - loss: 390.3145 - loglik: -3.8359e+02 - logprior: -6.7270e+00
Epoch 3/10
10/10 - 5s - loss: 377.6875 - loglik: -3.7908e+02 - logprior: 1.3961
Epoch 4/10
10/10 - 5s - loss: 371.0247 - loglik: -3.7589e+02 - logprior: 4.8651
Epoch 5/10
10/10 - 5s - loss: 367.4024 - loglik: -3.7421e+02 - logprior: 6.8106
Epoch 6/10
10/10 - 5s - loss: 365.3768 - loglik: -3.7335e+02 - logprior: 7.9740
Epoch 7/10
10/10 - 5s - loss: 364.0560 - loglik: -3.7282e+02 - logprior: 8.7636
Epoch 8/10
10/10 - 6s - loss: 363.4610 - loglik: -3.7284e+02 - logprior: 9.3821
Epoch 9/10
10/10 - 6s - loss: 362.8057 - loglik: -3.7271e+02 - logprior: 9.9031
Epoch 10/10
10/10 - 6s - loss: 362.0930 - loglik: -3.7246e+02 - logprior: 10.3682
Fitted a model with MAP estimate = -361.7860
Time for alignment: 137.4635
Fitting a model of length 167 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 613.5664 - loglik: -5.6076e+02 - logprior: -5.2806e+01
Epoch 2/10
10/10 - 5s - loss: 514.7059 - loglik: -5.0592e+02 - logprior: -8.7854e+00
Epoch 3/10
10/10 - 5s - loss: 451.7877 - loglik: -4.5017e+02 - logprior: -1.6146e+00
Epoch 4/10
10/10 - 5s - loss: 417.7505 - loglik: -4.1827e+02 - logprior: 0.5159
Epoch 5/10
10/10 - 5s - loss: 403.3169 - loglik: -4.0470e+02 - logprior: 1.3864
Epoch 6/10
10/10 - 5s - loss: 399.6143 - loglik: -4.0162e+02 - logprior: 2.0093
Epoch 7/10
10/10 - 5s - loss: 396.7122 - loglik: -3.9912e+02 - logprior: 2.4050
Epoch 8/10
10/10 - 5s - loss: 396.6438 - loglik: -3.9931e+02 - logprior: 2.6669
Epoch 9/10
10/10 - 5s - loss: 395.3914 - loglik: -3.9830e+02 - logprior: 2.9088
Epoch 10/10
10/10 - 5s - loss: 396.2552 - loglik: -3.9936e+02 - logprior: 3.1046
Fitted a model with MAP estimate = -395.1172
expansions: [(9, 2), (19, 2), (22, 3), (23, 1), (29, 2), (44, 1), (45, 2), (46, 1), (52, 2), (74, 1), (75, 2), (81, 2), (100, 1), (104, 1), (107, 2), (114, 1), (116, 3), (119, 1), (150, 5)]
discards: [0]
Fitting a model of length 201 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 475.0911 - loglik: -4.1456e+02 - logprior: -6.0533e+01
Epoch 2/2
10/10 - 6s - loss: 412.5692 - loglik: -3.9129e+02 - logprior: -2.1282e+01
Fitted a model with MAP estimate = -400.3667
expansions: [(0, 12)]
discards: [  0  21  26  27  66  99 129 142 182 183]
Fitting a model of length 203 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 444.1954 - loglik: -3.9714e+02 - logprior: -4.7059e+01
Epoch 2/2
10/10 - 7s - loss: 395.3256 - loglik: -3.8743e+02 - logprior: -7.8968e+00
Fitted a model with MAP estimate = -386.5758
expansions: [(20, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 63 98 99]
Fitting a model of length 190 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 437.8566 - loglik: -3.9206e+02 - logprior: -4.5794e+01
Epoch 2/10
10/10 - 6s - loss: 394.2852 - loglik: -3.8743e+02 - logprior: -6.8575e+00
Epoch 3/10
10/10 - 6s - loss: 381.5611 - loglik: -3.8289e+02 - logprior: 1.3296
Epoch 4/10
10/10 - 6s - loss: 375.6826 - loglik: -3.8054e+02 - logprior: 4.8534
Epoch 5/10
10/10 - 6s - loss: 371.1519 - loglik: -3.7794e+02 - logprior: 6.7929
Epoch 6/10
10/10 - 6s - loss: 368.9347 - loglik: -3.7684e+02 - logprior: 7.9037
Epoch 7/10
10/10 - 6s - loss: 368.4844 - loglik: -3.7719e+02 - logprior: 8.7022
Epoch 8/10
10/10 - 6s - loss: 366.8820 - loglik: -3.7622e+02 - logprior: 9.3390
Epoch 9/10
10/10 - 6s - loss: 367.4995 - loglik: -3.7738e+02 - logprior: 9.8802
Fitted a model with MAP estimate = -366.4307
Time for alignment: 150.0961
Fitting a model of length 167 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 613.4370 - loglik: -5.6063e+02 - logprior: -5.2809e+01
Epoch 2/10
10/10 - 5s - loss: 514.7552 - loglik: -5.0597e+02 - logprior: -8.7868e+00
Epoch 3/10
10/10 - 5s - loss: 450.9637 - loglik: -4.4936e+02 - logprior: -1.5999e+00
Epoch 4/10
10/10 - 5s - loss: 415.8593 - loglik: -4.1636e+02 - logprior: 0.4969
Epoch 5/10
10/10 - 5s - loss: 403.3277 - loglik: -4.0467e+02 - logprior: 1.3454
Epoch 6/10
10/10 - 5s - loss: 398.8519 - loglik: -4.0068e+02 - logprior: 1.8293
Epoch 7/10
10/10 - 5s - loss: 396.5553 - loglik: -3.9879e+02 - logprior: 2.2305
Epoch 8/10
10/10 - 5s - loss: 395.2207 - loglik: -3.9775e+02 - logprior: 2.5265
Epoch 9/10
10/10 - 5s - loss: 394.8851 - loglik: -3.9762e+02 - logprior: 2.7323
Epoch 10/10
10/10 - 5s - loss: 393.6511 - loglik: -3.9652e+02 - logprior: 2.8724
Fitted a model with MAP estimate = -393.9712
expansions: [(0, 3), (19, 2), (22, 1), (23, 3), (29, 3), (44, 3), (45, 1), (46, 1), (52, 1), (74, 1), (81, 1), (82, 1), (94, 1), (98, 1), (100, 1), (104, 1), (107, 3), (113, 1), (116, 1), (145, 3), (150, 4)]
discards: []
Fitting a model of length 204 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 479.4132 - loglik: -4.1083e+02 - logprior: -6.8582e+01
Epoch 2/2
10/10 - 7s - loss: 403.6065 - loglik: -3.8759e+02 - logprior: -1.6017e+01
Fitted a model with MAP estimate = -387.5841
expansions: []
discards: [  0   1   2  23  30  31  38 120 132 133 134 176 177 185 186]
Fitting a model of length 189 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 445.1417 - loglik: -3.9612e+02 - logprior: -4.9020e+01
Epoch 2/2
10/10 - 6s - loss: 396.1015 - loglik: -3.8823e+02 - logprior: -7.8720e+00
Fitted a model with MAP estimate = -387.9687
expansions: [(0, 9), (165, 2)]
discards: [53]
Fitting a model of length 199 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 458.4647 - loglik: -3.9375e+02 - logprior: -6.4720e+01
Epoch 2/10
10/10 - 6s - loss: 400.4238 - loglik: -3.8728e+02 - logprior: -1.3140e+01
Epoch 3/10
10/10 - 7s - loss: 384.3707 - loglik: -3.8371e+02 - logprior: -6.6466e-01
Epoch 4/10
10/10 - 7s - loss: 374.9639 - loglik: -3.7893e+02 - logprior: 3.9639
Epoch 5/10
10/10 - 7s - loss: 370.8045 - loglik: -3.7693e+02 - logprior: 6.1209
Epoch 6/10
10/10 - 8s - loss: 368.5457 - loglik: -3.7609e+02 - logprior: 7.5436
Epoch 7/10
10/10 - 8s - loss: 366.2541 - loglik: -3.7470e+02 - logprior: 8.4477
Epoch 8/10
10/10 - 8s - loss: 365.4562 - loglik: -3.7463e+02 - logprior: 9.1773
Epoch 9/10
10/10 - 8s - loss: 364.9300 - loglik: -3.7471e+02 - logprior: 9.7806
Epoch 10/10
10/10 - 8s - loss: 364.1890 - loglik: -3.7448e+02 - logprior: 10.2912
Fitted a model with MAP estimate = -364.0783
Time for alignment: 171.0916
Computed alignments with likelihoods: ['-361.7860', '-366.4307', '-364.0783']
Best model has likelihood: -361.7860
SP score = 0.7697
Training of 3 independent models on file PDZ.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f369911c7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34a6b0c220>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 238.0804 - loglik: -2.3496e+02 - logprior: -3.1210e+00
Epoch 2/10
19/19 - 2s - loss: 206.9412 - loglik: -2.0577e+02 - logprior: -1.1757e+00
Epoch 3/10
19/19 - 2s - loss: 196.7012 - loglik: -1.9546e+02 - logprior: -1.2458e+00
Epoch 4/10
19/19 - 2s - loss: 194.4696 - loglik: -1.9330e+02 - logprior: -1.1681e+00
Epoch 5/10
19/19 - 2s - loss: 193.3983 - loglik: -1.9224e+02 - logprior: -1.1572e+00
Epoch 6/10
19/19 - 2s - loss: 192.6116 - loglik: -1.9148e+02 - logprior: -1.1366e+00
Epoch 7/10
19/19 - 2s - loss: 192.4205 - loglik: -1.9130e+02 - logprior: -1.1239e+00
Epoch 8/10
19/19 - 2s - loss: 192.1679 - loglik: -1.9105e+02 - logprior: -1.1172e+00
Epoch 9/10
19/19 - 2s - loss: 192.1217 - loglik: -1.9100e+02 - logprior: -1.1170e+00
Epoch 10/10
19/19 - 2s - loss: 191.9383 - loglik: -1.9082e+02 - logprior: -1.1212e+00
Fitted a model with MAP estimate = -185.6979
expansions: [(3, 1), (4, 1), (5, 1), (6, 1), (16, 5), (17, 1), (18, 2), (23, 1), (46, 3), (47, 1), (48, 1), (49, 1), (52, 1), (55, 2), (58, 1)]
discards: [0]
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 198.3277 - loglik: -1.9454e+02 - logprior: -3.7832e+00
Epoch 2/2
19/19 - 2s - loss: 188.9201 - loglik: -1.8693e+02 - logprior: -1.9897e+00
Fitted a model with MAP estimate = -179.8759
expansions: [(0, 4), (27, 1), (67, 1)]
discards: [ 0 21 22 23 24 25 59]
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 190.9877 - loglik: -1.8814e+02 - logprior: -2.8467e+00
Epoch 2/2
19/19 - 2s - loss: 186.4741 - loglik: -1.8541e+02 - logprior: -1.0632e+00
Fitted a model with MAP estimate = -178.8077
expansions: [(28, 1)]
discards: [1 2 3]
Fitting a model of length 83 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 181.0051 - loglik: -1.7925e+02 - logprior: -1.7558e+00
Epoch 2/10
23/23 - 2s - loss: 178.4415 - loglik: -1.7760e+02 - logprior: -8.4217e-01
Epoch 3/10
23/23 - 2s - loss: 177.0841 - loglik: -1.7627e+02 - logprior: -8.1140e-01
Epoch 4/10
23/23 - 2s - loss: 176.7210 - loglik: -1.7596e+02 - logprior: -7.6335e-01
Epoch 5/10
23/23 - 2s - loss: 175.8870 - loglik: -1.7513e+02 - logprior: -7.5454e-01
Epoch 6/10
23/23 - 2s - loss: 175.4789 - loglik: -1.7475e+02 - logprior: -7.2421e-01
Epoch 7/10
23/23 - 3s - loss: 175.2550 - loglik: -1.7453e+02 - logprior: -7.2454e-01
Epoch 8/10
23/23 - 2s - loss: 174.8699 - loglik: -1.7416e+02 - logprior: -7.0612e-01
Epoch 9/10
23/23 - 2s - loss: 174.5148 - loglik: -1.7383e+02 - logprior: -6.8723e-01
Epoch 10/10
23/23 - 3s - loss: 174.5014 - loglik: -1.7383e+02 - logprior: -6.7228e-01
Fitted a model with MAP estimate = -174.1253
Time for alignment: 76.6927
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 238.1202 - loglik: -2.3500e+02 - logprior: -3.1194e+00
Epoch 2/10
19/19 - 2s - loss: 206.3957 - loglik: -2.0521e+02 - logprior: -1.1881e+00
Epoch 3/10
19/19 - 2s - loss: 195.4535 - loglik: -1.9417e+02 - logprior: -1.2868e+00
Epoch 4/10
19/19 - 2s - loss: 193.1508 - loglik: -1.9194e+02 - logprior: -1.2134e+00
Epoch 5/10
19/19 - 2s - loss: 192.3792 - loglik: -1.9118e+02 - logprior: -1.1968e+00
Epoch 6/10
19/19 - 2s - loss: 191.8876 - loglik: -1.9072e+02 - logprior: -1.1712e+00
Epoch 7/10
19/19 - 2s - loss: 191.5856 - loglik: -1.9042e+02 - logprior: -1.1639e+00
Epoch 8/10
19/19 - 2s - loss: 191.2953 - loglik: -1.9013e+02 - logprior: -1.1610e+00
Epoch 9/10
19/19 - 2s - loss: 191.4220 - loglik: -1.9027e+02 - logprior: -1.1556e+00
Fitted a model with MAP estimate = -184.7762
expansions: [(3, 1), (4, 1), (5, 1), (6, 1), (15, 1), (18, 3), (23, 1), (31, 1), (46, 2), (47, 1), (49, 2), (52, 1), (55, 2), (58, 1)]
discards: [0]
Fitting a model of length 82 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 198.4541 - loglik: -1.9466e+02 - logprior: -3.7983e+00
Epoch 2/2
19/19 - 2s - loss: 189.9219 - loglik: -1.8799e+02 - logprior: -1.9312e+00
Fitted a model with MAP estimate = -180.6316
expansions: [(0, 4), (60, 1)]
discards: [ 0 56]
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 190.6663 - loglik: -1.8782e+02 - logprior: -2.8480e+00
Epoch 2/2
19/19 - 2s - loss: 186.5298 - loglik: -1.8546e+02 - logprior: -1.0741e+00
Fitted a model with MAP estimate = -178.6910
expansions: [(24, 4)]
discards: [1 2 3]
Fitting a model of length 86 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 181.0174 - loglik: -1.7925e+02 - logprior: -1.7672e+00
Epoch 2/10
23/23 - 2s - loss: 177.6115 - loglik: -1.7675e+02 - logprior: -8.6253e-01
Epoch 3/10
23/23 - 2s - loss: 176.7368 - loglik: -1.7589e+02 - logprior: -8.4565e-01
Epoch 4/10
23/23 - 2s - loss: 175.4106 - loglik: -1.7461e+02 - logprior: -7.9977e-01
Epoch 5/10
23/23 - 2s - loss: 174.6908 - loglik: -1.7390e+02 - logprior: -7.9040e-01
Epoch 6/10
23/23 - 2s - loss: 173.7907 - loglik: -1.7300e+02 - logprior: -7.8994e-01
Epoch 7/10
23/23 - 2s - loss: 173.7982 - loglik: -1.7303e+02 - logprior: -7.6385e-01
Fitted a model with MAP estimate = -173.4708
Time for alignment: 68.4219
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 238.1230 - loglik: -2.3500e+02 - logprior: -3.1201e+00
Epoch 2/10
19/19 - 2s - loss: 206.0936 - loglik: -2.0492e+02 - logprior: -1.1706e+00
Epoch 3/10
19/19 - 2s - loss: 195.6637 - loglik: -1.9440e+02 - logprior: -1.2678e+00
Epoch 4/10
19/19 - 2s - loss: 192.9619 - loglik: -1.9174e+02 - logprior: -1.2204e+00
Epoch 5/10
19/19 - 2s - loss: 191.8752 - loglik: -1.9068e+02 - logprior: -1.1965e+00
Epoch 6/10
19/19 - 2s - loss: 191.5773 - loglik: -1.9041e+02 - logprior: -1.1720e+00
Epoch 7/10
19/19 - 2s - loss: 191.1497 - loglik: -1.9000e+02 - logprior: -1.1487e+00
Epoch 8/10
19/19 - 2s - loss: 190.9793 - loglik: -1.8984e+02 - logprior: -1.1351e+00
Epoch 9/10
19/19 - 2s - loss: 190.9850 - loglik: -1.8986e+02 - logprior: -1.1233e+00
Fitted a model with MAP estimate = -184.1288
expansions: [(0, 2), (3, 1), (4, 1), (16, 1), (17, 3), (18, 1), (23, 2), (24, 1), (46, 2), (47, 1), (48, 1), (49, 2), (53, 1), (55, 2), (58, 1)]
discards: []
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 197.3638 - loglik: -1.9320e+02 - logprior: -4.1662e+00
Epoch 2/2
19/19 - 2s - loss: 187.8552 - loglik: -1.8644e+02 - logprior: -1.4149e+00
Fitted a model with MAP estimate = -179.0581
expansions: [(9, 2)]
discards: [ 0 32 59]
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 191.1909 - loglik: -1.8738e+02 - logprior: -3.8158e+00
Epoch 2/2
19/19 - 2s - loss: 186.2733 - loglik: -1.8484e+02 - logprior: -1.4301e+00
Fitted a model with MAP estimate = -177.9801
expansions: [(24, 3), (27, 1), (28, 1), (74, 1)]
discards: [ 0  1  9 10 25]
Fitting a model of length 86 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 182.0087 - loglik: -1.7976e+02 - logprior: -2.2490e+00
Epoch 2/10
23/23 - 2s - loss: 178.0999 - loglik: -1.7717e+02 - logprior: -9.2676e-01
Epoch 3/10
23/23 - 2s - loss: 176.4902 - loglik: -1.7562e+02 - logprior: -8.6923e-01
Epoch 4/10
23/23 - 3s - loss: 175.7133 - loglik: -1.7488e+02 - logprior: -8.3491e-01
Epoch 5/10
23/23 - 2s - loss: 175.2168 - loglik: -1.7440e+02 - logprior: -8.2165e-01
Epoch 6/10
23/23 - 2s - loss: 175.1222 - loglik: -1.7431e+02 - logprior: -8.1046e-01
Epoch 7/10
23/23 - 3s - loss: 174.2866 - loglik: -1.7349e+02 - logprior: -7.9922e-01
Epoch 8/10
23/23 - 2s - loss: 173.9573 - loglik: -1.7317e+02 - logprior: -7.8530e-01
Epoch 9/10
23/23 - 2s - loss: 174.0861 - loglik: -1.7331e+02 - logprior: -7.7956e-01
Fitted a model with MAP estimate = -173.6420
Time for alignment: 71.4853
Computed alignments with likelihoods: ['-174.1253', '-173.4708', '-173.6420']
Best model has likelihood: -173.4708
SP score = 0.8687
Training of 3 independent models on file asp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34eafb9a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3676c0dc70>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 20s - loss: 794.8829 - loglik: -7.8916e+02 - logprior: -5.7242e+00
Epoch 2/10
22/22 - 18s - loss: 682.9554 - loglik: -6.8249e+02 - logprior: -4.6800e-01
Epoch 3/10
22/22 - 18s - loss: 647.9861 - loglik: -6.4661e+02 - logprior: -1.3740e+00
Epoch 4/10
22/22 - 19s - loss: 634.7413 - loglik: -6.3340e+02 - logprior: -1.3367e+00
Epoch 5/10
22/22 - 20s - loss: 633.5190 - loglik: -6.3211e+02 - logprior: -1.4086e+00
Epoch 6/10
22/22 - 20s - loss: 629.1033 - loglik: -6.2770e+02 - logprior: -1.3992e+00
Epoch 7/10
22/22 - 21s - loss: 631.3326 - loglik: -6.2992e+02 - logprior: -1.4168e+00
Fitted a model with MAP estimate = -630.0534
expansions: [(12, 1), (13, 1), (32, 2), (34, 3), (36, 1), (45, 1), (47, 1), (48, 1), (49, 1), (66, 1), (70, 1), (71, 2), (72, 1), (76, 1), (77, 1), (79, 1), (82, 1), (94, 1), (99, 1), (104, 1), (105, 2), (107, 1), (108, 1), (118, 1), (120, 2), (136, 1), (143, 1), (144, 2), (148, 1), (152, 1), (155, 3), (157, 1), (178, 3), (179, 1), (180, 1), (183, 2), (184, 1), (185, 2), (193, 2), (194, 1), (205, 1), (206, 1), (209, 1), (213, 3), (214, 1), (223, 2), (224, 1), (236, 1), (238, 1)]
discards: [0]
Fitting a model of length 317 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 36s - loss: 647.7922 - loglik: -6.3967e+02 - logprior: -8.1242e+00
Epoch 2/2
22/22 - 32s - loss: 616.5682 - loglik: -6.1432e+02 - logprior: -2.2525e+00
Fitted a model with MAP estimate = -614.3253
expansions: [(0, 3)]
discards: [  0  38 176 220 229 230 271 285 286]
Fitting a model of length 311 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 34s - loss: 624.6928 - loglik: -6.2000e+02 - logprior: -4.6902e+00
Epoch 2/2
22/22 - 32s - loss: 614.1641 - loglik: -6.1543e+02 - logprior: 1.2690
Fitted a model with MAP estimate = -610.9165
expansions: []
discards: [0 1 2]
Fitting a model of length 308 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 31s - loss: 628.6898 - loglik: -6.2154e+02 - logprior: -7.1507e+00
Epoch 2/10
22/22 - 27s - loss: 619.3384 - loglik: -6.1808e+02 - logprior: -1.2577e+00
Epoch 3/10
22/22 - 28s - loss: 612.3484 - loglik: -6.1262e+02 - logprior: 0.2684
Epoch 4/10
22/22 - 27s - loss: 607.7163 - loglik: -6.1048e+02 - logprior: 2.7682
Epoch 5/10
22/22 - 28s - loss: 605.1544 - loglik: -6.0823e+02 - logprior: 3.0768
Epoch 6/10
22/22 - 29s - loss: 603.1837 - loglik: -6.0640e+02 - logprior: 3.2159
Epoch 7/10
22/22 - 30s - loss: 603.5004 - loglik: -6.0687e+02 - logprior: 3.3662
Fitted a model with MAP estimate = -601.8452
Time for alignment: 545.7066
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 25s - loss: 796.2351 - loglik: -7.9050e+02 - logprior: -5.7327e+00
Epoch 2/10
22/22 - 22s - loss: 681.4366 - loglik: -6.8084e+02 - logprior: -5.9338e-01
Epoch 3/10
22/22 - 20s - loss: 644.8920 - loglik: -6.4327e+02 - logprior: -1.6185e+00
Epoch 4/10
22/22 - 18s - loss: 635.4484 - loglik: -6.3394e+02 - logprior: -1.5081e+00
Epoch 5/10
22/22 - 17s - loss: 632.9882 - loglik: -6.3147e+02 - logprior: -1.5211e+00
Epoch 6/10
22/22 - 16s - loss: 629.6718 - loglik: -6.2815e+02 - logprior: -1.5222e+00
Epoch 7/10
22/22 - 16s - loss: 630.8945 - loglik: -6.2935e+02 - logprior: -1.5490e+00
Fitted a model with MAP estimate = -630.0693
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 2), (35, 1), (45, 2), (48, 1), (49, 1), (50, 1), (61, 1), (69, 1), (70, 1), (72, 1), (76, 2), (79, 1), (81, 1), (83, 1), (97, 1), (99, 1), (104, 1), (105, 1), (107, 1), (108, 1), (118, 1), (120, 2), (131, 1), (142, 2), (143, 3), (150, 1), (151, 1), (155, 1), (156, 3), (157, 1), (178, 3), (179, 1), (180, 1), (183, 2), (184, 1), (185, 1), (193, 2), (194, 1), (205, 1), (206, 1), (209, 1), (214, 2), (215, 1), (224, 1), (225, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Fitting a model of length 319 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 25s - loss: 647.0707 - loglik: -6.3904e+02 - logprior: -8.0340e+00
Epoch 2/2
22/22 - 22s - loss: 618.3691 - loglik: -6.1631e+02 - logprior: -2.0631e+00
Fitted a model with MAP estimate = -615.0244
expansions: [(0, 3)]
discards: [  0  34 176 177 196 197 198 224 233 274]
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 25s - loss: 624.7529 - loglik: -6.2006e+02 - logprior: -4.6959e+00
Epoch 2/2
22/22 - 22s - loss: 616.0095 - loglik: -6.1734e+02 - logprior: 1.3337
Fitted a model with MAP estimate = -611.7210
expansions: []
discards: [  0   1   2 100]
Fitting a model of length 308 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 26s - loss: 628.9456 - loglik: -6.2173e+02 - logprior: -7.2163e+00
Epoch 2/10
22/22 - 24s - loss: 621.5267 - loglik: -6.2029e+02 - logprior: -1.2344e+00
Epoch 3/10
22/22 - 24s - loss: 614.3292 - loglik: -6.1477e+02 - logprior: 0.4427
Epoch 4/10
22/22 - 23s - loss: 607.5420 - loglik: -6.1046e+02 - logprior: 2.9229
Epoch 5/10
22/22 - 24s - loss: 606.4144 - loglik: -6.0956e+02 - logprior: 3.1431
Epoch 6/10
22/22 - 24s - loss: 604.9866 - loglik: -6.0827e+02 - logprior: 3.2826
Epoch 7/10
22/22 - 24s - loss: 603.5604 - loglik: -6.0707e+02 - logprior: 3.5145
Epoch 8/10
22/22 - 23s - loss: 602.2480 - loglik: -6.0593e+02 - logprior: 3.6837
Epoch 9/10
22/22 - 24s - loss: 603.6726 - loglik: -6.0754e+02 - logprior: 3.8661
Fitted a model with MAP estimate = -601.3723
Time for alignment: 501.2479
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 20s - loss: 795.0408 - loglik: -7.8931e+02 - logprior: -5.7301e+00
Epoch 2/10
22/22 - 18s - loss: 688.8713 - loglik: -6.8831e+02 - logprior: -5.6591e-01
Epoch 3/10
22/22 - 19s - loss: 644.1933 - loglik: -6.4260e+02 - logprior: -1.5972e+00
Epoch 4/10
22/22 - 20s - loss: 636.5807 - loglik: -6.3504e+02 - logprior: -1.5431e+00
Epoch 5/10
22/22 - 21s - loss: 637.5771 - loglik: -6.3606e+02 - logprior: -1.5177e+00
Fitted a model with MAP estimate = -634.0077
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 3), (35, 2), (45, 1), (47, 2), (48, 1), (65, 1), (69, 1), (70, 2), (71, 1), (75, 1), (76, 1), (80, 1), (82, 1), (98, 1), (103, 1), (104, 1), (106, 1), (108, 2), (120, 2), (143, 2), (144, 3), (148, 1), (149, 1), (155, 1), (157, 3), (158, 1), (176, 1), (178, 3), (179, 1), (180, 1), (184, 1), (185, 2), (194, 2), (207, 1), (208, 1), (209, 1), (210, 1), (214, 3), (215, 1), (224, 1), (225, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Fitting a model of length 319 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 35s - loss: 644.2480 - loglik: -6.3611e+02 - logprior: -8.1355e+00
Epoch 2/2
22/22 - 33s - loss: 617.5568 - loglik: -6.1537e+02 - logprior: -2.1846e+00
Fitted a model with MAP estimate = -614.0944
expansions: [(0, 3)]
discards: [  0  34  40 148 176 177 196 197 198 223]
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 35s - loss: 624.5392 - loglik: -6.1975e+02 - logprior: -4.7873e+00
Epoch 2/2
22/22 - 30s - loss: 615.4472 - loglik: -6.1675e+02 - logprior: 1.2995
Fitted a model with MAP estimate = -611.2013
expansions: []
discards: [  0   1   2 267]
Fitting a model of length 308 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 30s - loss: 628.7020 - loglik: -6.2151e+02 - logprior: -7.1875e+00
Epoch 2/10
22/22 - 30s - loss: 618.2906 - loglik: -6.1705e+02 - logprior: -1.2445e+00
Epoch 3/10
22/22 - 30s - loss: 614.9136 - loglik: -6.1549e+02 - logprior: 0.5791
Epoch 4/10
22/22 - 27s - loss: 607.6655 - loglik: -6.1054e+02 - logprior: 2.8708
Epoch 5/10
22/22 - 26s - loss: 606.1581 - loglik: -6.0917e+02 - logprior: 3.0123
Epoch 6/10
22/22 - 26s - loss: 602.5923 - loglik: -6.0574e+02 - logprior: 3.1447
Epoch 7/10
22/22 - 28s - loss: 605.9692 - loglik: -6.0938e+02 - logprior: 3.4062
Fitted a model with MAP estimate = -602.4362
Time for alignment: 502.0335
Computed alignments with likelihoods: ['-601.8452', '-601.3723', '-602.4362']
Best model has likelihood: -601.3723
SP score = 0.9168
Training of 3 independent models on file bowman.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f36996a25b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33206793d0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 172.1933 - loglik: -7.9804e+01 - logprior: -9.2390e+01
Epoch 2/10
10/10 - 1s - loss: 95.2405 - loglik: -6.8668e+01 - logprior: -2.6573e+01
Epoch 3/10
10/10 - 1s - loss: 73.1690 - loglik: -5.9890e+01 - logprior: -1.3279e+01
Epoch 4/10
10/10 - 1s - loss: 63.7426 - loglik: -5.5598e+01 - logprior: -8.1444e+00
Epoch 5/10
10/10 - 1s - loss: 58.9159 - loglik: -5.3381e+01 - logprior: -5.5345e+00
Epoch 6/10
10/10 - 1s - loss: 56.6146 - loglik: -5.2546e+01 - logprior: -4.0681e+00
Epoch 7/10
10/10 - 1s - loss: 55.3976 - loglik: -5.2171e+01 - logprior: -3.2267e+00
Epoch 8/10
10/10 - 1s - loss: 54.6921 - loglik: -5.1995e+01 - logprior: -2.6972e+00
Epoch 9/10
10/10 - 1s - loss: 54.2836 - loglik: -5.1916e+01 - logprior: -2.3677e+00
Epoch 10/10
10/10 - 1s - loss: 54.0020 - loglik: -5.1870e+01 - logprior: -2.1317e+00
Fitted a model with MAP estimate = -53.8799
expansions: [(0, 3), (13, 1)]
discards: []
Fitting a model of length 24 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 174.1998 - loglik: -4.9861e+01 - logprior: -1.2434e+02
Epoch 2/2
10/10 - 1s - loss: 87.4676 - loglik: -4.7233e+01 - logprior: -4.0235e+01
Fitted a model with MAP estimate = -70.4614
expansions: [(0, 1)]
discards: []
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 148.1105 - loglik: -4.5100e+01 - logprior: -1.0301e+02
Epoch 2/2
10/10 - 1s - loss: 80.8163 - loglik: -4.4317e+01 - logprior: -3.6500e+01
Fitted a model with MAP estimate = -66.3532
expansions: []
discards: []
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 128.9666 - loglik: -4.3178e+01 - logprior: -8.5789e+01
Epoch 2/10
10/10 - 0s - loss: 68.3375 - loglik: -4.3663e+01 - logprior: -2.4674e+01
Epoch 3/10
10/10 - 1s - loss: 55.9398 - loglik: -4.3890e+01 - logprior: -1.2050e+01
Epoch 4/10
10/10 - 1s - loss: 50.9910 - loglik: -4.4056e+01 - logprior: -6.9348e+00
Epoch 5/10
10/10 - 1s - loss: 48.1773 - loglik: -4.4006e+01 - logprior: -4.1714e+00
Epoch 6/10
10/10 - 1s - loss: 46.3128 - loglik: -4.3700e+01 - logprior: -2.6132e+00
Epoch 7/10
10/10 - 1s - loss: 45.2002 - loglik: -4.3529e+01 - logprior: -1.6716e+00
Epoch 8/10
10/10 - 1s - loss: 44.4889 - loglik: -4.3357e+01 - logprior: -1.1317e+00
Epoch 9/10
10/10 - 1s - loss: 44.0004 - loglik: -4.3196e+01 - logprior: -8.0426e-01
Epoch 10/10
10/10 - 1s - loss: 43.5964 - loglik: -4.3072e+01 - logprior: -5.2414e-01
Fitted a model with MAP estimate = -43.4211
Time for alignment: 29.0510
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 172.1933 - loglik: -7.9804e+01 - logprior: -9.2390e+01
Epoch 2/10
10/10 - 1s - loss: 95.2405 - loglik: -6.8668e+01 - logprior: -2.6573e+01
Epoch 3/10
10/10 - 1s - loss: 73.1690 - loglik: -5.9890e+01 - logprior: -1.3279e+01
Epoch 4/10
10/10 - 1s - loss: 63.7426 - loglik: -5.5598e+01 - logprior: -8.1444e+00
Epoch 5/10
10/10 - 1s - loss: 58.9159 - loglik: -5.3381e+01 - logprior: -5.5345e+00
Epoch 6/10
10/10 - 1s - loss: 56.6146 - loglik: -5.2546e+01 - logprior: -4.0681e+00
Epoch 7/10
10/10 - 1s - loss: 55.3976 - loglik: -5.2171e+01 - logprior: -3.2267e+00
Epoch 8/10
10/10 - 1s - loss: 54.6921 - loglik: -5.1995e+01 - logprior: -2.6972e+00
Epoch 9/10
10/10 - 1s - loss: 54.2836 - loglik: -5.1916e+01 - logprior: -2.3677e+00
Epoch 10/10
10/10 - 1s - loss: 54.0020 - loglik: -5.1870e+01 - logprior: -2.1317e+00
Fitted a model with MAP estimate = -53.8799
expansions: [(0, 3), (13, 1)]
discards: []
Fitting a model of length 24 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 174.1998 - loglik: -4.9861e+01 - logprior: -1.2434e+02
Epoch 2/2
10/10 - 1s - loss: 87.4676 - loglik: -4.7233e+01 - logprior: -4.0235e+01
Fitted a model with MAP estimate = -70.4614
expansions: [(0, 1)]
discards: []
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 148.1105 - loglik: -4.5100e+01 - logprior: -1.0301e+02
Epoch 2/2
10/10 - 1s - loss: 80.8163 - loglik: -4.4317e+01 - logprior: -3.6500e+01
Fitted a model with MAP estimate = -66.3532
expansions: []
discards: []
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 128.9667 - loglik: -4.3178e+01 - logprior: -8.5789e+01
Epoch 2/10
10/10 - 1s - loss: 68.3373 - loglik: -4.3663e+01 - logprior: -2.4674e+01
Epoch 3/10
10/10 - 1s - loss: 55.9397 - loglik: -4.3890e+01 - logprior: -1.2050e+01
Epoch 4/10
10/10 - 1s - loss: 50.9910 - loglik: -4.4056e+01 - logprior: -6.9348e+00
Epoch 5/10
10/10 - 1s - loss: 48.1769 - loglik: -4.4006e+01 - logprior: -4.1714e+00
Epoch 6/10
10/10 - 1s - loss: 46.3126 - loglik: -4.3699e+01 - logprior: -2.6131e+00
Epoch 7/10
10/10 - 1s - loss: 45.2001 - loglik: -4.3528e+01 - logprior: -1.6716e+00
Epoch 8/10
10/10 - 1s - loss: 44.4890 - loglik: -4.3357e+01 - logprior: -1.1317e+00
Epoch 9/10
10/10 - 1s - loss: 44.0006 - loglik: -4.3196e+01 - logprior: -8.0423e-01
Epoch 10/10
10/10 - 1s - loss: 43.5965 - loglik: -4.3072e+01 - logprior: -5.2412e-01
Fitted a model with MAP estimate = -43.4213
Time for alignment: 27.5702
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 172.1933 - loglik: -7.9804e+01 - logprior: -9.2390e+01
Epoch 2/10
10/10 - 1s - loss: 95.2405 - loglik: -6.8668e+01 - logprior: -2.6573e+01
Epoch 3/10
10/10 - 1s - loss: 73.1690 - loglik: -5.9890e+01 - logprior: -1.3279e+01
Epoch 4/10
10/10 - 1s - loss: 63.7426 - loglik: -5.5598e+01 - logprior: -8.1444e+00
Epoch 5/10
10/10 - 1s - loss: 58.9159 - loglik: -5.3381e+01 - logprior: -5.5345e+00
Epoch 6/10
10/10 - 1s - loss: 56.6146 - loglik: -5.2546e+01 - logprior: -4.0681e+00
Epoch 7/10
10/10 - 1s - loss: 55.3976 - loglik: -5.2171e+01 - logprior: -3.2267e+00
Epoch 8/10
10/10 - 1s - loss: 54.6922 - loglik: -5.1995e+01 - logprior: -2.6972e+00
Epoch 9/10
10/10 - 1s - loss: 54.2836 - loglik: -5.1916e+01 - logprior: -2.3677e+00
Epoch 10/10
10/10 - 1s - loss: 54.0020 - loglik: -5.1870e+01 - logprior: -2.1317e+00
Fitted a model with MAP estimate = -53.8798
expansions: [(0, 3), (13, 1)]
discards: []
Fitting a model of length 24 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 174.1998 - loglik: -4.9861e+01 - logprior: -1.2434e+02
Epoch 2/2
10/10 - 1s - loss: 87.4676 - loglik: -4.7233e+01 - logprior: -4.0235e+01
Fitted a model with MAP estimate = -70.4614
expansions: [(0, 1)]
discards: []
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 148.1105 - loglik: -4.5100e+01 - logprior: -1.0301e+02
Epoch 2/2
10/10 - 1s - loss: 80.8163 - loglik: -4.4317e+01 - logprior: -3.6500e+01
Fitted a model with MAP estimate = -66.3533
expansions: []
discards: []
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 128.9666 - loglik: -4.3178e+01 - logprior: -8.5789e+01
Epoch 2/10
10/10 - 1s - loss: 68.3374 - loglik: -4.3663e+01 - logprior: -2.4674e+01
Epoch 3/10
10/10 - 1s - loss: 55.9397 - loglik: -4.3890e+01 - logprior: -1.2050e+01
Epoch 4/10
10/10 - 1s - loss: 50.9910 - loglik: -4.4056e+01 - logprior: -6.9348e+00
Epoch 5/10
10/10 - 1s - loss: 48.1771 - loglik: -4.4006e+01 - logprior: -4.1714e+00
Epoch 6/10
10/10 - 1s - loss: 46.3128 - loglik: -4.3700e+01 - logprior: -2.6132e+00
Epoch 7/10
10/10 - 1s - loss: 45.2001 - loglik: -4.3528e+01 - logprior: -1.6716e+00
Epoch 8/10
10/10 - 1s - loss: 44.4889 - loglik: -4.3357e+01 - logprior: -1.1317e+00
Epoch 9/10
10/10 - 1s - loss: 44.0004 - loglik: -4.3196e+01 - logprior: -8.0424e-01
Epoch 10/10
10/10 - 1s - loss: 43.5964 - loglik: -4.3072e+01 - logprior: -5.2413e-01
Fitted a model with MAP estimate = -43.4213
Time for alignment: 27.4235
Computed alignments with likelihoods: ['-43.4211', '-43.4213', '-43.4213']
Best model has likelihood: -43.4211
SP score = 0.9419
Training of 3 independent models on file oxidored_q6.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34af7f4760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2ceee552b0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 362.6393 - loglik: -3.5033e+02 - logprior: -1.2307e+01
Epoch 2/10
11/11 - 2s - loss: 308.5507 - loglik: -3.0561e+02 - logprior: -2.9441e+00
Epoch 3/10
11/11 - 2s - loss: 268.6359 - loglik: -2.6667e+02 - logprior: -1.9676e+00
Epoch 4/10
11/11 - 2s - loss: 248.9522 - loglik: -2.4687e+02 - logprior: -2.0842e+00
Epoch 5/10
11/11 - 2s - loss: 242.7360 - loglik: -2.4059e+02 - logprior: -2.1476e+00
Epoch 6/10
11/11 - 2s - loss: 240.7736 - loglik: -2.3865e+02 - logprior: -2.1255e+00
Epoch 7/10
11/11 - 2s - loss: 236.9143 - loglik: -2.3483e+02 - logprior: -2.0872e+00
Epoch 8/10
11/11 - 2s - loss: 237.8829 - loglik: -2.3584e+02 - logprior: -2.0449e+00
Fitted a model with MAP estimate = -236.7347
expansions: [(8, 2), (9, 3), (10, 2), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (43, 1), (61, 1), (62, 3), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Fitting a model of length 118 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 254.7155 - loglik: -2.4054e+02 - logprior: -1.4180e+01
Epoch 2/2
11/11 - 3s - loss: 230.1358 - loglik: -2.2401e+02 - logprior: -6.1287e+00
Fitted a model with MAP estimate = -224.7368
expansions: [(0, 18)]
discards: [  0   8  14  77  78  85  88 111]
Fitting a model of length 128 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 233.3078 - loglik: -2.2152e+02 - logprior: -1.1787e+01
Epoch 2/2
11/11 - 3s - loss: 220.3088 - loglik: -2.1693e+02 - logprior: -3.3795e+00
Fitted a model with MAP estimate = -215.8039
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 229.0207 - loglik: -2.1687e+02 - logprior: -1.2150e+01
Epoch 2/10
11/11 - 3s - loss: 219.3170 - loglik: -2.1647e+02 - logprior: -2.8480e+00
Epoch 3/10
11/11 - 3s - loss: 216.2366 - loglik: -2.1501e+02 - logprior: -1.2236e+00
Epoch 4/10
11/11 - 3s - loss: 214.4743 - loglik: -2.1373e+02 - logprior: -7.4438e-01
Epoch 5/10
11/11 - 3s - loss: 212.4842 - loglik: -2.1187e+02 - logprior: -6.1898e-01
Epoch 6/10
11/11 - 3s - loss: 212.1327 - loglik: -2.1165e+02 - logprior: -4.8580e-01
Epoch 7/10
11/11 - 3s - loss: 211.8508 - loglik: -2.1144e+02 - logprior: -4.1368e-01
Epoch 8/10
11/11 - 3s - loss: 209.8554 - loglik: -2.0944e+02 - logprior: -4.1584e-01
Epoch 9/10
11/11 - 3s - loss: 211.2322 - loglik: -2.1077e+02 - logprior: -4.6184e-01
Fitted a model with MAP estimate = -209.9653
Time for alignment: 83.0863
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 362.3080 - loglik: -3.5000e+02 - logprior: -1.2306e+01
Epoch 2/10
11/11 - 2s - loss: 308.6235 - loglik: -3.0568e+02 - logprior: -2.9446e+00
Epoch 3/10
11/11 - 3s - loss: 267.5948 - loglik: -2.6564e+02 - logprior: -1.9551e+00
Epoch 4/10
11/11 - 2s - loss: 249.7028 - loglik: -2.4764e+02 - logprior: -2.0656e+00
Epoch 5/10
11/11 - 2s - loss: 244.5239 - loglik: -2.4240e+02 - logprior: -2.1213e+00
Epoch 6/10
11/11 - 2s - loss: 239.0459 - loglik: -2.3695e+02 - logprior: -2.0951e+00
Epoch 7/10
11/11 - 2s - loss: 238.4600 - loglik: -2.3638e+02 - logprior: -2.0760e+00
Epoch 8/10
11/11 - 2s - loss: 237.2675 - loglik: -2.3519e+02 - logprior: -2.0780e+00
Epoch 9/10
11/11 - 2s - loss: 237.7542 - loglik: -2.3565e+02 - logprior: -2.1012e+00
Fitted a model with MAP estimate = -236.6711
expansions: [(8, 2), (9, 3), (10, 1), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 3), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Fitting a model of length 116 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 253.6599 - loglik: -2.3949e+02 - logprior: -1.4173e+01
Epoch 2/2
11/11 - 4s - loss: 227.1549 - loglik: -2.2114e+02 - logprior: -6.0149e+00
Fitted a model with MAP estimate = -222.9619
expansions: [(0, 20)]
discards: [ 0  8 76 77 84 87]
Fitting a model of length 130 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 233.4510 - loglik: -2.2164e+02 - logprior: -1.1812e+01
Epoch 2/2
11/11 - 4s - loss: 219.6100 - loglik: -2.1623e+02 - logprior: -3.3815e+00
Fitted a model with MAP estimate = -217.6290
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 8s - loss: 228.7870 - loglik: -2.1636e+02 - logprior: -1.2426e+01
Epoch 2/10
11/11 - 3s - loss: 218.5611 - loglik: -2.1561e+02 - logprior: -2.9511e+00
Epoch 3/10
11/11 - 4s - loss: 215.2946 - loglik: -2.1397e+02 - logprior: -1.3225e+00
Epoch 4/10
11/11 - 4s - loss: 213.9171 - loglik: -2.1298e+02 - logprior: -9.3323e-01
Epoch 5/10
11/11 - 3s - loss: 213.3779 - loglik: -2.1269e+02 - logprior: -6.8562e-01
Epoch 6/10
11/11 - 3s - loss: 212.3440 - loglik: -2.1181e+02 - logprior: -5.3546e-01
Epoch 7/10
11/11 - 4s - loss: 211.8932 - loglik: -2.1140e+02 - logprior: -4.9401e-01
Epoch 8/10
11/11 - 3s - loss: 211.4821 - loglik: -2.1105e+02 - logprior: -4.2887e-01
Epoch 9/10
11/11 - 3s - loss: 210.4173 - loglik: -2.1006e+02 - logprior: -3.5227e-01
Epoch 10/10
11/11 - 3s - loss: 209.6609 - loglik: -2.0938e+02 - logprior: -2.8174e-01
Fitted a model with MAP estimate = -209.7451
Time for alignment: 96.8310
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 361.5096 - loglik: -3.4920e+02 - logprior: -1.2310e+01
Epoch 2/10
11/11 - 3s - loss: 310.3515 - loglik: -3.0740e+02 - logprior: -2.9471e+00
Epoch 3/10
11/11 - 3s - loss: 268.1761 - loglik: -2.6622e+02 - logprior: -1.9576e+00
Epoch 4/10
11/11 - 3s - loss: 249.0220 - loglik: -2.4694e+02 - logprior: -2.0802e+00
Epoch 5/10
11/11 - 3s - loss: 243.5299 - loglik: -2.4141e+02 - logprior: -2.1163e+00
Epoch 6/10
11/11 - 3s - loss: 241.4575 - loglik: -2.3934e+02 - logprior: -2.1129e+00
Epoch 7/10
11/11 - 3s - loss: 239.0851 - loglik: -2.3700e+02 - logprior: -2.0818e+00
Epoch 8/10
11/11 - 3s - loss: 239.0969 - loglik: -2.3704e+02 - logprior: -2.0589e+00
Fitted a model with MAP estimate = -237.1368
expansions: [(8, 2), (9, 3), (10, 2), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Fitting a model of length 116 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 255.1069 - loglik: -2.4092e+02 - logprior: -1.4184e+01
Epoch 2/2
11/11 - 3s - loss: 231.5000 - loglik: -2.2545e+02 - logprior: -6.0498e+00
Fitted a model with MAP estimate = -225.8870
expansions: [(0, 19)]
discards: [ 0  8 14 84 87]
Fitting a model of length 130 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 8s - loss: 234.9056 - loglik: -2.2311e+02 - logprior: -1.1794e+01
Epoch 2/2
11/11 - 4s - loss: 219.8849 - loglik: -2.1654e+02 - logprior: -3.3469e+00
Fitted a model with MAP estimate = -215.9334
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 92]
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 231.3755 - loglik: -2.1934e+02 - logprior: -1.2031e+01
Epoch 2/10
11/11 - 3s - loss: 219.2271 - loglik: -2.1640e+02 - logprior: -2.8286e+00
Epoch 3/10
11/11 - 4s - loss: 216.6060 - loglik: -2.1536e+02 - logprior: -1.2417e+00
Epoch 4/10
11/11 - 4s - loss: 215.5619 - loglik: -2.1478e+02 - logprior: -7.7800e-01
Epoch 5/10
11/11 - 4s - loss: 215.0934 - loglik: -2.1438e+02 - logprior: -7.1567e-01
Epoch 6/10
11/11 - 4s - loss: 211.9435 - loglik: -2.1134e+02 - logprior: -6.0115e-01
Epoch 7/10
11/11 - 3s - loss: 213.2714 - loglik: -2.1274e+02 - logprior: -5.2791e-01
Fitted a model with MAP estimate = -211.9896
Time for alignment: 85.9066
Computed alignments with likelihoods: ['-209.9653', '-209.7451', '-211.9896']
Best model has likelihood: -209.7451
SP score = 0.5609
Training of 3 independent models on file aldosered.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f349d27dd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32a9588eb0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 224 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 32s - loss: 809.8427 - loglik: -8.0735e+02 - logprior: -2.4901e+00
Epoch 2/10
19/19 - 31s - loss: 697.3477 - loglik: -6.9650e+02 - logprior: -8.4415e-01
Epoch 3/10
19/19 - 30s - loss: 635.6953 - loglik: -6.3433e+02 - logprior: -1.3662e+00
Epoch 4/10
19/19 - 30s - loss: 622.0798 - loglik: -6.2059e+02 - logprior: -1.4885e+00
Epoch 5/10
19/19 - 29s - loss: 617.5756 - loglik: -6.1607e+02 - logprior: -1.5100e+00
Epoch 6/10
19/19 - 30s - loss: 615.8538 - loglik: -6.1440e+02 - logprior: -1.4541e+00
Epoch 7/10
19/19 - 30s - loss: 615.6953 - loglik: -6.1426e+02 - logprior: -1.4365e+00
Epoch 8/10
19/19 - 30s - loss: 614.9095 - loglik: -6.1349e+02 - logprior: -1.4193e+00
Epoch 9/10
19/19 - 31s - loss: 614.1554 - loglik: -6.1274e+02 - logprior: -1.4200e+00
Epoch 10/10
19/19 - 30s - loss: 613.6631 - loglik: -6.1224e+02 - logprior: -1.4209e+00
Fitted a model with MAP estimate = -574.2710
expansions: [(12, 4), (14, 1), (16, 1), (17, 1), (36, 1), (37, 1), (39, 1), (46, 3), (47, 2), (59, 1), (60, 2), (62, 4), (67, 2), (68, 2), (69, 2), (99, 2), (123, 1), (127, 4), (128, 1), (129, 1), (140, 2), (142, 1), (149, 1), (163, 1), (164, 1), (165, 6), (166, 1), (167, 5), (168, 3), (169, 4), (170, 2), (172, 1), (181, 2), (182, 1), (191, 1), (193, 1), (203, 1), (210, 1), (212, 2), (213, 2), (215, 1), (217, 1), (218, 1)]
discards: [0]
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 46s - loss: 621.8544 - loglik: -6.1915e+02 - logprior: -2.7066e+00
Epoch 2/2
39/39 - 43s - loss: 593.1014 - loglik: -5.9200e+02 - logprior: -1.1038e+00
Fitted a model with MAP estimate = -548.0810
expansions: [(0, 2), (211, 2)]
discards: [  0  11  12  57  74  89  94 126 177 207 208 209 222 226 227 245 284 286]
Fitting a model of length 288 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 600.7375 - loglik: -5.9922e+02 - logprior: -1.5223e+00
Epoch 2/2
39/39 - 39s - loss: 591.2714 - loglik: -5.9079e+02 - logprior: -4.8597e-01
Fitted a model with MAP estimate = -548.6700
expansions: []
discards: [ 0 57]
Fitting a model of length 286 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 42s - loss: 554.3120 - loglik: -5.5263e+02 - logprior: -1.6842e+00
Epoch 2/10
45/45 - 45s - loss: 545.6336 - loglik: -5.4514e+02 - logprior: -4.9738e-01
Epoch 3/10
45/45 - 47s - loss: 542.2887 - loglik: -5.4190e+02 - logprior: -3.8436e-01
Epoch 4/10
45/45 - 52s - loss: 541.9063 - loglik: -5.4154e+02 - logprior: -3.6595e-01
Epoch 5/10
45/45 - 50s - loss: 538.4993 - loglik: -5.3830e+02 - logprior: -2.0262e-01
Epoch 6/10
45/45 - 50s - loss: 537.3607 - loglik: -5.3718e+02 - logprior: -1.8466e-01
Epoch 7/10
45/45 - 51s - loss: 538.1263 - loglik: -5.3808e+02 - logprior: -4.5377e-02
Fitted a model with MAP estimate = -537.3230
Time for alignment: 1011.0916
Fitting a model of length 224 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 32s - loss: 810.3377 - loglik: -8.0785e+02 - logprior: -2.4906e+00
Epoch 2/10
19/19 - 28s - loss: 701.0104 - loglik: -7.0014e+02 - logprior: -8.7191e-01
Epoch 3/10
19/19 - 29s - loss: 640.3982 - loglik: -6.3895e+02 - logprior: -1.4455e+00
Epoch 4/10
19/19 - 29s - loss: 627.3838 - loglik: -6.2576e+02 - logprior: -1.6218e+00
Epoch 5/10
19/19 - 29s - loss: 623.6968 - loglik: -6.2204e+02 - logprior: -1.6524e+00
Epoch 6/10
19/19 - 29s - loss: 621.1405 - loglik: -6.1954e+02 - logprior: -1.5973e+00
Epoch 7/10
19/19 - 29s - loss: 619.4917 - loglik: -6.1791e+02 - logprior: -1.5811e+00
Epoch 8/10
19/19 - 28s - loss: 620.0095 - loglik: -6.1843e+02 - logprior: -1.5752e+00
Fitted a model with MAP estimate = -577.7510
expansions: [(12, 4), (16, 1), (20, 1), (36, 1), (37, 1), (39, 1), (46, 3), (47, 2), (59, 1), (60, 1), (62, 4), (66, 3), (67, 2), (70, 1), (98, 2), (125, 3), (126, 2), (127, 2), (128, 1), (137, 1), (139, 2), (140, 3), (141, 7), (145, 1), (147, 1), (158, 3), (164, 2), (167, 3), (168, 4), (169, 2), (179, 1), (182, 1), (191, 1), (193, 1), (194, 1), (203, 1), (209, 1), (211, 2), (215, 1), (217, 1), (218, 1)]
discards: [0]
Fitting a model of length 300 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 43s - loss: 617.9753 - loglik: -6.1517e+02 - logprior: -2.8069e+00
Epoch 2/2
39/39 - 42s - loss: 591.8057 - loglik: -5.9070e+02 - logprior: -1.1044e+00
Fitted a model with MAP estimate = -547.3763
expansions: [(0, 2), (16, 1)]
discards: [  0  11  12  56  86 123 157 175 180 206 207 208 209 223 226 227 283]
Fitting a model of length 286 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 39s - loss: 600.8967 - loglik: -5.9936e+02 - logprior: -1.5342e+00
Epoch 2/2
39/39 - 37s - loss: 591.7618 - loglik: -5.9129e+02 - logprior: -4.7078e-01
Fitted a model with MAP estimate = -548.3785
expansions: [(201, 4)]
discards: [  0 154]
Fitting a model of length 288 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 42s - loss: 552.9516 - loglik: -5.5133e+02 - logprior: -1.6225e+00
Epoch 2/10
45/45 - 44s - loss: 545.4865 - loglik: -5.4496e+02 - logprior: -5.3091e-01
Epoch 3/10
45/45 - 48s - loss: 542.9839 - loglik: -5.4252e+02 - logprior: -4.6439e-01
Epoch 4/10
45/45 - 50s - loss: 540.6528 - loglik: -5.4032e+02 - logprior: -3.3208e-01
Epoch 5/10
45/45 - 53s - loss: 536.6752 - loglik: -5.3638e+02 - logprior: -2.9215e-01
Epoch 6/10
45/45 - 52s - loss: 537.2129 - loglik: -5.3702e+02 - logprior: -1.9201e-01
Fitted a model with MAP estimate = -536.7561
Time for alignment: 882.8897
Fitting a model of length 224 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 32s - loss: 809.5510 - loglik: -8.0706e+02 - logprior: -2.4882e+00
Epoch 2/10
19/19 - 30s - loss: 699.5436 - loglik: -6.9870e+02 - logprior: -8.4824e-01
Epoch 3/10
19/19 - 29s - loss: 634.8975 - loglik: -6.3346e+02 - logprior: -1.4332e+00
Epoch 4/10
19/19 - 28s - loss: 621.6381 - loglik: -6.2007e+02 - logprior: -1.5717e+00
Epoch 5/10
19/19 - 29s - loss: 618.4127 - loglik: -6.1685e+02 - logprior: -1.5647e+00
Epoch 6/10
19/19 - 30s - loss: 616.8359 - loglik: -6.1533e+02 - logprior: -1.5090e+00
Epoch 7/10
19/19 - 30s - loss: 615.6059 - loglik: -6.1411e+02 - logprior: -1.4921e+00
Epoch 8/10
19/19 - 31s - loss: 615.1795 - loglik: -6.1369e+02 - logprior: -1.4874e+00
Epoch 9/10
19/19 - 31s - loss: 615.7097 - loglik: -6.1423e+02 - logprior: -1.4794e+00
Fitted a model with MAP estimate = -574.8044
expansions: [(12, 4), (14, 1), (16, 1), (17, 1), (23, 2), (35, 1), (36, 1), (46, 3), (47, 2), (59, 1), (60, 2), (62, 4), (67, 2), (68, 2), (69, 2), (91, 1), (98, 2), (122, 1), (125, 4), (126, 2), (128, 1), (129, 1), (148, 1), (149, 1), (162, 1), (163, 1), (164, 6), (165, 1), (166, 4), (167, 1), (168, 2), (169, 4), (170, 2), (181, 2), (182, 1), (191, 1), (193, 1), (194, 1), (206, 1), (209, 2), (210, 1), (215, 1), (217, 1), (218, 1)]
discards: [0]
Fitting a model of length 301 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 44s - loss: 619.8107 - loglik: -6.1708e+02 - logprior: -2.7342e+00
Epoch 2/2
39/39 - 44s - loss: 592.4709 - loglik: -5.9131e+02 - logprior: -1.1615e+00
Fitted a model with MAP estimate = -549.0959
expansions: [(0, 1), (212, 2), (230, 1), (232, 1)]
discards: [  0  11  12  29  58  75  90  95 127 161 162 208 209 245 281]
Fitting a model of length 291 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 37s - loss: 600.7886 - loglik: -5.9933e+02 - logprior: -1.4614e+00
Epoch 2/2
39/39 - 36s - loss: 591.3244 - loglik: -5.9075e+02 - logprior: -5.7473e-01
Fitted a model with MAP estimate = -548.0265
expansions: [(199, 1), (200, 1)]
discards: [216]
Fitting a model of length 292 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 47s - loss: 551.3867 - loglik: -5.5021e+02 - logprior: -1.1775e+00
Epoch 2/10
45/45 - 40s - loss: 545.4711 - loglik: -5.4497e+02 - logprior: -5.0020e-01
Epoch 3/10
45/45 - 43s - loss: 541.7106 - loglik: -5.4124e+02 - logprior: -4.6995e-01
Epoch 4/10
45/45 - 45s - loss: 537.6989 - loglik: -5.3741e+02 - logprior: -2.9253e-01
Epoch 5/10
45/45 - 50s - loss: 539.0370 - loglik: -5.3877e+02 - logprior: -2.6269e-01
Fitted a model with MAP estimate = -536.6958
Time for alignment: 862.0481
Computed alignments with likelihoods: ['-537.3230', '-536.7561', '-536.6958']
Best model has likelihood: -536.6958
SP score = 0.8854
Training of 3 independent models on file sdr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34d230c1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32a998b250>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 15s - loss: 444.9726 - loglik: -4.4393e+02 - logprior: -1.0462e+00
Epoch 2/10
30/30 - 11s - loss: 372.0288 - loglik: -3.7092e+02 - logprior: -1.1135e+00
Epoch 3/10
30/30 - 11s - loss: 358.7266 - loglik: -3.5763e+02 - logprior: -1.0935e+00
Epoch 4/10
30/30 - 12s - loss: 356.6331 - loglik: -3.5556e+02 - logprior: -1.0777e+00
Epoch 5/10
30/30 - 12s - loss: 355.2867 - loglik: -3.5424e+02 - logprior: -1.0480e+00
Epoch 6/10
30/30 - 13s - loss: 354.1681 - loglik: -3.5313e+02 - logprior: -1.0345e+00
Epoch 7/10
30/30 - 12s - loss: 353.6324 - loglik: -3.5261e+02 - logprior: -1.0235e+00
Epoch 8/10
30/30 - 12s - loss: 352.9545 - loglik: -3.5193e+02 - logprior: -1.0214e+00
Epoch 9/10
30/30 - 12s - loss: 352.8620 - loglik: -3.5184e+02 - logprior: -1.0191e+00
Epoch 10/10
30/30 - 12s - loss: 352.1003 - loglik: -3.5108e+02 - logprior: -1.0180e+00
Fitted a model with MAP estimate = -346.2047
expansions: [(14, 1), (15, 1), (16, 2), (20, 2), (26, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (54, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (88, 1), (93, 1), (94, 1), (97, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (128, 2)]
discards: [0]
Fitting a model of length 179 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 20s - loss: 366.0385 - loglik: -3.6458e+02 - logprior: -1.4583e+00
Epoch 2/2
30/30 - 17s - loss: 345.0434 - loglik: -3.4412e+02 - logprior: -9.2382e-01
Fitted a model with MAP estimate = -331.3002
expansions: []
discards: [ 18  23  31  53  97 134 148 152 155]
Fitting a model of length 170 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 18s - loss: 347.5639 - loglik: -3.4635e+02 - logprior: -1.2140e+00
Epoch 2/2
30/30 - 15s - loss: 342.7342 - loglik: -3.4186e+02 - logprior: -8.7044e-01
Fitted a model with MAP estimate = -330.4632
expansions: []
discards: [47 90]
Fitting a model of length 168 on 50157 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 23s - loss: 332.9929 - loglik: -3.3218e+02 - logprior: -8.0971e-01
Epoch 2/10
43/43 - 21s - loss: 328.9529 - loglik: -3.2832e+02 - logprior: -6.3154e-01
Epoch 3/10
43/43 - 22s - loss: 325.4771 - loglik: -3.2486e+02 - logprior: -6.1504e-01
Epoch 4/10
43/43 - 22s - loss: 325.0384 - loglik: -3.2444e+02 - logprior: -5.9680e-01
Epoch 5/10
43/43 - 23s - loss: 323.3679 - loglik: -3.2278e+02 - logprior: -5.8831e-01
Epoch 6/10
43/43 - 22s - loss: 322.0092 - loglik: -3.2143e+02 - logprior: -5.7832e-01
Epoch 7/10
43/43 - 23s - loss: 321.1327 - loglik: -3.2056e+02 - logprior: -5.6889e-01
Epoch 8/10
43/43 - 23s - loss: 320.0788 - loglik: -3.1952e+02 - logprior: -5.6285e-01
Epoch 9/10
43/43 - 22s - loss: 319.6717 - loglik: -3.1912e+02 - logprior: -5.5334e-01
Epoch 10/10
43/43 - 22s - loss: 318.9783 - loglik: -3.1843e+02 - logprior: -5.4583e-01
Fitted a model with MAP estimate = -318.6818
Time for alignment: 567.8584
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 13s - loss: 444.7769 - loglik: -4.4373e+02 - logprior: -1.0519e+00
Epoch 2/10
30/30 - 10s - loss: 371.3795 - loglik: -3.7026e+02 - logprior: -1.1222e+00
Epoch 3/10
30/30 - 10s - loss: 358.7549 - loglik: -3.5766e+02 - logprior: -1.0973e+00
Epoch 4/10
30/30 - 10s - loss: 356.1344 - loglik: -3.5506e+02 - logprior: -1.0779e+00
Epoch 5/10
30/30 - 10s - loss: 355.1454 - loglik: -3.5410e+02 - logprior: -1.0468e+00
Epoch 6/10
30/30 - 10s - loss: 353.5601 - loglik: -3.5253e+02 - logprior: -1.0333e+00
Epoch 7/10
30/30 - 10s - loss: 353.2889 - loglik: -3.5227e+02 - logprior: -1.0239e+00
Epoch 8/10
30/30 - 10s - loss: 352.7171 - loglik: -3.5170e+02 - logprior: -1.0177e+00
Epoch 9/10
30/30 - 10s - loss: 352.3051 - loglik: -3.5129e+02 - logprior: -1.0174e+00
Epoch 10/10
30/30 - 10s - loss: 352.2131 - loglik: -3.5120e+02 - logprior: -1.0158e+00
Fitted a model with MAP estimate = -345.6977
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (54, 1), (56, 1), (72, 2), (73, 2), (75, 1), (78, 2), (82, 1), (88, 1), (93, 1), (94, 1), (97, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (128, 2)]
discards: [0]
Fitting a model of length 178 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 20s - loss: 365.8595 - loglik: -3.6441e+02 - logprior: -1.4475e+00
Epoch 2/2
30/30 - 16s - loss: 344.9285 - loglik: -3.4402e+02 - logprior: -9.1199e-01
Fitted a model with MAP estimate = -331.3434
expansions: []
discards: [ 25  51  95 103 133 147 151 154]
Fitting a model of length 170 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 15s - loss: 347.2191 - loglik: -3.4599e+02 - logprior: -1.2245e+00
Epoch 2/2
30/30 - 13s - loss: 343.0910 - loglik: -3.4223e+02 - logprior: -8.6596e-01
Fitted a model with MAP estimate = -330.3762
expansions: []
discards: [47 90]
Fitting a model of length 168 on 50157 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 22s - loss: 332.9907 - loglik: -3.3218e+02 - logprior: -8.0621e-01
Epoch 2/10
43/43 - 21s - loss: 328.6941 - loglik: -3.2807e+02 - logprior: -6.2845e-01
Epoch 3/10
43/43 - 21s - loss: 326.2363 - loglik: -3.2563e+02 - logprior: -6.1007e-01
Epoch 4/10
43/43 - 20s - loss: 324.4803 - loglik: -3.2388e+02 - logprior: -5.9891e-01
Epoch 5/10
43/43 - 19s - loss: 323.3466 - loglik: -3.2276e+02 - logprior: -5.8849e-01
Epoch 6/10
43/43 - 18s - loss: 321.5682 - loglik: -3.2099e+02 - logprior: -5.8025e-01
Epoch 7/10
43/43 - 18s - loss: 321.5973 - loglik: -3.2103e+02 - logprior: -5.7093e-01
Fitted a model with MAP estimate = -320.2370
Time for alignment: 446.3518
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 14s - loss: 445.2559 - loglik: -4.4420e+02 - logprior: -1.0527e+00
Epoch 2/10
30/30 - 10s - loss: 372.9281 - loglik: -3.7181e+02 - logprior: -1.1176e+00
Epoch 3/10
30/30 - 10s - loss: 358.6053 - loglik: -3.5751e+02 - logprior: -1.0984e+00
Epoch 4/10
30/30 - 10s - loss: 355.9596 - loglik: -3.5488e+02 - logprior: -1.0832e+00
Epoch 5/10
30/30 - 10s - loss: 355.1983 - loglik: -3.5414e+02 - logprior: -1.0550e+00
Epoch 6/10
30/30 - 11s - loss: 353.9011 - loglik: -3.5286e+02 - logprior: -1.0391e+00
Epoch 7/10
30/30 - 11s - loss: 353.4300 - loglik: -3.5240e+02 - logprior: -1.0276e+00
Epoch 8/10
30/30 - 11s - loss: 352.9152 - loglik: -3.5189e+02 - logprior: -1.0232e+00
Epoch 9/10
30/30 - 11s - loss: 352.1723 - loglik: -3.5115e+02 - logprior: -1.0195e+00
Epoch 10/10
30/30 - 12s - loss: 352.4223 - loglik: -3.5140e+02 - logprior: -1.0184e+00
Fitted a model with MAP estimate = -345.9456
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (26, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (55, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (88, 1), (93, 1), (94, 1), (97, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (128, 2)]
discards: [0]
Fitting a model of length 177 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 19s - loss: 365.0288 - loglik: -3.6358e+02 - logprior: -1.4468e+00
Epoch 2/2
30/30 - 17s - loss: 344.9773 - loglik: -3.4408e+02 - logprior: -9.0207e-01
Fitted a model with MAP estimate = -331.4601
expansions: []
discards: [ 30  51  95 132 146 150 153]
Fitting a model of length 170 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 20s - loss: 346.9073 - loglik: -3.4568e+02 - logprior: -1.2257e+00
Epoch 2/2
30/30 - 17s - loss: 342.8115 - loglik: -3.4194e+02 - logprior: -8.7407e-01
Fitted a model with MAP estimate = -330.5727
expansions: []
discards: [47 90]
Fitting a model of length 168 on 50157 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 25s - loss: 333.2222 - loglik: -3.3241e+02 - logprior: -8.0723e-01
Epoch 2/10
43/43 - 23s - loss: 328.7627 - loglik: -3.2813e+02 - logprior: -6.3379e-01
Epoch 3/10
43/43 - 24s - loss: 325.6186 - loglik: -3.2501e+02 - logprior: -6.0991e-01
Epoch 4/10
43/43 - 23s - loss: 324.5252 - loglik: -3.2393e+02 - logprior: -5.9763e-01
Epoch 5/10
43/43 - 22s - loss: 323.5863 - loglik: -3.2300e+02 - logprior: -5.8568e-01
Epoch 6/10
43/43 - 22s - loss: 321.7409 - loglik: -3.2116e+02 - logprior: -5.7895e-01
Epoch 7/10
43/43 - 22s - loss: 321.2235 - loglik: -3.2065e+02 - logprior: -5.7049e-01
Epoch 8/10
43/43 - 22s - loss: 319.7245 - loglik: -3.1916e+02 - logprior: -5.6116e-01
Epoch 9/10
43/43 - 23s - loss: 320.3349 - loglik: -3.1978e+02 - logprior: -5.5019e-01
Fitted a model with MAP estimate = -319.1594
Time for alignment: 546.7269
Computed alignments with likelihoods: ['-318.6818', '-320.2370', '-319.1594']
Best model has likelihood: -318.6818
SP score = 0.6717
Training of 3 independent models on file p450.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34eb3a9ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32e873ea60>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 79s - loss: 1225.1592 - loglik: -1.2234e+03 - logprior: -1.7586e+00
Epoch 2/10
40/40 - 81s - loss: 1093.8879 - loglik: -1.0912e+03 - logprior: -2.7233e+00
Epoch 3/10
40/40 - 81s - loss: 1069.8088 - loglik: -1.0672e+03 - logprior: -2.5650e+00
Epoch 4/10
40/40 - 71s - loss: 1052.1061 - loglik: -1.0495e+03 - logprior: -2.5923e+00
Epoch 5/10
40/40 - 71s - loss: 1041.0083 - loglik: -1.0383e+03 - logprior: -2.6873e+00
Epoch 6/10
40/40 - 74s - loss: 1038.0376 - loglik: -1.0353e+03 - logprior: -2.7699e+00
Epoch 7/10
40/40 - 61s - loss: 1036.2821 - loglik: -1.0335e+03 - logprior: -2.8027e+00
Epoch 8/10
40/40 - 69s - loss: 1034.2408 - loglik: -1.0314e+03 - logprior: -2.8202e+00
Epoch 9/10
40/40 - 71s - loss: 1033.7992 - loglik: -1.0309e+03 - logprior: -2.8713e+00
Epoch 10/10
40/40 - 62s - loss: 1032.5328 - loglik: -1.0296e+03 - logprior: -2.8951e+00
Fitted a model with MAP estimate = -808.0667
expansions: [(117, 1), (167, 1), (214, 1), (231, 1), (330, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 106 107 108 179
 180 181 182 183 184 185 186 187 234 235 236 237 238 239 240 241 242 243
 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261
 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279
 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297
 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315
 316 317 318 319 320 321 322 323 324 325 326 327 328 329]
Fitting a model of length 124 on 10507 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 20s - loss: 1357.9187 - loglik: -1.3531e+03 - logprior: -4.7923e+00
Epoch 2/2
20/20 - 18s - loss: 1280.5781 - loglik: -1.2785e+03 - logprior: -2.0763e+00
Fitted a model with MAP estimate = -920.6176
expansions: [(0, 145), (2, 10), (15, 17), (20, 103), (61, 7), (73, 2), (74, 6), (102, 1)]
discards: [ 0 62 63 64 65 66 67 68]
Fitting a model of length 407 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 122s - loss: 1161.5841 - loglik: -1.1598e+03 - logprior: -1.7774e+00
Epoch 2/2
40/40 - 104s - loss: 1058.4633 - loglik: -1.0576e+03 - logprior: -8.8973e-01
Fitted a model with MAP estimate = -777.3630
expansions: [(152, 2), (154, 3), (155, 6), (163, 1), (165, 10), (166, 7), (167, 6), (168, 5), (174, 1), (177, 6), (178, 3), (179, 6), (180, 1), (187, 9), (188, 4), (189, 2), (190, 2), (248, 1), (259, 2), (265, 1), (273, 1), (287, 2), (351, 1)]
discards: [  0  31  32  33  34  45  95  96  97  98  99 100 101 102 103 104 105 106
 107 120 121 122 204 205 206 207 208 209 210 211]
Fitting a model of length 459 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 133s - loss: 773.4622 - loglik: -7.7250e+02 - logprior: -9.5915e-01
Epoch 2/10
56/56 - 130s - loss: 756.7488 - loglik: -7.5618e+02 - logprior: -5.6991e-01
Epoch 3/10
56/56 - 125s - loss: 740.0948 - loglik: -7.3958e+02 - logprior: -5.1847e-01
Epoch 4/10
56/56 - 161s - loss: 731.3698 - loglik: -7.3086e+02 - logprior: -5.1463e-01
Epoch 5/10
56/56 - 165s - loss: 727.5595 - loglik: -7.2691e+02 - logprior: -6.4712e-01
Epoch 6/10
56/56 - 182s - loss: 722.7794 - loglik: -7.2234e+02 - logprior: -4.4221e-01
Epoch 7/10
56/56 - 194s - loss: 725.8445 - loglik: -7.2526e+02 - logprior: -5.8184e-01
Fitted a model with MAP estimate = -722.2971
Time for alignment: 2495.5996
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 71s - loss: 1243.7455 - loglik: -1.2427e+03 - logprior: -1.0550e+00
Epoch 2/10
40/40 - 69s - loss: 1133.9349 - loglik: -1.1341e+03 - logprior: 0.1757
Epoch 3/10
40/40 - 70s - loss: 1115.1533 - loglik: -1.1155e+03 - logprior: 0.2990
Epoch 4/10
40/40 - 70s - loss: 1104.1276 - loglik: -1.1044e+03 - logprior: 0.2669
Epoch 5/10
40/40 - 77s - loss: 1097.1746 - loglik: -1.0973e+03 - logprior: 0.1323
Epoch 6/10
40/40 - 80s - loss: 1054.7590 - loglik: -1.0519e+03 - logprior: -2.8396e+00
Epoch 7/10
40/40 - 77s - loss: 1044.7294 - loglik: -1.0418e+03 - logprior: -2.9250e+00
Epoch 8/10
40/40 - 77s - loss: 1042.1256 - loglik: -1.0391e+03 - logprior: -2.9776e+00
Epoch 9/10
40/40 - 70s - loss: 1041.2025 - loglik: -1.0382e+03 - logprior: -2.9835e+00
Epoch 10/10
40/40 - 68s - loss: 1039.6414 - loglik: -1.0366e+03 - logprior: -3.0168e+00
Fitted a model with MAP estimate = -823.2801
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  67
 155 156 157 158 159 206 207 208 209 210 211 212 213 214 215 216 217 218
 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236
 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254
 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272
 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290
 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308
 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326
 327 328 329]
Fitting a model of length 147 on 10507 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 22s - loss: 1374.9541 - loglik: -1.3708e+03 - logprior: -4.1457e+00
Epoch 2/2
20/20 - 20s - loss: 1298.4827 - loglik: -1.2970e+03 - logprior: -1.4896e+00
Fitted a model with MAP estimate = -929.6014
expansions: [(0, 58), (60, 69), (71, 12), (72, 116), (81, 1)]
discards: [ 76  77 124 125 137]
Fitting a model of length 398 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 113s - loss: 1162.1722 - loglik: -1.1605e+03 - logprior: -1.7092e+00
Epoch 2/2
40/40 - 107s - loss: 1054.0991 - loglik: -1.0535e+03 - logprior: -6.1036e-01
Fitted a model with MAP estimate = -776.0540
expansions: [(75, 1), (76, 2), (117, 4), (118, 9), (119, 12), (138, 4), (139, 8), (140, 6), (141, 4), (296, 1), (312, 1), (330, 2), (331, 1), (333, 2), (336, 1), (337, 9), (338, 2), (378, 2)]
discards: [  0  21  22  23  32  34  72 201 234 235 236 237 262]
Fitting a model of length 456 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 169s - loss: 775.5289 - loglik: -7.7449e+02 - logprior: -1.0430e+00
Epoch 2/10
56/56 - 157s - loss: 748.3887 - loglik: -7.4790e+02 - logprior: -4.9145e-01
Epoch 3/10
56/56 - 156s - loss: 748.2972 - loglik: -7.4789e+02 - logprior: -4.0420e-01
Epoch 4/10
56/56 - 195s - loss: 732.0267 - loglik: -7.3156e+02 - logprior: -4.7098e-01
Epoch 5/10
56/56 - 199s - loss: 728.6608 - loglik: -7.2825e+02 - logprior: -4.0739e-01
Epoch 6/10
56/56 - 198s - loss: 725.6562 - loglik: -7.2506e+02 - logprior: -5.9325e-01
Epoch 7/10
56/56 - 183s - loss: 723.4022 - loglik: -7.2303e+02 - logprior: -3.7241e-01
Epoch 8/10
56/56 - 162s - loss: 726.3882 - loglik: -7.2592e+02 - logprior: -4.6647e-01
Fitted a model with MAP estimate = -722.5983
Time for alignment: 2894.1124
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 71s - loss: 1245.0205 - loglik: -1.2440e+03 - logprior: -1.0473e+00
Epoch 2/10
40/40 - 61s - loss: 1134.0178 - loglik: -1.1342e+03 - logprior: 0.1834
Epoch 3/10
40/40 - 69s - loss: 1113.6560 - loglik: -1.1139e+03 - logprior: 0.2712
Epoch 4/10
40/40 - 79s - loss: 1101.3020 - loglik: -1.1016e+03 - logprior: 0.2606
Epoch 5/10
40/40 - 83s - loss: 1096.0217 - loglik: -1.0962e+03 - logprior: 0.1727
Epoch 6/10
40/40 - 85s - loss: 1068.3033 - loglik: -1.0665e+03 - logprior: -1.8117e+00
Epoch 7/10
40/40 - 82s - loss: 1050.7389 - loglik: -1.0480e+03 - logprior: -2.7345e+00
Epoch 8/10
40/40 - 78s - loss: 1047.1138 - loglik: -1.0442e+03 - logprior: -2.8943e+00
Epoch 9/10
40/40 - 78s - loss: 1045.0824 - loglik: -1.0421e+03 - logprior: -2.9534e+00
Epoch 10/10
40/40 - 80s - loss: 1044.0563 - loglik: -1.0411e+03 - logprior: -2.9873e+00
Fitted a model with MAP estimate = -825.6439
expansions: [(124, 1), (125, 1), (134, 2), (138, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  67  68
 153 154 155 156 157 160 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329]
Fitting a model of length 149 on 10507 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 20s - loss: 1367.1218 - loglik: -1.3635e+03 - logprior: -3.6233e+00
Epoch 2/2
20/20 - 18s - loss: 1295.6377 - loglik: -1.2941e+03 - logprior: -1.4970e+00
Fitted a model with MAP estimate = -926.5887
expansions: [(0, 60), (19, 2), (70, 23), (71, 4), (72, 5), (73, 13), (74, 2), (76, 69), (78, 12), (80, 1), (82, 70), (83, 9), (88, 17), (120, 1)]
discards: [84 85 86]
Fitting a model of length 434 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 112s - loss: 1151.3986 - loglik: -1.1497e+03 - logprior: -1.6808e+00
Epoch 2/2
40/40 - 110s - loss: 1037.8274 - loglik: -1.0372e+03 - logprior: -5.8912e-01
Fitted a model with MAP estimate = -764.3746
expansions: [(134, 1), (142, 1), (143, 1), (144, 1), (145, 1), (160, 1), (177, 2), (221, 2), (241, 1), (259, 1), (266, 1), (283, 1), (291, 1), (300, 2), (305, 1), (321, 1), (344, 1), (345, 1), (350, 2), (366, 3)]
discards: [  0  17  18  19  20  21  39  40  41 183 184 268 411 412]
Fitting a model of length 446 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 191s - loss: 769.6005 - loglik: -7.6868e+02 - logprior: -9.2024e-01
Epoch 2/10
56/56 - 124s - loss: 752.9108 - loglik: -7.5239e+02 - logprior: -5.2204e-01
Epoch 3/10
56/56 - 131s - loss: 744.9374 - loglik: -7.4468e+02 - logprior: -2.6024e-01
Epoch 4/10
56/56 - 151s - loss: 733.0568 - loglik: -7.3246e+02 - logprior: -5.9830e-01
Epoch 5/10
56/56 - 152s - loss: 731.1490 - loglik: -7.3082e+02 - logprior: -3.3272e-01
Epoch 6/10
56/56 - 152s - loss: 724.1982 - loglik: -7.2380e+02 - logprior: -3.9553e-01
Epoch 7/10
56/56 - 143s - loss: 724.3686 - loglik: -7.2402e+02 - logprior: -3.4749e-01
Fitted a model with MAP estimate = -723.6568
Time for alignment: 2546.9879
Computed alignments with likelihoods: ['-722.2971', '-722.5983', '-723.6568']
Best model has likelihood: -722.2971
SP score = 0.7221
Training of 3 independent models on file hla.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34a6e587c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34a7542d00>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 449.2039 - loglik: -4.4632e+02 - logprior: -2.8816e+00
Epoch 2/10
19/19 - 5s - loss: 283.7171 - loglik: -2.8232e+02 - logprior: -1.3990e+00
Epoch 3/10
19/19 - 5s - loss: 214.0748 - loglik: -2.1235e+02 - logprior: -1.7236e+00
Epoch 4/10
19/19 - 5s - loss: 202.8713 - loglik: -2.0097e+02 - logprior: -1.9010e+00
Epoch 5/10
19/19 - 5s - loss: 198.6455 - loglik: -1.9682e+02 - logprior: -1.8281e+00
Epoch 6/10
19/19 - 5s - loss: 196.4685 - loglik: -1.9458e+02 - logprior: -1.8891e+00
Epoch 7/10
19/19 - 5s - loss: 194.4722 - loglik: -1.9249e+02 - logprior: -1.9815e+00
Epoch 8/10
19/19 - 5s - loss: 193.6824 - loglik: -1.9167e+02 - logprior: -2.0170e+00
Epoch 9/10
19/19 - 5s - loss: 193.0201 - loglik: -1.9100e+02 - logprior: -2.0191e+00
Epoch 10/10
19/19 - 5s - loss: 192.9334 - loglik: -1.9091e+02 - logprior: -2.0200e+00
Fitted a model with MAP estimate = -187.7616
expansions: [(0, 2), (7, 1), (8, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (100, 1), (102, 1), (105, 1), (114, 1), (121, 1), (122, 2), (125, 1), (126, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Fitting a model of length 182 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 182.2051 - loglik: -1.7820e+02 - logprior: -4.0039e+00
Epoch 2/2
19/19 - 7s - loss: 143.7019 - loglik: -1.4265e+02 - logprior: -1.0559e+00
Fitted a model with MAP estimate = -141.9161
expansions: []
discards: [  0  51  77 153]
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 149.6038 - loglik: -1.4588e+02 - logprior: -3.7258e+00
Epoch 2/2
19/19 - 7s - loss: 141.8512 - loglik: -1.4072e+02 - logprior: -1.1349e+00
Fitted a model with MAP estimate = -141.9430
expansions: []
discards: []
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 11s - loss: 144.4936 - loglik: -1.4261e+02 - logprior: -1.8815e+00
Epoch 2/10
22/22 - 8s - loss: 139.5045 - loglik: -1.3878e+02 - logprior: -7.2171e-01
Epoch 3/10
22/22 - 9s - loss: 139.3543 - loglik: -1.3865e+02 - logprior: -7.0005e-01
Epoch 4/10
22/22 - 9s - loss: 134.5770 - loglik: -1.3380e+02 - logprior: -7.7811e-01
Epoch 5/10
22/22 - 9s - loss: 133.3490 - loglik: -1.3242e+02 - logprior: -9.2422e-01
Epoch 6/10
22/22 - 9s - loss: 131.7529 - loglik: -1.3083e+02 - logprior: -9.2610e-01
Epoch 7/10
22/22 - 9s - loss: 128.9304 - loglik: -1.2803e+02 - logprior: -9.0214e-01
Epoch 8/10
22/22 - 9s - loss: 129.4508 - loglik: -1.2859e+02 - logprior: -8.6384e-01
Fitted a model with MAP estimate = -128.5293
Time for alignment: 204.9146
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 449.2280 - loglik: -4.4634e+02 - logprior: -2.8913e+00
Epoch 2/10
19/19 - 5s - loss: 283.3974 - loglik: -2.8199e+02 - logprior: -1.4029e+00
Epoch 3/10
19/19 - 5s - loss: 213.8262 - loglik: -2.1212e+02 - logprior: -1.7039e+00
Epoch 4/10
19/19 - 5s - loss: 202.6467 - loglik: -2.0076e+02 - logprior: -1.8905e+00
Epoch 5/10
19/19 - 5s - loss: 198.2854 - loglik: -1.9648e+02 - logprior: -1.8061e+00
Epoch 6/10
19/19 - 6s - loss: 195.9748 - loglik: -1.9417e+02 - logprior: -1.8036e+00
Epoch 7/10
19/19 - 5s - loss: 195.2894 - loglik: -1.9336e+02 - logprior: -1.9264e+00
Epoch 8/10
19/19 - 5s - loss: 193.0131 - loglik: -1.9100e+02 - logprior: -2.0084e+00
Epoch 9/10
19/19 - 6s - loss: 192.1338 - loglik: -1.9013e+02 - logprior: -2.0055e+00
Epoch 10/10
19/19 - 6s - loss: 191.6545 - loglik: -1.8965e+02 - logprior: -2.0044e+00
Fitted a model with MAP estimate = -187.6964
expansions: [(0, 2), (6, 1), (7, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 1), (53, 1), (54, 1), (55, 1), (56, 2), (58, 2), (59, 3), (62, 2), (75, 1), (76, 1), (81, 1), (91, 1), (92, 1), (99, 1), (101, 1), (104, 1), (114, 1), (121, 1), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Fitting a model of length 185 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 183.6569 - loglik: -1.7964e+02 - logprior: -4.0195e+00
Epoch 2/2
19/19 - 9s - loss: 143.5002 - loglik: -1.4236e+02 - logprior: -1.1412e+00
Fitted a model with MAP estimate = -141.9348
expansions: []
discards: [  0  70  74  78  83 155 160]
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 149.6595 - loglik: -1.4592e+02 - logprior: -3.7427e+00
Epoch 2/2
19/19 - 9s - loss: 142.1123 - loglik: -1.4094e+02 - logprior: -1.1708e+00
Fitted a model with MAP estimate = -141.6380
expansions: []
discards: []
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 14s - loss: 144.9049 - loglik: -1.4276e+02 - logprior: -2.1490e+00
Epoch 2/10
22/22 - 10s - loss: 138.8439 - loglik: -1.3784e+02 - logprior: -1.0013e+00
Epoch 3/10
22/22 - 10s - loss: 137.0172 - loglik: -1.3600e+02 - logprior: -1.0202e+00
Epoch 4/10
22/22 - 10s - loss: 135.2699 - loglik: -1.3427e+02 - logprior: -9.9499e-01
Epoch 5/10
22/22 - 10s - loss: 133.6185 - loglik: -1.3265e+02 - logprior: -9.7187e-01
Epoch 6/10
22/22 - 10s - loss: 132.1942 - loglik: -1.3125e+02 - logprior: -9.4300e-01
Epoch 7/10
22/22 - 10s - loss: 129.9749 - loglik: -1.2905e+02 - logprior: -9.2418e-01
Epoch 8/10
22/22 - 10s - loss: 128.3517 - loglik: -1.2749e+02 - logprior: -8.6547e-01
Epoch 9/10
22/22 - 9s - loss: 129.6280 - loglik: -1.2878e+02 - logprior: -8.4316e-01
Fitted a model with MAP estimate = -128.2881
Time for alignment: 243.1178
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 449.6629 - loglik: -4.4677e+02 - logprior: -2.8937e+00
Epoch 2/10
19/19 - 6s - loss: 283.9070 - loglik: -2.8249e+02 - logprior: -1.4123e+00
Epoch 3/10
19/19 - 6s - loss: 213.7939 - loglik: -2.1207e+02 - logprior: -1.7253e+00
Epoch 4/10
19/19 - 6s - loss: 203.4876 - loglik: -2.0160e+02 - logprior: -1.8922e+00
Epoch 5/10
19/19 - 6s - loss: 198.3087 - loglik: -1.9649e+02 - logprior: -1.8166e+00
Epoch 6/10
19/19 - 6s - loss: 197.0058 - loglik: -1.9514e+02 - logprior: -1.8663e+00
Epoch 7/10
19/19 - 6s - loss: 194.0438 - loglik: -1.9195e+02 - logprior: -2.0968e+00
Epoch 8/10
19/19 - 6s - loss: 195.1082 - loglik: -1.9294e+02 - logprior: -2.1667e+00
Fitted a model with MAP estimate = -188.1539
expansions: [(7, 1), (8, 2), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 1), (53, 1), (54, 1), (55, 2), (56, 1), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (99, 1), (101, 1), (104, 1), (114, 1), (121, 1), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Fitting a model of length 182 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 180.9427 - loglik: -1.7824e+02 - logprior: -2.6994e+00
Epoch 2/2
19/19 - 8s - loss: 144.3530 - loglik: -1.4344e+02 - logprior: -9.0801e-01
Fitted a model with MAP estimate = -142.8758
expansions: []
discards: [ 67  76 152 157]
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 146.7046 - loglik: -1.4407e+02 - logprior: -2.6388e+00
Epoch 2/2
19/19 - 8s - loss: 141.1566 - loglik: -1.4023e+02 - logprior: -9.3049e-01
Fitted a model with MAP estimate = -141.4350
expansions: []
discards: []
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 12s - loss: 144.8822 - loglik: -1.4277e+02 - logprior: -2.1100e+00
Epoch 2/10
22/22 - 9s - loss: 138.8032 - loglik: -1.3777e+02 - logprior: -1.0321e+00
Epoch 3/10
22/22 - 9s - loss: 137.3597 - loglik: -1.3634e+02 - logprior: -1.0206e+00
Epoch 4/10
22/22 - 9s - loss: 136.7502 - loglik: -1.3576e+02 - logprior: -9.8798e-01
Epoch 5/10
22/22 - 10s - loss: 131.4489 - loglik: -1.3048e+02 - logprior: -9.6899e-01
Epoch 6/10
22/22 - 10s - loss: 132.4936 - loglik: -1.3155e+02 - logprior: -9.4510e-01
Fitted a model with MAP estimate = -130.5820
Time for alignment: 196.2809
Computed alignments with likelihoods: ['-128.5293', '-128.2881', '-130.5820']
Best model has likelihood: -128.2881
SP score = 1.0000
Training of 3 independent models on file ltn.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f36988e1280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34a7151fd0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 184 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 615.9086 - loglik: -5.7950e+02 - logprior: -3.6407e+01
Epoch 2/10
10/10 - 7s - loss: 539.0596 - loglik: -5.3344e+02 - logprior: -5.6168e+00
Epoch 3/10
10/10 - 7s - loss: 482.2625 - loglik: -4.8123e+02 - logprior: -1.0289e+00
Epoch 4/10
10/10 - 7s - loss: 442.7665 - loglik: -4.4230e+02 - logprior: -4.6348e-01
Epoch 5/10
10/10 - 7s - loss: 427.4447 - loglik: -4.2742e+02 - logprior: -2.7591e-02
Epoch 6/10
10/10 - 7s - loss: 422.8131 - loglik: -4.2326e+02 - logprior: 0.4496
Epoch 7/10
10/10 - 7s - loss: 419.5555 - loglik: -4.2026e+02 - logprior: 0.7049
Epoch 8/10
10/10 - 7s - loss: 417.8257 - loglik: -4.1862e+02 - logprior: 0.7974
Epoch 9/10
10/10 - 7s - loss: 417.9226 - loglik: -4.1889e+02 - logprior: 0.9640
Fitted a model with MAP estimate = -417.3089
expansions: [(11, 3), (21, 1), (30, 2), (31, 3), (32, 1), (41, 1), (48, 1), (60, 1), (61, 1), (62, 1), (63, 1), (75, 2), (76, 2), (77, 2), (86, 1), (87, 1), (90, 1), (92, 1), (101, 1), (102, 1), (118, 1), (126, 5), (127, 1), (129, 2), (130, 2), (137, 1), (146, 3), (149, 2), (153, 1), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Fitting a model of length 234 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 13s - loss: 460.3850 - loglik: -4.1958e+02 - logprior: -4.0807e+01
Epoch 2/2
10/10 - 9s - loss: 418.6732 - loglik: -4.0517e+02 - logprior: -1.3502e+01
Fitted a model with MAP estimate = -410.8486
expansions: [(0, 4)]
discards: [  0  35  36  91 156 165 191 220]
Fitting a model of length 230 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 435.8594 - loglik: -4.0467e+02 - logprior: -3.1191e+01
Epoch 2/2
10/10 - 9s - loss: 401.8916 - loglik: -3.9840e+02 - logprior: -3.4890e+00
Fitted a model with MAP estimate = -397.9623
expansions: []
discards: [  1   2   3 154 155]
Fitting a model of length 225 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 12s - loss: 431.4251 - loglik: -4.0121e+02 - logprior: -3.0210e+01
Epoch 2/10
10/10 - 8s - loss: 403.5932 - loglik: -4.0056e+02 - logprior: -3.0344e+00
Epoch 3/10
10/10 - 9s - loss: 395.7657 - loglik: -3.9849e+02 - logprior: 2.7204
Epoch 4/10
10/10 - 9s - loss: 392.2958 - loglik: -3.9751e+02 - logprior: 5.2114
Epoch 5/10
10/10 - 9s - loss: 391.5451 - loglik: -3.9814e+02 - logprior: 6.5907
Epoch 6/10
10/10 - 9s - loss: 389.9586 - loglik: -3.9737e+02 - logprior: 7.4158
Epoch 7/10
10/10 - 9s - loss: 390.4380 - loglik: -3.9836e+02 - logprior: 7.9246
Fitted a model with MAP estimate = -389.3670
Time for alignment: 182.5491
Fitting a model of length 184 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 615.6584 - loglik: -5.7925e+02 - logprior: -3.6406e+01
Epoch 2/10
10/10 - 7s - loss: 539.0905 - loglik: -5.3347e+02 - logprior: -5.6200e+00
Epoch 3/10
10/10 - 7s - loss: 483.3083 - loglik: -4.8224e+02 - logprior: -1.0716e+00
Epoch 4/10
10/10 - 7s - loss: 446.3405 - loglik: -4.4605e+02 - logprior: -2.8988e-01
Epoch 5/10
10/10 - 7s - loss: 431.7185 - loglik: -4.3206e+02 - logprior: 0.3448
Epoch 6/10
10/10 - 7s - loss: 424.6737 - loglik: -4.2525e+02 - logprior: 0.5792
Epoch 7/10
10/10 - 7s - loss: 420.5273 - loglik: -4.2134e+02 - logprior: 0.8127
Epoch 8/10
10/10 - 7s - loss: 420.7301 - loglik: -4.2167e+02 - logprior: 0.9431
Fitted a model with MAP estimate = -418.9633
expansions: [(11, 3), (19, 1), (30, 2), (31, 2), (32, 1), (41, 1), (44, 1), (58, 1), (61, 2), (62, 2), (74, 1), (75, 2), (76, 1), (77, 1), (86, 1), (88, 1), (90, 1), (92, 1), (101, 1), (102, 2), (118, 1), (126, 5), (127, 1), (129, 2), (130, 2), (137, 1), (139, 1), (146, 3), (148, 2), (153, 2), (173, 2), (174, 1), (175, 1)]
discards: [0]
Fitting a model of length 235 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 459.1592 - loglik: -4.1841e+02 - logprior: -4.0750e+01
Epoch 2/2
10/10 - 8s - loss: 416.8659 - loglik: -4.0351e+02 - logprior: -1.3354e+01
Fitted a model with MAP estimate = -409.2067
expansions: [(0, 4), (188, 1)]
discards: [  0  35 156 165 189 190 191 192 221]
Fitting a model of length 231 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 435.1288 - loglik: -4.0406e+02 - logprior: -3.1070e+01
Epoch 2/2
10/10 - 7s - loss: 403.2914 - loglik: -3.9977e+02 - logprior: -3.5174e+00
Fitted a model with MAP estimate = -397.5155
expansions: [(188, 2), (190, 1)]
discards: [  1   2   3 131 156 157]
Fitting a model of length 228 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 431.9345 - loglik: -4.0175e+02 - logprior: -3.0182e+01
Epoch 2/10
10/10 - 7s - loss: 402.1874 - loglik: -3.9910e+02 - logprior: -3.0838e+00
Epoch 3/10
10/10 - 7s - loss: 395.3155 - loglik: -3.9796e+02 - logprior: 2.6480
Epoch 4/10
10/10 - 7s - loss: 391.7314 - loglik: -3.9687e+02 - logprior: 5.1434
Epoch 5/10
10/10 - 7s - loss: 390.0558 - loglik: -3.9662e+02 - logprior: 6.5625
Epoch 6/10
10/10 - 7s - loss: 389.6082 - loglik: -3.9699e+02 - logprior: 7.3769
Epoch 7/10
10/10 - 7s - loss: 388.5873 - loglik: -3.9649e+02 - logprior: 7.9052
Epoch 8/10
10/10 - 7s - loss: 387.9633 - loglik: -3.9625e+02 - logprior: 8.2874
Epoch 9/10
10/10 - 7s - loss: 388.8146 - loglik: -3.9746e+02 - logprior: 8.6487
Fitted a model with MAP estimate = -387.6622
Time for alignment: 173.3351
Fitting a model of length 184 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 615.0609 - loglik: -5.7866e+02 - logprior: -3.6406e+01
Epoch 2/10
10/10 - 5s - loss: 539.5343 - loglik: -5.3390e+02 - logprior: -5.6315e+00
Epoch 3/10
10/10 - 5s - loss: 482.7025 - loglik: -4.8165e+02 - logprior: -1.0539e+00
Epoch 4/10
10/10 - 5s - loss: 443.1768 - loglik: -4.4280e+02 - logprior: -3.7375e-01
Epoch 5/10
10/10 - 5s - loss: 429.9595 - loglik: -4.3003e+02 - logprior: 0.0719
Epoch 6/10
10/10 - 5s - loss: 425.5748 - loglik: -4.2622e+02 - logprior: 0.6483
Epoch 7/10
10/10 - 5s - loss: 424.0594 - loglik: -4.2502e+02 - logprior: 0.9603
Epoch 8/10
10/10 - 5s - loss: 422.8915 - loglik: -4.2395e+02 - logprior: 1.0623
Epoch 9/10
10/10 - 5s - loss: 422.5335 - loglik: -4.2373e+02 - logprior: 1.1942
Epoch 10/10
10/10 - 5s - loss: 422.2645 - loglik: -4.2355e+02 - logprior: 1.2891
Fitted a model with MAP estimate = -421.6801
expansions: [(11, 3), (19, 1), (30, 2), (31, 2), (32, 2), (41, 1), (48, 1), (60, 1), (61, 1), (62, 1), (63, 1), (75, 1), (76, 2), (77, 2), (78, 2), (86, 1), (90, 1), (91, 1), (93, 1), (101, 1), (102, 1), (126, 1), (127, 3), (129, 3), (146, 1), (147, 2), (149, 3), (153, 2), (173, 2), (174, 1), (175, 1)]
discards: [0]
Fitting a model of length 231 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 462.9380 - loglik: -4.2214e+02 - logprior: -4.0796e+01
Epoch 2/2
10/10 - 7s - loss: 423.1301 - loglik: -4.0980e+02 - logprior: -1.3334e+01
Fitted a model with MAP estimate = -416.0250
expansions: [(0, 4)]
discards: [  0  35  39  95 154 188 217]
Fitting a model of length 228 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 439.5159 - loglik: -4.0841e+02 - logprior: -3.1110e+01
Epoch 2/2
10/10 - 8s - loss: 406.0649 - loglik: -4.0271e+02 - logprior: -3.3572e+00
Fitted a model with MAP estimate = -400.4958
expansions: [(156, 4)]
discards: [1 2 3]
Fitting a model of length 229 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 432.8114 - loglik: -4.0275e+02 - logprior: -3.0056e+01
Epoch 2/10
10/10 - 8s - loss: 403.0994 - loglik: -4.0026e+02 - logprior: -2.8418e+00
Epoch 3/10
10/10 - 9s - loss: 396.0155 - loglik: -3.9888e+02 - logprior: 2.8607
Epoch 4/10
10/10 - 9s - loss: 393.0837 - loglik: -3.9838e+02 - logprior: 5.2932
Epoch 5/10
10/10 - 9s - loss: 389.9476 - loglik: -3.9660e+02 - logprior: 6.6534
Epoch 6/10
10/10 - 9s - loss: 389.5783 - loglik: -3.9703e+02 - logprior: 7.4508
Epoch 7/10
10/10 - 9s - loss: 388.0487 - loglik: -3.9602e+02 - logprior: 7.9710
Epoch 8/10
10/10 - 10s - loss: 388.3325 - loglik: -3.9673e+02 - logprior: 8.3963
Fitted a model with MAP estimate = -387.4736
Time for alignment: 173.6407
Computed alignments with likelihoods: ['-389.3670', '-387.6622', '-387.4736']
Best model has likelihood: -387.4736
SP score = 0.9390
Training of 3 independent models on file aadh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32aab9cca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34afafde80>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 187 on 3127 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 15s - loss: 644.7458 - loglik: -6.3199e+02 - logprior: -1.2757e+01
Epoch 2/10
10/10 - 9s - loss: 570.5902 - loglik: -5.6825e+02 - logprior: -2.3411e+00
Epoch 3/10
10/10 - 10s - loss: 502.1785 - loglik: -5.0078e+02 - logprior: -1.4008e+00
Epoch 4/10
10/10 - 11s - loss: 461.8385 - loglik: -4.5993e+02 - logprior: -1.9094e+00
Epoch 5/10
10/10 - 10s - loss: 443.6880 - loglik: -4.4137e+02 - logprior: -2.3135e+00
Epoch 6/10
10/10 - 10s - loss: 436.1713 - loglik: -4.3373e+02 - logprior: -2.4385e+00
Epoch 7/10
10/10 - 11s - loss: 433.0154 - loglik: -4.3072e+02 - logprior: -2.2954e+00
Epoch 8/10
10/10 - 11s - loss: 432.7017 - loglik: -4.3055e+02 - logprior: -2.1517e+00
Epoch 9/10
10/10 - 11s - loss: 430.7301 - loglik: -4.2858e+02 - logprior: -2.1455e+00
Epoch 10/10
10/10 - 10s - loss: 430.0197 - loglik: -4.2783e+02 - logprior: -2.1888e+00
Fitted a model with MAP estimate = -429.7037
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (37, 1), (38, 2), (39, 2), (40, 1), (50, 1), (64, 4), (66, 1), (76, 8), (78, 1), (91, 1), (96, 1), (99, 2), (118, 2), (119, 1), (120, 1), (121, 1), (145, 1), (146, 1), (147, 1), (154, 1), (160, 1), (162, 1), (163, 1), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (177, 2)]
discards: [0]
Fitting a model of length 243 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 19s - loss: 428.1749 - loglik: -4.1842e+02 - logprior: -9.7561e+00
Epoch 2/2
21/21 - 16s - loss: 399.8250 - loglik: -3.9681e+02 - logprior: -3.0196e+00
Fitted a model with MAP estimate = -394.2018
expansions: []
discards: [ 13  18  28  49  83 134 154 219]
Fitting a model of length 235 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 17s - loss: 406.1810 - loglik: -3.9942e+02 - logprior: -6.7655e+00
Epoch 2/2
21/21 - 15s - loss: 394.0422 - loglik: -3.9437e+02 - logprior: 0.3305
Fitted a model with MAP estimate = -392.1361
expansions: []
discards: [82]
Fitting a model of length 234 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 17s - loss: 403.7063 - loglik: -3.9765e+02 - logprior: -6.0607e+00
Epoch 2/10
21/21 - 13s - loss: 393.4911 - loglik: -3.9409e+02 - logprior: 0.5941
Epoch 3/10
21/21 - 12s - loss: 391.6883 - loglik: -3.9309e+02 - logprior: 1.4064
Epoch 4/10
21/21 - 13s - loss: 390.0504 - loglik: -3.9184e+02 - logprior: 1.7883
Epoch 5/10
21/21 - 13s - loss: 390.1128 - loglik: -3.9207e+02 - logprior: 1.9543
Fitted a model with MAP estimate = -388.3821
Time for alignment: 279.6235
Fitting a model of length 187 on 3127 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 644.1005 - loglik: -6.3134e+02 - logprior: -1.2758e+01
Epoch 2/10
10/10 - 10s - loss: 570.8306 - loglik: -5.6847e+02 - logprior: -2.3573e+00
Epoch 3/10
10/10 - 10s - loss: 507.5031 - loglik: -5.0603e+02 - logprior: -1.4711e+00
Epoch 4/10
10/10 - 9s - loss: 462.5072 - loglik: -4.6041e+02 - logprior: -2.0956e+00
Epoch 5/10
10/10 - 10s - loss: 442.0702 - loglik: -4.3953e+02 - logprior: -2.5384e+00
Epoch 6/10
10/10 - 10s - loss: 435.8138 - loglik: -4.3320e+02 - logprior: -2.6103e+00
Epoch 7/10
10/10 - 11s - loss: 430.9552 - loglik: -4.2848e+02 - logprior: -2.4723e+00
Epoch 8/10
10/10 - 10s - loss: 431.4911 - loglik: -4.2917e+02 - logprior: -2.3220e+00
Fitted a model with MAP estimate = -429.9176
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (23, 1), (25, 1), (36, 1), (38, 2), (39, 1), (49, 1), (59, 1), (63, 2), (76, 7), (78, 2), (80, 1), (96, 1), (99, 1), (115, 1), (116, 1), (119, 1), (121, 3), (122, 1), (143, 1), (146, 1), (147, 2), (158, 1), (160, 1), (163, 2), (164, 1), (165, 1), (168, 1), (169, 2), (170, 1), (171, 1), (178, 1), (179, 1)]
discards: [0]
Fitting a model of length 243 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 18s - loss: 427.5545 - loglik: -4.1778e+02 - logprior: -9.7767e+00
Epoch 2/2
21/21 - 14s - loss: 397.5714 - loglik: -3.9475e+02 - logprior: -2.8187e+00
Fitted a model with MAP estimate = -391.6828
expansions: []
discards: [ 19  28  82 157 219]
Fitting a model of length 238 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 15s - loss: 401.8944 - loglik: -3.9528e+02 - logprior: -6.6115e+00
Epoch 2/2
21/21 - 12s - loss: 389.1913 - loglik: -3.8956e+02 - logprior: 0.3684
Fitted a model with MAP estimate = -387.5662
expansions: []
discards: []
Fitting a model of length 238 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 14s - loss: 398.8159 - loglik: -3.9288e+02 - logprior: -5.9366e+00
Epoch 2/10
21/21 - 12s - loss: 388.2321 - loglik: -3.8898e+02 - logprior: 0.7522
Epoch 3/10
21/21 - 12s - loss: 385.4666 - loglik: -3.8704e+02 - logprior: 1.5755
Epoch 4/10
21/21 - 12s - loss: 384.2797 - loglik: -3.8625e+02 - logprior: 1.9708
Epoch 5/10
21/21 - 13s - loss: 383.4810 - loglik: -3.8562e+02 - logprior: 2.1380
Epoch 6/10
21/21 - 13s - loss: 382.1805 - loglik: -3.8448e+02 - logprior: 2.2980
Epoch 7/10
21/21 - 13s - loss: 382.3397 - loglik: -3.8481e+02 - logprior: 2.4698
Fitted a model with MAP estimate = -381.4569
Time for alignment: 265.6859
Fitting a model of length 187 on 3127 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 13s - loss: 644.9747 - loglik: -6.3222e+02 - logprior: -1.2752e+01
Epoch 2/10
10/10 - 9s - loss: 569.6856 - loglik: -5.6733e+02 - logprior: -2.3577e+00
Epoch 3/10
10/10 - 10s - loss: 503.0536 - loglik: -5.0160e+02 - logprior: -1.4553e+00
Epoch 4/10
10/10 - 9s - loss: 460.8817 - loglik: -4.5888e+02 - logprior: -2.0058e+00
Epoch 5/10
10/10 - 11s - loss: 445.1997 - loglik: -4.4279e+02 - logprior: -2.4053e+00
Epoch 6/10
10/10 - 10s - loss: 437.8988 - loglik: -4.3542e+02 - logprior: -2.4813e+00
Epoch 7/10
10/10 - 11s - loss: 435.4693 - loglik: -4.3312e+02 - logprior: -2.3497e+00
Epoch 8/10
10/10 - 11s - loss: 433.7070 - loglik: -4.3151e+02 - logprior: -2.1936e+00
Epoch 9/10
10/10 - 10s - loss: 432.9013 - loglik: -4.3074e+02 - logprior: -2.1623e+00
Epoch 10/10
10/10 - 9s - loss: 432.0058 - loglik: -4.2984e+02 - logprior: -2.1663e+00
Fitted a model with MAP estimate = -432.0210
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (37, 1), (38, 2), (39, 2), (40, 1), (50, 1), (60, 1), (63, 3), (66, 2), (74, 1), (75, 4), (76, 2), (78, 1), (80, 1), (96, 1), (99, 2), (115, 1), (116, 1), (119, 1), (121, 1), (122, 1), (146, 2), (147, 1), (158, 1), (160, 1), (161, 1), (162, 2), (163, 1), (167, 1), (169, 2), (170, 1), (171, 1), (178, 1), (181, 1)]
discards: [0]
Fitting a model of length 243 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 15s - loss: 431.9470 - loglik: -4.2213e+02 - logprior: -9.8165e+00
Epoch 2/2
21/21 - 12s - loss: 401.9890 - loglik: -3.9897e+02 - logprior: -3.0179e+00
Fitted a model with MAP estimate = -397.7660
expansions: [(186, 1)]
discards: [ 13  18  28  47  87 134 219]
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 14s - loss: 409.3481 - loglik: -4.0248e+02 - logprior: -6.8639e+00
Epoch 2/2
21/21 - 12s - loss: 396.6131 - loglik: -3.9688e+02 - logprior: 0.2682
Fitted a model with MAP estimate = -394.6224
expansions: []
discards: [77]
Fitting a model of length 236 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 16s - loss: 405.9321 - loglik: -3.9993e+02 - logprior: -6.0012e+00
Epoch 2/10
21/21 - 12s - loss: 396.0308 - loglik: -3.9668e+02 - logprior: 0.6540
Epoch 3/10
21/21 - 13s - loss: 394.0931 - loglik: -3.9555e+02 - logprior: 1.4585
Epoch 4/10
21/21 - 13s - loss: 390.7863 - loglik: -3.9265e+02 - logprior: 1.8670
Epoch 5/10
21/21 - 14s - loss: 391.8734 - loglik: -3.9389e+02 - logprior: 2.0208
Fitted a model with MAP estimate = -390.3704
Time for alignment: 258.8100
Computed alignments with likelihoods: ['-388.3821', '-381.4569', '-390.3704']
Best model has likelihood: -381.4569
SP score = 0.5867
Training of 3 independent models on file gluts.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f350d07fb20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2cedb27fd0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 283.7301 - loglik: -2.8072e+02 - logprior: -3.0057e+00
Epoch 2/10
19/19 - 3s - loss: 254.6957 - loglik: -2.5381e+02 - logprior: -8.8955e-01
Epoch 3/10
19/19 - 3s - loss: 241.2036 - loglik: -2.4028e+02 - logprior: -9.2820e-01
Epoch 4/10
19/19 - 3s - loss: 237.0151 - loglik: -2.3617e+02 - logprior: -8.4271e-01
Epoch 5/10
19/19 - 3s - loss: 233.5858 - loglik: -2.3276e+02 - logprior: -8.2712e-01
Epoch 6/10
19/19 - 3s - loss: 231.6398 - loglik: -2.3081e+02 - logprior: -8.2593e-01
Epoch 7/10
19/19 - 3s - loss: 231.2216 - loglik: -2.3040e+02 - logprior: -8.1963e-01
Epoch 8/10
19/19 - 3s - loss: 230.1086 - loglik: -2.2928e+02 - logprior: -8.2398e-01
Epoch 9/10
19/19 - 3s - loss: 229.1423 - loglik: -2.2832e+02 - logprior: -8.2608e-01
Epoch 10/10
19/19 - 3s - loss: 229.9534 - loglik: -2.2914e+02 - logprior: -8.1379e-01
Fitted a model with MAP estimate = -228.4903
expansions: [(0, 8), (16, 3), (31, 2), (55, 1), (57, 2), (59, 4), (61, 2)]
discards: []
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 267.1404 - loglik: -2.6312e+02 - logprior: -4.0214e+00
Epoch 2/2
19/19 - 4s - loss: 243.4437 - loglik: -2.4224e+02 - logprior: -1.2037e+00
Fitted a model with MAP estimate = -237.7882
expansions: [(0, 11)]
discards: [ 8  9 10 11 12 13 14 15 16 17 18 19 24 43 75 81 82]
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 245.4770 - loglik: -2.4163e+02 - logprior: -3.8499e+00
Epoch 2/2
19/19 - 3s - loss: 238.9775 - loglik: -2.3773e+02 - logprior: -1.2448e+00
Fitted a model with MAP estimate = -235.6490
expansions: [(0, 7), (14, 3), (22, 2), (69, 2)]
discards: [ 1  2  3  4  5  6  7 70 71]
Fitting a model of length 98 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 242.2743 - loglik: -2.3860e+02 - logprior: -3.6720e+00
Epoch 2/10
19/19 - 4s - loss: 236.7013 - loglik: -2.3536e+02 - logprior: -1.3396e+00
Epoch 3/10
19/19 - 4s - loss: 233.6791 - loglik: -2.3263e+02 - logprior: -1.0529e+00
Epoch 4/10
19/19 - 4s - loss: 231.1233 - loglik: -2.3030e+02 - logprior: -8.2237e-01
Epoch 5/10
19/19 - 4s - loss: 228.2958 - loglik: -2.2752e+02 - logprior: -7.7482e-01
Epoch 6/10
19/19 - 4s - loss: 226.2353 - loglik: -2.2548e+02 - logprior: -7.5326e-01
Epoch 7/10
19/19 - 4s - loss: 224.6263 - loglik: -2.2389e+02 - logprior: -7.3718e-01
Epoch 8/10
19/19 - 4s - loss: 223.0449 - loglik: -2.2232e+02 - logprior: -7.2843e-01
Epoch 9/10
19/19 - 4s - loss: 222.3317 - loglik: -2.2161e+02 - logprior: -7.2037e-01
Epoch 10/10
19/19 - 4s - loss: 221.4224 - loglik: -2.2070e+02 - logprior: -7.2057e-01
Fitted a model with MAP estimate = -221.2897
Time for alignment: 111.5233
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 282.9931 - loglik: -2.7999e+02 - logprior: -3.0041e+00
Epoch 2/10
19/19 - 3s - loss: 250.0012 - loglik: -2.4913e+02 - logprior: -8.7266e-01
Epoch 3/10
19/19 - 3s - loss: 240.7694 - loglik: -2.3989e+02 - logprior: -8.7914e-01
Epoch 4/10
19/19 - 3s - loss: 236.7823 - loglik: -2.3600e+02 - logprior: -7.7771e-01
Epoch 5/10
19/19 - 3s - loss: 234.0003 - loglik: -2.3321e+02 - logprior: -7.8867e-01
Epoch 6/10
19/19 - 3s - loss: 231.6785 - loglik: -2.3088e+02 - logprior: -7.9356e-01
Epoch 7/10
19/19 - 3s - loss: 231.0005 - loglik: -2.3020e+02 - logprior: -8.0461e-01
Epoch 8/10
19/19 - 3s - loss: 229.7009 - loglik: -2.2890e+02 - logprior: -8.0465e-01
Epoch 9/10
19/19 - 3s - loss: 229.1412 - loglik: -2.2833e+02 - logprior: -8.0845e-01
Epoch 10/10
19/19 - 3s - loss: 229.5454 - loglik: -2.2875e+02 - logprior: -8.0032e-01
Fitted a model with MAP estimate = -228.2451
expansions: [(0, 8), (12, 1), (13, 4), (14, 1), (15, 2), (18, 1), (57, 2), (59, 4), (61, 2)]
discards: []
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 266.8259 - loglik: -2.6285e+02 - logprior: -3.9789e+00
Epoch 2/2
19/19 - 4s - loss: 241.7998 - loglik: -2.4063e+02 - logprior: -1.1705e+00
Fitted a model with MAP estimate = -236.4513
expansions: [(0, 10)]
discards: [ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 29 84 85]
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 243.8642 - loglik: -2.4005e+02 - logprior: -3.8119e+00
Epoch 2/2
19/19 - 4s - loss: 238.0499 - loglik: -2.3680e+02 - logprior: -1.2516e+00
Fitted a model with MAP estimate = -235.0009
expansions: [(0, 7), (13, 5), (16, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 71 72]
Fitting a model of length 97 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 241.7296 - loglik: -2.3812e+02 - logprior: -3.6120e+00
Epoch 2/10
19/19 - 4s - loss: 236.2373 - loglik: -2.3497e+02 - logprior: -1.2643e+00
Epoch 3/10
19/19 - 4s - loss: 233.3934 - loglik: -2.3242e+02 - logprior: -9.7392e-01
Epoch 4/10
19/19 - 4s - loss: 231.1192 - loglik: -2.3029e+02 - logprior: -8.3185e-01
Epoch 5/10
19/19 - 4s - loss: 227.6409 - loglik: -2.2694e+02 - logprior: -7.0066e-01
Epoch 6/10
19/19 - 4s - loss: 226.5893 - loglik: -2.2587e+02 - logprior: -7.1932e-01
Epoch 7/10
19/19 - 4s - loss: 223.9663 - loglik: -2.2322e+02 - logprior: -7.5080e-01
Epoch 8/10
19/19 - 4s - loss: 222.7488 - loglik: -2.2202e+02 - logprior: -7.2609e-01
Epoch 9/10
19/19 - 4s - loss: 221.3056 - loglik: -2.2058e+02 - logprior: -7.2240e-01
Epoch 10/10
19/19 - 4s - loss: 221.3133 - loglik: -2.2063e+02 - logprior: -6.8105e-01
Fitted a model with MAP estimate = -220.5505
Time for alignment: 113.9883
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 283.2966 - loglik: -2.8029e+02 - logprior: -3.0103e+00
Epoch 2/10
19/19 - 3s - loss: 251.9080 - loglik: -2.5103e+02 - logprior: -8.7667e-01
Epoch 3/10
19/19 - 3s - loss: 241.6010 - loglik: -2.4072e+02 - logprior: -8.8145e-01
Epoch 4/10
19/19 - 3s - loss: 237.4200 - loglik: -2.3662e+02 - logprior: -8.0418e-01
Epoch 5/10
19/19 - 3s - loss: 234.0148 - loglik: -2.3321e+02 - logprior: -8.0865e-01
Epoch 6/10
19/19 - 3s - loss: 231.5796 - loglik: -2.3077e+02 - logprior: -8.1275e-01
Epoch 7/10
19/19 - 3s - loss: 231.3212 - loglik: -2.3051e+02 - logprior: -8.1167e-01
Epoch 8/10
19/19 - 3s - loss: 230.0263 - loglik: -2.2920e+02 - logprior: -8.2365e-01
Epoch 9/10
19/19 - 3s - loss: 229.6313 - loglik: -2.2881e+02 - logprior: -8.2259e-01
Epoch 10/10
19/19 - 3s - loss: 229.1085 - loglik: -2.2829e+02 - logprior: -8.2084e-01
Fitted a model with MAP estimate = -228.2441
expansions: [(0, 8), (13, 4), (16, 2), (19, 1), (22, 2), (57, 2), (59, 4), (61, 2)]
discards: []
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 267.9927 - loglik: -2.6400e+02 - logprior: -3.9952e+00
Epoch 2/2
19/19 - 4s - loss: 243.1110 - loglik: -2.4189e+02 - logprior: -1.2241e+00
Fitted a model with MAP estimate = -237.6427
expansions: [(0, 11)]
discards: [ 6  7  8  9 10 11 12 13 14 15 16 17 25 38 84 85]
Fitting a model of length 97 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 244.7431 - loglik: -2.4083e+02 - logprior: -3.9171e+00
Epoch 2/2
19/19 - 4s - loss: 238.0507 - loglik: -2.3671e+02 - logprior: -1.3374e+00
Fitted a model with MAP estimate = -235.3964
expansions: [(0, 7), (15, 4), (18, 2)]
discards: [ 1  2  3  4  5  6  7  8  9 10 23 24 73 74]
Fitting a model of length 96 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 242.2831 - loglik: -2.3869e+02 - logprior: -3.5976e+00
Epoch 2/10
19/19 - 4s - loss: 236.7555 - loglik: -2.3544e+02 - logprior: -1.3138e+00
Epoch 3/10
19/19 - 4s - loss: 233.8214 - loglik: -2.3275e+02 - logprior: -1.0741e+00
Epoch 4/10
19/19 - 4s - loss: 231.5328 - loglik: -2.3053e+02 - logprior: -1.0050e+00
Epoch 5/10
19/19 - 4s - loss: 228.9615 - loglik: -2.2813e+02 - logprior: -8.3182e-01
Epoch 6/10
19/19 - 4s - loss: 226.4862 - loglik: -2.2571e+02 - logprior: -7.8039e-01
Epoch 7/10
19/19 - 4s - loss: 224.3512 - loglik: -2.2360e+02 - logprior: -7.5155e-01
Epoch 8/10
19/19 - 4s - loss: 223.2140 - loglik: -2.2248e+02 - logprior: -7.3432e-01
Epoch 9/10
19/19 - 4s - loss: 222.0493 - loglik: -2.2132e+02 - logprior: -7.2942e-01
Epoch 10/10
19/19 - 4s - loss: 222.4079 - loglik: -2.2169e+02 - logprior: -7.1804e-01
Fitted a model with MAP estimate = -221.7180
Time for alignment: 113.5199
Computed alignments with likelihoods: ['-221.2897', '-220.5505', '-221.7180']
Best model has likelihood: -220.5505
SP score = 0.3233
Training of 3 independent models on file tms.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34eb0f5640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f369935bc70>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 219 on 2118 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 18s - loss: 769.7484 - loglik: -7.5193e+02 - logprior: -1.7819e+01
Epoch 2/10
10/10 - 13s - loss: 660.4490 - loglik: -6.5765e+02 - logprior: -2.8000e+00
Epoch 3/10
10/10 - 14s - loss: 553.4003 - loglik: -5.5188e+02 - logprior: -1.5216e+00
Epoch 4/10
10/10 - 14s - loss: 473.9364 - loglik: -4.7152e+02 - logprior: -2.4139e+00
Epoch 5/10
10/10 - 14s - loss: 444.7549 - loglik: -4.4159e+02 - logprior: -3.1678e+00
Epoch 6/10
10/10 - 16s - loss: 435.1384 - loglik: -4.3175e+02 - logprior: -3.3893e+00
Epoch 7/10
10/10 - 14s - loss: 431.3919 - loglik: -4.2809e+02 - logprior: -3.2987e+00
Epoch 8/10
10/10 - 16s - loss: 429.3724 - loglik: -4.2627e+02 - logprior: -3.0989e+00
Epoch 9/10
10/10 - 15s - loss: 427.5292 - loglik: -4.2461e+02 - logprior: -2.9150e+00
Epoch 10/10
10/10 - 15s - loss: 428.9335 - loglik: -4.2606e+02 - logprior: -2.8689e+00
Fitted a model with MAP estimate = -427.2166
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (33, 2), (46, 1), (55, 1), (56, 1), (57, 1), (58, 1), (60, 1), (61, 1), (63, 1), (86, 2), (90, 1), (95, 1), (96, 1), (97, 1), (114, 2), (115, 3), (129, 3), (130, 2), (139, 1), (140, 1), (159, 1), (160, 1), (162, 1), (176, 1), (178, 1), (180, 1), (184, 1), (188, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Fitting a model of length 267 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 19s - loss: 431.0441 - loglik: -4.1526e+02 - logprior: -1.5786e+01
Epoch 2/2
17/17 - 17s - loss: 392.7153 - loglik: -3.8826e+02 - logprior: -4.4515e+00
Fitted a model with MAP estimate = -386.5692
expansions: [(0, 10)]
discards: [  0 135 156 159]
Fitting a model of length 273 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 21s - loss: 397.1729 - loglik: -3.8642e+02 - logprior: -1.0751e+01
Epoch 2/2
17/17 - 17s - loss: 382.0729 - loglik: -3.8247e+02 - logprior: 0.3952
Fitted a model with MAP estimate = -378.5651
expansions: []
discards: [1 2 3 4 5 6 7 8 9]
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 18s - loss: 393.6248 - loglik: -3.8372e+02 - logprior: -9.9088e+00
Epoch 2/10
17/17 - 16s - loss: 381.7219 - loglik: -3.8301e+02 - logprior: 1.2862
Epoch 3/10
17/17 - 15s - loss: 377.1441 - loglik: -3.8004e+02 - logprior: 2.8985
Epoch 4/10
17/17 - 14s - loss: 374.5085 - loglik: -3.7824e+02 - logprior: 3.7341
Epoch 5/10
17/17 - 14s - loss: 374.2476 - loglik: -3.7835e+02 - logprior: 4.0976
Epoch 6/10
17/17 - 13s - loss: 372.8219 - loglik: -3.7717e+02 - logprior: 4.3471
Epoch 7/10
17/17 - 13s - loss: 370.6247 - loglik: -3.7514e+02 - logprior: 4.5187
Epoch 8/10
17/17 - 14s - loss: 370.3466 - loglik: -3.7513e+02 - logprior: 4.7874
Epoch 9/10
17/17 - 14s - loss: 369.4856 - loglik: -3.7455e+02 - logprior: 5.0643
Epoch 10/10
17/17 - 14s - loss: 372.1642 - loglik: -3.7753e+02 - logprior: 5.3612
Fitted a model with MAP estimate = -369.5617
Time for alignment: 406.1908
Fitting a model of length 219 on 2118 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 14s - loss: 770.1492 - loglik: -7.5233e+02 - logprior: -1.7821e+01
Epoch 2/10
10/10 - 14s - loss: 658.9633 - loglik: -6.5617e+02 - logprior: -2.7929e+00
Epoch 3/10
10/10 - 14s - loss: 550.5382 - loglik: -5.4898e+02 - logprior: -1.5619e+00
Epoch 4/10
10/10 - 15s - loss: 473.9579 - loglik: -4.7133e+02 - logprior: -2.6266e+00
Epoch 5/10
10/10 - 14s - loss: 443.1331 - loglik: -4.3980e+02 - logprior: -3.3313e+00
Epoch 6/10
10/10 - 14s - loss: 434.4880 - loglik: -4.3102e+02 - logprior: -3.4670e+00
Epoch 7/10
10/10 - 13s - loss: 430.0003 - loglik: -4.2667e+02 - logprior: -3.3315e+00
Epoch 8/10
10/10 - 12s - loss: 428.0398 - loglik: -4.2495e+02 - logprior: -3.0883e+00
Epoch 9/10
10/10 - 12s - loss: 427.9959 - loglik: -4.2512e+02 - logprior: -2.8752e+00
Epoch 10/10
10/10 - 12s - loss: 425.9162 - loglik: -4.2314e+02 - logprior: -2.7802e+00
Fitted a model with MAP estimate = -426.2867
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (26, 1), (27, 1), (32, 1), (55, 1), (57, 1), (58, 2), (60, 3), (62, 1), (86, 1), (89, 1), (94, 2), (97, 1), (114, 2), (115, 3), (129, 2), (131, 1), (140, 1), (141, 1), (156, 1), (160, 1), (163, 1), (177, 1), (179, 1), (180, 1), (181, 1), (184, 1), (191, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 16s - loss: 428.4461 - loglik: -4.1274e+02 - logprior: -1.5708e+01
Epoch 2/2
17/17 - 12s - loss: 390.7908 - loglik: -3.8639e+02 - logprior: -4.4034e+00
Fitted a model with MAP estimate = -386.1767
expansions: [(0, 10)]
discards: [  0 135 155]
Fitting a model of length 273 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 15s - loss: 397.5558 - loglik: -3.8681e+02 - logprior: -1.0747e+01
Epoch 2/2
17/17 - 13s - loss: 381.5850 - loglik: -3.8198e+02 - logprior: 0.3920
Fitted a model with MAP estimate = -378.3246
expansions: []
discards: [1 2 3 4 5 6 7 8 9]
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 15s - loss: 394.4060 - loglik: -3.8446e+02 - logprior: -9.9429e+00
Epoch 2/10
17/17 - 14s - loss: 379.2639 - loglik: -3.8054e+02 - logprior: 1.2791
Epoch 3/10
17/17 - 14s - loss: 376.3819 - loglik: -3.7929e+02 - logprior: 2.9105
Epoch 4/10
17/17 - 15s - loss: 376.1679 - loglik: -3.7989e+02 - logprior: 3.7184
Epoch 5/10
17/17 - 15s - loss: 373.5704 - loglik: -3.7769e+02 - logprior: 4.1184
Epoch 6/10
17/17 - 15s - loss: 373.0499 - loglik: -3.7740e+02 - logprior: 4.3471
Epoch 7/10
17/17 - 14s - loss: 370.0056 - loglik: -3.7455e+02 - logprior: 4.5480
Epoch 8/10
17/17 - 14s - loss: 371.0134 - loglik: -3.7582e+02 - logprior: 4.8034
Fitted a model with MAP estimate = -370.2512
Time for alignment: 338.8314
Fitting a model of length 219 on 2118 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 14s - loss: 769.5285 - loglik: -7.5171e+02 - logprior: -1.7819e+01
Epoch 2/10
10/10 - 11s - loss: 660.7357 - loglik: -6.5795e+02 - logprior: -2.7869e+00
Epoch 3/10
10/10 - 11s - loss: 550.7001 - loglik: -5.4913e+02 - logprior: -1.5743e+00
Epoch 4/10
10/10 - 10s - loss: 473.7338 - loglik: -4.7098e+02 - logprior: -2.7529e+00
Epoch 5/10
10/10 - 11s - loss: 443.2931 - loglik: -4.3965e+02 - logprior: -3.6477e+00
Epoch 6/10
10/10 - 11s - loss: 432.7224 - loglik: -4.2888e+02 - logprior: -3.8460e+00
Epoch 7/10
10/10 - 11s - loss: 429.8198 - loglik: -4.2617e+02 - logprior: -3.6494e+00
Epoch 8/10
10/10 - 11s - loss: 426.6665 - loglik: -4.2326e+02 - logprior: -3.4051e+00
Epoch 9/10
10/10 - 12s - loss: 425.9725 - loglik: -4.2272e+02 - logprior: -3.2525e+00
Epoch 10/10
10/10 - 13s - loss: 426.1477 - loglik: -4.2296e+02 - logprior: -3.1904e+00
Fitted a model with MAP estimate = -424.6375
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (26, 1), (32, 1), (46, 1), (55, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (63, 1), (86, 4), (95, 2), (97, 1), (100, 1), (114, 1), (115, 2), (116, 1), (123, 1), (129, 2), (131, 1), (140, 1), (141, 1), (160, 1), (161, 1), (163, 1), (177, 1), (179, 1), (180, 1), (181, 1), (184, 1), (191, 1), (192, 1), (195, 1), (196, 1), (197, 1), (200, 1), (209, 1), (212, 1)]
discards: [0]
Fitting a model of length 268 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 17s - loss: 429.5639 - loglik: -4.1378e+02 - logprior: -1.5784e+01
Epoch 2/2
17/17 - 15s - loss: 390.5643 - loglik: -3.8610e+02 - logprior: -4.4618e+00
Fitted a model with MAP estimate = -385.9428
expansions: [(0, 10)]
discards: [  0 103 104 139 157]
Fitting a model of length 273 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 19s - loss: 397.9471 - loglik: -3.8711e+02 - logprior: -1.0836e+01
Epoch 2/2
17/17 - 17s - loss: 382.2457 - loglik: -3.8256e+02 - logprior: 0.3105
Fitted a model with MAP estimate = -378.7600
expansions: []
discards: [1 2 3 4 5 6 7 8 9]
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 20s - loss: 394.7729 - loglik: -3.8475e+02 - logprior: -1.0020e+01
Epoch 2/10
17/17 - 17s - loss: 380.0199 - loglik: -3.8122e+02 - logprior: 1.2006
Epoch 3/10
17/17 - 16s - loss: 377.5537 - loglik: -3.8039e+02 - logprior: 2.8341
Epoch 4/10
17/17 - 17s - loss: 375.8853 - loglik: -3.7956e+02 - logprior: 3.6792
Epoch 5/10
17/17 - 17s - loss: 374.4508 - loglik: -3.7850e+02 - logprior: 4.0442
Epoch 6/10
17/17 - 17s - loss: 371.9991 - loglik: -3.7628e+02 - logprior: 4.2827
Epoch 7/10
17/17 - 18s - loss: 371.5492 - loglik: -3.7604e+02 - logprior: 4.4871
Epoch 8/10
17/17 - 16s - loss: 370.6617 - loglik: -3.7544e+02 - logprior: 4.7744
Epoch 9/10
17/17 - 17s - loss: 370.2220 - loglik: -3.7530e+02 - logprior: 5.0747
Epoch 10/10
17/17 - 18s - loss: 369.9409 - loglik: -3.7530e+02 - logprior: 5.3600
Fitted a model with MAP estimate = -369.6011
Time for alignment: 390.3570
Computed alignments with likelihoods: ['-369.5617', '-370.2512', '-369.6011']
Best model has likelihood: -369.5617
SP score = 0.9483
Training of 3 independent models on file kunitz.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f349dc6ca60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34af797b50>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 179.3928 - loglik: -1.5938e+02 - logprior: -2.0016e+01
Epoch 2/10
10/10 - 1s - loss: 148.1250 - loglik: -1.4242e+02 - logprior: -5.7067e+00
Epoch 3/10
10/10 - 1s - loss: 130.0423 - loglik: -1.2682e+02 - logprior: -3.2253e+00
Epoch 4/10
10/10 - 1s - loss: 119.1055 - loglik: -1.1647e+02 - logprior: -2.6333e+00
Epoch 5/10
10/10 - 1s - loss: 114.9508 - loglik: -1.1248e+02 - logprior: -2.4720e+00
Epoch 6/10
10/10 - 1s - loss: 112.8113 - loglik: -1.1047e+02 - logprior: -2.3447e+00
Epoch 7/10
10/10 - 1s - loss: 111.8967 - loglik: -1.0967e+02 - logprior: -2.2250e+00
Epoch 8/10
10/10 - 1s - loss: 111.1191 - loglik: -1.0898e+02 - logprior: -2.1385e+00
Epoch 9/10
10/10 - 1s - loss: 110.8517 - loglik: -1.0878e+02 - logprior: -2.0766e+00
Epoch 10/10
10/10 - 1s - loss: 110.6095 - loglik: -1.0856e+02 - logprior: -2.0532e+00
Fitted a model with MAP estimate = -110.4583
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (20, 1), (22, 1), (35, 1), (36, 3)]
discards: [0]
Fitting a model of length 57 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 134.6159 - loglik: -1.1223e+02 - logprior: -2.2389e+01
Epoch 2/2
10/10 - 1s - loss: 115.5799 - loglik: -1.0605e+02 - logprior: -9.5277e+00
Fitted a model with MAP estimate = -112.3039
expansions: [(0, 2)]
discards: [ 0  6 13 15 18]
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 121.6968 - loglik: -1.0393e+02 - logprior: -1.7764e+01
Epoch 2/2
10/10 - 1s - loss: 107.6554 - loglik: -1.0279e+02 - logprior: -4.8615e+00
Fitted a model with MAP estimate = -105.7087
expansions: []
discards: [0]
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 124.9237 - loglik: -1.0471e+02 - logprior: -2.0210e+01
Epoch 2/10
10/10 - 1s - loss: 109.8643 - loglik: -1.0410e+02 - logprior: -5.7689e+00
Epoch 3/10
10/10 - 1s - loss: 106.1974 - loglik: -1.0330e+02 - logprior: -2.8931e+00
Epoch 4/10
10/10 - 1s - loss: 105.0392 - loglik: -1.0296e+02 - logprior: -2.0762e+00
Epoch 5/10
10/10 - 1s - loss: 103.9423 - loglik: -1.0247e+02 - logprior: -1.4729e+00
Epoch 6/10
10/10 - 1s - loss: 103.8275 - loglik: -1.0265e+02 - logprior: -1.1748e+00
Epoch 7/10
10/10 - 1s - loss: 102.9964 - loglik: -1.0194e+02 - logprior: -1.0550e+00
Epoch 8/10
10/10 - 1s - loss: 102.8457 - loglik: -1.0191e+02 - logprior: -9.3392e-01
Epoch 9/10
10/10 - 1s - loss: 102.5399 - loglik: -1.0167e+02 - logprior: -8.6866e-01
Epoch 10/10
10/10 - 1s - loss: 102.6654 - loglik: -1.0182e+02 - logprior: -8.4252e-01
Fitted a model with MAP estimate = -102.3122
Time for alignment: 30.0863
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 179.2982 - loglik: -1.5928e+02 - logprior: -2.0015e+01
Epoch 2/10
10/10 - 1s - loss: 148.0910 - loglik: -1.4239e+02 - logprior: -5.7006e+00
Epoch 3/10
10/10 - 1s - loss: 129.8273 - loglik: -1.2663e+02 - logprior: -3.1984e+00
Epoch 4/10
10/10 - 1s - loss: 119.1740 - loglik: -1.1659e+02 - logprior: -2.5806e+00
Epoch 5/10
10/10 - 1s - loss: 114.6900 - loglik: -1.1226e+02 - logprior: -2.4265e+00
Epoch 6/10
10/10 - 1s - loss: 113.1878 - loglik: -1.1087e+02 - logprior: -2.3164e+00
Epoch 7/10
10/10 - 1s - loss: 111.7869 - loglik: -1.0956e+02 - logprior: -2.2285e+00
Epoch 8/10
10/10 - 1s - loss: 111.3248 - loglik: -1.0918e+02 - logprior: -2.1424e+00
Epoch 9/10
10/10 - 1s - loss: 110.7659 - loglik: -1.0869e+02 - logprior: -2.0780e+00
Epoch 10/10
10/10 - 1s - loss: 110.6282 - loglik: -1.0858e+02 - logprior: -2.0487e+00
Fitted a model with MAP estimate = -110.4593
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (20, 1), (22, 1), (35, 1), (36, 3)]
discards: [0]
Fitting a model of length 57 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 134.5083 - loglik: -1.1212e+02 - logprior: -2.2388e+01
Epoch 2/2
10/10 - 1s - loss: 115.7915 - loglik: -1.0626e+02 - logprior: -9.5288e+00
Fitted a model with MAP estimate = -112.3180
expansions: [(0, 2)]
discards: [ 0  6 13 15 18]
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 121.4946 - loglik: -1.0373e+02 - logprior: -1.7764e+01
Epoch 2/2
10/10 - 1s - loss: 107.6355 - loglik: -1.0277e+02 - logprior: -4.8606e+00
Fitted a model with MAP estimate = -105.7099
expansions: []
discards: [0]
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 124.6378 - loglik: -1.0443e+02 - logprior: -2.0211e+01
Epoch 2/10
10/10 - 1s - loss: 110.0552 - loglik: -1.0429e+02 - logprior: -5.7700e+00
Epoch 3/10
10/10 - 1s - loss: 106.3916 - loglik: -1.0350e+02 - logprior: -2.8894e+00
Epoch 4/10
10/10 - 1s - loss: 104.9726 - loglik: -1.0293e+02 - logprior: -2.0473e+00
Epoch 5/10
10/10 - 1s - loss: 104.0041 - loglik: -1.0256e+02 - logprior: -1.4441e+00
Epoch 6/10
10/10 - 1s - loss: 103.5655 - loglik: -1.0238e+02 - logprior: -1.1814e+00
Epoch 7/10
10/10 - 0s - loss: 103.2457 - loglik: -1.0217e+02 - logprior: -1.0760e+00
Epoch 8/10
10/10 - 1s - loss: 102.7734 - loglik: -1.0184e+02 - logprior: -9.3226e-01
Epoch 9/10
10/10 - 1s - loss: 102.8395 - loglik: -1.0197e+02 - logprior: -8.7345e-01
Fitted a model with MAP estimate = -102.5159
Time for alignment: 28.8550
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 179.3190 - loglik: -1.5930e+02 - logprior: -2.0016e+01
Epoch 2/10
10/10 - 1s - loss: 148.0739 - loglik: -1.4236e+02 - logprior: -5.7120e+00
Epoch 3/10
10/10 - 1s - loss: 130.2324 - loglik: -1.2694e+02 - logprior: -3.2915e+00
Epoch 4/10
10/10 - 1s - loss: 118.0519 - loglik: -1.1527e+02 - logprior: -2.7793e+00
Epoch 5/10
10/10 - 1s - loss: 112.8235 - loglik: -1.1011e+02 - logprior: -2.7095e+00
Epoch 6/10
10/10 - 1s - loss: 110.8934 - loglik: -1.0823e+02 - logprior: -2.6643e+00
Epoch 7/10
10/10 - 1s - loss: 109.7793 - loglik: -1.0724e+02 - logprior: -2.5345e+00
Epoch 8/10
10/10 - 1s - loss: 109.3165 - loglik: -1.0693e+02 - logprior: -2.3913e+00
Epoch 9/10
10/10 - 1s - loss: 108.8663 - loglik: -1.0657e+02 - logprior: -2.3008e+00
Epoch 10/10
10/10 - 1s - loss: 108.7539 - loglik: -1.0648e+02 - logprior: -2.2730e+00
Fitted a model with MAP estimate = -108.5745
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (20, 1), (22, 2), (23, 1), (34, 1), (35, 1), (36, 1)]
discards: [0]
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 135.2983 - loglik: -1.1288e+02 - logprior: -2.2420e+01
Epoch 2/2
10/10 - 1s - loss: 115.8425 - loglik: -1.0623e+02 - logprior: -9.6090e+00
Fitted a model with MAP estimate = -112.4514
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 32]
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 121.6615 - loglik: -1.0389e+02 - logprior: -1.7772e+01
Epoch 2/2
10/10 - 1s - loss: 107.5529 - loglik: -1.0270e+02 - logprior: -4.8540e+00
Fitted a model with MAP estimate = -105.7183
expansions: []
discards: [0]
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 124.8531 - loglik: -1.0463e+02 - logprior: -2.0220e+01
Epoch 2/10
10/10 - 1s - loss: 109.9633 - loglik: -1.0419e+02 - logprior: -5.7709e+00
Epoch 3/10
10/10 - 1s - loss: 106.2357 - loglik: -1.0334e+02 - logprior: -2.8916e+00
Epoch 4/10
10/10 - 1s - loss: 104.9142 - loglik: -1.0284e+02 - logprior: -2.0708e+00
Epoch 5/10
10/10 - 1s - loss: 104.0590 - loglik: -1.0259e+02 - logprior: -1.4730e+00
Epoch 6/10
10/10 - 1s - loss: 103.6090 - loglik: -1.0244e+02 - logprior: -1.1702e+00
Epoch 7/10
10/10 - 1s - loss: 103.1577 - loglik: -1.0211e+02 - logprior: -1.0494e+00
Epoch 8/10
10/10 - 1s - loss: 102.9026 - loglik: -1.0197e+02 - logprior: -9.3021e-01
Epoch 9/10
10/10 - 1s - loss: 102.6739 - loglik: -1.0181e+02 - logprior: -8.6134e-01
Epoch 10/10
10/10 - 1s - loss: 102.2402 - loglik: -1.0140e+02 - logprior: -8.3945e-01
Fitted a model with MAP estimate = -102.3019
Time for alignment: 29.6146
Computed alignments with likelihoods: ['-102.3122', '-102.5159', '-102.3019']
Best model has likelihood: -102.3019
SP score = 0.9786
Training of 3 independent models on file rhv.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3698df4070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f36a9dbf1c0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 467.5901 - loglik: -4.6579e+02 - logprior: -1.8017e+00
Epoch 2/10
39/39 - 10s - loss: 387.3817 - loglik: -3.8628e+02 - logprior: -1.0996e+00
Epoch 3/10
39/39 - 9s - loss: 379.6882 - loglik: -3.7864e+02 - logprior: -1.0521e+00
Epoch 4/10
39/39 - 10s - loss: 378.1647 - loglik: -3.7714e+02 - logprior: -1.0209e+00
Epoch 5/10
39/39 - 10s - loss: 376.4561 - loglik: -3.7544e+02 - logprior: -1.0205e+00
Epoch 6/10
39/39 - 11s - loss: 375.4572 - loglik: -3.7442e+02 - logprior: -1.0363e+00
Epoch 7/10
39/39 - 11s - loss: 375.4437 - loglik: -3.7441e+02 - logprior: -1.0310e+00
Epoch 8/10
39/39 - 10s - loss: 374.6273 - loglik: -3.7359e+02 - logprior: -1.0330e+00
Epoch 9/10
39/39 - 11s - loss: 374.9777 - loglik: -3.7394e+02 - logprior: -1.0339e+00
Fitted a model with MAP estimate = -308.6677
expansions: [(0, 12), (10, 1), (15, 1), (18, 1), (28, 1), (29, 1), (33, 4), (35, 1), (38, 1), (43, 2), (44, 1), (45, 1), (56, 1), (71, 2), (88, 1), (89, 2), (90, 1), (102, 2), (108, 1), (125, 7), (126, 2), (130, 4)]
discards: []
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 361.4682 - loglik: -3.5882e+02 - logprior: -2.6440e+00
Epoch 2/2
39/39 - 17s - loss: 344.2021 - loglik: -3.4291e+02 - logprior: -1.2902e+00
Fitted a model with MAP estimate = -284.7288
expansions: [(55, 2), (149, 1), (166, 2), (167, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  66  99 123]
Fitting a model of length 178 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 350.2490 - loglik: -3.4798e+02 - logprior: -2.2738e+00
Epoch 2/2
39/39 - 15s - loss: 343.6309 - loglik: -3.4293e+02 - logprior: -6.9712e-01
Fitted a model with MAP estimate = -285.8388
expansions: [(0, 18), (114, 1)]
discards: [ 46 159 172]
Fitting a model of length 194 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 22s - loss: 282.3883 - loglik: -2.8091e+02 - logprior: -1.4826e+00
Epoch 2/10
52/52 - 20s - loss: 277.6551 - loglik: -2.7655e+02 - logprior: -1.1063e+00
Epoch 3/10
52/52 - 23s - loss: 275.6399 - loglik: -2.7455e+02 - logprior: -1.0906e+00
Epoch 4/10
52/52 - 21s - loss: 274.7640 - loglik: -2.7373e+02 - logprior: -1.0373e+00
Epoch 5/10
52/52 - 20s - loss: 274.4407 - loglik: -2.7348e+02 - logprior: -9.5773e-01
Epoch 6/10
52/52 - 20s - loss: 272.9800 - loglik: -2.7208e+02 - logprior: -9.0482e-01
Epoch 7/10
52/52 - 25s - loss: 273.1574 - loglik: -2.7230e+02 - logprior: -8.5362e-01
Fitted a model with MAP estimate = -272.3885
Time for alignment: 394.7309
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 467.4319 - loglik: -4.6565e+02 - logprior: -1.7798e+00
Epoch 2/10
39/39 - 10s - loss: 387.6772 - loglik: -3.8659e+02 - logprior: -1.0823e+00
Epoch 3/10
39/39 - 11s - loss: 378.8483 - loglik: -3.7774e+02 - logprior: -1.1113e+00
Epoch 4/10
39/39 - 11s - loss: 377.0167 - loglik: -3.7593e+02 - logprior: -1.0850e+00
Epoch 5/10
39/39 - 11s - loss: 376.0521 - loglik: -3.7497e+02 - logprior: -1.0825e+00
Epoch 6/10
39/39 - 11s - loss: 375.4126 - loglik: -3.7433e+02 - logprior: -1.0863e+00
Epoch 7/10
39/39 - 10s - loss: 375.1988 - loglik: -3.7412e+02 - logprior: -1.0780e+00
Epoch 8/10
39/39 - 11s - loss: 374.6166 - loglik: -3.7354e+02 - logprior: -1.0730e+00
Epoch 9/10
39/39 - 11s - loss: 374.3664 - loglik: -3.7329e+02 - logprior: -1.0754e+00
Epoch 10/10
39/39 - 10s - loss: 374.4786 - loglik: -3.7340e+02 - logprior: -1.0793e+00
Fitted a model with MAP estimate = -309.5897
expansions: [(0, 10), (10, 1), (11, 1), (18, 1), (28, 2), (29, 2), (33, 2), (34, 2), (37, 1), (42, 1), (44, 3), (71, 2), (88, 1), (89, 1), (90, 1), (91, 1), (103, 1), (107, 1), (108, 1), (112, 1), (126, 1), (130, 5)]
discards: []
Fitting a model of length 176 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 363.2497 - loglik: -3.6057e+02 - logprior: -2.6759e+00
Epoch 2/2
39/39 - 13s - loss: 348.6947 - loglik: -3.4742e+02 - logprior: -1.2711e+00
Fitted a model with MAP estimate = -289.5973
expansions: []
discards: [  1   2   3   4   5   6   7   8  44  45  68 147 167]
Fitting a model of length 163 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 356.9082 - loglik: -3.5500e+02 - logprior: -1.9047e+00
Epoch 2/2
39/39 - 12s - loss: 352.4292 - loglik: -3.5170e+02 - logprior: -7.2903e-01
Fitted a model with MAP estimate = -293.9158
expansions: []
discards: [87]
Fitting a model of length 162 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 18s - loss: 292.1728 - loglik: -2.9117e+02 - logprior: -1.0077e+00
Epoch 2/10
52/52 - 14s - loss: 289.1671 - loglik: -2.8855e+02 - logprior: -6.2152e-01
Epoch 3/10
52/52 - 16s - loss: 286.2115 - loglik: -2.8563e+02 - logprior: -5.8266e-01
Epoch 4/10
52/52 - 15s - loss: 284.2090 - loglik: -2.8367e+02 - logprior: -5.3654e-01
Epoch 5/10
52/52 - 16s - loss: 283.8651 - loglik: -2.8338e+02 - logprior: -4.8689e-01
Epoch 6/10
52/52 - 16s - loss: 283.6591 - loglik: -2.8323e+02 - logprior: -4.3230e-01
Epoch 7/10
52/52 - 15s - loss: 283.3965 - loglik: -2.8302e+02 - logprior: -3.7696e-01
Epoch 8/10
52/52 - 14s - loss: 281.6501 - loglik: -2.8121e+02 - logprior: -4.4506e-01
Epoch 9/10
52/52 - 14s - loss: 282.5941 - loglik: -2.8219e+02 - logprior: -4.0395e-01
Fitted a model with MAP estimate = -281.5362
Time for alignment: 371.3376
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 468.0814 - loglik: -4.6633e+02 - logprior: -1.7546e+00
Epoch 2/10
39/39 - 9s - loss: 389.0262 - loglik: -3.8808e+02 - logprior: -9.4269e-01
Epoch 3/10
39/39 - 9s - loss: 381.8275 - loglik: -3.8094e+02 - logprior: -8.8254e-01
Epoch 4/10
39/39 - 9s - loss: 379.8792 - loglik: -3.7902e+02 - logprior: -8.6239e-01
Epoch 5/10
39/39 - 9s - loss: 378.9058 - loglik: -3.7805e+02 - logprior: -8.5648e-01
Epoch 6/10
39/39 - 9s - loss: 378.2420 - loglik: -3.7738e+02 - logprior: -8.6558e-01
Epoch 7/10
39/39 - 9s - loss: 377.7570 - loglik: -3.7689e+02 - logprior: -8.6937e-01
Epoch 8/10
39/39 - 9s - loss: 377.2348 - loglik: -3.7636e+02 - logprior: -8.7652e-01
Epoch 9/10
39/39 - 10s - loss: 376.9303 - loglik: -3.7604e+02 - logprior: -8.8577e-01
Epoch 10/10
39/39 - 9s - loss: 376.7547 - loglik: -3.7587e+02 - logprior: -8.8838e-01
Fitted a model with MAP estimate = -310.5167
expansions: [(0, 12), (10, 1), (15, 1), (18, 1), (28, 1), (29, 1), (32, 1), (34, 3), (37, 1), (42, 1), (44, 2), (56, 1), (71, 1), (87, 1), (88, 1), (89, 2), (90, 2), (102, 1), (106, 1), (107, 1), (125, 2), (126, 5), (129, 5), (134, 4)]
discards: []
Fitting a model of length 186 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 365.8497 - loglik: -3.6301e+02 - logprior: -2.8409e+00
Epoch 2/2
39/39 - 15s - loss: 346.3918 - loglik: -3.4493e+02 - logprior: -1.4600e+00
Fitted a model with MAP estimate = -286.7849
expansions: []
discards: [  1   2   3   4   5   6   7   8   9  10  53  54  67 162 164 165 166 167
 168 174 182 183 184 185]
Fitting a model of length 162 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 360.0910 - loglik: -3.5828e+02 - logprior: -1.8110e+00
Epoch 2/2
39/39 - 12s - loss: 355.0620 - loglik: -3.5458e+02 - logprior: -4.8541e-01
Fitted a model with MAP estimate = -294.8505
expansions: [(0, 15), (151, 1), (162, 9)]
discards: []
Fitting a model of length 187 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 18s - loss: 289.0462 - loglik: -2.8753e+02 - logprior: -1.5113e+00
Epoch 2/10
52/52 - 18s - loss: 283.8181 - loglik: -2.8268e+02 - logprior: -1.1392e+00
Epoch 3/10
52/52 - 14s - loss: 280.8699 - loglik: -2.7998e+02 - logprior: -8.9340e-01
Epoch 4/10
52/52 - 17s - loss: 280.1473 - loglik: -2.7937e+02 - logprior: -7.8165e-01
Epoch 5/10
52/52 - 15s - loss: 278.0027 - loglik: -2.7727e+02 - logprior: -7.3126e-01
Epoch 6/10
52/52 - 18s - loss: 276.9555 - loglik: -2.7628e+02 - logprior: -6.7168e-01
Epoch 7/10
52/52 - 17s - loss: 278.4456 - loglik: -2.7785e+02 - logprior: -5.9769e-01
Fitted a model with MAP estimate = -276.7559
Time for alignment: 342.5200
Computed alignments with likelihoods: ['-272.3885', '-281.5362', '-276.7559']
Best model has likelihood: -272.3885
SP score = 0.1856
Training of 3 independent models on file blm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b7f3a760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f36a98a13d0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 34s - loss: 815.2098 - loglik: -8.1370e+02 - logprior: -1.5098e+00
Epoch 2/10
37/37 - 36s - loss: 724.5908 - loglik: -7.2407e+02 - logprior: -5.2565e-01
Epoch 3/10
37/37 - 38s - loss: 708.8756 - loglik: -7.0834e+02 - logprior: -5.3090e-01
Epoch 4/10
37/37 - 39s - loss: 704.2971 - loglik: -7.0389e+02 - logprior: -4.0959e-01
Epoch 5/10
37/37 - 38s - loss: 699.6569 - loglik: -6.9926e+02 - logprior: -4.0028e-01
Epoch 6/10
37/37 - 38s - loss: 695.6265 - loglik: -6.9507e+02 - logprior: -5.6112e-01
Epoch 7/10
37/37 - 39s - loss: 697.7036 - loglik: -6.9729e+02 - logprior: -4.1699e-01
Fitted a model with MAP estimate = -695.9011
expansions: [(0, 4), (30, 1), (32, 2), (34, 3), (61, 2), (66, 1), (92, 13), (141, 1), (156, 5), (189, 1), (240, 1)]
discards: [103 105 115 152 153 154]
Fitting a model of length 282 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 48s - loss: 742.4897 - loglik: -7.4009e+02 - logprior: -2.3958e+00
Epoch 2/2
37/37 - 45s - loss: 709.1642 - loglik: -7.0868e+02 - logprior: -4.8409e-01
Fitted a model with MAP estimate = -703.2437
expansions: [(0, 2), (39, 1), (40, 1), (119, 2), (164, 2), (178, 11)]
discards: [  0  72 165 166 167 168 175 179 180 181 182 183 184 185 186 281]
Fitting a model of length 285 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 46s - loss: 716.8782 - loglik: -7.1519e+02 - logprior: -1.6872e+00
Epoch 2/2
37/37 - 39s - loss: 703.0657 - loglik: -7.0296e+02 - logprior: -1.0556e-01
Fitted a model with MAP estimate = -699.3472
expansions: [(0, 2), (171, 1), (285, 3)]
discards: [1]
Fitting a model of length 290 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 39s - loss: 711.9639 - loglik: -7.1010e+02 - logprior: -1.8632e+00
Epoch 2/10
37/37 - 35s - loss: 702.8085 - loglik: -7.0283e+02 - logprior: 0.0189
Epoch 3/10
37/37 - 36s - loss: 695.4010 - loglik: -6.9558e+02 - logprior: 0.1767
Epoch 4/10
37/37 - 37s - loss: 689.9298 - loglik: -6.9017e+02 - logprior: 0.2396
Epoch 5/10
37/37 - 36s - loss: 686.8262 - loglik: -6.8683e+02 - logprior: 0.0017
Epoch 6/10
37/37 - 36s - loss: 682.2765 - loglik: -6.8210e+02 - logprior: -1.8087e-01
Epoch 7/10
37/37 - 34s - loss: 678.8561 - loglik: -6.7908e+02 - logprior: 0.2252
Epoch 8/10
37/37 - 31s - loss: 680.8228 - loglik: -6.8129e+02 - logprior: 0.4685
Fitted a model with MAP estimate = -679.0414
Time for alignment: 905.0753
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 27s - loss: 813.7064 - loglik: -8.1219e+02 - logprior: -1.5134e+00
Epoch 2/10
37/37 - 24s - loss: 728.4238 - loglik: -7.2802e+02 - logprior: -3.9967e-01
Epoch 3/10
37/37 - 26s - loss: 715.0623 - loglik: -7.1463e+02 - logprior: -4.3443e-01
Epoch 4/10
37/37 - 27s - loss: 709.9293 - loglik: -7.0956e+02 - logprior: -3.7154e-01
Epoch 5/10
37/37 - 29s - loss: 704.7761 - loglik: -7.0415e+02 - logprior: -6.2197e-01
Epoch 6/10
37/37 - 31s - loss: 703.9166 - loglik: -7.0343e+02 - logprior: -4.8916e-01
Epoch 7/10
37/37 - 32s - loss: 702.6338 - loglik: -7.0219e+02 - logprior: -4.3944e-01
Epoch 8/10
37/37 - 31s - loss: 703.3765 - loglik: -7.0286e+02 - logprior: -5.1777e-01
Fitted a model with MAP estimate = -701.9396
expansions: [(0, 4), (26, 1), (30, 1), (32, 4), (33, 2), (35, 1), (65, 1), (72, 1), (101, 1), (116, 3), (137, 6), (197, 14), (240, 1), (242, 1)]
discards: [102 103 104 105 106 107 151 153 154 155 156 188 189]
Fitting a model of length 282 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 41s - loss: 747.2772 - loglik: -7.4465e+02 - logprior: -2.6305e+00
Epoch 2/2
37/37 - 36s - loss: 709.7087 - loglik: -7.0892e+02 - logprior: -7.8891e-01
Fitted a model with MAP estimate = -704.5102
expansions: [(0, 2), (40, 1), (45, 1), (117, 7), (118, 3), (154, 1), (171, 4), (172, 2), (200, 1), (221, 1), (236, 3)]
discards: [  0 105 111 112 114 197 223 224]
Fitting a model of length 300 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 44s - loss: 714.1763 - loglik: -7.1234e+02 - logprior: -1.8371e+00
Epoch 2/2
37/37 - 40s - loss: 701.5719 - loglik: -7.0123e+02 - logprior: -3.3852e-01
Fitted a model with MAP estimate = -696.9784
expansions: [(0, 2), (113, 2), (121, 1), (213, 1), (236, 4)]
discards: [  1 125 183 186]
Fitting a model of length 306 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 51s - loss: 709.5931 - loglik: -7.0750e+02 - logprior: -2.0896e+00
Epoch 2/10
37/37 - 50s - loss: 698.2962 - loglik: -6.9811e+02 - logprior: -1.8304e-01
Epoch 3/10
37/37 - 51s - loss: 691.7366 - loglik: -6.9180e+02 - logprior: 0.0614
Epoch 4/10
37/37 - 50s - loss: 688.4195 - loglik: -6.8862e+02 - logprior: 0.2052
Epoch 5/10
37/37 - 50s - loss: 679.6758 - loglik: -6.7996e+02 - logprior: 0.2815
Epoch 6/10
37/37 - 50s - loss: 677.0168 - loglik: -6.7726e+02 - logprior: 0.2426
Epoch 7/10
37/37 - 51s - loss: 675.8256 - loglik: -6.7623e+02 - logprior: 0.4042
Epoch 8/10
37/37 - 49s - loss: 674.9607 - loglik: -6.7549e+02 - logprior: 0.5264
Epoch 9/10
37/37 - 44s - loss: 674.2581 - loglik: -6.7483e+02 - logprior: 0.5684
Epoch 10/10
37/37 - 46s - loss: 672.7424 - loglik: -6.7346e+02 - logprior: 0.7169
Fitted a model with MAP estimate = -672.9406
Time for alignment: 1048.6691
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 33s - loss: 815.3491 - loglik: -8.1381e+02 - logprior: -1.5360e+00
Epoch 2/10
37/37 - 30s - loss: 727.6045 - loglik: -7.2708e+02 - logprior: -5.2262e-01
Epoch 3/10
37/37 - 32s - loss: 711.0947 - loglik: -7.1068e+02 - logprior: -4.1371e-01
Epoch 4/10
37/37 - 33s - loss: 705.6147 - loglik: -7.0522e+02 - logprior: -3.9122e-01
Epoch 5/10
37/37 - 31s - loss: 700.0428 - loglik: -6.9959e+02 - logprior: -4.5636e-01
Epoch 6/10
37/37 - 31s - loss: 698.5437 - loglik: -6.9797e+02 - logprior: -5.7294e-01
Epoch 7/10
37/37 - 33s - loss: 697.5331 - loglik: -6.9704e+02 - logprior: -4.8876e-01
Epoch 8/10
37/37 - 35s - loss: 698.7050 - loglik: -6.9813e+02 - logprior: -5.7379e-01
Fitted a model with MAP estimate = -696.7991
expansions: [(0, 4), (26, 1), (30, 1), (32, 2), (34, 3), (38, 1), (66, 1), (89, 14), (100, 1), (138, 1), (211, 2), (239, 1)]
discards: [102 104 115 155]
Fitting a model of length 282 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 46s - loss: 746.7283 - loglik: -7.4403e+02 - logprior: -2.7025e+00
Epoch 2/2
37/37 - 46s - loss: 709.8774 - loglik: -7.0933e+02 - logprior: -5.4768e-01
Fitted a model with MAP estimate = -704.1263
expansions: [(0, 2), (39, 1), (41, 1), (87, 4), (107, 3), (108, 4), (223, 8)]
discards: [  0 238 281]
Fitting a model of length 302 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 50s - loss: 713.5810 - loglik: -7.1174e+02 - logprior: -1.8410e+00
Epoch 2/2
37/37 - 48s - loss: 701.3744 - loglik: -7.0118e+02 - logprior: -1.9233e-01
Fitted a model with MAP estimate = -695.7454
expansions: [(0, 2), (243, 1), (245, 4), (246, 2), (253, 2), (302, 3)]
discards: [  1 116 117 228 229]
Fitting a model of length 311 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 47s - loss: 709.5682 - loglik: -7.0753e+02 - logprior: -2.0347e+00
Epoch 2/10
37/37 - 47s - loss: 699.5870 - loglik: -6.9937e+02 - logprior: -2.1500e-01
Epoch 3/10
37/37 - 43s - loss: 692.3948 - loglik: -6.9242e+02 - logprior: 0.0234
Epoch 4/10
37/37 - 45s - loss: 685.7222 - loglik: -6.8584e+02 - logprior: 0.1205
Epoch 5/10
37/37 - 45s - loss: 680.3790 - loglik: -6.8045e+02 - logprior: 0.0751
Epoch 6/10
37/37 - 43s - loss: 675.8098 - loglik: -6.7578e+02 - logprior: -3.0565e-02
Epoch 7/10
37/37 - 47s - loss: 674.3373 - loglik: -6.7459e+02 - logprior: 0.2500
Epoch 8/10
37/37 - 52s - loss: 674.3663 - loglik: -6.7477e+02 - logprior: 0.4050
Fitted a model with MAP estimate = -672.9383
Time for alignment: 1011.3994
Computed alignments with likelihoods: ['-679.0414', '-672.9406', '-672.9383']
Best model has likelihood: -672.9383
SP score = 0.8756
Training of 3 independent models on file cyt3.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f350c74bb80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32aac80100>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 385.8952 - loglik: -2.7275e+02 - logprior: -1.1314e+02
Epoch 2/10
10/10 - 1s - loss: 288.6630 - loglik: -2.6143e+02 - logprior: -2.7238e+01
Epoch 3/10
10/10 - 1s - loss: 258.8107 - loglik: -2.4914e+02 - logprior: -9.6714e+00
Epoch 4/10
10/10 - 1s - loss: 242.3049 - loglik: -2.3901e+02 - logprior: -3.2916e+00
Epoch 5/10
10/10 - 1s - loss: 232.7110 - loglik: -2.3249e+02 - logprior: -2.2248e-01
Epoch 6/10
10/10 - 1s - loss: 227.0876 - loglik: -2.2860e+02 - logprior: 1.5161
Epoch 7/10
10/10 - 1s - loss: 224.0814 - loglik: -2.2659e+02 - logprior: 2.5078
Epoch 8/10
10/10 - 1s - loss: 222.2570 - loglik: -2.2550e+02 - logprior: 3.2455
Epoch 9/10
10/10 - 1s - loss: 221.0043 - loglik: -2.2476e+02 - logprior: 3.7585
Epoch 10/10
10/10 - 1s - loss: 220.0927 - loglik: -2.2425e+02 - logprior: 4.1613
Fitted a model with MAP estimate = -219.6807
expansions: [(0, 5), (37, 6), (50, 4), (51, 1)]
discards: []
Fitting a model of length 92 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 387.9966 - loglik: -2.3894e+02 - logprior: -1.4906e+02
Epoch 2/2
10/10 - 1s - loss: 269.6606 - loglik: -2.2776e+02 - logprior: -4.1896e+01
Fitted a model with MAP estimate = -247.5831
expansions: [(0, 4), (56, 3)]
discards: [0 1 2 3 4]
Fitting a model of length 94 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 345.2408 - loglik: -2.2751e+02 - logprior: -1.1773e+02
Epoch 2/2
10/10 - 1s - loss: 254.4945 - loglik: -2.2475e+02 - logprior: -2.9745e+01
Fitted a model with MAP estimate = -237.9890
expansions: [(0, 4), (42, 3)]
discards: []
Fitting a model of length 101 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 349.7246 - loglik: -2.2451e+02 - logprior: -1.2521e+02
Epoch 2/10
10/10 - 1s - loss: 257.7939 - loglik: -2.2276e+02 - logprior: -3.5033e+01
Epoch 3/10
10/10 - 1s - loss: 232.0240 - loglik: -2.2190e+02 - logprior: -1.0121e+01
Epoch 4/10
10/10 - 1s - loss: 220.5842 - loglik: -2.2137e+02 - logprior: 0.7889
Epoch 5/10
10/10 - 1s - loss: 215.2105 - loglik: -2.2083e+02 - logprior: 5.6174
Epoch 6/10
10/10 - 1s - loss: 212.0641 - loglik: -2.2021e+02 - logprior: 8.1509
Epoch 7/10
10/10 - 1s - loss: 209.9703 - loglik: -2.1966e+02 - logprior: 9.6909
Epoch 8/10
10/10 - 1s - loss: 208.3333 - loglik: -2.1904e+02 - logprior: 10.7093
Epoch 9/10
10/10 - 1s - loss: 206.9878 - loglik: -2.1848e+02 - logprior: 11.4963
Epoch 10/10
10/10 - 1s - loss: 205.8207 - loglik: -2.1797e+02 - logprior: 12.1470
Fitted a model with MAP estimate = -205.2221
Time for alignment: 39.9408
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 385.8952 - loglik: -2.7275e+02 - logprior: -1.1314e+02
Epoch 2/10
10/10 - 1s - loss: 288.6630 - loglik: -2.6143e+02 - logprior: -2.7238e+01
Epoch 3/10
10/10 - 1s - loss: 258.8107 - loglik: -2.4914e+02 - logprior: -9.6714e+00
Epoch 4/10
10/10 - 1s - loss: 242.3048 - loglik: -2.3901e+02 - logprior: -3.2916e+00
Epoch 5/10
10/10 - 1s - loss: 232.7110 - loglik: -2.3249e+02 - logprior: -2.2248e-01
Epoch 6/10
10/10 - 1s - loss: 227.0876 - loglik: -2.2860e+02 - logprior: 1.5161
Epoch 7/10
10/10 - 1s - loss: 224.0814 - loglik: -2.2659e+02 - logprior: 2.5078
Epoch 8/10
10/10 - 1s - loss: 222.2570 - loglik: -2.2550e+02 - logprior: 3.2455
Epoch 9/10
10/10 - 1s - loss: 221.0045 - loglik: -2.2476e+02 - logprior: 3.7585
Epoch 10/10
10/10 - 1s - loss: 220.0926 - loglik: -2.2425e+02 - logprior: 4.1613
Fitted a model with MAP estimate = -219.6804
expansions: [(0, 5), (37, 6), (50, 4), (51, 1)]
discards: []
Fitting a model of length 92 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 387.9960 - loglik: -2.3894e+02 - logprior: -1.4906e+02
Epoch 2/2
10/10 - 1s - loss: 269.6605 - loglik: -2.2776e+02 - logprior: -4.1896e+01
Fitted a model with MAP estimate = -247.5831
expansions: [(0, 4), (56, 3)]
discards: [0 1 2 3 4]
Fitting a model of length 94 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 345.2408 - loglik: -2.2751e+02 - logprior: -1.1773e+02
Epoch 2/2
10/10 - 1s - loss: 254.4944 - loglik: -2.2475e+02 - logprior: -2.9745e+01
Fitted a model with MAP estimate = -237.9891
expansions: [(0, 4), (42, 3)]
discards: []
Fitting a model of length 101 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 349.7246 - loglik: -2.2451e+02 - logprior: -1.2521e+02
Epoch 2/10
10/10 - 1s - loss: 257.7938 - loglik: -2.2276e+02 - logprior: -3.5033e+01
Epoch 3/10
10/10 - 1s - loss: 232.0246 - loglik: -2.2190e+02 - logprior: -1.0121e+01
Epoch 4/10
10/10 - 1s - loss: 220.5844 - loglik: -2.2137e+02 - logprior: 0.7889
Epoch 5/10
10/10 - 1s - loss: 215.2105 - loglik: -2.2083e+02 - logprior: 5.6178
Epoch 6/10
10/10 - 1s - loss: 212.0643 - loglik: -2.2022e+02 - logprior: 8.1510
Epoch 7/10
10/10 - 1s - loss: 209.9700 - loglik: -2.1966e+02 - logprior: 9.6912
Epoch 8/10
10/10 - 1s - loss: 208.3333 - loglik: -2.1904e+02 - logprior: 10.7095
Epoch 9/10
10/10 - 1s - loss: 206.9877 - loglik: -2.1848e+02 - logprior: 11.4967
Epoch 10/10
10/10 - 1s - loss: 205.8211 - loglik: -2.1797e+02 - logprior: 12.1478
Fitted a model with MAP estimate = -205.2227
Time for alignment: 39.3320
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 385.8952 - loglik: -2.7275e+02 - logprior: -1.1314e+02
Epoch 2/10
10/10 - 1s - loss: 288.6630 - loglik: -2.6143e+02 - logprior: -2.7238e+01
Epoch 3/10
10/10 - 1s - loss: 258.8107 - loglik: -2.4914e+02 - logprior: -9.6714e+00
Epoch 4/10
10/10 - 1s - loss: 242.3048 - loglik: -2.3901e+02 - logprior: -3.2916e+00
Epoch 5/10
10/10 - 1s - loss: 232.7111 - loglik: -2.3249e+02 - logprior: -2.2247e-01
Epoch 6/10
10/10 - 1s - loss: 227.0876 - loglik: -2.2860e+02 - logprior: 1.5161
Epoch 7/10
10/10 - 1s - loss: 224.0813 - loglik: -2.2659e+02 - logprior: 2.5078
Epoch 8/10
10/10 - 1s - loss: 222.2570 - loglik: -2.2550e+02 - logprior: 3.2455
Epoch 9/10
10/10 - 1s - loss: 221.0041 - loglik: -2.2476e+02 - logprior: 3.7584
Epoch 10/10
10/10 - 1s - loss: 220.0928 - loglik: -2.2425e+02 - logprior: 4.1613
Fitted a model with MAP estimate = -219.6808
expansions: [(0, 5), (37, 6), (50, 4), (51, 1)]
discards: []
Fitting a model of length 92 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 387.9961 - loglik: -2.3894e+02 - logprior: -1.4906e+02
Epoch 2/2
10/10 - 1s - loss: 269.6606 - loglik: -2.2776e+02 - logprior: -4.1896e+01
Fitted a model with MAP estimate = -247.5832
expansions: [(0, 4), (56, 3)]
discards: [0 1 2 3 4]
Fitting a model of length 94 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 345.2407 - loglik: -2.2751e+02 - logprior: -1.1773e+02
Epoch 2/2
10/10 - 1s - loss: 254.4944 - loglik: -2.2475e+02 - logprior: -2.9745e+01
Fitted a model with MAP estimate = -237.9890
expansions: [(0, 4), (42, 3)]
discards: []
Fitting a model of length 101 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 349.7246 - loglik: -2.2451e+02 - logprior: -1.2521e+02
Epoch 2/10
10/10 - 1s - loss: 257.7938 - loglik: -2.2276e+02 - logprior: -3.5033e+01
Epoch 3/10
10/10 - 1s - loss: 232.0248 - loglik: -2.2190e+02 - logprior: -1.0121e+01
Epoch 4/10
10/10 - 1s - loss: 220.5845 - loglik: -2.2137e+02 - logprior: 0.7887
Epoch 5/10
10/10 - 1s - loss: 215.2105 - loglik: -2.2083e+02 - logprior: 5.6176
Epoch 6/10
10/10 - 1s - loss: 212.0640 - loglik: -2.2021e+02 - logprior: 8.1508
Epoch 7/10
10/10 - 1s - loss: 209.9703 - loglik: -2.1966e+02 - logprior: 9.6909
Epoch 8/10
10/10 - 1s - loss: 208.3340 - loglik: -2.1904e+02 - logprior: 10.7091
Epoch 9/10
10/10 - 1s - loss: 206.9884 - loglik: -2.1848e+02 - logprior: 11.4962
Epoch 10/10
10/10 - 1s - loss: 205.8211 - loglik: -2.1797e+02 - logprior: 12.1473
Fitted a model with MAP estimate = -205.2222
Time for alignment: 40.6849
Computed alignments with likelihoods: ['-205.2221', '-205.2227', '-205.2222']
Best model has likelihood: -205.2221
SP score = 0.7433
Training of 3 independent models on file mofe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34f446ee50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32a9761c70>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 29s - loss: 995.7502 - loglik: -9.8824e+02 - logprior: -7.5054e+00
Epoch 2/10
19/19 - 27s - loss: 872.0939 - loglik: -8.7341e+02 - logprior: 1.3127
Epoch 3/10
19/19 - 26s - loss: 814.4497 - loglik: -8.1501e+02 - logprior: 0.5561
Epoch 4/10
19/19 - 26s - loss: 793.0228 - loglik: -7.9334e+02 - logprior: 0.3221
Epoch 5/10
19/19 - 27s - loss: 786.9438 - loglik: -7.8726e+02 - logprior: 0.3141
Epoch 6/10
19/19 - 26s - loss: 787.5150 - loglik: -7.8777e+02 - logprior: 0.2586
Fitted a model with MAP estimate = -782.9795
expansions: [(28, 1), (83, 1), (110, 1), (115, 1), (116, 2), (118, 1), (119, 1), (120, 3), (122, 8), (124, 3), (142, 1), (144, 1), (164, 2), (166, 1), (167, 1), (168, 2), (169, 1), (172, 1), (175, 1), (176, 1), (178, 1), (189, 3), (204, 1), (211, 2), (221, 1), (222, 1), (223, 1), (224, 1), (236, 3), (265, 3), (266, 1), (301, 1), (302, 3), (303, 1), (309, 1), (310, 3)]
discards: [0]
Fitting a model of length 380 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 37s - loss: 812.8889 - loglik: -8.0211e+02 - logprior: -1.0778e+01
Epoch 2/2
19/19 - 34s - loss: 766.5578 - loglik: -7.6435e+02 - logprior: -2.2070e+00
Fitted a model with MAP estimate = -760.4197
expansions: [(0, 2), (223, 1), (281, 1), (282, 1)]
discards: [  0 119 136 144 145 195 290 353]
Fitting a model of length 377 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 38s - loss: 775.1361 - loglik: -7.6900e+02 - logprior: -6.1393e+00
Epoch 2/2
19/19 - 34s - loss: 757.6422 - loglik: -7.5996e+02 - logprior: 2.3128
Fitted a model with MAP estimate = -752.3006
expansions: [(142, 2)]
discards: [  0 310]
Fitting a model of length 377 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 39s - loss: 775.3271 - loglik: -7.6598e+02 - logprior: -9.3501e+00
Epoch 2/10
19/19 - 35s - loss: 758.2184 - loglik: -7.5906e+02 - logprior: 0.8366
Epoch 3/10
19/19 - 36s - loss: 750.3560 - loglik: -7.5466e+02 - logprior: 4.3002
Epoch 4/10
19/19 - 35s - loss: 743.1668 - loglik: -7.4816e+02 - logprior: 4.9944
Epoch 5/10
19/19 - 35s - loss: 740.7320 - loglik: -7.4596e+02 - logprior: 5.2316
Epoch 6/10
19/19 - 36s - loss: 738.4493 - loglik: -7.4387e+02 - logprior: 5.4159
Epoch 7/10
19/19 - 37s - loss: 737.8660 - loglik: -7.4353e+02 - logprior: 5.6620
Epoch 8/10
19/19 - 36s - loss: 736.2596 - loglik: -7.4224e+02 - logprior: 5.9791
Epoch 9/10
19/19 - 35s - loss: 735.1901 - loglik: -7.4153e+02 - logprior: 6.3384
Epoch 10/10
19/19 - 35s - loss: 735.0167 - loglik: -7.4162e+02 - logprior: 6.6060
Fitted a model with MAP estimate = -734.2508
Time for alignment: 752.3468
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 29s - loss: 996.0632 - loglik: -9.8858e+02 - logprior: -7.4805e+00
Epoch 2/10
19/19 - 27s - loss: 865.4927 - loglik: -8.6668e+02 - logprior: 1.1839
Epoch 3/10
19/19 - 27s - loss: 804.2343 - loglik: -8.0469e+02 - logprior: 0.4520
Epoch 4/10
19/19 - 27s - loss: 791.1163 - loglik: -7.9152e+02 - logprior: 0.4045
Epoch 5/10
19/19 - 26s - loss: 785.4606 - loglik: -7.8580e+02 - logprior: 0.3396
Epoch 6/10
19/19 - 26s - loss: 781.3951 - loglik: -7.8126e+02 - logprior: -1.3317e-01
Epoch 7/10
19/19 - 27s - loss: 778.6857 - loglik: -7.7876e+02 - logprior: 0.0772
Epoch 8/10
19/19 - 27s - loss: 778.5549 - loglik: -7.7868e+02 - logprior: 0.1262
Epoch 9/10
19/19 - 27s - loss: 777.2790 - loglik: -7.7752e+02 - logprior: 0.2441
Epoch 10/10
19/19 - 28s - loss: 776.0280 - loglik: -7.7612e+02 - logprior: 0.0906
Fitted a model with MAP estimate = -776.1982
expansions: [(30, 1), (61, 1), (68, 1), (96, 1), (97, 1), (98, 2), (99, 1), (112, 1), (113, 1), (116, 1), (117, 1), (118, 1), (120, 2), (121, 4), (123, 3), (141, 1), (144, 2), (155, 1), (165, 3), (168, 1), (175, 1), (176, 1), (177, 2), (178, 2), (188, 1), (195, 1), (197, 1), (218, 1), (220, 1), (221, 1), (222, 2), (223, 1), (226, 1), (240, 1), (241, 2), (265, 7), (273, 1), (287, 4), (301, 1), (302, 5)]
discards: [0]
Fitting a model of length 386 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 44s - loss: 813.1424 - loglik: -8.0274e+02 - logprior: -1.0398e+01
Epoch 2/2
19/19 - 44s - loss: 762.0294 - loglik: -7.6032e+02 - logprior: -1.7087e+00
Fitted a model with MAP estimate = -753.4945
expansions: [(0, 2), (369, 1)]
discards: [  0  29 102 134 143 144 212]
Fitting a model of length 382 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 43s - loss: 767.0042 - loglik: -7.6129e+02 - logprior: -5.7153e+00
Epoch 2/2
19/19 - 40s - loss: 746.9285 - loglik: -7.4972e+02 - logprior: 2.7879
Fitted a model with MAP estimate = -741.7718
expansions: [(137, 1), (140, 2)]
discards: [0]
Fitting a model of length 384 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 41s - loss: 765.5468 - loglik: -7.5633e+02 - logprior: -9.2158e+00
Epoch 2/10
19/19 - 38s - loss: 750.0026 - loglik: -7.5086e+02 - logprior: 0.8604
Epoch 3/10
19/19 - 38s - loss: 735.7919 - loglik: -7.4016e+02 - logprior: 4.3692
Epoch 4/10
19/19 - 40s - loss: 734.7990 - loglik: -7.4002e+02 - logprior: 5.2211
Epoch 5/10
19/19 - 45s - loss: 729.0187 - loglik: -7.3454e+02 - logprior: 5.5199
Epoch 6/10
19/19 - 45s - loss: 726.2496 - loglik: -7.3197e+02 - logprior: 5.7190
Epoch 7/10
19/19 - 45s - loss: 729.7754 - loglik: -7.3577e+02 - logprior: 5.9979
Fitted a model with MAP estimate = -725.4322
Time for alignment: 838.7483
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 34s - loss: 996.8837 - loglik: -9.8937e+02 - logprior: -7.5107e+00
Epoch 2/10
19/19 - 31s - loss: 871.6968 - loglik: -8.7312e+02 - logprior: 1.4192
Epoch 3/10
19/19 - 31s - loss: 812.0750 - loglik: -8.1284e+02 - logprior: 0.7621
Epoch 4/10
19/19 - 31s - loss: 790.8771 - loglik: -7.9139e+02 - logprior: 0.5106
Epoch 5/10
19/19 - 31s - loss: 787.6585 - loglik: -7.8820e+02 - logprior: 0.5412
Epoch 6/10
19/19 - 31s - loss: 784.6422 - loglik: -7.8503e+02 - logprior: 0.3869
Epoch 7/10
19/19 - 31s - loss: 776.5655 - loglik: -7.7685e+02 - logprior: 0.2849
Epoch 8/10
19/19 - 28s - loss: 779.1968 - loglik: -7.7945e+02 - logprior: 0.2539
Fitted a model with MAP estimate = -777.4518
expansions: [(14, 1), (67, 1), (95, 1), (96, 4), (97, 2), (110, 1), (111, 1), (114, 1), (115, 2), (118, 2), (120, 4), (122, 3), (153, 1), (166, 3), (169, 1), (176, 2), (177, 1), (178, 1), (180, 1), (188, 1), (189, 2), (190, 1), (197, 1), (204, 1), (218, 1), (220, 1), (221, 1), (222, 2), (224, 1), (238, 1), (242, 2), (266, 5), (293, 9), (301, 5), (303, 1)]
discards: [0]
Fitting a model of length 387 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 42s - loss: 813.0529 - loglik: -8.0237e+02 - logprior: -1.0688e+01
Epoch 2/2
19/19 - 39s - loss: 763.5120 - loglik: -7.6142e+02 - logprior: -2.0922e+00
Fitted a model with MAP estimate = -755.1581
expansions: [(0, 2), (317, 1), (365, 1)]
discards: [  0 143 144 203 221]
Fitting a model of length 386 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 41s - loss: 767.4779 - loglik: -7.6123e+02 - logprior: -6.2442e+00
Epoch 2/2
19/19 - 40s - loss: 747.1599 - loglik: -7.4968e+02 - logprior: 2.5159
Fitted a model with MAP estimate = -742.6117
expansions: [(177, 1)]
discards: [  0 103]
Fitting a model of length 385 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 48s - loss: 768.8240 - loglik: -7.5966e+02 - logprior: -9.1631e+00
Epoch 2/10
19/19 - 45s - loss: 747.6568 - loglik: -7.4850e+02 - logprior: 0.8398
Epoch 3/10
19/19 - 44s - loss: 740.0659 - loglik: -7.4424e+02 - logprior: 4.1776
Epoch 4/10
19/19 - 44s - loss: 733.1911 - loglik: -7.3810e+02 - logprior: 4.9091
Epoch 5/10
19/19 - 43s - loss: 737.4438 - loglik: -7.4262e+02 - logprior: 5.1721
Fitted a model with MAP estimate = -731.9505
Time for alignment: 739.4696
Computed alignments with likelihoods: ['-734.2508', '-725.4322', '-731.9505']
Best model has likelihood: -725.4322
SP score = 0.7605
Training of 3 independent models on file ghf22.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32a95e9f70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2cede74b80>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 368.3948 - loglik: -3.1214e+02 - logprior: -5.6256e+01
Epoch 2/10
10/10 - 1s - loss: 290.2761 - loglik: -2.7693e+02 - logprior: -1.3344e+01
Epoch 3/10
10/10 - 1s - loss: 240.8865 - loglik: -2.3500e+02 - logprior: -5.8896e+00
Epoch 4/10
10/10 - 1s - loss: 212.4331 - loglik: -2.0871e+02 - logprior: -3.7237e+00
Epoch 5/10
10/10 - 1s - loss: 201.7985 - loglik: -1.9921e+02 - logprior: -2.5874e+00
Epoch 6/10
10/10 - 1s - loss: 197.7816 - loglik: -1.9590e+02 - logprior: -1.8784e+00
Epoch 7/10
10/10 - 1s - loss: 195.8769 - loglik: -1.9450e+02 - logprior: -1.3809e+00
Epoch 8/10
10/10 - 1s - loss: 195.0645 - loglik: -1.9393e+02 - logprior: -1.1341e+00
Epoch 9/10
10/10 - 1s - loss: 194.1166 - loglik: -1.9320e+02 - logprior: -9.1429e-01
Epoch 10/10
10/10 - 1s - loss: 194.2910 - loglik: -1.9353e+02 - logprior: -7.6259e-01
Fitted a model with MAP estimate = -193.7856
expansions: [(13, 4), (19, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (58, 1), (62, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Fitting a model of length 122 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 251.8737 - loglik: -1.8843e+02 - logprior: -6.3447e+01
Epoch 2/2
10/10 - 2s - loss: 200.9961 - loglik: -1.7646e+02 - logprior: -2.4540e+01
Fitted a model with MAP estimate = -191.8735
expansions: [(0, 2), (14, 1), (67, 1)]
discards: [ 0 96]
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 220.9548 - loglik: -1.7114e+02 - logprior: -4.9812e+01
Epoch 2/2
10/10 - 2s - loss: 178.7102 - loglik: -1.6787e+02 - logprior: -1.0843e+01
Fitted a model with MAP estimate = -172.3371
expansions: []
discards: [0]
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 230.3202 - loglik: -1.7009e+02 - logprior: -6.0229e+01
Epoch 2/10
10/10 - 2s - loss: 185.7965 - loglik: -1.6890e+02 - logprior: -1.6893e+01
Epoch 3/10
10/10 - 2s - loss: 172.6532 - loglik: -1.6870e+02 - logprior: -3.9528e+00
Epoch 4/10
10/10 - 2s - loss: 167.8503 - loglik: -1.6841e+02 - logprior: 0.5593
Epoch 5/10
10/10 - 2s - loss: 165.8035 - loglik: -1.6833e+02 - logprior: 2.5277
Epoch 6/10
10/10 - 2s - loss: 164.1563 - loglik: -1.6773e+02 - logprior: 3.5759
Epoch 7/10
10/10 - 2s - loss: 163.9574 - loglik: -1.6824e+02 - logprior: 4.2785
Epoch 8/10
10/10 - 2s - loss: 163.5512 - loglik: -1.6841e+02 - logprior: 4.8571
Epoch 9/10
10/10 - 2s - loss: 162.8932 - loglik: -1.6825e+02 - logprior: 5.3602
Epoch 10/10
10/10 - 2s - loss: 162.4343 - loglik: -1.6819e+02 - logprior: 5.7579
Fitted a model with MAP estimate = -162.4506
Time for alignment: 54.8604
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 368.5785 - loglik: -3.1232e+02 - logprior: -5.6256e+01
Epoch 2/10
10/10 - 1s - loss: 289.9504 - loglik: -2.7660e+02 - logprior: -1.3348e+01
Epoch 3/10
10/10 - 1s - loss: 243.1886 - loglik: -2.3724e+02 - logprior: -5.9456e+00
Epoch 4/10
10/10 - 1s - loss: 214.5896 - loglik: -2.1080e+02 - logprior: -3.7887e+00
Epoch 5/10
10/10 - 1s - loss: 202.8116 - loglik: -2.0019e+02 - logprior: -2.6197e+00
Epoch 6/10
10/10 - 1s - loss: 197.4626 - loglik: -1.9546e+02 - logprior: -2.0025e+00
Epoch 7/10
10/10 - 1s - loss: 195.0348 - loglik: -1.9346e+02 - logprior: -1.5702e+00
Epoch 8/10
10/10 - 1s - loss: 194.2747 - loglik: -1.9301e+02 - logprior: -1.2694e+00
Epoch 9/10
10/10 - 1s - loss: 193.5058 - loglik: -1.9245e+02 - logprior: -1.0538e+00
Epoch 10/10
10/10 - 2s - loss: 192.8475 - loglik: -1.9197e+02 - logprior: -8.8148e-01
Fitted a model with MAP estimate = -192.9002
expansions: [(13, 4), (17, 2), (19, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (60, 1), (62, 1), (78, 4), (81, 1), (86, 1), (90, 1), (91, 2)]
discards: [0]
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 250.5354 - loglik: -1.8709e+02 - logprior: -6.3444e+01
Epoch 2/2
10/10 - 2s - loss: 199.4992 - loglik: -1.7490e+02 - logprior: -2.4604e+01
Fitted a model with MAP estimate = -191.0887
expansions: [(0, 2), (13, 1), (69, 1), (111, 1)]
discards: [ 0 98]
Fitting a model of length 126 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 220.2336 - loglik: -1.7040e+02 - logprior: -4.9830e+01
Epoch 2/2
10/10 - 2s - loss: 177.5165 - loglik: -1.6664e+02 - logprior: -1.0878e+01
Fitted a model with MAP estimate = -171.1341
expansions: []
discards: [ 0 22 23]
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 231.0963 - loglik: -1.7076e+02 - logprior: -6.0334e+01
Epoch 2/10
10/10 - 2s - loss: 186.9472 - loglik: -1.6982e+02 - logprior: -1.7132e+01
Epoch 3/10
10/10 - 2s - loss: 172.8828 - loglik: -1.6880e+02 - logprior: -4.0840e+00
Epoch 4/10
10/10 - 2s - loss: 168.1249 - loglik: -1.6864e+02 - logprior: 0.5112
Epoch 5/10
10/10 - 2s - loss: 165.6683 - loglik: -1.6816e+02 - logprior: 2.4892
Epoch 6/10
10/10 - 2s - loss: 164.9044 - loglik: -1.6844e+02 - logprior: 3.5380
Epoch 7/10
10/10 - 2s - loss: 164.1487 - loglik: -1.6837e+02 - logprior: 4.2196
Epoch 8/10
10/10 - 2s - loss: 163.6874 - loglik: -1.6848e+02 - logprior: 4.7916
Epoch 9/10
10/10 - 2s - loss: 163.1936 - loglik: -1.6848e+02 - logprior: 5.2833
Epoch 10/10
10/10 - 2s - loss: 162.7121 - loglik: -1.6839e+02 - logprior: 5.6737
Fitted a model with MAP estimate = -162.6856
Time for alignment: 52.6480
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 368.3605 - loglik: -3.1211e+02 - logprior: -5.6254e+01
Epoch 2/10
10/10 - 1s - loss: 291.0392 - loglik: -2.7769e+02 - logprior: -1.3354e+01
Epoch 3/10
10/10 - 1s - loss: 241.7543 - loglik: -2.3577e+02 - logprior: -5.9819e+00
Epoch 4/10
10/10 - 1s - loss: 213.6187 - loglik: -2.0968e+02 - logprior: -3.9427e+00
Epoch 5/10
10/10 - 1s - loss: 202.4020 - loglik: -1.9953e+02 - logprior: -2.8712e+00
Epoch 6/10
10/10 - 1s - loss: 198.4672 - loglik: -1.9628e+02 - logprior: -2.1896e+00
Epoch 7/10
10/10 - 1s - loss: 196.0193 - loglik: -1.9426e+02 - logprior: -1.7635e+00
Epoch 8/10
10/10 - 1s - loss: 194.2423 - loglik: -1.9274e+02 - logprior: -1.5052e+00
Epoch 9/10
10/10 - 1s - loss: 193.3562 - loglik: -1.9202e+02 - logprior: -1.3407e+00
Epoch 10/10
10/10 - 1s - loss: 192.6229 - loglik: -1.9142e+02 - logprior: -1.1985e+00
Fitted a model with MAP estimate = -192.6766
expansions: [(13, 3), (16, 1), (17, 3), (25, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (60, 1), (62, 1), (76, 1), (78, 3), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 249.5152 - loglik: -1.8617e+02 - logprior: -6.3343e+01
Epoch 2/2
10/10 - 2s - loss: 197.7633 - loglik: -1.7319e+02 - logprior: -2.4569e+01
Fitted a model with MAP estimate = -188.8019
expansions: [(0, 2), (70, 1)]
discards: [ 0 20 21]
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 219.7200 - loglik: -1.6997e+02 - logprior: -4.9755e+01
Epoch 2/2
10/10 - 2s - loss: 177.8796 - loglik: -1.6704e+02 - logprior: -1.0842e+01
Fitted a model with MAP estimate = -171.7969
expansions: []
discards: [ 0 99]
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 230.4778 - loglik: -1.7021e+02 - logprior: -6.0271e+01
Epoch 2/10
10/10 - 2s - loss: 186.2406 - loglik: -1.6927e+02 - logprior: -1.6972e+01
Epoch 3/10
10/10 - 2s - loss: 172.5597 - loglik: -1.6855e+02 - logprior: -4.0062e+00
Epoch 4/10
10/10 - 2s - loss: 167.8653 - loglik: -1.6840e+02 - logprior: 0.5366
Epoch 5/10
10/10 - 2s - loss: 165.5659 - loglik: -1.6806e+02 - logprior: 2.4904
Epoch 6/10
10/10 - 2s - loss: 164.7267 - loglik: -1.6826e+02 - logprior: 3.5305
Epoch 7/10
10/10 - 2s - loss: 163.8646 - loglik: -1.6808e+02 - logprior: 4.2202
Epoch 8/10
10/10 - 2s - loss: 163.3189 - loglik: -1.6811e+02 - logprior: 4.7952
Epoch 9/10
10/10 - 2s - loss: 162.9672 - loglik: -1.6825e+02 - logprior: 5.2873
Epoch 10/10
10/10 - 2s - loss: 162.7895 - loglik: -1.6846e+02 - logprior: 5.6736
Fitted a model with MAP estimate = -162.4771
Time for alignment: 51.4185
Computed alignments with likelihoods: ['-162.4506', '-162.6856', '-162.4771']
Best model has likelihood: -162.4506
SP score = 0.9273
Training of 3 independent models on file flav.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2cee109610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2cedd1bb20>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 396.4530 - loglik: -3.8840e+02 - logprior: -8.0536e+00
Epoch 2/10
13/13 - 3s - loss: 353.6120 - loglik: -3.5179e+02 - logprior: -1.8223e+00
Epoch 3/10
13/13 - 3s - loss: 324.4875 - loglik: -3.2296e+02 - logprior: -1.5257e+00
Epoch 4/10
13/13 - 3s - loss: 310.3882 - loglik: -3.0871e+02 - logprior: -1.6791e+00
Epoch 5/10
13/13 - 3s - loss: 305.0282 - loglik: -3.0350e+02 - logprior: -1.5282e+00
Epoch 6/10
13/13 - 3s - loss: 303.3611 - loglik: -3.0189e+02 - logprior: -1.4698e+00
Epoch 7/10
13/13 - 3s - loss: 301.3860 - loglik: -2.9987e+02 - logprior: -1.5196e+00
Epoch 8/10
13/13 - 3s - loss: 301.2227 - loglik: -2.9969e+02 - logprior: -1.5353e+00
Epoch 9/10
13/13 - 3s - loss: 299.3696 - loglik: -2.9784e+02 - logprior: -1.5310e+00
Epoch 10/10
13/13 - 3s - loss: 299.5964 - loglik: -2.9807e+02 - logprior: -1.5245e+00
Fitted a model with MAP estimate = -299.4061
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 2), (25, 2), (26, 1), (27, 2), (29, 1), (51, 1), (52, 1), (55, 1), (58, 3), (64, 2), (79, 1), (82, 1), (99, 1), (100, 3)]
discards: [0]
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 320.9618 - loglik: -3.1160e+02 - logprior: -9.3582e+00
Epoch 2/2
13/13 - 3s - loss: 301.9571 - loglik: -2.9777e+02 - logprior: -4.1826e+00
Fitted a model with MAP estimate = -297.6460
expansions: [(0, 2)]
discards: [  0  29  32  74  75  82 124]
Fitting a model of length 129 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 303.1076 - loglik: -2.9609e+02 - logprior: -7.0144e+00
Epoch 2/2
13/13 - 3s - loss: 293.0687 - loglik: -2.9139e+02 - logprior: -1.6757e+00
Fitted a model with MAP estimate = -291.1249
expansions: [(113, 1)]
discards: [0]
Fitting a model of length 129 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 303.6168 - loglik: -2.9478e+02 - logprior: -8.8343e+00
Epoch 2/10
13/13 - 4s - loss: 294.0434 - loglik: -2.9152e+02 - logprior: -2.5204e+00
Epoch 3/10
13/13 - 3s - loss: 290.6726 - loglik: -2.8978e+02 - logprior: -8.9510e-01
Epoch 4/10
13/13 - 3s - loss: 288.5611 - loglik: -2.8801e+02 - logprior: -5.5371e-01
Epoch 5/10
13/13 - 3s - loss: 286.3831 - loglik: -2.8604e+02 - logprior: -3.4527e-01
Epoch 6/10
13/13 - 3s - loss: 286.6685 - loglik: -2.8639e+02 - logprior: -2.8305e-01
Fitted a model with MAP estimate = -285.7942
Time for alignment: 88.4787
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 396.4155 - loglik: -3.8835e+02 - logprior: -8.0659e+00
Epoch 2/10
13/13 - 3s - loss: 353.5554 - loglik: -3.5174e+02 - logprior: -1.8181e+00
Epoch 3/10
13/13 - 3s - loss: 324.5347 - loglik: -3.2300e+02 - logprior: -1.5331e+00
Epoch 4/10
13/13 - 3s - loss: 310.6500 - loglik: -3.0893e+02 - logprior: -1.7223e+00
Epoch 5/10
13/13 - 3s - loss: 304.1917 - loglik: -3.0256e+02 - logprior: -1.6293e+00
Epoch 6/10
13/13 - 3s - loss: 301.1134 - loglik: -2.9956e+02 - logprior: -1.5524e+00
Epoch 7/10
13/13 - 3s - loss: 301.0010 - loglik: -2.9944e+02 - logprior: -1.5603e+00
Epoch 8/10
13/13 - 3s - loss: 300.1890 - loglik: -2.9863e+02 - logprior: -1.5617e+00
Epoch 9/10
13/13 - 3s - loss: 299.4699 - loglik: -2.9791e+02 - logprior: -1.5572e+00
Epoch 10/10
13/13 - 3s - loss: 299.1121 - loglik: -2.9755e+02 - logprior: -1.5639e+00
Fitted a model with MAP estimate = -299.3799
expansions: [(6, 1), (15, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (26, 3), (27, 2), (28, 1), (32, 1), (52, 1), (55, 1), (58, 2), (64, 2), (79, 2), (80, 1), (81, 2), (82, 1), (100, 2), (101, 2)]
discards: [0]
Fitting a model of length 137 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 319.2809 - loglik: -3.0993e+02 - logprior: -9.3543e+00
Epoch 2/2
13/13 - 4s - loss: 299.7137 - loglik: -2.9553e+02 - logprior: -4.1789e+00
Fitted a model with MAP estimate = -295.3431
expansions: [(0, 2)]
discards: [ 0 34 36 74 81 99]
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 300.4235 - loglik: -2.9344e+02 - logprior: -6.9805e+00
Epoch 2/2
13/13 - 3s - loss: 291.2649 - loglik: -2.8964e+02 - logprior: -1.6229e+00
Fitted a model with MAP estimate = -289.0469
expansions: []
discards: [  0 124]
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 302.2131 - loglik: -2.9355e+02 - logprior: -8.6679e+00
Epoch 2/10
13/13 - 3s - loss: 293.0834 - loglik: -2.9071e+02 - logprior: -2.3697e+00
Epoch 3/10
13/13 - 3s - loss: 289.1331 - loglik: -2.8825e+02 - logprior: -8.8511e-01
Epoch 4/10
13/13 - 3s - loss: 287.1377 - loglik: -2.8661e+02 - logprior: -5.2935e-01
Epoch 5/10
13/13 - 4s - loss: 285.8791 - loglik: -2.8555e+02 - logprior: -3.3250e-01
Epoch 6/10
13/13 - 4s - loss: 285.9901 - loglik: -2.8572e+02 - logprior: -2.7032e-01
Fitted a model with MAP estimate = -284.8248
Time for alignment: 89.1331
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 396.3950 - loglik: -3.8832e+02 - logprior: -8.0731e+00
Epoch 2/10
13/13 - 3s - loss: 354.4998 - loglik: -3.5268e+02 - logprior: -1.8201e+00
Epoch 3/10
13/13 - 3s - loss: 324.5610 - loglik: -3.2306e+02 - logprior: -1.5020e+00
Epoch 4/10
13/13 - 3s - loss: 310.1332 - loglik: -3.0840e+02 - logprior: -1.7301e+00
Epoch 5/10
13/13 - 3s - loss: 304.5650 - loglik: -3.0294e+02 - logprior: -1.6232e+00
Epoch 6/10
13/13 - 3s - loss: 302.7132 - loglik: -3.0118e+02 - logprior: -1.5379e+00
Epoch 7/10
13/13 - 3s - loss: 301.9542 - loglik: -3.0038e+02 - logprior: -1.5767e+00
Epoch 8/10
13/13 - 3s - loss: 301.3647 - loglik: -2.9979e+02 - logprior: -1.5793e+00
Epoch 9/10
13/13 - 3s - loss: 301.1118 - loglik: -2.9955e+02 - logprior: -1.5647e+00
Epoch 10/10
13/13 - 3s - loss: 300.5077 - loglik: -2.9894e+02 - logprior: -1.5686e+00
Fitted a model with MAP estimate = -300.6463
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 2), (25, 2), (26, 1), (27, 1), (28, 1), (51, 1), (52, 1), (55, 1), (58, 3), (64, 2), (80, 1), (82, 1), (92, 8), (100, 2)]
discards: [0]
Fitting a model of length 139 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 322.0096 - loglik: -3.1268e+02 - logprior: -9.3303e+00
Epoch 2/2
13/13 - 4s - loss: 300.7232 - loglik: -2.9654e+02 - logprior: -4.1880e+00
Fitted a model with MAP estimate = -296.0607
expansions: [(0, 2)]
discards: [  0  28  73  74  81 117 129 130]
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 301.8758 - loglik: -2.9485e+02 - logprior: -7.0274e+00
Epoch 2/2
13/13 - 4s - loss: 292.8873 - loglik: -2.9120e+02 - logprior: -1.6828e+00
Fitted a model with MAP estimate = -290.7108
expansions: []
discards: [  0 115 116]
Fitting a model of length 130 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 304.5523 - loglik: -2.9585e+02 - logprior: -8.6977e+00
Epoch 2/10
13/13 - 4s - loss: 294.4006 - loglik: -2.9195e+02 - logprior: -2.4488e+00
Epoch 3/10
13/13 - 4s - loss: 290.0627 - loglik: -2.8911e+02 - logprior: -9.5063e-01
Epoch 4/10
13/13 - 4s - loss: 288.4295 - loglik: -2.8783e+02 - logprior: -6.0309e-01
Epoch 5/10
13/13 - 4s - loss: 287.0013 - loglik: -2.8659e+02 - logprior: -4.0970e-01
Epoch 6/10
13/13 - 4s - loss: 285.8285 - loglik: -2.8550e+02 - logprior: -3.3235e-01
Epoch 7/10
13/13 - 4s - loss: 285.3803 - loglik: -2.8506e+02 - logprior: -3.2424e-01
Epoch 8/10
13/13 - 4s - loss: 284.8343 - loglik: -2.8455e+02 - logprior: -2.8158e-01
Epoch 9/10
13/13 - 4s - loss: 284.9057 - loglik: -2.8465e+02 - logprior: -2.5650e-01
Fitted a model with MAP estimate = -284.4036
Time for alignment: 104.8714
Computed alignments with likelihoods: ['-285.7942', '-284.8248', '-284.4036']
Best model has likelihood: -284.4036
SP score = 0.8634
Training of 3 independent models on file sodfe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2cece5d760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2cedf85f70>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 222.8130 - loglik: -2.1423e+02 - logprior: -8.5865e+00
Epoch 2/10
13/13 - 1s - loss: 172.5449 - loglik: -1.7034e+02 - logprior: -2.2070e+00
Epoch 3/10
13/13 - 1s - loss: 146.9405 - loglik: -1.4518e+02 - logprior: -1.7600e+00
Epoch 4/10
13/13 - 2s - loss: 137.5478 - loglik: -1.3591e+02 - logprior: -1.6397e+00
Epoch 5/10
13/13 - 1s - loss: 134.1884 - loglik: -1.3267e+02 - logprior: -1.5157e+00
Epoch 6/10
13/13 - 1s - loss: 133.3890 - loglik: -1.3186e+02 - logprior: -1.5339e+00
Epoch 7/10
13/13 - 2s - loss: 132.8141 - loglik: -1.3132e+02 - logprior: -1.4891e+00
Epoch 8/10
13/13 - 1s - loss: 132.7016 - loglik: -1.3122e+02 - logprior: -1.4866e+00
Epoch 9/10
13/13 - 1s - loss: 132.5582 - loglik: -1.3110e+02 - logprior: -1.4535e+00
Epoch 10/10
13/13 - 1s - loss: 132.3012 - loglik: -1.3086e+02 - logprior: -1.4419e+00
Fitted a model with MAP estimate = -132.2897
expansions: [(0, 4), (13, 1), (20, 1), (36, 3), (37, 1), (42, 1), (43, 2), (44, 2), (45, 5)]
discards: []
Fitting a model of length 85 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 137.5604 - loglik: -1.2736e+02 - logprior: -1.0205e+01
Epoch 2/2
13/13 - 2s - loss: 121.6453 - loglik: -1.1850e+02 - logprior: -3.1423e+00
Fitted a model with MAP estimate = -118.1579
expansions: [(0, 2), (44, 1)]
discards: [54]
Fitting a model of length 87 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 126.0404 - loglik: -1.1591e+02 - logprior: -1.0133e+01
Epoch 2/2
13/13 - 2s - loss: 116.5045 - loglik: -1.1317e+02 - logprior: -3.3374e+00
Fitted a model with MAP estimate = -114.4384
expansions: []
discards: []
Fitting a model of length 87 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 120.8874 - loglik: -1.1282e+02 - logprior: -8.0700e+00
Epoch 2/10
13/13 - 2s - loss: 114.4081 - loglik: -1.1202e+02 - logprior: -2.3864e+00
Epoch 3/10
13/13 - 2s - loss: 113.3452 - loglik: -1.1151e+02 - logprior: -1.8325e+00
Epoch 4/10
13/13 - 2s - loss: 112.6637 - loglik: -1.1118e+02 - logprior: -1.4859e+00
Epoch 5/10
13/13 - 2s - loss: 112.2934 - loglik: -1.1090e+02 - logprior: -1.3972e+00
Epoch 6/10
13/13 - 2s - loss: 112.0359 - loglik: -1.1069e+02 - logprior: -1.3429e+00
Epoch 7/10
13/13 - 2s - loss: 111.9303 - loglik: -1.1062e+02 - logprior: -1.3120e+00
Epoch 8/10
13/13 - 2s - loss: 111.6900 - loglik: -1.1041e+02 - logprior: -1.2785e+00
Epoch 9/10
13/13 - 2s - loss: 111.5723 - loglik: -1.1033e+02 - logprior: -1.2391e+00
Epoch 10/10
13/13 - 2s - loss: 111.7808 - loglik: -1.1058e+02 - logprior: -1.1967e+00
Fitted a model with MAP estimate = -111.4832
Time for alignment: 61.6994
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 223.3454 - loglik: -2.1476e+02 - logprior: -8.5892e+00
Epoch 2/10
13/13 - 1s - loss: 172.4064 - loglik: -1.7019e+02 - logprior: -2.2152e+00
Epoch 3/10
13/13 - 1s - loss: 143.9256 - loglik: -1.4220e+02 - logprior: -1.7303e+00
Epoch 4/10
13/13 - 1s - loss: 137.4477 - loglik: -1.3587e+02 - logprior: -1.5825e+00
Epoch 5/10
13/13 - 2s - loss: 135.4340 - loglik: -1.3398e+02 - logprior: -1.4580e+00
Epoch 6/10
13/13 - 2s - loss: 134.4064 - loglik: -1.3294e+02 - logprior: -1.4686e+00
Epoch 7/10
13/13 - 1s - loss: 133.8382 - loglik: -1.3239e+02 - logprior: -1.4523e+00
Epoch 8/10
13/13 - 1s - loss: 133.0976 - loglik: -1.3165e+02 - logprior: -1.4461e+00
Epoch 9/10
13/13 - 1s - loss: 133.6260 - loglik: -1.3220e+02 - logprior: -1.4259e+00
Fitted a model with MAP estimate = -133.1951
expansions: [(0, 4), (13, 1), (36, 3), (37, 2), (38, 1), (39, 1), (43, 1), (44, 3), (45, 3)]
discards: []
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 138.6443 - loglik: -1.2845e+02 - logprior: -1.0199e+01
Epoch 2/2
13/13 - 2s - loss: 120.8417 - loglik: -1.1766e+02 - logprior: -3.1837e+00
Fitted a model with MAP estimate = -117.6161
expansions: [(0, 2)]
discards: [59 60 61 62 63]
Fitting a model of length 81 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 129.7678 - loglik: -1.1971e+02 - logprior: -1.0057e+01
Epoch 2/2
13/13 - 2s - loss: 120.7021 - loglik: -1.1750e+02 - logprior: -3.1995e+00
Fitted a model with MAP estimate = -118.5579
expansions: [(61, 4)]
discards: []
Fitting a model of length 85 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 124.7133 - loglik: -1.1676e+02 - logprior: -7.9512e+00
Epoch 2/10
13/13 - 2s - loss: 117.0646 - loglik: -1.1479e+02 - logprior: -2.2768e+00
Epoch 3/10
13/13 - 2s - loss: 114.9862 - loglik: -1.1323e+02 - logprior: -1.7543e+00
Epoch 4/10
13/13 - 2s - loss: 114.4885 - loglik: -1.1309e+02 - logprior: -1.3997e+00
Epoch 5/10
13/13 - 2s - loss: 113.9218 - loglik: -1.1261e+02 - logprior: -1.3092e+00
Epoch 6/10
13/13 - 2s - loss: 113.6064 - loglik: -1.1235e+02 - logprior: -1.2555e+00
Epoch 7/10
13/13 - 2s - loss: 113.5626 - loglik: -1.1234e+02 - logprior: -1.2249e+00
Epoch 8/10
13/13 - 2s - loss: 113.2700 - loglik: -1.1208e+02 - logprior: -1.1949e+00
Epoch 9/10
13/13 - 2s - loss: 113.3178 - loglik: -1.1217e+02 - logprior: -1.1527e+00
Fitted a model with MAP estimate = -113.1972
Time for alignment: 57.4064
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 222.3711 - loglik: -2.1378e+02 - logprior: -8.5896e+00
Epoch 2/10
13/13 - 1s - loss: 173.8035 - loglik: -1.7158e+02 - logprior: -2.2248e+00
Epoch 3/10
13/13 - 2s - loss: 145.0099 - loglik: -1.4320e+02 - logprior: -1.8083e+00
Epoch 4/10
13/13 - 1s - loss: 137.0919 - loglik: -1.3537e+02 - logprior: -1.7210e+00
Epoch 5/10
13/13 - 1s - loss: 135.3426 - loglik: -1.3375e+02 - logprior: -1.5951e+00
Epoch 6/10
13/13 - 1s - loss: 134.1499 - loglik: -1.3256e+02 - logprior: -1.5885e+00
Epoch 7/10
13/13 - 2s - loss: 134.2243 - loglik: -1.3268e+02 - logprior: -1.5422e+00
Fitted a model with MAP estimate = -133.8245
expansions: [(0, 4), (13, 1), (16, 1), (35, 3), (36, 1), (37, 1), (38, 2), (43, 2), (44, 3)]
discards: []
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 139.3032 - loglik: -1.2924e+02 - logprior: -1.0065e+01
Epoch 2/2
13/13 - 2s - loss: 123.0521 - loglik: -1.2008e+02 - logprior: -2.9713e+00
Fitted a model with MAP estimate = -119.9966
expansions: [(0, 2)]
discards: [49 61]
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 128.9908 - loglik: -1.1891e+02 - logprior: -1.0076e+01
Epoch 2/2
13/13 - 2s - loss: 119.6104 - loglik: -1.1641e+02 - logprior: -3.2040e+00
Fitted a model with MAP estimate = -117.6314
expansions: []
discards: []
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 124.1913 - loglik: -1.1623e+02 - logprior: -7.9628e+00
Epoch 2/10
13/13 - 2s - loss: 117.6956 - loglik: -1.1538e+02 - logprior: -2.3112e+00
Epoch 3/10
13/13 - 2s - loss: 116.5407 - loglik: -1.1477e+02 - logprior: -1.7695e+00
Epoch 4/10
13/13 - 2s - loss: 115.7116 - loglik: -1.1430e+02 - logprior: -1.4103e+00
Epoch 5/10
13/13 - 2s - loss: 115.5018 - loglik: -1.1416e+02 - logprior: -1.3380e+00
Epoch 6/10
13/13 - 2s - loss: 114.9912 - loglik: -1.1369e+02 - logprior: -1.2969e+00
Epoch 7/10
13/13 - 2s - loss: 114.7858 - loglik: -1.1352e+02 - logprior: -1.2678e+00
Epoch 8/10
13/13 - 2s - loss: 114.8618 - loglik: -1.1363e+02 - logprior: -1.2324e+00
Fitted a model with MAP estimate = -114.6701
Time for alignment: 49.9959
Computed alignments with likelihoods: ['-111.4832', '-113.1972', '-114.6701']
Best model has likelihood: -111.4832
SP score = 0.4964
Training of 3 independent models on file zf-CCHH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b86f06d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b8cc48e0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.6713 - loglik: -5.8813e+01 - logprior: -8.5792e-01
Epoch 2/10
41/41 - 1s - loss: 46.1748 - loglik: -4.5270e+01 - logprior: -9.0452e-01
Epoch 3/10
41/41 - 1s - loss: 45.2967 - loglik: -4.4416e+01 - logprior: -8.8078e-01
Epoch 4/10
41/41 - 1s - loss: 45.1465 - loglik: -4.4269e+01 - logprior: -8.7757e-01
Epoch 5/10
41/41 - 1s - loss: 44.9427 - loglik: -4.4068e+01 - logprior: -8.7426e-01
Epoch 6/10
41/41 - 1s - loss: 44.7675 - loglik: -4.3899e+01 - logprior: -8.6885e-01
Epoch 7/10
41/41 - 1s - loss: 44.6281 - loglik: -4.3759e+01 - logprior: -8.6877e-01
Epoch 8/10
41/41 - 1s - loss: 44.6343 - loglik: -4.3767e+01 - logprior: -8.6691e-01
Fitted a model with MAP estimate = -44.5826
expansions: [(4, 1), (9, 2), (10, 1), (11, 1), (12, 1)]
discards: []
Fitting a model of length 24 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 45.1108 - loglik: -4.4040e+01 - logprior: -1.0710e+00
Epoch 2/2
41/41 - 1s - loss: 43.3200 - loglik: -4.2469e+01 - logprior: -8.5055e-01
Fitted a model with MAP estimate = -42.5524
expansions: []
discards: [11]
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 44.0977 - loglik: -4.3051e+01 - logprior: -1.0467e+00
Epoch 2/2
41/41 - 1s - loss: 43.2696 - loglik: -4.2434e+01 - logprior: -8.3598e-01
Fitted a model with MAP estimate = -42.5439
expansions: []
discards: []
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 42.7544 - loglik: -4.2075e+01 - logprior: -6.7952e-01
Epoch 2/10
58/58 - 2s - loss: 42.0376 - loglik: -4.1457e+01 - logprior: -5.8062e-01
Epoch 3/10
58/58 - 2s - loss: 41.8469 - loglik: -4.1271e+01 - logprior: -5.7564e-01
Epoch 4/10
58/58 - 1s - loss: 41.5571 - loglik: -4.0985e+01 - logprior: -5.7239e-01
Epoch 5/10
58/58 - 2s - loss: 41.6348 - loglik: -4.1066e+01 - logprior: -5.6907e-01
Fitted a model with MAP estimate = -41.4235
Time for alignment: 61.0708
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.7337 - loglik: -5.8879e+01 - logprior: -8.5443e-01
Epoch 2/10
41/41 - 1s - loss: 46.1635 - loglik: -4.5257e+01 - logprior: -9.0614e-01
Epoch 3/10
41/41 - 1s - loss: 45.3750 - loglik: -4.4496e+01 - logprior: -8.7869e-01
Epoch 4/10
41/41 - 1s - loss: 45.0987 - loglik: -4.4224e+01 - logprior: -8.7480e-01
Epoch 5/10
41/41 - 1s - loss: 44.9096 - loglik: -4.4038e+01 - logprior: -8.7137e-01
Epoch 6/10
41/41 - 1s - loss: 44.7065 - loglik: -4.3838e+01 - logprior: -8.6891e-01
Epoch 7/10
41/41 - 1s - loss: 44.5338 - loglik: -4.3667e+01 - logprior: -8.6717e-01
Epoch 8/10
41/41 - 1s - loss: 44.6665 - loglik: -4.3800e+01 - logprior: -8.6608e-01
Fitted a model with MAP estimate = -44.4951
expansions: [(8, 2), (9, 2), (10, 2), (11, 2), (12, 1)]
discards: []
Fitting a model of length 27 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 46.0624 - loglik: -4.4972e+01 - logprior: -1.0906e+00
Epoch 2/2
41/41 - 1s - loss: 43.2695 - loglik: -4.2391e+01 - logprior: -8.7804e-01
Fitted a model with MAP estimate = -42.5979
expansions: []
discards: [ 8 12 14 17]
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.1698 - loglik: -4.3118e+01 - logprior: -1.0521e+00
Epoch 2/2
41/41 - 1s - loss: 43.2628 - loglik: -4.2429e+01 - logprior: -8.3349e-01
Fitted a model with MAP estimate = -42.6099
expansions: []
discards: []
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 42.6582 - loglik: -4.1978e+01 - logprior: -6.8040e-01
Epoch 2/10
58/58 - 2s - loss: 42.0552 - loglik: -4.1474e+01 - logprior: -5.8152e-01
Epoch 3/10
58/58 - 2s - loss: 41.9259 - loglik: -4.1351e+01 - logprior: -5.7494e-01
Epoch 4/10
58/58 - 2s - loss: 41.6171 - loglik: -4.1045e+01 - logprior: -5.7257e-01
Epoch 5/10
58/58 - 2s - loss: 41.5782 - loglik: -4.1009e+01 - logprior: -5.6969e-01
Epoch 6/10
58/58 - 2s - loss: 41.5414 - loglik: -4.0973e+01 - logprior: -5.6836e-01
Epoch 7/10
58/58 - 2s - loss: 41.3276 - loglik: -4.0760e+01 - logprior: -5.6728e-01
Epoch 8/10
58/58 - 2s - loss: 41.3755 - loglik: -4.0811e+01 - logprior: -5.6490e-01
Fitted a model with MAP estimate = -41.2677
Time for alignment: 64.2964
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.5406 - loglik: -5.8673e+01 - logprior: -8.6774e-01
Epoch 2/10
41/41 - 1s - loss: 46.1430 - loglik: -4.5244e+01 - logprior: -8.9864e-01
Epoch 3/10
41/41 - 1s - loss: 45.1980 - loglik: -4.4317e+01 - logprior: -8.8110e-01
Epoch 4/10
41/41 - 1s - loss: 45.2832 - loglik: -4.4406e+01 - logprior: -8.7714e-01
Fitted a model with MAP estimate = -44.5148
expansions: [(4, 1), (9, 2), (10, 1), (11, 2), (12, 1)]
discards: []
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 45.4727 - loglik: -4.4394e+01 - logprior: -1.0786e+00
Epoch 2/2
41/41 - 1s - loss: 43.2560 - loglik: -4.2391e+01 - logprior: -8.6458e-01
Fitted a model with MAP estimate = -42.5501
expansions: []
discards: [11 15]
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.0964 - loglik: -4.3050e+01 - logprior: -1.0468e+00
Epoch 2/2
41/41 - 1s - loss: 43.2520 - loglik: -4.2415e+01 - logprior: -8.3710e-01
Fitted a model with MAP estimate = -42.5475
expansions: []
discards: []
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 42.6308 - loglik: -4.1952e+01 - logprior: -6.7919e-01
Epoch 2/10
58/58 - 2s - loss: 42.0636 - loglik: -4.1481e+01 - logprior: -5.8237e-01
Epoch 3/10
58/58 - 2s - loss: 41.9433 - loglik: -4.1369e+01 - logprior: -5.7467e-01
Epoch 4/10
58/58 - 2s - loss: 41.5041 - loglik: -4.0933e+01 - logprior: -5.7074e-01
Epoch 5/10
58/58 - 2s - loss: 41.6189 - loglik: -4.1049e+01 - logprior: -5.7007e-01
Fitted a model with MAP estimate = -41.4308
Time for alignment: 54.4436
Computed alignments with likelihoods: ['-41.4235', '-41.2677', '-41.4308']
Best model has likelihood: -41.2677
SP score = 0.9665
Training of 3 independent models on file scorptoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34a6cbb7c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b84869d0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 280.9415 - loglik: -1.5677e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 1s - loss: 168.9647 - loglik: -1.3519e+02 - logprior: -3.3774e+01
Epoch 3/10
10/10 - 1s - loss: 134.1365 - loglik: -1.1814e+02 - logprior: -1.5993e+01
Epoch 4/10
10/10 - 1s - loss: 118.0228 - loglik: -1.0848e+02 - logprior: -9.5403e+00
Epoch 5/10
10/10 - 0s - loss: 109.6145 - loglik: -1.0364e+02 - logprior: -5.9696e+00
Epoch 6/10
10/10 - 0s - loss: 105.6357 - loglik: -1.0179e+02 - logprior: -3.8492e+00
Epoch 7/10
10/10 - 1s - loss: 103.6435 - loglik: -1.0109e+02 - logprior: -2.5497e+00
Epoch 8/10
10/10 - 1s - loss: 102.4926 - loglik: -1.0078e+02 - logprior: -1.7172e+00
Epoch 9/10
10/10 - 0s - loss: 101.8291 - loglik: -1.0071e+02 - logprior: -1.1191e+00
Epoch 10/10
10/10 - 0s - loss: 101.3851 - loglik: -1.0071e+02 - logprior: -6.7910e-01
Fitted a model with MAP estimate = -101.2020
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (23, 3), (25, 1), (31, 3)]
discards: [0]
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 240.7664 - loglik: -1.0190e+02 - logprior: -1.3887e+02
Epoch 2/2
10/10 - 1s - loss: 152.6857 - loglik: -9.5418e+01 - logprior: -5.7268e+01
Fitted a model with MAP estimate = -137.8383
expansions: [(0, 2)]
discards: [ 0  8 29]
Fitting a model of length 55 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 203.2475 - loglik: -9.1691e+01 - logprior: -1.1156e+02
Epoch 2/2
10/10 - 1s - loss: 119.6942 - loglik: -8.9879e+01 - logprior: -2.9816e+01
Fitted a model with MAP estimate = -107.2240
expansions: []
discards: [ 0 18]
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 221.9536 - loglik: -9.2692e+01 - logprior: -1.2926e+02
Epoch 2/10
10/10 - 1s - loss: 128.8720 - loglik: -9.1891e+01 - logprior: -3.6981e+01
Epoch 3/10
10/10 - 1s - loss: 105.9803 - loglik: -9.1798e+01 - logprior: -1.4183e+01
Epoch 4/10
10/10 - 1s - loss: 97.7957 - loglik: -9.1861e+01 - logprior: -5.9347e+00
Epoch 5/10
10/10 - 1s - loss: 93.7506 - loglik: -9.1953e+01 - logprior: -1.7978e+00
Epoch 6/10
10/10 - 1s - loss: 91.3811 - loglik: -9.1955e+01 - logprior: 0.5743
Epoch 7/10
10/10 - 1s - loss: 89.9501 - loglik: -9.1930e+01 - logprior: 1.9797
Epoch 8/10
10/10 - 1s - loss: 89.0531 - loglik: -9.1981e+01 - logprior: 2.9283
Epoch 9/10
10/10 - 1s - loss: 88.4144 - loglik: -9.2028e+01 - logprior: 3.6133
Epoch 10/10
10/10 - 1s - loss: 87.9052 - loglik: -9.2087e+01 - logprior: 4.1822
Fitted a model with MAP estimate = -87.6574
Time for alignment: 29.5739
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 280.9415 - loglik: -1.5677e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 1s - loss: 168.9647 - loglik: -1.3519e+02 - logprior: -3.3774e+01
Epoch 3/10
10/10 - 1s - loss: 134.1365 - loglik: -1.1814e+02 - logprior: -1.5993e+01
Epoch 4/10
10/10 - 1s - loss: 118.0228 - loglik: -1.0848e+02 - logprior: -9.5403e+00
Epoch 5/10
10/10 - 1s - loss: 109.6145 - loglik: -1.0364e+02 - logprior: -5.9696e+00
Epoch 6/10
10/10 - 1s - loss: 105.6356 - loglik: -1.0179e+02 - logprior: -3.8492e+00
Epoch 7/10
10/10 - 1s - loss: 103.6436 - loglik: -1.0109e+02 - logprior: -2.5497e+00
Epoch 8/10
10/10 - 0s - loss: 102.4925 - loglik: -1.0078e+02 - logprior: -1.7172e+00
Epoch 9/10
10/10 - 0s - loss: 101.8291 - loglik: -1.0071e+02 - logprior: -1.1191e+00
Epoch 10/10
10/10 - 1s - loss: 101.3852 - loglik: -1.0071e+02 - logprior: -6.7911e-01
Fitted a model with MAP estimate = -101.2022
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (23, 3), (25, 1), (31, 3)]
discards: [0]
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 240.7664 - loglik: -1.0190e+02 - logprior: -1.3887e+02
Epoch 2/2
10/10 - 1s - loss: 152.6858 - loglik: -9.5418e+01 - logprior: -5.7268e+01
Fitted a model with MAP estimate = -137.8383
expansions: [(0, 2)]
discards: [ 0  8 29]
Fitting a model of length 55 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 203.2475 - loglik: -9.1691e+01 - logprior: -1.1156e+02
Epoch 2/2
10/10 - 1s - loss: 119.6942 - loglik: -8.9879e+01 - logprior: -2.9816e+01
Fitted a model with MAP estimate = -107.2241
expansions: []
discards: [ 0 18]
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 221.9535 - loglik: -9.2692e+01 - logprior: -1.2926e+02
Epoch 2/10
10/10 - 0s - loss: 128.8719 - loglik: -9.1891e+01 - logprior: -3.6981e+01
Epoch 3/10
10/10 - 0s - loss: 105.9803 - loglik: -9.1798e+01 - logprior: -1.4183e+01
Epoch 4/10
10/10 - 0s - loss: 97.7957 - loglik: -9.1861e+01 - logprior: -5.9346e+00
Epoch 5/10
10/10 - 1s - loss: 93.7506 - loglik: -9.1953e+01 - logprior: -1.7976e+00
Epoch 6/10
10/10 - 1s - loss: 91.3799 - loglik: -9.1954e+01 - logprior: 0.5743
Epoch 7/10
10/10 - 0s - loss: 89.9498 - loglik: -9.1930e+01 - logprior: 1.9798
Epoch 8/10
10/10 - 1s - loss: 89.0530 - loglik: -9.1981e+01 - logprior: 2.9282
Epoch 9/10
10/10 - 1s - loss: 88.4145 - loglik: -9.2028e+01 - logprior: 3.6133
Epoch 10/10
10/10 - 1s - loss: 87.9052 - loglik: -9.2087e+01 - logprior: 4.1822
Fitted a model with MAP estimate = -87.6572
Time for alignment: 27.3575
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 280.9415 - loglik: -1.5677e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 0s - loss: 168.9647 - loglik: -1.3519e+02 - logprior: -3.3774e+01
Epoch 3/10
10/10 - 1s - loss: 134.1365 - loglik: -1.1814e+02 - logprior: -1.5993e+01
Epoch 4/10
10/10 - 0s - loss: 118.0228 - loglik: -1.0848e+02 - logprior: -9.5403e+00
Epoch 5/10
10/10 - 0s - loss: 109.6145 - loglik: -1.0364e+02 - logprior: -5.9696e+00
Epoch 6/10
10/10 - 0s - loss: 105.6357 - loglik: -1.0179e+02 - logprior: -3.8492e+00
Epoch 7/10
10/10 - 1s - loss: 103.6435 - loglik: -1.0109e+02 - logprior: -2.5497e+00
Epoch 8/10
10/10 - 1s - loss: 102.4925 - loglik: -1.0078e+02 - logprior: -1.7172e+00
Epoch 9/10
10/10 - 1s - loss: 101.8291 - loglik: -1.0071e+02 - logprior: -1.1191e+00
Epoch 10/10
10/10 - 1s - loss: 101.3852 - loglik: -1.0071e+02 - logprior: -6.7910e-01
Fitted a model with MAP estimate = -101.2020
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (23, 3), (25, 1), (31, 3)]
discards: [0]
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 240.7664 - loglik: -1.0190e+02 - logprior: -1.3887e+02
Epoch 2/2
10/10 - 1s - loss: 152.6858 - loglik: -9.5418e+01 - logprior: -5.7268e+01
Fitted a model with MAP estimate = -137.8383
expansions: [(0, 2)]
discards: [ 0  8 29]
Fitting a model of length 55 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 203.2475 - loglik: -9.1691e+01 - logprior: -1.1156e+02
Epoch 2/2
10/10 - 1s - loss: 119.6942 - loglik: -8.9879e+01 - logprior: -2.9816e+01
Fitted a model with MAP estimate = -107.2241
expansions: []
discards: [ 0 18]
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 221.9535 - loglik: -9.2692e+01 - logprior: -1.2926e+02
Epoch 2/10
10/10 - 1s - loss: 128.8719 - loglik: -9.1891e+01 - logprior: -3.6981e+01
Epoch 3/10
10/10 - 1s - loss: 105.9802 - loglik: -9.1798e+01 - logprior: -1.4183e+01
Epoch 4/10
10/10 - 1s - loss: 97.7956 - loglik: -9.1861e+01 - logprior: -5.9347e+00
Epoch 5/10
10/10 - 1s - loss: 93.7505 - loglik: -9.1953e+01 - logprior: -1.7976e+00
Epoch 6/10
10/10 - 1s - loss: 91.3797 - loglik: -9.1954e+01 - logprior: 0.5744
Epoch 7/10
10/10 - 1s - loss: 89.9498 - loglik: -9.1930e+01 - logprior: 1.9798
Epoch 8/10
10/10 - 0s - loss: 89.0530 - loglik: -9.1981e+01 - logprior: 2.9282
Epoch 9/10
10/10 - 1s - loss: 88.4145 - loglik: -9.2028e+01 - logprior: 3.6133
Epoch 10/10
10/10 - 1s - loss: 87.9052 - loglik: -9.2087e+01 - logprior: 4.1822
Fitted a model with MAP estimate = -87.6573
Time for alignment: 26.5975
Computed alignments with likelihoods: ['-87.6574', '-87.6572', '-87.6573']
Best model has likelihood: -87.6572
SP score = 0.8302
Training of 3 independent models on file biotin_lipoyl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34d218bee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f36990b18b0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 195.3544 - loglik: -1.9224e+02 - logprior: -3.1188e+00
Epoch 2/10
19/19 - 1s - loss: 155.9771 - loglik: -1.5475e+02 - logprior: -1.2315e+00
Epoch 3/10
19/19 - 1s - loss: 140.9794 - loglik: -1.3966e+02 - logprior: -1.3211e+00
Epoch 4/10
19/19 - 1s - loss: 138.9311 - loglik: -1.3765e+02 - logprior: -1.2795e+00
Epoch 5/10
19/19 - 1s - loss: 138.2216 - loglik: -1.3700e+02 - logprior: -1.2189e+00
Epoch 6/10
19/19 - 1s - loss: 137.8823 - loglik: -1.3669e+02 - logprior: -1.1928e+00
Epoch 7/10
19/19 - 1s - loss: 137.7636 - loglik: -1.3660e+02 - logprior: -1.1665e+00
Epoch 8/10
19/19 - 1s - loss: 137.6517 - loglik: -1.3649e+02 - logprior: -1.1594e+00
Epoch 9/10
19/19 - 1s - loss: 137.5994 - loglik: -1.3645e+02 - logprior: -1.1463e+00
Epoch 10/10
19/19 - 1s - loss: 137.5736 - loglik: -1.3643e+02 - logprior: -1.1456e+00
Fitted a model with MAP estimate = -137.9646
expansions: [(0, 3), (13, 2), (14, 2), (19, 1), (24, 1), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 141.7532 - loglik: -1.3742e+02 - logprior: -4.3285e+00
Epoch 2/2
19/19 - 1s - loss: 131.6504 - loglik: -1.3014e+02 - logprior: -1.5101e+00
Fitted a model with MAP estimate = -131.4412
expansions: [(0, 2)]
discards: [19 49 60 65]
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.7467 - loglik: -1.2984e+02 - logprior: -3.9097e+00
Epoch 2/2
19/19 - 1s - loss: 128.3255 - loglik: -1.2696e+02 - logprior: -1.3657e+00
Fitted a model with MAP estimate = -128.2651
expansions: []
discards: [0]
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 132.6433 - loglik: -1.2921e+02 - logprior: -3.4377e+00
Epoch 2/10
21/21 - 1s - loss: 129.1822 - loglik: -1.2747e+02 - logprior: -1.7167e+00
Epoch 3/10
21/21 - 1s - loss: 127.8289 - loglik: -1.2668e+02 - logprior: -1.1511e+00
Epoch 4/10
21/21 - 1s - loss: 127.2395 - loglik: -1.2617e+02 - logprior: -1.0728e+00
Epoch 5/10
21/21 - 1s - loss: 126.6505 - loglik: -1.2558e+02 - logprior: -1.0741e+00
Epoch 6/10
21/21 - 1s - loss: 126.5923 - loglik: -1.2553e+02 - logprior: -1.0585e+00
Epoch 7/10
21/21 - 1s - loss: 126.4571 - loglik: -1.2541e+02 - logprior: -1.0432e+00
Epoch 8/10
21/21 - 1s - loss: 126.3848 - loglik: -1.2536e+02 - logprior: -1.0234e+00
Epoch 9/10
21/21 - 1s - loss: 126.2098 - loglik: -1.2521e+02 - logprior: -1.0048e+00
Epoch 10/10
21/21 - 1s - loss: 126.3914 - loglik: -1.2541e+02 - logprior: -9.8219e-01
Fitted a model with MAP estimate = -126.1730
Time for alignment: 55.3439
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 195.4224 - loglik: -1.9230e+02 - logprior: -3.1205e+00
Epoch 2/10
19/19 - 1s - loss: 155.5025 - loglik: -1.5428e+02 - logprior: -1.2272e+00
Epoch 3/10
19/19 - 1s - loss: 142.0264 - loglik: -1.4072e+02 - logprior: -1.3020e+00
Epoch 4/10
19/19 - 1s - loss: 139.2065 - loglik: -1.3794e+02 - logprior: -1.2702e+00
Epoch 5/10
19/19 - 1s - loss: 138.5968 - loglik: -1.3738e+02 - logprior: -1.2128e+00
Epoch 6/10
19/19 - 1s - loss: 138.3383 - loglik: -1.3715e+02 - logprior: -1.1912e+00
Epoch 7/10
19/19 - 1s - loss: 138.2049 - loglik: -1.3704e+02 - logprior: -1.1677e+00
Epoch 8/10
19/19 - 1s - loss: 138.0863 - loglik: -1.3693e+02 - logprior: -1.1530e+00
Epoch 9/10
19/19 - 1s - loss: 138.0566 - loglik: -1.3691e+02 - logprior: -1.1437e+00
Epoch 10/10
19/19 - 1s - loss: 137.9245 - loglik: -1.3678e+02 - logprior: -1.1420e+00
Fitted a model with MAP estimate = -138.4353
expansions: [(0, 3), (13, 2), (14, 2), (15, 1), (20, 1), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 141.9787 - loglik: -1.3765e+02 - logprior: -4.3245e+00
Epoch 2/2
19/19 - 1s - loss: 131.8515 - loglik: -1.3034e+02 - logprior: -1.5130e+00
Fitted a model with MAP estimate = -131.7150
expansions: [(0, 2)]
discards: [19 49 60 65]
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 134.2430 - loglik: -1.3033e+02 - logprior: -3.9100e+00
Epoch 2/2
19/19 - 1s - loss: 128.6096 - loglik: -1.2722e+02 - logprior: -1.3921e+00
Fitted a model with MAP estimate = -128.4773
expansions: []
discards: [0]
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 132.8616 - loglik: -1.2945e+02 - logprior: -3.4119e+00
Epoch 2/10
21/21 - 1s - loss: 129.3575 - loglik: -1.2760e+02 - logprior: -1.7563e+00
Epoch 3/10
21/21 - 1s - loss: 127.8753 - loglik: -1.2671e+02 - logprior: -1.1663e+00
Epoch 4/10
21/21 - 1s - loss: 127.1292 - loglik: -1.2605e+02 - logprior: -1.0778e+00
Epoch 5/10
21/21 - 1s - loss: 126.8607 - loglik: -1.2579e+02 - logprior: -1.0658e+00
Epoch 6/10
21/21 - 1s - loss: 126.7229 - loglik: -1.2566e+02 - logprior: -1.0594e+00
Epoch 7/10
21/21 - 2s - loss: 126.4739 - loglik: -1.2543e+02 - logprior: -1.0400e+00
Epoch 8/10
21/21 - 1s - loss: 126.3177 - loglik: -1.2530e+02 - logprior: -1.0161e+00
Epoch 9/10
21/21 - 1s - loss: 126.3288 - loglik: -1.2532e+02 - logprior: -1.0041e+00
Fitted a model with MAP estimate = -126.1358
Time for alignment: 54.0920
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 195.2938 - loglik: -1.9218e+02 - logprior: -3.1179e+00
Epoch 2/10
19/19 - 1s - loss: 155.2458 - loglik: -1.5401e+02 - logprior: -1.2348e+00
Epoch 3/10
19/19 - 1s - loss: 140.9715 - loglik: -1.3966e+02 - logprior: -1.3147e+00
Epoch 4/10
19/19 - 1s - loss: 138.7853 - loglik: -1.3751e+02 - logprior: -1.2784e+00
Epoch 5/10
19/19 - 1s - loss: 138.1814 - loglik: -1.3696e+02 - logprior: -1.2235e+00
Epoch 6/10
19/19 - 1s - loss: 138.1022 - loglik: -1.3690e+02 - logprior: -1.1993e+00
Epoch 7/10
19/19 - 1s - loss: 137.7711 - loglik: -1.3660e+02 - logprior: -1.1715e+00
Epoch 8/10
19/19 - 1s - loss: 137.7692 - loglik: -1.3661e+02 - logprior: -1.1618e+00
Epoch 9/10
19/19 - 1s - loss: 137.5791 - loglik: -1.3643e+02 - logprior: -1.1460e+00
Epoch 10/10
19/19 - 1s - loss: 137.4997 - loglik: -1.3635e+02 - logprior: -1.1457e+00
Fitted a model with MAP estimate = -138.0786
expansions: [(0, 3), (13, 2), (14, 2), (19, 1), (20, 1), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 141.7486 - loglik: -1.3742e+02 - logprior: -4.3246e+00
Epoch 2/2
19/19 - 1s - loss: 131.4375 - loglik: -1.2993e+02 - logprior: -1.5124e+00
Fitted a model with MAP estimate = -131.5281
expansions: [(0, 2)]
discards: [19 49 60 65]
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.9025 - loglik: -1.3001e+02 - logprior: -3.8884e+00
Epoch 2/2
19/19 - 1s - loss: 128.4161 - loglik: -1.2700e+02 - logprior: -1.4190e+00
Fitted a model with MAP estimate = -128.4888
expansions: []
discards: [0]
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 6s - loss: 132.7281 - loglik: -1.2931e+02 - logprior: -3.4144e+00
Epoch 2/10
21/21 - 1s - loss: 129.3736 - loglik: -1.2755e+02 - logprior: -1.8252e+00
Epoch 3/10
21/21 - 1s - loss: 127.8008 - loglik: -1.2660e+02 - logprior: -1.1983e+00
Epoch 4/10
21/21 - 1s - loss: 127.3177 - loglik: -1.2625e+02 - logprior: -1.0640e+00
Epoch 5/10
21/21 - 1s - loss: 126.7654 - loglik: -1.2569e+02 - logprior: -1.0731e+00
Epoch 6/10
21/21 - 1s - loss: 126.4550 - loglik: -1.2539e+02 - logprior: -1.0601e+00
Epoch 7/10
21/21 - 1s - loss: 126.5799 - loglik: -1.2554e+02 - logprior: -1.0385e+00
Fitted a model with MAP estimate = -126.3360
Time for alignment: 51.6625
Computed alignments with likelihoods: ['-126.1730', '-126.1358', '-126.3360']
Best model has likelihood: -126.1358
SP score = 0.9420
Training of 3 independent models on file uce.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f35ed25ceb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32c807efa0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 390.9527 - loglik: -3.8284e+02 - logprior: -8.1168e+00
Epoch 2/10
13/13 - 2s - loss: 338.3250 - loglik: -3.3644e+02 - logprior: -1.8838e+00
Epoch 3/10
13/13 - 3s - loss: 297.7613 - loglik: -2.9596e+02 - logprior: -1.7963e+00
Epoch 4/10
13/13 - 2s - loss: 282.8048 - loglik: -2.8072e+02 - logprior: -2.0818e+00
Epoch 5/10
13/13 - 2s - loss: 277.8695 - loglik: -2.7577e+02 - logprior: -2.0994e+00
Epoch 6/10
13/13 - 2s - loss: 275.4331 - loglik: -2.7343e+02 - logprior: -2.0020e+00
Epoch 7/10
13/13 - 3s - loss: 275.4583 - loglik: -2.7349e+02 - logprior: -1.9663e+00
Fitted a model with MAP estimate = -274.7734
expansions: [(7, 2), (8, 3), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (35, 1), (36, 1), (37, 1), (38, 1), (50, 1), (51, 1), (70, 2), (75, 1), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 284.0092 - loglik: -2.7456e+02 - logprior: -9.4540e+00
Epoch 2/2
13/13 - 3s - loss: 264.9707 - loglik: -2.6118e+02 - logprior: -3.7954e+00
Fitted a model with MAP estimate = -261.5394
expansions: [(0, 3)]
discards: [0 9]
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 267.2111 - loglik: -2.5992e+02 - logprior: -7.2884e+00
Epoch 2/2
13/13 - 3s - loss: 257.0865 - loglik: -2.5539e+02 - logprior: -1.6977e+00
Fitted a model with MAP estimate = -255.6084
expansions: []
discards: [ 0  2 88]
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 269.7963 - loglik: -2.6065e+02 - logprior: -9.1429e+00
Epoch 2/10
13/13 - 4s - loss: 259.6085 - loglik: -2.5693e+02 - logprior: -2.6803e+00
Epoch 3/10
13/13 - 3s - loss: 256.3489 - loglik: -2.5539e+02 - logprior: -9.6067e-01
Epoch 4/10
13/13 - 4s - loss: 253.8925 - loglik: -2.5355e+02 - logprior: -3.4718e-01
Epoch 5/10
13/13 - 3s - loss: 253.5322 - loglik: -2.5332e+02 - logprior: -2.1307e-01
Epoch 6/10
13/13 - 4s - loss: 253.0773 - loglik: -2.5291e+02 - logprior: -1.6455e-01
Epoch 7/10
13/13 - 4s - loss: 252.1658 - loglik: -2.5201e+02 - logprior: -1.5712e-01
Epoch 8/10
13/13 - 4s - loss: 251.6521 - loglik: -2.5151e+02 - logprior: -1.4154e-01
Epoch 9/10
13/13 - 4s - loss: 250.8259 - loglik: -2.5071e+02 - logprior: -1.1305e-01
Epoch 10/10
13/13 - 4s - loss: 252.0551 - loglik: -2.5197e+02 - logprior: -8.1431e-02
Fitted a model with MAP estimate = -251.1220
Time for alignment: 93.7538
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 390.3715 - loglik: -3.8226e+02 - logprior: -8.1090e+00
Epoch 2/10
13/13 - 3s - loss: 337.4911 - loglik: -3.3562e+02 - logprior: -1.8671e+00
Epoch 3/10
13/13 - 3s - loss: 296.2135 - loglik: -2.9450e+02 - logprior: -1.7138e+00
Epoch 4/10
13/13 - 3s - loss: 282.3945 - loglik: -2.8043e+02 - logprior: -1.9619e+00
Epoch 5/10
13/13 - 3s - loss: 277.3711 - loglik: -2.7533e+02 - logprior: -2.0361e+00
Epoch 6/10
13/13 - 3s - loss: 274.4487 - loglik: -2.7247e+02 - logprior: -1.9749e+00
Epoch 7/10
13/13 - 3s - loss: 274.0885 - loglik: -2.7213e+02 - logprior: -1.9605e+00
Epoch 8/10
13/13 - 3s - loss: 273.7418 - loglik: -2.7178e+02 - logprior: -1.9612e+00
Epoch 9/10
13/13 - 3s - loss: 273.4143 - loglik: -2.7146e+02 - logprior: -1.9511e+00
Epoch 10/10
13/13 - 3s - loss: 274.0835 - loglik: -2.7213e+02 - logprior: -1.9503e+00
Fitted a model with MAP estimate = -273.2434
expansions: [(7, 2), (8, 2), (11, 1), (15, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (61, 2), (70, 4), (71, 1), (75, 2), (76, 3), (97, 1), (98, 2), (99, 3), (100, 1), (101, 1)]
discards: [0]
Fitting a model of length 145 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 286.0920 - loglik: -2.7662e+02 - logprior: -9.4768e+00
Epoch 2/2
13/13 - 4s - loss: 263.8802 - loglik: -2.5993e+02 - logprior: -3.9532e+00
Fitted a model with MAP estimate = -261.0512
expansions: [(0, 3)]
discards: [  0  77  88  89  90 101 130]
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 268.0525 - loglik: -2.6079e+02 - logprior: -7.2669e+00
Epoch 2/2
13/13 - 4s - loss: 257.5850 - loglik: -2.5590e+02 - logprior: -1.6815e+00
Fitted a model with MAP estimate = -255.8873
expansions: []
discards: [ 0  2 24]
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 269.4036 - loglik: -2.6026e+02 - logprior: -9.1395e+00
Epoch 2/10
13/13 - 3s - loss: 259.9436 - loglik: -2.5719e+02 - logprior: -2.7515e+00
Epoch 3/10
13/13 - 4s - loss: 256.8510 - loglik: -2.5589e+02 - logprior: -9.6396e-01
Epoch 4/10
13/13 - 4s - loss: 253.7518 - loglik: -2.5340e+02 - logprior: -3.5416e-01
Epoch 5/10
13/13 - 4s - loss: 253.6109 - loglik: -2.5339e+02 - logprior: -2.2344e-01
Epoch 6/10
13/13 - 4s - loss: 252.8198 - loglik: -2.5265e+02 - logprior: -1.7431e-01
Epoch 7/10
13/13 - 4s - loss: 251.8183 - loglik: -2.5165e+02 - logprior: -1.6685e-01
Epoch 8/10
13/13 - 4s - loss: 251.6938 - loglik: -2.5153e+02 - logprior: -1.6028e-01
Epoch 9/10
13/13 - 4s - loss: 251.5496 - loglik: -2.5141e+02 - logprior: -1.4230e-01
Epoch 10/10
13/13 - 4s - loss: 251.2762 - loglik: -2.5113e+02 - logprior: -1.4123e-01
Fitted a model with MAP estimate = -251.0111
Time for alignment: 107.5236
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 390.8434 - loglik: -3.8272e+02 - logprior: -8.1203e+00
Epoch 2/10
13/13 - 3s - loss: 336.5281 - loglik: -3.3466e+02 - logprior: -1.8701e+00
Epoch 3/10
13/13 - 3s - loss: 294.9039 - loglik: -2.9318e+02 - logprior: -1.7232e+00
Epoch 4/10
13/13 - 3s - loss: 281.9210 - loglik: -2.7994e+02 - logprior: -1.9790e+00
Epoch 5/10
13/13 - 3s - loss: 278.0792 - loglik: -2.7608e+02 - logprior: -2.0029e+00
Epoch 6/10
13/13 - 3s - loss: 276.1278 - loglik: -2.7417e+02 - logprior: -1.9577e+00
Epoch 7/10
13/13 - 3s - loss: 275.4042 - loglik: -2.7345e+02 - logprior: -1.9523e+00
Epoch 8/10
13/13 - 3s - loss: 274.6859 - loglik: -2.7273e+02 - logprior: -1.9608e+00
Epoch 9/10
13/13 - 3s - loss: 274.4516 - loglik: -2.7249e+02 - logprior: -1.9664e+00
Epoch 10/10
13/13 - 3s - loss: 274.7303 - loglik: -2.7277e+02 - logprior: -1.9614e+00
Fitted a model with MAP estimate = -274.2863
expansions: [(7, 2), (8, 2), (11, 1), (14, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 2), (53, 1), (70, 4), (71, 1), (75, 2), (76, 3), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Fitting a model of length 144 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 286.4781 - loglik: -2.7700e+02 - logprior: -9.4817e+00
Epoch 2/2
13/13 - 4s - loss: 263.7866 - loglik: -2.5987e+02 - logprior: -3.9181e+00
Fitted a model with MAP estimate = -261.0418
expansions: [(0, 3)]
discards: [  0  66  88  89  90 101]
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 266.8853 - loglik: -2.5960e+02 - logprior: -7.2855e+00
Epoch 2/2
13/13 - 4s - loss: 258.7602 - loglik: -2.5705e+02 - logprior: -1.7073e+00
Fitted a model with MAP estimate = -255.8872
expansions: []
discards: [0 2]
Fitting a model of length 139 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 268.8294 - loglik: -2.5965e+02 - logprior: -9.1771e+00
Epoch 2/10
13/13 - 3s - loss: 260.1746 - loglik: -2.5740e+02 - logprior: -2.7795e+00
Epoch 3/10
13/13 - 3s - loss: 255.9552 - loglik: -2.5494e+02 - logprior: -1.0135e+00
Epoch 4/10
13/13 - 4s - loss: 254.2765 - loglik: -2.5390e+02 - logprior: -3.7671e-01
Epoch 5/10
13/13 - 3s - loss: 253.6833 - loglik: -2.5343e+02 - logprior: -2.5721e-01
Epoch 6/10
13/13 - 4s - loss: 252.3932 - loglik: -2.5220e+02 - logprior: -1.9597e-01
Epoch 7/10
13/13 - 4s - loss: 251.8176 - loglik: -2.5163e+02 - logprior: -1.8551e-01
Epoch 8/10
13/13 - 3s - loss: 251.7750 - loglik: -2.5159e+02 - logprior: -1.8546e-01
Epoch 9/10
13/13 - 4s - loss: 251.3612 - loglik: -2.5121e+02 - logprior: -1.4917e-01
Epoch 10/10
13/13 - 4s - loss: 250.4430 - loglik: -2.5031e+02 - logprior: -1.3132e-01
Fitted a model with MAP estimate = -250.9321
Time for alignment: 105.2788
Computed alignments with likelihoods: ['-251.1220', '-251.0111', '-250.9321']
Best model has likelihood: -250.9321
SP score = 0.9559
Training of 3 independent models on file ghf1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2ce2e525e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f350c982f70>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 51s - loss: 1079.5256 - loglik: -1.0761e+03 - logprior: -3.4303e+00
Epoch 2/10
25/25 - 52s - loss: 814.7090 - loglik: -8.1359e+02 - logprior: -1.1227e+00
Epoch 3/10
25/25 - 51s - loss: 763.9177 - loglik: -7.6183e+02 - logprior: -2.0894e+00
Epoch 4/10
25/25 - 45s - loss: 754.9636 - loglik: -7.5279e+02 - logprior: -2.1754e+00
Epoch 5/10
25/25 - 43s - loss: 750.2614 - loglik: -7.4805e+02 - logprior: -2.2135e+00
Epoch 6/10
25/25 - 41s - loss: 748.0457 - loglik: -7.4587e+02 - logprior: -2.1781e+00
Epoch 7/10
25/25 - 42s - loss: 746.6780 - loglik: -7.4442e+02 - logprior: -2.2534e+00
Epoch 8/10
25/25 - 41s - loss: 748.4987 - loglik: -7.4629e+02 - logprior: -2.2053e+00
Fitted a model with MAP estimate = -746.1107
expansions: [(0, 2), (46, 1), (125, 1), (134, 1), (144, 1), (163, 1), (164, 2), (170, 1), (174, 5), (175, 3), (176, 1), (177, 1), (188, 1), (189, 1), (191, 3), (192, 1), (193, 1), (196, 1), (197, 1), (198, 1), (199, 1), (201, 1), (202, 1), (204, 1), (206, 1), (208, 1), (209, 1), (212, 1), (213, 1), (215, 1), (217, 1), (218, 1), (222, 1), (223, 1), (224, 1), (226, 1), (227, 1), (229, 1), (230, 1), (231, 1), (238, 2), (239, 1), (250, 1), (251, 1), (253, 1), (254, 2), (255, 4), (256, 2), (258, 5), (280, 2), (282, 1), (283, 1), (299, 1), (300, 1), (301, 2), (303, 1), (313, 1), (316, 1), (317, 1), (318, 1), (326, 1), (327, 2), (352, 1), (355, 1), (356, 1), (358, 1), (360, 1), (366, 1)]
discards: [353]
Fitting a model of length 460 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 52s - loss: 741.6498 - loglik: -7.3607e+02 - logprior: -5.5781e+00
Epoch 2/2
25/25 - 47s - loss: 704.0343 - loglik: -7.0413e+02 - logprior: 0.0974
Fitted a model with MAP estimate = -696.2469
expansions: [(412, 1)]
discards: [  2 187 214 314 315 316 317 322 323]
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 46s - loss: 705.7280 - loglik: -7.0322e+02 - logprior: -2.5075e+00
Epoch 2/2
25/25 - 43s - loss: 699.6498 - loglik: -7.0059e+02 - logprior: 0.9405
Fitted a model with MAP estimate = -694.8278
expansions: [(309, 1)]
discards: []
Fitting a model of length 453 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 47s - loss: 704.8688 - loglik: -7.0267e+02 - logprior: -2.1959e+00
Epoch 2/10
25/25 - 43s - loss: 697.5410 - loglik: -6.9934e+02 - logprior: 1.8029
Epoch 3/10
25/25 - 43s - loss: 693.0934 - loglik: -6.9567e+02 - logprior: 2.5741
Epoch 4/10
25/25 - 42s - loss: 690.3038 - loglik: -6.9315e+02 - logprior: 2.8479
Epoch 5/10
25/25 - 42s - loss: 691.6174 - loglik: -6.9466e+02 - logprior: 3.0419
Fitted a model with MAP estimate = -688.1661
Time for alignment: 928.3979
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 33s - loss: 1075.8433 - loglik: -1.0725e+03 - logprior: -3.3753e+00
Epoch 2/10
25/25 - 30s - loss: 816.9704 - loglik: -8.1616e+02 - logprior: -8.0605e-01
Epoch 3/10
25/25 - 30s - loss: 763.7737 - loglik: -7.6187e+02 - logprior: -1.9025e+00
Epoch 4/10
25/25 - 30s - loss: 748.2823 - loglik: -7.4617e+02 - logprior: -2.1150e+00
Epoch 5/10
25/25 - 30s - loss: 751.4981 - loglik: -7.4935e+02 - logprior: -2.1437e+00
Fitted a model with MAP estimate = -747.3968
expansions: [(50, 1), (135, 1), (144, 1), (145, 1), (146, 1), (162, 1), (164, 1), (170, 1), (174, 3), (175, 2), (176, 2), (179, 1), (190, 1), (191, 1), (192, 3), (193, 1), (194, 2), (197, 1), (198, 1), (199, 1), (200, 1), (202, 1), (203, 1), (205, 1), (207, 1), (209, 1), (210, 1), (211, 1), (212, 1), (213, 1), (218, 1), (224, 1), (225, 1), (226, 3), (227, 1), (228, 1), (230, 1), (231, 1), (232, 1), (237, 1), (239, 1), (240, 1), (251, 1), (253, 1), (254, 1), (257, 2), (258, 3), (260, 2), (267, 3), (268, 1), (280, 1), (282, 1), (283, 1), (299, 1), (300, 2), (301, 1), (304, 1), (318, 2), (319, 2), (325, 2), (327, 2), (352, 2), (354, 1), (358, 1), (360, 1), (366, 1)]
discards: []
Fitting a model of length 457 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 46s - loss: 732.9991 - loglik: -7.2897e+02 - logprior: -4.0326e+00
Epoch 2/2
25/25 - 43s - loss: 703.0770 - loglik: -7.0266e+02 - logprior: -4.2010e-01
Fitted a model with MAP estimate = -696.0272
expansions: [(404, 1)]
discards: [184 211 212 216 266 307 370 433 434]
Fitting a model of length 449 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 45s - loss: 709.4053 - loglik: -7.0682e+02 - logprior: -2.5805e+00
Epoch 2/2
25/25 - 41s - loss: 700.3423 - loglik: -7.0176e+02 - logprior: 1.4210
Fitted a model with MAP estimate = -697.0774
expansions: [(210, 2)]
discards: []
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 45s - loss: 708.5912 - loglik: -7.0649e+02 - logprior: -2.1053e+00
Epoch 2/10
25/25 - 42s - loss: 696.1092 - loglik: -6.9804e+02 - logprior: 1.9314
Epoch 3/10
25/25 - 42s - loss: 693.7380 - loglik: -6.9627e+02 - logprior: 2.5284
Epoch 4/10
25/25 - 42s - loss: 694.2049 - loglik: -6.9702e+02 - logprior: 2.8196
Fitted a model with MAP estimate = -690.8401
Time for alignment: 646.9043
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 34s - loss: 1078.7748 - loglik: -1.0754e+03 - logprior: -3.3756e+00
Epoch 2/10
25/25 - 30s - loss: 821.9897 - loglik: -8.2118e+02 - logprior: -8.0652e-01
Epoch 3/10
25/25 - 30s - loss: 766.2827 - loglik: -7.6441e+02 - logprior: -1.8738e+00
Epoch 4/10
25/25 - 30s - loss: 754.8397 - loglik: -7.5281e+02 - logprior: -2.0293e+00
Epoch 5/10
25/25 - 30s - loss: 751.7544 - loglik: -7.4976e+02 - logprior: -1.9964e+00
Epoch 6/10
25/25 - 30s - loss: 748.8019 - loglik: -7.4680e+02 - logprior: -1.9996e+00
Epoch 7/10
25/25 - 30s - loss: 750.1666 - loglik: -7.4816e+02 - logprior: -2.0067e+00
Fitted a model with MAP estimate = -748.3320
expansions: [(126, 1), (145, 1), (146, 1), (164, 1), (165, 1), (170, 1), (171, 1), (175, 5), (176, 2), (177, 2), (178, 1), (179, 1), (192, 1), (193, 1), (194, 1), (195, 2), (196, 1), (198, 1), (199, 1), (200, 2), (201, 1), (203, 1), (204, 1), (206, 1), (208, 1), (210, 1), (211, 1), (212, 1), (213, 1), (214, 1), (219, 1), (220, 1), (224, 1), (225, 1), (226, 2), (227, 1), (228, 1), (229, 2), (230, 1), (231, 1), (238, 3), (239, 2), (250, 1), (251, 1), (252, 1), (255, 3), (256, 2), (258, 3), (280, 2), (281, 1), (282, 1), (299, 1), (300, 1), (301, 2), (303, 1), (313, 1), (316, 1), (322, 2), (325, 2), (327, 2), (350, 1), (353, 1), (358, 1), (360, 1), (363, 1)]
discards: []
Fitting a model of length 458 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 46s - loss: 735.9760 - loglik: -7.3171e+02 - logprior: -4.2695e+00
Epoch 2/2
25/25 - 43s - loss: 702.0712 - loglik: -7.0159e+02 - logprior: -4.7840e-01
Fitted a model with MAP estimate = -696.8929
expansions: [(404, 1), (436, 1)]
discards: [184 185 186 216 290 309 400 438]
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 45s - loss: 710.6570 - loglik: -7.0775e+02 - logprior: -2.9119e+00
Epoch 2/2
25/25 - 42s - loss: 697.1891 - loglik: -6.9836e+02 - logprior: 1.1716
Fitted a model with MAP estimate = -694.8340
expansions: [(184, 3)]
discards: [430]
Fitting a model of length 454 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 46s - loss: 706.6599 - loglik: -7.0471e+02 - logprior: -1.9507e+00
Epoch 2/10
25/25 - 42s - loss: 697.5804 - loglik: -6.9956e+02 - logprior: 1.9771
Epoch 3/10
25/25 - 42s - loss: 694.3926 - loglik: -6.9697e+02 - logprior: 2.5792
Epoch 4/10
25/25 - 42s - loss: 690.2676 - loglik: -6.9247e+02 - logprior: 2.2041
Epoch 5/10
25/25 - 42s - loss: 690.2986 - loglik: -6.9317e+02 - logprior: 2.8680
Fitted a model with MAP estimate = -688.9090
Time for alignment: 752.8223
Computed alignments with likelihoods: ['-688.1661', '-690.8401', '-688.9090']
Best model has likelihood: -688.1661
SP score = 0.9117
Training of 3 independent models on file ldh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f350d43d910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34b87ab8b0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 326.3397 - loglik: -3.2186e+02 - logprior: -4.4834e+00
Epoch 2/10
16/16 - 3s - loss: 252.5575 - loglik: -2.5111e+02 - logprior: -1.4443e+00
Epoch 3/10
16/16 - 4s - loss: 222.7766 - loglik: -2.2097e+02 - logprior: -1.8068e+00
Epoch 4/10
16/16 - 4s - loss: 215.3751 - loglik: -2.1365e+02 - logprior: -1.7216e+00
Epoch 5/10
16/16 - 3s - loss: 207.9737 - loglik: -2.0631e+02 - logprior: -1.6686e+00
Epoch 6/10
16/16 - 4s - loss: 204.0587 - loglik: -2.0237e+02 - logprior: -1.6887e+00
Epoch 7/10
16/16 - 4s - loss: 202.1205 - loglik: -2.0045e+02 - logprior: -1.6719e+00
Epoch 8/10
16/16 - 4s - loss: 200.1443 - loglik: -1.9846e+02 - logprior: -1.6833e+00
Epoch 9/10
16/16 - 3s - loss: 199.9020 - loglik: -1.9821e+02 - logprior: -1.6912e+00
Epoch 10/10
16/16 - 4s - loss: 199.0188 - loglik: -1.9733e+02 - logprior: -1.6897e+00
Fitted a model with MAP estimate = -199.2664
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 3), (25, 4), (57, 1), (81, 1), (97, 1)]
discards: [0 2]
Fitting a model of length 124 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 216.4963 - loglik: -2.1081e+02 - logprior: -5.6847e+00
Epoch 2/2
16/16 - 4s - loss: 200.3537 - loglik: -1.9751e+02 - logprior: -2.8443e+00
Fitted a model with MAP estimate = -198.7818
expansions: [(0, 2), (19, 1)]
discards: [ 0 31 32 33]
Fitting a model of length 123 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 6s - loss: 203.0573 - loglik: -1.9856e+02 - logprior: -4.4976e+00
Epoch 2/2
16/16 - 4s - loss: 195.5250 - loglik: -1.9380e+02 - logprior: -1.7223e+00
Fitted a model with MAP estimate = -194.7205
expansions: []
discards: []
Fitting a model of length 123 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 9s - loss: 199.8175 - loglik: -1.9523e+02 - logprior: -4.5874e+00
Epoch 2/10
16/16 - 4s - loss: 196.5845 - loglik: -1.9490e+02 - logprior: -1.6858e+00
Epoch 3/10
16/16 - 4s - loss: 192.6311 - loglik: -1.9119e+02 - logprior: -1.4449e+00
Epoch 4/10
16/16 - 4s - loss: 193.0120 - loglik: -1.9165e+02 - logprior: -1.3590e+00
Fitted a model with MAP estimate = -192.2023
Time for alignment: 99.0208
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 326.8402 - loglik: -3.2234e+02 - logprior: -4.5048e+00
Epoch 2/10
16/16 - 4s - loss: 254.0880 - loglik: -2.5262e+02 - logprior: -1.4656e+00
Epoch 3/10
16/16 - 4s - loss: 221.5355 - loglik: -2.1981e+02 - logprior: -1.7239e+00
Epoch 4/10
16/16 - 3s - loss: 207.5071 - loglik: -2.0584e+02 - logprior: -1.6688e+00
Epoch 5/10
16/16 - 4s - loss: 205.1851 - loglik: -2.0352e+02 - logprior: -1.6630e+00
Epoch 6/10
16/16 - 3s - loss: 204.0021 - loglik: -2.0234e+02 - logprior: -1.6652e+00
Epoch 7/10
16/16 - 4s - loss: 202.4904 - loglik: -2.0083e+02 - logprior: -1.6572e+00
Epoch 8/10
16/16 - 4s - loss: 202.1405 - loglik: -2.0050e+02 - logprior: -1.6450e+00
Epoch 9/10
16/16 - 4s - loss: 201.5083 - loglik: -1.9986e+02 - logprior: -1.6441e+00
Epoch 10/10
16/16 - 4s - loss: 200.2936 - loglik: -1.9865e+02 - logprior: -1.6413e+00
Fitted a model with MAP estimate = -200.6182
expansions: [(0, 1), (13, 1), (15, 1), (16, 2), (17, 1), (71, 1), (72, 1), (73, 1), (97, 1), (98, 1)]
discards: []
Fitting a model of length 123 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 210.6446 - loglik: -2.0527e+02 - logprior: -5.3696e+00
Epoch 2/2
16/16 - 4s - loss: 196.8540 - loglik: -1.9506e+02 - logprior: -1.7972e+00
Fitted a model with MAP estimate = -195.4377
expansions: []
discards: []
Fitting a model of length 123 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 200.9599 - loglik: -1.9635e+02 - logprior: -4.6114e+00
Epoch 2/10
16/16 - 4s - loss: 195.9661 - loglik: -1.9422e+02 - logprior: -1.7465e+00
Epoch 3/10
16/16 - 4s - loss: 193.3871 - loglik: -1.9189e+02 - logprior: -1.4922e+00
Epoch 4/10
16/16 - 4s - loss: 191.9117 - loglik: -1.9052e+02 - logprior: -1.3967e+00
Epoch 5/10
16/16 - 4s - loss: 191.4355 - loglik: -1.9009e+02 - logprior: -1.3469e+00
Epoch 6/10
16/16 - 4s - loss: 189.5102 - loglik: -1.8821e+02 - logprior: -1.3022e+00
Epoch 7/10
16/16 - 4s - loss: 189.8102 - loglik: -1.8857e+02 - logprior: -1.2415e+00
Fitted a model with MAP estimate = -189.1936
Time for alignment: 94.7423
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 325.9175 - loglik: -3.2141e+02 - logprior: -4.5026e+00
Epoch 2/10
16/16 - 3s - loss: 253.7805 - loglik: -2.5231e+02 - logprior: -1.4694e+00
Epoch 3/10
16/16 - 4s - loss: 222.4321 - loglik: -2.2058e+02 - logprior: -1.8499e+00
Epoch 4/10
16/16 - 4s - loss: 208.9034 - loglik: -2.0712e+02 - logprior: -1.7853e+00
Epoch 5/10
16/16 - 4s - loss: 203.8421 - loglik: -2.0217e+02 - logprior: -1.6770e+00
Epoch 6/10
16/16 - 4s - loss: 201.9607 - loglik: -2.0026e+02 - logprior: -1.6971e+00
Epoch 7/10
16/16 - 4s - loss: 200.3892 - loglik: -1.9871e+02 - logprior: -1.6795e+00
Epoch 8/10
16/16 - 4s - loss: 198.6780 - loglik: -1.9701e+02 - logprior: -1.6664e+00
Epoch 9/10
16/16 - 3s - loss: 198.0187 - loglik: -1.9635e+02 - logprior: -1.6684e+00
Epoch 10/10
16/16 - 4s - loss: 196.2780 - loglik: -1.9461e+02 - logprior: -1.6675e+00
Fitted a model with MAP estimate = -197.4439
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 3), (17, 1), (72, 1), (73, 1), (97, 1), (98, 1)]
discards: [0 2]
Fitting a model of length 122 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 211.9542 - loglik: -2.0637e+02 - logprior: -5.5826e+00
Epoch 2/2
16/16 - 4s - loss: 200.5365 - loglik: -1.9782e+02 - logprior: -2.7135e+00
Fitted a model with MAP estimate = -196.7318
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 123 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 200.2131 - loglik: -1.9582e+02 - logprior: -4.3963e+00
Epoch 2/2
16/16 - 4s - loss: 193.4393 - loglik: -1.9182e+02 - logprior: -1.6182e+00
Fitted a model with MAP estimate = -192.5279
expansions: []
discards: [24]
Fitting a model of length 122 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 198.9140 - loglik: -1.9445e+02 - logprior: -4.4679e+00
Epoch 2/10
16/16 - 4s - loss: 192.9879 - loglik: -1.9137e+02 - logprior: -1.6182e+00
Epoch 3/10
16/16 - 4s - loss: 193.0437 - loglik: -1.9169e+02 - logprior: -1.3580e+00
Fitted a model with MAP estimate = -191.4690
Time for alignment: 94.0548
Computed alignments with likelihoods: ['-192.2023', '-189.1936', '-191.4690']
Best model has likelihood: -189.1936
SP score = 0.5259
Training of 3 independent models on file Rhodanese.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32a9d82b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34a5f4f1f0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 308.5375 - loglik: -3.0541e+02 - logprior: -3.1320e+00
Epoch 2/10
19/19 - 2s - loss: 275.7481 - loglik: -2.7460e+02 - logprior: -1.1509e+00
Epoch 3/10
19/19 - 2s - loss: 259.7387 - loglik: -2.5827e+02 - logprior: -1.4704e+00
Epoch 4/10
19/19 - 2s - loss: 255.3108 - loglik: -2.5390e+02 - logprior: -1.4115e+00
Epoch 5/10
19/19 - 2s - loss: 253.5562 - loglik: -2.5220e+02 - logprior: -1.3516e+00
Epoch 6/10
19/19 - 2s - loss: 252.1956 - loglik: -2.5087e+02 - logprior: -1.3274e+00
Epoch 7/10
19/19 - 2s - loss: 251.5357 - loglik: -2.5023e+02 - logprior: -1.3069e+00
Epoch 8/10
19/19 - 2s - loss: 250.9168 - loglik: -2.4962e+02 - logprior: -1.3011e+00
Epoch 9/10
19/19 - 2s - loss: 250.4926 - loglik: -2.4919e+02 - logprior: -1.3005e+00
Epoch 10/10
19/19 - 2s - loss: 250.2894 - loglik: -2.4899e+02 - logprior: -1.2957e+00
Fitted a model with MAP estimate = -239.0831
expansions: [(6, 3), (7, 2), (10, 1), (21, 2), (33, 9), (38, 2), (39, 1), (42, 2), (58, 2), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 275.1848 - loglik: -2.7127e+02 - logprior: -3.9136e+00
Epoch 2/2
19/19 - 3s - loss: 253.8723 - loglik: -2.5174e+02 - logprior: -2.1339e+00
Fitted a model with MAP estimate = -235.5460
expansions: [(0, 2)]
discards: [ 0  8 27 55 80 87]
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 252.0066 - loglik: -2.4912e+02 - logprior: -2.8871e+00
Epoch 2/2
19/19 - 3s - loss: 247.3383 - loglik: -2.4622e+02 - logprior: -1.1199e+00
Fitted a model with MAP estimate = -232.4806
expansions: [(42, 3)]
discards: [0]
Fitting a model of length 106 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 236.0551 - loglik: -2.3345e+02 - logprior: -2.6022e+00
Epoch 2/10
23/23 - 3s - loss: 231.2370 - loglik: -2.3015e+02 - logprior: -1.0912e+00
Epoch 3/10
23/23 - 3s - loss: 228.8354 - loglik: -2.2782e+02 - logprior: -1.0133e+00
Epoch 4/10
23/23 - 3s - loss: 228.1221 - loglik: -2.2715e+02 - logprior: -9.7038e-01
Epoch 5/10
23/23 - 3s - loss: 226.2259 - loglik: -2.2528e+02 - logprior: -9.4521e-01
Epoch 6/10
23/23 - 3s - loss: 224.8043 - loglik: -2.2386e+02 - logprior: -9.4141e-01
Epoch 7/10
23/23 - 3s - loss: 223.9753 - loglik: -2.2304e+02 - logprior: -9.3156e-01
Epoch 8/10
23/23 - 3s - loss: 222.3514 - loglik: -2.2142e+02 - logprior: -9.3513e-01
Epoch 9/10
23/23 - 3s - loss: 222.1993 - loglik: -2.2127e+02 - logprior: -9.2649e-01
Epoch 10/10
23/23 - 3s - loss: 221.4348 - loglik: -2.2052e+02 - logprior: -9.1200e-01
Fitted a model with MAP estimate = -221.2469
Time for alignment: 92.0081
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 308.3700 - loglik: -3.0524e+02 - logprior: -3.1328e+00
Epoch 2/10
19/19 - 2s - loss: 275.9362 - loglik: -2.7477e+02 - logprior: -1.1688e+00
Epoch 3/10
19/19 - 2s - loss: 260.8225 - loglik: -2.5936e+02 - logprior: -1.4596e+00
Epoch 4/10
19/19 - 2s - loss: 255.9049 - loglik: -2.5450e+02 - logprior: -1.4009e+00
Epoch 5/10
19/19 - 2s - loss: 253.4930 - loglik: -2.5214e+02 - logprior: -1.3556e+00
Epoch 6/10
19/19 - 2s - loss: 252.1949 - loglik: -2.5087e+02 - logprior: -1.3257e+00
Epoch 7/10
19/19 - 2s - loss: 251.9226 - loglik: -2.5061e+02 - logprior: -1.3096e+00
Epoch 8/10
19/19 - 2s - loss: 251.1729 - loglik: -2.4987e+02 - logprior: -1.3003e+00
Epoch 9/10
19/19 - 2s - loss: 250.5638 - loglik: -2.4927e+02 - logprior: -1.2952e+00
Epoch 10/10
19/19 - 2s - loss: 250.6525 - loglik: -2.4936e+02 - logprior: -1.2938e+00
Fitted a model with MAP estimate = -239.4784
expansions: [(6, 3), (7, 2), (10, 1), (34, 9), (39, 2), (40, 1), (43, 2), (55, 2), (58, 2), (60, 1), (63, 2), (65, 2), (66, 1), (67, 1), (69, 1)]
discards: [0]
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 273.4686 - loglik: -2.6954e+02 - logprior: -3.9303e+00
Epoch 2/2
19/19 - 3s - loss: 252.8824 - loglik: -2.5073e+02 - logprior: -2.1484e+00
Fitted a model with MAP estimate = -235.6251
expansions: [(0, 2)]
discards: [ 0  8 43 44 45 54 75 80 92]
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 252.9082 - loglik: -2.5002e+02 - logprior: -2.8871e+00
Epoch 2/2
19/19 - 2s - loss: 248.3351 - loglik: -2.4723e+02 - logprior: -1.1098e+00
Fitted a model with MAP estimate = -233.1963
expansions: [(42, 4)]
discards: [ 0 82]
Fitting a model of length 104 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 236.8244 - loglik: -2.3423e+02 - logprior: -2.5993e+00
Epoch 2/10
23/23 - 3s - loss: 232.1016 - loglik: -2.3099e+02 - logprior: -1.1115e+00
Epoch 3/10
23/23 - 3s - loss: 229.2788 - loglik: -2.2825e+02 - logprior: -1.0251e+00
Epoch 4/10
23/23 - 3s - loss: 228.1250 - loglik: -2.2715e+02 - logprior: -9.7245e-01
Epoch 5/10
23/23 - 3s - loss: 226.2609 - loglik: -2.2530e+02 - logprior: -9.6097e-01
Epoch 6/10
23/23 - 3s - loss: 225.2692 - loglik: -2.2433e+02 - logprior: -9.4260e-01
Epoch 7/10
23/23 - 3s - loss: 224.0174 - loglik: -2.2308e+02 - logprior: -9.3908e-01
Epoch 8/10
23/23 - 3s - loss: 223.0736 - loglik: -2.2214e+02 - logprior: -9.3557e-01
Epoch 9/10
23/23 - 3s - loss: 222.1055 - loglik: -2.2118e+02 - logprior: -9.3022e-01
Epoch 10/10
23/23 - 3s - loss: 221.8459 - loglik: -2.2093e+02 - logprior: -9.1932e-01
Fitted a model with MAP estimate = -221.6274
Time for alignment: 91.1224
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 308.4082 - loglik: -3.0528e+02 - logprior: -3.1321e+00
Epoch 2/10
19/19 - 2s - loss: 276.5298 - loglik: -2.7536e+02 - logprior: -1.1672e+00
Epoch 3/10
19/19 - 2s - loss: 260.3185 - loglik: -2.5885e+02 - logprior: -1.4699e+00
Epoch 4/10
19/19 - 2s - loss: 255.9133 - loglik: -2.5450e+02 - logprior: -1.4099e+00
Epoch 5/10
19/19 - 2s - loss: 253.7221 - loglik: -2.5237e+02 - logprior: -1.3533e+00
Epoch 6/10
19/19 - 2s - loss: 252.4171 - loglik: -2.5109e+02 - logprior: -1.3287e+00
Epoch 7/10
19/19 - 2s - loss: 252.0170 - loglik: -2.5070e+02 - logprior: -1.3135e+00
Epoch 8/10
19/19 - 2s - loss: 251.5729 - loglik: -2.5026e+02 - logprior: -1.3089e+00
Epoch 9/10
19/19 - 2s - loss: 250.2125 - loglik: -2.4891e+02 - logprior: -1.3055e+00
Epoch 10/10
19/19 - 2s - loss: 250.2567 - loglik: -2.4895e+02 - logprior: -1.3068e+00
Fitted a model with MAP estimate = -239.3004
expansions: [(6, 3), (7, 2), (8, 1), (10, 1), (29, 1), (33, 10), (38, 2), (39, 1), (42, 2), (58, 2), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 275.1983 - loglik: -2.7129e+02 - logprior: -3.9087e+00
Epoch 2/2
19/19 - 3s - loss: 253.3034 - loglik: -2.5117e+02 - logprior: -2.1301e+00
Fitted a model with MAP estimate = -235.2843
expansions: [(0, 2)]
discards: [ 0  8  9 44 45 46 47 56 81 88]
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 254.1378 - loglik: -2.5127e+02 - logprior: -2.8638e+00
Epoch 2/2
19/19 - 2s - loss: 248.5062 - loglik: -2.4742e+02 - logprior: -1.0910e+00
Fitted a model with MAP estimate = -233.2889
expansions: []
discards: [0]
Fitting a model of length 100 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 236.2960 - loglik: -2.3375e+02 - logprior: -2.5418e+00
Epoch 2/10
23/23 - 3s - loss: 232.5937 - loglik: -2.3153e+02 - logprior: -1.0614e+00
Epoch 3/10
23/23 - 3s - loss: 230.3807 - loglik: -2.2937e+02 - logprior: -1.0063e+00
Epoch 4/10
23/23 - 3s - loss: 228.8982 - loglik: -2.2794e+02 - logprior: -9.6175e-01
Epoch 5/10
23/23 - 3s - loss: 227.5469 - loglik: -2.2660e+02 - logprior: -9.4508e-01
Epoch 6/10
23/23 - 3s - loss: 225.9910 - loglik: -2.2506e+02 - logprior: -9.3334e-01
Epoch 7/10
23/23 - 3s - loss: 224.9590 - loglik: -2.2402e+02 - logprior: -9.3444e-01
Epoch 8/10
23/23 - 3s - loss: 223.9006 - loglik: -2.2297e+02 - logprior: -9.3341e-01
Epoch 9/10
23/23 - 3s - loss: 222.8595 - loglik: -2.2194e+02 - logprior: -9.2314e-01
Epoch 10/10
23/23 - 3s - loss: 222.7003 - loglik: -2.2179e+02 - logprior: -9.0959e-01
Fitted a model with MAP estimate = -222.4681
Time for alignment: 90.8840
Computed alignments with likelihoods: ['-221.2469', '-221.6274', '-222.4681']
Best model has likelihood: -221.2469
SP score = 0.8031
Training of 3 independent models on file slectin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32e829be20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34f4405340>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 432.2487 - loglik: -3.8677e+02 - logprior: -4.5482e+01
Epoch 2/10
10/10 - 2s - loss: 368.5034 - loglik: -3.5821e+02 - logprior: -1.0292e+01
Epoch 3/10
10/10 - 2s - loss: 330.5046 - loglik: -3.2643e+02 - logprior: -4.0712e+00
Epoch 4/10
10/10 - 2s - loss: 310.7893 - loglik: -3.0880e+02 - logprior: -1.9875e+00
Epoch 5/10
10/10 - 2s - loss: 301.0565 - loglik: -3.0016e+02 - logprior: -9.0114e-01
Epoch 6/10
10/10 - 2s - loss: 297.5886 - loglik: -2.9724e+02 - logprior: -3.5353e-01
Epoch 7/10
10/10 - 2s - loss: 294.7183 - loglik: -2.9469e+02 - logprior: -2.8026e-02
Epoch 8/10
10/10 - 2s - loss: 294.3895 - loglik: -2.9461e+02 - logprior: 0.2165
Epoch 9/10
10/10 - 2s - loss: 293.7352 - loglik: -2.9413e+02 - logprior: 0.3917
Epoch 10/10
10/10 - 2s - loss: 292.9988 - loglik: -2.9352e+02 - logprior: 0.5227
Fitted a model with MAP estimate = -292.9122
expansions: [(7, 2), (10, 1), (17, 1), (18, 1), (26, 2), (27, 3), (42, 1), (58, 2), (59, 1), (62, 2), (73, 1), (85, 1), (92, 6), (93, 1)]
discards: [0]
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 347.5132 - loglik: -2.9701e+02 - logprior: -5.0503e+01
Epoch 2/2
10/10 - 2s - loss: 305.2206 - loglik: -2.8655e+02 - logprior: -1.8668e+01
Fitted a model with MAP estimate = -297.5705
expansions: [(9, 3)]
discards: [69]
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 331.2770 - loglik: -2.8304e+02 - logprior: -4.8239e+01
Epoch 2/2
10/10 - 2s - loss: 292.8800 - loglik: -2.7986e+02 - logprior: -1.3017e+01
Fitted a model with MAP estimate = -283.5961
expansions: []
discards: []
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 318.9218 - loglik: -2.7917e+02 - logprior: -3.9747e+01
Epoch 2/10
10/10 - 2s - loss: 285.3521 - loglik: -2.7783e+02 - logprior: -7.5256e+00
Epoch 3/10
10/10 - 2s - loss: 277.9938 - loglik: -2.7711e+02 - logprior: -8.8696e-01
Epoch 4/10
10/10 - 2s - loss: 274.8818 - loglik: -2.7667e+02 - logprior: 1.7915
Epoch 5/10
10/10 - 2s - loss: 272.2689 - loglik: -2.7542e+02 - logprior: 3.1466
Epoch 6/10
10/10 - 2s - loss: 271.8057 - loglik: -2.7567e+02 - logprior: 3.8605
Epoch 7/10
10/10 - 2s - loss: 271.0217 - loglik: -2.7538e+02 - logprior: 4.3602
Epoch 8/10
10/10 - 2s - loss: 269.7392 - loglik: -2.7457e+02 - logprior: 4.8346
Epoch 9/10
10/10 - 2s - loss: 270.1595 - loglik: -2.7540e+02 - logprior: 5.2371
Fitted a model with MAP estimate = -269.4135
Time for alignment: 66.1656
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 432.1698 - loglik: -3.8669e+02 - logprior: -4.5483e+01
Epoch 2/10
10/10 - 2s - loss: 368.9677 - loglik: -3.5868e+02 - logprior: -1.0287e+01
Epoch 3/10
10/10 - 2s - loss: 330.7671 - loglik: -3.2669e+02 - logprior: -4.0759e+00
Epoch 4/10
10/10 - 2s - loss: 309.3842 - loglik: -3.0747e+02 - logprior: -1.9116e+00
Epoch 5/10
10/10 - 2s - loss: 300.3806 - loglik: -2.9955e+02 - logprior: -8.3387e-01
Epoch 6/10
10/10 - 2s - loss: 296.8905 - loglik: -2.9655e+02 - logprior: -3.4369e-01
Epoch 7/10
10/10 - 2s - loss: 295.3690 - loglik: -2.9538e+02 - logprior: 0.0121
Epoch 8/10
10/10 - 2s - loss: 293.6267 - loglik: -2.9391e+02 - logprior: 0.2879
Epoch 9/10
10/10 - 2s - loss: 293.2549 - loglik: -2.9374e+02 - logprior: 0.4871
Epoch 10/10
10/10 - 2s - loss: 292.8705 - loglik: -2.9348e+02 - logprior: 0.6068
Fitted a model with MAP estimate = -292.6762
expansions: [(7, 2), (10, 1), (17, 1), (18, 1), (26, 2), (27, 3), (49, 2), (59, 3), (62, 3), (73, 1), (85, 1), (92, 6), (93, 1)]
discards: [0]
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 348.4046 - loglik: -2.9787e+02 - logprior: -5.0534e+01
Epoch 2/2
10/10 - 2s - loss: 305.4348 - loglik: -2.8648e+02 - logprior: -1.8957e+01
Fitted a model with MAP estimate = -297.7530
expansions: [(9, 3)]
discards: [58 71 77]
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 331.9732 - loglik: -2.8367e+02 - logprior: -4.8304e+01
Epoch 2/2
10/10 - 2s - loss: 292.1151 - loglik: -2.7907e+02 - logprior: -1.3050e+01
Fitted a model with MAP estimate = -283.3217
expansions: []
discards: [36]
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 319.5771 - loglik: -2.7982e+02 - logprior: -3.9762e+01
Epoch 2/10
10/10 - 2s - loss: 284.8543 - loglik: -2.7728e+02 - logprior: -7.5696e+00
Epoch 3/10
10/10 - 2s - loss: 278.3880 - loglik: -2.7743e+02 - logprior: -9.5841e-01
Epoch 4/10
10/10 - 2s - loss: 274.3191 - loglik: -2.7604e+02 - logprior: 1.7247
Epoch 5/10
10/10 - 2s - loss: 272.5767 - loglik: -2.7566e+02 - logprior: 3.0874
Epoch 6/10
10/10 - 2s - loss: 271.3101 - loglik: -2.7512e+02 - logprior: 3.8098
Epoch 7/10
10/10 - 2s - loss: 270.3268 - loglik: -2.7464e+02 - logprior: 4.3114
Epoch 8/10
10/10 - 2s - loss: 270.6741 - loglik: -2.7545e+02 - logprior: 4.7764
Fitted a model with MAP estimate = -269.7509
Time for alignment: 61.4836
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 432.3717 - loglik: -3.8689e+02 - logprior: -4.5481e+01
Epoch 2/10
10/10 - 2s - loss: 368.6463 - loglik: -3.5836e+02 - logprior: -1.0282e+01
Epoch 3/10
10/10 - 2s - loss: 330.5941 - loglik: -3.2657e+02 - logprior: -4.0284e+00
Epoch 4/10
10/10 - 2s - loss: 308.5140 - loglik: -3.0660e+02 - logprior: -1.9098e+00
Epoch 5/10
10/10 - 2s - loss: 300.0671 - loglik: -2.9917e+02 - logprior: -9.0035e-01
Epoch 6/10
10/10 - 2s - loss: 296.8129 - loglik: -2.9652e+02 - logprior: -2.8900e-01
Epoch 7/10
10/10 - 2s - loss: 295.3858 - loglik: -2.9543e+02 - logprior: 0.0466
Epoch 8/10
10/10 - 2s - loss: 293.5566 - loglik: -2.9378e+02 - logprior: 0.2227
Epoch 9/10
10/10 - 2s - loss: 293.3044 - loglik: -2.9368e+02 - logprior: 0.3755
Epoch 10/10
10/10 - 2s - loss: 293.0820 - loglik: -2.9361e+02 - logprior: 0.5277
Fitted a model with MAP estimate = -292.7249
expansions: [(7, 2), (10, 1), (17, 1), (18, 1), (27, 1), (28, 3), (48, 1), (58, 2), (59, 1), (62, 3), (73, 1), (85, 1), (92, 6), (93, 1)]
discards: [0]
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 347.6857 - loglik: -2.9712e+02 - logprior: -5.0570e+01
Epoch 2/2
10/10 - 2s - loss: 305.3438 - loglik: -2.8649e+02 - logprior: -1.8853e+01
Fitted a model with MAP estimate = -297.8786
expansions: [(9, 3)]
discards: [33 68 75]
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 332.5793 - loglik: -2.8430e+02 - logprior: -4.8275e+01
Epoch 2/2
10/10 - 2s - loss: 292.2049 - loglik: -2.7922e+02 - logprior: -1.2988e+01
Fitted a model with MAP estimate = -283.9087
expansions: []
discards: []
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 319.8943 - loglik: -2.8013e+02 - logprior: -3.9768e+01
Epoch 2/10
10/10 - 2s - loss: 285.2610 - loglik: -2.7768e+02 - logprior: -7.5841e+00
Epoch 3/10
10/10 - 2s - loss: 278.4565 - loglik: -2.7751e+02 - logprior: -9.4418e-01
Epoch 4/10
10/10 - 2s - loss: 275.4033 - loglik: -2.7713e+02 - logprior: 1.7315
Epoch 5/10
10/10 - 2s - loss: 273.0045 - loglik: -2.7609e+02 - logprior: 3.0829
Epoch 6/10
10/10 - 2s - loss: 272.2982 - loglik: -2.7609e+02 - logprior: 3.7944
Epoch 7/10
10/10 - 2s - loss: 271.4047 - loglik: -2.7571e+02 - logprior: 4.3007
Epoch 8/10
10/10 - 2s - loss: 270.7567 - loglik: -2.7553e+02 - logprior: 4.7760
Epoch 9/10
10/10 - 2s - loss: 270.1181 - loglik: -2.7530e+02 - logprior: 5.1818
Epoch 10/10
10/10 - 2s - loss: 269.9464 - loglik: -2.7543e+02 - logprior: 5.4865
Fitted a model with MAP estimate = -269.5442
Time for alignment: 65.7577
Computed alignments with likelihoods: ['-269.4135', '-269.7509', '-269.5442']
Best model has likelihood: -269.4135
SP score = 0.8902
Training of 3 independent models on file hip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f350d4a7fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2cef23a850>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 452.9295 - loglik: -1.8602e+02 - logprior: -2.6691e+02
Epoch 2/10
10/10 - 1s - loss: 230.3375 - loglik: -1.6043e+02 - logprior: -6.9912e+01
Epoch 3/10
10/10 - 1s - loss: 170.3563 - loglik: -1.3968e+02 - logprior: -3.0673e+01
Epoch 4/10
10/10 - 1s - loss: 141.8501 - loglik: -1.2527e+02 - logprior: -1.6577e+01
Epoch 5/10
10/10 - 1s - loss: 127.8468 - loglik: -1.1902e+02 - logprior: -8.8277e+00
Epoch 6/10
10/10 - 1s - loss: 120.3214 - loglik: -1.1663e+02 - logprior: -3.6875e+00
Epoch 7/10
10/10 - 1s - loss: 115.5618 - loglik: -1.1518e+02 - logprior: -3.8420e-01
Epoch 8/10
10/10 - 1s - loss: 112.5275 - loglik: -1.1428e+02 - logprior: 1.7490
Epoch 9/10
10/10 - 1s - loss: 110.5294 - loglik: -1.1381e+02 - logprior: 3.2847
Epoch 10/10
10/10 - 1s - loss: 109.2640 - loglik: -1.1373e+02 - logprior: 4.4649
Fitted a model with MAP estimate = -108.7434
expansions: [(0, 3), (10, 1), (17, 1), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 464.7389 - loglik: -1.0970e+02 - logprior: -3.5503e+02
Epoch 2/2
10/10 - 1s - loss: 203.2529 - loglik: -9.6962e+01 - logprior: -1.0629e+02
Fitted a model with MAP estimate = -154.2835
expansions: []
discards: [ 0 36 47 52]
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 397.7095 - loglik: -9.7399e+01 - logprior: -3.0031e+02
Epoch 2/2
10/10 - 1s - loss: 209.5400 - loglik: -9.5263e+01 - logprior: -1.1428e+02
Fitted a model with MAP estimate = -179.4177
expansions: []
discards: []
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 369.6625 - loglik: -9.4876e+01 - logprior: -2.7479e+02
Epoch 2/10
10/10 - 1s - loss: 166.7136 - loglik: -9.3703e+01 - logprior: -7.3011e+01
Epoch 3/10
10/10 - 1s - loss: 115.4244 - loglik: -9.3315e+01 - logprior: -2.2109e+01
Epoch 4/10
10/10 - 1s - loss: 96.7874 - loglik: -9.3183e+01 - logprior: -3.6039e+00
Epoch 5/10
10/10 - 1s - loss: 87.3511 - loglik: -9.3280e+01 - logprior: 5.9286
Epoch 6/10
10/10 - 1s - loss: 81.8362 - loglik: -9.3368e+01 - logprior: 11.5321
Epoch 7/10
10/10 - 1s - loss: 78.3497 - loglik: -9.3437e+01 - logprior: 15.0871
Epoch 8/10
10/10 - 1s - loss: 75.9217 - loglik: -9.3520e+01 - logprior: 17.5986
Epoch 9/10
10/10 - 1s - loss: 74.0685 - loglik: -9.3605e+01 - logprior: 19.5365
Epoch 10/10
10/10 - 1s - loss: 72.5315 - loglik: -9.3671e+01 - logprior: 21.1397
Fitted a model with MAP estimate = -71.7650
Time for alignment: 29.3854
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 452.9295 - loglik: -1.8602e+02 - logprior: -2.6691e+02
Epoch 2/10
10/10 - 1s - loss: 230.3375 - loglik: -1.6043e+02 - logprior: -6.9912e+01
Epoch 3/10
10/10 - 1s - loss: 170.3564 - loglik: -1.3968e+02 - logprior: -3.0673e+01
Epoch 4/10
10/10 - 1s - loss: 141.8502 - loglik: -1.2527e+02 - logprior: -1.6577e+01
Epoch 5/10
10/10 - 1s - loss: 127.8467 - loglik: -1.1902e+02 - logprior: -8.8277e+00
Epoch 6/10
10/10 - 1s - loss: 120.3214 - loglik: -1.1663e+02 - logprior: -3.6875e+00
Epoch 7/10
10/10 - 1s - loss: 115.5617 - loglik: -1.1518e+02 - logprior: -3.8420e-01
Epoch 8/10
10/10 - 1s - loss: 112.5276 - loglik: -1.1428e+02 - logprior: 1.7490
Epoch 9/10
10/10 - 1s - loss: 110.5295 - loglik: -1.1381e+02 - logprior: 3.2847
Epoch 10/10
10/10 - 1s - loss: 109.2641 - loglik: -1.1373e+02 - logprior: 4.4649
Fitted a model with MAP estimate = -108.7433
expansions: [(0, 3), (10, 1), (17, 1), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 464.7389 - loglik: -1.0970e+02 - logprior: -3.5503e+02
Epoch 2/2
10/10 - 1s - loss: 203.2529 - loglik: -9.6962e+01 - logprior: -1.0629e+02
Fitted a model with MAP estimate = -154.2835
expansions: []
discards: [ 0 36 47 52]
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 397.7094 - loglik: -9.7399e+01 - logprior: -3.0031e+02
Epoch 2/2
10/10 - 1s - loss: 209.5400 - loglik: -9.5263e+01 - logprior: -1.1428e+02
Fitted a model with MAP estimate = -179.4178
expansions: []
discards: []
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 369.6625 - loglik: -9.4876e+01 - logprior: -2.7479e+02
Epoch 2/10
10/10 - 1s - loss: 166.7136 - loglik: -9.3703e+01 - logprior: -7.3011e+01
Epoch 3/10
10/10 - 1s - loss: 115.4244 - loglik: -9.3315e+01 - logprior: -2.2109e+01
Epoch 4/10
10/10 - 1s - loss: 96.7874 - loglik: -9.3183e+01 - logprior: -3.6039e+00
Epoch 5/10
10/10 - 1s - loss: 87.3511 - loglik: -9.3280e+01 - logprior: 5.9285
Epoch 6/10
10/10 - 1s - loss: 81.8362 - loglik: -9.3368e+01 - logprior: 11.5321
Epoch 7/10
10/10 - 1s - loss: 78.3498 - loglik: -9.3437e+01 - logprior: 15.0870
Epoch 8/10
10/10 - 1s - loss: 75.9217 - loglik: -9.3520e+01 - logprior: 17.5986
Epoch 9/10
10/10 - 1s - loss: 74.0686 - loglik: -9.3605e+01 - logprior: 19.5365
Epoch 10/10
10/10 - 1s - loss: 72.5316 - loglik: -9.3671e+01 - logprior: 21.1396
Fitted a model with MAP estimate = -71.7649
Time for alignment: 29.5388
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 452.9295 - loglik: -1.8602e+02 - logprior: -2.6691e+02
Epoch 2/10
10/10 - 1s - loss: 230.3375 - loglik: -1.6043e+02 - logprior: -6.9912e+01
Epoch 3/10
10/10 - 1s - loss: 170.3563 - loglik: -1.3968e+02 - logprior: -3.0673e+01
Epoch 4/10
10/10 - 1s - loss: 141.8501 - loglik: -1.2527e+02 - logprior: -1.6577e+01
Epoch 5/10
10/10 - 1s - loss: 127.8467 - loglik: -1.1902e+02 - logprior: -8.8277e+00
Epoch 6/10
10/10 - 1s - loss: 120.3214 - loglik: -1.1663e+02 - logprior: -3.6875e+00
Epoch 7/10
10/10 - 1s - loss: 115.5618 - loglik: -1.1518e+02 - logprior: -3.8420e-01
Epoch 8/10
10/10 - 1s - loss: 112.5275 - loglik: -1.1428e+02 - logprior: 1.7491
Epoch 9/10
10/10 - 1s - loss: 110.5294 - loglik: -1.1381e+02 - logprior: 3.2847
Epoch 10/10
10/10 - 1s - loss: 109.2641 - loglik: -1.1373e+02 - logprior: 4.4649
Fitted a model with MAP estimate = -108.7431
expansions: [(0, 3), (10, 1), (17, 1), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 464.7389 - loglik: -1.0970e+02 - logprior: -3.5503e+02
Epoch 2/2
10/10 - 1s - loss: 203.2529 - loglik: -9.6962e+01 - logprior: -1.0629e+02
Fitted a model with MAP estimate = -154.2835
expansions: []
discards: [ 0 36 47 52]
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 397.7095 - loglik: -9.7399e+01 - logprior: -3.0031e+02
Epoch 2/2
10/10 - 1s - loss: 209.5400 - loglik: -9.5263e+01 - logprior: -1.1428e+02
Fitted a model with MAP estimate = -179.4179
expansions: []
discards: []
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 369.6626 - loglik: -9.4876e+01 - logprior: -2.7479e+02
Epoch 2/10
10/10 - 1s - loss: 166.7136 - loglik: -9.3703e+01 - logprior: -7.3011e+01
Epoch 3/10
10/10 - 1s - loss: 115.4243 - loglik: -9.3315e+01 - logprior: -2.2109e+01
Epoch 4/10
10/10 - 1s - loss: 96.7873 - loglik: -9.3183e+01 - logprior: -3.6039e+00
Epoch 5/10
10/10 - 1s - loss: 87.3512 - loglik: -9.3280e+01 - logprior: 5.9286
Epoch 6/10
10/10 - 1s - loss: 81.8361 - loglik: -9.3368e+01 - logprior: 11.5321
Epoch 7/10
10/10 - 1s - loss: 78.3498 - loglik: -9.3437e+01 - logprior: 15.0871
Epoch 8/10
10/10 - 1s - loss: 75.9217 - loglik: -9.3520e+01 - logprior: 17.5987
Epoch 9/10
10/10 - 1s - loss: 74.0684 - loglik: -9.3605e+01 - logprior: 19.5365
Epoch 10/10
10/10 - 1s - loss: 72.5316 - loglik: -9.3671e+01 - logprior: 21.1397
Fitted a model with MAP estimate = -71.7651
Time for alignment: 28.9413
Computed alignments with likelihoods: ['-71.7650', '-71.7649', '-71.7651']
Best model has likelihood: -71.7649
SP score = 0.8474
Training of 3 independent models on file peroxidase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34af696bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f367f1ab130>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 200 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 11s - loss: 733.4387 - loglik: -7.2640e+02 - logprior: -7.0344e+00
Epoch 2/10
13/13 - 8s - loss: 643.3656 - loglik: -6.4232e+02 - logprior: -1.0453e+00
Epoch 3/10
13/13 - 8s - loss: 590.7886 - loglik: -5.8954e+02 - logprior: -1.2484e+00
Epoch 4/10
13/13 - 8s - loss: 577.5432 - loglik: -5.7613e+02 - logprior: -1.4120e+00
Epoch 5/10
13/13 - 8s - loss: 570.6005 - loglik: -5.6934e+02 - logprior: -1.2624e+00
Epoch 6/10
13/13 - 8s - loss: 571.0981 - loglik: -5.6992e+02 - logprior: -1.1762e+00
Fitted a model with MAP estimate = -569.4426
expansions: [(174, 1)]
discards: []
Fitting a model of length 201 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 11s - loss: 580.3839 - loglik: -5.7304e+02 - logprior: -7.3419e+00
Epoch 2/2
13/13 - 8s - loss: 574.1770 - loglik: -5.7238e+02 - logprior: -1.7974e+00
Fitted a model with MAP estimate = -571.2668
expansions: []
discards: []
Fitting a model of length 201 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 11s - loss: 577.6382 - loglik: -5.7036e+02 - logprior: -7.2774e+00
Epoch 2/10
13/13 - 8s - loss: 573.2936 - loglik: -5.7158e+02 - logprior: -1.7144e+00
Epoch 3/10
13/13 - 8s - loss: 569.9093 - loglik: -5.6873e+02 - logprior: -1.1786e+00
Epoch 4/10
13/13 - 8s - loss: 569.6366 - loglik: -5.6872e+02 - logprior: -9.2087e-01
Epoch 5/10
13/13 - 8s - loss: 569.5059 - loglik: -5.6864e+02 - logprior: -8.6990e-01
Epoch 6/10
13/13 - 8s - loss: 566.2740 - loglik: -5.6542e+02 - logprior: -8.5615e-01
Epoch 7/10
13/13 - 8s - loss: 566.7269 - loglik: -5.6588e+02 - logprior: -8.4729e-01
Fitted a model with MAP estimate = -566.7264
Time for alignment: 154.9102
Fitting a model of length 200 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 11s - loss: 733.7516 - loglik: -7.2671e+02 - logprior: -7.0377e+00
Epoch 2/10
13/13 - 8s - loss: 643.7070 - loglik: -6.4270e+02 - logprior: -1.0039e+00
Epoch 3/10
13/13 - 8s - loss: 591.9996 - loglik: -5.9106e+02 - logprior: -9.3891e-01
Epoch 4/10
13/13 - 8s - loss: 577.0064 - loglik: -5.7589e+02 - logprior: -1.1126e+00
Epoch 5/10
13/13 - 8s - loss: 572.7608 - loglik: -5.7167e+02 - logprior: -1.0875e+00
Epoch 6/10
13/13 - 8s - loss: 571.1114 - loglik: -5.7010e+02 - logprior: -1.0101e+00
Epoch 7/10
13/13 - 8s - loss: 568.1833 - loglik: -5.6715e+02 - logprior: -1.0374e+00
Epoch 8/10
13/13 - 8s - loss: 570.0030 - loglik: -5.6893e+02 - logprior: -1.0715e+00
Fitted a model with MAP estimate = -568.6992
expansions: [(11, 1), (55, 1), (95, 2), (173, 1)]
discards: []
Fitting a model of length 205 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 11s - loss: 582.0147 - loglik: -5.7482e+02 - logprior: -7.1929e+00
Epoch 2/2
13/13 - 8s - loss: 570.0491 - loglik: -5.6839e+02 - logprior: -1.6609e+00
Fitted a model with MAP estimate = -569.2113
expansions: []
discards: [ 0 97]
Fitting a model of length 203 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 11s - loss: 581.9388 - loglik: -5.7263e+02 - logprior: -9.3057e+00
Epoch 2/2
13/13 - 8s - loss: 574.9018 - loglik: -5.7118e+02 - logprior: -3.7239e+00
Fitted a model with MAP estimate = -572.7338
expansions: [(0, 5)]
discards: [0]
Fitting a model of length 207 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 11s - loss: 579.2931 - loglik: -5.7214e+02 - logprior: -7.1517e+00
Epoch 2/10
13/13 - 9s - loss: 570.4986 - loglik: -5.6907e+02 - logprior: -1.4269e+00
Epoch 3/10
13/13 - 9s - loss: 568.9937 - loglik: -5.6808e+02 - logprior: -9.1181e-01
Epoch 4/10
13/13 - 9s - loss: 567.2820 - loglik: -5.6644e+02 - logprior: -8.4045e-01
Epoch 5/10
13/13 - 9s - loss: 565.8318 - loglik: -5.6515e+02 - logprior: -6.8126e-01
Epoch 6/10
13/13 - 9s - loss: 564.9438 - loglik: -5.6430e+02 - logprior: -6.3902e-01
Epoch 7/10
13/13 - 9s - loss: 563.7919 - loglik: -5.6319e+02 - logprior: -6.0306e-01
Epoch 8/10
13/13 - 8s - loss: 564.9142 - loglik: -5.6436e+02 - logprior: -5.5423e-01
Fitted a model with MAP estimate = -563.8175
Time for alignment: 217.7661
Fitting a model of length 200 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 11s - loss: 731.8546 - loglik: -7.2482e+02 - logprior: -7.0355e+00
Epoch 2/10
13/13 - 8s - loss: 644.8103 - loglik: -6.4375e+02 - logprior: -1.0643e+00
Epoch 3/10
13/13 - 8s - loss: 590.7273 - loglik: -5.8955e+02 - logprior: -1.1775e+00
Epoch 4/10
13/13 - 8s - loss: 579.1062 - loglik: -5.7767e+02 - logprior: -1.4317e+00
Epoch 5/10
13/13 - 8s - loss: 573.7371 - loglik: -5.7236e+02 - logprior: -1.3736e+00
Epoch 6/10
13/13 - 8s - loss: 570.9222 - loglik: -5.6960e+02 - logprior: -1.3230e+00
Epoch 7/10
13/13 - 8s - loss: 568.9199 - loglik: -5.6759e+02 - logprior: -1.3290e+00
Epoch 8/10
13/13 - 8s - loss: 568.2020 - loglik: -5.6684e+02 - logprior: -1.3573e+00
Epoch 9/10
13/13 - 8s - loss: 568.3139 - loglik: -5.6695e+02 - logprior: -1.3633e+00
Fitted a model with MAP estimate = -568.4098
expansions: [(108, 3), (174, 2)]
discards: [1]
Fitting a model of length 204 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 11s - loss: 583.0396 - loglik: -5.7550e+02 - logprior: -7.5438e+00
Epoch 2/2
13/13 - 8s - loss: 572.9368 - loglik: -5.7081e+02 - logprior: -2.1285e+00
Fitted a model with MAP estimate = -570.1655
expansions: []
discards: [109]
Fitting a model of length 203 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 13s - loss: 578.1884 - loglik: -5.7070e+02 - logprior: -7.4885e+00
Epoch 2/2
13/13 - 8s - loss: 571.3186 - loglik: -5.6937e+02 - logprior: -1.9503e+00
Fitted a model with MAP estimate = -569.7123
expansions: []
discards: []
Fitting a model of length 203 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 11s - loss: 577.8305 - loglik: -5.7045e+02 - logprior: -7.3772e+00
Epoch 2/10
13/13 - 8s - loss: 570.8508 - loglik: -5.6904e+02 - logprior: -1.8157e+00
Epoch 3/10
13/13 - 8s - loss: 568.1442 - loglik: -5.6688e+02 - logprior: -1.2620e+00
Epoch 4/10
13/13 - 8s - loss: 567.8457 - loglik: -5.6687e+02 - logprior: -9.7321e-01
Epoch 5/10
13/13 - 8s - loss: 565.9371 - loglik: -5.6501e+02 - logprior: -9.2294e-01
Epoch 6/10
13/13 - 8s - loss: 566.5065 - loglik: -5.6561e+02 - logprior: -8.9436e-01
Fitted a model with MAP estimate = -565.4644
Time for alignment: 205.7242
Computed alignments with likelihoods: ['-566.7264', '-563.8175', '-565.4644']
Best model has likelihood: -563.8175
SP score = 0.6213
Training of 3 independent models on file TNF.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f33204a55e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f36a9d35c10>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 421.9284 - loglik: -3.4519e+02 - logprior: -7.6736e+01
Epoch 2/10
10/10 - 1s - loss: 335.3614 - loglik: -3.1778e+02 - logprior: -1.7577e+01
Epoch 3/10
10/10 - 1s - loss: 295.4626 - loglik: -2.8914e+02 - logprior: -6.3244e+00
Epoch 4/10
10/10 - 1s - loss: 275.7024 - loglik: -2.7343e+02 - logprior: -2.2685e+00
Epoch 5/10
10/10 - 1s - loss: 268.1136 - loglik: -2.6794e+02 - logprior: -1.7262e-01
Epoch 6/10
10/10 - 1s - loss: 263.5222 - loglik: -2.6436e+02 - logprior: 0.8405
Epoch 7/10
10/10 - 1s - loss: 261.0124 - loglik: -2.6252e+02 - logprior: 1.5051
Epoch 8/10
10/10 - 1s - loss: 259.4744 - loglik: -2.6147e+02 - logprior: 1.9971
Epoch 9/10
10/10 - 1s - loss: 258.8287 - loglik: -2.6122e+02 - logprior: 2.3963
Epoch 10/10
10/10 - 1s - loss: 258.3159 - loglik: -2.6099e+02 - logprior: 2.6733
Fitted a model with MAP estimate = -257.9569
expansions: [(5, 2), (6, 1), (7, 1), (10, 1), (12, 3), (19, 2), (36, 4), (45, 2), (48, 3), (63, 4), (83, 3), (88, 4), (89, 1)]
discards: [0]
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 350.4812 - loglik: -2.6511e+02 - logprior: -8.5372e+01
Epoch 2/2
10/10 - 2s - loss: 281.8195 - loglik: -2.4929e+02 - logprior: -3.2531e+01
Fitted a model with MAP estimate = -269.1226
expansions: [(0, 4), (17, 1), (60, 1), (111, 3)]
discards: [  0   5  47  48  81 107]
Fitting a model of length 130 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 313.5669 - loglik: -2.4638e+02 - logprior: -6.7188e+01
Epoch 2/2
10/10 - 2s - loss: 252.0067 - loglik: -2.3834e+02 - logprior: -1.3670e+01
Fitted a model with MAP estimate = -242.2347
expansions: []
discards: [1 2 3]
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 304.7210 - loglik: -2.3949e+02 - logprior: -6.5230e+01
Epoch 2/10
10/10 - 2s - loss: 249.7242 - loglik: -2.3688e+02 - logprior: -1.2840e+01
Epoch 3/10
10/10 - 2s - loss: 237.6895 - loglik: -2.3574e+02 - logprior: -1.9445e+00
Epoch 4/10
10/10 - 2s - loss: 232.2783 - loglik: -2.3505e+02 - logprior: 2.7679
Epoch 5/10
10/10 - 2s - loss: 229.4186 - loglik: -2.3487e+02 - logprior: 5.4485
Epoch 6/10
10/10 - 2s - loss: 227.7522 - loglik: -2.3480e+02 - logprior: 7.0498
Epoch 7/10
10/10 - 2s - loss: 226.1428 - loglik: -2.3426e+02 - logprior: 8.1137
Epoch 8/10
10/10 - 2s - loss: 225.5765 - loglik: -2.3442e+02 - logprior: 8.8455
Epoch 9/10
10/10 - 2s - loss: 224.9923 - loglik: -2.3439e+02 - logprior: 9.3961
Epoch 10/10
10/10 - 2s - loss: 224.0915 - loglik: -2.3394e+02 - logprior: 9.8469
Fitted a model with MAP estimate = -223.9824
Time for alignment: 51.1157
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 421.7911 - loglik: -3.4505e+02 - logprior: -7.6736e+01
Epoch 2/10
10/10 - 1s - loss: 336.4114 - loglik: -3.1883e+02 - logprior: -1.7581e+01
Epoch 3/10
10/10 - 1s - loss: 296.8487 - loglik: -2.9047e+02 - logprior: -6.3769e+00
Epoch 4/10
10/10 - 1s - loss: 276.9953 - loglik: -2.7454e+02 - logprior: -2.4558e+00
Epoch 5/10
10/10 - 1s - loss: 269.2961 - loglik: -2.6884e+02 - logprior: -4.5824e-01
Epoch 6/10
10/10 - 1s - loss: 264.6967 - loglik: -2.6522e+02 - logprior: 0.5227
Epoch 7/10
10/10 - 1s - loss: 262.1144 - loglik: -2.6323e+02 - logprior: 1.1126
Epoch 8/10
10/10 - 1s - loss: 260.7544 - loglik: -2.6232e+02 - logprior: 1.5678
Epoch 9/10
10/10 - 1s - loss: 259.6362 - loglik: -2.6165e+02 - logprior: 2.0129
Epoch 10/10
10/10 - 1s - loss: 258.7494 - loglik: -2.6097e+02 - logprior: 2.2206
Fitted a model with MAP estimate = -258.2292
expansions: [(5, 2), (6, 1), (10, 1), (12, 2), (13, 1), (15, 1), (19, 2), (36, 4), (45, 3), (83, 3), (88, 5), (89, 1)]
discards: [0]
Fitting a model of length 122 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 352.5869 - loglik: -2.6688e+02 - logprior: -8.5710e+01
Epoch 2/2
10/10 - 2s - loss: 284.5834 - loglik: -2.5203e+02 - logprior: -3.2554e+01
Fitted a model with MAP estimate = -272.0602
expansions: [(0, 3), (79, 3), (107, 2), (109, 1)]
discards: [  0  47  48 101]
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 316.2778 - loglik: -2.4892e+02 - logprior: -6.7354e+01
Epoch 2/2
10/10 - 2s - loss: 254.5240 - loglik: -2.4067e+02 - logprior: -1.3850e+01
Fitted a model with MAP estimate = -244.8747
expansions: []
discards: [0 1]
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 323.7222 - loglik: -2.4255e+02 - logprior: -8.1171e+01
Epoch 2/10
10/10 - 2s - loss: 261.7140 - loglik: -2.3995e+02 - logprior: -2.1760e+01
Epoch 3/10
10/10 - 2s - loss: 242.4940 - loglik: -2.3846e+02 - logprior: -4.0339e+00
Epoch 4/10
10/10 - 2s - loss: 235.2463 - loglik: -2.3753e+02 - logprior: 2.2820
Epoch 5/10
10/10 - 2s - loss: 231.3911 - loglik: -2.3656e+02 - logprior: 5.1724
Epoch 6/10
10/10 - 2s - loss: 229.3636 - loglik: -2.3613e+02 - logprior: 6.7676
Epoch 7/10
10/10 - 2s - loss: 227.7483 - loglik: -2.3551e+02 - logprior: 7.7612
Epoch 8/10
10/10 - 2s - loss: 227.1513 - loglik: -2.3561e+02 - logprior: 8.4632
Epoch 9/10
10/10 - 2s - loss: 226.7901 - loglik: -2.3584e+02 - logprior: 9.0514
Epoch 10/10
10/10 - 2s - loss: 226.0030 - loglik: -2.3557e+02 - logprior: 9.5679
Fitted a model with MAP estimate = -225.7159
Time for alignment: 50.3284
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 421.7069 - loglik: -3.4497e+02 - logprior: -7.6735e+01
Epoch 2/10
10/10 - 1s - loss: 336.1660 - loglik: -3.1858e+02 - logprior: -1.7582e+01
Epoch 3/10
10/10 - 1s - loss: 295.2550 - loglik: -2.8893e+02 - logprior: -6.3300e+00
Epoch 4/10
10/10 - 1s - loss: 275.1973 - loglik: -2.7284e+02 - logprior: -2.3534e+00
Epoch 5/10
10/10 - 1s - loss: 267.0500 - loglik: -2.6668e+02 - logprior: -3.7271e-01
Epoch 6/10
10/10 - 1s - loss: 263.1391 - loglik: -2.6381e+02 - logprior: 0.6758
Epoch 7/10
10/10 - 1s - loss: 260.5901 - loglik: -2.6202e+02 - logprior: 1.4285
Epoch 8/10
10/10 - 1s - loss: 259.3933 - loglik: -2.6137e+02 - logprior: 1.9726
Epoch 9/10
10/10 - 1s - loss: 258.8148 - loglik: -2.6116e+02 - logprior: 2.3492
Epoch 10/10
10/10 - 1s - loss: 258.1103 - loglik: -2.6078e+02 - logprior: 2.6707
Fitted a model with MAP estimate = -257.7972
expansions: [(5, 2), (6, 2), (11, 3), (12, 1), (19, 2), (36, 4), (45, 3), (48, 2), (63, 3), (83, 2), (86, 1), (87, 6), (89, 1)]
discards: [0]
Fitting a model of length 128 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 349.0955 - loglik: -2.6392e+02 - logprior: -8.5179e+01
Epoch 2/2
10/10 - 2s - loss: 279.6941 - loglik: -2.4770e+02 - logprior: -3.1998e+01
Fitted a model with MAP estimate = -267.0921
expansions: []
discards: [ 0 14 47 48]
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 330.1310 - loglik: -2.4630e+02 - logprior: -8.3836e+01
Epoch 2/2
10/10 - 2s - loss: 270.4225 - loglik: -2.4064e+02 - logprior: -2.9784e+01
Fitted a model with MAP estimate = -258.5619
expansions: [(6, 1)]
discards: [0 1]
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 327.6542 - loglik: -2.4349e+02 - logprior: -8.4163e+01
Epoch 2/10
10/10 - 2s - loss: 272.1842 - loglik: -2.4092e+02 - logprior: -3.1265e+01
Epoch 3/10
10/10 - 2s - loss: 259.9755 - loglik: -2.3979e+02 - logprior: -2.0184e+01
Epoch 4/10
10/10 - 2s - loss: 253.8894 - loglik: -2.3859e+02 - logprior: -1.5303e+01
Epoch 5/10
10/10 - 2s - loss: 249.7374 - loglik: -2.3821e+02 - logprior: -1.1526e+01
Epoch 6/10
10/10 - 2s - loss: 239.9311 - loglik: -2.3758e+02 - logprior: -2.3535e+00
Epoch 7/10
10/10 - 2s - loss: 230.6194 - loglik: -2.3707e+02 - logprior: 6.4509
Epoch 8/10
10/10 - 2s - loss: 229.2080 - loglik: -2.3732e+02 - logprior: 8.1083
Epoch 9/10
10/10 - 2s - loss: 227.7128 - loglik: -2.3651e+02 - logprior: 8.7930
Epoch 10/10
10/10 - 2s - loss: 227.5299 - loglik: -2.3688e+02 - logprior: 9.3475
Fitted a model with MAP estimate = -227.2541
Time for alignment: 49.7444
Computed alignments with likelihoods: ['-223.9824', '-225.7159', '-227.2541']
Best model has likelihood: -223.9824
SP score = 0.7410
Training of 3 independent models on file egf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34c9c34730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f369907ad90>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 97.1105 - loglik: -9.2686e+01 - logprior: -4.4242e+00
Epoch 2/10
17/17 - 1s - loss: 75.0312 - loglik: -7.3482e+01 - logprior: -1.5492e+00
Epoch 3/10
17/17 - 1s - loss: 64.8908 - loglik: -6.3218e+01 - logprior: -1.6727e+00
Epoch 4/10
17/17 - 1s - loss: 62.8890 - loglik: -6.1257e+01 - logprior: -1.6322e+00
Epoch 5/10
17/17 - 1s - loss: 62.3283 - loglik: -6.0779e+01 - logprior: -1.5492e+00
Epoch 6/10
17/17 - 1s - loss: 62.0292 - loglik: -6.0470e+01 - logprior: -1.5593e+00
Epoch 7/10
17/17 - 1s - loss: 61.7947 - loglik: -6.0257e+01 - logprior: -1.5378e+00
Epoch 8/10
17/17 - 1s - loss: 61.6748 - loglik: -6.0151e+01 - logprior: -1.5239e+00
Epoch 9/10
17/17 - 1s - loss: 61.5174 - loglik: -6.0002e+01 - logprior: -1.5153e+00
Epoch 10/10
17/17 - 1s - loss: 61.4493 - loglik: -5.9943e+01 - logprior: -1.5059e+00
Fitted a model with MAP estimate = -61.3940
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (14, 1), (15, 1)]
discards: [0]
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 69.9447 - loglik: -6.4460e+01 - logprior: -5.4852e+00
Epoch 2/2
17/17 - 1s - loss: 61.7321 - loglik: -5.9156e+01 - logprior: -2.5764e+00
Fitted a model with MAP estimate = -59.3342
expansions: []
discards: [13 16]
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 62.3153 - loglik: -5.7958e+01 - logprior: -4.3576e+00
Epoch 2/2
17/17 - 1s - loss: 58.4228 - loglik: -5.6771e+01 - logprior: -1.6514e+00
Fitted a model with MAP estimate = -57.9994
expansions: []
discards: []
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 5s - loss: 61.5282 - loglik: -5.7294e+01 - logprior: -4.2345e+00
Epoch 2/10
17/17 - 1s - loss: 58.3188 - loglik: -5.6684e+01 - logprior: -1.6349e+00
Epoch 3/10
17/17 - 1s - loss: 57.8794 - loglik: -5.6471e+01 - logprior: -1.4088e+00
Epoch 4/10
17/17 - 1s - loss: 57.4097 - loglik: -5.6063e+01 - logprior: -1.3470e+00
Epoch 5/10
17/17 - 1s - loss: 57.3525 - loglik: -5.6040e+01 - logprior: -1.3121e+00
Epoch 6/10
17/17 - 1s - loss: 57.0179 - loglik: -5.5728e+01 - logprior: -1.2899e+00
Epoch 7/10
17/17 - 1s - loss: 56.9718 - loglik: -5.5704e+01 - logprior: -1.2673e+00
Epoch 8/10
17/17 - 1s - loss: 56.7630 - loglik: -5.5513e+01 - logprior: -1.2499e+00
Epoch 9/10
17/17 - 1s - loss: 56.5668 - loglik: -5.5325e+01 - logprior: -1.2418e+00
Epoch 10/10
17/17 - 1s - loss: 56.5091 - loglik: -5.5279e+01 - logprior: -1.2299e+00
Fitted a model with MAP estimate = -56.4109
Time for alignment: 36.9133
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 97.1859 - loglik: -9.2763e+01 - logprior: -4.4227e+00
Epoch 2/10
17/17 - 1s - loss: 74.9976 - loglik: -7.3472e+01 - logprior: -1.5258e+00
Epoch 3/10
17/17 - 1s - loss: 64.9078 - loglik: -6.3248e+01 - logprior: -1.6602e+00
Epoch 4/10
17/17 - 1s - loss: 63.3575 - loglik: -6.1733e+01 - logprior: -1.6240e+00
Epoch 5/10
17/17 - 1s - loss: 62.7387 - loglik: -6.1205e+01 - logprior: -1.5339e+00
Epoch 6/10
17/17 - 1s - loss: 62.5917 - loglik: -6.1044e+01 - logprior: -1.5482e+00
Epoch 7/10
17/17 - 1s - loss: 62.3631 - loglik: -6.0838e+01 - logprior: -1.5248e+00
Epoch 8/10
17/17 - 1s - loss: 62.1372 - loglik: -6.0624e+01 - logprior: -1.5134e+00
Epoch 9/10
17/17 - 1s - loss: 62.0680 - loglik: -6.0562e+01 - logprior: -1.5061e+00
Epoch 10/10
17/17 - 1s - loss: 61.9421 - loglik: -6.0442e+01 - logprior: -1.5002e+00
Fitted a model with MAP estimate = -61.9490
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 70.3206 - loglik: -6.4833e+01 - logprior: -5.4880e+00
Epoch 2/2
17/17 - 1s - loss: 61.7654 - loglik: -5.9188e+01 - logprior: -2.5772e+00
Fitted a model with MAP estimate = -59.3478
expansions: []
discards: [13 16]
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 62.3756 - loglik: -5.8023e+01 - logprior: -4.3523e+00
Epoch 2/2
17/17 - 1s - loss: 58.3224 - loglik: -5.6661e+01 - logprior: -1.6618e+00
Fitted a model with MAP estimate = -57.9953
expansions: []
discards: []
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 5s - loss: 61.5443 - loglik: -5.7317e+01 - logprior: -4.2275e+00
Epoch 2/10
17/17 - 1s - loss: 58.4099 - loglik: -5.6769e+01 - logprior: -1.6411e+00
Epoch 3/10
17/17 - 1s - loss: 57.7684 - loglik: -5.6362e+01 - logprior: -1.4064e+00
Epoch 4/10
17/17 - 1s - loss: 57.5379 - loglik: -5.6196e+01 - logprior: -1.3416e+00
Epoch 5/10
17/17 - 1s - loss: 57.2033 - loglik: -5.5889e+01 - logprior: -1.3145e+00
Epoch 6/10
17/17 - 1s - loss: 57.0556 - loglik: -5.5770e+01 - logprior: -1.2857e+00
Epoch 7/10
17/17 - 1s - loss: 56.8663 - loglik: -5.5602e+01 - logprior: -1.2640e+00
Epoch 8/10
17/17 - 1s - loss: 56.8512 - loglik: -5.5592e+01 - logprior: -1.2588e+00
Epoch 9/10
17/17 - 1s - loss: 56.5779 - loglik: -5.5344e+01 - logprior: -1.2343e+00
Epoch 10/10
17/17 - 1s - loss: 56.4286 - loglik: -5.5206e+01 - logprior: -1.2224e+00
Fitted a model with MAP estimate = -56.3907
Time for alignment: 34.4256
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 97.0638 - loglik: -9.2639e+01 - logprior: -4.4243e+00
Epoch 2/10
17/17 - 1s - loss: 74.6509 - loglik: -7.3117e+01 - logprior: -1.5338e+00
Epoch 3/10
17/17 - 1s - loss: 65.0807 - loglik: -6.3430e+01 - logprior: -1.6502e+00
Epoch 4/10
17/17 - 1s - loss: 63.3748 - loglik: -6.1758e+01 - logprior: -1.6167e+00
Epoch 5/10
17/17 - 1s - loss: 62.7053 - loglik: -6.1181e+01 - logprior: -1.5239e+00
Epoch 6/10
17/17 - 1s - loss: 62.5693 - loglik: -6.1028e+01 - logprior: -1.5416e+00
Epoch 7/10
17/17 - 1s - loss: 62.3026 - loglik: -6.0780e+01 - logprior: -1.5231e+00
Epoch 8/10
17/17 - 1s - loss: 62.0951 - loglik: -6.0581e+01 - logprior: -1.5144e+00
Epoch 9/10
17/17 - 1s - loss: 62.0539 - loglik: -6.0549e+01 - logprior: -1.5050e+00
Epoch 10/10
17/17 - 1s - loss: 61.9814 - loglik: -6.0484e+01 - logprior: -1.4976e+00
Fitted a model with MAP estimate = -61.9513
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 70.3568 - loglik: -6.4868e+01 - logprior: -5.4887e+00
Epoch 2/2
17/17 - 1s - loss: 61.7878 - loglik: -5.9209e+01 - logprior: -2.5789e+00
Fitted a model with MAP estimate = -59.3645
expansions: []
discards: [13 16]
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 5s - loss: 62.4277 - loglik: -5.8069e+01 - logprior: -4.3591e+00
Epoch 2/2
17/17 - 1s - loss: 58.3557 - loglik: -5.6700e+01 - logprior: -1.6559e+00
Fitted a model with MAP estimate = -57.9884
expansions: []
discards: []
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 61.5634 - loglik: -5.7335e+01 - logprior: -4.2285e+00
Epoch 2/10
17/17 - 1s - loss: 58.3683 - loglik: -5.6729e+01 - logprior: -1.6390e+00
Epoch 3/10
17/17 - 1s - loss: 57.8159 - loglik: -5.6411e+01 - logprior: -1.4046e+00
Epoch 4/10
17/17 - 1s - loss: 57.4876 - loglik: -5.6143e+01 - logprior: -1.3441e+00
Epoch 5/10
17/17 - 1s - loss: 57.2129 - loglik: -5.5911e+01 - logprior: -1.3016e+00
Epoch 6/10
17/17 - 1s - loss: 57.1292 - loglik: -5.5842e+01 - logprior: -1.2874e+00
Epoch 7/10
17/17 - 1s - loss: 56.8427 - loglik: -5.5572e+01 - logprior: -1.2706e+00
Epoch 8/10
17/17 - 1s - loss: 56.8001 - loglik: -5.5549e+01 - logprior: -1.2507e+00
Epoch 9/10
17/17 - 1s - loss: 56.5922 - loglik: -5.5354e+01 - logprior: -1.2380e+00
Epoch 10/10
17/17 - 1s - loss: 56.5347 - loglik: -5.5312e+01 - logprior: -1.2225e+00
Fitted a model with MAP estimate = -56.4070
Time for alignment: 33.8708
Computed alignments with likelihoods: ['-56.4109', '-56.3907', '-56.4070']
Best model has likelihood: -56.3907
SP score = 0.8275
Training of 3 independent models on file HMG_box.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2cee914760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f349d7b40a0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 3s - loss: 193.6361 - loglik: -1.8555e+02 - logprior: -8.0905e+00
Epoch 2/10
13/13 - 1s - loss: 163.6646 - loglik: -1.6145e+02 - logprior: -2.2098e+00
Epoch 3/10
13/13 - 1s - loss: 146.5545 - loglik: -1.4465e+02 - logprior: -1.9056e+00
Epoch 4/10
13/13 - 1s - loss: 138.9940 - loglik: -1.3699e+02 - logprior: -1.9994e+00
Epoch 5/10
13/13 - 1s - loss: 136.0112 - loglik: -1.3412e+02 - logprior: -1.8885e+00
Epoch 6/10
13/13 - 1s - loss: 134.5671 - loglik: -1.3275e+02 - logprior: -1.8168e+00
Epoch 7/10
13/13 - 1s - loss: 133.7453 - loglik: -1.3190e+02 - logprior: -1.8447e+00
Epoch 8/10
13/13 - 1s - loss: 133.1962 - loglik: -1.3137e+02 - logprior: -1.8226e+00
Epoch 9/10
13/13 - 1s - loss: 132.9080 - loglik: -1.3110e+02 - logprior: -1.8082e+00
Epoch 10/10
13/13 - 1s - loss: 132.7730 - loglik: -1.3097e+02 - logprior: -1.7994e+00
Fitted a model with MAP estimate = -132.5615
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (34, 1), (41, 1), (44, 1), (45, 1), (46, 2)]
discards: [0]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 3s - loss: 144.5473 - loglik: -1.3502e+02 - logprior: -9.5275e+00
Epoch 2/2
13/13 - 1s - loss: 129.4826 - loglik: -1.2506e+02 - logprior: -4.4260e+00
Fitted a model with MAP estimate = -126.8200
expansions: [(0, 2), (13, 1)]
discards: [ 0 23]
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 129.8399 - loglik: -1.2258e+02 - logprior: -7.2561e+00
Epoch 2/2
13/13 - 1s - loss: 122.1942 - loglik: -1.2006e+02 - logprior: -2.1334e+00
Fitted a model with MAP estimate = -121.1609
expansions: []
discards: [0]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 132.1843 - loglik: -1.2296e+02 - logprior: -9.2269e+00
Epoch 2/10
13/13 - 1s - loss: 124.5866 - loglik: -1.2115e+02 - logprior: -3.4337e+00
Epoch 3/10
13/13 - 1s - loss: 121.8259 - loglik: -1.2017e+02 - logprior: -1.6524e+00
Epoch 4/10
13/13 - 1s - loss: 120.3117 - loglik: -1.1901e+02 - logprior: -1.2988e+00
Epoch 5/10
13/13 - 1s - loss: 119.7980 - loglik: -1.1864e+02 - logprior: -1.1603e+00
Epoch 6/10
13/13 - 1s - loss: 118.8527 - loglik: -1.1766e+02 - logprior: -1.1930e+00
Epoch 7/10
13/13 - 1s - loss: 119.0314 - loglik: -1.1782e+02 - logprior: -1.2084e+00
Fitted a model with MAP estimate = -118.3999
Time for alignment: 34.5209
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 193.7081 - loglik: -1.8562e+02 - logprior: -8.0890e+00
Epoch 2/10
13/13 - 1s - loss: 163.1438 - loglik: -1.6094e+02 - logprior: -2.2060e+00
Epoch 3/10
13/13 - 1s - loss: 145.6066 - loglik: -1.4371e+02 - logprior: -1.8972e+00
Epoch 4/10
13/13 - 1s - loss: 138.9931 - loglik: -1.3702e+02 - logprior: -1.9686e+00
Epoch 5/10
13/13 - 1s - loss: 135.9838 - loglik: -1.3413e+02 - logprior: -1.8520e+00
Epoch 6/10
13/13 - 1s - loss: 134.4895 - loglik: -1.3267e+02 - logprior: -1.8173e+00
Epoch 7/10
13/13 - 1s - loss: 133.4870 - loglik: -1.3163e+02 - logprior: -1.8538e+00
Epoch 8/10
13/13 - 1s - loss: 133.0486 - loglik: -1.3122e+02 - logprior: -1.8280e+00
Epoch 9/10
13/13 - 1s - loss: 132.9321 - loglik: -1.3112e+02 - logprior: -1.8137e+00
Epoch 10/10
13/13 - 1s - loss: 132.5586 - loglik: -1.3074e+02 - logprior: -1.8159e+00
Fitted a model with MAP estimate = -132.2611
expansions: [(12, 1), (14, 1), (17, 5), (18, 2), (29, 1), (34, 1), (41, 1), (44, 1), (45, 2), (46, 2)]
discards: [0]
Fitting a model of length 71 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 144.3060 - loglik: -1.3482e+02 - logprior: -9.4910e+00
Epoch 2/2
13/13 - 1s - loss: 128.0881 - loglik: -1.2368e+02 - logprior: -4.4032e+00
Fitted a model with MAP estimate = -125.8693
expansions: [(0, 2)]
discards: [ 0 24 57]
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 129.3133 - loglik: -1.2207e+02 - logprior: -7.2478e+00
Epoch 2/2
13/13 - 1s - loss: 121.9532 - loglik: -1.1982e+02 - logprior: -2.1312e+00
Fitted a model with MAP estimate = -121.1144
expansions: []
discards: [0]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 3s - loss: 132.2231 - loglik: -1.2299e+02 - logprior: -9.2298e+00
Epoch 2/10
13/13 - 1s - loss: 124.7035 - loglik: -1.2125e+02 - logprior: -3.4513e+00
Epoch 3/10
13/13 - 1s - loss: 121.7594 - loglik: -1.2009e+02 - logprior: -1.6685e+00
Epoch 4/10
13/13 - 1s - loss: 120.3827 - loglik: -1.1909e+02 - logprior: -1.2895e+00
Epoch 5/10
13/13 - 1s - loss: 119.7259 - loglik: -1.1856e+02 - logprior: -1.1616e+00
Epoch 6/10
13/13 - 1s - loss: 118.8957 - loglik: -1.1772e+02 - logprior: -1.1802e+00
Epoch 7/10
13/13 - 1s - loss: 118.7097 - loglik: -1.1750e+02 - logprior: -1.2081e+00
Epoch 8/10
13/13 - 1s - loss: 118.4652 - loglik: -1.1729e+02 - logprior: -1.1745e+00
Epoch 9/10
13/13 - 1s - loss: 118.1477 - loglik: -1.1699e+02 - logprior: -1.1575e+00
Epoch 10/10
13/13 - 1s - loss: 117.9730 - loglik: -1.1684e+02 - logprior: -1.1342e+00
Fitted a model with MAP estimate = -117.8859
Time for alignment: 36.8477
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 3s - loss: 193.7854 - loglik: -1.8569e+02 - logprior: -8.0912e+00
Epoch 2/10
13/13 - 1s - loss: 163.8889 - loglik: -1.6168e+02 - logprior: -2.2136e+00
Epoch 3/10
13/13 - 1s - loss: 146.8577 - loglik: -1.4492e+02 - logprior: -1.9391e+00
Epoch 4/10
13/13 - 1s - loss: 139.7405 - loglik: -1.3769e+02 - logprior: -2.0545e+00
Epoch 5/10
13/13 - 1s - loss: 136.8541 - loglik: -1.3494e+02 - logprior: -1.9108e+00
Epoch 6/10
13/13 - 1s - loss: 134.7916 - loglik: -1.3296e+02 - logprior: -1.8354e+00
Epoch 7/10
13/13 - 1s - loss: 133.3505 - loglik: -1.3148e+02 - logprior: -1.8666e+00
Epoch 8/10
13/13 - 1s - loss: 132.7148 - loglik: -1.3087e+02 - logprior: -1.8453e+00
Epoch 9/10
13/13 - 1s - loss: 132.8302 - loglik: -1.3100e+02 - logprior: -1.8275e+00
Fitted a model with MAP estimate = -132.3583
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (32, 1), (35, 1), (41, 1), (45, 1), (46, 2)]
discards: [0]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 144.6523 - loglik: -1.3512e+02 - logprior: -9.5355e+00
Epoch 2/2
13/13 - 1s - loss: 129.2196 - loglik: -1.2480e+02 - logprior: -4.4211e+00
Fitted a model with MAP estimate = -127.1639
expansions: [(0, 2), (13, 1)]
discards: [ 0 23]
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 3s - loss: 129.7525 - loglik: -1.2248e+02 - logprior: -7.2738e+00
Epoch 2/2
13/13 - 1s - loss: 122.7391 - loglik: -1.2061e+02 - logprior: -2.1318e+00
Fitted a model with MAP estimate = -121.2522
expansions: []
discards: [0]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 3s - loss: 132.2601 - loglik: -1.2303e+02 - logprior: -9.2289e+00
Epoch 2/10
13/13 - 1s - loss: 124.8057 - loglik: -1.2137e+02 - logprior: -3.4370e+00
Epoch 3/10
13/13 - 1s - loss: 121.4996 - loglik: -1.1984e+02 - logprior: -1.6606e+00
Epoch 4/10
13/13 - 1s - loss: 120.6816 - loglik: -1.1939e+02 - logprior: -1.2929e+00
Epoch 5/10
13/13 - 1s - loss: 119.4305 - loglik: -1.1826e+02 - logprior: -1.1708e+00
Epoch 6/10
13/13 - 1s - loss: 119.0536 - loglik: -1.1786e+02 - logprior: -1.1926e+00
Epoch 7/10
13/13 - 1s - loss: 118.9460 - loglik: -1.1774e+02 - logprior: -1.2016e+00
Epoch 8/10
13/13 - 1s - loss: 118.5053 - loglik: -1.1732e+02 - logprior: -1.1842e+00
Epoch 9/10
13/13 - 1s - loss: 117.7250 - loglik: -1.1656e+02 - logprior: -1.1623e+00
Epoch 10/10
13/13 - 1s - loss: 118.2809 - loglik: -1.1714e+02 - logprior: -1.1436e+00
Fitted a model with MAP estimate = -117.8919
Time for alignment: 35.6145
Computed alignments with likelihoods: ['-118.3999', '-117.8859', '-117.8919']
Best model has likelihood: -117.8859
SP score = 0.9731
Training of 3 independent models on file hpr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f349d26c970>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34c9ccf100>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 240.8787 - loglik: -2.2834e+02 - logprior: -1.2540e+01
Epoch 2/10
11/11 - 1s - loss: 207.8723 - loglik: -2.0463e+02 - logprior: -3.2421e+00
Epoch 3/10
11/11 - 1s - loss: 181.0152 - loglik: -1.7896e+02 - logprior: -2.0562e+00
Epoch 4/10
11/11 - 1s - loss: 164.8379 - loglik: -1.6306e+02 - logprior: -1.7804e+00
Epoch 5/10
11/11 - 1s - loss: 159.8335 - loglik: -1.5833e+02 - logprior: -1.5069e+00
Epoch 6/10
11/11 - 1s - loss: 157.9906 - loglik: -1.5658e+02 - logprior: -1.4070e+00
Epoch 7/10
11/11 - 1s - loss: 156.4117 - loglik: -1.5523e+02 - logprior: -1.1798e+00
Epoch 8/10
11/11 - 1s - loss: 155.9563 - loglik: -1.5491e+02 - logprior: -1.0418e+00
Epoch 9/10
11/11 - 1s - loss: 155.6402 - loglik: -1.5469e+02 - logprior: -9.5428e-01
Epoch 10/10
11/11 - 1s - loss: 155.2406 - loglik: -1.5434e+02 - logprior: -9.0497e-01
Fitted a model with MAP estimate = -155.1035
expansions: [(0, 6), (22, 1), (27, 1), (29, 2), (32, 2), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Fitting a model of length 87 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 171.9768 - loglik: -1.5638e+02 - logprior: -1.5599e+01
Epoch 2/2
11/11 - 1s - loss: 150.3333 - loglik: -1.4592e+02 - logprior: -4.4164e+00
Fitted a model with MAP estimate = -147.1888
expansions: []
discards: [ 0 37 42]
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 162.8339 - loglik: -1.4877e+02 - logprior: -1.4067e+01
Epoch 2/2
11/11 - 1s - loss: 151.2205 - loglik: -1.4581e+02 - logprior: -5.4059e+00
Fitted a model with MAP estimate = -148.7688
expansions: []
discards: []
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.2764 - loglik: -1.4606e+02 - logprior: -1.2217e+01
Epoch 2/10
11/11 - 1s - loss: 148.2772 - loglik: -1.4523e+02 - logprior: -3.0493e+00
Epoch 3/10
11/11 - 1s - loss: 144.9273 - loglik: -1.4318e+02 - logprior: -1.7496e+00
Epoch 4/10
11/11 - 1s - loss: 144.1381 - loglik: -1.4305e+02 - logprior: -1.0886e+00
Epoch 5/10
11/11 - 1s - loss: 143.0559 - loglik: -1.4229e+02 - logprior: -7.6761e-01
Epoch 6/10
11/11 - 1s - loss: 142.4733 - loglik: -1.4179e+02 - logprior: -6.8262e-01
Epoch 7/10
11/11 - 1s - loss: 142.1141 - loglik: -1.4154e+02 - logprior: -5.7470e-01
Epoch 8/10
11/11 - 1s - loss: 141.3572 - loglik: -1.4080e+02 - logprior: -5.6147e-01
Epoch 9/10
11/11 - 1s - loss: 141.7284 - loglik: -1.4121e+02 - logprior: -5.1786e-01
Fitted a model with MAP estimate = -141.3362
Time for alignment: 36.2778
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 240.9562 - loglik: -2.2842e+02 - logprior: -1.2540e+01
Epoch 2/10
11/11 - 1s - loss: 207.8228 - loglik: -2.0457e+02 - logprior: -3.2517e+00
Epoch 3/10
11/11 - 1s - loss: 181.1633 - loglik: -1.7907e+02 - logprior: -2.0977e+00
Epoch 4/10
11/11 - 1s - loss: 165.3980 - loglik: -1.6357e+02 - logprior: -1.8245e+00
Epoch 5/10
11/11 - 1s - loss: 159.5373 - loglik: -1.5800e+02 - logprior: -1.5370e+00
Epoch 6/10
11/11 - 1s - loss: 157.8803 - loglik: -1.5647e+02 - logprior: -1.4111e+00
Epoch 7/10
11/11 - 1s - loss: 156.4622 - loglik: -1.5529e+02 - logprior: -1.1675e+00
Epoch 8/10
11/11 - 1s - loss: 155.8327 - loglik: -1.5479e+02 - logprior: -1.0382e+00
Epoch 9/10
11/11 - 1s - loss: 155.7932 - loglik: -1.5484e+02 - logprior: -9.4929e-01
Epoch 10/10
11/11 - 1s - loss: 155.3525 - loglik: -1.5445e+02 - logprior: -8.9804e-01
Fitted a model with MAP estimate = -155.1367
expansions: [(0, 6), (21, 1), (27, 1), (29, 2), (32, 2), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Fitting a model of length 87 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 171.8772 - loglik: -1.5625e+02 - logprior: -1.5628e+01
Epoch 2/2
11/11 - 1s - loss: 150.3674 - loglik: -1.4594e+02 - logprior: -4.4263e+00
Fitted a model with MAP estimate = -147.1605
expansions: []
discards: [ 0 37 42]
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 162.7737 - loglik: -1.4870e+02 - logprior: -1.4077e+01
Epoch 2/2
11/11 - 1s - loss: 151.4366 - loglik: -1.4602e+02 - logprior: -5.4200e+00
Fitted a model with MAP estimate = -148.7887
expansions: []
discards: []
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 158.2540 - loglik: -1.4602e+02 - logprior: -1.2233e+01
Epoch 2/10
11/11 - 1s - loss: 148.1902 - loglik: -1.4514e+02 - logprior: -3.0505e+00
Epoch 3/10
11/11 - 1s - loss: 145.0427 - loglik: -1.4329e+02 - logprior: -1.7491e+00
Epoch 4/10
11/11 - 1s - loss: 143.8406 - loglik: -1.4275e+02 - logprior: -1.0871e+00
Epoch 5/10
11/11 - 1s - loss: 143.3387 - loglik: -1.4258e+02 - logprior: -7.5877e-01
Epoch 6/10
11/11 - 1s - loss: 142.4790 - loglik: -1.4179e+02 - logprior: -6.8799e-01
Epoch 7/10
11/11 - 1s - loss: 141.9312 - loglik: -1.4135e+02 - logprior: -5.7902e-01
Epoch 8/10
11/11 - 1s - loss: 141.7923 - loglik: -1.4123e+02 - logprior: -5.6242e-01
Epoch 9/10
11/11 - 1s - loss: 141.3121 - loglik: -1.4080e+02 - logprior: -5.1567e-01
Epoch 10/10
11/11 - 1s - loss: 141.3225 - loglik: -1.4082e+02 - logprior: -4.9784e-01
Fitted a model with MAP estimate = -141.2235
Time for alignment: 38.2371
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 240.7044 - loglik: -2.2816e+02 - logprior: -1.2541e+01
Epoch 2/10
11/11 - 1s - loss: 207.6866 - loglik: -2.0444e+02 - logprior: -3.2494e+00
Epoch 3/10
11/11 - 1s - loss: 180.6105 - loglik: -1.7855e+02 - logprior: -2.0562e+00
Epoch 4/10
11/11 - 1s - loss: 164.9740 - loglik: -1.6320e+02 - logprior: -1.7776e+00
Epoch 5/10
11/11 - 1s - loss: 159.9998 - loglik: -1.5850e+02 - logprior: -1.4958e+00
Epoch 6/10
11/11 - 1s - loss: 157.4859 - loglik: -1.5611e+02 - logprior: -1.3763e+00
Epoch 7/10
11/11 - 1s - loss: 156.1946 - loglik: -1.5504e+02 - logprior: -1.1592e+00
Epoch 8/10
11/11 - 1s - loss: 155.9985 - loglik: -1.5497e+02 - logprior: -1.0300e+00
Epoch 9/10
11/11 - 1s - loss: 155.7199 - loglik: -1.5478e+02 - logprior: -9.4436e-01
Epoch 10/10
11/11 - 1s - loss: 155.1679 - loglik: -1.5427e+02 - logprior: -8.9689e-01
Fitted a model with MAP estimate = -155.1180
expansions: [(0, 6), (22, 1), (27, 1), (29, 2), (32, 2), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Fitting a model of length 87 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 171.8614 - loglik: -1.5624e+02 - logprior: -1.5617e+01
Epoch 2/2
11/11 - 1s - loss: 150.9518 - loglik: -1.4653e+02 - logprior: -4.4182e+00
Fitted a model with MAP estimate = -147.2073
expansions: []
discards: [ 0 37 42]
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 162.8818 - loglik: -1.4880e+02 - logprior: -1.4078e+01
Epoch 2/2
11/11 - 1s - loss: 151.5099 - loglik: -1.4610e+02 - logprior: -5.4098e+00
Fitted a model with MAP estimate = -148.7985
expansions: []
discards: []
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 158.2941 - loglik: -1.4607e+02 - logprior: -1.2229e+01
Epoch 2/10
11/11 - 1s - loss: 147.9226 - loglik: -1.4487e+02 - logprior: -3.0480e+00
Epoch 3/10
11/11 - 1s - loss: 145.2818 - loglik: -1.4355e+02 - logprior: -1.7363e+00
Epoch 4/10
11/11 - 1s - loss: 144.2561 - loglik: -1.4317e+02 - logprior: -1.0824e+00
Epoch 5/10
11/11 - 1s - loss: 143.0670 - loglik: -1.4232e+02 - logprior: -7.5082e-01
Epoch 6/10
11/11 - 1s - loss: 142.2019 - loglik: -1.4152e+02 - logprior: -6.8418e-01
Epoch 7/10
11/11 - 1s - loss: 142.0683 - loglik: -1.4150e+02 - logprior: -5.7005e-01
Epoch 8/10
11/11 - 1s - loss: 141.5179 - loglik: -1.4096e+02 - logprior: -5.5321e-01
Epoch 9/10
11/11 - 1s - loss: 141.9853 - loglik: -1.4147e+02 - logprior: -5.1293e-01
Fitted a model with MAP estimate = -141.3514
Time for alignment: 34.9368
Computed alignments with likelihoods: ['-141.3362', '-141.2235', '-141.3514']
Best model has likelihood: -141.2235
SP score = 0.9930
Training of 3 independent models on file tim.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34af707190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32a9ea0d60>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 192 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 572.9639 - loglik: -5.6396e+02 - logprior: -9.0039e+00
Epoch 2/10
12/12 - 5s - loss: 477.9410 - loglik: -4.7627e+02 - logprior: -1.6757e+00
Epoch 3/10
12/12 - 5s - loss: 392.8454 - loglik: -3.9087e+02 - logprior: -1.9748e+00
Epoch 4/10
12/12 - 5s - loss: 355.7048 - loglik: -3.5311e+02 - logprior: -2.5927e+00
Epoch 5/10
12/12 - 5s - loss: 347.0988 - loglik: -3.4444e+02 - logprior: -2.6627e+00
Epoch 6/10
12/12 - 5s - loss: 345.5823 - loglik: -3.4311e+02 - logprior: -2.4713e+00
Epoch 7/10
12/12 - 5s - loss: 342.9324 - loglik: -3.4060e+02 - logprior: -2.3326e+00
Epoch 8/10
12/12 - 5s - loss: 343.2710 - loglik: -3.4099e+02 - logprior: -2.2857e+00
Fitted a model with MAP estimate = -342.4046
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (16, 2), (17, 2), (18, 1), (34, 2), (35, 1), (36, 1), (37, 1), (39, 2), (46, 1), (47, 1), (48, 1), (61, 1), (65, 1), (75, 1), (82, 1), (87, 1), (90, 1), (93, 1), (110, 1), (113, 1), (118, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (158, 1), (172, 1), (173, 3), (174, 1), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Fitting a model of length 241 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 348.5555 - loglik: -3.3788e+02 - logprior: -1.0676e+01
Epoch 2/2
12/12 - 7s - loss: 322.0895 - loglik: -3.1824e+02 - logprior: -3.8532e+00
Fitted a model with MAP estimate = -317.0719
expansions: [(0, 3), (192, 1), (194, 1)]
discards: [ 0 12 53]
Fitting a model of length 243 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 12s - loss: 322.1110 - loglik: -3.1411e+02 - logprior: -7.9971e+00
Epoch 2/2
12/12 - 7s - loss: 309.8132 - loglik: -3.0850e+02 - logprior: -1.3160e+00
Fitted a model with MAP estimate = -307.9446
expansions: []
discards: [ 0  1 26]
Fitting a model of length 240 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 323.5898 - loglik: -3.1340e+02 - logprior: -1.0186e+01
Epoch 2/10
12/12 - 7s - loss: 313.7700 - loglik: -3.1062e+02 - logprior: -3.1537e+00
Epoch 3/10
12/12 - 7s - loss: 311.9445 - loglik: -3.1043e+02 - logprior: -1.5113e+00
Epoch 4/10
12/12 - 7s - loss: 307.8757 - loglik: -3.0813e+02 - logprior: 0.2545
Epoch 5/10
12/12 - 7s - loss: 306.1487 - loglik: -3.0699e+02 - logprior: 0.8380
Epoch 6/10
12/12 - 7s - loss: 305.6421 - loglik: -3.0651e+02 - logprior: 0.8722
Epoch 7/10
12/12 - 7s - loss: 306.1270 - loglik: -3.0696e+02 - logprior: 0.8312
Fitted a model with MAP estimate = -305.2968
Time for alignment: 166.0849
Fitting a model of length 192 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 572.7904 - loglik: -5.6380e+02 - logprior: -8.9879e+00
Epoch 2/10
12/12 - 5s - loss: 477.4195 - loglik: -4.7579e+02 - logprior: -1.6294e+00
Epoch 3/10
12/12 - 5s - loss: 393.5527 - loglik: -3.9163e+02 - logprior: -1.9247e+00
Epoch 4/10
12/12 - 5s - loss: 356.1182 - loglik: -3.5355e+02 - logprior: -2.5686e+00
Epoch 5/10
12/12 - 5s - loss: 349.2458 - loglik: -3.4653e+02 - logprior: -2.7133e+00
Epoch 6/10
12/12 - 5s - loss: 342.9842 - loglik: -3.4044e+02 - logprior: -2.5440e+00
Epoch 7/10
12/12 - 5s - loss: 343.4520 - loglik: -3.4102e+02 - logprior: -2.4322e+00
Fitted a model with MAP estimate = -342.4088
expansions: [(11, 1), (12, 3), (16, 1), (17, 1), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (47, 1), (48, 1), (49, 1), (59, 1), (63, 1), (65, 1), (76, 1), (82, 1), (87, 1), (90, 1), (93, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (155, 2), (158, 1), (171, 1), (173, 2), (174, 2), (186, 1), (190, 1)]
discards: [0]
Fitting a model of length 238 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 349.4229 - loglik: -3.3869e+02 - logprior: -1.0738e+01
Epoch 2/2
12/12 - 7s - loss: 321.5542 - loglik: -3.1764e+02 - logprior: -3.9134e+00
Fitted a model with MAP estimate = -317.6730
expansions: [(0, 2), (14, 1), (15, 1), (232, 1)]
discards: [  0 215 216]
Fitting a model of length 240 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 324.8152 - loglik: -3.1674e+02 - logprior: -8.0788e+00
Epoch 2/2
12/12 - 7s - loss: 311.5121 - loglik: -3.1022e+02 - logprior: -1.2922e+00
Fitted a model with MAP estimate = -309.7296
expansions: [(195, 1), (218, 1)]
discards: [0]
Fitting a model of length 241 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 324.7671 - loglik: -3.1459e+02 - logprior: -1.0182e+01
Epoch 2/10
12/12 - 7s - loss: 311.8974 - loglik: -3.0869e+02 - logprior: -3.2117e+00
Epoch 3/10
12/12 - 7s - loss: 310.9149 - loglik: -3.0933e+02 - logprior: -1.5817e+00
Epoch 4/10
12/12 - 7s - loss: 306.4422 - loglik: -3.0668e+02 - logprior: 0.2330
Epoch 5/10
12/12 - 7s - loss: 305.5326 - loglik: -3.0630e+02 - logprior: 0.7672
Epoch 6/10
12/12 - 7s - loss: 304.5662 - loglik: -3.0537e+02 - logprior: 0.8040
Epoch 7/10
12/12 - 7s - loss: 305.1840 - loglik: -3.0594e+02 - logprior: 0.7558
Fitted a model with MAP estimate = -304.3366
Time for alignment: 159.7337
Fitting a model of length 192 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 572.2761 - loglik: -5.6330e+02 - logprior: -8.9752e+00
Epoch 2/10
12/12 - 5s - loss: 478.6150 - loglik: -4.7700e+02 - logprior: -1.6196e+00
Epoch 3/10
12/12 - 5s - loss: 399.6014 - loglik: -3.9772e+02 - logprior: -1.8773e+00
Epoch 4/10
12/12 - 5s - loss: 360.7721 - loglik: -3.5829e+02 - logprior: -2.4776e+00
Epoch 5/10
12/12 - 5s - loss: 350.8128 - loglik: -3.4820e+02 - logprior: -2.6127e+00
Epoch 6/10
12/12 - 5s - loss: 347.6901 - loglik: -3.4526e+02 - logprior: -2.4252e+00
Epoch 7/10
12/12 - 5s - loss: 344.6181 - loglik: -3.4228e+02 - logprior: -2.3351e+00
Epoch 8/10
12/12 - 5s - loss: 344.0724 - loglik: -3.4177e+02 - logprior: -2.3044e+00
Epoch 9/10
12/12 - 5s - loss: 343.6902 - loglik: -3.4140e+02 - logprior: -2.2918e+00
Epoch 10/10
12/12 - 5s - loss: 342.6897 - loglik: -3.4039e+02 - logprior: -2.2983e+00
Fitted a model with MAP estimate = -342.7685
expansions: [(11, 1), (12, 3), (16, 1), (18, 1), (19, 2), (35, 2), (36, 1), (37, 1), (38, 1), (40, 1), (47, 1), (48, 1), (49, 1), (62, 1), (63, 1), (65, 1), (83, 1), (85, 1), (88, 1), (90, 1), (93, 1), (110, 1), (113, 1), (116, 1), (117, 1), (119, 1), (120, 2), (122, 1), (149, 1), (152, 2), (153, 5), (154, 1), (171, 1), (173, 3), (185, 1), (187, 1)]
discards: [0]
Fitting a model of length 239 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 11s - loss: 348.8160 - loglik: -3.3811e+02 - logprior: -1.0703e+01
Epoch 2/2
12/12 - 7s - loss: 321.8435 - loglik: -3.1798e+02 - logprior: -3.8640e+00
Fitted a model with MAP estimate = -317.8070
expansions: [(0, 2), (13, 1), (14, 1), (214, 1)]
discards: [ 0 24]
Fitting a model of length 242 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 322.2245 - loglik: -3.1420e+02 - logprior: -8.0244e+00
Epoch 2/2
12/12 - 7s - loss: 310.7526 - loglik: -3.0957e+02 - logprior: -1.1865e+00
Fitted a model with MAP estimate = -308.3222
expansions: [(192, 1)]
discards: [0]
Fitting a model of length 242 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 322.1696 - loglik: -3.1206e+02 - logprior: -1.0106e+01
Epoch 2/10
12/12 - 7s - loss: 313.4242 - loglik: -3.1029e+02 - logprior: -3.1298e+00
Epoch 3/10
12/12 - 7s - loss: 308.7895 - loglik: -3.0728e+02 - logprior: -1.5075e+00
Epoch 4/10
12/12 - 7s - loss: 307.0029 - loglik: -3.0731e+02 - logprior: 0.3100
Epoch 5/10
12/12 - 7s - loss: 305.7272 - loglik: -3.0659e+02 - logprior: 0.8599
Epoch 6/10
12/12 - 7s - loss: 305.3927 - loglik: -3.0628e+02 - logprior: 0.8839
Epoch 7/10
12/12 - 7s - loss: 304.1646 - loglik: -3.0500e+02 - logprior: 0.8402
Epoch 8/10
12/12 - 7s - loss: 303.7461 - loglik: -3.0459e+02 - logprior: 0.8418
Epoch 9/10
12/12 - 7s - loss: 303.4870 - loglik: -3.0425e+02 - logprior: 0.7604
Epoch 10/10
12/12 - 7s - loss: 303.6989 - loglik: -3.0457e+02 - logprior: 0.8668
Fitted a model with MAP estimate = -303.1647
Time for alignment: 198.2729
Computed alignments with likelihoods: ['-305.2968', '-304.3366', '-303.1647']
Best model has likelihood: -303.1647
SP score = 0.9730
Training of 3 independent models on file tgfb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34d221b490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f350c74bdf0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 303.5849 - loglik: -2.7652e+02 - logprior: -2.7067e+01
Epoch 2/10
10/10 - 1s - loss: 247.1726 - loglik: -2.4030e+02 - logprior: -6.8733e+00
Epoch 3/10
10/10 - 1s - loss: 208.5799 - loglik: -2.0503e+02 - logprior: -3.5480e+00
Epoch 4/10
10/10 - 1s - loss: 188.2996 - loglik: -1.8583e+02 - logprior: -2.4660e+00
Epoch 5/10
10/10 - 1s - loss: 180.0533 - loglik: -1.7799e+02 - logprior: -2.0643e+00
Epoch 6/10
10/10 - 1s - loss: 176.4918 - loglik: -1.7454e+02 - logprior: -1.9472e+00
Epoch 7/10
10/10 - 1s - loss: 175.1912 - loglik: -1.7345e+02 - logprior: -1.7432e+00
Epoch 8/10
10/10 - 1s - loss: 174.2500 - loglik: -1.7268e+02 - logprior: -1.5734e+00
Epoch 9/10
10/10 - 1s - loss: 174.5778 - loglik: -1.7302e+02 - logprior: -1.5574e+00
Fitted a model with MAP estimate = -173.7803
expansions: [(0, 2), (11, 1), (12, 1), (34, 1), (35, 3), (36, 2), (49, 2), (69, 1), (70, 1), (71, 1), (72, 2), (73, 1), (74, 2)]
discards: []
Fitting a model of length 103 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 205.6752 - loglik: -1.7144e+02 - logprior: -3.4240e+01
Epoch 2/2
10/10 - 1s - loss: 171.2238 - loglik: -1.6113e+02 - logprior: -1.0091e+01
Fitted a model with MAP estimate = -164.9452
expansions: [(58, 2)]
discards: [85]
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 184.0111 - loglik: -1.5873e+02 - logprior: -2.5282e+01
Epoch 2/2
10/10 - 1s - loss: 161.8844 - loglik: -1.5547e+02 - logprior: -6.4106e+00
Fitted a model with MAP estimate = -158.1396
expansions: []
discards: []
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.2140 - loglik: -1.5511e+02 - logprior: -2.4107e+01
Epoch 2/10
10/10 - 1s - loss: 159.7604 - loglik: -1.5372e+02 - logprior: -6.0355e+00
Epoch 3/10
10/10 - 1s - loss: 156.5258 - loglik: -1.5405e+02 - logprior: -2.4739e+00
Epoch 4/10
10/10 - 1s - loss: 153.2307 - loglik: -1.5211e+02 - logprior: -1.1254e+00
Epoch 5/10
10/10 - 1s - loss: 152.4834 - loglik: -1.5198e+02 - logprior: -5.0040e-01
Epoch 6/10
10/10 - 1s - loss: 151.5843 - loglik: -1.5142e+02 - logprior: -1.6219e-01
Epoch 7/10
10/10 - 1s - loss: 151.1280 - loglik: -1.5122e+02 - logprior: 0.0938
Epoch 8/10
10/10 - 1s - loss: 150.9504 - loglik: -1.5126e+02 - logprior: 0.3099
Epoch 9/10
10/10 - 1s - loss: 150.8184 - loglik: -1.5128e+02 - logprior: 0.4637
Epoch 10/10
10/10 - 1s - loss: 150.5605 - loglik: -1.5112e+02 - logprior: 0.5614
Fitted a model with MAP estimate = -150.4633
Time for alignment: 40.3831
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 303.7767 - loglik: -2.7671e+02 - logprior: -2.7068e+01
Epoch 2/10
10/10 - 1s - loss: 247.3575 - loglik: -2.4048e+02 - logprior: -6.8760e+00
Epoch 3/10
10/10 - 1s - loss: 209.3519 - loglik: -2.0578e+02 - logprior: -3.5768e+00
Epoch 4/10
10/10 - 1s - loss: 188.1485 - loglik: -1.8555e+02 - logprior: -2.5989e+00
Epoch 5/10
10/10 - 1s - loss: 178.8200 - loglik: -1.7651e+02 - logprior: -2.3098e+00
Epoch 6/10
10/10 - 1s - loss: 175.3899 - loglik: -1.7318e+02 - logprior: -2.2144e+00
Epoch 7/10
10/10 - 1s - loss: 173.9279 - loglik: -1.7193e+02 - logprior: -2.0010e+00
Epoch 8/10
10/10 - 1s - loss: 173.3687 - loglik: -1.7155e+02 - logprior: -1.8213e+00
Epoch 9/10
10/10 - 1s - loss: 172.8522 - loglik: -1.7107e+02 - logprior: -1.7777e+00
Epoch 10/10
10/10 - 1s - loss: 172.1756 - loglik: -1.7040e+02 - logprior: -1.7732e+00
Fitted a model with MAP estimate = -172.1925
expansions: [(0, 2), (11, 2), (12, 1), (24, 1), (35, 3), (36, 2), (48, 2), (49, 2), (60, 1), (69, 1), (70, 1), (72, 2), (73, 1), (74, 2)]
discards: []
Fitting a model of length 106 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 205.6020 - loglik: -1.7109e+02 - logprior: -3.4508e+01
Epoch 2/2
10/10 - 1s - loss: 169.9430 - loglik: -1.5963e+02 - logprior: -1.0314e+01
Fitted a model with MAP estimate = -163.1458
expansions: [(60, 1)]
discards: [13 91]
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 182.7175 - loglik: -1.5742e+02 - logprior: -2.5299e+01
Epoch 2/2
10/10 - 1s - loss: 160.8176 - loglik: -1.5443e+02 - logprior: -6.3895e+00
Fitted a model with MAP estimate = -157.8650
expansions: []
discards: []
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 178.4831 - loglik: -1.5443e+02 - logprior: -2.4054e+01
Epoch 2/10
10/10 - 1s - loss: 159.0388 - loglik: -1.5305e+02 - logprior: -5.9871e+00
Epoch 3/10
10/10 - 1s - loss: 155.2860 - loglik: -1.5286e+02 - logprior: -2.4213e+00
Epoch 4/10
10/10 - 1s - loss: 152.7189 - loglik: -1.5163e+02 - logprior: -1.0843e+00
Epoch 5/10
10/10 - 1s - loss: 151.6438 - loglik: -1.5119e+02 - logprior: -4.5361e-01
Epoch 6/10
10/10 - 1s - loss: 150.5207 - loglik: -1.5039e+02 - logprior: -1.3007e-01
Epoch 7/10
10/10 - 1s - loss: 150.6182 - loglik: -1.5076e+02 - logprior: 0.1368
Fitted a model with MAP estimate = -150.0999
Time for alignment: 36.6386
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 303.5690 - loglik: -2.7650e+02 - logprior: -2.7068e+01
Epoch 2/10
10/10 - 1s - loss: 246.8309 - loglik: -2.3995e+02 - logprior: -6.8805e+00
Epoch 3/10
10/10 - 1s - loss: 209.8979 - loglik: -2.0630e+02 - logprior: -3.6010e+00
Epoch 4/10
10/10 - 1s - loss: 189.4029 - loglik: -1.8680e+02 - logprior: -2.6055e+00
Epoch 5/10
10/10 - 1s - loss: 180.6422 - loglik: -1.7834e+02 - logprior: -2.2989e+00
Epoch 6/10
10/10 - 1s - loss: 176.9989 - loglik: -1.7488e+02 - logprior: -2.1152e+00
Epoch 7/10
10/10 - 1s - loss: 175.0435 - loglik: -1.7317e+02 - logprior: -1.8745e+00
Epoch 8/10
10/10 - 1s - loss: 174.2624 - loglik: -1.7253e+02 - logprior: -1.7290e+00
Epoch 9/10
10/10 - 1s - loss: 173.4422 - loglik: -1.7175e+02 - logprior: -1.6918e+00
Epoch 10/10
10/10 - 1s - loss: 172.6299 - loglik: -1.7092e+02 - logprior: -1.7065e+00
Fitted a model with MAP estimate = -172.4459
expansions: [(0, 2), (5, 1), (12, 1), (24, 1), (36, 4), (48, 2), (49, 2), (60, 1), (69, 1), (70, 1), (72, 3), (73, 1)]
discards: []
Fitting a model of length 103 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 204.6118 - loglik: -1.7008e+02 - logprior: -3.4528e+01
Epoch 2/2
10/10 - 1s - loss: 169.3364 - loglik: -1.5909e+02 - logprior: -1.0250e+01
Fitted a model with MAP estimate = -162.7571
expansions: [(43, 1)]
discards: []
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 181.7830 - loglik: -1.5644e+02 - logprior: -2.5339e+01
Epoch 2/2
10/10 - 1s - loss: 160.7561 - loglik: -1.5434e+02 - logprior: -6.4119e+00
Fitted a model with MAP estimate = -157.2564
expansions: [(60, 1)]
discards: []
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 178.4406 - loglik: -1.5441e+02 - logprior: -2.4029e+01
Epoch 2/10
10/10 - 1s - loss: 159.3449 - loglik: -1.5332e+02 - logprior: -6.0245e+00
Epoch 3/10
10/10 - 1s - loss: 154.8567 - loglik: -1.5237e+02 - logprior: -2.4888e+00
Epoch 4/10
10/10 - 1s - loss: 153.1066 - loglik: -1.5197e+02 - logprior: -1.1405e+00
Epoch 5/10
10/10 - 1s - loss: 151.8602 - loglik: -1.5137e+02 - logprior: -4.9095e-01
Epoch 6/10
10/10 - 1s - loss: 151.0970 - loglik: -1.5094e+02 - logprior: -1.5725e-01
Epoch 7/10
10/10 - 1s - loss: 150.2807 - loglik: -1.5038e+02 - logprior: 0.1002
Epoch 8/10
10/10 - 1s - loss: 150.3037 - loglik: -1.5063e+02 - logprior: 0.3263
Fitted a model with MAP estimate = -150.1223
Time for alignment: 36.7783
Computed alignments with likelihoods: ['-150.4633', '-150.0999', '-150.1223']
Best model has likelihood: -150.0999
SP score = 0.7367
Training of 3 independent models on file HLH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34da795d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32aa841d90>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 151.0961 - loglik: -1.4600e+02 - logprior: -5.0931e+00
Epoch 2/10
16/16 - 1s - loss: 126.3794 - loglik: -1.2480e+02 - logprior: -1.5798e+00
Epoch 3/10
16/16 - 1s - loss: 113.8683 - loglik: -1.1213e+02 - logprior: -1.7406e+00
Epoch 4/10
16/16 - 1s - loss: 109.5377 - loglik: -1.0774e+02 - logprior: -1.7975e+00
Epoch 5/10
16/16 - 1s - loss: 108.4525 - loglik: -1.0672e+02 - logprior: -1.7345e+00
Epoch 6/10
16/16 - 1s - loss: 107.8458 - loglik: -1.0617e+02 - logprior: -1.6735e+00
Epoch 7/10
16/16 - 1s - loss: 107.6639 - loglik: -1.0603e+02 - logprior: -1.6316e+00
Epoch 8/10
16/16 - 1s - loss: 107.8308 - loglik: -1.0621e+02 - logprior: -1.6180e+00
Fitted a model with MAP estimate = -107.4695
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (17, 1), (23, 4), (25, 1), (26, 2), (29, 1)]
discards: [0]
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 115.0115 - loglik: -1.0875e+02 - logprior: -6.2580e+00
Epoch 2/2
16/16 - 1s - loss: 105.1924 - loglik: -1.0212e+02 - logprior: -3.0706e+00
Fitted a model with MAP estimate = -103.8886
expansions: [(0, 1)]
discards: [ 0 30 36]
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 6s - loss: 106.6429 - loglik: -1.0205e+02 - logprior: -4.5971e+00
Epoch 2/2
16/16 - 1s - loss: 102.4904 - loglik: -1.0088e+02 - logprior: -1.6123e+00
Fitted a model with MAP estimate = -101.8736
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 108.8773 - loglik: -1.0271e+02 - logprior: -6.1650e+00
Epoch 2/10
16/16 - 1s - loss: 104.4519 - loglik: -1.0173e+02 - logprior: -2.7221e+00
Epoch 3/10
16/16 - 1s - loss: 102.3918 - loglik: -1.0087e+02 - logprior: -1.5221e+00
Epoch 4/10
16/16 - 1s - loss: 101.2668 - loglik: -9.9996e+01 - logprior: -1.2706e+00
Epoch 5/10
16/16 - 1s - loss: 100.9219 - loglik: -9.9670e+01 - logprior: -1.2523e+00
Epoch 6/10
16/16 - 1s - loss: 100.3710 - loglik: -9.9133e+01 - logprior: -1.2383e+00
Epoch 7/10
16/16 - 1s - loss: 100.2586 - loglik: -9.9042e+01 - logprior: -1.2166e+00
Epoch 8/10
16/16 - 1s - loss: 99.3900 - loglik: -9.8193e+01 - logprior: -1.1973e+00
Epoch 9/10
16/16 - 1s - loss: 99.7959 - loglik: -9.8619e+01 - logprior: -1.1768e+00
Fitted a model with MAP estimate = -99.5079
Time for alignment: 47.5653
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 151.0086 - loglik: -1.4592e+02 - logprior: -5.0926e+00
Epoch 2/10
16/16 - 1s - loss: 127.4362 - loglik: -1.2587e+02 - logprior: -1.5663e+00
Epoch 3/10
16/16 - 1s - loss: 116.0414 - loglik: -1.1437e+02 - logprior: -1.6700e+00
Epoch 4/10
16/16 - 1s - loss: 110.2641 - loglik: -1.0858e+02 - logprior: -1.6855e+00
Epoch 5/10
16/16 - 1s - loss: 108.7029 - loglik: -1.0706e+02 - logprior: -1.6438e+00
Epoch 6/10
16/16 - 1s - loss: 107.9924 - loglik: -1.0639e+02 - logprior: -1.6060e+00
Epoch 7/10
16/16 - 1s - loss: 107.9021 - loglik: -1.0634e+02 - logprior: -1.5666e+00
Epoch 8/10
16/16 - 1s - loss: 107.5357 - loglik: -1.0598e+02 - logprior: -1.5511e+00
Epoch 9/10
16/16 - 1s - loss: 107.7312 - loglik: -1.0620e+02 - logprior: -1.5330e+00
Fitted a model with MAP estimate = -107.5203
expansions: [(3, 1), (6, 1), (12, 2), (15, 1), (17, 1), (23, 5), (24, 2), (33, 1)]
discards: [0]
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 114.2567 - loglik: -1.0797e+02 - logprior: -6.2865e+00
Epoch 2/2
16/16 - 1s - loss: 105.3986 - loglik: -1.0229e+02 - logprior: -3.1091e+00
Fitted a model with MAP estimate = -104.0428
expansions: [(0, 1)]
discards: [ 0 31 32]
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 106.6958 - loglik: -1.0210e+02 - logprior: -4.6001e+00
Epoch 2/2
16/16 - 1s - loss: 102.6504 - loglik: -1.0102e+02 - logprior: -1.6256e+00
Fitted a model with MAP estimate = -101.9095
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 108.8563 - loglik: -1.0268e+02 - logprior: -6.1792e+00
Epoch 2/10
16/16 - 1s - loss: 104.3579 - loglik: -1.0174e+02 - logprior: -2.6155e+00
Epoch 3/10
16/16 - 1s - loss: 101.6777 - loglik: -1.0019e+02 - logprior: -1.4874e+00
Epoch 4/10
16/16 - 1s - loss: 101.9805 - loglik: -1.0068e+02 - logprior: -1.2967e+00
Fitted a model with MAP estimate = -101.0097
Time for alignment: 41.5904
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 151.0050 - loglik: -1.4592e+02 - logprior: -5.0883e+00
Epoch 2/10
16/16 - 1s - loss: 127.1116 - loglik: -1.2556e+02 - logprior: -1.5522e+00
Epoch 3/10
16/16 - 1s - loss: 116.4925 - loglik: -1.1484e+02 - logprior: -1.6514e+00
Epoch 4/10
16/16 - 1s - loss: 111.5916 - loglik: -1.0993e+02 - logprior: -1.6588e+00
Epoch 5/10
16/16 - 1s - loss: 109.4344 - loglik: -1.0780e+02 - logprior: -1.6306e+00
Epoch 6/10
16/16 - 1s - loss: 108.6913 - loglik: -1.0708e+02 - logprior: -1.6149e+00
Epoch 7/10
16/16 - 1s - loss: 108.5915 - loglik: -1.0703e+02 - logprior: -1.5591e+00
Epoch 8/10
16/16 - 1s - loss: 108.4070 - loglik: -1.0685e+02 - logprior: -1.5559e+00
Epoch 9/10
16/16 - 1s - loss: 108.1899 - loglik: -1.0665e+02 - logprior: -1.5385e+00
Epoch 10/10
16/16 - 1s - loss: 108.1052 - loglik: -1.0657e+02 - logprior: -1.5371e+00
Fitted a model with MAP estimate = -108.0846
expansions: [(3, 1), (6, 1), (12, 2), (13, 1), (18, 1), (20, 1), (23, 6), (25, 2)]
discards: [0]
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 116.3260 - loglik: -1.1005e+02 - logprior: -6.2750e+00
Epoch 2/2
16/16 - 1s - loss: 106.4066 - loglik: -1.0326e+02 - logprior: -3.1448e+00
Fitted a model with MAP estimate = -104.9809
expansions: [(0, 1)]
discards: [ 0 14 31 37]
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 108.1662 - loglik: -1.0356e+02 - logprior: -4.6024e+00
Epoch 2/2
16/16 - 1s - loss: 102.7816 - loglik: -1.0117e+02 - logprior: -1.6164e+00
Fitted a model with MAP estimate = -102.0057
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 109.2807 - loglik: -1.0311e+02 - logprior: -6.1680e+00
Epoch 2/10
16/16 - 1s - loss: 104.1294 - loglik: -1.0138e+02 - logprior: -2.7446e+00
Epoch 3/10
16/16 - 1s - loss: 102.1736 - loglik: -1.0065e+02 - logprior: -1.5260e+00
Epoch 4/10
16/16 - 1s - loss: 101.6798 - loglik: -1.0042e+02 - logprior: -1.2592e+00
Epoch 5/10
16/16 - 1s - loss: 100.7001 - loglik: -9.9464e+01 - logprior: -1.2362e+00
Epoch 6/10
16/16 - 1s - loss: 100.1732 - loglik: -9.8946e+01 - logprior: -1.2274e+00
Epoch 7/10
16/16 - 1s - loss: 100.2752 - loglik: -9.9071e+01 - logprior: -1.2043e+00
Fitted a model with MAP estimate = -99.8267
Time for alignment: 46.2303
Computed alignments with likelihoods: ['-99.5079', '-101.0097', '-99.8267']
Best model has likelihood: -99.5079
SP score = 0.9031
Training of 3 independent models on file blmb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3320315be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2ce15420a0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 155 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 608.4633 - loglik: -6.0570e+02 - logprior: -2.7666e+00
Epoch 2/10
19/19 - 8s - loss: 562.5162 - loglik: -5.6187e+02 - logprior: -6.4621e-01
Epoch 3/10
19/19 - 8s - loss: 534.6769 - loglik: -5.3372e+02 - logprior: -9.5847e-01
Epoch 4/10
19/19 - 8s - loss: 525.1476 - loglik: -5.2424e+02 - logprior: -9.0432e-01
Epoch 5/10
19/19 - 8s - loss: 520.3635 - loglik: -5.1948e+02 - logprior: -8.8672e-01
Epoch 6/10
19/19 - 8s - loss: 516.4460 - loglik: -5.1558e+02 - logprior: -8.6994e-01
Epoch 7/10
19/19 - 8s - loss: 513.8304 - loglik: -5.1295e+02 - logprior: -8.8035e-01
Epoch 8/10
19/19 - 8s - loss: 511.7691 - loglik: -5.1087e+02 - logprior: -8.9420e-01
Epoch 9/10
19/19 - 8s - loss: 510.6202 - loglik: -5.0970e+02 - logprior: -9.1729e-01
Epoch 10/10
19/19 - 8s - loss: 511.0356 - loglik: -5.1011e+02 - logprior: -9.2207e-01
Fitted a model with MAP estimate = -481.7571
expansions: [(6, 1), (7, 1), (11, 1), (13, 1), (21, 2), (25, 2), (27, 2), (54, 1), (55, 1), (70, 1), (111, 2), (122, 4), (123, 1), (125, 5), (127, 1)]
discards: [  0 134 135 136]
Fitting a model of length 177 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 571.7531 - loglik: -5.6798e+02 - logprior: -3.7721e+00
Epoch 2/2
19/19 - 9s - loss: 535.4387 - loglik: -5.3340e+02 - logprior: -2.0377e+00
Fitted a model with MAP estimate = -481.4478
expansions: [(0, 2), (29, 5), (35, 1), (89, 2), (90, 7), (140, 2), (141, 2), (159, 1)]
discards: [  0  26  82  83  84  85  86 124 149 150 151 152 153 154 155 156 157]
Fitting a model of length 182 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 534.0552 - loglik: -5.3126e+02 - logprior: -2.7935e+00
Epoch 2/2
19/19 - 10s - loss: 523.4232 - loglik: -5.2258e+02 - logprior: -8.4376e-01
Fitted a model with MAP estimate = -474.9540
expansions: [(92, 2), (162, 3), (164, 1)]
discards: [ 0 37]
Fitting a model of length 186 on 17200 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 16s - loss: 475.6406 - loglik: -4.7372e+02 - logprior: -1.9191e+00
Epoch 2/10
25/25 - 13s - loss: 467.2220 - loglik: -4.6654e+02 - logprior: -6.8196e-01
Epoch 3/10
25/25 - 13s - loss: 461.7616 - loglik: -4.6129e+02 - logprior: -4.7277e-01
Epoch 4/10
25/25 - 13s - loss: 458.2862 - loglik: -4.5781e+02 - logprior: -4.8084e-01
Epoch 5/10
25/25 - 13s - loss: 455.3299 - loglik: -4.5486e+02 - logprior: -4.6826e-01
Epoch 6/10
25/25 - 13s - loss: 450.4445 - loglik: -4.4997e+02 - logprior: -4.7122e-01
Epoch 7/10
25/25 - 13s - loss: 449.6476 - loglik: -4.4917e+02 - logprior: -4.7593e-01
Epoch 8/10
25/25 - 13s - loss: 446.2806 - loglik: -4.4578e+02 - logprior: -5.0356e-01
Epoch 9/10
25/25 - 13s - loss: 445.5948 - loglik: -4.4508e+02 - logprior: -5.1657e-01
Epoch 10/10
25/25 - 13s - loss: 446.9986 - loglik: -4.4649e+02 - logprior: -5.0650e-01
Fitted a model with MAP estimate = -445.2444
Time for alignment: 323.6527
Fitting a model of length 155 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 608.6128 - loglik: -6.0586e+02 - logprior: -2.7523e+00
Epoch 2/10
19/19 - 8s - loss: 564.0053 - loglik: -5.6338e+02 - logprior: -6.2984e-01
Epoch 3/10
19/19 - 8s - loss: 534.5678 - loglik: -5.3366e+02 - logprior: -9.0454e-01
Epoch 4/10
19/19 - 8s - loss: 525.4552 - loglik: -5.2460e+02 - logprior: -8.5457e-01
Epoch 5/10
19/19 - 8s - loss: 518.5876 - loglik: -5.1775e+02 - logprior: -8.3604e-01
Epoch 6/10
19/19 - 8s - loss: 513.9213 - loglik: -5.1312e+02 - logprior: -8.0385e-01
Epoch 7/10
19/19 - 8s - loss: 511.7391 - loglik: -5.1092e+02 - logprior: -8.1920e-01
Epoch 8/10
19/19 - 8s - loss: 510.4597 - loglik: -5.0960e+02 - logprior: -8.6104e-01
Epoch 9/10
19/19 - 8s - loss: 509.2386 - loglik: -5.0836e+02 - logprior: -8.7907e-01
Epoch 10/10
19/19 - 8s - loss: 509.3897 - loglik: -5.0851e+02 - logprior: -8.7948e-01
Fitted a model with MAP estimate = -481.1984
expansions: [(6, 1), (7, 1), (11, 1), (13, 1), (21, 2), (23, 6), (25, 1), (27, 2), (56, 1), (71, 1), (78, 8), (87, 1), (93, 1), (113, 1), (114, 2), (123, 4), (126, 5)]
discards: []
Fitting a model of length 194 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 566.7932 - loglik: -5.6387e+02 - logprior: -2.9197e+00
Epoch 2/2
19/19 - 11s - loss: 530.1349 - loglik: -5.2889e+02 - logprior: -1.2483e+00
Fitted a model with MAP estimate = -478.2571
expansions: [(41, 1), (87, 1), (157, 2), (158, 2), (162, 1)]
discards: [  0  27  35  88  89  90  91  92  93 102 103 104 164 165 166 167 168 169
 170 171 172 173 174 175]
Fitting a model of length 177 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 534.6566 - loglik: -5.3106e+02 - logprior: -3.5991e+00
Epoch 2/2
19/19 - 9s - loss: 525.0046 - loglik: -5.2336e+02 - logprior: -1.6453e+00
Fitted a model with MAP estimate = -476.8849
expansions: [(0, 2), (93, 2), (150, 2), (151, 2), (156, 5)]
discards: [  0  35 132 160]
Fitting a model of length 186 on 17200 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 18s - loss: 474.4845 - loglik: -4.7295e+02 - logprior: -1.5306e+00
Epoch 2/10
25/25 - 13s - loss: 465.8406 - loglik: -4.6509e+02 - logprior: -7.4609e-01
Epoch 3/10
25/25 - 13s - loss: 461.7260 - loglik: -4.6105e+02 - logprior: -6.7135e-01
Epoch 4/10
25/25 - 13s - loss: 457.3211 - loglik: -4.5668e+02 - logprior: -6.3970e-01
Epoch 5/10
25/25 - 13s - loss: 453.9182 - loglik: -4.5330e+02 - logprior: -6.1662e-01
Epoch 6/10
25/25 - 13s - loss: 450.7833 - loglik: -4.5018e+02 - logprior: -6.0161e-01
Epoch 7/10
25/25 - 13s - loss: 447.3292 - loglik: -4.4672e+02 - logprior: -6.0906e-01
Epoch 8/10
25/25 - 13s - loss: 446.2772 - loglik: -4.4567e+02 - logprior: -6.0956e-01
Epoch 9/10
25/25 - 13s - loss: 444.8305 - loglik: -4.4422e+02 - logprior: -6.1429e-01
Epoch 10/10
25/25 - 13s - loss: 444.9210 - loglik: -4.4431e+02 - logprior: -6.0721e-01
Fitted a model with MAP estimate = -444.4120
Time for alignment: 326.9460
Fitting a model of length 155 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 608.3491 - loglik: -6.0559e+02 - logprior: -2.7548e+00
Epoch 2/10
19/19 - 8s - loss: 564.5912 - loglik: -5.6394e+02 - logprior: -6.5215e-01
Epoch 3/10
19/19 - 8s - loss: 534.9019 - loglik: -5.3394e+02 - logprior: -9.5687e-01
Epoch 4/10
19/19 - 8s - loss: 525.4651 - loglik: -5.2455e+02 - logprior: -9.1961e-01
Epoch 5/10
19/19 - 8s - loss: 519.8923 - loglik: -5.1903e+02 - logprior: -8.6227e-01
Epoch 6/10
19/19 - 8s - loss: 516.2359 - loglik: -5.1540e+02 - logprior: -8.3655e-01
Epoch 7/10
19/19 - 8s - loss: 513.2567 - loglik: -5.1240e+02 - logprior: -8.5169e-01
Epoch 8/10
19/19 - 8s - loss: 511.7171 - loglik: -5.1084e+02 - logprior: -8.7771e-01
Epoch 9/10
19/19 - 8s - loss: 511.1137 - loglik: -5.1022e+02 - logprior: -8.9836e-01
Epoch 10/10
19/19 - 8s - loss: 510.6982 - loglik: -5.0979e+02 - logprior: -9.0341e-01
Fitted a model with MAP estimate = -484.7431
expansions: [(7, 1), (11, 1), (13, 1), (21, 2), (25, 1), (27, 2), (55, 2), (56, 1), (70, 2), (76, 8), (110, 3), (111, 2), (120, 3), (123, 2), (125, 1)]
discards: [136 137]
Fitting a model of length 185 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 572.6318 - loglik: -5.6969e+02 - logprior: -2.9435e+00
Epoch 2/2
19/19 - 10s - loss: 535.4983 - loglik: -5.3418e+02 - logprior: -1.3176e+00
Fitted a model with MAP estimate = -481.3999
expansions: [(6, 1), (29, 6), (34, 1), (149, 2), (150, 5)]
discards: [  0  26  65  82  83  84  85  86  87  96  97  98 132 154 155 156 157 158
 159 160 161 162 163 164 165 166]
Fitting a model of length 174 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 539.0272 - loglik: -5.3540e+02 - logprior: -3.6238e+00
Epoch 2/2
19/19 - 9s - loss: 527.8577 - loglik: -5.2620e+02 - logprior: -1.6615e+00
Fitted a model with MAP estimate = -478.2351
expansions: [(0, 2), (89, 1), (93, 3), (125, 1), (154, 5), (157, 2)]
discards: [  0  28 129 130 148]
Fitting a model of length 183 on 17200 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 16s - loss: 475.5745 - loglik: -4.7406e+02 - logprior: -1.5165e+00
Epoch 2/10
25/25 - 13s - loss: 466.3684 - loglik: -4.6563e+02 - logprior: -7.3972e-01
Epoch 3/10
25/25 - 13s - loss: 462.1223 - loglik: -4.6146e+02 - logprior: -6.6146e-01
Epoch 4/10
25/25 - 13s - loss: 458.5767 - loglik: -4.5796e+02 - logprior: -6.1351e-01
Epoch 5/10
25/25 - 13s - loss: 454.6229 - loglik: -4.5403e+02 - logprior: -5.9086e-01
Epoch 6/10
25/25 - 13s - loss: 452.0140 - loglik: -4.5143e+02 - logprior: -5.8006e-01
Epoch 7/10
25/25 - 13s - loss: 448.3973 - loglik: -4.4783e+02 - logprior: -5.7071e-01
Epoch 8/10
25/25 - 13s - loss: 446.8221 - loglik: -4.4625e+02 - logprior: -5.6838e-01
Epoch 9/10
25/25 - 13s - loss: 445.8127 - loglik: -4.4524e+02 - logprior: -5.7039e-01
Epoch 10/10
25/25 - 13s - loss: 445.0277 - loglik: -4.4446e+02 - logprior: -5.6490e-01
Fitted a model with MAP estimate = -445.3086
Time for alignment: 318.2301
Computed alignments with likelihoods: ['-445.2444', '-444.4120', '-445.3086']
Best model has likelihood: -444.4120
SP score = 0.6747
Training of 3 independent models on file proteasome.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f345820e6d0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f345820e3d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345820edc0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824a0d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ac40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f345824aac0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a1c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a3a0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a8b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a910>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a820>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824abe0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824af10>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a4c0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824ad90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f345824ad60> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f345824ab20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f345824a340> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f345824d790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2cef293310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f34f42074f0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : True , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 490.7928 - loglik: -4.8486e+02 - logprior: -5.9305e+00
Epoch 2/10
14/14 - 4s - loss: 437.0572 - loglik: -4.3582e+02 - logprior: -1.2369e+00
Epoch 3/10
14/14 - 4s - loss: 401.2813 - loglik: -4.0000e+02 - logprior: -1.2809e+00
Epoch 4/10
14/14 - 4s - loss: 386.5448 - loglik: -3.8534e+02 - logprior: -1.2005e+00
Epoch 5/10
14/14 - 4s - loss: 380.2076 - loglik: -3.7903e+02 - logprior: -1.1817e+00
Epoch 6/10
14/14 - 4s - loss: 376.8459 - loglik: -3.7565e+02 - logprior: -1.1942e+00
Epoch 7/10
14/14 - 4s - loss: 375.8478 - loglik: -3.7469e+02 - logprior: -1.1628e+00
Epoch 8/10
14/14 - 4s - loss: 374.6713 - loglik: -3.7350e+02 - logprior: -1.1674e+00
Epoch 9/10
14/14 - 4s - loss: 373.2756 - loglik: -3.7210e+02 - logprior: -1.1743e+00
Epoch 10/10
14/14 - 4s - loss: 372.7840 - loglik: -3.7158e+02 - logprior: -1.2040e+00
Fitted a model with MAP estimate = -372.3270
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (57, 1), (69, 1), (71, 1), (82, 1), (89, 1), (102, 4), (117, 1), (120, 2), (129, 1), (131, 1), (133, 2), (136, 1), (139, 1)]
discards: []
Fitting a model of length 180 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 401.5226 - loglik: -3.9435e+02 - logprior: -7.1683e+00
Epoch 2/2
14/14 - 5s - loss: 372.3926 - loglik: -3.7087e+02 - logprior: -1.5265e+00
Fitted a model with MAP estimate = -367.2426
expansions: [(125, 2), (128, 1)]
discards: [ 41 146 164]
Fitting a model of length 180 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 10s - loss: 375.5676 - loglik: -3.7038e+02 - logprior: -5.1828e+00
Epoch 2/2
14/14 - 5s - loss: 366.6475 - loglik: -3.6545e+02 - logprior: -1.1925e+00
Fitted a model with MAP estimate = -364.3337
expansions: []
discards: [  0 111]
Fitting a model of length 178 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 376.6728 - loglik: -3.6974e+02 - logprior: -6.9374e+00
Epoch 2/10
14/14 - 5s - loss: 368.4555 - loglik: -3.6580e+02 - logprior: -2.6506e+00
Epoch 3/10
14/14 - 5s - loss: 364.9551 - loglik: -3.6348e+02 - logprior: -1.4796e+00
Epoch 4/10
14/14 - 5s - loss: 361.6516 - loglik: -3.6124e+02 - logprior: -4.0717e-01
Epoch 5/10
14/14 - 5s - loss: 359.0092 - loglik: -3.5873e+02 - logprior: -2.8024e-01
Epoch 6/10
14/14 - 5s - loss: 355.9762 - loglik: -3.5571e+02 - logprior: -2.6308e-01
Epoch 7/10
14/14 - 5s - loss: 354.9693 - loglik: -3.5471e+02 - logprior: -2.5816e-01
Epoch 8/10
14/14 - 5s - loss: 354.2381 - loglik: -3.5401e+02 - logprior: -2.2689e-01
Epoch 9/10
14/14 - 5s - loss: 352.8791 - loglik: -3.5268e+02 - logprior: -1.9648e-01
Epoch 10/10
14/14 - 5s - loss: 352.0090 - loglik: -3.5186e+02 - logprior: -1.5339e-01
Fitted a model with MAP estimate = -351.6638
Time for alignment: 141.1925
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 491.4182 - loglik: -4.8547e+02 - logprior: -5.9438e+00
Epoch 2/10
14/14 - 4s - loss: 435.3262 - loglik: -4.3407e+02 - logprior: -1.2543e+00
Epoch 3/10
14/14 - 4s - loss: 401.1206 - loglik: -3.9987e+02 - logprior: -1.2555e+00
Epoch 4/10
14/14 - 4s - loss: 388.0751 - loglik: -3.8696e+02 - logprior: -1.1148e+00
Epoch 5/10
14/14 - 4s - loss: 382.3965 - loglik: -3.8129e+02 - logprior: -1.1059e+00
Epoch 6/10
14/14 - 4s - loss: 380.4509 - loglik: -3.7933e+02 - logprior: -1.1162e+00
Epoch 7/10
14/14 - 4s - loss: 377.9172 - loglik: -3.7681e+02 - logprior: -1.1113e+00
Epoch 8/10
14/14 - 4s - loss: 377.5893 - loglik: -3.7648e+02 - logprior: -1.1104e+00
Epoch 9/10
14/14 - 4s - loss: 376.2862 - loglik: -3.7519e+02 - logprior: -1.0995e+00
Epoch 10/10
14/14 - 4s - loss: 376.0653 - loglik: -3.7497e+02 - logprior: -1.0991e+00
Fitted a model with MAP estimate = -375.2893
expansions: [(0, 3), (22, 1), (23, 2), (24, 2), (25, 1), (26, 1), (32, 2), (47, 1), (50, 1), (52, 1), (53, 1), (56, 1), (66, 1), (69, 1), (71, 2), (72, 1), (89, 1), (102, 4), (117, 1), (120, 2), (129, 1), (131, 1), (133, 2), (134, 1), (139, 1)]
discards: []
Fitting a model of length 182 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 405.4512 - loglik: -3.9828e+02 - logprior: -7.1717e+00
Epoch 2/2
14/14 - 5s - loss: 375.4968 - loglik: -3.7392e+02 - logprior: -1.5735e+00
Fitted a model with MAP estimate = -370.2613
expansions: [(127, 2), (130, 1)]
discards: [ 25  30  42  93 148 166]
Fitting a model of length 179 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 378.7769 - loglik: -3.7358e+02 - logprior: -5.1943e+00
Epoch 2/2
14/14 - 5s - loss: 369.3821 - loglik: -3.6817e+02 - logprior: -1.2152e+00
Fitted a model with MAP estimate = -367.1248
expansions: [(94, 1)]
discards: [0]
Fitting a model of length 179 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 378.4953 - loglik: -3.7158e+02 - logprior: -6.9184e+00
Epoch 2/10
14/14 - 5s - loss: 371.2017 - loglik: -3.6856e+02 - logprior: -2.6442e+00
Epoch 3/10
14/14 - 5s - loss: 366.6617 - loglik: -3.6520e+02 - logprior: -1.4584e+00
Epoch 4/10
14/14 - 5s - loss: 363.6113 - loglik: -3.6320e+02 - logprior: -4.0706e-01
Epoch 5/10
14/14 - 5s - loss: 361.0091 - loglik: -3.6072e+02 - logprior: -2.9311e-01
Epoch 6/10
14/14 - 5s - loss: 359.3747 - loglik: -3.5910e+02 - logprior: -2.7964e-01
Epoch 7/10
14/14 - 5s - loss: 356.0930 - loglik: -3.5583e+02 - logprior: -2.6737e-01
Epoch 8/10
14/14 - 5s - loss: 356.6067 - loglik: -3.5636e+02 - logprior: -2.4793e-01
Fitted a model with MAP estimate = -355.6178
Time for alignment: 131.8451
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 491.0201 - loglik: -4.8508e+02 - logprior: -5.9364e+00
Epoch 2/10
14/14 - 4s - loss: 434.0345 - loglik: -4.3279e+02 - logprior: -1.2441e+00
Epoch 3/10
14/14 - 4s - loss: 397.8407 - loglik: -3.9659e+02 - logprior: -1.2490e+00
Epoch 4/10
14/14 - 4s - loss: 383.6339 - loglik: -3.8251e+02 - logprior: -1.1254e+00
Epoch 5/10
14/14 - 4s - loss: 378.8938 - loglik: -3.7779e+02 - logprior: -1.1011e+00
Epoch 6/10
14/14 - 4s - loss: 375.2727 - loglik: -3.7417e+02 - logprior: -1.1073e+00
Epoch 7/10
14/14 - 4s - loss: 373.6993 - loglik: -3.7261e+02 - logprior: -1.0902e+00
Epoch 8/10
14/14 - 4s - loss: 373.3288 - loglik: -3.7224e+02 - logprior: -1.0893e+00
Epoch 9/10
14/14 - 4s - loss: 372.3664 - loglik: -3.7127e+02 - logprior: -1.0988e+00
Epoch 10/10
14/14 - 4s - loss: 371.6752 - loglik: -3.7056e+02 - logprior: -1.1143e+00
Fitted a model with MAP estimate = -371.4797
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (66, 1), (71, 4), (72, 1), (89, 2), (102, 4), (117, 1), (120, 2), (128, 1), (131, 1), (133, 1), (136, 1), (139, 1)]
discards: []
Fitting a model of length 182 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 401.1652 - loglik: -3.9398e+02 - logprior: -7.1828e+00
Epoch 2/2
14/14 - 5s - loss: 370.7240 - loglik: -3.6914e+02 - logprior: -1.5795e+00
Fitted a model with MAP estimate = -365.3010
expansions: [(128, 2), (131, 1)]
discards: [ 41  91 112 115 149]
Fitting a model of length 180 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 374.3660 - loglik: -3.6917e+02 - logprior: -5.1957e+00
Epoch 2/2
14/14 - 5s - loss: 365.0030 - loglik: -3.6377e+02 - logprior: -1.2347e+00
Fitted a model with MAP estimate = -363.1527
expansions: [(90, 2)]
discards: [0]
Fitting a model of length 181 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 374.1045 - loglik: -3.6713e+02 - logprior: -6.9760e+00
Epoch 2/10
14/14 - 5s - loss: 366.6109 - loglik: -3.6388e+02 - logprior: -2.7357e+00
Epoch 3/10
14/14 - 5s - loss: 362.3288 - loglik: -3.6076e+02 - logprior: -1.5702e+00
Epoch 4/10
14/14 - 5s - loss: 358.4232 - loglik: -3.5794e+02 - logprior: -4.8625e-01
Epoch 5/10
14/14 - 5s - loss: 355.5275 - loglik: -3.5516e+02 - logprior: -3.6920e-01
Epoch 6/10
14/14 - 5s - loss: 354.0829 - loglik: -3.5373e+02 - logprior: -3.5390e-01
Epoch 7/10
14/14 - 5s - loss: 352.5583 - loglik: -3.5220e+02 - logprior: -3.5848e-01
Epoch 8/10
14/14 - 5s - loss: 350.7713 - loglik: -3.5045e+02 - logprior: -3.2058e-01
Epoch 9/10
14/14 - 5s - loss: 350.3355 - loglik: -3.5004e+02 - logprior: -2.9546e-01
Epoch 10/10
14/14 - 5s - loss: 349.2933 - loglik: -3.4904e+02 - logprior: -2.4909e-01
Fitted a model with MAP estimate = -349.3100
Time for alignment: 142.6097
Computed alignments with likelihoods: ['-351.6638', '-355.6178', '-349.3100']
Best model has likelihood: -349.3100
SP score = 0.7998
