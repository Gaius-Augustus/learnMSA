Training of 3 independent models on file PF00079.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b83836eb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3bec462700>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3bec46ffd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b9d43f880>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b8c7325b0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b72fbf850>, <__main__.SimpleDirichletPrior object at 0x7f399043c730>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3b83abf5e0>

Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 768.7359 - loglik: -7.6655e+02 - logprior: -1.7421e+00
Epoch 2/10
39/39 - 35s - loss: 640.4393 - loglik: -6.3641e+02 - logprior: -2.3194e+00
Epoch 3/10
39/39 - 35s - loss: 630.4583 - loglik: -6.2639e+02 - logprior: -2.3104e+00
Epoch 4/10
39/39 - 37s - loss: 626.5866 - loglik: -6.2282e+02 - logprior: -2.3342e+00
Epoch 5/10
39/39 - 37s - loss: 625.6061 - loglik: -6.2194e+02 - logprior: -2.3596e+00
Epoch 6/10
39/39 - 38s - loss: 624.9854 - loglik: -6.2135e+02 - logprior: -2.4229e+00
Epoch 7/10
39/39 - 39s - loss: 624.0025 - loglik: -6.2044e+02 - logprior: -2.4459e+00
Epoch 8/10
39/39 - 39s - loss: 623.4619 - loglik: -6.1997e+02 - logprior: -2.4704e+00
Epoch 9/10
39/39 - 40s - loss: 622.9692 - loglik: -6.1954e+02 - logprior: -2.4735e+00
Epoch 10/10
39/39 - 40s - loss: 623.2173 - loglik: -6.1957e+02 - logprior: -2.7754e+00
Fitted a model with MAP estimate = -621.2963
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (16, 1), (49, 1), (51, 4), (55, 1), (61, 2), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (76, 1), (77, 1), (82, 1), (83, 1), (84, 1), (85, 1), (90, 1), (94, 1), (97, 1), (98, 1), (99, 1), (112, 1), (118, 1), (119, 1), (120, 1), (133, 1), (139, 1), (142, 1), (145, 1), (158, 1), (161, 1), (163, 1), (164, 1), (166, 1), (167, 1), (173, 1), (186, 2), (189, 1), (192, 1), (194, 1), (195, 1), (196, 1), (198, 1), (199, 2), (200, 1), (214, 1), (215, 1), (218, 1), (220, 1), (224, 1), (229, 1), (235, 1), (236, 2), (238, 1), (263, 1), (264, 1), (265, 1), (266, 3), (267, 2), (268, 1), (277, 1), (279, 1), (280, 3)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 368 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 63s - loss: 603.1385 - loglik: -6.0014e+02 - logprior: -2.1315e+00
Epoch 2/2
39/39 - 61s - loss: 593.0964 - loglik: -5.9109e+02 - logprior: -1.0402e+00
Fitted a model with MAP estimate = -589.5847
expansions: []
discards: [ 57  74 251]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 63s - loss: 592.2183 - loglik: -5.8930e+02 - logprior: -1.8591e+00
Epoch 2/2
39/39 - 59s - loss: 591.8898 - loglik: -5.9001e+02 - logprior: -7.9103e-01
Fitted a model with MAP estimate = -588.5305
expansions: [(145, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 366 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 63s - loss: 590.7260 - loglik: -5.8807e+02 - logprior: -1.6042e+00
Epoch 2/10
39/39 - 59s - loss: 590.4225 - loglik: -5.8891e+02 - logprior: -4.5588e-01
Epoch 3/10
39/39 - 60s - loss: 589.0839 - loglik: -5.8776e+02 - logprior: -2.9495e-01
Epoch 4/10
39/39 - 62s - loss: 588.4797 - loglik: -5.8747e+02 - logprior: -1.0690e-01
Epoch 5/10
39/39 - 63s - loss: 588.0283 - loglik: -5.8719e+02 - logprior: 0.0087
Epoch 6/10
39/39 - 65s - loss: 587.8964 - loglik: -5.8733e+02 - logprior: 0.2179
Epoch 7/10
39/39 - 65s - loss: 587.6573 - loglik: -5.8729e+02 - logprior: 0.3810
Epoch 8/10
39/39 - 66s - loss: 587.1639 - loglik: -5.8700e+02 - logprior: 0.5455
Epoch 9/10
39/39 - 68s - loss: 586.9293 - loglik: -5.8697e+02 - logprior: 0.7141
Epoch 10/10
39/39 - 71s - loss: 587.2655 - loglik: -5.8752e+02 - logprior: 0.8932
Fitted a model with MAP estimate = -585.6557
Time for alignment: 1485.9943
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 768.9565 - loglik: -7.6679e+02 - logprior: -1.7369e+00
Epoch 2/10
39/39 - 46s - loss: 639.9021 - loglik: -6.3617e+02 - logprior: -2.4300e+00
Epoch 3/10
39/39 - 46s - loss: 629.6564 - loglik: -6.2568e+02 - logprior: -2.4274e+00
Epoch 4/10
39/39 - 45s - loss: 627.6458 - loglik: -6.2393e+02 - logprior: -2.3472e+00
Epoch 5/10
39/39 - 45s - loss: 626.6586 - loglik: -6.2303e+02 - logprior: -2.3670e+00
Epoch 6/10
39/39 - 43s - loss: 624.8529 - loglik: -6.2125e+02 - logprior: -2.3913e+00
Epoch 7/10
39/39 - 41s - loss: 624.8843 - loglik: -6.2133e+02 - logprior: -2.4086e+00
Fitted a model with MAP estimate = -622.7132
expansions: [(9, 1), (12, 1), (14, 1), (15, 1), (51, 1), (52, 3), (56, 1), (62, 2), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (77, 1), (78, 1), (81, 1), (83, 1), (84, 1), (85, 1), (90, 1), (91, 1), (98, 1), (99, 1), (100, 1), (105, 1), (106, 1), (117, 1), (119, 1), (121, 1), (134, 1), (140, 1), (141, 1), (142, 1), (146, 1), (158, 1), (159, 1), (160, 1), (162, 1), (164, 1), (165, 2), (173, 1), (184, 1), (187, 1), (191, 1), (193, 1), (194, 1), (195, 1), (197, 1), (198, 2), (199, 1), (213, 1), (214, 1), (217, 1), (219, 1), (223, 1), (228, 1), (234, 2), (235, 2), (263, 1), (264, 2), (265, 1), (266, 2), (267, 3), (268, 1), (277, 1), (278, 2), (279, 1), (280, 1)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 369 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 63s - loss: 605.6960 - loglik: -6.0228e+02 - logprior: -2.2389e+00
Epoch 2/2
39/39 - 63s - loss: 593.6900 - loglik: -5.9122e+02 - logprior: -1.2335e+00
Fitted a model with MAP estimate = -590.1880
expansions: [(207, 1)]
discards: [ 73 250 296 338]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 366 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 69s - loss: 593.0985 - loglik: -5.8981e+02 - logprior: -1.8740e+00
Epoch 2/2
39/39 - 66s - loss: 592.3561 - loglik: -5.9020e+02 - logprior: -7.9810e-01
Fitted a model with MAP estimate = -588.7443
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 366 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 69s - loss: 591.7560 - loglik: -5.8872e+02 - logprior: -1.6709e+00
Epoch 2/10
39/39 - 67s - loss: 591.1230 - loglik: -5.8936e+02 - logprior: -4.7813e-01
Epoch 3/10
39/39 - 61s - loss: 589.9246 - loglik: -5.8838e+02 - logprior: -2.6403e-01
Epoch 4/10
39/39 - 59s - loss: 589.0670 - loglik: -5.8786e+02 - logprior: -1.0957e-01
Epoch 5/10
39/39 - 58s - loss: 589.0984 - loglik: -5.8812e+02 - logprior: 0.0405
Fitted a model with MAP estimate = -587.0055
Time for alignment: 1138.1112
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 44s - loss: 769.2125 - loglik: -7.6701e+02 - logprior: -1.7643e+00
Epoch 2/10
39/39 - 41s - loss: 638.7177 - loglik: -6.3450e+02 - logprior: -2.4344e+00
Epoch 3/10
39/39 - 41s - loss: 628.2752 - loglik: -6.2411e+02 - logprior: -2.4141e+00
Epoch 4/10
39/39 - 41s - loss: 625.2430 - loglik: -6.2141e+02 - logprior: -2.3976e+00
Epoch 5/10
39/39 - 41s - loss: 623.7772 - loglik: -6.2007e+02 - logprior: -2.4078e+00
Epoch 6/10
39/39 - 41s - loss: 623.2560 - loglik: -6.1963e+02 - logprior: -2.4209e+00
Epoch 7/10
39/39 - 41s - loss: 623.1478 - loglik: -6.1959e+02 - logprior: -2.4370e+00
Epoch 8/10
39/39 - 41s - loss: 622.4159 - loglik: -6.1893e+02 - logprior: -2.4545e+00
Epoch 9/10
39/39 - 41s - loss: 621.7003 - loglik: -6.1827e+02 - logprior: -2.4580e+00
Epoch 10/10
39/39 - 41s - loss: 621.3472 - loglik: -6.1797e+02 - logprior: -2.4630e+00
Fitted a model with MAP estimate = -619.9207
expansions: [(13, 1), (14, 1), (15, 1), (46, 1), (52, 4), (56, 1), (62, 1), (64, 1), (65, 1), (66, 1), (68, 1), (69, 1), (78, 1), (79, 1), (82, 1), (84, 1), (85, 1), (86, 1), (91, 1), (92, 1), (95, 1), (98, 1), (100, 1), (103, 1), (106, 1), (108, 1), (117, 1), (120, 1), (121, 1), (135, 1), (140, 1), (141, 1), (142, 1), (146, 1), (158, 1), (159, 1), (160, 1), (162, 1), (163, 1), (164, 2), (165, 1), (171, 1), (172, 1), (183, 1), (187, 1), (193, 1), (194, 1), (195, 1), (197, 1), (198, 2), (199, 1), (200, 1), (213, 1), (217, 1), (219, 1), (223, 1), (228, 1), (234, 1), (235, 2), (237, 1), (239, 1), (261, 1), (262, 1), (263, 1), (265, 2), (266, 3), (267, 1), (278, 2), (279, 1), (280, 1)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 368 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 63s - loss: 603.0542 - loglik: -5.9992e+02 - logprior: -2.2256e+00
Epoch 2/2
39/39 - 60s - loss: 592.4870 - loglik: -5.9037e+02 - logprior: -1.1074e+00
Fitted a model with MAP estimate = -589.3218
expansions: []
discards: [ 56 250 335 337]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 364 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 64s - loss: 592.4492 - loglik: -5.8950e+02 - logprior: -1.8692e+00
Epoch 2/2
39/39 - 61s - loss: 591.8463 - loglik: -5.8997e+02 - logprior: -7.8820e-01
Fitted a model with MAP estimate = -588.6401
expansions: [(17, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 65s - loss: 590.8735 - loglik: -5.8816e+02 - logprior: -1.6508e+00
Epoch 2/10
39/39 - 61s - loss: 590.4120 - loglik: -5.8893e+02 - logprior: -4.4417e-01
Epoch 3/10
39/39 - 61s - loss: 589.3759 - loglik: -5.8808e+02 - logprior: -2.7051e-01
Epoch 4/10
39/39 - 60s - loss: 588.8401 - loglik: -5.8780e+02 - logprior: -1.4557e-01
Epoch 5/10
39/39 - 60s - loss: 588.3840 - loglik: -5.8762e+02 - logprior: 0.0865
Epoch 6/10
39/39 - 59s - loss: 588.0233 - loglik: -5.8746e+02 - logprior: 0.2141
Epoch 7/10
39/39 - 60s - loss: 587.9167 - loglik: -5.8754e+02 - logprior: 0.3777
Epoch 8/10
39/39 - 60s - loss: 587.5248 - loglik: -5.8734e+02 - logprior: 0.5140
Epoch 9/10
39/39 - 60s - loss: 587.6224 - loglik: -5.8768e+02 - logprior: 0.7324
Fitted a model with MAP estimate = -586.1763
Time for alignment: 1433.0419
Computed alignments with likelihoods: ['-585.6557', '-587.0055', '-586.1763']
Best model has likelihood: -585.6557
time for generating output: 0.3584
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00079.projection.fasta
SP score = 0.9215686274509803
Training of 3 independent models on file PF01381.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b8c7325b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b9d1ab640>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3bec46f040>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3bec46fdf0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b9d43f880>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3bec462700>, <__main__.SimpleDirichletPrior object at 0x7f3bc7b3d820>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3bc7a9c0d0>

Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.1666 - loglik: -1.4691e+02 - logprior: -3.2265e+00
Epoch 2/10
19/19 - 1s - loss: 128.5652 - loglik: -1.2679e+02 - logprior: -1.4191e+00
Epoch 3/10
19/19 - 1s - loss: 119.4135 - loglik: -1.1728e+02 - logprior: -1.6649e+00
Epoch 4/10
19/19 - 1s - loss: 117.5975 - loglik: -1.1573e+02 - logprior: -1.5428e+00
Epoch 5/10
19/19 - 1s - loss: 116.8919 - loglik: -1.1511e+02 - logprior: -1.5181e+00
Epoch 6/10
19/19 - 1s - loss: 116.7960 - loglik: -1.1508e+02 - logprior: -1.4798e+00
Epoch 7/10
19/19 - 1s - loss: 116.4723 - loglik: -1.1478e+02 - logprior: -1.4719e+00
Epoch 8/10
19/19 - 1s - loss: 116.4560 - loglik: -1.1477e+02 - logprior: -1.4644e+00
Epoch 9/10
19/19 - 1s - loss: 116.3051 - loglik: -1.1464e+02 - logprior: -1.4544e+00
Epoch 10/10
19/19 - 1s - loss: 116.3413 - loglik: -1.1468e+02 - logprior: -1.4530e+00
Fitted a model with MAP estimate = -115.8406
expansions: [(6, 1), (7, 1), (8, 1), (16, 1), (19, 2), (26, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 119.4712 - loglik: -1.1505e+02 - logprior: -4.2073e+00
Epoch 2/2
19/19 - 1s - loss: 111.6321 - loglik: -1.0922e+02 - logprior: -2.1818e+00
Fitted a model with MAP estimate = -109.9594
expansions: [(0, 2)]
discards: [ 0 23 34 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 110.8899 - loglik: -1.0749e+02 - logprior: -3.0984e+00
Epoch 2/2
19/19 - 1s - loss: 107.8801 - loglik: -1.0626e+02 - logprior: -1.2996e+00
Fitted a model with MAP estimate = -106.9984
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 111.8917 - loglik: -1.0775e+02 - logprior: -3.8200e+00
Epoch 2/10
19/19 - 1s - loss: 108.2189 - loglik: -1.0641e+02 - logprior: -1.5003e+00
Epoch 3/10
19/19 - 1s - loss: 107.7905 - loglik: -1.0612e+02 - logprior: -1.3542e+00
Epoch 4/10
19/19 - 1s - loss: 107.5110 - loglik: -1.0590e+02 - logprior: -1.3090e+00
Epoch 5/10
19/19 - 1s - loss: 107.4792 - loglik: -1.0591e+02 - logprior: -1.2825e+00
Epoch 6/10
19/19 - 1s - loss: 107.5481 - loglik: -1.0600e+02 - logprior: -1.2727e+00
Fitted a model with MAP estimate = -107.0363
Time for alignment: 47.5890
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 150.0376 - loglik: -1.4679e+02 - logprior: -3.2254e+00
Epoch 2/10
19/19 - 1s - loss: 128.5255 - loglik: -1.2675e+02 - logprior: -1.4074e+00
Epoch 3/10
19/19 - 1s - loss: 119.9887 - loglik: -1.1782e+02 - logprior: -1.6289e+00
Epoch 4/10
19/19 - 1s - loss: 117.4260 - loglik: -1.1557e+02 - logprior: -1.5277e+00
Epoch 5/10
19/19 - 1s - loss: 116.8120 - loglik: -1.1506e+02 - logprior: -1.5092e+00
Epoch 6/10
19/19 - 1s - loss: 116.4273 - loglik: -1.1472e+02 - logprior: -1.4885e+00
Epoch 7/10
19/19 - 1s - loss: 116.1959 - loglik: -1.1452e+02 - logprior: -1.4836e+00
Epoch 8/10
19/19 - 1s - loss: 116.0301 - loglik: -1.1436e+02 - logprior: -1.4738e+00
Epoch 9/10
19/19 - 1s - loss: 116.0654 - loglik: -1.1439e+02 - logprior: -1.4709e+00
Fitted a model with MAP estimate = -115.5691
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (19, 2), (26, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 119.4153 - loglik: -1.1501e+02 - logprior: -4.2054e+00
Epoch 2/2
19/19 - 1s - loss: 111.7411 - loglik: -1.0934e+02 - logprior: -2.1797e+00
Fitted a model with MAP estimate = -110.0827
expansions: [(0, 2)]
discards: [ 0 23 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 111.0488 - loglik: -1.0765e+02 - logprior: -3.1144e+00
Epoch 2/2
19/19 - 1s - loss: 107.9300 - loglik: -1.0630e+02 - logprior: -1.3132e+00
Fitted a model with MAP estimate = -107.0863
expansions: []
discards: [ 0 34]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 112.0637 - loglik: -1.0790e+02 - logprior: -3.8360e+00
Epoch 2/10
19/19 - 1s - loss: 108.3360 - loglik: -1.0652e+02 - logprior: -1.5035e+00
Epoch 3/10
19/19 - 1s - loss: 107.7793 - loglik: -1.0611e+02 - logprior: -1.3479e+00
Epoch 4/10
19/19 - 1s - loss: 107.6659 - loglik: -1.0605e+02 - logprior: -1.3112e+00
Epoch 5/10
19/19 - 1s - loss: 107.4446 - loglik: -1.0587e+02 - logprior: -1.2833e+00
Epoch 6/10
19/19 - 1s - loss: 107.4526 - loglik: -1.0589e+02 - logprior: -1.2720e+00
Fitted a model with MAP estimate = -107.0392
Time for alignment: 46.5365
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.0597 - loglik: -1.4681e+02 - logprior: -3.2260e+00
Epoch 2/10
19/19 - 1s - loss: 128.0361 - loglik: -1.2624e+02 - logprior: -1.4234e+00
Epoch 3/10
19/19 - 1s - loss: 119.4364 - loglik: -1.1730e+02 - logprior: -1.6399e+00
Epoch 4/10
19/19 - 1s - loss: 117.3008 - loglik: -1.1542e+02 - logprior: -1.5425e+00
Epoch 5/10
19/19 - 1s - loss: 116.5154 - loglik: -1.1473e+02 - logprior: -1.5427e+00
Epoch 6/10
19/19 - 1s - loss: 116.3056 - loglik: -1.1456e+02 - logprior: -1.5159e+00
Epoch 7/10
19/19 - 1s - loss: 116.0568 - loglik: -1.1434e+02 - logprior: -1.4993e+00
Epoch 8/10
19/19 - 1s - loss: 115.8804 - loglik: -1.1418e+02 - logprior: -1.4880e+00
Epoch 9/10
19/19 - 1s - loss: 116.0297 - loglik: -1.1433e+02 - logprior: -1.4796e+00
Fitted a model with MAP estimate = -115.4616
expansions: [(6, 1), (7, 1), (8, 2), (15, 1), (19, 2), (20, 1), (27, 2), (28, 2), (29, 2), (30, 2), (31, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 121.0110 - loglik: -1.1655e+02 - logprior: -4.2468e+00
Epoch 2/2
19/19 - 1s - loss: 111.9634 - loglik: -1.0944e+02 - logprior: -2.2711e+00
Fitted a model with MAP estimate = -110.1521
expansions: [(0, 2)]
discards: [ 0  9 24 35 37 41 46]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 110.9616 - loglik: -1.0756e+02 - logprior: -3.1016e+00
Epoch 2/2
19/19 - 1s - loss: 107.9030 - loglik: -1.0629e+02 - logprior: -1.3001e+00
Fitted a model with MAP estimate = -107.0113
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 111.9544 - loglik: -1.0779e+02 - logprior: -3.8339e+00
Epoch 2/10
19/19 - 1s - loss: 108.3392 - loglik: -1.0652e+02 - logprior: -1.5075e+00
Epoch 3/10
19/19 - 1s - loss: 107.7112 - loglik: -1.0604e+02 - logprior: -1.3539e+00
Epoch 4/10
19/19 - 1s - loss: 107.5710 - loglik: -1.0595e+02 - logprior: -1.3103e+00
Epoch 5/10
19/19 - 1s - loss: 107.4318 - loglik: -1.0585e+02 - logprior: -1.2865e+00
Epoch 6/10
19/19 - 1s - loss: 107.4221 - loglik: -1.0587e+02 - logprior: -1.2655e+00
Epoch 7/10
19/19 - 1s - loss: 107.3367 - loglik: -1.0579e+02 - logprior: -1.2585e+00
Epoch 8/10
19/19 - 1s - loss: 107.4253 - loglik: -1.0591e+02 - logprior: -1.2369e+00
Fitted a model with MAP estimate = -106.9449
Time for alignment: 49.5190
Computed alignments with likelihoods: ['-106.9984', '-107.0392', '-106.9449']
Best model has likelihood: -106.9449
time for generating output: 0.1176
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01381.projection.fasta
SP score = 0.6331238701902384
Training of 3 independent models on file PF02878.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f38ac1692e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f38ac1544f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f38ac15ebb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f38ac3f6700>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f38ac233e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3bec315a90>, <__main__.SimpleDirichletPrior object at 0x7f37fcd8f640>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3bc78708b0>

Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 360.5263 - loglik: -3.5738e+02 - logprior: -3.0805e+00
Epoch 2/10
19/19 - 3s - loss: 301.4868 - loglik: -2.9999e+02 - logprior: -1.2963e+00
Epoch 3/10
19/19 - 3s - loss: 275.4740 - loglik: -2.7330e+02 - logprior: -1.5357e+00
Epoch 4/10
19/19 - 4s - loss: 271.3697 - loglik: -2.6912e+02 - logprior: -1.5207e+00
Epoch 5/10
19/19 - 4s - loss: 268.9661 - loglik: -2.6686e+02 - logprior: -1.4737e+00
Epoch 6/10
19/19 - 4s - loss: 268.1031 - loglik: -2.6611e+02 - logprior: -1.4373e+00
Epoch 7/10
19/19 - 4s - loss: 267.6420 - loglik: -2.6572e+02 - logprior: -1.4129e+00
Epoch 8/10
19/19 - 4s - loss: 267.4545 - loglik: -2.6557e+02 - logprior: -1.4082e+00
Epoch 9/10
19/19 - 4s - loss: 267.0315 - loglik: -2.6516e+02 - logprior: -1.4107e+00
Epoch 10/10
19/19 - 4s - loss: 267.1351 - loglik: -2.6527e+02 - logprior: -1.4001e+00
Fitted a model with MAP estimate = -266.3456
expansions: [(12, 1), (14, 4), (23, 1), (26, 2), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (97, 1), (99, 1), (103, 1), (107, 1), (108, 1), (109, 1), (110, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 265.8756 - loglik: -2.6131e+02 - logprior: -4.1176e+00
Epoch 2/2
19/19 - 5s - loss: 255.7491 - loglik: -2.5308e+02 - logprior: -2.1787e+00
Fitted a model with MAP estimate = -253.9881
expansions: [(0, 2)]
discards: [ 0 16 31 38 76]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 256.1527 - loglik: -2.5270e+02 - logprior: -2.9109e+00
Epoch 2/2
19/19 - 5s - loss: 253.7693 - loglik: -2.5197e+02 - logprior: -1.2311e+00
Fitted a model with MAP estimate = -252.2939
expansions: []
discards: [  0 135]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 257.3858 - loglik: -2.5279e+02 - logprior: -3.9773e+00
Epoch 2/10
19/19 - 5s - loss: 254.7307 - loglik: -2.5245e+02 - logprior: -1.6366e+00
Epoch 3/10
19/19 - 5s - loss: 253.2292 - loglik: -2.5155e+02 - logprior: -9.7436e-01
Epoch 4/10
19/19 - 5s - loss: 252.6210 - loglik: -2.5097e+02 - logprior: -9.1711e-01
Epoch 5/10
19/19 - 5s - loss: 252.3906 - loglik: -2.5075e+02 - logprior: -8.7921e-01
Epoch 6/10
19/19 - 5s - loss: 252.0066 - loglik: -2.5038e+02 - logprior: -8.1792e-01
Epoch 7/10
19/19 - 5s - loss: 251.7044 - loglik: -2.5013e+02 - logprior: -7.6606e-01
Epoch 8/10
19/19 - 5s - loss: 251.3140 - loglik: -2.4977e+02 - logprior: -7.2463e-01
Epoch 9/10
19/19 - 5s - loss: 249.4838 - loglik: -2.4799e+02 - logprior: -6.8885e-01
Epoch 10/10
19/19 - 5s - loss: 250.8359 - loglik: -2.4938e+02 - logprior: -6.5452e-01
Fitted a model with MAP estimate = -249.3126
Time for alignment: 139.0557
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 360.3861 - loglik: -3.5724e+02 - logprior: -3.0896e+00
Epoch 2/10
19/19 - 4s - loss: 303.2084 - loglik: -3.0165e+02 - logprior: -1.3229e+00
Epoch 3/10
19/19 - 4s - loss: 275.9792 - loglik: -2.7356e+02 - logprior: -1.5915e+00
Epoch 4/10
19/19 - 4s - loss: 270.5111 - loglik: -2.6812e+02 - logprior: -1.5503e+00
Epoch 5/10
19/19 - 4s - loss: 268.3271 - loglik: -2.6613e+02 - logprior: -1.5238e+00
Epoch 6/10
19/19 - 4s - loss: 266.9327 - loglik: -2.6486e+02 - logprior: -1.5021e+00
Epoch 7/10
19/19 - 4s - loss: 266.4007 - loglik: -2.6439e+02 - logprior: -1.4710e+00
Epoch 8/10
19/19 - 4s - loss: 266.2763 - loglik: -2.6432e+02 - logprior: -1.4597e+00
Epoch 9/10
19/19 - 4s - loss: 266.0694 - loglik: -2.6412e+02 - logprior: -1.4551e+00
Epoch 10/10
19/19 - 4s - loss: 265.4252 - loglik: -2.6350e+02 - logprior: -1.4498e+00
Fitted a model with MAP estimate = -264.9748
expansions: [(12, 1), (14, 3), (16, 1), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (95, 1), (101, 1), (103, 1), (107, 1), (108, 1), (109, 1), (110, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 264.6027 - loglik: -2.6001e+02 - logprior: -4.1420e+00
Epoch 2/2
19/19 - 5s - loss: 255.4706 - loglik: -2.5280e+02 - logprior: -2.1727e+00
Fitted a model with MAP estimate = -253.4151
expansions: [(0, 2)]
discards: [ 0 37 75]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 254.7896 - loglik: -2.5128e+02 - logprior: -2.9573e+00
Epoch 2/2
19/19 - 5s - loss: 252.4402 - loglik: -2.5064e+02 - logprior: -1.2409e+00
Fitted a model with MAP estimate = -251.1950
expansions: []
discards: [  0  17 136]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 256.5887 - loglik: -2.5199e+02 - logprior: -3.9974e+00
Epoch 2/10
19/19 - 5s - loss: 253.6785 - loglik: -2.5152e+02 - logprior: -1.5700e+00
Epoch 3/10
19/19 - 5s - loss: 252.3260 - loglik: -2.5068e+02 - logprior: -1.0247e+00
Epoch 4/10
19/19 - 5s - loss: 252.1398 - loglik: -2.5051e+02 - logprior: -1.0035e+00
Epoch 5/10
19/19 - 5s - loss: 251.6223 - loglik: -2.5003e+02 - logprior: -9.3920e-01
Epoch 6/10
19/19 - 5s - loss: 251.0780 - loglik: -2.4951e+02 - logprior: -8.7823e-01
Epoch 7/10
19/19 - 5s - loss: 251.0940 - loglik: -2.4953e+02 - logprior: -8.2839e-01
Fitted a model with MAP estimate = -250.0399
Time for alignment: 127.9295
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 360.6495 - loglik: -3.5751e+02 - logprior: -3.0756e+00
Epoch 2/10
19/19 - 4s - loss: 302.6268 - loglik: -3.0108e+02 - logprior: -1.3123e+00
Epoch 3/10
19/19 - 4s - loss: 275.3329 - loglik: -2.7299e+02 - logprior: -1.6312e+00
Epoch 4/10
19/19 - 4s - loss: 269.6035 - loglik: -2.6722e+02 - logprior: -1.6086e+00
Epoch 5/10
19/19 - 4s - loss: 268.0514 - loglik: -2.6587e+02 - logprior: -1.5413e+00
Epoch 6/10
19/19 - 4s - loss: 266.8409 - loglik: -2.6478e+02 - logprior: -1.4766e+00
Epoch 7/10
19/19 - 4s - loss: 266.3437 - loglik: -2.6436e+02 - logprior: -1.4464e+00
Epoch 8/10
19/19 - 4s - loss: 266.2350 - loglik: -2.6429e+02 - logprior: -1.4465e+00
Epoch 9/10
19/19 - 4s - loss: 265.6335 - loglik: -2.6371e+02 - logprior: -1.4416e+00
Epoch 10/10
19/19 - 4s - loss: 265.5593 - loglik: -2.6363e+02 - logprior: -1.4442e+00
Fitted a model with MAP estimate = -264.9136
expansions: [(12, 1), (14, 3), (16, 1), (19, 1), (26, 1), (28, 1), (29, 3), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (93, 1), (96, 1), (101, 1), (107, 1), (108, 1), (109, 1), (110, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 264.9401 - loglik: -2.6038e+02 - logprior: -4.1089e+00
Epoch 2/2
19/19 - 5s - loss: 256.1209 - loglik: -2.5349e+02 - logprior: -2.1545e+00
Fitted a model with MAP estimate = -253.9586
expansions: [(0, 2)]
discards: [ 0 37 38 76]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 255.6792 - loglik: -2.5226e+02 - logprior: -2.9056e+00
Epoch 2/2
19/19 - 5s - loss: 253.0573 - loglik: -2.5129e+02 - logprior: -1.2283e+00
Fitted a model with MAP estimate = -251.8036
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 256.7528 - loglik: -2.5226e+02 - logprior: -3.9336e+00
Epoch 2/10
19/19 - 5s - loss: 253.6995 - loglik: -2.5151e+02 - logprior: -1.6193e+00
Epoch 3/10
19/19 - 5s - loss: 252.6589 - loglik: -2.5111e+02 - logprior: -9.6679e-01
Epoch 4/10
19/19 - 5s - loss: 252.3921 - loglik: -2.5090e+02 - logprior: -9.2058e-01
Epoch 5/10
19/19 - 5s - loss: 252.1385 - loglik: -2.5067e+02 - logprior: -8.9523e-01
Epoch 6/10
19/19 - 5s - loss: 251.7680 - loglik: -2.5032e+02 - logprior: -8.5734e-01
Epoch 7/10
19/19 - 5s - loss: 251.8026 - loglik: -2.5037e+02 - logprior: -8.3069e-01
Fitted a model with MAP estimate = -250.9183
Time for alignment: 128.9964
Computed alignments with likelihoods: ['-249.3126', '-250.0399', '-250.9183']
Best model has likelihood: -249.3126
time for generating output: 0.1880
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02878.projection.fasta
SP score = 0.8264900662251655
Training of 3 independent models on file PF00970.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b72d38700>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f322072ba90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f389c4ba100>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b83d748e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f387c77f220>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f37fe5f77f0>, <__main__.SimpleDirichletPrior object at 0x7f3b9d35fd00>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f37fe4b8430>

Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 274.5460 - loglik: -2.7136e+02 - logprior: -3.1203e+00
Epoch 2/10
19/19 - 3s - loss: 235.3291 - loglik: -2.3371e+02 - logprior: -1.3206e+00
Epoch 3/10
19/19 - 3s - loss: 219.5043 - loglik: -2.1737e+02 - logprior: -1.5908e+00
Epoch 4/10
19/19 - 3s - loss: 216.6286 - loglik: -2.1459e+02 - logprior: -1.5892e+00
Epoch 5/10
19/19 - 3s - loss: 215.8147 - loglik: -2.1390e+02 - logprior: -1.5133e+00
Epoch 6/10
19/19 - 3s - loss: 215.2061 - loglik: -2.1333e+02 - logprior: -1.4971e+00
Epoch 7/10
19/19 - 3s - loss: 215.1762 - loglik: -2.1336e+02 - logprior: -1.4706e+00
Epoch 8/10
19/19 - 3s - loss: 214.7314 - loglik: -2.1293e+02 - logprior: -1.4649e+00
Epoch 9/10
19/19 - 3s - loss: 214.7620 - loglik: -2.1298e+02 - logprior: -1.4512e+00
Fitted a model with MAP estimate = -213.8465
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (18, 2), (19, 1), (28, 1), (29, 1), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (76, 1), (77, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 214.1782 - loglik: -2.0986e+02 - logprior: -3.9936e+00
Epoch 2/2
19/19 - 3s - loss: 204.5436 - loglik: -2.0277e+02 - logprior: -1.4574e+00
Fitted a model with MAP estimate = -202.7831
expansions: []
discards: [ 0 73 78 81]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 207.7868 - loglik: -2.0326e+02 - logprior: -4.1551e+00
Epoch 2/2
19/19 - 3s - loss: 204.2053 - loglik: -2.0219e+02 - logprior: -1.6251e+00
Fitted a model with MAP estimate = -202.5848
expansions: [(0, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 205.1445 - loglik: -2.0182e+02 - logprior: -2.9235e+00
Epoch 2/10
19/19 - 3s - loss: 202.9290 - loglik: -2.0131e+02 - logprior: -1.2108e+00
Epoch 3/10
19/19 - 3s - loss: 202.6121 - loglik: -2.0095e+02 - logprior: -1.2468e+00
Epoch 4/10
19/19 - 3s - loss: 202.5597 - loglik: -2.0102e+02 - logprior: -1.1270e+00
Epoch 5/10
19/19 - 3s - loss: 202.3515 - loglik: -2.0082e+02 - logprior: -1.1134e+00
Epoch 6/10
19/19 - 3s - loss: 202.2379 - loglik: -2.0073e+02 - logprior: -1.0775e+00
Epoch 7/10
19/19 - 3s - loss: 202.0778 - loglik: -2.0058e+02 - logprior: -1.0620e+00
Epoch 8/10
19/19 - 3s - loss: 202.0751 - loglik: -2.0061e+02 - logprior: -1.0305e+00
Epoch 9/10
19/19 - 3s - loss: 202.0933 - loglik: -2.0063e+02 - logprior: -1.0140e+00
Fitted a model with MAP estimate = -201.4271
Time for alignment: 100.9838
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 274.6349 - loglik: -2.7146e+02 - logprior: -3.1209e+00
Epoch 2/10
19/19 - 3s - loss: 236.0974 - loglik: -2.3436e+02 - logprior: -1.3406e+00
Epoch 3/10
19/19 - 3s - loss: 220.1526 - loglik: -2.1784e+02 - logprior: -1.5840e+00
Epoch 4/10
19/19 - 3s - loss: 216.8827 - loglik: -2.1484e+02 - logprior: -1.5597e+00
Epoch 5/10
19/19 - 3s - loss: 215.7723 - loglik: -2.1388e+02 - logprior: -1.4955e+00
Epoch 6/10
19/19 - 3s - loss: 215.0995 - loglik: -2.1327e+02 - logprior: -1.4658e+00
Epoch 7/10
19/19 - 3s - loss: 214.9555 - loglik: -2.1314e+02 - logprior: -1.4685e+00
Epoch 8/10
19/19 - 3s - loss: 214.3356 - loglik: -2.1253e+02 - logprior: -1.4612e+00
Epoch 9/10
19/19 - 3s - loss: 214.2508 - loglik: -2.1245e+02 - logprior: -1.4661e+00
Epoch 10/10
19/19 - 3s - loss: 214.4031 - loglik: -2.1261e+02 - logprior: -1.4566e+00
Fitted a model with MAP estimate = -213.4076
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (17, 1), (18, 1), (21, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 214.6105 - loglik: -2.1023e+02 - logprior: -4.0564e+00
Epoch 2/2
19/19 - 3s - loss: 204.5735 - loglik: -2.0275e+02 - logprior: -1.4843e+00
Fitted a model with MAP estimate = -202.6633
expansions: []
discards: [ 0 74 79 82 84 95]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 208.1815 - loglik: -2.0367e+02 - logprior: -4.1243e+00
Epoch 2/2
19/19 - 3s - loss: 204.4700 - loglik: -2.0250e+02 - logprior: -1.5628e+00
Fitted a model with MAP estimate = -202.8839
expansions: []
discards: [44]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 98 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 205.6389 - loglik: -2.0209e+02 - logprior: -3.1220e+00
Epoch 2/10
19/19 - 3s - loss: 203.5403 - loglik: -2.0183e+02 - logprior: -1.2801e+00
Epoch 3/10
19/19 - 3s - loss: 202.9301 - loglik: -2.0128e+02 - logprior: -1.1843e+00
Epoch 4/10
19/19 - 3s - loss: 202.8119 - loglik: -2.0123e+02 - logprior: -1.1209e+00
Epoch 5/10
19/19 - 3s - loss: 202.5457 - loglik: -2.0100e+02 - logprior: -1.0870e+00
Epoch 6/10
19/19 - 3s - loss: 202.9696 - loglik: -2.0147e+02 - logprior: -1.0404e+00
Fitted a model with MAP estimate = -202.0388
Time for alignment: 92.9335
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 274.7662 - loglik: -2.7158e+02 - logprior: -3.1279e+00
Epoch 2/10
19/19 - 3s - loss: 236.2837 - loglik: -2.3463e+02 - logprior: -1.3336e+00
Epoch 3/10
19/19 - 3s - loss: 219.8055 - loglik: -2.1768e+02 - logprior: -1.5864e+00
Epoch 4/10
19/19 - 3s - loss: 216.0409 - loglik: -2.1395e+02 - logprior: -1.5743e+00
Epoch 5/10
19/19 - 3s - loss: 215.4019 - loglik: -2.1343e+02 - logprior: -1.5214e+00
Epoch 6/10
19/19 - 3s - loss: 214.8132 - loglik: -2.1295e+02 - logprior: -1.4701e+00
Epoch 7/10
19/19 - 3s - loss: 214.3711 - loglik: -2.1256e+02 - logprior: -1.4413e+00
Epoch 8/10
19/19 - 3s - loss: 214.4421 - loglik: -2.1265e+02 - logprior: -1.4496e+00
Fitted a model with MAP estimate = -213.4674
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (18, 2), (21, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (63, 3), (64, 1), (71, 2), (77, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 213.7422 - loglik: -2.0939e+02 - logprior: -4.0250e+00
Epoch 2/2
19/19 - 4s - loss: 204.4275 - loglik: -2.0264e+02 - logprior: -1.4708e+00
Fitted a model with MAP estimate = -202.7855
expansions: []
discards: [ 0 45 74 78 94]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 207.6757 - loglik: -2.0314e+02 - logprior: -4.1559e+00
Epoch 2/2
19/19 - 3s - loss: 204.1351 - loglik: -2.0213e+02 - logprior: -1.6245e+00
Fitted a model with MAP estimate = -202.6766
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 205.2465 - loglik: -2.0171e+02 - logprior: -3.1257e+00
Epoch 2/10
19/19 - 3s - loss: 203.2659 - loglik: -2.0157e+02 - logprior: -1.2932e+00
Epoch 3/10
19/19 - 3s - loss: 202.9092 - loglik: -2.0130e+02 - logprior: -1.1887e+00
Epoch 4/10
19/19 - 3s - loss: 202.4264 - loglik: -2.0090e+02 - logprior: -1.1126e+00
Epoch 5/10
19/19 - 3s - loss: 202.5630 - loglik: -2.0105e+02 - logprior: -1.0983e+00
Fitted a model with MAP estimate = -201.9973
Time for alignment: 85.3514
Computed alignments with likelihoods: ['-201.4271', '-202.0388', '-201.9973']
Best model has likelihood: -201.4271
time for generating output: 0.2180
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00970.projection.fasta
SP score = 0.6515527669946737
Training of 3 independent models on file PF00313.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f38444accd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f38641febb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3bc7b72c70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f37fc989e80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f37fc989430>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f389c03c460>, <__main__.SimpleDirichletPrior object at 0x7f3a8efca1f0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3becf63d30>

Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.5030 - loglik: -1.7324e+02 - logprior: -3.2538e+00
Epoch 2/10
19/19 - 2s - loss: 133.0979 - loglik: -1.3158e+02 - logprior: -1.4976e+00
Epoch 3/10
19/19 - 2s - loss: 116.7608 - loglik: -1.1506e+02 - logprior: -1.5543e+00
Epoch 4/10
19/19 - 2s - loss: 113.7166 - loglik: -1.1190e+02 - logprior: -1.5877e+00
Epoch 5/10
19/19 - 1s - loss: 112.9516 - loglik: -1.1123e+02 - logprior: -1.5150e+00
Epoch 6/10
19/19 - 2s - loss: 112.7426 - loglik: -1.1106e+02 - logprior: -1.4860e+00
Epoch 7/10
19/19 - 2s - loss: 112.3643 - loglik: -1.1070e+02 - logprior: -1.4756e+00
Epoch 8/10
19/19 - 2s - loss: 112.0776 - loglik: -1.1043e+02 - logprior: -1.4621e+00
Epoch 9/10
19/19 - 2s - loss: 112.2556 - loglik: -1.1060e+02 - logprior: -1.4602e+00
Fitted a model with MAP estimate = -111.8477
expansions: [(0, 2), (8, 1), (15, 1), (20, 1), (27, 1), (28, 2), (29, 1), (30, 1), (31, 1), (32, 1), (35, 1), (36, 1), (40, 1), (41, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 110.9413 - loglik: -1.0653e+02 - logprior: -4.2201e+00
Epoch 2/2
19/19 - 2s - loss: 102.3585 - loglik: -1.0077e+02 - logprior: -1.3557e+00
Fitted a model with MAP estimate = -100.9911
expansions: []
discards: [ 0 35]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 105.9485 - loglik: -1.0152e+02 - logprior: -4.1738e+00
Epoch 2/2
19/19 - 2s - loss: 102.1954 - loglik: -1.0030e+02 - logprior: -1.6178e+00
Fitted a model with MAP estimate = -101.2848
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 103.8434 - loglik: -1.0023e+02 - logprior: -3.3262e+00
Epoch 2/10
19/19 - 2s - loss: 101.4929 - loglik: -9.9720e+01 - logprior: -1.4918e+00
Epoch 3/10
19/19 - 2s - loss: 101.6240 - loglik: -9.9952e+01 - logprior: -1.3799e+00
Fitted a model with MAP estimate = -100.8893
Time for alignment: 49.3977
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.4976 - loglik: -1.7324e+02 - logprior: -3.2491e+00
Epoch 2/10
19/19 - 1s - loss: 133.1204 - loglik: -1.3159e+02 - logprior: -1.5060e+00
Epoch 3/10
19/19 - 1s - loss: 117.4120 - loglik: -1.1571e+02 - logprior: -1.5153e+00
Epoch 4/10
19/19 - 2s - loss: 114.1768 - loglik: -1.1237e+02 - logprior: -1.5452e+00
Epoch 5/10
19/19 - 1s - loss: 112.7322 - loglik: -1.1104e+02 - logprior: -1.5066e+00
Epoch 6/10
19/19 - 1s - loss: 112.6283 - loglik: -1.1099e+02 - logprior: -1.4832e+00
Epoch 7/10
19/19 - 1s - loss: 112.5784 - loglik: -1.1096e+02 - logprior: -1.4641e+00
Epoch 8/10
19/19 - 2s - loss: 112.1150 - loglik: -1.1050e+02 - logprior: -1.4584e+00
Epoch 9/10
19/19 - 1s - loss: 112.0590 - loglik: -1.1044e+02 - logprior: -1.4524e+00
Epoch 10/10
19/19 - 1s - loss: 112.0165 - loglik: -1.1040e+02 - logprior: -1.4498e+00
Fitted a model with MAP estimate = -111.7599
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (26, 1), (27, 1), (28, 1), (29, 2), (30, 1), (31, 1), (34, 1), (40, 1), (41, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 111.1012 - loglik: -1.0665e+02 - logprior: -4.2739e+00
Epoch 2/2
19/19 - 2s - loss: 102.1883 - loglik: -1.0061e+02 - logprior: -1.3604e+00
Fitted a model with MAP estimate = -101.0553
expansions: []
discards: [ 0 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 105.9147 - loglik: -1.0148e+02 - logprior: -4.1757e+00
Epoch 2/2
19/19 - 2s - loss: 102.2725 - loglik: -1.0038e+02 - logprior: -1.6152e+00
Fitted a model with MAP estimate = -101.2756
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 103.8898 - loglik: -1.0028e+02 - logprior: -3.3217e+00
Epoch 2/10
19/19 - 2s - loss: 101.5618 - loglik: -9.9782e+01 - logprior: -1.4901e+00
Epoch 3/10
19/19 - 2s - loss: 101.2618 - loglik: -9.9594e+01 - logprior: -1.3755e+00
Epoch 4/10
19/19 - 2s - loss: 101.0842 - loglik: -9.9485e+01 - logprior: -1.3210e+00
Epoch 5/10
19/19 - 1s - loss: 101.5061 - loglik: -9.9936e+01 - logprior: -1.2829e+00
Fitted a model with MAP estimate = -100.7617
Time for alignment: 52.5124
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 176.4460 - loglik: -1.7318e+02 - logprior: -3.2518e+00
Epoch 2/10
19/19 - 1s - loss: 133.9706 - loglik: -1.3245e+02 - logprior: -1.4948e+00
Epoch 3/10
19/19 - 2s - loss: 116.9786 - loglik: -1.1522e+02 - logprior: -1.5129e+00
Epoch 4/10
19/19 - 1s - loss: 113.9966 - loglik: -1.1218e+02 - logprior: -1.5354e+00
Epoch 5/10
19/19 - 2s - loss: 112.8620 - loglik: -1.1118e+02 - logprior: -1.4763e+00
Epoch 6/10
19/19 - 2s - loss: 112.8037 - loglik: -1.1117e+02 - logprior: -1.4445e+00
Epoch 7/10
19/19 - 2s - loss: 112.4790 - loglik: -1.1086e+02 - logprior: -1.4291e+00
Epoch 8/10
19/19 - 1s - loss: 112.2555 - loglik: -1.1065e+02 - logprior: -1.4229e+00
Epoch 9/10
19/19 - 2s - loss: 112.0755 - loglik: -1.1047e+02 - logprior: -1.4143e+00
Epoch 10/10
19/19 - 1s - loss: 112.3946 - loglik: -1.1079e+02 - logprior: -1.4111e+00
Fitted a model with MAP estimate = -111.8514
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (34, 1), (40, 1), (41, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 110.7379 - loglik: -1.0626e+02 - logprior: -4.2779e+00
Epoch 2/2
19/19 - 2s - loss: 102.1185 - loglik: -1.0053e+02 - logprior: -1.3507e+00
Fitted a model with MAP estimate = -100.9387
expansions: []
discards: [ 0 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 105.8809 - loglik: -1.0144e+02 - logprior: -4.1676e+00
Epoch 2/2
19/19 - 2s - loss: 102.0763 - loglik: -1.0018e+02 - logprior: -1.6077e+00
Fitted a model with MAP estimate = -101.2527
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 103.9304 - loglik: -1.0032e+02 - logprior: -3.3215e+00
Epoch 2/10
19/19 - 2s - loss: 101.5275 - loglik: -9.9750e+01 - logprior: -1.4901e+00
Epoch 3/10
19/19 - 1s - loss: 101.1543 - loglik: -9.9481e+01 - logprior: -1.3814e+00
Epoch 4/10
19/19 - 1s - loss: 101.1276 - loglik: -9.9519e+01 - logprior: -1.3222e+00
Epoch 5/10
19/19 - 2s - loss: 101.1691 - loglik: -9.9596e+01 - logprior: -1.2823e+00
Fitted a model with MAP estimate = -100.7475
Time for alignment: 53.5850
Computed alignments with likelihoods: ['-100.8893', '-100.7617', '-100.7475']
Best model has likelihood: -100.7475
time for generating output: 0.1157
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00313.projection.fasta
SP score = 0.8142414860681114
Training of 3 independent models on file PF00009.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3bec7df0a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b9d1e8df0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f389c2a41f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f387c38ef40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f37fc096070>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f37fc3fb040>, <__main__.SimpleDirichletPrior object at 0x7f386460e1c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f37fe7835e0>

Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 566.1821 - loglik: -5.6409e+02 - logprior: -1.9507e+00
Epoch 2/10
39/39 - 18s - loss: 479.2724 - loglik: -4.7725e+02 - logprior: -1.4318e+00
Epoch 3/10
39/39 - 18s - loss: 471.1935 - loglik: -4.6878e+02 - logprior: -1.4260e+00
Epoch 4/10
39/39 - 18s - loss: 469.3114 - loglik: -4.6708e+02 - logprior: -1.3822e+00
Epoch 5/10
39/39 - 18s - loss: 467.9201 - loglik: -4.6575e+02 - logprior: -1.3733e+00
Epoch 6/10
39/39 - 18s - loss: 468.0750 - loglik: -4.6596e+02 - logprior: -1.3708e+00
Fitted a model with MAP estimate = -465.1292
expansions: [(25, 1), (30, 1), (31, 1), (55, 1), (56, 1), (75, 1), (76, 1), (79, 1), (80, 2), (81, 1), (101, 1), (105, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 462.8649 - loglik: -4.5992e+02 - logprior: -2.2195e+00
Epoch 2/2
39/39 - 20s - loss: 458.8828 - loglik: -4.5702e+02 - logprior: -1.1845e+00
Fitted a model with MAP estimate = -456.0214
expansions: []
discards: [88]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 459.3991 - loglik: -4.5660e+02 - logprior: -2.0774e+00
Epoch 2/2
39/39 - 20s - loss: 458.1383 - loglik: -4.5645e+02 - logprior: -1.0015e+00
Fitted a model with MAP estimate = -455.6074
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 460.2922 - loglik: -4.5672e+02 - logprior: -2.8771e+00
Epoch 2/10
39/39 - 19s - loss: 458.5653 - loglik: -4.5639e+02 - logprior: -1.4961e+00
Epoch 3/10
39/39 - 19s - loss: 456.7870 - loglik: -4.5541e+02 - logprior: -6.8970e-01
Epoch 4/10
39/39 - 19s - loss: 456.6365 - loglik: -4.5534e+02 - logprior: -6.3773e-01
Epoch 5/10
39/39 - 19s - loss: 456.5087 - loglik: -4.5531e+02 - logprior: -5.4364e-01
Epoch 6/10
39/39 - 20s - loss: 456.5736 - loglik: -4.5548e+02 - logprior: -4.5371e-01
Fitted a model with MAP estimate = -455.3183
Time for alignment: 398.0263
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 567.5022 - loglik: -5.6533e+02 - logprior: -1.9852e+00
Epoch 2/10
39/39 - 18s - loss: 477.5259 - loglik: -4.7508e+02 - logprior: -1.3557e+00
Epoch 3/10
39/39 - 18s - loss: 469.9037 - loglik: -4.6768e+02 - logprior: -1.2367e+00
Epoch 4/10
39/39 - 18s - loss: 467.7403 - loglik: -4.6558e+02 - logprior: -1.2492e+00
Epoch 5/10
39/39 - 18s - loss: 466.7578 - loglik: -4.6469e+02 - logprior: -1.2436e+00
Epoch 6/10
39/39 - 18s - loss: 466.2530 - loglik: -4.6425e+02 - logprior: -1.2460e+00
Epoch 7/10
39/39 - 19s - loss: 466.2046 - loglik: -4.6422e+02 - logprior: -1.2458e+00
Epoch 8/10
39/39 - 19s - loss: 465.2169 - loglik: -4.6325e+02 - logprior: -1.2469e+00
Epoch 9/10
39/39 - 18s - loss: 465.7423 - loglik: -4.6381e+02 - logprior: -1.2472e+00
Fitted a model with MAP estimate = -462.9921
expansions: [(25, 1), (30, 4), (54, 1), (56, 1), (76, 2), (78, 1), (81, 1), (100, 1), (139, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 460.8690 - loglik: -4.5799e+02 - logprior: -2.1788e+00
Epoch 2/2
39/39 - 20s - loss: 456.5805 - loglik: -4.5468e+02 - logprior: -1.1906e+00
Fitted a model with MAP estimate = -453.7653
expansions: [(153, 1)]
discards: [ 0 34 83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 460.6633 - loglik: -4.5690e+02 - logprior: -3.0228e+00
Epoch 2/2
39/39 - 20s - loss: 458.6364 - loglik: -4.5640e+02 - logprior: -1.5057e+00
Fitted a model with MAP estimate = -454.7028
expansions: [(0, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 456.7087 - loglik: -4.5414e+02 - logprior: -1.8014e+00
Epoch 2/10
39/39 - 20s - loss: 454.7290 - loglik: -4.5310e+02 - logprior: -8.7135e-01
Epoch 3/10
39/39 - 20s - loss: 454.4133 - loglik: -4.5290e+02 - logprior: -7.3338e-01
Epoch 4/10
39/39 - 20s - loss: 454.8522 - loglik: -4.5348e+02 - logprior: -6.2901e-01
Fitted a model with MAP estimate = -453.4288
Time for alignment: 420.9967
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 566.8578 - loglik: -5.6475e+02 - logprior: -1.9589e+00
Epoch 2/10
39/39 - 18s - loss: 476.9705 - loglik: -4.7479e+02 - logprior: -1.5332e+00
Epoch 3/10
39/39 - 19s - loss: 470.2895 - loglik: -4.6799e+02 - logprior: -1.4200e+00
Epoch 4/10
39/39 - 19s - loss: 468.6322 - loglik: -4.6643e+02 - logprior: -1.3740e+00
Epoch 5/10
39/39 - 18s - loss: 467.8385 - loglik: -4.6573e+02 - logprior: -1.3694e+00
Epoch 6/10
39/39 - 18s - loss: 467.2570 - loglik: -4.6517e+02 - logprior: -1.3724e+00
Epoch 7/10
39/39 - 18s - loss: 466.9695 - loglik: -4.6489e+02 - logprior: -1.3762e+00
Epoch 8/10
39/39 - 19s - loss: 466.0766 - loglik: -4.6401e+02 - logprior: -1.3809e+00
Epoch 9/10
39/39 - 18s - loss: 466.9424 - loglik: -4.6489e+02 - logprior: -1.3821e+00
Fitted a model with MAP estimate = -463.8001
expansions: [(25, 1), (30, 4), (57, 1), (58, 1), (60, 1), (61, 2), (62, 1), (80, 1), (81, 1), (100, 1), (101, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 186 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 461.8943 - loglik: -4.5903e+02 - logprior: -2.2242e+00
Epoch 2/2
39/39 - 20s - loss: 457.7964 - loglik: -4.5595e+02 - logprior: -1.1955e+00
Fitted a model with MAP estimate = -455.0299
expansions: []
discards: [ 0 33 69]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 461.8045 - loglik: -4.5811e+02 - logprior: -3.0168e+00
Epoch 2/2
39/39 - 20s - loss: 459.2580 - loglik: -4.5716e+02 - logprior: -1.4500e+00
Fitted a model with MAP estimate = -455.8849
expansions: [(0, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 457.7874 - loglik: -4.5534e+02 - logprior: -1.7811e+00
Epoch 2/10
39/39 - 20s - loss: 456.1762 - loglik: -4.5466e+02 - logprior: -8.4838e-01
Epoch 3/10
39/39 - 20s - loss: 456.0777 - loglik: -4.5468e+02 - logprior: -7.1238e-01
Epoch 4/10
39/39 - 20s - loss: 455.9027 - loglik: -4.5464e+02 - logprior: -6.0486e-01
Epoch 5/10
39/39 - 20s - loss: 455.1270 - loglik: -4.5395e+02 - logprior: -5.0472e-01
Epoch 6/10
39/39 - 20s - loss: 455.3664 - loglik: -4.5428e+02 - logprior: -4.0751e-01
Fitted a model with MAP estimate = -454.3168
Time for alignment: 463.5937
Computed alignments with likelihoods: ['-455.3183', '-453.4288', '-454.3168']
Best model has likelihood: -453.4288
time for generating output: 0.2675
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00009.projection.fasta
SP score = 0.7716818623421269
Training of 3 independent models on file PF14604.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3804602730>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f37fdb690a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f38646f0cd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3824234f40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3824234400>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f37fc83ffa0>, <__main__.SimpleDirichletPrior object at 0x7f37fc876490>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3bc7baeee0>

Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.7991 - loglik: -1.3955e+02 - logprior: -3.2411e+00
Epoch 2/10
19/19 - 1s - loss: 113.7424 - loglik: -1.1224e+02 - logprior: -1.4578e+00
Epoch 3/10
19/19 - 1s - loss: 103.7610 - loglik: -1.0183e+02 - logprior: -1.6642e+00
Epoch 4/10
19/19 - 1s - loss: 102.1418 - loglik: -1.0037e+02 - logprior: -1.5184e+00
Epoch 5/10
19/19 - 1s - loss: 101.0857 - loglik: -9.9360e+01 - logprior: -1.5016e+00
Epoch 6/10
19/19 - 1s - loss: 100.8297 - loglik: -9.9123e+01 - logprior: -1.4906e+00
Epoch 7/10
19/19 - 1s - loss: 100.5777 - loglik: -9.8875e+01 - logprior: -1.4753e+00
Epoch 8/10
19/19 - 1s - loss: 100.3798 - loglik: -9.8668e+01 - logprior: -1.4708e+00
Epoch 9/10
19/19 - 1s - loss: 100.4100 - loglik: -9.8684e+01 - logprior: -1.4655e+00
Fitted a model with MAP estimate = -99.9581
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (27, 2), (28, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 106.1912 - loglik: -1.0172e+02 - logprior: -4.2437e+00
Epoch 2/2
19/19 - 1s - loss: 98.5924 - loglik: -9.6066e+01 - logprior: -2.2293e+00
Fitted a model with MAP estimate = -96.6950
expansions: [(0, 1)]
discards: [ 0  8 16 26 27 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 99.2546 - loglik: -9.5689e+01 - logprior: -3.2799e+00
Epoch 2/2
19/19 - 1s - loss: 95.6017 - loglik: -9.3749e+01 - logprior: -1.5615e+00
Fitted a model with MAP estimate = -94.9104
expansions: []
discards: [36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 97.9132 - loglik: -9.4291e+01 - logprior: -3.3293e+00
Epoch 2/10
19/19 - 1s - loss: 95.6881 - loglik: -9.3865e+01 - logprior: -1.5242e+00
Epoch 3/10
19/19 - 1s - loss: 95.4158 - loglik: -9.3684e+01 - logprior: -1.4298e+00
Epoch 4/10
19/19 - 1s - loss: 95.2133 - loglik: -9.3534e+01 - logprior: -1.3729e+00
Epoch 5/10
19/19 - 1s - loss: 95.1645 - loglik: -9.3518e+01 - logprior: -1.3395e+00
Epoch 6/10
19/19 - 1s - loss: 95.1410 - loglik: -9.3504e+01 - logprior: -1.3216e+00
Epoch 7/10
19/19 - 1s - loss: 95.1251 - loglik: -9.3500e+01 - logprior: -1.3076e+00
Epoch 8/10
19/19 - 1s - loss: 94.9789 - loglik: -9.3362e+01 - logprior: -1.2887e+00
Epoch 9/10
19/19 - 1s - loss: 95.3259 - loglik: -9.3722e+01 - logprior: -1.2748e+00
Fitted a model with MAP estimate = -94.6343
Time for alignment: 51.9647
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 142.7813 - loglik: -1.3953e+02 - logprior: -3.2444e+00
Epoch 2/10
19/19 - 1s - loss: 114.0272 - loglik: -1.1252e+02 - logprior: -1.4536e+00
Epoch 3/10
19/19 - 1s - loss: 103.7281 - loglik: -1.0184e+02 - logprior: -1.6539e+00
Epoch 4/10
19/19 - 1s - loss: 101.9076 - loglik: -1.0016e+02 - logprior: -1.5182e+00
Epoch 5/10
19/19 - 1s - loss: 101.1952 - loglik: -9.9497e+01 - logprior: -1.5044e+00
Epoch 6/10
19/19 - 1s - loss: 100.8708 - loglik: -9.9185e+01 - logprior: -1.4891e+00
Epoch 7/10
19/19 - 1s - loss: 100.4117 - loglik: -9.8729e+01 - logprior: -1.4765e+00
Epoch 8/10
19/19 - 1s - loss: 100.4279 - loglik: -9.8766e+01 - logprior: -1.4681e+00
Fitted a model with MAP estimate = -100.0458
expansions: [(6, 1), (7, 2), (8, 1), (13, 1), (18, 1), (20, 2), (21, 2), (26, 1), (28, 2), (31, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 105.6681 - loglik: -1.0126e+02 - logprior: -4.2274e+00
Epoch 2/2
19/19 - 1s - loss: 98.5919 - loglik: -9.6169e+01 - logprior: -2.1792e+00
Fitted a model with MAP estimate = -96.7357
expansions: [(0, 1)]
discards: [ 0  8 25 26 38 43]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 99.5905 - loglik: -9.6084e+01 - logprior: -3.2586e+00
Epoch 2/2
19/19 - 1s - loss: 96.0103 - loglik: -9.4217e+01 - logprior: -1.5295e+00
Fitted a model with MAP estimate = -95.3553
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 97.8170 - loglik: -9.4220e+01 - logprior: -3.3244e+00
Epoch 2/10
19/19 - 1s - loss: 95.7439 - loglik: -9.3954e+01 - logprior: -1.5199e+00
Epoch 3/10
19/19 - 1s - loss: 95.4455 - loglik: -9.3738e+01 - logprior: -1.4249e+00
Epoch 4/10
19/19 - 1s - loss: 95.2362 - loglik: -9.3580e+01 - logprior: -1.3731e+00
Epoch 5/10
19/19 - 1s - loss: 95.3222 - loglik: -9.3702e+01 - logprior: -1.3415e+00
Fitted a model with MAP estimate = -94.8772
Time for alignment: 45.6503
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 142.7967 - loglik: -1.3955e+02 - logprior: -3.2442e+00
Epoch 2/10
19/19 - 1s - loss: 113.6633 - loglik: -1.1215e+02 - logprior: -1.4531e+00
Epoch 3/10
19/19 - 1s - loss: 104.0644 - loglik: -1.0210e+02 - logprior: -1.6373e+00
Epoch 4/10
19/19 - 1s - loss: 102.2261 - loglik: -1.0054e+02 - logprior: -1.4959e+00
Epoch 5/10
19/19 - 1s - loss: 101.6329 - loglik: -9.9981e+01 - logprior: -1.4840e+00
Epoch 6/10
19/19 - 1s - loss: 101.4155 - loglik: -9.9778e+01 - logprior: -1.4693e+00
Epoch 7/10
19/19 - 1s - loss: 101.0155 - loglik: -9.9353e+01 - logprior: -1.4693e+00
Epoch 8/10
19/19 - 1s - loss: 100.7169 - loglik: -9.9018e+01 - logprior: -1.4646e+00
Epoch 9/10
19/19 - 1s - loss: 100.6247 - loglik: -9.8908e+01 - logprior: -1.4628e+00
Epoch 10/10
19/19 - 1s - loss: 100.5188 - loglik: -9.8783e+01 - logprior: -1.4592e+00
Fitted a model with MAP estimate = -100.1141
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (20, 2), (21, 1), (27, 1), (28, 2), (29, 2), (31, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 106.5870 - loglik: -1.0209e+02 - logprior: -4.2490e+00
Epoch 2/2
19/19 - 1s - loss: 98.5833 - loglik: -9.6053e+01 - logprior: -2.2587e+00
Fitted a model with MAP estimate = -96.8235
expansions: [(0, 1)]
discards: [ 0  8 16 26 38 40 44]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 99.4116 - loglik: -9.5880e+01 - logprior: -3.2635e+00
Epoch 2/2
19/19 - 1s - loss: 95.9523 - loglik: -9.4120e+01 - logprior: -1.5384e+00
Fitted a model with MAP estimate = -95.2481
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 97.7614 - loglik: -9.4127e+01 - logprior: -3.3333e+00
Epoch 2/10
19/19 - 1s - loss: 95.6373 - loglik: -9.3809e+01 - logprior: -1.5258e+00
Epoch 3/10
19/19 - 1s - loss: 95.4179 - loglik: -9.3679e+01 - logprior: -1.4329e+00
Epoch 4/10
19/19 - 1s - loss: 95.2312 - loglik: -9.3548e+01 - logprior: -1.3801e+00
Epoch 5/10
19/19 - 1s - loss: 95.2338 - loglik: -9.3582e+01 - logprior: -1.3425e+00
Fitted a model with MAP estimate = -94.8213
Time for alignment: 47.9754
Computed alignments with likelihoods: ['-94.6343', '-94.8772', '-94.8213']
Best model has likelihood: -94.6343
time for generating output: 0.1149
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF14604.projection.fasta
SP score = 0.7553269654665687
Training of 3 independent models on file PF00625.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f380472eb20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3864503f40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f38ac3c3e80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b9d1d1400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3bc7a50af0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3844348f40>, <__main__.SimpleDirichletPrior object at 0x7f3219615fd0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3bc7baeee0>

Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 458.3994 - loglik: -4.5542e+02 - logprior: -2.9478e+00
Epoch 2/10
19/19 - 6s - loss: 381.0596 - loglik: -3.7950e+02 - logprior: -1.4666e+00
Epoch 3/10
19/19 - 6s - loss: 347.6393 - loglik: -3.4525e+02 - logprior: -1.9992e+00
Epoch 4/10
19/19 - 6s - loss: 339.8596 - loglik: -3.3724e+02 - logprior: -2.1163e+00
Epoch 5/10
19/19 - 6s - loss: 338.4587 - loglik: -3.3592e+02 - logprior: -2.0242e+00
Epoch 6/10
19/19 - 6s - loss: 336.8787 - loglik: -3.3439e+02 - logprior: -1.9897e+00
Epoch 7/10
19/19 - 6s - loss: 336.9701 - loglik: -3.3453e+02 - logprior: -1.9673e+00
Fitted a model with MAP estimate = -332.7359
expansions: [(0, 2), (11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (20, 1), (22, 1), (23, 1), (43, 2), (44, 1), (48, 1), (50, 1), (52, 1), (57, 1), (66, 1), (67, 1), (69, 1), (70, 1), (71, 1), (80, 1), (87, 2), (88, 1), (89, 1), (99, 1), (100, 1), (112, 1), (113, 1), (114, 2), (115, 1), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (136, 1), (137, 1), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 318.8672 - loglik: -3.1576e+02 - logprior: -2.5848e+00
Epoch 2/2
39/39 - 11s - loss: 308.2053 - loglik: -3.0644e+02 - logprior: -1.2334e+00
Fitted a model with MAP estimate = -303.8697
expansions: []
discards: [  0 113]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 311.2233 - loglik: -3.0746e+02 - logprior: -3.1691e+00
Epoch 2/2
39/39 - 11s - loss: 308.0244 - loglik: -3.0624e+02 - logprior: -1.2164e+00
Fitted a model with MAP estimate = -303.6526
expansions: [(112, 1)]
discards: [54]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 305.4744 - loglik: -3.0289e+02 - logprior: -2.0143e+00
Epoch 2/10
39/39 - 11s - loss: 304.2549 - loglik: -3.0283e+02 - logprior: -8.9024e-01
Epoch 3/10
39/39 - 11s - loss: 303.2604 - loglik: -3.0197e+02 - logprior: -7.5024e-01
Epoch 4/10
39/39 - 11s - loss: 303.2026 - loglik: -3.0206e+02 - logprior: -6.5134e-01
Epoch 5/10
39/39 - 11s - loss: 303.3099 - loglik: -3.0227e+02 - logprior: -5.5664e-01
Fitted a model with MAP estimate = -302.1128
Time for alignment: 209.7078
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 457.4019 - loglik: -4.5441e+02 - logprior: -2.9550e+00
Epoch 2/10
19/19 - 6s - loss: 382.3756 - loglik: -3.8075e+02 - logprior: -1.5236e+00
Epoch 3/10
19/19 - 7s - loss: 352.2076 - loglik: -3.4984e+02 - logprior: -1.9676e+00
Epoch 4/10
19/19 - 7s - loss: 343.9489 - loglik: -3.4146e+02 - logprior: -2.0406e+00
Epoch 5/10
19/19 - 7s - loss: 340.4163 - loglik: -3.3798e+02 - logprior: -1.9811e+00
Epoch 6/10
19/19 - 7s - loss: 338.0031 - loglik: -3.3561e+02 - logprior: -1.9821e+00
Epoch 7/10
19/19 - 7s - loss: 337.7052 - loglik: -3.3531e+02 - logprior: -1.9698e+00
Epoch 8/10
19/19 - 7s - loss: 336.8540 - loglik: -3.3449e+02 - logprior: -1.9495e+00
Epoch 9/10
19/19 - 7s - loss: 336.3768 - loglik: -3.3400e+02 - logprior: -1.9628e+00
Epoch 10/10
19/19 - 7s - loss: 336.3458 - loglik: -3.3397e+02 - logprior: -1.9640e+00
Fitted a model with MAP estimate = -332.4488
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (21, 1), (22, 1), (30, 1), (43, 2), (44, 1), (48, 1), (50, 1), (52, 1), (57, 1), (58, 1), (65, 2), (66, 1), (68, 1), (71, 1), (80, 1), (87, 1), (88, 1), (89, 1), (92, 1), (99, 1), (100, 1), (111, 1), (113, 1), (114, 2), (116, 1), (118, 1), (122, 1), (123, 1), (136, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 318.9036 - loglik: -3.1576e+02 - logprior: -2.6946e+00
Epoch 2/2
39/39 - 11s - loss: 308.5230 - loglik: -3.0679e+02 - logprior: -1.2488e+00
Fitted a model with MAP estimate = -304.2218
expansions: []
discards: [ 0 55 84]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 311.4683 - loglik: -3.0776e+02 - logprior: -3.1533e+00
Epoch 2/2
39/39 - 11s - loss: 308.2314 - loglik: -3.0653e+02 - logprior: -1.1688e+00
Fitted a model with MAP estimate = -303.8838
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 306.0021 - loglik: -3.0349e+02 - logprior: -1.9775e+00
Epoch 2/10
39/39 - 11s - loss: 304.7824 - loglik: -3.0341e+02 - logprior: -8.5461e-01
Epoch 3/10
39/39 - 11s - loss: 304.0034 - loglik: -3.0276e+02 - logprior: -7.1807e-01
Epoch 4/10
39/39 - 11s - loss: 304.1019 - loglik: -3.0300e+02 - logprior: -6.2365e-01
Fitted a model with MAP estimate = -303.1179
Time for alignment: 222.6558
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 458.0065 - loglik: -4.5502e+02 - logprior: -2.9573e+00
Epoch 2/10
19/19 - 6s - loss: 381.4794 - loglik: -3.7988e+02 - logprior: -1.4810e+00
Epoch 3/10
19/19 - 6s - loss: 350.3132 - loglik: -3.4780e+02 - logprior: -1.9450e+00
Epoch 4/10
19/19 - 6s - loss: 344.0600 - loglik: -3.4152e+02 - logprior: -2.0039e+00
Epoch 5/10
19/19 - 6s - loss: 340.6904 - loglik: -3.3821e+02 - logprior: -1.9653e+00
Epoch 6/10
19/19 - 7s - loss: 340.3894 - loglik: -3.3796e+02 - logprior: -1.9358e+00
Epoch 7/10
19/19 - 7s - loss: 339.7195 - loglik: -3.3734e+02 - logprior: -1.9234e+00
Epoch 8/10
19/19 - 7s - loss: 339.0790 - loglik: -3.3672e+02 - logprior: -1.9110e+00
Epoch 9/10
19/19 - 7s - loss: 338.5221 - loglik: -3.3616e+02 - logprior: -1.9181e+00
Epoch 10/10
19/19 - 7s - loss: 338.5457 - loglik: -3.3620e+02 - logprior: -1.9153e+00
Fitted a model with MAP estimate = -334.8313
expansions: [(0, 2), (11, 1), (14, 1), (15, 1), (16, 1), (17, 2), (18, 2), (22, 1), (23, 1), (38, 1), (44, 1), (48, 1), (49, 1), (50, 1), (51, 1), (66, 4), (68, 1), (71, 1), (87, 2), (88, 1), (89, 1), (99, 1), (100, 1), (113, 1), (114, 2), (115, 1), (116, 1), (117, 1), (122, 1), (123, 1), (135, 1), (136, 1), (137, 1), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 186 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 320.8275 - loglik: -3.1766e+02 - logprior: -2.7193e+00
Epoch 2/2
39/39 - 11s - loss: 311.0132 - loglik: -3.0917e+02 - logprior: -1.3433e+00
Fitted a model with MAP estimate = -306.5566
expansions: []
discards: [ 0 26 85]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 313.4048 - loglik: -3.0969e+02 - logprior: -3.1776e+00
Epoch 2/2
39/39 - 11s - loss: 310.4945 - loglik: -3.0874e+02 - logprior: -1.2376e+00
Fitted a model with MAP estimate = -306.1735
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 308.3139 - loglik: -3.0575e+02 - logprior: -2.0468e+00
Epoch 2/10
39/39 - 11s - loss: 306.6765 - loglik: -3.0525e+02 - logprior: -9.2641e-01
Epoch 3/10
39/39 - 11s - loss: 306.6576 - loglik: -3.0535e+02 - logprior: -8.0323e-01
Epoch 4/10
39/39 - 11s - loss: 306.1790 - loglik: -3.0501e+02 - logprior: -7.0479e-01
Epoch 5/10
39/39 - 11s - loss: 305.9407 - loglik: -3.0489e+02 - logprior: -5.9552e-01
Epoch 6/10
39/39 - 11s - loss: 306.5616 - loglik: -3.0561e+02 - logprior: -5.0885e-01
Fitted a model with MAP estimate = -305.2245
Time for alignment: 238.1837
Computed alignments with likelihoods: ['-302.1128', '-303.1179', '-305.2245']
Best model has likelihood: -302.1128
time for generating output: 0.3607
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00625.projection.fasta
SP score = 0.39871573176393144
Training of 3 independent models on file PF00476.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b9d1d5d30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f37fdae3790>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f37fc0f0550>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3204516040>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b9d618c40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f38ac27b220>, <__main__.SimpleDirichletPrior object at 0x7f38443e2fa0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3bc7baeee0>

Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 50s - loss: 904.4703 - loglik: -9.0266e+02 - logprior: -1.6107e+00
Epoch 2/10
39/39 - 50s - loss: 721.3594 - loglik: -7.1766e+02 - logprior: -2.1892e+00
Epoch 3/10
39/39 - 53s - loss: 703.5812 - loglik: -6.9963e+02 - logprior: -2.2744e+00
Epoch 4/10
39/39 - 52s - loss: 698.3260 - loglik: -6.9453e+02 - logprior: -2.2944e+00
Epoch 5/10
39/39 - 51s - loss: 696.1116 - loglik: -6.9233e+02 - logprior: -2.4622e+00
Epoch 6/10
39/39 - 51s - loss: 694.9727 - loglik: -6.9140e+02 - logprior: -2.3920e+00
Epoch 7/10
39/39 - 51s - loss: 694.0286 - loglik: -6.9068e+02 - logprior: -2.2949e+00
Epoch 8/10
39/39 - 51s - loss: 693.3820 - loglik: -6.9018e+02 - logprior: -2.2501e+00
Epoch 9/10
39/39 - 50s - loss: 693.4711 - loglik: -6.9032e+02 - logprior: -2.2582e+00
Fitted a model with MAP estimate = -691.2401
expansions: [(0, 5), (25, 1), (46, 1), (60, 1), (63, 2), (67, 1), (73, 1), (74, 1), (80, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (93, 1), (113, 1), (114, 1), (119, 1), (123, 2), (125, 3), (141, 1), (142, 1), (144, 1), (146, 1), (149, 1), (154, 1), (155, 1), (157, 1), (159, 1), (161, 1), (162, 1), (164, 1), (165, 1), (186, 1), (188, 2), (189, 1), (190, 1), (193, 1), (205, 1), (207, 2), (208, 2), (209, 3), (227, 1), (228, 1), (231, 4), (232, 1), (256, 4), (258, 2), (259, 2), (260, 1), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 388 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 78s - loss: 660.3035 - loglik: -6.5640e+02 - logprior: -3.0328e+00
Epoch 2/2
39/39 - 76s - loss: 641.1479 - loglik: -6.3861e+02 - logprior: -1.6480e+00
Fitted a model with MAP estimate = -638.0018
expansions: []
discards: [  1   2   3 148 153 232 259 261 290 327 367 368]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 78s - loss: 642.3660 - loglik: -6.3947e+02 - logprior: -1.9381e+00
Epoch 2/2
39/39 - 75s - loss: 640.2150 - loglik: -6.3860e+02 - logprior: -7.3525e-01
Fitted a model with MAP estimate = -637.8808
expansions: [(1, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 377 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 76s - loss: 639.3254 - loglik: -6.3685e+02 - logprior: -1.5365e+00
Epoch 2/10
39/39 - 71s - loss: 637.2832 - loglik: -6.3586e+02 - logprior: -4.6594e-01
Epoch 3/10
39/39 - 73s - loss: 636.7169 - loglik: -6.3553e+02 - logprior: -2.3113e-01
Epoch 4/10
39/39 - 72s - loss: 635.7577 - loglik: -6.3478e+02 - logprior: -4.6341e-02
Epoch 5/10
39/39 - 77s - loss: 635.7123 - loglik: -6.3491e+02 - logprior: 0.1124
Epoch 6/10
39/39 - 73s - loss: 635.2323 - loglik: -6.3466e+02 - logprior: 0.3091
Epoch 7/10
39/39 - 63s - loss: 635.8794 - loglik: -6.3555e+02 - logprior: 0.5523
Fitted a model with MAP estimate = -633.7496
Time for alignment: 1617.2963
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 904.5461 - loglik: -9.0278e+02 - logprior: -1.6086e+00
Epoch 2/10
39/39 - 43s - loss: 720.5975 - loglik: -7.1711e+02 - logprior: -2.2979e+00
Epoch 3/10
39/39 - 43s - loss: 702.2702 - loglik: -6.9837e+02 - logprior: -2.3816e+00
Epoch 4/10
39/39 - 43s - loss: 697.1429 - loglik: -6.9323e+02 - logprior: -2.4114e+00
Epoch 5/10
39/39 - 43s - loss: 694.8040 - loglik: -6.9104e+02 - logprior: -2.4016e+00
Epoch 6/10
39/39 - 43s - loss: 693.6724 - loglik: -6.9000e+02 - logprior: -2.4382e+00
Epoch 7/10
39/39 - 43s - loss: 693.4985 - loglik: -6.9004e+02 - logprior: -2.3673e+00
Epoch 8/10
39/39 - 43s - loss: 692.6064 - loglik: -6.8927e+02 - logprior: -2.3403e+00
Epoch 9/10
39/39 - 43s - loss: 691.9493 - loglik: -6.8868e+02 - logprior: -2.3544e+00
Epoch 10/10
39/39 - 43s - loss: 691.9848 - loglik: -6.8865e+02 - logprior: -2.4684e+00
Fitted a model with MAP estimate = -690.2744
expansions: [(0, 4), (24, 1), (42, 1), (45, 1), (55, 1), (62, 2), (66, 1), (72, 1), (80, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (93, 1), (113, 1), (114, 1), (119, 1), (123, 2), (125, 3), (141, 1), (145, 1), (147, 1), (149, 1), (156, 1), (159, 1), (161, 2), (162, 2), (163, 1), (165, 1), (166, 1), (180, 1), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (229, 1), (230, 1), (231, 3), (257, 4), (259, 2), (260, 2), (261, 3), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 386 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 71s - loss: 659.4781 - loglik: -6.5561e+02 - logprior: -3.0223e+00
Epoch 2/2
39/39 - 73s - loss: 641.0110 - loglik: -6.3855e+02 - logprior: -1.5902e+00
Fitted a model with MAP estimate = -637.7349
expansions: []
discards: [  0 147 152 197 232 325 326 365 366]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 75s - loss: 643.1520 - loglik: -6.3952e+02 - logprior: -2.7196e+00
Epoch 2/2
39/39 - 75s - loss: 639.8979 - loglik: -6.3826e+02 - logprior: -7.3865e-01
Fitted a model with MAP estimate = -636.9879
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 377 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 79s - loss: 639.0883 - loglik: -6.3653e+02 - logprior: -1.6321e+00
Epoch 2/10
39/39 - 68s - loss: 637.3868 - loglik: -6.3602e+02 - logprior: -4.0906e-01
Epoch 3/10
39/39 - 75s - loss: 636.2641 - loglik: -6.3515e+02 - logprior: -1.8128e-01
Epoch 4/10
39/39 - 80s - loss: 635.7921 - loglik: -6.3482e+02 - logprior: -4.1363e-02
Epoch 5/10
39/39 - 78s - loss: 635.8630 - loglik: -6.3515e+02 - logprior: 0.1867
Fitted a model with MAP estimate = -634.0946
Time for alignment: 1422.1428
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 55s - loss: 902.6800 - loglik: -9.0088e+02 - logprior: -1.6269e+00
Epoch 2/10
39/39 - 53s - loss: 721.3413 - loglik: -7.1762e+02 - logprior: -2.2092e+00
Epoch 3/10
39/39 - 53s - loss: 703.8414 - loglik: -6.9983e+02 - logprior: -2.3516e+00
Epoch 4/10
39/39 - 53s - loss: 698.4399 - loglik: -6.9463e+02 - logprior: -2.3125e+00
Epoch 5/10
39/39 - 51s - loss: 696.3896 - loglik: -6.9274e+02 - logprior: -2.3154e+00
Epoch 6/10
39/39 - 49s - loss: 695.0797 - loglik: -6.9155e+02 - logprior: -2.3288e+00
Epoch 7/10
39/39 - 52s - loss: 693.5625 - loglik: -6.9010e+02 - logprior: -2.3966e+00
Epoch 8/10
39/39 - 54s - loss: 693.8860 - loglik: -6.9044e+02 - logprior: -2.4700e+00
Fitted a model with MAP estimate = -691.5767
expansions: [(0, 4), (43, 1), (46, 1), (55, 1), (60, 1), (62, 1), (66, 1), (67, 1), (72, 1), (80, 1), (81, 1), (83, 2), (84, 1), (85, 2), (92, 1), (93, 1), (102, 1), (112, 1), (113, 1), (118, 1), (122, 2), (124, 1), (125, 1), (142, 1), (144, 1), (146, 1), (149, 1), (155, 1), (158, 1), (160, 2), (161, 2), (162, 1), (164, 1), (165, 1), (186, 1), (188, 1), (189, 1), (190, 1), (197, 1), (205, 1), (207, 2), (208, 2), (209, 3), (227, 1), (228, 1), (231, 3), (233, 1), (256, 1), (257, 3), (258, 2), (259, 2), (260, 1), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 385 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 79s - loss: 658.2330 - loglik: -6.5430e+02 - logprior: -2.9818e+00
Epoch 2/2
39/39 - 77s - loss: 639.8117 - loglik: -6.3735e+02 - logprior: -1.5300e+00
Fitted a model with MAP estimate = -636.6046
expansions: []
discards: [  0 147 196 257 259 323 364 365]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 82s - loss: 642.4669 - loglik: -6.3859e+02 - logprior: -2.8691e+00
Epoch 2/2
39/39 - 82s - loss: 639.1450 - loglik: -6.3708e+02 - logprior: -1.0956e+00
Fitted a model with MAP estimate = -636.2293
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 378 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 80s - loss: 638.5926 - loglik: -6.3589e+02 - logprior: -1.7144e+00
Epoch 2/10
39/39 - 77s - loss: 637.0082 - loglik: -6.3555e+02 - logprior: -4.9785e-01
Epoch 3/10
39/39 - 72s - loss: 636.0079 - loglik: -6.3479e+02 - logprior: -2.5425e-01
Epoch 4/10
39/39 - 76s - loss: 635.7227 - loglik: -6.3478e+02 - logprior: -2.9654e-02
Epoch 5/10
39/39 - 76s - loss: 635.2142 - loglik: -6.3446e+02 - logprior: 0.1370
Epoch 6/10
39/39 - 69s - loss: 635.1823 - loglik: -6.3452e+02 - logprior: 0.2070
Epoch 7/10
39/39 - 77s - loss: 635.8821 - loglik: -6.3545e+02 - logprior: 0.4115
Fitted a model with MAP estimate = -633.7791
Time for alignment: 1617.1194
Computed alignments with likelihoods: ['-633.7496', '-634.0946', '-633.7791']
Best model has likelihood: -633.7496
time for generating output: 0.4325
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00476.projection.fasta
SP score = 0.950999394306481
Training of 3 independent models on file PF02836.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f32047b8e50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b72c51220>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f37fc1a1790>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f37fc9e3820>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f37fc9e3af0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3bec6bc460>, <__main__.SimpleDirichletPrior object at 0x7f3205fb4070>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3bec7e3f70>

Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 645.2867 - loglik: -6.4307e+02 - logprior: -1.9910e+00
Epoch 2/10
39/39 - 19s - loss: 554.4828 - loglik: -5.5181e+02 - logprior: -1.7162e+00
Epoch 3/10
39/39 - 19s - loss: 543.8087 - loglik: -5.4038e+02 - logprior: -1.8840e+00
Epoch 4/10
39/39 - 20s - loss: 540.3519 - loglik: -5.3677e+02 - logprior: -1.9780e+00
Epoch 5/10
39/39 - 20s - loss: 538.5505 - loglik: -5.3500e+02 - logprior: -2.0209e+00
Epoch 6/10
39/39 - 20s - loss: 537.2402 - loglik: -5.3373e+02 - logprior: -2.0605e+00
Epoch 7/10
39/39 - 21s - loss: 536.4247 - loglik: -5.3295e+02 - logprior: -2.0808e+00
Epoch 8/10
39/39 - 21s - loss: 535.8368 - loglik: -5.3243e+02 - logprior: -2.0988e+00
Epoch 9/10
39/39 - 22s - loss: 535.2355 - loglik: -5.3188e+02 - logprior: -2.1109e+00
Epoch 10/10
39/39 - 22s - loss: 534.8944 - loglik: -5.3158e+02 - logprior: -2.1162e+00
Fitted a model with MAP estimate = -532.5522
expansions: [(4, 1), (6, 1), (31, 1), (34, 1), (77, 1), (81, 8), (89, 1), (90, 2), (92, 2), (116, 1), (117, 8), (118, 2), (119, 1), (120, 2), (126, 1), (129, 5), (130, 3), (131, 2), (136, 1), (138, 1), (141, 1), (143, 8), (144, 1), (146, 1), (148, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1), (159, 2), (160, 4), (172, 5), (173, 1), (184, 1)]
discards: [  0 161 162 163 164 165 166 167 168 169 175 187 188 189 190 191 192 193
 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 251 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 541.1874 - loglik: -5.3679e+02 - logprior: -3.0399e+00
Epoch 2/2
39/39 - 28s - loss: 526.1105 - loglik: -5.2260e+02 - logprior: -1.6481e+00
Fitted a model with MAP estimate = -520.8951
expansions: [(221, 1), (222, 1), (228, 1), (229, 1), (233, 3), (234, 3), (249, 4), (251, 11)]
discards: [  0  80  81  82  83 137 157 164 165 196 239 240]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 523.7057 - loglik: -5.1866e+02 - logprior: -3.2386e+00
Epoch 2/2
39/39 - 30s - loss: 513.7004 - loglik: -5.1031e+02 - logprior: -1.6648e+00
Fitted a model with MAP estimate = -509.2958
expansions: [(0, 2), (228, 1), (264, 3)]
discards: [ 79  80  81  83 197 229 239 240 241 242 243 244 245 246 247 248 249 250
 251 252 253 254 255 256 257 258 259 260 261 262 263]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 239 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 526.3830 - loglik: -5.2246e+02 - logprior: -2.1969e+00
Epoch 2/10
39/39 - 23s - loss: 522.5621 - loglik: -5.2019e+02 - logprior: -8.2100e-01
Epoch 3/10
39/39 - 22s - loss: 521.6092 - loglik: -5.1923e+02 - logprior: -6.5198e-01
Epoch 4/10
39/39 - 22s - loss: 520.4094 - loglik: -5.1792e+02 - logprior: -5.4499e-01
Epoch 5/10
39/39 - 22s - loss: 518.8021 - loglik: -5.1632e+02 - logprior: -4.4965e-01
Epoch 6/10
39/39 - 22s - loss: 517.7638 - loglik: -5.1556e+02 - logprior: -3.5113e-01
Epoch 7/10
39/39 - 22s - loss: 517.3704 - loglik: -5.1544e+02 - logprior: -2.3123e-01
Epoch 8/10
39/39 - 22s - loss: 517.2308 - loglik: -5.1552e+02 - logprior: -1.1688e-01
Epoch 9/10
39/39 - 22s - loss: 516.5627 - loglik: -5.1502e+02 - logprior: -6.5215e-03
Epoch 10/10
39/39 - 22s - loss: 516.3082 - loglik: -5.1494e+02 - logprior: 0.1199
Fitted a model with MAP estimate = -514.7145
Time for alignment: 673.9135
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 647.2772 - loglik: -6.4499e+02 - logprior: -2.0344e+00
Epoch 2/10
39/39 - 19s - loss: 551.4203 - loglik: -5.4860e+02 - logprior: -1.8754e+00
Epoch 3/10
39/39 - 19s - loss: 541.5305 - loglik: -5.3810e+02 - logprior: -1.9730e+00
Epoch 4/10
39/39 - 19s - loss: 539.0285 - loglik: -5.3566e+02 - logprior: -1.9645e+00
Epoch 5/10
39/39 - 19s - loss: 537.4410 - loglik: -5.3417e+02 - logprior: -1.9825e+00
Epoch 6/10
39/39 - 19s - loss: 536.3431 - loglik: -5.3314e+02 - logprior: -2.0238e+00
Epoch 7/10
39/39 - 19s - loss: 535.3484 - loglik: -5.3214e+02 - logprior: -2.0423e+00
Epoch 8/10
39/39 - 19s - loss: 535.3862 - loglik: -5.3220e+02 - logprior: -2.0538e+00
Fitted a model with MAP estimate = -533.0550
expansions: [(7, 1), (27, 1), (38, 1), (52, 1), (82, 2), (86, 1), (120, 1), (121, 10), (122, 2), (123, 1), (130, 1), (131, 2), (133, 8), (136, 1), (137, 1), (139, 1), (140, 1), (141, 3), (142, 10), (145, 1), (146, 1), (153, 1), (156, 1), (159, 1), (160, 2), (161, 2), (171, 1), (172, 5), (173, 1), (183, 1)]
discards: [  0 150 162 163 164 165 166 167 168 175 176 177 178 179 180 181 184 185
 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203
 204 205 206 207 208 209 210 211]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 234 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 548.3801 - loglik: -5.4398e+02 - logprior: -3.0308e+00
Epoch 2/2
39/39 - 21s - loss: 532.8040 - loglik: -5.2926e+02 - logprior: -1.7237e+00
Fitted a model with MAP estimate = -527.6671
expansions: [(214, 1), (215, 7), (223, 4), (224, 2), (231, 4), (233, 9), (234, 3)]
discards: [  0 139 153 158 159 160 175 177 185]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 255 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 526.2536 - loglik: -5.2141e+02 - logprior: -3.0955e+00
Epoch 2/2
39/39 - 24s - loss: 517.0633 - loglik: -5.1399e+02 - logprior: -1.3828e+00
Fitted a model with MAP estimate = -513.1068
expansions: [(0, 2), (208, 1), (225, 1), (227, 1), (255, 4)]
discards: [ 80  81  82 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244
 245 246 247 248 249 250 251 252 253 254]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 527.2990 - loglik: -5.2343e+02 - logprior: -2.1874e+00
Epoch 2/10
39/39 - 22s - loss: 523.4530 - loglik: -5.2103e+02 - logprior: -9.5080e-01
Epoch 3/10
39/39 - 22s - loss: 522.2461 - loglik: -5.1993e+02 - logprior: -7.7895e-01
Epoch 4/10
39/39 - 22s - loss: 521.8345 - loglik: -5.1983e+02 - logprior: -6.5401e-01
Epoch 5/10
39/39 - 22s - loss: 521.0330 - loglik: -5.1918e+02 - logprior: -5.3683e-01
Epoch 6/10
39/39 - 21s - loss: 520.7239 - loglik: -5.1904e+02 - logprior: -4.2793e-01
Epoch 7/10
39/39 - 22s - loss: 520.4268 - loglik: -5.1889e+02 - logprior: -3.1194e-01
Epoch 8/10
39/39 - 22s - loss: 519.7379 - loglik: -5.1832e+02 - logprior: -2.0285e-01
Epoch 9/10
39/39 - 21s - loss: 519.8347 - loglik: -5.1854e+02 - logprior: -9.3764e-02
Fitted a model with MAP estimate = -518.1480
Time for alignment: 548.5276
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 645.5101 - loglik: -6.4329e+02 - logprior: -1.9979e+00
Epoch 2/10
39/39 - 19s - loss: 550.3362 - loglik: -5.4735e+02 - logprior: -1.9331e+00
Epoch 3/10
39/39 - 19s - loss: 540.1172 - loglik: -5.3650e+02 - logprior: -2.0923e+00
Epoch 4/10
39/39 - 19s - loss: 537.7492 - loglik: -5.3413e+02 - logprior: -2.0855e+00
Epoch 5/10
39/39 - 19s - loss: 535.8596 - loglik: -5.3227e+02 - logprior: -2.1290e+00
Epoch 6/10
39/39 - 19s - loss: 534.9302 - loglik: -5.3144e+02 - logprior: -2.1377e+00
Epoch 7/10
39/39 - 18s - loss: 534.1628 - loglik: -5.3075e+02 - logprior: -2.1618e+00
Epoch 8/10
39/39 - 18s - loss: 533.5130 - loglik: -5.3012e+02 - logprior: -2.1786e+00
Epoch 9/10
39/39 - 18s - loss: 533.1263 - loglik: -5.2978e+02 - logprior: -2.1963e+00
Epoch 10/10
39/39 - 18s - loss: 532.7062 - loglik: -5.2938e+02 - logprior: -2.2036e+00
Fitted a model with MAP estimate = -530.8881
expansions: [(4, 1), (6, 1), (31, 1), (32, 1), (51, 1), (77, 1), (80, 8), (81, 1), (87, 1), (88, 1), (89, 1), (92, 1), (112, 1), (113, 8), (114, 2), (115, 1), (116, 1), (128, 5), (129, 2), (132, 1), (133, 1), (135, 1), (138, 2), (139, 10), (140, 1), (145, 1), (149, 1), (153, 1), (156, 1), (159, 1), (160, 1), (161, 2), (173, 5), (186, 1)]
discards: [  0 162 163 164 165 166 167 168 169 170 171 188 189 190 191 192 193 194
 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 246 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 541.0341 - loglik: -5.3670e+02 - logprior: -2.9733e+00
Epoch 2/2
39/39 - 22s - loss: 527.6562 - loglik: -5.2447e+02 - logprior: -1.4379e+00
Fitted a model with MAP estimate = -522.6889
expansions: [(221, 3), (229, 3), (246, 17)]
discards: [  0  80  81  82  93 134 161 162 176 177 178 192 203 225 235]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 254 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 527.0573 - loglik: -5.2184e+02 - logprior: -3.2787e+00
Epoch 2/2
39/39 - 23s - loss: 517.9349 - loglik: -5.1439e+02 - logprior: -1.7520e+00
Fitted a model with MAP estimate = -513.9541
expansions: [(0, 2), (86, 6), (215, 1), (219, 1), (220, 3), (227, 1), (236, 4), (254, 2)]
discards: [  0  79 208 209 210 211 212 239 240 241 242 243 244 245 246 247 248 249
 250 251 252 253]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 252 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 523.6665 - loglik: -5.1982e+02 - logprior: -2.1791e+00
Epoch 2/10
39/39 - 23s - loss: 518.2539 - loglik: -5.1559e+02 - logprior: -9.2339e-01
Epoch 3/10
39/39 - 23s - loss: 516.6991 - loglik: -5.1411e+02 - logprior: -7.0688e-01
Epoch 4/10
39/39 - 23s - loss: 515.3428 - loglik: -5.1279e+02 - logprior: -5.7031e-01
Epoch 5/10
39/39 - 23s - loss: 514.1741 - loglik: -5.1164e+02 - logprior: -4.7960e-01
Epoch 6/10
39/39 - 23s - loss: 513.1484 - loglik: -5.1092e+02 - logprior: -3.8207e-01
Epoch 7/10
39/39 - 23s - loss: 512.5749 - loglik: -5.1063e+02 - logprior: -2.4588e-01
Epoch 8/10
39/39 - 23s - loss: 512.2716 - loglik: -5.1053e+02 - logprior: -1.5112e-01
Epoch 9/10
39/39 - 23s - loss: 511.7555 - loglik: -5.1021e+02 - logprior: -2.6588e-02
Epoch 10/10
39/39 - 23s - loss: 511.8163 - loglik: -5.1043e+02 - logprior: 0.0793
Fitted a model with MAP estimate = -509.8142
Time for alignment: 620.9412
Computed alignments with likelihoods: ['-509.2958', '-513.1068', '-509.8142']
Best model has likelihood: -509.2958
time for generating output: 0.3199
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02836.projection.fasta
SP score = 0.9083112624542817
Training of 3 independent models on file PF02777.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3220dd7f70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f32385f0310>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b72d89940>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3218445c10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3220685e20>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f32206e4970>, <__main__.SimpleDirichletPrior object at 0x7f37fd47d280>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3bbf5ab3a0>

Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 269.8696 - loglik: -2.6668e+02 - logprior: -3.1790e+00
Epoch 2/10
19/19 - 3s - loss: 200.9489 - loglik: -1.9952e+02 - logprior: -1.3904e+00
Epoch 3/10
19/19 - 3s - loss: 176.5403 - loglik: -1.7463e+02 - logprior: -1.8046e+00
Epoch 4/10
19/19 - 3s - loss: 171.6557 - loglik: -1.6962e+02 - logprior: -1.7793e+00
Epoch 5/10
19/19 - 3s - loss: 170.6364 - loglik: -1.6873e+02 - logprior: -1.6986e+00
Epoch 6/10
19/19 - 3s - loss: 169.9377 - loglik: -1.6805e+02 - logprior: -1.6866e+00
Epoch 7/10
19/19 - 3s - loss: 170.1134 - loglik: -1.6827e+02 - logprior: -1.6565e+00
Fitted a model with MAP estimate = -169.2388
expansions: [(4, 1), (6, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 5), (47, 1), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 162.6470 - loglik: -1.5938e+02 - logprior: -3.0767e+00
Epoch 2/2
19/19 - 3s - loss: 151.4240 - loglik: -1.4988e+02 - logprior: -1.3423e+00
Fitted a model with MAP estimate = -150.3761
expansions: []
discards: [42 59 60]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 153.2418 - loglik: -1.4992e+02 - logprior: -3.0783e+00
Epoch 2/2
19/19 - 3s - loss: 150.6334 - loglik: -1.4910e+02 - logprior: -1.2918e+00
Fitted a model with MAP estimate = -149.7647
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 152.0124 - loglik: -1.4872e+02 - logprior: -3.0489e+00
Epoch 2/10
19/19 - 3s - loss: 150.0702 - loglik: -1.4858e+02 - logprior: -1.2467e+00
Epoch 3/10
19/19 - 3s - loss: 149.6165 - loglik: -1.4820e+02 - logprior: -1.1615e+00
Epoch 4/10
19/19 - 3s - loss: 149.4620 - loglik: -1.4809e+02 - logprior: -1.1187e+00
Epoch 5/10
19/19 - 3s - loss: 149.2473 - loglik: -1.4791e+02 - logprior: -1.0762e+00
Epoch 6/10
19/19 - 3s - loss: 149.4704 - loglik: -1.4816e+02 - logprior: -1.0466e+00
Fitted a model with MAP estimate = -149.0492
Time for alignment: 86.6192
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 269.8602 - loglik: -2.6667e+02 - logprior: -3.1767e+00
Epoch 2/10
19/19 - 3s - loss: 200.4306 - loglik: -1.9900e+02 - logprior: -1.3922e+00
Epoch 3/10
19/19 - 3s - loss: 176.2714 - loglik: -1.7432e+02 - logprior: -1.7875e+00
Epoch 4/10
19/19 - 3s - loss: 172.9996 - loglik: -1.7100e+02 - logprior: -1.7391e+00
Epoch 5/10
19/19 - 3s - loss: 171.7156 - loglik: -1.6986e+02 - logprior: -1.6643e+00
Epoch 6/10
19/19 - 3s - loss: 170.4524 - loglik: -1.6860e+02 - logprior: -1.6589e+00
Epoch 7/10
19/19 - 3s - loss: 170.8845 - loglik: -1.6904e+02 - logprior: -1.6512e+00
Fitted a model with MAP estimate = -169.9581
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 5), (47, 1), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 166.3894 - loglik: -1.6211e+02 - logprior: -4.1076e+00
Epoch 2/2
19/19 - 3s - loss: 154.4820 - loglik: -1.5208e+02 - logprior: -2.2091e+00
Fitted a model with MAP estimate = -152.4665
expansions: [(0, 2)]
discards: [ 0 42 59 60]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 153.8182 - loglik: -1.5052e+02 - logprior: -3.0639e+00
Epoch 2/2
19/19 - 3s - loss: 150.1790 - loglik: -1.4867e+02 - logprior: -1.2704e+00
Fitted a model with MAP estimate = -149.2435
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 155.3682 - loglik: -1.5110e+02 - logprior: -4.0199e+00
Epoch 2/10
19/19 - 3s - loss: 150.9855 - loglik: -1.4914e+02 - logprior: -1.6006e+00
Epoch 3/10
19/19 - 3s - loss: 150.0025 - loglik: -1.4855e+02 - logprior: -1.2050e+00
Epoch 4/10
19/19 - 3s - loss: 149.7835 - loglik: -1.4835e+02 - logprior: -1.1860e+00
Epoch 5/10
19/19 - 3s - loss: 149.6904 - loglik: -1.4828e+02 - logprior: -1.1514e+00
Epoch 6/10
19/19 - 3s - loss: 149.2980 - loglik: -1.4792e+02 - logprior: -1.1198e+00
Epoch 7/10
19/19 - 3s - loss: 149.7154 - loglik: -1.4836e+02 - logprior: -1.0906e+00
Fitted a model with MAP estimate = -149.1159
Time for alignment: 88.4189
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 270.0427 - loglik: -2.6686e+02 - logprior: -3.1772e+00
Epoch 2/10
19/19 - 3s - loss: 200.2677 - loglik: -1.9884e+02 - logprior: -1.3900e+00
Epoch 3/10
19/19 - 3s - loss: 176.6119 - loglik: -1.7469e+02 - logprior: -1.7811e+00
Epoch 4/10
19/19 - 3s - loss: 172.8947 - loglik: -1.7090e+02 - logprior: -1.7333e+00
Epoch 5/10
19/19 - 3s - loss: 171.4846 - loglik: -1.6960e+02 - logprior: -1.6825e+00
Epoch 6/10
19/19 - 3s - loss: 170.8118 - loglik: -1.6892e+02 - logprior: -1.6854e+00
Epoch 7/10
19/19 - 3s - loss: 170.3270 - loglik: -1.6847e+02 - logprior: -1.6692e+00
Epoch 8/10
19/19 - 3s - loss: 169.7801 - loglik: -1.6792e+02 - logprior: -1.6672e+00
Epoch 9/10
19/19 - 3s - loss: 169.7336 - loglik: -1.6787e+02 - logprior: -1.6670e+00
Epoch 10/10
19/19 - 3s - loss: 169.8367 - loglik: -1.6798e+02 - logprior: -1.6696e+00
Fitted a model with MAP estimate = -169.2806
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 165.8053 - loglik: -1.6146e+02 - logprior: -4.1450e+00
Epoch 2/2
19/19 - 3s - loss: 154.0114 - loglik: -1.5156e+02 - logprior: -2.2293e+00
Fitted a model with MAP estimate = -152.2764
expansions: [(0, 2)]
discards: [ 0 42 61 62]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 153.1950 - loglik: -1.4989e+02 - logprior: -3.0724e+00
Epoch 2/2
19/19 - 3s - loss: 149.9287 - loglik: -1.4842e+02 - logprior: -1.2678e+00
Fitted a model with MAP estimate = -149.0040
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 155.1304 - loglik: -1.5089e+02 - logprior: -3.9861e+00
Epoch 2/10
19/19 - 3s - loss: 150.7154 - loglik: -1.4887e+02 - logprior: -1.5904e+00
Epoch 3/10
19/19 - 3s - loss: 149.7674 - loglik: -1.4829e+02 - logprior: -1.2137e+00
Epoch 4/10
19/19 - 3s - loss: 150.0088 - loglik: -1.4857e+02 - logprior: -1.1780e+00
Fitted a model with MAP estimate = -149.2346
Time for alignment: 88.5052
Computed alignments with likelihoods: ['-149.0492', '-149.1159', '-149.0040']
Best model has likelihood: -149.0040
time for generating output: 0.1532
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02777.projection.fasta
SP score = 0.9268085106382978
Training of 3 independent models on file PF07654.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f38644cca90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3bbf593c40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f37fe6ece80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f387c2ca370>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f387c2caee0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f37fc615700>, <__main__.SimpleDirichletPrior object at 0x7f38041d8f40>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3bbf5ab3a0>

Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 231.9381 - loglik: -2.2875e+02 - logprior: -3.1708e+00
Epoch 2/10
19/19 - 2s - loss: 196.7433 - loglik: -1.9529e+02 - logprior: -1.3534e+00
Epoch 3/10
19/19 - 2s - loss: 184.6228 - loglik: -1.8283e+02 - logprior: -1.4597e+00
Epoch 4/10
19/19 - 2s - loss: 180.9960 - loglik: -1.7921e+02 - logprior: -1.4347e+00
Epoch 5/10
19/19 - 2s - loss: 179.7877 - loglik: -1.7807e+02 - logprior: -1.4137e+00
Epoch 6/10
19/19 - 2s - loss: 179.5073 - loglik: -1.7786e+02 - logprior: -1.3863e+00
Epoch 7/10
19/19 - 2s - loss: 178.8465 - loglik: -1.7722e+02 - logprior: -1.3740e+00
Epoch 8/10
19/19 - 2s - loss: 178.5016 - loglik: -1.7688e+02 - logprior: -1.3636e+00
Epoch 9/10
19/19 - 2s - loss: 178.4611 - loglik: -1.7683e+02 - logprior: -1.3637e+00
Epoch 10/10
19/19 - 2s - loss: 178.7570 - loglik: -1.7713e+02 - logprior: -1.3548e+00
Fitted a model with MAP estimate = -178.0943
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (32, 1), (33, 1), (34, 2), (35, 1), (47, 1), (49, 1), (50, 2), (53, 2), (55, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 180.2993 - loglik: -1.7601e+02 - logprior: -4.0411e+00
Epoch 2/2
19/19 - 2s - loss: 173.1457 - loglik: -1.7059e+02 - logprior: -2.2531e+00
Fitted a model with MAP estimate = -171.3838
expansions: [(0, 2), (10, 1)]
discards: [ 0 43 65 70]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 172.7664 - loglik: -1.6937e+02 - logprior: -3.0384e+00
Epoch 2/2
19/19 - 2s - loss: 169.6424 - loglik: -1.6799e+02 - logprior: -1.2740e+00
Fitted a model with MAP estimate = -168.7751
expansions: []
discards: [ 0 69]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 173.4274 - loglik: -1.6930e+02 - logprior: -3.7521e+00
Epoch 2/10
19/19 - 2s - loss: 170.3417 - loglik: -1.6860e+02 - logprior: -1.3536e+00
Epoch 3/10
19/19 - 2s - loss: 169.1819 - loglik: -1.6763e+02 - logprior: -1.1744e+00
Epoch 4/10
19/19 - 2s - loss: 169.0909 - loglik: -1.6761e+02 - logprior: -1.1248e+00
Epoch 5/10
19/19 - 2s - loss: 168.9370 - loglik: -1.6750e+02 - logprior: -1.0948e+00
Epoch 6/10
19/19 - 2s - loss: 168.6528 - loglik: -1.6725e+02 - logprior: -1.0607e+00
Epoch 7/10
19/19 - 2s - loss: 168.9765 - loglik: -1.6758e+02 - logprior: -1.0452e+00
Fitted a model with MAP estimate = -168.2154
Time for alignment: 68.2269
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 231.7893 - loglik: -2.2860e+02 - logprior: -3.1723e+00
Epoch 2/10
19/19 - 2s - loss: 195.3141 - loglik: -1.9385e+02 - logprior: -1.3524e+00
Epoch 3/10
19/19 - 2s - loss: 183.8362 - loglik: -1.8195e+02 - logprior: -1.4492e+00
Epoch 4/10
19/19 - 2s - loss: 180.8044 - loglik: -1.7904e+02 - logprior: -1.4089e+00
Epoch 5/10
19/19 - 2s - loss: 179.3295 - loglik: -1.7763e+02 - logprior: -1.3963e+00
Epoch 6/10
19/19 - 2s - loss: 179.3855 - loglik: -1.7774e+02 - logprior: -1.3862e+00
Fitted a model with MAP estimate = -178.5525
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (32, 1), (33, 1), (34, 1), (35, 2), (47, 1), (49, 1), (50, 2), (53, 2), (55, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 180.5134 - loglik: -1.7625e+02 - logprior: -4.0360e+00
Epoch 2/2
19/19 - 2s - loss: 173.0258 - loglik: -1.7050e+02 - logprior: -2.2187e+00
Fitted a model with MAP estimate = -171.4648
expansions: [(0, 2)]
discards: [ 0 44 65 70]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 173.1248 - loglik: -1.6972e+02 - logprior: -3.0367e+00
Epoch 2/2
19/19 - 2s - loss: 169.9224 - loglik: -1.6827e+02 - logprior: -1.2873e+00
Fitted a model with MAP estimate = -169.1812
expansions: []
discards: [ 0 11 68]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 174.4552 - loglik: -1.7032e+02 - logprior: -3.7612e+00
Epoch 2/10
19/19 - 2s - loss: 171.0002 - loglik: -1.6928e+02 - logprior: -1.3563e+00
Epoch 3/10
19/19 - 2s - loss: 170.1591 - loglik: -1.6864e+02 - logprior: -1.1661e+00
Epoch 4/10
19/19 - 2s - loss: 169.9284 - loglik: -1.6846e+02 - logprior: -1.1296e+00
Epoch 5/10
19/19 - 2s - loss: 169.7757 - loglik: -1.6836e+02 - logprior: -1.0874e+00
Epoch 6/10
19/19 - 2s - loss: 169.7380 - loglik: -1.6835e+02 - logprior: -1.0664e+00
Epoch 7/10
19/19 - 2s - loss: 169.6049 - loglik: -1.6823e+02 - logprior: -1.0482e+00
Epoch 8/10
19/19 - 2s - loss: 169.8378 - loglik: -1.6849e+02 - logprior: -1.0278e+00
Fitted a model with MAP estimate = -169.1829
Time for alignment: 61.8801
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 231.9618 - loglik: -2.2877e+02 - logprior: -3.1687e+00
Epoch 2/10
19/19 - 2s - loss: 196.1885 - loglik: -1.9472e+02 - logprior: -1.3497e+00
Epoch 3/10
19/19 - 2s - loss: 184.0202 - loglik: -1.8218e+02 - logprior: -1.4435e+00
Epoch 4/10
19/19 - 2s - loss: 181.2175 - loglik: -1.7947e+02 - logprior: -1.4158e+00
Epoch 5/10
19/19 - 2s - loss: 179.9586 - loglik: -1.7826e+02 - logprior: -1.4171e+00
Epoch 6/10
19/19 - 2s - loss: 179.5373 - loglik: -1.7790e+02 - logprior: -1.3909e+00
Epoch 7/10
19/19 - 2s - loss: 179.4397 - loglik: -1.7780e+02 - logprior: -1.3812e+00
Epoch 8/10
19/19 - 2s - loss: 179.3393 - loglik: -1.7772e+02 - logprior: -1.3697e+00
Epoch 9/10
19/19 - 2s - loss: 179.0523 - loglik: -1.7743e+02 - logprior: -1.3714e+00
Epoch 10/10
19/19 - 2s - loss: 179.2357 - loglik: -1.7761e+02 - logprior: -1.3708e+00
Fitted a model with MAP estimate = -178.6419
expansions: [(7, 2), (8, 3), (9, 3), (23, 1), (30, 2), (31, 1), (35, 2), (37, 1), (46, 1), (48, 1), (53, 1), (55, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 180.0441 - loglik: -1.7578e+02 - logprior: -4.0235e+00
Epoch 2/2
19/19 - 2s - loss: 172.6533 - loglik: -1.7012e+02 - logprior: -2.2166e+00
Fitted a model with MAP estimate = -171.3313
expansions: [(0, 2)]
discards: [ 0 38 73]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 172.9195 - loglik: -1.6958e+02 - logprior: -3.0080e+00
Epoch 2/2
19/19 - 2s - loss: 170.1849 - loglik: -1.6861e+02 - logprior: -1.2427e+00
Fitted a model with MAP estimate = -169.5349
expansions: []
discards: [0 7]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 174.1064 - loglik: -1.7003e+02 - logprior: -3.7408e+00
Epoch 2/10
19/19 - 2s - loss: 170.9923 - loglik: -1.6929e+02 - logprior: -1.3641e+00
Epoch 3/10
19/19 - 2s - loss: 169.9563 - loglik: -1.6844e+02 - logprior: -1.1856e+00
Epoch 4/10
19/19 - 2s - loss: 170.0920 - loglik: -1.6862e+02 - logprior: -1.1504e+00
Fitted a model with MAP estimate = -169.5877
Time for alignment: 60.3646
Computed alignments with likelihoods: ['-168.2154', '-169.1812', '-169.5349']
Best model has likelihood: -168.2154
time for generating output: 0.1384
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07654.projection.fasta
SP score = 0.8061224489795918
Training of 3 independent models on file PF04082.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3220588550>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3220c66a00>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3bbf242df0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f32115a0d90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f37fc4177f0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f321134e370>, <__main__.SimpleDirichletPrior object at 0x7f3a98247850>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3ba5ae0ee0>

Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 641.3784 - loglik: -6.3898e+02 - logprior: -1.8732e+00
Epoch 2/10
39/39 - 20s - loss: 584.5582 - loglik: -5.8098e+02 - logprior: -1.4062e+00
Epoch 3/10
39/39 - 20s - loss: 573.0981 - loglik: -5.6870e+02 - logprior: -1.5393e+00
Epoch 4/10
39/39 - 20s - loss: 569.1243 - loglik: -5.6456e+02 - logprior: -1.5594e+00
Epoch 5/10
39/39 - 20s - loss: 566.0339 - loglik: -5.6173e+02 - logprior: -1.5928e+00
Epoch 6/10
39/39 - 20s - loss: 564.0571 - loglik: -5.6021e+02 - logprior: -1.6106e+00
Epoch 7/10
39/39 - 20s - loss: 562.7339 - loglik: -5.5924e+02 - logprior: -1.6388e+00
Epoch 8/10
39/39 - 20s - loss: 561.8055 - loglik: -5.5856e+02 - logprior: -1.6525e+00
Epoch 9/10
39/39 - 20s - loss: 561.6572 - loglik: -5.5857e+02 - logprior: -1.6503e+00
Epoch 10/10
39/39 - 20s - loss: 560.5497 - loglik: -5.5753e+02 - logprior: -1.6547e+00
Fitted a model with MAP estimate = -558.4857
expansions: [(23, 1), (24, 6), (28, 1), (49, 6), (60, 1), (62, 2), (63, 2), (65, 1), (80, 1), (81, 2), (82, 3), (83, 2), (103, 1), (104, 4), (105, 2), (109, 1), (120, 1), (123, 1), (126, 1), (133, 1), (149, 6), (161, 1), (163, 1), (165, 1), (168, 1), (169, 1), (170, 1), (174, 1), (175, 2)]
discards: [  0 189]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 245 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 553.6192 - loglik: -5.4873e+02 - logprior: -3.2395e+00
Epoch 2/2
39/39 - 26s - loss: 540.2180 - loglik: -5.3574e+02 - logprior: -1.7249e+00
Fitted a model with MAP estimate = -532.9561
expansions: [(245, 2)]
discards: [ 79 103 106 133 134]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 537.6486 - loglik: -5.3228e+02 - logprior: -2.5402e+00
Epoch 2/2
39/39 - 25s - loss: 534.4998 - loglik: -5.3076e+02 - logprior: -1.2401e+00
Fitted a model with MAP estimate = -530.5587
expansions: []
discards: [ 79 130 240]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 239 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 534.4896 - loglik: -5.2993e+02 - logprior: -2.3233e+00
Epoch 2/10
39/39 - 24s - loss: 532.4655 - loglik: -5.2965e+02 - logprior: -9.6572e-01
Epoch 3/10
39/39 - 23s - loss: 531.7934 - loglik: -5.2928e+02 - logprior: -7.9867e-01
Epoch 4/10
39/39 - 23s - loss: 530.9756 - loglik: -5.2887e+02 - logprior: -6.2957e-01
Epoch 5/10
39/39 - 23s - loss: 530.6172 - loglik: -5.2882e+02 - logprior: -4.6554e-01
Epoch 6/10
39/39 - 24s - loss: 530.1713 - loglik: -5.2863e+02 - logprior: -3.0514e-01
Epoch 7/10
39/39 - 24s - loss: 529.9708 - loglik: -5.2869e+02 - logprior: -1.2382e-01
Epoch 8/10
39/39 - 24s - loss: 529.2557 - loglik: -5.2820e+02 - logprior: 0.0413
Epoch 9/10
39/39 - 24s - loss: 529.5186 - loglik: -5.2867e+02 - logprior: 0.2127
Fitted a model with MAP estimate = -527.9369
Time for alignment: 625.6270
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 641.4650 - loglik: -6.3906e+02 - logprior: -1.8704e+00
Epoch 2/10
39/39 - 19s - loss: 587.4103 - loglik: -5.8365e+02 - logprior: -1.2818e+00
Epoch 3/10
39/39 - 19s - loss: 577.4798 - loglik: -5.7288e+02 - logprior: -1.4472e+00
Epoch 4/10
39/39 - 19s - loss: 570.9996 - loglik: -5.6614e+02 - logprior: -1.5496e+00
Epoch 5/10
39/39 - 19s - loss: 566.5416 - loglik: -5.6203e+02 - logprior: -1.6338e+00
Epoch 6/10
39/39 - 19s - loss: 564.3750 - loglik: -5.6041e+02 - logprior: -1.6725e+00
Epoch 7/10
39/39 - 19s - loss: 562.6683 - loglik: -5.5907e+02 - logprior: -1.7069e+00
Epoch 8/10
39/39 - 19s - loss: 561.5548 - loglik: -5.5818e+02 - logprior: -1.7200e+00
Epoch 9/10
39/39 - 19s - loss: 560.6655 - loglik: -5.5739e+02 - logprior: -1.7314e+00
Epoch 10/10
39/39 - 19s - loss: 559.6563 - loglik: -5.5631e+02 - logprior: -1.7547e+00
Fitted a model with MAP estimate = -557.2081
expansions: [(23, 1), (25, 4), (28, 1), (44, 2), (48, 5), (59, 1), (61, 2), (62, 1), (64, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 2), (103, 1), (104, 1), (105, 2), (108, 1), (109, 1), (123, 1), (124, 1), (126, 1), (133, 1), (155, 7), (161, 1), (163, 1), (167, 1), (168, 1), (169, 1), (170, 1), (179, 1)]
discards: [  1 189]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 238 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 551.2276 - loglik: -5.4677e+02 - logprior: -2.5420e+00
Epoch 2/2
39/39 - 24s - loss: 539.1945 - loglik: -5.3519e+02 - logprior: -1.3661e+00
Fitted a model with MAP estimate = -534.3609
expansions: [(238, 2)]
discards: [104 130]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 238 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 538.0984 - loglik: -5.3299e+02 - logprior: -2.5999e+00
Epoch 2/2
39/39 - 25s - loss: 535.3510 - loglik: -5.3184e+02 - logprior: -1.2768e+00
Fitted a model with MAP estimate = -532.1740
expansions: []
discards: [ 77 214 215 216 217 236]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 232 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 536.5585 - loglik: -5.3216e+02 - logprior: -2.3681e+00
Epoch 2/10
39/39 - 25s - loss: 534.0305 - loglik: -5.3122e+02 - logprior: -1.0956e+00
Epoch 3/10
39/39 - 25s - loss: 533.2192 - loglik: -5.3067e+02 - logprior: -9.8019e-01
Epoch 4/10
39/39 - 25s - loss: 532.3362 - loglik: -5.3013e+02 - logprior: -8.4075e-01
Epoch 5/10
39/39 - 25s - loss: 532.2648 - loglik: -5.3030e+02 - logprior: -7.2226e-01
Epoch 6/10
39/39 - 25s - loss: 531.6641 - loglik: -5.2989e+02 - logprior: -6.1311e-01
Epoch 7/10
39/39 - 25s - loss: 531.3859 - loglik: -5.2983e+02 - logprior: -4.6953e-01
Epoch 8/10
39/39 - 25s - loss: 531.0546 - loglik: -5.2971e+02 - logprior: -3.0304e-01
Epoch 9/10
39/39 - 25s - loss: 531.3130 - loglik: -5.3016e+02 - logprior: -1.5420e-01
Fitted a model with MAP estimate = -529.7222
Time for alignment: 625.4622
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 642.4322 - loglik: -6.4003e+02 - logprior: -1.8721e+00
Epoch 2/10
39/39 - 20s - loss: 587.2515 - loglik: -5.8345e+02 - logprior: -1.3152e+00
Epoch 3/10
39/39 - 21s - loss: 574.9760 - loglik: -5.7045e+02 - logprior: -1.5829e+00
Epoch 4/10
39/39 - 21s - loss: 569.8252 - loglik: -5.6508e+02 - logprior: -1.6235e+00
Epoch 5/10
39/39 - 21s - loss: 566.8366 - loglik: -5.6239e+02 - logprior: -1.6737e+00
Epoch 6/10
39/39 - 21s - loss: 564.5610 - loglik: -5.6061e+02 - logprior: -1.7087e+00
Epoch 7/10
39/39 - 21s - loss: 562.8422 - loglik: -5.5924e+02 - logprior: -1.7396e+00
Epoch 8/10
39/39 - 21s - loss: 562.0742 - loglik: -5.5869e+02 - logprior: -1.7581e+00
Epoch 9/10
39/39 - 21s - loss: 560.6966 - loglik: -5.5736e+02 - logprior: -1.7807e+00
Epoch 10/10
39/39 - 21s - loss: 560.9008 - loglik: -5.5754e+02 - logprior: -1.7928e+00
Fitted a model with MAP estimate = -557.5555
expansions: [(23, 1), (25, 4), (28, 1), (44, 2), (48, 5), (60, 1), (62, 2), (63, 1), (64, 1), (65, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 2), (95, 1), (102, 1), (103, 1), (104, 2), (107, 1), (108, 1), (119, 1), (122, 1), (125, 1), (132, 1), (156, 3), (158, 1), (160, 1), (165, 1), (166, 1), (167, 1), (169, 1), (174, 1)]
discards: [  1 189]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 551.7052 - loglik: -5.4717e+02 - logprior: -2.5525e+00
Epoch 2/2
39/39 - 26s - loss: 540.3813 - loglik: -5.3643e+02 - logprior: -1.3345e+00
Fitted a model with MAP estimate = -535.4415
expansions: [(236, 2)]
discards: [106 131 190]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 539.2029 - loglik: -5.3409e+02 - logprior: -2.5895e+00
Epoch 2/2
39/39 - 25s - loss: 536.4614 - loglik: -5.3295e+02 - logprior: -1.2610e+00
Fitted a model with MAP estimate = -533.1251
expansions: []
discards: [ 78 174 175 176 233]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 230 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 537.4643 - loglik: -5.3300e+02 - logprior: -2.3829e+00
Epoch 2/10
39/39 - 25s - loss: 535.7626 - loglik: -5.3299e+02 - logprior: -1.0238e+00
Epoch 3/10
39/39 - 25s - loss: 535.1398 - loglik: -5.3261e+02 - logprior: -8.7836e-01
Epoch 4/10
39/39 - 25s - loss: 534.2869 - loglik: -5.3213e+02 - logprior: -7.3239e-01
Epoch 5/10
39/39 - 26s - loss: 534.0897 - loglik: -5.3221e+02 - logprior: -5.8369e-01
Epoch 6/10
39/39 - 26s - loss: 533.6967 - loglik: -5.3207e+02 - logprior: -4.3096e-01
Epoch 7/10
39/39 - 26s - loss: 533.4704 - loglik: -5.3210e+02 - logprior: -2.4951e-01
Epoch 8/10
39/39 - 26s - loss: 532.7122 - loglik: -5.3153e+02 - logprior: -1.1254e-01
Epoch 9/10
39/39 - 25s - loss: 532.8361 - loglik: -5.3187e+02 - logprior: 0.0613
Fitted a model with MAP estimate = -531.6189
Time for alignment: 652.3574
Computed alignments with likelihoods: ['-527.9369', '-529.7222', '-531.6189']
Best model has likelihood: -527.9369
time for generating output: 0.2828
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF04082.projection.fasta
SP score = 0.5618081180811808
Training of 3 independent models on file PF00046.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b8c4f1760>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3a8ef46430>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f38441dcd00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3219a43b80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3219a43820>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f37fc21cd00>, <__main__.SimpleDirichletPrior object at 0x7f32187bc190>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f386413f040>

Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 154.1236 - loglik: -1.5086e+02 - logprior: -3.2590e+00
Epoch 2/10
19/19 - 1s - loss: 124.1960 - loglik: -1.2263e+02 - logprior: -1.5308e+00
Epoch 3/10
19/19 - 1s - loss: 111.4483 - loglik: -1.0961e+02 - logprior: -1.6024e+00
Epoch 4/10
19/19 - 1s - loss: 107.9754 - loglik: -1.0602e+02 - logprior: -1.6445e+00
Epoch 5/10
19/19 - 1s - loss: 106.9812 - loglik: -1.0519e+02 - logprior: -1.5751e+00
Epoch 6/10
19/19 - 1s - loss: 106.7497 - loglik: -1.0496e+02 - logprior: -1.5849e+00
Epoch 7/10
19/19 - 1s - loss: 106.3376 - loglik: -1.0461e+02 - logprior: -1.5455e+00
Epoch 8/10
19/19 - 1s - loss: 106.3718 - loglik: -1.0464e+02 - logprior: -1.5421e+00
Fitted a model with MAP estimate = -105.8919
expansions: [(3, 1), (5, 1), (7, 2), (9, 1), (16, 1), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 103.9901 - loglik: -1.0066e+02 - logprior: -3.1478e+00
Epoch 2/2
19/19 - 1s - loss: 97.7783 - loglik: -9.6196e+01 - logprior: -1.4005e+00
Fitted a model with MAP estimate = -96.9403
expansions: []
discards: [ 0 36 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 103.0450 - loglik: -9.8720e+01 - logprior: -4.1380e+00
Epoch 2/2
19/19 - 1s - loss: 99.6135 - loglik: -9.7119e+01 - logprior: -2.2990e+00
Fitted a model with MAP estimate = -98.4527
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 101.9932 - loglik: -9.7652e+01 - logprior: -4.1421e+00
Epoch 2/10
19/19 - 1s - loss: 98.3872 - loglik: -9.6329e+01 - logprior: -1.8623e+00
Epoch 3/10
19/19 - 2s - loss: 97.5842 - loglik: -9.6012e+01 - logprior: -1.3661e+00
Epoch 4/10
19/19 - 1s - loss: 97.4769 - loglik: -9.5967e+01 - logprior: -1.2982e+00
Epoch 5/10
19/19 - 1s - loss: 97.2784 - loglik: -9.5794e+01 - logprior: -1.2666e+00
Epoch 6/10
19/19 - 1s - loss: 97.3212 - loglik: -9.5857e+01 - logprior: -1.2415e+00
Fitted a model with MAP estimate = -96.9634
Time for alignment: 50.7197
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 154.1425 - loglik: -1.5088e+02 - logprior: -3.2516e+00
Epoch 2/10
19/19 - 1s - loss: 123.8350 - loglik: -1.2230e+02 - logprior: -1.5084e+00
Epoch 3/10
19/19 - 1s - loss: 111.2379 - loglik: -1.0930e+02 - logprior: -1.6806e+00
Epoch 4/10
19/19 - 1s - loss: 107.6894 - loglik: -1.0569e+02 - logprior: -1.6740e+00
Epoch 5/10
19/19 - 1s - loss: 106.5274 - loglik: -1.0468e+02 - logprior: -1.6249e+00
Epoch 6/10
19/19 - 1s - loss: 105.8515 - loglik: -1.0403e+02 - logprior: -1.6172e+00
Epoch 7/10
19/19 - 1s - loss: 105.9838 - loglik: -1.0421e+02 - logprior: -1.5823e+00
Fitted a model with MAP estimate = -105.4555
expansions: [(3, 1), (5, 1), (7, 1), (10, 1), (16, 1), (17, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 104.8452 - loglik: -1.0149e+02 - logprior: -3.1708e+00
Epoch 2/2
19/19 - 1s - loss: 97.8870 - loglik: -9.6261e+01 - logprior: -1.4357e+00
Fitted a model with MAP estimate = -97.0262
expansions: []
discards: [ 0 22 37 40]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 103.0617 - loglik: -9.8731e+01 - logprior: -4.1410e+00
Epoch 2/2
19/19 - 1s - loss: 99.6618 - loglik: -9.7201e+01 - logprior: -2.2744e+00
Fitted a model with MAP estimate = -98.4940
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 102.0724 - loglik: -9.7722e+01 - logprior: -4.1502e+00
Epoch 2/10
19/19 - 1s - loss: 98.4074 - loglik: -9.6333e+01 - logprior: -1.8821e+00
Epoch 3/10
19/19 - 1s - loss: 97.6208 - loglik: -9.6041e+01 - logprior: -1.3699e+00
Epoch 4/10
19/19 - 1s - loss: 97.3620 - loglik: -9.5843e+01 - logprior: -1.3047e+00
Epoch 5/10
19/19 - 1s - loss: 97.3295 - loglik: -9.5833e+01 - logprior: -1.2741e+00
Epoch 6/10
19/19 - 1s - loss: 97.2271 - loglik: -9.5758e+01 - logprior: -1.2454e+00
Epoch 7/10
19/19 - 1s - loss: 97.3988 - loglik: -9.5945e+01 - logprior: -1.2258e+00
Fitted a model with MAP estimate = -96.9233
Time for alignment: 49.4615
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.0624 - loglik: -1.5080e+02 - logprior: -3.2567e+00
Epoch 2/10
19/19 - 1s - loss: 123.9900 - loglik: -1.2241e+02 - logprior: -1.5424e+00
Epoch 3/10
19/19 - 1s - loss: 110.6222 - loglik: -1.0862e+02 - logprior: -1.6679e+00
Epoch 4/10
19/19 - 1s - loss: 107.1362 - loglik: -1.0511e+02 - logprior: -1.7487e+00
Epoch 5/10
19/19 - 1s - loss: 105.7719 - loglik: -1.0385e+02 - logprior: -1.7180e+00
Epoch 6/10
19/19 - 1s - loss: 105.5989 - loglik: -1.0376e+02 - logprior: -1.6776e+00
Epoch 7/10
19/19 - 1s - loss: 105.2174 - loglik: -1.0341e+02 - logprior: -1.6426e+00
Epoch 8/10
19/19 - 1s - loss: 105.2435 - loglik: -1.0345e+02 - logprior: -1.6278e+00
Fitted a model with MAP estimate = -104.8197
expansions: [(5, 1), (7, 1), (10, 1), (14, 1), (16, 1), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 104.7416 - loglik: -1.0121e+02 - logprior: -3.3802e+00
Epoch 2/2
19/19 - 1s - loss: 98.1286 - loglik: -9.6403e+01 - logprior: -1.5582e+00
Fitted a model with MAP estimate = -97.4435
expansions: []
discards: [35 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 99.8990 - loglik: -9.6424e+01 - logprior: -3.2966e+00
Epoch 2/2
19/19 - 1s - loss: 97.6570 - loglik: -9.6012e+01 - logprior: -1.4554e+00
Fitted a model with MAP estimate = -97.0162
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.5634 - loglik: -9.6109e+01 - logprior: -3.2677e+00
Epoch 2/10
19/19 - 1s - loss: 97.3244 - loglik: -9.5688e+01 - logprior: -1.4401e+00
Epoch 3/10
19/19 - 1s - loss: 97.1257 - loglik: -9.5592e+01 - logprior: -1.3291e+00
Epoch 4/10
19/19 - 2s - loss: 97.0263 - loglik: -9.5544e+01 - logprior: -1.2774e+00
Epoch 5/10
19/19 - 1s - loss: 96.8259 - loglik: -9.5382e+01 - logprior: -1.2356e+00
Epoch 6/10
19/19 - 1s - loss: 96.9367 - loglik: -9.5500e+01 - logprior: -1.2204e+00
Fitted a model with MAP estimate = -96.6027
Time for alignment: 48.3778
Computed alignments with likelihoods: ['-96.9403', '-96.9233', '-96.6027']
Best model has likelihood: -96.6027
time for generating output: 0.1077
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00046.projection.fasta
SP score = 0.9537037037037037
Training of 3 independent models on file PF01814.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f320452aee0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f32193e6ee0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3becbb9fd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f38ac145b20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f38ac145910>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b94bf5970>, <__main__.SimpleDirichletPrior object at 0x7f37fc558ca0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3bbf572af0>

Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 365.3443 - loglik: -3.6217e+02 - logprior: -3.0589e+00
Epoch 2/10
19/19 - 4s - loss: 336.2259 - loglik: -3.3425e+02 - logprior: -1.1054e+00
Epoch 3/10
19/19 - 4s - loss: 321.2468 - loglik: -3.1832e+02 - logprior: -1.4318e+00
Epoch 4/10
19/19 - 4s - loss: 313.8406 - loglik: -3.1070e+02 - logprior: -1.4511e+00
Epoch 5/10
19/19 - 4s - loss: 311.2662 - loglik: -3.0807e+02 - logprior: -1.5021e+00
Epoch 6/10
19/19 - 4s - loss: 309.8794 - loglik: -3.0695e+02 - logprior: -1.5104e+00
Epoch 7/10
19/19 - 4s - loss: 308.8668 - loglik: -3.0617e+02 - logprior: -1.5099e+00
Epoch 8/10
19/19 - 4s - loss: 308.1375 - loglik: -3.0561e+02 - logprior: -1.5090e+00
Epoch 9/10
19/19 - 4s - loss: 307.3309 - loglik: -3.0491e+02 - logprior: -1.5112e+00
Epoch 10/10
19/19 - 4s - loss: 308.0339 - loglik: -3.0573e+02 - logprior: -1.5106e+00
Fitted a model with MAP estimate = -306.4194
expansions: [(15, 1), (17, 1), (19, 1), (21, 4), (24, 3), (27, 1), (28, 1), (29, 1), (32, 1), (51, 1), (53, 3), (73, 1), (74, 1), (76, 3), (83, 1), (84, 3), (86, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 305.3822 - loglik: -3.0144e+02 - logprior: -3.1366e+00
Epoch 2/2
19/19 - 5s - loss: 298.5898 - loglik: -2.9638e+02 - logprior: -1.3554e+00
Fitted a model with MAP estimate = -296.1436
expansions: [(32, 1)]
discards: [  2 109]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 299.5928 - loglik: -2.9561e+02 - logprior: -2.9832e+00
Epoch 2/2
19/19 - 5s - loss: 296.9390 - loglik: -2.9464e+02 - logprior: -1.1963e+00
Fitted a model with MAP estimate = -294.7629
expansions: [(22, 1), (102, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 134 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 297.5859 - loglik: -2.9340e+02 - logprior: -3.0574e+00
Epoch 2/10
19/19 - 5s - loss: 295.3250 - loglik: -2.9295e+02 - logprior: -1.2581e+00
Epoch 3/10
19/19 - 5s - loss: 294.2561 - loglik: -2.9196e+02 - logprior: -1.1676e+00
Epoch 4/10
19/19 - 5s - loss: 293.6954 - loglik: -2.9152e+02 - logprior: -1.1107e+00
Epoch 5/10
19/19 - 5s - loss: 293.4447 - loglik: -2.9134e+02 - logprior: -1.0613e+00
Epoch 6/10
19/19 - 5s - loss: 293.0333 - loglik: -2.9092e+02 - logprior: -1.0173e+00
Epoch 7/10
19/19 - 5s - loss: 293.0876 - loglik: -2.9091e+02 - logprior: -9.7136e-01
Fitted a model with MAP estimate = -291.1330
Time for alignment: 135.4335
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 365.5312 - loglik: -3.6236e+02 - logprior: -3.0584e+00
Epoch 2/10
19/19 - 4s - loss: 335.4165 - loglik: -3.3345e+02 - logprior: -1.0991e+00
Epoch 3/10
19/19 - 4s - loss: 321.2603 - loglik: -3.1810e+02 - logprior: -1.4014e+00
Epoch 4/10
19/19 - 4s - loss: 315.0868 - loglik: -3.1153e+02 - logprior: -1.3568e+00
Epoch 5/10
19/19 - 4s - loss: 312.4498 - loglik: -3.0913e+02 - logprior: -1.3739e+00
Epoch 6/10
19/19 - 4s - loss: 309.5810 - loglik: -3.0673e+02 - logprior: -1.4214e+00
Epoch 7/10
19/19 - 4s - loss: 308.4383 - loglik: -3.0591e+02 - logprior: -1.4267e+00
Epoch 8/10
19/19 - 4s - loss: 308.1141 - loglik: -3.0569e+02 - logprior: -1.4361e+00
Epoch 9/10
19/19 - 4s - loss: 307.5021 - loglik: -3.0516e+02 - logprior: -1.4356e+00
Epoch 10/10
19/19 - 4s - loss: 306.8092 - loglik: -3.0452e+02 - logprior: -1.4342e+00
Fitted a model with MAP estimate = -306.0513
expansions: [(18, 1), (20, 2), (22, 6), (23, 2), (27, 1), (29, 1), (32, 1), (51, 1), (53, 3), (54, 2), (74, 1), (82, 1), (83, 1), (84, 3), (86, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 129 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 308.0928 - loglik: -3.0317e+02 - logprior: -4.0827e+00
Epoch 2/2
19/19 - 5s - loss: 300.4467 - loglik: -2.9739e+02 - logprior: -2.1898e+00
Fitted a model with MAP estimate = -297.8259
expansions: [(0, 1), (32, 1)]
discards: [  0  68 107]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 299.7998 - loglik: -2.9594e+02 - logprior: -2.9188e+00
Epoch 2/2
19/19 - 5s - loss: 296.9936 - loglik: -2.9473e+02 - logprior: -1.2542e+00
Fitted a model with MAP estimate = -295.2040
expansions: [(31, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 129 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 298.1665 - loglik: -2.9413e+02 - logprior: -3.0424e+00
Epoch 2/10
19/19 - 5s - loss: 296.3527 - loglik: -2.9415e+02 - logprior: -1.2370e+00
Epoch 3/10
19/19 - 5s - loss: 295.6320 - loglik: -2.9348e+02 - logprior: -1.1415e+00
Epoch 4/10
19/19 - 5s - loss: 295.0955 - loglik: -2.9302e+02 - logprior: -1.0911e+00
Epoch 5/10
19/19 - 5s - loss: 294.7570 - loglik: -2.9272e+02 - logprior: -1.0433e+00
Epoch 6/10
19/19 - 5s - loss: 294.5551 - loglik: -2.9248e+02 - logprior: -9.9850e-01
Epoch 7/10
19/19 - 5s - loss: 293.8439 - loglik: -2.9166e+02 - logprior: -9.5979e-01
Epoch 8/10
19/19 - 5s - loss: 293.6355 - loglik: -2.9129e+02 - logprior: -9.3170e-01
Epoch 9/10
19/19 - 5s - loss: 292.7057 - loglik: -2.9035e+02 - logprior: -8.9776e-01
Epoch 10/10
19/19 - 5s - loss: 292.6595 - loglik: -2.9044e+02 - logprior: -8.6207e-01
Fitted a model with MAP estimate = -290.7151
Time for alignment: 143.9758
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 365.3731 - loglik: -3.6220e+02 - logprior: -3.0598e+00
Epoch 2/10
19/19 - 4s - loss: 335.6790 - loglik: -3.3370e+02 - logprior: -1.1126e+00
Epoch 3/10
19/19 - 4s - loss: 322.3235 - loglik: -3.1921e+02 - logprior: -1.3957e+00
Epoch 4/10
19/19 - 4s - loss: 315.5604 - loglik: -3.1218e+02 - logprior: -1.3764e+00
Epoch 5/10
19/19 - 4s - loss: 312.0262 - loglik: -3.0883e+02 - logprior: -1.4135e+00
Epoch 6/10
19/19 - 4s - loss: 310.2788 - loglik: -3.0750e+02 - logprior: -1.4148e+00
Epoch 7/10
19/19 - 4s - loss: 308.5939 - loglik: -3.0601e+02 - logprior: -1.4178e+00
Epoch 8/10
19/19 - 4s - loss: 307.5945 - loglik: -3.0516e+02 - logprior: -1.4196e+00
Epoch 9/10
19/19 - 4s - loss: 307.6516 - loglik: -3.0531e+02 - logprior: -1.4319e+00
Fitted a model with MAP estimate = -306.1012
expansions: [(18, 1), (20, 2), (22, 4), (24, 5), (25, 1), (28, 1), (31, 1), (53, 3), (70, 1), (74, 1), (78, 1), (83, 1), (84, 3), (87, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 308.1397 - loglik: -3.0315e+02 - logprior: -4.0884e+00
Epoch 2/2
19/19 - 4s - loss: 300.5828 - loglik: -2.9751e+02 - logprior: -2.1949e+00
Fitted a model with MAP estimate = -297.9579
expansions: [(0, 1), (27, 2)]
discards: [  0 106]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 129 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 299.7769 - loglik: -2.9589e+02 - logprior: -2.9193e+00
Epoch 2/2
19/19 - 5s - loss: 297.2571 - loglik: -2.9493e+02 - logprior: -1.2822e+00
Fitted a model with MAP estimate = -295.0737
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 129 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 297.9040 - loglik: -2.9382e+02 - logprior: -3.0245e+00
Epoch 2/10
19/19 - 5s - loss: 296.4484 - loglik: -2.9419e+02 - logprior: -1.2168e+00
Epoch 3/10
19/19 - 5s - loss: 295.3883 - loglik: -2.9322e+02 - logprior: -1.1301e+00
Epoch 4/10
19/19 - 5s - loss: 295.1094 - loglik: -2.9304e+02 - logprior: -1.0796e+00
Epoch 5/10
19/19 - 5s - loss: 294.5392 - loglik: -2.9256e+02 - logprior: -1.0234e+00
Epoch 6/10
19/19 - 5s - loss: 294.6763 - loglik: -2.9276e+02 - logprior: -9.8325e-01
Fitted a model with MAP estimate = -293.3778
Time for alignment: 118.9751
Computed alignments with likelihoods: ['-291.1330', '-290.7151', '-293.3778']
Best model has likelihood: -290.7151
time for generating output: 0.1579
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01814.projection.fasta
SP score = 0.8491674828599413
Training of 3 independent models on file PF00155.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3221121ee0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f387c4990d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3bc79acd00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f387c4e2ac0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32114a5100>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b9d227220>, <__main__.SimpleDirichletPrior object at 0x7f32198d1bb0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3becf07d30>

Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 917.1494 - loglik: -9.1498e+02 - logprior: -1.6275e+00
Epoch 2/10
39/39 - 34s - loss: 819.8674 - loglik: -8.1603e+02 - logprior: -1.7508e+00
Epoch 3/10
39/39 - 34s - loss: 802.8340 - loglik: -7.9841e+02 - logprior: -2.1152e+00
Epoch 4/10
39/39 - 34s - loss: 798.0250 - loglik: -7.9358e+02 - logprior: -2.1043e+00
Epoch 5/10
39/39 - 34s - loss: 795.9232 - loglik: -7.9169e+02 - logprior: -2.0864e+00
Epoch 6/10
39/39 - 34s - loss: 794.3336 - loglik: -7.9027e+02 - logprior: -2.1172e+00
Epoch 7/10
39/39 - 34s - loss: 793.4243 - loglik: -7.8948e+02 - logprior: -2.1662e+00
Epoch 8/10
39/39 - 33s - loss: 792.7930 - loglik: -7.8901e+02 - logprior: -2.1723e+00
Epoch 9/10
39/39 - 33s - loss: 791.7496 - loglik: -7.8810e+02 - logprior: -2.1902e+00
Epoch 10/10
39/39 - 33s - loss: 791.3615 - loglik: -7.8787e+02 - logprior: -2.1738e+00
Fitted a model with MAP estimate = -781.8632
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (23, 3), (24, 1), (25, 1), (30, 1), (40, 1), (42, 1), (43, 2), (44, 2), (56, 1), (59, 1), (60, 1), (61, 1), (78, 1), (79, 1), (80, 1), (81, 1), (86, 1), (87, 1), (94, 2), (103, 1), (119, 1), (121, 1), (123, 1), (124, 1), (130, 1), (133, 1), (144, 1), (147, 1), (148, 1), (152, 2), (153, 1), (154, 3), (156, 2), (184, 1), (186, 1), (187, 1), (188, 1), (189, 1), (197, 1), (205, 3), (207, 1), (208, 1), (209, 1), (210, 1), (219, 1), (222, 1), (226, 2), (227, 1), (237, 1), (240, 3), (242, 1), (251, 1), (258, 1), (263, 1), (271, 3), (272, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 358 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 54s - loss: 775.2832 - loglik: -7.7116e+02 - logprior: -2.8330e+00
Epoch 2/2
39/39 - 51s - loss: 762.1472 - loglik: -7.5926e+02 - logprior: -1.3841e+00
Fitted a model with MAP estimate = -750.4632
expansions: [(334, 1)]
discards: [  1  30 192 196 201 288 306 321 345]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 350 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 761.2365 - loglik: -7.5763e+02 - logprior: -1.9806e+00
Epoch 2/2
39/39 - 49s - loss: 760.6288 - loglik: -7.5791e+02 - logprior: -1.0433e+00
Fitted a model with MAP estimate = -749.6771
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 350 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 752.4520 - loglik: -7.4913e+02 - logprior: -1.7483e+00
Epoch 2/10
39/39 - 48s - loss: 752.7009 - loglik: -7.5047e+02 - logprior: -6.4001e-01
Fitted a model with MAP estimate = -748.6059
Time for alignment: 877.9215
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 915.4016 - loglik: -9.1322e+02 - logprior: -1.6441e+00
Epoch 2/10
39/39 - 34s - loss: 813.7962 - loglik: -8.0961e+02 - logprior: -1.9120e+00
Epoch 3/10
39/39 - 34s - loss: 800.8404 - loglik: -7.9603e+02 - logprior: -1.9616e+00
Epoch 4/10
39/39 - 34s - loss: 797.1417 - loglik: -7.9271e+02 - logprior: -2.0037e+00
Epoch 5/10
39/39 - 34s - loss: 794.5125 - loglik: -7.9032e+02 - logprior: -2.0476e+00
Epoch 6/10
39/39 - 34s - loss: 793.2477 - loglik: -7.8923e+02 - logprior: -2.1023e+00
Epoch 7/10
39/39 - 34s - loss: 792.0981 - loglik: -7.8813e+02 - logprior: -2.2377e+00
Epoch 8/10
39/39 - 34s - loss: 791.5882 - loglik: -7.8789e+02 - logprior: -2.1915e+00
Epoch 9/10
39/39 - 35s - loss: 790.6149 - loglik: -7.8710e+02 - logprior: -2.1723e+00
Epoch 10/10
39/39 - 35s - loss: 790.8704 - loglik: -7.8747e+02 - logprior: -2.1905e+00
Fitted a model with MAP estimate = -781.1243
expansions: [(0, 2), (15, 2), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (30, 1), (33, 1), (39, 1), (41, 1), (42, 2), (44, 2), (55, 1), (58, 1), (61, 1), (64, 1), (79, 1), (80, 1), (81, 1), (85, 1), (87, 1), (94, 3), (102, 1), (119, 1), (120, 1), (123, 1), (124, 1), (130, 1), (142, 1), (144, 1), (149, 1), (154, 1), (155, 2), (169, 1), (181, 1), (184, 1), (185, 1), (187, 1), (188, 1), (189, 1), (206, 4), (207, 1), (208, 1), (209, 1), (211, 1), (219, 1), (220, 1), (226, 1), (228, 1), (240, 4), (246, 1), (251, 1), (258, 1), (261, 2), (270, 2), (271, 1), (272, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 353 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 774.7993 - loglik: -7.7077e+02 - logprior: -2.8355e+00
Epoch 2/2
39/39 - 49s - loss: 762.1267 - loglik: -7.5937e+02 - logprior: -1.3463e+00
Fitted a model with MAP estimate = -750.7387
expansions: [(331, 1)]
discards: [  0 301 340]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 351 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 50s - loss: 761.9039 - loglik: -7.5723e+02 - logprior: -3.1202e+00
Epoch 2/2
39/39 - 48s - loss: 760.0788 - loglik: -7.5730e+02 - logprior: -1.1212e+00
Fitted a model with MAP estimate = -749.1182
expansions: [(0, 2), (115, 1)]
discards: [314]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 353 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 59s - loss: 752.5820 - loglik: -7.4881e+02 - logprior: -2.1966e+00
Epoch 2/10
39/39 - 58s - loss: 751.7385 - loglik: -7.4939e+02 - logprior: -7.1127e-01
Epoch 3/10
39/39 - 60s - loss: 750.1621 - loglik: -7.4806e+02 - logprior: -5.0840e-01
Epoch 4/10
39/39 - 57s - loss: 749.0458 - loglik: -7.4735e+02 - logprior: -2.8698e-01
Epoch 5/10
39/39 - 54s - loss: 748.4477 - loglik: -7.4696e+02 - logprior: -1.1970e-01
Epoch 6/10
39/39 - 57s - loss: 746.7235 - loglik: -7.4545e+02 - logprior: 0.0457
Epoch 7/10
39/39 - 57s - loss: 748.3954 - loglik: -7.4731e+02 - logprior: 0.2433
Fitted a model with MAP estimate = -745.1759
Time for alignment: 1166.9962
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 39s - loss: 916.1313 - loglik: -9.1397e+02 - logprior: -1.6268e+00
Epoch 2/10
39/39 - 36s - loss: 819.5740 - loglik: -8.1588e+02 - logprior: -1.6743e+00
Epoch 3/10
39/39 - 38s - loss: 803.9658 - loglik: -8.0002e+02 - logprior: -1.9458e+00
Epoch 4/10
39/39 - 39s - loss: 800.0205 - loglik: -7.9598e+02 - logprior: -1.9538e+00
Epoch 5/10
39/39 - 38s - loss: 797.5521 - loglik: -7.9368e+02 - logprior: -1.9629e+00
Epoch 6/10
39/39 - 36s - loss: 796.1929 - loglik: -7.9243e+02 - logprior: -2.0072e+00
Epoch 7/10
39/39 - 36s - loss: 795.1154 - loglik: -7.9140e+02 - logprior: -2.0232e+00
Epoch 8/10
39/39 - 37s - loss: 794.2363 - loglik: -7.9064e+02 - logprior: -2.0427e+00
Epoch 9/10
39/39 - 39s - loss: 793.6086 - loglik: -7.9009e+02 - logprior: -2.0978e+00
Epoch 10/10
39/39 - 42s - loss: 793.3051 - loglik: -7.8992e+02 - logprior: -2.0887e+00
Fitted a model with MAP estimate = -783.6038
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2), (26, 1), (30, 1), (40, 1), (42, 1), (43, 2), (45, 2), (56, 1), (59, 1), (62, 1), (65, 1), (78, 1), (80, 1), (82, 1), (86, 1), (88, 1), (95, 2), (101, 1), (103, 1), (121, 1), (124, 2), (132, 2), (145, 1), (148, 1), (149, 1), (153, 2), (154, 1), (155, 2), (169, 1), (181, 1), (184, 1), (186, 1), (187, 1), (188, 1), (201, 1), (206, 4), (207, 1), (208, 1), (209, 1), (218, 1), (220, 1), (222, 1), (226, 1), (228, 1), (240, 2), (242, 1), (245, 1), (250, 1), (261, 1), (262, 1), (263, 1), (270, 3), (271, 1), (272, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 66s - loss: 775.8309 - loglik: -7.7180e+02 - logprior: -2.7570e+00
Epoch 2/2
39/39 - 64s - loss: 762.9768 - loglik: -7.6017e+02 - logprior: -1.3074e+00
Fitted a model with MAP estimate = -751.3480
expansions: [(158, 1), (304, 1)]
discards: [  2  30 165 186 192 340]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 351 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 60s - loss: 761.5494 - loglik: -7.5780e+02 - logprior: -2.0499e+00
Epoch 2/2
39/39 - 58s - loss: 760.7113 - loglik: -7.5800e+02 - logprior: -1.0173e+00
Fitted a model with MAP estimate = -749.5237
expansions: []
discards: [313]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 350 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 60s - loss: 752.9695 - loglik: -7.4955e+02 - logprior: -1.7956e+00
Epoch 2/10
39/39 - 54s - loss: 753.0332 - loglik: -7.5072e+02 - logprior: -6.8468e-01
Fitted a model with MAP estimate = -748.8954
Time for alignment: 1033.5482
Computed alignments with likelihoods: ['-748.6059', '-745.1759', '-748.8954']
Best model has likelihood: -745.1759
time for generating output: 1.1093
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00155.projection.fasta
SP score = 0.44903074687176797
Training of 3 independent models on file PF00450.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f37fcb90f40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3220289130>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f38040a7dc0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3824523f70>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f38044d1d90>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f38047c0100>, <__main__.SimpleDirichletPrior object at 0x7f3220882a90>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3219ba2040>

Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 62s - loss: 893.2341 - loglik: -8.9128e+02 - logprior: -1.7187e+00
Epoch 2/10
39/39 - 57s - loss: 753.9930 - loglik: -7.5117e+02 - logprior: -1.9813e+00
Epoch 3/10
39/39 - 55s - loss: 742.1305 - loglik: -7.3884e+02 - logprior: -2.0983e+00
Epoch 4/10
39/39 - 58s - loss: 738.5530 - loglik: -7.3524e+02 - logprior: -2.1355e+00
Epoch 5/10
39/39 - 60s - loss: 736.0868 - loglik: -7.3283e+02 - logprior: -2.1638e+00
Epoch 6/10
39/39 - 55s - loss: 733.6829 - loglik: -7.3036e+02 - logprior: -2.2039e+00
Epoch 7/10
39/39 - 56s - loss: 733.2775 - loglik: -7.2997e+02 - logprior: -2.2092e+00
Epoch 8/10
39/39 - 61s - loss: 733.9668 - loglik: -7.3066e+02 - logprior: -2.2310e+00
Fitted a model with MAP estimate = -730.6846
expansions: [(9, 1), (19, 2), (20, 1), (63, 1), (66, 1), (77, 1), (100, 1), (104, 1), (106, 1), (107, 1), (108, 1), (115, 1), (120, 1), (139, 1), (141, 1), (142, 1), (144, 2), (145, 3), (160, 1), (162, 2), (163, 1), (168, 1), (175, 1), (177, 1), (178, 5), (183, 4), (184, 1), (188, 1), (190, 2), (191, 2), (192, 1), (201, 1), (204, 2), (211, 2), (219, 2), (220, 1), (221, 1), (226, 1), (234, 1), (235, 3), (243, 2), (247, 1), (248, 1), (253, 1), (254, 1), (257, 1), (267, 1), (278, 1), (279, 2), (282, 1), (283, 4), (284, 1), (289, 1), (297, 1), (307, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 408 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 101s - loss: 716.3271 - loglik: -7.1208e+02 - logprior: -3.1292e+00
Epoch 2/2
39/39 - 99s - loss: 704.2975 - loglik: -7.0089e+02 - logprior: -1.9349e+00
Fitted a model with MAP estimate = -699.5676
expansions: [(4, 2), (227, 1)]
discards: [  0   1 159 206 207 237 238 239 240 241 248 267 289 290 352 353 354]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 394 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 88s - loss: 709.7828 - loglik: -7.0515e+02 - logprior: -2.8369e+00
Epoch 2/2
39/39 - 88s - loss: 705.3576 - loglik: -7.0228e+02 - logprior: -1.2518e+00
Fitted a model with MAP estimate = -700.0285
expansions: [(4, 1), (280, 2), (290, 1)]
discards: [215 235 236 237 238 239 240 254]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 390 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 85s - loss: 704.7153 - loglik: -7.0099e+02 - logprior: -1.8323e+00
Epoch 2/10
39/39 - 74s - loss: 702.0186 - loglik: -6.9964e+02 - logprior: -5.7325e-01
Epoch 3/10
39/39 - 77s - loss: 701.1489 - loglik: -6.9888e+02 - logprior: -3.6033e-01
Epoch 4/10
39/39 - 80s - loss: 700.0576 - loglik: -6.9813e+02 - logprior: -2.2246e-01
Epoch 5/10
39/39 - 74s - loss: 698.4590 - loglik: -6.9677e+02 - logprior: -2.6577e-02
Epoch 6/10
39/39 - 82s - loss: 698.6442 - loglik: -6.9708e+02 - logprior: 0.0848
Fitted a model with MAP estimate = -695.9423
Time for alignment: 1695.1675
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 67s - loss: 895.3720 - loglik: -8.9339e+02 - logprior: -1.7252e+00
Epoch 2/10
39/39 - 65s - loss: 754.2184 - loglik: -7.5114e+02 - logprior: -2.1388e+00
Epoch 3/10
39/39 - 64s - loss: 740.3500 - loglik: -7.3668e+02 - logprior: -2.3573e+00
Epoch 4/10
39/39 - 55s - loss: 737.4253 - loglik: -7.3382e+02 - logprior: -2.3683e+00
Epoch 5/10
39/39 - 52s - loss: 735.1743 - loglik: -7.3159e+02 - logprior: -2.3775e+00
Epoch 6/10
39/39 - 50s - loss: 734.2655 - loglik: -7.3069e+02 - logprior: -2.3894e+00
Epoch 7/10
39/39 - 51s - loss: 733.7626 - loglik: -7.3015e+02 - logprior: -2.4431e+00
Epoch 8/10
39/39 - 53s - loss: 732.6768 - loglik: -7.2914e+02 - logprior: -2.4342e+00
Epoch 9/10
39/39 - 53s - loss: 732.3683 - loglik: -7.2882e+02 - logprior: -2.4723e+00
Epoch 10/10
39/39 - 54s - loss: 731.8125 - loglik: -7.2830e+02 - logprior: -2.4456e+00
Fitted a model with MAP estimate = -729.7748
expansions: [(9, 1), (19, 1), (22, 1), (31, 1), (61, 1), (66, 1), (68, 1), (76, 1), (99, 1), (102, 2), (103, 1), (106, 1), (112, 1), (117, 1), (118, 1), (135, 1), (136, 1), (137, 1), (138, 1), (140, 3), (141, 1), (142, 1), (156, 1), (157, 1), (159, 1), (160, 1), (161, 1), (165, 1), (174, 1), (175, 1), (176, 3), (179, 1), (180, 5), (181, 1), (183, 1), (184, 1), (187, 1), (188, 1), (189, 1), (190, 1), (203, 2), (217, 1), (218, 1), (219, 1), (220, 1), (225, 1), (227, 1), (232, 1), (236, 1), (239, 1), (241, 1), (243, 2), (245, 1), (246, 1), (252, 1), (255, 1), (268, 1), (269, 1), (277, 1), (278, 1), (281, 1), (282, 4), (288, 1), (297, 1), (314, 1), (317, 2), (319, 1), (321, 1)]
discards: [  0   1 192 193 194 195 196]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 402 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 76s - loss: 718.3928 - loglik: -7.1416e+02 - logprior: -3.1518e+00
Epoch 2/2
39/39 - 70s - loss: 705.7766 - loglik: -7.0242e+02 - logprior: -1.9697e+00
Fitted a model with MAP estimate = -700.5047
expansions: [(4, 2), (110, 1), (283, 1)]
discards: [  0   1 161 207 208 217 239 240 241 242 245 256 298 347 348]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 391 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 71s - loss: 708.1946 - loglik: -7.0364e+02 - logprior: -2.8271e+00
Epoch 2/2
39/39 - 69s - loss: 704.2525 - loglik: -7.0124e+02 - logprior: -1.2500e+00
Fitted a model with MAP estimate = -699.3399
expansions: [(4, 1), (248, 13)]
discards: [236 237]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 403 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 78s - loss: 701.7955 - loglik: -6.9809e+02 - logprior: -1.8184e+00
Epoch 2/10
39/39 - 75s - loss: 698.4404 - loglik: -6.9590e+02 - logprior: -5.9432e-01
Epoch 3/10
39/39 - 79s - loss: 696.5657 - loglik: -6.9413e+02 - logprior: -3.8119e-01
Epoch 4/10
39/39 - 83s - loss: 695.3752 - loglik: -6.9315e+02 - logprior: -3.2167e-01
Epoch 5/10
39/39 - 81s - loss: 695.3156 - loglik: -6.9327e+02 - logprior: -9.9130e-02
Epoch 6/10
39/39 - 81s - loss: 693.0482 - loglik: -6.9096e+02 - logprior: 0.0582
Epoch 7/10
39/39 - 81s - loss: 691.7717 - loglik: -6.8944e+02 - logprior: 0.1581
Epoch 8/10
39/39 - 82s - loss: 690.9486 - loglik: -6.8876e+02 - logprior: 0.2411
Epoch 9/10
39/39 - 82s - loss: 690.0355 - loglik: -6.8835e+02 - logprior: 0.5326
Epoch 10/10
39/39 - 83s - loss: 687.7178 - loglik: -6.8633e+02 - logprior: 0.7018
Fitted a model with MAP estimate = -686.0042
Time for alignment: 1938.7595
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 59s - loss: 894.7914 - loglik: -8.9284e+02 - logprior: -1.6880e+00
Epoch 2/10
39/39 - 56s - loss: 751.6432 - loglik: -7.4881e+02 - logprior: -1.9342e+00
Epoch 3/10
39/39 - 56s - loss: 739.7252 - loglik: -7.3639e+02 - logprior: -2.0635e+00
Epoch 4/10
39/39 - 56s - loss: 736.4962 - loglik: -7.3321e+02 - logprior: -2.0935e+00
Epoch 5/10
39/39 - 58s - loss: 734.3705 - loglik: -7.3108e+02 - logprior: -2.1463e+00
Epoch 6/10
39/39 - 58s - loss: 733.8124 - loglik: -7.3051e+02 - logprior: -2.2123e+00
Epoch 7/10
39/39 - 58s - loss: 733.2827 - loglik: -7.3004e+02 - logprior: -2.1941e+00
Epoch 8/10
39/39 - 59s - loss: 732.0753 - loglik: -7.2892e+02 - logprior: -2.1492e+00
Epoch 9/10
39/39 - 59s - loss: 732.1776 - loglik: -7.2896e+02 - logprior: -2.2212e+00
Fitted a model with MAP estimate = -729.3272
expansions: [(9, 1), (19, 1), (31, 1), (67, 1), (69, 1), (101, 1), (105, 1), (107, 1), (110, 1), (122, 1), (123, 1), (141, 2), (142, 1), (143, 1), (145, 2), (146, 3), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (169, 1), (178, 1), (179, 4), (182, 1), (183, 5), (184, 1), (186, 1), (188, 1), (189, 1), (190, 1), (191, 2), (192, 1), (193, 1), (202, 1), (204, 1), (211, 2), (219, 2), (220, 1), (221, 1), (226, 1), (228, 1), (233, 3), (240, 1), (242, 2), (243, 2), (245, 1), (246, 1), (248, 1), (249, 1), (251, 1), (254, 1), (274, 1), (275, 1), (276, 2), (281, 1), (287, 1), (288, 2), (296, 1), (313, 1), (317, 1), (319, 1), (321, 1)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 408 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 83s - loss: 715.1011 - loglik: -7.1164e+02 - logprior: -2.4203e+00
Epoch 2/2
39/39 - 81s - loss: 703.2907 - loglik: -7.0073e+02 - logprior: -1.1594e+00
Fitted a model with MAP estimate = -698.8089
expansions: [(5, 1), (23, 1)]
discards: [160 207 216 238 239 240 241 242 268 305 362 363]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 398 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 75s - loss: 706.2311 - loglik: -7.0242e+02 - logprior: -2.0904e+00
Epoch 2/2
39/39 - 71s - loss: 703.0765 - loglik: -7.0050e+02 - logprior: -7.9289e-01
Fitted a model with MAP estimate = -698.9180
expansions: [(354, 1)]
discards: [237 238 243 257]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 395 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 70s - loss: 702.6500 - loglik: -6.9888e+02 - logprior: -1.8003e+00
Epoch 2/10
39/39 - 67s - loss: 700.6974 - loglik: -6.9827e+02 - logprior: -5.6072e-01
Epoch 3/10
39/39 - 68s - loss: 698.9462 - loglik: -6.9658e+02 - logprior: -4.0759e-01
Epoch 4/10
39/39 - 70s - loss: 698.1815 - loglik: -6.9614e+02 - logprior: -2.9132e-01
Epoch 5/10
39/39 - 71s - loss: 697.2448 - loglik: -6.9544e+02 - logprior: -1.1781e-01
Epoch 6/10
39/39 - 71s - loss: 696.0874 - loglik: -6.9449e+02 - logprior: 0.0939
Epoch 7/10
39/39 - 71s - loss: 696.1884 - loglik: -6.9470e+02 - logprior: 0.2387
Fitted a model with MAP estimate = -693.2762
Time for alignment: 1637.2022
Computed alignments with likelihoods: ['-695.9423', '-686.0042', '-693.2762']
Best model has likelihood: -686.0042
time for generating output: 0.4993
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00450.projection.fasta
SP score = 0.8356183418249189
Training of 3 independent models on file PF07679.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f38445c52e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f37fdf50c10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f37fe495760>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7290e4f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7292a0a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3205b51550>, <__main__.SimpleDirichletPrior object at 0x7f3211bfbdf0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3220676310>

Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 252.5379 - loglik: -2.4934e+02 - logprior: -3.1144e+00
Epoch 2/10
19/19 - 2s - loss: 222.7126 - loglik: -2.2106e+02 - logprior: -1.3119e+00
Epoch 3/10
19/19 - 2s - loss: 207.3132 - loglik: -2.0510e+02 - logprior: -1.5844e+00
Epoch 4/10
19/19 - 2s - loss: 204.6630 - loglik: -2.0270e+02 - logprior: -1.4930e+00
Epoch 5/10
19/19 - 2s - loss: 203.9099 - loglik: -2.0202e+02 - logprior: -1.4766e+00
Epoch 6/10
19/19 - 2s - loss: 203.5557 - loglik: -2.0172e+02 - logprior: -1.4583e+00
Epoch 7/10
19/19 - 2s - loss: 202.9902 - loglik: -2.0116e+02 - logprior: -1.4633e+00
Epoch 8/10
19/19 - 2s - loss: 202.6704 - loglik: -2.0086e+02 - logprior: -1.4618e+00
Epoch 9/10
19/19 - 2s - loss: 203.0012 - loglik: -2.0117e+02 - logprior: -1.4592e+00
Fitted a model with MAP estimate = -201.9452
expansions: [(8, 1), (9, 3), (10, 1), (12, 2), (13, 1), (14, 1), (34, 1), (39, 1), (41, 2), (43, 2), (48, 1), (49, 2), (50, 1), (59, 1), (60, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 203.2686 - loglik: -1.9880e+02 - logprior: -4.0704e+00
Epoch 2/2
19/19 - 2s - loss: 196.2580 - loglik: -1.9352e+02 - logprior: -2.2364e+00
Fitted a model with MAP estimate = -194.1518
expansions: [(0, 2)]
discards: [ 0 10 51 64]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 195.4128 - loglik: -1.9170e+02 - logprior: -3.0446e+00
Epoch 2/2
19/19 - 2s - loss: 192.3607 - loglik: -1.9037e+02 - logprior: -1.2529e+00
Fitted a model with MAP estimate = -190.8817
expansions: [(54, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 195.9014 - loglik: -1.9122e+02 - logprior: -3.8695e+00
Epoch 2/10
19/19 - 2s - loss: 191.9477 - loglik: -1.8973e+02 - logprior: -1.4372e+00
Epoch 3/10
19/19 - 2s - loss: 191.1247 - loglik: -1.8914e+02 - logprior: -1.2203e+00
Epoch 4/10
19/19 - 2s - loss: 190.7136 - loglik: -1.8885e+02 - logprior: -1.1705e+00
Epoch 5/10
19/19 - 2s - loss: 190.7660 - loglik: -1.8899e+02 - logprior: -1.1371e+00
Fitted a model with MAP estimate = -189.8215
Time for alignment: 70.9976
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 253.1142 - loglik: -2.4991e+02 - logprior: -3.1161e+00
Epoch 2/10
19/19 - 2s - loss: 223.5618 - loglik: -2.2194e+02 - logprior: -1.3266e+00
Epoch 3/10
19/19 - 2s - loss: 208.9734 - loglik: -2.0692e+02 - logprior: -1.5908e+00
Epoch 4/10
19/19 - 2s - loss: 204.1366 - loglik: -2.0208e+02 - logprior: -1.5420e+00
Epoch 5/10
19/19 - 2s - loss: 202.4614 - loglik: -2.0048e+02 - logprior: -1.5675e+00
Epoch 6/10
19/19 - 2s - loss: 201.9569 - loglik: -2.0004e+02 - logprior: -1.5387e+00
Epoch 7/10
19/19 - 2s - loss: 201.7338 - loglik: -1.9983e+02 - logprior: -1.5242e+00
Epoch 8/10
19/19 - 2s - loss: 201.4610 - loglik: -1.9954e+02 - logprior: -1.5125e+00
Epoch 9/10
19/19 - 2s - loss: 201.0497 - loglik: -1.9914e+02 - logprior: -1.5050e+00
Epoch 10/10
19/19 - 2s - loss: 200.9790 - loglik: -1.9905e+02 - logprior: -1.5066e+00
Fitted a model with MAP estimate = -200.2081
expansions: [(9, 1), (10, 1), (11, 1), (13, 1), (15, 1), (18, 1), (19, 1), (24, 1), (34, 1), (39, 1), (40, 2), (43, 2), (48, 1), (49, 2), (58, 1), (59, 1), (60, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 202.9203 - loglik: -1.9836e+02 - logprior: -4.0725e+00
Epoch 2/2
19/19 - 2s - loss: 195.9552 - loglik: -1.9313e+02 - logprior: -2.2396e+00
Fitted a model with MAP estimate = -193.9096
expansions: [(0, 2)]
discards: [ 0 49 63]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 195.0094 - loglik: -1.9124e+02 - logprior: -3.0497e+00
Epoch 2/2
19/19 - 2s - loss: 192.1825 - loglik: -1.9016e+02 - logprior: -1.2581e+00
Fitted a model with MAP estimate = -190.6421
expansions: [(54, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 195.7735 - loglik: -1.9112e+02 - logprior: -3.8806e+00
Epoch 2/10
19/19 - 2s - loss: 191.9888 - loglik: -1.8983e+02 - logprior: -1.4598e+00
Epoch 3/10
19/19 - 2s - loss: 190.9794 - loglik: -1.8905e+02 - logprior: -1.2246e+00
Epoch 4/10
19/19 - 2s - loss: 190.6855 - loglik: -1.8886e+02 - logprior: -1.1782e+00
Epoch 5/10
19/19 - 2s - loss: 190.4826 - loglik: -1.8873e+02 - logprior: -1.1405e+00
Epoch 6/10
19/19 - 2s - loss: 190.4808 - loglik: -1.8879e+02 - logprior: -1.1101e+00
Epoch 7/10
19/19 - 2s - loss: 190.2891 - loglik: -1.8865e+02 - logprior: -1.0855e+00
Epoch 8/10
19/19 - 2s - loss: 190.1708 - loglik: -1.8858e+02 - logprior: -1.0523e+00
Epoch 9/10
19/19 - 2s - loss: 190.2816 - loglik: -1.8872e+02 - logprior: -1.0333e+00
Fitted a model with MAP estimate = -189.5332
Time for alignment: 80.8889
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 252.8597 - loglik: -2.4965e+02 - logprior: -3.1182e+00
Epoch 2/10
19/19 - 2s - loss: 221.3716 - loglik: -2.1973e+02 - logprior: -1.3400e+00
Epoch 3/10
19/19 - 2s - loss: 207.6303 - loglik: -2.0556e+02 - logprior: -1.5846e+00
Epoch 4/10
19/19 - 2s - loss: 204.7296 - loglik: -2.0275e+02 - logprior: -1.4842e+00
Epoch 5/10
19/19 - 2s - loss: 204.1531 - loglik: -2.0221e+02 - logprior: -1.5013e+00
Epoch 6/10
19/19 - 2s - loss: 203.4844 - loglik: -2.0158e+02 - logprior: -1.4741e+00
Epoch 7/10
19/19 - 2s - loss: 203.0555 - loglik: -2.0117e+02 - logprior: -1.4602e+00
Epoch 8/10
19/19 - 2s - loss: 203.1803 - loglik: -2.0130e+02 - logprior: -1.4526e+00
Fitted a model with MAP estimate = -202.2246
expansions: [(9, 1), (10, 1), (11, 1), (13, 1), (15, 1), (18, 1), (19, 1), (20, 1), (34, 1), (39, 1), (40, 2), (43, 2), (48, 1), (49, 2), (60, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 203.8853 - loglik: -1.9940e+02 - logprior: -4.0532e+00
Epoch 2/2
19/19 - 2s - loss: 196.4151 - loglik: -1.9368e+02 - logprior: -2.2100e+00
Fitted a model with MAP estimate = -194.3673
expansions: [(0, 2)]
discards: [ 0 50 63]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 195.5765 - loglik: -1.9188e+02 - logprior: -3.0409e+00
Epoch 2/2
19/19 - 2s - loss: 192.6663 - loglik: -1.9067e+02 - logprior: -1.2473e+00
Fitted a model with MAP estimate = -191.0364
expansions: [(54, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 196.2043 - loglik: -1.9149e+02 - logprior: -3.8687e+00
Epoch 2/10
19/19 - 2s - loss: 192.1118 - loglik: -1.8987e+02 - logprior: -1.4477e+00
Epoch 3/10
19/19 - 2s - loss: 191.2324 - loglik: -1.8925e+02 - logprior: -1.2171e+00
Epoch 4/10
19/19 - 2s - loss: 190.9523 - loglik: -1.8906e+02 - logprior: -1.1738e+00
Epoch 5/10
19/19 - 2s - loss: 190.5913 - loglik: -1.8880e+02 - logprior: -1.1365e+00
Epoch 6/10
19/19 - 2s - loss: 190.5090 - loglik: -1.8878e+02 - logprior: -1.1099e+00
Epoch 7/10
19/19 - 2s - loss: 190.3744 - loglik: -1.8871e+02 - logprior: -1.0791e+00
Epoch 8/10
19/19 - 2s - loss: 190.4221 - loglik: -1.8880e+02 - logprior: -1.0585e+00
Fitted a model with MAP estimate = -189.6325
Time for alignment: 72.7464
Computed alignments with likelihoods: ['-189.8215', '-189.5332', '-189.6325']
Best model has likelihood: -189.5332
time for generating output: 0.1478
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07679.projection.fasta
SP score = 0.7538759689922481
Training of 3 independent models on file PF07686.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f322171b310>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f37fd32f1c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3bc7d81ac0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f31fd686a90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f31fc3f4430>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3210364b20>, <__main__.SimpleDirichletPrior object at 0x7f37fe270dc0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3ba5ae5c10>

Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 301.5909 - loglik: -2.9842e+02 - logprior: -3.0893e+00
Epoch 2/10
19/19 - 2s - loss: 275.1908 - loglik: -2.7340e+02 - logprior: -1.3074e+00
Epoch 3/10
19/19 - 2s - loss: 259.1489 - loglik: -2.5708e+02 - logprior: -1.3829e+00
Epoch 4/10
19/19 - 2s - loss: 254.6190 - loglik: -2.5255e+02 - logprior: -1.2851e+00
Epoch 5/10
19/19 - 2s - loss: 253.3242 - loglik: -2.5131e+02 - logprior: -1.2512e+00
Epoch 6/10
19/19 - 2s - loss: 252.0643 - loglik: -2.5002e+02 - logprior: -1.2479e+00
Epoch 7/10
19/19 - 2s - loss: 251.5035 - loglik: -2.4947e+02 - logprior: -1.2473e+00
Epoch 8/10
19/19 - 2s - loss: 250.6341 - loglik: -2.4861e+02 - logprior: -1.2360e+00
Epoch 9/10
19/19 - 2s - loss: 250.1639 - loglik: -2.4818e+02 - logprior: -1.2520e+00
Epoch 10/10
19/19 - 2s - loss: 249.6909 - loglik: -2.4776e+02 - logprior: -1.2545e+00
Fitted a model with MAP estimate = -248.5891
expansions: [(0, 2), (17, 1), (20, 3), (21, 2), (22, 1), (23, 1), (38, 1), (41, 3), (42, 1), (44, 2), (51, 1), (52, 1), (53, 1), (75, 4), (76, 2), (77, 2), (78, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 113 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 250.9342 - loglik: -2.4597e+02 - logprior: -4.3126e+00
Epoch 2/2
19/19 - 3s - loss: 244.3330 - loglik: -2.4211e+02 - logprior: -1.4675e+00
Fitted a model with MAP estimate = -241.6676
expansions: []
discards: [ 1 59 98]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 245.0312 - loglik: -2.4101e+02 - logprior: -3.0485e+00
Epoch 2/2
19/19 - 3s - loss: 242.4487 - loglik: -2.4000e+02 - logprior: -1.3710e+00
Fitted a model with MAP estimate = -240.1495
expansions: []
discards: [26]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 243.2312 - loglik: -2.3893e+02 - logprior: -3.0166e+00
Epoch 2/10
19/19 - 3s - loss: 240.6310 - loglik: -2.3791e+02 - logprior: -1.3321e+00
Epoch 3/10
19/19 - 3s - loss: 239.7476 - loglik: -2.3701e+02 - logprior: -1.1782e+00
Epoch 4/10
19/19 - 3s - loss: 238.2027 - loglik: -2.3540e+02 - logprior: -1.1377e+00
Epoch 5/10
19/19 - 3s - loss: 237.3137 - loglik: -2.3449e+02 - logprior: -1.1197e+00
Epoch 6/10
19/19 - 3s - loss: 236.6317 - loglik: -2.3395e+02 - logprior: -1.0988e+00
Epoch 7/10
19/19 - 3s - loss: 235.9972 - loglik: -2.3349e+02 - logprior: -1.0709e+00
Epoch 8/10
19/19 - 3s - loss: 235.3789 - loglik: -2.3301e+02 - logprior: -1.0474e+00
Epoch 9/10
19/19 - 3s - loss: 235.2456 - loglik: -2.3299e+02 - logprior: -1.0161e+00
Epoch 10/10
19/19 - 3s - loss: 234.7554 - loglik: -2.3257e+02 - logprior: -9.9811e-01
Fitted a model with MAP estimate = -233.5625
Time for alignment: 99.0672
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 301.8687 - loglik: -2.9869e+02 - logprior: -3.0932e+00
Epoch 2/10
19/19 - 2s - loss: 271.4684 - loglik: -2.6972e+02 - logprior: -1.2990e+00
Epoch 3/10
19/19 - 2s - loss: 258.5802 - loglik: -2.5645e+02 - logprior: -1.3190e+00
Epoch 4/10
19/19 - 2s - loss: 255.2853 - loglik: -2.5333e+02 - logprior: -1.2105e+00
Epoch 5/10
19/19 - 2s - loss: 253.7183 - loglik: -2.5177e+02 - logprior: -1.2290e+00
Epoch 6/10
19/19 - 2s - loss: 252.7045 - loglik: -2.5071e+02 - logprior: -1.2213e+00
Epoch 7/10
19/19 - 2s - loss: 252.1027 - loglik: -2.5010e+02 - logprior: -1.2245e+00
Epoch 8/10
19/19 - 2s - loss: 251.0737 - loglik: -2.4910e+02 - logprior: -1.2261e+00
Epoch 9/10
19/19 - 2s - loss: 251.1034 - loglik: -2.4918e+02 - logprior: -1.2284e+00
Fitted a model with MAP estimate = -249.6072
expansions: [(0, 2), (17, 1), (20, 2), (21, 1), (22, 1), (23, 1), (28, 2), (38, 2), (41, 3), (42, 1), (52, 1), (53, 3), (75, 2), (76, 2), (77, 4), (78, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 113 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 251.2447 - loglik: -2.4629e+02 - logprior: -4.2647e+00
Epoch 2/2
19/19 - 3s - loss: 244.3768 - loglik: -2.4209e+02 - logprior: -1.4724e+00
Fitted a model with MAP estimate = -241.5802
expansions: []
discards: [  1  24 100]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 245.1494 - loglik: -2.4109e+02 - logprior: -3.0603e+00
Epoch 2/2
19/19 - 3s - loss: 242.6549 - loglik: -2.4022e+02 - logprior: -1.3741e+00
Fitted a model with MAP estimate = -240.4289
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 243.4219 - loglik: -2.3920e+02 - logprior: -3.0336e+00
Epoch 2/10
19/19 - 3s - loss: 241.0483 - loglik: -2.3849e+02 - logprior: -1.3427e+00
Epoch 3/10
19/19 - 3s - loss: 239.9434 - loglik: -2.3729e+02 - logprior: -1.1994e+00
Epoch 4/10
19/19 - 3s - loss: 238.8880 - loglik: -2.3610e+02 - logprior: -1.1692e+00
Epoch 5/10
19/19 - 3s - loss: 238.1909 - loglik: -2.3545e+02 - logprior: -1.1463e+00
Epoch 6/10
19/19 - 3s - loss: 237.2854 - loglik: -2.3461e+02 - logprior: -1.1323e+00
Epoch 7/10
19/19 - 3s - loss: 236.4568 - loglik: -2.3392e+02 - logprior: -1.1002e+00
Epoch 8/10
19/19 - 3s - loss: 236.1117 - loglik: -2.3372e+02 - logprior: -1.0727e+00
Epoch 9/10
19/19 - 3s - loss: 235.9671 - loglik: -2.3368e+02 - logprior: -1.0376e+00
Epoch 10/10
19/19 - 3s - loss: 235.7547 - loglik: -2.3356e+02 - logprior: -1.0139e+00
Fitted a model with MAP estimate = -234.3205
Time for alignment: 97.3969
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 301.6498 - loglik: -2.9847e+02 - logprior: -3.0976e+00
Epoch 2/10
19/19 - 2s - loss: 271.9402 - loglik: -2.7018e+02 - logprior: -1.2872e+00
Epoch 3/10
19/19 - 2s - loss: 258.4024 - loglik: -2.5629e+02 - logprior: -1.3564e+00
Epoch 4/10
19/19 - 2s - loss: 255.4274 - loglik: -2.5343e+02 - logprior: -1.2339e+00
Epoch 5/10
19/19 - 2s - loss: 254.1987 - loglik: -2.5218e+02 - logprior: -1.2573e+00
Epoch 6/10
19/19 - 2s - loss: 253.0185 - loglik: -2.5102e+02 - logprior: -1.2462e+00
Epoch 7/10
19/19 - 2s - loss: 252.3086 - loglik: -2.5026e+02 - logprior: -1.2539e+00
Epoch 8/10
19/19 - 2s - loss: 252.0699 - loglik: -2.5005e+02 - logprior: -1.2465e+00
Epoch 9/10
19/19 - 3s - loss: 250.7729 - loglik: -2.4881e+02 - logprior: -1.2413e+00
Epoch 10/10
19/19 - 3s - loss: 250.3139 - loglik: -2.4839e+02 - logprior: -1.2555e+00
Fitted a model with MAP estimate = -248.9774
expansions: [(0, 2), (17, 1), (20, 2), (21, 1), (22, 1), (23, 1), (24, 1), (39, 2), (41, 2), (42, 2), (44, 3), (53, 2), (54, 1), (74, 1), (75, 2), (76, 1), (77, 4), (78, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 251.3497 - loglik: -2.4645e+02 - logprior: -4.2690e+00
Epoch 2/2
19/19 - 3s - loss: 244.1977 - loglik: -2.4199e+02 - logprior: -1.4552e+00
Fitted a model with MAP estimate = -241.6041
expansions: []
discards: [ 1 55 72 98]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 245.2725 - loglik: -2.4122e+02 - logprior: -3.0644e+00
Epoch 2/2
19/19 - 3s - loss: 242.6576 - loglik: -2.4022e+02 - logprior: -1.3657e+00
Fitted a model with MAP estimate = -240.4899
expansions: [(99, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 243.2344 - loglik: -2.3901e+02 - logprior: -3.0244e+00
Epoch 2/10
19/19 - 3s - loss: 241.0110 - loglik: -2.3841e+02 - logprior: -1.3209e+00
Epoch 3/10
19/19 - 3s - loss: 239.5975 - loglik: -2.3687e+02 - logprior: -1.1799e+00
Epoch 4/10
19/19 - 3s - loss: 238.2929 - loglik: -2.3551e+02 - logprior: -1.1370e+00
Epoch 5/10
19/19 - 3s - loss: 237.5146 - loglik: -2.3475e+02 - logprior: -1.1196e+00
Epoch 6/10
19/19 - 3s - loss: 236.3666 - loglik: -2.3373e+02 - logprior: -1.1004e+00
Epoch 7/10
19/19 - 3s - loss: 235.7690 - loglik: -2.3326e+02 - logprior: -1.0779e+00
Epoch 8/10
19/19 - 3s - loss: 235.4207 - loglik: -2.3306e+02 - logprior: -1.0559e+00
Epoch 9/10
19/19 - 3s - loss: 235.0331 - loglik: -2.3278e+02 - logprior: -1.0187e+00
Epoch 10/10
19/19 - 3s - loss: 235.0485 - loglik: -2.3287e+02 - logprior: -9.9802e-01
Fitted a model with MAP estimate = -233.5229
Time for alignment: 101.5372
Computed alignments with likelihoods: ['-233.5625', '-234.3205', '-233.5229']
Best model has likelihood: -233.5229
time for generating output: 0.1705
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07686.projection.fasta
SP score = 0.8130487374336596
Training of 3 independent models on file PF00505.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f31f478d670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f32058c6b20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3219e38760>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f32058e85e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f389c0a9ee0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3a98421b50>, <__main__.SimpleDirichletPrior object at 0x7f3218f95400>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f389c4c73a0>

Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 186.2886 - loglik: -1.8305e+02 - logprior: -3.2294e+00
Epoch 2/10
19/19 - 2s - loss: 154.1565 - loglik: -1.5262e+02 - logprior: -1.4573e+00
Epoch 3/10
19/19 - 2s - loss: 142.0147 - loglik: -1.3999e+02 - logprior: -1.6485e+00
Epoch 4/10
19/19 - 1s - loss: 138.4755 - loglik: -1.3656e+02 - logprior: -1.5939e+00
Epoch 5/10
19/19 - 2s - loss: 137.7665 - loglik: -1.3590e+02 - logprior: -1.6167e+00
Epoch 6/10
19/19 - 1s - loss: 137.0421 - loglik: -1.3525e+02 - logprior: -1.6030e+00
Epoch 7/10
19/19 - 2s - loss: 137.0581 - loglik: -1.3530e+02 - logprior: -1.5974e+00
Fitted a model with MAP estimate = -136.4914
expansions: [(12, 1), (14, 1), (16, 1), (17, 1), (18, 1), (21, 1), (22, 3), (23, 1), (34, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 136.7692 - loglik: -1.3242e+02 - logprior: -4.1891e+00
Epoch 2/2
19/19 - 2s - loss: 127.8011 - loglik: -1.2550e+02 - logprior: -2.1164e+00
Fitted a model with MAP estimate = -126.2565
expansions: [(0, 2)]
discards: [ 0 28]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 127.1036 - loglik: -1.2380e+02 - logprior: -3.0965e+00
Epoch 2/2
19/19 - 2s - loss: 124.2398 - loglik: -1.2274e+02 - logprior: -1.2831e+00
Fitted a model with MAP estimate = -123.6088
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 128.8616 - loglik: -1.2474e+02 - logprior: -3.9063e+00
Epoch 2/10
19/19 - 2s - loss: 125.0147 - loglik: -1.2330e+02 - logprior: -1.4970e+00
Epoch 3/10
19/19 - 2s - loss: 124.1837 - loglik: -1.2268e+02 - logprior: -1.3002e+00
Epoch 4/10
19/19 - 2s - loss: 124.3097 - loglik: -1.2284e+02 - logprior: -1.2615e+00
Fitted a model with MAP estimate = -123.8744
Time for alignment: 48.6272
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 186.5397 - loglik: -1.8330e+02 - logprior: -3.2313e+00
Epoch 2/10
19/19 - 2s - loss: 156.4877 - loglik: -1.5494e+02 - logprior: -1.4626e+00
Epoch 3/10
19/19 - 2s - loss: 143.0749 - loglik: -1.4109e+02 - logprior: -1.6367e+00
Epoch 4/10
19/19 - 2s - loss: 139.4423 - loglik: -1.3758e+02 - logprior: -1.5747e+00
Epoch 5/10
19/19 - 2s - loss: 138.6574 - loglik: -1.3683e+02 - logprior: -1.6056e+00
Epoch 6/10
19/19 - 2s - loss: 137.6652 - loglik: -1.3588e+02 - logprior: -1.5804e+00
Epoch 7/10
19/19 - 2s - loss: 137.3215 - loglik: -1.3555e+02 - logprior: -1.5917e+00
Epoch 8/10
19/19 - 1s - loss: 136.6949 - loglik: -1.3492e+02 - logprior: -1.6004e+00
Epoch 9/10
19/19 - 2s - loss: 136.4458 - loglik: -1.3468e+02 - logprior: -1.6079e+00
Epoch 10/10
19/19 - 1s - loss: 136.1235 - loglik: -1.3436e+02 - logprior: -1.6064e+00
Fitted a model with MAP estimate = -135.8519
expansions: [(12, 1), (14, 1), (16, 1), (17, 1), (18, 1), (21, 2), (22, 2), (23, 1), (34, 1), (35, 1), (40, 1), (41, 1), (42, 1), (46, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 136.5913 - loglik: -1.3224e+02 - logprior: -4.1901e+00
Epoch 2/2
19/19 - 2s - loss: 127.9192 - loglik: -1.2556e+02 - logprior: -2.1835e+00
Fitted a model with MAP estimate = -126.3434
expansions: [(0, 2)]
discards: [ 0 28]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.7223 - loglik: -1.2343e+02 - logprior: -3.0944e+00
Epoch 2/2
19/19 - 2s - loss: 123.7996 - loglik: -1.2231e+02 - logprior: -1.2866e+00
Fitted a model with MAP estimate = -123.2713
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 128.6072 - loglik: -1.2450e+02 - logprior: -3.9068e+00
Epoch 2/10
19/19 - 2s - loss: 124.6959 - loglik: -1.2300e+02 - logprior: -1.4940e+00
Epoch 3/10
19/19 - 2s - loss: 123.9123 - loglik: -1.2242e+02 - logprior: -1.2967e+00
Epoch 4/10
19/19 - 2s - loss: 124.0084 - loglik: -1.2256e+02 - logprior: -1.2547e+00
Fitted a model with MAP estimate = -123.6194
Time for alignment: 53.4698
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 186.4231 - loglik: -1.8318e+02 - logprior: -3.2334e+00
Epoch 2/10
19/19 - 1s - loss: 154.8669 - loglik: -1.5333e+02 - logprior: -1.4564e+00
Epoch 3/10
19/19 - 2s - loss: 143.8903 - loglik: -1.4198e+02 - logprior: -1.6157e+00
Epoch 4/10
19/19 - 1s - loss: 139.8947 - loglik: -1.3806e+02 - logprior: -1.5416e+00
Epoch 5/10
19/19 - 2s - loss: 138.6386 - loglik: -1.3681e+02 - logprior: -1.5811e+00
Epoch 6/10
19/19 - 1s - loss: 138.0807 - loglik: -1.3632e+02 - logprior: -1.5482e+00
Epoch 7/10
19/19 - 2s - loss: 137.7268 - loglik: -1.3599e+02 - logprior: -1.5473e+00
Epoch 8/10
19/19 - 1s - loss: 137.6189 - loglik: -1.3589e+02 - logprior: -1.5493e+00
Epoch 9/10
19/19 - 2s - loss: 137.4777 - loglik: -1.3577e+02 - logprior: -1.5407e+00
Epoch 10/10
19/19 - 2s - loss: 137.1145 - loglik: -1.3541e+02 - logprior: -1.5368e+00
Fitted a model with MAP estimate = -136.9772
expansions: [(12, 1), (16, 1), (17, 1), (18, 1), (22, 5), (23, 1), (35, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 136.5407 - loglik: -1.3217e+02 - logprior: -4.2023e+00
Epoch 2/2
19/19 - 2s - loss: 127.8068 - loglik: -1.2545e+02 - logprior: -2.1852e+00
Fitted a model with MAP estimate = -126.3415
expansions: [(0, 2)]
discards: [ 0 27]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.6372 - loglik: -1.2335e+02 - logprior: -3.0964e+00
Epoch 2/2
19/19 - 2s - loss: 123.9400 - loglik: -1.2247e+02 - logprior: -1.2805e+00
Fitted a model with MAP estimate = -123.3472
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 128.6120 - loglik: -1.2453e+02 - logprior: -3.8908e+00
Epoch 2/10
19/19 - 2s - loss: 124.8045 - loglik: -1.2312e+02 - logprior: -1.4910e+00
Epoch 3/10
19/19 - 2s - loss: 124.0128 - loglik: -1.2253e+02 - logprior: -1.2910e+00
Epoch 4/10
19/19 - 1s - loss: 124.0337 - loglik: -1.2259e+02 - logprior: -1.2505e+00
Fitted a model with MAP estimate = -123.6871
Time for alignment: 51.7637
Computed alignments with likelihoods: ['-123.6088', '-123.2713', '-123.3472']
Best model has likelihood: -123.2713
time for generating output: 0.1205
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00505.projection.fasta
SP score = 0.9407031443426591
Training of 3 independent models on file PF13393.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f386444beb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3211468040>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f32114687c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b83baf0d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3a8ede1af0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3205fb4130>, <__main__.SimpleDirichletPrior object at 0x7f32045d5460>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3ba5ae0160>

Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 731.2819 - loglik: -7.2904e+02 - logprior: -1.8124e+00
Epoch 2/10
39/39 - 24s - loss: 621.8481 - loglik: -6.1889e+02 - logprior: -1.8085e+00
Epoch 3/10
39/39 - 24s - loss: 607.9676 - loglik: -6.0462e+02 - logprior: -1.9254e+00
Epoch 4/10
39/39 - 24s - loss: 603.3078 - loglik: -6.0023e+02 - logprior: -1.9297e+00
Epoch 5/10
39/39 - 24s - loss: 600.1662 - loglik: -5.9718e+02 - logprior: -1.9639e+00
Epoch 6/10
39/39 - 24s - loss: 599.8430 - loglik: -5.9695e+02 - logprior: -1.9944e+00
Epoch 7/10
39/39 - 24s - loss: 598.3990 - loglik: -5.9559e+02 - logprior: -1.9867e+00
Epoch 8/10
39/39 - 24s - loss: 598.3651 - loglik: -5.9561e+02 - logprior: -1.9960e+00
Epoch 9/10
39/39 - 24s - loss: 597.8720 - loglik: -5.9515e+02 - logprior: -2.0031e+00
Epoch 10/10
39/39 - 25s - loss: 597.3343 - loglik: -5.9466e+02 - logprior: -1.9991e+00
Fitted a model with MAP estimate = -595.8534
expansions: [(10, 1), (11, 1), (13, 1), (14, 3), (16, 1), (33, 1), (34, 1), (36, 1), (41, 1), (45, 1), (46, 1), (51, 1), (68, 1), (69, 1), (70, 2), (72, 1), (76, 1), (102, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (123, 1), (132, 2), (134, 1), (136, 2), (148, 2), (149, 1), (150, 1), (154, 1), (165, 3), (174, 3), (175, 1), (179, 1), (184, 1), (185, 3), (193, 1), (194, 1), (197, 1), (211, 1), (212, 3), (214, 3), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 308 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 37s - loss: 580.0662 - loglik: -5.7637e+02 - logprior: -2.9984e+00
Epoch 2/2
39/39 - 34s - loss: 567.1818 - loglik: -5.6448e+02 - logprior: -1.9152e+00
Fitted a model with MAP estimate = -563.8916
expansions: [(0, 3), (80, 1), (147, 1)]
discards: [  0  85 161 182 183 231 272]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 306 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 564.9407 - loglik: -5.6217e+02 - logprior: -1.8911e+00
Epoch 2/2
39/39 - 34s - loss: 562.2538 - loglik: -5.6055e+02 - logprior: -8.6000e-01
Fitted a model with MAP estimate = -559.6302
expansions: [(184, 1), (196, 4)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 309 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 564.4000 - loglik: -5.6104e+02 - logprior: -2.4972e+00
Epoch 2/10
39/39 - 39s - loss: 560.4492 - loglik: -5.5887e+02 - logprior: -7.7434e-01
Epoch 3/10
39/39 - 40s - loss: 558.3989 - loglik: -5.5721e+02 - logprior: -3.4607e-01
Epoch 4/10
39/39 - 39s - loss: 558.4204 - loglik: -5.5746e+02 - logprior: -1.8331e-01
Fitted a model with MAP estimate = -556.7800
Time for alignment: 698.0062
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 31s - loss: 731.3531 - loglik: -7.2909e+02 - logprior: -1.8221e+00
Epoch 2/10
39/39 - 27s - loss: 614.8900 - loglik: -6.1105e+02 - logprior: -2.0324e+00
Epoch 3/10
39/39 - 27s - loss: 598.7157 - loglik: -5.9481e+02 - logprior: -2.2199e+00
Epoch 4/10
39/39 - 27s - loss: 595.3055 - loglik: -5.9184e+02 - logprior: -2.2367e+00
Epoch 5/10
39/39 - 27s - loss: 593.6743 - loglik: -5.9041e+02 - logprior: -2.2198e+00
Epoch 6/10
39/39 - 27s - loss: 592.7871 - loglik: -5.8963e+02 - logprior: -2.2282e+00
Epoch 7/10
39/39 - 27s - loss: 592.4597 - loglik: -5.8936e+02 - logprior: -2.2452e+00
Epoch 8/10
39/39 - 27s - loss: 591.6617 - loglik: -5.8862e+02 - logprior: -2.2446e+00
Epoch 9/10
39/39 - 27s - loss: 591.9218 - loglik: -5.8894e+02 - logprior: -2.2450e+00
Fitted a model with MAP estimate = -589.7060
expansions: [(10, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (32, 2), (34, 1), (36, 1), (42, 1), (44, 1), (45, 1), (46, 1), (53, 1), (67, 1), (68, 1), (69, 1), (71, 1), (77, 1), (78, 1), (102, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (125, 1), (132, 1), (134, 1), (135, 1), (137, 1), (138, 1), (139, 1), (140, 1), (147, 1), (149, 1), (154, 1), (155, 6), (163, 3), (164, 2), (174, 1), (175, 1), (177, 1), (180, 1), (183, 1), (185, 1), (192, 2), (193, 1), (194, 1), (197, 1), (211, 1), (212, 4), (213, 3), (227, 1), (230, 1), (231, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 317 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 44s - loss: 572.3675 - loglik: -5.6858e+02 - logprior: -3.0576e+00
Epoch 2/2
39/39 - 42s - loss: 556.7784 - loglik: -5.5400e+02 - logprior: -1.9953e+00
Fitted a model with MAP estimate = -552.7667
expansions: [(0, 2), (149, 1), (200, 1)]
discards: [  0  39 251 281]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 317 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 553.9883 - loglik: -5.5119e+02 - logprior: -1.8889e+00
Epoch 2/2
39/39 - 44s - loss: 551.6996 - loglik: -5.5001e+02 - logprior: -8.1574e-01
Fitted a model with MAP estimate = -549.0445
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 316 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 44s - loss: 555.3194 - loglik: -5.5176e+02 - logprior: -2.6693e+00
Epoch 2/10
39/39 - 40s - loss: 551.5058 - loglik: -5.4984e+02 - logprior: -8.0829e-01
Epoch 3/10
39/39 - 41s - loss: 550.3821 - loglik: -5.4919e+02 - logprior: -3.4477e-01
Epoch 4/10
39/39 - 40s - loss: 549.6232 - loglik: -5.4864e+02 - logprior: -1.6555e-01
Epoch 5/10
39/39 - 42s - loss: 548.7267 - loglik: -5.4792e+02 - logprior: -8.9963e-03
Epoch 6/10
39/39 - 43s - loss: 548.4620 - loglik: -5.4780e+02 - logprior: 0.1360
Epoch 7/10
39/39 - 45s - loss: 548.4706 - loglik: -5.4794e+02 - logprior: 0.2760
Fitted a model with MAP estimate = -546.8923
Time for alignment: 908.7340
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 32s - loss: 729.4878 - loglik: -7.2724e+02 - logprior: -1.8230e+00
Epoch 2/10
39/39 - 29s - loss: 611.4918 - loglik: -6.0827e+02 - logprior: -2.0048e+00
Epoch 3/10
39/39 - 29s - loss: 597.8005 - loglik: -5.9438e+02 - logprior: -2.1428e+00
Epoch 4/10
39/39 - 29s - loss: 595.4819 - loglik: -5.9227e+02 - logprior: -2.1099e+00
Epoch 5/10
39/39 - 29s - loss: 593.9647 - loglik: -5.9091e+02 - logprior: -2.1006e+00
Epoch 6/10
39/39 - 29s - loss: 592.7886 - loglik: -5.8980e+02 - logprior: -2.1077e+00
Epoch 7/10
39/39 - 28s - loss: 592.9186 - loglik: -5.9000e+02 - logprior: -2.1076e+00
Fitted a model with MAP estimate = -590.4078
expansions: [(10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (30, 1), (42, 1), (46, 1), (47, 1), (48, 1), (51, 1), (67, 1), (70, 3), (76, 1), (86, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (123, 1), (132, 1), (134, 1), (137, 1), (149, 1), (154, 1), (155, 7), (161, 1), (164, 2), (173, 2), (176, 1), (177, 1), (179, 1), (183, 1), (185, 1), (188, 1), (192, 1), (193, 1), (196, 1), (210, 1), (211, 3), (213, 2), (227, 1), (230, 1), (231, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 307 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 43s - loss: 574.6528 - loglik: -5.7077e+02 - logprior: -3.0335e+00
Epoch 2/2
39/39 - 38s - loss: 561.2294 - loglik: -5.5845e+02 - logprior: -1.9190e+00
Fitted a model with MAP estimate = -557.2232
expansions: [(0, 2), (147, 1), (167, 2), (206, 1), (244, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 313 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 43s - loss: 557.1407 - loglik: -5.5416e+02 - logprior: -2.0226e+00
Epoch 2/2
39/39 - 41s - loss: 553.9067 - loglik: -5.5213e+02 - logprior: -8.5800e-01
Fitted a model with MAP estimate = -550.8088
expansions: [(208, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 314 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 40s - loss: 556.4659 - loglik: -5.5291e+02 - logprior: -2.6523e+00
Epoch 2/10
39/39 - 35s - loss: 552.6647 - loglik: -5.5105e+02 - logprior: -7.7053e-01
Epoch 3/10
39/39 - 35s - loss: 550.2785 - loglik: -5.4908e+02 - logprior: -3.4619e-01
Epoch 4/10
39/39 - 36s - loss: 550.3755 - loglik: -5.4939e+02 - logprior: -2.0875e-01
Fitted a model with MAP estimate = -549.0543
Time for alignment: 687.6901
Computed alignments with likelihoods: ['-556.7800', '-546.8923', '-549.0543']
Best model has likelihood: -546.8923
time for generating output: 0.3715
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13393.projection.fasta
SP score = 0.4149354108403249
Training of 3 independent models on file PF00202.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3bc7911040>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3210099e80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f37fc7b6b80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f37fdb79640>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f38246e2100>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f38ac37f1f0>, <__main__.SimpleDirichletPrior object at 0x7f3a8ec90580>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3218b28430>

Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 971.7767 - loglik: -9.6976e+02 - logprior: -1.5781e+00
Epoch 2/10
39/39 - 47s - loss: 807.1483 - loglik: -8.0411e+02 - logprior: -1.6660e+00
Epoch 3/10
39/39 - 47s - loss: 792.9409 - loglik: -7.8922e+02 - logprior: -1.8179e+00
Epoch 4/10
39/39 - 47s - loss: 789.5094 - loglik: -7.8593e+02 - logprior: -1.7747e+00
Epoch 5/10
39/39 - 47s - loss: 787.4617 - loglik: -7.8397e+02 - logprior: -1.8287e+00
Epoch 6/10
39/39 - 47s - loss: 786.1748 - loglik: -7.8280e+02 - logprior: -1.8614e+00
Epoch 7/10
39/39 - 47s - loss: 784.9627 - loglik: -7.8166e+02 - logprior: -1.8738e+00
Epoch 8/10
39/39 - 47s - loss: 784.9668 - loglik: -7.8165e+02 - logprior: -1.9906e+00
Fitted a model with MAP estimate = -774.8687
expansions: [(0, 3), (20, 1), (52, 1), (54, 1), (58, 2), (71, 1), (72, 1), (96, 1), (100, 1), (121, 2), (122, 1), (123, 1), (124, 1), (147, 3), (149, 1), (164, 1), (171, 1), (172, 1), (176, 1), (177, 1), (189, 1), (193, 2), (196, 1), (197, 3), (218, 1), (219, 1), (220, 1), (222, 1), (223, 1), (224, 3), (246, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 2), (257, 1), (258, 1), (262, 1), (264, 1), (284, 7), (285, 3), (286, 1), (287, 1), (295, 1), (296, 1), (309, 1), (313, 2), (314, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 392 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 66s - loss: 761.8099 - loglik: -7.5764e+02 - logprior: -2.9289e+00
Epoch 2/2
39/39 - 60s - loss: 747.0267 - loglik: -7.4435e+02 - logprior: -1.3383e+00
Fitted a model with MAP estimate = -734.8502
expansions: [(345, 2)]
discards: [133 220 228 337 384]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 389 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 65s - loss: 744.7041 - loglik: -7.4130e+02 - logprior: -1.9385e+00
Epoch 2/2
39/39 - 62s - loss: 743.5740 - loglik: -7.4131e+02 - logprior: -8.2262e-01
Fitted a model with MAP estimate = -732.5421
expansions: [(334, 1), (362, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 392 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 66s - loss: 734.8166 - loglik: -7.3179e+02 - logprior: -1.6238e+00
Epoch 2/10
39/39 - 61s - loss: 734.4399 - loglik: -7.3257e+02 - logprior: -5.0053e-01
Epoch 3/10
39/39 - 61s - loss: 732.7986 - loglik: -7.3122e+02 - logprior: -2.1964e-01
Epoch 4/10
39/39 - 63s - loss: 732.2812 - loglik: -7.3099e+02 - logprior: -7.4407e-02
Epoch 5/10
39/39 - 66s - loss: 731.0659 - loglik: -7.3003e+02 - logprior: 0.1272
Epoch 6/10
39/39 - 69s - loss: 731.7303 - loglik: -7.3094e+02 - logprior: 0.2884
Fitted a model with MAP estimate = -729.3368
Time for alignment: 1308.4292
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 975.0556 - loglik: -9.7306e+02 - logprior: -1.5587e+00
Epoch 2/10
39/39 - 50s - loss: 810.1504 - loglik: -8.0764e+02 - logprior: -1.5411e+00
Epoch 3/10
39/39 - 53s - loss: 794.0812 - loglik: -7.9074e+02 - logprior: -1.8147e+00
Epoch 4/10
39/39 - 54s - loss: 790.0358 - loglik: -7.8660e+02 - logprior: -1.7890e+00
Epoch 5/10
39/39 - 53s - loss: 788.4662 - loglik: -7.8507e+02 - logprior: -1.8097e+00
Epoch 6/10
39/39 - 53s - loss: 787.0140 - loglik: -7.8374e+02 - logprior: -1.8373e+00
Epoch 7/10
39/39 - 52s - loss: 786.3683 - loglik: -7.8320e+02 - logprior: -1.8469e+00
Epoch 8/10
39/39 - 51s - loss: 785.8596 - loglik: -7.8273e+02 - logprior: -1.8881e+00
Epoch 9/10
39/39 - 51s - loss: 785.0585 - loglik: -7.8201e+02 - logprior: -1.9081e+00
Epoch 10/10
39/39 - 53s - loss: 784.9180 - loglik: -7.8192e+02 - logprior: -1.9142e+00
Fitted a model with MAP estimate = -775.0694
expansions: [(0, 3), (42, 1), (51, 2), (52, 1), (56, 1), (69, 1), (70, 1), (73, 2), (74, 2), (98, 1), (119, 1), (121, 1), (122, 1), (146, 3), (148, 1), (163, 1), (170, 1), (174, 1), (175, 1), (176, 1), (193, 2), (195, 1), (196, 1), (205, 1), (219, 1), (220, 1), (223, 2), (224, 3), (245, 1), (246, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 2), (257, 1), (258, 1), (262, 1), (264, 1), (265, 2), (284, 7), (286, 1), (287, 1), (288, 1), (296, 1), (297, 1), (298, 3), (309, 1), (313, 2), (314, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 394 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 79s - loss: 761.6290 - loglik: -7.5760e+02 - logprior: -3.0227e+00
Epoch 2/2
39/39 - 74s - loss: 745.1411 - loglik: -7.4261e+02 - logprior: -1.4012e+00
Fitted a model with MAP estimate = -733.6330
expansions: [(343, 2)]
discards: [ 83  84 260 317 364 387]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 390 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 70s - loss: 744.1100 - loglik: -7.4086e+02 - logprior: -1.9746e+00
Epoch 2/2
39/39 - 60s - loss: 742.6220 - loglik: -7.4063e+02 - logprior: -7.0408e-01
Fitted a model with MAP estimate = -731.8884
expansions: [(0, 2)]
discards: [2 3]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 390 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 66s - loss: 736.3795 - loglik: -7.3263e+02 - logprior: -2.4494e+00
Epoch 2/10
39/39 - 62s - loss: 734.4510 - loglik: -7.3273e+02 - logprior: -4.1042e-01
Epoch 3/10
39/39 - 64s - loss: 734.0752 - loglik: -7.3266e+02 - logprior: -1.6873e-01
Epoch 4/10
39/39 - 69s - loss: 732.2191 - loglik: -7.3111e+02 - logprior: 2.1325e-04
Epoch 5/10
39/39 - 71s - loss: 731.0485 - loglik: -7.3022e+02 - logprior: 0.1946
Epoch 6/10
39/39 - 70s - loss: 731.9255 - loglik: -7.3132e+02 - logprior: 0.3737
Fitted a model with MAP estimate = -730.1001
Time for alignment: 1549.5027
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 970.2064 - loglik: -9.6821e+02 - logprior: -1.5726e+00
Epoch 2/10
39/39 - 51s - loss: 811.8460 - loglik: -8.0968e+02 - logprior: -1.3923e+00
Epoch 3/10
39/39 - 51s - loss: 794.6548 - loglik: -7.9201e+02 - logprior: -1.6810e+00
Epoch 4/10
39/39 - 52s - loss: 790.1924 - loglik: -7.8709e+02 - logprior: -1.7182e+00
Epoch 5/10
39/39 - 54s - loss: 788.7730 - loglik: -7.8559e+02 - logprior: -1.7466e+00
Epoch 6/10
39/39 - 54s - loss: 787.1061 - loglik: -7.8389e+02 - logprior: -1.8103e+00
Epoch 7/10
39/39 - 53s - loss: 786.2309 - loglik: -7.8308e+02 - logprior: -1.8356e+00
Epoch 8/10
39/39 - 53s - loss: 785.4946 - loglik: -7.8238e+02 - logprior: -1.8976e+00
Epoch 9/10
39/39 - 53s - loss: 784.9958 - loglik: -7.8196e+02 - logprior: -1.8937e+00
Epoch 10/10
39/39 - 52s - loss: 784.7663 - loglik: -7.8176e+02 - logprior: -1.9209e+00
Fitted a model with MAP estimate = -774.8865
expansions: [(0, 2), (24, 1), (42, 1), (52, 1), (53, 1), (57, 1), (62, 1), (70, 1), (71, 1), (75, 2), (99, 1), (120, 2), (121, 1), (122, 1), (123, 1), (131, 1), (145, 3), (146, 1), (147, 1), (163, 1), (169, 1), (173, 1), (175, 1), (177, 1), (192, 2), (195, 1), (196, 3), (219, 1), (220, 1), (221, 1), (222, 2), (223, 3), (247, 1), (248, 1), (250, 1), (252, 1), (254, 1), (255, 1), (256, 1), (260, 1), (261, 3), (262, 1), (263, 1), (265, 2), (284, 6), (285, 3), (286, 1), (287, 1), (295, 1), (296, 1), (297, 2), (308, 1), (313, 2), (314, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 398 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 76s - loss: 761.4916 - loglik: -7.5738e+02 - logprior: -3.0293e+00
Epoch 2/2
39/39 - 73s - loss: 744.8470 - loglik: -7.4220e+02 - logprior: -1.3890e+00
Fitted a model with MAP estimate = -733.1498
expansions: [(0, 2), (349, 2)]
discards: [  3   4  85 133 168 221 229 263 313 321 368 391]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 390 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 75s - loss: 745.7276 - loglik: -7.4155e+02 - logprior: -2.7484e+00
Epoch 2/2
39/39 - 74s - loss: 743.2632 - loglik: -7.4102e+02 - logprior: -8.0541e-01
Fitted a model with MAP estimate = -732.5001
expansions: []
discards: [335 336 337]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 387 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 72s - loss: 735.9686 - loglik: -7.3298e+02 - logprior: -1.5853e+00
Epoch 2/10
39/39 - 69s - loss: 735.5897 - loglik: -7.3374e+02 - logprior: -4.4960e-01
Epoch 3/10
39/39 - 71s - loss: 734.4545 - loglik: -7.3289e+02 - logprior: -2.0560e-01
Epoch 4/10
39/39 - 75s - loss: 732.9669 - loglik: -7.3173e+02 - logprior: -2.8490e-02
Epoch 5/10
39/39 - 70s - loss: 732.1928 - loglik: -7.3119e+02 - logprior: 0.1495
Epoch 6/10
39/39 - 66s - loss: 732.1732 - loglik: -7.3142e+02 - logprior: 0.3180
Epoch 7/10
39/39 - 63s - loss: 732.8345 - loglik: -7.3227e+02 - logprior: 0.4697
Fitted a model with MAP estimate = -730.3716
Time for alignment: 1676.8160
Computed alignments with likelihoods: ['-729.3368', '-730.1001', '-730.3716']
Best model has likelihood: -729.3368
time for generating output: 1.1671
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00202.projection.fasta
SP score = 0.3602752868511264
Training of 3 independent models on file PF00018.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3218faae20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f389c49ee20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f389c49ee50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f38ac2d38b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f38ac2d3670>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3220482220>, <__main__.SimpleDirichletPrior object at 0x7f31ecdc6160>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f37fdc01d30>

Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 133.7307 - loglik: -1.3048e+02 - logprior: -3.2480e+00
Epoch 2/10
19/19 - 1s - loss: 109.1206 - loglik: -1.0761e+02 - logprior: -1.4315e+00
Epoch 3/10
19/19 - 1s - loss: 100.8067 - loglik: -9.9012e+01 - logprior: -1.6039e+00
Epoch 4/10
19/19 - 1s - loss: 98.8226 - loglik: -9.7171e+01 - logprior: -1.4953e+00
Epoch 5/10
19/19 - 1s - loss: 98.1685 - loglik: -9.6587e+01 - logprior: -1.4669e+00
Epoch 6/10
19/19 - 1s - loss: 98.2530 - loglik: -9.6685e+01 - logprior: -1.4576e+00
Fitted a model with MAP estimate = -97.7492
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.2673 - loglik: -9.6930e+01 - logprior: -4.2413e+00
Epoch 2/2
19/19 - 1s - loss: 94.4101 - loglik: -9.2178e+01 - logprior: -2.1217e+00
Fitted a model with MAP estimate = -92.9857
expansions: [(0, 1)]
discards: [ 0  9 12 29 39 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 95.4675 - loglik: -9.2178e+01 - logprior: -3.1618e+00
Epoch 2/2
19/19 - 1s - loss: 92.7678 - loglik: -9.1118e+01 - logprior: -1.5227e+00
Fitted a model with MAP estimate = -92.1552
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 94.5541 - loglik: -9.1137e+01 - logprior: -3.2731e+00
Epoch 2/10
19/19 - 1s - loss: 92.3118 - loglik: -9.0645e+01 - logprior: -1.5192e+00
Epoch 3/10
19/19 - 1s - loss: 92.1377 - loglik: -9.0561e+01 - logprior: -1.4236e+00
Epoch 4/10
19/19 - 1s - loss: 91.9548 - loglik: -9.0421e+01 - logprior: -1.3750e+00
Epoch 5/10
19/19 - 1s - loss: 91.9659 - loglik: -9.0467e+01 - logprior: -1.3386e+00
Fitted a model with MAP estimate = -91.6993
Time for alignment: 40.2718
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 133.8074 - loglik: -1.3056e+02 - logprior: -3.2424e+00
Epoch 2/10
19/19 - 1s - loss: 109.1300 - loglik: -1.0764e+02 - logprior: -1.4157e+00
Epoch 3/10
19/19 - 1s - loss: 101.3877 - loglik: -9.9608e+01 - logprior: -1.5978e+00
Epoch 4/10
19/19 - 1s - loss: 98.8874 - loglik: -9.7243e+01 - logprior: -1.4917e+00
Epoch 5/10
19/19 - 1s - loss: 98.2589 - loglik: -9.6663e+01 - logprior: -1.4701e+00
Epoch 6/10
19/19 - 1s - loss: 98.0796 - loglik: -9.6503e+01 - logprior: -1.4563e+00
Epoch 7/10
19/19 - 1s - loss: 97.8474 - loglik: -9.6296e+01 - logprior: -1.4406e+00
Epoch 8/10
19/19 - 1s - loss: 97.9085 - loglik: -9.6362e+01 - logprior: -1.4316e+00
Fitted a model with MAP estimate = -97.6214
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.2848 - loglik: -9.6932e+01 - logprior: -4.2468e+00
Epoch 2/2
19/19 - 1s - loss: 94.4236 - loglik: -9.2142e+01 - logprior: -2.1637e+00
Fitted a model with MAP estimate = -93.0614
expansions: [(0, 1)]
discards: [ 0  9 12 29 39 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 95.4307 - loglik: -9.2130e+01 - logprior: -3.1703e+00
Epoch 2/2
19/19 - 1s - loss: 92.7270 - loglik: -9.1066e+01 - logprior: -1.5264e+00
Fitted a model with MAP estimate = -92.1109
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 94.4694 - loglik: -9.1042e+01 - logprior: -3.2842e+00
Epoch 2/10
19/19 - 1s - loss: 92.3740 - loglik: -9.0707e+01 - logprior: -1.5142e+00
Epoch 3/10
19/19 - 1s - loss: 92.0900 - loglik: -9.0509e+01 - logprior: -1.4244e+00
Epoch 4/10
19/19 - 1s - loss: 91.9595 - loglik: -9.0424e+01 - logprior: -1.3755e+00
Epoch 5/10
19/19 - 1s - loss: 91.8851 - loglik: -9.0382e+01 - logprior: -1.3351e+00
Epoch 6/10
19/19 - 1s - loss: 91.9217 - loglik: -9.0431e+01 - logprior: -1.3222e+00
Fitted a model with MAP estimate = -91.6463
Time for alignment: 43.2194
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.7717 - loglik: -1.3052e+02 - logprior: -3.2456e+00
Epoch 2/10
19/19 - 1s - loss: 109.0618 - loglik: -1.0757e+02 - logprior: -1.4235e+00
Epoch 3/10
19/19 - 1s - loss: 100.8049 - loglik: -9.9011e+01 - logprior: -1.6032e+00
Epoch 4/10
19/19 - 1s - loss: 98.8695 - loglik: -9.7224e+01 - logprior: -1.4886e+00
Epoch 5/10
19/19 - 1s - loss: 98.2503 - loglik: -9.6660e+01 - logprior: -1.4698e+00
Epoch 6/10
19/19 - 1s - loss: 97.9640 - loglik: -9.6400e+01 - logprior: -1.4595e+00
Epoch 7/10
19/19 - 1s - loss: 97.9419 - loglik: -9.6394e+01 - logprior: -1.4409e+00
Epoch 8/10
19/19 - 1s - loss: 97.8879 - loglik: -9.6349e+01 - logprior: -1.4294e+00
Epoch 9/10
19/19 - 1s - loss: 97.7775 - loglik: -9.6240e+01 - logprior: -1.4248e+00
Epoch 10/10
19/19 - 1s - loss: 97.8157 - loglik: -9.6282e+01 - logprior: -1.4185e+00
Fitted a model with MAP estimate = -97.5382
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.2838 - loglik: -9.6927e+01 - logprior: -4.2503e+00
Epoch 2/2
19/19 - 1s - loss: 94.4013 - loglik: -9.2092e+01 - logprior: -2.1935e+00
Fitted a model with MAP estimate = -93.1027
expansions: [(0, 1)]
discards: [ 0  9 12 29 39 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 95.3996 - loglik: -9.2103e+01 - logprior: -3.1664e+00
Epoch 2/2
19/19 - 1s - loss: 92.7231 - loglik: -9.1062e+01 - logprior: -1.5255e+00
Fitted a model with MAP estimate = -92.1158
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 94.4997 - loglik: -9.1080e+01 - logprior: -3.2756e+00
Epoch 2/10
19/19 - 1s - loss: 92.3757 - loglik: -9.0702e+01 - logprior: -1.5164e+00
Epoch 3/10
19/19 - 1s - loss: 92.0931 - loglik: -9.0514e+01 - logprior: -1.4237e+00
Epoch 4/10
19/19 - 1s - loss: 91.9966 - loglik: -9.0467e+01 - logprior: -1.3694e+00
Epoch 5/10
19/19 - 1s - loss: 91.8785 - loglik: -9.0374e+01 - logprior: -1.3399e+00
Epoch 6/10
19/19 - 1s - loss: 91.8865 - loglik: -9.0396e+01 - logprior: -1.3179e+00
Fitted a model with MAP estimate = -91.6585
Time for alignment: 44.3237
Computed alignments with likelihoods: ['-91.6993', '-91.6463', '-91.6585']
Best model has likelihood: -91.6463
time for generating output: 0.1042
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00018.projection.fasta
SP score = 0.8426273861415903
Training of 3 independent models on file PF00687.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3221359850>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3a8ef56ee0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3221326670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3204f47850>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3204f47c70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3221acfdf0>, <__main__.SimpleDirichletPrior object at 0x7f38443e2250>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3bbf595b80>

Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 471.2171 - loglik: -4.6927e+02 - logprior: -1.9261e+00
Epoch 2/10
39/39 - 11s - loss: 373.5489 - loglik: -3.7134e+02 - logprior: -1.9051e+00
Epoch 3/10
39/39 - 11s - loss: 365.6640 - loglik: -3.6336e+02 - logprior: -1.9242e+00
Epoch 4/10
39/39 - 12s - loss: 362.6261 - loglik: -3.6039e+02 - logprior: -1.8742e+00
Epoch 5/10
39/39 - 12s - loss: 362.0047 - loglik: -3.5982e+02 - logprior: -1.8468e+00
Epoch 6/10
39/39 - 12s - loss: 360.5365 - loglik: -3.5840e+02 - logprior: -1.8256e+00
Epoch 7/10
39/39 - 12s - loss: 359.8346 - loglik: -3.5768e+02 - logprior: -1.7960e+00
Epoch 8/10
39/39 - 12s - loss: 358.6993 - loglik: -3.5649e+02 - logprior: -1.7976e+00
Epoch 9/10
39/39 - 11s - loss: 357.9527 - loglik: -3.5576e+02 - logprior: -1.7982e+00
Epoch 10/10
39/39 - 12s - loss: 357.5547 - loglik: -3.5539e+02 - logprior: -1.8004e+00
Fitted a model with MAP estimate = -357.1291
expansions: [(4, 1), (5, 2), (12, 1), (13, 2), (20, 1), (26, 1), (27, 1), (35, 1), (40, 2), (41, 1), (52, 1), (58, 1), (59, 1), (61, 2), (63, 1), (66, 1), (75, 1), (98, 2), (99, 1), (100, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 3), (123, 1), (127, 1), (129, 1), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 196 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 331.5732 - loglik: -3.2932e+02 - logprior: -1.8945e+00
Epoch 2/2
39/39 - 15s - loss: 319.8342 - loglik: -3.1862e+02 - logprior: -8.4004e-01
Fitted a model with MAP estimate = -318.6667
expansions: []
discards: [ 77 119 147 171]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 321.0668 - loglik: -3.1902e+02 - logprior: -1.6373e+00
Epoch 2/2
39/39 - 15s - loss: 319.4120 - loglik: -3.1843e+02 - logprior: -5.8539e-01
Fitted a model with MAP estimate = -318.6924
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 320.3967 - loglik: -3.1852e+02 - logprior: -1.4843e+00
Epoch 2/10
39/39 - 15s - loss: 319.2116 - loglik: -3.1834e+02 - logprior: -4.8627e-01
Epoch 3/10
39/39 - 15s - loss: 318.4912 - loglik: -3.1769e+02 - logprior: -4.0080e-01
Epoch 4/10
39/39 - 15s - loss: 318.6987 - loglik: -3.1802e+02 - logprior: -2.9454e-01
Fitted a model with MAP estimate = -318.0894
Time for alignment: 316.0597
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 472.3948 - loglik: -4.7045e+02 - logprior: -1.9178e+00
Epoch 2/10
39/39 - 11s - loss: 372.0750 - loglik: -3.6992e+02 - logprior: -1.9018e+00
Epoch 3/10
39/39 - 11s - loss: 363.7872 - loglik: -3.6156e+02 - logprior: -1.8899e+00
Epoch 4/10
39/39 - 11s - loss: 360.8240 - loglik: -3.5866e+02 - logprior: -1.8186e+00
Epoch 5/10
39/39 - 11s - loss: 359.9326 - loglik: -3.5781e+02 - logprior: -1.7868e+00
Epoch 6/10
39/39 - 11s - loss: 358.7661 - loglik: -3.5667e+02 - logprior: -1.7356e+00
Epoch 7/10
39/39 - 11s - loss: 357.9355 - loglik: -3.5585e+02 - logprior: -1.7164e+00
Epoch 8/10
39/39 - 12s - loss: 356.6339 - loglik: -3.5451e+02 - logprior: -1.7226e+00
Epoch 9/10
39/39 - 11s - loss: 356.5562 - loglik: -3.5444e+02 - logprior: -1.7170e+00
Epoch 10/10
39/39 - 11s - loss: 356.2325 - loglik: -3.5414e+02 - logprior: -1.7089e+00
Fitted a model with MAP estimate = -355.3226
expansions: [(4, 1), (5, 2), (12, 3), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (40, 2), (42, 2), (52, 1), (58, 1), (59, 1), (61, 2), (63, 1), (67, 1), (75, 1), (98, 1), (100, 3), (102, 1), (109, 1), (111, 1), (113, 1), (114, 1), (116, 3), (123, 1), (127, 1), (129, 1), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 198 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 329.3815 - loglik: -3.2711e+02 - logprior: -1.9107e+00
Epoch 2/2
39/39 - 15s - loss: 317.8529 - loglik: -3.1651e+02 - logprior: -9.7043e-01
Fitted a model with MAP estimate = -316.4537
expansions: []
discards: [ 16  55  80 125 149 173]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 319.1337 - loglik: -3.1707e+02 - logprior: -1.6457e+00
Epoch 2/2
39/39 - 15s - loss: 317.4866 - loglik: -3.1642e+02 - logprior: -6.5562e-01
Fitted a model with MAP estimate = -316.2558
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 318.1710 - loglik: -3.1622e+02 - logprior: -1.5302e+00
Epoch 2/10
39/39 - 15s - loss: 316.9873 - loglik: -3.1606e+02 - logprior: -5.1375e-01
Epoch 3/10
39/39 - 15s - loss: 316.5419 - loglik: -3.1573e+02 - logprior: -3.9489e-01
Epoch 4/10
39/39 - 15s - loss: 316.1804 - loglik: -3.1549e+02 - logprior: -2.8147e-01
Epoch 5/10
39/39 - 15s - loss: 316.5836 - loglik: -3.1599e+02 - logprior: -1.8127e-01
Fitted a model with MAP estimate = -315.8634
Time for alignment: 329.6872
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 472.0013 - loglik: -4.7006e+02 - logprior: -1.9236e+00
Epoch 2/10
39/39 - 12s - loss: 371.9650 - loglik: -3.6971e+02 - logprior: -1.9210e+00
Epoch 3/10
39/39 - 12s - loss: 363.5166 - loglik: -3.6112e+02 - logprior: -1.9558e+00
Epoch 4/10
39/39 - 12s - loss: 360.9547 - loglik: -3.5867e+02 - logprior: -1.9073e+00
Epoch 5/10
39/39 - 12s - loss: 358.7725 - loglik: -3.5655e+02 - logprior: -1.8771e+00
Epoch 6/10
39/39 - 12s - loss: 358.5788 - loglik: -3.5639e+02 - logprior: -1.8690e+00
Epoch 7/10
39/39 - 12s - loss: 358.0927 - loglik: -3.5594e+02 - logprior: -1.8592e+00
Epoch 8/10
39/39 - 12s - loss: 357.4242 - loglik: -3.5530e+02 - logprior: -1.8512e+00
Epoch 9/10
39/39 - 12s - loss: 357.2591 - loglik: -3.5513e+02 - logprior: -1.8205e+00
Epoch 10/10
39/39 - 12s - loss: 356.5869 - loglik: -3.5439e+02 - logprior: -1.8064e+00
Fitted a model with MAP estimate = -355.1442
expansions: [(4, 1), (5, 2), (9, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (34, 1), (37, 1), (40, 2), (41, 1), (42, 2), (58, 1), (59, 1), (61, 2), (63, 1), (66, 1), (74, 3), (97, 2), (100, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 1), (123, 1), (127, 1), (129, 2), (130, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 197 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 329.9161 - loglik: -3.2755e+02 - logprior: -1.9760e+00
Epoch 2/2
39/39 - 15s - loss: 318.3271 - loglik: -3.1693e+02 - logprior: -1.0031e+00
Fitted a model with MAP estimate = -317.0295
expansions: []
discards: [ 51  56  79  97 122]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 319.7928 - loglik: -3.1763e+02 - logprior: -1.7382e+00
Epoch 2/2
39/39 - 15s - loss: 318.0832 - loglik: -3.1693e+02 - logprior: -7.3575e-01
Fitted a model with MAP estimate = -317.1391
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 318.8280 - loglik: -3.1682e+02 - logprior: -1.5830e+00
Epoch 2/10
39/39 - 15s - loss: 317.3083 - loglik: -3.1632e+02 - logprior: -5.6212e-01
Epoch 3/10
39/39 - 15s - loss: 317.3538 - loglik: -3.1649e+02 - logprior: -4.2873e-01
Fitted a model with MAP estimate = -316.6014
Time for alignment: 301.4358
Computed alignments with likelihoods: ['-318.0894', '-315.8634', '-316.6014']
Best model has likelihood: -315.8634
time for generating output: 0.2039
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00687.projection.fasta
SP score = 0.7566992769034453
Training of 3 independent models on file PF03129.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f321891d790>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f37fe7455b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f37fe745f40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3210650940>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3220ecd580>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3204a1adf0>, <__main__.SimpleDirichletPrior object at 0x7f37fd854640>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3b83f88280>

Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 258.0869 - loglik: -2.5485e+02 - logprior: -3.1571e+00
Epoch 2/10
19/19 - 2s - loss: 226.2731 - loglik: -2.2443e+02 - logprior: -1.3545e+00
Epoch 3/10
19/19 - 2s - loss: 211.6720 - loglik: -2.0940e+02 - logprior: -1.4992e+00
Epoch 4/10
19/19 - 2s - loss: 206.8687 - loglik: -2.0509e+02 - logprior: -1.3680e+00
Epoch 5/10
19/19 - 2s - loss: 205.2391 - loglik: -2.0363e+02 - logprior: -1.3240e+00
Epoch 6/10
19/19 - 2s - loss: 204.6065 - loglik: -2.0312e+02 - logprior: -1.2817e+00
Epoch 7/10
19/19 - 2s - loss: 204.3947 - loglik: -2.0295e+02 - logprior: -1.2681e+00
Epoch 8/10
19/19 - 2s - loss: 204.2611 - loglik: -2.0284e+02 - logprior: -1.2555e+00
Epoch 9/10
19/19 - 2s - loss: 204.1377 - loglik: -2.0272e+02 - logprior: -1.2522e+00
Epoch 10/10
19/19 - 2s - loss: 203.9985 - loglik: -2.0260e+02 - logprior: -1.2493e+00
Fitted a model with MAP estimate = -203.4826
expansions: [(7, 1), (8, 2), (9, 4), (12, 1), (15, 1), (18, 1), (30, 2), (37, 1), (38, 1), (43, 1), (50, 2), (55, 1), (57, 1), (63, 1), (72, 1), (73, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 204.1012 - loglik: -1.9986e+02 - logprior: -4.0828e+00
Epoch 2/2
19/19 - 3s - loss: 195.0807 - loglik: -1.9281e+02 - logprior: -2.0869e+00
Fitted a model with MAP estimate = -192.8542
expansions: [(0, 2)]
discards: [ 0  8  9 12 39 64]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 194.6993 - loglik: -1.9150e+02 - logprior: -2.9737e+00
Epoch 2/2
19/19 - 2s - loss: 191.8960 - loglik: -1.9048e+02 - logprior: -1.1681e+00
Fitted a model with MAP estimate = -190.7667
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 195.5107 - loglik: -1.9174e+02 - logprior: -3.4986e+00
Epoch 2/10
19/19 - 2s - loss: 192.1322 - loglik: -1.9054e+02 - logprior: -1.3317e+00
Epoch 3/10
19/19 - 2s - loss: 191.4280 - loglik: -1.8994e+02 - logprior: -1.2053e+00
Epoch 4/10
19/19 - 2s - loss: 191.2958 - loglik: -1.8986e+02 - logprior: -1.1540e+00
Epoch 5/10
19/19 - 2s - loss: 191.0031 - loglik: -1.8962e+02 - logprior: -1.1070e+00
Epoch 6/10
19/19 - 2s - loss: 191.1337 - loglik: -1.8978e+02 - logprior: -1.0862e+00
Fitted a model with MAP estimate = -190.6152
Time for alignment: 79.5917
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 258.0688 - loglik: -2.5482e+02 - logprior: -3.1600e+00
Epoch 2/10
19/19 - 2s - loss: 224.0955 - loglik: -2.2224e+02 - logprior: -1.3620e+00
Epoch 3/10
19/19 - 2s - loss: 209.0327 - loglik: -2.0684e+02 - logprior: -1.5133e+00
Epoch 4/10
19/19 - 2s - loss: 205.5154 - loglik: -2.0364e+02 - logprior: -1.4558e+00
Epoch 5/10
19/19 - 2s - loss: 204.3073 - loglik: -2.0250e+02 - logprior: -1.4475e+00
Epoch 6/10
19/19 - 2s - loss: 203.7782 - loglik: -2.0206e+02 - logprior: -1.4199e+00
Epoch 7/10
19/19 - 2s - loss: 203.0163 - loglik: -2.0138e+02 - logprior: -1.3597e+00
Epoch 8/10
19/19 - 2s - loss: 202.4904 - loglik: -2.0087e+02 - logprior: -1.3531e+00
Epoch 9/10
19/19 - 2s - loss: 201.8467 - loglik: -2.0024e+02 - logprior: -1.3503e+00
Epoch 10/10
19/19 - 2s - loss: 201.5930 - loglik: -2.0003e+02 - logprior: -1.3441e+00
Fitted a model with MAP estimate = -200.8623
expansions: [(8, 3), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (27, 1), (29, 1), (32, 1), (37, 2), (38, 1), (54, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 202.7818 - loglik: -1.9844e+02 - logprior: -4.1261e+00
Epoch 2/2
19/19 - 2s - loss: 193.9965 - loglik: -1.9169e+02 - logprior: -2.0834e+00
Fitted a model with MAP estimate = -191.9760
expansions: [(0, 2)]
discards: [ 0  7  8 48]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 193.5551 - loglik: -1.9032e+02 - logprior: -2.9832e+00
Epoch 2/2
19/19 - 3s - loss: 190.7236 - loglik: -1.8928e+02 - logprior: -1.1720e+00
Fitted a model with MAP estimate = -189.6117
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 194.3570 - loglik: -1.9057e+02 - logprior: -3.4862e+00
Epoch 2/10
19/19 - 2s - loss: 191.0101 - loglik: -1.8936e+02 - logprior: -1.3403e+00
Epoch 3/10
19/19 - 3s - loss: 190.4840 - loglik: -1.8895e+02 - logprior: -1.2152e+00
Epoch 4/10
19/19 - 2s - loss: 190.1824 - loglik: -1.8871e+02 - logprior: -1.1632e+00
Epoch 5/10
19/19 - 2s - loss: 190.0780 - loglik: -1.8865e+02 - logprior: -1.1225e+00
Epoch 6/10
19/19 - 2s - loss: 189.8707 - loglik: -1.8848e+02 - logprior: -1.0944e+00
Epoch 7/10
19/19 - 3s - loss: 189.8249 - loglik: -1.8846e+02 - logprior: -1.0684e+00
Epoch 8/10
19/19 - 3s - loss: 190.0822 - loglik: -1.8874e+02 - logprior: -1.0483e+00
Fitted a model with MAP estimate = -189.4622
Time for alignment: 82.8163
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 258.0835 - loglik: -2.5484e+02 - logprior: -3.1649e+00
Epoch 2/10
19/19 - 2s - loss: 227.1183 - loglik: -2.2533e+02 - logprior: -1.3779e+00
Epoch 3/10
19/19 - 2s - loss: 211.5763 - loglik: -2.0948e+02 - logprior: -1.5134e+00
Epoch 4/10
19/19 - 2s - loss: 206.6352 - loglik: -2.0471e+02 - logprior: -1.4881e+00
Epoch 5/10
19/19 - 2s - loss: 205.0441 - loglik: -2.0325e+02 - logprior: -1.4267e+00
Epoch 6/10
19/19 - 2s - loss: 203.7958 - loglik: -2.0214e+02 - logprior: -1.3637e+00
Epoch 7/10
19/19 - 2s - loss: 203.7888 - loglik: -2.0220e+02 - logprior: -1.3352e+00
Epoch 8/10
19/19 - 2s - loss: 203.2637 - loglik: -2.0171e+02 - logprior: -1.3164e+00
Epoch 9/10
19/19 - 2s - loss: 203.1714 - loglik: -2.0165e+02 - logprior: -1.3129e+00
Epoch 10/10
19/19 - 2s - loss: 203.3358 - loglik: -2.0183e+02 - logprior: -1.3130e+00
Fitted a model with MAP estimate = -202.5338
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (24, 1), (30, 2), (32, 1), (37, 2), (38, 1), (50, 1), (55, 1), (61, 2), (64, 1), (72, 1), (73, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 204.4408 - loglik: -2.0015e+02 - logprior: -4.1075e+00
Epoch 2/2
19/19 - 3s - loss: 194.5491 - loglik: -1.9223e+02 - logprior: -2.1142e+00
Fitted a model with MAP estimate = -192.4176
expansions: [(0, 2)]
discards: [ 0  7 37 47 77]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 193.8109 - loglik: -1.9059e+02 - logprior: -2.9771e+00
Epoch 2/2
19/19 - 2s - loss: 190.6827 - loglik: -1.8925e+02 - logprior: -1.1647e+00
Fitted a model with MAP estimate = -189.7504
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 194.4836 - loglik: -1.9069e+02 - logprior: -3.5003e+00
Epoch 2/10
19/19 - 2s - loss: 191.2562 - loglik: -1.8963e+02 - logprior: -1.3324e+00
Epoch 3/10
19/19 - 3s - loss: 190.3999 - loglik: -1.8889e+02 - logprior: -1.2072e+00
Epoch 4/10
19/19 - 2s - loss: 190.2207 - loglik: -1.8876e+02 - logprior: -1.1561e+00
Epoch 5/10
19/19 - 3s - loss: 190.0641 - loglik: -1.8866e+02 - logprior: -1.1127e+00
Epoch 6/10
19/19 - 2s - loss: 190.1679 - loglik: -1.8879e+02 - logprior: -1.0884e+00
Fitted a model with MAP estimate = -189.6173
Time for alignment: 78.9490
Computed alignments with likelihoods: ['-190.6152', '-189.4622', '-189.6173']
Best model has likelihood: -189.4622
time for generating output: 0.1620
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF03129.projection.fasta
SP score = 0.9675360050334154
Training of 3 independent models on file PF13378.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f32219aef40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f321065e610>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f31f4576490>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b9d7c1070>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b9d7c1ca0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3211c47340>, <__main__.SimpleDirichletPrior object at 0x7f38445e9940>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3221ec9790>

Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 566.8615 - loglik: -5.6450e+02 - logprior: -1.8582e+00
Epoch 2/10
39/39 - 13s - loss: 486.8879 - loglik: -4.8408e+02 - logprior: -1.5626e+00
Epoch 3/10
39/39 - 13s - loss: 475.1044 - loglik: -4.7223e+02 - logprior: -1.6911e+00
Epoch 4/10
39/39 - 13s - loss: 473.0413 - loglik: -4.7034e+02 - logprior: -1.6320e+00
Epoch 5/10
39/39 - 13s - loss: 472.1764 - loglik: -4.6957e+02 - logprior: -1.6343e+00
Epoch 6/10
39/39 - 13s - loss: 471.4265 - loglik: -4.6890e+02 - logprior: -1.6362e+00
Epoch 7/10
39/39 - 13s - loss: 471.0754 - loglik: -4.6858e+02 - logprior: -1.6414e+00
Epoch 8/10
39/39 - 13s - loss: 470.4513 - loglik: -4.6795e+02 - logprior: -1.6473e+00
Epoch 9/10
39/39 - 13s - loss: 470.3318 - loglik: -4.6784e+02 - logprior: -1.6536e+00
Epoch 10/10
39/39 - 13s - loss: 469.7708 - loglik: -4.6729e+02 - logprior: -1.6632e+00
Fitted a model with MAP estimate = -466.5124
expansions: [(0, 3), (11, 1), (18, 4), (20, 1), (35, 2), (45, 2), (46, 1), (49, 1), (51, 1), (65, 1), (68, 1), (72, 1), (75, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (92, 1), (102, 1), (107, 1), (113, 1), (116, 1), (119, 1), (125, 1), (126, 1), (132, 1), (141, 3), (146, 2), (148, 2), (149, 1), (150, 2), (153, 1), (157, 1), (158, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 221 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 458.1309 - loglik: -4.5427e+02 - logprior: -3.0546e+00
Epoch 2/2
39/39 - 17s - loss: 448.1972 - loglik: -4.4587e+02 - logprior: -1.2680e+00
Fitted a model with MAP estimate = -443.3624
expansions: []
discards: [ 44  57 113]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 218 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 447.8560 - loglik: -4.4444e+02 - logprior: -2.2352e+00
Epoch 2/2
39/39 - 18s - loss: 446.3419 - loglik: -4.4410e+02 - logprior: -1.0593e+00
Fitted a model with MAP estimate = -441.8210
expansions: []
discards: [186]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 217 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 444.3563 - loglik: -4.4113e+02 - logprior: -2.0276e+00
Epoch 2/10
39/39 - 18s - loss: 443.6245 - loglik: -4.4161e+02 - logprior: -8.4677e-01
Epoch 3/10
39/39 - 18s - loss: 442.4156 - loglik: -4.4055e+02 - logprior: -6.9798e-01
Epoch 4/10
39/39 - 18s - loss: 441.9278 - loglik: -4.4029e+02 - logprior: -5.8979e-01
Epoch 5/10
39/39 - 18s - loss: 441.6202 - loglik: -4.4015e+02 - logprior: -4.8591e-01
Epoch 6/10
39/39 - 18s - loss: 441.1151 - loglik: -4.3981e+02 - logprior: -3.7630e-01
Epoch 7/10
39/39 - 18s - loss: 441.2508 - loglik: -4.4007e+02 - logprior: -2.7697e-01
Fitted a model with MAP estimate = -439.6848
Time for alignment: 423.0306
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 566.3458 - loglik: -5.6396e+02 - logprior: -1.8730e+00
Epoch 2/10
39/39 - 13s - loss: 486.0280 - loglik: -4.8322e+02 - logprior: -1.5230e+00
Epoch 3/10
39/39 - 13s - loss: 476.1977 - loglik: -4.7333e+02 - logprior: -1.5832e+00
Epoch 4/10
39/39 - 13s - loss: 474.2234 - loglik: -4.7150e+02 - logprior: -1.5560e+00
Epoch 5/10
39/39 - 13s - loss: 473.2800 - loglik: -4.7067e+02 - logprior: -1.5566e+00
Epoch 6/10
39/39 - 13s - loss: 472.6738 - loglik: -4.7013e+02 - logprior: -1.5704e+00
Epoch 7/10
39/39 - 12s - loss: 471.9174 - loglik: -4.6941e+02 - logprior: -1.5880e+00
Epoch 8/10
39/39 - 12s - loss: 471.4493 - loglik: -4.6895e+02 - logprior: -1.5948e+00
Epoch 9/10
39/39 - 12s - loss: 470.8582 - loglik: -4.6838e+02 - logprior: -1.5997e+00
Epoch 10/10
39/39 - 12s - loss: 470.7015 - loglik: -4.6823e+02 - logprior: -1.6068e+00
Fitted a model with MAP estimate = -466.8096
expansions: [(0, 3), (19, 2), (20, 2), (21, 1), (26, 1), (36, 1), (45, 2), (46, 1), (49, 1), (51, 1), (65, 1), (68, 1), (72, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (92, 1), (102, 1), (107, 1), (108, 1), (112, 1), (116, 1), (124, 1), (125, 1), (126, 1), (148, 6), (150, 1), (153, 1), (157, 1), (158, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 218 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 458.3641 - loglik: -4.5442e+02 - logprior: -3.0548e+00
Epoch 2/2
39/39 - 16s - loss: 448.3932 - loglik: -4.4607e+02 - logprior: -1.2588e+00
Fitted a model with MAP estimate = -443.7358
expansions: [(185, 3)]
discards: [ 25  56  95 113]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 217 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 447.8762 - loglik: -4.4458e+02 - logprior: -2.1706e+00
Epoch 2/2
39/39 - 16s - loss: 446.2747 - loglik: -4.4414e+02 - logprior: -1.0014e+00
Fitted a model with MAP estimate = -441.7843
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 217 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 444.3279 - loglik: -4.4120e+02 - logprior: -2.0058e+00
Epoch 2/10
39/39 - 16s - loss: 443.4495 - loglik: -4.4148e+02 - logprior: -8.2680e-01
Epoch 3/10
39/39 - 16s - loss: 442.5234 - loglik: -4.4073e+02 - logprior: -6.7623e-01
Epoch 4/10
39/39 - 16s - loss: 441.6346 - loglik: -4.4006e+02 - logprior: -5.6873e-01
Epoch 5/10
39/39 - 16s - loss: 441.8799 - loglik: -4.4045e+02 - logprior: -4.6132e-01
Fitted a model with MAP estimate = -440.2184
Time for alignment: 359.7523
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 565.9178 - loglik: -5.6353e+02 - logprior: -1.8808e+00
Epoch 2/10
39/39 - 12s - loss: 488.3322 - loglik: -4.8568e+02 - logprior: -1.4987e+00
Epoch 3/10
39/39 - 12s - loss: 477.0490 - loglik: -4.7436e+02 - logprior: -1.5955e+00
Epoch 4/10
39/39 - 12s - loss: 474.5434 - loglik: -4.7189e+02 - logprior: -1.5649e+00
Epoch 5/10
39/39 - 12s - loss: 473.4931 - loglik: -4.7094e+02 - logprior: -1.5610e+00
Epoch 6/10
39/39 - 12s - loss: 472.8258 - loglik: -4.7031e+02 - logprior: -1.5671e+00
Epoch 7/10
39/39 - 12s - loss: 472.2285 - loglik: -4.6971e+02 - logprior: -1.5720e+00
Epoch 8/10
39/39 - 12s - loss: 471.5462 - loglik: -4.6905e+02 - logprior: -1.5776e+00
Epoch 9/10
39/39 - 13s - loss: 471.3915 - loglik: -4.6889e+02 - logprior: -1.5808e+00
Epoch 10/10
39/39 - 13s - loss: 470.7915 - loglik: -4.6830e+02 - logprior: -1.5888e+00
Fitted a model with MAP estimate = -467.2701
expansions: [(0, 3), (11, 1), (18, 4), (20, 1), (35, 2), (45, 2), (46, 1), (49, 1), (50, 1), (65, 1), (68, 1), (72, 1), (75, 1), (84, 1), (85, 1), (86, 1), (88, 1), (89, 2), (92, 1), (102, 1), (107, 1), (108, 1), (116, 1), (125, 3), (132, 1), (140, 2), (141, 4), (146, 2), (149, 1), (150, 1), (151, 2), (153, 1), (159, 1), (161, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 223 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 458.2183 - loglik: -4.5432e+02 - logprior: -3.0252e+00
Epoch 2/2
39/39 - 17s - loss: 447.7291 - loglik: -4.4538e+02 - logprior: -1.2561e+00
Fitted a model with MAP estimate = -442.9421
expansions: []
discards: [ 24  44  57 113 176 179 180 192]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 215 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 447.9340 - loglik: -4.4462e+02 - logprior: -2.1764e+00
Epoch 2/2
39/39 - 17s - loss: 446.4363 - loglik: -4.4429e+02 - logprior: -1.0020e+00
Fitted a model with MAP estimate = -442.0857
expansions: [(189, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 216 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 444.2819 - loglik: -4.4113e+02 - logprior: -2.0017e+00
Epoch 2/10
39/39 - 18s - loss: 443.5033 - loglik: -4.4157e+02 - logprior: -8.2450e-01
Epoch 3/10
39/39 - 18s - loss: 442.2498 - loglik: -4.4046e+02 - logprior: -6.8662e-01
Epoch 4/10
39/39 - 18s - loss: 441.8153 - loglik: -4.4026e+02 - logprior: -5.7415e-01
Epoch 5/10
39/39 - 18s - loss: 441.6877 - loglik: -4.4029e+02 - logprior: -4.6558e-01
Epoch 6/10
39/39 - 18s - loss: 441.5214 - loglik: -4.4028e+02 - logprior: -3.5658e-01
Epoch 7/10
39/39 - 18s - loss: 440.8965 - loglik: -4.3979e+02 - logprior: -2.4065e-01
Epoch 8/10
39/39 - 18s - loss: 440.8236 - loglik: -4.3986e+02 - logprior: -1.2992e-01
Epoch 9/10
39/39 - 18s - loss: 440.3766 - loglik: -4.3952e+02 - logprior: -3.4407e-02
Epoch 10/10
39/39 - 18s - loss: 440.6276 - loglik: -4.3988e+02 - logprior: 0.0716
Fitted a model with MAP estimate = -439.2398
Time for alignment: 461.4843
Computed alignments with likelihoods: ['-439.6848', '-440.2184', '-439.2398']
Best model has likelihood: -439.2398
time for generating output: 0.3076
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13378.projection.fasta
SP score = 0.7191265585744859
Training of 3 independent models on file PF00084.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3864615e50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b94f97ac0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b94f979d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b94f9adf0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b94f9acd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3210934250>, <__main__.SimpleDirichletPrior object at 0x7f31fc72f070>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3bbf43a4c0>

Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 164.8644 - loglik: -1.6161e+02 - logprior: -3.2190e+00
Epoch 2/10
19/19 - 1s - loss: 136.7777 - loglik: -1.3534e+02 - logprior: -1.4237e+00
Epoch 3/10
19/19 - 1s - loss: 128.2372 - loglik: -1.2650e+02 - logprior: -1.5428e+00
Epoch 4/10
19/19 - 1s - loss: 126.3912 - loglik: -1.2468e+02 - logprior: -1.4033e+00
Epoch 5/10
19/19 - 1s - loss: 126.0067 - loglik: -1.2435e+02 - logprior: -1.4000e+00
Epoch 6/10
19/19 - 1s - loss: 125.7926 - loglik: -1.2414e+02 - logprior: -1.3759e+00
Epoch 7/10
19/19 - 1s - loss: 125.3042 - loglik: -1.2366e+02 - logprior: -1.3648e+00
Epoch 8/10
19/19 - 1s - loss: 125.4106 - loglik: -1.2377e+02 - logprior: -1.3585e+00
Fitted a model with MAP estimate = -124.9470
expansions: [(11, 5), (12, 2), (13, 1), (18, 1), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 131.5847 - loglik: -1.2709e+02 - logprior: -4.1772e+00
Epoch 2/2
19/19 - 1s - loss: 125.5066 - loglik: -1.2287e+02 - logprior: -2.2745e+00
Fitted a model with MAP estimate = -123.0218
expansions: []
discards: [12 13 39 40 49]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 124.9330 - loglik: -1.2117e+02 - logprior: -3.4145e+00
Epoch 2/2
19/19 - 1s - loss: 121.6824 - loglik: -1.1984e+02 - logprior: -1.4868e+00
Fitted a model with MAP estimate = -121.0015
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 123.6544 - loglik: -1.2001e+02 - logprior: -3.2712e+00
Epoch 2/10
19/19 - 1s - loss: 121.6058 - loglik: -1.1975e+02 - logprior: -1.4784e+00
Epoch 3/10
19/19 - 1s - loss: 121.0916 - loglik: -1.1931e+02 - logprior: -1.3756e+00
Epoch 4/10
19/19 - 1s - loss: 121.1710 - loglik: -1.1944e+02 - logprior: -1.3208e+00
Fitted a model with MAP estimate = -120.4494
Time for alignment: 48.2077
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 164.9531 - loglik: -1.6170e+02 - logprior: -3.2217e+00
Epoch 2/10
19/19 - 1s - loss: 135.9677 - loglik: -1.3451e+02 - logprior: -1.4306e+00
Epoch 3/10
19/19 - 1s - loss: 127.9261 - loglik: -1.2615e+02 - logprior: -1.5325e+00
Epoch 4/10
19/19 - 1s - loss: 126.3925 - loglik: -1.2473e+02 - logprior: -1.3887e+00
Epoch 5/10
19/19 - 1s - loss: 126.0020 - loglik: -1.2434e+02 - logprior: -1.4025e+00
Epoch 6/10
19/19 - 1s - loss: 125.6965 - loglik: -1.2405e+02 - logprior: -1.3771e+00
Epoch 7/10
19/19 - 1s - loss: 125.5928 - loglik: -1.2394e+02 - logprior: -1.3646e+00
Epoch 8/10
19/19 - 1s - loss: 125.2495 - loglik: -1.2359e+02 - logprior: -1.3582e+00
Epoch 9/10
19/19 - 1s - loss: 125.4016 - loglik: -1.2374e+02 - logprior: -1.3521e+00
Fitted a model with MAP estimate = -124.8848
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (28, 2), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 132.0509 - loglik: -1.2755e+02 - logprior: -4.1863e+00
Epoch 2/2
19/19 - 1s - loss: 125.4953 - loglik: -1.2281e+02 - logprior: -2.3141e+00
Fitted a model with MAP estimate = -123.0824
expansions: [(0, 2)]
discards: [ 0 13 14 16 17 37 42 43 52]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 125.4416 - loglik: -1.2205e+02 - logprior: -3.0684e+00
Epoch 2/2
19/19 - 1s - loss: 121.3989 - loglik: -1.1978e+02 - logprior: -1.2844e+00
Fitted a model with MAP estimate = -120.4401
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 126.3683 - loglik: -1.2203e+02 - logprior: -3.9499e+00
Epoch 2/10
19/19 - 2s - loss: 121.7464 - loglik: -1.1980e+02 - logprior: -1.5565e+00
Epoch 3/10
19/19 - 1s - loss: 121.2001 - loglik: -1.1941e+02 - logprior: -1.3811e+00
Epoch 4/10
19/19 - 1s - loss: 120.9300 - loglik: -1.1919e+02 - logprior: -1.3251e+00
Epoch 5/10
19/19 - 1s - loss: 120.7433 - loglik: -1.1903e+02 - logprior: -1.2863e+00
Epoch 6/10
19/19 - 1s - loss: 120.6758 - loglik: -1.1895e+02 - logprior: -1.2704e+00
Epoch 7/10
19/19 - 2s - loss: 120.7092 - loglik: -1.1899e+02 - logprior: -1.2536e+00
Fitted a model with MAP estimate = -119.9865
Time for alignment: 54.4034
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 164.9485 - loglik: -1.6169e+02 - logprior: -3.2209e+00
Epoch 2/10
19/19 - 1s - loss: 136.4179 - loglik: -1.3496e+02 - logprior: -1.4238e+00
Epoch 3/10
19/19 - 1s - loss: 128.2386 - loglik: -1.2643e+02 - logprior: -1.5532e+00
Epoch 4/10
19/19 - 1s - loss: 126.9812 - loglik: -1.2529e+02 - logprior: -1.4071e+00
Epoch 5/10
19/19 - 1s - loss: 126.5131 - loglik: -1.2484e+02 - logprior: -1.4010e+00
Epoch 6/10
19/19 - 1s - loss: 126.3880 - loglik: -1.2474e+02 - logprior: -1.3710e+00
Epoch 7/10
19/19 - 1s - loss: 126.1496 - loglik: -1.2449e+02 - logprior: -1.3654e+00
Epoch 8/10
19/19 - 1s - loss: 125.7649 - loglik: -1.2411e+02 - logprior: -1.3552e+00
Epoch 9/10
19/19 - 1s - loss: 125.7179 - loglik: -1.2407e+02 - logprior: -1.3468e+00
Epoch 10/10
19/19 - 1s - loss: 125.9070 - loglik: -1.2425e+02 - logprior: -1.3463e+00
Fitted a model with MAP estimate = -125.3986
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (18, 1), (30, 1), (34, 3), (36, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 131.2602 - loglik: -1.2676e+02 - logprior: -4.1796e+00
Epoch 2/2
19/19 - 1s - loss: 125.3438 - loglik: -1.2271e+02 - logprior: -2.2671e+00
Fitted a model with MAP estimate = -123.1291
expansions: [(0, 2)]
discards: [ 0 12 13 16 17 46 50]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 125.3756 - loglik: -1.2198e+02 - logprior: -3.0631e+00
Epoch 2/2
19/19 - 1s - loss: 121.2946 - loglik: -1.1968e+02 - logprior: -1.2805e+00
Fitted a model with MAP estimate = -120.4102
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 126.3096 - loglik: -1.2198e+02 - logprior: -3.9424e+00
Epoch 2/10
19/19 - 1s - loss: 121.7022 - loglik: -1.1975e+02 - logprior: -1.5525e+00
Epoch 3/10
19/19 - 1s - loss: 121.1477 - loglik: -1.1935e+02 - logprior: -1.3829e+00
Epoch 4/10
19/19 - 1s - loss: 120.8455 - loglik: -1.1911e+02 - logprior: -1.3220e+00
Epoch 5/10
19/19 - 1s - loss: 120.7961 - loglik: -1.1907e+02 - logprior: -1.2856e+00
Epoch 6/10
19/19 - 1s - loss: 120.8323 - loglik: -1.1910e+02 - logprior: -1.2744e+00
Fitted a model with MAP estimate = -120.0666
Time for alignment: 51.7441
Computed alignments with likelihoods: ['-120.4494', '-119.9865', '-120.0666']
Best model has likelihood: -119.9865
time for generating output: 0.1168
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00084.projection.fasta
SP score = 0.9036144578313253
Training of 3 independent models on file PF00232.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f31fd0ef5e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f31fcb15a90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3218f76a60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3221f57640>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3211c49100>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f31fc8827f0>, <__main__.SimpleDirichletPrior object at 0x7f380416e1f0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f37fe7830d0>

Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 59s - loss: 903.9150 - loglik: -9.0208e+02 - logprior: -1.7277e+00
Epoch 2/10
39/39 - 56s - loss: 716.7878 - loglik: -7.1425e+02 - logprior: -1.8508e+00
Epoch 3/10
39/39 - 55s - loss: 702.2177 - loglik: -6.9934e+02 - logprior: -1.8715e+00
Epoch 4/10
39/39 - 55s - loss: 700.3390 - loglik: -6.9750e+02 - logprior: -1.7823e+00
Epoch 5/10
39/39 - 56s - loss: 698.1140 - loglik: -6.9527e+02 - logprior: -1.7935e+00
Epoch 6/10
39/39 - 56s - loss: 697.9815 - loglik: -6.9514e+02 - logprior: -1.8212e+00
Epoch 7/10
39/39 - 56s - loss: 697.2220 - loglik: -6.9440e+02 - logprior: -1.8291e+00
Epoch 8/10
39/39 - 58s - loss: 695.7803 - loglik: -6.9299e+02 - logprior: -1.8312e+00
Epoch 9/10
39/39 - 62s - loss: 696.8087 - loglik: -6.9406e+02 - logprior: -1.8368e+00
Fitted a model with MAP estimate = -694.3268
expansions: [(0, 3), (43, 1), (95, 1), (131, 1), (134, 1), (135, 1), (161, 1), (162, 1), (163, 1), (167, 1), (173, 8), (174, 1), (175, 1), (176, 1), (187, 2), (188, 5), (189, 1), (190, 1), (193, 1), (194, 1), (195, 1), (196, 2), (197, 1), (198, 1), (200, 1), (201, 1), (204, 1), (209, 1), (213, 1), (215, 1), (216, 1), (220, 3), (221, 4), (222, 2), (224, 1), (225, 2), (226, 3), (227, 2), (228, 1), (239, 1), (243, 1), (244, 3), (245, 2), (247, 2), (248, 1), (249, 1), (250, 4), (252, 1), (254, 1), (270, 1), (272, 1), (286, 1), (288, 1), (289, 3), (291, 1), (302, 1), (304, 2), (305, 2), (313, 1), (327, 1), (329, 1), (339, 1), (349, 1), (355, 6)]
discards: [  2 127]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 458 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 117s - loss: 666.0959 - loglik: -6.6221e+02 - logprior: -2.9660e+00
Epoch 2/2
39/39 - 112s - loss: 649.1823 - loglik: -6.4637e+02 - logprior: -1.5454e+00
Fitted a model with MAP estimate = -644.8787
expansions: [(323, 1)]
discards: [  2 186 187 212 265 313 314 315 456]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 450 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 86s - loss: 651.5104 - loglik: -6.4798e+02 - logprior: -2.0075e+00
Epoch 2/2
39/39 - 82s - loss: 649.2842 - loglik: -6.4670e+02 - logprior: -1.0179e+00
Fitted a model with MAP estimate = -645.4932
expansions: [(312, 1)]
discards: [320]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 450 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 92s - loss: 649.1338 - loglik: -6.4563e+02 - logprior: -1.8553e+00
Epoch 2/10
39/39 - 91s - loss: 647.7870 - loglik: -6.4547e+02 - logprior: -6.9157e-01
Epoch 3/10
39/39 - 91s - loss: 646.5816 - loglik: -6.4432e+02 - logprior: -5.5684e-01
Epoch 4/10
39/39 - 92s - loss: 645.4042 - loglik: -6.4358e+02 - logprior: -3.3137e-01
Epoch 5/10
39/39 - 89s - loss: 645.3457 - loglik: -6.4383e+02 - logprior: -8.7264e-02
Epoch 6/10
39/39 - 88s - loss: 644.0218 - loglik: -6.4278e+02 - logprior: 0.1061
Epoch 7/10
39/39 - 87s - loss: 644.0713 - loglik: -6.4299e+02 - logprior: 0.1995
Fitted a model with MAP estimate = -641.9010
Time for alignment: 1950.8491
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 61s - loss: 903.5967 - loglik: -9.0177e+02 - logprior: -1.7114e+00
Epoch 2/10
39/39 - 60s - loss: 719.4084 - loglik: -7.1710e+02 - logprior: -1.6964e+00
Epoch 3/10
39/39 - 60s - loss: 702.5116 - loglik: -6.9945e+02 - logprior: -1.8996e+00
Epoch 4/10
39/39 - 60s - loss: 698.3149 - loglik: -6.9533e+02 - logprior: -1.8969e+00
Epoch 5/10
39/39 - 60s - loss: 698.1337 - loglik: -6.9517e+02 - logprior: -1.8902e+00
Epoch 6/10
39/39 - 60s - loss: 696.2626 - loglik: -6.9331e+02 - logprior: -1.9160e+00
Epoch 7/10
39/39 - 60s - loss: 695.5646 - loglik: -6.9263e+02 - logprior: -1.9313e+00
Epoch 8/10
39/39 - 60s - loss: 695.3461 - loglik: -6.9241e+02 - logprior: -1.9723e+00
Epoch 9/10
39/39 - 60s - loss: 695.3431 - loglik: -6.9246e+02 - logprior: -1.9452e+00
Epoch 10/10
39/39 - 61s - loss: 694.9401 - loglik: -6.9206e+02 - logprior: -1.9654e+00
Fitted a model with MAP estimate = -692.8905
expansions: [(0, 3), (38, 1), (65, 1), (131, 1), (133, 1), (134, 1), (135, 1), (161, 1), (163, 1), (174, 8), (175, 1), (176, 1), (186, 1), (188, 1), (189, 1), (190, 6), (191, 1), (194, 1), (195, 1), (196, 1), (197, 1), (199, 1), (200, 2), (201, 2), (202, 1), (205, 1), (210, 1), (214, 1), (217, 1), (222, 3), (223, 5), (226, 1), (227, 3), (228, 1), (229, 1), (230, 2), (231, 1), (242, 1), (244, 1), (245, 2), (246, 3), (247, 1), (249, 2), (250, 6), (252, 1), (255, 1), (271, 1), (272, 1), (273, 1), (286, 1), (288, 1), (289, 1), (290, 2), (292, 1), (293, 1), (301, 2), (303, 1), (304, 1), (305, 1), (313, 1), (326, 1), (330, 1), (339, 1), (349, 1), (355, 6)]
discards: [  1 127]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 459 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 91s - loss: 664.8505 - loglik: -6.6103e+02 - logprior: -2.9037e+00
Epoch 2/2
39/39 - 86s - loss: 646.7590 - loglik: -6.4424e+02 - logprior: -1.2822e+00
Fitted a model with MAP estimate = -642.9994
expansions: [(215, 1)]
discards: [  2 186 187 237 288 308 314 323 457]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 451 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 93s - loss: 648.4205 - loglik: -6.4477e+02 - logprior: -2.1917e+00
Epoch 2/2
39/39 - 92s - loss: 646.0561 - loglik: -6.4364e+02 - logprior: -9.5086e-01
Fitted a model with MAP estimate = -642.4401
expansions: []
discards: [214 215]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 449 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 99s - loss: 647.3649 - loglik: -6.4395e+02 - logprior: -1.8499e+00
Epoch 2/10
39/39 - 93s - loss: 646.0846 - loglik: -6.4391e+02 - logprior: -6.4234e-01
Epoch 3/10
39/39 - 86s - loss: 644.7025 - loglik: -6.4268e+02 - logprior: -4.3288e-01
Epoch 4/10
39/39 - 82s - loss: 644.1871 - loglik: -6.4255e+02 - logprior: -2.2352e-01
Epoch 5/10
39/39 - 84s - loss: 643.7250 - loglik: -6.4221e+02 - logprior: -1.5474e-01
Epoch 6/10
39/39 - 84s - loss: 642.7076 - loglik: -6.4151e+02 - logprior: 0.1106
Epoch 7/10
39/39 - 85s - loss: 642.3856 - loglik: -6.4146e+02 - logprior: 0.3445
Epoch 8/10
39/39 - 87s - loss: 642.3412 - loglik: -6.4165e+02 - logprior: 0.5560
Epoch 9/10
39/39 - 86s - loss: 641.2404 - loglik: -6.4080e+02 - logprior: 0.7761
Epoch 10/10
39/39 - 84s - loss: 640.7551 - loglik: -6.4054e+02 - logprior: 0.9905
Fitted a model with MAP estimate = -639.4662
Time for alignment: 2227.9665
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 60s - loss: 901.8075 - loglik: -8.9992e+02 - logprior: -1.7759e+00
Epoch 2/10
39/39 - 58s - loss: 715.8271 - loglik: -7.1275e+02 - logprior: -2.1105e+00
Epoch 3/10
39/39 - 58s - loss: 701.7388 - loglik: -6.9827e+02 - logprior: -2.3969e+00
Epoch 4/10
39/39 - 58s - loss: 698.7104 - loglik: -6.9525e+02 - logprior: -2.4451e+00
Epoch 5/10
39/39 - 58s - loss: 697.1981 - loglik: -6.9368e+02 - logprior: -2.5332e+00
Epoch 6/10
39/39 - 58s - loss: 696.8492 - loglik: -6.9325e+02 - logprior: -2.6323e+00
Epoch 7/10
39/39 - 58s - loss: 695.1773 - loglik: -6.9168e+02 - logprior: -2.5438e+00
Epoch 8/10
39/39 - 59s - loss: 694.7973 - loglik: -6.9134e+02 - logprior: -2.5493e+00
Epoch 9/10
39/39 - 62s - loss: 694.9579 - loglik: -6.9147e+02 - logprior: -2.5940e+00
Fitted a model with MAP estimate = -692.9130
expansions: [(37, 1), (41, 1), (42, 1), (105, 1), (133, 1), (138, 1), (162, 1), (164, 1), (170, 1), (174, 8), (175, 1), (176, 1), (186, 1), (188, 1), (189, 1), (190, 6), (191, 1), (194, 1), (195, 1), (196, 1), (197, 1), (199, 1), (200, 1), (202, 1), (203, 1), (206, 1), (208, 1), (210, 1), (217, 1), (218, 1), (222, 3), (223, 7), (225, 1), (226, 2), (227, 3), (228, 2), (229, 1), (240, 1), (244, 2), (245, 2), (246, 3), (248, 2), (249, 5), (250, 2), (251, 1), (253, 1), (271, 1), (272, 1), (285, 1), (286, 2), (287, 1), (288, 2), (290, 1), (291, 1), (300, 1), (302, 1), (303, 1), (304, 1), (306, 1), (325, 2), (328, 1), (345, 1), (347, 1), (349, 2), (351, 1)]
discards: [126]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 457 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 101s - loss: 664.7035 - loglik: -6.6132e+02 - logprior: -2.5057e+00
Epoch 2/2
39/39 - 99s - loss: 647.8225 - loglik: -6.4526e+02 - logprior: -1.2945e+00
Fitted a model with MAP estimate = -644.2410
expansions: [(214, 1), (279, 1)]
discards: [185 186 264 285 306]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 454 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 108s - loss: 648.0845 - loglik: -6.4444e+02 - logprior: -2.1982e+00
Epoch 2/2
39/39 - 100s - loss: 645.7244 - loglik: -6.4335e+02 - logprior: -8.9939e-01
Fitted a model with MAP estimate = -642.1067
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 454 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 105s - loss: 645.2546 - loglik: -6.4188e+02 - logprior: -1.8647e+00
Epoch 2/10
39/39 - 106s - loss: 644.3762 - loglik: -6.4224e+02 - logprior: -6.5950e-01
Epoch 3/10
39/39 - 106s - loss: 643.3306 - loglik: -6.4123e+02 - logprior: -5.2213e-01
Epoch 4/10
39/39 - 106s - loss: 642.1865 - loglik: -6.4058e+02 - logprior: -1.9625e-01
Epoch 5/10
39/39 - 101s - loss: 641.2892 - loglik: -6.3986e+02 - logprior: -4.7320e-02
Epoch 6/10
39/39 - 100s - loss: 641.1039 - loglik: -6.3980e+02 - logprior: 0.0306
Epoch 7/10
39/39 - 102s - loss: 641.2725 - loglik: -6.4025e+02 - logprior: 0.2787
Fitted a model with MAP estimate = -638.3170
Time for alignment: 2128.1169
Computed alignments with likelihoods: ['-641.9010', '-639.4662', '-638.3170']
Best model has likelihood: -638.3170
time for generating output: 0.4514
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00232.projection.fasta
SP score = 0.8346316492321388
Training of 3 independent models on file PF13522.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3204192670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b94c9bfd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3204ba81c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3204ba8070>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3204ba8160>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3210fcafa0>, <__main__.SimpleDirichletPrior object at 0x7f3221b5bdf0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f37fddd0b80>

Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 409.3148 - loglik: -4.0630e+02 - logprior: -2.9962e+00
Epoch 2/10
19/19 - 5s - loss: 325.0309 - loglik: -3.2390e+02 - logprior: -1.0828e+00
Epoch 3/10
19/19 - 5s - loss: 294.9465 - loglik: -2.9319e+02 - logprior: -1.3470e+00
Epoch 4/10
19/19 - 5s - loss: 289.3322 - loglik: -2.8745e+02 - logprior: -1.3046e+00
Epoch 5/10
19/19 - 5s - loss: 288.2249 - loglik: -2.8643e+02 - logprior: -1.2585e+00
Epoch 6/10
19/19 - 5s - loss: 287.4420 - loglik: -2.8565e+02 - logprior: -1.2275e+00
Epoch 7/10
19/19 - 5s - loss: 286.7346 - loglik: -2.8499e+02 - logprior: -1.2073e+00
Epoch 8/10
19/19 - 5s - loss: 286.3292 - loglik: -2.8458e+02 - logprior: -1.1991e+00
Epoch 9/10
19/19 - 5s - loss: 286.1028 - loglik: -2.8436e+02 - logprior: -1.1951e+00
Epoch 10/10
19/19 - 5s - loss: 286.3954 - loglik: -2.8466e+02 - logprior: -1.1941e+00
Fitted a model with MAP estimate = -285.0170
expansions: [(0, 6), (7, 3), (8, 2), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (55, 2), (56, 1), (58, 2), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (97, 1), (98, 1), (100, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 158 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 284.3655 - loglik: -2.7972e+02 - logprior: -4.1272e+00
Epoch 2/2
19/19 - 6s - loss: 272.4308 - loglik: -2.7048e+02 - logprior: -1.3565e+00
Fitted a model with MAP estimate = -270.1618
expansions: []
discards: [  1   2   3   5  21  73  78 100 101 146]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 275.5450 - loglik: -2.7205e+02 - logprior: -2.8437e+00
Epoch 2/2
19/19 - 6s - loss: 272.5836 - loglik: -2.7098e+02 - logprior: -9.9918e-01
Fitted a model with MAP estimate = -271.1641
expansions: [(0, 4)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 276.2235 - loglik: -2.7082e+02 - logprior: -4.7512e+00
Epoch 2/10
19/19 - 6s - loss: 271.6631 - loglik: -2.6974e+02 - logprior: -1.2804e+00
Epoch 3/10
19/19 - 6s - loss: 270.5402 - loglik: -2.6895e+02 - logprior: -9.2385e-01
Epoch 4/10
19/19 - 6s - loss: 270.1384 - loglik: -2.6869e+02 - logprior: -8.0613e-01
Epoch 5/10
19/19 - 6s - loss: 269.7016 - loglik: -2.6834e+02 - logprior: -7.2199e-01
Epoch 6/10
19/19 - 6s - loss: 269.9435 - loglik: -2.6864e+02 - logprior: -6.6720e-01
Fitted a model with MAP estimate = -268.8976
Time for alignment: 155.3601
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 408.8165 - loglik: -4.0581e+02 - logprior: -2.9905e+00
Epoch 2/10
19/19 - 4s - loss: 325.7237 - loglik: -3.2457e+02 - logprior: -1.0751e+00
Epoch 3/10
19/19 - 4s - loss: 294.3729 - loglik: -2.9244e+02 - logprior: -1.3254e+00
Epoch 4/10
19/19 - 4s - loss: 289.6861 - loglik: -2.8769e+02 - logprior: -1.2787e+00
Epoch 5/10
19/19 - 5s - loss: 287.9607 - loglik: -2.8613e+02 - logprior: -1.2458e+00
Epoch 6/10
19/19 - 5s - loss: 287.3227 - loglik: -2.8552e+02 - logprior: -1.2279e+00
Epoch 7/10
19/19 - 5s - loss: 286.9083 - loglik: -2.8514e+02 - logprior: -1.2075e+00
Epoch 8/10
19/19 - 5s - loss: 285.8212 - loglik: -2.8408e+02 - logprior: -1.1941e+00
Epoch 9/10
19/19 - 5s - loss: 286.2116 - loglik: -2.8448e+02 - logprior: -1.1986e+00
Fitted a model with MAP estimate = -285.1712
expansions: [(0, 6), (7, 3), (8, 2), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (55, 2), (56, 2), (58, 2), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (97, 1), (98, 1), (100, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 159 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 285.0886 - loglik: -2.8044e+02 - logprior: -4.1347e+00
Epoch 2/2
19/19 - 6s - loss: 272.3125 - loglik: -2.7033e+02 - logprior: -1.3753e+00
Fitted a model with MAP estimate = -270.2522
expansions: []
discards: [  1   2   3   5  21  73  75  79 101 102 148]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 275.4908 - loglik: -2.7201e+02 - logprior: -2.8428e+00
Epoch 2/2
19/19 - 6s - loss: 272.5555 - loglik: -2.7095e+02 - logprior: -9.9511e-01
Fitted a model with MAP estimate = -271.1238
expansions: [(0, 4)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 276.3626 - loglik: -2.7087e+02 - logprior: -4.8395e+00
Epoch 2/10
19/19 - 6s - loss: 271.6632 - loglik: -2.6972e+02 - logprior: -1.3057e+00
Epoch 3/10
19/19 - 6s - loss: 270.4510 - loglik: -2.6885e+02 - logprior: -9.3589e-01
Epoch 4/10
19/19 - 6s - loss: 270.2383 - loglik: -2.6878e+02 - logprior: -8.2006e-01
Epoch 5/10
19/19 - 6s - loss: 269.5328 - loglik: -2.6817e+02 - logprior: -7.3240e-01
Epoch 6/10
19/19 - 6s - loss: 269.7309 - loglik: -2.6842e+02 - logprior: -6.8087e-01
Fitted a model with MAP estimate = -268.8815
Time for alignment: 148.8375
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 409.0577 - loglik: -4.0605e+02 - logprior: -2.9905e+00
Epoch 2/10
19/19 - 5s - loss: 325.4761 - loglik: -3.2431e+02 - logprior: -1.1043e+00
Epoch 3/10
19/19 - 5s - loss: 296.4677 - loglik: -2.9463e+02 - logprior: -1.3868e+00
Epoch 4/10
19/19 - 5s - loss: 291.9981 - loglik: -2.9007e+02 - logprior: -1.3570e+00
Epoch 5/10
19/19 - 5s - loss: 290.2341 - loglik: -2.8835e+02 - logprior: -1.3252e+00
Epoch 6/10
19/19 - 5s - loss: 288.8948 - loglik: -2.8702e+02 - logprior: -1.3084e+00
Epoch 7/10
19/19 - 5s - loss: 288.7553 - loglik: -2.8693e+02 - logprior: -1.2763e+00
Epoch 8/10
19/19 - 5s - loss: 288.6506 - loglik: -2.8680e+02 - logprior: -1.2768e+00
Epoch 9/10
19/19 - 5s - loss: 287.5791 - loglik: -2.8574e+02 - logprior: -1.2686e+00
Epoch 10/10
19/19 - 5s - loss: 287.5022 - loglik: -2.8567e+02 - logprior: -1.2751e+00
Fitted a model with MAP estimate = -286.6622
expansions: [(0, 6), (6, 1), (8, 2), (9, 5), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (55, 2), (56, 2), (58, 2), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (96, 1), (97, 1), (100, 1), (105, 1), (112, 1), (113, 1), (115, 1)]
discards: [0 1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 158 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 284.7067 - loglik: -2.8120e+02 - logprior: -3.0024e+00
Epoch 2/2
19/19 - 7s - loss: 273.7281 - loglik: -2.7190e+02 - logprior: -1.2239e+00
Fitted a model with MAP estimate = -271.3327
expansions: []
discards: [  1   2   3   4   5  21  73  75  79 101 102]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 147 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 276.2509 - loglik: -2.7281e+02 - logprior: -2.8158e+00
Epoch 2/2
19/19 - 6s - loss: 273.4997 - loglik: -2.7195e+02 - logprior: -9.6717e-01
Fitted a model with MAP estimate = -271.9281
expansions: [(0, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 275.3505 - loglik: -2.7108e+02 - logprior: -3.6570e+00
Epoch 2/10
19/19 - 6s - loss: 271.8556 - loglik: -2.7006e+02 - logprior: -1.1862e+00
Epoch 3/10
19/19 - 6s - loss: 270.7252 - loglik: -2.6908e+02 - logprior: -9.9773e-01
Epoch 4/10
19/19 - 6s - loss: 269.8660 - loglik: -2.6829e+02 - logprior: -9.4730e-01
Epoch 5/10
19/19 - 6s - loss: 270.3786 - loglik: -2.6889e+02 - logprior: -8.7244e-01
Fitted a model with MAP estimate = -269.1435
Time for alignment: 153.2216
Computed alignments with likelihoods: ['-268.8976', '-268.8815', '-269.1435']
Best model has likelihood: -268.8815
time for generating output: 0.1818
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13522.projection.fasta
SP score = 0.6685760832435558
Training of 3 independent models on file PF00037.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f38242f2d00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f31ec2ecdf0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f31fda2dc40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f31fda800d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3218f5aac0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f31ec2e0d30>, <__main__.SimpleDirichletPrior object at 0x7f31fcd90a90>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3a8ee7c940>

Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 69.0055 - loglik: -6.5623e+01 - logprior: -3.3684e+00
Epoch 2/10
19/19 - 1s - loss: 52.2019 - loglik: -5.0745e+01 - logprior: -1.4481e+00
Epoch 3/10
19/19 - 1s - loss: 46.4432 - loglik: -4.4909e+01 - logprior: -1.4771e+00
Epoch 4/10
19/19 - 1s - loss: 44.6345 - loglik: -4.2925e+01 - logprior: -1.5532e+00
Epoch 5/10
19/19 - 1s - loss: 44.3003 - loglik: -4.2652e+01 - logprior: -1.5270e+00
Epoch 6/10
19/19 - 1s - loss: 44.0396 - loglik: -4.2375e+01 - logprior: -1.5156e+00
Epoch 7/10
19/19 - 1s - loss: 43.9590 - loglik: -4.2286e+01 - logprior: -1.5085e+00
Epoch 8/10
19/19 - 1s - loss: 43.8726 - loglik: -4.2199e+01 - logprior: -1.5091e+00
Epoch 9/10
19/19 - 1s - loss: 43.7993 - loglik: -4.2123e+01 - logprior: -1.4983e+00
Epoch 10/10
19/19 - 1s - loss: 43.8150 - loglik: -4.2124e+01 - logprior: -1.5031e+00
Fitted a model with MAP estimate = -43.5398
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 46.3148 - loglik: -4.1342e+01 - logprior: -4.7792e+00
Epoch 2/2
19/19 - 1s - loss: 41.3067 - loglik: -3.9715e+01 - logprior: -1.3874e+00
Fitted a model with MAP estimate = -40.3346
expansions: [(2, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 43.1606 - loglik: -3.9550e+01 - logprior: -3.3957e+00
Epoch 2/2
19/19 - 1s - loss: 40.8169 - loglik: -3.9023e+01 - logprior: -1.5612e+00
Fitted a model with MAP estimate = -40.3312
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 42.7581 - loglik: -3.9195e+01 - logprior: -3.3284e+00
Epoch 2/10
19/19 - 1s - loss: 40.6371 - loglik: -3.8854e+01 - logprior: -1.5399e+00
Epoch 3/10
19/19 - 1s - loss: 40.4086 - loglik: -3.8708e+01 - logprior: -1.4537e+00
Epoch 4/10
19/19 - 1s - loss: 40.3825 - loglik: -3.8721e+01 - logprior: -1.4045e+00
Epoch 5/10
19/19 - 1s - loss: 40.2659 - loglik: -3.8631e+01 - logprior: -1.3730e+00
Epoch 6/10
19/19 - 1s - loss: 40.1116 - loglik: -3.8484e+01 - logprior: -1.3573e+00
Epoch 7/10
19/19 - 1s - loss: 40.2756 - loglik: -3.8652e+01 - logprior: -1.3500e+00
Fitted a model with MAP estimate = -39.8569
Time for alignment: 36.4131
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 68.8757 - loglik: -6.5498e+01 - logprior: -3.3634e+00
Epoch 2/10
19/19 - 1s - loss: 51.8811 - loglik: -5.0411e+01 - logprior: -1.4597e+00
Epoch 3/10
19/19 - 1s - loss: 46.2369 - loglik: -4.4614e+01 - logprior: -1.5293e+00
Epoch 4/10
19/19 - 1s - loss: 44.6915 - loglik: -4.2970e+01 - logprior: -1.5551e+00
Epoch 5/10
19/19 - 1s - loss: 44.2023 - loglik: -4.2531e+01 - logprior: -1.5292e+00
Epoch 6/10
19/19 - 1s - loss: 44.0877 - loglik: -4.2415e+01 - logprior: -1.5216e+00
Epoch 7/10
19/19 - 1s - loss: 43.9080 - loglik: -4.2224e+01 - logprior: -1.5163e+00
Epoch 8/10
19/19 - 1s - loss: 43.8213 - loglik: -4.2143e+01 - logprior: -1.5070e+00
Epoch 9/10
19/19 - 1s - loss: 43.8217 - loglik: -4.2139e+01 - logprior: -1.5052e+00
Fitted a model with MAP estimate = -43.5792
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 46.2633 - loglik: -4.1372e+01 - logprior: -4.7020e+00
Epoch 2/2
19/19 - 1s - loss: 41.2827 - loglik: -3.9698e+01 - logprior: -1.3823e+00
Fitted a model with MAP estimate = -40.3457
expansions: [(2, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 43.1764 - loglik: -3.9568e+01 - logprior: -3.3969e+00
Epoch 2/2
19/19 - 1s - loss: 40.8017 - loglik: -3.9013e+01 - logprior: -1.5643e+00
Fitted a model with MAP estimate = -40.3405
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 42.7657 - loglik: -3.9208e+01 - logprior: -3.3259e+00
Epoch 2/10
19/19 - 1s - loss: 40.6713 - loglik: -3.8884e+01 - logprior: -1.5456e+00
Epoch 3/10
19/19 - 1s - loss: 40.4041 - loglik: -3.8706e+01 - logprior: -1.4522e+00
Epoch 4/10
19/19 - 1s - loss: 40.3065 - loglik: -3.8650e+01 - logprior: -1.4047e+00
Epoch 5/10
19/19 - 1s - loss: 40.2418 - loglik: -3.8608e+01 - logprior: -1.3719e+00
Epoch 6/10
19/19 - 1s - loss: 40.2688 - loglik: -3.8639e+01 - logprior: -1.3615e+00
Fitted a model with MAP estimate = -39.8748
Time for alignment: 33.4658
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 68.8683 - loglik: -6.5492e+01 - logprior: -3.3618e+00
Epoch 2/10
19/19 - 1s - loss: 51.7886 - loglik: -5.0341e+01 - logprior: -1.4388e+00
Epoch 3/10
19/19 - 1s - loss: 46.4383 - loglik: -4.5028e+01 - logprior: -1.3909e+00
Epoch 4/10
19/19 - 1s - loss: 45.0886 - loglik: -4.3810e+01 - logprior: -1.2779e+00
Epoch 5/10
19/19 - 1s - loss: 44.5140 - loglik: -4.3279e+01 - logprior: -1.2211e+00
Epoch 6/10
19/19 - 1s - loss: 44.2497 - loglik: -4.2904e+01 - logprior: -1.2148e+00
Epoch 7/10
19/19 - 1s - loss: 44.1196 - loglik: -4.2817e+01 - logprior: -1.2019e+00
Epoch 8/10
19/19 - 1s - loss: 44.0932 - loglik: -4.2777e+01 - logprior: -1.1886e+00
Epoch 9/10
19/19 - 1s - loss: 44.0363 - loglik: -4.2724e+01 - logprior: -1.1818e+00
Epoch 10/10
19/19 - 1s - loss: 44.0290 - loglik: -4.2706e+01 - logprior: -1.1792e+00
Fitted a model with MAP estimate = -43.8398
expansions: [(0, 2), (4, 1), (11, 1), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 46.5781 - loglik: -4.1705e+01 - logprior: -4.7177e+00
Epoch 2/2
19/19 - 1s - loss: 41.4656 - loglik: -3.9560e+01 - logprior: -1.7274e+00
Fitted a model with MAP estimate = -40.8012
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 43.0406 - loglik: -3.9524e+01 - logprior: -3.3325e+00
Epoch 2/10
19/19 - 1s - loss: 40.8695 - loglik: -3.9123e+01 - logprior: -1.5519e+00
Epoch 3/10
19/19 - 1s - loss: 40.5923 - loglik: -3.8935e+01 - logprior: -1.4584e+00
Epoch 4/10
19/19 - 1s - loss: 40.4929 - loglik: -3.8868e+01 - logprior: -1.4127e+00
Epoch 5/10
19/19 - 1s - loss: 40.4217 - loglik: -3.8819e+01 - logprior: -1.3822e+00
Epoch 6/10
19/19 - 1s - loss: 40.3643 - loglik: -3.8761e+01 - logprior: -1.3710e+00
Epoch 7/10
19/19 - 1s - loss: 40.2074 - loglik: -3.8603e+01 - logprior: -1.3605e+00
Epoch 8/10
19/19 - 1s - loss: 40.2500 - loglik: -3.8649e+01 - logprior: -1.3501e+00
Fitted a model with MAP estimate = -39.9030
Time for alignment: 29.8398
Computed alignments with likelihoods: ['-39.8569', '-39.8748', '-39.9030']
Best model has likelihood: -39.8569
time for generating output: 0.0892
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00037.projection.fasta
SP score = 0.8081861958266453
Training of 3 independent models on file PF00224.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3804382f70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f37fc1a20a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f31fd6803d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f389c4d15e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f389c4d1610>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f32195629d0>, <__main__.SimpleDirichletPrior object at 0x7f31f5755df0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3becc128b0>

Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 32s - loss: 735.7943 - loglik: -7.3409e+02 - logprior: -1.6892e+00
Epoch 2/10
39/39 - 30s - loss: 529.5359 - loglik: -5.2734e+02 - logprior: -1.8870e+00
Epoch 3/10
39/39 - 31s - loss: 517.4073 - loglik: -5.1511e+02 - logprior: -1.8539e+00
Epoch 4/10
39/39 - 32s - loss: 514.9856 - loglik: -5.1275e+02 - logprior: -1.7685e+00
Epoch 5/10
39/39 - 33s - loss: 513.9235 - loglik: -5.1170e+02 - logprior: -1.7649e+00
Epoch 6/10
39/39 - 34s - loss: 512.8854 - loglik: -5.1067e+02 - logprior: -1.7629e+00
Epoch 7/10
39/39 - 34s - loss: 513.1780 - loglik: -5.1097e+02 - logprior: -1.7689e+00
Fitted a model with MAP estimate = -511.4685
expansions: [(0, 2), (1, 1), (14, 1), (15, 1), (16, 1), (19, 1), (38, 1), (39, 1), (44, 1), (45, 1), (46, 2), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (105, 1), (109, 1), (110, 1), (111, 1), (113, 1), (132, 3), (133, 1), (145, 2), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (181, 1), (182, 1), (188, 1), (189, 1), (206, 1), (212, 1), (214, 2), (216, 1), (217, 1), (228, 1), (230, 2), (231, 1), (244, 1), (259, 1), (265, 1), (267, 1), (269, 3), (271, 1), (272, 2), (273, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 475.5563 - loglik: -4.7262e+02 - logprior: -2.4972e+00
Epoch 2/2
39/39 - 47s - loss: 459.3274 - loglik: -4.5769e+02 - logprior: -1.1563e+00
Fitted a model with MAP estimate = -457.5215
expansions: []
discards: [  0   1  57  98 164 180 215]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 51s - loss: 461.6010 - loglik: -4.5904e+02 - logprior: -2.0051e+00
Epoch 2/2
39/39 - 50s - loss: 459.1641 - loglik: -4.5818e+02 - logprior: -4.4387e-01
Fitted a model with MAP estimate = -457.6817
expansions: [(0, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 54s - loss: 459.2503 - loglik: -4.5702e+02 - logprior: -1.6567e+00
Epoch 2/10
39/39 - 48s - loss: 457.4976 - loglik: -4.5642e+02 - logprior: -5.2358e-01
Epoch 3/10
39/39 - 48s - loss: 456.7068 - loglik: -4.5596e+02 - logprior: -1.5618e-01
Epoch 4/10
39/39 - 47s - loss: 456.6106 - loglik: -4.5583e+02 - logprior: -2.1074e-01
Epoch 5/10
39/39 - 46s - loss: 455.9112 - loglik: -4.5547e+02 - logprior: 0.1327
Epoch 6/10
39/39 - 47s - loss: 456.1878 - loglik: -4.5577e+02 - logprior: 0.1559
Fitted a model with MAP estimate = -454.9322
Time for alignment: 961.7945
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 736.0571 - loglik: -7.3426e+02 - logprior: -1.7545e+00
Epoch 2/10
39/39 - 32s - loss: 527.3616 - loglik: -5.2481e+02 - logprior: -1.9795e+00
Epoch 3/10
39/39 - 30s - loss: 514.2560 - loglik: -5.1162e+02 - logprior: -2.0052e+00
Epoch 4/10
39/39 - 30s - loss: 511.5081 - loglik: -5.0897e+02 - logprior: -1.9399e+00
Epoch 5/10
39/39 - 30s - loss: 509.8165 - loglik: -5.0734e+02 - logprior: -1.9380e+00
Epoch 6/10
39/39 - 29s - loss: 509.3556 - loglik: -5.0692e+02 - logprior: -1.9412e+00
Epoch 7/10
39/39 - 29s - loss: 508.8646 - loglik: -5.0643e+02 - logprior: -1.9501e+00
Epoch 8/10
39/39 - 29s - loss: 508.9741 - loglik: -5.0656e+02 - logprior: -1.9546e+00
Fitted a model with MAP estimate = -507.4247
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (44, 1), (46, 1), (47, 1), (63, 1), (67, 2), (68, 1), (70, 1), (77, 1), (79, 2), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (96, 1), (108, 1), (110, 1), (111, 1), (113, 1), (120, 1), (132, 1), (133, 1), (142, 1), (147, 1), (148, 1), (149, 1), (163, 1), (166, 1), (168, 1), (169, 1), (171, 2), (172, 1), (179, 1), (182, 1), (188, 1), (189, 1), (206, 1), (212, 1), (214, 2), (216, 1), (217, 1), (228, 1), (229, 3), (230, 1), (243, 1), (258, 1), (264, 1), (267, 1), (269, 1), (270, 1), (271, 1), (272, 2), (273, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 45s - loss: 473.9229 - loglik: -4.7108e+02 - logprior: -2.4073e+00
Epoch 2/2
39/39 - 42s - loss: 459.0094 - loglik: -4.5746e+02 - logprior: -1.0195e+00
Fitted a model with MAP estimate = -456.8097
expansions: []
discards: [  0  98 213]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 343 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 45s - loss: 462.2288 - loglik: -4.5886e+02 - logprior: -2.7817e+00
Epoch 2/2
39/39 - 41s - loss: 458.9319 - loglik: -4.5740e+02 - logprior: -9.7999e-01
Fitted a model with MAP estimate = -456.8003
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 344 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 458.3492 - loglik: -4.5619e+02 - logprior: -1.5567e+00
Epoch 2/10
39/39 - 40s - loss: 456.9899 - loglik: -4.5612e+02 - logprior: -3.0229e-01
Epoch 3/10
39/39 - 39s - loss: 456.2327 - loglik: -4.5527e+02 - logprior: -3.5359e-01
Epoch 4/10
39/39 - 39s - loss: 456.2635 - loglik: -4.5573e+02 - logprior: 0.0573
Fitted a model with MAP estimate = -455.2524
Time for alignment: 769.6228
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 32s - loss: 735.7917 - loglik: -7.3408e+02 - logprior: -1.6924e+00
Epoch 2/10
39/39 - 27s - loss: 529.2778 - loglik: -5.2708e+02 - logprior: -1.7443e+00
Epoch 3/10
39/39 - 27s - loss: 516.8259 - loglik: -5.1458e+02 - logprior: -1.7240e+00
Epoch 4/10
39/39 - 27s - loss: 514.4042 - loglik: -5.1221e+02 - logprior: -1.6663e+00
Epoch 5/10
39/39 - 27s - loss: 512.9760 - loglik: -5.1083e+02 - logprior: -1.6449e+00
Epoch 6/10
39/39 - 27s - loss: 512.8693 - loglik: -5.1074e+02 - logprior: -1.6620e+00
Epoch 7/10
39/39 - 27s - loss: 512.5167 - loglik: -5.1039e+02 - logprior: -1.6765e+00
Epoch 8/10
39/39 - 27s - loss: 511.7518 - loglik: -5.0964e+02 - logprior: -1.6852e+00
Epoch 9/10
39/39 - 28s - loss: 511.9626 - loglik: -5.0984e+02 - logprior: -1.6924e+00
Fitted a model with MAP estimate = -510.6805
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (63, 1), (67, 2), (68, 1), (70, 1), (79, 3), (82, 1), (83, 1), (84, 2), (90, 1), (105, 1), (109, 1), (110, 1), (111, 1), (113, 1), (132, 3), (133, 1), (145, 2), (147, 1), (148, 1), (159, 1), (163, 1), (167, 1), (170, 1), (171, 3), (172, 1), (183, 1), (184, 1), (188, 1), (205, 1), (209, 1), (215, 2), (217, 3), (230, 3), (231, 1), (244, 1), (259, 1), (265, 1), (267, 1), (269, 1), (270, 1), (272, 3), (273, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 43s - loss: 474.3469 - loglik: -4.7135e+02 - logprior: -2.5770e+00
Epoch 2/2
39/39 - 40s - loss: 458.7121 - loglik: -4.5712e+02 - logprior: -1.0851e+00
Fitted a model with MAP estimate = -456.9345
expansions: []
discards: [  0  98 164 180 215]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 343 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 43s - loss: 461.9626 - loglik: -4.5867e+02 - logprior: -2.7357e+00
Epoch 2/2
39/39 - 40s - loss: 458.9291 - loglik: -4.5747e+02 - logprior: -9.1236e-01
Fitted a model with MAP estimate = -456.6865
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 344 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 458.2057 - loglik: -4.5616e+02 - logprior: -1.4615e+00
Epoch 2/10
39/39 - 42s - loss: 457.2417 - loglik: -4.5613e+02 - logprior: -5.4720e-01
Epoch 3/10
39/39 - 45s - loss: 456.2766 - loglik: -4.5554e+02 - logprior: -1.4054e-01
Epoch 4/10
39/39 - 47s - loss: 456.5859 - loglik: -4.5566e+02 - logprior: -3.3946e-01
Fitted a model with MAP estimate = -455.2915
Time for alignment: 782.0063
Computed alignments with likelihoods: ['-454.9322', '-455.2524', '-455.2915']
Best model has likelihood: -454.9322
time for generating output: 0.2939
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00224.projection.fasta
SP score = 0.9265097100292631
Training of 3 independent models on file PF13365.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3219963190>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f31ed1b1fd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f31ed1b1970>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f31e423fa60>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f31ece18670>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3becc379a0>, <__main__.SimpleDirichletPrior object at 0x7f3b72d4ba60>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3bc7a99f70>

Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 397.6440 - loglik: -3.9454e+02 - logprior: -3.0885e+00
Epoch 2/10
19/19 - 6s - loss: 325.4368 - loglik: -3.2401e+02 - logprior: -1.3284e+00
Epoch 3/10
19/19 - 6s - loss: 303.0024 - loglik: -3.0057e+02 - logprior: -1.7365e+00
Epoch 4/10
19/19 - 6s - loss: 296.9286 - loglik: -2.9427e+02 - logprior: -1.5917e+00
Epoch 5/10
19/19 - 6s - loss: 296.1126 - loglik: -2.9356e+02 - logprior: -1.5376e+00
Epoch 6/10
19/19 - 6s - loss: 293.9338 - loglik: -2.9150e+02 - logprior: -1.5052e+00
Epoch 7/10
19/19 - 6s - loss: 293.7519 - loglik: -2.9141e+02 - logprior: -1.5248e+00
Epoch 8/10
19/19 - 6s - loss: 293.5500 - loglik: -2.9127e+02 - logprior: -1.5185e+00
Epoch 9/10
19/19 - 6s - loss: 292.1776 - loglik: -2.8998e+02 - logprior: -1.5156e+00
Epoch 10/10
19/19 - 6s - loss: 291.6704 - loglik: -2.8949e+02 - logprior: -1.5141e+00
Fitted a model with MAP estimate = -290.5198
expansions: [(7, 2), (22, 1), (23, 1), (24, 1), (25, 2), (29, 2), (35, 1), (36, 1), (37, 2), (46, 3), (49, 2), (50, 2), (57, 2), (71, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 147 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 291.0334 - loglik: -2.8710e+02 - logprior: -3.2593e+00
Epoch 2/2
39/39 - 10s - loss: 282.2451 - loglik: -2.8001e+02 - logprior: -1.5942e+00
Fitted a model with MAP estimate = -279.2412
expansions: []
discards: [ 29  35  47  59  65  67  76 107 139]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 282.3915 - loglik: -2.7944e+02 - logprior: -2.2744e+00
Epoch 2/2
39/39 - 9s - loss: 280.7622 - loglik: -2.7892e+02 - logprior: -1.2048e+00
Fitted a model with MAP estimate = -278.6865
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 280.5876 - loglik: -2.7780e+02 - logprior: -2.1477e+00
Epoch 2/10
39/39 - 9s - loss: 279.5983 - loglik: -2.7790e+02 - logprior: -1.0850e+00
Epoch 3/10
39/39 - 9s - loss: 279.3942 - loglik: -2.7779e+02 - logprior: -9.7136e-01
Epoch 4/10
39/39 - 9s - loss: 278.1893 - loglik: -2.7671e+02 - logprior: -8.8735e-01
Epoch 5/10
39/39 - 9s - loss: 278.9948 - loglik: -2.7758e+02 - logprior: -8.2098e-01
Fitted a model with MAP estimate = -277.9109
Time for alignment: 205.0709
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 397.8085 - loglik: -3.9471e+02 - logprior: -3.0920e+00
Epoch 2/10
19/19 - 6s - loss: 328.8460 - loglik: -3.2741e+02 - logprior: -1.3315e+00
Epoch 3/10
19/19 - 6s - loss: 303.9122 - loglik: -3.0146e+02 - logprior: -1.7039e+00
Epoch 4/10
19/19 - 6s - loss: 297.9870 - loglik: -2.9533e+02 - logprior: -1.5635e+00
Epoch 5/10
19/19 - 6s - loss: 296.8086 - loglik: -2.9427e+02 - logprior: -1.5172e+00
Epoch 6/10
19/19 - 6s - loss: 294.4889 - loglik: -2.9205e+02 - logprior: -1.5110e+00
Epoch 7/10
19/19 - 6s - loss: 293.9807 - loglik: -2.9162e+02 - logprior: -1.5100e+00
Epoch 8/10
19/19 - 6s - loss: 293.5071 - loglik: -2.9126e+02 - logprior: -1.5043e+00
Epoch 9/10
19/19 - 6s - loss: 292.6498 - loglik: -2.9047e+02 - logprior: -1.4951e+00
Epoch 10/10
19/19 - 6s - loss: 294.3593 - loglik: -2.9221e+02 - logprior: -1.4956e+00
Fitted a model with MAP estimate = -291.1338
expansions: [(7, 2), (21, 3), (22, 1), (25, 2), (29, 2), (35, 1), (36, 1), (37, 2), (46, 3), (49, 2), (50, 2), (57, 2), (71, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 291.1933 - loglik: -2.8724e+02 - logprior: -3.2702e+00
Epoch 2/2
39/39 - 10s - loss: 282.1793 - loglik: -2.7993e+02 - logprior: -1.6083e+00
Fitted a model with MAP estimate = -279.0960
expansions: []
discards: [ 23  30  36  48  60  66  68  77 108 140]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 282.4180 - loglik: -2.7947e+02 - logprior: -2.2654e+00
Epoch 2/2
39/39 - 9s - loss: 280.7350 - loglik: -2.7889e+02 - logprior: -1.2080e+00
Fitted a model with MAP estimate = -278.6421
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 280.9576 - loglik: -2.7817e+02 - logprior: -2.1469e+00
Epoch 2/10
39/39 - 9s - loss: 279.1341 - loglik: -2.7744e+02 - logprior: -1.0819e+00
Epoch 3/10
39/39 - 9s - loss: 279.4836 - loglik: -2.7788e+02 - logprior: -9.6381e-01
Fitted a model with MAP estimate = -278.1379
Time for alignment: 184.0938
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 397.8275 - loglik: -3.9472e+02 - logprior: -3.0990e+00
Epoch 2/10
19/19 - 6s - loss: 329.4955 - loglik: -3.2805e+02 - logprior: -1.3171e+00
Epoch 3/10
19/19 - 6s - loss: 303.2353 - loglik: -3.0075e+02 - logprior: -1.7142e+00
Epoch 4/10
19/19 - 6s - loss: 297.2710 - loglik: -2.9451e+02 - logprior: -1.6014e+00
Epoch 5/10
19/19 - 6s - loss: 295.2125 - loglik: -2.9258e+02 - logprior: -1.5905e+00
Epoch 6/10
19/19 - 6s - loss: 293.7805 - loglik: -2.9139e+02 - logprior: -1.5534e+00
Epoch 7/10
19/19 - 6s - loss: 293.3721 - loglik: -2.9111e+02 - logprior: -1.5362e+00
Epoch 8/10
19/19 - 6s - loss: 292.3163 - loglik: -2.9014e+02 - logprior: -1.5152e+00
Epoch 9/10
19/19 - 6s - loss: 292.0082 - loglik: -2.8987e+02 - logprior: -1.5157e+00
Epoch 10/10
19/19 - 6s - loss: 292.9545 - loglik: -2.9085e+02 - logprior: -1.5041e+00
Fitted a model with MAP estimate = -290.4934
expansions: [(7, 2), (22, 1), (23, 1), (24, 1), (25, 2), (29, 2), (35, 1), (37, 1), (45, 2), (46, 3), (49, 2), (50, 2), (55, 1), (71, 1), (81, 1), (84, 4), (85, 2), (86, 3), (106, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 146 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 290.3342 - loglik: -2.8648e+02 - logprior: -3.2155e+00
Epoch 2/2
39/39 - 10s - loss: 282.1136 - loglik: -2.7995e+02 - logprior: -1.5145e+00
Fitted a model with MAP estimate = -278.9874
expansions: []
discards: [ 29  35  57  59  65  67 138]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 282.2598 - loglik: -2.7932e+02 - logprior: -2.2620e+00
Epoch 2/2
39/39 - 9s - loss: 280.5162 - loglik: -2.7868e+02 - logprior: -1.2045e+00
Fitted a model with MAP estimate = -278.6082
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 280.6397 - loglik: -2.7786e+02 - logprior: -2.1426e+00
Epoch 2/10
39/39 - 9s - loss: 279.2886 - loglik: -2.7759e+02 - logprior: -1.0825e+00
Epoch 3/10
39/39 - 9s - loss: 278.8520 - loglik: -2.7726e+02 - logprior: -9.6393e-01
Epoch 4/10
39/39 - 9s - loss: 278.4568 - loglik: -2.7697e+02 - logprior: -8.8950e-01
Epoch 5/10
39/39 - 9s - loss: 278.7251 - loglik: -2.7733e+02 - logprior: -8.0787e-01
Fitted a model with MAP estimate = -277.7361
Time for alignment: 202.0843
Computed alignments with likelihoods: ['-277.9109', '-278.1379', '-277.7361']
Best model has likelihood: -277.7361
time for generating output: 0.2793
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13365.projection.fasta
SP score = 0.3957854506579043
Training of 3 independent models on file PF00078.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f31ec502f40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f31edf41700>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3205ed3730>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f387c1e9fa0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f38241b9c70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f31fcc25fd0>, <__main__.SimpleDirichletPrior object at 0x7f31f4603310>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f31fccf85e0>

Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 469.6830 - loglik: -4.6721e+02 - logprior: -2.0162e+00
Epoch 2/10
39/39 - 11s - loss: 426.6948 - loglik: -4.2371e+02 - logprior: -1.4879e+00
Epoch 3/10
39/39 - 11s - loss: 418.8304 - loglik: -4.1519e+02 - logprior: -1.6322e+00
Epoch 4/10
39/39 - 11s - loss: 414.7063 - loglik: -4.1150e+02 - logprior: -1.6733e+00
Epoch 5/10
39/39 - 11s - loss: 413.3498 - loglik: -4.1037e+02 - logprior: -1.7028e+00
Epoch 6/10
39/39 - 11s - loss: 412.1613 - loglik: -4.0934e+02 - logprior: -1.7190e+00
Epoch 7/10
39/39 - 11s - loss: 411.4884 - loglik: -4.0884e+02 - logprior: -1.7132e+00
Epoch 8/10
39/39 - 11s - loss: 411.4042 - loglik: -4.0885e+02 - logprior: -1.7082e+00
Epoch 9/10
39/39 - 11s - loss: 410.8979 - loglik: -4.0839e+02 - logprior: -1.7119e+00
Epoch 10/10
39/39 - 11s - loss: 410.6863 - loglik: -4.0823e+02 - logprior: -1.7102e+00
Fitted a model with MAP estimate = -409.8114
expansions: [(19, 1), (21, 1), (32, 3), (33, 1), (34, 1), (47, 1), (48, 1), (49, 1), (50, 2), (54, 1), (58, 2), (59, 2), (60, 1), (74, 1), (75, 1), (76, 1), (79, 1), (80, 9), (92, 1), (93, 1), (105, 1), (106, 1), (107, 2), (108, 1)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 165 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 402.4082 - loglik: -3.9943e+02 - logprior: -2.2250e+00
Epoch 2/2
39/39 - 14s - loss: 395.9358 - loglik: -3.9371e+02 - logprior: -1.2651e+00
Fitted a model with MAP estimate = -393.4502
expansions: []
discards: [ 70  74 104 105 106 107 108]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 158 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 397.3705 - loglik: -3.9401e+02 - logprior: -2.2317e+00
Epoch 2/2
39/39 - 13s - loss: 395.3929 - loglik: -3.9322e+02 - logprior: -1.1489e+00
Fitted a model with MAP estimate = -393.2997
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 158 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 395.4187 - loglik: -3.9230e+02 - logprior: -2.0860e+00
Epoch 2/10
39/39 - 13s - loss: 394.7496 - loglik: -3.9273e+02 - logprior: -1.0137e+00
Epoch 3/10
39/39 - 13s - loss: 393.9291 - loglik: -3.9199e+02 - logprior: -9.1376e-01
Epoch 4/10
39/39 - 13s - loss: 393.4694 - loglik: -3.9169e+02 - logprior: -8.4295e-01
Epoch 5/10
39/39 - 13s - loss: 393.6160 - loglik: -3.9193e+02 - logprior: -7.6271e-01
Fitted a model with MAP estimate = -392.1718
Time for alignment: 299.6039
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 467.6701 - loglik: -4.6520e+02 - logprior: -2.0320e+00
Epoch 2/10
39/39 - 10s - loss: 425.9455 - loglik: -4.2315e+02 - logprior: -1.4982e+00
Epoch 3/10
39/39 - 11s - loss: 419.5879 - loglik: -4.1622e+02 - logprior: -1.5831e+00
Epoch 4/10
39/39 - 11s - loss: 417.4302 - loglik: -4.1439e+02 - logprior: -1.5563e+00
Epoch 5/10
39/39 - 11s - loss: 416.2802 - loglik: -4.1348e+02 - logprior: -1.5662e+00
Epoch 6/10
39/39 - 11s - loss: 415.3104 - loglik: -4.1266e+02 - logprior: -1.5676e+00
Epoch 7/10
39/39 - 11s - loss: 414.6916 - loglik: -4.1215e+02 - logprior: -1.5720e+00
Epoch 8/10
39/39 - 11s - loss: 414.6663 - loglik: -4.1219e+02 - logprior: -1.5758e+00
Epoch 9/10
39/39 - 11s - loss: 414.1046 - loglik: -4.1168e+02 - logprior: -1.5845e+00
Epoch 10/10
39/39 - 11s - loss: 413.4903 - loglik: -4.1110e+02 - logprior: -1.5991e+00
Fitted a model with MAP estimate = -412.3038
expansions: [(19, 2), (20, 1), (31, 1), (32, 2), (33, 3), (49, 2), (51, 1), (74, 2), (76, 1), (78, 1), (80, 8), (94, 1), (97, 1), (98, 1), (106, 1), (107, 2), (108, 1)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 158 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 406.9465 - loglik: -4.0392e+02 - logprior: -2.2636e+00
Epoch 2/2
39/39 - 12s - loss: 401.7505 - loglik: -3.9950e+02 - logprior: -1.3199e+00
Fitted a model with MAP estimate = -399.5853
expansions: []
discards: [ 35  38  85  98  99 100]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 403.1583 - loglik: -3.9983e+02 - logprior: -2.2524e+00
Epoch 2/2
39/39 - 12s - loss: 401.6781 - loglik: -3.9940e+02 - logprior: -1.1851e+00
Fitted a model with MAP estimate = -399.7909
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 401.7220 - loglik: -3.9849e+02 - logprior: -2.1589e+00
Epoch 2/10
39/39 - 12s - loss: 401.0622 - loglik: -3.9883e+02 - logprior: -1.1647e+00
Epoch 3/10
39/39 - 12s - loss: 400.5399 - loglik: -3.9839e+02 - logprior: -1.0676e+00
Epoch 4/10
39/39 - 12s - loss: 399.7187 - loglik: -3.9775e+02 - logprior: -9.9974e-01
Epoch 5/10
39/39 - 12s - loss: 399.7909 - loglik: -3.9792e+02 - logprior: -9.1784e-01
Fitted a model with MAP estimate = -398.3894
Time for alignment: 285.7782
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 468.7281 - loglik: -4.6626e+02 - logprior: -2.0267e+00
Epoch 2/10
39/39 - 11s - loss: 426.5592 - loglik: -4.2337e+02 - logprior: -1.4747e+00
Epoch 3/10
39/39 - 11s - loss: 419.7512 - loglik: -4.1627e+02 - logprior: -1.5645e+00
Epoch 4/10
39/39 - 11s - loss: 416.9524 - loglik: -4.1391e+02 - logprior: -1.5813e+00
Epoch 5/10
39/39 - 11s - loss: 415.5301 - loglik: -4.1270e+02 - logprior: -1.5997e+00
Epoch 6/10
39/39 - 11s - loss: 414.5490 - loglik: -4.1185e+02 - logprior: -1.6221e+00
Epoch 7/10
39/39 - 11s - loss: 414.2575 - loglik: -4.1165e+02 - logprior: -1.6266e+00
Epoch 8/10
39/39 - 11s - loss: 413.7964 - loglik: -4.1127e+02 - logprior: -1.6308e+00
Epoch 9/10
39/39 - 11s - loss: 413.1867 - loglik: -4.1074e+02 - logprior: -1.6253e+00
Epoch 10/10
39/39 - 11s - loss: 413.3693 - loglik: -4.1094e+02 - logprior: -1.6358e+00
Fitted a model with MAP estimate = -411.8548
expansions: [(19, 1), (21, 1), (32, 3), (33, 1), (34, 1), (47, 1), (48, 1), (49, 1), (50, 2), (56, 2), (57, 1), (58, 6), (76, 1), (78, 2), (79, 4), (92, 1), (93, 1), (97, 1), (106, 3), (108, 1)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 162 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 404.2653 - loglik: -4.0125e+02 - logprior: -2.2209e+00
Epoch 2/2
39/39 - 13s - loss: 398.1626 - loglik: -3.9591e+02 - logprior: -1.2770e+00
Fitted a model with MAP estimate = -395.7634
expansions: []
discards: [ 75  76  77  78 104]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 157 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 400.5266 - loglik: -3.9716e+02 - logprior: -2.2578e+00
Epoch 2/2
39/39 - 13s - loss: 398.7530 - loglik: -3.9648e+02 - logprior: -1.2386e+00
Fitted a model with MAP estimate = -396.6772
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 157 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 398.8937 - loglik: -3.9569e+02 - logprior: -2.1738e+00
Epoch 2/10
39/39 - 13s - loss: 397.8574 - loglik: -3.9575e+02 - logprior: -1.1203e+00
Epoch 3/10
39/39 - 13s - loss: 397.5294 - loglik: -3.9548e+02 - logprior: -1.0249e+00
Epoch 4/10
39/39 - 13s - loss: 397.1330 - loglik: -3.9525e+02 - logprior: -9.4341e-01
Epoch 5/10
39/39 - 13s - loss: 396.8011 - loglik: -3.9502e+02 - logprior: -8.6385e-01
Epoch 6/10
39/39 - 13s - loss: 396.2773 - loglik: -3.9459e+02 - logprior: -7.8959e-01
Epoch 7/10
39/39 - 13s - loss: 396.0379 - loglik: -3.9443e+02 - logprior: -7.1579e-01
Epoch 8/10
39/39 - 13s - loss: 396.2727 - loglik: -3.9475e+02 - logprior: -6.4473e-01
Fitted a model with MAP estimate = -394.7360
Time for alignment: 333.6084
Computed alignments with likelihoods: ['-392.1718', '-398.3894', '-394.7360']
Best model has likelihood: -392.1718
time for generating output: 0.2015
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00078.projection.fasta
SP score = 0.8573136707851734
Training of 3 independent models on file PF00150.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f31fc425850>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3210c0fd00>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f31fd933760>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f37fe11c970>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f37fe11cd30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f32044f3940>, <__main__.SimpleDirichletPrior object at 0x7f3b839c2d30>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3b8c1241f0>

Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 30s - loss: 792.2476 - loglik: -7.9021e+02 - logprior: -1.5550e+00
Epoch 2/10
39/39 - 29s - loss: 720.4218 - loglik: -7.1797e+02 - logprior: -9.8616e-01
Epoch 3/10
39/39 - 29s - loss: 707.6652 - loglik: -7.0420e+02 - logprior: -1.2455e+00
Epoch 4/10
39/39 - 27s - loss: 702.5293 - loglik: -6.9856e+02 - logprior: -1.2643e+00
Epoch 5/10
39/39 - 27s - loss: 698.1491 - loglik: -6.9401e+02 - logprior: -1.3301e+00
Epoch 6/10
39/39 - 27s - loss: 696.0204 - loglik: -6.9210e+02 - logprior: -1.3797e+00
Epoch 7/10
39/39 - 27s - loss: 693.7175 - loglik: -6.9014e+02 - logprior: -1.3978e+00
Epoch 8/10
39/39 - 28s - loss: 692.3325 - loglik: -6.8897e+02 - logprior: -1.4357e+00
Epoch 9/10
39/39 - 27s - loss: 690.9411 - loglik: -6.8772e+02 - logprior: -1.4604e+00
Epoch 10/10
39/39 - 27s - loss: 689.8522 - loglik: -6.8660e+02 - logprior: -1.4730e+00
Fitted a model with MAP estimate = -686.2375
expansions: [(0, 4), (9, 1), (24, 1), (40, 1), (43, 1), (51, 2), (52, 1), (62, 1), (85, 2), (87, 14), (103, 1), (114, 3), (117, 1), (121, 1), (154, 2), (173, 2), (176, 4), (179, 1), (183, 1), (206, 6), (208, 1), (214, 1)]
discards: [157]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 39s - loss: 680.3105 - loglik: -6.7515e+02 - logprior: -3.0182e+00
Epoch 2/2
39/39 - 37s - loss: 666.1327 - loglik: -6.6201e+02 - logprior: -1.3271e+00
Fitted a model with MAP estimate = -660.4272
expansions: [(191, 1), (207, 2)]
discards: [ 1  6 60]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 41s - loss: 663.9499 - loglik: -6.5903e+02 - logprior: -2.3143e+00
Epoch 2/2
39/39 - 38s - loss: 660.9434 - loglik: -6.5734e+02 - logprior: -1.2621e+00
Fitted a model with MAP estimate = -656.7921
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 280 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 660.0515 - loglik: -6.5576e+02 - logprior: -2.1270e+00
Epoch 2/10
39/39 - 37s - loss: 658.2927 - loglik: -6.5545e+02 - logprior: -9.5317e-01
Epoch 3/10
39/39 - 34s - loss: 657.8540 - loglik: -6.5530e+02 - logprior: -7.6019e-01
Epoch 4/10
39/39 - 34s - loss: 657.0054 - loglik: -6.5488e+02 - logprior: -5.4876e-01
Epoch 5/10
39/39 - 33s - loss: 656.7549 - loglik: -6.5493e+02 - logprior: -3.7427e-01
Epoch 6/10
39/39 - 33s - loss: 655.9561 - loglik: -6.5440e+02 - logprior: -1.8575e-01
Epoch 7/10
39/39 - 33s - loss: 656.1619 - loglik: -6.5483e+02 - logprior: -1.4731e-02
Fitted a model with MAP estimate = -654.3344
Time for alignment: 832.5106
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 792.1353 - loglik: -7.9009e+02 - logprior: -1.5603e+00
Epoch 2/10
39/39 - 25s - loss: 719.4377 - loglik: -7.1674e+02 - logprior: -1.0495e+00
Epoch 3/10
39/39 - 25s - loss: 707.1039 - loglik: -7.0310e+02 - logprior: -1.3038e+00
Epoch 4/10
39/39 - 25s - loss: 701.6252 - loglik: -6.9732e+02 - logprior: -1.3308e+00
Epoch 5/10
39/39 - 25s - loss: 697.8229 - loglik: -6.9352e+02 - logprior: -1.3960e+00
Epoch 6/10
39/39 - 25s - loss: 694.9393 - loglik: -6.9095e+02 - logprior: -1.4686e+00
Epoch 7/10
39/39 - 25s - loss: 692.1473 - loglik: -6.8846e+02 - logprior: -1.5244e+00
Epoch 8/10
39/39 - 25s - loss: 690.4999 - loglik: -6.8704e+02 - logprior: -1.5391e+00
Epoch 9/10
39/39 - 25s - loss: 689.7076 - loglik: -6.8634e+02 - logprior: -1.5666e+00
Epoch 10/10
39/39 - 25s - loss: 688.2658 - loglik: -6.8486e+02 - logprior: -1.5673e+00
Fitted a model with MAP estimate = -685.0848
expansions: [(0, 3), (9, 1), (10, 1), (20, 1), (50, 1), (51, 1), (52, 1), (86, 7), (90, 1), (93, 1), (105, 1), (116, 4), (117, 2), (118, 1), (122, 1), (128, 1), (149, 1), (153, 2), (172, 4), (175, 2), (178, 1), (206, 6), (214, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 274 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 680.2520 - loglik: -6.7505e+02 - logprior: -3.0245e+00
Epoch 2/2
39/39 - 31s - loss: 667.5593 - loglik: -6.6365e+02 - logprior: -1.2492e+00
Fitted a model with MAP estimate = -661.6547
expansions: [(0, 5), (103, 2), (104, 2), (202, 2)]
discards: [ 17 138 142]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 282 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 665.1516 - loglik: -6.5945e+02 - logprior: -3.0946e+00
Epoch 2/2
39/39 - 33s - loss: 660.1829 - loglik: -6.5658e+02 - logprior: -1.1998e+00
Fitted a model with MAP estimate = -655.8464
expansions: [(0, 3)]
discards: [10]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 284 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 660.2346 - loglik: -6.5487e+02 - logprior: -3.1500e+00
Epoch 2/10
39/39 - 34s - loss: 656.5207 - loglik: -6.5350e+02 - logprior: -1.0449e+00
Epoch 3/10
39/39 - 34s - loss: 656.1080 - loglik: -6.5349e+02 - logprior: -7.5384e-01
Epoch 4/10
39/39 - 34s - loss: 655.2068 - loglik: -6.5300e+02 - logprior: -5.5346e-01
Epoch 5/10
39/39 - 34s - loss: 654.7820 - loglik: -6.5292e+02 - logprior: -3.5261e-01
Epoch 6/10
39/39 - 34s - loss: 654.3810 - loglik: -6.5279e+02 - logprior: -1.6993e-01
Epoch 7/10
39/39 - 33s - loss: 654.0532 - loglik: -6.5273e+02 - logprior: 0.0200
Epoch 8/10
39/39 - 32s - loss: 653.5764 - loglik: -6.5246e+02 - logprior: 0.1885
Epoch 9/10
39/39 - 31s - loss: 653.7145 - loglik: -6.5284e+02 - logprior: 0.3834
Fitted a model with MAP estimate = -651.9056
Time for alignment: 820.1695
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 793.4957 - loglik: -7.9145e+02 - logprior: -1.5510e+00
Epoch 2/10
39/39 - 24s - loss: 721.3833 - loglik: -7.1872e+02 - logprior: -9.4699e-01
Epoch 3/10
39/39 - 25s - loss: 706.5610 - loglik: -7.0245e+02 - logprior: -1.2831e+00
Epoch 4/10
39/39 - 25s - loss: 700.2970 - loglik: -6.9588e+02 - logprior: -1.3061e+00
Epoch 5/10
39/39 - 25s - loss: 696.3392 - loglik: -6.9207e+02 - logprior: -1.3633e+00
Epoch 6/10
39/39 - 25s - loss: 693.9302 - loglik: -6.9005e+02 - logprior: -1.4115e+00
Epoch 7/10
39/39 - 25s - loss: 692.0098 - loglik: -6.8846e+02 - logprior: -1.4407e+00
Epoch 8/10
39/39 - 26s - loss: 691.3079 - loglik: -6.8805e+02 - logprior: -1.4581e+00
Epoch 9/10
39/39 - 26s - loss: 690.0724 - loglik: -6.8691e+02 - logprior: -1.4883e+00
Epoch 10/10
39/39 - 26s - loss: 688.9513 - loglik: -6.8581e+02 - logprior: -1.4920e+00
Fitted a model with MAP estimate = -686.2566
expansions: [(0, 4), (23, 1), (49, 1), (50, 1), (51, 1), (80, 1), (84, 2), (86, 9), (95, 1), (116, 4), (117, 2), (118, 2), (122, 1), (125, 1), (136, 2), (158, 2), (172, 2), (173, 4), (175, 1), (176, 1), (179, 1), (183, 1), (206, 7), (214, 1)]
discards: [144]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 281 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 681.0732 - loglik: -6.7603e+02 - logprior: -2.9640e+00
Epoch 2/2
39/39 - 34s - loss: 666.0406 - loglik: -6.6187e+02 - logprior: -1.2866e+00
Fitted a model with MAP estimate = -659.3148
expansions: [(0, 5), (48, 1)]
discards: [144 145 168]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 284 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 37s - loss: 663.5682 - loglik: -6.5745e+02 - logprior: -3.3248e+00
Epoch 2/2
39/39 - 35s - loss: 659.4760 - loglik: -6.5564e+02 - logprior: -1.3485e+00
Fitted a model with MAP estimate = -655.2841
expansions: [(0, 2), (109, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 287 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 39s - loss: 659.6566 - loglik: -6.5414e+02 - logprior: -3.2499e+00
Epoch 2/10
39/39 - 35s - loss: 656.5088 - loglik: -6.5348e+02 - logprior: -1.0481e+00
Epoch 3/10
39/39 - 35s - loss: 655.8528 - loglik: -6.5320e+02 - logprior: -7.7203e-01
Epoch 4/10
39/39 - 35s - loss: 655.2314 - loglik: -6.5301e+02 - logprior: -5.7430e-01
Epoch 5/10
39/39 - 35s - loss: 654.6185 - loglik: -6.5270e+02 - logprior: -3.9636e-01
Epoch 6/10
39/39 - 35s - loss: 654.3871 - loglik: -6.5277e+02 - logprior: -1.9589e-01
Epoch 7/10
39/39 - 34s - loss: 653.8434 - loglik: -6.5248e+02 - logprior: -6.9581e-03
Epoch 8/10
39/39 - 34s - loss: 654.0544 - loglik: -6.5294e+02 - logprior: 0.1872
Fitted a model with MAP estimate = -652.1108
Time for alignment: 822.5680
Computed alignments with likelihoods: ['-654.3344', '-651.9056', '-652.1108']
Best model has likelihood: -651.9056
time for generating output: 0.4338
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00150.projection.fasta
SP score = 0.558584729981378
Training of 3 independent models on file PF13561.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f38ac222220>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3bed4f7bb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f31ed4839d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f31f5b0b670>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f31f5b0b1c0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f31ed8cc7c0>, <__main__.SimpleDirichletPrior object at 0x7f31fda1b340>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f31ece234c0>

Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 574.2178 - loglik: -5.7181e+02 - logprior: -1.9648e+00
Epoch 2/10
39/39 - 14s - loss: 477.6946 - loglik: -4.7431e+02 - logprior: -1.9135e+00
Epoch 3/10
39/39 - 14s - loss: 469.4899 - loglik: -4.6636e+02 - logprior: -1.9232e+00
Epoch 4/10
39/39 - 14s - loss: 466.8799 - loglik: -4.6403e+02 - logprior: -1.8924e+00
Epoch 5/10
39/39 - 14s - loss: 465.8895 - loglik: -4.6310e+02 - logprior: -1.8689e+00
Epoch 6/10
39/39 - 14s - loss: 465.2112 - loglik: -4.6243e+02 - logprior: -1.8891e+00
Epoch 7/10
39/39 - 13s - loss: 464.7342 - loglik: -4.6197e+02 - logprior: -1.8944e+00
Epoch 8/10
39/39 - 13s - loss: 464.4798 - loglik: -4.6174e+02 - logprior: -1.8996e+00
Epoch 9/10
39/39 - 13s - loss: 463.9899 - loglik: -4.6130e+02 - logprior: -1.9010e+00
Epoch 10/10
39/39 - 13s - loss: 464.0984 - loglik: -4.6145e+02 - logprior: -1.9097e+00
Fitted a model with MAP estimate = -460.0618
expansions: [(12, 1), (15, 1), (18, 1), (23, 2), (24, 1), (29, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 2), (37, 1), (42, 1), (45, 1), (46, 1), (48, 1), (66, 1), (67, 2), (69, 1), (70, 1), (83, 1), (86, 1), (87, 1), (88, 1), (93, 1), (96, 1), (108, 1), (109, 2), (111, 2), (112, 1), (121, 1), (149, 5), (150, 3), (153, 1), (154, 1), (155, 1), (156, 1), (163, 1), (168, 1), (176, 1), (183, 1), (184, 1), (188, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 452.4590 - loglik: -4.4864e+02 - logprior: -3.0205e+00
Epoch 2/2
39/39 - 17s - loss: 441.8945 - loglik: -4.3910e+02 - logprior: -1.6996e+00
Fitted a model with MAP estimate = -435.9947
expansions: [(3, 1)]
discards: [  0  26  85 139 142 187]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 237 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 443.2960 - loglik: -4.3926e+02 - logprior: -2.8623e+00
Epoch 2/2
39/39 - 16s - loss: 440.5396 - loglik: -4.3813e+02 - logprior: -1.1963e+00
Fitted a model with MAP estimate = -435.1490
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 237 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 439.8684 - loglik: -4.3592e+02 - logprior: -2.7592e+00
Epoch 2/10
39/39 - 16s - loss: 437.8129 - loglik: -4.3513e+02 - logprior: -1.4846e+00
Epoch 3/10
39/39 - 16s - loss: 436.0179 - loglik: -4.3425e+02 - logprior: -5.8747e-01
Epoch 4/10
39/39 - 17s - loss: 435.1409 - loglik: -4.3360e+02 - logprior: -4.8521e-01
Epoch 5/10
39/39 - 17s - loss: 435.0436 - loglik: -4.3368e+02 - logprior: -3.7011e-01
Epoch 6/10
39/39 - 17s - loss: 434.9160 - loglik: -4.3371e+02 - logprior: -2.5979e-01
Epoch 7/10
39/39 - 17s - loss: 434.2598 - loglik: -4.3320e+02 - logprior: -1.4764e-01
Epoch 8/10
39/39 - 17s - loss: 434.1963 - loglik: -4.3327e+02 - logprior: -3.7335e-02
Epoch 9/10
39/39 - 17s - loss: 433.8951 - loglik: -4.3306e+02 - logprior: 0.0681
Epoch 10/10
39/39 - 17s - loss: 433.6241 - loglik: -4.3289e+02 - logprior: 0.1781
Fitted a model with MAP estimate = -432.1986
Time for alignment: 469.8816
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 572.9778 - loglik: -5.7061e+02 - logprior: -1.9636e+00
Epoch 2/10
39/39 - 14s - loss: 477.9548 - loglik: -4.7484e+02 - logprior: -1.9980e+00
Epoch 3/10
39/39 - 14s - loss: 469.9145 - loglik: -4.6666e+02 - logprior: -2.0736e+00
Epoch 4/10
39/39 - 14s - loss: 467.9401 - loglik: -4.6488e+02 - logprior: -1.9939e+00
Epoch 5/10
39/39 - 14s - loss: 466.9493 - loglik: -4.6396e+02 - logprior: -1.9819e+00
Epoch 6/10
39/39 - 14s - loss: 466.1437 - loglik: -4.6321e+02 - logprior: -1.9840e+00
Epoch 7/10
39/39 - 14s - loss: 465.8386 - loglik: -4.6293e+02 - logprior: -1.9901e+00
Epoch 8/10
39/39 - 14s - loss: 465.2787 - loglik: -4.6242e+02 - logprior: -2.0037e+00
Epoch 9/10
39/39 - 14s - loss: 465.2468 - loglik: -4.6244e+02 - logprior: -2.0055e+00
Epoch 10/10
39/39 - 14s - loss: 464.8080 - loglik: -4.6205e+02 - logprior: -2.0131e+00
Fitted a model with MAP estimate = -461.0998
expansions: [(12, 1), (15, 1), (22, 1), (23, 2), (24, 1), (27, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (48, 1), (50, 1), (66, 1), (67, 2), (71, 1), (72, 1), (83, 1), (88, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (129, 1), (133, 1), (149, 2), (150, 1), (151, 4), (153, 1), (155, 1), (156, 1), (157, 2), (163, 1), (168, 1), (176, 1), (182, 2), (183, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 452.2089 - loglik: -4.4835e+02 - logprior: -3.0391e+00
Epoch 2/2
39/39 - 18s - loss: 441.9562 - loglik: -4.3914e+02 - logprior: -1.7163e+00
Fitted a model with MAP estimate = -435.9592
expansions: [(3, 1)]
discards: [  0  26  85 139 142 190]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 237 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 443.3669 - loglik: -4.3927e+02 - logprior: -2.8741e+00
Epoch 2/2
39/39 - 18s - loss: 440.5586 - loglik: -4.3818e+02 - logprior: -1.1642e+00
Fitted a model with MAP estimate = -435.2132
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 237 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 439.8880 - loglik: -4.3595e+02 - logprior: -2.7418e+00
Epoch 2/10
39/39 - 18s - loss: 438.0824 - loglik: -4.3541e+02 - logprior: -1.4855e+00
Epoch 3/10
39/39 - 18s - loss: 436.3520 - loglik: -4.3460e+02 - logprior: -5.8431e-01
Epoch 4/10
39/39 - 18s - loss: 435.5474 - loglik: -4.3406e+02 - logprior: -4.6058e-01
Epoch 5/10
39/39 - 18s - loss: 434.8716 - loglik: -4.3357e+02 - logprior: -3.5044e-01
Epoch 6/10
39/39 - 18s - loss: 434.8959 - loglik: -4.3375e+02 - logprior: -2.3331e-01
Fitted a model with MAP estimate = -433.5249
Time for alignment: 420.9802
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 574.6252 - loglik: -5.7223e+02 - logprior: -1.9613e+00
Epoch 2/10
39/39 - 14s - loss: 478.5984 - loglik: -4.7545e+02 - logprior: -1.9834e+00
Epoch 3/10
39/39 - 14s - loss: 470.6948 - loglik: -4.6745e+02 - logprior: -2.0376e+00
Epoch 4/10
39/39 - 14s - loss: 468.8439 - loglik: -4.6581e+02 - logprior: -1.9832e+00
Epoch 5/10
39/39 - 14s - loss: 467.7143 - loglik: -4.6477e+02 - logprior: -1.9789e+00
Epoch 6/10
39/39 - 14s - loss: 466.8112 - loglik: -4.6391e+02 - logprior: -1.9880e+00
Epoch 7/10
39/39 - 14s - loss: 466.6258 - loglik: -4.6373e+02 - logprior: -2.0021e+00
Epoch 8/10
39/39 - 14s - loss: 466.1980 - loglik: -4.6333e+02 - logprior: -2.0061e+00
Epoch 9/10
39/39 - 14s - loss: 465.5656 - loglik: -4.6273e+02 - logprior: -2.0107e+00
Epoch 10/10
39/39 - 14s - loss: 465.5853 - loglik: -4.6279e+02 - logprior: -2.0187e+00
Fitted a model with MAP estimate = -461.6931
expansions: [(12, 1), (15, 1), (18, 1), (23, 2), (24, 1), (29, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (46, 1), (48, 1), (49, 1), (52, 1), (66, 1), (67, 2), (71, 1), (72, 1), (83, 1), (88, 1), (89, 1), (92, 1), (95, 1), (108, 1), (110, 2), (111, 2), (112, 1), (121, 1), (122, 1), (149, 2), (150, 1), (151, 4), (153, 3), (154, 1), (155, 1), (156, 1), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 452.5320 - loglik: -4.4866e+02 - logprior: -3.0305e+00
Epoch 2/2
39/39 - 17s - loss: 441.8566 - loglik: -4.3896e+02 - logprior: -1.7296e+00
Fitted a model with MAP estimate = -435.8857
expansions: [(3, 1)]
discards: [  0  26  85 139 141 190 191 192 195]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 443.6999 - loglik: -4.3958e+02 - logprior: -2.8818e+00
Epoch 2/2
39/39 - 16s - loss: 440.7290 - loglik: -4.3832e+02 - logprior: -1.1765e+00
Fitted a model with MAP estimate = -435.4499
expansions: [(3, 1), (186, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 238 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 440.0423 - loglik: -4.3610e+02 - logprior: -2.7306e+00
Epoch 2/10
39/39 - 16s - loss: 437.9255 - loglik: -4.3523e+02 - logprior: -1.4886e+00
Epoch 3/10
39/39 - 17s - loss: 435.6234 - loglik: -4.3386e+02 - logprior: -5.9022e-01
Epoch 4/10
39/39 - 17s - loss: 435.5956 - loglik: -4.3407e+02 - logprior: -4.8813e-01
Epoch 5/10
39/39 - 17s - loss: 434.8959 - loglik: -4.3354e+02 - logprior: -3.8175e-01
Epoch 6/10
39/39 - 17s - loss: 434.2865 - loglik: -4.3311e+02 - logprior: -2.6338e-01
Epoch 7/10
39/39 - 17s - loss: 434.3828 - loglik: -4.3334e+02 - logprior: -1.5575e-01
Fitted a model with MAP estimate = -432.9600
Time for alignment: 423.4519
Computed alignments with likelihoods: ['-432.1986', '-433.5249', '-432.9600']
Best model has likelihood: -432.1986
time for generating output: 0.3467
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13561.projection.fasta
SP score = 0.7240687018952733
Training of 3 independent models on file PF05746.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f399043c670>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f399043c7c0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f399043c3d0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3990388c70>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39903884c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990388610>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f39903885e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413850>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413040>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413dc0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904134f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904131c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413790>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f39904132b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413a60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413430>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f39904135e0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f31fc1377c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3205600400>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f31f56eb4f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f31fcaf1e80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f39903d1670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3990413370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f31fcaf1cd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3bece8c4c0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3aa65170d0>, <function make_default_emission_matrix at 0x7f3aa65170d0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f32190fae20>, <__main__.SimpleDirichletPrior object at 0x7f32180f4190>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f3bec16e700>

Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 320.8904 - loglik: -3.1765e+02 - logprior: -3.1419e+00
Epoch 2/10
19/19 - 3s - loss: 275.1195 - loglik: -2.7309e+02 - logprior: -1.4386e+00
Epoch 3/10
19/19 - 3s - loss: 251.2154 - loglik: -2.4848e+02 - logprior: -1.8931e+00
Epoch 4/10
19/19 - 3s - loss: 245.3439 - loglik: -2.4279e+02 - logprior: -1.8219e+00
Epoch 5/10
19/19 - 3s - loss: 243.2680 - loglik: -2.4095e+02 - logprior: -1.8036e+00
Epoch 6/10
19/19 - 3s - loss: 242.3891 - loglik: -2.4019e+02 - logprior: -1.7698e+00
Epoch 7/10
19/19 - 3s - loss: 241.7027 - loglik: -2.3959e+02 - logprior: -1.7251e+00
Epoch 8/10
19/19 - 3s - loss: 241.8504 - loglik: -2.3978e+02 - logprior: -1.7155e+00
Fitted a model with MAP estimate = -240.9464
expansions: [(17, 2), (19, 1), (20, 3), (22, 1), (23, 1), (24, 1), (28, 2), (32, 2), (41, 1), (44, 1), (48, 1), (50, 1), (56, 1), (63, 1), (64, 1), (65, 2), (70, 1), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 244.2242 - loglik: -2.3973e+02 - logprior: -4.1398e+00
Epoch 2/2
19/19 - 4s - loss: 233.8482 - loglik: -2.3127e+02 - logprior: -2.2164e+00
Fitted a model with MAP estimate = -231.8631
expansions: [(0, 2), (21, 1)]
discards: [ 0 36 42 85]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 232.0781 - loglik: -2.2858e+02 - logprior: -3.0796e+00
Epoch 2/2
19/19 - 4s - loss: 228.9043 - loglik: -2.2723e+02 - logprior: -1.2380e+00
Fitted a model with MAP estimate = -227.7606
expansions: []
discards: [ 0 24]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 119 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 234.1039 - loglik: -2.2981e+02 - logprior: -3.8508e+00
Epoch 2/10
19/19 - 4s - loss: 230.2675 - loglik: -2.2842e+02 - logprior: -1.4379e+00
Epoch 3/10
19/19 - 4s - loss: 228.9473 - loglik: -2.2727e+02 - logprior: -1.2518e+00
Epoch 4/10
19/19 - 4s - loss: 228.5643 - loglik: -2.2697e+02 - logprior: -1.1904e+00
Epoch 5/10
19/19 - 4s - loss: 228.5611 - loglik: -2.2700e+02 - logprior: -1.1634e+00
Epoch 6/10
19/19 - 4s - loss: 228.2484 - loglik: -2.2674e+02 - logprior: -1.1198e+00
Epoch 7/10
19/19 - 4s - loss: 228.4438 - loglik: -2.2697e+02 - logprior: -1.0922e+00
Fitted a model with MAP estimate = -227.6704
Time for alignment: 106.5620
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 320.8280 - loglik: -3.1759e+02 - logprior: -3.1450e+00
Epoch 2/10
19/19 - 3s - loss: 276.1880 - loglik: -2.7409e+02 - logprior: -1.4372e+00
Epoch 3/10
19/19 - 3s - loss: 252.8094 - loglik: -2.4982e+02 - logprior: -1.8428e+00
Epoch 4/10
19/19 - 3s - loss: 247.1395 - loglik: -2.4451e+02 - logprior: -1.8053e+00
Epoch 5/10
19/19 - 3s - loss: 245.3158 - loglik: -2.4288e+02 - logprior: -1.7947e+00
Epoch 6/10
19/19 - 3s - loss: 244.4466 - loglik: -2.4224e+02 - logprior: -1.7670e+00
Epoch 7/10
19/19 - 3s - loss: 243.6056 - loglik: -2.4149e+02 - logprior: -1.7371e+00
Epoch 8/10
19/19 - 3s - loss: 243.4924 - loglik: -2.4141e+02 - logprior: -1.7192e+00
Epoch 9/10
19/19 - 3s - loss: 243.2033 - loglik: -2.4114e+02 - logprior: -1.7060e+00
Epoch 10/10
19/19 - 3s - loss: 242.9515 - loglik: -2.4091e+02 - logprior: -1.7017e+00
Fitted a model with MAP estimate = -242.5427
expansions: [(17, 2), (18, 4), (19, 2), (22, 1), (24, 1), (28, 2), (32, 2), (34, 1), (40, 1), (44, 1), (48, 1), (56, 1), (62, 3), (63, 1), (65, 2), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 244.9973 - loglik: -2.4047e+02 - logprior: -4.1942e+00
Epoch 2/2
19/19 - 4s - loss: 234.1670 - loglik: -2.3154e+02 - logprior: -2.2706e+00
Fitted a model with MAP estimate = -231.5204
expansions: [(0, 2)]
discards: [ 0 37 43 81 87]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 231.6619 - loglik: -2.2817e+02 - logprior: -3.0749e+00
Epoch 2/2
19/19 - 4s - loss: 228.5932 - loglik: -2.2692e+02 - logprior: -1.2442e+00
Fitted a model with MAP estimate = -227.3245
expansions: []
discards: [ 0 17 18 19 25]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 116 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 235.3399 - loglik: -2.3103e+02 - logprior: -3.8562e+00
Epoch 2/10
19/19 - 4s - loss: 230.9704 - loglik: -2.2911e+02 - logprior: -1.4317e+00
Epoch 3/10
19/19 - 4s - loss: 229.5296 - loglik: -2.2782e+02 - logprior: -1.2731e+00
Epoch 4/10
19/19 - 4s - loss: 229.3437 - loglik: -2.2772e+02 - logprior: -1.2045e+00
Epoch 5/10
19/19 - 4s - loss: 229.0673 - loglik: -2.2750e+02 - logprior: -1.1628e+00
Epoch 6/10
19/19 - 4s - loss: 228.8003 - loglik: -2.2727e+02 - logprior: -1.1313e+00
Epoch 7/10
19/19 - 4s - loss: 228.9944 - loglik: -2.2749e+02 - logprior: -1.1021e+00
Fitted a model with MAP estimate = -228.4118
Time for alignment: 113.8941
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 320.8773 - loglik: -3.1764e+02 - logprior: -3.1436e+00
Epoch 2/10
19/19 - 3s - loss: 276.0313 - loglik: -2.7392e+02 - logprior: -1.4505e+00
Epoch 3/10
19/19 - 3s - loss: 252.7956 - loglik: -2.4981e+02 - logprior: -1.8397e+00
Epoch 4/10
19/19 - 3s - loss: 246.4688 - loglik: -2.4387e+02 - logprior: -1.8090e+00
Epoch 5/10
19/19 - 3s - loss: 244.1158 - loglik: -2.4177e+02 - logprior: -1.7957e+00
Epoch 6/10
19/19 - 3s - loss: 243.5483 - loglik: -2.4134e+02 - logprior: -1.7644e+00
Epoch 7/10
19/19 - 3s - loss: 242.4621 - loglik: -2.4029e+02 - logprior: -1.7540e+00
Epoch 8/10
19/19 - 3s - loss: 242.0697 - loglik: -2.3994e+02 - logprior: -1.7407e+00
Epoch 9/10
19/19 - 3s - loss: 242.1011 - loglik: -2.4001e+02 - logprior: -1.7304e+00
Fitted a model with MAP estimate = -241.3827
expansions: [(17, 2), (20, 4), (27, 1), (28, 2), (32, 2), (34, 1), (40, 1), (43, 1), (48, 1), (56, 1), (59, 1), (62, 1), (63, 1), (65, 2), (71, 1), (76, 1), (77, 1), (79, 1), (81, 1), (84, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 244.3211 - loglik: -2.3980e+02 - logprior: -4.1570e+00
Epoch 2/2
19/19 - 4s - loss: 234.0368 - loglik: -2.3145e+02 - logprior: -2.2249e+00
Fitted a model with MAP estimate = -231.9659
expansions: [(0, 2), (21, 2)]
discards: [ 0 34 40 83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 232.0770 - loglik: -2.2860e+02 - logprior: -3.0699e+00
Epoch 2/2
19/19 - 4s - loss: 229.0668 - loglik: -2.2741e+02 - logprior: -1.2320e+00
Fitted a model with MAP estimate = -227.8810
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 119 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 233.8282 - loglik: -2.2952e+02 - logprior: -3.8717e+00
Epoch 2/10
19/19 - 4s - loss: 229.8952 - loglik: -2.2806e+02 - logprior: -1.4297e+00
Epoch 3/10
19/19 - 4s - loss: 228.6749 - loglik: -2.2700e+02 - logprior: -1.2542e+00
Epoch 4/10
19/19 - 4s - loss: 228.4946 - loglik: -2.2689e+02 - logprior: -1.1910e+00
Epoch 5/10
19/19 - 4s - loss: 227.9814 - loglik: -2.2644e+02 - logprior: -1.1473e+00
Epoch 6/10
19/19 - 4s - loss: 227.9950 - loglik: -2.2650e+02 - logprior: -1.1057e+00
Fitted a model with MAP estimate = -227.5156
Time for alignment: 106.3350
Computed alignments with likelihoods: ['-227.6704', '-227.3245', '-227.5156']
Best model has likelihood: -227.3245
time for generating output: 0.1573
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF05746.projection.fasta
SP score = 0.8504334121355398
